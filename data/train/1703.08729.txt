{
  "article_text": [
    "a successful approach to statistical estimation and statistical learning suggests to estimate the object of interest by solving an optimization problem , for instance motivated by maximum likelihood , or empirical risk minimization . in modern applications ,",
    "the unknown object is often combinatorial , e.g. a sparse vector in high - dimensional regression or a partition in clustering . in these cases , the resulting optimization problem is computationally intractable and convex relaxations have been a method of choice for obtaining tractable and yet statistically efficient estimators .    in this paper",
    "we consider the following specific semidefinite program @xmath3 , \\\\ & & & x \\succeq 0\\ , , \\end{aligned}\\ ] ] as well as some of its generalizations .",
    "this sdp famously arises as a convex relaxation of the maxcut problem and want to partition the vertices in two sets as to maximize the number of edges across the partition .",
    "] , whereby the matrix @xmath4 is the opposite of the adjacency matrix of the graph to be cut . in a seminal paper , goemans and williamson @xcite proved that this sdp provides a @xmath5 approximation of the combinatorial problem . under the unique games conjecture ,",
    "this approximation factor is optimal for polynomial time algorithms @xcite .",
    "more recently , sdps of this form ( see below for generalizations ) have been studied in the context of group synchronization and community detection problems .",
    "an incomplete list of references includes @xcite . in community detection , we try to partition the vertices of a graph into tightly connected communities under a statistical model for the edges .",
    "synchronization aims at estimating @xmath6 elements @xmath7 in a group @xmath8 , from the pairwise noisy measurement of the group differences @xmath9 .",
    "examples include @xmath10 synchronization in which @xmath11 ( the group with elements @xmath12 and usual multiplication ) , angular synchronization in which @xmath13 ( the multiplicative group of complex numbers of modulo one ) , and @xmath14 synchronization in which we need to estimate @xmath6 rotations @xmath15 from the special orthogonal group . in this paper",
    ", we will focus on @xmath10 synchronization and @xmath14 synchronization .",
    "although sdps can be solved to arbitrary precision in polynomial time @xcite , generic solvers do not scale well to large instances . in order to address the scalability problem",
    ", @xcite proposed to reduce the problem dimensions by imposing the rank constraint @xmath16 .",
    "this constraint can be solved by setting @xmath17 where @xmath18 . in the case of ( [ sdp ] ) , we obtain the following non - convex problem , with decision variable @xmath19 : @xmath20^\\st \\in \\mathbb r^{n \\times k},\\\\ & & & \\vert \\sigma_{i } \\vert_2 = 1 , \\quad i \\in [ n ]",
    ". \\\\ \\end{aligned}\\ ] ] provided that @xmath21 , the solution of ( [ sdp ] ) corresponds to the global maximum of ( [ ncvxsdp ] ) @xcite .",
    "recently , @xcite proved that , as long as @xmath22 , for almost all matrices @xmath4 , the problem ( [ ncvxsdp ] ) has a unique local maximum which is also the global maximum .",
    "this paper proposed to use the riemannian trust - region method to solve the non - convex sdp problem , and provided computational complexity guarantees on the resulting algorithm .",
    "while the theory of @xcite suggests the choice @xmath23 , it has been observed empirically that setting @xmath24 yields excellent solutions and scales well to large scale applications @xcite . in order to explain this phenomenon , @xcite considered the @xmath10 synchronization problem with @xmath25 , and established theoretical guarantees for the local maxima , provided the noise level is small enough . a different point of view",
    "was taken in a recent unpublished technical note @xcite , which proposed a grothendieck - type inequality for the local maxima of ( [ ncvxsdp ] ) . in this paper",
    "we continue and develop the preliminary work in @xcite , to obtain explicit computational guarantees for the non - convex approach with rank constraint @xmath24 .",
    "as mentioned above , we extend our analysis beyond the maxcut type problem ( [ ncvxsdp ] ) to treat an optimization problem motivated by @xmath14 synchronization .",
    "@xmath14 synchronization ( with @xmath26 ) has applications to computer vision @xcite and cryo - electron microscopy ( cryo - em ) @xcite",
    ". a natural sdp relaxation of the maximum likelihood estimator is given by the problem @xmath27 , \\\\ & & & x \\succeq 0,\\\\ \\end{aligned}\\ ] ] with decision variable @xmath28 .",
    "here @xmath29 , @xmath30 are matrices with @xmath31 blocks denoted by @xmath32 , @xmath33 .",
    "this semidefinite program is also known as orthogonal - cut sdp . in the context of @xmath14 synchronization",
    ", @xmath34 is a noisy measurement of the pairwise group differences @xmath35 where @xmath36 .    by imposing the rank constraint @xmath16",
    ", we obtain a non - convex analogue of ( [ oc - sdp ] ) , namely : @xmath37^{\\st } \\in { \\mathbb{r}}^{n\\times k},\\\\ & & & \\sigma_i^\\st \\sigma_i = \\id_d , \\quad i \\in [ m ] .",
    "\\end{aligned}\\ ] ] here the decision variables are matrices @xmath38 .    according to the result in @xcite , as long as @xmath39 , the global maximum of the problem ( [ ncvx - oc - sdp ] ) coincides with the maximum of the problem ( [ oc - sdp ] ) . as proved in @xcite , with the same value of @xmath1 for almost all matrices @xmath4",
    ", the non - convex problem has no local maximum other than the global maximum .",
    "@xcite proposed to choose the rank @xmath1 adaptively : as @xmath1 is not large enough , increase @xmath1 to find a better solution .",
    "however , none of these works considers @xmath24 , which is the focus of the present paper ( under the assumption that @xmath40 is of order one as well ) .      a main result of our paper is a grothendieck - type inequality that generalizes and strengthens the preliminary technical result of @xcite .",
    "namely , we prove that for any @xmath41-approximate concave point @xmath19 of the rank-@xmath1 non - convex sdp ( [ ncvxsdp ] ) , we have @xmath42 where @xmath43 denotes the maximum value of the problem ( [ sdp ] ) and @xmath44 is the objective function in ( [ ncvxsdp ] ) . an @xmath41-approximate concave point is a point at which the eigenvalues of the hessian of @xmath45 are upper bounded by @xmath41 ( see below for formal definitions ) .",
    "surprisingly , this result connects a second order local property , namely the highest local curvature of the cost function , to its global position .",
    "in particular , all the local maxima ( corresponding to @xmath46 ) are within a @xmath47-gap of the sdp value .",
    "namely , for any local maximizer @xmath48 , we have @xmath49 all the points outside this gap , with an @xmath50-margin have a direction of positive curvature of at least size @xmath41 .",
    "figure [ fig : landscape ] illustrates the landscape of the rank-@xmath1 non - convex maxcut sdp problem ( [ ncvxsdp ] ) .",
    "we show that this structure implies global convergence rates for approximately solving ( [ ncvxsdp ] ) .",
    "we study the riemannian trust - region method in theorem [ thm : trustregion ] .",
    "in particular , we show that this algorithm with any initialization returns a @xmath51 approximation of the maxcut of a random @xmath40-regular graph in @xmath52 iterations , cf .",
    "theorem [ thm : maxcut ] .",
    "non - convex sdp , scaledwidth=80.0% ]    in the case of @xmath10 synchronization , we show that for any signal - to - noise ratio @xmath53 , all the local maxima of the rank-@xmath1 non - convex sdp correlate non - trivially with the ground truth when @xmath54 ( theorem [ thm : z2sync1 ] ) . furthermore , theorem [ thm : correlationlimit ] provides a lower bound on the correlation between local maxima and the ground truth that converges to one when @xmath55 goes to infinity .",
    "these results improve over the earlier ones of @xcite , by establishing the tight phase transition location , and the correct qualitative behavior .",
    "we extend these results to the two - groups symmetric stochastic block model .    for @xmath14 synchronization ,",
    "we consider the problem ( [ ncvx - oc - sdp ] ) and generalize our main grothendieck - type inequality to this case , cf .",
    "theorem [ thm : sod_grothendieck ] .",
    "namely , for any @xmath41-approximate concave point @xmath19 of the rank-@xmath1 non - convex orthogonal - cut sdp ( [ ncvx - oc - sdp ] ) , we have @xmath56 where @xmath57 , @xmath58 denotes the maximum value of the problem ( [ oc - sdp ] ) and @xmath44 is the objective function in ( [ ncvx - oc - sdp ] ) .",
    "we expect that the statistical analysis of local maxima , as well as the analysis of optimization algorithms , should extend to this case as well , but we leave this to future work .      given a matrix @xmath59 , we write @xmath60 for its operator @xmath61-norm , @xmath62 for its operator @xmath63-norm ( largest singular value ) , and @xmath64 for its frobenius norm . for two matrices @xmath65 , we write @xmath66 for the inner product associated to the frobenius norm @xmath67",
    ". in particular for two vectors @xmath68 , @xmath69 corresponds to the inner product of the vectors @xmath70 and @xmath71 associated to the euclidean norm on @xmath72 .",
    "we denote by @xmath73 the matrix obtained from @xmath74 by setting to zero all the entries outside the diagonal .    given a real symmetric matrix @xmath75 , we write @xmath43 for value of the sdp problem ( [ sdp ] ) .",
    "that is , @xmath76 \\}.\\end{aligned}\\ ] ] optimization is performed over the convex set of positive - semidefinite matrices with diagonal entries equal to one , also known as the _",
    "elliptope_. we write @xmath77 for the length of the range of the sdp with data @xmath4 ( noticing that for every matrix @xmath28 in the elliptope , we have @xmath78 ) .    for the rank-@xmath1 non - convex sdp problem ( [ ncvxsdp ] )",
    ", we define the manifold @xmath79 as @xmath80 where @xmath81 is the unit sphere in @xmath82 .",
    "given a real symmetric matrix @xmath75 , for @xmath83 , we write @xmath84 the objective function of the rank-@xmath1 non - convex sdp ( [ ncvxsdp ] ) .",
    "our optimization algorithm makes use of the riemannian gradient and the hessian of the function @xmath85 .",
    "we anticipate their formulas here , deferring to section [ sec : geometry ] for further details . defining @xmath86 ,",
    "the gradient is given by : @xmath87 the hessian is uniquely defined by the following holding for all @xmath88 in the tangent space @xmath89 : @xmath90 \\rangle = 2 \\langle v , ( a - \\lambda ) u \\rangle,\\end{aligned}\\ ] ]",
    "first we define the notion of approximate concave point of a function @xmath85 on a manifold @xmath91 .",
    "let @xmath85 be a twice differentiable function on a riemannian manifold @xmath91 .",
    "we say @xmath92 is an @xmath41-approximate concave point of @xmath85 on @xmath91 , if @xmath19 satisfies @xmath93 \\rangle \\leq { \\varepsilon}\\langle u , u\\rangle , \\quad \\forall u\\in t_{\\sigma } \\cm,\\ ] ] where @xmath94 denotes the riemannian ( intrinsic ) hessian of @xmath85 at point @xmath19 , @xmath95 is the tangent space , and @xmath96 is the scalar product on @xmath95 .",
    "note that an approximate concave point may not be a stationary point , or may not even be an approximate stationary point . both local maximizers and saddles with largest eigenvalue of the hessian close to zero are approximate concave points .",
    "the classical grothendieck inequality relates the global maximum of a non - convex optimization problem to the maximum of its sdp relaxation @xcite .",
    "our main tool is instead an inequality that applies to all approximate concave ponts in the non - convex problem .",
    "[ thm : apprxconcavepoint ] for any @xmath41-approximate concave point @xmath83 of the rank-@xmath1 non - convex problem ( [ ncvxsdp ] ) , we have @xmath97      we can use the structural information in theorem [ thm : apprxconcavepoint ] , to develop an algorithm that approximately solves the problem ( [ ncvxsdp ] ) , and hence the maxcut sdp ( [ sdp ] ) .",
    "the algorithm we propose is a variant of the riemannian trust - region algorithm .",
    "the riemannian trust - region algorithm ( rtr ) @xcite is a generalization of the trust - region algorithm to manifolds . to maximize the objective function @xmath98 on the manifold @xmath91 , rtr proceeds as follows : at each step , we find a direction @xmath99 that maximizes the quadratic approximation of @xmath98 over a ball of small radius @xmath100 @xmath101\\rangle , \\quad   \\xi \\in t_{\\sigma } \\cm\\ , , \\;\\ ;   \\vert \\xi \\vert \\leq \\eta_{\\sigma}\\big\\ } , \\ ] ] where @xmath102 is the manifold gradient of @xmath98 , and the radius @xmath100 is chosen to ensure that the higher order terms remain small .",
    "the next iterate @xmath103 is obtained by projecting @xmath104 back onto the manifold .    solving the trust - region problem ( [ rtr - update ] )",
    "exactly is computationally expensive . in order to obtain a faster algorithm",
    ", we adopt two variants in the rtr algorithm . first , if the gradient of @xmath98 at the current estimate @xmath105 is sufficiently large , we only use gradient information to determine the new direction : we call this a _ gradient - step _ ; if the gradient is small ( i.e. we are at an approximately stationary point ) , we try to maximize uniquely the hessian contribution : we call this an _ eigen - step_. second , in an eigen - step , we only approximately maximize the hessian contribution .",
    "let us emphasize that these two variants are commonly used and we do not claim they are novel .    for the non - convex maxcut sdp problem ( [ ncvxsdp ] )",
    ", we describe the algorithm concretely as follows . in each step , first we find a direction @xmath106 using the direction - finding routine outlined below and vectors @xmath107 are represented by matrices @xmath108 and hence the norm on @xmath109 is identified with the frobenius norm @xmath110 . ] .",
    "ll +   +   +   +   + 1 : & compute @xmath111 ; + 2 : & if @xmath112 + 3 : & return @xmath113 ; + 4 : & else + 5 : & use power method to construct a direction @xmath114 such that + & @xmath115 , @xmath116 \\rangle \\geq \\lambda_{\\max}(\\thess \\func(\\sigma^t))/2 $ ] , + & and @xmath117 ; return @xmath106 .",
    "+ 6 : & end +   +    given this direction @xmath106 , we update our current estimate by @xmath118 with @xmath119 an appropriately chosen step size .",
    "we consider two specific implementations for the parameter @xmath120 and the choice of step size :    1 .",
    "take @xmath121 , which means that only eigen - steps are used . in this implementation",
    ", we take the step size @xmath122 \\rangle/(100 \\vert a \\vert_1)$ ] .",
    "2 .   take @xmath123 .",
    "when @xmath124 , we choose the step size @xmath125 .",
    "when @xmath126 , we choose the step size @xmath127 , where @xmath128 \\rangle$ ] .    in each eigen - step , we need to compute a direction @xmath129 such that @xmath130 and @xmath131 \\rangle \\geq \\lambda_{\\max}(\\thess \\func(\\sigma))/2 $ ] .",
    "this can be done using the following power method .",
    "( note that the condition @xmath132 can always be ensured eventually by replacing @xmath106 by @xmath133 . )",
    "ll +   +   +   +   + 1 : & sample a @xmath134 uniformly randomly on @xmath109 with @xmath135 ; + 2 : & for @xmath136 + 3 : & @xmath137 + \\mu_h \\cdot u^{i-1}$ ] ; + 4 : & @xmath138 ; + 5 : & end + 6 : & return @xmath139 .",
    "+   +    the shifting parameter @xmath140 can be chosen as @xmath141 which is an upper bound of @xmath142 .",
    "we take the parameter @xmath143 with a large absolute constant @xmath144 . in practice ,",
    "when choosing the parameter @xmath145 , we do not know @xmath146 for each @xmath19 , but we can replace it by a lower bound , or estimate it using some heuristics . it is a classical result that with high probability the power method with this number of iterations finds a solution @xmath106 with the required curvature @xcite .",
    "[ thm : trustregion ] there exists a universal constant @xmath147 such that , for any matrix @xmath4 and @xmath148 , the fast riemannian trust - region method with step size as described above for each iteration and initialized with any @xmath149 returns a point @xmath150 with @xmath151 within the following number of steps with each implementation    1 .",
    "taking @xmath152 ( i.e. only eigen - steps are used ) , then it is sufficient to run @xmath153 steps .",
    "2 .   taking @xmath123 , then it is sufficient to run @xmath154 steps in which there are @xmath155 eigen - steps and @xmath156 gradient - steps .",
    "the gap @xmath157 in eq .",
    "( [ eq : trustregion ] ) , is due to the fact that theorem [ thm : apprxconcavepoint ] does not rule out the presence of local maxima within an interval @xmath158 from the global maximum .",
    "it is therefore natural to set @xmath159 , to obtain the following corollary .",
    "[ cor : rtrapprox ] there exists a universal constant @xmath147 such that for any matrix @xmath4 , the fast riemannian trust - region method with step size as described above for each iteration and initialized with any @xmath149 returns a point @xmath150 with @xmath160 within the following number of steps with each implementation    1 .",
    "taking @xmath152 , then it is sufficient to run @xmath161 eigen - steps .",
    "2 .   taking @xmath123 , then it is sufficient to run @xmath154 steps in which there are @xmath162 eigen - steps and @xmath156 gradient - steps .    in order to develop some intuition on these complexity bounds ,",
    "let us consider two specific examples .",
    "consider the problem of finding the minimum bisection of a random @xmath40-regular graph @xmath163 , with adjacency matrix @xmath164 .",
    "a natural sdp relaxation is given by the sdp ( [ sdp ] ) with @xmath165 the centered adjacency matrix .",
    "for this choice of @xmath4 , we have @xmath166 , @xmath167 @xcite , @xmath168 and @xmath169 @xcite ( with high probability ) . using implementation @xmath170 ( only eigen - steps ) ,",
    "the bound on the number of iterations in corollary [ cor : rtrapprox ] scales as @xmath171 . in implementation @xmath172 , we choose @xmath173 , and the number of gradient - steps and eigen - steps scale respectively as @xmath174 and @xmath175 . in terms of floating point operations , in each gradient - step ,",
    "the computation of the gradient costs @xmath176 operations ; in each eigen - step , each iteration of the power method costs @xmath176 operations and the number of iterations in each power method scales as @xmath177 .",
    "implementation @xmath172 presents a better scaling .",
    "the total number of floating point operations to find a @xmath178 approximate solution of the minimum bisection sdp of a random @xmath40-regular graph is ( with high probability ) upper bounded by @xmath179 .    as a second example , consider the maxcut problem for a @xmath40-regular graph @xmath163 , with adjacency matrix @xmath164 .",
    "this can be addressed by considering the sdp ( [ sdp ] ) with @xmath180 , and the corresponding non - convex version ( [ ncvxsdp ] ) . as shown in the next section ,",
    "finding a @xmath181-approximate concave point of ( [ ncvxsdp ] ) yields an @xmath182-approximation of the maxcut of @xmath163 .",
    "for this choice of @xmath4 , we have @xmath183 , @xmath184 , and @xmath185 .",
    "therefore , in implementation @xmath170 where all the steps are eigen - step , the number of iterations given by corollary [ cor : rtrapprox ] scales as @xmath186 . in implementation @xmath172 , we choose @xmath187 , and the number of gradient - steps and eigen - steps scale respectively as @xmath188 and @xmath189 . in terms of floating point operations ,",
    "the computational costs of one gradient - step and one eigen - step power iteration are the same ( which are @xmath190 ) as in the example of minimum bisection sdp .",
    "the number of iterations in the power method scales as @xmath191 .",
    "therefore , the two approaches are equivalent .",
    "the total number of floating point operations to find a @xmath182 approximate solution of the maxcut of a @xmath40-regular graph is upper bounded by @xmath192 .",
    "let us emphasize that the complexity bound in theorem [ thm : trustregion ] is not superior to the ones available for some alternative approaches .",
    "there is a vast literatures that studies fast sdp solvers @xcite .",
    "in particular , @xcite give nearly linear - time algorithms to approximate ( [ sdp ] ) .",
    "these algorithms are different from the one studied here , and rely on the multiplicative weight update method @xcite . using sketching techniques ,",
    "their complexity can be further reduced @xcite . however , in practice , the burer - monteiro approach studied here is extremely simple and scales well to large instances @xcite .",
    "empirically , it appears to have better complexity than what is guaranteed by our theorem .",
    "it would be interesting to compare the multiplicative weight update method and the non - convex approach both theoretically and experimentally .",
    "let @xmath193 denote the weighted adjacency matrix of a non - negative weighted graph @xmath163 .",
    "the maxcut of @xmath163 is given by the following integer program @xmath194 we consider the following semidefinite programming relaxation @xmath195 denote by @xmath196 the solution of this sdp .",
    "goemans and williamson @xcite proposed a celebrated rounding scheme using this @xmath196 , which is guaranteed to find an @xmath197-approximate solution to the maxcut problem ( [ eq : maxcut ] ) , where @xmath198}2\\theta/(\\pi(1-\\cos\\theta))$ ] , @xmath199 .",
    "the corresponding rank-@xmath1 non - convex formulation is given by @xmath200\\right\\}\\ , .",
    "\\ ] ] applying theorem [ thm : apprxconcavepoint ] , we obtain the following result .",
    "[ thm : maxcut ] for any @xmath201 , if @xmath48 is a local maximizer of the rank-@xmath1 non - convex sdp problem ( [ eq : ncvxsdp ] ) , then using @xmath48 we can find an @xmath202-approximate solution of the maxcut problem ( [ eq : maxcut ] ) .",
    "if @xmath48 is a @xmath203-approximate concave point , then using @xmath48 we can find an @xmath204-approximate solution of the maxcut problem .",
    "the proof is deferred to section [ sec : proof_thm3 ] .      recall the definition of the gaussian orthogonal ensemble .",
    "we write @xmath206 if @xmath207 is symmetric with @xmath208 independent , with distribution @xmath209 and @xmath210 for @xmath211 .    in the @xmath205 synchronization problem",
    ", we are required to estimate the vector @xmath212 from noisy pairwise measurements @xmath213 where @xmath214 , and @xmath55 is a signal - to - noise ratio .",
    "the random matrix model ( [ spiked ] ) is also known as the ` spiked model ' @xcite or ` deformed wigner matrix ' and has attracted significant attention across statistics and probability theory @xcite .",
    "the maximum likelihood estimator for recovering the labels @xmath215 is given by @xmath216 a natural sdp relaxation of this optimization problem is given once more by ( [ sdp ] ) .",
    "it is known that @xmath2 synchronization undergoes a phase transition at @xmath217 .",
    "for @xmath218 , no statistical estimator @xmath219 achieves scalar product @xmath220 bounded away from @xmath221 as @xmath222 . for @xmath223",
    ", there exists an estimator with @xmath220 bounded away from @xmath221 ( ` better than random guessing ' ) @xcite .",
    "further , for @xmath224 it is not possible to distinguish whether @xmath4 is drawn from the spiked model or @xmath225 with probability of error converging to @xmath221 as @xmath222 .",
    "this is instead possible for @xmath226 .",
    "it was proved in @xcite that the sdp relaxation ( [ sdp ] ) with a suitable rounding scheme achieves the information - theoretic threshold @xmath217 for this problem . in this paper , we prove a similar result for the non - convex problem ( [ ncvxsdp ] ) .",
    "namely , we show that for any signal - to - noise ratio @xmath53 there exists a sufficiently large @xmath1 such that every local maximizer has a non trivial correlation to the ground truth .",
    "below we denote by @xmath227 the set of local maximizers of problem ( [ ncvxsdp ] ) .",
    "[ thm : z2sync1 ] for any @xmath228 , there exists a function @xmath229 , such that for any @xmath230 , with high probability , any local maximizer @xmath19 of the rank-@xmath1 non - convex sdp ( [ ncvxsdp ] ) problem has non - vanishing correlation with the ground truth parameter .",
    "explicitly , there exists @xmath231 such that @xmath232    the proof of this theorem is deferred to section [ sec : proof_thm4 ] .",
    "note that this guarantee is weaker than the one of @xcite , which also presents an explicit rounding scheme to obtain an estimator @xmath233 .",
    "however , we expect that the techniques of @xcite should be generalizable to the present setting .",
    "a simple rounding scheme takes the sign of principal left singular vector of @xmath19 .",
    "we will use this estimator in our numerical experiments in section [ numsim ] .",
    "this theorem can be compared with the one of @xcite which uses @xmath25 but requires @xmath234 . as a side result which improves over @xcite for @xmath235",
    ", we obtain the following lower bound on the correlation for any @xmath236 .",
    "[ thm : correlationlimit ] for any @xmath236 , the following holds almost surely @xmath237    the proof is deferred to section [ sec : proof_thm7 ] .",
    "our lower bound converges to @xmath238 at large @xmath55 , which is the qualitatively correct behavior .",
    "the planted partition problem ( two - groups symmetric stochastic block model ) , is another well - studied statistical estimation problem that can be reduced to ( [ sdp ] ) @xcite .",
    "we write @xmath239 if @xmath163 is a graph over @xmath240 vertices generated as follows ( for simplicity of notation , we assume @xmath6 even ) .",
    "let @xmath215 be a vector of labels that is uniformly random with @xmath241 .",
    "conditional on this partition , edges are drawn independently with @xmath242 we consider the case when @xmath243 and @xmath244 with @xmath245 , and @xmath246 , and denote by @xmath247 the average degree .",
    "a phase transition occurs as the following signal - to - noise parameter increases @xmath248 for @xmath223 there exists an efficient estimator that correlates with the true labels with high probability @xcite , whereas no estimator exists below this threshold , regardless of its computational complexity @xcite .",
    "the maximum likelihood estimator of the vertex labels is given by @xmath249 where @xmath164 is the adjacency matrix of the graph @xmath163 .",
    "this optimization problem can again be attacked using the relaxation ( [ sdp ] ) , where @xmath250 is the scaled and centered adjacency matrix .    in order to emphasize the relationship between this problem and @xmath10 synchronization , we rewrite @xmath251 where @xmath252 has zero mean and @xmath253 are independent with distribution @xmath254 where @xmath255 for @xmath256 and @xmath257 for @xmath258 . in analogy with theorem [ thm : z2sync1 ] , we have the following results on the rank - constrained approach to the two - groups stochastic block model .    [ thm : sbm1 ] consider the rank-@xmath1 non - convex sdp ( [ ncvxsdp ] ) with @xmath259 the centered , scaled adjacency matrix of graph @xmath260 . for any @xmath261 , there exists an average degree @xmath262 and a rank @xmath263 , such that for any @xmath264 and @xmath265 , with high probability , any local maximizer @xmath19 has non - vanishing correlation with the true labels .",
    "explicitly , there exists an @xmath231 such that @xmath266    the proof of this theorem can be found in section [ sec : proof_sbm ] . as mentioned above , efficient algorithms that estimate the hidden partition better than random guessing for @xmath223 and any @xmath267 have been developed , among others , in @xcite .",
    "however , we expect the optimization approach ( [ ncvxsdp ] ) to share some of the robustness properties of semidefinite programming @xcite , while scaling well to large instances .      in @xmath14 synchronization",
    "we would like to estimate @xmath268 matrices @xmath269 in the special orthogonal group @xmath270 from noisy measurements of the pairwise group differences @xmath271 for each pairs @xmath272 \\times [ m]$ ] . here",
    "@xmath273 is a measurement , and @xmath274 is noise .",
    "the maximum likelihood estimator for recovering the group elements @xmath275 solves the problem of the form @xmath276 which can be relaxed to the orthogonal - cut sdp ( [ oc - sdp ] ) .",
    "the non - convex rank - constrained approach fixes @xmath277 , and solves the problem ( [ ncvx - oc - sdp ] ) .",
    "this is a smooth optimization problem with objective function @xmath278 over the manifold @xmath279 , where @xmath280 is the set of @xmath281 orthogonal matrices .",
    "we also denote the maximum value of the sdp ( [ oc - sdp ] ) by @xmath282 \\}.\\end{aligned}\\ ] ]    in analogy with the maxcut sdp , we obtain the following grothendieck - type inequality .    [",
    "thm : sod_grothendieck ] for an @xmath41-approximate concave point @xmath283 of the rank-@xmath1 non - convex orthogonal - cut sdp problem ( [ ncvx - oc - sdp ] ) , we have @xmath284 where @xmath57 .",
    "the proof of this theorem is a generalization of the proof of theorem [ thm : apprxconcavepoint ] , and is deferred to section [ sec : proof_sod ] .",
    "in this section we present the proof of theorem [ thm : apprxconcavepoint ] , while deferring other proofs to section [ sec : proofs ] .",
    "notice that the present proof is simpler and provides a tighter bound with respect to the one of @xcite . before passing to the actual proof",
    ", we make a few remarks about the geometry of optimization on @xmath79 .",
    "the set @xmath285 as defined in ( [ eq : mkdef ] ) is a smooth submanifold of @xmath286 .",
    "we endow @xmath285 with the riemannian geometry induced by the euclidean space @xmath287 . at any point @xmath288 ,",
    "the tangent space is obtained by taking the differential of the equality constraints @xmath289 \\big\\rbrace \\ , .\\ ] ] in words , @xmath290 is the set of matrices @xmath291 such that each row @xmath292 of @xmath70 is orthogonal to the corresponding row @xmath293 of @xmath19 .",
    "equivalently , @xmath89 is the direct product of the tangent spaces of the @xmath6 unit spheres @xmath294 at @xmath295 ,  , @xmath296 .",
    "let @xmath297 be the orthogonal projection operator from @xmath287 onto @xmath89 .",
    "we have @xmath298 where we denoted by @xmath299 the operator on the matrix space that sets all off - diagonal entries to zero .    in problem ( [ ncvxsdp ] )",
    ", we consider the cost function @xmath300 on the submanifold @xmath79 . at @xmath83 ,",
    "we denote @xmath301 and @xmath302 respectively the euclidean gradient in @xmath303 and the riemannian gradient of @xmath98 .",
    "the former is @xmath304 , and the latter is the projection of the first onto the tangent space : @xmath305 we will write @xmath306 and often drop the dependence on @xmath19 for simplicity . at @xmath83 ,",
    "let @xmath307 and @xmath308 be respectively the euclidean and the riemannian hessian of @xmath98 .",
    "the riemannian hessian is a symmetric operator on the tangent space and is given by projecting the directional derivative of the gradient vector field ( we use @xmath309 to denote the directional derivative ) : @xmath310 = \\proj^\\perp \\left ( d \\tgrad \\func ( \\sigma ) [ u ] \\right ) = \\proj^\\perp \\left [ 2(a - \\lambda)u - 2 \\ddiag \\left ( a \\sigma u^\\st + a u \\sigma^\\st \\right ) \\sigma \\right].\\ ] ] in particular , we will use the following identity @xmath311 \\rangle = 2 \\langle v , ( a - \\lambda ) u \\rangle,\\end{aligned}\\ ] ] where we used that the projection operator @xmath297 is self - adjoint and @xmath312 by definition of the tangent space .",
    "we observe that the riemannian hessian has a similar interpretation as in euclidean geometry , namely it provides a second order approximation of the function @xmath85 in a neighborhood of @xmath19 .",
    "let @xmath19 be an @xmath41-approximate concave point of @xmath313 on @xmath79 . using the definition and equation ( [ eq : hess_expression ] )",
    ", we have ( for @xmath314 ) @xmath315 let @xmath316^\\st \\in \\mathbb r^{n\\times n}$ ] be such that @xmath317 is an optimal solution of ( [ sdp ] ) problem .",
    "let @xmath318 be a random matrix with independent entries @xmath319 , and denote by @xmath320 the projection onto the subspace orthogonal to @xmath321 in @xmath82 .",
    "we use @xmath163 to obtain a random projection @xmath322^\\st \\in t_{\\sigma}\\cm_k$ ] . from ( [ eq : apprx ] ) , we have @xmath323 where the expectation is taken over the random matrix @xmath163 .",
    "the left hand side of the last equation gives @xmath324\\\\ = & \\sum_{i , j = 1}^n ( \\lambda   - a)_{ij } \\sum_{s , t=1}^n v_{is } v_{jt } \\delta_{st } \\frac{1}{k } { \\text{tr}}(\\proj_i^\\perp \\proj_j^\\perp)\\\\ = & \\sum_{i , j = 1}^n ( \\lambda   - a)_{ij",
    "} \\langle v_i , v_j \\rangle \\frac{1}{k } { \\text{tr}}\\left ( \\id_k - \\sigma_i \\sigma_i^\\st - \\sigma_j \\sigma_j^\\st + \\sigma_i \\sigma_i^\\st\\sigma_j \\sigma_j^\\st \\right)\\\\ = & \\sum_{i , j = 1}^n ( \\lambda   - a)_{ij } \\langle v_i , v_j \\rangle \\left(1 - \\frac{2}{k } + \\frac{1}{k } \\langle \\sigma_i , \\sigma_j \\rangle ^2 \\right)\\\\ = & \\left(1-\\frac{1}{k}\\right){\\text{tr}}(\\lambda ) - \\left(1-\\frac{2}{k}\\right ) { { \\textup{\\rm sdp}}}(a ) - \\frac{1}{k}\\sum_{i , j=1}^n a_{ij } \\langle v_i , v_j \\rangle \\ , ( \\langle \\sigma_i , \\sigma_j \\rangle)^2 ,   \\ ] ] whereas the right hand side verifies @xmath325    note that @xmath326 .",
    "crucially , if we let @xmath327 , we have @xmath328 and @xmath329 .",
    "thus we have @xmath330 .",
    "therefore , we have @xmath331 rearranging the terms gives the conclusion .",
    ") with @xmath332 : ( a ) @xmath313 as a function of the iteration number for a single realization of the trajectory ; ( b ) @xmath333 as a function of the iteration number . , title=\"fig:\",scaledwidth=95.0% ] ( a )    ) with @xmath332 : ( a ) @xmath313 as a function of the iteration number for a single realization of the trajectory ; ( b ) @xmath333 as a function of the iteration number . ,",
    "title=\"fig:\",scaledwidth=95.0% ] ( b )    in this section we carry out some numerical experiments to illustrate our results .",
    "we also find interesting phenomena which are not captured by our analysis .",
    "although theorem [ thm : trustregion ] provides a complexity bound for the riemannian trust - region method ( rtr ) , we observe that ( projected ) gradient ascent also converges very fast .",
    "that is , gradient ascent rapidly increases the objective function , is not trapped at a saddle point , and converges to a local maximizer eventually . in figure",
    "[ fig : convergence_gd ] , we take @xmath334 , and use projected gradient ascent to solve the optimization problem ( [ ncvxsdp ] ) with a random initialization and fixed step size . figure [ fig : convergence_gd]a shows that the objective function increases rapidly and converges within a small interval from the local maximum ( which is upper bounded by the value @xmath43 ) . also the gap between the value obtained by this procedure and the value @xmath43 decreases rapidly with @xmath1 .",
    "figure [ fig : convergence_gd]b shows that the riemannian gradient decreases very rapidly , but presents some non - monotonicity .",
    "we believe these bumps occur when the iterates are close to saddle points .",
    "non - convex sdp , where @xmath332 .",
    "( a ) . @xmath335 versus @xmath336 .",
    "@xmath337 for different @xmath1 , where @xmath150 is a local maximizer .",
    ", title=\"fig:\",scaledwidth=95.0% ] ( a )     non - convex sdp , where @xmath332 .",
    "( a ) . @xmath335 versus @xmath336 .",
    "@xmath337 for different @xmath1 , where @xmath150 is a local maximizer .",
    ", title=\"fig:\",scaledwidth=95.0% ] ( b )    in figure [ fig : geometry ] , we examine some geometric properties of the rank-@xmath1 non - convex sdp . as",
    "above , we explore the landscape of this problem by projected gradient ascent . in figure",
    "[ fig : geometry]a , we plot the curvature @xmath335 versus the gap from the sdp value @xmath336 along the iterations .",
    "when @xmath313 is far from @xmath43 , there is a linear relationship between these two quantities , which is consistent with theorem [ thm : apprxconcavepoint ] . in figure",
    "[ fig : geometry]b , we plot the gap between @xmath43 and @xmath338 for a local maximizer @xmath150 that is produced by projected gradient ascent , for different values of @xmath1 . these data are averaged over @xmath339 realizations of the random matrix @xmath4 .",
    "this gap converges to zero as @xmath1 gets large , and is upper bounded by the curve @xmath340 .",
    "this coincides with theorem [ thm : apprxconcavepoint ] , which predicts that this gap must be smaller than @xmath341 .",
    "note however that in this case theorem [ thm : apprxconcavepoint ] is overly pessimistic , and the gap appears to decrease very rapidly with @xmath1 .",
    "non - convex sdp , for erds - rnyi random graphs with @xmath342 and average degree @xmath343 .",
    "data are averaged over @xmath339 realizations .",
    ", scaledwidth=45.0% ]    now we turn to study the maxcut problem .",
    "note that theorem [ thm : maxcut ] gives a guarantee for the approximation ratio for the cut induced by any local maximizer of the rank-@xmath1 non - convex sdp ( [ ncvxsdp ] ) .",
    "in figure [ fig : maxcut ] , we take the graph to be an erds - rnyi graph with @xmath344 and average degree @xmath345 .",
    "we plot the cut value found by rounding the maximizer of the rank-@xmath1 non - convex sdp , for @xmath1 from @xmath346 to @xmath339 , and also for @xmath347 which corresponds to the ( [ sdp ] ) .",
    "surprisingly , the cut value found by solving rank-@xmath1 non - convex problem is typically bigger than the cut value found by solving the original sdp .",
    "this provides a further reason to adopt the non - convex approach ( [ ncvxsdp ] ) .",
    "it appears to provide a significantly tight relaxation for random instances .",
    "synchronization : correlation between estimator and ground truth @xmath348 and @xmath349 versus @xmath55 .",
    ", title=\"fig:\",scaledwidth=95.0% ] ( a )     synchronization : correlation between estimator and ground truth @xmath348 and @xmath349 versus @xmath55 .",
    ", title=\"fig:\",scaledwidth=95.0% ] ( b )    in order to study @xmath10 synchronization , we consider the matrix @xmath350 where @xmath351 for @xmath344 .",
    "figure [ fig : z2sync]a shows the correlation @xmath348 of a local maximizer @xmath83 produced by projected gradient ascent , with the ground truth @xmath70 . in figure [ fig : z2sync]b we construct label estimates @xmath352 where @xmath353 is the principal left singular vector of @xmath354 .",
    "we plot the correlation @xmath355 as a function of @xmath55 . in both cases ,",
    "results are averaged over @xmath339 realizations of the matrix @xmath4 .",
    "surprisingly , the resulting correlation is strongly concentrated , despite the fact that gradient ascent converges to a random local maximum @xmath83 .     for different @xmath1 , where @xmath356 is a local maximizer . , scaledwidth=45.0% ]    finally , we turn to the @xmath357 synchronization problem , and study the local maximizer of the orthogonal - cut sdp ( [ oc - sdp ] ) .",
    "we sample a matrix @xmath358 , and find the local maximum of the rank-@xmath1 non - convex orthogonal - cut sdp ( [ ncvx - oc - sdp ] ) . in figure",
    "[ fig : ocsdp ] we plot the gap between @xmath58 and @xmath338 for a local maximizer @xmath359 produced by projected gradient ascent for different @xmath1 .",
    "this gap converges to zero as @xmath1 is larger , and is upper bounded by @xmath360 .",
    "this is in agreement with theorem [ thm : sod_grothendieck ] , which predicts that the gap is smaller than @xmath361 .",
    "note that problem ( [ eq : ncvxsdp ] ) is equivalent to problem ( [ ncvxsdp ] ) with matrix @xmath180 . applying theorem [ thm : apprxconcavepoint ] , and noting that the elements of @xmath164 are non - negative , we for any local maximizer @xmath48 of the problem ( [ eq : ncvxsdp ] ) , and any @xmath196 optimal solution of the sdp ( [ eq : sdpcut ] ) , @xmath362    thus , we have @xmath363\\\\ = & \\left(1-\\frac{1}{k-1}\\right ) \\times \\frac{1}{4 } \\sum_{i , j=1}^n a_{g , ij}(1 - x_{ij}^ * ) = \\left(1-\\frac{1}{k-1}\\right ) \\times \\text{sdpcut}(g ) \\\\",
    "\\geq & \\left(1-\\frac{1}{k-1}\\right)\\times \\text{maxcut}(g ) .",
    "\\end{aligned}\\ ] ]    applying the randomized rounding scheme of @xcite , we sample a vector @xmath364 , and define @xmath365 by @xmath366 , then we obtain @xmath367 \\geq \\alpha _ * \\times \\frac{1}{4 } \\sum_{i , j=1}^n a_{g , ij}(1-\\langle \\sigma_i^ * , \\sigma_j^ * \\rangle ) \\geq \\alpha _ * \\times \\left(1-\\frac{1}{k-1}\\right ) \\times \\text{maxcut}(g).\\ ] ] therefore , for any local maximizer @xmath48 , it gives an @xmath368-approximate solution of the maxcut problem .    if @xmath48 is an @xmath369-approximate concave point , using theorem [ thm : apprxconcavepoint ] and the same argument , we can prove that it gives an @xmath370-approximate solution of the maxcut problem .",
    "let @xmath371 .",
    "for any local maximum @xmath372 of the rank-@xmath1 non - convex maxcut sdp problem , according to theorem [ thm : apprxconcavepoint ] , we have @xmath373 therefore @xmath374    using the convergence of the sdp value as proved in ( * ? ? ?",
    "* theorem 5 ) , for any @xmath228 , there exists @xmath375 such that , for any @xmath376 , the following holds with high probability @xmath377    therefore , we have with high probability @xmath378 \\\\ = &   \\left(1-\\frac{1}{k-1}\\right ) \\frac{\\delta(\\lambda)}{\\lambda }   - \\frac{4 + \\delta}{k-1 } \\cdot \\frac{1}{\\lambda } - \\frac{\\delta}{\\lambda}.   \\end{aligned}\\ ] ] since @xmath379 for @xmath53 , there exists a @xmath263 such that the above expression is greater than @xmath41 for sufficiently small @xmath41 and @xmath380 , which concludes the proof .",
    "we decompose the proof into two parts . in part",
    "@xmath170 , we prove that almost surely @xmath381 using only the second order optimality condition .",
    "in part @xmath172 , we incorporate the first order optimality condition and prove that as @xmath382 , we have almost surely @xmath383      the proof of this part is similar to the proof of theorem [ thm : apprxconcavepoint ] .",
    "we replace the matrix @xmath4 by the expression @xmath384 , where @xmath385 and @xmath386 .",
    "let @xmath387 , @xmath388 , and @xmath389^\\st \\in t_{\\sigma}\\cm_k$ ] , where @xmath390 .",
    "due to the second order optimality condition , similar to the calculation in theorem [ thm : apprxconcavepoint ] , we have for any local maximizer @xmath19 of the rank-@xmath1 non - convex sdp problem : @xmath391 plugging in the expression of @xmath4 , we obtain @xmath392 \\geq 0.\\ ] ] letting @xmath393 , we have @xmath394.\\ ] ] recall that @xmath395 , and @xmath396 .",
    "thus , we get the lower bound @xmath397 also note that @xmath398 is a feasible point of ( [ sdp ] ) .",
    "therefore , @xmath399 which implies that @xmath400 where we used the fact that for a goe matrix @xmath401 , we have @xmath402 almost surely @xcite .      in part @xmath170 we only used the second order optimality condition . in this part of the proof",
    ", we will incorporate the first order optimality condition . note that as @xmath403 , the bound in part @xmath170 is better .",
    "so in this part , we only consider the case when @xmath382 .    without loss of generality ,",
    "let @xmath404 , the vector with all entries equal to one .",
    "let @xmath405 be a local optimizer of the rank-@xmath1 non - convex sdp problem .",
    "we remark that the cost function is invariant by a right rotation of @xmath19 .",
    "we can therefore assume that @xmath406 where @xmath407 and @xmath408 for @xmath409 ( take the svd decomposition @xmath410 and consider @xmath411 ) .",
    "let @xmath17 and @xmath412 . for simplicity , we will sometimes omit the dependence on @xmath55 and write @xmath413 .",
    "we decompose the proof into the following steps .",
    "* step 1 * _ upper bound on @xmath414 , for @xmath415 , using the first order optimality condition .",
    "_    the first order optimality condition gives @xmath416 , which implies that @xmath417 for any @xmath418 , where we denoted @xmath419 the entry - wise product of @xmath70 and @xmath71 .",
    "replacing @xmath4 by its expression gives @xmath420 which implies @xmath421.\\ ] ] we take the norm of this expression and , recalling that @xmath422 , we obtain @xmath423 ^ 2 .",
    "\\\\ \\end{aligned}\\ ] ] notice that @xmath424 $ ] , hence @xmath425 ^ 2 \\\\ \\leq & \\frac{n^2}{\\lambda^2}\\left\\vert w_n \\right\\vert_{{{\\rm op}}}^2 \\left [ \\left \\vert v_i \\right\\vert_2+\\left\\vert v_j \\right\\vert_2 \\right]^2 \\\\ \\leq & \\frac{2n^2}{\\lambda^2 } \\left\\vert w_n \\right\\vert_{{{\\rm op}}}^2 \\left(\\vert v_i \\vert_2 ^ 2 + \\vert v_j \\vert_2 ^ 2\\right ) .",
    "\\end{aligned}\\ ] ]    without loss of generality , let us assume that @xmath426 for @xmath427 which implies @xmath428 we deduce the following upper bound @xmath429 for @xmath430 , where we use the fact that for a goe matrix @xmath401 , we have @xmath402 almost surely .    * step 2 * _ lower bound on @xmath431 .",
    "_    we combine equation ( [ eq : clb1 ] ) and ( [ eq : ub1 ] ) to get almost surely @xmath432 \\geq   1 - \\frac{1}{k } - \\frac{4 } { \\lambda } - \\frac{16k}{\\lambda^2}.\\ ] ] since we assumed that @xmath382 and @xmath433 , we obtain , almost surely , @xmath434 the second inequality above is loose but it is sufficient for our purposes .",
    "* step 3 * _ upper bound on @xmath435 for @xmath436 .",
    "_    in equation ( [ eq : foc1 ] ) , let us take @xmath437 and @xmath438 , we have @xmath439    combining equation ( [ eq : ub3 ] ) and ( [ eq : lb2 ] ) results in the following upper bound for @xmath382 , @xmath440 holding almost surely for any @xmath436 .    * step 4 * _ lower bound on @xmath441 . _    by second order optimality of @xmath19 , for any vectors",
    "@xmath442 satisfying @xmath443 , we have @xmath444 where @xmath445^\\st$ ] and @xmath446 .",
    "take @xmath447 , where @xmath448 is the @xmath449-th canonical basis vector in @xmath450 , @xmath451 .",
    "noting that @xmath452 , we have @xmath453 .",
    "therefore , we have @xmath454 using the second order stationarity condition with this choice of @xmath455 , we have @xmath456 which implies @xmath457    consider the first term @xmath458 .",
    "it is easy to see that the second order stationary condition implies @xmath459 .",
    "thus , we have @xmath460 } \\vert w_{n , ii}\\vert \\cdot \\sum_{i=1}^n v_{a , i}^2 \\\\ \\geq & -\\vert w_n \\vert_{{{\\rm op } } } \\vert v_{a}\\vert_2 ^ 2 .",
    "\\end{aligned}\\ ] ]    next consider the second term @xmath461 .",
    "we have @xmath462 where the last inequality is because @xmath463 so that @xmath464",
    ".    finally , consider the last term @xmath465 .",
    "@xmath466 where the last inequality used a fact that if @xmath467 is in the elliptope , we have @xmath468 for any @xmath469",
    ".    here is the justification of the above fact . for @xmath28 in the elliptope",
    ", we have @xmath470 and @xmath471 . for any @xmath472 satisfying @xmath473 and @xmath474",
    ", @xmath475 also satisfies @xmath476 and @xmath477 .",
    "therefore , using the variational representation of the operator norm , we have @xmath478    * step 5 * _",
    "finish the proof .",
    "_    noting that @xmath479 and @xmath480 , we rewrite equation ( [ eq : thm_eq1 ] ) as following @xmath481 plug in the lower bound of @xmath458 , @xmath461 , @xmath465 , we have almost surely @xmath482 here we used equation ( [ eq : ub2 ] ) , @xmath483 , and the fact that for a goe matrix @xmath401 , we have @xmath402 almost surely .      the proof is similar to the proof of theorem [ thm : z2sync1 ] , where the goe matrix @xmath401 is replaced by the noise matrix @xmath484 .",
    "applying theorem [ thm : apprxconcavepoint ] with the matrix @xmath485 , similar to equation ( [ eq : nonvanishing1 ] ) , we have @xmath486    according to ( * ? ? ?",
    "* theorem 8) , the gap between the sdps with the two different noise matrices is bounded with high probability by a function of the average degree @xmath40 @xmath487 where @xmath371 corresponds to the @xmath10 synchronization model and @xmath488 is a function of @xmath55 bounded for any fixed @xmath55 .    according to ( * ? ? ?",
    "* theorem 5 ) , for any @xmath376 and @xmath228 , there exists a function @xmath375 such that with high probability , we have @xmath489    combining the above results , we have for any @xmath376 , with high probability @xmath490 for a sufficiently small @xmath491 , taking @xmath380 sufficiently small , and taking successively @xmath40 and @xmath1 sufficiently large , the above expression will be greater than @xmath41 , which concludes the proof",
    ".      we decompose the proof into three parts .",
    "in the first part , we do the calculation for a general non - convex problem .",
    "in the second part , we focus on the non - convex problem ( [ ncvx - oc - sdp ] ) . in the third part",
    ", we prove a claim we made in the second part .",
    "first , let s consider a general sdp problem .",
    "given a symmetric matrix @xmath492 , symmetric matrices @xmath493 and real numbers @xmath494 , we consider the following sdp : @xmath495 , \\\\ & \\quad x \\succeq 0 .",
    "\\end{aligned}\\ ] ] let @xmath496 $ ] and @xmath497 .",
    "we denote @xmath498 the maximum of the above sdp problem : @xmath499 \\}.\\ ] ] we assume @xmath500 .    for a fixed integer @xmath1 ,",
    "the burer - monteiro approach considers the following non - convex problem : @xmath501 , \\end{aligned}\\ ] ] with decision variable @xmath405    define the manifold @xmath502 \\}$ ] . at each point @xmath503",
    ", the tangent space is given by @xmath504 \\rbrace$ ] .",
    "we denote @xmath505 the projection of @xmath506 onto @xmath507 : @xmath508 where @xmath509 . the riemannian gradient is therefore given by @xmath510 with @xmath511 .",
    "we will write @xmath512 .",
    "the riemannian hessian @xmath513 applied on the direction @xmath514 gives @xmath515\\rangle = 2 \\langle u , ( a - \\lambda ) u\\rangle.\\ ] ] therefore , according to the definition of the @xmath41-approximate concave point @xmath516 , we have @xmath517    let @xmath518 such that @xmath519 is a solution of the general sdp problem ( [ sdpgen ] ) , and @xmath318 , @xmath319 i.i.d . , be a random mapping from @xmath72 onto @xmath82 .",
    "let @xmath19 be a local maximizer of the rank-@xmath1 non - convex sdp ( [ optkgen ] ) , and take @xmath520 , a random projection of @xmath521 onto @xmath522 .",
    "due to the definition of the approximate concave point , we have @xmath523 where the expectation is taken over the random mapping @xmath163 .",
    "expanding the left hand side gives @xmath524 the second term in the last equation gives @xmath525\\\\ = & \\sum_{i , j=1}^s m_{ij } { \\mathbb{e}}\\left [ \\left\\langle v^\\st \\left(\\lambda - a\\right ) b_i \\sigma , g^\\st \\right\\rangle \\left\\langle v^\\st b_j \\sigma , g^\\st \\right\\rangle\\right]\\\\ = & \\frac{1}{k } \\sum_{i , j = 1}^s m_{ij } \\left\\langle v^\\st \\left(\\lambda - a\\right ) b_i \\sigma , v^\\st b_j \\sigma \\right\\rangle \\\\ = & \\frac{1}{k }   \\left\\langle \\left(\\lambda - a\\right ) , v v^\\st   \\sum_{i , j = 1}^s m_{ij } b_j \\sigma \\sigma^\\st b_i \\right\\rangle .",
    "\\\\ \\end{aligned}\\ ] ]    the third term gives @xmath526\\\\ = &   \\frac{1}{k}\\sum_{ijkl = 1}^s m_{ij } m_{kl } \\left\\langle b_i , \\left(\\lambda - a\\right ) b_k \\sigma\\sigma^\\st \\right\\rangle \\left\\langle x^ * b_j , b_l \\sigma \\sigma^\\st\\right\\rangle . \\end{aligned}\\ ] ]    for the fourth term , we have @xmath527\\\\ = & n - \\frac{1}{k } \\sum_{ij=1}^n \\sum_{kl=1}^n m_{ij } m_{kl } \\left \\langle b_i \\sigma , b_k \\sigma \\right\\rangle \\cdot \\left\\langle v^\\st b_j \\sigma , v^\\st b_l \\sigma \\right\\rangle . \\end{aligned}\\ ] ]      now let s consider the case of the rank-@xmath1 non - convex orthogonal - cut sdp problem ( [ ncvx - oc - sdp ] ) .",
    "there are @xmath528 constraints corresponding to the set @xmath529\\ } = \\ { ( e_{ii } , 1 ) : i \\in [ n]\\ } \\bigcup \\cup_{t=1}^m\\ { ( ( e_{ij } + e_{ji})/\\sqrt{2},0 ) : ( t-1 ) d+1 \\le i < j \\le t d\\}$ ] , where @xmath530 .",
    "we will denote @xmath531 the optimization manifold : @xmath532 \\rbrace.\\ ] ] it is straightforward to verify that for any @xmath283 , we have @xmath533 .",
    "thus , we have @xmath534 . in the following calculation ,",
    "we write @xmath17 . recall that @xmath196 is a global maximizer of problem ( [ oc - sdp ] ) , and @xmath519 .",
    "now , let us calculate each term in equation ( [ eq : coreequation ] ) , for the specific problem ( [ ncvx - oc - sdp ] ) .",
    "for the second term in equation ( [ eq : coreequation ] ) , we derived equation ( [ eq:2rdterm ] )",
    ". one can check with some calculations that for any @xmath283 , we have @xmath535 for the fourth term in equation ( [ eq : coreequation ] ) , we derived equation ( [ eq:4thterm ] ) . following the calculation in equation ( [ eq:4thterm ] )",
    ", we have @xmath536 for the third term in equation ( [ eq : coreequation ] ) , we derived equation ( [ eq:3rdterm ] ) . following the calculation in equation ( [ eq:3rdterm ] ) , we have @xmath537 where we define @xmath538 . here",
    ", we claim that @xmath398 is a feasible point of the orthogonal - cut sdp problem ( [ oc - sdp ] ) .",
    "we will prove this claim in part @xmath539 .",
    "for any feasible point @xmath28 of the orthogonal - cut sdp problem ( [ oc - sdp ] ) , we have @xmath540 .",
    "therefore , from equation ( [ eq : coreequation ] ) , we obtain @xmath541 letting @xmath57 , rearranging the above inequality , we have @xmath542 which finally gives the desired inequality @xmath543      now , let us check that @xmath398 is a feasible point of the orthogonal - cut sdp problem ( [ oc - sdp ] ) .",
    "the reason is given by the following fact @xmath170 and @xmath172 .",
    "* fact @xmath170*. @xmath398 is p.s.d .. indeed , for any @xmath544 , recall that @xmath17 and @xmath519 , we have @xmath545 the matrix @xmath546 , where @xmath547 $ ] .",
    "similarly , we have @xmath548 .",
    "thus , @xmath549 for any @xmath544",
    ". then @xmath398 is p.s.d ..    * fact @xmath172 * the @xmath550th block of @xmath398 equals @xmath551 . to show this , we assume @xmath552 , and due to the symmetry",
    ", we just need to check @xmath553 and @xmath554 .",
    "we denote @xmath555 , and we rewrite @xmath556 as @xmath557 where @xmath558 .",
    "we have the following series of simplification @xmath559 the third equality used the fact that @xmath28 and @xmath196 are feasible point so that their @xmath550th block are @xmath551 .",
    "similarly , we have @xmath560 the last equality is because @xmath561 is always a diagonal matrix .",
    "therefore , we proved that @xmath398 is a feasible point of the orthogonal - cut sdp problem ( [ oc - sdp ] ) .",
    "given a point @xmath288 and a tangent vector @xmath562 with @xmath130 , we denote @xmath563 the update with searching direction @xmath70 and step size @xmath564 .",
    "the next three lemmas ensure a sufficient increment of the objective function at each step of the rtr algorithm .",
    "( gradient - step ) fix @xmath565 . for any point @xmath288 such that @xmath566 , taking searching direction @xmath567 and step size @xmath568 , we have @xmath569 [ lem : grad ]    the second order expansion of @xmath570 around @xmath221 with @xmath571 gives @xmath572}\\frac{1}{2}(\\func\\circ   \\sigma)''(\\xi ) t^2 \\\\ & \\geq \\langle \\tgrad \\func ( \\sigma ) , u \\rangle t - \\frac{1}{2}\\vert a \\vert_1 \\cdot ( 4   + 8 t + 8t^2 ) \\cdot t^2 \\\\ & \\geq   \\vert \\tgrad \\func ( \\sigma ) \\vert_f t - 10 \\vert a \\vert_1 t^2 . \\end{aligned}\\ ] ] the second inequality used the bound on the second order derivative in lemma [ lem : secondderivative ] in appendix [ app : derivativesbounds ] .",
    "now we take @xmath573 . since @xmath565",
    ", we have @xmath571 . plugging this @xmath564 into",
    "the above equation completes the proof .",
    "( eigen - step ) for any point @xmath288 , and @xmath574 satisfying @xmath130 , @xmath575 , and @xmath576 > 0 $ ] , choosing @xmath577 , we have @xmath578 [ lem : hess1 ]    the third order expansion of @xmath570 around @xmath221 for @xmath571 gives @xmath579\\rangle t^2 - \\frac{1}{6 }   \\sup_{\\xi \\in [ 0,t ] } ( \\func\\circ   \\sigma)'''(\\xi ) t^3 \\\\ & \\geq \\frac{1}{2 } \\lambda_h t^2 -   \\frac{1}{6 } \\vert a \\vert_1   \\cdot ( 12 + 36 t + 48 t^2 + 48 t^3)\\cdot t^3\\\\ & \\geq   \\frac{1}{2 } \\lambda_h t^2 - 24 \\vert a \\vert_1 t^3 .   \\end{aligned}\\ ] ] the second inequality used the bound on the third order derivative in lemma [ lem : thirdderivative ] in appendix [ app : derivativesbounds ] .",
    "now we take @xmath580 .",
    "note that we always have @xmath581 , and therefore we have @xmath582 .",
    "plugging this @xmath564 into the above equation completes the proof .",
    "the last lower bound on the increment of objective function for eigen - step used the loose bound in lemma [ lem : thirdderivative ] . using lemma [ lem : improvedthird ]",
    ", we can give an improved bound for the eigen - step when the norm of the gradient is small . in particular we take @xmath123 .",
    "( improved bound for eigen - step ) for any point @xmath288 with @xmath583 , and @xmath574 satisfying @xmath130 , @xmath575 and @xmath584   > 0 $ ] , choosing @xmath585 , we have @xmath586 [ lem : hess2 ]    the third order expansion of @xmath570 around @xmath221 for @xmath571 gives @xmath587\\rangle t^2 - \\frac{1}{6 }   \\sup_{\\xi \\in [ 0,t ] } ( \\func\\circ   \\sigma)'''(\\xi ) t^3 \\\\ & \\geq \\frac{1}{2 } \\lambda_h t^2 -   \\frac{1}{6 } ( 6\\vert a \\vert_2 + 3 \\vert \\tgrad \\func ( \\sigma(0))\\vert_f ) \\cdot t^3 - \\frac{1}{6 } \\vert a \\vert_1 \\cdot ( 42   + 72 t + 48 t^2 ) \\cdot t^4 \\\\ & \\geq   \\frac{1}{2 } \\lambda_h t^2 - \\frac{3}{2 } \\vert a \\vert_2 t^3 - 27 \\vert a \\vert_1 t^4 . \\end{aligned}\\ ] ] the first inequality used the improved bound on the third order derivative of lemma [ lem : improvedthird ] in appendix [ app : derivativesbounds ] , which imply in particular @xmath588 . taking @xmath589 completes the proof .",
    "we are now at a good position to prove theorem [ thm : trustregion ] .",
    "denote @xmath590 and @xmath591 .",
    "let @xmath592 be the number of iterations and @xmath593 the iterates returned by our rtr algorithm from an arbitrary initialization @xmath594 .",
    "we are only interested in the convergence rate as @xmath595 , namely the convergence rate below the gap .",
    "since our algorithm is an ascent algorithm , without loss of generality , we assume @xmath596 ( otherwise the theorem will hold automatically ) .    at each point @xmath288 , theorem [ thm : apprxconcavepoint ] gives the following lower bound on the highest curvature @xmath597\\rangle}{\\langle u , u \\rangle } \\geq 2\\frac{g(\\sigma)}{n } > 0.\\ ] ] we will use this information to bound the algorithm s convergence rate .",
    "* case 1 . *",
    "first , we consider the case when all the rtr steps are eigen - steps . in each iteration",
    ", the algorithm constructs an update direction @xmath106 with curvature @xmath598 . according to lemma [ lem : hess2 ]",
    ", we have @xmath599 which implies @xmath600 .",
    "thus , we have @xmath601 summing over @xmath602 , we have @xmath603 therefore , we obtain the convergence rate @xmath604 .",
    "this implies that @xmath605 as soon as @xmath606 .",
    "* then , we consider the case where we set @xmath123 , and we use the gradient step as @xmath607 , and use the eigen - step as @xmath608 . first let us bound the number of gradient steps . according to lemma [ lem :",
    "grad ] , we have @xmath609 hence , we deduce the upper bound @xmath610 .",
    "then let us bound the number of eigen - steps .",
    "let us denote @xmath611 and @xmath612 the subsets of indices corresponding to eigensteps with respectively @xmath613 and @xmath614 . according to lemma [ lem : hess2 ]",
    ", we have for all @xmath615 @xmath616 whereas for @xmath617 @xmath618 summing the contributions of the above two equations gives the convergence rate @xmath619 for a universal constant @xmath147 .",
    "this guarantees that @xmath620 as soon as @xmath621 for some universal constant @xmath622 .",
    "a.m. was partially supported by the nsf grant ccf-1319979 .",
    "s.m . was supported by office of technology licensing stanford graduate fellowship .",
    "ankks@xmath62312    p - a absil , christopher  g baker , and kyle  a gallivan , _ trust - region methods on riemannian manifolds _ , foundations of computational mathematics * 7 * ( 2007 ) , no .  3 , 303330 .",
    "emmanuel abbe , afonso  s bandeira , and georgina hall , _ exact recovery in the stochastic block model _ ,",
    "ieee transactions on information theory * 62 * ( 2016 ) , no .  1 , 471487 .    greg  w anderson , alice guionnet , and ofer zeitouni , _ an introduction to random matrices _ , vol .",
    "118 , cambridge university press , 2010 .",
    "sanjeev arora , elad hazan , and satyen kale , _ fast algorithms for approximate semidefinite programming using the multiplicative weights update method _ ,",
    "foundations of computer science , 2005 .",
    "focs 2005 .",
    "46th annual ieee symposium on , ieee , 2005 , pp .",
    "339348 .",
    "to3em , _ the multiplicative weights update method : a meta - algorithm and applications .",
    "_ , theory of computing * 8 * ( 2012 ) , no .  1 , 121164 .    sanjeev arora and satyen kale , _ a combinatorial , primal - dual approach to semidefinite programs _ , proceedings of the thirty - ninth annual acm symposium on theory of computing , acm , 2007 , pp",
    ".  227236 .",
    "mica arie - nachimson , shahar  z kovalsky , ira kemelmacher - shlizerman , amit singer , and ronen basri , _ global motion estimation from point matches _ , 3d imaging , modeling , processing , visualization and transmission ( 3dimpvt ) , 2012 second international conference on , ieee , 2012 , pp .  8188 .",
    "jinho baik , grard  ben arous , sandrine pch , et  al .",
    ", _ phase transition of the largest eigenvalue for nonnull complex sample covariance matrices _ , the annals of probability * 33 * ( 2005 ) , no .  5 , 16431697 .    alexander  i. barvinok , _ problems of distance geometry and convex properties of quadratic maps _ , discrete & computational geometry * 13 * ( 1995 ) , no .  2 , 189202 .",
    "afonso  s bandeira , nicolas boumal , and vladislav voroninski , _ on the low - rank approach for semidefinite programs arising in synchronization and community detection _ , arxiv preprint arxiv:1602.04426 ( 2016 ) .",
    "afonso  s bandeira , moses charikar , amit singer , and andy zhu , _ multireference alignment using semidefinite programming _ , proceedings of the 5th conference on innovations in theoretical computer science , acm , 2014 , pp .",
    "459470 .",
    "samuel burer and renato  dc monteiro , _ a nonlinear programming algorithm for solving semidefinite programs via low - rank factorization _",
    ", mathematical programming * 95 * ( 2003 ) , no .  2 , 329357 .",
    "nicolas boumal , _ a riemannian low - rank method for optimization over semidefinite matrices with block - diagonal constraints _",
    ", arxiv:1506.00575 ( 2015 ) .",
    "nicolas boumal , vlad voroninski , and afonso bandeira , _ the non - convex burer - monteiro approach works on smooth semidefinite programs _ , advances in neural information processing systems , 2016 , pp .  27572765 .",
    "yash deshpande , emmanuel abbe , and andrea montanari , _",
    "asymptotic mutual information for the two - groups stochastic block model _ ,",
    "arxiv preprint arxiv:1507.08685 ( 2015 ) .",
    "joel friedman , _ a proof of alon s second eigenvalue conjecture _ , proceedings of the thirty - fifth annual acm symposium on theory of computing , acm , 2003 , pp .",
    "720724 .",
    "dan garber and elad hazan , _ approximating semidefinite programs in sublinear time _ , advances in neural information processing systems , 2011 , pp .",
    "10801088 .",
    "alexander grothendieck , _ rsum de la thorie mtrique des produits tensoriels topologiques _",
    ", resenhas do instituto de matemtica e estatstica da universidade de so paulo * 2 * ( 1996 ) , no .  4 , 401481 .",
    "olivier gudon and roman vershynin , _ community detection in sparse networks via grothendieck s inequality _ , probability theory and related fields * 165 * ( 2016 ) , no .  3 - 4 , 10251049 .",
    "michel  x goemans and david  p williamson , _ improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming _ , journal of the acm ( jacm ) * 42 * ( 1995 ) , no .  6 , 11151145 .",
    "bruce hajek , yihong wu , and jiaming xu , _ achieving exact cluster recovery threshold via semidefinite programming _ , ieee transactions on information theory * 62 * ( 2016 ) , no .  5 , 27882797 .",
    "adel javanmard , andrea montanari , and federico ricci - tersenghi , _ phase transitions in semidefinite relaxations _ , proceedings of the national academy of sciences * 113 * ( 2016 ) , no .  16 , e2218e2223",
    ".    iain  m johnstone , _ on the distribution of the largest eigenvalue in principal components analysis _ , annals of statistics ( 2001 ) , 295327 .",
    "subhash khot , guy kindler , elchanan mossel , and ryan odonnell , _ optimal inapproximability results for max - cut and other 2-variable csps ? _ , siam journal on computing * 37 * ( 2007 ) , no .  1 , 319357 .",
    "satish  babu korada and nicolas macris , _ exact solution of the gauge symmetric p - spin glass model on a complete graph _ , journal of statistical physics * 136 * ( 2009 ) , no .  2 , 205230 .",
    "subhash khot and assaf naor , _ grothendieck - type inequalities in combinatorial optimization _ , communications on pure and applied mathematics * 65 * ( 2012 ) , no .  7 , 9921035 .    j  kuczyski and h  woniakowski , _ estimating the largest eigenvalue by the power and lanczos algorithms with a random start _ , siam journal on matrix analysis and applications * 13 * ( 1992 ) , no .  4 , 10941122 .",
    "laurent massouli , _ community detection thresholds and the weak ramanujan property _ , proceedings of the 46th annual acm symposium on theory of computing , acm , 2014 , pp .",
    "694703 .",
    "elchanan mossel , joe neeman , and allan sly , _ a proof of the block model threshold conjecture _ , arxiv:1311.4115 ( 2013 ) .",
    "to3em , _ reconstruction and estimation in the planted partition model _ , probability theory and related fields * 162 * ( 2015 ) , no .",
    "3 - 4 , 431461 .",
    "andrea montanari , _ a grothendieck - type inequality for local maxima _ , arxiv:1603.04064 ( 2016 ) .",
    "ankur moitra , william perry , and alexander  s wein , _ how robust are reconstruction thresholds for community detection ? _ , proceedings of the 48th annual acm sigact symposium on theory of computing , acm , 2016 , pp .",
    "828841 .",
    "andrea montanari and subhabrata sen , _",
    "semidefinite programs on sparse random graphs and their application to community detection _ , proceedings of the 48th annual acm sigact symposium on theory of computing , acm , 2016 , pp",
    ".  814827 .",
    "yurii nesterov , _ introductory lectures on convex optimization : a basic course _ , vol .  87 , springer science & amp ; business media , 2013 .",
    "gbor pataki , _ on the rank of extreme matrices in semidefinite programs and the multiplicity of optimal eigenvalues _ , mathematics of operations research",
    "* 23 * ( 1998 ) , no .  2 , 339358 .",
    "amit singer , _ angular synchronization by eigenvectors and semidefinite programming _ , applied and computational harmonic analysis * 30 * ( 2011 ) , no .  1 , 2036 .",
    "amit singer and yoel shkolnisky , _ three - dimensional structure determination from common lines in cryo - em by eigenvectors and semidefinite programming _ , siam journal on imaging sciences * 4 * ( 2011 ) , no .  2 , 543572 .",
    "david steurer , _ fast sdp algorithms for constraint satisfaction problems _ , proceedings of the twenty - first annual acm - siam symposium on discrete algorithms , siam , 2010 , pp .  684697 .",
    "in this section , we give an upper bound to the second and third derivatives of @xmath625 ( these notations are defined below ) . these bounds are important in bounding the complexity of the riemannian trust - region method in solving the non - convex sdp problem .",
    "fix a point @xmath626 on the manifold , and a tangent vector @xmath627^\\st \\in { \\mathbb{r}}^{n \\times k } : u_i \\in { \\mathbb{r}}^k , \\langle \\sigma_i , u_i \\rangle = 0 , \\forall i \\in [ n ] \\}$ ] with @xmath130 .",
    "let @xmath628 be the orthogonal projection of @xmath629 onto the manifold @xmath79 .",
    "for a given symmetric matrix @xmath630 , let @xmath631 .",
    "we would like to study the derivatives of @xmath632 with respect to @xmath564 .",
    "furthermore , we define @xmath633 , @xmath634^\\st$ ] , @xmath635)$ ] , and @xmath636 . for convenience",
    ", we will denote @xmath637 , @xmath638 , @xmath639 , and @xmath640 .",
    "for any @xmath288 and @xmath641 , let @xmath628 .",
    "we have @xmath642 @xmath643 \\sigma(t ) - 2 t d(t ) u(t ) , \\\\",
    "\\sigma'''(t ) = & \\left[9 t d(t)^2 - 15 t^3 d(t)^3\\right ] \\sigma(t ) + \\left[-3d(t ) + 9 t^2 d(t)^2\\right ] u(t ) . \\\\ \\end{aligned}\\ ] ] [ lem : derivatives ]    to calculate the first three derivatives of @xmath644 , we expand each row of @xmath645 up to third order in @xmath646 : @xmath647",
    "\\sigma_i(t ) + u_i(t ) \\right\\rbrace r \\\\ & + \\left\\lbrace \\left[-\\frac{1}{2}\\vert u_i(t ) \\vert_2 ^ 2 + \\frac{3}{2 } t^2 \\vert u_i(t)\\vert_2 ^ 4\\right ] \\sigma_i(t ) + \\left[- t \\vert u_i(t)\\vert_2 ^ 2\\right ] u_i(t ) \\right\\rbrace r^2 \\\\ & + \\left\\lbrace \\left[\\frac{3}{2 } t \\vert u_i(t)\\vert_2 ^ 4 - \\frac{5}{2 } t^3 \\vert u_i(t ) \\vert_2 ^ 6\\right ] \\sigma_i(t ) + \\left[-\\frac{1}{2}\\vert u_i(t ) \\vert_2 ^ 2 + \\frac{3}{2 } t^2 \\vert u_i(t)\\vert_2 ^ 4\\right ] u_i(t ) \\right\\rbrace r^3 + o(r^3 )",
    ". \\end{aligned}\\ ] ] by matching each expansion coefficient to the corresponding derivative , we obtain the desired result .",
    "we explicitly calculate the second derivative @xmath650 \\rangle + \\langle \\nabla \\func ( \\sigma(t ) ) , \\sigma''(t ) \\rangle\\\\   = & \\langle \\sigma'(t ) , 2 a   \\sigma'(t ) \\rangle + \\langle 2 a \\sigma(t ) ,",
    "\\sigma''(t ) \\rangle\\\\ = & \\langle - t \\td \\tsigma + \\tu , 2 a   [ - t\\td \\tsigma + \\tu ] \\rangle + \\langle 2 a   \\tsigma , [ -\\td + 3 t^2 \\td^2 ] \\tsigma - 2 t \\td \\tu \\rangle\\\\ = & 2 \\langle \\tu , ( a - \\tlambda ) \\tu \\rangle - 4 t [ \\langle \\tu , a \\td \\tsigma \\rangle + \\langle a \\tsigma , \\td \\tu \\rangle ] + t^2 [ 2 \\langle \\td\\tsigma , a\\td \\tsigma \\rangle + 6 \\langle a \\tsigma , \\td^2 \\tsigma \\rangle ] .   \\end{aligned}\\ ] ] noticing that @xmath651 , we can use the bounds derived in appendix [ app : bounds ] to obtain the following inequality @xmath652 + t^2 [ 2 |\\langle \\td\\tsigma , a\\td \\tsigma \\rangle | + 6 |\\langle a \\tsigma , \\td^2 \\tsigma \\rangle | ] \\\\ & \\leq 4 \\vert a \\vert_1 + 8 t \\vert a \\vert_1 + 8t^2 \\vert a \\vert_1 . \\end{aligned}\\ ] ]    for @xmath648 as defined above @xmath649 } \\vert ( \\func\\circ   \\sigma ) ' '' ( \\xi ) \\vert \\leq \\vert a \\vert_1 \\cdot ( 12 + 36 t + 48 t^2 + 48 t^3),\\quad \\forall t \\geq 0 . \\end{aligned}\\ ] ] [ lem : thirdderivative ]    we explicitly calculate the third derivative @xmath653 \\rangle\\\\ = & \\langle \\nabla \\func ( \\tsigma ) , [ 9 t \\td^2 - 15 t^3 \\td^3 ] \\tsigma + [ -3\\td + 9 t^2 \\td^2 ] \\tu \\rangle \\\\ & + 3 \\langle- t\\td \\tsigma + \\tu , \\nabla^2 \\func ( \\tsigma ) [ ( -\\td + 3t^2 \\td^2)\\tsigma - 2t\\td\\tu ]   \\rangle\\\\ = & -6 [ \\langle \\tsigma , a \\td \\tu \\rangle + \\langle \\tu , a\\td \\tsigma \\rangle]+ [ 18 \\langle \\tsigma , a \\td^2 \\tsigma \\rangle + 6 \\langle \\td \\tsigma , a \\td \\tsigma \\rangle - 12 \\langle \\tu , a\\td \\tu \\rangle ] t\\\\ & + [ 18 \\langle \\tsigma , a\\td^2 \\tu \\rangle + 12 \\langle \\td\\tsigma , a\\td \\tu \\rangle + 18 \\langle \\tu , a \\td^2 \\tsigma \\rangle]t^2 \\\\ & + [ -30 \\langle \\tsigma , a \\td^3 \\tsigma \\rangle - 18 \\langle \\td \\tsigma , a \\td^2 \\tsigma \\rangle]t^3 . \\end{aligned}\\ ] ] the inequality is obtained by upper bounding each term using the bounds derived in appendix [ app : bounds ] .      for @xmath648",
    "as defined above , an improved bound on its third derivative gives @xmath649 } \\vert ( \\func\\circ   \\sigma ) ' '' ( \\xi ) \\vert \\leq 6 \\vert a \\vert_2 + 3 \\vert \\tgrad \\func ( \\sigma(0))\\vert_f + \\vert a \\vert_1 \\cdot ( 42 t   + 72 t^2 + 48 t^3 ) , \\quad \\forall t \\geq 0 .",
    "\\end{aligned}\\ ] ] [ lem : improvedthird ]      we next bound more carefully @xmath659 .",
    "simple calculation gives us @xmath660 hence , @xmath661 t. \\end{aligned}\\ ] ] according to the bounds in appendix [ app : bounds ] , we have @xmath662 in the meanwhile , we have @xmath663 according to the taylor expansion of @xmath664 around @xmath221 and @xmath665 at first order , we have @xmath666 } \\vert g'(\\xi ) \\vert \\leq \\frac{1}{2 } \\vert \\tgrad \\func ( \\sigma(0))\\vert_f + t\\vert a \\vert_2 + 4 t^2 \\vert a \\vert_1.\\ ] ] hence the improved bound follows .      in this section",
    "we give all the bounds used in the proof of lemma [ lem : secondderivative ] , [ lem : thirdderivative ] and [ lem : improvedthird ] .",
    "let @xmath83 and @xmath667 with @xmath668 .",
    "note that here we do not require @xmath129 .",
    "denote @xmath669)$ ] and @xmath670 .",
    "we have the following bound for each term ."
  ],
  "abstract_text": [
    "<S> a number of statistical estimation problems can be addressed by semidefinite programs ( sdp ) . </S>",
    "<S> while sdps are solvable in polynomial time using interior point methods , in practice generic sdp solvers do not scale well to high - dimensional problems . in order to cope with this problem , burer and </S>",
    "<S> monteiro proposed a non - convex rank - constrained formulation , which has good performance in practice but is still poorly understood theoretically .    </S>",
    "<S> in this paper we study the rank - constrained version of sdps arising in maxcut and in synchronization problems . </S>",
    "<S> we establish a grothendieck - type inequality that proves that all the local maxima and dangerous saddle points are within a small multiplicative gap from the global maximum . </S>",
    "<S> we use this structural information to prove that sdps can be solved within a known accuracy , by applying the riemannian trust - region method to this non - convex problem , while constraining the rank to be of order one . for the maxcut problem </S>",
    "<S> , our inequality implies that any local maximizer of the rank - constrained sdp provides a @xmath0 approximation of the maxcut , when the rank is fixed to @xmath1 .    </S>",
    "<S> we then apply our results to data matrices generated according to the gaussian @xmath2 synchronization problem , and the two - groups stochastic block model with large bounded degree . </S>",
    "<S> we prove that the error achieved by local maximizers undergoes a phase transition at the same threshold as for information - theoretically optimal methods . </S>"
  ]
}