{
  "article_text": [
    "as we enter the era of gravitational wave astronomy , a number of experiments to detect and study gravitational waves have been set up and some others are presently under development ( see  @xcite for a recent account ) . among the second group",
    "we find one of the experiments that is presently attracting a considerable amount of attention : the laser interferometer space antenna ( lisa )  @xcite , a collaboration between esa and nasa that is scheduled to be launched in the next decade .",
    "extreme - mass - ratio binaries ( emrbs ) are considered to be a primary source of gravitational radiation to be detected by lisa  @xcite .",
    "they consist of a `` small '' object , such a main sequence star , a stellar mass black hole , or a neutron star , with mass @xmath0 ranging from @xmath1 to @xmath2 , orbiting a massive black hole ( mbh ) with mass @xmath3 ranging from @xmath4 ( if we consider the case of intermediate mass black holes ) to @xmath5 ( the case of big supermassive black holes sitting in the center of galaxies ) .",
    "this translates to emrbs with mass ratios , @xmath6 in the range @xmath7 .",
    "in order to exploit this type of systems through lisa , it is crucial to have a good theoretical understanding of the evolution of these systems , good enough to produce accurate waveform templates in support of data analysis efforts .    because there is no significant coupling between the strong curvature effects between the mbh and its companion ,",
    "relativistic perturbation theory is a well suited tool to study embrs .",
    "clearly , the accuracy of this approximation depends on the smallness of the ratio @xmath8 .",
    "the goal is to study the perturbations generated by the small body in the ( background ) gravitational field of the mbh , and how these perturbations affect the motion of the small body itself , which follows the geodesics of the perturbed spacetime .",
    "that is , one is after studying how the presence of the small body affects its own trajectory .",
    "this problem is usually known in literature as the _ radiation reaction problem_. this is an old problem and several approaches to deal with it have been proposed ( see the recent reviews by poisson  @xcite , detweiler  @xcite and mino  @xcite ) .",
    "a pragmatic approach is to use energy - momentum balance arguments  @xcite . under this approach ,",
    "one estimates the changes in the small body _ constants of motion _ by computing the fluxes of energy and angular momentum at infinity and through the mbh horizon .",
    "this approach works well in the adiabatic regime , when the time scale of the radiation reaction is much bigger than the orbital time scale .",
    "until now it has dealt with special orbits of the kerr black hole , but it has not yet produced results for generic orbits because of the difficulty of adjusting the third constant of motion , the carter constant , present in generic geodesics of the kerr spacetime .",
    "however , there have been some recent advances in this direction  @xcite .",
    "an alternative approach consists in trying to describe the radiation reaction effects on the small body as the action of a local _ self - force _ that is responsible for the deviations from the geodesic motion .",
    "a rigorous formulation of this concept has been given by the first time by mino , sasaki and tanaka  @xcite , and later , adopting an axiomatic approach , by quinn and wald  @xcite .",
    "these works give a formal prescription to compute the self - force .",
    "for the practical implementation of these prescription some techniques have been proposed ( see  @xcite for a recent progress report ) : the _ mode - sum _ scheme  @xcite , and a regularization scheme based on zeta - function regularization techniques  @xcite .    in this paper",
    "we want to advocate an alternative , and at the same time complementary , approach : the direct numerical integration of the ( linearized ) evolution equations for the gravitational field together with the evolution equations for the small body , which form a system of partial differential equations ( pdes ) coupled to a system of ordinary differential equations ( odes ) .",
    "we can already find in the literature an attempt to use numerical methods for simulating emrbs  @xcite . in this work ,",
    "the _ perturbations _ are computed by using the full general relativistic equations in the framework of the characteristic formulation ( in contrast to a cauchy - type formulation ) , with the small body being described by an energy - momentum distribution that moves `` rigidly '' along a geodesic of the numerically computed spacetime .",
    "the main drawback of this work is the size of the small body , which is bigger than the mbh horizon .",
    "another approach that has been used in the literature  @xcite is to describe the gravitational field by using the teukolsky formalism  @xcite implemented in the time domain ( using a numerical code introduced in  @xcite ) and by modelling the small body by smearing the singularities in the source term by the use of narrow gaussian distributions .",
    "this work has also the problem that the size of the small body is too big ( in comparison with the size of the mbh ) .",
    "this shows the main underlying difficulty in the numerical simulation of emrbs , namely , the problem involves a vast range of physical scales ( spatial and temporal ) that expand over several orders of magnitude .",
    "specifically , one needs to handle not only large wavelength scales comparable to the massive black hole , but also to resolve the scales in the vicinity of the small object where radiation reaction effects play a crucial role .",
    "an obvious conclusion we can extract from these facts is that , in order to carry out successful numerical simulations of emrbs , we need a high degree of adaptivity .",
    "our proposal is the use of the finite element method ( fem ) as a natural choice to achieve this high level of adaptivity .",
    "finite element methods have not been used occasionally in general relativity ( see  @xcite ) to demonstrate that these numerical techniques have great potential of leading to successful simulations of emrbs , we present results from simulations of a toy problem consisting of a point - like source orbiting a black hole in scalar gravitation in @xmath9 dimensions .",
    "our aim is to test fem techniques in a simple representative problem that possesses the main ingredients and challenges of the astrophysical emrbs .",
    "that is , at this stage we are basically conducting a feasibility study . in our toy model ,",
    "the spacetime metric is fixed and aims at describing the gravitational field of a non - rotating black hole . for computational efficiency , we have made a reduction from three spatial dimensions to two . in this reduction , which we explain in detail later , the metric we work with is not longer a solution of the einstein vacuum equations but it keeps the important property that its geodesics coincide with the equatorial geodesics of the schwarzschild spacetime",
    ". this metric is not dynamical but fixed .",
    "the dynamical gravitational field is described by a scalar field on this spacetime , satisfying a wave - like equation with a source term that describes the presence of the small object .",
    "an important ingredient of our model is the use of a particle description for the small object .",
    "the equations of motion of this particle are the geodesics of the fixed spacetime metric modified by the presence of ( spatial ) components of the gradient of the gravitational scalar field . in this way we have",
    "that the particle orbits the black hole subject to radiation reaction : indeed , the particle generates the scalar gravitational field which affects its own motion .    in this work",
    "we compare numerical simulations that use the simple classical fem with simulations that use an adaptive - mesh fem .",
    "the essence of the adaptive mesh technique is to produce real - time local mesh coarsening or refinement to achieve the desired level of smoothness in the solution . to that end , a good posteriori error estimator to predict the regions in the computational domain where rapidly changes take place is extremely important .",
    "there are several ways to approach this problem",
    ". theoretical research on this subject  @xcite has found that the hessian matrix of the numerical solution can accurately predict where the steepest gradients of the solution would take place .",
    "we demonstrate that this technique when applied to our toy model easily captures the dynamics of the field in the vicinity of the particle since around the particle location either the field or the source term will change much more than anywhere else .",
    "we then refine the local area surrounding the particle and resolve it more accurately so that we are able to achieve our final target .",
    "the plan of this paper is the following : in section  [ sec2 ] we introduce the particular theoretical model we want to study , namely : the description of the gravitational field and of the point - like source , the computational setup , and an energy balance test that can be used to test the numerical computations . in section  [ sec3 ]",
    "we describe the computational techniques that we use for the time - domain simulations of our theoretical model : the fem , the discretization of our equations using finite elements , the numerical method for solving the equations of motion of the particle , and finally , the adaptive finite element method ( afem ) . in section  [ sec4 ]",
    "we present and discuss the numerical results of the simulations . here , we distinguish between the simulations that use the classical fem and those that use the afem .",
    "we devote the last section  [ conclusions ] to discuss the main results of this work and the future perspectives that it opens .",
    "in the following , we describe in detail the main aspects of our toy model , namely : ( i ) the gravitational theory we use , ( ii ) the derivation of the equations to be solved , ( iii ) the particle s description , and ( iv ) the computational setup .",
    "we use physical units in which @xmath10 we use lowercase greek letters for spacetime indices and lowercase latin letters for _ spatial _ indices . in subsections  [ scagrav ] and  [ decompo ] indices run as : @xmath11 @xmath12      scalar gravitation is a theory of gravity that although it can not we applied to our physical world ( we know it can not fit all the available experimental data ) , is a good laboratory for numerical relativity due to its simplicity .",
    "of particular interest for our purposes is the fact that ( scalar ) gravitational waves exist in scalar gravity .",
    "in this theory , the gravitational field is described by a scalar field @xmath13 on a spacetime geometry described by a non - dynamical metric tensor @xmath14 , which we just prescribe ( the version presented in problem 7.1 of the textbook by misner , thorne and wheeler  @xcite and in  @xcite considers only the case in which the non - dynamical spacetime is the flat spacetime ) .",
    "the scalar field does not affect to the spacetime structure defined by @xmath14 .",
    "we are interested in studying the evolution of a particle - like object orbiting a black hole in this theory . to that end",
    ", we consider a background spacetime metric @xmath14 describing the geometry of a non - rotating black hole , and a particle of mass @xmath0 that follows the worldline @xmath15 , where @xmath16 denotes the particle proper time .",
    "the action of this particle - field system is @xmath17 where the comoving density @xmath18 , describing the particle , is given by @xmath19d\\tau       = \\frac{m}{u^t\\sqrt{-g}}\\delta^3[x^i - z^i(t ) ] \\,,\\ ] ] where @xmath20 is a time coordinate and @xmath21 is the corresponding component of the velocity of the particle : @xmath22 varying the action  ( [ scalar ] ) with respect to the scalar field @xmath23 we obtain the gravitational field equation : @xmath24 varying now the action with respect to @xmath25 we obtain the particle s equations of motion @xmath26 equations ( [ fe ] ) and ( [ pe ] ) form a coupled system of partial and ordinary differential equations respectively .",
    "equation ( [ fe ] ) is a hyperbolic and nonlinear equation that describes the dynamical gravitational field whereas ( [ pe ] ) is a system of equations of motion for the particle . in the absence of @xmath27",
    "the particle follows the geodesics of the background spacetime @xmath14 , but in the present of @xmath27 it will not longer follow the geodesic of the background .",
    "the way in which @xmath27 influences the motion of the particle is through its gradient projected orthogonally to the four - velocity of the particle . at the same time , the particle influences the gravitational field @xmath27 through the source term @xmath18 .",
    "the coupling between the particle and the field is where the radiation reaction is encoded : the particle induces a nonzero @xmath23 and this field induces a deviation in the motion of the particle from the background geodesic motion , and at the same this motion affects the evolution of the field , and so on .",
    "therefore , the situation is the same as in the general relativistic case where the particle is treated as a perturbation on the background spacetime of the mbh , and where the radiation reaction mechanism works in the same way .",
    "this system has a well - defined total energy - momentum tensor @xmath28 which is given by @xmath29 it has two differentiated components , one associated with the scalar gravitational field , @xmath30 , and the other with the particle , @xmath31 : @xmath32 .",
    "their expressions are respectively @xmath33 @xmath34 one can show that the energy - momentum conservation equation @xmath35 follows provided matter conservation holds , that is , @xmath36 .      in order to numerically solve the equations ( [ fe],[pe ] ) it is convenient to rewrite them in a 3 + 1 language , which allows for an initial - value problem formulation . to that end , we follow the usual @xmath37 decomposition used in numerical relativity .",
    "that is , we write our spacetime line - element as follows @xmath38 where @xmath39 is the spatial metric of the @xmath40 hypersurfaces , @xmath41 denotes the lapse function and @xmath42 the shift vector .",
    "the normal to the hypersurfaces is : @xmath43 the covariant and contravariant components of the spacetime metric are @xmath44 where @xmath45 is the inverse of @xmath39 .",
    "then , @xmath46 is the projector orthogonal to the hypersurfaces @xmath47 , which also contains the induced metric that these hypersurfaces inherit : its spatial components coincide with @xmath39 .",
    "equation  ( [ fe ] ) for @xmath27 is second - order in space and in time .",
    "we are going to split it into two equations that are first - order in time . to that end",
    ", we introduce the following definition : @xmath48 then , equation ( [ fe ] ) can be split into the following two equations : @xmath49 @xmath50 in the second equation , @xmath51 denotes the trace of the extrinsic curvature @xmath52 of the hypersurfaces @xmath47 @xmath53 the symbols @xmath54 and @xmath55 denote the covariant derivative and laplacian associated with the induced metric @xmath39 . then we can write @xmath56 moreover , @xmath57 denotes the spatial components of @xmath58 . after some calculations we find the following expression for @xmath57 @xmath59    now ,",
    "let us perform the 3 + 1 decomposition of the equations for the trajectory of the particle .",
    "our coordinate system @xmath60 is , in principle , not adapted to the particle trajectory in the sense that @xmath20 is not the proper time and @xmath61 are not comoving coordinates .",
    "then , in this coordinate system the trajectory of the particle is given by @xmath62 , and the unit tangent velocity vector is : @xmath63 but also @xmath64 then , on the trajectory of the particle the following holds @xmath65 due to the fact that @xmath66 is a unit timelike vector field ( @xmath67 ) we do not need to solve for all the components .",
    "one possibility is to solve the particle equations for the quantities @xmath68 .",
    "then , the equations that we get are : @xmath69 where @xmath70 \\partial_\\rho\\phi \\ , , \\label{af}\\end{aligned}\\ ] ] and the velocity component @xmath21 can be written in terms of @xmath71 and the spacetime metric as follows @xmath72 the term @xmath73 gives the contribution to the geodesic motion in the background spacetime @xmath14 , whereas the term @xmath74 describe the deviation from the geodesic motion due to the action of the scalar gravitational field @xmath27 .",
    "we want the background metric @xmath14 to represent the geometry of a non - rotating black hole . in 3 + 1",
    "we can use the schwarzschild metric .",
    "however , to reduce the computational cost of our simulations , we reduce the space of our toy model to two spatial dimensions .",
    "there are several ways in which this reduction can be performed .",
    "for instance , one can start directly with the @xmath9 metric obtained from the schwarzschild metric on the hypersurface @xmath75 .",
    "another possibility would be to start from the equations in the @xmath37 setup , expanding all the different terms and then , to neglect all the derivatives with respect to the coordinate @xmath76 and the components of the different objects in that direction , and finally to set @xmath75 . in performing these dimensional reductions",
    "the metrics we obtain are no longer solutions of einstein s equations in the dimensionally - reduced spacetime , but this is not an issue in our toy model since the spacetime metric is not a dynamical object .",
    "the important thing is that the metric we obtain from these reductions keeps most of the important properties of the schwarzschild metric , in particular , the fact that from the two different reductions mentioned above the equations for the geodesics are the same , and they coincide with the geodesics of schwarzschild in the plane @xmath75 .",
    "the equations for @xmath27 , when we introduce the expression of the dimensionally - reduced metrics , would be in general different . in this work we use the first possibility for the dimensional reduction . as a consequence of this reduction the indices we use in this subsection run as follows : @xmath77 @xmath78    in what follows we specify the form of the 2 + 1 background .",
    "we start from the schwarzschild metric in cartesian kerr - schild coordinates but reducing a space dimension .",
    "that is , our background metric is described by the following line element : @xmath79 where @xmath80 is the 2 + 1 minkowski metric , @xmath81 is a future - directed light - like vector field ( both with respect to the minkowski and schwarzschild metric ) , and @xmath82 is a scalar . in cartesian kerr - schild",
    "coordinates these three objects are given by @xmath83 where @xmath3 is the black hole mass and @xmath84 .",
    "we choose the @xmath85 foliation of the 2 + 1 spacetime .",
    "then , the values of the relevant 2 + 1 quantities that result from this choice are : @xmath86 @xmath87 @xmath88 @xmath89 @xmath90 @xmath91 as it happens in any kerr - schild metric , the determinant of the spacetime metric is equal to @xmath92 and hence we have the following relation between the lapse and the determinant of the spatial metric : @xmath93 which agrees with equations ( [ met1],[met4 ] ) .    the dirac delta function that appears in the source @xmath18",
    "is regularized by using a gaussian distribution : @xmath94   \\approx   \\frac{1}{\\left(\\sqrt{2\\pi}\\,\\sigma\\right)^2 } \\ ; e^{-\\frac{r^2}{2\\,\\sigma^2}}\\,,\\ ] ] where @xmath95 ^ 2\\,.\\ ] ] the source function @xmath18 takes the form @xmath96 as we have already mentioned , @xmath0 is the total rest mass of the particle .",
    "we can recover this quantity , which is a constant of motion , from the following expression : @xmath97 where @xmath98 note that by making the approximation ( [ eq : delta ] ) , the exact character of this relation does not change due to the fact that the integral of the dirac delta and a given gaussian over the whole two - dimensional domain yields the same result . on the other hand",
    ", the matter conservation relation @xmath99 ensures that @xmath0 , as defined by equation  ( [ mass ] ) , is a conserved quantity .",
    "the next thing we need is the explicit form of the equations of motions for the particle , which means that we need to compute the right - hand side of equation  ( [ peq2 ] ) , or equivalently , the terms ( [ ag],[af ] ) , for our spacetime metric  ( [ ksmetric ] ) .",
    "the expressions that we obtain are : @xmath100 @xmath101\\left\\ {   \\left[\\frac{2m}{r^2}x^i-\\left(1+\\frac{2m}{r}\\right ) v^i\\right]\\frac{\\pi}{\\sqrt{1+\\frac{2m}{r } } } \\right .",
    "\\nonumber\\\\ & & \\left .",
    "+ \\frac{\\frac{2m}{r}}{1+\\frac{2m}{r } } \\frac{x^i}{r } \\frac{x^j}{r}\\partial_j\\phi - \\partial^i\\phi \\right\\ } \\ , , \\label{forcep}\\end{aligned}\\ ] ] where we have used that @xmath102^{-1 } \\ , .",
    "\\label{ut2}\\ ] ] it is important to remark that the expression ( [ forceg ] ) has the same form as in the 3 + 1 case , confirming the fact that the geodesics of our 2 + 1 spacetime coincide with the equatorial geodesics ( @xmath75 ) of the schwarzschild spacetime .",
    "to summarize , we have to solve a set of six equations .",
    "two of them are pdes , equations ( [ fe1],[fe2 ] ) , and contain a non linear term , the one that introduces the coupling between the gravitational scalar field and the matter sources .",
    "the other four equations are odes , equations  ( [ peq1],[peq2 ] ) , describing the trajectory followed by the particle of mass @xmath0 .",
    "the two sets of equations are coupled .    to integrate these equations we are going to consider the following type of spacetime domains : @xmath103\\times \\omega$ ] , where @xmath104 and @xmath105 are the initial and final integration times , and @xmath106 is the spatial domain ( see fig .",
    "[ domain ] ) , the domain at every @xmath47 slice , consisting of two circular boundaries : an outer boundary @xmath107 at @xmath108 and a inner boundary @xmath109 at @xmath110 where @xmath111 is the horizon _ radius_. the aim is to locate @xmath112 far enough close to the radiation zone so we can impose standard outgoing radiation boundary conditions . with regard to the inner boundary , the idea is to _ excise _ the black hole singularity from the computational domain , as it is done in many full numerical relativity calculations of black hole dynamics  @xcite , without affecting the computation of the field .",
    "this can be done as long as we have @xmath113 since the characteristics of the pdes ( [ fe1],[fe2 ] ) , for @xmath114 all point in the direction of @xmath115 which entitles us to perform the singularity excision . moreover , because of this property of the characteristics , we do not need to impose any kind of boundary conditions at the inner boundary . on the outer boundary ( @xmath116 ) we use the two - dimensional sommerfeld boundary condition (",
    "see , e.g.  @xcite ) : @xmath117 we can rewrite this condition , using polar coordinates , like @xmath118 .",
    "however , in contrast to what happens in the three - dimensional case , where @xmath119 is an exact solution coinciding with the radiative behaviour of the field ( at large @xmath120 ) , in the two - dimensional case @xmath121 is not a solution of the equations , and therefore the boundary condition ( [ bdc ] ) does not capture correctly the radiative behaviour of the model . in this sense , this boundary condition could be improved along the lines shown in references  @xcite , by considering higher - order derivative boundary conditions , or along the works  @xcite where _ exact _ radiative boundary conditions are explored .",
    ".[domain],title=\"fig:\",height=268 ]    the initial data consists of two functions on @xmath122 , @xmath123 or equivalently @xmath124 , the initial position of the particle @xmath125 and its initial velocity @xmath126 . for the scalar gravitational field we are going to use the simplest initial data , which consists in setting the initial value of @xmath27 and its time derivative equal to zero [ @xmath127 : @xmath128 in terms of @xmath129 this translates to @xmath130 from a physical point of view , this data corresponds to a situation in which the particle comes into existence at the initial time @xmath131 and through the source term in equation ( [ fe2 ] ) induces a non - zero scalar gravitational field .",
    "a consequence of this initial setup is the triggering of an spurious burst of radiation .",
    "the particle s initial data , @xmath132 , is chosen in such a way that it coincides with initial data that , in the absence of scalar gravitational field , would correspond to circular geodesics of our background spacetime  ( [ ksmetric ] ) . without loss of generality , we assume that the particle is located initially at @xmath133 where here we can prescribe freely the initial position @xmath134 . then , the initial velocity is @xmath135 by using our knowledge of the geodesics [ we recall that the geodesics of  ( [ ksmetric ] ) coincide with the equatorial geodesics of the 3 + 1 schwarzschild spacetime ] , we can have an estimation of the time that the particle takes for completing a circular orbit , which we call @xmath136 .",
    "this time can be obtained by using kepler s third law : @xmath137 where @xmath138 is the radius of the circular orbit .",
    "it is important to remark that this expression is exact when we neglect the effect of the scalar gravitational field @xmath27 , otherwise it just gives an estimation for the orbital period .",
    "for an orbital radius @xmath139 we have @xmath140      an important feature of the toy model is the existence of a local conservation law  ( [ energy ] ) that involves both the energy - momentum of the particle and of the dynamical scalar gravitational field .",
    "this local conservation law together with the symmetries of the spacetime lead to global conservation laws .",
    "we can use these global conservation laws as a test for the numerical simulations of our toy model .",
    "let us derive the conservation law associated with the timelike killing vector field that makes our spacetime static , @xmath141 .",
    "contracting equation ( [ energy ] ) with @xmath142 and using the killing equations , @xmath143 we obtain : @xmath144 we are now going to integrate this relation over a compact region of the spacetime , @xmath145 and we use the gauss theorem to convert volume integrals into surface integrals : @xmath146 where @xmath147 is the boundary of @xmath148 , which is a closed hypersurface , and @xmath149 the volume 1-form associated with @xmath150 . in this way we have obtained an energy - momentum conservation law that tell us that the flux of the vector @xmath151 across the boundary of the region @xmath148 must vanish .    the region @xmath148 is chosen as follows ( see figure  [ intdomain ] ) : it consists of two cylinders , @xmath152 and @xmath153 , that extend along the timelike killing direction , and two slices , @xmath154 and @xmath155 , orthogonal to the timelike killing , such that they cut the cylinders forming a closed 2 + 1 spacetime region .",
    "then , the spacetime region @xmath148 can be described as @xmath156 where the angle @xmath157 is defined by @xmath158 @xmath159 and @xmath105 can be chosen as the initial and final integration times respectively ; @xmath160 can be taken to coincide with the outer boundary and @xmath161 can be chosen so that @xmath162 , and hence the black hole region is excluded and @xmath163 is timelike everywhere in @xmath148 . the boundary is then given by @xmath164 where @xmath165 @xmath166     [ see equation ( [ volume ] ) ] where the conservation law ( [ elaw ] ) is tested .",
    "[ intdomain],title=\"fig:\",width=432,height=403 ]    when we apply the conservation law ( [ elaw ] ) to our domain , we obtain the following relation between boundary integrals : @xmath167 this relation can be interpreted by saying that the difference in energy between two given instances of time , @xmath159 and @xmath168 ( which corresponds to the integrals on the spacelike slices @xmath154 and @xmath155 ) is due to the loss of energy through the cylinders @xmath152 ( gravitational waves being absorbed by the horizon ) and @xmath153 ( gravitational waves escaping to infinity ) .    in order to evaluate these surface integrals",
    "we need first to find the normal vector everywhere on the boundary @xmath150 , that is , on each of the disjoint pieces of ( [ boundary ] ) .",
    "the pieces @xmath154 and @xmath155 [ see equation  ( [ boundary2 ] ) ] have constant time @xmath20 , therefore the ( timelike ) normals there are given by @xmath169 whereas the pieces @xmath152 and @xmath153 [ see equation  ( [ boundary1 ] ) ] are cylinders of constant radius @xmath120 . here , it is important to remark that both of the cylinders are assumed to be located at @xmath170 , so that they are timelike hypersurfaces . in practice , we are going to take @xmath161 very close to @xmath171 taking this into account we can write the ( spacelike ) normals as follows : @xmath172    then , the contributions of the gravitational scalar field energy - momentum to the surface integrals are : @xmath173 @xmath174 @xmath175 \\left [ -\\frac{2m}{r_1}\\pi \\nonumber \\right . \\\\ & & \\left .",
    "+ \\frac{1}{\\sqrt{1 + 2m / r_1 } } \\left(\\cos\\theta\\partial_x\\phi+\\sin\\theta\\partial_y\\phi \\right)\\right]\\ , , \\label{intphic1}\\end{aligned}\\ ] ] @xmath176 \\left [ -\\frac{2m}{r_2}\\pi \\nonumber \\right . \\\\ & & \\left .",
    "+ \\frac{1}{\\sqrt{1 + 2m / r_2 } } \\left(\\cos\\theta\\partial_x\\phi+\\sin\\theta\\partial_y\\phi \\right)\\right]\\ , .",
    "\\label{intphic2}\\end{aligned}\\ ] ] the contributions of the particle s energy - momentum to the surface integrals are : @xmath177\\left(u^t \\right)^2\\ , , \\label{intpars1}\\end{aligned}\\ ] ] @xmath178\\left(u^t \\right)^2\\ , .",
    "\\label{intpars2}\\end{aligned}\\ ] ] since the particle density is very small outside a small neighbourhood around the particle location , the contribution due to the particle to the integrals on the cylinders @xmath152 and @xmath153 can be neglected : @xmath179 where in ( [ intphis1 ] ) and ( [ intpars1 ] ) , @xmath27 and @xmath180 mean @xmath181 and @xmath182 respectively . in ( [ intphis2 ] ) and ( [ intpars2 ] ) , @xmath27 and @xmath180 mean @xmath183 and @xmath184 respectively .",
    "both in ( [ intphic1 ] ) and in ( [ intphic2 ] ) we have used polar coordinates instead of cartesian ones ( @xmath185 , @xmath186 , @xmath187 ) . therefore , in ( [ intphic1 ] ) , @xmath27 and @xmath180 mean @xmath188 and @xmath189 respectively ; and in ( [ intphic2 ] ) , @xmath27 and @xmath180 mean @xmath190 and @xmath191 respectively . in ( [ intpars1 ] ) and ( [ intpars2 ] ) , the objects @xmath18 , and @xmath21 must be substituted by their expressions ( [ source ] ) and ( [ ut2 ] ) respectively .",
    "then , the conservation law that we have to test numerically is : @xmath192 \\left [ -\\frac{2m}{r_1}\\pi + \\frac{\\cos\\theta\\partial_x\\phi+\\sin\\theta\\partial_y\\phi } { \\sqrt{1 + 2m / r_1}}\\right]\\nonumber \\\\ % + \\frac{1}{4\\pi}\\frac{r_2}{1 + 2m / r_2}\\int^{t_f}_{t_i}dt \\int^{2\\pi}_0 d\\theta   \\left [ \\pi - \\frac{2m / r_2}{\\sqrt{1 + 2m / r_2 } } \\left(\\cos\\theta\\partial_x\\phi+\\sin\\theta\\partial_y\\phi \\right)\\right ] \\left [ -\\frac{2m}{r_2}\\pi + \\frac{\\cos\\theta\\partial_x\\phi + \\sin\\theta\\partial_y\\phi}{\\sqrt{1 + 2m / r_2}}\\right]\\nonumber \\\\ % + \\int\\int^{}_{{\\cal s}_1 } dx\\,dy\\ ; \\rho\\mbox{e}^\\phi \\left[1-\\frac{2m}{r}+\\frac{2m}{r^2}(x_i v^i)\\right]\\left(u^t",
    "\\right)^2 \\hspace{7.5 cm } \\nonumber \\\\ % -\\int\\int^{}_{{\\cal s}_2 } dx\\,dy\\ ;",
    "\\rho\\mbox{e}^\\phi \\left[1-\\frac{2m}{r}+\\frac{2m}{r^2}(x_i v^i)\\right]\\left(u^t \\right)^2=0\\ , . \\hspace{7 cm }   \\label{claw}\\end{aligned}\\ ] ] in order to decide whether the result from the numerical computation is satisfactory , we can normalize the previous expression with respect , for instance , the initial energy of the particle , i.e. the last but one line in this conservation law .",
    "the goal of this section is twofold .",
    "first , we want to give a brief introduction to the finite element method , a numerical technique that has rarely been used in numerical relativistic calculations .",
    "a detailed basic exposition of the fem can be found in classical textbooks like  @xcite .",
    "second , we want to describe the adaptive finite element method , where we present a new local mesh refinement technique based on an interpolation error estimate introduced recently by chen , sun and xu  @xcite .",
    "this is the technique that we investigate in this paper as a possible tool for achieving the adaptivity that the simulations of emrbs require .",
    "we start from the discretization of the computational domain @xmath106 into an assembly of disjoint element domains @xmath197 , that is @xmath198 in two dimensions the element domains are typically triangles and quadrilaterals . in this work",
    "we use only triangles . in practice ,",
    "mesh generation is carried out by using the software included in the general - purpose software package fepg  @xcite . a typical _ mesh _ ( domain discretization ) for our example",
    "is given in figure  [ meshexample ] .",
    "every element is equipped with a finite - dimensional functional space @xmath199 , so that we approximate our physical solution locally , at every element , as a linear combination of functions of @xmath199 [ usually , a special treatment of the boundary elements is required ] .",
    "it is very common that the functional spaces @xmath199 are formed by piecewise polynomials . in this paper",
    "we consider _ linear elements _ , where the element functions are first - order polynomials , i.e. @xmath200 this choice of element functions implies second - order convergence to the solution in the @xmath201 norm .     in equation  ( [ exeq ] ) .",
    "[ meshexample],title=\"fig:\",width=432,height=403 ]    a very important ingredient of the fem is that it works with an integral form of the equation we want to solve , what is called the _ weak form _ of the problem . to obtain it",
    ", we multiply the equation ( [ exeq ] ) by a _ test _",
    "function @xmath202 , integrate over the domain @xmath106 , and use the gauss or divergence theorem which introduces the boundary conditions ( [ vnbc ] ) .",
    "the result can be written in the form : @xmath203\\equiv ( \\phi\\,,\\,\\partial^2_t \\psi ) + ( \\nabla \\phi\\,,\\,\\nabla\\psi ) - 2\\int_{\\partial\\omega } r\\,\\phi\\,ds + 2\\ , ( \\phi,1 ) = 0 \\ , , \\label{weak}\\ ] ] where @xmath204 denotes the inner product @xmath205 @xmath206 denotes the gradient operator , and @xmath207 is a coordinate on @xmath208 it is important to note that the third term is the result of introducing into the weak formulation the boundary conditions  ( [ vnbc ] ) .    using the element functional spaces we can expand our solution in terms of _ nodal _ basis functions , @xmath209 ( @xmath210 being @xmath211 the number of nodes ) , which are associated with the nodes or grid points of the mesh .",
    "nodal functions use to take the unity value at the node to which they are associated and zero at all the other nodes ( @xmath212 for any node @xmath213 ) . since we are going to produce a fem discretization only in space , the expansion of our solution in terms of the nodal functions can be written in the form : @xmath214 where @xmath215 is the sobolev space of functions on @xmath106 that are , together with their first and second generalized spatial derivatives , square integrable , that is , they belong to @xmath216 we are assuming that @xmath217 belongs to @xmath215 for any time @xmath218\\,.$ ] the subscript @xmath219 denotes a scale associated with the domain discretization , for instance it may be proportional to the square root of the average area of the elements that compose the mesh . in our example @xmath219 refers to the maximum mesh diameter in the whole domain . an obvious property of @xmath219 , is that the size of the elements goes to zero as @xmath219 goes to zero .    in a time - dependent problem like the one we are considering , the unknowns are going to be the functions @xmath220",
    "the equations for these functions are obtained from the spatial discretization . in a galerkin - type formulation of the fem ,",
    "the discretized equations come from the imposition of the vanishing of the residuals : @xmath221 = 0 \\ , ,   \\label{residuals}\\ ] ] which consists in taking @xmath222 in ( [ weak ] ) .",
    "introducing also the expansion ( [ femexpan ] ) yields the following system of equations for the functions @xmath220 @xmath223 where the matrix @xmath224 , the so - called _ mass _ matrix , and the matrix @xmath225 , the so - called _ stiffness _ matrix , are symmetric and positive - definite matrices .",
    "the vector @xmath226 is sometimes called the _ force _ vector .",
    "equation ( [ femdiscre ] ) is the outcome of the fem spatial discretization , which is sometimes called the _ semi - discrete _ form because it consist of a linear system of second - order ordinary differential equations in time .",
    "they are usually solved by using finite - differences methods .",
    "one of the most popular methods for second - order in time equations is the newmark method , which is second - order accurate in time ( see , e.g.  @xcite , for details ) .",
    "before we discuss the numerical implementation it is worth mentioning two important features of the fem : ( i ) the piecewise approximations ( piecewise linear in our example ) of physical fields on finite elements provide good precision even with simple approximating functions . increasing the number of elements",
    "we can achieve the desired precision .",
    "( ii ) the local character of the approximation leads to sparse systems of equations once the problem is discretized .",
    "this helps considerably to solve problems with a very large number of nodal unknowns .",
    "the numerical implementation of the equations of this paper has been carried out by using the general - purpose software package fepg  @xcite , which can automatically generate finite element fortran source code based on component programming .",
    "it can handle many types of problems , including time - dependent non - linear ones , like the one we are interested in . as a test",
    ", we have implemented the example described in this section in fepg ( see a snapshot of the evolution in figure  [ examplesnapshot ] ) .",
    "we have also studied the convergence properties of the solution . here",
    ", it is important to point out that in an unstructured mesh , like the one we are using in this example , to perform a convergence test is not as straightforward as it is for structured meshes .",
    "we have to define properly the scale @xmath219 such that by changing it we obtain the correct convergence . starting from the initial mesh ( see figure  [ meshexample ] ) , we call the solution we obtain @xmath217",
    ". then , we globally refine this initial mesh by transforming every initial triangular element into four smaller triangular elements by connecting the three mid points of each edge . solving our example equation on",
    "this mesh leads to a more accurate solution that we call @xmath227 , and whose associated scale is @xmath228 . by repeating this refinement process",
    "we get finer meshes , and by solving our equation on them we obtain more accurate solutions , @xmath229 , with associated scale @xmath230 we have checked that the solution we obtain converges quadratically in the scale @xmath219 to the exact solution  ( [ exactsol ] ) by studying the norms @xmath231 ( see the left of figure  [ graphsconvergence ] ) .",
    "we have also checked that it convergences quadratically in the usual way , without making use of the exact solution , just by comparing the norms @xmath232 and @xmath233 ( see the right of figure  [ graphsconvergence ] ) .      in this section",
    ", we are going to develop a fem formulation for our toy model in the spirit of the ideas presented above .",
    "the ingredients of this problem are the following : ( i ) the computational domain was described in subsection  [ toysu ] and shown in figure  [ domain ] .",
    "( ii ) the equations that describe our model are the pdes given by equations  ( [ fe1],[fe2 ] ) .",
    "( iii ) the boundary conditions and initial data are described in subsection  [ toysu ] .",
    "we start from the discretization of the computational domain ( see fig .",
    "[ domain ] ) .",
    "we use linear triangular elements . the aspect of the resulting triangularization is shown in fig .",
    "[ triangularization ] .",
    "it is worth pointing out that the fem is specially well suited for complex domains . in our case , this allows us to use circular boundaries , which adapt better to the characteristics of our problem .",
    "this is specially important in the case of the inner boundary , which corresponds to the fact that we have excised the black hole singularity from the computational domain .    .",
    "[ triangularization],title=\"fig:\",width=432,height=403 ]    the next step in our development is the finite element discretization of the equations  ( [ fe1],[fe2 ] ) .",
    "we need to construct a _ weak _ formulation of these equations . to that end , it is very convenient to rewrite the equations in the following equivalent form : @xmath234 where we have used the fact that @xmath235 the main reason for casting the equations in this form is that it brings the second - derivative terms into the form of the divergence of a spatial vector , without any additional factors .",
    "as we did with the example of the previous subsection , we are going to discretize the system of pdes  ( [ mfe1],[mfe2 ] ) for the unknowns @xmath27 and @xmath180 by using a fem discretization ( a galerkin - type formulation ) for the spatial dimensions and by using finite differences methods in time .",
    "in this sense , it is important to take into account that our system of equations is of first order with respect to time derivatives and of second order with respect to spatial derivatives .",
    "in order to discretize these equations , particular attention has to be paid to the convection terms in order to keep them under control .",
    "we deal with this issue by including additional artificial viscosity into the finite element equations that we obtain .    for the fem spatial discretization the finite element space that we use",
    "is @xmath236 , which consists of piecewise triangular linear interpolation functions .",
    "then , taking all this into account and operating in a similar way as we did in the example of subsection  [ example ] , the discretized problem that we obtain can be introduced in the following way : we need to find @xmath237 such that the equations @xmath238 @xmath239 hold for any @xmath240 and for @xmath241 @xmath242 where @xmath243 is the total computational time .",
    "@xmath244 is the time - step size we use to evolve @xmath27 and @xmath245 we use the notation @xmath246 to denote the inner product in @xmath247 , which is defined as @xmath248 .",
    "finally , to deal with the _ convection _ terms , arising from the derivatives of the fields along the shift vector , we use a streamline diffusion scheme that includes artificial viscosity and which is specially adapted to the fem .",
    "the factor @xmath249 is the penalty ratio of artificial viscosity , which of course depends on the mesh size @xmath219 .",
    "it has to be tuned properly in order to obtain a stable computation and at the same time an accurate solution .",
    "the last terms of ( [ femfe2 ] ) consist of integrals on the boundaries @xmath109 and @xmath107 .",
    "the way they come into play is the following : in the case of the outer boundary @xmath107 , we obtain the last but one integral in ( [ femfe2 ] ) after integration by parts of the term with second - order spatial derivatives in  ( [ mfe2 ] ) and imposition of the outgoing boundary condition  ( [ bdc ] ) . in the case of the inner boundary @xmath250",
    ", we obtain the last integral in ( [ femfe2 ] ) from the same integration by parts .",
    "however , the resulting integral does not correspond to a proper boundary condition . as we have already mentioned before",
    ", we do not need any boundary condition at the inner boundary due the particular structure of the characteristics there ( remember that the inner boundary is inside the horizon ) , which all point inwards .",
    "therefore , the resulting integral is just an integration of a term proportional to @xmath251 on the inner boundary .",
    "equations ( [ femfe1],[femfe2 ] ) are the basis of the computational procedure that we follow to obtain the solution of our problem .",
    "the basic algorithm consists in computing first @xmath252 from ( [ femfe1 ] ) in terms of @xmath253 and @xmath254 .",
    "the next step is to introduce @xmath252 into the right - hand side of ( [ femfe2 ] ) to obtain @xmath255 in terms of @xmath253 and @xmath254 together .",
    "then , @xmath255 has to be used in order to compute the value of @xmath27 in the next time step , and so on .",
    "it is not difficult to show that this algorithm is second - order accurate both in space and in time .      in the procedure",
    "we have just described we have omitted the role of the motion of the particle .",
    "it is clear that in order to evaluate the right - hand side of ( [ femfe2 ] ) we need to introduce the position of the particle . and for that , we need to solve the set of equations ( [ peq1],[peq2 ] ) .",
    "these odes have to be integrated simultaneously with the pdes , which means that every time we evaluate the right - hand of ( [ femfe2 ] ) we need to evolve them a time step @xmath256 and then , to use use the outcome of the integration of the odes ( new position and velocity of the particle ) in order to evaluate the source of equation  ( [ fe2 ] ) [ equation  ( [ femfe2 ] ) in the discretized system ] .",
    "the type of numerical algorithm we use to solve the odes ( [ peq1],[peq2 ] ) has to take into account the particular structure of the equations  ( [ peq2 ] ) , which are the non - trivial ones .",
    "these equations contain two differentiated terms , @xmath73 and @xmath74 .",
    "the first term would give us the geodesic motion around a schwarzschild black hole , and therefore the time scale of the changes induced by the term @xmath73 is the orbital period of the geodesic that the particle would follow by ignoring the radiation reaction effects .",
    "the second term contains the gradients of the scalar field @xmath27 in the neighbourhood of the particle position .",
    "these terms are the responsible , in our toy model , of the radiation reaction effects , and therefore , the time scale of the changes they induce will be in general much smaller than the orbital time scale .",
    "the afem is the application of the classical fem on a series of local adaptive meshes in order to get more accurate numerical results with less computational cost .",
    "starting with a given initial coarse mesh , the adaptive mesh on each level is generated locally and adaptively in terms of a posteriori error estimate of the finite element solution .",
    "usually local refinement takes place in places where the a posteriori errors are much bigger than elsewhere , or in other words , where the finite element solution changes steeply . on the other hand , for those places in which the a posteriori errors are sufficiently small",
    ", the local derefinement will operate in order to eliminate extra grids because in those places the solution changes slowly .",
    "thus , during the finite element computation we can adaptively adjust the mesh density without losing the numerical accuracy but reducing considerably the computational cost .",
    "it is obvious that the key part of the afem is the a posteriori error estimate . to have a good such estimate",
    "means that one can precisely find out in which places the mesh should be refined or derefined without introducing much numerical pollution .",
    "after this is done , the rest of the procedure is standard .",
    "for instance , the mesh bisection and the finite element approximation . there is presently a number of works in the literature on these issues ( see , e.g.  @xcite ) . in what follows",
    "we introduce our own a posteriori error estimate , which is an interpolation error estimate .",
    "then , we elaborate on our local mesh improvement techniques such as refinement , coarsening , and the smoothing strategy , which aim to minimize the interpolation error .",
    "the interpolation error estimate comes from recent work by chen , sun , and xu  @xcite .",
    "this estimate can be seen as the theoretical foundations of our adaptive mesh techniques , that is , our algorithms are aimed at minimizing ( or at least reducing ) the interpolation error by iteratively modifying the grids .",
    "we introduce the estimate through the following definition : `` let @xmath106 be a bounded domain in @xmath270 given a function @xmath271 , we say that a symmetric positive definite matrix @xmath272 is a majoring hessian of @xmath273 if @xmath274 for some positive constant @xmath275 '' . here , @xmath276 denotes the transpose of the vector @xmath277 we then use the majoring hessian to define a new metric @xmath278    there are two conditions for a triangulation @xmath279 , where @xmath211 is the number of simplexes ( generic elements ) , to be a nearly optimal mesh in the sense of minimizing the interpolation error in the @xmath280 norm .",
    "the first condition consists in asking the mesh to capture the high oscillations of the hessian metric , that is , @xmath82 should not change very much on each element .",
    "this condition can be expressed in a more precise way by means of the following statement : `` there exists two positive constants @xmath281 and @xmath282 such that @xmath283 where @xmath284 denotes the average of @xmath82 over @xmath285 '' .",
    "the second condition demands the triangularization @xmath279 to be _ quasi - uniform _ under the new metric induced by @xmath286 .",
    "this condition can also be express in a more precise way through the following statement :  there exists two positive constants , @xmath287 and @xmath288 , such that @xmath289 where @xmath290 denotes the volume of @xmath291 and @xmath292 the length of the @xmath293-th edge of @xmath16 under the new metric @xmath294 the first inequality in ( [ a2 ] ) means that each @xmath16 is shape - regular under the metric @xmath295 .",
    "the second inequality means that all elements @xmath16 are of comparable size ( also under the new metric ) , which is a global condition .",
    "this means that the mesh is more dense at the regions where @xmath296 is larger . in  @xcite",
    ", it has been proven the important result that a triangulation that satisfies both the local condition ( [ a1 ] ) and the global one ( [ a2 ] ) yields a good approximation .",
    "this result can be expressed in a precise way in the form of a theorem : _",
    "`` let @xmath273 be a function belonging to @xmath297 , @xmath279 a triangularization satisfying the conditions ( [ a1 ] ) and ( [ a2 ] ) , and @xmath298 the linear finite element interpolation of @xmath273 based on the triangulation @xmath299 then , the following error estimate holds : @xmath300{\\det(h)}\\right\\|^{}_{l^\\frac{pn}{2p+n}(\\omega ) } \\,,\\ ] ] for some constant @xmath301 .",
    "this error estimate is optimal in the sense that for a strictly convex ( or concave ) function , the above inequality holds in a reversed direction . '' _",
    "the result expressed by this theorem is the basis of the grid adaptation algorithms that we use in this work . roughly speaking , for a given function @xmath273",
    ", we will adapt our grids in such a way that the conditions given in ( [ a1 ] ) and ( [ a2 ] ) are satisfied in the best possible way .",
    "one important remark we need to make is that the validity of the theorem stated above allows for a few exceptions of the condition ( [ a2 ] ) for @xmath302 ( see  @xcite for details ) .",
    "this is of particular importance since in practice it is very difficult to guarantee that the condition ( [ a2 ] ) is satisfied everywhere .    the next point is to see how the hessian matrix of the solution can be obtained when the linear finite element approximation is used for the discretization of the pdes of our problem . since taking piecewise second derivatives of piecewise linear functions",
    "will give no approximation to the hessian matrix , special post - processing techniques need to be used in order to obtain a reasonable hessian matrix approximation from linear finite elements .",
    "one of the most popular techniques is the patch recovery technique proposed by zienkiewicz and zhu ( zz )  @xcite , which is based on the least squares fitting on local patches .",
    "results from the application of this technique demonstrate that it is robust and efficient .",
    "the theoretical reason why the zz method works is largely understood to be related to the superconvergence phenomenon for second - order elliptic boundary - value problems discretized on a finite element grid that has certain local symmetry ( see the works of walhbin  @xcite and babuka and strouboulis  @xcite ) . these classic superconvergence results can be used in order to justify the effectiveness of the zz method .",
    "a significant improvement of this type of analysis was introduced recently by bank and xu  @xcite . in  @xcite",
    "they give superconvergence estimates for piecewise linear finite element approximations on quasi - uniform triangular meshes where most pairs of triangles that share a common edge form approximate parallelograms . in  @xcite",
    "they also analyze a post - processing gradient recovery scheme , showing that @xmath303 , where @xmath304 is the global @xmath201 projection , is a superconvergent approximation to @xmath305 .",
    "this result leads to a theoretical justification of the zz method for such type of grids ( see xu and zhang  @xcite for details ) .",
    "the gradient recovery algorithm that we use in the numerical examples of this paper is based on a new approach due to bank and xu  @xcite , where they use the smoothing iteration of the multigrid method to develop a post - processing gradient recovery scheme .",
    "this scheme proves to be very efficient for recovering hessian matrices .",
    "all the methods mentioned above can be extended to anisotropic grids with some appropriate modifications , but a theoretical justification of such extensions is still lacking .",
    "nevertheless , numerical experiments have given satisfactory results .",
    "let us now discuss techniques to improve the mesh quality .",
    "we define the mesh quality of a given triangulation @xmath306 in terms of the interpolation error : @xmath307 there are three main ways of improving a mesh : ( i ) refinement or coarsening through splitting or merging of edges  @xcite ; ( ii ) edge swapping by replacing sets of elements by other such sets while preserving the position of the points ( nodes )  @xcite ; and ( iii ) mesh smoothing , which moves the vertexes of the mesh while keeping the connectivity  @xcite .",
    "we derive those techniques by minimizing the interpolation error in the @xmath280 norm .    for the first method ,",
    "this can be done by equidistributing the edge lengths with respect to the new metric .",
    "thus , we compute the edge lengths with respect to the new metric @xmath295 and mark edges whose length is greater than @xmath308 , where @xmath309 is a parameter and @xmath310 is a fixed edge length , the global average edge length . then , we connect marked edges element - wise according to the different situations that can be given .",
    "this is illustrated in figure  [ fig : refinecoarsening ] .",
    "the coarsening operates like an inverse procedure to the refinement process .",
    "it marks the edges whose length is smaller than @xmath311 , where @xmath312 is another parameter .",
    "we then shrink this edge to a point and connect to the vertexes of the patch of the edge .",
    ", title=\"fig:\",width=326,height=134 ]    we consider now the case of edge swapping involving four points @xmath313 ( @xmath314 ) constituting two adjacent triangles and a convex quadrilateral .",
    "let @xmath315 and @xmath316 be two triangularizations , where @xmath317 stands for the triangle made up of the points @xmath318 @xmath319 and @xmath320 then , we choose the triangulation @xmath321 if and only if @xmath322 for some @xmath323 in  @xcite , we show this criteria is equivalent to the empty circle criteria when @xmath324 thus it is an appropriate generation of the edge swapping used in the isotropic case to the anisotropic case .    finally , local smoothing of the mesh adjusts the location of a vertex in its patch @xmath325 , which consists of all simplexes containing the vertex @xmath326 , without changing the connectivity .",
    "the moving of a vertex to its new location is expected to improve the quality of the mesh .",
    "several sweeps throughout the whole mesh can be performed to improve the overall mesh quality . by minimizing the interpolation error in @xmath325",
    ", we move the vertex @xmath326 to the position @xmath327 in such a way that @xmath328 the derivation of this formula can be found in  @xcite .    for the application to our numerical computations we use @xmath329 and @xmath330 in ( [ eq : center ] ) .",
    "moreover , we need to make choices for the different parameters of this adaptive mesh technique . for the order of the @xmath331 of the @xmath280 norm we use @xmath332 , the @xmath333 norm . according to  @xcite ,",
    "the @xmath333 norm can catch singularities more efficiently than other norms . for the multiple @xmath120 ( @xmath334 in the text ) of the global average edge length @xmath310 under the new metric @xmath295 ( we use @xmath335 as the threshold to determine which edge lengths under the new metric are bigger than it",
    ", we will then bisect those edges in the local refinement process ) we take @xmath336 initially .",
    "our local refinement procedure is a nested iteration process .",
    "there is an outer iteration for which , on each step , we reduce the multiple @xmath120 till @xmath337 ( we take @xmath338 ) .",
    "the main purpose of this iteration is to resolve the singularity as precisely as possible and to control the refinement pollution at the lowest level . on each outer iteration step",
    ", we perform an inner iteration , and there another number @xmath120 to control this iteration . usually we do not need this number to establish a criterion to stop the inner iteration of local refinement .",
    "for the sake of getting the optimal mesh , we let this iteration go until all the edge lengths are smaller than @xmath335 as before . in practice",
    "there are occasions when this may take too much time , and in those cases we set up a criterion as the ones before in order to control the number of inner iterations .",
    "in this section we report the results of the simulations of the toy model .",
    "we have distinguished two types of simulations .",
    "those that were performed by just using the classical fem , without adding any extra adaptivity .",
    "the results are discussed in subsection  [ without ] . and those that were performed by using the afem described in the previous section .",
    "the results of these simulations are discussed in subsection  [ with ] .",
    "the mass ratio we have considered is @xmath339 , that is the particle s mass is @xmath340 the inner boundary is located at @xmath341 , inside the horizon @xmath342 the outer boundary has been located at @xmath343 . regarding the initial data , the only parameters that need to be given , in addition to the ones already given , in order to completely specify it [ see subsection  [ toysu ] ] are the width of the gaussian that we use to regularize the dirac delta distribution , which we take to be @xmath344 , and the initial position of the particle , which we take to be @xmath345 finally , we give the resolution that we have used for the simulations . to give a measure of the spatial resolution",
    "we describe the structure of the mesh .",
    "it is composed of @xmath346 triangles , quasi - uniformly distributed , more concentrated near the center and coarsening gradually as we approach the outer boundary .",
    "regarding the resolution in the time direction , the step size we use for the time evolution is @xmath347 .",
    "the algorithm we use to study the evolution of the gravitational scalar field @xmath27 and the motion of the particle is completely described by the explicit schemes of equations ( [ femfe1],[femfe2 ] ) and ( [ particlescheme ] ) .",
    "what we have observed is that the orbit of the particle ( remember that the initial data corresponds to a circular orbit in the absence of the scalar gravitational field ) shrinks gradually until the particle reaches the horizon at @xmath348 .",
    "the trajectory followed by the particle is drawn in figure  [ trajectoryfem ] .",
    "snapshots characterized by the time @xmath20 and time - step number @xmath349 of the evolution of the scalar gravitational field are shown by means of the contour plots given in figures  [ snapshot1]-[snapshot5 ] .",
    "the position of the particle is evident in these graphs .",
    "we have checked the energy - balance law  ( [ claw ] ) for the parameters given above .",
    "the result can be found in figure  [ etesta ] , where the horizontal axis denotes the time and the vertical axis indicates the value of the left - hand side of ( [ claw ] ) in units of @xmath350 . as we can see there , the energy law  ( [ claw ] ) is satisfied up to a certain level along the evolution .",
    "however , from @xmath351 the error in the energy - balance test grows and stabilizes around a different but bigger value .",
    "this growth is due to outer boundary effects : the particle started from a position @xmath139 and the outer boundary is located at @xmath352 as we have mentioned in the discussion of the boundary conditions in subsection  [ toysu ] , the outer boundary condition  ( [ bdc ] ) is not optimal , and induces some error in the solution .",
    "this could be improved by either moving the outer boundary further out or by using an improved outer boundary condition .    , title=\"fig : \" ]    the key point in these computations is the ability of our numerical scheme to resolve the source term describing the particle .",
    "it enters the equations for the scalar gravitational field @xmath27 as a very singular source term .",
    "this source term depends on the position of the particle , but at the same time , the position of the particle depends of the gradients of @xmath269 therefore , it is very important first to compute properly the effect of the source @xmath18 on the field @xmath27 , and then to compute accurately the gradient of the field itself .",
    "we have already mentioned that in the computations shown in this subsection the width of the gaussian has been taken to be @xmath344 in order to make the source sufficiently smooth for the resolution we have used . in this sense , if we try to use a smaller width , for instance @xmath353 the classical fem will fail to provide the expected accuracy , although it still provides a numerical solution .",
    "here is where one realizes the potential of the afem as a better choice to carry numerical computation for a model presenting features similar to the ones of our model . in the next subsection",
    "we describe how we have implemented the afem in the case of the toy model and show that it provides a reasonable solution for the case in which @xmath354    , title=\"fig:\",width=480 ]      we have just concluded that the central part of the numerical simulation of our toy model , and more in general of emrbs , is the proper resolution of the matter source , @xmath355 and of the gravitational field , @xmath23 in the surroundings of the particle position , which is a key issue in order to compute accurately the motion of the particle and hence , the waves that are emitted as a consequence . to that end , within the framework we have established above , we want to able to compute accurately for small values of the particle s width @xmath356 . in the limit @xmath357",
    "we recover the distributional description of the particle s that appears in the continuum description .",
    "then , it is obvious that better accuracy in the terms discussed above means to increase the resolution around the particle s position , and in this sense the afem is a natural choice since it provides the resolution required at the different regions of the computational domain , which maintains the computational cost at realistic levels .",
    "according to the theoretical foundations of the adaptive mesh technique presented above , in order to apply mesh adaptivity successfully in our numerical computations , first of all , we have to study which quantity in the problem we are dealing with can be used in order to determine where and how the mesh should be refined . in technical terms",
    "this means that we have to be able to determine the places where refinement ( or derefinement ) has to take place in terms of the hessian matrix of the selected quantity .",
    "we have to look for place where the majoring hessian matrix is much bigger than anywhere else , which are the places where the quantity we have chosen changes very fast . in the case of our toy model there is no too much choice .",
    "we can either take the field @xmath27 or the source @xmath18 . in the case of @xmath27",
    ", it is interesting in the sense that it is one of the goals of our computations .",
    "however , since it is the solution of a wave - type equation , it evolves like a wave in the sense that the profile of the solution will have maxima and minima which will be captured by the adaptive mesh procedure , and hence most parts of the domain will be eventually refined , and this may not be the most efficient way to proceed .",
    "the other choice , the particle s source @xmath355 is a better choice for the simple reason that we are also solving for the particle s position at the same time that we are solving the pdes for the field .",
    "therefore , the recognition of the regions that need refinement is going to be simpler in our problem .",
    "that is , the most efficient way of adapting the mesh for our computations is to concentrate on the region where the particle is present and follow it from there .",
    "if we look at the behaviour found in the previous subsections ( see figures  [ snapshot1]-[snapshot2 ] ) , it happens that the bigger values of @xmath27 occur around the particle s position .",
    "therefore , refining there is very convenient at that stage of the computation .",
    "a possible drawback of using the particle s source as the refinement criterion is that the situation changes at later times when the field @xmath27 is present all over the computational domain and it may be a need for refinement in other places like close to the horizon .",
    "however , in the numerical simulations we have performed we find that using the source term @xmath18 for refinement is much better than using the field @xmath269 by calculating the hessian matrix of the particle s source we can exactly know where and how to get local mesh refinement on the particle .    for the numerical simulations of the toy model with the afem we use the same parameters as in subsection  [ without ] , excepting for the width of the gaussian , which we have been able to reduce down to @xmath354 and this has been done using a number of triangular elements in the interval @xmath358 ( the number of elements changes with the evolution , depending on how the adaptivity is implemented ) , a number comparable to the one used in the calculations without adaptivity .",
    "that is , working with the afem we have been able to use a width @xmath356 an order of magnitude smaller than with the classical fem , using a comparable number of elements .",
    "the time step is now @xmath359 half the one used in the previous simulations .",
    "the trajectory followed by the particle is drawn in figure  [ trajectoryafem ] .",
    "we show the evolution of the scalar gravitational field in figures  [ afemshot1]-[afemshot5 ] .",
    "these are contour plots as the ones we used previously . in order to show how we refine the mesh in the evolution we have superposed the mesh to the contour plots . in this way",
    "we can see how the high - resolution part of the mesh moves with the particle .",
    "one can clearly see the difference between this case and the previous case : the particle inspirals much faster in this case , and it hardly completes more than one orbit . to understand this difference , it is important to remark the fact that the particle sources are different since the gaussian profiles have a different width @xmath356 and the fact that the sources couple non - linearly with the scalar gravitational field . as a consequence ,",
    "the difference in @xmath356 leads to a different evolution .",
    "we have checked that when take @xmath360 with the afem we recover the type of trajectories we get in the case without adaptivity .",
    ", title=\"fig : \" ]    a more detailed view of the structure of the refined region around the particle is shown in figure  [ localref ] , where one can see the different levels of refinement , and how the resolution increases as one approaches the particle .",
    "a global perspective of the structure of the mesh , containing the excised region around the singularity of the black - hole , and the refined region around the particle can be seen in the three - dimensional graph shown in figure  [ 3dplot ] .",
    ", title=\"fig : \" ]    , title=\"fig : \" ]    we have also checked the energy - balance law  ( [ claw ] ) for the simulations with adaptivity .",
    "we have plotted the result in figure  [ etestwa ] , where again the horizontal axis denotes time and the vertical axis the value of the left - hand side of ( [ claw ] ) normalized with the mass @xmath0 of the particle .",
    "the behaviour of the error in the conservation law is very similar to the one in the case without adaptivity , where the error in the energy - balance test grows after the boundary affects the particle . in this case",
    ", the error does not seem to stabilize as clearly as in the previous case , and in average the error in the conservation law is bigger .",
    "however , one has to take into account that the evolution in this case is much faster since the particle has plunged very quickly .",
    "moreover , since the number of elements in both cases is comparable , but in the case of the afem a considerable amount of them have been used to increase the resolution around the particle , this means that the density of elements near the boundaries is less and therefore if errors are propagated from there , they can affect more the result than in the case without adaptivity .",
    "this can be avoided by using more computational power to push the boundaries far enough so that we can neglect their effect during the significant part of the evolution .    ,",
    "in this work we have studied the application of finite element techniques to the time - domain numerical simulations of emrbs .",
    "more specifically , we have shown how finite elements can help us in achieving the degree of adaptivity that the computation of radiation reaction effects around the small body requires . however , adaptivity is not the only advantage in using finite elements for the study of emrbs , some of which are obvious from the study of the toy model we have considered in this work . in short , the main additional advantages that finite elements provide are : ( i ) versatility .",
    "the fem can be applied to a wide range of problems : static , quasi - static , transient , highly dynamical , linear and nonlinear , etc .",
    "moreover , the practical implementation of the fem can be easily designed in a modular way , as it shown by the existence of a number of multi - purpose finite element packages .",
    "this is a good property for the design of complex numerical codes , as the ones one may need for the description of astrophysical emrbs .",
    "( ii ) many of the procedures that one uses in the framework of the fem have a solid theoretical foundation , in the sense that behind them one often finds rigorous mathematical analysis ( examples of this can be found in the classical textbooks  @xcite ) .",
    "( iii ) the ability of the fem to manage problems with complex geometries .",
    "for instance we can easily consider spherical boundaries which is helpful in imposing boundary conditions and also in implementing excision techniques for black hole singularities .",
    "this is something that may be worth to export to fully numerical relativistic calculations of spacetimes containing black holes .",
    "( iv ) in relation with the previous issue , the imposition of boundary conditions can be done in a somewhat natural way in a fem framework .",
    "the idea is that by adapting the mesh to the geometry of the problem under study , boundary conditions which are also adapted to that geometry will be incorporated in a natural way into the fem discretization through the _ weak _ formulation of the problem .",
    "the outgoing boundary conditions that we have used in our toy model are a good example , its implementation is simpler that it would have been in a similar finite differences scheme .",
    "( v ) another advantage of the fem , that we have not explored in this paper , is that it is to some extent natural to handle distributions , like for instance the typical dirac delta distributions that appear in physical systems containing particles ( e.g. , emrbs ) .",
    "the idea is that we can use the exact properties of the distributions at the level of the _ weak _ formulation of the equations , which in the case of dirac delta distributions leads automatically to a regularization of the singularity associated with that kind of distributions .",
    "we can apply this idea , for instance , to the calculation of the general - relativistic perturbations of a non - rotating black hole , produced by the presence of a point - like object , via the regge - wheeler - zerilli formalism .",
    "then , the type of fem discretization we would get is similar to the one obtained by price and lousto  @xcite , where they also used an integral form of the equations in order to obtain a discretization .",
    "in addition , the fem procedure can be generalized to higher - dimensional problems in a straightforward way .",
    "these ideas will be the subject of future investigations    in this paper we have illustrated the capabilities of the fem and its suitability to simulate emrbs , by applying it to a simplified model , a toy model that retains the basic ingredients and difficulties of general relativistic emrbs .",
    "we have shown simulations of this system based on the classical fem and simulations based on the afem .",
    "an outcome of this work is the realization that in order to increase the resolution around the small body , treated as a particle , the introduction of adaptivity in the region of the particle is necessary and in this sense the afem is a natural tool to use .",
    "the primary benefit of the afem is that it can provide an efficient , accurate and reliable computational analysis of very large continuum problems , for only a relatively small fraction of the cost associated with the non - adaptive fem .",
    "the accuracy of a finite element solution is directly dependent on the number of free parameters used to mathematically represent the problem , and how effectively those parameters , or mathematical degrees of freedom , are distributed over the problem space .",
    "furthermore , the full computational cost associated with obtaining a finite element solution is related to both the number and the interconnectivity of the degrees of freedom used in the problem discretization .",
    "consequently , the most efficient distribution of degrees of freedom for a problem is the one that provides a sufficiently accurate solution for the lowest number of free parameters .",
    "currently , the only practical way to achieve this objective is by using adaptive solution strategies which are capable of cleverly evolving and improving an efficient distribution of degrees of freedom over the problem domain by establishing solution error distributions , and then adjusting or adding degrees of freedom to the discretization in order to correct them . by increasing the numbers of degrees of freedom only in the regions of higher error in the solution , it is possible to make the most significant improvement in the global accuracy of the finite element solution for the minimum additional computational cost . in this paper",
    "we employed the hessian matrix of the solution to describe the solution error distribution , which can perfectly guide us to where and how we have to locally refine the mesh without any unnecessary pollution . in this sense , it is important to emphasize that the rest of adaptive strategies available do not have this advantageous property .",
    "moreover , this comes without paying an extra price since the main advantages of the other adaptivity techniques are present in the afem that we have used .",
    "the natural continuation of this work is the transfer of the technology used here to the general relativistic problem .",
    "one possible way is to consider the general framework of metric perturbations in the setup of the 3 + 1 decomposition , that is , to use 3d perturbative numerical relativity , trying at the same time to profit from the experience gained in 3d full numerical relativity .",
    "however , 3d calculations using the afem may be at present computationally too demanding and therefore , other avenues should be also explored . among them",
    "we can consider 1d calculations restricted to the case of a non - rotating mbh , by using the well - known regge - wheeler and zerilli - moncrief formalisms , or just by using a harmonic gauge , where the computation of self - forces seems more natural .",
    "this would be an interesting benchmark to test further the fem techniques in a general - relativistic context . from here , one can study problems of more interest for gravitational wave physics related to lisa ( involving spinning mbhs ) by considering 2d calculations using the curvature based formalism of teukolsky for linear perturbations around kerr black holes .",
    "one can go beyond by considering also perturbations of kerr black holes using metric perturbations in a harmonic gauge , where the computation of self - forces can be carried out by using techniques already present in the literature .",
    "cfs and pl acknowledge the support of the center for gravitational wave physics funded by the national science foundation under cooperative agreement phy-0114375 . this work was partially supported by nsf grant phy-0244788 to penn state university ."
  ],
  "abstract_text": [
    "<S> extreme mass ratio binary systems , binaries involving stellar mass objects orbiting massive black holes , are considered to be a primary source of gravitational radiation to be detected by the space - based interferometer lisa . </S>",
    "<S> the numerical modelling of these binary systems is extremely challenging because the scales involved expand over several orders of magnitude . </S>",
    "<S> one needs to handle large wavelength scales comparable to the size of the massive black hole and , at the same time , to resolve the scales in the vicinity of the small companion where radiation reaction effects play a crucial role . </S>",
    "<S> adaptive finite element methods , in which quantitative control of errors is achieved automatically by finite element mesh adaptivity based on posteriori error estimation , are a natural choice that has great potential for achieving the high level of adaptivity required in these simulations . to demonstrate this , we present the results of simulations of a toy model , consisting of a point - like source orbiting a black hole under the action of a scalar gravitational field . </S>"
  ]
}