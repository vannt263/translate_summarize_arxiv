{
  "article_text": [
    "a tight lower bound for the mutual information between a binary and an arbitrary finite random variable with joint distributions that have a variational distance not greater than a known value to a known joint distribution can be found by minimizing over this set of joint distributions .",
    "unfortunately , in general this minimization problem is hard to solve , since the mutual information is not convex in the joint distribution .",
    "therefore this minimization problem is split up into two subproblems .",
    "if the marginal probability of the binary random variable is fixed , then the mutual information can easily be minimized over the conditional probabilities of the second random variable , since the mutual information is convex in the conditional probabilities ( * ? ? ?",
    "* theorem 2.7.4 ) and the set of conditional probabilities is convex ( see theorem [ theorem1 ] ) and therefore this optimization problem is convex .",
    "this constitutes the first subproblem which can easily be solved by standard methods for convex optimization .    in the second subproblem ,",
    "having a closer look on the marginal probability distribution of the binary random variable , one first recognizes that this is only one - dimensional since the two probabilities have to sum up to 1 .",
    "next , the variational distance between the joint probabilities is greater or equal than the variational distance of the marginal probabilities , as is shown in ( [ eq_var ] ) .",
    "therefore one can simply generate sufficiently many marginal probability distributions equidistantly in the one dimension left , solve the first subproblem for every of these marginal probability distributions and return the smallest mutual information calculated that way .    in the next section",
    "the notation is fixed . in section [ sec_results ] the details of the method",
    "are given . in section [ sec_num ]",
    "some numerical examples are shown .",
    "let @xmath0 , @xmath1 be a pair of finite discrete random variables , with joint probability distribution @xmath2 here @xmath3 and @xmath4 and it is w.l.o.g",
    ". assumed that @xmath5 and that @xmath6 .",
    "the marginal probability distributions are @xmath7 and @xmath8 .",
    "they are calculated from the joint probalility distributions as usual .",
    "the conditional probability distributions are @xmath9 it is defined that @xmath10 .",
    "the product of the marginal distributions is denoted as @xmath11    for any two joint probability distributions @xmath12 , @xmath13 the relative entropy or kullback - leibler distance @xcite is defined as @xmath14 and the mutual information between @xmath0 and @xmath1 @xcite as the relative entropy between the joint probability distribution and product of the marginal probability distributions of @xmath0 and @xmath1 @xmath15 all @xmath16s are assumed to be natural if not stated otherwise .",
    "the variational distance between two joint probability distributions is defined as @xmath17 and similarly for the marginal distributions .",
    "it can be easily seen , that @xmath18 $ ] for any two probability distributions .",
    "first it is shown that set of all conditional probability distributions constrained by a maximal variational distance is convex .",
    "[ theorem1 ] let @xmath19 be any fixed joint probability distribution of any two two discrete finite random variables @xmath0 , @xmath1 , let @xmath20 be any fixed probability distribution of @xmath0 and let @xmath21 be any fixed number @xmath22 $ ] .",
    "then the set @xmath23 is convex .",
    "let @xmath24 be any two conditional probability distributions @xmath25 .",
    "then one only has to show that the convex combination @xmath26 , with @xmath27 $ ] is also in @xmath28 . before this",
    "is done , it is defined that @xmath29 , @xmath30 and @xmath31 .",
    "now , to proof that @xmath32 , one only has to show that @xmath33 .",
    "herefore @xmath34 where the fact that any norm ball is convex ( * ? ? ?",
    "* section 2.2.3 ) has been used in ( [ eq_normball ] ) .",
    "also , the further constraints implied by the probability simplex ( which is convex ) are no problem since an intersection of convex sets is always convex ( * ? ? ?",
    "* section 2.3.1 ) .",
    "since the empty set is convex , no restriction on @xmath35 ( e.g. @xmath36 ) is necessary .",
    "[ cor1 ] let @xmath12 be any fixed joint probability distribution of any two two discrete finite random variables @xmath0 , @xmath1 , let @xmath20 be any fixed probability distribution of @xmath0 and let @xmath21 be any fixed number @xmath22 $ ] . then , the optimization problem @xmath37 is convex .",
    "the mutual information @xmath38 is a convex function of the conditional probabilities @xmath39 when @xmath20 is fixed , and the set @xmath40 is convex .",
    "corollary [ cor1 ] basically says that the optimization problem given is practically solvable .",
    "however , since it is a general convex optimization problem , it can still be cumbersome to find a suitable algorithm with the correct parameters .",
    "fortunately the problem can be restated in such a way , that it can be handled by disciplined convex programming ( dcp ) @xcite , which works perfectly well for this problem as can be seen in section  [ sec_num ] .",
    "the minimization problem in corollary [ cor1 ] can not be solved in a straightforward manner with dcp , since this would violate the no product rule of dcp ( see ( [ eq_rel_entr ] ) , ( [ eq_mi ] ) ) , also there is no built function in cvx ( which is the software which implements dcp ) for the mutual information as a function of the conditional probabilities when the corresponding marginal probability is fixed .",
    "therefore the relative entropy , which is a built in function in cvx and is convex in its two input arguments , is used .",
    "then it can be seen that @xmath41 and @xmath42 are affine functions of @xmath43 as @xmath44 are .",
    "hence , the convexity of @xmath45 is preserved @xcite , and it is straightforward to implement the minimization problem in corollary [ cor1 ] with cvx with this knowledge .",
    "next the second subproblem , namely the minimization of the mutual information over the marginal probability distribution @xmath20 , is solved .",
    "herefore it is first shown that @xmath46 therefore only @xmath20 with @xmath47 have to be considered . until here",
    "all results are applicable to any finite @xmath48 , but from here the restriction @xmath49 applies . in this case",
    "@xmath20 is one dimensional obviously , and the set of all @xmath20 is simply @xmath50 \\}$ ] .",
    "practically , the minimization problem @xmath51 is then simply solved by generating sufficiently many @xmath20 equidistantly in @xmath52 , solve the optimization problem of corollary  [ cor1 ] for every @xmath20 and return the smallest mutual information calculated that way . here the number of @xmath20s is considered to be sufficient if one gets a smooth graph for the mutual information minimized over the conditional probabilities @xmath39 as a function of @xmath52 .",
    "together with the bound on the probability of a maximal variational distance between the true joint distribution and an empirical joint distribution ( see @xcite , and especially an refinement of it which drops the dependence on the true distribution ( * ? ? ?",
    "* lemma 3 ) ) the given bound can be used to construct a reasonably tight lower bound of the confidence interval for mutual information .",
    "such an application can be found in @xcite . in mutual information estimation with confidence intervals ,",
    "the bound given is especially useful , when the marginal probability distribuition is far from being uniform .",
    "such a situation can be found in @xcite . in the case of two binary random variables",
    "the results seem to coincide with lower bound of @xcite .",
    "in the first example ( fig .  [ fig : mi1 ] ) a distribution @xmath12 and a maximal variational distance @xmath21 was handpicked to show that the mutual information minimized over the transitional probabilies @xmath39 as a function of @xmath52 is neither convex nor concave ( even for two binary random variables ) and seems to be not differentiable at @xmath53 , as can be seen in fig .",
    "[ fig : mi1 ] . the parameters chosen therefore are @xmath54 then",
    ", @xmath55    in all figures @xmath56 is equal to the minimum of @xmath38 over @xmath39 for fixed @xmath57 , constrained by @xmath58 , and @xmath59 points were generated equidistantly for @xmath60 $ ] .",
    "[ c][c]@xmath56 [ c][c]@xmath52     in the second example ( fig .",
    "[ fig : mi2 ] ) @xmath61 and the following joint distribution was chosen at random ( rounded for easier reproducibility ) @xmath62 then , @xmath63    [ c][c]@xmath56 [ c][c]@xmath52     in the last example ( fig .",
    "[ fig : mi3 ] ) @xmath64 and the following joint distribution was chosen at random ( rounded for easier reproducibility ) @xmath65 then , @xmath66    [ c][c]@xmath56 [ c][c]@xmath52",
    "the authors would like to thank the dfg for supporting their research with spp1395 in the projects hu634_7 and sti155_3 .    1",
    "t.  m. cover and j.  a. thomas , _ elements of information theory _",
    "new york : wiley , 2006 .",
    "s. boyd and l. vandenberghe , _",
    "convex optimization_. cambridge university press , 2004 .",
    "m.  c. grant , s. boyd , and y. ye , _ cvx : matlab software for disciplined convex programming _ , 2005 .",
    "available at http://www.stanford.edu/`~`boyd/cvx/ s .- w . ho and r.  w. yeung , `` the interplay between entropy and variational distance , '' _ ieee trans .",
    "inform . theory _",
    "59065929 , dec . 2010 .",
    "a.  g. stefani , j.  b. huber , c. jardin and h. sticht , `` towards confidence intervals for the mutual information between two binary random variables , '' in _ proc . workshop computational systems biology ( wcsb 2012 ) _ , ulm , germany , jun . 46 , 2012 . t. weissman , e. ordentlich , g. seroussi , s. verd and m.j .",
    "weiberger , `` inequalities for the l1 deviation of the empirical distribution , '' tech .",
    "rept . , hp laboratories palo alto , hpl-2003 - 97 ( r.1 ) , jun . 2003 .",
    "o.  g. othersen , a.  g. stefani , j.  b. huber and h. sticht , `` application of information theory to feature selection in protein docking , '' _ j mol model .",
    "_ , vol .  18 , no",
    ". 4 , pp . 12851297 , jul",
    "a.  g. stefani , j.  b. huber , c. jardin and h. sticht , `` confidence intervals for the mutual information , '' available at http://arxiv.org/abs/1301.5942 ."
  ],
  "abstract_text": [
    "<S> `` this paper is eligible for the student paper award '' .    in this paper </S>",
    "<S> a numerical method is presented , which finds a lower bound for the mutual information between a binary and an arbitrary finite random variable with joint distributions that have a variational distance not greater than a known value to a known joint distribution . </S>",
    "<S> this lower bound can be applied to mutual information estimation with confidence intervals . </S>"
  ]
}