{
  "article_text": [
    "trace - norm regularization plays a vital role in various areas , such as machine learning  @xcite , data mining  @xcite , computer vision and image processing  @xcite .",
    "most trace - norm based problems can be formulated into the following general formulation  @xcite : @xmath2 where @xmath3 is a regularization parameter , @xmath4 is the trace - norm ( also known as the nuclear - norm ) of a matrix @xmath5 , both @xmath6 and @xmath7 are linear operators depending on specific applications  @xcite , @xmath8 denotes data or observations , @xmath9 can be considered as an error term and the trace norm @xmath10 is the tightest convex lower bound to the rank function @xmath11  @xcite , and the minimization of ( [ eq : general_form ] ) encourages the variable @xmath12 to be low - rank  @xcite . among various trace - norm",
    "based problems , the low - rank matrix recovery ( mr )  @xcite , have gained particular interest in the last decade .    mr  seeks to recover a low - rank matrix @xmath12 from partial observations that are recorded in a vector @xmath13 , where @xmath14 .",
    "if there are no outliers in the observations , one can recover @xmath12 with high probability by solving the following problem  @xcite : @xmath15 can be deemed as a simplified version of formulation ( [ eq : general_form ] ) .",
    "mr has been successfully applied in many tasks such as matrix completion filtering  @xcite . however , the recovery performance by solving problem ( [ eq : mc ] ) might be seriously degraded if the observations contain severe outliers  @xcite . to improve the robustness",
    ", we may introduce an additional variable @xmath9 into the constraint @xmath16 as in ( [ eq : general_form ] ) , and regularize it using @xmath17-norm regularization (  @xmath18 ) or @xmath19-norm regularization (  @xmath20 )  @xcite .    seeks to find a low - rank representation of given data @xmath21 by solving an optimization problem of the following form : @xmath22 where denotes the , and @xmath23 encourages the representation error @xmath9 to be column - wise sparse .",
    "lrr has been widely applied in many real - world tasks such as motion segmentation and face clustering  @xcite .",
    "many algorithms have been proposed to solve trace - norm regularized problems  @xcite , but most focus on solving problem ( [ eq : mc ] ) , such as the singular value thresholding ( svt )  @xcite , augmented lagrangian method ( alm ) and alternating direction method ( adm )  @xcite . unlike these methods , some researchers proposed to solve an equivalent problem to problem ( [ eq : mc ] )  @xcite : @xmath24 where @xmath25 is a regularization parameter .",
    "this problem is known as the _ matrix lasso _ , and can be addressed by proximal gradient ( pg ) or accelerated proximal gradient ( apg )  @xcite .",
    "the optimization of problem ( [ eq : general_form ] ) is more challenging due to the additional variable @xmath9 . by minimizing @xmath12 and @xmath9",
    "alternatively , the aforementioned methods ( _ e.g. _ , adm and pg ) have been extended to solve this problem  @xcite .",
    "the above methods have shown great success in practice  @xcite .",
    "however , the optimization usually involves repetitive svds due to the svt operation , making them inefficient on large - scale problems  @xcite .",
    "using homotopy strategies and applying rank prediction techniques may accelerate the convergence speed with truncated svds  @xcite . however , the rank prediction could be non - trivial in general , and large - rank svds is still inevitable if the optimal solution has a large rank .    to develop more scalable algorithms ,",
    "some researchers have tackled a version of the problem in which it is assumed that the rank of @xmath12 is known ,  @xmath26 , and thus proposed to solve a variational form of problem ( [ eq : mc ] )  @xcite : @xmath27 where @xmath28 and @xmath29 .",
    "many methods have been developed to address this problem , such as gradient based methods  @xcite and stochastic gradient methods  @xcite .",
    "however , these methods may still suffer from slow convergence speeds  @xcite .",
    "recently , fixed - rank methods by exploiting the smooth geometry of matrices on fixed - rank manifolds have shown great advantages in computation for solving matrix recovery problems  @xcite , such as the low - rank geometric conjugate gradient method ( lrgeomcg )  @xcite , the quotient geometric matrix completion method ( qgeommc )  @xcite , and the method of scaled gradients on grassmann manifolds for matrix completion ( scgrassmc )  @xcite .",
    "however , these methods can only deal with smooth objectives .",
    "moreover , the rank parameter @xmath1 is usually unknown in practice , and nontrivial to discover .    motivated by the superiority of riemannian gradient - based methods on low - rank matrix recovery problems  @xcite , in this paper , we exploit classical proximal gradient methods and geometries of the real - algebraic variety @xmath0 to address the problem in ( [ eq : general_form ] ) .",
    "the main contributions of this paper are as follows :    we propose a proximal riemannian gradient ( prg ) scheme to address trace - norm regularized problems with explicit rank constraint @xmath30 . by exploiting geometries on @xmath0",
    ", prg avoids repetitive large - scale svds of classical proximal methods , making it more scalable .    to address general trace - norm regularized problems in ( 1 ) , we present a simple and novel active subspace framework which incorporates prg as a slave solver",
    ". this framework does not require the prior knowledge of @xmath1 and large - rank svds , and can even accelerate the convergence speed of prg .",
    "let the superscript @xmath31 denote the transpose of a vector / matrix , @xmath32 be a vector / matrix with all zeros , @xmath33 be a diagonal matrix with diagonal elements equal to @xmath34 , @xmath35 be the inner product of @xmath36 and @xmath37 , and @xmath38 be the @xmath39-norm of a vector @xmath34 .",
    "let @xmath6 be a linear operator with @xmath40 being its adjoint operator .",
    "the operator @xmath41 operates on each dimension of @xmath42 .",
    "let @xmath43 be the svd of @xmath44 .",
    "the nuclear norm of @xmath12 is defined as @xmath45 and the frobenius norm of @xmath12 is defined as @xmath46 .",
    "lastly , for any convex function @xmath47 , let @xmath48 denote its subdifferential at @xmath12 .",
    "we now introduce some of the basic notions of the geometry of fixed - rank matrices and matrix varieties as follows .",
    "* geometries of fixed - rank matrices*. the fixed rank-@xmath1 matrices lie on a smooth submanifold defined below @xmath49 where @xmath50 denotes the stiefel manifold of @xmath51 real and orthonormal matrices , and the entries in @xmath42 are in descending order  @xcite .",
    "moreover , the tangent space @xmath52 at @xmath12 is given by    @xmath53    given @xmath54 and @xmath55 , by defining a metric @xmath56 , @xmath57 is a * riemannian manifold * by restricting @xmath58 to the _ tangent bundle _",
    "@xcite . ] the norm of a tangent vector @xmath59 evaluated at @xmath12 is defined as @xmath60 .",
    "once the metric is fixed , the notion of the gradient of an objective function can be introduced . for a riemannian manifold",
    ", the * riemannian gradient * of a smooth function @xmath61 at @xmath62 is defined as the unique tangent vector @xmath63 in @xmath52 , such that @xmath64 , ~~\\forall \\bxi \\in t_{\\bx}\\mm_{r}$ ] . as @xmath65",
    "is embedded in @xmath66 , the riemannian gradient of @xmath67 is given as the * orthogonal projection * of the gradient of @xmath67 onto the tangent space . here , the orthogonal projection of any @xmath68 onto the tangent space @xmath52 at @xmath69 is defined as @xmath70 where @xmath71 and @xmath72 .",
    "moreover , define @xmath73 when @xmath74  @xcite .",
    "letting @xmath75 be the gradient of @xmath76 on vector space , it follows that @xmath77 the _ retraction _ mapping on @xmath57 relates an an element in the tangent space to a corresponding point on the manifold .",
    "one of the issues associated with such retraction mappings is to find the best rank-@xmath1 approximation to @xmath78 in terms of the frobenius norm @xmath79 in general , this problem can be addressed by performing svd on @xmath80 , which may be computationally expensive .    since @xmath81 , as in algorithm 6 in  @xcite with efficient qr decompositions on low rank matrices @xmath82 and @xmath83 .",
    "@xmath84 , where @xmath85  @xcite .",
    "* varieties of low - rank matrices*. note that the submanifold @xmath57 is open , and the manifold properties break down at the boundary where @xmath86 , and the convergence analysis on @xmath57 will be difficult accordingly  @xcite .",
    "therefore , it would be more convenient to consider the closure of @xmath65 : @xmath87 which is a real - algebraic variety  @xcite .",
    "let @xmath88 be the column space of @xmath12 . in the singular points where @xmath89",
    ", we will construct search directions in @xmath90 where @xmath91 and @xmath92 .",
    "let @xmath93 be the projection of @xmath94 on @xmath95 .",
    "it can be computed by @xmath96 given a search direction @xmath97 , we need perform retraction which finds the best approximation by a matrix of rank at most @xmath1 as measured in terms of the frobenius norm ,  @xmath98 since @xmath99 , @xmath100 w.r.t .",
    "@xmath0 can be efficiently computed with the same complexity as on @xmath65",
    "directly solving the general trace - norm regularized problem in ( [ eq : general_form ] ) can be computationally expensive due to the unknown rank of variables ( regarding fixed - rank methods ) or large - rank singular value decompositions ( svds ) ( regarding proximal gradient based methods ) .",
    "@xmath101 here , the parameter @xmath1 is supposed to be known .",
    "nevertheless , based on ( [ eq : general_form_fixed_rank ] ) , we will propose a subspace pursuit paradigm to solve see details in section 4 .",
    "the penalty method is adopted to deal with the equality constraint @xmath102 in ( [ eq : general_form_fixed_rank ] ) , and it minimizes a penalized function over @xmath0 in the following form is a vector , the @xmath103-norm will be replaced by the @xmath104-norm . ] :    @xmath105    where @xmath25 is a penalty parameter . note that when there are no outliers , we can let @xmath106 and @xmath107 , and the objective function @xmath108 is reduced to @xmath109 @xmath110 is also the objective function of the _ matrix lasso _",
    "problem  @xcite , so one can adapt classical proximal methods  @xcite to address it .",
    "however , proximal gradient methods which directly operate on vector spaces could be very expensive if large - rank svds are required . in this section , we extend classical proximal methods on vector space  @xcite , and propose a _",
    "proximal riemannian gradient _ scheme to minimize ( [ eq : form_penaly ] ) and ( [ eq : form_penaly_nooutlier ] ) by exploiting geometries over the matrix variety @xmath0 . is a closure of the riemannian submanifold @xmath65 . here , we abuse  riemannian \" for simplicity . ]",
    "the objective function @xmath110 regarding non - outlier cases is much simpler than @xmath108 .",
    "are no outliers , we solve the following optimization problem : @xmath111 where @xmath76 is any smoothing function , for example @xmath112 .    to introduce proximal methods on @xmath0 , similarly as in  @xcite , we introduce a local model of @xmath110 on @xmath0 around @xmath113 but keeping @xmath4 intact : @xmath114 where @xmath115 and @xmath116 . note the above local model is different from that on vector spaces ( see  @xcite ) in the sense that @xmath117 is restricted on @xmath118 . similar to classical proximal gradient methods  @xcite , our proximal rimannian gradient method problem ( [ eq : general_trace_rank ] ) by minimizing @xmath119 on @xmath0 iteratively . in other words , given @xmath120 in the @xmath121th iteration , we need to solve the following optimization problem to obtain @xmath122 : @xmath123 for convenience , let @xmath124 be a minimizer of ( [ eq : local_model ] ) .",
    "then it can be computed as follows .",
    "[ lemma : closed_solution ] let @xmath125 . denoting the svd of @xmath126 as @xmath127",
    ", it follows that @xmath128 .",
    "please find the proof in supplementary file .",
    "@xmath129 can be efficiently computed in the sense that @xmath130 can be cheaply computed without expensive svds .",
    "@xmath131 , penalty parameter @xmath25 , parameter @xmath1 , stopping tolerance @xmath132 . for",
    "@xmath133 +  compute @xmath134 according to ( [ eq : grad_mr ] ) or ( [ eq : grad ] ) . +  choose",
    "@xmath135 to satisfy ( [ eq : armijo ] ) , and set @xmath136 .",
    "terminate if stopping conditions are achieved .",
    "end return @xmath137 .",
    "\\1 ) compute a search direction in step 3 , and 2 ) update here , @xmath138 can be deemed as the step size , and it can be determined using armijo line search . specifically , given a descent direction @xmath139 , @xmath135 is determined such that @xmath140 where @xmath141 .",
    "* optimality condition * of ( [ eq : general_trace_rank ] ) .",
    "a point @xmath142 is a local minimizer of ( [ eq : general_trace_rank ] ) if and only if there exists @xmath143 such that  @xcite @xmath144    the following lemma guarantees the existence of @xmath135 .    [ lemma : step_size ] let @xmath145 , and @xmath146 be a descent direction .",
    "then there exists an @xmath135 that satisfies the condition in ( [ eq : armijo ] ) .",
    "since @xmath147 is a descent direction , it follows that @xmath148 and @xmath149 .",
    "since @xmath150 is continuous in @xmath151 , there must exist an @xmath152 such that @xmath153 @xmath154 .    in general , optimization methods on riemannian manifolds",
    "are guaranteed to be locally convergent , and it is nontrivial to check whether a limit point @xmath155 is a global solution or not . however , for prg , the limit point @xmath155 will be a global solution if @xmath156 .",
    "[ sec : global ] let @xmath157 be an infinite sequence of iterates generated by algorithm [ alg : pgr ]",
    ". then every accumulation point of @xmath157 is a critical point of @xmath67 over @xmath0 .",
    "furthermore , @xmath158 . let @xmath155 denote the limit point .",
    "in particular , if @xmath159 , then we have @xmath160 , _ i.e. _ , @xmath155 is a global optimum to ( [ eq : general_trace_rank ] ) .",
    "note that @xmath110 is bounded below .",
    "the proof can be completed by adapting the proof of theorem 3.9 in @xcite .    * stopping conditions * of prg . for simplicity , we stop prg if the following condition is achieved : @xmath161 where @xmath132 denotes a tolerance value .",
    "now , we extend prg to minimize @xmath108 in([eq : form_penaly ] ) regarding the outlier cases . for convenience ,",
    "define    @xmath162    we then need to solve the following problem : @xmath163 following  @xcite , we optimize the two variables @xmath12 and @xmath9 using an alternating approach .",
    "let the pair @xmath164 denote the variables obtained from the @xmath121-iteration . at the @xmath165th iteration , we update @xmath12 and @xmath9 as below :    to update @xmath12 , we fix @xmath166 and minimize a local model of @xmath108 w.r.t .",
    "@xmath12 : @xmath167where @xmath168 , @xmath169 , and @xmath170 is a positive number .",
    "let @xmath171 denote the minimizer of @xmath172 .",
    "then @xmath171 can be computed according to lemma [ lemma : closed_solution ] , where @xmath151 is determined by armijo line search to make a sufficient decrease of the objective .",
    "to update @xmath9 , we fix @xmath173 and solve a problem : @xmath174 solving this problem with general @xmath7 would be very difficult . however , for mr and lrr , @xmath175 and @xmath176 is either @xmath177 or @xmath23 . as a result , the problem ( [ eq : update_e ] ) has a closed - form solution .",
    "let us define @xmath178 .",
    "then @xmath179 is a vector for mr and a matrix in the form of @xmath180 $ ] for lrr .",
    "the closed - form solution , denoted by @xmath181 , is shown in table [ table : update_e ] . in cases where the problem ( [ eq : update_e ] ) can not be solved in closed - form",
    ", one may adopt iterative procedures to solve it .",
    "the detailed algorithm , which is referred to as robust prg ( rprg ) , is shown in algorithm [ alg : prg_penlty ] . due to the possible ill - conditioned issues , is very small or @xmath25 is very large , @xmath177 can be very large at the beginning due to the thresholding in table 1 , making @xmath182 far from its optimum .",
    "] we apply a homotopy continuation technique to accelerate the convergence speed .",
    "starting from an initial guess @xmath183 , we set @xmath184 and compute @xmath185 , where @xmath186 is chosen from @xmath187 .",
    "clearly , @xmath188 is non - increasing w.r.t . @xmath121 .",
    ".computation of @xmath189 . [ cols=\"^,^,^,^ \" , ]      we conduct experiments on the extended yale face database b ( * extyaleb * ) for face clustering , and the human activity recognition using smartphones dataset ( * harus * ) @xcite for human activity clustering",
    ".  following @xcite , the clustering performance is measured by _ clustering accuracy _ , namely the number of correctly clustered samples over the total number of samples .",
    "the extyaleb dataset contains @xmath190 frontal face images of  38 subjects with different lighting , poses and illumination conditions , where each subject has round 64 faces .",
    "following @xcite , we use @xmath191 faces from the first 10 subjects . each face image",
    "is resized to @xmath192 pixels and then reshaped as a @xmath193-dimensional gray - level intensity feature .",
    "the harus dataset is a large dataset ( containing 10,299 signals w.r.t .  6 activities ) with data collected using embedded sensors on the smartphones carried by volunteers on their waists , when they are conducting daily activities ( _ e.g. _ ,  walking , sitting , laying ) .",
    "the captured sensor signals are pre - processed to filter noise and post - processed . finally ,",
    "a 561-dimensional feature vector with time and frequency domain variables is extracted for each signal .    the best clustering accuracies and the corresponding running times are reported in it can observed that , outperforms the two existing lrr solvers in terms of efficiency , since our algorithm does not svds  w.r.t . large matrices .",
    "moreover , our algorithm achieves comparable clustering performance with @xcite .",
    "in contrast , the lrr solver in @xcite achieves lower clustering accuracy on the harus dataset , possibly because that algorithm is not guaranteed to obtain a globally optimal solution .",
    "classical proximal methods may require many large - rank svds when addressing the trace - norm regularized problems on vector spaces . to overcome this , we first propose a proximal riemannian gradient ( prg ) method to address trace - norm regularized problems over a matrix variety @xmath0 , where @xmath1 is supposed to be known . by performing optimization on @xmath0",
    ", prg does not require svds , thus can greatly reduce the computation cost .",
    "a robust version of prg method has also been proposed to to address general trace - norm regularized problems , a subspace pursuit strategy is proposed by iteratively activating a number of active subspaces .",
    "extensive experiments on two classical trace - norm based tasks , namely low - rank matrix completion and lrr based clustering , demonstrate the superior efficiency of the proposed methods over other methods .",
    "d.  anguita , a.  ghio , l.  oneto , x.  parra , and j.  l reyes - ortiz .",
    "human activity recognition on smartphones using a multiclass hardware - friendly support vector machine . in _",
    "ambient assisted living and home care _ , pages 216223 ."
  ],
  "abstract_text": [
    "<S> trace - norm regularization plays a vital role in many learning tasks , such as low - rank matrix recovery ( mr ) , and low - rank representation ( lrr ) . solving this problem </S>",
    "<S> directly can be computationally expensive due to the unknown rank of variables or large - rank singular value decompositions ( svds ) . to address this </S>",
    "<S> , we propose a proximal riemannian gradient ( prg ) scheme which can efficiently solve trace - norm regularized problems defined on real - algebraic variety @xmath0 of real matrices of rank at most @xmath1 . based on prg , </S>",
    "<S> we further present a simple and novel subspace pursuit ( sp ) paradigm for general trace - norm regularized problems without the explicit rank constraint @xmath0 . </S>",
    "<S> the proposed paradigm is very scalable by avoiding large - rank svds . </S>",
    "<S> empirical studies on several tasks , such as matrix completion and lrr based subspace clustering , demonstrate the superiority of the proposed paradigms over existing methods . </S>"
  ]
}