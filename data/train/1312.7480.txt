{
  "article_text": [
    "malliavin weight sampling ( mws ) is a method for computing derivatives of averaged system properties with respect to parameters in stochastic simulations @xcite .",
    "the method has been used in quantitative financial modelling to obtain the `` greeks '' ( price sensitivities ) @xcite ; and , as the girsanov transform , in kinetic monte carlo simulations for systems biology @xcite .",
    "similar ideas have been used to study fluctuation - dissipation relations in supercooled liquids @xcite .",
    "however , mws appears to be relatively unknown in the fields of soft matter , chemical and biological physics , perhaps because the theory is relatively impenetrable for non - specialists , being couched in the language of abstract mathematics ( _ e.g. _ ,  martingales , girsanov transform , malliavin calculus , _ etc .",
    "_ ) ; an exception in financial modelling is ref .",
    "@xcite .",
    "mws works by introducing an auxiliary stochastic quantity , the malliavin weight , for each parameter of interest .",
    "the malliavin weights are updated alongside the system s usual ( unperturbed ) dynamics , according to a set of rules .",
    "the derivative of any system function , @xmath0 , with respect to a parameter of interest is then given by the average of the product of @xmath0 with the relevant malliavin weight , or in other words by a weighted average of @xmath0 , in which the weight function is given by the malliavin weight .",
    "importantly , mws works for non - equilibrium situations , such as time - dependent processes or driven steady states .",
    "it thus complements existing methods based on equilibrium statistical mechanics , which are widely used in soft matter and chemical physics .",
    "mws has so far been discussed only in the context of specific simulation algorithms . in this paper , we present a pedagogical and generic approach to the construction of malliavin weights , which can be applied to any stochastic simulation scheme .",
    "we further describe its practical implementation in some detail using as our example one dimensional brownian motion in a force field .",
    "the rules for the propagation of malliavin weights have been derived for the kinetic monte - carlo algorithm @xcite , for the metropolis monte - carlo scheme @xcite and for both underdamped and overdamped brownian dynamics @xcite . here",
    ", we present a generic theoretical framework , which encompasses these algorithms and also allows extension to other stochastic simulation schemes .",
    "we suppose that our system evolves in some state space , and a point in this state space is denoted as @xmath1 . here",
    ", we assume that the state space is continuous , but our approach can easily be translated to discrete or mixed discrete - continuous state spaces .",
    "since the system is stochastic , its state at time @xmath2 is described by a probability distribution , @xmath3 . in each simulation step",
    ", the state of the system changes according to a propagator , @xmath4 , which gives the probability that the system moves from point @xmath1 to point @xmath5 during an application of the update algorithm .",
    "the propagator has the property that @xmath6 where @xmath7 is the probability distribution after the update step has been applied and the integral is over the whole state space .",
    "we shall write this in a shorthand notation as @xmath8 integrating eq .   over @xmath5 , we see that the propagator must obey @xmath9 .",
    "it is important to note , however , that we do _ not _ assume the detailed balance condition , @xmath10 for some equilibrium @xmath11 .",
    "thus , our results apply to systems whose dynamical rules do not obey detailed balance ( such as chemical models of gene regulatory networks @xcite ) , as well as to systems out of steady state .",
    "we observe that the ( finite ) product @xmath12 is proportional to the probability of occurrence of a trajectory of states , @xmath13 , and can be interpreted as a _ trajectory weight_.",
    "let us now consider the average of some quantity , @xmath14 , over the state space , in shorthand @xmath15 the quantity , @xmath0 , might well be a complicated function of the state of the system : for example the extent of crystalline order in a particle - based simulation , or a combination of the concentrations of various chemical species in a simulation of a biochemical network .",
    "we suppose that we are interested in the sensitivity of @xmath16 to variations in some parameter of the simulation , which we denote as @xmath17 .",
    "this might be one of the force field parameters ( or the temperature ) in a particle - based simulation or a rate constant in a kinetic monte carlo simulation .",
    "we are interested in computing @xmath18 .",
    "this quantity can be written as @xmath19 where @xmath20 let us now suppose that we track in our simulation not only the physical state of the system , but also an auxiliary stochastic variable , which we term @xmath21 . at each simulation step , @xmath21 is updated according to a rule that depends on the system state ; this does not perturb the system s dynamics , but merely acts as a `` readout '' . by tracking @xmath21 ,",
    "we _ extend _ the state space , so that @xmath1 becomes @xmath22 .",
    "we can then define the average @xmath23 , which is an average of the value of @xmath21 in the extended state space , with the constraint that the original ( physical ) state space point is fixed at @xmath1 ( see further below ) .",
    "our aim is to define a set of rules for updating @xmath21 , such that @xmath24 ,",
    "_ i.e. _ ,   such that the average of the auxiliary variable , for a particular state space point , measures the _ derivative _ of the probability distribution with respect to the parameter of interest , @xmath17 . if this is the case then , from eq .",
    "@xmath25 the auxiliary variable , @xmath21 , is the malliavin weight corresponding to the parameter , @xmath17 .",
    "how do we go about finding the correct updating rule ?",
    "if the malliavin weight exists , we should be able to derive its updating rule from the system s underlying stochastic equations of motion .",
    "we obtain an important clue from differentiating eq .   with respect to @xmath17 .",
    "extending the shorthand notation , one finds @xmath26 this strongly suggests that the rule for updating the malliavin weight should be @xmath27 in fact , this is correct .",
    "the proof is not difficult and , for the case of brownian dynamics , can be found in the supplementary material for ref .",
    "it involves averaging eq .   in the extended state space ,",
    "@xmath22 .    from a practical point of view , for each time step , we implement the following procedure :    * propagate the system from its current state , @xmath1 , to a new state , @xmath5 , using the algorithm that implements the stochastic equations of motion ( brownian , kinetic monte - carlo , _ etc . _ ) ; * with knowledge of @xmath1 and @xmath5 , and the propagator , @xmath4 , calculate the change in the malliavin weight @xmath28 ; * update the malliavin weight according to @xmath29 .    at the start of the simulation , the malliavin weight is usually initialised to @xmath30 .",
    "let us first suppose that our system is not in steady state , but rather the quantity @xmath16 in which we are interested is changing in time , and likewise @xmath31 is a time - dependent quantity . to compute @xmath31 , we run @xmath32 independent simulations , in each one tracking as a function of time @xmath33 , @xmath34 and the product , @xmath35 .",
    "the quantities @xmath36 and @xmath31 are then given by    [ eq : samp ] @xmath37 & \\frac{\\partial{\\langle a(t)\\rangle}}{\\partial\\lambda}\\approx   \\frac{1}{n}\\sum_{i=1}^n a_i(t)\\,q_{\\lambda , i}(t)\\,,\\end{aligned}\\ ] ]    where @xmath38 is the value of @xmath33 recorded in the @xmath39th simulation run ( and likewise for @xmath40 ) .",
    "error estimates can be obtained from replicate simulations .",
    "if , instead , our system is in steady state , the procedure needs to be modified slightly .",
    "this is because the variance in the values of @xmath34 across replicate simulations increases linearly in time ( this point is discussed further below ) . for long times , computation of @xmath18 using eq .",
    "therefore incurs a large statistical error .",
    "fortunately , this problem can easily be solved , by computing the correlation  function @xmath41\\rangle}\\ , .",
    "\\label{eq : c1}\\ ] ] in steady state , @xmath42 , with the property that @xmath43 as @xmath44 . in a single simulation run ,",
    "we simply measure @xmath45 and @xmath33 at time intervals separated by @xmath46 ( which is typically multiple simulation steps ) . at each measurement , we compute @xmath47 $ ] .",
    "we then average this latter quantity over the whole simulation run to obtain an estimate of @xmath18 . for this estimate to be accurate , we require that @xmath46 is long enough that @xmath48 has reached its plateau value ; this typically means that @xmath46 should be longer than the typical relaxation time of the system s dynamics .",
    "the correlation function approach is discussed in more detail in refs .",
    "@xcite .    returning to a more theoretical perspective",
    ", it is interesting to note that the rule for updating the malliavin weight , eq .  , depends deterministically on @xmath1 and @xmath5 .",
    "this implies that the value of the malliavin weight at time @xmath2 is completely determined by the trajectory of system states during the time interval , @xmath49 .",
    "in fact , it is easy to show that @xmath50 where @xmath51 is the trajectory weight defined in eq .  .",
    "similar expressions are given in refs .",
    "thus , the malliavin weight , @xmath21 , is not fixed by the state point , @xmath1 , but by the entire trajectory of states that have led to state point @xmath1 . since many different trajectories can lead to @xmath1 , many values of @xmath21 are possible for the same state point , @xmath1 .",
    "the average @xmath52 is actually the expectation value of the malliavin weight , averaged over all trajectories that reach state point @xmath1 at time @xmath2 .",
    "this can be used to obtain an alternative proof that @xmath53 .",
    "suppose we sample @xmath32 trajectories , of which @xmath54 end up at state point @xmath1 ( or a suitably defined vicinity thereof , in a continuous state space ) .",
    "we have @xmath55 .",
    "then , the malliavin property implies @xmath56 , and hence , @xmath57 .",
    "up to now , we have assumed that the quantity , @xmath0 , does not depend on the parameter , @xmath17",
    ". there may be cases , however , when @xmath0 does have an explicit @xmath17-dependence . in these cases ,",
    "eq .   should be replaced by @xmath58 this reveals a kind of ` algebra ' for malliavin weights : we see that the operations of taking an expectation value and taking a derivative can be commuted , provided the malliavin weight is introduced as the  commutator .",
    "we can also extend our analysis further to allow us to compute higher derivatives with respect to the parameters .",
    "these may be useful , for example , for increasing the efficiency of gradient - based parameter optimisation algorithms .",
    "taking the derivative of eq .",
    "with respect to a second parameter ,  @xmath59 ,  gives @xmath60 & \\quad={\\bigl\\langle \\frac{\\partial^2\\ !",
    "a}{\\partial\\lambda\\partial\\mu}\\bigr\\rangle } + { \\bigl\\langle \\frac{\\partial a}{\\partial\\lambda}\\ , q_\\mu \\bigr\\rangle } + { \\bigl\\langle a\\,\\frac{\\partial q_\\lambda}{\\partial\\mu}\\bigr\\rangle}\\\\[6pt ] & { } \\hspace{9em } { } + { \\bigl\\langle \\frac{\\partial a}{\\partial\\mu}\\,q_\\lambda \\bigr\\rangle } + { \\langle a\\,q_\\lambda \\,q_\\mu\\rangle}\\nonumber \\\\[6pt ] & = { \\langle a\\,(q_{\\lambda\\mu}+q_\\lambda q_\\mu)\\rangle } + { \\bigl\\langle \\frac{\\partial a}{\\partial\\lambda}\\,q_\\mu \\bigr\\rangle } + { \\bigl\\langle \\frac{\\partial a}{\\partial\\mu}\\,q_\\lambda \\bigr\\rangle } + { \\bigl\\langle \\frac{\\partial^2\\ ! a}{\\partial\\lambda\\partial\\mu}\\bigr\\rangle}\\,.\\nonumber\\end{aligned}\\ ] ] in the second line , we iterate the commutation relation and , in the third line , we collect like terms and introduce @xmath61 in the case where @xmath0 is independent of the parameters , this result simplifies to @xmath62 the quantity , @xmath63 , here is a new , second order malliavin weight , which , from eqs .   and  , satisfies @xmath64 to compute second derivatives with respect to the parameters",
    ", we should therefore track these second order malliavin weights in our simulation , updating them alongside the existing malliavin weights by the rule @xmath65 a corollary , if we take @xmath0 as a constant in eqs .   and respectively",
    ", is that quite generally @xmath66 and @xmath67 .",
    "steady state problems can be approached by extending the correlation function method to second order weights .",
    "define , _ cf . _  eq .  , @xmath68\\\\[3pt ] & \\hspace{9em}{}-[q_{\\lambda\\mu}(t')+q_\\lambda(t ' ) q_\\mu(t')]\\}\\rangle\\ , .",
    "\\end{split}\\ ] ] as in the first order case , in steady state , we expect @xmath42 , with the property that @xmath69 as @xmath70 .",
    "we now demonstrate this machinery by way of a practical but very simple example , namely one - dimensional ( overdamped ) brownian motion in a force field . in this case",
    ", the state space is specified by the particle position , @xmath71 , which evolves according to the langevin equation @xmath72 in this @xmath73 is the force field and @xmath74 is gaussian white noise of amplitude @xmath75 , where @xmath76 is temperature . without loss of generality",
    "we have chosen units so that there is no prefactor multiplying the force field .",
    "we discretise the langevin equation to the following updating rule @xmath77 where @xmath78 is the time step and @xmath79 is a gaussian random variate with zero mean and variance @xmath80 . corresponding to this updating rule is an explicit expression for the propagator @xmath81 this follows from the statistical distribution of @xmath79 .",
    "let us suppose that the parameter of interest , @xmath17 , enters into the force field ( the temperature , @xmath76 , could also be chosen as a parameter ) . making this assumption",
    "@xmath82 we can simplify this result by noting that from eq .",
    ", @xmath83 . making use of this , the final updating rule for the malliavin weight is @xmath84 where @xmath79 is the _ exact same _ value that was used for updating the position in eq .  .",
    "because the value of @xmath79 is the same for the updates of position and of @xmath21 , the change in @xmath21 is completely determined by the end points , @xmath71 and @xmath85 .",
    "the derivative , @xmath86 , should be evaluated at @xmath71 , since that is the position at which the force is computed in eq .  .",
    "since @xmath79 in eq",
    ".   is a random variate uncorrelated with @xmath71 , averaging eq .",
    "shows that @xmath87 .",
    "as the initial condition is @xmath30 , this means that @xmath88 , as predicted in the previous section .",
    "is essentially the same as that derived in ref .",
    "@xcite .    if we differentiate eq .   with respect to a second parameter , @xmath59 , we get @xmath89 & \\hspace{7em}{}-\\frac{{\\delta t}}{2t}\\,\\frac{\\partial f}{\\partial\\lambda }   \\,\\frac{\\partial f}{\\partial\\mu}\\ , .",
    "\\end{split } \\label{eq : e4}\\ ] ] hence , the updating rule for the second order malliavin weight can be written as @xmath90 where , again , @xmath79 is the exact same value as that used for updating the position in eq .  .",
    "if we average eq .   over replicate simulation runs , we find @xmath91 .",
    "hence , the mean value , @xmath92 , drifts in time , unlike @xmath93 or @xmath94 .",
    "however , one can show that the mean value of the sum , @xmath95 , is constant in time and equal to zero , as long as , initially , @xmath96 .",
    "now , let us consider the simplest case of a particle in a linear force field , @xmath97 ( also discussed in ref .",
    "this corresponds to a harmonic trap with the potential @xmath98 .",
    "we let the particle start from @xmath99 at @xmath100 and track its time - dependent relaxation to the steady state .",
    "we shall set @xmath101 for simplicity .",
    "the langevin equation can be solved exactly for this case , and the mean position evolves according to @xmath102 we suppose that we are interested in derivatives with respect to both @xmath103 and @xmath104 , for a `` baseline '' parameter set in which @xmath104 is finite , but @xmath105 .",
    "taking derivatives of eq .   and setting @xmath105 , we find @xmath106 & \\frac{\\partial^2{\\langle x(t)\\rangle}}{\\partial h\\partial \\kappa } = \\frac{t e^{-\\kappa t}}{\\kappa } - \\frac{1-e^{-\\kappa t}}{\\kappa^2}\\ , .",
    "\\end{split } \\label{eq:1dtrap}\\ ] ] we now show how to compute these derivatives using malliavin weight sampling . applying the definitions in eqs .   and , the malliavin weight increments are @xmath107 and the position update itself is @xmath108 we track these malliavin weights in our simulation and use them to calculate derivatives according to @xmath109 & \\frac{\\partial^2{\\langle x(t)\\rangle}}{\\partial h\\partial \\kappa } = { \\langle x(t ) ( q_{h\\kappa}(t)+q_h(t ) q_\\kappa(t))\\rangle}\\ , .",
    "\\end{split } \\label{eq : mws2}\\ ] ] eqs .",
    " have been coded up as a matlab  script , described in appendix [ app : script ] . a typical result generated by running this script",
    "is shown in fig .",
    "eqs .   and",
    "are iterated with @xmath110 up to @xmath111 , for a trap strength @xmath112 and initial position @xmath113 .",
    "the weighted averages in eq .   are evaluated as a function of time , for @xmath114 samples , as in eq .  .",
    "these results are shown as the solid lines in fig .",
    "the dashed lines are theoretical predictions for the time dependent derivatives from eqs .  .",
    "as can be seen , the agreement between the time - dependent derivatives and the malliavin weight averages is very good .",
    "( top curve , blue ) , @xmath115 ( middle curve , green ) and @xmath116 ( bottom curve , red ) . solid lines ( slightly noisy ) are the malliavin weight averages , generated by running the matlab  script described in appendix [ app : script ] . dashed lines are theoretical predictions from eqs .  .",
    "]    as discussed briefly above , in this procedure , the sampling error in the computation of @xmath117 is expected to grow with time .",
    "[ fig2 ] shows the mean square malliavin weight as a function of time for the same problem .",
    "for the first order weights , @xmath118 and @xmath119 , the growth rate is typically linear in time . indeed , from eqs .",
    ", one can prove that in the limit @xmath120 ( see appendix [ app : anal ] ) @xmath121 thus @xmath118 behaves exactly as a random walk , as should be obvious from the updating rule .",
    "the other weight , @xmath119 , also ultimately behaves as a random walk , since @xmath122 in steady state ( from equipartition ) .",
    "[ fig2 ] also shows that the second order weight , @xmath123 , grows superdiffusively ; one can show that , eventually , @xmath124 , although the transient behaviour is complicated .",
    "full expressions are given in appendix [ app : anal ] .",
    "this suggests that computation of second order derivatives is likely to suffer more severely from statistical sampling problems than the computation of first order derivatives .",
    "in this paper , we have provided an outline of the generic use of malliavin weights for sampling derivatives in stochastic simulations , with an emphasis on practical aspects .",
    "the usefulness of mws for a particular simulation scheme hinges on the simplicity , or otherwise , of constructing the propagator , @xmath4 , which fixes the updating rule for the malliavin weights according to eq .  .",
    "the propagator is determined by the algorithm used to implement the stochastic equations of motion ; mws may be easier to implement for some algorithms than for others .",
    "we note , however , that there is often some freedom of choice about the algorithm , such as the choice of a stochastic thermostat in molecular dynamics , or the order in which update steps are implemented . in these cases ,",
    "a suitable choice may simplify the construction of the propagator and facilitate the use of malliavin weights .",
    "ooo    rosalind j. allen is supported by a royal society university research fellowship .    .",
    "here , we present analytic results for the growth in time of the mean square malliavin weights . we can express the rate of growth of the mean of a generic function , @xmath125 , as @xmath126 on the right - hand side ( rhs ) , the values of @xmath85 , @xmath127 , @xmath128 and @xmath123 are substituted from the updating rules in eqs .   and .",
    "in calculating the rhs average , we note that the distribution of @xmath79 is a gaussian independent of the position and malliavin weights , and thus , one can substitute @xmath129 , @xmath130 , @xmath131 , @xmath132 , _ etc._. proceeding in this way , with judicious choices for @xmath133 , one can obtain the following set of coupled ordinary differential equations ( odes ) @xmath134 & \\frac{d{\\langle x^2\\rangle}}{dt}+2\\kappa{\\langle x^2\\rangle}=2\\,,\\quad \\frac{d{\\langle x q_h\\rangle}}{dt}+\\kappa{\\langle x q_h\\rangle}=1\\,,\\nonumber\\\\[9pt ] & \\frac{d{\\langle x^2q_h^2\\rangle}}{dt}+2\\kappa{\\langle x^2q_h^2\\rangle } = 2{\\langle q_h^2\\rangle}+4{\\langle x q_h\\rangle}+\\frac{{\\langle x^2\\rangle}}{2}\\,,\\nonumber\\\\[9pt ] & \\frac{d{\\langle xq_hq_\\kappa\\rangle}}{dt}+\\kappa{\\langle xq_hq_\\kappa\\rangle } = -{\\langle x q_h\\rangle}-\\frac{{\\langle x^2\\rangle}}{2}\\,,\\\\[9pt ] & \\frac{d{\\langle ( q_{h\\kappa}+q_hq_\\kappa)^2\\rangle}}{dt } = \\frac{{\\langle q_\\kappa^2\\rangle}}{2}-{\\langle x q_hq_\\kappa\\rangle } + \\frac{{\\langle x^2q_h^2\\rangle}}{2}\\nonumber\\\\[6pt ] & \\hspace{9em}\\bigl({}=\\frac{{\\langle ( q_\\kappa - x q_h)^2\\rangle}}{2}\\bigr)\\,.\\nonumber\\end{aligned}\\ ] ] some of these have already been encountered in the main text .",
    "the last one is for the desired mean square second order weight .",
    "the odes can be solved with the initial conditions that at @xmath100 , all averages involving malliavin weights vanish , but @xmath135 .",
    "the results include _",
    "inter alia_@xmath136 & { \\langle ( q_{h\\kappa}+q_hq_\\kappa)^2\\rangle}= \\frac{2\\kappa^2t^2+(19+\\kappa x_0 ^",
    "2)\\kappa t+2\\kappa x_0 ^ 2 - 34 } { 8\\kappa^3}\\nonumber\\\\[9pt ] & \\hspace{6em}{}+\\frac{2\\kappa t+10-\\kappa x_0 ^ 2}{2\\kappa^3 } \\,e^{-\\kappa t}\\nonumber\\\\[6pt ] & \\hspace{9em}{}+ \\frac{(1-\\kappa x_0 ^ 2)\\kappa t+2\\kappa x_0 ^ 2 - 6}{8\\kappa^3}\\,e^{-2\\kappa t}\\ , .",
    "\\label{eq : app1}\\end{aligned}\\ ] ] these are shown as the dashed lines in fig .",
    "the leading behaviour of the last as @xmath137 is @xmath138 however , the approach to this limit is slow .",
    "the matlab  script in listing [ list1 ] was used to generate the results shown in fig .",
    "it implements eqs .  ",
    "above , making extensive use of the compact matlab  syntax for array operations , for instance , invoking ` . * ' for element - by - element multiplication of arrays .    here is a brief explanation of the script . _",
    "lines  13 _ initialise the problem and the parameter values . _ lines  4 _ and  _ 5 _ calculate the number of points in a trajectory and initialise a vector containing the time coordinate of each point .",
    "_ lines  69 _ set aside storage for the actual trajectory , malliavin weights and cumulative statistics . _ lines  1023 _",
    "implement a pair of nested loops , which are the kernel of the simulation . within the outer ( trajectory sampling ) loop , _ line  11 _ initialises the particle position and malliavin weights , _ line  12 _ precomputes a vector of random displacements ( gaussian random variates ) and _ lines  1318 _ generate the actual trajectory . within the inner (",
    "trajectory generating loop ) , _ lines  1417 _ are a direct implementation of eqs .   and .",
    "after each individual trajectory has been generated , the cumulative sampling step implied by eq .   is done in _",
    "lines  1922 _ ; after all the trajectories have been generated , these quantities are normalised in _ lines  24 _ and  _ 25_. finally , _ lines  2632 _ generate a plot similar to fig .  [ fig1 ] ( albeit with the addition of @xmath139 ) , and _",
    "lines  33 _ and  _ 34 _ show how the data can be exported in tabular format for replotting using an external package .",
    "listing [ list1 ] is complete and self - contained .",
    "it will run in either matlab  or octave .",
    "one minor comment is perhaps in order .",
    "the choice was made to precompute a vector of gaussian random variates , which are used as random displacements to generate the trajectory and update the malliavin weights .",
    "one could equally well generate random displacements on - the - fly , in the inner loop .",
    "for this one - dimensional problem , storage is not an issue , and it seems more elegant and efficient to exploit the vectorisation capabilities of matlab . for a more realistic three - dimensional problem , with many particles ( and a different programming language ) , it is obviously preferable to use an on - the - fly approach . +                        .... clear all randn('seed ' , 12345 ) ; kappa = 2 ; x0 = 1 ; tend = 5 ; dt = 0.01 ; nsamp = 10 ^ 5 ; npt = round(tend / dt ) + 1 ; t = ( 0:npt-1 ) ' * dt ; x = zeros(npt , 1 ) ; xi = zeros(npt , 1 ) ; qh = zeros(npt , 1 ) ; qk = zeros(npt , 1 ) ; qhk = zeros(npt , 1 ) ;   x_av = zeros(npt , 1 ) ; xqh_av = zeros(npt , 1 ) ;   xqk_av = zeros(npt ,",
    "1 ) ; xqhk_av = zeros(npt , 1 ) ; for samp = 1:nsamp    x(1 ) = x0 ; qh(1 ) = 0 ; qk(1 ) = 0 ; qhk(1 ) = 0 ;    xi = randn(npt , 1 ) * sqrt(2*dt ) ;    for i = 1:npt-1      x(i+1 ) = x(i ) - kappa*x(i)*dt + xi(i ) ;      qh(i+1 ) = qh(i ) + 0.5*xi(i ) ;      qk(i+1 ) = qk(i ) - 0.5*x(i)*xi(i ) ;      qhk(i+1 ) = qhk(i ) + 0.5*x(i)*dt ;    end    x_av = x_av + x ;    xqh_av = xqh_av + x.*qh ;    xqk_av = xqk_av + x.*qk ;    xqhk_av = xqhk_av + x.*(qhk + qh.*qk ) ; end x_av = x_av / nsamp ; xqh_av = xqh_av / nsamp ; xqk_av = xqk_av / nsamp ; xqhk_av = xqhk_av / nsamp ; hold on plot(t , x_av , ' k ' ) ; plot(t , xqh_av , ' b ' ) plot(t , xqk_av , ' g ' ) ; plot(t , xqhk_av , ' r ' ) plot(t , x0*exp(-kappa*t ) , ' k-- ' ) plot(t , ( 1-exp(-kappa*t))/kappa , ' b-- ' ) plot(t ,",
    "-x0*t.*exp(-kappa*t ) , ' g-- ' ) plot(t , t.*exp(-kappa*t)/kappa-(1-exp(-kappa*t))/(kappa^2 ) , ' r-- ' ) result = [ t x_av xqh_av xqk_av xqhk_av ] ; save('result.dat ' , ' -ascii ' , ' result ' ) ...."
  ],
  "abstract_text": [
    "<S> malliavin weight sampling ( mws ) is a stochastic calculus technique for computing the derivatives of averaged system properties with respect to parameters in stochastic simulations , without perturbing the system s dynamics . </S>",
    "<S> it applies to systems in or out of equilibrium , in steady state or time - dependent situations , and has applications in the calculation of response coefficients , parameter sensitivities and jacobian matrices for gradient - based parameter optimisation algorithms . </S>",
    "<S> the implementation of mws has been described in the specific contexts of kinetic monte carlo and brownian dynamics simulation algorithms . here , we present a general theoretical framework for deriving the appropriate mws update rule for any stochastic simulation algorithm </S>",
    "<S> . we also provide pedagogical information on its practical implementation . </S>"
  ]
}