{
  "article_text": [
    "the study of elastic @xmath0 scattering provides an opportunity to explore the structure of the proton . from @xmath0",
    "cross section data the magnetic ( @xmath3 ) and electric ( @xmath4 ) proton form factors ( ffs ) are obtained via longitudinal - transverse ( lt ) separation @xcite . in the data analysis , it is convenient to consider the reduced cross section , which in the one - photon exchange ( ope ) approximation , reads as @xmath5 where @xmath2 and @xmath6 are the four - momentum transfer squared and scattering angle respectively .",
    "the ff ratio , @xmath7 ( @xmath8 , the magnetic moment of the proton ) can be extracted from the so - called polarization transfer ( pt ) measurements @xcite .",
    "it turns out that the systematic discrepancy between the ff ratio data obtained via the lt separation and the pt measurements exists .",
    "it seems that taking into account the two - photon exchange ( tpe ) correction , the one which is not included in the classical treatment of the radiative corrections @xcite , cancels this discrepancy @xcite .",
    "moreover , it is claimed that tpe correction to the @xmath9 ratio , extracted from the pt measurements , is negligible @xcite .",
    "however , taking into account the tpe contribution , in the lt separation , affects significantly the extracted values of the proton ffs .",
    "the reduced cross section is modified by the tpe correction @xmath10 , namely , @xmath11    the tpe effect has been studied extensively during the past few years , for reviews and references , see refs .",
    "@xcite . the recent studies can be found in refs .",
    "@xcite .",
    "the dominant part of @xmath10 is given by the interference between the ope and the tpe amplitudes .",
    "hence , for the @xmath12 scattering , @xmath13 therefore , the magnitude of the tpe term can be evaluated by measuring the ratio of the @xmath12 to @xmath14 elastic cross sections @xcite , @xmath15 a deviation of this function from unity indicates the importance of the tpe effect @xcite .",
    "a direct prediction of the proton ffs and tpe correction is a difficult task .",
    "one has to deal with the problems of quantum chromodynamics in the non - perturbative regime .",
    "the successful approaches are rather phenomenological , and contain many internal parameters , which are fixed to reproduce the experimental data ( for reviews see refs .",
    "@xcite ) .",
    "on the other hand , the existing elastic polarized and unpolarized @xmath16 and @xmath17p scattering data cover kinematical region broad enough to reconstruct the ffs dependence on @xmath2 . to combine the cross section data with the pt measurements and",
    "the @xmath18p ratio data allows one to obtain information about the tpe contribution .",
    "the aim of this paper is to find the approximation of the ffs and the tpe contribution by mainly relying on the experimental data .",
    "we reduce the model - dependent assumptions to the necessary minimum .",
    "only three complex ffs , which depend on @xmath2 and @xmath1 , are required to describe the elastic unpolarized and polarized @xmath0 @xcite scattering amplitudes .",
    "hence , six real functions have to be determined from the data .",
    "we assume that the pt ratio data are not affected by the tpe effect .",
    "then , one can show that only three unknown functions have to be found : two proton ffs and the @xmath19 correcting term [ see eq .",
    "( [ sigma_r_2gamma ] ) ] .",
    "analyses with similar tpe assumptions have been performed by many groups @xcite .    in this paper",
    ", we consider the cross section , the pt , and the @xmath20 ratio data .",
    "to consider at least three different data types appeared to be necessary because of the limited model assumptions about the tpe term .    to approximate the ffs and tpe function",
    "one has to assume particular empirical parametrization .",
    "however , it is obvious that the choice of the functional form of the parametrization has an impact on the fit and its uncertainties . in particular",
    ", it is the case of the tpe contribution .",
    "this problem was not discussed in the previous analyses .    in the approach presented in this paper",
    ", fitting the data means the construction of the statistical model with the ability to predict the ffs and the tpe term .",
    "we apply the methods of the bayesian statistics , which allows performing a model comparison .",
    "indeed , we consider as many different data parametrizations as possible , and the best model is indicated by the objective bayesian procedure .    in practice",
    ", one has to evaluate the probability distribution @xmath21 in the space of all functional parametrizations of the ffs and the tpe contribution .",
    "the best model should maximize this probability .",
    "it is obvious that the magnetic and electric ffs as well as the tpe correction function are correlated .",
    "all of them should be determined by the same underlying fundamental model .",
    "therefore , one can imagine that there exists a multidimensional function , defined by the set of parameters , which simultaneously describe all @xmath3 , @xmath4 and @xmath19 . in this paper",
    ", we use the artificial neural networks ( anns ) to approximate this function .",
    "we consider a particular type of ann , the feed forward neural network ( nn ) in the so - called multi layer perceptron ( mlp ) configuration .",
    "the experimental data , which are analyzed here , depend on either one ( only @xmath2 ) or two ( @xmath2 and @xmath1 ) kinematical variables .",
    "hence , the mlp must map two - dimensional input space [ @xmath22 to output space , spanned by three functions @xmath23 .",
    "we consider mlp networks that consist of three layers of units : input , hidden layer , and output ( see fig .",
    "[ fig_sieci ] ) .",
    "each single neuron ( unit ) of the network calculates its output value as an activation function @xmath24 of the weighted sum of its inputs @xmath25 , where @xmath26 denotes the i - th weight parameter , while @xmath27 represents the output value of the unit from the previous layer .    for the activation functions we take the sigmoid @xmath28 and the linear functions for the hidden and output units , respectively .",
    "it has been proven @xcite that the maps given by the networks with one hidden layer and with the sigmoid like activation functions , in this layer , are sufficient to approximate any continuous real function .",
    "indeed , we assume that the ffs and tpe term are the continuous functions of kinematical variables .",
    "notice that the efficiency of approximation depends on the number of hidden units . in some problems it might be a very large number .",
    "additionally let us mention a useful property of the sigmoid function .",
    "its effective support is concentrated in the close neighborhood of @xmath29 . with increasing @xmath30 , the sigmoid saturates .",
    "this property allows restricting the effective range of the weight parameters .",
    "the @xmath3 and @xmath4 only depend on @xmath2 .",
    "this property is achieved by the particular choice of the architecture of mlp , namely some of the connections are erased . as a result",
    ", the network is divided into two sectors .",
    "one , called the latter ff sector , which is disconnected with the @xmath1 input and the second , called the latter tpe sector , which is connected with both input values .",
    "the ffs and tpe correction are still determined by the large subset of common weights .",
    "an example of the 2-(3 - 2)-3 network ( @xmath31 ) is drawn in fig .",
    "[ fig_sieci ] .",
    "it consists of two input units , five units in the hidden layer ( three units belong to the ff sector , and two units belong to the tpe sector ) , and 3 output units .    the choice of the particular configuration of units and number of the hidden units , defines the network architecture @xmath32 . for given @xmath32 the particular map @xmath33",
    "is defined as @xmath34 to have network @xmath32 , the optimal values of the weight parameters have to be found .",
    "the process of establishing them is called the training of the network and it is described in the next part of the paper .    it is obvious that , to find the optimal network architecture , which approximates desired map well , the number of hidden units has to be varied . in this paper",
    "we apply the method that allows estimating the optimal size of the hidden layer .",
    "the mlps with a larger number of units ( with many weights ) have a better ability to represent the data .",
    "however , usually too complex parametrizations exactly resemble the data and usually tend to reflect the statistical fluctuations .",
    "thus , the generality of the description is lost , and the data are over - fitted . moreover , the complex parametrizations may lead to larger uncertainties than the simple models . on the other hand ,",
    "too simple parametrizations are not capable of coding all the important information hidden in the measurements .",
    "the fits described by the simple functions might be characterized by the underestimated uncertainties .",
    "a task of finding the optimal statistical model that represents the data accurately enough but does not overfit the data is known in statistics as the bias - variance trade off problem . in the previous global analyses of the @xmath0 data",
    "the degree of the complexity of the ffs and tpe parametrizations was chosen with the help of phenomenological arguments and common sense . in this paper",
    ", we wish to apply the objective bayesian methods , which allow quantitatively investigating the complexity of the ffs and tpe correcting functions .    the bayesian framework ( bf ) formulated for the nn computations @xcite faces the problems described above .",
    "this approach has already been adapted to approximate the electromagnetic nucleon ffs @xcite and , here , it is developed to study the tpe effect .",
    "the bf was designed to :    * quantitatively classify the statistical hypothesis ; * objectively choose the best network architectures and consequently , the number of hidden units ; * find the optimal values of the weight parameters ; * objectively establish the training parameters , such as the regularization parameter @xmath35 ( it will be explained below ) .",
    "notice that to deal with the overfitting problem one can also use the cross - validation technique .",
    "it is complementary approach which has been applied by the nnpdf group for fitting the parton distribution functions @xcite .    at the beginning of the bayesian analysis",
    ", we assume that all possible models are equally likely , @xmath36 where @xmath37 denotes the prior probability .    with the help of bayes theorem the posterior probability for a given model ( network )",
    "is obtained as @xmath38 where @xmath39 is the experimental data , @xmath40 is the probability of the model given data @xmath39 , and @xmath41 is some constant real number . because of the prior assumption ( [ prior_assumtion ] ) , it is obvious that , to classify the hypothesis , it is enough to evaluate the evidence @xmath42the probabilistic measure of goodness of fit .",
    "for given network architecture @xmath32 , the optimal @xmath43 weight parameters should maximize the posterior probability , @xmath44 where @xmath45 is the likelihood function of the data , @xmath46 denotes the prior probability , and @xmath47 denotes the set of initial constraints .",
    "the data likelihood function is defined by @xmath48 where @xmath49 is the experimental error function . by @xmath50 , we denote the error functions of the cross section ( [ chi2_sigma ] ) , the pt ( [ chi2_pt ] ) and the @xmath51 ratio ( [ chi2_positron ] ) data . eventually , @xmath52 denotes the error function introduced to take the two artificial ff points into account ( see the discussion below ) .",
    "we distinguish between the ann and the physical initial constraints @xmath53 .    the ann constraints @xmath54 are introduced to face the overfitting problem .",
    "indeed , defining the prior probability as follows : @xmath55 prevents getting the overfitted parametrizations .",
    "the physical constraints @xmath56 are motivated by the general properties of the ffs and the tpe term @xcite .",
    "we assume that , at @xmath57 , @xmath58 and @xmath59 .",
    "in practice , three artificial data points are added to the experimental data sets , namely , [ @xmath60 , [ @xmath61 , and [ @xmath62 , where @xmath63 .",
    "the influence of the @xmath64 value on the fits and the training process were investigated in the preliminary stage of the analysis .",
    "it was obtained that , with @xmath65 , the efficiency of the training process was very low , while retaining @xmath66 was not sufficient to attract the fit for the desired value at the constraint points .    one can show that the maximum of the posterior probability ( [ posterior_w ] ) corresponds to the minimum of the total error function , @xmath67 let @xmath43 denote the weight configuration , which minimizes the above expression . to find the minimum ( [ total_error_function ] ) ,",
    "the quick - prop gradient descent algorithm @xcite , is applied .",
    "the weight parameters are updated iteratively , because of the algorithm , until the minimum is reached .",
    "the proper choice of the @xmath35 parameter is crucial for getting the fits and for further model comparison .",
    "if the @xmath35 parameter is small then the penalty term ( [ penalty_term ] ) does not significantly affect the results of the training process . in the language of the bayesian statistics",
    ", a small @xmath35 value corresponds to the large width of the prior probability distribution ( [ prior_w ] ) .",
    "the bf provides a recipe on how to establish the optimal @xmath35 parameter . for the optimal @xmath68 and @xmath43 the probability distribution ,",
    "@xmath69 is maximized .    from the above expression",
    ", one can obtain , the necessary condition ( [ alpha_mp_parameter ] ) , which has to be satisfied by @xmath68 . in this version of the bf approach",
    ", we consider the so - called ladder approximation , where the expression ( [ alpha_mp_parameter ] ) is used to iteratively update the @xmath35 value ( during the training process ) , as long as it converges . in reality , because eq . ( [ alpha_mp_parameter ] ) is valid in the neighborhood of the minimum , the @xmath35 parameter is not changed until the training process approaches the close surrounding of the minimum . in this part of the training @xmath70",
    "is fixed and equals 0.01 .",
    "then , it starts to be updated .    in general , for every weight parameter , one would introduce its own regularization factor .",
    "however , notice that @xmath35 is an example of the scale parameter of the model , given that network @xmath32 is symmetric under the permutation of units in the hidden layer .. this property allows reducing the number of independent regularization parameters to six : three in the ff sector ( hidden , bias , and linear regularization factors ) and three in the tpe sector ( similar to before ) . on the other hand ,",
    "@xmath19 is given by the linear combination of the nucleon ffs , multiplied by the additional tpe - like ffs ( see eq .",
    "( 14 ) of ref .",
    "it means that , if the parameters of the ff sector are scaled , then the weights of the tpe sector should also be rescaled .",
    "this property only seems to be approximate , but we use it to reduce the number of regularization parameters to three .",
    "eventually , we noticed that in our previous paper , it was shown that it was enough to consider one regularization parameter to fit the ff data @xcite .",
    "hence , to simplify the numerical calculations and also to accelerate the training process ( more than 45 000 training processes have been performed ) , we consider the simplest regularization scenario with one @xmath35 parameter .",
    "however , as described above , this part of the approach can be improved @xcite .    by having the optimal values of the weight and",
    "@xmath35 parameters the evidence is computed from eq .",
    "( [ log_of_evidence ] ) the logarithm of evidence is given by two main contributions : the misfit of the approximate data ( the experimental error function at the minimum ) and the occam factor .",
    "the latter penalizes complex models .",
    "the most optimal model has the highest value of evidence .",
    "the evidence formula and the description of its properties can be found in appendix [ appendix_evidence ] .",
    "as mentioned in the previous section , we consider three types of measurements : the cross section ( 27 sets ) , the pt ( 14 sets ) and the @xmath51 ratio ( 3 sets ) data .    the selection of the cross section and pt ratio data sets is the same as in one of our previous papers @xcite .",
    "however , in the case of the pt data , two data sets are replaced with their recent updates @xcite .",
    "additionally , we also include the latest pt measurements of the ff ratio @xcite .",
    "since the presence of the pt ratio data is required to properly extract the tpe contribution , we only consider the cross section points below @xmath71  gev@xmath72 . above this limit ,",
    "the pt data are not available .    in the case of the cross section data , similar to ref .",
    "@xcite , the systematic normalization uncertainties are taken into account .",
    "for every data set , a normalization parameter is introduced and it is established during training . the procedure is described in appendix [ appendix_error_normalization ] .",
    "we consider the networks of type 2-(g - t)-3 , where @xmath73 . in the preliminary stage of the analysis",
    ", it has been observed that the networks with either g=1 , or t=1 have not been able to approximate the data well ( similar to the networks with @xmath74 ) .",
    "therefore we only consider models with @xmath75 .",
    "finally we discuss 45 different ann architectures . for every network @xmath32 type @xmath76 networks , with randomly chosen initial values of weights , have been trained . among them , the parametrization with the highest evidence was used for further model comparison .",
    "it turned out that the highest evidence value was obtained for network 2-(5 - 6)-3 ( see fig .",
    "[ fig_evidence ] ) .    in fig .",
    "[ fig_ff_and_ratio](a ) , we plot the ff ratio @xmath77 computed with the network @xmath78 ( our best fit ) .",
    "the shaded areas denote @xmath79 uncertainty computed from the covariance matrix of the fit .",
    "our predictions of the ffs are compared to the results of ref .",
    "@xcite where the tpe function was postulated based on the phenomenological arguments .",
    "the discrepancy between our fits and those of ref .",
    "@xcite appears above @xmath2=4 gev@xmath72 .    in fig .",
    "[ fig_ff_and_ratio](b ) , the @xmath2 dependence of ratio @xmath80 is presented .",
    "we see that at @xmath81  gev@xmath72 the tpe correcting term has a local minimum and it becomes the decreasing function of @xmath2 above 2  gev@xmath72 . with growing @xmath2 , fit uncertainty also enlarges .",
    "indeed , above @xmath82  gev@xmath72 , the number of experimental points is limited and the data are not accurate enough to get an exact approximation .",
    "it is interesting to mention that , for large @xmath1 ( above 0.8 ) and @xmath2 around 1.5  gev@xmath72 , the tpe correction is positive .    in fig .",
    "[ fig_ff_and_ratio](b ) , we also plot the tpe contribution ( dotted lines ) predicted by the models : @xmath83 , @xmath84 , @xmath85 , @xmath86 , @xmath87 , and @xmath88 .",
    "they are characterized by lower evidence values than the @xmath78 model , but they could be acceptable because of the @xmath89 method ( their @xmath90 values are much lower than the number of points ) .",
    "the difference between these fits and the prediction of @xmath78 is spectacular .",
    "it demonstrates that the model comparison is crucial for the proper choice of tpe parametrization .    by keeping the forthcoming measurements of the elastic @xmath16 and @xmath91 scatterings @xcite in mind , in fig .",
    "[ fig_ratio_pos](a ) , we plot our predictions of ratio @xmath92 .",
    "although , we have not assumed the linearity of the tpe term in @xmath1 , the final fit behaves like a linear function of @xmath1 , as observed in the previous global analysis @xcite .",
    "although nonlinearities appear at the low @xmath1 and @xmath2 values ( bottom panel of fig .",
    "[ fig_ratio_pos ] ) , in this kinematical domain , the fits have large uncertainties , and the obtained results are in agreement with the linear approximation .",
    "the obtained tpe function has a particular analytical form ( see appendix [ appendix_fits ] ) , which can be written as the taylor series in @xmath1 .",
    "if one neglects higher rather than linear @xmath1 terms , then the tpe correction is the sum of two contributions , which play a particular role in the lt separation .",
    "one of them corrects the magnetic ff , and it appears to be negative . the other modifies the electric ff , and it is the positive function of @xmath2 .    notice that the pt data are not present below @xmath93  gev@xmath72 .",
    "hence its influence on the extraction of the tpe in this kinematical range is small .",
    "although because of the lack of pt data , the tpe is still constrained in the low @xmath2 region .",
    "namely , there are several @xmath94 ratio data points @xcite .",
    "additionally , we keep one artificial point at @xmath57 , and @xmath95 which constrains @xmath19 . also , there are plenty of cross section data points and the two artificial ff points .",
    "the presence of the ff points determines the low @xmath2 behavior of the @xmath96 .",
    "all together , provides restrictions on the extraction of the tpe term .    in fig .",
    "[ fig_ratio_pos](b ) , we show the @xmath1 dependence of @xmath19 at several values of @xmath2 . it can be seen that at very low @xmath2 , the tpe term becomes positive .",
    "however , similar to the above , because of the large uncertainties , this effect is consistent with @xmath97 .",
    "the aim of this paper was to find the approximation of the proton ffs and the tpe function based on the knowledge of the elastic @xmath0 scattering data .",
    "it was performed by adapting the bayesian statistical methods developed for the feed forward nns .",
    "we assumed that the tpe correction does not affect the pt ratio data .",
    "this assumption turned out to be necessary to perform the numerical analysis , but one should keep in mind that there is no perfect approximation at low @xmath2 .",
    "we discussed as many different nn parametrizations as required to find the optimal model .",
    "the best model was indicated by the bayesian algorithm . from this point of view",
    "the results are model independent .",
    "the obtained tpe fit turned out to have nontrivial @xmath2 dependence . in some kinematical regions ( very low @xmath2 and @xmath98  gev@xmath72 , @xmath99 ) , it is the positive function .",
    "let us emphasize that we considered the simplest working bf .",
    "only one regularization parameter was discussed , and the hessian approximation was applied .",
    "further development of the approach might improve the results of the analysis .",
    "in particular , it allows going beyond the covariance matrix approximation used for the estimation of the fit uncertainty .",
    "the improvements require introducing modifications at every step of the bf .",
    "the new approach will also need greater computational power than the present one .",
    "the analytical form of the fits is shown in appendix [ appendix_fits ] , whereas the covariance matrix can be taken from ref .",
    "all numerical computations have been performed with the c++ library developed by k.m.g .",
    "this work was supported by the polish ministry of science grant , project no .",
    "n n202 368439 .",
    "we thank c. giunti for inspiring discussions in the early stage of the project .",
    "we acknowledge very instructive discussions with r. sulej and p. plonski , we thank j. zmuda and j. nowak for reading the manuscript and we thank j. arrington for his remarks on the previous version of the paper .",
    "the cross section error function reads as @xmath100,\\ ] ] where @xmath101 is the number of independent cross section data sets , @xmath102 is the number of points in the @xmath103th data set , @xmath104 is the normalization parameter for the @xmath103th data set , @xmath105 is the normalization ( systematic ) uncertainty , @xmath106 is the experimental value of the reduced cross section of the @xmath107th data point in the @xmath103th data set measured for @xmath108 and @xmath109 , @xmath110 denotes the corresponding experimental uncertainty , and @xmath111 .",
    "the pt ratio data error function reads as @xmath112 where @xmath113 is the number of pt ratio data points , @xmath114 denotes the experimental value of the @xmath107th point , measured for @xmath115 , @xmath116 is the corresponding experimental uncertainty , and @xmath117 .",
    "analogically the positron - electron ratio data error function reads as @xmath118 where @xmath119 is the number of pt ratio data points , @xmath120 denotes the experimental value of the @xmath107th point , measured for @xmath115 and @xmath121 values , @xmath122 is the corresponding experimental uncertainty , and @xmath123 .",
    "the @xmath124 reads as @xmath125 where @xmath126 or @xmath4 .",
    "the optimal values of the normalization parameters @xmath104 , @xmath127 , at the minimum of the total error function ( [ total_error_function ] ) , must satisfy the property , @xmath128 which can be rewritten as @xmath129 the above expression is used for updating the normalization parameters during training .",
    "the procedure turned out to be convergent , as long as the minimum of the total error function was reached .",
    "it is interesting to notice that the normalization parameters obtained in this paper are very similar to those obtained in our previous global analysis @xcite , where the minuit package ( now it is one of the packages of the root library ) was applied to find the optimal values of the fit and normalization parameters .",
    "the @xmath68 parameter is computed in the so - called hessian approximation @xcite .",
    "it is given by the solution of the equation , @xmath130 where @xmath131 s are eigenvalues of the matrix @xmath132 and @xmath133 .",
    "in practice , to find the optimal @xmath68 , the @xmath35 parameter is iteratively updated during the training process , i.e. , @xmath134 where @xmath135 denotes the value of the normalization parameter in the @xmath103th iteration step of training .",
    "the logarithm of evidence , the terms that are the same for different network architectures that are omitted , reads as    @xmath136    where @xmath137 is the number of weigh parameters , and @xmath138 is the determinant of the hessian matrix @xmath139 .    the first term in eq .",
    "( [ log_of_evidence ] ) , usually of low - value , is the misfit of the approximated data , while the other terms contribute to the occam factor .",
    "the latter penalizes the complex models .",
    "an additional contribution to this quantity is given by the symmetry factor .    in the network @xmath32 , some hidden units can be interchanged , but the response of the network ( output values ) reminds unchanged .",
    "it means that for a given configuration of weights , there exists some number of equivalent networks , which differ only by the appropriate permutation of the weights .",
    "it gives rise to the appearance of the additional combinatorial factor in the evidence .",
    "however , in this paper , it does not play a significant role .",
    "we now have , @xmath140        the ff and tpe parametrizations are obtained for @xmath142 and @xmath143 .",
    "however , one should keep the problems of the extraction of tpe at very low @xmath2 in mind ( see the discussion in the last section of the paper ) .",
    "g.  ron _ et al .",
    "_ , _ low @xmath2 measurements of the proton form factor ratio @xmath144 _ , arxiv:1103.5784 [ nucl - ex ] ; a.  j.  r.  puckett _ et al .",
    "_ , _ reanalysis of proton form factor ratio data at @xmath145 4.0 , 4.8 , and 5.6 gev@xmath146 _ , arxiv:1102.5737 [ nucl - ex ] .",
    "m.  meziane _ et al . _",
    "[ gep2gamma collaboration ] , phys .",
    "* 106 * ( 2011 ) 132501 . c.  e.  carlson and m.  vanderhaeghen , _ two - photon physics in hadronic processes _ , ann .",
    "nucl .  part .",
    "* 57 * ( 2007 ) 171 .",
    "d. j. c. mackay , neural computation 4 ( 3 ) , ( 1992 ) 415 ; d. j. c. mackay , neural computation 4 ( 3 ) , ( 1992 ) 448 ; d. j. c. mackay , _ bayesian methods for backpropagation networks _ , in e. domany , j. l. van hemmen , and k. schulten ( eds . ) , _ models of neural networks iii _ , sec . 6 .",
    "new york : springer - verlag ( 1994 ) .",
    "j.  arrington _ et al .",
    "_ , _ two - photon exchange and elastic scattering of electrons / positrons on the proton .",
    "( proposal for an experiment at vepp-3 ) _ , arxiv : nucl - ex/0408020 ; jefferson lab experiment e04 - 116 , _ beyond the born approximation : a precise comparison of @xmath12 and @xmath16 scattering in clas _ , w. k. brooks , _ et al .",
    "_ , spokespersons ."
  ],
  "abstract_text": [
    "<S> an approach to the extraction of the two - photon exchange ( tpe ) correction from elastic @xmath0 scattering data is presented . the cross section , polarization transfer ( pt ) , and charge asymmetry data are considered . </S>",
    "<S> it is assumed that the tpe correction to the pt data is negligible . </S>",
    "<S> the form factors and tpe correcting term are given by one multidimensional function approximated by the feed forward neural network ( nn ) . to find a model - independent approximation the bayesian framework for the nns </S>",
    "<S> is adapted . </S>",
    "<S> a large number of different parametrizations is considered . </S>",
    "<S> the most optimal model is indicated by the bayesian algorithm . </S>",
    "<S> the obtained fit of the tpe correction behaves linearly in @xmath1 but it has a nontrivial @xmath2 dependence . a strong dependence of the tpe fit on the choice of parametrization is observed . </S>"
  ]
}