{
  "article_text": [
    "dropout is a technique to regularize artificial neural networks  it prevents overfitting @xcite . a fully connected network with two hidden layers of 80 units each can learn to classify the mnist training set perfectly in about 20 training epochs  unfortunately the test error is quite high , about 2% . increasing the number of hidden units by a factor of 10 and using dropout results in a lower test error , about 1.1% .",
    "the dropout network takes longer to train in two senses : each training epoch takes several times longer , and the number of training epochs needed increases too .",
    "we consider a technique for speeding up training with dropout  it can substantially reduce the time needed per epoch .    consider a very simple @xmath0-layer fully connected neural network with dropout . to train it with a minibatch of @xmath1 samples ,",
    "the forward pass is described by the equations : @xmath2\\times w_{k}\\qquad k=0,\\dots,\\ell-1.\\ ] ] here @xmath3 is a @xmath4 matrix of input / hidden / output units , @xmath5 is a @xmath4 dropout - mask matrix of independent bernoulli(@xmath6 ) random variables , @xmath7 denotes the probability of dropping out units in level @xmath8 , and @xmath9 is an @xmath10 matrix of weights connecting level @xmath8 with level @xmath11 .",
    "we are using @xmath12 for ( hadamard ) element - wise multiplication and @xmath13 for matrix multiplication .",
    "we have forgotten to include non - linear functions ( e.g. the rectifier function for the hidden units , and softmax for the output units ) but for the introduction we will keep the network as simple as possible .    the network can be trained using the backpropagation algorithm to calculate the gradients of a cost function ( e.g. negative log - likelihood ) with respect to the @xmath9 : @xmath14^{\\mathsf{t}}\\times\\frac{\\partial\\mathrm{cost}}{\\partial x_{k+1}}\\\\ \\frac{\\partial\\mathrm{cost}}{\\partial x_{k } } & = \\left(\\frac{\\partial\\mathrm{cost}}{\\partial x_{k+1}}\\times w_{k}^{\\mathrm{\\mathsf{t}}}\\right)\\cdot d_{k}.\\end{aligned}\\ ] ] with dropout training , we are trying to minimize the cost function averaged over an ensemble of closely related networks .",
    "however , networks typically contain thousands of hidden units , so the size of the ensemble is _ much _ larger than the number of training samples that can possibly be ` seen ' during training .",
    "this suggests that the independence of the rows of the dropout mask matrices @xmath5 might not be terribly important ; the success of dropout simply can not depend on exploring a large fraction of the available dropout masks .",
    "some machine learning libraries such as pylearn2 allow dropout to be applied batchwise instead of independently ] .",
    "this is done by replacing @xmath5 with a @xmath15 row matrix of independent bernoulli@xmath16 random variables , and then copying it vertically @xmath1 times to get the right shape .",
    "to be practical , it is important that each training minibatch can be processed quickly . a crude way of estimating the processing time is to count the number of floating point multiplication operations needed ( naively ) to evaluate the @xmath13 matrix multiplications specified above :",
    "@xmath17 however , when we take into account the effect of the dropout mask , we see that many of these multiplications are unnecessary .",
    "the @xmath18-th element of the @xmath9 weight matrix effectively ` drops - out ' of the calculations if unit @xmath19 is dropped in level @xmath8 , or if unit @xmath20 is dropped in level @xmath11 . applying 50% dropout in levels @xmath8 and @xmath11 renders 75% of the multiplications unnecessary .",
    "if we apply dropout independently , then the parts of @xmath9 that disappear are different for each sample .",
    "this makes it effectively impossible to take advantage of the redundancy ",
    "it is slower to check if a multiplication is necessary than to just do the multiplication .",
    "however , if we apply dropout batchwise , then it becomes easy to take advantage of the redundancy .",
    "we can literally drop - out redundant parts of the calculations .",
    "the binary @xmath15 batchwise dropout matrices @xmath5 naturally define submatrices of the weight and hidden - unit matrices .",
    "let @xmath21 $ ] denote the submatrix of @xmath3 consisting of the level-@xmath8 hidden units that survive dropout .",
    "let @xmath22 $ ] denote the submatrix of @xmath9 consisting of weights that connect active units in level @xmath8 to active units in level @xmath11 .",
    "the network can then be trained using the equations : @xmath23 the redundant multiplications have been eliminated .",
    "there is an additional benefit in terms of memory needed to store the hidden units : @xmath24 needs less space than @xmath3 . in section [ sec : implementation ]",
    "we look at the performance improvement that can be achieved using cuda / cublas code running on a gpu .",
    "roughly speaking , processing a minibatch with 50% batchwise dropout takes as long as training a 50% smaller network on the same data .",
    "this explains the nearly overlapping pairs of lines in figure [ fig : training - time ] .",
    "we should emphasize that batchwise dropout only improves performance during training ; during testing the full @xmath9 matrix is used as normal , scaled by a factor of @xmath25 .",
    "however , machine learning research is often constrained by long training times and high costs of equipment . in section [ sec",
    ": results - for - fully - connected ] we show that all other things being equal , batchwise dropout is similar to independent dropout , but faster .",
    "moreover , with the increase in speed , all other things do not have to be equal . with the same resources ,",
    "batchwise dropout can be used to    * increase the number of training epochs , * increase the number of hidden units , * increase the number of validation runs used to optimize `` hyper - parameters '' , or * to train a number of independent copies of the network to form a committee .",
    "these possibilities will often be useful as ways of improving generalization / reducing test error .    in section [ sec : convolutional - networks ]",
    "we look at batchwise dropout for convolutional networks .",
    "dropout for convolutional networks is more complicated as weights are shared across spatial locations .",
    "a minibatch passing up through a convolutional network might be represented at an intermediate hidden layer by an array of size @xmath26 : 100 samples , the output of 32 convolutional filters , at each of @xmath27 spatial locations .",
    "it is conventional to use a dropout mask with shape @xmath26 ; we will call this independent dropout .",
    "in contrast , if we want to apply batchwise dropout efficiently by adapting the submatrix trick , then we will effectively be using a dropout mask with shape @xmath28 .",
    "this looks like a significant change : we are modifying the ensemble over which the average cost is optimized . during training , the error rates are higher .",
    "however , testing the networks gives very similar error rates .",
    "we might have called batchwise dropout _ fast dropout _ but that name is already taken _ _ @xcite .",
    "fast dropout is very different approach to solving the problem of training large neural network quickly without overfitting .",
    "we discuss some of the differences of the two techniques in the appendix .",
    "in theory , for @xmath29 matrices , addition is an @xmath30 operation , and multiplication is @xmath31 by the coppersmithwinograd algorithm .",
    "this suggests that the bulk of our processing time should be spent doing matrix multiplication , and that a performance improvement of about 60% should be possible compared to networks using independent dropout , or no dropout at all .",
    "in practice , sgemm functions use strassen s algorithm or naive matrix multiplication , so performance improvement of up to 75% should be possible .",
    "we implemented batchwise dropout for fully - connected and convolutional neural networks using cuda / cublas ] .",
    "we found that using the highly optimized function to do the bulk of the work , with cuda kernels used to form the submatrices @xmath32 and to update the @xmath9 using @xmath33 , worked well .",
    "better performance may well be obtained by writing a sgemm - like matrix multiplication function that understands submatrices .    for large networks and minibatches , we found that batchwise dropout was substantially faster , see figure [ fig : training - time ] .",
    "the approximate overlap of some of the lines on the left indicates that 50% batchwise dropout reduces the training time in a similar manner to halving the number of hidden units .",
    "the graph on the right show the time saving obtained by using submatrices to implement dropout .",
    "note that for consistency with the left hand side , the graph compares batchwise dropout with dropout - free networks , _ not _ with networks using independent dropout .",
    "the need to implement dropout masks for independent dropout means that figure 1 slightly undersells the performance benefits of batchwise dropout as an alternative to independent dropout .    for smaller networks ,",
    "the performance improvement is lower  bandwidth issues result in the gpu being under utilized .",
    "if you were implementing batchwise dropout for cpus , you would expect to see greater performance gains for smaller networks as cpus have a lower processing - power to bandwidth ratio .",
    "if you have @xmath34 hidden units and you drop out @xmath35% of them , then the number of dropped units is approximately @xmath36 , but with some small variation as you are really dealing with a binomial@xmath37 random variable ",
    "its standard deviation is @xmath38 .",
    "the sizes of the submatrices @xmath32 and @xmath24 are therefore slightly random . in the interests of efficiency and simplicity ,",
    "it is convenient to remove this randomness .",
    "an alternative to dropping each unit independently with probability @xmath39 is to drop a subset of exactly @xmath40 of the hidden units , uniformly at random from the set of all @xmath41 such subsets .",
    "it is still the case that each unit is dropped out with probability @xmath39 .",
    "however , within a hidden layer we no longer have strict independence regarding which units are dropped out .",
    "the probability of dropping out the first two hidden units changes very slightly , from @xmath42 also , we used a modified form of nag - momentum minibatch gradient descent @xcite .",
    "after each minibatch , we only updated the elements of @xmath32 , not all the element of @xmath9 . with @xmath43 and @xmath44 denoting the momentum matrix / submatrix corresponding to @xmath9 and @xmath32 , our update was @xmath45 the momentum still functions as an autoregressive process , smoothing out the gradients , we are just reducing the rate of decay @xmath46 by a factor of @xmath47 .",
    "dropout networks trained using a restricted the number of dropout patterns ( each @xmath13 is from an independent experiment ) .",
    "the blue line marks the test error for a network with half as many hidden units trained without dropout .",
    ", scaledwidth=45.0% ]",
    "the fact that batchwise dropout takes less time per training epoch would count for nothing if a much larger number of epochs was needed to train the network , or if a large number of validation runs were needed to optimize the training process .",
    "we have carried out a number of simple experiment to compare independent and batchwise dropout . in many cases",
    "we could have produced better results by increasing the training time , annealing the learning rate , using validation to adjust the learning process , etc .",
    "we choose not to do this as the primary motivation for batchwise dropout is efficiency , and excessive use of fine - tuning is not efficient .",
    "for datasets , we used :    * the mnist set of @xmath48 pixel handwritten digits . * the cifar-10 dataset of 32x32 pixel color pictures ( @xcite ) . * an artificial dataset designed to be easy to overfit .",
    "following @xcite , for mnist and cifar-10 we trained networks with 20% dropout in the input layer , and 50% dropout in the hidden layers .",
    "for the artificial dataset we increased the input - layer dropout to 50% as this reduced the test error . in some cases ,",
    "we have used relatively small networks so that we would have time to train a number of independent copies of the networks .",
    "this was useful in order to see if the apparent differences between batchwise and independent dropout are significant or just noise .",
    "our first experiment explores the effect of dramatically restricting the number of dropout patterns seen during training .",
    "consider a network with three hidden layers of size 1000 , trained for 1000 epochs using minibatches of size 100 .",
    "the number of distinct dropout patterns , @xmath49 , is so large that we can assume that we will never generate the same dropout mask twice . during independent dropout training",
    "we will see 60 million different dropout patterns , during batchwise dropout training we will see 100 times fewer dropout patterns .    for both types of dropout , we trained 12 independent networks for 1000 epochs , with batches of size 100 . for batchwise dropout we got a mean test error of 1.04% [ range ( 0.92%,1.1% ) , s.d .",
    "0.057% ] and for independent dropout we got a mean test errors of 1.03% [ range ( 0.98%,1.08% ) , s.d . 0.033% ] .",
    "the difference in the mean test errors is not statistically significant .    to explore further",
    "the reduction in the number of dropout patterns seen , we changed our code for ( pseudo)randomly generating batchwise dropout patterns to restrict the number of distinct dropout patterns used",
    ". we modified it to have period @xmath50 minibatches , with @xmath51 ; see figure [ fig : limited - dropout - patterns ] . for @xmath52",
    "this corresponds to only ever using one dropout mask , so that 50% of the network s 3000 hidden weights are never actually trained ( and 20% of the 784 input features are ignored ) . during training",
    "this corresponds to training a dropout - free network with half as many hidden units ",
    "the test error for such a network is marked by a blue line in figure [ fig : limited - dropout - patterns ] .",
    "the error during testing is higher than the blue line because the untrained weights add noise to the network .",
    "if @xmath50 is less than thirteen , is it likely that some of the networks 3000 hidden units are dropped out every time and so receive no training . if @xmath50 is in the range thirteen to fifty , then it is likely that every hidden unit receives some training , but some pairs of hidden units in adjacent layers will not get the chance to interact during training , so the corresponding connection weight is untrained . as the number of dropout masks increases into the hundreds , we see that it is quickly a case of diminishing returns .",
    "to test the effect of changing network size , we created an artificial dataset .",
    "it has 100 classes , each containing 1000 training samples and 100 test samples .",
    "each class is defined using an independent random walk of length 1000 in the discrete cube @xmath53 . for each class we generated the random walk , and then used it to produce the training and test samples by randomly picking points along the length of walk ( giving binary sequences of length 1000 ) and then randomly flipping 40% of the bits .",
    "we trained three layer networks with @xmath54 hidden units per layer with minibatches of size 100 .",
    "see figure  [ fig : artificial - dataset.-100 ] .",
    "looking at the training error against training epochs , independent dropout seems to learn slightly faster .",
    "however , looking at the test errors over time , there does not seem to be much difference between the two forms of dropout . note that the @xmath55-axis is the number of training epochs , not the training time .",
    "the batchwise dropout networks are learning much faster in terms of real time .",
    "learning cifar-10 using a fully connected network is rather difficult .",
    "we trained three layer networks with @xmath56 hidden units per layer with minibatches of size 1000 .",
    "we augmented the training data with horizontal flips .",
    "see figure [ fig : cifar - fc ] .",
    "dropout for convolutional networks is more complicated as weights are shared across spatial locations .",
    "suppose layer @xmath8 has spatial size @xmath57 with @xmath58 features per spatial location , and if the @xmath8-th operation is a convolution with @xmath59 filters .",
    "for a minibatch of size @xmath1 , the convolution involves arrays with sizes : @xmath60 dropout is normally applied using dropout masks with the same size as the layers .",
    "we will call this independent dropout  independent decisions are mode at every spatial location .",
    "in contrast , we define batchwise dropout to mean using a dropout mask with shape @xmath61 .",
    "each minibatch , each convolutional filter is either on or off  across all spatial locations .",
    "these two forms of regularization seem to be doing quite different things .",
    "consider a filter that detects the color red , and a picture with a red truck in it .",
    "if dropout is applied independently , then by the law of averages the message `` red '' will be transmitted with very high probability , but with some loss of spatial information .",
    "in contrast , with batchwise dropout there is a 50% chance we delete the entire filter output .",
    "experimentally , the only substantial difference we could detect was that batchwise dropout resulted in larger errors during training .    to implement batchwise dropout efficiently , notice that the @xmath61 dropout masks corresponds to forming subarrays @xmath32 of the weight arrays @xmath9 with size @xmath62 the forward - pass is then simply a regular convolutional operation using @xmath32",
    "; that makes it possible , for example , to take advantage of the highly optimized @xmath63 function from the https://developer.nvidia.com/cudnn[nvidia cudnn ] package .      for mnist , we trained a lenet-5 type cnn with two layers of @xmath64 filters , two layers of @xmath65 max - pooling , and a fully connected layer @xcite",
    ". there are three places for applying 50% dropout : @xmath66 the test errors for the two dropout methods are similar , see figure [ fig : mnist - test - errors , ] .    , scaledwidth=38.0% ]      for a first experiment with cifar-10 we used a small convolutional network with small filters .",
    "the network is a scaled down version of the network from @xcite ; there are four places to apply dropout : @xmath67 the input layer is @xmath68 .",
    "we trained the network for 1000 epochs using randomly chosen subsets of the training images , and reflected each image horizontally with probability one half .",
    "for testing we used the centers of the images .    in figure",
    "[ fig : cifar-10-results - using ] we show the effect of varying the dropout probability @xmath39 .",
    "the training errors are increasing with @xmath39 , and the training errors are higher for batchwise dropout .",
    "the test - error curves both seem to have local minima around @xmath69 .",
    "the batchwise test error curve seems to be shifted slightly to the left of the independent one , suggesting that for any given value of @xmath39 , batchwise dropout is a slightly stronger form of regularization .    .",
    "batchwise dropout produces a slightly lower minimum test error .",
    "[ fig : cifar-10-results - using],scaledwidth=45.0% ]      we trained a deep convolutional network on cifar-10 _ without _ data augmentation . using the notation of @xcite , our network has the form @xmath70{2})_{12}-832c2 - 896c1-\\mathrm{output},\\ ] ]",
    "i.e. it consists of 12 @xmath65 convolutions with @xmath71 filters in the @xmath50-th layer , 12 layers max - pooling , followed by two fully connected layers ; the network has 12.6 million parameters .",
    "we used an increasing amount of dropout per layer , rising linearly from 0% dropout after the third layer to 50% dropout after the 14th . even though the amount of dropout used in the middle layers is small",
    ", batchwise dropout took less than half as long per epoch as independent dropout ; this is because applying small amounts of independent dropout in large hidden - layers creates a bandwidth performance - bottleneck .",
    "as the network s max - pooling operation is stochastic , the test errors can be reduced by repetition .",
    "batchwise dropout resulted in a average test error of 7.70% ( down to 5.78% with 12-fold testing ) .",
    "independent dropout resulted in an average test error of 7.63% ( reduced to 5.67% with 12-fold testing ) .",
    "we have implemented an efficient form of batchwise dropout . all other things being equal",
    ", it seems to learn at roughly the same speed as independent dropout , but each epoch is faster . given a fixed computational budget , it will often allow you to train better networks .",
    "there are other potential uses for batchwise dropout that we have not explored yet :    * restricted boltzmann machines can be trained by contrastive divergence @xcite with dropout @xcite . batchwise dropout could be used to increase the speed of training . *",
    "when a fully connected network sits on top of a convolutional network , training the top and bottom of the network can be separated over different computational nodes @xcite .",
    "the fully connected top - parts of the network typically contains 95% of the parameters  keeping the nodes synchronized is difficult due to the large size of the matrices . with batchwise dropout",
    ", nodes could communicate @xmath33 instead of @xmath72 and so reducing the bandwidth needed .",
    "* using independent dropout with recurrent neural networks can be too disruptive to allow effective learning ; one solution is to only apply dropout to some parts of the network @xcite .",
    "batchwise dropout may provide a less damaging form of dropout , as each unit will either be on or off for the whole time period . *",
    "dropout is normally only used during training .",
    "it is generally more accurate use the whole network for testing purposes ; this is equivalent to averaging over the ensemble of dropout patterns .",
    "however , in a `` real - time '' setting , such as analyzing successive frames from a video camera , it may be more efficient to use dropout during testing , and then to average the output of the network over time .",
    "* nested dropout @xcite is a variant of regular dropout that extends some of the properties of pca to deep networks .",
    "batchwise nested dropout is particularly easy to implement as the submatrices are regular enough to qualify as matrices in the context of the sgemm function ( using the lda argument ) .",
    "* dropconnect is an alternative form of regularization to dropout @xcite . instead of dropping hidden units ,",
    "individual elements of the weight matrix are dropped out . using a modification similar to the one in section [ sub : fixed - dropout - amounts ] , there are opportunities for speeding up dropconnect training by approximately a factor of two",
    ".    10    d.  ciresan , u.  meier , and j.  schmidhuber .",
    "link : www.idsia.ch/~juergen / cvpr2012.pd[multi - column deep neural networks for image classification ] . in _ computer vision and pattern recognition ( cvpr ) , 2012 ieee conference on _ , pages 36423649 , 2012 .",
    "ben graham .",
    "fractional max - pooling , 2014 .",
    "http://arxiv.org/abs/1412.6071 .",
    "hinton and salakhutdinov .",
    "http://www.cs.toronto.edu/~hinton/science.pdf[reducing the dimensionality of data with neural networks ] .",
    ", 313 , 2006 .",
    "alex krizhevsky .",
    "http://www.cs.toronto.edu/~kriz/cifar.html[learning multiple layers of features from tiny images ] .",
    "technical report , 2009 .",
    "alex krizhevsky .",
    "one weird trick for parallelizing convolutional neural networks , 2014 .",
    "http://arxiv.org/abs/1404.5997 .",
    "y.  l. le  cun , l.  bottou , y.  bengio , and p.  haffner .",
    "gradient - based learning applied to document recognition .",
    ", 86(11):22782324 , november 1998 .",
    "oren rippel , michael  a. gelbart , and ryan  p. adams .",
    "learning ordered representations with nested dropout , 2014 .",
    "http://arxiv.org/abs/1402.0915 .",
    "nitish srivastava , geoffrey hinton , alex krizhevsky , ilya sutskever , and ruslan salakhutdinov .",
    "http://jmlr.org/papers/v15/srivastava14a.html[dropout : a simple way to prevent neural networks from overfitting ] . , 15:19291958 , 2014 .",
    "ilya sutskever , james martens , george  e. dahl , and geoffrey  e. hinton .",
    "http://jmlr.org/proceedings/papers/v28/[on the importance of initialization and momentum in deep learning ] . in",
    "volume  28 of _ jmlr proceedings _",
    ", pages 11391147 .",
    "jmlr.org , 2013 .    li  wan , matthew zeiler , sixin zhang , yann lecun , and rob fergus .",
    "http://jmlr.org/proceedings/papers/v28/wan13.html[regularization of neural networks using dropconnect ] , 2013 .",
    "jmlr w&cp 28 ( 3 ) : 10581066 , 2013 .",
    "sida wang and christopher manning .",
    "http://jmlr.csail.mit.edu/proceedings/papers/v28/wang13a.html[fast dropout training ] .",
    ", 28(2):118126 , 2013 .",
    "wojciech zaremba , ilya sutskever , and oriol vinyals .",
    "recurrent neural network regularization , 2014 .",
    "we might have called batchwise dropout _ fast dropout _ but that name is already taken _ _",
    "fast dropout is an alternative form of regularization that uses a probabilistic modeling technique to imitate the effect of dropout ; each hidden unit is replaced with a gaussian probability distribution .",
    "the _ fast _ relates to reducing the number of training epochs needed compared to regular dropout ( with reference to results in a preprint of @xcite ) . training a network 784 - 800 - 800 - 10 on the mnist dataset with 20% input dropout and 50% hidden - layer dropout",
    ", fast dropout converges to a test error of 1.29% after 100 epochs of l - bfgs .",
    "this appears to be substantially better than the test error obtained in the preprint after 100 epochs of regular dropout training .    however , this is a dangerous comparison to make .",
    "the authors of @xcite used a learning - rate scheme designed to produce optimal accuracy eventually , _ not _ after just one hundred epochs .",
    "we tried using batchwise dropout with minibatches of size 100 and an annealed learning rate of @xmath73 .",
    "we trained a network with two hidden layers of 800 rectified linear units each .",
    "training for 100 epochs resulted in a test error of 1.22% ( s.d .",
    "0.03% ) . after 200 epochs",
    "the test error has reduced further to 1.12% ( s.d .",
    "moreover , per epoch , batchwise - dropout is faster than regular dropout while fast - dropout is slower . assuming we can make comparisons across different programs , the 200 epochs of batchwise dropout training take less time than the 100 epoch of fast dropout training ."
  ],
  "abstract_text": [
    "<S> dropout is a popular technique for regularizing artificial neural networks . </S>",
    "<S> dropout networks are generally trained by minibatch gradient descent with a dropout mask turning off some of the units  a different pattern of dropout is applied to every sample in the minibatch . </S>",
    "<S> we explore a very simple alternative to the dropout mask . </S>",
    "<S> instead of masking dropped out units by setting them to zero , we perform matrix multiplication using a submatrix of the weight matrix  unneeded hidden units are never calculated . performing dropout _ batchwise _ , so that one pattern of dropout is used for each sample in a minibatch , we can substantially reduce training times . </S>",
    "<S> batchwise dropout can be used with fully - connected and convolutional neural networks . </S>"
  ]
}