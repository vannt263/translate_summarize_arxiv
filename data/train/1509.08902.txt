{
  "article_text": [
    "learning distance metrics for comparing multi - dimensional vectors is a fundamental problem . if a perfect task adaptive distance metric is available , many important computer vision problems such as image ( object , scene etc . ) classification and retrieval become trivial using nearest neighbor search @xcite .",
    "metric learning is also applicable to many other important computer vision tasks requiring vector comparisons @xcite .    in the present paper , we are interested in learning a distance metric by embedding the vectors into a low dimensional euclidean space .",
    "we work with pairwise constraints of the form @xmath3 where @xmath4 are the feature vectors and @xmath5 if they are semantically similar and should have a small distance , and @xmath6 otherwise .",
    "this is a practical setting as obtaining such side information is easier , by getting user feedback , than annotating all the vectors for their classes .",
    "moreover , in such scenarios , as no class specific models are learned , the distances and embeddings learned are generic .",
    "[ fig : illus ]    metric learning has attracted substantial attention in the machine learning community @xcite and has specifically achieved much success in computer vision for image auto - annotation @xcite , face verification @xcite , visual tracking @xcite , person reidentification @xcite and nearest neighbor based image classification @xcite . while initial work on metric learning was mostly for learning a linear mahalanobis - like distance , nonlinear metric learning",
    "has also been explored @xcite .",
    "linear methods have been extended to be nonlinear by _ kernelization _ @xcite ( we discuss in detail in  [ sec : relwork ] ) and other nonlinear learning methods have been proposed @xcite . however , such nonlinear methods have not been demonstrated to be scalable ( to order of millions of training points in spaces of order of thousands of dimensions ) .",
    "similar to kernel support vector machines ( svm ) , the complexity ( number of model parameters and evaluation time ) of kernelized versions is often linear in the number of training examples ( number of support vectors ) whose expectation is bounded below by @xmath7 where @xmath8 is the expectation of the probability of error on a test vector and @xmath9 is the number of training examples @xcite .",
    "hence , the complexity can be expected to scale approximately linearly with the number of training examples . ] . in view of such undesirable",
    "scaling , we make the following contributions in this paper .",
    "( i ) inspired by recent work on efficient nonlinear svms @xcite , we propose a novel metric learning method using kernels .",
    "the proposed method can also be seen as a kernel neural network with one hidden layer ( fig .",
    "[ fig : illus ] ) , trained to optimize the verification objective bringing similar pairs close and pushing dissimilar ones far .",
    "we propose to use an efficient stochastic gradient descent ( sgd ) algorithm for training the system .",
    "use of sgd combined with the fact that the complexity ( number of model parameters and evaluation time ) of the proposed method does not depend on the number of training examples makes the method scalable the number of training examples .",
    "while in the present paper we work with nonlinearity based on the popular @xmath10 kernel @xcite , the method is generalizable to kernels whose derivatives can be computed analytically .",
    "( ii ) we show consistent improvement obtained by the method for the task of semantic category based retrieval with seven challenging publicly available image datasets of materials , birds , human attributes , scenes , flowers , objects and butterflies .",
    "our experimental results support the datasets .",
    "( iii ) we also demonstrate scalability by training with order of millions of training pairs of 4096 dimensional state - of - the - art cnn features @xcite and compare with five existing competitive baselines .",
    "metric learning has been an active topic of research ( we encourage the interested reader to see @xcite for extensive surveys ) with applications to face verification @xcite , person reidentification @xcite , image auto - annotation @xcite , visual tracking @xcite , nearest neighbor based image classification @xcite in computer vision .",
    "starting from the seminal paper of xing @xcite , many different approaches for learning metrics have been proposed @xcite .",
    "different types of supervision has been used for learning metrics .",
    "while some methods require class level supervision @xcite , others only require triplet constraints , @xmath11 , where @xmath12 should be closer to @xmath13 than to @xmath14 @xcite , and others still , only pairwise constraints , @xmath3 where @xmath15 if @xmath16 are similar and @xmath6 if they are dissimilar @xcite .",
    "most of the initial metric learning methods were linear , the semidefinite programming formulation by xing @xcite , large margin formulation for @xmath17-nn classification by weinberger @xcite , ` collapsing classes ' formulation ( make the distance between vectors of same class zero and between those of different classes large ) of globerson and roweis @xcite and neighbourhood component analysis of goldberger @xcite .    towards scalability of metric learning methods , jain @xcite proposed online metric learning and more recently simonyan @xcite proposed to use stochastic gradient descent for face verification problem",
    ".    works also reported learning nonlinear metrics .",
    "many linear metric learning approaches were shown to be kernelizable @xcite .",
    "tsang and kwok @xcite proposed a metric learning problem similar to the @xmath18-svm @xcite , which they solved in the dual allowing the use of kernels .",
    "schultz and joachims @xcite proposed support vector machine based algorithm which was accordingly kernelized while chatpatanasiri @xcite proposed to use kernel pca for nonlinear metric learning . mignon and jurie @xcite proposed to use kernels for metric learning with sparse pairwise constraints with a logistic loss based objective function  we are interested in a similar weakly supervised setting and we discuss this in more detail in the next section (  [ sec : background ] ) .",
    "other than nonlinearity via kernelization , alternate nonlinear forms of metrics have also been studied based on variations and adaptations of neural networks @xcite , boosting @xcite binary codes with hamming distances @xcite .",
    "weinberger @xcite also proposed to use local metric learning for introducing nonlinearity in the learnt metric .",
    "given a dataset @xmath19 of positive and negative pairs of vectors @xmath20 , with @xmath21 and @xmath22 and @xmath23 being an index set , the task is to learn a distance function in @xmath24 .",
    "many metric learning approaches learn , from @xmath19 , a mahalanobis - like metric parametrized by matrix @xmath25 , @xmath26 @xmath27 is required to be symmetric positive semi - definite ( psd ) matrix , for the distance to be a valid metric , and hence can be factorized as @xmath28 , with @xmath29 and @xmath30 .",
    "the metric learning can then be seen as learning a projection upon which the comparison is done using euclidean distance in the resulting space @xmath31    learning such distance function has been achieved by using , among other methods , optimization of probabilistic objectives ( based on likelihood ) or loss functions based on the max - margin principle for the task of face verification in computer vision @xcite . here , we minimize the objective with hinge loss , @xmath32 which aims to learn @xmath33 such that the positive pairs are at distances less than @xmath34 , to each other , while the negatives are at distances greater than @xmath35 , with @xmath36 being the bias parameter a threshold on the distance between two vectors to decide if they are same or not .",
    "explicit regularization is absent as often @xmath37 the rank @xmath2 of the learnt metric @xmath28 is fixed to be small .",
    "while the distance function learned as above is linear , the problem may be complex and require nonlinear distance function . a popular way of learning a nonlinear distance function is by kernelizing the metric , as inspired by the traditional kernel based methods kpca and klda ; invoke representer theorem like condition and write the rows of @xmath33 as linear combinations of the input vectors @xmath38 ( where @xmath39 is the matrix of all vectors @xmath12 as columns ) .",
    "noticing that the distance function in eq .",
    "[ eqn : dist ] depends only on the dot products of the vectors , allows nonlinearizing the algorithm as follows .",
    "mapping the vectors with a non - linear _ feature map _ @xmath40 and then using the _ kernel trick _ @xmath41 , we can proceed as follows , @xmath42 where , @xmath43 $ ] is the matrix of @xmath44 mapped vectors and @xmath45^t\\ ] ] is the t@xmath46 column of the kernel matrix .",
    "such reasoning was used by mignon and jurie @xcite recently . while this is a successful way of nonlinearizing the algorithm , it is costly and not scalable as training requires the whole kernel matrix .",
    "we now give the details of the proposed nonlinear embeddings by approximate kernelization of mahalanobis - like distance metric learning .",
    "continuing from the discussion in the previous section [ sec : background ] , we note that the rows of @xmath33 can be thought of as a basis set ( albeit not necessarily orthogonal ) on which the test vectors are projected by taking dot products . after projection the comparison",
    "is simply done using the euclidean distance .",
    "now consider again a nonlinear feature map @xmath40 , and the corresponding kernel function @xmath47 ; we can view the projection , with matrix @xmath48 in the @xmath44 mapped space @xmath49 , as @xmath50,\\ ] ] where @xmath51 are the rows of @xmath48 . instead of writing each @xmath52 as linear combinations of @xmath53 ,",
    "as done traditionally , we take an alternate route and make an approximating assumption as follows .",
    "we recall the concept of pre - image , in @xmath54 , of a vector in @xmath49 @xmath55 corresponding to a feature space vector @xmath56 such that @xmath57 , which has been studied in the past in the context of different kernel methods @xcite .",
    "we assume that there exists @xmath58 such that either @xmath59 @xmath60 is the pre - image of @xmath51 or , if such pre - image does nt exist , then @xmath61 @xmath60 is an approximation for the pre - image of @xmath62 .",
    "once we have @xmath59 , we can then write @xmath63 \\\\                  &",
    "= \\left[k(\\l_1,\\x ) , k(\\l_2,\\x),\\ldots , k(\\l_d,\\x ) \\right].\\end{aligned}\\ ] ] intuitively , what we did here is that instead of a linear projection of the test vector on the rows of @xmath33 as in the linear case (  [ sec : background ] above ) , we now do a ` nonlinear projection ' on the rows of @xmath64^t$ ] .",
    "this can also be seen in the similar spirit as dimensionality reduction using nonlinear kernel methods in kernel pca the principal components come out to be linear combinations of @xmath44 mapped input vectors @xmath65 , and the test vectors are projected nonlinearly to these principal components .",
    "similarly , our method can be seen as a way of doing nonlinear dimensionality reduction with , as we will detail in the following , discriminative supervised learning using similar and dissimilar pairs annotations .",
    "_ given _ : training set ( @xmath19 ) , margin ( @xmath66 ) , learning rate ( @xmath67 ) _ initialize _",
    ": @xmath68 , @xmath69 random@xmath70 randomly sample a training pair @xmath71 compute @xmath72 using eq .",
    "[ eqn : nonlindist ] // ref .",
    "[ eqn : grad ] @xmath73    following our assumption , the distance function computed in the feature space becomes , @xmath74 with this formulation , the parameters to be learned are the elements of the matrix @xmath75 @xmath76 .",
    "note that this matrix is different from the @xmath33 matrix in the linear distance function case above in ",
    "[ sec : background ] .",
    "we propose to learn @xmath75 with a standard max margin hinge loss based objective .",
    "the optimization problem thus takes the form , @xmath77 where the fixed margin of @xmath78 is replaced by a free parameter @xmath66 .",
    "this is important as depending on the kernel used the distances may be bounded from above with histogram based kernels commonly used in computer vision , say @xmath10 kernel , the maximum distance between two vectors ( histograms ) is bounded by zero from below and unity from above .",
    "hence , clearly the bias @xmath36 ( recall that this is like a threshold on the distances , to decide if a pair is same or not ) has to be less than unity and consequently the margin has to be less than unity as well . as we will show later in experiments ,",
    "the method is not very sensitive to these parameters and we fixed it once for different datasets .    finally , we substitute for the distance function in feature space using eq .",
    "[ eqn : nonlindist ] and propose to optimize the objective efficiently using sgd . at each iteration",
    "we sample an annotated training pair and update the @xmath75 matrix based on the sub - gradients of the objective function . while the objective function is nonlinear we find that the optima obtained with our sgd implementation perform consistently and well in practice , without per instance / database tuning .",
    "[ algo : ml ] gives the pseudo code of the procedure used for generating the experimental results in this paper .",
    "the sub - gradients required for the sgd iterations in the algorithm are calculable analytically and are as follows , @xmath79 where @xmath80 and @xmath81 if @xmath82 and @xmath83 , otherwise . as a last detail , we use the shifted @xmath10 kernel , shown to be quite effective for image classification @xcite , given by @xmath84 the gradient for the @xmath10 kernel is also analytically calculable and is given by , @xmath85    the complexity of the stochastic updates is @xmath0 kernel based on the earth movers distance ( emd ) @xcite , we work here only with those with linear complexity . ] ( kernel evaluations with @xmath86 ) , while the complexity of similar updates with the traditional parametrization will be @xmath87 where @xmath88 is the number of distinct training vectors , as the @xmath89 are linear combinations of @xmath90 . since @xmath2 is fixed and small ( @xmath91 ) the updates are relatively inexpensive and the algorithm is , thus , scalable to order of millions of training points .",
    "also , since the learning is using a sgd based algorithm which takes the training pairs sequentially , it can also be applied in an online setting where the training examples are only available with time .",
    "another important salient feature of the algorithm is that it can directly work with high dimensional vectors and does not require them to be compressed ( preprocessed ) first with unsupervised methods , pca @xcite or kpca @xcite .",
    "this potentially allows the method to exploit the full representative power of the high dimensional space of the vector for the current discrimination task , which might be otherwise lost in the case of unsupervised dimensionality reduction as a preprocessing .",
    "* datasets . *",
    "we report experiments with seven publicly available datasets : pascal voc 2007 @xcite , scene-15 @xcite , flickr materials ( fmd ) @xcite , oxford 102-flowers @xcite , leeds butterflies @xcite , caltech ucsd birds 200 - 2011 ( cub ) @xcite and human attributes ( hat ) @xcite . tab .",
    "[ tab : datasets ] gives the statistics , number of classes and number of training , validation and testing images , for the seven datasets .",
    "we use the provided ` train+val ` sets for training and ` test ` set for testing where available .",
    "otherwise , for scene-15 we randomly take 100 images per class for training and rest for testing and for leeds butterflies we take the first 20 images of each class for testing and rest for training .",
    "we use only the classes with less than 1000 positive images in the hat dataset , as including the images with high number of positives was giving saturated results , while respecting the ` train / val / test ` split provided . +",
    "* image features . *",
    "cnn features have been quite successful in image classification after the seminal work of krizhevsky @xcite , have been competitive for many different computer vision tasks @xcite .",
    "thus , we use cnn features using the ` matconvnet ` @xcite library , with the 16 layer model @xcite which is pre - trained on the imagenet dataset @xcite .",
    "we use the outputs of the last fully connected layer after linear rectification our features are non - negative . to validate the baseline implementation we used the extracted 4096 dimensional cnn features with linear svm to perform the classification task for the different datasets .",
    "[ tab : datasets ] gives the performance and those of the best method on the dataset .",
    "we see that the features used here perform competitively to existing methods .",
    "hence , we conclude that the features we use in the experiments are relevant and comparable to the state - of - the - art .",
    "dataset & # classes & # train+val & # test & present & existing + scene-15@xmath92 @xcite & 15 & 1500 & 2985 & 90.3 &   91.6   @xcite + flickr materials@xmath92 @xcite & 10 & 500 & 500 & 79.6 &   82.8    @xcite   + leeds butterflies@xmath92 @xcite & 10 & 632 & 200 & 99.0 &   96.4   @xcite + pascal voc 2007@xmath93 @xcite & 20 & 5011 & 4952 & 86.1 &   89.7   @xcite + human attributes@xmath93 @xcite & 27 & 7000 & 2344 & 55.4 &   59.7   @xcite + oxford 102-flowers@xmath92 @xcite & 102 & 2020 & 6149 & 84.3 &   86.8   @xcite + caltech ucsd birds@xmath92 @xcite & 200 & 5994 & 5794 & 63.1 &   69.1    @xcite   +     + * baselines .",
    "* we report results with five baselines . in all cases the final vectors",
    "are compared using euclidean distance . as a reference",
    ", we take the full ( @xmath94 normalized ) 4096 dimensional cnn features ( denoted ` no proj ' in the figures ) with euclidean distance .",
    "such a system was recently shown to be competitive for instance retrieval @xcite .",
    "as the first baseline , we do principal component analysis ( pca ) based dimensionality reduction . as the second baseline , we learn a metric using the neighborhood component analysis ( nca ) @xcite . as the third and fourth baseline , we learn a metric using the large margin nearest neighbor ( lmnn ) @xcite algorithm with ( i ) vectors reduced to dimension @xmath2 with pca , and learning a square projection metric , denoted lmnn(s ) , and ( ii ) vectors reduced to 256 dimensions ( 98% variance on average ) with pca ( for efficiency ) and then learning a rectangular projection matrix , denoted lmnn(r ) .",
    "hat and voc 2007 datasets have some images which have multiple labels ; for training nca and lmnn baselines such images were removed from the training set as these algorithm use a multi - class supervision .",
    "publicly available code is used for nca and lmnn algorithms . as the final baseline we use the linear metric learning",
    "( ml ) algorithm as described in ",
    "[ sec : background ] with similar objective function ( modulo nonlinearity ) and the same training data as the proposed method . +",
    "* evaluation .",
    "* we report results for the retrieval setting .",
    "we use the ` train+val ` sets of the datasets for training our baselines and the proposed nonlinear method .",
    "we use each ` test ` image as a query and the rest of the ` test ` images as the gallery and report the mean precision@@xmath95 ( mprec@@xmath95 ) average of precision@@xmath95 is computed for queries of a given class , averaged over the classes .",
    "the cnn features for the test images are transformed by the respective methods and are compared with euclidean distance . +",
    "* implementation details .",
    "* we fix the number of iterations to one million .",
    "we sample ( up to ) 500,000 pairs of similar and dissimilar vectors from the ` train+val ` set for training both the baseline linear metric learning and the proposed nonlinear metric learning ( nml ) .",
    "the baseline methods lmnn and nca do not use similar pairwise constraints but constraints derived using class labels .",
    "the vectors were @xmath94 normalized for all baselines normalization with @xmath96 and @xmath10 distances for the reference 4096 dimensional vectors , the results were similar ( see supplementary material ) . ] and were @xmath96 normalized for the proposed method ( as the @xmath10 kernel is based on histograms ) .",
    "the bias and margin for the linear metric learning baseline were fixed to @xmath68 and @xmath97 while those for the proposed nonlinear method were fixed to @xmath98 and @xmath99 .",
    "these values were chosen based on preliminary experiments on the voc 2007 validation set and were kept _",
    "constant for all experiments _ reported no dataset specific tuning was done .",
    "the algorithm is not very sensitive to the choice of @xmath66 and @xmath36 , fig .",
    "[ fig : perf_m_b ] show the performances for a range of @xmath66 and @xmath36 values ( with the other fixed resp . ) on the flickr materials @xcite and pascal voc 2007 @xcite datasets .",
    "the bias and margin need to be kept sufficiently low .",
    "the reason for low values of bias , and resp",
    ".  margin , for the nonlinear method is that since the vectors are @xmath96 normalized histograms , the maximum score they can give with the @xmath10 kernel is 1 ( which only happens with the vector itself ) and hence the scale of the distances is expected to be less than 1 for the proposed nonlinear method .",
    "( fixed @xmath98 ) and bias @xmath36 ( fixed @xmath99 ) , for the proposed method ( @xmath100 ) . ]",
    "[ fig : perf_m_b ]          +         [ fig : prec_d ]          +          we present some results for typical setting and analyse them .",
    "we refer the reader to the supplementary material for additional results .",
    "[ fig : prec_d ] shows the performance of the different methods for different projection dimensionality @xmath101 , with a fixed @xmath102 , the number of top images retrieved .",
    "[ fig : prec_k ] shows the performance for different @xmath103 $ ] with a fixed @xmath104 .",
    "the birds and butterflies datasets have @xmath105 and @xmath106 images per class and hence we only report for @xmath95 up to @xmath107 and @xmath108 for them , respectively .",
    "we observe , from fig .",
    "[ fig : prec_d ] , for @xmath102 precision for the top 10 retrieved images , that among the baselines , supervised lmnn is generally better than supervised nca and unsupervised pca while the ml baseline is the most competitive .",
    "the proposed nml is generally better or at least as good as the baselines , notably the most competitive ml baseline .",
    "it is outperformed by the baselines only on the flowers dataset at relatively higher dimensional ( @xmath109 ) projections .",
    "the improvements are consistent over materials , birds , objects and scenes datasets for all @xmath110 . for the human attributes",
    ", nml does not improve the baseline ml , but is essentially similar to it does not deteriorate either . on the flowers",
    "dataset nml is outperformed by the baselines at higher dimensions .",
    "further , from fig .  [ fig : prec_k ] we note a similar general trend , for a fixed projection dimension @xmath104 while observing the precision at varying number of @xmath95 top retrievals .",
    "the proposed nml consistently performs better on the materials , birds , objects and scenes datasets while on the human attributes and butterflies datasets it performs similar , compared to the baselines .",
    "in addition to the results shown here , in general nml was found to be better or at least similar to the baseline ml methods ( see supplementary material ) .",
    "the few cases where it is not better are those at high projection dimensions @xmath2 , which could be explained by the higher need of nonlinearity of the model at lower dimensions , where an equivalent linear model with similar number of parameters may not suffice , while at higher dimensions , the larger number of parameters for the linear method are sufficient for the task .",
    "thus , we conclude that except in a single case , the flowers dataset at relatively higher dimensional projections , nml improves over the reported baselines demonstrating the benefit of the proposed scalable nonlinear embedding and efficient sgd based learning thereof .    tab .",
    "[ tab : kml_vs_nml ] gives typical performances of the kernelized version (  [ sec : background ] ) of baseline ml ( kml ) the proposed nml , on the materials dataset .",
    "nml obtains similar performances while being @xmath111 at test time @xmath112 for kml .",
    "it is also interesting to note the performance of the method after compression compared to using the full 4096 dimensional cnn feature without any compression . in both fig .",
    "[ fig : prec_d ] and fig .",
    "[ fig : prec_k ] the dotted black line shows this performance as a reference .",
    "we see that more often than not , the discriminatively learnt nml projection improves the performance over the full features . for the materials , human attributes , scenes , and butterflies",
    "datasets this improvement is significant . in the case of birds and",
    "voc 2007 objects the performance drops below the reference for low dimension projections while gradually improving at higher dimensional projections . only in the case of the flowers",
    "dataset , nml projection does not improve this reference but is roughly equal to it at higher dimensions , @xmath113 .",
    "thus , we conclude that the proposed nml is also beneficial to improve performance of the reference system with full feature vector ( 4096 dimensional ) without compression .",
    "[ cols=\"^,^,^\",options=\"header \" , ]",
    "[ supp_fig : prec_k_d8][supp_fig : prec_k_d64 ] show the performance vs.  @xmath95 for different @xmath114 and fig .",
    "[ supp_fig : prec_d_k1][supp_fig : prec_d_k20 ] show the performance of the method vs.  the projection dimension @xmath2 for different values of top retrieved examples @xmath115 .",
    "the results support the discussion in the paper  the proposed method ( nml ) improves the performance over the baselines in most of the datasets for most of the settings .",
    "otherwise , it is as good as the best baseline and only in very few cases , is worse than any baseline ."
  ],
  "abstract_text": [
    "<S> we propose a novel algorithm for the task of supervised discriminative distance learning by nonlinearly embedding vectors into a low dimensional euclidean space . </S>",
    "<S> we work in the challenging setting where supervision is with constraints on similar and dissimilar pairs while training . </S>",
    "<S> the proposed method is derived by an approximate kernelization of a linear mahalanobis - like distance metric learning algorithm and can also be seen as a kernel neural network . </S>",
    "<S> the number of model parameters and test time evaluation complexity of the proposed method are @xmath0 where @xmath1 is the dimensionality of the input features and @xmath2 is the dimension of the projection space  this is in contrast to the usual kernelization methods as , unlike them , the complexity does not scale linearly with the number of training examples . </S>",
    "<S> we propose a stochastic gradient based learning algorithm which makes the method scalable ( the number of training examples ) , while being nonlinear . </S>",
    "<S> we train the method with up to half a million training pairs of 4096 dimensional cnn features . </S>",
    "<S> we give empirical comparisons with relevant baselines on seven challenging datasets for the task of low dimensional semantic category based image retrieval . </S>"
  ]
}