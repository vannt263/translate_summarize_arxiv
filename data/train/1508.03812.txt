{
  "article_text": [
    "detecting causal relationships in data is an important data analytics task as causal relationships can provide better insights into data , as well as actionable knowledge for correct decision making and timely intervening in processes at risk .",
    "causal relationships are normally identified with experiments , such as randomised controlled trials  @xcite , which are effective but expensive and often impossible to be conducted .",
    "causal relationships can also be found by observational studies , such as cohort studies and case control studies  @xcite .",
    "an observational study takes a causal hypothesis and tests it using samples selected from historical data or collected passively over the period of time when observing the subjects of interest .",
    "therefore observational studies need domain experts knowledge and interactions in data selection or collection and the process is normally time consuming .",
    "currently there is a lack of scalable and automated methods for causal relationship exploration in data .",
    "these methods should be able to find causal signals in data without requiring domain knowledge or any hypothesis established beforehand .",
    "the methods must also be efficient to deal with the increasing amount of big data .",
    "classification methods are fast and have the potential to become practical substitutes for finding causal signals in data since the discovery of causal relationships is a type of supervised learning when the target or outcome variable is fixed . decision trees  @xcite are a good example of classification methods , and they have been widely used in many areas , including social and medical data analyses",
    ".    however , classification methods are not designed with causal discovery in mind and a classification method may find false causal signals in data and miss true causal signals .",
    "for example , figure  [ fig_hypnotizeddt ] shows a decision tree built from a hypothesised data set of the recovery of a disease .",
    "based on the decision tree , we may conclude that the use of tinder ( a matchmaking mobile app ) helps cure the disease .",
    "however , it is misleading since the majority of people using tinder are young whereas most people not using tinder are old .",
    "young people will recover from the disease anyway and old people have a lower chance of recovery .",
    "this misleading decision tree is caused by an unfair comparison between the two different groups of people .",
    "it may be a good classification tree to predict the likelihood of recovery , but it does not imply the causes of recovery and its nodes do not have any causal interpretation .",
    "r0.25        a classification method fails to take account of the effects of other variables on the class or outcome variable when examining the relationship between a variable and the class variable , and this is the major reason for the false discoveries ( of causal relationships ) . for example , when we study the relationship between using tinder and the recovery of a disease , the effect of other variables such as age , gender , and health condition of patients ( who may or may not use tinder ) should be considered .",
    "the objective is not simply to maximise the difference of the conditional probabilities of recovered and not recovered conditioning on the use of tinder when a classifier is being sought .    in this paper , we design a causal decision tree ( cdt ) where nodes have causal interpretations . as presented in the following sections , our method follows a well established causal inference framework , the potential outcome model , and it makes use of a classic statistical test , mantel - haenszel test .",
    "the proposed cdt is practical for uncovering causal signals in large data .    the paths in a cdt are not interpreted as `` if - then",
    "'' first order logic rules as in a normal decision tree .",
    "for example , figure  [ fig_titanic_adult ] ( left ) shows a cdt learnt from the titanic data set",
    ". it does not read as `` if female = n then survived = n '' .",
    "a node in a cdt indicates a causal factor of the outcome attribute .",
    "the node ` female ' indicates that being female or not is causally related to survived or not ; the node ` thirdclass ' shows that in the female group ( the context ) , staying in a third class cabin or not is causally related to survived or not .",
    "the main contributions of this paper are as follows :    * we systematically analyse the limitations of decision trees for causal discovery and identify the underlying reasons .",
    "* we propose the cdt method , which can be used to represent and identify simple and interpretable causal relationships in data , including big data .",
    "let @xmath0 be a predictive attribute and @xmath1 the outcome attribute where @xmath2 and @xmath3 we aim to find out if there is a causal relationship between @xmath0 and @xmath1 .",
    "for easy discussion , we consider that @xmath4 is a treatment and @xmath5 the recovery",
    ". we will establish if the treatment is effective for the recovery .",
    "the potential outcome or counterfactual model @xcite is a well established framework for causal inference .",
    "here we introduce the basic concepts of the model and a principle for estimating the average causal effect , mainly following the introduction in  @xcite .    with the potential outcome model ,",
    "an individual @xmath6 in a population has two potential outcomes for a treatment @xmath0 : @xmath7 when taking the treatment and @xmath8 when not taking the treatment .",
    "we say that @xmath7 is the potential outcome in the treatment state and @xmath8 is the potential outcome in the control state",
    ". then we have the following definition .",
    "the individual level causal effect is defined as the difference of two potential outcomes of an individual , i.e. @xmath9 .    in practice we can only find out one outcome @xmath7 or @xmath8 since one person can be placed in either the treatment group ( @xmath4 ) or the control group ( @xmath10 ) .",
    "one of the two potential outcomes has to be estimated .",
    "so the potential outcome model is also called counterfactual model .",
    "for example , we know that mary has a headache ( the outcome ) and she did not take aspirin ( the treatment ) , i.e. we know @xmath8 .",
    "the question is what the outcome would be if mary took aspirin one hour ago , i.e. we want to know @xmath7 and to estimate the ice of aspirin on mary s condition ( having headache or not ) .",
    "if we had both @xmath7 and @xmath8 of an individual we would aggregate the causal effects of individuals in a population to get the average causal effect as defined below , where @xmath11 $ ] stands for expectation operator in probability theory .",
    "the average causal effect of a population is the average of the individual level causal effects in the population , i.e. @xmath12 = e[y^1_i ] - e[y^0_i]$ ] .",
    "note that @xmath6 is kept in the above formula as other work in the counterfactual framework to indicate individual level heterogeneity of potential outcomes and causal effects .",
    "assuming that @xmath13 proportion of samples take the treatment and @xmath14 proportion do not , and the sample size is large so the error caused by sampling is negligible , given a data set @xmath15 , the ace , @xmath12 $ ] can be estimated as : @xmath16 & \\!\\!\\!= \\!\\!\\ ! & \\pi ( e_\\textbf{d}[y^1_i | x_i = 1 ] - e_\\textbf{d}[y^0_i | x_i = 1 ] ) + \\nonumber \\\\   &   & ( 1-\\pi ) ( e_\\textbf{d}[y^1_i | x_i = 0 ] - e_\\textbf{d}[y^0_i | x_i = 0])\\end{aligned}\\ ] ] that is , the ace of the population is the ace in the treatment group plus the ace in the control group , where @xmath17 indicates that an individual takes the treatment and the causal effect is @xmath18 .",
    "similarly , @xmath19 indicates that an individual does not take the treatment and the causal effect is @xmath20 .    in a data set",
    ", we can observe the potential outcomes in the treatment state for those treated , ( @xmath21 ) , and the potential outcomes in the control state for those not treated , ( @xmath22 ) .",
    "however , we can not observe the potential outcomes in the control state for those treated , ( @xmath23 ) , or the potential outcomes in the treatment state for those not treated , ( @xmath24 ) .",
    "we have to estimate what the potential outcome , @xmath25 , would be if an individual did not take the treatment ( in fact she has ) ; and what potential outcome , ( @xmath24 ) , would be if an individual took the treatment ( in fact she has not ) .    with a data set @xmath15 we can obtain the following `` nave '' estimation of the ace : @xmath26 = e_\\textbf{d}[y^1_i | x_i = 1 ] - e_\\textbf{d}[y^0_i | x_i = 0]\\end{aligned}\\ ] ]    the question is when the nave estimation  ( equation ( [ eqn_2 ] ) ) will approach the true estimation  ( equation ( [ eqn_1 ] ) ) .    if the assignment of individuals to the treatment and control groups is purely random , the estimation in equation  ( 2 ) approaches the estimation in equation  ( 1 ) . in an observational data set ,",
    "however , the random assignment is not possible .",
    "how can we estimate the average causal effect ?",
    "a solution is by perfect stratification .",
    "let the differences of individuals in a data set be characterised by a set of attributes @xmath27 ( excluding @xmath0 and @xmath1 ) and let the data set be perfectly stratified by @xmath27 . in each stratum ,",
    "apart from the fact of taking treatment or not , all individuals are indistinguishable from each other . under the perfect stratification assumption ,",
    "we have : @xmath28 = e[y^1_i | x_i = 1 , \\textbf{s } = \\textbf{s}_i ]    \\label{firsteqn}\\\\ e[y^0_i | x_i = 1 , \\textbf{s } = \\textbf{s}_i ] = e[y^0_i | x_i = 0 , \\textbf{s}=\\textbf{s}_i]\\label{secondeqn}\\end{aligned}\\ ] ]    where @xmath29 indicates a stratum of perfect stratification .",
    "since individuals are indistinguishable in the stratum , unobserved potential outcomes can be estimated by observed ones .",
    "specifically , the mean potential outcome in the treatment state for those untreated is the same as that in the treatment state for those treated ( equation   ( [ firsteqn ] ) ) , and the mean potential outcome in the control state for those treated is the same as that in the control state for those untreated ( equation  ( [ secondeqn ] ) ) . by replacing equation  ( 1 ) with equations  ( [ firsteqn ] ) and ( [ secondeqn ] )",
    ", we have : @xmath30 \\nonumber\\\\   & = & \\pi ( e_\\textbf{d}[y^1_i | x_i = 1 , \\textbf{s } = \\textbf{s}_i ] - e_\\textbf{d}[y^0_i | x_i = 1 , \\textbf{s } = \\textbf{s}_i ] ) +   \\nonumber \\\\   &    & ( 1-\\pi ) ( e_\\textbf{d}[y^1_i | x_i = 0 , \\textbf{s } = \\textbf{s}_i ] - e_\\textbf{d}[y^0_i | x_i = 0 , \\textbf{s } = \\textbf{s}_i ] )   \\nonumber \\\\   & = & \\pi ( e_\\textbf{d}[y^1_i | x_i = 1 , \\textbf{s } = \\textbf{s}_i ] - e_\\textbf{d}[y^0_i | x_i = 0 , \\textbf{s } = \\textbf{s}_i ] ) +   \\nonumber \\\\   &    & ( 1-\\pi ) ( e_\\textbf{d}[y^1_i | x_i = 1 , \\textbf{s } = \\textbf{s}_i ] - e_\\textbf{d}[y^0_i | x_i = 0 , \\textbf{s } = \\textbf{s}_i ] )   \\nonumber \\\\   & = & e_\\textbf{d}[y^1_i | x_i = 1 , \\textbf{s } = \\textbf{s}_i ] - e_\\textbf{d}[y^0_i | x_i = 0 , \\textbf{s } = \\textbf{s}_i ]   \\nonumber \\\\   & = & e^{naive}_\\textbf{d}[\\delta_i | \\textbf{s } = \\textbf{s}_i ] \\nonumber\\end{aligned}\\ ] ]    as a result , the nave estimation approximates the true average causal effect , and we have the following observation .",
    "[ principle][principle for estimating average causal effect ] the average causal effect can be estimated by taking weighted sum of nave estimators in stratified sub data sets .",
    "this principle ensures that each comparison is between individuals with no observable differences , and hence the estimated causal effect is not resulted from other factors than the studied one . in the following",
    ", we will use this principle to estimate causal effect in observational data sets .    as a result",
    ", we have the following results .",
    "@xmath31 & = & ( \\pi e[y^1_i | x_i = 1 ] + ( 1-\\pi ) e[y^1_i | x_i = 0 ] ) - ( \\pi e[y^0_i | x_i = 1 ] +   ( 1-\\pi ) e[y^0_i | x_i = 0 ] ) \\\\              & = & e[y^1_i | x_i = 1 , s_i ] - e[y^0_i | x_i = 0 , s_i]\\end{aligned}\\ ] ]    in the above formulae , both potential outcomes are discoverable since individuals in data are interchangeable",
    ". we can remove superscripts and subscripts .",
    "@xmath32 - e[y=1 | x = 0 , s]\\ ] ]    the key for estimating causal effect of a treatment becomes finding perfect stratification samples .",
    "decision trees are a popular classification model , with two types of nodes : branching and leaf nodes .",
    "a branching node represents a predictive attribute and each of its values denotes a choice and leads to another branching node or a leaf node representing a class .",
    "now we use the potential outcome model to explain why decision trees may not encode causality .    [ cols=\"^,^ \" , ]     [ fig_titanic_adult ]    the cdt built from the data set is shown in figure  [ fig_titanic_adult ] ( left ) . at the first level , the tree reveals a causal relationship between ` female ' ( gender ) and ` survived ' .",
    "this relationship is sensible as we know that if someone was a female , she was likely to have higher priority to board the limited number of lifeboats . at the second level , the tree gives a context specific causal relationship between ` thirdclass ' and ` survived ' in the female group , which is reasonable too as passengers in the lowest class cabins would have less chance to escape .",
    "therefore the tree is simple but it gives insights about the causes of surviving for people on titanic , and the results are logic .",
    "the adult data set ( table  [ tab_titanic_adult_data ] ) was retrieved from the uci machine learning repository @xcite and it is an extraction of 1994 usa census database .",
    "it is a well known classification data set to predict whether a person earns over 50k or not in a year .",
    "we recoded the data set to make the causes for high / low income more clearly and easily understandable .",
    "the objective is to find the causal factors of high ( or low ) income .    from the cdt built with the data set ( figure  [ fig_titanic_adult ] , right )",
    ", there is a causal relationship between ` age@xmath3330 ' ( or not ) and income , i.e. young adults have lower income , which follows common knowledge . for older adults ,",
    "year of education is causally related to income , i.e. adults with education shorter than 12 years would have low income , which makes good sense too . for older and highly educated adults ,",
    "gender affects income such that females have lower income than male ( an unfortunate finding but it could be true in reality ) . in the highly educated and older male group ,",
    "occupation is a causal factor of income so that those in professional occupations earn more than those not in professional occupations .",
    "we see that the cdt gives sensible explanations for the causes of high or low income .",
    "[ fig_adultc45 ]    from the normal decision tree from the adult data set ( figure  [ fig_adultc45 ] ) , we have observed that : ( 1 ) a normal decision may be large for high classification accuracy but a large tree reduces its interoperability .",
    "the objectives of causal discovery and classification may not be consistent ; ( 2 ) causality based and classification based criteria do not make the same choice . in the top level ,",
    "the branching attribute of the normal decision tree is ` education - num@xmath3412 ' while the branching attribute of the cdt is ` age@xmath3330 ' . from common knowledge",
    ", age should have stronger influence on income than years of education since young adults usually get lower income in most cases , simply because of their lack of working experience .",
    "formally , there are more strata ( 13.3% of all strata ) violating the causal relationship between ` education - num@xmath3412 ' and ` @xmath3450k ' than those ( 7.75% of all strata ) violating the causal relationship between ` age @xmath3330 ' and ` @xmath33 50k ' .",
    "this is why the cdt chooses ` age @xmath3330 ' as the root .",
    "in contrast , since attribute ` education - num @xmath34 12 ' has a higher information gain than ` age @xmath3330 ' it is chosen to split the data set firstly in a normal decision tree .",
    "different choices lead to different trees . in this example",
    ", the different choices do not cause significant difference as ` age @xmath3330 ' is chosen immediately after ` education - num @xmath34 12 ' , but in other data sets , the difference could be significant . for causal discoveries ,",
    "it is better to choose a causal based criterion .    with the above data sets , the first few levels at the top of a normal decision tree is quite interpretable since the causal relationships are evident in data .",
    "a cdt and a normal decision tree will be different when the causal relationships are subtle or with noises . to demonstrate this point",
    ", we build a cdt and a normal decision tree with a randomised data set where there is no relationship at all .",
    "values in each of 10 attributes were randomly drawn with 50% 1s and 50% 0s in the data set . when we tried to learn a cdt from the data , no tree was returned and this is expected .",
    "however , c4.5 grew a decision tree as in figure  [ fig_randomdatac45 ] .",
    "this result shows that the relationships in a normal decision tree may not be meaningful at all and a more interpretable decision tree , like a cdt , is necessary .            to show that cdt is competent to discover causal relationships , we use 5 groups of synthetic data sets , each group containing 10 data sets with the same number of variables , to compare the findings of cdt and the pc algorithm from the data . in total 50 data sets",
    "are used , and each data set contains 10k samples .",
    "the data sets are generated using the tetrad tool ( http://www.phil.cmu.edu/tetrad/ ) . to create a data set , in tetrad",
    "we firstly generate randomly a causal bayesian network structure with the specified number of variables ( 20 , 40 , 60 , 80 , or 100 ) , and randomly select a node with a specified degree ( i.e. number of parent and children nodes , which is in the range of 3 to 7 ) as the outcome attribute for the data set .",
    "the conditional probability tables of the causal bayesian network are also randomly assigned .",
    "the data set is then generated using the built - in bayes instantiated model ( bayes i m ) based on the conditional probability tables .",
    "the ground truth of the data is the set of nodes directly connected to the outcome variable in the causal bayesian network structure .",
    "we then apply cdt and pc to each of the 50 data sets , and for each group of the data sets , the average recalls of the algorithms are shown in table [ tableavgrecall ] ( part a ) .",
    "it can be seen that in general cdt can detect similar percentages of causal relationships as pc does , indicating that cdt has comparable ability and has obtained consistent results in discovering causal relationships as the commonly used approach .",
    "we are aware that the causal relationships identified by cdt are context specific while those discovered by pc is global or context free .",
    "however , it is reasonable to assume that if a causal relationship exists with no context , it should appear in the contexts too , and these relationships have been mostly picked up by the cdts .",
    "c c c c c c + group & # d & # v & recall ( cdt ) & recall ( pc ) + [ 0ex ] 1 & 10 & 20 & 85% & 75% + 2 & 10 & 40 & 77% & 79% + 3 & 10 & 60 & 89% & 78% + 4 & 10 & 80 & 94% &",
    "90% + 5 & 10 & 100 & 85% & 94% +   + group & # d & # v & recall ( cdt ) & recall ( pc ) + [ 0ex ] 6 & 10 & 20 & 81% & n / a + 7 & 10 & 40 & 77% & n / a + 8 & 10 & 60 & 85% & n / a + 9 & 10 & 80 & 89% & n / a + 10 & 10 & 100 & 78% &",
    "n / a +   +   +    [ tableavgrecall ]      in order to test the performance of cdt in finding context specific causal relationships , we also use 5 groups of synthetic data sets , each group containing 10 data sets with the same number of variables ( 20 , 40 , 60 , 80 or 100 ) .    to create a data set , e.g. with 20 binary variables , @xmath35",
    ", we firstly create a causal bayesian network structure that contains only one edge , e.g. between @xmath36 and @xmath37 , and all other nodes are isolated nodes . based on this structure ,",
    "we use logistic regression to simulate the data set for the bayesian network .",
    "one of the two causally related variables , e.g. @xmath37 is chosen as the outcome variable , then @xmath36 in this example is the ground truth of the global causal node of @xmath37 .",
    "however , we do not know the ground truth of the context specific causal relationships around @xmath37 .",
    "our solution is to use @xmath36 as the context variable , and apply pc - select @xcite ( also known as pc - simple @xcite ) to the two partitions of the data set respectively , one partition containing all the samples with ( @xmath38 ) and one containing all the samples with ( @xmath39 ) ( while the @xmath36 column is excluded ) . in this way",
    ", we identify the variables that are causally related to @xmath37 within each of the two contexts , ( @xmath38 ) and ( @xmath39 ) , and use the findings as the ground truth of the context specific causal relationships around @xmath37 in the data set .",
    "pc - select is a simplified version of the pc algorithm for finding causal relationships around a given outcome variable .",
    "we then apply cdt to each of the 50 data sets generated .",
    "the cdt built from each of the data sets always has the node that is causally related to the output attribute as its root , i.e. the cdt correctly finds the global causal relationship .",
    "moreover , each of the cdts also contains context specific causal relationships .",
    "we did not prune cdt trees in these data sets since some randomly generated data sets have skewed distribution , which makes the pruning too aggressive .",
    "we will design a pruning strategy for skewed data sets in future work .",
    "table [ tableavgrecall ] ( part b ) summarises the average recall of cdt in finding the context specific causal relationships .",
    "from the table , cdt is able to discover the majority of the context specific causal relationships .",
    "pc , in contrast , does not find any context specific causal relationships in the data sets since it is not design for the purpose .",
    "if we want to use pc to find the context specific causal relationships , we have to run pc in each context specific data set , which is impractical . on the other hand",
    ", cdt can find context specific causal relationships in the complete data sets .",
    "we test the scalability of the cdt algorithm by comparing it with the c4.5 @xcite algorithm implemented in weka @xcite and the pc algorithm  @xcite .",
    "we use 12 synthetic data sets generated with the same procedure as for generating the data sets in section [ exp_cdt_causal]-1 ) . to be fair among data sets , we chose the nodes with the same degree as the target variables .",
    "the comparisons were carried out using the same desktop computer ( quad core cpu 3.4 ghz and 16 gb of memory ) .",
    "the comparison results are shown in figure  [ fig_scalability ] .",
    "the run time of cdt is almost linear to the size of the data sets and the number of attributes .",
    "it is less efficient than c4.5 but more efficient than pc .",
    "the results have shown that the proposed cdt is practical for high dimensional and large data sets .",
    "+     [ fig_scalability ]",
    "in this section , we will differentiate our cdts from other causal trees derived from causal bayesian networks , including the conditional probability table tree ( cpt - tree ) @xcite and causal explanation tree @xcite .    a causal bayesian network ( cbn ) @xcite consists of a causal structure of a directed acyclic graph ( dag ) , with nodes and arcs representing random variables and causal relationships between the variables respectively , and a joint probability distribution of the variables .",
    "given the dag of a cbn , the joint probability distribution can be represented by a set of conditional probabilities attached to the corresponding nodes ( given their parents ) .",
    "a cbn provides a graphical visualisation of causal relationships , a reasoning machinery for deriving new knowledge ( effects ) when evidence ( changes of causes ) is fed into the given network ; as well as a mechanism for learning causal relationships in observational data . in recent decades ,",
    "cbns have emerged , especially in the areas of machine learning and data mining , as a core methodology for causal discovery and inference in data .",
    "[ fig_adultbayesinnetwork ]    a cbn depicts the relationships of all attributes under consideration , and it can be complex when the number of attributes is more than just a few .",
    "for example , it takes some effort to understand the cbn in figure  [ fig_adultbayesinnetwork ] learnt from the adult data set , even though there are only 14 attributes in the data set .",
    "a cbn does not give a simple model to explain the causes of an outcome as our cdt does .",
    "the conditional probability table tree ( cpt - tree ) @xcite is designed to summarise the conditional probability tables of a cbn for concise presentation and fast inference .",
    "an example of cpt - trees is shown in figure  [ fig_cpttree ] .",
    "the probabilistic dependence relationships among the outcome @xmath1 and its parent nodes @xmath40 and @xmath41 ( causes of @xmath1 ) are specified by a conditional probability table where the probabilities of @xmath1 given all value assignments of its parents are listed .",
    "the size of a conditional probability table is exponential to the number of parent nodes of @xmath1 and can be very large .",
    "for example , for 20 parent nodes , the conditional probability table will have 1,048,576 rows .",
    "this table will be difficult to display and the inference based on the table is inefficient too . given a context , i.e. one or more parent nodes taking an assignment of a value , the probability of @xmath1 may be constant ( without being affected by the values of other parents ) .",
    "so a conditional probability table can be represented clearly with a tree structure , called a conditional probability table tree ( cpt - tree ) , as illustrated in figure [ fig_cpttree ] . in the cpt - tree ,",
    "the causal semantics is naturally linked to the cbn where all parent nodes are direct causes of variable @xmath1 .    ;",
    "( r ) : cpt - tree.,scaledwidth=45.0% ]    [ fig_cpttree ]    there are two major differences between a cpt - tree and a cdt .",
    "firstly , cpt - trees are built from cbns and cdts are built from data sets directly . before building the cpt - trees , we already know the causal relationships , and a cpt tree specifies how the assignments of some causal variables link to outcome values .",
    "this is impractical in many real world applications since we do not know the cbn or we could not build a cbn from a data set , particularly a large data set , as existing algorithms for learning cbns can not handle a large number of variables and they often only present a partially oriented cbn . secondly , in a cbn",
    ", the parents of a node @xmath1 are all global causes of @xmath1 . as a cpt - tree",
    "is derived from a cbn , all the variables included in a cpt - tree are all global causes . however , as discussed previously , it is possible that under a context , a variable becomes causally related to @xmath1 .",
    "for example , in the titanic data set , ` thirdclass ' cabin is not a causal factor in the whole data set ( i.e. it is not a global cause of ` survived ' ) , but it becomes a causal factor in the context of female passengers / crew .",
    "so such causal relationships will not be discovered or represented by a cbn and thus not by the cpt - trees too , but they can be be revealed and represented by our cdts .    a causal explanation tree @xcite aims at explaining the outcome values using a series of value assignments of a subset of attributes in a cbn .",
    "a series of value assignments of attributes form a path of a causal explanation tree , and a path is determined by a causal information flow .",
    "the assignment of a set of attributes along a path represents an intervention in the causal inference in a cbn .",
    "the causal interpretation is based on the causal information flow criterion used for building a causal explanation tree .",
    "however this method is impractical since we do not have a cbn in most real world applications as explained previously .",
    "similarly a causal explanation tree can not capture the context specific causal relationships encoded in a cdt , because the explanation tree is obtained from a cbn , which only encodes global causes .",
    "causal discovery is based on assumptions . in the causal bayesian network discovery framework ,",
    "some assumptions , such as causal markov condition , faithfulness and causal sufficiency  @xcite , are used to ensure the causal semantics of the discoveries .",
    "simply speaking , markov condition requires that every edge in a causal bayesian network implies a probabilistic dependence .",
    "the faithfulness assumption ensures that for two variables that are probabilistically dependent , there is a corresponding edge between the two variables in the causal bayesian network .",
    "the causal sufficiency assumes that there is no unmeasured or hidden causes in data . up to now , we have not explicitly discussed the causal assumptions .",
    "however , we do need certain assumptions which will be discussed in the following .",
    "the causal interpretation of a cdt is ensured by the evaluation in the stratified data sets of the difference in the potential outcomes of a possible causal attribute @xmath42 . in each stratum , the individuals are indistinguishable , or the attributes possibly affecting the outcome @xmath1 take the same values and they do not affect the estimation of the causal effect of @xmath42 on @xmath1 .",
    "therefore , the causal effect estimated using the stratified data sets approaches the true causal effect .",
    "an assumption here is that the differences of individuals should be captured by the set of attributes used for stratification .",
    "this assumption implies causal sufficiency that all causes are measured and included in the data set .",
    "a nave choice is to select all attributes other than the attribute being tested ( @xmath42 ) and the outcome ( @xmath1 ) , for stratification .",
    "however , this is not workable for high dimensional data sets since many strata will contain very few or no samples when the number of attributes is large . as a result ,",
    "the cdt algorithm may not find any causal relationship .",
    "for example , diverse information , such as demographic information , education , hobbies and liked movies , is collected as personal profile in a data set . however , if all the attributes are used for stratification , they reduce the chance of finding sizable strata for reliable discovery . in fact , it is unwise to use any irrelevant attributes , such as hobbies and liked movies , for stratification when the objective is to study , e.g. the causal effect of a treatment on a disease .",
    "a reasonable and practical choice of stratifying attributes is the set of attributes that may affect the outcome , called relevant attributes in this paper .",
    "differences in irrelevant attributes that do not affect the outcome should not impact the estimation of the causal effect of the studied attribute on the outcome . for example , different hobbies and liked movies may not affect the estimation of the causal effect of a treatment on a disease . therefore , only those relevant attributes should be used for stratifying a data set , and this is what we have used in the algorithm for building a cdt . in case there are many relevant variables , which may result in many small strata , we restrict the maximum number of relevant variables to ten according the strength of correlation .",
    "the purpose of this work is to design a fast algorithm to find causal signals in data automatically without user interactions .",
    "we do tolerate certain false positives and expect that a real causal relationship will be refined by a dedicated follow - up observational study .    in many real world studies ,",
    "the stratification is based on a limited number of demographic attributes , e.g. gender , age group and residential areas .",
    "thinking about a heath study , it is very difficult to recruit volunteers with the same background ( age , diet , education , etc . ) , and stratification on more than a few attributes is just impractical .",
    "considering some stratifying attributes is better than considering none .",
    "cdts help practitioners with the discovery of causal relationships in the following ways although it may not confirm causal relationships : ( 1 ) because of stratification , many spurious relationships that are definitely not causal will be excluded from the resulting cdts , so practitioners will have a smaller set of quality hypotheses for further studying ; ( 2 ) context specific causal relationships are more difficult to be observed than global causal relationships .",
    "cdts are useful for practitioners to find hidden context specific causal hypotheses .",
    "discovering causal relationships in passively observed data has attracted enormous research efforts in the past decades , due to the high cost , low efficiency and unknown feasibility of experiments based approaches , as well as the increasing availability of observational data . to the credit of the theoretical development by a group of statisticians , philosophers and computer scientists , including pearl @xcite , spirtes , glymour @xcite and others , we have seen graphical causal models playing dominant role in causality discovery . among these graphical models , causal bayesian networks ( cbns ) @xcite have been the most developed and used one .",
    "many algorithms have been developed for learning cbns @xcite .",
    "however in general learning a complete cbn is np - hard @xcite and the methods are able to handle a cbn with only tens of variables , or hundreds if the causal relationships are sparse @xcite .",
    "consequently , local causal relationship discovery around a given target ( outcome ) variable has been actively explored recently as in practice we are often more interested in knowing the direct causes or effects of a variable , especially in the early stage of investigations .",
    "the work presented in this paper is along the line of local causal discovery .",
    "existing methods for local causal discovery around a given a target fall into two broad categories : ( 1 ) methods that adapt the algorithms or ideas for learning a complete cbn into local causal discovery , such as pc - simple @xcite , a simplified version of the well - known pc algorithm @xcite for cbn learning ; and hiton - pc @xcite , which applies the basic idea of pc to find variables strongly ( and causally ) related to a given target ; ( 2 ) methods that are designed to exploit the high efficiency of popular data mining approaches and the causal discovery ability of traditional statistical methods , including the work in @xcite and @xcite , both using association rule mining for identifying causal rules ; and the decision tree based approach @xcite for finding the markov blanket of a given variable .",
    "the cdt proposed in this paper belongs to the second category , as it takes advantage of decision tree induction and partial association tests . comparing to other methods in the category",
    ", however , the proposed cdt approach is distinct because it is aimed at finding a sequence of causal factors ( variables along the path from the root to a leaf of a cdt ) where a preceding factor is a context under which the following factors can have impact on the target , while the other methods identify a set of causal factors each being a cause or an effect of the given target , and they only discover global causal relationships .",
    "however , in practice , a variable may not be a cause of another variable globally , but under certain context , it may affect other variables .",
    "a cdt provides a way to identify such context specific causal relationships .",
    "additionally because a context specific causal relationship contains information about the conditions in which a causal relationship holds , such relationships are more prescriptive and actionable and thus are more suitable for decision support and action planning .    in terms of using decision trees as a means for causality investigation , except from the above mentioned method for identifying markov blankets @xcite , most existing work takes decision trees as a tool for causal relationship representation and/or inference , assuming that the causal relationships are known in advance .",
    "examples include the cpt - trees @xcite and causal explanation tree @xcite introduced in the discussions section , which are both derived from a known causal bayesian network . unlike these trees , our cdt is mainly used as a tool for detecting causal relationships in data , without any assumption of known causal relationships .",
    "in this paper , we have proposed causal decision trees ( cdts ) , a novel model for representing and discovering causal relationships in data .",
    "a cdt provides a compact and precise graphical representation of the causal relationships between a set of predicate attributes and an outcome attribute .",
    "the context specific causal relationships represented by a cdt are of great practical use and they are not encoded by existing causal models .",
    "the algorithm developed for constructing a cdt utilises the divide and conquer strategy for building a normal decision tree and thus is fast and scalable to large data sets .",
    "the criterion used for selecting branching attributes of a cdt is based on the well established potential outcome model and partial association tests , ensuring the causal semantics of the tree .    given the increasing availability of big data , we believe that the proposed cdts will be a promising tool for automated discovery of causal relationships in big data , thus to support better decision making and action planning in various areas .      c.  f. aliferis , a.  statnikov , i.  tsamardinos , s.  mani , and x.  d. koutsoukos .",
    "local causal and markov blanket induction for causal discovery and feature selection for classification part i : algorithms and empirical evaluation .",
    ", 11:171234 , 2010 .",
    "l.  frey , d.  fisher , i.  tsamardinos , c.  aliferis , and a.  statnikov .",
    "identifying markov blankets with decision tree induction . in _ data mining , 2003 .",
    "icdm 2003 .",
    "3rd ieee int . conf . on _ ,",
    "pages 5966 , 2003 ."
  ],
  "abstract_text": [
    "<S> uncovering causal relationships in data is a major objective of data analytics . </S>",
    "<S> causal relationships are normally discovered with designed experiments , e.g. randomised controlled trials , which , however are expensive or infeasible to be conducted in many cases . </S>",
    "<S> causal relationships can also be found using some well designed observational studies , but they require domain experts knowledge and the process is normally time consuming . hence there is a need for scalable and automated methods for causal relationship exploration in data . </S>",
    "<S> classification methods are fast and they could be practical substitutes for finding causal signals in data . </S>",
    "<S> however , classification methods are not designed for causal discovery and a classification method may find false causal signals and miss the true ones . in this paper </S>",
    "<S> , we develop a causal decision tree where nodes have causal interpretations . </S>",
    "<S> our method follows a well established causal inference framework and makes use of a classic statistical test . </S>",
    "<S> the method is practical for finding causal signals in large data sets .    </S>",
    "<S> decision tree , causal relationship , potential outcome model , partial association </S>"
  ]
}