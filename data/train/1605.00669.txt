{
  "article_text": [
    "we prove the weak convergence of a system of interacting diffusions to the unique solution of a non - linear stochastic pde on the half - line . in our model",
    "the diffusions are absorbed at the origin and the proportion of absorbed particles influences the diffusion coefficients , which leads to a description of the limiting system as the solution to a stochastic mckean ",
    "vlasov problem .",
    "the motivation for studying the model in this paper is to extend the mathematical framework of @xcite for the pricing of large portfolio credit derivatives to include processes whose dynamics are driven by statistics of the entire population . with more complicated interaction terms ,",
    "the methods in @xcite are no longer tractable and so we require new techniques .",
    "in particular , it is very difficult to analyse the correlation between pairs of particles in our model ( an essential ingredient of @xcite ) and , from a practical perspective , it is desirable to allow the coefficients of the diffusions to be discontinuous , which presents a further complication .",
    "portfolio credit derivatives ( such as the collateralised debt obligation  cdo ) have a pay - off structure which depends on the total notional value of the loss due to default of entities in the portfolio across the lifetime of the product , after a process of partial asset recovery takes place .",
    "we will not explore the financial details of these contracts ( see @xcite ) , but two important effects the modeller must capture are the intensity of defaults and the tendency for defaults to occur simultaneously .",
    "common modelling approaches include copula - based models , in which the joint probability of default over a fixed time period is modelled directly , and reduced - form models , in which the default rates are modelled as correlated stochastic processes .",
    "the model we will consider is a _ structural model _ : default times are represented as the threshold hitting times of a collection of correlated stochastic processes .",
    "these models were introduced in the context of portfolio derivatives by @xcite and @xcite , and their origins trace back to @xcite and @xcite for single - name derivatives .",
    "our general framework is as follows .",
    "suppose we have a collection of @xmath1 defaultable entities and a fixed finite time horizon @xmath2 .",
    "assign the @xmath3  entity a risk process , @xmath4 , called the _ distance - to - default _ process , with @xmath5 chosen to be positive independent random variables supported on @xmath6 with common law @xmath7 .",
    "default of the @xmath3  entity is modelled as the first hitting time of zero of the distance - to - default process : @xmath8 the _ empirical _ and _ loss processes _ then track the spatial evolution of the surviving particles and the proportion of killed particles ; defined respectively as @xmath9 here , @xmath10 denotes the dirac delta measure of the point @xmath11 .",
    "the empirical process takes values in the sub - probability measures on @xmath12  and the loss process takes values in @xmath12 . for @xmath13 , @xmath14 is simply the proportion of the diffusions that take values in @xmath15 at time @xmath16 that have not yet hit the origin by time @xmath16 : @xmath17 hence we have the relationship @xmath18    in practice , once the dynamics of @xmath4 have been specified , the model could be used to generate realisations of @xmath19 from which portfolio credit derivatives ( options on @xmath19 ) could be priced using monte carlo routines .",
    "instead , we will approximate @xmath19 by its limit as @xmath20 .",
    "this is known as a large portfolio approximation , an idea first introduced in @xcite and now found in several modern frameworks for copula - based models @xcite and reduced - form models @xcite .",
    "we will return to the question of how this approximation is generated in practice after a precise description of the limiting objects and mode of convergence .",
    "we will model the processes @xmath21 as correlated diffusions with parameters that are functions of the current proportional loss : @xmath22 here , @xmath23 are independent standard brownian motions and the precise conditions on the coefficients are given in assumption  [ not_def_coefficients ] .",
    "in particular we assume @xmath24 is piecewise lipschitz with finitely many discontinuities in the loss variable @xmath25 .",
    "( it is easy , but perhaps not immediate , to show that this collection of processes exists , see remark [ not_rem_welldefined ] . )    in @xcite this model is analysed for the case when the coefficients are constants and it is shown that the sequence of empirical process , @xmath26 , converges to a stochastic limit which can be characterised as the unique solution to a heat equation with constant coefficients and a random transport term driven by the systemic brownian motion @xmath27 ( * ? ? ?",
    "however , numerical experiments show that the constant coefficient model is too simple to adequately capture the traded prices of cdos across all tranches simultaneously ( * ? ? ?",
    "* sct .  5 ) .",
    "this problem is common for gaussian models  the tails of the risk processes are too light to produce large losses and so a large correlation parameter is required to generate scenarios in which many defaults occur over a given time horizon @xcite .",
    "consequently , different products on the same underlying portfolio may produce different correlation parameters when calibrated to market prices .",
    "this phenomenon is known as _ correlation skew _ ( see figure  [ intro_fig_skew ] ) .",
    "there is a large literature addressing the drawbacks of gaussian credit models .",
    "examples include the addition of jump processes and heavy - tailed distributions @xcite , stochastic parameters and inhomogeneity @xcite and contagion effects @xcite .",
    "close relatives to our framework include @xcite , in which a jump process is added to the systemic factor , but in a discretised version of the system , and @xcite , in which the particles are taken to be general diffusions . in @xcite",
    "the constant coefficient model is studied on the unit interval with absorbing boundaries at 0 and 1 and with an additional multiplicative killing rate as a model for mortgage pools .",
    "implied correlation for each tranche for the data set from ( * ? ? ?",
    "* figure 2 , 7 year maturity ) . with @xmath7 , @xmath28 and @xmath29",
    "fixed in the constant coefficient model , the implied correlation for a given tranche is the value of the correlation parameter required to give a model spread equal to the market spread for that tranche .",
    "this is an example of correlation skew.,width=302 ]    our present approach is inspired by figure  [ intro_fig_skew ] .",
    "suppose @xmath28 and @xmath29 are fixed constants and @xmath24 is only a function of @xmath30 .",
    "if @xmath31 was piecewise constant across intervals corresponding to the cdo tranches in figure  [ intro_fig_skew ] , then an obvious strategy for calibrating @xmath24 to the market prices is to calibrate the first level of @xmath24 to the traded spread of the most junior tranche , fix this value , repeat the calibration procedure for the next most junior tranche spread and continue for all tranches .",
    "it is therefore a natural assumption to allow the diffusion coefficients in ( [ eq : intro_coefficients ] ) to have finitely many discontinuities .",
    "piecewise lipschitz coefficients encompass this class of models whilst giving an analytically tractable system .",
    "the dynamics of an individual distance - to - default process , @xmath4 , are controlled by the population behaviour , hence we have an example of a mckean  vlasov system  see @xcite for an overview .",
    "some applications of these systems include the modelling of large collections of neurons and threshold hitting times for membrane potential levels in mathematical neuroscience @xcite , the modelling of a large number of non - cooperative agents in mean - field games @xcite , filtering theory @xcite and mathematical genetics @xcite .",
    "examples in portfolio credit modelling include @xcite in which systems with contagion effects are analysed under their large population limits .    as @xmath20",
    ", we will find that the influence of the idiosyncratic brownian drivers , @xmath32 , averages - out to a deterministic effect , but that the randomness due to the systemic brownian motion , @xmath27 , remains present .",
    "hence the system must be characterised as the pair @xmath33 , and we will follow an established strategy to demonstrate the convergence in law of this pair and to characterise the limiting law :    a.   prove tightness of @xmath34 ( in a suitable topology ) , b.   characterise the limit points as weak solutions of a non - linear evolution equation , c.   prove uniqueness of solutions for this equation , d.   conclude all limiting laws agree , and hence that we have convergence in law .",
    "the mathematical challenge comes from the interaction of the individuals through the boundary behaviour of the population and the discontinuities in the diffusion coefficients .",
    "a similar model has recently been studied where the particles interact through the quantiles of the empirical measure @xcite , however there is no general uniqueness theory for this problem . for a model without systemic noise",
    "there is a uniqueness theory in @xcite .",
    "discontinuous coefficients have been considered in @xcite , but only on the whole space and in the deterministic setting . in our model , parameter discontinuities are allowed because the limiting realisations of the loss process are strictly increasing ( proposition  [ prob_prop_lossinc ] ) .",
    "this implies the infinite system spends a null set of time at points where the discontinuities in the coefficients prevent the application of the continuous mapping theorem ( corollary  [ tight_cor_fiddly ] ) .",
    "stochastic pdes of mckean  vlasov type are popular tools in the analysis of mean - field games with common noise @xcite . in @xcite a system of diffusions on the half - line is studied in which each particle undergoes a proportional jump towards zero whenever any of the particles hits the absorbing boundary at zero . the purpose of the model is to describe the self - excitatory behaviour of a large collection of neurons . for small values of the feedback",
    "parameter , existence and uniqueness theorems hold for the limiting system .",
    "it is shown in @xcite , however , that for large values of the feedback parameter the limiting system must blow - up ( in the sense that no continuous solutions exist ) and a complete existence and uniqueness theory in this case remains a challenge .",
    "the topology we will use for establishing tightness of the sequence of laws of @xmath34 is the product topology @xmath35 , where @xmath36 is the @xmath37  topological space of distribution - valued cdlg  processes on @xmath38 $ ] , introduced in @xcite , and @xmath39 is the space of real - valued continuous functions on @xmath38 $ ] with the topology of uniform convergence .",
    "( throughout ,",
    "  denotes the space of rapidly decreasing functions and @xmath40  the space of tempered distributions . )",
    "it will not be necessary to explain the full details of the construction of @xmath36 , as the proof theorem  [ intro_thm_exist ] uses only theorem  3.2 and proposition  2.7 of @xcite , together with facts about the classical m1 toplogy on @xmath41 .",
    "the @xmath37  topology is helpful because monotone real - valued processes are automatically tight in @xmath42 , a fact which has been exploited in many other applications ( see @xcite for references ) . in our infinite - dimensional setting ,",
    "the decomposition trick in ( * ? ? ? * prop .",
    "4.2 ) enables us to exploit the monotonicity of the loss process in proving tightness of the empirical process .",
    "tightness on the product space implies the existence of subsequential limit points , whereby we recover :    [ intro_thm_exist ] let @xmath43 realise a limiting law of the sequence @xmath34 .",
    "then @xmath44 is a continuous process taking values in the sub - probability measures and satisfies the regularity conditions of assumption  [ not_def_regcond ] and the _ limit spde _ : @xmath45 for every @xmath46 $ ] and @xmath47 , with probability 1 .",
    "furthermore , if the limit point is attained along the subsequence @xmath48 , then @xmath49 converges in law to @xmath50 on the product space @xmath51 .",
    "heat plot for the solution , @xmath44 , of the limit spde for a fixed sample path of @xmath27 .",
    "time is plotted on the horizontal axis , space on the vertical axis and the value of a pixel represents the ( scaled ) intensity of @xmath44 at that space - time point ( blue for level zero increasing to dark red for maximal value ) .",
    "the initial condition is a step function , @xmath52 , @xmath53 and @xmath24 is given above .",
    "markers are added to show the times at which the loss process , @xmath0 , reaches levels @xmath54 , @xmath55 , @xmath56 and @xmath57 .",
    "notice the corresponding three periods of smooth heat flow between the two periods of highly correlated motion .",
    "( figure produced using the algorithm outlined in section  [ sect_open].),width=415 ]    the limit spde is a non - linear heat equation with stochastic transport term driven by the systemic brownian motion ( see figure  [ intro_fig_lossdepheat ] for an example with an exaggerated correlation change ) , and the space of test functions , @xmath58 , encodes the dirichlet boundary conditions . in the limit , the idiosyncratic noise averages - out to produce the diffusive evolution equation .",
    "the intuition for this effect is explained easily in section  [ sect_dynamics ] , however a full proof of theorem  [ intro_thm_exist ] requires more technical details and is given in section  [ sect_tightness ] .",
    "several estimates involving purely probabilistic arguments are presented in section  [ sect_probestimates ] , where a key result is proposition  [ prob_prop_lossinc ] which shows ( in an asymptotic sense ) that over any non - zero time interval the system must lose a non - zero proportion of mass , and hence any limiting loss process is strictly increasing .    with theorem  [ intro_thm_exist ] established , demonstrating",
    "the full weak convergence of @xmath34 is a matter of proving uniqueness of solutions to the limit spde :    [ intro_thm_unique ] let @xmath7 satisfy assumption  [ not_def_coefficients ] .",
    "suppose that @xmath43 realises a limiting law of @xmath34 and that @xmath59 satisfies assumption  [ not_def_regcond ] .",
    "if @xmath44 and @xmath59 solve the limit spde in theorem  [ intro_thm_exist ] with respect to @xmath27 and @xmath7 , then with probability 1 @xmath60 \\textrm { and borel measurable } s \\subseteq { \\ensuremath{\\mathbb{r}}}.\\ ] ] hence there exists a unique law of a solution to the limit spde on @xmath35 and @xmath34 converges weakly to this law .",
    "furthermore , if @xmath43 realises the unique law , then @xmath61 converges in law to @xmath50 on @xmath51 , where @xmath62 .",
    "[ intro_rem_strongsolutions ] theorem  [ intro_thm_unique ] shows that all weak solutions realise limiting laws , and amongst limiting laws we have pathwise uniqueness . following  ( * ? ? ?",
    "5.3.23 ) , we deduce that strong solutions exist on a sufficiently rich probability space , whereby @xmath44 ( and hence @xmath0 ) is adapted to the filtration generated by @xmath27 .",
    "( density ) in corollary  [ unique_cor_l2reg ] we show that @xmath44 has a density process @xmath63 such that @xmath64 for all @xmath65 and @xmath46 $ ] .",
    "it is then instructive to write the limit spde formally as @xmath66    to prove theorem  [ intro_thm_unique ] ( section  [ sect_uniqueness ] ) we use the kernel smoothing method from @xcite , which is a technique for mollifying potentially exotic solutions to the limit spde in order to work with smooth tractable objects , at the expense of a small approximation error .",
    "the technique was used on the whole space in @xcite . in @xcite",
    "the approximation error is controlled in the space @xmath67 and there the key quantity to control is the second moment of the mass near the origin : @xmath68 , for a candidate solution @xmath44 .",
    "this approach succeeds because the quantity can be written in terms of the law of a two - dimensional brownian motion in a wedge , for which explicit formulae are available . in that case",
    "the kernel smoothing method can be used to give a precise description of the regularity of the solution @xcite .",
    "as the particle interactions in our model are more complicated , however , these explicit formula are no longer available .",
    "although we are able to show that the unique solution to the limit spde has a density in @xmath69 ( corollary  [ unique_cor_l2reg ] ) , which is an auxiliary result towards theorem  [ intro_thm_unique ] , that method can not be used to fully establish uniqueness as it relies on a crude upper bound for @xmath44 which neglects the effect of the absorbing boundary ( remark  [ unique_rem_crude ] ) .",
    "our solution to this problem is to adapt the kernel smoothing method to the dual of the first sobolev space , which then only requires us to control the first moment @xmath70 ( section  [ sect_kernel ] ) .",
    "this is an easier quantity to estimate as only individual particles need to be studied and not pairs of particles , hence we do not need to consider the complicated correlation between particles ( see propositions  [ prob_prop_boundary ] and  [ tight_prop_regcond ] ) .    we must also deal with discontinuities in the coefficients of the limit spde and here the strict monotonicity of the limiting loss processes is again important .",
    "our strategy is to prove uniqueness up to the first time the level of the loss reaches a discontinuity point of the coefficients , whereby continuity allows us to propagate the argument onto the next such time interval . with a strictly increasing loss process and only finitely many discontinuities",
    ", this argument terminates after finitely many iterations , whereby we have uniqueness on the whole time horizon @xmath38 $ ] .",
    "[ ch5_remark_badrho ] we can not choose @xmath24 arbitrarily and expect theorem  [ intro_thm_unique ] to hold . as an example , let @xmath52 , @xmath53 and @xmath71 for @xmath72 , @xmath19 is supported on @xmath73 , hence @xmath74 behaves as the basic constant correlation system with @xmath75 , which we denote @xmath76 .",
    "therefore @xmath77 converges weakly to @xmath76 as @xmath78 , hence there is a distinct limit point for every prime , so weak convergence fails for this example .    in section  [ sect_mv ]",
    "we recast our results as a stochastic mckean ",
    "vlasov problem ( with randomness from @xmath27 ) and this shows that @xmath44 can be written as the conditional law of a single tagged particle :    [ intro_thm_mv ] let @xmath43 be a strong solution to the limit spde ( remark  [ intro_rem_strongsolutions ] ) . for any independent brownian motion , @xmath79 , there exists a continuous real - valued process , @xmath80 , satisfying @xmath81 , \\\\ l_{t } = { \\ensuremath{\\mathbf{p}}}(\\tau \\leq t | w ) .",
    "\\end{cases}\\ ] ] ( here , @xmath82 has law @xmath7 and is independent of all other random variables . ) furthermore , the law of @xmath83 is unique .",
    "returning to the question of applying our model , regarding a portfolio credit derivative as an option on the loss process , @xmath0 , with some payoff function , @xmath84 , the main practical question is how to accurately estimate @xmath85 .",
    "this comes in two parts : we must first generate an approximation to @xmath0 ( through @xmath44 ) to a given level of precision for a fixed brownian trajectory and then we must combine such estimates into a random sample . in section  [ sect_open ]",
    "we give an outline of a discrete - time algorithm for approximating the system and some potential variance reduction techniques .",
    "we leave the tasks of checking the benefits and correctness of these methods as open problems .",
    "a number of potential modifications to the model are also stated , along with their corresponding mathematical challenges .      in section  [ sect_not ] we state the main technical assumptions on the model parameters and",
    "review their purpose . in section  [ sect_dynamics ]",
    "we derive the evolution equation satisfied by the empirical measure of the finite system , which gives a heuristic explanation for arriving at the limit spde in theorem  [ intro_thm_exist ] . in section  [ sect_probestimates ] several probabilistic estimates",
    "are derived for the finite system and these are applied in section  [ sect_tightness ] to give a proof of theorem  [ intro_thm_exist ] . in section  [ sect_kernel ]",
    "we describe the kernel smoothing method , which is the main tool for the proof of theorem  [ intro_thm_unique ] in section  [ sect_uniqueness ] . in section  [ sect_lemmas ] several technical lemmas",
    "are presented which are used to in section  [ sect_uniqueness ] , but which are deferred for readability . in section  [ sect_mv ]",
    "we use our results to give a short proof of theorem  [ intro_thm_mv ] . in section  [ sect_open ]",
    "we outline an algorithm for simulating the solution to the limit spde and discuss open problems relating to this and to potential model extensions .",
    "the purpose of this section is to lay out the technical definitions omitted in the introduction and to explain their purpose .",
    "[ not_def_coefficients ] let @xmath86 \\times { \\ensuremath{\\mathbb{r}}}\\times [ 0,1 ] \\to { \\ensuremath{\\mathbb{r}}}$ ] , @xmath87 \\times { \\ensuremath{\\mathbb{r}}}\\to [ 0,\\infty)$ ] and @xmath88 \\times [ 0,1 ] \\to [ 0,1)$ ] be the coefficients in ( [ eq : intro_coefficients ] ) and @xmath7 be the common law of the initial values of the distance - to - default processes introduced above ( [ intro_eq_tau ] ) .",
    "we assume that we have a sufficient large constant , @xmath89 , such that all the following hold :    a.   ( initial condition ) [ not_def_coefficients_1 ] the probability measure @xmath7 is supported on @xmath6 , has a density @xmath90 and satisfies @xmath91 for every @xmath92 .",
    "( note : @xmath90 implies @xmath93 as @xmath94 . ) b.   [ not_def_coefficients_smooth ] ( spatial regularity ) for all fixed @xmath46 $ ] and @xmath95 $ ] , @xmath96 with @xmath97 for all @xmath46 $ ] , @xmath11 , @xmath98 $ ] and @xmath99 , c.   ( non - degeneracy ) [ not_def_coefficients_3 ] for all @xmath46 $ ] , @xmath11 , @xmath98 $ ] @xmath100 d.   ( piecewise lipschitz in loss ) [ not_def_coefficients_4 ] there exists @xmath101 such that @xmath102 whenever @xmath46 $ ] , @xmath11 and both @xmath103 for some @xmath104 , e.   ( integral constraint ) [ not_def_coefficients_5 ] @xmath105 } \\int^{\\infty}_{0 } |\\partial_{t } \\sigma(s , y)| dy   < \\infty$ ] .",
    "[ not_rem_welldefined ] to see that we can find @xmath106 satisfying ( [ eq : intro_coefficients ] ) notice that initially @xmath107 , so we can find @xmath108 diffusions satisfying ( [ eq : intro_coefficients ] ) up to the first time one of the diffusions hits the origin ( i.e.  with coefficients of the form @xmath109 )  notice that the coefficients are globally lipschitz by ( [ not_def_coefficients_smooth ] ) of assumption  [ not_def_coefficients ] , so standard diffusion theory applies . at this",
    "stopping time @xmath110 , and so the process can be restarted as a diffusion with coefficients @xmath111 .",
    "this gives a solution up to the first time two particles have hit the origin . repeating this argument gives the construction of @xmath21 .",
    "condition  ( [ not_def_coefficients_1 ] ) ensures that limiting realisations of the system satisfy the regularity conditions in assumption  [ not_def_regcond ] , as required for theorem  [ intro_thm_exist ] .",
    "the tail assumption and boundary behaviour of @xmath7 are used in proposition  [ prob_prop_boundary ] and  [ prob_prop_tail ] to show that @xmath74 inherits the corresponding properties at times @xmath112 , and this is transferred to limit points by proposition  [ tight_prop_regcond ] .    the boundedness assumption on the coefficients , given by the case @xmath113 in condition  ( [ not_def_coefficients_smooth ] ) ,",
    "is used many times throughout this paper .",
    "the cases @xmath114 and @xmath115 are used in lemma  [ prob_lem_scale ] and  [ prob_lem_remove ] to relate the law of @xmath116 to that of a standard brownian motion , and in lemma  [ lemmas_lem_switch1storder ] and  [ lemmas_lem_switch2ndorder ] to interchange coefficients and measures in the proof of theorem  [ intro_thm_unique ] .",
    "condition  ( [ not_def_coefficients_3 ] ) implies that there is always a diffusive effect acting on the system , and this ensures that the limiting system does not become degenerate .",
    "if @xmath117 or @xmath118 then the particles are completely dependent and move according to a drift term given by @xmath28 and @xmath27 .",
    "the assumption that @xmath24 is bounded away from 1 is used directly in the proof of theorem  [ intro_thm_unique ] in  ( [ eq : unique_rholess1 ] ) and  ( [ eq : unique_anti5 ] ) .",
    "condition ( [ not_def_coefficients_4 ] ) is the main motivating assumption , which we have discussed at length in section  [ sect_intro ] .",
    "condition ( [ not_def_coefficients_5 ] ) is purely a technical assumption to ensure that the drift coefficient , @xmath119 , in lemma  [ prob_lem_scale ] is uniformly bounded by a deterministic constant .",
    "finally , we will remark on the specific form of @xmath120 and @xmath121 . from ( [ eq : intro_coefficients ] ) we can write the dynamics of a single particle as @xmath122 where @xmath123 is a brownian motion . although the @xmath124 are coupled through @xmath19 , this representation allows us to relate the law of an _ individual _ particle to a standard brownian motion as in lemmas  [ prob_lem_scale ] and  [ prob_lem_remove ] , since @xmath28 is bounded and @xmath29 is independent of @xmath19 .",
    "a second advantage of the taking @xmath29 and @xmath24 in this form is that the pairwise correlation between particles is purely a function of @xmath125 , and so is the same for all pairs .",
    "this is explicitly made use of in the construction of the time - change defined in ( [ eq : def_v ] ) , and there it is again important that the correlation function is bounded strictly away from 1 , so that the system can be compared to a standard multi - dimensional brownian motion",
    ". below are the constraints we place on solutions to the limit spde in theorem  [ intro_thm_unique ] to ensure that we have uniqueness . as theorem",
    "[ intro_thm_exist ] indicates , these conditions are natural in the sense that all limit points of the finite system satisfy them .",
    "[ not_def_regcond ] let @xmath44 be a cdlg  process taking values in the space of sub - probability measures on @xmath12 .",
    "the regularity conditions on @xmath44 are    a.   [ not_def_regcond1 ] ( loss function ) the process defined by @xmath126 is non - decreasing at all times and is strictly increasing when @xmath127 , b.   [ not_def_regcond2 ] ( support ) for every @xmath46 $ ] , @xmath128 is supported on @xmath129 , c.   [ not_def_regcond3 ] ( exponential tails ) for every @xmath92 @xmath130 d.   [ not_def_regcond4 ] ( boundary decay ) there exists @xmath131 such that + @xmath132 e.   [ not_def_regcond5 ] ( spatial concentration ) there exists @xmath133 and @xmath134 such that @xmath135    it is essential that limit points satisfy condition  ( [ not_def_regcond1 ] ) in order to apply the continuous mapping theorem to recover the limit spde for limit points ( corollary  [ tight_cor_fiddly ] ) . there",
    ", strict monotonicity ensures that there are only finitely many @xmath16 such that @xmath136 for some @xmath137 , and hence that this set of times is negligible in the limit .",
    "knowing that @xmath0 is monotone also allows us to split @xmath38 $ ] into consecutive intervals such that in the @xmath138 interval @xmath139 , and this argument is used in the uniqueness proof in section  [ sect_uniqueness ]  ( case 2 ) .",
    "condition  ( [ not_def_regcond2 ] ) is natural since @xmath74 is supported on @xmath129 by construction .",
    "however , it is also convenient to take our test functions , @xmath58 , to be supported on @xmath12 , hence ( [ not_def_regcond2 ] ) is needed to rule out pathological solutions that have support on the negative half - line and that would otherwise break the uniqueness claim .    condition  ( [ not_def_regcond3 ] ) is used several times throughout section  [ sect_lemmas ] to check various integrability requirements .",
    "it is also used in lemma  [ lemmas_lem_lossinh1 ] to relate @xmath44 and @xmath0 via the @xmath140 norm .    condition  ( [ not_def_regcond4 ] ) is the key boundary estimate discussed in section  [ sect_intro ] .",
    "its main use is in lemma  [ unique_lem_boundary ] .",
    "condition  ( [ not_def_regcond5 ] ) guarantees that solutions can not become too concentrated in spatial locations .",
    "this is used to interchange coefficients and measures in lemma  [ lemmas_lem_switch1storder ] and  [ lemmas_lem_switch2ndorder ] .",
    "this section introduces the empirical process approximation to the limit spde from theorem  [ intro_thm_exist ] and explains the intuition behind the convergence of @xmath26 . throughout",
    ", we will drop the dependence of the coefficients on the time , space and loss variables and use the following short - hand when it is safe to do so :    [ finite_rem_shorthand ] for fixed @xmath108 , when there is no confusion , we may use the functional notation @xmath141    [ finite_prop_evolution ] for every @xmath1 , @xmath142}}}$ ] and @xmath143 @xmath144 where we have the _ idiosyncratic driver _ @xmath145    apply it s formula to @xmath146 to obtain @xmath147 if @xmath143 , then @xmath148 substituting this expression into the left - hand side above , summing over @xmath149 and multiplying by @xmath150 gives the result .",
    "we need to ensure that our test functions satisfy @xmath151 so that equation ( [ eq : whyctest ] ) is valid .",
    "since the idiosyncratic noise , @xmath152 , is a sum of martingales with zero covariation , the process converges to zero in the limit as @xmath153 .",
    "this explains why we arrive at the limit spde in theorem  [ intro_thm_exist ] .",
    "[ finite_prop_vanish ] for every @xmath143 @xmath154 } } } } |i^{n}_{t}(\\phi)|^{2 }     = \\vert { \\ensuremath{\\partial_{x}}}\\phi \\vert_{\\infty}^{2 } \\cdot o(n^{-1 } ) ,    \\qquad \\textrm{as } n \\to \\infty.\\ ] ]    since @xmath29 and @xmath155 are bounded , the result follows from doob s martingale inequality and the fact that @xmath156_{t }    = \\frac{1}{n^{2 } } \\sum_{i=1}^{n } \\int_{0}^{t } \\sigma(s , x^{i , n}_{s})^{2}(1 - \\rho(s , l^{n}_{s})^{2 } ) { \\ensuremath{\\partial_{x}}}\\phi ( x^{i , n}_{s})^{2}ds.\\ ] ]      in the proceeding sections it will be useful to work with the process defined by @xmath157 which is a probability - measure valued processes on the whole of @xmath12 .",
    "clearly it is the case that @xmath158}}}\\textrm { and } s \\subseteq { \\ensuremath{\\mathbb{r}}}.\\ ] ]    since @xmath159 is not affected by the absorbing boundary , from the work in proposition  [ finite_prop_evolution ] it follows that @xmath159 satisfies the same evolution equation as @xmath74 , but on the whole space .",
    "this is encoded through the test functions :    [ finite_prop_evonub ] for every @xmath1 , @xmath46 $ ] and @xmath160 @xmath161 where @xmath162",
    "here we collect the main probabilistic estimates used in later proofs . the reader may wish to skip this section and use it only as a reference .",
    "we begin by noting the following simple result , which is just a consequence of the fact that @xmath163 are identically distributed : for any measurable @xmath13 , @xmath1 and @xmath46 $ ] @xmath164       = { \\ensuremath{\\mathbf{p}}}(x^{1,n}_{t } \\in s ; t < \\tau^{1,n}).\\ ] ] under ",
    ", @xmath165 is a diffusion and with lemmas  [ prob_lem_scale ] and  [ prob_lem_remove ] we are able to estimate ( [ eq : prob_linear ] ) for relevant choices of @xmath15 by relating the law of @xmath165 to that of standard brownian motion .",
    "specifically , in corollary  [ prob_cor_spatialconc ] and propositions  [ prob_prop_boundary ] and  [ prob_prop_tail ] we show that @xmath74 satisfies the corresponding estimates to those in assumption  [ not_def_regcond ]  ( [ not_def_regcond3 ] ) ,  ( [ not_def_regcond4 ] ) and  ( [ not_def_regcond5 ] ) , which is of direct use in proposition  [ tight_prop_regcond ] when we take a limit as @xmath20 . in propositions  [ prob_prop_lossinc ] and  [ prob_prop_lowerloss ] we prove two estimates for which ( [ eq : prob_linear ] ) is not helpful .",
    "these results require us to express the quantities of interest in terms of independent particles to show that certain events concerning the increments in the loss process are asymptotically negligible .",
    "[ prob_lem_scale ] define @xmath166 \\times { \\ensuremath{\\mathbb{r}}}\\to { \\ensuremath{\\mathbb{r}}}$ ] by @xmath167 and @xmath168 .",
    "then @xmath169 and @xmath170 where @xmath171 is the brownian motion @xmath172 and the drift coefficient , @xmath119 , is given by @xmath173 which is uniformly bounded ( in @xmath108 and @xmath16 ) .",
    "straightforward application of it s formula coupled with assumption  [ not_def_coefficients ] ) .",
    "[ prob_lem_remove ] for every @xmath174 , there exists @xmath175 such that @xmath176 where @xmath177 is the marginal law of a killed brownian motion at time @xmath16 with initial distribution @xmath178 and @xmath179 is as defined in lemma  [ prob_lem_scale ] .",
    "likewise , if @xmath180 is the marginal law of the brownian motion without killing at the origin and with the same initial distribution @xmath181    let @xmath182 be as in lemma  [ prob_lem_scale ] , then @xmath183 is also the first hitting time , @xmath184 , of 0 by @xmath182 so @xmath185 apply girsanov s theorem with the change of measure @xmath186 then under @xmath187 , @xmath182 is a standard brownian motion with @xmath188 , and , for any @xmath189 and @xmath190 , hlder s inequality gives @xmath191     \\leq \\mathbf{e_{q } } [ \\xi_{t}^{-p}]^{\\frac{1}{p } } \\mathbf{q}(e)^{\\frac{1}{q } } \\\\    =   \\mathbf{e_{p}}[\\xi_{t}^{1-p}]^{\\frac{1}{p } } \\mathbf{q}(e)^{\\frac{1}{q } }      =   \\exp\\bigl\\ { c_{p}\\int_{0}^{t } d_{s}^{2 } ds\\bigr\\ } \\mathbf{q}(e)^{\\frac{1}{q } }       \\leq c_{q } \\mathbf{q}(e)^{\\frac{1}{q } } , \\end{gathered}\\ ] ] for some constant @xmath192 as @xmath119 is uniformly bounded .",
    "applying this bound to ( [ eq : prob_calc1a ] ) gives @xmath193 the result is then complete by taking @xmath194 .",
    "the case involving @xmath180 follows by dropping the dependence on @xmath195 .",
    "the following result is a simple consequence of lemma  [ prob_lem_remove ] and controls the expected mass concentrated in an interval .",
    "[ prob_cor_spatialconc ] for every @xmath174 there exists @xmath175 such that @xmath196 for all @xmath197 and @xmath1 .",
    "notice that @xmath198 $ ] , so with @xmath180 as in lemma  [ prob_lem_remove ] @xmath199 and then the result is immediate from lemma  [ prob_lem_remove ] since @xmath200 is integrable at the origin .",
    "a sharper application of lemma  [ prob_lem_remove ] gives control of the concentration of mass near the origin .",
    "notice the stronger rate of convergence due to the absorption at the boundary :    [ prob_prop_boundary ] there exists @xmath131 and @xmath174 such that as @xmath94 @xmath201 where the @xmath202 s are uniform in @xmath46 $ ] and @xmath1 .",
    "let @xmath203 be as in lemma  [ prob_lem_remove ] .",
    "the heat kernel for a brownian motion absorbed at the origin is @xmath204 , \\label{eq : hkabsorbing}\\ ] ] for @xmath205 . by using the bounds @xmath206 and @xmath207 which follows from the simple estimate @xmath208 , for an arbitrary function @xmath209 we have , writing @xmath210 , that @xmath211 where @xmath212 is a numerical constant . by assumption  [ not_def_coefficients](i )",
    "we have a constant @xmath213 such that @xmath214 since the function @xmath215 is maximised at @xmath216 , we have the bound @xmath217 taking @xmath218 gives @xmath219 since @xmath220 as @xmath221 ( recall assumption  [ not_def_coefficients]([not_def_coefficients_1 ] ) ) .",
    "the result is complete by applying lemma  [ prob_lem_remove ] and noting that @xmath222 \\subseteq [ 0,c{\\ensuremath{\\varepsilon}}]$ ] .",
    "a similar analysis applies for the decay of the mass that escapes to infinity .",
    "[ prob_prop_tail ] for every @xmath92 , as @xmath223 @xmath224.\\ ] ]    working with @xmath180 from lemma  [ prob_lem_remove ] and splitting the range of integration at @xmath225 gives @xmath226 where @xmath227 . by the conditions of assumption  [ not_def_coefficients ] , @xmath228 , so @xmath229 as @xmath230 , for every @xmath92 .",
    "the result follows since @xmath231 is uniformly bounded for @xmath232 , and using lemma  [ prob_lem_remove ] with the fact that @xmath233 .",
    "so far the probabilistic estimates we have seen are consequences of the behaviour of the first moment of the diffusion processes .",
    "the next two estimates require knowledge of the correlation between particles and so are harder to prove .",
    "heuristically , the first result shows that over any non - zero time interval a non - zero proportion of particles hit the absorbing boundary . later in proposition  [ tight_prop_regcond ] this result will directly imply that limiting loss functions are strictly increasing whenever there is a non - zero proportion of mass remaining in the system .",
    "[ prob_prop_lossinc ] for all @xmath234 , @xmath235 ( such that @xmath236 $ ] ) and @xmath237 @xmath238    begin by noticing that , for any @xmath239 , if @xmath240 and @xmath241 , then @xmath242 . by applying markov s inequality and proposition  [ prob_prop_tail ] we get the bound @xmath243 therefore fix @xmath244 , for @xmath245 , to arrive at @xmath246    we now concentrate on the first term in the right - hand side above with @xmath108 , @xmath16 and @xmath247 fixed .",
    "let @xmath248 denote the random set of indices @xmath249 if @xmath250 , then @xmath251 , so by conditioning on @xmath248 ( which is @xmath252-measurable ) @xmath253 and @xmath254    to estimate the right - hand side of ( [ eq : prob_lossinc2 ] ) take @xmath179 as in lemma  [ prob_lem_scale ] and define @xmath255 for @xmath256 . by assumption  [ not_def_coefficients",
    "] , there exists a constant @xmath212 such that @xmath257 for all @xmath16 . returning to ( [ eq : prob_lossinc2 ] ) , since @xmath258 if and only if @xmath259 , we have @xmath260 from the bound @xmath261 , for @xmath262 , where @xmath263 we obtain @xmath264 from assumption  [ not_def_coefficients ] @xmath265 , so we have @xmath213 such that @xmath266    our next step is to remove the dependence on the process @xmath267 in ( [ eq : prob_lossinc3 ] ) . to do this we split the probability on the event @xmath268 to get @xmath269 since @xmath267 is a martingale ,",
    "this final probability is @xmath270 as @xmath271 , by doob s maximal inequality .",
    "we have reduced the problem far enough to apply a time - change in order to extract the independence between the particles . to this end ,",
    "conditioned on the event @xmath272 , define @xmath273 then @xmath171 , where @xmath274 , is an @xmath275-valued standard brownian motion , therefore @xmath276 } b^{i}_{u } \\leq -2c_{2 } a - c_{2 }   \\ } < n\\delta | { \\ensuremath{\\mathcal{i}}}= { \\ensuremath{\\mathcal{i}}}_{0 } )      + o(1).\\end{gathered}\\ ] ] by assumption  [ not_def_coefficients ] , @xmath277 , hence @xmath278 } b^{i}_{u } \\leq -2c_{2 } a - c_{2 }   \\ } < n\\delta | { \\ensuremath{\\mathcal{i}}}= { \\ensuremath{\\mathcal{i}}}_{0 } )      + o(1 )    \\\\      & \\leq { \\ensuremath{\\mathbf{p}}}(\\#\\ { i \\in { \\ensuremath{\\mathcal{i}}}_{0 } :   b^{i}_{h / c_{3 } } \\leq -2c_{2 } a - c_{2 }   \\ } < n\\delta | { \\ensuremath{\\mathcal{i}}}= { \\ensuremath{\\mathcal{i}}}_{0 } )      + o(1 )   \\\\      & \\leq { \\ensuremath{\\mathbf{p}}}\\big(\\frac{1}{n } \\sum_{i \\in { \\ensuremath{\\mathcal{i}}}_{0 } } { \\ensuremath{\\mathbf{1}}}_{\\xi^{i } \\leq -c_{4 } ( a + 1 ) } < \\delta\\big )      + o(1),\\end{aligned}\\ ] ] where @xmath279 is a collection of i.i.d .",
    "standard normal random variables and @xmath280 are further numerical constants . by symmetry , this final probability depends only on @xmath281 , hence @xmath282    returning to ( [ eq : prob_lossinc1 ] ) we now have @xmath283 so the law of large numbers gives @xmath284 where @xmath285 and where we have substituted back into ( [ eq : prob_lossinc0 ] ) .",
    "this inequality holds for all @xmath247 and @xmath286 , with the @xmath270 term denoting convergence as @xmath271 .",
    "we now choose the free parameter @xmath247 to be a function of @xmath286 , specifically @xmath287 this guarantees that @xmath288 as @xmath289 , but also @xmath290 as @xmath289 , where we have used the well - known gaussian estimate @xmath291 , for @xmath292 and @xmath293 the c.d.f .  and",
    "p.d.f .  of the standard normal distribution . using this choice of @xmath294 in ( [ eq : prob_lossinc4 ] ) completes the result .",
    "the following is a partial converse of the previous result in that it shows that the system can not lose a large amount of mass in a short period of time .",
    "it will be used in proposition  [ tight_prop_tight ] to verify a sufficient condition for the tightness of @xmath34 .",
    "[ prob_prop_lowerloss ] for every @xmath46 $ ] and @xmath295 @xmath296    with @xmath297 fixed , we have @xmath298 where the second line uses markov s inequality and ( [ eq : prob_linear ] ) and the third line uses proposition  [ prob_prop_boundary ] for @xmath299 and assumption  [ not_def_coefficients ] ( [ not_def_coefficients_1 ] ) for @xmath300 . define @xmath248 to be the random set of indices @xmath301 then conditioning on @xmath248 gives @xmath302 the conditional expectation in the summand can be bounded by @xmath303 } x^{i , n}_{s } \\leq 0 \\ } \\geq \\frac{n\\eta}{2 }   | { \\ensuremath{\\mathcal{i}}}= { \\ensuremath{\\mathcal{i}}}_{0 } )    \\\\      & \\leq { \\ensuremath{\\mathbf{p}}}(\\#\\{i \\in { \\ensuremath{\\mathcal{i}}}_{0 } : \\inf_{s \\in [ t , t+ \\delta ] } ( x^{i , n}_{s } - x^{i , n}_{t } ) \\leq -{\\ensuremath{\\varepsilon}}\\ } \\geq \\frac{n\\eta}{2 }   | { \\ensuremath{\\mathcal{i}}}= { \\ensuremath{\\mathcal{i}}}_{0}).\\end{aligned}\\ ] ]    with @xmath16 fixed , define the process @xmath304 , then @xmath305 } u^{i}_{s } \\leq -c_{5}{\\ensuremath{\\varepsilon}}\\ } \\geq \\frac{n\\eta}{2 }   | { \\ensuremath{\\mathcal{i}}}= { \\ensuremath{\\mathcal{i}}}_{0})\\ ] ] for @xmath306 a numerical constant . as for @xmath182 in lemma  [ prob_lem_scale ]",
    ", we have @xmath307 where @xmath308 is uniformly bounded by assumption  [ not_def_coefficients ] , therefore we can find @xmath309 such that @xmath310 } j^{i}_{s } \\leq -c_{6}({\\ensuremath{\\varepsilon}}-\\delta - a ) \\ } \\geq \\frac{n\\eta}{2 }   | { \\ensuremath{\\mathcal{i}}}= { \\ensuremath{\\mathcal{i}}}_{0 } )   \\\\      & \\qquad + { \\ensuremath{\\mathbf{p}}}(\\sup_{s \\in [ 0,\\delta]}|i_{s}| \\geq a | { \\ensuremath{\\mathcal{i}}}= { \\ensuremath{\\mathcal{i}}}_{0 } ) .\\end{aligned}\\ ] ] by applying the time - change argument from ( [ eq : def_v ] ) and using markov and doob s maximal inequality we have @xmath311 } b^{i}_{s } \\leq -c_{7}({\\ensuremath{\\varepsilon}}-\\delta - a ) \\ } \\geq \\frac{n\\eta}{2 } ) + o(\\delta a^{-2}),\\end{gathered}\\ ] ] where @xmath123 are independent standard brownian motions , @xmath312 and @xmath313 is a numerical constant .    returning to ( [ eq : lossloweriii ] ) and noticing the the right - hand side above is maximised when @xmath314 @xmath315 } b^{i}_{s } \\leq -c_{7}({\\ensuremath{\\varepsilon}}-\\delta - a ) \\ } } \\geq \\eta / 2 \\big )       + o(\\delta a^{-2}).\\end{gathered}\\ ] ] the law of large numbers and the distribution of the minimum of brownian motion gives @xmath316 provided @xmath317 , where @xmath292 is the normal c.d.f .",
    "we now make the choice @xmath318 which guarantees @xmath319 as @xmath289 .",
    "hence the result follows from ( [ eq : lossloweri ] ) , ( [ eq : lossloweriii ] ) and ( [ eq : lowerlossiv ] ) .",
    "we will now use the results from section  [ sect_probestimates ] to prove theorem  [ intro_thm_exist ] , which follows directly from the combination of propositions  [ tight_prop_convloss ] ,  [ tight_prop_regcond ] and  [ tight_prop_evoeqn ] .",
    "we first establish tightness of the sequence of the laws of @xmath34 ( proposition  [ tight_prop_tight ] ) using the framework of @xcite .",
    "the reader is referred to that article for the technical definitions of the topological spaces used in this section .",
    "once we have tightness we can then extract limit points of the sequence @xmath34 , and propositions  [ tight_prop_measure ] ,  [ tight_prop_convloss ] and  [ tight_prop_regcond ] are devoted to recovering the properties of the limiting laws from the probabilistic properties of the finite system .",
    "finally , the limit points are shown to satisfy the evolution equation in theorem  [ intro_thm_exist ] via a martingale argument ( proposition  [ tight_prop_evoeqn ] ) and care needs to be taken over the discontinuities in the coefficients of the limit spde ( corollary  [ tight_cor_fiddly ] ) .",
    "[ tight_prop_tight ] the sequence @xmath26 is tight on the space @xmath36 , hence @xmath34 is tight on the space @xmath320 , where @xmath321 is the space of real - valued continuous paths with the topology of uniform convergence .",
    "we note that a version of this result is given in ( * ? ? ?",
    "4.3 ) for the case @xmath52 , @xmath53 .",
    "the second statement follows from the first and the fact that joint tightness is implied by marginal tightness .    by (",
    "3.2 ) it suffices to show that @xmath322 is tight on @xmath42 for every @xmath160 . to prove this we verify the conditions of ( *",
    "12.12.2 ) , the first of which is trivial because @xmath74 is a sub - probability measure so @xmath323 .",
    "hence we concentrate on condition ( ii ) , which is implied by ( * ? ? ?",
    "4.1 ) , therefore we are done if we can find @xmath324 such that @xmath325 for all @xmath1 , @xmath295 and @xmath326 , where @xmath327 and if latexmath:[\\[\\label{eq : tight2 } \\lim_{n \\to \\infty } { \\ensuremath{\\mathbf{p}}}(\\sup_{t \\in ( 0 , \\delta ) } |\\nu^{n}_{t}(\\phi ) - \\nu^{n}_{0}(\\phi)| + \\sup_{t \\in ( t - \\delta , t ) }     @xmath295 .",
    "with @xmath159 as defined in ( [ eq : finite_nub ] ) , the decomposition in ( * ? ? ?",
    "4.2 ) and markov s inequality give @xmath329   \\\\      & \\leq 8 \\eta^{-4 } ( { \\ensuremath{\\mathbf{e}}}|{\\ensuremath{\\bar{\\nu}}}^{n}_{t_{1}}(\\phi)-{\\ensuremath{\\bar{\\nu}}}^{n}_{t_{2}}(\\phi)|^{4 } +       { \\ensuremath{\\mathbf{e}}}|{\\ensuremath{\\bar{\\nu}}}^{n}_{t_{2}}(\\phi)-{\\ensuremath{\\bar{\\nu}}}^{n}_{t_{3}}(\\phi)|^{4}).\\end{aligned}\\ ] ] for any @xmath330 $ ] , from hlder s inequality we obtain @xmath331 where @xmath332 is the lipschitz constant of @xmath293 . by assumption  [ not_def_coefficients ] and the burkholder  davis",
    " gundy inequality ( * ? ? ? * thm .",
    "iv.42.1 ) , the final expectation above is @xmath333 uniformly in @xmath108 .",
    "therefore we have ( [ eq : tight1 ] ) with @xmath334 and @xmath335 .",
    "now consider the first supremum in ( [ eq : tight2 ] ) . by again using the decomposition from (",
    "4.2 ) , that is @xmath336 , we have @xmath337 the first term on the right - hand side vanishes as @xmath289 by the same work as for ( [ eq : tight1 ] ) and the second term vanishes by proposition  [ prob_prop_lowerloss ] . therefore @xmath338 and likewise for @xmath339 , so we have ( [ eq : tight2 ] ) , which completes the proof .      tightness of @xmath34 ensures that the sequence is relatively compact ( * ? ? ? * thm .",
    "3.2 ) , hence every subsequence of @xmath34 has a further subsequence which converges in law . to avoid possible confusion about multiple distinct limit points",
    ", we will denote by @xmath340 any pair of processes that realises one of these limiting laws . using @xmath341 to denote convergence in law , we have @xmath342 as @xmath343 , for some subsequence @xmath344 . establishing full weak convergence",
    "is equivalent to showing that there is exactly one limiting law .",
    "so far we have that any limiting empirical process , @xmath345 , is an element of @xmath346 .",
    "the following result recovers @xmath345 as a probability - measure - valued process :    [ tight_prop_measure ] let @xmath340 realise a limiting law . then @xmath347 is a sub - probability measure supported on @xmath129 for every @xmath46 $ ] , with probability 1 .",
    "technically , what we will show is that , for every @xmath16 , @xmath347 agrees with a sub - probability measure on @xmath348 and from now on we associate @xmath347 with this measure .",
    "take @xmath349 .",
    "fix @xmath160 , then by ( * ? ? ?",
    "* prop .  2.7 ( i ) ) @xmath350 on @xmath42 .",
    "lemma  13.4.1 of @xcite gives @xmath351 } |\\nu^{n_{k}}_{t}(\\phi)|      \\rightarrow      \\sup_{t \\in [ 0,t ] } |\\nu^{*}_{t}(\\phi)| ,      \\qquad \\textrm{on } { \\ensuremath{\\mathbb{r}}},\\ ] ] therefore the portmanteau theorem  ( * ? ? ?",
    "2.1 ) gives @xmath352 } |\\nu^{*}_{t}(\\phi)| > \\vert \\phi \\vert_{\\infty } )      \\leq \\liminf_{k \\to \\infty } { \\ensuremath{\\mathbf{p}}}(\\sup_{t \\in [ 0,t ] } |\\nu^{n_{k}}_{t}(\\phi)| > \\vert\\phi \\vert_{\\infty } )      = 0,\\ ] ] with the final equality due to @xmath353 being a sub - probability measure .",
    "( the supremum over @xmath16 ensures that the following argument holds for all @xmath16 simultaneously . ) by a similar analysis we have that @xmath354 is non - negative when @xmath293 is non - negative and @xmath355 when @xmath293 is supported on @xmath356 .",
    "hence , @xmath347 is a positive linear functional on  , so extends to a positive linear functional , @xmath357 , on the space , @xmath358 , of continuous and compactly support function on @xmath12  with the uniform topology . the riesz representation theorem  ( * ? ? ?",
    ".  2.14 ) then implies that , for every @xmath16 , there exists a regular borel measure , @xmath359 , such that @xmath360 associating @xmath179 and @xmath345 gives the result .",
    "now that it is safe to regard a limit point , @xmath361 , as taking values in the sub - probability measures , it makes sense to introduce the _ limit loss process _ as @xmath362 .",
    "of course we would like to know that @xmath363 on @xmath42 , however the function @xmath364 is not an element of ",
    ", so ( * ? ? ?",
    "2.7 ) does not allow us to deduce this fact from the continuous mapping theorem . to remedy this we must work slightly harder :    [ tight_prop_convloss ]",
    "suppose that @xmath48 converges weakly to @xmath340 and that @xmath365",
    ". then @xmath49 converges weakly to @xmath366 on @xmath51 .    for a contradiction suppose that the weak convergence does not hold .",
    "since @xmath367 is increasing , @xmath368 $ ] and we have proposition  [ prob_prop_lowerloss ] , the conditions of ( * ? ? ?",
    "12.12.2 ) are satisfied and so @xmath369 is tight on @xmath42 , and because marginal tightness implies joint tightness , @xmath61 is also tight . by taking a further subsequence",
    "if needed , assume that @xmath370 for some @xmath371 .",
    "notice from ( * ? ? ?",
    "12.4.1 ) that the canonical time projection from @xmath42 to @xmath12  is only continuous at times for which its argument does not jump .",
    "that is , for every @xmath16 , @xmath372 is continuous at @xmath373 if and only if @xmath374 . to this end , define @xmath375 : { \\ensuremath{\\mathbf{p}}}(l^{\\dagger}_{s- } = l^{\\dagger}_{s } ) = 1\\}$ ] , which we know by (",
    "13 ) is cocountable in @xmath38 $ ] . for @xmath376 define @xmath377 to be any function satisfying @xmath378 on @xmath379 $ ] , @xmath380 on @xmath381 and @xmath382 otherwise . by ( * ? ? ?",
    "2.7(i ) ) @xmath383 , and define @xmath384 :   { \\ensuremath{\\mathbf{p}}}(\\nu^{*}_{s-}(\\phi_{\\lambda } ) = \\nu^{*}_{s}(\\phi_{\\lambda } ) ) = 1\\}$ ] .",
    "take @xmath385 which is cocountable ( since it is the countable intersection of cocountable sets ) and so is dense in @xmath38 $ ] .",
    "since @xmath386 and @xmath387 is dense in @xmath38 $ ] , if @xmath366 and @xmath388 are not equal in law on @xmath42 , then it must be the case that not all of the finite - dimensional marginals of @xmath389 and @xmath390 on @xmath387 are equal in law .",
    "it is no loss of generality to assume that there exists @xmath297 , @xmath391 , @xmath392 bounded and lipschitz and @xmath393 such that @xmath394 by proposition  [ prob_prop_tail ] @xmath395 as @xmath230 , therefore the lipschitz property of @xmath396 gives @xmath397 but @xmath398 , so @xmath399 since @xmath347 is a probability measure @xmath400 ( recall from proposition  [ tight_prop_measure ] that @xmath347 is supported on @xmath129 ) , so taking @xmath230 gives the required contradiction .",
    "we are now in a position to verify the first half of theorem  [ intro_thm_exist ] , which is that any limit point must satisfy the regularity conditions from assumption  [ not_def_regcond ] :    [ tight_prop_regcond ] if @xmath340 realises a limiting law of @xmath34 , then @xmath345 satisfies assumption  [ not_def_regcond ] .",
    "firstly , @xmath345 takes values in the sub - probability measures by proposition  [ tight_prop_measure ] , and that result also gives assumption  [ not_def_regcond ]  ( [ not_def_regcond2 ] ) .    for conditions  ( [ not_def_regcond4 ] ) and  ( [ not_def_regcond5 ] ) of assumption  [ not_def_regcond ] , let @xmath401 be any finite open interval . for @xmath134 , take any @xmath402 satisfying @xmath403 on @xmath267 , @xmath404 on @xmath405 and @xmath406 otherwise .",
    "taking @xmath349 and noting that @xmath407 in @xmath12   by ( * ? ? ?",
    "11.5.1 ) and that these integrals are uniformly bounded ( by @xmath408 ) , we have @xmath409 for both conditions  ( [ not_def_regcond4 ] ) and  ( [ not_def_regcond5 ] ) we have bounds on the right - hand side which are independent of @xmath410 ( propositions  [ prob_cor_spatialconc ] and  [ prob_prop_boundary ] ) , and then the conditions hold by sending @xmath289 . for condition  ( [ not_def_regcond3 ] )",
    "we have @xmath411 , so @xmath412 .",
    "however , for @xmath413 with @xmath295 , the above work gives @xmath414 so sending @xmath289 and @xmath415 ( using the dominated convergence theorem ) gives the result .",
    "it remains to show  ( [ not_def_regcond1 ] ) of assumption  [ not_def_regcond ] .",
    "first we prove that @xmath389 is non - decreasing . by (",
    "13 ) there is a ( deterministic ) cocountable set , @xmath387 , on which @xmath416 in @xmath417 .",
    "so for @xmath418 in @xmath387 ( * ? ? ?",
    "2.1 ) implies @xmath419 and hence @xmath389 is non - decreasing on @xmath387 .",
    "but @xmath387 is dense in @xmath38 $ ] and @xmath389 cdlg , so we conclude @xmath389 is non - decreasing on @xmath38 $ ] . to deduce the strict monotonicity , proposition  [ prob_prop_lossinc ]",
    "implies @xmath420 whenever @xmath237 and sending @xmath421 gives the required result .",
    "so far we have seen no reason why it is important @xmath389 should be strictly increasing whenever the mass in the system is not completely depleted ( @xmath422 ) .",
    "the following result is such an example and shows why this condition is needed to pass to a weak limit .",
    "the result will be applied directly in the next subsection .",
    "[ tight_cor_fiddly ] fix @xmath46 $ ] and @xmath160 .",
    "let @xmath423 be equal to either @xmath424 , @xmath425 or @xmath426 .",
    "define @xmath427 to be all elements in @xmath346 that take values in the sub - probability measures and let @xmath428}\\subseteq d_{{\\ensuremath{\\mathbb{r}}}}$ ] .",
    "then the map @xmath429 is continuous ( with respect to the product topology on @xmath430 } , { \\ensuremath{\\mathrm{m1}}})$ ] ) at all point @xmath431 which satisfy the conditions of assumption  [ not_def_regcond ] .",
    "consequently , if @xmath349 then @xmath432    for short - hand we will denote this map @xmath433 .",
    "suppose that @xmath434 in @xmath435 , then @xmath436 we will control @xmath267 and @xmath437 separately .",
    "begin by fixing @xmath297 and @xmath134 .",
    "take @xmath438 sufficiently large so that @xmath439 for all @xmath440 $ ] , @xmath441 $ ] and @xmath95 $ ] , which is possible because @xmath442 is bounded and @xmath293 is rapidly decreasing .",
    "let @xmath443 be a mollifier and set @xmath444 , then we have @xmath445 since @xmath446 and @xmath160 , @xmath447 so the first term vanishes as @xmath448 .",
    "we can then split the second term as @xmath449 } |g^{{\\ensuremath{\\varepsilon}}}(s , x,\\ell_{s } ) - g(s , x,\\ell_{s})| ds \\\\ & \\qquad + 2c \\int^{t}_{0 } \\sup_{x \\in { \\ensuremath{\\mathbb{r}}}\\smallsetminus [ -2k,2k ] } |\\phi(x)| ds,\\end{aligned}\\ ] ] and here the first term vanishes as @xmath94 by ( * ? ? ?",
    "c , thm .  6 ) since @xmath450 $ ] is compact and the second term can be guaranteed to be less than @xmath451 for @xmath452 sufficiently large .",
    "taking @xmath289 gives @xmath453 .    to deal with @xmath437 in ( [ eq : fiddlycor1 ] ) , first notice that since @xmath454 @xmath455 define @xmath456 : \\ell_{s } = \\theta_{i } \\textrm { for some } i \\in \\{0,1,\\dots , k\\ } \\}$ ] , where we recall assumption  [ not_def_coefficients ] condition  [ not_def_coefficients_5 ] .",
    "for @xmath134 , let @xmath457 : \\min_{0 \\leq i \\leq k}|\\theta_{i } - \\ell_s| < \\delta\\}$ ] .",
    "define @xmath458 to be all @xmath459 $ ] such that @xmath460 , which we know is a cocountable set  ( * ? ? ?",
    "12.2.1 ) . for @xmath461 , @xmath462 in @xmath12",
    ", so if @xmath463 then eventually @xmath464 for some @xmath104 , whence @xmath465 by assumption  [ not_def_coefficients ] condition  ( [ not_def_coefficients_4 ] ) .",
    "we conclude @xmath466 \\smallsetminus \\mathbb{t}_{1 } ) \\cup   \\mathbb{t}_{0}^{\\delta } } ds      \\leq c_{1 } k \\delta ,      \\qquad \\textrm{for every } \\delta > 0,\\ ] ] where @xmath212 is a numerical constant due to assumption  [ not_def_coefficients ] .",
    "this completes the result .",
    "we complete this section and the proof of theorem  [ intro_thm_exist ] by showing that the limit spde holds for a general limit point . for this we will use a martingale argument and we introduce three processes :    [ tight_def_mgcomp ] for a fixed test function @xmath143 , define the maps :    a.   @xmath467 } \\to d_{{\\ensuremath{\\mathbb{r}}}}$ ] , @xmath468 b.   @xmath469 } \\to d_{{\\ensuremath{\\mathbb{r}}}}$ ] , @xmath470 c.   @xmath471 , @xmath472    these processes capture the dynamics of the limit spde :    [ tight_lem_mgapp ] let @xmath27 be a standard brownian motion and let @xmath473 and @xmath474 be random processes satisfying the conditions of assumption  [ not_def_regcond ] . if @xmath475 are martingales for every @xmath143 , then @xmath473 , @xmath0 and @xmath27 satisfy the limit spde from theorem  [ intro_thm_exist ] .",
    "the hypothesis gives @xmath476_{t } = \\int_{0}^{t } \\xi_{s}(\\sigma(s , \\cdot , l_{s } ) \\rho(s , l_{s } ) { \\ensuremath{\\partial_{x}}}\\phi)^{2 } ds ,    \\\\",
    "[ m^{\\phi}(\\xi , l),w]_{t } = \\int^{t}_{0 } \\xi_{s}(\\sigma(s , \\cdot , l_{s } ) \\rho(s , l_{s}){\\ensuremath{\\partial_{x}}}\\phi)ds,\\end{gathered}\\ ] ] hence @xmath477_{t } = 0,\\ ] ] for every @xmath142}}}$ ] , which completes the proof .",
    "our strategy is to take a limit in proposition  [ finite_prop_evolution ] and apply weak convergence .",
    "first notice that we have :    [ tight_lem_weakconvhelper ] for every fixed @xmath143 , there exists a deterministic cocountable subset of @xmath38 $ ] on which @xmath478 furthermore , these sequences are uniformly bounded ( for fixed @xmath293 ) .    note that all the above processes are uniformly bounded ( for fixed @xmath293 ) since @xmath74 is a probability measure .",
    "the result then follows by corollary  [ tight_cor_fiddly ] .",
    "[ tight_prop_evoeqn ] suppose @xmath349 .",
    "then , for every @xmath143 , the processes @xmath479 , @xmath480 and @xmath481 from definition  [ tight_def_mgcomp ] are martingales .",
    "hence @xmath345 and @xmath27 satisfy the evolution equation from theorem  [ intro_thm_exist ] .",
    "furthermore , @xmath345 is continuous .",
    "fix @xmath143 and let @xmath387 be the cocountable set of times on which we have the conclusion of lemma  [ tight_lem_weakconvhelper ] . to show that @xmath482 is a martingale , it is enough to show that , for any arbitrary @xmath483 , @xmath484 , @xmath485 \\cap \\mathbb{t}$ ] and @xmath486 continuous and bounded , that the map defined by @xmath487 satisfies @xmath488 . by lemma",
    "[ tight_lem_weakconvhelper ] and the boundedness and continuity of the @xmath489 s @xmath490 however , from proposition  [ finite_prop_evolution ] , we have that @xmath491 is a martingale since @xmath492 therefore @xmath493 and so @xmath479 is a martingale .    for @xmath494 ,",
    "define the map @xmath495 by applying it s formula to ( [ eq : exist_martingaleito ] ) , we have @xmath496_{t}.\\ ] ] so be the boundedness of the @xmath489 and proposition  [ finite_prop_vanish ] @xmath497 so @xmath498 and @xmath480 is a martingale .",
    "the work for @xmath499 follows similarly , so we omit it .",
    "the result is then complete by lemma  [ tight_lem_mgapp ] , and the continuity of @xmath500 follows by the fact that the right - hand side of the evolution equation in theorem  [ intro_thm_exist ] is continuous .",
    "the kernel smoothing method converts a measure into an approximating family of functions and , by establishing uniform results on the functions , enables us to show the existence of a density for the measure . in the next section we will use this to prove theorem  [ intro_thm_unique ] .",
    "let @xmath179 be a finite signed - measure and @xmath501 the gaussian heat kernel @xmath502 begin by noting the familiar fact that @xmath179 can be approximated by its convolution with @xmath501 : for every continuous and bounded @xmath503 @xmath504 as @xmath505 , and @xmath506 is a @xmath507 function .",
    "we will sometimes abuse notation and write @xmath508 when @xmath503 is a function . with @xmath509 denoting the usual @xmath510 inner product , we have @xmath511    our first observation is that @xmath512 is a contraction on @xmath510 :    [ ksm_prop_contract ] let @xmath513 .",
    "then @xmath514 , where @xmath515 is the @xmath69 norm on @xmath12 .",
    "the cauchy ",
    "schwarz inequality gives @xmath516 the first integral on the right - hand side integrates to one , then integrating over @xmath11 completes the proof .",
    "we now give a condition which shows how to recover the existence of a density via kernel smoothing .",
    "[ ksm_prop_liminfl2 ] suppose that @xmath179 is a finite signed measure and @xmath517 then @xmath179 has an @xmath510 density , i.e.  there exists @xmath518 such that @xmath519 , for every @xmath520 .",
    "furthermore @xmath521 in @xmath12 .",
    "the hypothesis gives a bounded sequence @xmath522 in @xmath510 , with @xmath523 . by (",
    "d , thm .",
    "3 ) , we can extract a weakly convergent subsequence @xmath524 for some @xmath513 .",
    "but by ( [ eq : kernel_basicsmooth ] ) we conclude that @xmath519 for all @xmath160 , and this gives the first result since ",
    "is dense in @xmath510 .",
    "we now have that @xmath525 , therefore by proposition  [ ksm_prop_contract ] @xmath526 by ( [ eq : kernel_basicsmooth ] ) we also have @xmath527 so @xmath528 , which completes the proof",
    ".      the material above will be used to establish a preliminary regularity result ( proposition  [ unique_prop_l2reg ] ) in section  [ sect_uniqueness ] . however , for the main uniqueness proof we will work in a space of lower regularity and on the half - line .",
    "recall that the first sobolev space with dirichlet boundary condition , @xmath529 , is defined to be the closure of @xmath530 under the norm @xmath531 the dual of @xmath529 will be denoted by @xmath140 and its norm by @xmath532 this is a natural space for us to work in due to the following .",
    "[ ksm_prop_signedmeasure ] if @xmath179 is a finite signed measure , then @xmath533 .",
    "first observe that @xmath534 , for every @xmath535 .",
    "morrey s inequality ( * ? ? ?",
    "5.6 , thm .",
    "4 ) gives a universal constant , @xmath133 , such that @xmath536 , and this completes the proof .",
    "to work on the half - line we will use the absorbing heat kernel defined , as in the proof of proposition  [ prob_prop_boundary ] , by @xmath537 and define @xmath538 notice that @xmath539 for every @xmath540 , so @xmath541 is an element of @xmath58 , and also notice that @xmath542 . for @xmath543 to approximate @xmath179",
    ", we need @xmath179 to be supported on @xmath129 :    [ ksm_basicsmooth ] if @xmath179 is supported on @xmath129 , then @xmath544 as @xmath94 , for every @xmath293 continuous , bounded and supported on @xmath6 :    let @xmath545 , then from ( [ eq : kernel_basicsmooth ] ) @xmath546 but by the hypotheses @xmath547 , as required .    to access the @xmath140 norm",
    ", we will use the _ anti - derivative _ defined by @xmath548 notice that @xmath549 , and if @xmath550 is also integrable , then @xmath551 too .",
    "the result we will use in section  [ sect_uniqueness ] is the following .",
    "[ ksm_prop_liminfh-1 ] if @xmath533 , then @xmath552 .",
    "first notice that for fixed @xmath553@xmath554 so @xmath555 is integrable and hence @xmath556 is well - defined .",
    "integration by parts gives @xmath557 for @xmath558 .",
    "therefore by proposition  [ ksm_basicsmooth ] we have @xmath559 which gives the result .",
    "in this section we will prove theorem  [ intro_thm_unique ] . therefore take @xmath44 , @xmath59 and @xmath27 as in the statement with @xmath560 along some subsequence .",
    "let @xmath62 and @xmath561 . the first step will be to show that @xmath44 has some @xmath69 regularity ( proposition  [ unique_prop_l2reg ] ) , which is due to a comparison with @xmath562 from ( [ eq : finite_nub ] ) and from the dynamics of proposition  [ finite_prop_evonub ] .",
    "we then use this fact , along with energy estimates in @xmath140 , to complete the proof .",
    "several technical lemmas are used throughout this section , however , to aid readability , their full statements and proofs are deferred until section  [ sect_lemmas ] .",
    "the result we will prove in this subsection is the following :    [ unique_prop_l2reg ] with @xmath44 as introduced at the start of section  [ sect_uniqueness ] , @xmath563 } \\sup_{{\\ensuremath{\\varepsilon } } > 0 } \\vert t_{{\\ensuremath{\\varepsilon}}}\\nu_{s } \\vert_{2}^{2 } < \\infty ,       \\qquad \\textrm{with probability 1}.\\ ] ]    we would like to work with some process @xmath564 defined analogously to ( [ eq : finite_nub ] ) that would satisfy the bound @xmath565 , for every @xmath566 $ ] and @xmath13 . at this stage , however , we are dealing only with weak limit points , so must recover the required process through a limiting procedure on @xmath567 :    [ unique_lem_wholespace ] on a sufficiently rich probability space , there exists @xmath568 such that @xmath340 is equal in law to @xmath43 , @xmath569 , for every @xmath46 $ ] and @xmath570 , and @xmath571 satisfies the limit spde on the whole space : @xmath572 for every @xmath46 $ ] and @xmath160 , together with condition  ( [ not_def_regcond5 ] ) of assumption  [ not_def_regcond ] and the two - sided tail bound @xmath573 for every @xmath92 .",
    "notice that in proposition  [ tight_prop_tight ] we have carried out sufficient work to prove @xmath567 is tight on @xmath36 , hence @xmath574 is tight .",
    "we can therefore conclude that there is a subsequence @xmath575 for which @xmath576 converges in law .",
    "any realisation of this limit must have a marginal law that agrees with the law of @xmath43 .",
    "as the work in propositions  [ tight_prop_measure ] and  [ tight_prop_evoeqn ] is unchanged for @xmath577 in place of @xmath74 , we conclude that @xmath571 is probability - measure - valued and , due to proposition  [ finite_prop_evonub ] , that @xmath571 satisfies the limit spde on the whole space .",
    "finally , we note that for every @xmath160 with @xmath578 we have @xmath579 , therefore @xmath580 for every @xmath581 , by ( * ? ? ?",
    "this inequality holds for all @xmath16 by the continuity of @xmath345 and @xmath571 ( which follows from being solutions to the limit spde ) and suffices to give the required dominance .",
    "condition  ( [ not_def_regcond5 ] ) of assumption  [ not_def_regcond ] is satisfied by @xmath571 because the proof of corollary  [ prob_cor_spatialconc ] uses only the behaviour of @xmath577 .",
    "likewise , the two - sided tail estimate is satisfied due to the same work as in proposition  [ prob_prop_tail ] .",
    "our strategy is to use the kernel smoothing method with @xmath69-energy estimates on the spde satisfied by @xmath571 .",
    "this is possible because we do not have to take boundary effects into account , which is the main difficulty in the uniqueness proof that will follow .",
    "the following lemma relates @xmath571 to proposition  [ unique_prop_l2reg ] .    with @xmath44 and @xmath571 as above and @xmath512 as in ( [ eq : ksm_tbar ] ) , if @xmath582 }   \\vert \\bar{t}_{{\\ensuremath{\\varepsilon}}}\\bar{\\nu}^{*}_{s } \\vert_{2}^{2}\\ , ] < \\infty\\ ] ] then proposition [ unique_prop_l2reg ] holds .    since @xmath583 , @xmath584 }   \\vert \\bar{t}_{{\\ensuremath{\\varepsilon}}}\\nu^{*}_{s }   \\vert_{2}^{2}\\ , ]",
    "< \\infty$ ] .",
    "we would first like to deduce that this fact also holds for @xmath585 , but since the map @xmath586 might not be continuous on @xmath40 , more care must be taken .    by fixing @xmath587 to be the haar basis of @xmath510 we have @xmath588 } \\vert \\bar{t}_{{\\ensuremath{\\varepsilon } } } \\nu_{t }",
    "\\vert_{2}^{2 }      & = { \\ensuremath{\\mathbf{e}}}\\sup_{t \\in [ 0,t ] } \\lim_{k \\to \\infty } \\sum_{i = 1}^{k } ( \\bar{t}_{{\\ensuremath{\\varepsilon } } } \\nu_{t } , \\phi_{i})^{2}_{2 } \\\\      & \\leq \\liminf_{k \\to \\infty } { \\ensuremath{\\mathbf{e}}}\\sup_{t \\in [ 0,t ] } \\sum_{i = 1}^{k } \\nu_{t}(\\bar{t}_{{\\ensuremath{\\varepsilon } } } \\phi_{i})^{2}. \\nonumber\\end{aligned}\\ ] ] by ( [ eq : unique_l2switch ] ) and fatou s lemma .",
    "since each @xmath589 is compactly supported , we have that @xmath590 , therefore @xmath591 is equal in law to @xmath592 , so by ( * ? ? ?",
    "13.4.1 ) @xmath351 } \\sum_{i = 1}^{k } \\nu_{t}(\\bar{t}_{{\\ensuremath{\\varepsilon } } } \\phi_{i})^{2 }       = _ { \\textrm{law } } \\sup_{t \\in [ 0,t ] } \\sum_{i = 1}^{k } \\nu^{*}_{t}(\\bar{t}_{{\\ensuremath{\\varepsilon } } } \\phi_{i})^{2}.\\ ] ] returning to ( [ eq : unique_lem_l2_1 ] ) , we now have that @xmath593 } \\vert \\bar{t}_{{\\ensuremath{\\varepsilon } } } \\nu_{t } \\vert_{2}^{2 }      \\leq \\liminf_{k \\to \\infty } { \\ensuremath{\\mathbf{e}}}\\sup_{t \\in [ 0,t ] } \\sum_{i = 1}^{k } \\nu^{*}_{t}(\\bar{t}_{{\\ensuremath{\\varepsilon } } } \\phi_{i})^{2 }      \\leq { \\ensuremath{\\mathbf{e}}}\\sup_{t \\in [ 0,t ] } \\vert \\bar{t}_{{\\ensuremath{\\varepsilon } } } \\nu^{*}_{t } \\vert_{2}^{2}.\\ ] ] by noting that @xmath594 and applying fatou s lemma once more we arrive at : @xmath595 } \\vert t_{{\\ensuremath{\\varepsilon}}}\\nu_{s } \\vert_{2}^{2 } \\ , ]      & \\leq { \\ensuremath{\\mathbf{e } } } [ \\ , \\liminf_{{\\ensuremath{\\varepsilon}}\\to \\infty } \\sup_{s \\in [ 0,t ] } \\vert \\bar{t}_{{\\ensuremath{\\varepsilon}}}\\nu_{s } \\vert_{2}^{2 } \\ , ] \\\\      & \\leq \\liminf_{{\\ensuremath{\\varepsilon}}\\to \\infty } { \\ensuremath{\\mathbf{e}}}\\sup_{t \\in [ 0,t ] } \\vert \\bar{t}_{{\\ensuremath{\\varepsilon } } } \\nu^{*}_{t } \\vert_{2}^{2 }       < \\infty.\\end{aligned}\\ ] ]    we now have that @xmath596 , for every @xmath440 $ ] , with probability 1 .",
    "proposition  [ ksm_prop_liminfl2 ] implies that @xmath128 has an @xmath510-density , @xmath597 , for every @xmath16 and that @xmath598 } \\vert t_{{\\ensuremath{\\varepsilon}}}\\nu_{s } \\vert_{2},\\ ] ] therefore @xmath105 } \\vert v_{s } \\vert_{2 } < \\infty$ ] , with probability 1 . then by proposition  [ ksm_prop_contract ]",
    "@xmath563 } \\sup_{{\\ensuremath{\\varepsilon } } > 0 } \\vert t_{{\\ensuremath{\\varepsilon } } } \\nu_{s } \\vert_{2 }       \\leq \\sup_{s \\in [ 0,t ] }   \\vert   v_{s } \\vert_{2 }        < \\infty,\\ ] ] almost surely , as required .    as an immediate consequence of the final part of the previous proof and of the forthcoming proof of proposition [ unique_prop_l2reg ]",
    ", we have the existence of a density process for @xmath44 :    [ unique_cor_l2reg ] with probability 1 , for every @xmath46 $ ] there exists @xmath599 such that @xmath597 is supported on @xmath129 and is a density of @xmath128 , i.e.   @xmath600 furthermore @xmath601 } \\vert v_{t } \\vert_{2 } < \\infty$ ] , with probability 1 .",
    "[ unique_rem_crude ] we might hope that this argument could be used to prove uniqueness . however , notice that we have no control over @xmath602 , as all we have are upper bounds on solutions .",
    "fix @xmath11 and set the function @xmath603 into the spde from lemma [ unique_lem_wholespace ] to get @xmath604 with the short - hand from remark  [ finite_rem_shorthand ] .",
    "we would like to move the diffusion coefficients out of the integral against @xmath571 , and to do so we use lemma  [ lemmas_lem_switch2ndorder ] : @xmath605 where @xmath606 is as defined in lemma  [ lemmas_lem_switch2ndorder ] and the dependence on @xmath540 is omitted for clarity . applying it s formula",
    "to @xmath607 gives @xmath608    our strategy is to integrate over @xmath11 , take a supremum over @xmath46 $ ] and then take an expectation over the previous equation .",
    "for the first task we appeal to lemma  [ lemmas_lem_switch2ndorder ] , lemma  [ lemmas_lem_stochasticfubini ] and young s inequality with free parameter @xmath295 to obtain @xmath609     ( { \\ensuremath{\\partial_{x}}}\\bar{t}_{{\\ensuremath{\\varepsilon } } } { \\ensuremath{\\bar{\\nu}}}^{*}_{s})^{2}dx ds    \\\\    & \\qquad - 2 \\int_{0}^{t } \\int_{{\\ensuremath{\\mathbb{r } } } } \\rho_{s } \\bar{t}_{{\\ensuremath{\\varepsilon } } } { \\ensuremath{\\bar{\\nu}}}_{s}^ { * } ( \\sigma_{s } { \\ensuremath{\\partial_{x}}}\\bar{t}_{{\\ensuremath{\\varepsilon } } } { \\ensuremath{\\bar{\\nu}}}_{s}^ { * }     + { \\ensuremath{\\partial_{x}}}\\sigma_{s } \\bar{\\mathcal{h}}_{s,{\\ensuremath{\\varepsilon } } } + \\bar{\\mathcal{e}}^{\\sigma}_{s , { \\ensuremath{\\varepsilon } } } ) dx dw_{s }   \\end{aligned}\\ ] ] where @xmath610 is a constant depending only on @xmath611 . considering the third line , by assumption  [ not_def_coefficients ] it is possible to choose @xmath295 small enough so that @xmath612,\\ ] ] therefore @xmath613 using lemma  [ lemmas_lem_b  d",
    " g ] to take a supremum over @xmath16 and then expectation gives @xmath614}\\big\\vert \\bar{t}_{{\\ensuremath{\\varepsilon } } } { \\ensuremath{\\bar{\\nu}}}_{s}^ { * } \\big\\vert^{2}_{2 }    \\leq \\big\\vert \\bar{t}_{{\\ensuremath{\\varepsilon } } } \\nu_{0 } \\big\\vert^{2}_{2 }     + c_{1 } { \\ensuremath{\\mathbf{e}}}\\int^{t}_{0 } \\big\\vert \\bar{t}_{{\\ensuremath{\\varepsilon } } } { \\ensuremath{\\bar{\\nu}}}_{s}^ { * } \\big\\vert_{2}^{2}ds + c_{1 } { \\ensuremath{\\mathbf{e}}}\\int_{0}^{t }      \\big\\vert \\bar{t}_{2{\\ensuremath{\\varepsilon } } } { \\ensuremath{\\bar{\\nu}}}_{s}^ { * } \\big\\vert_{2}^{2}ds   \\\\",
    "\\qquad + c_{1 } { \\ensuremath{\\mathbf{e}}}\\int_{0}^{t } \\big ( \\big\\vert \\bar{\\mathcal{e}}_{s , { \\ensuremath{\\varepsilon}}}^{\\mu } \\big\\vert_{2}^{2 } +       \\big\\vert \\bar{\\mathcal{e}}_{s , { \\ensuremath{\\varepsilon}}}^{\\sigma^{2 } } \\big\\vert_{2}^{2 } +       \\big\\vert \\bar{\\mathcal{e}}_{s , { \\ensuremath{\\varepsilon}}}^{\\sigma } \\bigr\\vert_{2}^{2 } \\big ) ds,\\end{gathered}\\ ] ] where @xmath212 is a numerical constant .",
    "taking @xmath615 as @xmath94 over the previous inequality and applying proposition  [ ksm_prop_contract ] ( to @xmath616 ) and lemma  [ lemmas_lem_switch2ndorder ] yields @xmath617 } \\vert \\bar{t}_{{\\ensuremath{\\varepsilon } } } \\bar{\\nu}^{*}_{s } \\vert_{2}^{2 } \\\\      & \\leq c_{1 } \\vert v_{0 } \\vert_{2}^{2 }      + 2c_{1 } \\liminf_{{\\ensuremath{\\varepsilon}}\\to 0 } { \\ensuremath{\\mathbf{e}}}\\int^{t}_{0 } \\vert \\bar{t}_{{\\ensuremath{\\varepsilon } } } \\bar{\\nu}^{*}_{s } \\vert_{2}^{2 } ds \\\\      & \\leq c_{1 } \\vert v_{0 } \\vert_{2}^{2 }      + 2c_{1}tf(t).\\end{aligned}\\ ] ] hence for @xmath618 we have @xmath619 . the proof is completed by propagating the argument onto @xmath620 $ ] by the same work as above but started from @xmath621 , rather than @xmath622 .",
    "this gives @xmath623 } \\vert \\bar{t}_{{\\ensuremath{\\varepsilon } } } \\bar{\\nu}^{*}_{s } \\vert_{2}^{2}\\ , ] \\\\",
    "\\leq 2c_{1 } \\liminf_{{\\ensuremath{\\varepsilon}}\\to 0 } { \\ensuremath{\\mathbf{e}}}\\ , [ \\sup_{s \\in [ 0 , ( 4c_{1})^{-1 } ] } \\vert \\bar{t}_{{\\ensuremath{\\varepsilon } } } \\bar{\\nu}^{*}_{s } \\vert_{2}^{2}\\ , ]      \\leq ( 2c_{1})^{2},\\end{gathered}\\ ] ] and so in general @xmath624 } \\vert \\bar{t}_{{\\ensuremath{\\varepsilon } } } \\bar{\\nu}^{*}_{s } \\vert_{2}^{2}\\ , ]      \\leq ( 2c_{1})^{k+1 } ,      \\qquad \\textrm{for } k \\geq 0.\\ ] ] since the largest such @xmath452 we need to take is @xmath625 , the simple bound @xmath626 }       \\vert \\bar{t}_{{\\ensuremath{\\varepsilon } } } \\bar{\\nu}^{*}_{s } \\vert_{2}^{2 }      \\leq \\sum_{k = 0}^{k_{0 } - 1 } ( 2c_{1})^{k + 1 } < \\infty\\ ] ] completes the proof .",
    "returning to proof of theorem  [ intro_thm_unique ] , notice that for a fixed @xmath627 , the function @xmath628 from ( [ eq : ksm_absorbing ] ) is an element of @xmath58 . setting into the spde for @xmath44 gives @xmath629 and by applying lemma  [ lemmas_lem_switchderiv ] @xmath630 to introduce the anti - derivative we integrate the above equation over @xmath627 and apply lemma  [ lemmas_lem_stochasticfubini ] to switch the time and space integrals .",
    "( note : lemma  [ lemmas_lem_stochasticfubini ] is stated for @xmath571 , however the proof only relies on the tail bound from assumption  [ not_def_regcond ] condition  ( [ not_def_regcond3 ] ) , which is satisfied by @xmath44 and @xmath59 . ) we arrive at @xmath631 which , after rewriting using the notation from lemma  [ lemmas_lem_switch1storder ] , becomes @xmath632    we will now introduce the simplifying notation @xmath633 to denote any family of @xmath67-valued processes , @xmath634}\\}_{{\\ensuremath{\\varepsilon } } > 0}$ ] , satisfying @xmath635 thus a formal linear combination of @xmath633 terms is of order @xmath633 . therefore ( [ eq : unique_anti1 ] ) can be written ( using lemma  [ lemmas_lem_switch1storder ] ) as @xmath636 and we claim that the integrands in the final two terms are also of order @xmath633 .",
    "this claim is in fact the critical boundary result from @xcite , but here we only need first moment estimates :    [ unique_lem_boundary ] we have @xmath637 hence @xmath638 and @xmath639",
    ".    begin by noting that @xmath640,\\end{aligned}\\ ] ] for @xmath641 a free parameter and @xmath212 a universal constant .",
    "squaring and integrating over @xmath627 gives @xmath642,\\ ] ] with @xmath213 another numerical constant .",
    "condition  ( [ not_def_regcond4 ] ) of assumption  [ not_def_regcond ] and the fact that @xmath643 , since @xmath128 is a sub - probability measure , allows us to write @xmath644 which vanishes if we choose @xmath611 to satisfy @xmath645 and this completes the proof .",
    "with lemma  [ unique_lem_boundary ] , we can now reduce ( [ eq : unique_anti2 ] ) to @xmath646 and this equation is also satisfied by @xmath59 , as so far all we have used is assumption  [ not_def_regcond ] .",
    "writing @xmath647 and @xmath648 , taking the difference of ( [ eq : unique_anti3 ] ) for @xmath44 and @xmath59 yields @xmath649 where @xmath650 is as in lemma  [ lemmas_lem_switch1storder ] , but with @xmath44 replaced by @xmath59 . applying it s formula to the square @xmath651 gives @xmath652 note that the initial condition for this equation is zero because @xmath44 and @xmath59 have the same initial condition .",
    "since the work in establishing the bounds in lemma  [ lemmas_lem_stochasticfubini ] only uses the tail estimate ( [ not_def_regcond3 ] ) of assumption  [ not_def_regcond ] , they remain valid and so , together with lemma  [ lemmas_lem_antitail ] , the stochastic integrals in ( [ eq : unique_anti4 ] ) are martingales for fixed @xmath540 and @xmath653 . therefore first taking an expectation and",
    "then integrating over @xmath627 and using young s inequality with free parameter @xmath295 produces a constant @xmath610 such that @xmath654|t_{{\\ensuremath{\\varepsilon}}}\\delta_{s}|^{2}dx ds \\nonumber \\\\    & \\quad + o(1),\\nonumber\\end{aligned}\\ ] ] where the terms involving @xmath633 have collapsed to order @xmath270 . also notice that ( [ eq : unique_anti5 ] ) remains valid if @xmath16 is a stopping time",
    ".    if it was the case that @xmath655 , then by proposition  [ ksm_prop_liminfl2 ] we would have @xmath656 on @xmath657 $ ] , and so would have completed the proof for this value of @xmath16 .",
    "it is therefore no loss of generality to assume that this value is bounded away from zero for all @xmath297 sufficiently small .",
    "then by taking @xmath295 we can find a positive value @xmath658 such that @xmath659 for @xmath660 constant .",
    "we now want to introduce a comparison between solutions in the @xmath286 terms , and to do so we consider two cases .",
    "first consider the simpler case where @xmath28 and @xmath24 are lipschitz in the loss variable , rather than piecewise lipschitz .",
    "therefore we have @xmath661 , so the inequality in ( [ eq : unique_anti6 ] ) becomes @xmath662 with @xmath212 constant .    to bound the second term above",
    ", we introduce the stopping times @xmath663 } \\sup_{{\\ensuremath{\\varepsilon } } > 0 } \\vert t_{{\\ensuremath{\\varepsilon } } } \\tilde{\\nu}_{s } \\vert_{2}^{2 } > n \\ } \\wedge t.\\ ] ] from proposition  [ unique_prop_l2reg ] we know that @xmath664 as @xmath78 , with probability 1 .",
    "since ( [ eq : unique_anti5 ] ) is valid for stopping times we have @xmath665 by using the integrating factor @xmath666 we obtain @xmath667 and applying fatou s lemma and propositions  [ ksm_prop_signedmeasure ] and  [ ksm_prop_liminfh-1 ] gives @xmath668 where @xmath669 .",
    "finally we apply lemma  [ lemmas_lem_lossinh1 ] to the above inequality to reintroduce @xmath670 to the right - hand side . with fixed @xmath92",
    "we have @xmath671 where @xmath213 does not depend on @xmath672 ( but does depend on @xmath673 ) .",
    "now fix @xmath674 so that we have @xmath675 with @xmath676 independent of @xmath672 . by using the integrating factor @xmath677 we deduce @xmath678",
    "so setting @xmath679 and sending @xmath230 gives @xmath680 .",
    "therefore @xmath681 on @xmath682 $ ] , and since @xmath683 we have theorem  [ intro_thm_unique ] in case 1 .",
    "to extend the argument to the general case , we use a stopping argument and consider the system only on time intervals where the loss processes are in the same interval @xmath684  recall assumption  [ not_def_coefficients ] .    define the stopping times @xmath685 and @xmath686 . for the reason immediately proceeding ( [ eq : unique_anti4 ] ) , the argument in case 1 can be replicated on @xmath687 by replacing @xmath16 by @xmath688 , since before @xmath689 , the coefficients can be compared using the lipschitz property on @xmath690",
    ". therefore we conclude @xmath691 for @xmath692 , which forces @xmath693 for @xmath692 and thus @xmath694 .",
    "we can then repeat the argument for the interval @xmath695 , since @xmath696 ( by continuity of @xmath44 and @xmath59 ) , where @xmath697 and @xmath698 .",
    "continuing upto @xmath699 covers all the @xmath684 intervals , and this completes the proof , since @xmath0 and @xmath700 are increasing ( assumption  [ not_def_regcond ] , condition  ( [ not_def_regcond1 ] ) ) so @xmath38 \\subseteq \\cup_{i = 0}^{k-1 } [ s_{i } , s_{i+1})$ ] .",
    "this section collects all the technical lemmas that were used in section  [ sect_uniqueness ] , and should be read only as a reference .",
    "[ lemmas_lem_switch1storder ] let @xmath701 where @xmath442 is one of @xmath28 , @xmath29 or @xmath702 and @xmath703 .",
    "define the error term @xmath704 then @xmath705    let @xmath706 , as @xmath505 , be a function that we will specify later .",
    "for any @xmath707 @xmath708 with @xmath212 a universal constant , and where the second line follows by splitting the integral on @xmath709 and its complement . by considering the range @xmath710 and using condition  ( [ not_def_regcond5 ] ) of assumption  [ not_def_regcond ] @xmath711 for some @xmath712 , by fixing @xmath611 in the range @xmath713    now consider the range @xmath714 . decomposing the @xmath715-integral on the range @xmath716 and",
    "its complement gives @xmath717 with @xmath213 another universal constant .",
    "therefore @xmath718 summing ( [ eq : kerenl_switch1 ] ) and ( [ eq : kerenl_switch2 ] ) and fixing @xmath719 completes the proof .",
    "[ lemmas_lem_switch2ndorder ] let @xmath720 where @xmath442 is one of @xmath28 , @xmath29 or @xmath702 and @xmath721 .",
    "define the error term @xmath722 then @xmath723 and there exists a numerical constant @xmath660 such that @xmath724 , x \\in { \\ensuremath{\\mathbb{r}}}\\textrm { and } { \\ensuremath{\\varepsilon } } > 0.\\ ] ]    interchanging differentiation and integration with respect to @xmath725 gives @xmath726{\\ensuremath{\\partial_{x}}}p_{{\\ensuremath{\\varepsilon}}}(x - y)\\bar{\\nu}^{*}_{t}(dy).\\ ] ] by bounding with the second - order derivative and using @xmath727 gives @xmath728 we therefore have the same order of @xmath553  as in lemma  [ lemmas_lem_switch1storder ] , so the first result follows by the same work . for the second result ,",
    "notice that @xmath729 and @xmath730 .",
    "[ lemmas_lem_stochasticfubini ] for all @xmath731 , @xmath297 and @xmath142}}}$ ] @xmath732   ds \\bigr)^{1/2 } dx < \\infty,\\ ] ] hence the stochastic fubini theorem @xcite gives @xmath733 whenever @xmath734 , x \\in { \\ensuremath{\\mathbb{r}}}}|g_{t}(x)| < \\infty$ ] .    by applying young s inequality and concavity of @xmath735",
    ", it suffices to show that @xmath736 ds \\bigr)^{1/2 } dx < \\infty.\\ ] ] first notice that @xmath737 where @xmath738 is a polynomial of degree @xmath673 .",
    "since @xmath739 is a probability measure , hlder s inequality gives @xmath740    \\leq { \\ensuremath{\\mathbf{e}}}\\int_{{\\ensuremath{\\mathbb{r } } } } |p_{n}({\\ensuremath{\\varepsilon}}^{-1}(x - y))|^{4}p_{{\\ensuremath{\\varepsilon}}}(x - y)^{4 } { \\ensuremath{\\bar{\\nu}}}^{*}_{s}(dy).\\ ] ] for any value of @xmath540 , the integrand above is bounded ( recall that @xmath553  is fixed ) . hence it suffices to bound the right - hand side of ( [ eq : lemmas_interchangefinite1 ] ) in terms of @xmath540 only for large values of @xmath741 .",
    "splitting the @xmath715-integral on the region @xmath742 and its complement gives the bound @xmath743 \\\\",
    "\\leq c_{{\\ensuremath{\\varepsilon } } } { \\ensuremath{\\mathbf{e}}}{\\ensuremath{\\bar{\\nu}}}^{*}_{s}((x/2 , + \\infty ) \\cup ( -\\infty , -x/2 ) )    + c_{{\\ensuremath{\\varepsilon } } } \\exp\\ { - x^{2}/2{\\ensuremath{\\varepsilon}}\\ }    = o(e^{-x}),\\end{gathered}\\ ] ] where @xmath744 and the @xmath202 depend only on @xmath553  and where we have used the tail estimate from lemma  [ unique_lem_wholespace ] .",
    "this suffices to complete the proof .",
    "[ lemmas_lem_easyibp ] let @xmath745 be bounded with bounded first derivatives .",
    "assume also that these functions and their first derivatives vanish at @xmath746 .",
    "then @xmath747    integration by parts .",
    "[ lemmas_lem_b  d  g ] there exists a constant @xmath660 such that @xmath748 } \\big| 2 \\int_{0}^{u } \\int_{{\\ensuremath{\\mathbb{r } } } } \\rho_{s}\\bar{t}_{{\\ensuremath{\\varepsilon } } } { \\ensuremath{\\bar{\\nu}}}_{s}^ { * } ( \\sigma_{s } { \\ensuremath{\\partial_{x}}}\\bar{t}_{{\\ensuremath{\\varepsilon } } }   { \\ensuremath{\\bar{\\nu}}}_{s}^ { * } + { \\ensuremath{\\partial_{x}}}\\sigma_{s } \\bar{\\mathcal{h}}_{s , { \\ensuremath{\\varepsilon } } } + \\bar{\\mathcal{e}}_{s , { \\ensuremath{\\varepsilon}}}^{\\sigma } ) dx dw_{s } \\big|     \\\\    \\leq \\frac{1}{2 } { \\ensuremath{\\mathbf{e}}}\\sup_{s \\in [ 0,t ] } \\big\\vert \\bar{t}_{{\\ensuremath{\\varepsilon } } } { \\ensuremath{\\bar{\\nu}}}_{s}^ { * } \\big\\vert_{2}^{2 }    + c { \\ensuremath{\\mathbf{e}}}\\int^{t}_{0 } \\big\\vert \\bar{t}_{{\\ensuremath{\\varepsilon } } } { \\ensuremath{\\bar{\\nu}}}_{s}^ { * } \\big\\vert_{2}^{2 } ds    + c { \\ensuremath{\\mathbf{e}}}\\int^{t}_{0 } \\big\\vert \\bar{\\mathcal{e}}_{s , { \\ensuremath{\\varepsilon}}}^{\\sigma } \\big\\vert_{2}^{2 } ds\\end{gathered}\\ ] ] for all @xmath46 $ ] .    by a similar analysis to ( [ eq : lemmas_interchangefinite1 ] ) we know that , for every fixed @xmath553 , the integrand above is a rapidly decaying function of @xmath540 , hence the stochastic integral is a martingale , so the burkholder ",
    "gundy inequality  ( * ? ? ?",
    "iv.42.1 ) gives a universal constant , @xmath749 , for which the left - hand side above is bounded by @xmath750.\\ ] ] by lemma  [ lemmas_lem_easyibp ] , this is equal to a constant multiple of @xmath751,\\ ] ] which , by hlder s inequality , is bounded by a constant multiple of @xmath752   \\\\    \\leq { \\ensuremath{\\mathbf{e}}}\\big [ \\sup_{s \\in [ 0,t ] } \\big\\vert \\bar{t}_{{\\ensuremath{\\varepsilon } } } { \\ensuremath{\\bar{\\nu}}}_{s}^ { * } \\big\\vert_{2 } \\big ( \\int^{t}_{0 } \\big\\vert - { \\ensuremath{\\partial_{x}}}\\sigma_{s }      \\bar{t}_{{\\ensuremath{\\varepsilon } } } { \\ensuremath{\\bar{\\nu}}}_{s}^ { * } + { \\ensuremath{\\partial_{x}}}\\sigma_{s } \\bar{\\mathcal{h}}^{\\sigma}_{s , { \\ensuremath{\\varepsilon } } } + \\bar{\\mathcal{e}}^{\\sigma}_{s , { \\ensuremath{\\varepsilon } } }     \\big\\vert_{2}^{2 } ds \\big)^{1/2 } \\big].\\end{gathered}\\ ] ] the result then follows by applying young s inequality with parameter @xmath753 and using the boundedness of the coefficients .    [ lemmas_lem_switchderiv ] for all",
    "@xmath754 and @xmath297 we have    a.   @xmath755 , b.   @xmath756 .",
    "an easy calculation .",
    "[ lemmas_lem_antitail ] for all @xmath627 , @xmath46 $ ] and @xmath297 @xmath757    split the integral @xmath758 at @xmath759 and its complement to obtain @xmath760 the triangle inequality completes the result .",
    "[ lemmas_lem_lossinh1 ] let @xmath44 , @xmath59 , @xmath0 , @xmath700 and @xmath670 be as in section  [ sect_uniqueness ] .",
    "for every @xmath92 there exists a constant @xmath761 such that @xmath762 for all @xmath763 $ ] , @xmath764 and @xmath232 , where @xmath765 is a constant that does not depend on @xmath672 .    for @xmath766 and @xmath232 ,",
    "let @xmath767 be any cut - off function satisfying @xmath768 @xmath769 and @xmath770 , for some constant @xmath749 .",
    "then @xmath771 for @xmath772 a constant .",
    "therefore @xmath773 and so the result follows from conditions  ( [ not_def_regcond3 ] ) and  ( [ not_def_regcond4 ] ) of assumption  [ not_def_regcond ] ( and that @xmath774 for all @xmath13 ) .    the following result will be used in section  [ sect_mv ] .",
    "[ lemmas_lem_interchange ] suppose we are working on a probability space with filtration @xmath775 and @xmath27 is a standard brownian motion with natural filtration @xmath776 .",
    "let @xmath777 be a real - valued @xmath775-adapted process with @xmath778 then , with probability 1 , @xmath779=\\int_{0}^{t } \\mathbf{e}\\left[\\left.h_{s}\\right|\\mathcal{f}_{s}^{w}\\right]dw_{s}\\ ] ] and @xmath780=0\\ ] ] for every @xmath781.$ ]    as we can multiply @xmath782 by @xmath783 , it suffices to take @xmath784 .",
    "first , suppose that @xmath777 is a basic process , that is @xmath785 where @xmath786 are real numbers and @xmath182 is @xmath787-measurable",
    ". then @xmath788 & = \\mathbf{e}\\left[\\left.z\\left(w_{s_{2}}- w_{s_{1}}\\right)\\right|\\mathcal{f}_{t}^{w}\\right]\\\\   & = \\mathbf{e}\\left[\\left.z\\right|\\mathcal{f}_{s_{1}}^{w}\\right]\\left(w_{s_{2}}-w_{s_{1}}\\right)\\\\   & = \\int_{0}^{t}\\mathbf{e}\\left[\\left.z\\right|\\mathcal{f}_{s}^{w}\\right]\\mathbf{1}_{s_{1}<s\\leq s_{2}}dw_{s}\\\\   & = \\int_{0}^{t}\\mathbf{e}\\left[\\left.h_{s}\\right|\\mathcal{f}_{s}^{w}\\right]dw_{s}\\end{aligned}\\ ] ] and @xmath789 &   = \\mathbf{e}\\left[\\left.z\\left(w_{s_{2}}^{1}-w_{s_{1}}^{1}\\right)\\right|\\mathcal{f}_{t}^{w}\\right]\\\\   & = \\mathbf{e}\\left[\\left.\\mathbf{e}\\left[\\left.z\\left(w_{s_{2}}^{1}-w_{s_{1}}^{1}\\right)\\right|\\sigma\\left(\\mathcal{f}_{t}^{w } ,   \\mathcal{f}_{s_{1}}\\right)\\right]\\right|\\mathcal{f}_{t}^{w}\\right]\\\\   & = \\mathbf{e}\\left[\\left.z\\mathbf{e}\\left[\\left.\\left(w_{s_{2}}^{1}-w_{s_{1}}^{1}\\right)\\right|\\sigma\\left(\\mathcal{f}_{t}^{w } ,   \\mathcal{f}_{s_{1}}\\right)\\right]\\right|\\mathcal{f}_{t}^{w}\\right]\\\\   & = \\mathbf{e}\\left[\\left.z\\mathbf{e}\\left[w_{s_{2}}^{1}-w_{s_{1}}^{1}\\right]\\right|\\mathcal{f}_{t}^{w}\\right]=0,\\end{aligned}\\ ] ] where we have used the fact that @xmath790 is independent of @xmath791 since @xmath792 and @xmath27 are independent and @xmath792 has independent increments . so the result holds in this case and immediately extends to linear combinations of basic processes . the usual density argument then allows us to extend the result to all required @xmath777 .",
    "this section presents a short proof of theorem  [ intro_thm_mv ] .",
    "take a strong solution @xmath43 to the limit spde ( remark  [ intro_rem_strongsolutions ] ) , an independent brownian motion @xmath79 and define @xmath80 by @xmath793 ( it is possible to find such an @xmath80 by standard diffusion theory , since @xmath794 is given and fixed . ) let @xmath59 be the conditional law of @xmath80 given @xmath27 killed at zero , that is @xmath795 we will have the existence statement of theorem  [ intro_thm_mv ] if we can prove @xmath681 .",
    "applying it s formula to @xmath796 as in the proof of proposition  [ finite_prop_evolution ] gives @xmath797 take a conditional expectation with respect to @xmath27 by applying lemma  [ lemmas_lem_interchange ] ( and using that @xmath0 is @xmath798-measurable ) to get @xmath799 now , @xmath44 also satisfies this equation , however in both cases the coefficients depend only on @xmath0 .",
    "therefore we can regard @xmath0 as fixed and @xmath44 and @xmath59 as solving the limit spde in the special case when coefficients do not depend on the loss - variable .",
    "this is a much easier linear problem and theorem  [ intro_thm_unique ] is certainly sufficient to conclude @xmath681 , as required .",
    "we have also just shown that if @xmath800 solves the mckean ",
    "vlasov problem in theorem  [ intro_thm_mv ] , then its conditional law @xmath681 solves the limit spde . by theorem  [ intro_thm_unique",
    "] , this fixes the law of @xmath44 , hence we have the uniqueness statement too .",
    "we end by giving some open problems arising from our model and its related extensions :    a.   as indicated at the end of section  [ sect_intro ] , the most important practical question is how do we numerically approximate @xmath44 from a given realisation of @xmath27 ?",
    "this leads to the further questions of how do we combine these approximations to get an estimator for @xmath85 , where @xmath84 is some pay - off function , and how do we calibrate the model to any data on traded prices for options with payoff @xmath801 ? + our proposed algorithm for the first problem is as follows . here",
    ", we discretise the time variable and treat the outputs of the following subroutines as functions on @xmath129  in practise we would also need a discretisation scheme for the spatial variable too , but we will not consider that problem here . fix a precision level @xmath134 and assume we are given a piecewise constant or piecewise linear approximation to a brownian trajectory @xmath802 to precision at least @xmath286 ( generating such a path contributes negligible computational cost in this algorithm ) and an initial density @xmath803 . set @xmath804 . for @xmath805 , form @xmath806",
    "recursively by setting @xmath807 where @xmath808 solves the deterministic linear pde @xmath809 for @xmath810 $ ] and @xmath627 .",
    "set @xmath811 ( calculated using some quadrature routine ) .",
    "our approximation to the density process , @xmath812 , of @xmath44 and the loss process , @xmath0 , are given by piecewise interpolation of @xmath813 and @xmath814 : @xmath815 ) } + \\mathrm{frac}\\{s\\ } v^{([s ] + 1 ) }      \\\\",
    "\\tilde{l}_{t }       & : = ( 1-\\mathrm{frac}\\{s\\ } ) l^{([s ] ) } + \\mathrm{frac}\\{s\\ } l^{([s ] + 1)},\\end{aligned}\\ ] ] where @xmath816 , @xmath817 $ ] is the floor of @xmath818 and @xmath819 $ ] .",
    "+ in the case when @xmath29 and @xmath28 are constant and @xmath24 depends only on the loss variable and @xmath820 is given as a piecewise constant interpolation of @xmath27 with precision @xmath286 , the solution to ( [ eq : open_algo1 ] ) can be written explicitly in terms of the brownian transition kernel .",
    "a numerical solution can then be found by quadrature .",
    "( this instance of the algorithm was used to produce figure  [ intro_fig_lossdepheat ] . )",
    "if these assumption do not hold , then further approximations may be necessary . in @xcite ( [ eq : open_algo1 ] ) is solved ( for the constant coefficient case ) by finite element methods and the scheme is proven to converge when the system is considered on the whole space . the authors conjecture and provide numerical evidence for a convergence rate for the scheme on the half - line with space - time discretisation .",
    "a first open problem is to verify that the piecewise - constant time - discretisation , @xmath821 , above converges in law to the solution @xmath44 of limit spde as @xmath289 .",
    "a harder problem is to establish the rate of convergence , in some appropriate norm , averaged over realisations of @xmath27 .",
    "+ returning to the task of calculating the pay - off @xmath85 , we have the estimator @xmath822 where @xmath823 are independent standard brownian motions and @xmath824 denotes the approximation to the loss function using the algorithm above with precision @xmath286 and brownian trajectory @xmath820 .",
    "as the monte carlo routine depends on @xmath286 , a natural variance reduction technique is to use multi - level monte carlo as in @xcite .",
    "another potentially useful technique is to alter the drift coefficient in ( [ eq : intro_coefficients ] ) using girsanov s theorem to produce a reweighted estimator . in the case when the pay - off function , @xmath825 , is supported on large losses , and hence is sensitive only to rare events , changing the measure to one under which the particles have a large negative drift and multiplying by the appropriate radon  nikodym derivative is a form of importance sampling .",
    "a simpler observation in this scenario is that if the systemic brownian motion has a realisation that has followed a largely increasing path on @xmath38 $ ] , then although that realisation is likely to contribute little to @xmath826 , the negative of this realisation is likely to give a heavy contribution .",
    "hence the simple antithetic sampling routine in which we draw @xmath827 samples of the common brownian motion in pairs @xmath828 is a candidate for variance reduction .",
    "an open problem is to verify the usefulness of these techniques either numerically or analytically .",
    "b.   following on from the previous point , a natural extension to the model is to replace the systemic brownian motion term in ( [ eq : intro_coefficients ] ) with a lvy process .",
    "this would allow the possibility of generating extreme losses .",
    "mathematically we expect to arrive at a non - linear spde driven by a lvy process on the half - line  see , for example , @xcite . c.   another possibility for generating large systemic losses is to incorporate a contagion term in the particle dynamics along the lines of @xcite . for simplicity",
    ", consider the model where particles move according to the dynamics @xmath829 with @xmath92 .",
    "whenever a particle hits the origin , every other particle jumps by size @xmath830 towards the boundary .",
    "this can begin an avalanche effect where a default causes many other entities to default .",
    "convergence of a finite particle system to a limiting mckean  vlasov equation is shown in @xcite , and it is known that for small values of @xmath672 the solution is unique . for large values of @xmath672",
    "the limiting system undergoes a jump , whereby a macroscopic proportion of mass is lost in an infinitesimal period of time .",
    "it remains a challenge to prove uniqueness of solutions in this regime and to characterise a critical value of @xmath672 . from our perspective , a natural extension is to consider the system with a common brownian noise term between particles .",
    "the authors thank the anonymous referees for their helpful corrections .",
    "we are grateful to andreas sojmark for his very thorough reading and suggestions for improvements .",
    "sl thanks christoph reisinger and francois delarue for discussions on this material .                                                                                                        a .- s .",
    "sznitman . .",
    "in paul - louis hennequin , editor , _",
    "ecole det de probabilits de saint - flour xix  1989 _ , volume 1464 of _ lecture notes in mathematics _ , chapter  3 , pages 165251 .",
    "springer berlin heidelberg , 1991 ."
  ],
  "abstract_text": [
    "<S> we study a finite system of diffusions on the half - line , absorbed when they hit zero , with a correlation effect that is controlled by the proportion of the processes that have been absorbed . </S>",
    "<S> as the number of processes in the system becomes large , the empirical measure of the population converges to the solution of a non - linear stochastic heat equation with dirichlet boundary condition . </S>",
    "<S> the diffusion coefficients are allowed to have finitely many discontinuities ( piecewise lipschitz ) and we prove pathwise uniqueness of solutions to the limiting stochastic pde . as a corollary </S>",
    "<S> we obtain a representation of the limit as the unique solution to a stochastic mckean  </S>",
    "<S> vlasov problem . </S>",
    "<S> our techniques involve energy estimation in the dual of the first sobolev space , which connects the regularity of solutions to their boundary behaviour , and tightness calculations in the skorokhod m1 topology defined for distribution - valued processes , which exploits the monotonicity of the loss process @xmath0 . </S>",
    "<S> the motivation for this model comes from the analysis of large portfolio credit problems in finance . </S>"
  ]
}