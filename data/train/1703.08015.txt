{
  "article_text": [
    "the lattice - boltzmann method ( lbm ) is a highly parallel alternative to classical navier - stokes solvers for computational fluid dynamics ( cfd ) .",
    "many researchers have shown its excellent absolute performance and scalability on modern parallel machines , especially graphic processing units ( gpu ) .",
    "a thorough review of gpu lbm implementations is presented in @xcite .    in many areas of cfd application ( e.g. biomedical or porous media simulations )",
    "the geometry is sparse ( contains many areas without fluid ) .",
    "although many optimization techniques for lbm implementations on gpu are known @xcite , they are not specialized for sparse geometries",
    ". simulations for sparse geometries can be run using methods designed for large - scale multi - gpu implementations @xcite , but the lack of specific optimizations can significantly increase memory usage and computational effort required for simulation , what decreases the domain size that can be simulated on a single machine , and increases the number of processors required for a domain of a given size .",
    "this can be especially unfavorable for gpus , where the memory amount per processor is fixed and usually smaller than in typical cpu configurations .",
    "thus , efficient handling of sparse geometries is crucial to achieving a high performance and a good hardware utilization in many real - world simulations .",
    "current gpu implementations of lbm specialized for sparse geometries presented in @xcite are based on indirect addressing .",
    "this limits memory usage but causes significantly larger memory traffic than in dense implementations . for example , a highly optimized sparse implementation from @xcite achieves 64% of performance shown in @xcite for the same collision model and hardware ( tesla k20 ) .    in this work we present the gpu implementation of lbm for sparse geometries where the information about geometry is stored using the uniform grid of small , fixed - size tiles",
    "the use of tiles reduces to almost negligible values a number of ancillary data transfers needed to manage the geometry sparsity . for this method we provide the theoretical analysis , which allows to find the performance limits and compare them with indirect addressing solutions .",
    "we also show the results for two real implementations : a two - dimensional with minimized memory usage and a three - dimensional with high performance .",
    "the structure of the paper is as follows . in section [ sec_backgrounds ]",
    "we briefly introduce the lattice - boltzmann method and analyze existing techniques of sparse geometries handling .",
    "section [ sec_tiling ] contains the description of our implementation with the detailed analysis of introduced overheads .",
    "the performance comparison with existing implementations is presented in section [ sec_results ] .",
    "section [ sec_conclusions ] contains conclusions .",
    "in the lbm , as in the conventional cfd , the geometry , initial and boundary conditions must be specified to solve the initial - value problem . the computational domain is uniformly partitioned and computational nodes are placed in vertexes of adjacent cells , giving the lattice .",
    "one of the lattice structures used in this study , d2q9 , is presented in fig .",
    "[ fig_lattice ] .",
    "the first number in the notation ( d2 ) is the space dimension ; the second one ( q9 ) is the lattice links number .",
    ", @xmath0 are vectors along the @xmath1 lattice direction ; @xmath2 denotes the lattice unit . ]",
    "let @xmath3 represent the probability distribution function ( pdf ) along the @xmath1 lattice direction , @xmath4 and @xmath5 are the lattice spacing and the lattice time step respectively , @xmath6 is the lattice speed , @xmath7 is the unit vector along the @xmath1 lattice direction , and @xmath8 is the lattice velocity vector in the velocity space along the @xmath1 lattice direction .",
    "the core of the lattice - boltzmann method is the discretized in velocity @xmath9 space form of the boltzmann transport equation that can be written for each @xmath1 of @xmath10 directions ( lattice linkages ) in velocity space as follows : @xmath11 where @xmath12 is the collision operator .",
    "the collision operator can be approximated with the most popular bhatnagar - gross - krook model , @xmath13 where @xmath14 is the relaxation time in lb units related to the lattice fluid viscosity : @xmath15 and @xmath16 is the equilibrium distribution function along the @xmath1 lattice direction given by the following formula for quasi - compressible model : @xmath17 and for incompressible model : @xmath18 where @xmath19 is the lattice speed of sound that is a lattice constant ; @xmath20 is the macroscopic fluid velocity vector expressed in lb units ; @xmath21 is a fluid density that is related to a pressure @xmath22 , both expressed in lb units , and @xmath23 is a weighting scalar for the @xmath1 lattice direction .",
    "the macroscopic velocity for quasi - compressible model can be determined from @xmath24 and for incompressible model from @xmath25 integrating eqn .",
    "( [ equ_lbm_1 ] ) from @xmath26 to @xmath27 along the @xmath1 lattice direction and assuming that the collision term is constant during the interval , we can obtain discretized in time form of bgk - lbm equation @xmath28 } _ { collision }      ,      \\label{equ_lbm_bgk}\\ ] ] where @xmath29 is a position vector in the velocity space .",
    "the therm on the left hand side is known as the streaming ( propagation ) step ; the latter represents the collision step .",
    "these two steps are repeated sequentially during the simulation giving velocity , density and pressure distributions at each time step .",
    "the bgk - lbm uses the single relaxation time to characterize the collision effects",
    ". however , physically , these rates should be different during collision processes . to overcome this limitation , a collision matrix with different eigenvalues or multiple relaxation times can be used .",
    "the lbm with a mrt collision operator can be expressed as : @xmath30 } _ { collision }      ,      \\label{equ_lbm_mrt}\\ ] ] where @xmath31 is the collision matrix . in this study",
    "we use both of the above mentioned collision models in compressible and quasi - compressible versions .      to compare different implementations , we employ a widely used , simple computational complexity model based on @xcite . in this model , we assume that the data from device memory is read only once and cached in fast , internal memory ( registers / cache / scratchpad ) , and that the device memory can be effectively read in single - byte transactions .",
    "these assumptions are rather unrealistic for current machines because both cpu and gpu use dram memories with burst transactions and have the limited size of registers and cache .",
    "however , such `` ideal '' machine model allows finding the minimal amounts of data that have to be stored and transferred during computations .",
    "the minimal amounts of data define both the minimum memory usage and the maximum achievable performance for bandwidth bound implementations .    for simplification",
    ", we use the same measure of complexity for fluid and boundary nodes . in many geometries",
    "the boundary nodes are only a small portion of all non - solid nodes . moreover",
    ", many boundary nodes do not require significantly different amounts of stored and transferred data .",
    "even if additional values are used , they are often shared between many nodes and can be placed in cache / constant memory .",
    "let @xmath10 denote the number of pdf @xmath3 and @xmath32 denote the number of bytes for storing a single @xmath3 value ( e.g. 4/8 b for a single / double precision floating point ) .",
    "thus , the minimum number of memory bytes needed for single node datum is @xmath33 }      \\label{equ_mnode}\\ ] ] and the minimum number of bytes transferred per one lbm iteration for a single node is @xmath34 }      \\label{equ_bnode }      .\\ ] ] notice that we assume that an information about the node type is not stored in memory .",
    "formulas defining the number of floating point operations ( flop , not to be confused with floating point operations per second , flops ) are much more complex to determine .",
    "a simple count of the operations resulting from a naive implementation of equations ( [ equ_lbm_bgk ] ) and ( [ equ_lbm_mrt ] ) gives numbers significantly larger than in real implementations due to optimizations carried out at compilation stage ( multiplications by constants @xmath35 , common subexpression elimination , constant folding etc . ) .",
    "thus , for numbers of computational operations , we show only specific values obtained by a disassemble of a gpu binary code using the _ nvdisasm _ utility ( we count only floating point arithmetic operations , and fused multiply - add is treated as two operations ) . the complete number of flop per d2q9 lattice node ( including computations of velocity and density ) varies from @xmath36 flop for bgk incompressible to 145 flop for mrt quasi - compressible , where the floating point inverse must be computed on a gpu . for d3q19 lattice",
    "the computations require between 304 flop for bgk incompressible and 1165 flop for mrt quasi - compressible .",
    "the numbers of flop can not be treated as the minimal numbers of operations but can be used to estimate whether a performance of lbm implementation on a given machine is bound by memory bandwidth , instruction throughput , or latency . according to eqn .",
    "( [ equ_bnode ] ) each node requires the transfer of 144 bytes for d2q9 and 304 bytes for d3q19 lattice arrangements and double precision data .",
    "this gives between @xmath37 b / flop ( bgk incompressible ) and @xmath38 b / flop ( mrt quasi - compressible ) for d2q9 and between 1 b / flop ( bgk incompressible ) and 0.26 b / flop ( mrt quasi - compressible ) for d3q19 lattice arrangement .",
    "comparing these numbers with defined in @xcite the _ machine balance _ for gpu ( about @xmath39 b / flop for fermi , @xmath40 b / flop for kepler , and @xmath41 b / flop for pascal architectures ) it can be seen that lbm implementations on gpu should be usually bandwidth - bound .",
    "only the performance of mrt quasi - compressible implementation for d3q19 lattice arrangement on fermi architectures can be limited by arithmetic operations .",
    "compared with the minimal requirements defined in section [ sect_performance_model ] the techniques for sparse geometries bring some memory and bandwidth overheads due to indirect addressing .",
    "we ignore the computational overhead because the lbm implementations on gpu are usually bandwidth bound .",
    "let the overhead @xmath42 ( @xmath43 - memory overhead , @xmath44 - bandwidth overhead ) be defined as a ratio of additional operations to the minimum numbers defined in eqn .",
    "( [ equ_mnode ] ) and ( [ equ_bnode ] ) .",
    "only a single lbm time iteration is analyzed since all iterations are processed in the same way .",
    "let @xmath45 and @xmath46 denote the number of solid and non - solid ( fluid and boundary ) nodes in a geometry .",
    "the number of all nodes in the geometry is then @xmath47 .",
    "we can also define a geometry porosity @xmath48 and a solidity @xmath49 where both @xmath50 $ ] and @xmath51 .",
    "the gpu implementations of connectivity matrix ( cm ) , shown in @xcite and an optimized version in @xcite , use pointers to the neighbor node for each propagated function @xmath3 .",
    "no data are stored for solid nodes , but the additional pointers for each @xmath3 function are needed for each non - solid node .",
    "since the pointer can be replaced by an index to a linear array , then for gpu implementations the @xmath52 byte index is enough ( 6 gb allows for storing less than @xmath53 nodes for the d2q9 lattice and a single copy of single precision @xmath3 values ) .",
    "thus , each non - solid node requires an additional storage of @xmath54 indexes ( @xmath10 for implementation from @xcite ) and the memory overhead is @xmath55 where the `` @xmath56 '' term denotes the second copy of @xmath3 data for each non - solid node used to avoid race conditions in both @xcite and @xcite . for double precision @xmath3 values and @xmath52 bytes the left term in eqn .",
    "( [ equ_dmcm ] ) is less than @xmath57 .",
    "the bandwidth overhead for cm results only from reads of indexes from cm .",
    "thus , using the minimum transfer for a single node defined in eqn .",
    "( [ equ_bnode ] ) , the bandwidth overhead is @xmath58 the value of @xmath59 depends mainly on the ratio of @xmath60 to @xmath32 , and is @xmath61 for single , and @xmath62 for double precision @xmath3 values .",
    "the gpu implementation of fluid index array ( fia ) technique , presented in @xcite , uses a `` bitmap '' that for each node contains either a pointer to the non - solid node data or @xmath63 when a node is solid .",
    "the values of @xmath3 functions are stored only for non - solid nodes .",
    "thus , the additional memory is used only for pointers from fia ( single pointer per each solid / non - solid node ) .",
    "similarly as for cm , pointers can be replaced with @xmath64 b indexes .",
    "the memory overhead for fia is @xmath65 where the `` @xmath56 '' term denotes the second copy of @xmath3 data for each non - solid node used in @xcite to avoid race conditions .    for geometries with a low number of solid nodes the left term from eqn .",
    "( [ equ_dmfia ] ) is much smaller than @xmath66 ( about @xmath67 for single and about @xmath68 for double precision @xmath3 values and d2q9 lattice arrangement ) .",
    "however , when a geometry is very sparse , then the value of @xmath69 grows fast : for d2q9 lattice arrangement and single precision @xmath3 values it exceeds @xmath70 for @xmath71 .    the bandwidth overhead for fia based implementation is more difficult to define .",
    "fast lbm implementations fuse collision and streaming steps into a single kernel .",
    "unfortunately , the original implementation from @xcite uses two separate kernels . the first kernel , launched only for fluid nodes , reads and writes @xmath3 values and realizes collision step .",
    "the second kernel , launched for all nodes ( also the solid ones ) , is responsible for the streaming step and in addition to reads and writes of @xmath3 values also reads the fia indexes for the current node and for all neighbor nodes if the current node is non - solid .",
    "this approach significantly increases bandwidth overhead ( to values larger than 1 ) due to double access to @xmath3 values . in our estimation",
    ", we treat the additional reads / writes of @xmath3 values as required for fia based implementations and model these by adding the `` + 1 '' term to the bandwidth overhead . the rest of the overhead results only from additional reads of indexes from fia .",
    "since we assume that the indexes from fia are read from device memory only once and cached internally , the bandwidth overhead is @xmath72 notice that @xmath73 equals to half of the left term from eqn .",
    "( [ equ_dmfia ] ) .      the overhead estimates shown in eqn .",
    "( [ equ_dmcm])([equ_dbfia ] ) take into account the characteristics of real implementations , what results in the additional `` @xmath56 '' terms in equations for @xmath74 , @xmath75 and @xmath76 .",
    "we can also consider the `` ideal '' implementations ( with these `` @xmath56 '' terms removed in some way ) that can be used to define the performance limits of cm / fia techniques .",
    "the comparison of estimated bandwidth overheads for cm and fia techniques is shown in fig .",
    "[ fig_cm_fia_bw ] .",
    "although the real implementation of fia has 4 times higher bandwidth overhead than cm , in theory the ideal fia implementation can result in lower bandwidth overhead than cm for geometries with porosity lower than @xmath77 ( @xmath78 ) for d2q9 ( d3q19 ) lattice arrangement .",
    "the plots of estimated memory overheads for cm and fia have a shape similar to shown in fig .",
    "[ fig_cm_fia_bw ] bottom for both real and ideal implementations .",
    "notice that the overheads for fia decrease with an increase of the lattice links number .",
    "the bandwidth overheads for ideal implementations of fia and cm strongly depend on @xmath79 and @xmath60 .",
    "by decreasing the size of indexes to 2 or 1 b the overhead can be decreased twice or four times . however , decreasing the size of the neighbor node index seems not trivial for large geometries .",
    "simulations for sparse geometries can be also run using codes for dense geometries , usually on multi - gpu clusters due to large memory requirements .",
    "depending on the applied solutions , the memory and performance penalties caused by geometry sparsity can vary .    for the simplest case",
    "all data must be stored and transferred for all nodes ( fluid and solid ) , thus @xmath80 times more memory and transfers are needed .",
    "for example , storing data for all nodes from a geometry with @xmath71 requires ten times more memory than storing data only for fluid nodes .",
    "since fia / cm require about 2@xmath81 more memory per node than implementations for dense geometries , thus the use of code designed for dense geometries to simulate the sparse ones requires about 45@xmath82 more memory when @xmath83 .",
    "the bandwidth overhead of implementation for dense geometries can be greatly reduced through skipping of operations for solid nodes after checking a type of node .",
    "this optimization can reduce the bandwidth overhead to values lower even than for ideal implementations of fia because usually the node type can be encoded on a smaller number of bytes than @xmath79 .",
    "however , this does not take into account the uncoalesced memory transactions resulting from interlacing in memory data for solid and non - solid nodes .",
    "the interesting technique is presented in walberla , the large - scale multi - gpu framework @xcite , where the geometry is divided using the hierarchical structure of `` patches '' composed of `` blocks '' . for sparse geometries empty blocks can be removed reducing memory usage and computational complexity .",
    "though tiles presented in this work use a similar concept , blocks are a higher level data structure designed rather for efficient multi - processor implementations , where a load balancing and a communication affect performance .",
    "the main differences between tiles and blocks are size ( tiles have small , fixed size , additionally limited by hardware parameters , for example warp / memory transaction / cache line size , whereas blocks are much larger , up to more than @xmath84 nodes , to minimize communication overhead ) , additional data ( blocks contain management information ) , data layout ( because of small tile size a special memory layout minimizing the number of uncoalesced transactions is needed ) , and hardware mapping granularity ( blocks are mapped per single process , whereas tiles are processed by separate gpu thread blocks ) .",
    "in our implementation the whole geometry is covered by the uniform mesh of fixed - size tiles .",
    "let @xmath85 denote the number of nodes per tile edge .",
    "the number of nodes per tile is @xmath86 for 2d , and @xmath87 for 3d lattice arrangements .",
    "if a geometry size is not divisible by @xmath85 , then the geometry is extended with solid nodes .",
    "the tiling is implemented by the host code and done once at the geometry load .",
    "we used a very simple algorithm : first , the geometry is covered by the uniform mesh of tiles starting at node ( 0,0 ) , and next the tiles containing only solid nodes are removed .    during a single lbm time step",
    "the tiles can be processed independently and in any order provided that values at the tile edges are correctly propagated .",
    "we implemented two different methods for data at tile edges synchronization .",
    "the first method ( t2c ) uses two copies of @xmath3 data and the _ gather _ pattern , e.g. values from neighbor nodes are accessed during the read stage .",
    "the information about tile placement is stored in an additional _ tilemap _ array with pointers to data for non - empty tiles ( for empty tiles @xmath63 is used ) .",
    "the pointers are used to find neighbor tiles needed during propagation stage .",
    "the _ tilemap _",
    "array is similar to fia but uses the tile granularity what decreases overheads proportionally to the number of nodes per tile .    in the second method ( tgb )",
    "we used only one copy of @xmath3 data and additional _ ghost buffers _ at tile edges .",
    "the ghost buffers contain copies of propagated @xmath3 functions . during the propagation in a single lbm time",
    "step the values from ghost buffers can be read and written at the same time ( and in any order ) . to prevent data races from occurring ,",
    "we used two copies of ghost buffers : one copy is used for the data store , the other copy is used for the data read .",
    "after each lbm time iteration the copies are exchanged ( by pointers exchange ) .",
    "the idea of the two - step propagation is shown in fig .",
    "[ fig_propagation2step ] . for data inside tile we used the _ scatter _ pattern -",
    "the computed values are stored to neighbor nodes . for ghost buffers",
    "the _ gather _ pattern is used .",
    "compared with the minimal requirements defined in section [ sect_performance_model ] , the tiles bring some memory , bandwidth and computational overheads due to : solid nodes inside tiles , additional data for storing information about tiles placement , additional data for storing node type , methods of avoiding race conditions ( either two copies of @xmath3 data or ghost buffers ) , uncoalesced memory transactions due to data layout , computational overhead for addresses calculation , and others .",
    "the reasons for overheads fall into one of the two classes : the overheads caused by the method and the overheads resulting from implementation . in this section",
    "we focus only on the former class . for simplification , we consider only these lattice arrangements that require only a single layer of ghost buffers ( up to d2q9 for 2d and d3q27 for 3d geometries ) .",
    "let @xmath88 denote the average number of solid nodes per tile , and @xmath89 denote the average number of non - solid nodes per tile .",
    "we can define the average tile solidity @xmath90 where @xmath91 is the average tile porosity , @xmath92 $ ] and @xmath93 .",
    "the minimum memory required to store all non - solid nodes from tile is then @xmath94 } \\label{equ_mtile}\\ ] ] and the minimum transfer for tile is @xmath95 } .",
    "\\label{equ_btile}\\ ] ]      the memory overhead for tiles is a sum of four components : the overhead resulting from solid nodes inside a tile @xmath96 , the memory required to store node type @xmath97 , the memory used to avoid race conditions @xmath98 , and the memory needed for storing the additional tile data @xmath99 .    for all nodes in a tile ( independently of the node type )",
    "all @xmath3 values must be stored .",
    "thus the memory overhead resulting from solid nodes in a tile is equal to @xmath100    let @xmath101 denote a number of bytes used to store the node type ( we assume that at least one byte is necessary , despite the fact that for simple cases a few bits may be enough ) .",
    "since the node type is stored for all nodes inside a tile , the overhead caused by the memory required to store the node type is @xmath102    the values of @xmath98 and @xmath99 depend on the method of race condition prevention .",
    "[ [ two - copies ] ] two copies + + + + + + + + + +    in this method an additional copy of all @xmath3 values is needed for each node in a tile .",
    "the memory overhead is then @xmath103    the additional tile datum is only the _ tilemap _ array where one index per each tile ( empty and non - empty ) is stored . in practice ,",
    "the index is @xmath104 byte wide what allows to use @xmath105 non - empty tiles , which require much more memory than is available on current gpus .",
    "let @xmath106 denote the number of all tiles and @xmath107 denote the number of non - empty tiles .",
    "the overhead resulting from memory needed to store _ tilemap _ array is then @xmath108 notice that since @xmath109 even for d2q9 lattice , single precision data and small tiles containing @xmath110 nodes , thus @xmath99 is negligible unless the ratio @xmath111 is large . for geometries used in this work @xmath112 .",
    "the complete memory overhead for two copies of @xmath3 data is then @xmath113 usually the components @xmath97 and @xmath99 can be skipped ( @xmath114 even for d2q9 lattice and single precision data ) , what allows to approximate the memory overhead as @xmath115 .    equation ( [ equ_dm_t2c ] ) contains many parameters , but for concrete implementation most of them are constant . for example , for double precision data ( @xmath116 ) , two bytes per node type ( @xmath117 ) , and @xmath118 the memory overhead is @xmath119 for d2q9 lattice and tiles containing @xmath120 nodes . for d3q19 lattice and @xmath121 nodes per tile @xmath122    [ [ sec_dm_gb ] ] ghost buffers + + + + + + + + + + + + +",
    "the memory needed for ghost buffers at tile edges depends on two factors .",
    "the first is the number of @xmath3 functions propagated in directions toward the faces , the edges , and the corners .",
    "the second is the number of common faces , edges , and corners between neighbor tiles .",
    "for example , even if the current tile has only neighbors contacting either vertexes ( for 2d/3d ) or edges ( for 3d ) , the additional buffers on the edges / faces must be allocated ( see fig .",
    "[ fig_propagation2step]b the black node in the corner ) .",
    "although for tiles contacting vertexes only the single - element ghost buffers could be allocated , to simplify the implementation we assumed that ghost buffers are allocated only in full size ( @xmath123 values per buffer what gives @xmath85 for 2d and @xmath124 for 3d geometries ) .",
    "the performance penalty caused by this simplification is small - for test cases from table [ tab_cases ] in section [ sec_results ] only less than 2% of allocated ghost buffers could have the reduced size .    in practice",
    "some ghost buffers are unnecessary . even for geometries completely filled with tiles , the ghost buffers at geometry edges",
    "are not used .",
    "if the geometry is sparse and some tiles are removed , then the corresponding ghost buffers can be removed as well ( fig .",
    "[ fig_edges ] ) . to take into account these removed ghost buffers , we use an additional , geometry dependent coefficient @xmath125 that is the ratio of allocated ghost buffers to all possible ghost buffers . in general , we define only that @xmath126 ( @xmath125 is equal to @xmath66 for infinite geometries without edges ) . for our test cases",
    "@xmath127 was computed for each case separately after the geometry tiling ( see table [ tab_cases ] ) .    to prevent data races from occurring ,",
    "each pdf @xmath3 needs a set of two ghost buffers : one for a read and one for a write .",
    "let @xmath128 denote the number of @xmath3 functions for which only one set of ghost buffers is needed ( the functions propagated toward the edges in 2d and toward the faces in 3d geometries ) , @xmath129 denote the number of @xmath3 functions for which two sets of ghost buffers are needed ( see fig .",
    "[ fig_propagation2step ] , the functions propagated toward the corners in 2d and toward the edges in 3d geometries ) , and @xmath130 denote the number of functions requiring three sets of ghost buffers ( the functions propagated toward the corners for 3d geometries ) .",
    "for d2q9 lattice we get @xmath131 , for d3q19 lattice the values are @xmath132 , and for d3q27 @xmath133 .",
    "the number @xmath10 of all @xmath3 functions is then @xmath134 ( the `` @xmath56 '' is for the not propagated function @xmath135 ) .",
    "the maximum number of ghost buffers per tile is @xmath136 .",
    "having these relationships , the average ( per tile ) real memory usage for ghost buffers is @xmath137 }      \\label{equ_medge}\\ ] ] where @xmath32 is defined in section [ sect_performance_model ] as the number of bytes for storing a single @xmath3 value . using @xmath138 defined in eqn .",
    "( [ equ_mtile ] ) the memory overhead resulting from ghost buffers is @xmath139 where @xmath140 is a constant defined for given lattice arrangement .",
    "for d2q9 , d3q19 and d3q27 lattice arrangements the value of @xmath141 is @xmath142 , @xmath143 , and @xmath144 .",
    "the last element affecting the memory overhead is the memory needed for storing the additional tile data .",
    "the majority of this memory are pointers to ghost buffers .",
    "since pointers can be replaced with array indexes , in typical cases each pointer ( index ) requires @xmath145 bytes that allow addressing more ghost buffers than it can be stored in current generation gpus . due to",
    "the _ gather _ pattern ( data propagation during the read stage ) , for each tile we need to use @xmath146 pointers to ghost buffers that are written , and @xmath147 pointers to ghost buffers that are read .",
    "the overhead resulting from storing ghost buffer indexes is then @xmath148 where @xmath149 is a constant defined for given lattice arrangement . for the d2q9 , d3q19 and d3q27 lattice arrangements",
    "@xmath150 is equal to 28 , 72 , and 152 indexes per tile .",
    "the complete memory overhead for tiles with ghost buffers is then @xmath151 for double precision data",
    "( @xmath116 ) , two bytes per node type ( @xmath117 ) , and @xmath152 the memory overhead is @xmath153 for d2q9 lattice and tiles containing @xmath120 nodes . for d3q19 lattice and @xmath121 nodes per tile @xmath154 notice",
    "that for the d3q19 lattice arrangement the memory overhead @xmath98 caused by ghost buffers is almost 5 times larger than for d2q9 , mainly due to 4 times smaller tile edge .      in bandwidth",
    "overhead estimation we assume that for each node in each non - empty tile only the node type field must be read ( @xmath101 bytes per node ) .",
    "when a node is solid , no other operations are done , thus no unnecessary transfers of @xmath3 values occur . also , the @xmath3 values for non - solid nodes are transferred from / to either tile or ghost buffers , thus for each node only a minimal amount of @xmath3 data ( see eqn .",
    "( [ equ_bnode ] ) ) is read / written .",
    "this way the bandwidth overhead results only from transfers of the node type field @xmath155 and transfers of additional tile data @xmath156 .    for simplification",
    "we also ignore the fact that some writes of @xmath3 values can be skipped , when the neighbor node is solid .",
    "additionally , since we use the same measure for all non - solid nodes ( fluid and boundary ) , then for the whole geometry each node requires the same amount of transferred data .    the node type field",
    "must be read not only for the current node , but also for all ( @xmath54 ) neighbor nodes of every non - solid node to avoid propagation to / from solid nodes . however , we assume that the node type fields for a single tile are internally buffered in some way ( registers / cache / scratchpad ) , thus for each tile the node type fields are read only once for nodes inside a tile and once for nodes forming a `` halo '' ( 1 node wide ) around the tile .",
    "the value of @xmath155 is then @xmath157 where @xmath158 is the space dimension and @xmath85 is the number of nodes per tile edge .",
    "the value of @xmath156 depends on the method of race conditions prevention .",
    "[ [ two - copies-1 ] ] two copies + + + + + + + + + +    in this method each non - empty tile needs to load pointers to the neighbor tiles required for propagation , thus @xmath54 indexes of @xmath159 bytes are loaded for each non - empty tile .",
    "we do not need to load the index of the current tile because it can be computed in another way .",
    "the bandwidth overhead caused by additional data is then @xmath160 and the complete bandwidth overhead for t2c is @xmath161 for double precision data , two bytes per node type and four bytes per tile index ( @xmath104 ) the bandwidth overhead @xmath162 for d2q9 lattice with @xmath120 nodes per tile and @xmath163 for d3q19 lattice and @xmath121 nodes per tile .",
    "[ [ ghost - buffers ] ] ghost buffers + + + + + + + + + + + + +    for the implementation based on ghost buffers each non - empty tile needs to load @xmath150 indexes of @xmath164 bytes each .",
    "though some indexes could be skipped when all nodes placed on tile face / edge are solid , the detection of such situation seems a complex operation .",
    "moreover , the number of skipped indexes depends on geometry ; in fact , this number may be proportional to @xmath165 , which for cases from table [ tab_cases ] is smaller than @xmath166 . also , fig .",
    "[ fig_ov_bw ] shows that the value of @xmath156 was significantly lower than @xmath155 for all test geometries , thus the decrease of @xmath156 even by 30% results in much lower decrease of total overhead .",
    "therefore , in the below considerations we can safely assume that each non - empty tile always loads all indexes to ghost buffers .",
    "the bandwidth overhead caused by additional data is then @xmath167 and the complete overhead for tgb is @xmath168 for double precision data , two bytes per node type and four bytes per ghost buffer index ( @xmath145 ) the bandwidth overhead @xmath169 for d2q9 lattice with @xmath120 nodes per tile and @xmath170 for d3q19 lattice and @xmath121 nodes per tile .",
    "[ [ burst - transactions - impact ] ] burst transactions impact + + + + + + + + + + + + + + + + + + + + + + + + +    the overhead estimates presented above define the minimal possible overheads obtainable on the ideal machine ( section [ sect_performance_model ] ) . for the real parallel machines the additional overheads appear .",
    "modern machines with dram memory use _ burst _ transactions including an aligned block of at least @xmath171 bytes ( usually @xmath172 ) , therefore always several neighbor values are transferred during a single memory transaction . to achieve the minimal bandwidth overhead",
    "the node data must be placed in memory in a way that allows to fully utilize all memory transactions .",
    "since this may be difficult , we also consider a bandwidth overhead for implementation with suboptimal memory layout .    in the extreme case ,",
    "each read / write of a value can induce a separate @xmath171byte wide transaction .",
    "however , due to the spatial and temporal locality of data references , it seems reasonable to assume that the maximum transfer per tile contains , in addition to the node type fields and the additional tile data , all @xmath3 values from tile and all @xmath3 values from ghost buffers ( for tiles with ghost buffers ) .",
    "we should also take into account that for ghost buffers containing the values from tile corners only a single transaction is needed to read a single @xmath3 value ( writes of single corner values does not occur ) .",
    "notice that for 3d tiles we assume that even for ghost buffers containing only a single edge of values ( @xmath85 values ) the full ghost buffer with @xmath124 values is transferred because it does not require special data arrangement .",
    "the bandwidth overhead resulting from transfers of full tile data is @xmath173    let @xmath174 denote the number of corner values that are read in separate , @xmath175 byte transactions ( @xmath176 for 2d and @xmath177 for 3d lattice arrangements ) .",
    "the additional bandwidth resulting from transfers of all ghost buffers contains two components : the transfers of all ghost buffers except the ones containing only single , corner value @xmath178 and the reads of all corner values @xmath179 in real geometries some ghost buffers are not needed , thus the maximum transfer should be scaled by a coefficient @xmath180 ( similar to @xmath125 ) defined as a ratio of actually transferred ghost buffer values to the maximum number of transferred ghost buffer values . notice that @xmath181 slightly differs from @xmath125 because for tile the read ghost buffers differ from the written ghost buffers . in our sparse 2d geometries",
    "@xmath181 was slightly lower than @xmath125 ( @xmath182 ) .",
    "the bandwidth overhead including burst transactions impact is then @xmath183 for tiles with two copies and @xmath184 for tiles with ghost buffers .",
    "both values defined in eqn . ( [ equ_db_t2cbt ] ) and ( [ equ_db_tgbbt ] ) may be treated as the pessimistic estimates of bandwidth overhead .",
    "* load * node type * load * all @xmath3 load ghost buffers process boundary nodes * store * all @xmath3 collide store ghost buffers * scatter * all @xmath3    * load * copy of tile bitmap * load * node types from current tile * load * node types from neighbor tiles * barrier * * load * @xmath135 * compute address * of neighbor node in direction @xmath1 * gather * @xmath3 from neighbor node process boundary collide * store * all @xmath3    the code was written in cuda c language ver .",
    "7.5 , the first version with official support for many c++11 features .",
    "the support of c++11 allowed us to design a heavily templated generic kernel code that can be specialized for fluid and collision models , tile size , and enabled optimizations .",
    "the main disadvantage of this solution is a long compilation time .    in our implementation",
    "a single gpu thread block is responsible for processing of a single tile what allows for easy synchronization of computations within the tile .",
    "consecutive gpu threads are assigned to nodes from tile using row order . to minimize the number of memory transfers , we combined collision , propagation ( streaming ) and boundary computations for a single node into a single gpu kernel .",
    "the node data ( @xmath185 , node type ) are transferred only once and local copies are stored in registers and in the shared memory .",
    "figures [ fig_kernel_2d ] and [ fig_kernel_3d ] show the two versions of the kernel .",
    "we started from the version of tgb for d2q9 lattice arrangement and @xmath120 nodes per tile .",
    "then we prepared the version that uses t2c for d3q19 lattice arrangement and @xmath121 nodes per tile , where we also fixed issues observed in the d2q9 version .",
    "the main difference between tgb and t2c implementations is in the code responsible for propagation step .",
    "the propagation between two nodes is done only when the target and source nodes are not solid . in both implementations we could use the node type fields for nodes inside the tile , but nodes from neighbor tiles had to be treated in a different way .",
    "for tgb a partial information about the node type for nodes from neighbor tiles is stored in the values put into the ghost buffers - if the node corresponding to the given value in the ghost buffer is solid , then the value in the ghost buffer is set as the predefined not a number ( nan ) . in t2c version",
    "the node type fields from neighbor tiles are tested directly . in both versions we used shared memory to store either ghost buffers propagated horizontally ( line 5 in fig .",
    "[ fig_kernel_2d ] ) or node types from current and neighbor tiles ( lines 23 in fig .",
    "[ fig_kernel_3d ] ) .",
    "this allows minimizing the memory bandwidth usage caused by multiple reads of the same values and uncoalesced memory accesses .",
    "however , since the number of either ghost buffers or node type fields for current tile with an additional halo does not map well to gpu threads , we used a warp level programming ( wlp ) to balance the load of warps assigned to a tile .",
    "though wlp requires additional synchronization in the kernel , it is amortized with interest - in the kernels without wlp both the performance and the real gpu utilization were lower . the unfavorable issue of the wlp based solution is a low code portability , because each change of the tile size , thread to nodes mapping , or warp size requires a new version of the propagation code .",
    "besides the method of providing information about node types from neighbor tiles , the second difference between the tgb / t2c implementations of propagation is the data access pattern . in the tgb version",
    "the propagation is implemented as a scattering data to neighbor nodes and is additionally partitioned into two separated steps because the propagation of @xmath3 values for nodes at tile edges requires a sequence of writes and the following reads to / from ghost buffers ( see fig .",
    "[ fig_propagation2step ] ) .",
    "notice that both these propagation steps must be done before the collision , and that the synchronization between the first and the second step is required to prevent data races for nodes at tile edges ( because the tiles are processed in parallel ) .",
    "thus , our single iteration starts with the second step of propagation ( line 5 in fig .",
    "[ fig_kernel_2d ] ) and performs the first propagation step at the iteration end ( lines 1316 in fig .",
    "[ fig_kernel_2d ] ) .    additionally , for the propagation done as the data scattering some @xmath3 values can be not correctly updated from neighbor nodes ( for example when these nodes are solid ) . to avoid this , lines 810 in fig .",
    "[ fig_kernel_2d ] are responsible for writing to global memory the values for these @xmath3 functions that will not be overwritten during propagation .    for the t2c kernel we implemented the propagation as a gathering data from neighbor nodes .",
    "this allowed us to simplify the code because no equivalent of lines 810 from fig .",
    "[ fig_kernel_2d ] was needed . due to a lack of accesses to ghost buffers , we could also remove synchronization points during writes ( all writes are only to nodes within a tile and are perfectly coalesced ) , what allowed for much simpler , regular code with the single barrier at line 4 .    the last important difference between kernels",
    "tgb and t2c is in the neighbor node address computations ( line 8 in fig .",
    "[ fig_kernel_3d ] , not shown in fig . [ fig_kernel_2d ] ) . in the tgb version",
    "the address of neighbor node data depends only on a location of the current node and can be determined based only on current tile data .",
    "the t2c version additionally needs locations of neighbor tiles , thus the two - step procedure is needed .",
    "first , the index of the tile containing the neighbor node is computed - we used the values from @xmath186 due to the local copy of the tile bitmap ( created in line 1 ) .",
    "then , when the neighbor tile is not empty , a few indexes for the neighbor node are computed since we need both information about the neighbor node type ( this is gathered from the copy in shared memory ) and the value of proper @xmath3 ( which is stored in global memory ) .",
    "we also applied a set of widely know optimizations : loop unrolling to increase instruction level parallelism due to interlacing of independent streams of instructions , using shared memory to decrease register pressure , tuning the usage of registers for each specialization of the kernels , minimizing cost of divergent branches by placing time - consuming memory accesses outside the divergent code , replacing divisions with multiplications by inverse , avoiding transfers of @xmath187 values , and the use of inline methods and compiler pragmas to connect clean , structured code with high performance . additionally , for the t2c kernel we used an optimized data layout in memory for each array of @xmath3 values to minimize the number of uncoalesced memory transactions .",
    "after this optimization , the real number of memory transactions for each completely filled tile was only about 1% higher than the minimum value defined by eqn .",
    "( [ equ_btile ] ) due to a minimized number of uncoalesced reads and an increased cache utilization .",
    "the address calculations are done by _ constexpr _ methods .",
    "all simulations were run in double precision on a computer with the nvidia gtx titan device ( kepler architecture ) clocked at 823 mhz with 6 gb gddr5 memory clocked at 3.004 ghz , intel i7 - 3930k cpu and 64 gb 4-channel ddr3 dram .",
    "simulations for d3q19 lattice arrangement were done using the t2c kernel , for d2q9 we used the tgb kernel .",
    "the simulations were run for dense and sparse 2d and 3d geometries .",
    "the dense geometries consisted of a fluid flow in a square chamber with a moving lid .",
    "the sparse geometries for the d3q19 lattice arrangement were prepared on the base of cases similar to presented in @xcite ( three arrays of randomly arranged spheres with the diameter of 40 lattice units and the blood flow in the cerebral aneurysm ) and @xcite ( aorta with coarctation ) . for the d2q9 lattice arrangement we used the microvascular structures from @xcite ( fig .",
    "[ fig_case_chipb ] chipa ) and @xcite ( chipb ) .",
    "the parameters of the cases are shown in table [ tab_cases ] .",
    "the tile size is @xmath120 nodes for 2d and @xmath121 nodes for 3d .",
    "the ratio of boundary to all non - solid nodes varies from @xmath188 ( for the cerebral aneurysm ) up to @xmath57 ( for ras_0.7 ) .",
    "the porosity of geometries varies between @xmath189 and @xmath190 .",
    "[ tab_cases ]          the value of the most important tile parameter , the tile solidity @xmath191 , depends mainly on other factors than the geometry solidity @xmath192 .",
    "this is especially visible for 2d cases where different values of @xmath191 are obtained for the same @xmath192 . also , for the aneurysm and coarctation cases @xmath191 becomes high despite the low @xmath192 .",
    "[ fig_chip2d_params ] shows tile parameters as a function of the number of nodes per channel width .",
    "only chipa is analyzed , but for chipb the values are similar .",
    "the value of @xmath191 greatly depends on the channel width : @xmath191 becomes larger than @xmath193 for 27 node wide channels ( @xmath194 tile edge ) and larger than @xmath195 for 39 node wide channels ( @xmath196 tile edge ) .",
    "the values of @xmath125 and @xmath181 are between @xmath197 and @xmath198 .",
    "both coefficients are close to each other ( the difference was below @xmath199 ) and @xmath181 is usually lower than @xmath125 .",
    "the comparison of memory overhead estimates for sparse test geometries is shown in table [ tab_cases ] and fig .",
    "[ fig_ov_mem ] , which also shows the components of both @xmath200 and @xmath201 and the values of @xmath43 for `` ideal '' implementations of cm and fia .",
    "the results show that for 2d geometries with a high tile solidity the tiles with ghost buffers allow for significant reduction of memory usage .",
    "in other cases the memory reduction is more difficult to achieve .",
    "though for d3q19 lattice arrangement t2c has the memory overhead similar ( about 2030% higher ) to tgb , for d2q9 the memory overhead for t2c is 2.53.4 times higher than for tgb .",
    "this results from a low overhead for tgb due to large tiles and a small number of ghost buffers needed for d2q9 , and allows achieving the lowest memory overhead for all tested 2d geometries . for both chipa_32 and chipb_32 geometries",
    "the total memory usage for fia / cm methods is 1.61.7 times higher than for tgb ( @xmath202 and @xmath203 ) .",
    "less favorable results are for d3q19 lattice arrangement - the memory overhead for tiles is either close to or higher than for fia / cm .",
    "tgb has the lowest memory overhead only for the geometries with the high tile solidity ( aneurysm and coarctation ) , and the gains are much lower than for d2q9 .",
    "memory overhead for t2c is always higher than for fia .",
    "compared with cm , t2c requires less additional memory only for the aneurysm case .    for all 3d cases , and for t2c on d2q9 lattice , the majority of the tile memory overhead is @xmath98 ( either the second copy of @xmath3 or ghost buffers ) . only for tgb on d2q9 lattice",
    "the memory overhead is close to @xmath96 , which can be treated as an overhead for `` ideal '' implementation of tiles .",
    "removal of @xmath98 could significantly decrease memory requirements for tiles to values lower than for implementations of cm / fia .",
    "however , if we compare @xmath96 with overheads for `` ideal '' implementations of cm / fia , the tiles have lower memory usage only for cases with high tile solidity , and the gains are small .     and @xmath99 .",
    "]      the comparison of memory bandwidth overhead estimates for sparse test geometries is shown in table [ tab_cases ] and fig .",
    "[ fig_ov_bw ] . for all tested geometries the overhead estimates for both versions of tile processing",
    "are significantly lower than for cm / fia , up to @xmath204 than cm and up to @xmath205 than fia . also , the overhead estimates for tiles are lower than the ideal estimates for cm / fia for all cases but ras_0.7 .",
    "the bandwidth overhead estimates for the ideal implementation for tiles ( equal to @xmath155 ) are always lower than the ideal estimates for cm / fia .",
    "these results show that the tiles allow achieving the highest performance for sparse geometries .",
    "the good implementations for tiles could allow achieving the performance for presented cases only a few percent lower than the peak performance for dense geometries .     for all cases .",
    "]    the bandwidth overhead for tiles is determined by two components : @xmath155 and @xmath156 . in all cases",
    "@xmath156 is smaller than @xmath155 , thus performance optimizations should focus on minimizing transfers of the node type fields .",
    "however , since for all test cases the overall bandwidth overhead is lower than 0.1 ( usually close to 0.05 ) , it can be difficult to decrease it further .",
    "[ tab_perf_dense ]    to provide some measure allowing for comparison of different implementations for different lattice arrangements and on different machines , we calculated the number of bytes transferred per node from eqn .",
    "( [ equ_bnode ] ) and the minimum required memory bandwidth to achieve the reported performance @xmath206 [ mlups ] as @xmath207 . for sparse geometries we took into account the transfers only for non - solid nodes .",
    "next , we compared this value with the maximum theoretical gpu bandwidth @xmath208 and calculated a bandwidth utilization @xmath209 shown in table [ tab_perf_dense ] .",
    "this measure is based on @xcite and is similar to measures used in many papers ( for example @xcite , @xcite , @xcite ) . due to its strict assumptions , it shows how the given implementation is close to the ideal one for bandwidth bound codes . according to estimations shown in section [ sect_performance_model ] , the lbm implementations on gpu are bandwidth bound except either the implementations of complex models on the previous generations gpus or the lattice arrangements with a high number of links @xcite .",
    "notice only that we used the theoretical maximum of memory bandwidth instead of the measured .",
    "performance comparison of different lbm implementations for dense geometries is shown in table [ tab_perf_dense ] .",
    "we omitted results when performance was limited by the low computational power of the gpu , for example double precision computations on gtx280 @xcite , which resulted in @xmath210 .    for d3q19 lattice arrangement and double precision , only @xcite had @xmath211 higher @xmath212 than the tile - based solution .",
    "our implementation had @xmath212 better than @xcite ( @xmath211 ) and than @xcite ( @xmath213 ) .",
    "also many implementations for single precision had lower @xmath212 than our version .",
    "since we compared our solution designed for sparse geometries with highly optimized implementations for dense geometries ( without overheads ) , these results show a low additional cost imposed by tiles .",
    "notice that performance of our implementation is decreased not only by bandwidth overhead defined in eqn .",
    "( [ equ_db_t2c ] ) for @xmath214 , but also by uncoalesced memory transactions resulting from data placement inside tiles .",
    "the results for d2q9 lattice arrangement were less favorable for our implementation . despite the higher ratio of bandwidth to computations ( section [ sect_performance_model ] ) and the lower register pressure due to about two times smaller size of data per node ,",
    "the tile based implementation for d2q9 had up to @xmath215 lower @xmath212 than for d3q19 lattice .",
    "this low performance results from many synchronization points and a high complexity of code for ghost buffers handling . after removing the barriers the performance for d2q9 lattice and bgk incompressible model increased to 1300 mlups ( @xmath216 ) .",
    "when we completely removed the code responsible for ghost buffers handling , the performance was 1615 mlups ( @xmath217 ) .",
    "the second reason of a low performance is a lack of optimized memory layout in kernels for the d2q9 lattice .",
    "table [ tab_perf_dense ] shows also an additional , interesting phenomena : though in theory the floating point calculations should be completely masked by memory transfers @xcite , the performance of our code decreased for more complex collision and fluid models .",
    "this results from explicit barriers in our code - after removing all synchronization points from the d2q9 bgk - incompressible kernel the performance was the same for versions with and without collision computations .",
    "the decrease of performance for more complex models is also caused by a lower gpu occupancy resulting from increased register usage ( especially visible for mrt on d3q19 lattice ) .",
    "performance comparison for sparse 3d geometries is shown in table [ tab_perf_sparse_3d ] .",
    "for the cm method we placed only results from @xcite , which are for optimized implementation of @xcite on more recent hardware .",
    "[ tab_perf_sparse_3d ]    the results confirm theoretical analysis showing that the performance of our solution strongly depends on the tile solidity .",
    "for the geometry with low tile solidity ( @xmath218 for ras_0.9 ) our implementation achieved @xmath212 13% lower than @xcite .",
    "however , when the tile solidity increased , the @xmath212 value for the tile - based solution became significantly higher than for @xcite ( up to @xmath219 for the aneurysm case ) .",
    "although for the aneurysm case we compared our single - gpu implementation with the multi - gpu version from @xcite , which requires additional communication , the communication overhead in @xcite is low ( for single precision and large geometries @xmath212 for @xmath220 tesla c1060 version is usually @xmath221 of single gpu @xmath212 , e.g. 1016 vs 288 mlups ) .",
    "even if we assume that the single - gpu version of @xcite achieves @xmath222 higher @xmath212 , the value of @xmath212 for our implementation is still @xmath223 higher .",
    "moreover , for the aneurysm we compared our double precision implementation ( more difficult to achieve high performance ) with the single precision version from @xcite .",
    "the values from tables [ tab_perf_dense ] and [ tab_perf_sparse_3d ] show that the performance penalty for sparse geometries is much higher than it is apparent from eqn .",
    "( [ equ_db_t2c ] ) .",
    "it results , inter alia , from an additional memory traffic due to burst transactions partially filled with data for non - solid nodes . in practice",
    ", the bandwidth overhead for tiles with high porosity is between values defined in eqn .",
    "( [ equ_db_t2c ] ) and ( [ equ_db_t2cbt ] ) .",
    "it is especially visible for the ras_0.9 case where despite the low bandwidth overhead shown in fig .",
    "[ fig_ov_bw ] the real performance for t2c is low , but much higher than it results from @xmath224 .",
    "these observations show that the performance of our code could be improved by a better memory layout tuned for porous tiles .",
    "in this paper , the gpu based lbm implementation for fluid flows in sparse geometries was presented . in contrary to the previous propositions for sparse geometries , which have been using the indirect node addressing , our solution covers the geometry with the uniform mesh of square tiles .",
    "two implementations were shown : tiles with ghost buffers ( tgb ) and a single copy of probability distribution functions ( pdf ) , and tiles with two copies of pdf values ( t2c ) .",
    "for t2c we achieved up to 682 mlups ( @xmath225% utilization of maximum theoretical memory bandwidth ) on gtx titan for d3q19 lattice , double precision data , and bgk incompressible model .",
    "for the presented method we provided the detailed theoretical model that allows to analyze the memory and bandwidth overheads for different tile sizes , machine constraints ( e.g. memory transaction size ) and geometry layouts ( defined for example by factors describing the solidity of both tiles and geometry ) .",
    "the model was used to compare the overheads introduced by tiles with overheads of other sparse lbm implementations .",
    "it can also be used to determine the  quality  of implementation and to find code inefficiencies through the comparison of real performance with theoretical limits .    based on theoretical estimates , for presented cases tiles with two copies required only 25% more data traffic than implementations for dense geometries .",
    "this is a significant ( up to @xmath204 over cm and up to @xmath205 over fia ) reduction of an amount of ancillary data transfers required to handle geometry sparsity .",
    "although real implementation brings additional memory transfers due to uncoalesced transactions , for d3q19 lattice arrangement tiles with two copies allowed to achieve up to @xmath226 higher device memory bandwidth utilization than cm technique ( for the aneurysm case ) and up to @xmath227 higher than fia ( for the coarctation case ) .",
    "the performance of our implementation was close even to the fastest implementations for dense geometries - we achieved 85% of bandwidth utilization from @xcite .",
    "tiles can also reduce memory usage compared with fia / cm implementations , but large gains are possible only for the lattice arrangements with a small number of links ( d2q9 ) and the tiles with ghost buffers . for our d2q9 test cases the estimated memory usage for fia / cm techniques was up to 1.61.7 times higher than for tiles with ghost buffers .",
    "in other cases , the use of tiles brought small reductions of memory usage only when tiles contained a small number of non - solid nodes .",
    "moreover , in many cases tiles with 2 copies of data required more memory than cm / fia due to solid nodes inside tiles .    for 3d geometries tiles with",
    "ghost buffers had memory overhead similar to tiles with two copies .",
    "since ghost buffers require more complex and slower code than two copies , thus for 3d geometries the t2c version is usually a better solution .",
    "the main drawbacks of the presented method are a rapid decrease in performance and increase of memory usage when the tiles contain solid nodes , and a complex code , especially for tiles with ghost buffers , which require many synchronization points hindering performance .",
    "future work includes multi - gpu version that will allow to quickly simulate the large geometries .",
    "we are also going to improve performance and memory usage by memory layouts tuned for tiles with high porosity and better tiling algorithms , which should allow to achieve a higher tile solidity .",
    "authors acknowledge the financial support from the national science centre grant no .",
    "n n501 042140 and from the wroclaw university of science and technology , faculty of electronics , chair of computer engineering statutory funds .      m.  j. mawson and a.  j. revell , `` memory transfer optimization for a lattice boltzmann solver on kepler architecture nvidia gpus , '' _ computer physics communications _",
    "185 , no .",
    "2566  2574 , 2014 .",
    "p.  bailey , j.  myre , s.  walsh , d.  lilja , and m.  saar , `` accelerating lattice boltzmann fluid flow simulations using graphics processors , '' in _ parallel processing , 2009 .",
    "international conference on _ , sept 2009 , pp .",
    "550557 .    c.  obrecht , f.  kuznik , b.  tourancheau , and j .- j .",
    "roux , `` a new approach to the lattice boltzmann method for graphics processing units , '' _ computers & mathematics with applications _",
    "61 , no .  12 , pp . 3628  3638 , 2011 .",
    "j.  habich , c.  feichtinger , h.  kstler , g.  hager , and g.  wellein , `` performance engineering for the lattice boltzmann method on gpgpus : architectural requirements and performance results , '' _ computers & fluids _ , vol .",
    "80 , no .",
    "276  282 , 2013 .              c.  feichtinger , j.  habich , h.  kstler , g.  hager , u.  rde , and g.  wellein , `` a flexible patch - based lattice boltzmann parallelization approach for heterogeneous gpu  cpu clusters , '' _ parallel computing _ , vol .",
    "37 , no .  9 , pp . 536  549 , 2011 .    c.  feichtinger , j.  habich , h.  kstler , u.  rde , and t.  aoki , `` performance modeling and analysis of heterogeneous lattice boltzmann simulations on cpu ",
    "gpu clusters , '' _ parallel computing _ , vol .",
    "46 , pp . 1  13 , 2015 .",
    "m.  bernaschi , m.  fatica , s.  melchionna , s.  succi , and e.  kaxiras , `` a flexible high - performance lattice boltzmann gpu code for the simulations of fluid flows in complex geometries , '' _ concurr .",
    "_ , vol .  22 , no .  1 ,",
    "114 , jan . 2010 .    c.  huang , b.  shi , z.  guo , and z.  chai , `` multi - gpu based lattice boltzmann method for hemodynamic simulation in patient - specific cerebral aneurysm , '' _ communications in computational physics _",
    "17 , no .  4 , pp . 960974 , 2015 .",
    "r.  d.  m. travasso , e.  corvera  poir , m.  castro , j.  c. rodrguez - manzaneque , and a.  hernndez - machado , `` tumor angiogenesis and vascular patterning : a mathematical model , '' _ plos one _ , vol .  6 , no .  5 , p. e19989 , may 2011 .    k.  y. lin , m.  maricevich , n.  bardeesy , r.  weissleder , and u.  mahmood , `` in vivo quantitative microvasculature phenotype imaging of healthy and malignant tissues using a fiber - optic confocal laser microprobe , '' _ translational oncology _ , vol .  1 , no .  2 , p. 8494",
    ", jul . 2008 .",
    "e.  calore , a.  gabbana , j.  kraus , s.  f. schifano , and r.  tripiccione , `` performance and portability of accelerated lattice boltzmann applications with openacc , '' _ concurrency and computation : practice and experience _ , vol .",
    "28 , no .  12 , pp .",
    "34853502 , 2016 , cpe.3862 .",
    "tadeusz tomczak received m.s . and ph.d .",
    "degrees in computer science all from the institute of computers , control , and robotics , wroclaw university of technology , poland , in 2002 and 2007 respectively .",
    "his research interests include residue number systems , fast computational hardware and parallel computing .",
    "roman grzegorz szafran received m.s . and",
    "degrees in chemical engineering all from the faculty of chemistry , department of chemical engineering , wroclaw university of technology , poland , in 1999 and 2004 respectively .",
    "his research interests include cfd / lattice - boltzmann modeling of flow hydrodynamics in microfluidic devices , bio - mems , multiphase flows , coating and encapsulation ."
  ],
  "abstract_text": [
    "<S> we describe a high - performance implementation of the lattice - boltzmann method ( lbm ) for sparse geometries on graphic processors . in our implementation </S>",
    "<S> we cover the whole geometry with a uniform mesh of small tiles and carry out calculations for each tile independently with a proper data synchronization at tile edges . for this method </S>",
    "<S> we provide both the theoretical analysis of complexity and the results for real implementations for 2d and 3d geometries . </S>",
    "<S> based on the theoretical model , we show that tiles offer significantly smaller bandwidth overhead than solutions based on indirect addressing . for 2-dimensional lattice arrangements a reduction of memory usage is also possible , though at the cost of diminished performance . </S>",
    "<S> we reached the performance of 682 mlups on gtx titan ( 72% of peak theoretical memory bandwidth ) for d3q19 lattice arrangement and double precision data .    </S>",
    "<S> gpu , cuda , lbm , cfd , parallel computing </S>"
  ]
}