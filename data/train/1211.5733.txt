{
  "article_text": [
    "consider a normal distribution with zero mean and an unknown covariance matrix , @xmath2 let denote the eigenvalues of @xmath3 by @xmath4 and eigenvectors matrix by @xmath5 , hence we have the spectral decomposition @xmath6 where @xmath7 means the diagonal matrix with the @xmath8th diagonal element @xmath9 .",
    "it is needless to say that inference on @xmath3 is an important task in many practical situations in such a diversity of fields as engineering , biology , chemistry , finance , psychology etc .",
    "especially we often encounter the cases where the property of interest depends on @xmath3 only through its eigenvalues @xmath10 .",
    "we treat an inference problem on the eigenvalues in this paper .",
    "geometrically viewed , the family of normal distributions @xmath11 is taken as a manifold ( say @xmath12 ) with a single coordinate system @xmath3 .",
    "hence , @xmath12 is identified with the space of symmetric positive definite matrices . geometrically analyzing the space of symmetric positive definite matrices is an interesting topic in a mathematical or engineering point of view .",
    "for example , see smith @xcite , fletcher and joshi , , lenglet et .",
    "al . @xcite . from a statistical point of view",
    ", it could give us some new insights for the inference on @xmath3 . in this paper , we analyze @xmath12 from the standpoint of information geometry while focusing on the inference on the eigenvalues of @xmath13 the two kinds of submanifolds play an important role , that is , a submanifold given by fixed eigenvectors of the covariance matrix and the one given by fixed eigenvalues .    based on independent @xmath14 samples @xmath15 from this distribution , we want to make inference on the unknown @xmath10 .",
    "it is well - known that the product - sum matrix @xmath16 is sufficient statistic for both unknown @xmath10 and @xmath5 .",
    "the spectrum decomposition of @xmath17 is given by @xmath18 where @xmath19 are the eigenvalues of @xmath17 , and @xmath20 is the corresponding eigenvectors matrix .",
    "this decomposition gives us two statistics available , i.e. the sample eigenvalues @xmath21 and the sample eigenvectors @xmath20 .",
    "however it is almost customary that we only use the sample eigenvalues , discarding the information contained in @xmath20 . in the past literature on the inference for the population eigenvalues ,",
    "every notable estimator is based simply on the sample eigenvalues .",
    "here we just mention the literature in the statistical field that deals with the estimation of @xmath10 under the situation @xmath22 ; stein @xcite , takemura @xcite , dey and srinivasan , haff @xcite , yang and berger for orthogonally invariant estimators of @xmath3 ; dey @xcite , hydorn and muirhead , jin @xcite , sheena and takemura for a direct estimator of @xmath10 .    in a sense",
    "it is natural to implicitly associate the sample eigenvalues to the population eigenvalues , and the sample eigenvectors to the population counterpart .",
    "however the sample eigenvalues is not sufficient for the unknown population eigenvalues .",
    "therefore it is important to evaluate how much information is lost by neglecting the sample eigenvectors .",
    "following amari @xcite , we gain an understanding of the asymptotic information loss with geometric terms such as fisher information metric and embedding curvatures .",
    "another statistically interesting topic is the bias of @xmath21 .",
    "it is well known that @xmath21 is largely biased and the estimators mentioned above are all modification of @xmath21 to correct the bias , that is , `` shrinkage estimators . ''",
    "we show that the bias is closely related to the embedding curvatures .",
    "moreover the geometric structure of @xmath12 naturally leads us to new estimators , which are also shrinkage estimators .",
    "we briefly introduce the order of the contents in this paper . in section [ section : riemannian_manifold_and_metric ] and",
    "[ section : embedding_curvature ] , we treat respectively fisher information metrics and embedding curvatures , and present their specific forms related to the eigenvalues inference . in section [ section : embedding_curvature ] , we refer to the bias of @xmath21 in relation to the curvatures . in section [ section : information_loss ] , we get the explicit form of the asymptotic information loss caused by discarding the sample eigenvectors in the first and second orders w.r.t . the sample size . in the final section ,",
    "we propose the new estimators which is naturally derived in a geometrical view .",
    "all the proofs of the propositions are collected in appendix .    for geometrical concepts used in this paper ,",
    "refer to amari @xcite , amari and nagaoka .",
    "the density of the normal distribution @xmath11 is given by @xmath23 if we let @xmath24 and @xmath25 denote the @xmath26 element of respectively @xmath3 and @xmath27 , then the log likelihood equals @xmath28 where @xmath29 and @xmath30 are given by @xmath31 and @xmath32 the summations in the equation is an abbreviation for @xmath33 and @xmath34 , and we will use these kinds of notations hereafter without mention .",
    "the expression gives natural coordinate system @xmath35 of the manifold @xmath12 as an full exponential family .",
    "another coordinate system , so called expectation parameters , are also useful , which are defined as ; @xmath36    for the analysis of the information carried by @xmath21 and @xmath20 , we need to prepare another coordinate system .",
    "the matrix exponential expression of an orthogonal matrix @xmath37 is given by @xmath38 where @xmath39 is the @xmath40-dimensional unit matrix , @xmath41 is a skew - symmetric matrix and parametrized by @xmath42 as @xmath43 the function @xmath44 is diffeomorphic , and @xmath45 gives `` normal coordinate '' for the group of orthogonal matrices .",
    "we can use this coordinate as local system around @xmath39 and construct an atlas for the entire space of p - dimensional orthogonal matrices ( note this space is compact ) ; for each @xmath5 , there exists an open neighborhood and some open ball @xmath46 in @xmath47 around the origin such that these spaces are diffeomorphic by the function @xmath48 on @xmath46 .",
    "we will use @xmath49 as the third coordinate system of @xmath12 and call it `` spectral coordinate ( system ) '' .",
    "notice that this coordinate system is associated with the following submanifolds in @xmath12 .",
    "if we fix @xmath5 in , then we get a submanifold @xmath50 embedded in @xmath12 with a coordinate system @xmath10 .",
    "this is a subfamily in @xmath11 and called curved exponential family .",
    "its log - likelihood is expressed , as we emphasize it as a function of @xmath10 , to be @xmath51 on the contrary , if we fix @xmath10 in , we get another submanifold @xmath52 in @xmath12 , whose coordinate system is given by @xmath45 in a neighborhood of each point of @xmath52 .",
    "its log - likelihood expression is given by @xmath53    first we consider a metric , that is , a field of symmetric , positive definite , bilinear form on @xmath12 .",
    "the statistically most natural metric is fisher information metric .",
    "suppose @xmath54 is a parametric family of probability density functions , whose coordinate as a manifold is given by @xmath55 .",
    "then the @xmath26 component of fisher information metric with respect to @xmath56 is given by @xmath57.\\ ] ] for the multivariate normal distribution family , @xmath58 ( @xmath59 , the mean parameter is also included ) , skovgaard @xcite gives a clear form of fisher information metric .",
    "the tangent vector space at a fixed point @xmath3 w.r.t .",
    "@xmath60 coordinate can be identified with the space of symmetric matrices . for any symmetric matrix @xmath61 , @xmath62 , the metric with respect to the @xmath63 coordinate system",
    "is given by @xmath64    we are interested in fisher information metric with respect to the spectral coordinate @xmath65 .",
    "let @xmath66 denote the tangent vectors w.r.t .",
    "the @xmath10 coordinate , @xmath67 denote the tangent vectors w.r.t .",
    "the @xmath45 coordinate .",
    "namely @xmath68 these tangent vectors ( exactly speaking , vector fields ) are invariant with respect to the orthogonal transformation of @xmath3 ; for some orthogonal matrix @xmath37 , the orthogonal transformation @xmath69of @xmath12 is defined as @xmath70 for any @xmath71 @xmath72    [ metric_spectra ] let @xmath73 denote fisher information metric based on @xmath74 , then the components of the metric with respect to @xmath49 is given as follows ; @xmath75 @xmath76 equals one if the logic inside the parenthesis is correct , otherwise zero .",
    "there are two remarkable properties of the metric for the spectral coordinate .",
    "first since the metric components matrix is diagonal , @xmath77 is an orthogonal coordinate system , especially that the submanifolds @xmath50 and @xmath52 are orthogonal to each other for any @xmath10 and @xmath5 .",
    "second it is independent of @xmath5 , hence the metric is invariant with respect to the orthogonal transformation @xmath69 in for any orthogonal matrix @xmath78 ( second property is instantly derived from the expression . )    the fundamental structure of @xmath12 is determined by an affine connection , and it gives birth to curvature , geodesics , etc . as it is seen in amari @xcite , @xcite , amari and nagaoka , @xmath0-connection and @xmath1-connection is important for the analysis of statistical exponential family , but we do nt make further reference here .",
    "all results related to levi - civita connection for @xmath12 can be found in skovgaard @xcite .",
    "for @xmath79-connection for the multivariate normal family with zero mean , refer to eguchi @xcite .",
    "a family of multivariate normal distributions as a riemannian manifold is studied by several authors .",
    "refer to fletcher and joshi , lenglet et .",
    "@xcite and lovri et .",
    "al . @xcite .",
    "we just mention the fact that @xmath12 is @xmath0-flat and @xmath1-flat , and corresponding affine coordinations are given respectively by @xmath80 and @xmath81 .",
    "for the analysis of the asymptotic distribution @xmath82 , it is important to observe the embedding curvatures of @xmath83 and @xmath84 .",
    "( see amari and kumon , kumon and amari . )",
    "especially , asymptotic information loss can be explained in view of these curvatures .",
    "embedding curvature shows how the submanifold @xmath83 or @xmath84 is placed in the manifold @xmath85 we consider the following embedding curvatures ;    \\1 .",
    "embedding curvature of @xmath83 with respect to @xmath0-connection or @xmath1-connection .",
    "@xmath86 where @xmath87 is the covariant derivative of @xmath88 in the direction of @xmath89 with respect to @xmath0-connection .",
    "@xmath90 is similarly defined .",
    "embedding curvature of @xmath84 with respect to @xmath1-connection @xmath91 where @xmath92 is the covariant derivative of @xmath93 in the direction of @xmath94 with respect to @xmath1-connection .    on these curvatures at the point @xmath95 , we have the following results .    [ embedding_c ] for @xmath96 @xmath97 for @xmath98 @xmath99    another expression of the embedding curvature of @xmath84",
    "is given by @xmath100 with this notation , the orthogonal projection of the covariant derivative @xmath101 onto the tangent space of @xmath83 is given by @xmath102 from proposition [ metric_spectra ] , [ embedding_c ] , we have @xmath103 hence @xmath104 an embedding curvature has full information about the `` extrinsic curvature '' of the embedded submanifold in any direction .",
    "sometimes it is convenient to compress it into a scalar measure of the curvature .",
    "`` statistical curvature '' by efron ( see efron @xcite , murray and rice ) is such a measure ; for @xmath84 , it is defined by ( see p159 of amari @xcite ) @xmath105 which attains the following value at the point @xmath95 .",
    "@xmath106    from these results , we notice that if @xmath12 is endowed with @xmath1-connection , then 1 ) the embedding curvatures and the statistical curvatures of @xmath84 are independent of @xmath5 , 2 ) any one - parameter curve @xmath107 given by the parameter @xmath108 , while @xmath10 and the other elements of @xmath109 are fixed , is curved in the direction of @xmath110 and contained in a two - dimensional plane composed by @xmath93 and @xmath110 , 3 ) the statistical curvature of @xmath84 could be quite large when @xmath10 are close to each other , while @xmath83 is flat everywhere .    here",
    "we introduce another submanifold @xmath111 which is contrasting to @xmath84 in the sense that @xmath111 is flat with respect to @xmath1-connection .",
    "for a point @xmath95 , let @xmath112 we easily notice that @xmath111 is the minimum distance points with respect to kullback - leibler divergence .",
    "that is , @xmath113 where @xmath114 is the kullback - leibler divergence between @xmath11 and @xmath115 , which is specifically given by @xmath116 the minimum distance points with respect to the kullback - leibler distance consists of all the points on the @xmath1-geodesics which pass through the point @xmath95 and are orthogonal to @xmath50 at that point .",
    "( see theorem in a2 of amari [ 1 ] ) .",
    "we can visualize the structure of @xmath12 endowed with @xmath1-connection for the two dimensional case .",
    "see figure [ fig1 ] , where @xmath117 , @xmath118 and @xmath119 are drawn .",
    "when @xmath120 , @xmath83 is a two - dimensional autoparallel submanifold with the affine coordinate @xmath121 , while @xmath84 is a one - dimensional submanifold with an coordinate @xmath122 .",
    "as it is seen in proposition [ metric_spectra ] , all the tangent vectors @xmath123 , @xmath124 , @xmath125 are orthogonal to each other .",
    "@xmath111 is a `` straight '' line which is also orthogonal to @xmath83 .",
    "the arrow on @xmath83 is the line @xmath126 , and the arrow head indicates the direction in which @xmath127 increases .",
    "the statistical curvature turns out to be the increasing function of @xmath128 ; @xmath129    , @xmath84 and @xmath111,width=377 ]    we can analyze the bias of @xmath130 from the geometrical structure of @xmath12 .",
    "it is well known that @xmath131\\ ( i=1,\\ldots , p)$ ] majorizes @xmath132 , that is , @xmath133",
    "\\geq \\sum_{i=1}^j \\lambda_i,\\quad 1\\leq \\forall j \\leq p-1 , \\qquad \\sum_{i=1}^p e[\\,\\bar{l}_i\\ , ] = \\sum_{i=1}^p \\lambda_i.\\ ] ] the bias @xmath131 $ ] is quite large when @xmath14 is small and @xmath9 s are close to each other ( see lawley @xcite , anderson @xcite ) . for the case @xmath120 , @xmath134 \\geq \\lambda_1,\\qquad e[\\,\\bar{l}_2\\ , ] \\leq \\lambda_2.\\ ] ] suppose a sample @xmath135 takes the value at a point @xmath136 let @xmath137 denote the point on @xmath50 designated by the eigenvalues of @xmath138 , namely @xmath139 .",
    "the curve @xmath140 connects @xmath141 and @xmath137 .",
    "if we define @xmath142 as the point on @xmath50 designated by @xmath143 , then @xmath144 connects @xmath141 and @xmath142 .",
    "the three points @xmath141 , @xmath137 and @xmath142 are on the same plane , and if we move from @xmath137 in the direction to @xmath142 , then the statistical curvature of @xmath84 increases ( see figure [ fig2 ] ) .",
    "if we estimate @xmath145 by @xmath146 , then the estimand is the point @xmath137 , while for the unbiased estimator @xmath147 , the estimand is the point @xmath142 .",
    "since the @xmath128-coordinate of @xmath137 is always smaller than that of @xmath142 , the estimator @xmath148 is likely to estimate @xmath149 and @xmath150 too apart , which causes the bias .",
    "it is also seen that the bias gets larger when @xmath128 approaches to one , that is , @xmath149 and @xmath150 get closer to each other .     and @xmath111 on @xmath128-coordinate , width=377 ]",
    "now we consider the information loss caused by ignoring @xmath20 for the inference on @xmath10 .",
    "information loss matrix @xmath151 at a fixed point @xmath152 is given by @xmath153=g_{ab}(\\bm{s})-g_{ab}({\\bm{l}}),\\ ] ] where @xmath154 are the components of the metrics w.r.t .",
    "@xmath89 and @xmath88 based on respectively the distributions @xmath17 , @xmath21 and the conditional distribution of @xmath17 given @xmath21 , all of which are measured at the point @xmath152 .",
    "amari @xcite found that the asymptotic information loss can be expressed in terms of the metric and the embedding curvatures ; @xmath155    [ info_loss_result ] @xmath156 where @xmath157    @xmath158 at the point @xmath95 depends only on @xmath159 when the information loss of a statistic has the order @xmath160 , we call the statistic is the @xmath161th order sufficient . consequently the statistic @xmath21 is the first order sufficient , but not the second order sufficient .",
    "@xmath158 , the information loss in the first order term ( @xmath162 ) is negligible when the eigenvalues are separated to some extent . on the contrary ,",
    "it is quite large when the population eigenvalues are close to each other .",
    "note that the information carried by @xmath21 is given by the formula ; @xmath163 since @xmath164 is positive definite , @xmath165 must be bounded in the neighborhood of a point where @xmath166 .",
    "this indicates that the term of order @xmath167 in @xmath158 is also unbounded in such a neighborhood .",
    "hence the expansion of the information loss with respect to @xmath14 is not useful when the population eigenvalues are close to each other .     and",
    "@xmath111 on @xmath128-coordinate , width=377 ]    in order to check the information loss numerically , we carried out a simple simulation .",
    "consider the null hypothesis @xmath168 the ordinary likelihood ratio test based on @xmath17 ( say the first test ) is given by the rejection region @xmath169 where @xmath170 is chosen so that the size of the test is @xmath79 .",
    "see e.g. theorem 8.4.2 of muirhead @xcite . on the other hand ,",
    "the likelihood ratio test based on the distribution of @xmath21 ( say the second test ) is given by @xmath171 since the density function of @xmath21 is given by @xmath172 where @xmath173 is the uniform probability on the group of @xmath40-dimensional orthogonal matrices , @xmath174 , and @xmath175 is the normalizing constant ( see theorem 3 .",
    "2 . 18 of muirhead @xcite ) .",
    "figure [ fig3 ] shows the simulation result on the powers of the two tests for the case @xmath120 , @xmath176 .",
    "the alternative hypotheses are taken in various directions from the null point @xmath177 ; @xmath178 where @xmath179 ( note that the alternatives are not at the same distance from the null w.r.t .",
    "the metric of proposition [ metric_spectra ] . ) the powers for each alternative are the mean values from @xmath180 times repetition . for the integral calculation in",
    ", we picked up 100 points from @xmath181 in an equidistant manner .",
    "the graph shows that the second test performs as well as the first test , hence it indicates the information loss is not serious at least around the point @xmath177 .",
    "in this section , we apply the geometrical analysis in the previous sections to the estimation of @xmath10 . we consider two cases where the population eigenvectors , @xmath5 , is respectively known and unknown .",
    "first let s consider the case where @xmath5 is known . in this case , statistics",
    "@xmath182 is the second order sufficient , since @xmath183 is orthogonal to @xmath50 , and the @xmath1-embedding curvature of @xmath183 vanishes ( see the equation ) .",
    "therefore the estimator @xmath184 is superior to the ordinary estimator @xmath185 in view of the asymptotic information loss . especially consider the case where we know that @xmath186 , that is , each variate is independent .",
    "then the estimator @xmath187 becomes the ordinary sample variance for the @xmath8th variate .",
    "the lesson is that if each variate is independent , then we should tackle with each variate separately , do nt treat them as a multivariate set .     and @xmath188 as @xmath10 changes , width=377 ]    we made a numerical evaluation of the superiority of @xmath147 to @xmath189 .",
    "we used kullback - leibler divergence as a loss function ( say kl - loss ) .",
    "namely for the true parameter @xmath190 and the estimators @xmath191 we evaluate the loss of the estimator as @xmath192 we compared the risks @xmath193,\\ i=1,2 $ ] for the case @xmath194 under the condition @xmath195 , where @xmath128 varies from one to 0.02 by the increment of 0.02 .",
    "note that both estimators and the loss function are all scale invariant .",
    "figure [ fig4 ] shows the result .",
    "we repeated @xmath180 times risk evaluation for each @xmath128 and took the average .",
    "theoretically the risk of @xmath188 is constant .",
    "the perturbation of the dotted line is due to the simulation error .",
    "we observe that the risk of @xmath147 is reduced by nearly 30 % compared to @xmath189 in the neighborhood of @xmath196 , where the second order term of the information loss of @xmath189 is maximized .     and @xmath188 as @xmath5 changes , width=377 ]    in a practical situation , it is rare to know the exact population eigenvectors beforehand .",
    "it is more likely that we only have a vague knowledge about the eigenvectors .",
    "for example , we sometimes have a prior knowledge that each variate is almost uncorrelated to each other although they are not perfectly independent .",
    "in this situation , which is better to use @xmath197 or @xmath189 ?",
    "the following simulation suggest that the superiority of @xmath147 to @xmath189 could hold good in a relatively large neighborhood of @xmath198 .",
    "figure [ fig5 ] shows the change of the risks of the two estimators w.r.t .",
    "k - l loss under the condition @xmath199 as @xmath5 varies with @xmath200 as @xmath201 for each @xmath5 , the repetition was done @xmath180 times and we took the average as the risks .",
    "@xmath202 has a constant risk , though the risk line for @xmath202 is perturbed by the simulation error .",
    "the graph shows that @xmath188 still performs better than @xmath202 in spite of the situation @xmath203      as we saw in the previous subsection , if we have information about @xmath5 , the estimator @xmath147 works well .",
    "if we do not have information about @xmath5 beforehand , a naive idea is to estimate it by the sample eigenvectors matrix @xmath20 and substitute it into @xmath147 .",
    "however this only reproduces @xmath189 .",
    "@xmath20 is a point estimator ( m.l.e . ) of @xmath5 , while we could construct a distribution of @xmath5 based on the sample @xmath17 .",
    "if we take the expectation of @xmath204 with this distribution , it could produce a reasonable estimator . first consider the conditional distribution of @xmath20 when @xmath21 is given . since @xmath205 is distributed as wishart matrix @xmath206 , its density w.r.t .",
    "the uniform probability @xmath173 on the group of @xmath40-dimensional orthogonal matrices @xmath174 equals @xmath207 where normalizing constant @xmath208 is given by @xmath209 this conditional distribution depends on @xmath3 . if we substitute @xmath3 with the estimator @xmath138 , a density of @xmath5 on @xmath174 with respect to @xmath210 is given by @xmath211 where @xmath212 take the expectation of @xmath213 w.r.t .",
    "the density , @xmath214 let @xmath215 denote @xmath216 .",
    "because of the invariance of @xmath217 , it turns out that @xmath218 where @xmath219     and @xmath220 as @xmath128 changes , width=377 ]    we propose @xmath221 as a new estimator of @xmath10 .",
    "it is easily proved that @xmath222 as @xmath223 , and @xmath224 is a `` shrinkage '' estimator .",
    "the analytic evaluation of this estimator s performance seems difficult even for the large sample case .",
    "instead we show the numerical result comparing @xmath146 and @xmath224 .",
    "we simulated the risks of both estimators w.r.t .",
    "k - l loss for the case @xmath120 .",
    "since they are functions of @xmath21 and scale invariant , it is enough to measure the risks for @xmath225 .",
    "we varied @xmath128 from 0.04 to 1.00 by the increment 0.04 , and for each @xmath128 we repeated the risk evaluation @xmath226 times and took the average .",
    "for the integral calculation of and , we picked up 50 points from @xmath181 in an equidistant manner .",
    "figure [ fig6 ] shows the result .",
    "it seems that the new estimator performs better compared to @xmath189 , especially @xmath10 are close to each other .",
    "as a base for the vector space of real symmetric matrices , we consider @xmath227 which is a @xmath228 matrix defined by @xmath229 where @xmath230 is the @xmath231 matrix whose @xmath232 element equals one , and all the other elements are zero .",
    "the one to one correspondence @xmath233 gives the component expression of @xmath234 since @xmath235 we have the following relations @xmath236 where @xmath237 , @xmath238    for the first order derivative at @xmath239 , we only have to consider @xmath3 up to the term to the first power w.r.t .",
    "@xmath45 , hence we put @xmath240 as @xmath241 therefore we have @xmath242 where @xmath243 which leads to @xmath244 and @xmath245 from and , we have the following results on tangent vectors ; @xmath246 where @xmath247 is the @xmath248th column of @xmath5 , and @xmath249 if we substitute and into , and , we get the results as follows ; @xmath250 @xmath251      note that @xmath252 , hence @xmath253 this means @xmath83 is an affine subspace of @xmath12 w.r.t .",
    "an @xmath35 , which is an affine coordinate system of @xmath12 with @xmath0-connection .",
    "consequently @xmath83 is @xmath0-flat , i.e. @xmath254 .",
    "@xmath255 is similarly proved .",
    "see theorem 1.1 in amari and nagaoka .",
    "now we consider @xmath256 . using ( 4.14 ) in amari @xcite , it is calculated as @xmath257 where @xmath228 matrices @xmath61 , @xmath62 are given by @xmath258 in order to calculate @xmath61 , we only have to consider @xmath3 up to the terms powered by two w.r.t .",
    "@xmath45 ; @xmath259 therefore @xmath24 is expressed as @xmath260 where @xmath261 since @xmath262 truns out to be @xmath263 from this we have @xmath264 where @xmath265 @xmath266 @xmath267 @xmath268",
    "furthermore we have @xmath269 where @xmath270 since @xmath271 we have @xmath272 from and , we have @xmath273 the following equalities hold ; @xmath274 consequently @xmath275      as we will see in the next subsection , @xmath276 combine this with proposition [ metric_spectra ] , we have @xmath277      the term of the order @xmath14 in vanishes since @xmath278 equals zero for @xmath279 .",
    "we consider the term of order @xmath162 . since @xmath280 also vanishes for @xmath281 , @xmath282 , we only have to consider the term @xmath283 because of",
    ", the above term equals @xmath284 if @xmath285 , then equals @xmath286 if @xmath287 , then equals @xmath288",
    "the author really appreciates dr .",
    "m. kumon kindly answering his question on a basic fact of the information loss .                                    c. lenglet , m. rousson , r. deriche and o. faugeras .",
    "statistics on the manifold of multivariate normal distributions : theory and application to diffusion tensor mri processing .",
    "_ journal of mathematical imaging and vision _ , 25 : 423 - 444 , 2006"
  ],
  "abstract_text": [
    "<S> we consider an inference on the eigenvalues of the covariance matrix of a multivariate normal distribution . </S>",
    "<S> the family of multivariate normal distributions with a fixed mean is seen as a riemannian manifold with fisher information metric . </S>",
    "<S> two submanifolds naturally arises ; one is the submanifold given by fixed eigenvectors of the covariance matrix , the other is the one given by fixed eigenvalues . </S>",
    "<S> we analyze the geometrical structures of these manifolds such as metric , embedding curvature under @xmath0-connection or @xmath1-connection . </S>",
    "<S> based on these results , we study 1 ) the bias of the sample eigenvalues , 2)the information loss caused by neglecting the sample eigenvectors , 3)new estimators that are naturally derived from the geometrical view .    </S>",
    "<S> msc(2010 ) _ subject classification _ : primary 62h05 ; secondary 62f12 + _ key words and phrases : _ curved exponential family , information loss , fisher information metric , embedding curvature , affine connection , positive definite matrix . </S>"
  ]
}