{
  "article_text": [
    "frequently , in x - ray computed tomography ( ct ) , the amount of collected projection data is lower that it is required by the nyquist sampling theorem @xcite . in medical imaging ,",
    "the restrictions are applied to minimize the ionizing radiation which can harm living tissue cells @xcite . in material science ,",
    "the aim is to better resolve temporal resolution via higher frame rate acquisition @xcite . in such cases of limited data",
    ", iterative techniques can provide better reconstructions than analytical methods @xcite .    dealing with ill - posed and ill - conditioned inverse problems , iterative methods require regularization to constrain the space of desirable solutions .",
    "due to edge - preserving properties , total variation ( tv ) regularization @xcite has been extensively used in tomographic iterative reconstruction for the last three decades .",
    "however , tv penalty produces piecewise - constant images ( so - called `` cartoon '' or `` staircasing '' effect ) even if the original object is smooth @xcite .",
    "this `` cartoon '' effect can be particularly undesirable in emission tomography ( et ) @xcite , such as positron emission tomography ( pet ) or single - photon emission computed tomography ( spect ) where due to limited resolution of the imaging system and low photon - number statistics , reconstructed images are naturally blurred . since in et",
    "the activity distribution is piecewise - smooth and the use of tv penalty can result in undesirable artifacts , this can potentially bias the following clinical interpretation of reconstructed images @xcite .",
    "various improvements have been made to reduce `` cartoon '' appearance of the recovered images @xcite .",
    "the most straightforward way is to couple tv term with @xmath0 norm @xcite .",
    "the goal of this approach is to establish a trade - off ( by means of regularization constants ) between piecewise - constant and piecewise - smooth regularization terms .",
    "this , however , complicates the optimization problem and for practical reconstruction purposes it can be a challenging task to establish the required regularization parameters .",
    "therefore , the use of a single penalty term which can accommodate the required properties is highly desirable for tomographic reconstruction .",
    "some single term penalties have been proposed for image denoising and they based on the _ edge preserving laplacian _ @xcite or generalized forms of tv norm @xcite .    in this paper , we propose a high - order penalty which has similarities with the laplacian based methods @xcite but it also possess some of unique qualities .",
    "we adapt our high - order penalty to the tomographic reconstruction problem using the two - step fixed point iteration algorithm @xcite . to demonstrate applicability of the proposed regularizer to image reconstruction problems we compare it with the classical tv @xcite and combined tv-@xmath0 methods for ct and et image reconstruction .",
    "all techniques are compared quantitatively and visually .",
    "tomographic image reconstruction problem consists of determining the inner structure of an object based on its x - ray observations from the several different angular positions .",
    "incoming photons with different energies ( due to absorption ) are registered by detectors and information about the path length a photon has travelled along the line can be decoded @xcite . therefore , by solving the inverse problem where projection data is given , the degree of absorption or attenuation coefficient of the object can be recovered . in mathematical terms , the reconstruction problem can be formulated as the least squares ( ls ) problem : @xmath1 where @xmath2 is a discrete function of the number of the detector bins and the observation angles describing the projection data ( sinogram ) , @xmath3 is a function of spacial variables describing the observed object ( e.g. density of the object s material ) and @xmath4 is a sparse system projection matrix ( discrete approximation of the continuous radon transform @xcite ) mapping the `` space of objects '' to the `` space of observations '' . in continuous space",
    "the operator @xmath5 is an integral operator , and zero is a condensation point of its singular values , which makes the problem ( [ ls ] ) ill - posed @xcite .",
    "depending on the numbers of spacial grid points , detectors and angles , the matrix @xmath5 can be `` fat '' ( the number of rows is less than the number of columns ) or `` tall '' ( the number of rows is greater than the number of columns ) .",
    "irrespective of the shape , the singular values of @xmath5 form a very tight cluster near zero owing to the same property of the integral operator from which @xmath5 derives @xcite .      the quadratic functional @xmath6 with its `` normal '' form @xmath7 can be minimized by a suitable iterative minimization algorithm , e.g. conjugate gradient least squares ( cgls ) algorithm @xcite",
    ", however , the convergence of iterations can be very slow because of the poor conditioning of the hessian @xmath8 @xcite . the slow convergence of iterations is closely related to another difficulty in dealing with this kind of problem , which is the ill - posedness of the problem . generally , the solution @xmath9 of the problem ( [ ls ] ) is not unique ( if @xmath5 is `` fat '' ) , and even if it is , in practical computation @xmath9 is indistinguishable from any @xmath10 if @xmath11 is below the round - off error level , which , in the case at hand , may happen even if @xmath12 is substantial .",
    "both difficulties can be tackled by a _ regularization _",
    "technique , whereby ( [ ls ] ) is replaced with @xmath13 where @xmath14 is a suitable regularization functional , and @xmath15 is a positive scalar parameter . the ls data term in ( [ ls.reg ] )",
    "can be replaced by other fit - to - data functional , for example , poisson log- likelihood functional , which is better suited noise model for et reconstruction @xcite .",
    "optimization problem with regularized poisson log - likelihood is @xmath16 @xmath17_{i } + \\bm{b}_{i}\\log[\\mathrm{a}\\bm{u}]_{i}\\ } + \\alpha r(\\bm{u}).\\end{aligned}\\ ] ] now we consider various regularization terms @xmath14 that can be used in ( [ ls.reg ] ) and ( [ kl.reg ] ) cost functions .",
    "a prime example of the regularization is known as tikhonov s @xcite , where @xmath18 regularization makes the minimization problems ( [ ls.reg ] ) and ( [ kl.reg ] ) well - posed , the eigenvalues of the hessian of @xmath19 being not less than @xmath15 .",
    "tikhonov regularization is quadratic , therefore high frequencies which are related to the object boundaries are penalized more , resulting in a smooth recovery of @xmath9 . to preserve boundaries one needs to consider non - quadratic penalties , e.g. the tv penalty @xcite can significantly improve oscillatory solutions",
    "the differentiable ( due to small @xmath20 constant ) tv penalty is given as : @xmath21 unlike tikhonov s penalty ( [ tikhpen ] ) , tv can recover function while preserving sharp discontinuities , however , it also leads to the patchy appearance of the image @xcite .",
    "since tv is unable to recover smooth variations alone , it can be coupled with @xmath0-norm based penalty @xcite resulting in the combined functional @xmath22 where @xmath23 is an additional regularization parameter for the quadratic term . in @xcite , the following combined functional has been proposed : @xmath24 where @xmath25 denotes an isotropic laplacian . in this work , we will be using ( [ tvpen ] ) and ( [ combpen2 ] ) functionals for comparison with the proposed penalty .",
    "it is intuitively obvious that the regularization parameter @xmath15 in ( [ ls.reg ] ) and ( [ kl.reg ] ) must not be large . in order to get some further insight into the issue ,",
    "let us estimate the difference between @xmath26 and @xmath27 assuming for simplicity that all singular values of @xmath5 are positive .",
    "let us assume @xmath28 , where @xmath29 is a square non - degenerate matrix , and denote @xmath30 , and @xmath31 . in the case at hand , @xmath26 ( where @xmath15 may be zero ) satisfies @xmath32 which implies the following equation for the regularization error , @xmath33 : @xmath34 multiplication by @xmath35 yields @xmath36 now , in the left - hand side of ( [ corr.prod ] ) we have @xmath37 and in the right - hand side @xmath38 @xmath39 thus , ( [ corr.prod ] ) implies @xmath40      at first glance , the regularization appears to be merely a compromise move that distorts the problem so that it becomes solvable . while this is certainly so in the case of tikhonov s regularization ,",
    "an alternative viewpoint can be offered , which is helpful in designing a proper regularization for the problem at hand .",
    "one observes that the problem ( [ ls ] ) can only be solved approximately , if only because of the inexactness of computer arithmetic .",
    "one observes further that it may have infinitely many approximate solutions that are indistinguishable in practical computation , as pointed out in the previous section , if one is only guided by the smallness of the goal data fidelity functional @xmath41 .",
    "the regularization can be viewed as some kind of additional criterion that helps to verify whether a particular computed solution is acceptable .",
    "this viewpoint is supported by the fact that in the case where @xmath42 , the regularized problem ( [ ls.reg ] ) is equivalent to the original problem ( [ ls ] ) for these extended @xmath5 and @xmath43 : @xmath44 , \\quad \\bm{b}_\\alpha = \\left [ \\begin{array}{c } \\bm{b } \\\\ 0 \\end{array } \\right].\\end{aligned}\\ ] ] in the problem ( [ ls ] ) , @xmath5 is a discretization of an integral operator , owing to which @xmath45 is small on oscillating grid functions @xmath46 with wavelengths that are close to the grid step .",
    "hence , if one directly applies e.g. cgls algorithm to the minimization of the quadratic functional ( [ ls ] ) , then after sufficiently many iterations one is likely to end up with a quickly oscillating approximate solution @xmath27 .",
    "but most images that one encounters in practice do not feature such oscillations and can be actually represented by piecewise - smooth functions @xmath27 .",
    "this suggests that the value of @xmath14 should be large on short - wavelength functions @xmath47 . at the same time",
    ", @xmath14 should remain small on the boundaries ( walls ) between objects constituting the image , where @xmath47 is discontinuous or has large gradients .",
    "the following regularizer is therefore suggested : @xmath48 where the weights @xmath49 and @xmath50 are given by @xmath51 @xmath52 is a positive scalar parameter and it determines which points are considered to be on an edge between objects rather than inside it - the smaller the value of @xmath52 , the smaller the edge area .",
    "constants @xmath53 and @xmath54 are the average @xmath55- and @xmath56-derivatives of @xmath47 .",
    "these averages are introduced purely for the sake of scale - invariance , and can be computed e.g. as @xmath57 and @xmath58 where @xmath59 is the maximum of @xmath47 and @xmath60 and @xmath61 are the sizes of the square containing the image .    by design ,",
    "@xmath62 ( where el stands for the edge preserving laplacian ) is large on a short - wavelength functions @xmath47 with small oscillation amplitude ( approaching to isotropic smoothing ) inside the reconstructed objects , and small on the edges between them ( owing to the smallness of weights @xmath49 and @xmath50 ) .",
    "the edge points are identified as points where the derivatives of @xmath47 are significantly higher than their average values .",
    "the regularizer ( [ wcr ] ) shares some similarities with the high - order penalty proposed for image denoising in @xcite : @xmath63 where @xmath64 is a noisy image and weights defined as @xmath65 where @xmath66 denotes the gaussian filter with the kernel width @xmath67 and @xmath68 is the convolution .",
    "there are few differences of the proposed penalty ( [ wcr ] ) from ( [ liumeth ] ) . for tomographic reconstruction",
    "a good initialization @xmath69 is not usually available , therefore our regularizer is built on a different principle of `` catching '' prominent edges .",
    "the proposed penalty ( [ wcr ] ) is independent of the smoothed gradient while remain stable to the presence of noise ( we will justify it with our numerical experiments ) .",
    "moreover , we incorporate directional high - order smoothing in our term with consideration of variant weights ( second derivatives are weighted _ independently _ ) , in our penalty @xmath70 ( cross partial derivatives can be also added ) .",
    "overall the penalty ( [ wcr ] ) is more rigorous than ( [ liumeth ] ) and it will not allow any harmonic noise into the solution .      in order to simplify the computation of the gradient of @xmath19 and dealing with the minimization of a quadratic functional , we resort to an inner - outer iterative scheme with lagged diffusivity fixed point iteration @xcite , with @xmath49 and @xmath50 only updated on restarts ( this approach helps to deal with non - convexity of penalty ( [ wcr ] ) cf .",
    "widely used trust region technique @xcite ) .",
    "let us denote by @xmath71 and @xmath72 the matrices representing the discretized second partial @xmath55- and @xmath56-derivatives with neumann boundary conditions , and by @xmath73 and @xmath74 the diagonal matrices representing @xmath49 and @xmath50 .",
    "the symmetric gradient matrix @xmath75 , such as @xmath76 , is given for el term as ( ignoring the dependence of @xmath49 and @xmath50 on @xmath47  cf .",
    "inner - outer iterative scheme ) @xmath77 we compare the proposed el penalty with tv regularization term ( [ tvpen ] ) with the following gradient matrix @xmath78 : @xmath79 where @xmath80 and @xmath81 are matrices representing the discretized first partial @xmath55- and @xmath56-derivatives with neumann boundary conditions , @xmath82 is a diagonal matrix whose diagonal elements are @xmath83 , @xmath84 .",
    "different choices can be used for @xmath85 function to approximate @xmath86 norm , for example the huber function @xcite .",
    "we also compare the proposed penalty ( [ wcr ] ) with tv-@xmath0 functional with the gradient matrix defined as @xmath87 where @xmath88 is a diagonal matrix with elements @xmath89 and @xmath90 is a diagonal matrix with elements @xmath91 . values @xmath20 and @xmath92 were chosen similar to @xcite , @xmath93 and @xmath94 .    here",
    "we used the lagged diffusivity fixed point iteration @xcite ( see algorithm [ fp_iter ] ) to solve regularized ls optimization problem with penalties ( [ wcr.h],[wcr.htv ] ) and ( [ wcr.htvl2 ] ) .",
    "+ @xmath95 + @xmath96 initialization + begin outer ( fixed point ) iterations +  @xmath97  @xmath98 _ gradient _ +  @xmath99  @xmath98 _ approximate hessian _",
    "+  @xmath100  @xmath98 _ quasi - newton step _ +  @xmath101  @xmath98 _ update approximate solution _",
    "+  @xmath102 + end outer ( fixed point ) iterations    the system @xmath103 is solved by usual cg method @xcite , let @xmath104 be an iteration index and @xmath105 is the maximum number of inner iterations for cg method .",
    "inner and outer iterations can be terminated earlier if the following stopping criteria met @xmath106 and @xmath107 respectively .",
    "we chose @xmath108 to be a small constant .",
    "the largest eigenvalue of the matrix @xmath29 is of the order @xmath109 , @xmath110 , and the smallest are of the order @xmath111 .",
    "if @xmath15 is not very small ( considerably larger than @xmath112 ) , then the large condition number of @xmath113 can slow down the convergence of cg iterations for the minimization of @xmath19 . to alleviate this problem",
    ", we introduce preconditioning that consists in the multiplication of the gradient of @xmath19 by the inverse of @xmath114 in the course of cg iterations , where @xmath67 is a scalar value of the order of the largest singular value of @xmath5 .",
    "since matrix @xmath115 is very sparse , the application of its inverse to a vector can be efficiently performed ( via the factorization of @xmath115 ) by modern state - of - the art sparse direct solvers @xcite .      to solve regularized poisson log - likelihood problem ( [ kl.reg ] ) we use the splitting technique introduced in @xcite and used for spect reconstruction in @xcite .",
    "the main idea is to decouple data - fidelity term from the regularizer by solving two problems independently ( see algorithm [ split_rec ] ) .",
    "+ @xmath95 + @xmath96 initialization with ones + begin outer iterations +  @xmath116  @xmath98 _ mlem step _ +  @xmath117   +  @xmath118   +  @xmath119  @xmath98 _ image denoising _ +  @xmath120  @xmath98 _ update approximate solution _",
    "+  @xmath102 + end outer iterations    from the structure of algorithm [ split_rec ] , one can see that the optimization problem with poisson log - likelihood term is solved independently with maximum likelihood expectation maximization ( mlem ) algorithm @xcite .",
    "regularization is performed as an additional image denoising step . in mlem step",
    ", @xmath121 denotes the vector of 1 s , and @xmath122 and @xmath123 denote componentwise multiplication and division , respectively .",
    "additionally the weights @xmath49 and @xmath50 for the proposed term ( [ wcr.h ] ) are calculated after each mlem step and kept fixed in image denoising updates .",
    "in this section we present two different numerical experiments with the proposed penalty el ( [ wcr.h ] ) and compare it with tv ( [ wcr.htv ] ) and tv-@xmath0 ( [ wcr.htvl2 ] ) penalties . in the first experiment we model reconstruction of the synthetic phantom with algorithm [ fp_iter ] and in the second experiment",
    "we perform reconstruction of more realistic phantom for emission tomography with algorithm [ split_rec ] .",
    "the matlab code for tomographic reconstruction using tv ( [ wcr.htv ] ) regularization and the proposed el penalty ( [ wcr.h ] ) is provided @xcite .      to test the proposed penalty in regularized tomographic reconstruction we designed an analytical phantom which consists of a smooth ( two gaussians and two parabolas ) and piecewise - constant ( one rectangle ) functions ( see fig . [ phantom ] ) .    to avoid of reconstructing on the same grid where projection data was generated ( so - called reconstruction with `` inverse crime '' @xcite ) , we used a higher resolution of the phantom on a @xmath124 isotropic pixel grid to generate projections with a strip kernel @xcite .",
    "then poisson distributed noise was applied to the projection data , assuming an incoming beam intensity of 3@xmath125 ( photon count ) .",
    "reconstructions were calculated on a @xmath126 isotropic pixel grid and with a linear projection model @xcite .",
    "we used 90 projection angles in 180 degrees ( assuming a parallel beam geometry ) to reconstruct the phantom .    for a fair comparison of different regularizing penalties we initially optimized the regularization parameters ( see fig .",
    "[ reg_param ] ) with respect to the value of root - mean - square - error ( rmse ) , defined as : @xmath127 where @xmath128 is an exact image and @xmath27 is a reconstructed image .",
    "we found empirically that @xmath129 for el penalty ( [ wcr.h ] ) gives good results for the presented experiments , therefore we will keep it fixed for the rest of our tests . with fixed optimal regularization parameters ( see fig .",
    "[ reg_param ] ) we perform @xmath130 outer ( fixed point ) iterations and 5 inner iterations of algorithm [ fp_iter ] with different penalties ( see fig . [ iter_alg ] ) .    in fig .",
    "[ iter_alg ] one can see that the cgls method diverges quickly while regularized methods converge to a steady - state solution .",
    "the lowest rmse value is achieved with the proposed el penalty ( see table [ tab : rmse1 ] ) .",
    "reconstruction with tv penalty gives the highest rmse value ( from all compared regularized methods ) however it shows faster convergence on first fixed point iterations .",
    "tv-@xmath0 penalty performs slightly better than tv , but still with higher rmse than el method .",
    ".rmse values for cgls , cgls - tv , cgls - tv-@xmath0 and cgls - el methods [ cols=\"<,<,<,<,<\",options=\"header \" , ]     [ tab : rmse1 ]    reconstructed images are presented in fig .",
    "[ rec1 ] . since cgls - tv-@xmath0 reconstruction might look more appealing than cgls - el we also show the surface representations of reconstructed images ( see fig .",
    "[ rec2 ] ) and horizontal middle cross - sections ( see fig .",
    "[ rec2_plot ] ) .",
    "+ ) . left side images show the region selected and the right side images show the magnified region .",
    "it can be clearly seen that the el penalty performs much better for smooth objects while slightly more perturbed in uniform areas ( e.g. the top of the rectangle).,title=\"fig:\",scaledwidth=30.0% ] ) .",
    "left side images show the region selected and the right side images show the magnified region .",
    "it can be clearly seen that the el penalty performs much better for smooth objects while slightly more perturbed in uniform areas ( e.g. the top of the rectangle).,title=\"fig:\",scaledwidth=40.0% ]    one can notice that cgls reconstruction is very noisy .",
    "cgls - tv method better suppresses noise , however smooth features are strongly affected by the `` staircasing '' effect .",
    "cgls - tv-@xmath0 method provides reconstruction with smoother features and cgls - el method resolves smooth features even better ( e.g. cone - shaped parabola ) .",
    "although cgls - el method performs very well for smooth objects one can notice the wave - like variations of intensity in the background and also at the top of the rectangle ( see fig .",
    "[ rec2_plot ] ) .",
    "this issue can be explained by the properties of our regularizer , in contrast to tv , our penalty does not seek the sparsest solution and does not penalize strongly ( pushing to the constant value ) a small intensity perturbations .",
    "the el term tends to preserve all sharp edges while uniform noise is smoothed isotropically with the laplacian . in fig .",
    "[ rec2_plot ] one can see that the cgls - el method provides better recovery of smooth features while slightly higher ( compare to tv and tv-@xmath0 ) perturbations visible in uniform areas ( the top of the rectangle ) , however , the edges of the rectangle are defined sharper with the el penalty .      to simulate emission tomography reconstruction we designed a more realistic phantom from the high - quality x - ray scan of a mice bone .",
    "the data was acquired on a nikon metris custom bay cone - beam scanner at the henry moseley manchester x - ray facility , and was reconstructed with the feldkamp algorithm ( see fig .",
    "[ et_phant ] ( left ) ) .",
    "we thresholded the obtained reconstruction and added six gaussians with various kernel widths ( see fig . [ et_phant ] ( middle and right ) ) .    to simulate pet projection data we used niftyrec @xcite , a software for tomographic reconstruction , providing gpu - accelerated reconstruction for emission and transmission computed tomography",
    "the phantom size is @xmath131 pixels and 300 projections was simulated .",
    "poisson noise was added to projections with an expected number of @xmath132 photon counts in total .",
    "twenty noise realizations were simulated to estimate methods quantitatively .",
    "the point spread function of the pet system was modelled ( with convolution of the sinogram columns with a gaussian of full width half maximum of three pixels ) in the projection and back - projection operations .",
    "no scatter was simulated in this study . for our experiments ( see algorithm [ split_rec ] ) we performed 130 mlem iterations and 5 inner iterations ( denoising step ) .    to quantify",
    "obtained reconstructions we used averaged over all noise realizations rmse ( [ mse ] ) values in the bone region ( br ) and in gaussian regions ( gr ) .",
    "all regularization parameters were carefully selected by comparing the mean of all rmse values over all noise realizations in gr and br ( see fig .",
    "[ et_regul ] ) .",
    "after estimation of regularization parameters we performed twenty reconstructions for each method with various poisson noise distributions .",
    "the mean values for gr and br over all noise realizations are shown in fig .",
    "[ et_regul2 ] .",
    "this result proves that the el penalty is very successful in resolving smooth features ( six gaussians in this case ) and also quite competitive for the br ( lower rmse value than for tv ) .    in fig .",
    "[ et_rec1 ] and [ et_rec2 ] we demonstrate reconstructed images for the best selected rmse values .    in fig .",
    "[ et_rec1 ] and [ et_rec2 ] one can notice that the br is very smooth for tv and tv-@xmath0 penalties and some long - wave oscillations can be seen in the reconstructed image with el penalty .",
    "this result corresponds to the expected behaviour of the el penalty .",
    "we note here that the phantoms background ( see fig . [ et_phant ] ) is not as flat as tv and tv-@xmath0 penalty recovered it .",
    "furthermore , a small size dot - like feature ( approximately in the centre of the phantom ) is almost smoothed out with tv and tv-@xmath0 recovery . however , it is visible and well recovered with el penalty .",
    "the sharp features , overall , are reconstructed very well with mlem - el method and seem even sharper compare to other methods ( see the bone outer rim in fig [ et_rec1 ] ) .",
    "in this paper , we emphasize that piecewise - smooth images are more favourable than piecewise - constant ones , this , however might not be the case for all reconstructed objects .",
    "therefore , some prior knowledge about the investigated object is needed to choose an appropriate regularization term . for instance , the activity distribution in et is smooth , therefore , penalties like el are particularly suitable .",
    "the proposed penalty gives more gentle approach to regularization in uniform areas and does not penalize small perturbations aggressively .",
    "moreover , the proposed method can recover small features successfully ( e.g. lesions in et case ) with both step or ramp intensity variations .    in terms of the choice of parameters , complexity of computer implementation and the speed of computation ,",
    "our method is very similar to reconstruction techniques with tv penalty .",
    "our future work will be to explore further the space of parameters and give some recommendations for automated choice of some parameters .",
    "additionally we will look into the issue of low - frequency oscillations of our penalty in the uniform areas .",
    "in this paper , we presented a novel two - step iterative reconstruction algorithm with high - order regularization penalty .",
    "our method outperforms in terms of signal - to - noise ratio the conventional total variation based reconstruction techniques and it is competitive with other state - of - the - art high - order based approaches . from the preliminary experiments , the proposed method is well suited for the limited data problems in x - ray tomography as well as emission tomography .",
    "this work has been supported by the engineering and physical sciences research council under grants ep / j010553/1 , ep / j010456/1 and ep / i02249x/1 .",
    "99        kazantsev d and van eyndhoven g and lionheart w r b and withers p j and dobson k j and mcdonald s a and atwood r and lee p d 2015 employing temporal self - similarity across the entire time domain in computed tomography reconstruction _ philosophical transactions of the royal society of london a : mathematical , physical and engineering sciences _ * 373(2043 ) * 20140389              chan r h and liang h and wei s and nikolova m and tai x c 2015 high - order total variation regularization approach for axially symmetric object tomography from a single radiograph _",
    "inverse problems and imaging _ * 9(1 ) * 5577          do s and karl w c and kalra m k and brady t j and pien h 2010 clinical low dose ct image reconstruction using high - order total variation techniques _ in spie medical imaging , international society for optics and photonics _",
    "76225d-76225d    lysaker m and lundervold a and tai x c 2003 noise removal using fourth - order partial differential equation with applications to medical magnetic resonance images in space and time _ image processing , ieee transactions on _ * 12(12 ) * 15791590 .",
    "kazantsev d and arridge s r and pedemonte s and bousse a and erlandsson k and hutton b f and ourselin s 2012 , an anatomically driven anisotropic diffusion filtering method for 3d spect reconstruction _ physics in medicine and biology _ * 57(12 ) * 3793    pedemonte s and bousse a and erlandsson k and modat m and arridge s and hutton b f and ourselin s 2010 gpu accelerated rotation - based emission tomography reconstruction _ in nuclear science symposium conference record ( nss / mic ) _ 26572661"
  ],
  "abstract_text": [
    "<S> in this paper we present a new two - level iterative algorithm for tomographic image reconstruction . </S>",
    "<S> the algorithm uses a regularization technique , which we call edge - preserving laplacian , that preserves sharp edges between objects while damping spurious oscillations in the areas where the reconstructed image is smooth . </S>",
    "<S> our numerical simulations demonstrate that the proposed method outperforms total variation ( tv ) regularization and it is competitive with the combined tv-@xmath0 penalty . </S>",
    "<S> obtained reconstructed images show increased signal - to - noise ratio and visually appealing structural features . </S>",
    "<S> computer implementation and parameter control of the proposed technique is straightforward , which increases the feasibility of it across many tomographic applications . in this paper , we applied our method to the under - sampled computed tomography ( ct ) projection data and also considered a case of reconstruction in emission tomography the matlab code is provided to support obtained results . </S>"
  ]
}