{
  "article_text": [
    "state - space models ( ssm ) are a very popular class of non - linear and non - gaussian time series models in statistics , econometrics and information engineering ; see for example @xcite , @xcite , @xcite .",
    "an ssm is comprised of a pair of discrete - time stochastic processes , @xmath0 and @xmath1 , where the former is an @xmath2-valued unobserved process and the latter is a @xmath3-valued process which is observed .",
    "the hidden process @xmath4 is a  markov process with initial density @xmath5 and markov transition density @xmath6 , i.e.@xmath7 it is assumed that the observations @xmath1  conditioned upon @xmath0 are statistically independent and have marginal density @xmath8 , i.e.@xmath9 we also assume that @xmath10 , @xmath11 and @xmath8 are densities with respect to ( w.r.t . )",
    "suitable dominating measures denoted generically as @xmath12 and @xmath13 . for example , if @xmath14 and @xmath15 then the dominating measures could be the lebesgue measures .",
    "the variable @xmath16  in the densities of these random variables are the particular parameters of the model .",
    "the set of possible values for @xmath16 is denoted @xmath17 .",
    "the model ( [ eq : evol])-([eq : obs ] ) is also referred to as a hidden markov model ( hmm ) in the literature .    for any sequence @xmath18 and integers @xmath19 ,",
    "let @xmath20 denote the set @xmath21 .",
    "( when @xmath22  this is to be understood as the empty set . ) equations ( [ eq : evol ] ) and ( [ eq : obs ] ) define the joint density of @xmath23,@xmath24 which yields the marginal likelihood , @xmath25 let @xmath26 , @xmath27 , be a sequence of functions and @xmath28 , @xmath29 , be the corresponding sequence of additive functionals constructed from @xmath30 as follows  on @xmath31 , i.e. @xmath32 is the sums of term of the form @xmath33 , is merely a matter of redefining @xmath30 in the computations to follow.]@xmath34  there are many instances where it is necessary to be able to compute the following expectations recursively in time,@xmath35   .",
    "\\label{eq : representationadditivefunctional}\\ ] ] the conditioning implies the expectation should be computed w.r.t .",
    "the density of @xmath36 given @xmath37 , i.e. @xmath38 and for this reason @xmath39 is referred to as a _ smoothed additive functional_.    as the first example of the need to perform such computations , consider the problem of computing the score vector , @xmath40 .",
    "the score is a vector in @xmath41 and its @xmath42 component is@xmath43   _ { i}=\\frac{\\partial\\log\\text { } p_{\\theta}\\left (   y_{0:n}\\right )   } { \\partial \\theta^{i}}. \\label{eq : score}\\ ] ] using fisher s identity , the problem of computing the score becomes an instance of ( [ eq : representationadditivefunctional ] ) , i.e.@xmath44   + \\sum \\limits_{k=0}^{n}\\mathbb{e}_{\\theta}\\left [   \\left .",
    "\\nabla\\log g_{\\theta } \\left (   \\left .",
    "y_{k}\\right\\vert x_{k}\\right )   \\right\\vert y_{0:n}\\right ] \\nonumber\\\\ &   + \\mathbb{e}_{\\theta}\\left [   \\left .   \\nabla\\log\\mu_{\\theta}\\left ( x_{0}\\right )   \\right\\vert y_{0:n}\\right ]   .",
    "\\label{eq : scoreadditivefunctional}\\ ] ] an alternative representation of the score as a smoothed additive functional based on infinitesimal perturbation analysis is given in @xcite .",
    "the score has applications to maximum likelihood ( ml )  parameter estimation @xcite , @xcite .",
    "the second example is ml  parameter estimation using the expectation - maximization ( em ) algorithm .",
    "let @xmath45 be a batch of data and the aim is to maximise @xmath46 w.r.t .",
    "@xmath16 . given a current estimate @xmath47 , a new estimate @xmath48 is obtained by maximizing the function@xmath49   + \\sum \\limits_{k=0}^{n}\\mathbb{e}_{\\theta^{\\prime}}\\left [   \\left .",
    "\\log g_{\\theta } \\left (   \\left .",
    "y_{k}\\right\\vert x_{k}\\right )   \\right\\vert y_{0:n}\\right ] \\\\ &   + \\mathbb{e}_{\\theta^{\\prime}}\\left [   \\left .   \\log\\mu_{\\theta}\\left ( x_{0}\\right )   \\right\\vert y_{0:n}\\right]\\end{aligned}\\ ] ] w.r.t . @xmath16 and setting @xmath50  to the maximising argument .",
    "a fundamental property of the em algorithm  is @xmath51 . for linear gaussian models and finite state - space hmm ,",
    "it is possible to perform the computations in the definition of @xmath52 . for general non - linear non - gaussian state - space models of the form ( [ eq : evol])-([eq : obs ] ) , we need to rely on numerical approximation schemes .",
    "smc methods are a class of algorithms that sequentially  approximate the sequence of posterior distributions @xmath53 using a set of @xmath54 weighted random samples called particles . specifically , the smc  approximation of @xmath55 , for @xmath56 , is @xmath57 where @xmath58 is the importance weight associated to particle @xmath59 and @xmath60  is the dirac measure with an atom at @xmath61 .",
    "the particles are propagated forward in time using a combination of importance sampling and resampling steps and there are several variants of both these steps ; see @xcite , @xcite for details .",
    "smc  methods are parallelisable and flexible , the latter in the sense that smc approximations of the posterior densities for a variety of ssms can be constructed quite easily .",
    "smc methods were popularized by the many successful applications to ssm .",
    "a smc  approximation of @xmath39 may be constructed by replacing @xmath62 in eq .",
    "( [ eq : representationadditivefunctional ] ) with its smc  approximation in eq .",
    "( [ eq : filteringdistribution ] ) - we call this the _ path space _",
    "method since the smc  approximation of @xmath62 , which is a probability distribution on @xmath63 , is used .",
    "fortunately there is no need to store the entire ancestry of each particle , i.e. @xmath64 , which would require a growing memory .",
    "also , this estimate can be computed recursively . however , the reliance on the approximation of the joint distribution @xmath65 is bad .",
    "it is well - known in the smc  literature that the approximation of @xmath65  becomes progressively impoverished as @xmath66 increases because of the successive resampling steps @xcite , @xcite , @xcite .",
    "that is , the number of distinct samples representing @xmath67  for any fixed @xmath68 diminishes as @xmath66 increases  this is known as the _ particle path degeneracy _ problem . hence ,",
    "whatever being the number of particles , @xmath67 will eventually be approximated by a single unique particle for all ( sufficiently large ) @xmath66 .",
    "this has severe consequences for the smc  estimate @xmath39 . in @xcite , under favourable mixing assumptions , the authors established an upper bound on the @xmath69 error which is proportional to @xmath70 . under similar assumptions ,",
    "it was shown in @xcite that the asymptotic variance of this estimate  increases  at least quadratically with @xmath66 . to reduce the variance ,",
    "alternative methods have been proposed .",
    "the technique proposed in @xcite relies on the fact that for a ssm with good  forgetting properties , @xmath71 when the horizon @xmath72 is large enough ; that is observations collected after times @xmath73 bring little additional information about @xmath74 .",
    "( see ( * ? ? ?",
    "* corollary 2.9 ) for exponential error bounds . )",
    "this suggests that a very simple scheme to curb particle degeneracy is to stop updating the smc  estimate beyond time @xmath73 .",
    "this algorithm is trivial to implement but the main practical problem is that of determining an appropriate value for @xmath72  such that the two densities in eq .",
    "( [ eq : forgetting ] ) are close enough and particle degeneracy is low .",
    "these are conflicting requirements .",
    "a too small value for the horizon will result in @xmath75 being a poor approximation of @xmath76 but the particle degeneracy will be low .",
    "on the other hand , a larger horizon improves the approximation in eq .",
    "( [ eq : forgetting ] )  but particle degeneracy will creep in .",
    "automating the selection of @xmath72 is difficult .",
    "additionally , for any finite @xmath72 the smc  estimate of @xmath39 will suffer from a non vanishing bias even as @xmath77 . in @xcite , for an optimized value of @xmath72 which is dependent on @xmath66 and the typically unknown mixing properties of the model , the smc  estimates of @xmath39 based on the approximation in eq .",
    "( [ eq : forgetting ] ) were shown to have an @xmath69 error and bias upper bounded by quantities proportional to @xmath78 and @xmath79 under regularity assumptions .",
    "the computational cost of the smc  approximation of @xmath39  computed using either the path space method or the truncated horizon method of @xcite is @xmath80 .",
    "a standard alternative to computing @xmath39 is to use smc  approximations of fixed - interval smoothing techniques such as the forward filtering backward smoothing ( ffbs ) algorithm @xcite , @xcite .",
    "theoretical results on the smc  approximations of the ffbs  algorithm have been recently established in @xcite ; this includes a central limit theorem and exponential deviation inequalities . in particular , under appropriate mixing assumptions ,",
    "the authors have obtained time - uniform deviation inequalities for the smc - ffbs  approximations of the marginals @xmath81 ( * ? ? ? * section 5 ) ; see @xcite for alternative proofs and complementary results .",
    "let @xmath82 denote the smc - ffbs  estimate of @xmath39 .",
    "in this work it is established that the asymptotic variance of @xmath83 only grows linearly with time @xmath66 ; a fact which was also alluded to in @xcite .",
    "the main advantage of the smc  implementation of the ffbs  algorithm is that it does not have any tuning parameter other than the number of particles @xmath54 .",
    "however , the improved theoretical properties comes at a computational price ; this algorithm has a computational complexity of @xmath84 compared to @xmath85 for the methods previously discussed .",
    "( it is possible to use fast computational methods to reduce the computational cost to @xmath86 @xcite . )",
    "another restriction is that the smc implementation of the ffbs  algorithm does not yield an online algorithm .",
    "the contributions of this article are as follows .",
    "* we propose an original online  implementation of the smc - ffbs estimate of @xmath39 .",
    "a  particular case of this new algorithm was presented in @xcite , @xcite to compute the score vector ( [ eq : score ] ) . however , because it was catered to estimating the score , the authors failed to realise its full generality . * an upper bound for the _ non - asymptotic _ mean square error of the smc - ffbs  estimate @xmath82 of @xmath39 is derived under regularity assumptions .",
    "it follows from this bound that the asymptotic variance of @xmath87 is bounded by a quantity proportional to @xmath66 .",
    "this complements results recently obtained in @xcite , @xcite .",
    "* we demonstrate how the online implementation  of the smc - ffbs estimate of @xmath39 can be applied to the problem of recursively estimating the parameters of a ssm from data .",
    "we present original smc  implementations of recursive maximum likelihood ( rml ) @xcite , @xcite , @xcite and of the online em  algorithm @xcite , @xcite , @xcite , @xcite , @xcite , ( * ? ? ?",
    "* section 3.2 . ) .",
    "these smc implementations do not suffer from the particle path degeneracy problem .",
    "the remainder of this paper is organized as follows . in section [ sec : smoothedadditivefunctionals ] the standard ffbs  recursion and its smc  implementation is presented .",
    "it is then shown how this recursion and its smc  implementation can be implemented exactly with only a forward pass .",
    "a non - asymptotic variance bound is presented in section [ sec : theory ] .",
    "recursive parameter estimation procedures are presented in section [ sec : experiments ]  and numerical results are given in section [ sec : simulations ] .",
    "we conclude in section [ sec : discussion ] and the proof of the main theoretical result is given in the appendix .",
    "we first review the standard ffbs  recursion and its smc  approximation @xcite , @xcite .",
    "this is then followed by a derivation of a forward - only  version of the ffbs  recursion and its corresponding smc  implementation .",
    "the algorithms presented in this section do not depend on any specific smc  implementation to approximate @xmath88 .",
    "recall the definition of @xmath39  in eq .",
    "( [ eq : representationadditivefunctional ] ) . the standard ffbs procedure to compute @xmath39 proceeds in two steps . in the first step , which is the forward pass ,",
    "the filtering densities @xmath89 are computed using bayes formula:@xmath90 the second step is the backward pass that computes the following marginal smoothed densities which are needed to evaluate each term in the sum that defines @xmath39:@xmath91 where @xmath92 we compute eq .",
    "( [ eq : standardforwardbackward ] ) commencing at @xmath93 and then , decrementing @xmath94 each time , until @xmath95 .",
    "( integrating eq .",
    "( [ eq : standardforwardbackward ] )  w.r.t .",
    "@xmath96 will yield @xmath97 which is needed for the next computation . ) to compute @xmath39 , @xmath66 backward steps must be executed and then @xmath66 expectations computed .",
    "this must then be repeated at time @xmath98 to incorporate the effect of the new observation @xmath99 on these calculations .",
    "clearly this is not an online procedure for computing @xmath100 .",
    "the smc  implementation of the ffbs  recursion is straightforward @xcite . in the forward pass ,",
    "we compute and store the smc approximation @xmath101 of @xmath102 for @xmath103 . in the backward pass",
    ", we simply substitute this smc  approximation in the place of @xmath102  in eq .",
    "( [ eq : standardforwardbackward ] ) .",
    "let @xmath104 be the smc approximation of @xmath105 , @xmath106 , initialised at @xmath93 by setting @xmath107 . by substituting @xmath108 for @xmath109 in eq .",
    "( [ eq : conditionaldensity ] ) , we obtain @xmath110 this approximation is combined with @xmath111  ( see eq .",
    "( [ eq : standardforwardbackward ] ) ) to obtain @xmath112 marginalising this approximation will give the approximation  to @xmath113 , that is @xmath114 , where @xmath115 the smc  estimate @xmath82 of @xmath39 is then given by @xmath116 the backward recursion for the weights , given in eq .",
    "( [ eq : backwardweights ] ) , makes this an off - line algorithm for computing @xmath82 .      to circumvent the need for the backward pass in the computation of @xmath39 , the following auxiliary function ( on @xmath2 )",
    "is introduced , @xmath117 it is apparent that @xmath118 the following proposition establishes a forward recursion to compute @xmath119 , which is henceforth referred to as the _ forward smoothing recursion_. for sake of completeness , the proof of this proposition is given .",
    "[ propdn ] for @xmath120 , we have@xmath121   p_{\\theta}\\left ( \\left .   x_{n-1}\\right\\vert y_{0:n-1},x_{n}\\right )   dx_{n-1 } , \\label{eq : recursionadditivefunctional}\\ ] ] where @xmath122 .    * proof . *",
    "the proof is straightforward @xmath123   \\text { } p_{\\theta}\\left (   \\left .",
    "x_{0:n-1}\\right\\vert y_{0:n-1},x_{n}\\right ) dx_{0:n-1}\\\\ &   = \\int\\left [   \\int s_{n-1}\\left (   x_{0:n-1}\\right )   p_{\\theta}\\left ( \\left .",
    "x_{0:n-2}\\right\\vert y_{0:n-2},x_{n-1}\\right )   dx_{0:n-2}\\right ] p_{\\theta}\\left (   \\left .",
    "x_{n-1}\\right\\vert y_{0:n-1},x_{n}\\right ) dx_{n-1}\\\\ &   + \\int s_{n}\\left (   x_{n-1},x_{n}\\right )   \\text { } p_{\\theta}\\left (   \\left .",
    "x_{n-1}\\right\\vert y_{0:n-1},x_{n}\\right )   dx_{n-1}.\\end{aligned}\\ ] ] the integrand in the first equality is @xmath124  while the integrand in the first integral of the second equality is @xmath125 .",
    "@xmath126    this recursion is not new and is actually a special instance of dynamic programming for markov processes ; see for example @xcite . for a fully observed markov process with transition density @xmath127 , the dynamic programming recursion to compute the expectation of @xmath128 with respect to the law of the markov process",
    "is usually implemented using a backward recursion going from time @xmath66 to time @xmath129 . in the partially observed scenario considered here ,",
    "@xmath130 conditional on @xmath45 is a backward  markov process with non - homogeneous transition densities @xmath131 .",
    "thus ( [ eq : recursionadditivefunctional ] ) is the corresponding dynamic programming recursion to compute @xmath39 with respect to @xmath132 for this backward markov chain .",
    "this recursion is the foundation of the online em algorithm and is described at length in @xcite ( pioneered in @xcite ) where the density @xmath133 appearing in @xmath134 is usually written as @xmath135 or as in eq .",
    "( [ eq : conditionaldensity ] ) in @xcite , @xcite , @xcite .",
    "the forward smoothing recursion has been rediscovered independently several times ; see @xcite , @xcite for example .",
    "a simple smc  scheme to approximate @xmath39  can be devised by exploiting equations ( [ eq : additivesmoothfunctionalsasfunctionoft ] ) and ( [ eq : recursionadditivefunctional ] ) .",
    "this is summarised as algorithm smc - fs below .",
    "* algorithm smc - fs : forward - only smc  computation of the ffbs estimate *    @xmath136 @xmath137@xmath138@xmath139@xmath140@xmath141    @xmath136 @xmath66@xmath142@xmath143@xmath144   } { \\sum_{j=1}^{n}w_{n-1}^{\\left ( j\\right )   } f_{\\theta}\\left (   x_{n}^{(i)}|x_{n-1}^{(j)}\\right )   } , \\quad1\\leq i\\leq n,\\label{eq : tapproximation}\\\\ \\widehat{\\mathcal{s}}_{n}^{\\theta }   &   = \\sum_{i=1}^{n}w_{n}^{\\left (   i\\right ) } \\text { } \\widehat{t}_{n}^{\\theta}\\left (   x_{n}^{\\left (   i\\right )   } \\right )   .",
    "\\label{eq : smcapproxadditivefunctionals}\\ ] ]    this algorithm is initialized by setting @xmath145 for @xmath146 it has a computational complexity of @xmath84 which can be reduced by using fast computational methods @xcite .",
    "the rationale for this algorithm is as follows . by using @xmath147",
    "defined in eq .",
    "( [ eq : mcconditionaldensity ] ) in place of @xmath148 in eq .",
    "( [ eq : recursionadditivefunctional ] ) , we obtain an approximation @xmath149 of @xmath134 which is computed at the particle locations @xmath150 . the approximation of @xmath39  in eq .",
    "( [ eq : smcapproxadditivefunctionals ] ) now follows from eq .",
    "( [ eq : additivesmoothfunctionalsasfunctionoft ] ) by using @xmath151  in place of @xmath152 .",
    "it is valid to use the same notation for the estimates in eq .",
    "( [ eq : batchsmcestimate ] ) and in eq .",
    "( [ eq : smcapproxadditivefunctionals ] ) as they are indeed the same . the verification of this assertion may be accomplished by unfolding the recursion in eq .",
    "( [ eq : tapproximation ] ) .",
    "in this section , we present a bound on the non - asymptotic mean square error of the estimate @xmath82 of @xmath153 . for sake of simplicity , the result is established for additive functionals of the type @xmath154 where @xmath155 , and when algorithm smc - fs is implemented using the bootstrap particle filter ; see @xcite , @xcite  for a definition of this vanilla  particle filter .",
    "the result can be generalised to accommodate an auxiliary implementation of the particle filter @xcite , @xcite , @xcite . likewise , the conclusion is also valid for additive functionals of the type in ( [ eq : additivefunctional ] ) ; the proof uses similar arguments but is more complicated",
    ".    the following regularity condition will be assumed .    * ( a ) * there exist constants @xmath156 such that for all @xmath157 , @xmath158 and @xmath159 , @xmath160 admittedly , this assumption is restrictive and  typically holds when @xmath2  and @xmath3  are finite or are compact spaces . in general , quantifying the errors of smc approximations under weaker assumptions is possible @xcite .",
    "( more precise but complicated error bounds for the particle estimate of @xmath39  are also presented in @xcite  under weaker assumptions .",
    ") however , when ( a ) holds , the bounds can be greatly simplified to the extent that they can usually be expressed as linear or quadratic functions of the time horizon @xmath66 .",
    "these simple rates of growth are meaningful as they have also been observed in numerical studies even in scenarios where assumption a  is not satisfied @xcite .    for a function @xmath161 ,",
    "let @xmath162 .",
    "the oscillation of @xmath163 , denoted @xmath164 , is defined to be @xmath165 .",
    "the  main result in this section is the following non - asymptotic bound for the mean square error of the estimate @xmath82 of @xmath39 given in eq .",
    "( [ eq : smcapproxadditivefunctionals ] ) .",
    "[ nonasymptheo ] assume ( a ) .",
    "consider the additive functional @xmath32  in ( [ eq : additivefunctionalsimple ] ) with @xmath166 and @xmath167  for @xmath168 .",
    "then , for any @xmath56 and @xmath159 , @xmath169 where @xmath170 is a finite constant that is independent of time @xmath66 , @xmath16 and the particular choice of functions @xmath171 .    the proof is given in the appendix .",
    "it follows that the asymptotic variance of@xmath87 , i.e. as the number of particles @xmath54 goes to infinity , is upper bounded by a quantity proportional to @xmath172 as the bias of the estimate is @xmath173 ( * ? ? ?",
    "* corollary 5.3 ) .",
    "let @xmath174  denote the smc estimate of @xmath39 obtained with the standard path space method .",
    "this estimate can have a much larger asymptotic variance as is illustrated with the following very simple model .",
    "let @xmath175 , i.e. @xmath176 is an i.i.d .",
    "sequence , and let @xmath177 and @xmath178 for all @xmath179 where @xmath163 is some real valued function on @xmath2 , and @xmath180 .",
    "it can be easily established that the formula for the asymptotic variance of @xmath181 given in @xcite , ( * ? ? ?",
    "( 9.13 ) , page 304 ) simplifies to @xmath182   ^{2}}{\\mu_{\\theta}\\left ( x\\right )   } dx+\\frac{n\\left (   n-1\\right )   } { 2}\\int\\frac{\\pi_{\\theta}\\left ( \\left .",
    "x\\right\\vert y\\right )   ^{2}}{\\mu_{\\theta}\\left (   x\\right )   } dx\\text { } \\int\\widetilde{s}_{\\theta}\\left (   x\\right )   ^{2}\\pi_{\\theta}\\left (   \\left .",
    "x\\right\\vert y\\right )   dx \\label{eq : asymptoticvariancepathbased}\\ ] ] where @xmath183 thus the asymptotic variance increases quadratically with time @xmath66 .",
    "note though that the asymptotic variance of @xmath184 converges as @xmath66 tends to infinity to a positive constant .",
    "thus path space method can provide stable estimates of @xmath185   $ ] , i.e. when the additive functionals are time - averaged .",
    "let @xmath186 where @xmath187 is a positive non - increasing sequence that satisfies the following constraints : @xmath188 and @xmath189 .",
    "when @xmath190 then @xmath191 .",
    "one important choice for recursive parameter estimation ( see section [ sec : experiments ] ) is @xmath192 it is also of interest to quantify the stability of the path space method when applied to estimate @xmath193   $ ]  in this more general time - averaging setting .",
    "once again let @xmath194  denote the smc estimate of @xmath195   $ ] obtained with the standard path space method .",
    "using the formula for the asymptotic variance of @xmath196 given in @xcite , ( * ? ? ?",
    "( 9.13 ) , page 304 ) it can be verified that this asymptotic variance is @xmath197   ^{2}}{\\mu_{\\theta}\\left ( x\\right )   } dx\\sum\\limits_{k=1}^{n}\\gamma_{k}^{2}\\prod\\limits_{i = k+1}^{n}(1-\\gamma_{i})^{2}\\\\ &   + \\int\\frac{\\pi_{\\theta}\\left (   \\left .",
    "x\\right\\vert y\\right )   ^{2}}{\\mu_{\\theta}\\left (   x\\right )   } dx\\text { } \\int\\widetilde{s}_{\\theta}\\left ( x\\right )   ^{2}\\pi_{\\theta}\\left (   \\left .",
    "x\\right\\vert y\\right ) dx\\sum\\limits_{k=2}^{n}\\sum\\limits_{i=1}^{k-1}\\gamma_{i}^{2}(1-\\gamma _ { i+1})^{2}\\cdots(1-\\gamma_{n})^{2}\\ ] ] it follows from lemma [ lem : stepdiscounting ] in appendix that any accumulation point of this sequence ( in @xmath66 ) has to be positive .",
    "in contrast , the asymptotic variance of @xmath198 , i.e. when @xmath199   $ ] is computed using algorithm smc - fs , will converge to zero as @xmath66 tends to infinity .",
    "an important application of the forward smoothing recursion is to parameter estimation for non - linear non - gaussian ssms .",
    "we will assume that observations are generated from an unknown ` true ' model with parameter value @xmath200 , i.e. @xmath201 . the static parameter estimation problem has generated a lot of interest over the past decade and many smc  techniques have been proposed to solve it ; see @xcite for a recent review .      in a bayesian approach to the problem",
    ", a prior distribution is assigned to @xmath16 and  the sequence of posterior densities @xmath202 is estimated recursively using smc  algorithms combined with markov chain monte carlo ( mcmc ) steps @xcite , @xcite , @xcite .",
    "unfortunately these methods suffer from the particle path degeneracy problem and will result in unreliable estimates of the model parameters ; see @xcite , @xcite for a discussion of this issue . given a fixed observation record @xmath45 , an alternative offline mcmc  approach to estimate @xmath203 has been recently proposed which relies on proposals built using the smc  approximation of @xmath204 @xcite .    in a ml approach ,",
    "the estimate of @xmath205 is the maximising argument of the likelihood of the observed data .",
    "the ml estimate can be calculated using a gradient ascent algorithm either offline for a fixed batch of data or online @xcite ; see section [ subsec : rml ] .",
    "likewise , the em algorithm can also be implemented offline or online .",
    "the online em algorithm , assuming all calculations can be performed exactly , is presented in @xcite , @xcite , @xcite , @xcite and @xcite . for a general ssm for which the quantities required by the online em can not be calculated exactly , an smc implementation is possible @xcite , ( * ? ? ?",
    "* section 3.2 . ) ; see section [ sec : parameterestimation ] .      to maximise the likelihood @xmath206 w.r.t .",
    "@xmath16 , we can use a simple gradient algorithm .",
    "let @xmath207  be the sequence of parameter estimates of the gradient algorithm .",
    "we update the parameter at iteration @xmath208 using@xmath209 where @xmath210 is the score vector computed at @xmath211 and @xmath212 is a sequence of positive non - increasing step - sizes defined in ( [ eq : stepchoice ] ) . for a general ssm",
    ", we need to approximate @xmath213 .",
    "as mentioned in the introduction , the score vector admits several smoothed additive functional representations ; see eq .",
    "( [ eq : score ] ) and @xcite .",
    "( [ eq : score ] ) , it is possible to approximate the score with algorithm smc - fs .    in the online implementation , the parameter estimate at time @xmath98 is updated according to @xcite , @xcite @xmath214 upon receiving @xmath215 , @xmath216 is updated in the direction of ascent of the predictive density of this new observation .",
    "a necessary requirement for an online implementation is that the previous values of the model parameter estimates ( other than @xmath216 ) are also used in the evaluation of @xmath217 at @xmath218 .",
    "this is indicated in the notation @xmath219 .",
    "( not doing so would require browsing through the entire history of observations . )",
    "this approach was suggested by @xcite for the finite state - space case and is named rml .",
    "the asymptotic properties of this algorithm ( i.e. the behavior of @xmath216  in the limit as @xmath66  goes to infinity ) have been studied in the case of an i.i.d . hidden process by @xcite and for an hmm with a finite state - space by @xcite . under suitable regularity assumptions ,",
    "convergence to @xmath205 and a central limit theorem for the estimation error has been established .    for a general ssm",
    ", we can compute a smc  estimate of @xmath220 using algorithm smc - fs upon noting that @xmath219 is equal to @xmath221 in particular , at time @xmath66 , a particle approximation @xmath222 of @xmath223 is computed using the particle approximation at time @xmath137 and parameter value @xmath218 . similarly , the computation of eq .",
    "( [ eq : tapproximation ] ) is performed using @xmath218  and with @xmath224 the estimate of @xmath219  is now the difference of the estimate in eq .",
    "( [ eq : smcapproxadditivefunctionals ] ) with the same estimate computed at time @xmath137 .    under the regularity assumptions given in section [ sec : theory ] , it follows from the results in the appendix that the asymptotic variance ( i.e. as @xmath77 ) of the smc  estimate of @xmath219 computed using algorithm smc - fs is uniformly ( in time ) bounded . on the contrary , the standard path - based smc  estimate of @xmath220 has an asymptotic variance that increases linearly with @xmath66 .",
    "gradient ascent algorithms are more generally applicable than the em algorithm . however , their main drawback in practice is that it is difficult to properly scale the components of the computed gradient vector . for this reason",
    "the em  algorithm is usually favoured by practitioners whenever it is applicable .",
    "let @xmath207  be the sequence of parameter estimates of the em algorithm . in the offline approach , at iteration @xmath208 ,",
    "the function@xmath225 is computed and then maximized .",
    "the maximizing argument is the new estimate @xmath226 .",
    "if  @xmath227 belongs to the exponential family , then the maximization step is usually straightforward .",
    "we now give an example of this .",
    "let @xmath228 , @xmath229 , be a collection of functions with corresponding additive functionals@xmath230 and let @xmath231 the collection @xmath232 is also referred to as the _ summary statistics _ in the literature .",
    "typically , the maximising argument of @xmath233  can be characterised explicitly through a suitable function @xmath234 , i.e. @xmath235 where @xmath236_{l}=\\mathcal{s}_{l , n}^{\\theta}$ ] .",
    "as an example of this , consider the following stochastic volatility model @xcite .",
    "[ ex : stochvol]the stochastic volatility model is a ssm defined by the following equations : @xmath237 where @xmath238 and @xmath239 are independent and identically distributed standard normal noise sequences , which are also independent of each other and of the initial state @xmath240 .",
    "the model parameters @xmath241 are to be estimated .",
    "to apply the em algorithm to this model , let@xmath242 for large @xmath66 , we can safely ignore the terms associated to the initial density @xmath10 and the solution to the maximisation step is characterised by the function @xmath243    the smc  implementation of the forward smoothing recursion has advantages even for the batch em algorithm . as there is no backward pass ,",
    "there is no need to store the particle approximations of @xmath244 , which can result in a significant memory saving for large data sets .    in the online implementation ,",
    "running averages of the sufficient statistics are computed instead @xcite , @xcite , @xcite , @xcite , ( * ? ? ?",
    "* section 3.2 . ) , @xcite .",
    "let @xmath245 be the sequence of parameter estimates of the online em  algorithm computed sequentially based on @xmath45 .",
    "when @xmath99 is received , for each @xmath229 , compute @xmath246{l}$\\mathcal{s}_{l , n+1}=\\gamma_{n+1}\\text { } \\int s^{l}\\left (   x_{n},x_{n+1},y_{n+1}\\right )   p_{\\theta_{0:n}}(x_{n},x_{n+1}|y_{0:n+1})dx_{n : n+1}$\\\\ $ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + \\left (   1-\\gamma_{n+1}\\right )   \\int\\sum_{k=1}^{n}\\left (   \\prod\\limits_{i = k+1}^{n}\\left (   1-\\gamma_{i}\\right )   \\right ) \\gamma_{k}s^{l}\\left (   x_{k-1},x_{k},y_{k}\\right )   p_{\\theta_{0:n}}(x_{0:n}|y_{0:n+1})dx_{0:n},$\\end{tabular } \\ \\ \\ \\ \\ \\ \\label{eq : suffstatonline}\\ ] ] and then set@xmath247 where @xmath248_{l}=\\mathcal{s}_{l , n+1}$ ] . here",
    "@xmath249 is a step - size sequence satisfying the same conditions stipulated for the rml in section [ subsec : rml ] .",
    "( the recursive implementation of @xmath250  is standard @xcite . )",
    "the subscript @xmath251on @xmath252 indicates that the posterior density is being computed sequentially using the parameter @xmath253 at time @xmath94 ( and @xmath254 at time @xmath129 . )",
    "references @xcite , @xcite , @xcite and @xcite have proposed an online em algorithm , implemented as above , for finite state hmms . in the finite state setting all computations involved can be done exactly in contrast to general ssms where numerical procedures are called for .",
    "it is also possible to do all the calculations exactly for linear gaussian models @xcite .",
    "define the vector valued function @xmath255 as follows : @xmath256^{\\text{t}}$ ] .",
    "computing @xmath257 sequentially using smc - fs is straightforward and detailed in the following  algorithm .",
    "* smc - fs implementation of online em *    @xmath258@xmath254    @xmath258@xmath259@xmath260    @xmath258@xmath261@xmath262    @xmath258@xmath263@xmath264 .",
    "@xmath258@xmath260 , @xmath265   } { \\sum_{j=1}^{n}w_{n-1}^{\\left (   j\\right )   } f_{\\theta_{n-1}}\\left (   x_{n}^{(i)}|x_{n-1}^{(j)}\\right )   } .\\ ] ]    @xmath258@xmath266 @xmath267    it was suggested in ( * ? ? ?",
    "* section 3.2 . ) that the two other smc methods discussed in section [ sec : litreview ] could be used to approximate @xmath257 ; the path space approach  to implement the online em was also independently proposed in @xcite .",
    "doing so would yield a cheaper alternative to algorithm smc - em above with computational cost @xmath80 , but not without its drawbacks .",
    "the fixed - lag approximation of @xcite would introduce a bias which might be difficult to control and the path space approach suffers from the usual particle path degeneracy problem .",
    "consider the step - size sequence in ( [ eq : stepchoice ] ) .",
    "if the path space method is used to estimate  @xmath257 then the theory in section [ sec : theory ] tells us that , even under strong mixing assumptions , the asymptotic variance of the estimate of @xmath257  will not converge to zero for @xmath268 .",
    "thus it will not yield a theoretically convergent algorithm .",
    "numerical experiments in @xcite appear to provide stable results which we attribute to the fact that this variance might be very small in the scenarios considered is assigned a prior distribution and we estimate @xmath269 @xcite , @xcite , @xcite , the path degeneracy problem has much more severe consequences than in the ml framework considered here as illustrated in @xcite .",
    "indeed in the ml framework , the filter @xmath270 will have , under regularity assumptions , exponential forgetting properties for any @xmath159 whereas this will never be the case for @xmath271 .",
    "in contrast , the asymptotic variance of the @xmath84 estimate  converges to zero in time @xmath66 for the entire range @xmath272 under the same mixing conditions . the original @xmath84 implementation proposed here",
    "has been recently successfully adopted in @xcite to solve a complex parameter estimation problem arising in robotics .",
    "we commence with a study of a scalar linear gaussian ssm for which we may calculate smoothed functionals analytically .",
    "we use these exact values as benchmarks for the smc  approximations .",
    "the model is @xmath273 where @xmath274 and @xmath275 .",
    "we compared the exact values of the following smoothed functionals@xmath276   , \\quad\\mathcal{s}_{2,n}^{\\theta } = \\mathbb{e}_{\\theta}\\left [   \\left .",
    "\\sum_{k=1}^{n}x_{k-1}\\right\\vert y_{0:n}\\right ]   , \\quad\\mathcal{s}_{3,n}^{\\theta}=\\mathbb{e}_{\\theta}\\left [ \\left",
    ".   \\sum_{k=1}^{n}x_{k-1}x_{k}\\right\\vert y_{0:n}\\right ]   , \\label{eq : benchmarkfunctionals}\\ ] ] computed at @xmath277  with the bootstrap filter implementation of algorithm smc - fs and the path space method .",
    "comparisons were made after 2500 , 5000 , 7500 and 10,000 observations to monitor the increase in variance and the experiment was replicated 50 times to generate the box - plots in figure [ combinednandn2boxplots ] .",
    "( all replications used the same data record . )",
    "both estimates were computed using @xmath278 particles .    ) for a linear gaussian ssm .",
    "estimates were computed with path space method ( left column ) and algorithm smc - fs ( right column ) .",
    "the long horizontal line intersecting the box indicates the true value.,scaledwidth=100.0% ]    from figure [ combinednandn2boxplots ]  it is evident that the smc  estimates of algorithm smc - fs significantly  outperforms the corresponding smc  estimates of the path space method .",
    "however one should bear in mind that the former algorithm has @xmath279 computational complexity while the latter is @xmath280 .",
    "thus a comparison that takes this difference into consideration is important . from theorem [ nonasymptheo ]  and the discussion after it",
    ", we expect the variance of algorithm smc - fs s estimate to grow only linearly with the time index compared to a quadratic in time growth of variance for the path space method .",
    "hence , for the same computational effort we argue that , for large observation records , the estimate of algorithm smc - fs is always going to outperform the path space estimates .",
    "specifically , for a large enough @xmath66 , the variance of algorithm smc - fs s estimate with @xmath54 particles will be significantly less than the variance of the path space estimate with @xmath281 particles .",
    "if the number of observations is small then , taking into account the computational complexity , it might be better to use the path space estimate as the variance benefit of using algorithm smc - fs may not be appreciable to justify the increased computational load .",
    "figure [ fig : constantstep ] shows the parameter estimates obtained using the smc implementation of online em for the stochastic volatility model discussed in example [ ex : stochvol ] .",
    "the true value of the parameters were @xmath282  and 500 particles were used .",
    "smc - em  was started at the initial guess @xmath283 .",
    "for the first 100 observations , only the e - step was executed .",
    "that is the step @xmath284 ,  which is the m - step was skipped .",
    "smc - em  was run in its entirety for observations 101 and onwards .",
    "the step size used was @xmath285 for @xmath286 and @xmath287 for @xmath288 .",
    "figure [ fig : constantstep ] shows the sequence of parameter estimates computed with a very long  observation sequence .    .",
    "true and converged values ( average of the last 1000 iterations ) are indicated on the left and the right of the plot respectively.,scaledwidth=60.0% ]",
    "we proposed a new smc algorithm to compute the expectation of additive functionals recursively in time . essentially , it is an online  implementation of the ffbs smc algorithm proposed in @xcite .",
    "this algorithm has an @xmath279 computational complexity where @xmath54 is the number of particles .",
    "it was mentioned how a standard path space smc  estimator to compute the same expectations recursively in time could be developed .",
    "this would have an @xmath280 computational complexity .",
    "however , as conjectured in @xcite , it was shown here that the asymptotic variance of the smc - ffbs estimator  increased linearly with time whereas that of the @xmath280  method increased quadratically .",
    "the online smc - ffbs estimator was then used to  perform recursive parameter estimation .",
    "while the convergence of rml and online em have been established when they can be implemented exactly , the convergence of the smc  implementation of these algorithms have yet to be established and is currently under investigation .",
    "the authors would like to thank olivier capp , thomas flury , sinan yildirim and ric moulines for comments and references that helped improve the first version of this paper .",
    "the authors are also grateful to rong chen for pointing out the link between the forward smoothing recursion and dynamic programming",
    ". finally , we are thankful to robert elliott to have pointed out to us references @xcite , @xcite and @xcite .",
    "the proofs in this section hold for any fixed @xmath16 and therefore @xmath289  is omitted from the notation .",
    "this section commences with some essential definitions .",
    "consider the measurable space @xmath290 .",
    "let @xmath291 denote the set of all finite signed measures  and @xmath292 the set of all probability measures on @xmath293 .",
    "let @xmath294  denote the banach space of all bounded and measurable functions @xmath295 equipped with the uniform norm @xmath296 .",
    "let @xmath297 , i.e. @xmath298 is the lebesgue integral of the function @xmath299 w.r.t .",
    "the measure @xmath300 . if @xmath301  is a density w.r.t .",
    "some dominating measure @xmath12 on @xmath293 then , @xmath302 . we recall that a bounded integral kernel @xmath303 from a measurable space @xmath290 into an auxiliary measurable space @xmath304 is an operator @xmath305 from @xmath306 into @xmath294 such that the functions @xmath307 are @xmath308-measurable and bounded , for any @xmath309 . in the above displayed formulae",
    ", @xmath310 stands for an infinitesimal neighborhood of a point @xmath311 in @xmath312 .",
    "let @xmath313 denote the dobrushin coefficient of @xmath314 which defined by the following formula @xmath315 where @xmath316 stands the set of @xmath317-measurable functions @xmath295 with oscillation less than or equal to 1 .",
    "the kernel @xmath314 also generates a dual operator @xmath318 from @xmath291 into @xmath319 defined by @xmath320 .",
    "a markov kernel is a positive and bounded integral operator @xmath314 with @xmath321 . given a pair of bounded integral operators @xmath322 , we let @xmath323 the composition operator defined by @xmath324 . for time",
    "homogenous state spaces , we denote by @xmath325 the @xmath326-th composition of a given bounded integral operator @xmath314 , with @xmath327 .    given a positive function @xmath328 on @xmath293 ,",
    "let @xmath329 be the bayes transformation defined by @xmath330 the definitions above also apply if @xmath301  is a density and @xmath314 is a transition density .",
    "in this case all instances of @xmath331 should be replaced with @xmath332  and @xmath303 by @xmath333 where @xmath12 and @xmath310 are the dominating measures .",
    "the proofs below will apply to any fixed sequence of observation @xmath334 and it is convenient to introduce the following transition kernels , @xmath335 with the convention that @xmath336 , the identity operator .",
    "note that @xmath337 .",
    "let the mapping @xmath338 , @xmath179 , be defined as follows @xmath339 several probability densities and their smc  approximations are introduced to simplify the exposition .",
    "the _ predicted filter _ is denoted by @xmath340 with the understanding that @xmath341 is the initial distribution of @xmath240 .",
    "let @xmath342  denote its smc approximation with @xmath54 particles .",
    "( this notation for the smc approximation is opted for , instead of the usual @xmath343 , to make the number of particles explicit . )",
    "the bounded integral operator @xmath344 from @xmath2 into @xmath63 is defined as @xmath345 @xmath344 is defined for any pair of time indices @xmath346 satisfying @xmath347 with the convention that @xmath348 for @xmath349 and @xmath350 . the smc  approximation , @xmath351 , is@xmath352 where @xmath353 is the smc  approximation of @xmath354 obtained from the smc - ffbs approximation of section [ sec : ffbs ] , i.e.@xmath355 where the backward markov transition kernels @xmath356  are defined through @xmath357 it is easily established that the smc - ffbs  approximation of @xmath358 , @xmath106 , is precisely the marginal of @xmath359 where @xmath360  was defined in ( [ eq : filteringdistribution ] ) . finally , we define @xmath361 the following estimates are a straightforward consequence of assumption ( a ) . for time indices",
    "@xmath362 , @xmath363 and for @xmath364 , @xmath365      [ lem : kintchine ] let @xmath368 , @xmath56 , be the natural filtration associated with the @xmath54-particle approximation model and @xmath369 be the trivial sigma field . for any @xmath370 , there exist a finite ( non random ) constant @xmath371 such that the following inequality holds for all @xmath372 and @xmath373measurable functions @xmath374  s.t . @xmath375,@xmath376",
    "[ lem : lperrorfilter]for any @xmath370 , there exists a constant @xmath371 such that the following inequality holds for all @xmath372 and @xmath378  s.t .",
    "@xmath379 , @xmath380(\\varphi ) \\right\\vert ^{r}\\right )   ^{\\frac{1}{r}}\\leq a_{r}~\\sum_{k=0}^{n}~b_{k , n}~\\beta\\left (   \\frac{q_{k , n}}{q_{k , n}(1)}\\right )   .",
    "\\label{eq : lperrorfilter}\\ ] ]              the following decomposition is central @xmath385with the convention that @xmath386 , for @xmath349 . lemma [ lem : dpn ] states that @xmath387and therefore the decomposition can be also written as @xmath388with the convention @xmath389 , for @xmath349 . let @xmath390then every term in the r.h.s .",
    "of ( [ eq : decompsn ] ) takes the following form @xmath391where the integral operators @xmath392 are defined as follows , @xmath393finally , using ( [ eq : decompsn ] ) and ( [ eq : decompsnterm ] ) , @xmath394  is expressed as @xmath395where the first order term is @xmath396and the second order remainder term is @xmath397the non - asymptotic variance bound is based on the triangle inequality @xmath398and bounds are derived below for the individual expressions on the right - hand side of this equation.using the fact that @xmath399  is zero mean and uncorrelated,@xmath400the following results are needed to bound the right - hand side of ( eq : fisrtordererror ) .",
    "first , observe that @xmath401 , and @xmath402 .",
    "now using the decomposition , @xmath403 ~\\psi _ { q_{k , n}(1)}(\\phi _ { k}(\\eta _ { k-1}^{n}))(dx_{k}^{\\prime } ) , \\end{array}\\]]it follows that @xmath404for linear functionals of the form ( [ eq : additivefunctionalsimple ] ) , it is easily checked that @xmath405 ( s_{q})+\\sum_{k < q\\leq n}q_{k , q}(s_{q}~q_{q , n}(1))\\]]with the convention @xmath406 , the identity operator , for @xmath407 . recalling that @xmath401 , we conclude that @xmath408 ( s_{q})+\\sum_{k <",
    "q\\leq n}\\frac{q_{k , q}(q_{q , n}(1)~s_{q})}{q_{k , q}(q_{q , n}(1))}\\]]and therefore @xmath409 ( s_{q})+\\sum_{k\\leq q\\leq n}\\frac{q_{k , q}(q_{q , n}(1)~s_{q})}{q_{k , q}(q_{q , n}(1))}\\]]thus , @xmath410using the estimates in ( [ eq : contractionest ] ) and ( eq : contractionest2 ) for the contraction coefficients , and the estimate in ( [ eq : contractionest ] ) for  @xmath411 , it follows that there exists some finite ( non random ) constant @xmath412 such that the bound @xmath413holds for any pair of time indexes @xmath346 satisfying @xmath168 , particle number @xmath54  and choice of functions @xmath171 . the desired bound for ( [ eq : oscboundparticleppn ] ) is now obtained by combining this result with lemma [ lem : kintchine]:@xmath414where @xmath415 is a constant whose value does not depend on @xmath416.concerning the term @xmath417  in ( [ eq : nonasympvartriainequality]).@xmath418 ^{2}\\right\\ } ^{\\frac{1}{2 } }   \\notag \\\\ & \\leq \\sum_{0\\leq k\\leq n}\\frac{1}{\\sqrt{n}}b_{k ,",
    "n}\\mathbb{e}\\left\\ { \\left [ \\sqrt{n}\\left ( \\eta _ { k}-\\eta _ { k}^{n}\\right ) \\overline{d}_{k , n}(1)\\times \\sqrt{n}v_{k}^{n}\\left ( \\overline{d}_{k , n}^{n}(\\widetilde{s}_{k , n}^{n})\\right ) \\right ] ^{2}\\right\\ } ^{\\frac{1}{2 } }   \\notag \\\\ & \\leq \\sum_{0\\leq k\\leq n}\\frac{1}{\\sqrt{n}}b_{k , n}\\mathbb{e}\\left\\ { \\left [ \\sqrt{n}\\left ( \\eta _ { k}-\\eta _ { k}^{n}\\right ) \\overline{d}_{k , n}(1)\\right ] ^{4}\\right\\ } ^{\\frac{1}{4 } }   \\notag \\\\ & \\times \\mathbb{e}\\left\\ { \\left [ \\sqrt{n}v_{k}^{n}\\left ( \\overline{d}_{k , n}^{n}(\\widetilde{s}_{k , n}^{n})\\right ) \\right ] ^{4}\\right\\ } ^{\\frac{1}{4 } }   \\notag \\\\ & \\leq \\frac{1}{\\sqrt{n}}e(n+1 )   \\label{eq : secondorderrem_l2}\\end{aligned}\\]]where @xmath419 is a constant whose value does not depend on @xmath416 .",
    "the second line follows from ( [ eq : contractionest ] ) and the third by the cauchy - schwartz inequality .",
    "the final line was arrived at by the same reasoning used to derive bound ( [ eq : fisrtordererror_meansquare ] ) and lemma [ lem : lperrorfilter ]",
    ". the assertion of the theorem may be verified by substituting bounds ( [ eq : fisrtordererror_meansquare ] ) and ( eq : secondorderrem_l2 ) into ( [ eq : nonasympvartriainequality ] ) .",
    "let @xmath425 denote the largest integer less than or equal to @xmath170 .",
    "since the result is obvious for @xmath426 , let @xmath427.@xmath428 where @xmath429 and @xmath430 it may be verified that @xmath431 and @xmath432 hence the result follows .",
    "rou , f. , del moral , p. and guyader , a. ( 2008 ) . a non asymptotic variance theorem for unnormalized feynman - kac particle models , technical report inria-00337392 .",
    "available at http://hal.inria.fr/inria-00337392_v1/                            elliott , r.j .",
    ", ford , j.j . and moore , j.b .",
    "( 2002 ) on - line almost - sure parameter estimation for partially observed discrete - time linear systems with known noise characteristics . _",
    "control sig .",
    "_ , * 16 * , 435 - 453 .",
    "kantas , n. , doucet , a. , singh , s.s . and maciejowski , j.m .",
    "an overview of sequential monte carlo methods for parameter estimation in general state - space models . in _",
    "proceedings ifac system identification _ ( sysid ) meeting .",
    "poyiadjis , g. , doucet , a. and singh , s.s .",
    "particle approximations of the score and observed information matrix in state - space models with application to parameter estimation . _",
    "biometrika _ , to appear ."
  ],
  "abstract_text": [
    "<S> sequential monte carlo ( smc ) methods are a widely used set of computational tools for inference in non - linear non - gaussian state - space models . </S>",
    "<S> we propose a new smc algorithm to compute the expectation of additive functionals recursively . essentially , it is an online or forward - only  implementation of a forward filtering backward smoothing smc algorithm proposed in @xcite . compared to the standard path space smc  estimator </S>",
    "<S> whose asymptotic variance increases quadratically with time even under favourable mixing assumptions , the asymptotic variance of the proposed smc  estimator only increases linearly with time . </S>",
    "<S> this forward smoothing procedure allows us to implement on - line maximum likelihood parameter estimation algorithms which do not suffer from the particle path degeneracy problem .    _ some key words _ : expectation - maximization , forward filtering backward smoothing , recursive maximum likelihood , sequential monte carlo , smoothing , state - space models . </S>"
  ]
}