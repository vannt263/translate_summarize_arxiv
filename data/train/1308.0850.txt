{
  "article_text": [
    "recurrent neural networks ( rnns ) are a rich class of dynamic models that have been used to generate sequences in domains as diverse as music  @xcite , text  @xcite and motion capture data  @xcite .",
    "rnns can be trained for sequence generation by processing real data sequences one step at a time and predicting what comes next . assuming the predictions",
    "are probabilistic , novel sequences can be generated from a trained network by iteratively sampling from the network s output distribution , then feeding in the sample as input at the next step .",
    "in other words by making the network treat its inventions as if they were real , much like a person dreaming .",
    "although the network itself is deterministic , the stochasticity injected by picking samples induces a distribution over sequences .",
    "this distribution is conditional , since the internal state of the network , and hence its predictive distribution , depends on the previous inputs .",
    "rnns are ` fuzzy ' in the sense that they do not use exact templates from the training data to make predictions , but rather  like other neural networks ",
    "use their internal representation to perform a high - dimensional interpolation between training examples .",
    "this distinguishes them from n - gram models and compression algorithms such as prediction by partial matching  @xcite , whose predictive distributions are determined by counting exact matches between the recent history and the training set .",
    "the result  which is immediately apparent from the samples in this paper  is that rnns ( unlike template - based algorithms ) synthesise and reconstitute the training data in a complex way , and rarely generate the same thing twice .",
    "furthermore , fuzzy predictions do not suffer from the curse of dimensionality , and are therefore much better at modelling real - valued or multivariate data than exact matches .    in principle",
    "a large enough rnn should be sufficient to generate sequences of arbitrary complexity . in practice however , standard rnns are unable to store information about past inputs for very long  @xcite . as well as diminishing their ability to model long - range structure ,",
    "this ` amnesia ' makes them prone to instability when generating sequences .",
    "the problem ( common to all conditional generative models ) is that if the network s predictions are only based on the last few inputs , and these inputs were themselves predicted by the network , it has little opportunity to recover from past mistakes . having a longer memory",
    "has a stabilising effect , because even if the network can not make sense of its recent history , it can look further back in the past to formulate its predictions .",
    "the problem of instability is especially acute with real - valued data , where it is easy for the predictions to stray from the manifold on which the training data lies .",
    "one remedy that has been proposed for conditional models is to inject noise into the predictions before feeding them back into the model  @xcite , thereby increasing the model s robustness to surprising inputs .",
    "however we believe that a better memory is a more profound and effective solution .",
    "long short - term memory ( lstm )  @xcite is an rnn architecture designed to be better at storing and accessing information than standard rnns .",
    "lstm has recently given state - of - the - art results in a variety of sequence processing tasks , including speech and handwriting recognition  @xcite .",
    "the main goal of this paper is to demonstrate that lstm can use its memory to generate complex , realistic sequences containing long - range structure .",
    "section  [ sec : pred_net ] defines a ` deep ' rnn composed of stacked lstm layers , and explains how it can be trained for next - step prediction and hence sequence generation .",
    "section  [ sec : text ] applies the prediction network to text from the penn treebank and hutter prize wikipedia datasets .",
    "the network s performance is competitive with state - of - the - art language models , and it works almost as well when predicting one character at a time as when predicting one word at a time .",
    "the highlight of the section is a generated sample of wikipedia text , which showcases the network s ability to model long - range dependencies .",
    "section  [ sec : hand_pred ] demonstrates how the prediction network can be applied to real - valued data through the use of a mixture density output layer , and provides experimental results on the iam online handwriting database .",
    "it also presents generated handwriting samples proving the network s ability to learn letters and short words direct from pen traces , and to model global features of handwriting style .",
    "section  [ sec : hand_synth ] introduces an extension to the prediction network that allows it to condition its outputs on a short annotation sequence whose alignment with the predictions is unknown .",
    "this makes it suitable for handwriting synthesis , where a human user inputs a text and the algorithm generates a handwritten version of it .",
    "the synthesis network is trained on the iam database , then used to generate cursive handwriting samples , some of which can not be distinguished from real data by the naked eye . a method for biasing the samples towards higher probability ( and greater legibility )",
    "is described , along with a technique for ` priming ' the samples on real data and thereby mimicking a particular writer s style . finally , concluding remarks and directions for future work are given in section  [ sec : conclusion ] .",
    "[ sec : pred_net ] fig .",
    "[ fig : deep_predictor ] illustrates the basic recurrent neural network prediction architecture used in this paper .",
    "an input vector sequence @xmath0 is passed through weighted connections to a stack of @xmath1 recurrently connected hidden layers to compute first the hidden vector sequences @xmath2 and then the output vector sequence @xmath3 .",
    "each output vector @xmath4 is used to parameterise a predictive distribution @xmath5 over the possible next inputs @xmath6 .",
    "the first element @xmath7 of every input sequence is always a null vector whose entries are all zero ; the network therefore emits a prediction for @xmath8 , the first real input , with no prior information .",
    "the network is ` deep ' in both space and time , in the sense that every piece of information passing either vertically or horizontally through the computation graph will be acted on by multiple successive weight matrices and nonlinearities .",
    "[ fig : deep_predictor ]    note the ` skip connections ' from the inputs to all hidden layers , and from all hidden layers to the outputs .",
    "these make it easier to train deep networks , by reducing the number of processing steps between the bottom of the network and the top , and thereby mitigating the ` vanishing gradient ' problem  @xcite . in the special case that @xmath9",
    "the architecture reduces to an ordinary , single layer next step prediction rnn .",
    "the hidden layer activations are computed by iterating the following equations from @xmath10 to @xmath11 and from @xmath12 to @xmath1 : @xmath13 where the @xmath14 terms denote weight matrices ( e.g.  @xmath15 is the weight matrix connecting the inputs to the @xmath16 hidden layer , @xmath17 is the recurrent connection at the first hidden layer , and so on ) , the @xmath18 terms denote bias vectors ( e.g.  @xmath19 is output bias vector ) and @xmath20 is the hidden layer function .    given the hidden sequences , the output sequence is computed as follows : @xmath21 where @xmath22 is the output layer function .",
    "the complete network therefore defines a function , parameterised by the weight matrices , from input histories @xmath23 to output vectors @xmath4 .",
    "the output vectors @xmath4 are used to parameterise the predictive distribution @xmath5 for the next input .",
    "the form of @xmath5 must be chosen carefully to match the input data .",
    "in particular , finding a good predictive distribution for high - dimensional , real - valued data ( usually referred to as _ density modelling _ ) , can be very challenging .",
    "the probability given by the network to the input sequence @xmath24 is @xmath25 and the sequence loss @xmath26 used to train the network is the negative logarithm of @xmath27 : @xmath28 the partial derivatives of the loss with respect to the network weights can be efficiently calculated with backpropagation through time  @xcite applied to the computation graph shown in fig .",
    "[ fig : deep_predictor ] , and the network can then be trained with gradient descent .",
    "[ sec : lstm ] in most rnns the hidden layer function @xmath20 is an elementwise application of a sigmoid function .",
    "however we have found that the long short - term memory ( lstm ) architecture  @xcite , which uses purpose - built _ memory cells _ to store information , is better at finding and exploiting long range dependencies in the data .",
    "[ fig : lstm ] illustrates a single lstm memory cell . for the version of lstm used in this paper  @xcite @xmath20",
    "is implemented by the following composite function : @xmath29 where @xmath30 is the logistic sigmoid function , and @xmath31 , @xmath32 , @xmath33 and @xmath34 are respectively the _ input gate _ , _ forget gate _ , _ output gate _",
    ", _ cell _ and _ cell input _ activation vectors , all of which are the same size as the hidden vector @xmath35 .",
    "the weight matrix subscripts have the obvious meaning , for example @xmath36 is the hidden - input gate matrix , @xmath37 is the input - output gate matrix etc .",
    "the weight matrices from the cell to gate vectors ( e.g.  @xmath38 ) are diagonal , so element @xmath39 in each gate vector only receives input from element @xmath39 of the cell vector .",
    "the bias terms ( which are added to @xmath31 , @xmath32 , @xmath34 and @xmath33 ) have been omitted for clarity .",
    "[ fig : lstm ]    the original lstm algorithm used a custom designed approximate gradient calculation that allowed the weights to be updated after every timestep  @xcite .",
    "however the full gradient can instead be calculated with backpropagation through time  @xcite , the method used in this paper .",
    "one difficulty when training lstm with the full gradient is that the derivatives sometimes become excessively large , leading to numerical problems . to prevent this ,",
    "all the experiments in this paper clipped the derivative of the loss with respect to the network inputs to the lstm layers ( before the sigmoid and @xmath40 functions are applied ) to lie within a predefined range .",
    "[ sec : text ] text data is discrete , and is typically presented to neural networks using ` one - hot ' input vectors .",
    "that is , if there are @xmath41 text classes in total , and class @xmath42 is fed in at time @xmath43 , then @xmath44 is a length @xmath41 vector whose entries are all zero except for the @xmath45 , which is one .",
    "@xmath46 is therefore a multinomial distribution , which can be naturally parameterised by a softmax function at the output layer : @xmath47 substituting into eq .",
    "( 6 ) we see that @xmath48 the only thing that remains to be decided is which set of classes to use . in most cases ,",
    "text prediction ( usually referred to as _ language modelling _ ) is performed at the word level .",
    "@xmath41 is therefore the number of words in the dictionary .",
    "this can be problematic for realistic tasks , where the number of words ( including variant conjugations , proper names , etc . )",
    "often exceeds 100,000 .",
    "as well as requiring many parameters to model , having so many classes demands a huge amount of training data to adequately cover the possible contexts for the words . in the case of softmax models , a further difficulty is the high computational cost of evaluating all the exponentials during training ( although several methods have been to devised make training large softmax layers more efficient , including tree - based models  @xcite , low rank approximations  @xcite and stochastic derivatives  @xcite ) .",
    "furthermore , word - level models are not applicable to text data containing non - word strings , such as multi - digit numbers or web addresses .",
    "character - level language modelling with neural networks has recently been considered  @xcite , and found to give slightly worse performance than equivalent word - level models .",
    "nonetheless , predicting one character at a time is more interesting from the perspective of sequence generation , because it allows the network to invent novel words and strings . in general , the experiments in this paper aim to predict at the finest granularity found in the data , so as to maximise the generative flexibility of the network .",
    "the first set of text prediction experiments focused on the penn treebank portion of the wall street journal corpus  @xcite .",
    "this was a preliminary study whose main purpose was to gauge the predictive power of the network , rather than to generate interesting sequences .    although a relatively small text corpus ( a little over a million words in total ) , the penn treebank data is widely used as a language modelling benchmark .",
    "the training set contains 930,000 words , the validation set contains 74,000 words and the test set contains 82,000 words .",
    "the vocabulary is limited to 10,000 words , with all other words mapped to a special ` unknown word ' token .",
    "the end - of - sentence token was included in the input sequences , and was counted in the sequence loss .",
    "the start - of - sentence marker was ignored , because its role is already fulfilled by the null vectors that begin the sequences ( c.f .",
    "section  [ sec : pred_net ] ) .",
    "the experiments compared the performance of word and character - level lstm predictors on the penn corpus . in both cases ,",
    "the network architecture was a single hidden layer with 1000 lstm units .",
    "for the character - level network the input and output layers were size 49 , giving approximately 4.3 m weights in total , while the word - level network had 10,000 inputs and outputs and around 54 m weights .",
    "the comparison is therefore somewhat unfair , as the word - level network had many more parameters .",
    "however , as the dataset is small , both networks were easily able to overfit the training data , and it is not clear whether the character - level network would have benefited from more weights .",
    "all networks were trained with stochastic gradient descent , using a learn rate of 0.0001 and a momentum of 0.99 .",
    "the lstm derivates were clipped in the range @xmath49 $ ] ( c.f .",
    "section  [ sec : lstm ] ) .",
    "neural networks are usually evaluated on test data with fixed weights . for prediction problems however , where the inputs _ are _ the targets , it is legitimate to allow the network to adapt its weights as it is being evaluated ( so long as it only sees the test data once ) .",
    "mikolov refers to this as _",
    "dynamic evaluation_. dynamic evaluation allows for a fairer comparison with compression algorithms , for which there is no division between training and test sets , as all data is only predicted once .    since both networks overfit the training data , we also experiment with two types of regularisation : weight noise  @xcite with a std .",
    "deviation of 0.075 applied to the network weights at the start of each training sequence , and adaptive weight noise  @xcite , where the variance of the noise is learned along with the weights using a minimum description length ( or equivalently , variational inference ) loss function .",
    "when weight noise was used , the network was initialised with the final weights of the unregularised network .",
    "similarly , when adaptive weight noise was used , the weights were initialised with those of the network trained with weight noise .",
    "we have found that retraining with iteratively increased regularisation is considerably faster than training from random weights with regularisation .",
    "adaptive weight noise was found to be prohibitively slow for the word - level network , so it was regularised with fixed - variance weight noise only .",
    "one advantage of adaptive weight is that early stopping is not needed ( the network can safely be stopped at the point of minimum total ` description length ' on the training data ) .",
    "however , to keep the comparison fair , the same training , validation and test sets were used for all experiments .",
    "the results are presented with two equivalent metrics : _ bits - per - character _ ( bpc ) , which is the average value of @xmath50 over the whole test set ; and _ perplexity _ which is two to the power of the average number of bits per word ( the average word length on the test set is about 5.6 characters , so perplexity @xmath51 ) .",
    "perplexity is the usual performance measure for language modelling .    [",
    "tab : penn ] 0.15 in    .*penn treebank test set results .",
    "* ` bpc ' is bits - per - character .",
    "` error ' is next - step classification error rate , for either characters or words . [",
    "cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]     -0.1 in      given @xmath52 , an unbiased sample can be picked from @xmath53 by iteratively drawing @xmath6 from @xmath54 , just as for the prediction network .",
    "the only difference is that we must also decide when the synthesis network has finished writing the text and should stop making any future decisions .",
    "to do this , we use the following heuristic : as soon as @xmath55 the current input @xmath44 is defined as the end of the sequence and sampling ends",
    ". examples of unbiased synthesis samples are shown in fig .",
    "[ fig : synth_val ] . these and all subsequent figures were generated using the synthesis network retrained with adaptive weight noise .",
    "notice how stylistic traits , such as character size , slant , cursiveness etc .",
    "vary widely between the samples , but remain more - or - less consistent within them .",
    "this suggests that the network identifies the traits early on in the sequence , then remembers them until the end . by looking through enough samples for a given text",
    ", it appears to be possible to find virtually any combination of stylistic traits , which suggests that the network models them independently both from each other and from the text . `",
    "blind taste tests ' carried out by the author during presentations suggest that at least some unbiased samples can not be distinguished from real handwriting by the human eye .",
    "nonetheless the network does make mistakes we would not expect a human writer to make , often involving missing , confused or garbled letters seems to have done . ] ; this suggests that the network sometimes has trouble determining the alignment between the characters and the trace .",
    "the number of mistakes increases markedly when less common words or phrases are included in the character sequence .",
    "presumably this is because the network learns an implicit character - level language model from the training set that gets confused when rare or unknown transitions occur .",
    "+   +   +   +   +    [ fig : synth_val ]      one problem with unbiased samples is that they tend to be difficult to read ( partly because real handwriting is difficult to read , and partly because the network is an imperfect model ) .",
    "intuitively , we would expect the network to give higher probability to good handwriting because it tends to be smoother and more predictable than bad handwriting .",
    "if this is true , we should aim to output more probable elements of @xmath53 if we want the samples to be easier to read .",
    "a principled search for high probability samples could lead to a difficult inference problem , as the probability of every output depends on all previous outputs .",
    "however a simple heuristic , where the sampler is biased towards more probable predictions at each step independently , generally gives good results .",
    "define the _ probability bias _",
    "@xmath18 as a real number greater than or equal to zero .",
    "before drawing a sample from @xmath5 , each standard deviation @xmath56 in the gaussian mixture is recalculated from eq .",
    "( 21 ) to @xmath57 and each mixture weight is recalculated from eq .",
    "( 19 ) to @xmath58 this artificially reduces the variance in both the choice of component from the mixture , and in the distribution of the component itself . when @xmath59 unbiased sampling is recovered , and as @xmath60 the variance in the sampling disappears and the network always outputs the mode of the most probable component in the mixture ( which is not necessarily the mode of the mixture , but at least a reasonable approximation ) .",
    "[ fig : synth_biased ] shows the effect of progressively increasing the bias , and fig .",
    "[ fig : synth_val_bias ] shows samples generated with a low bias for the same texts as fig .",
    "[ fig : synth_val ] .     are shown at the left . as the bias increases the diversity decreases and the samples tend towards a kind of ` average handwriting ' which is extremely regular and easy to read ( easier , in fact , than most of the real handwriting in the training set ) .",
    "note that even when the variance disappears , the same letter is not written the same way at different points in a sequence ( for examples the  e s in `` exactly the same '' , the ",
    "l s in `` until they all look '' ) , because the predictions are still influenced by the previous outputs .",
    "if you look closely you can see that the last three lines are not quite exactly the same . ]",
    "[ fig : synth_biased ]     +   +   +   +   +    [ fig : synth_val_bias ]      another reason to constrain the sampling would be to generate handwriting in the style of a particular writer ( rather than in a randomly selected style ) . the easiest way to do this would be to retrain it on that writer only .",
    "but even without retraining , it is possible to mimic a particular style by ` priming ' the network with a real sequence , then generating an extension with the real sequence still in the network s memory .",
    "this can be achieved for a real @xmath24 , @xmath52 and a synthesis character string @xmath61 by setting the character sequence to @xmath62 and clamping the data inputs to @xmath24 for the first @xmath11 timesteps , then sampling as usual until the sequence ends .",
    "examples of primed samples are shown in figs .",
    "[ fig : primed_samples_1 ] and  [ fig : primed_samples_2 ] .",
    "the fact that priming works proves that the network is able to remember stylistic features identified earlier on in the sequence .",
    "this technique appears to work better for sequences in the training data than those the network has never seen .",
    "[ fig : primed_samples_1 ]    primed sampling and reduced variance sampling can also be combined . as shown in figs .",
    "[ fig : bias_primed_1 ] and  [ fig : bias_primed_2 ] this tends to produce samples in a ` cleaned up ' version of the priming style , with overall stylistic traits such as slant and cursiveness retained , but the strokes appearing smoother and more regular .",
    "a possible application would be the artificial enhancement of poor handwriting .",
    "[ fig : bias_primed_1 ]",
    "[ sec : conclusion ] this paper has demonstrated the ability of long short - term memory recurrent neural networks to generate both discrete and real - valued sequences with complex , long - range structure using next - step prediction .",
    "it has also introduced a novel convolutional mechanism that allows a recurrent network to condition its predictions on an auxiliary annotation sequence , and used this approach to synthesise diverse and realistic samples of online handwriting .",
    "furthermore , it has shown how these samples can be biased towards greater legibility , and how they can be modelled on the style of a particular writer .",
    "several directions for future work suggest themselves .",
    "one is the application of the network to speech synthesis , which is likely to be more challenging than handwriting synthesis due to the greater dimensionality of the data points .",
    "another is to gain a better insight into the internal representation of the data , and to use this to manipulate the sample distribution directly",
    ". it would also be interesting to develop a mechanism to automatically extract high - level annotations from sequence data . in the case of handwriting , this could allow for more nuanced annotations than just text , for example stylistic features , different forms of the same letter , information about stroke order and so on .",
    "thanks to yichuan tang , ilya sutskever , navdeep jaitly , geoffrey hinton and other colleagues at the university of toronto for numerous useful comments and suggestions .",
    "this work was supported by a global scholarship from the canadian institute for advanced research ."
  ],
  "abstract_text": [
    "<S> this paper shows how long short - term memory recurrent neural networks can be used to generate complex sequences with long - range structure , simply by predicting one data point at a time . </S>",
    "<S> the approach is demonstrated for text ( where the data are discrete ) and online handwriting ( where the data are real - valued ) . </S>",
    "<S> it is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence . </S>",
    "<S> the resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles . </S>"
  ]
}