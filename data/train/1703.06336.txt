{
  "article_text": [
    "consider the multiple testing problem of simultaneously testing a large number @xmath0 of hypotheses .",
    "when @xmath0 is large , standard multiple testing procedures suffer from low  power \" and are unable to distinguish between null and alternative effects because extremely small @xmath1-values are required if one properly accounts for type 1 error control , such as the familywise error rate ( fwer ) ; see lehmann and romano ( 2005 ) .",
    "it is only by weakening the measure of error control , such as the false discovery rate ( fdr ) , that some discoveries may be found ( benjamin and hochberg , 1995 ) .",
    "but , such discoveries are not as forceful as when they arise while controlling the fwer .    when  most \" null hypotheses are  true \" , a common and useful approach is to first reduce the number of hypotheses being testing in order to construct methods which are better able to distinguish alternative hypotheses .",
    "that is , one applies some selection , filtering or screening technique based on some selection statistics in order to reduce the number of hypotheses being tested .",
    "then , one can use standard stepwise methods to test the reduced number of tests .",
    "such two - stage methods have been extensively used in practice to deal with various problems of multiple testing ( mcclintick and edenberg , 2006 ; talloen et al .",
    ", 2007 ; hackstadt and hess , 2009 ) .",
    "as in the bulk of this paper , such approaches are called two - stage procedures . in the first stage , some screening or selection method is applied in order to reduce the number of tests . in the second stage ,",
    "the reduced number of tests is tested .",
    "a major limitation of these methods is there lacks a systematic consideration of the selection effect .",
    "in other words , one can not simply apply some method to the reduced number of hypotheses without accounting for selection in error control .",
    "that is , one can not in general ",
    "forget \" about the screening stage .",
    "in other words , in order to properly control type 1 error rates , one must in general account for the screening stage by considering the error rate conditional on the method of selection . otherwise ,",
    "lose of type 1 error control , whether it is fdr , fwer , or an alternative measure , results .",
    "but , if screening statistics at the first stage are chosen to be independent of the testing statistics at the second stage ( at least under the null hypothesis ) , then error control simplifies as the conditional distributions and unconditional distributions of the test statistics are the same ( at least under its respective null distribution )",
    ". indeed , bourgon , gentleman , and huber ( 2010 ) introduced such a novel approach of independence filtering to avoid the effect of selection , in which the selection or filtering statistics at the first stage are chosen to be independent of the test statistics ( at least when the corresponding null hypotheses are true ) .",
    "two new two - stage methods , which respectively combine the approach of independence filtering with the conventional bonferroni and benjamini - hochberg procedures ( benjamini and hochberg , 1995 ) , are proposed and shown to control both the fwer and fdr under independence of test statistics . by using the same idea of independence filtering , dai et al .",
    "( 2012 ) develop several two - stage testing procedures to detect gene - environment interaction in genome - wide association studies .",
    "kim and schliekelman ( 2016 ) further discuss some key questions on how to best apply the approach of independence filtering and quantify the effects of the quality of the filter information , the filter cutoff and other factors on the effectiveness of the filter .",
    "another commonly used approach to avoid the selection effect is sample splitting in which the data is split in two independent parts .",
    "one uses the first part of the data to construct the selection or filtering statistics and the second part to construct the test statistics . by combining sample splitting with conventional stepwise procedures",
    ", one can develop two - stage procedures that guarantee control of type 1 error rates ( cox , 1975 ; rubin , dudoit , and van der laan , 2006 ; wasserman and roeder , 2009 ) .",
    "these methods completely remove the effect of selection ; however , they often result in power loss due to reduced sample size for testing ( skol , et al . , 2006 ; fithian , sun and taylor , 2014 ) .",
    "in recent years , there has been a growing interest in selective inference ( benjamin and yekutieli , 2005 ; benjamini , 2010 ; taylor and tibshirani , 2015 ) and several novel breakthroughs have been made in the context of high - dimensional regression ( berk et al 2013 ; barber and cand@xmath2s 2015 ; lee et al 2016 ; fithian et al .",
    "all of these developments take model selection rules as given and develop methods to preform valid inference after taking into account selection effects . along these lines ,",
    "a number of selective inference / post selection inference methods have been developed for various model selection algorithms ( barber and candes , 2016 ; benjamini and bogomolov , 2014 ; fithian et al .",
    ", 2015 ; heller et al . , 2016 ; tian and taylor , 2015a , b ; weinstein , fithian and benjamini , 2013 ; yekutieli ; 2012 ) . in this literature , the problem of how to choose selection rules is often overlooked ; however , in practice one can often choose a desired selection rule to lead to favorable conditional properties of inference after selection .",
    "in contrast , rather than treat the selected hypotheses as given , we can propose a rule in both stages so that the overall procedure has good unconditional error control properties .",
    "another popular way of exploiting information in the data is , rather than completely eliminating tests under consideration , to construct weights for the null hypotheses and then develop data - driven weighted multiple testing procedures ( roeder and wasserman , 2009 ; poisson et al , 2012 ) .",
    "the data - driven weighted methods are pretty general and filtering methods can regarded as its special case .",
    "a limitation of such methods is that it is not clear how to assign weights in a data - driven way to ensure control of the fwer or fdr .",
    "very recently , by using  covariates \" to construct weights which are independent of the test statistics under the null hypotheses , several bonferroni - based and benjamini - hochberg based data driven weighted methods have been developed that increase power while controlling the fwer and fdr , respectively ( fino and salmaso , 2007 ; ignatiadis , et al , 2016 ; li and barber , 2016 ; lei and fithian , 2016 ; ignatiadis and huber , 2017 ) .",
    "in addition , when developing more powerful multiple testing methods , there are several other ways of using such additional covariate information that have recently introduced in the literature , such as local fdr based approaches ( cai and sun , 2009 ) , stratified benjamini - hochberg ( yoo , et al . , 2010 ) , grouped benjamini - hochberg ( hu , zhao and zhou , 2010 ) , and single - index modulated method ( du and zhang , 2014 ) , etc .    in summary , there is a growing literature of approaches to dimension reduction in high dimensional ( single and multiple ) hypothesis testing , including some useful , novel , and somewhat ad hoc procedures .",
    "the contribution of this paper is to perform a detailed error analysis in a large scale setting .",
    "we consider an ideal gaussian model , as is often assumed in the literature . as described in the setup in section 2 .",
    "there , we introduce a specific two - stage procedure that we will analyze and compare later with other procedures .",
    "control of the fwer is presented , though the less formal argument already appears in bourgon , gentleman and huber ( 2010 ) .",
    "( the analysis applies to the joint but single testing problem of testing all means zero against the alternative that at least one is not , but the exposition emphasizes the multiple testing problem . ) the remainder of the paper is new . in section 3 , under a large @xmath0 asymptotic framework with a sparsity assumption on the number of false hypotheses , we present detection boundaries for mean levels that can ( or can not be ) detected by the two - stage procedure . in section 4 , a refinement is obtained so that the exact cutoff is calculated .",
    "section 5 considers the unknown variance case , where the basic finite sample control of the fwer is replaced by asymptotic control , but the same power analysis holds as when the variance is known . in section 6 ,",
    "we allow for dependence between the test statistics .",
    "section 7 theoretically compares the two - stage approach with other methods : bonferroni and split - sample methods . by proper choice of how to split",
    ", the split sample technique can only perform as well as bonferroni , with neither approach performing as well as the two - stage method .",
    "a simulation study is presented in section 8 .",
    "both global tests of a single hypothesis ( in a high dimensional setting ) as well as multiple tests are considered . in the former case , the higher criticism ( donoho and jin , 2004 ; donoho and jin , 2015 )",
    "is also compared ( but it can not readily be used in the multiple testing case ) . in both cases , the two - stage approach offers both control of the type 1 error rate as well as it performs quite well under various scenarios .",
    "in particular , the two - stage method shows good performance even when variances are unequal and especially under dependence .",
    "a very stylized gaussian setup is assumed , as is conventional in large scale testing .",
    "the problem is testing @xmath0 means from independent populations , where @xmath0 is large .",
    "assume that , for @xmath3 , a sample of size @xmath4 from a normal population with unknown mean @xmath5 and variance @xmath6 is observed ; that is , data @xmath7 where @xmath0 is the number of hypotheses of interest representing the number of samples or populations , and @xmath4 is the sample size for the @xmath8th sample .",
    "the @xmath0 samples are assumed mutually independent .",
    "when @xmath0 is large , it is typically assumed that the @xmath9 are known as well , in which case one can take @xmath10 ( by sufficiency ) .",
    "for now , we will assume @xmath11 and @xmath12 , though we will discuss the unknown variances case later .    for @xmath13 ,",
    "consider testing hypotheses @xmath14 ( one may also treat the case of one - sided alternatives with easy modifications . ) define the following two statistics @xmath15 and @xmath16 where @xmath17 and @xmath18 are respectively the sample mean and ( unbiased ) sample variance for the @xmath8th sample , i.e. , @xmath19 and @xmath20 .",
    "the basic two - stage strategy for our method is as follows .",
    "the statistics @xmath21 are first used to  select \" which of the hypotheses to  test \" in the second stage , at which point the statistics @xmath22 are used .",
    "there are various choices for the selection statistics , as well as test statistics .",
    "for example , one could use the @xmath23-statistics @xmath22 in both stages .",
    "regardless , the first consideration would then be how to set critical values in each stage in order to ensure some measure of type 1 error control , such as the familywise error rate ( fwer ) , the probability of at least one false rejection",
    ". we will be specific about the critical values soon , but the key motivation for the choice of the sum of squares selection statistic @xmath21 and test statistic @xmath24 is based on the following well - known facts .",
    "first , under @xmath25 ( and @xmath12 ) we have that @xmath26 that is , @xmath21 has the chi - squared distribution with @xmath27 degrees of freedom and @xmath22 has the @xmath23-distribution with @xmath28 degrees of freedom .",
    "but , the more important reason motivating our choice is that , by basu s theorem , @xmath21 and @xmath22 are independent under @xmath29 ( lehmann and romano , 2005 ) . note that @xmath30 , so that larger values of @xmath21 are indicative of larger values of @xmath31 .",
    "a simple selection rule is used for selecting which hypotheses @xmath29 are to be tested at the second stage . given a threshold @xmath32 , @xmath29 is selected iff @xmath33 .",
    "let @xmath34 denote the indices of selected hypotheses , with @xmath35 the number of selected hypotheses .",
    "at the second stage , one can simply apply the bonferroni test ; that is , reject @xmath29 iff @xmath36 , the @xmath37 quantile of the @xmath23-distribution with @xmath28 degrees of freedom .",
    "[ lemma : control ] for any choice of the threshold @xmath32 , the above two - stage procedure controls the fwer at level @xmath38 .",
    "like all proofs , see the appendix .",
    "the proof of lemma [ lemma : control ] requires that any test statistic @xmath22 be independent of the selection statistics @xmath39 , if @xmath29 is true .",
    "note that it is not required that the test statistics @xmath40 are jointly independent of the selection statistics .",
    "more generally , the two - stage procedure controls the familywise error rate when any test statistic is independent of the selection statistics , even outside our stylized gaussian model .",
    "the simple two - stage method can be improved by a holm - type stepdown improvement . to describe the method , simply apply the holm method ( holm , 1979 ) to the @xmath1-values based on the selected set of hypotheses .",
    "more specifically , let @xmath41 denote the marginal @xmath1-value when testing @xmath29 based on @xmath22 .",
    "of course , in the model above , this is just the probability that a @xmath23-distribution with @xmath28 degrees of freedom exceeds the observed value of @xmath42 .",
    "let @xmath43 be one if @xmath29 is not selected and equal to @xmath41 if it is selected .",
    "let @xmath44 denote the ordered @xmath1-values , so that @xmath45 is the index of the @xmath8th most significant @xmath1-value .",
    "now , apply holm s procedure based on the @xmath1-values @xmath46 with @xmath47 .",
    "thus , @xmath48 is rejected if @xmath49 for @xmath50 .    [ theorem : holm ] under the setting of lemma [ lemma : control ] , apply the holm method to the selected set of hypotheses .",
    "then , this modified procedure controls the fwer at level @xmath38 .",
    "thus , one can do even better by using a holm - like stepdown method , or even a stepdown version of sidak s procedure ; see lehmann and romano ( 2005 ) and guo and romano ( 2007 ) .",
    "indeed , conditional on the selection statistics , all computed true null @xmath1-values based on  detection \" statistics at the second stage are conditionally uniform on @xmath51 and hence unconditionally as well .",
    "thus , any multiple testing method based on @xmath1-values is available .",
    "for example , one can also apply the benjamini - hochberg procedure based on the selected @xmath1-values for controlling the false discovery rate ( benjamini and hochberg , 1995 ) . in all such cases ,",
    "the motivation is that gains are possible because at the second stage only a reduced number of hypotheses are tested , with the hopes of increased ability to detect or discover false null hypotheses .",
    "furthermore , both the selection and detection stages are based on the full data ( rather than a split sample approach which is used to obtain independence of the stages ) and there is no selection effect because of independence between the selection and test statistics when the corresponding hypothesis is true .",
    "so far , the threshold for selection has been just generically set at some constant @xmath32 .",
    "we now discuss this choice . for our method",
    ", we will choose @xmath32 of the form @xmath52 , the @xmath53 quantile of @xmath54 .",
    "since @xmath55 when @xmath29 is true , such a selection threshold @xmath56 ensures that roughly @xmath57 hypotheses are selected , at least if most null hypotheses are true .",
    "the question now is how to choose @xmath58 .",
    "let @xmath59 and @xmath60 , where @xmath61 is a given positive constant satisfying @xmath62 .",
    "then , roughly @xmath63 hypotheses are selected for testing .",
    "a choice of @xmath61 must still be specified .",
    "since type 1 error control is ensured regardless of the choice of @xmath61 , we now turn to studying the power of the procedure . in our asymptotic analysis ,",
    "the following is assumed .",
    "* assumption a : * @xmath64 , where @xmath65 is a nonnegative constant .    note that as @xmath0 is equal to @xmath66 , @xmath67 or @xmath68 , the values of @xmath69 are respectively @xmath70 and @xmath71 .",
    "so , it is reasonable and often sufficient to characterize the relationship between @xmath0 and @xmath27 by imposing assumption a. in applications , @xmath72 and @xmath27 ( and hence @xmath73 ) are known , and generally we will have @xmath74 .",
    "we will consider the probability of rejecting a null hypothesis @xmath75 having mean @xmath76 , which without loss of generality can be taken to be positive .",
    "further assume without loss of generality that it is @xmath77 that is false with mean @xmath78 .",
    "if @xmath79 is constant , then under assumptions a and @xmath80 , we have @xmath81 . on the other hand , if @xmath79 varies with @xmath0 ( and @xmath27 ) such that @xmath82 as @xmath0 approaches infinity , then @xmath83 finally , if the sample size @xmath27 is very large , so that @xmath84 is very small compared to the sample size @xmath27 , then the value of @xmath65 should be taken to be 0 . in the following ,",
    "we mainly perform asymptotic power analyses under assumption a. sometimes , @xmath80 is assumed , in which case the case @xmath85 can either be treated separately with ease , or by a limiting argument as @xmath65 tends to zero .",
    "in order to analyze the power of the two - stage procedure , we break up the analysis in two parts . the first part analyzes the probability of  selection \" in the first stage , while the second will analyze the probability of  detection \" in the second stage .",
    "rejection of @xmath29 then occurs when both @xmath29 has been selected at the first stage and then detection occurs at the second stage .",
    "roughly , the basic goal will be to determine how large in absolute value an alternative mean must be in order to ensure that the probability of rejection tends to one .",
    "consider the case where @xmath78 is a constant , so that @xmath77 is false .",
    "we now consider the asymptotic behavior of the probability that @xmath77 is selected in the first stage of the two - stage procedure .",
    "recall that @xmath56 denotes the @xmath86 quantile of @xmath54 , the chi - squared distribution with @xmath27 degrees of freedom , i.e. , @xmath87 hypothesis @xmath77 is selected if @xmath88 .",
    "[ lemma : select ]    \\(i ) under assumption a , if @xmath89 then @xmath90    \\(ii ) under assumption a , if @xmath91 then @xmath92    in lemma [ lemma : select](i ) , if @xmath85 , then the condition ( [ equation : w1 ] ) always holds , while in ( ii ) if @xmath85 the condition ( [ equation : w2 ] ) never holds , which implies @xmath77 is selected with probability tending to one .",
    "note that there exists a gap between the two detection thresholds in lemma [ lemma : select ] , but we will derive an improved , exact result in section 4 .",
    "we now consider the probability that @xmath79 is detected at the second stage using the @xmath23-statistic @xmath93 .",
    "that is , we now analyze the probability that @xmath94 exceeds @xmath95 , regardless of whether or not @xmath77 is selected at the first stage .",
    "later , we will analyze the two stages jointly , but for now note that if @xmath77 is false , then it is no longer the case that the selection statistic @xmath96 and the detection statistic @xmath93 are independent .",
    "first , in order to understand the detection probability , we need to understand @xmath35 , the number of selections from the first stage ( as it is random ) .",
    "let @xmath97 denote the indices of true null hypotheses from @xmath98 to @xmath0 , and let @xmath99 denote the indices of false null hypotheses from @xmath98 to @xmath0 .",
    "let @xmath100 and @xmath101 denote the number of true and false null hypotheses , respectively , from @xmath102 .",
    "we will assume some degree of sparsity in the sense @xmath103 for some @xmath104 .",
    "we will even allow @xmath105 , treating the  needle in the haystack \" problem , where exactly one alternative hypothesis is true .",
    "[ lemma : cheb ] the number of selected hypotheses @xmath35 satisfies @xmath106 if we assume the sparsity condition ( [ equation : sparse ] ) , then @xmath107 .",
    "[ lemma : detect ] under assumptions a and ( [ equation : sparse ] ) , we have    * when @xmath108 , @xmath109 ; * when @xmath110 , @xmath111 .    obviously ,",
    "if @xmath85 , then @xmath112 for any @xmath78 .",
    "we now combine the two stages to determine the value of @xmath5 that leads to rejection of @xmath29 .",
    "let @xmath113 be the event that @xmath29 is selected in the first stage and let @xmath114 be the event that @xmath115 at the second stage .",
    "note @xmath113 and @xmath114 are dependent in general .",
    "then , the power of the two - stage method , i.e. , the probability that @xmath29 is rejected , is @xmath116 therefore , in order for rejection of @xmath29 to occur with probability tending to one , it is sufficient to show both @xmath113 and @xmath114 have probability tending to one . also , we have @xmath117    combining lemma [ lemma : detect ] and [ lemma : select ] , the following result holds .",
    "[ theorem:31 ] under assumption a and ( [ equation : sparse ] ) , we have    * when @xmath118 , @xmath119 * when @xmath120 @xmath121    [ corollary:31 ] under assumption a with @xmath85 , for any given @xmath62 , ( [ equation : sparse ] ) and any @xmath122 , @xmath123    of course , in multiple testing problems , there are many notions of power one might wish to maximize : the probability of rejecting at least one false null hypothesis , the probability of rejecting all false null hypotheses , the probability of rejecting at least @xmath124 false null hypotheses ( for any given @xmath124 ) , the expected number ( or proportion ) of rejections among false null hypothesis , etc .",
    "theorem [ theorem:31 ] and corollary [ corollary:31 ] apply directly to the expected proportion of false null hypotheses rejected .",
    "for example , in the setting where all false null hypotheses have a common mean @xmath79 , then the expected proportion of correct rejections equals the probability that any one of them is rejected , which tends to one ( or not ) based on the threshold for @xmath79 .",
    "in order to improve theorem [ theorem:31 ] , we need to derive improved bounds on extreme chi - squared quantiles .",
    "( note the slack in the bounds provided in lemmas [ lemma : lm ] and [ lemma : i ] . )",
    "let @xmath125 which is increasing on @xmath126 .",
    "then , define @xmath127 ^ 2~,\\ ] ] which is decreasing in @xmath128 .",
    "[ lemma : cstar ] given the value @xmath61 used in stage one for selection with @xmath129 , and @xmath65 in assumption a , with @xmath80 , define @xmath130 to be the solution of the equation @xmath131",
    "\\(i ) for any @xmath132 and sufficiently large @xmath27 , @xmath133    \\(i ) for any @xmath134 and sufficiently large @xmath27 , @xmath135    based on lemma [ lemma : cstar ] , lemma [ lemma : select ] can be improved as follows .",
    "[ lemma : improve ] under assumption a and ( [ equation : sparse ] ) , we have    * when @xmath136 , @xmath137 . * when @xmath138 , @xmath139 .    combining lemma [ lemma : improve ] and lemma [ lemma : detect ] , theorem [ theorem:31 ]",
    "can be improved as follows .",
    "[ theorem:41 ] under assumption a and ( [ equation : sparse ] ) , we have    * when @xmath140 , @xmath141 * when @xmath142 , @xmath121    theorem [ theorem:41 ] offers an approach of determining the value of tuning parameter @xmath61 . by minimizing the right - hand side of the inequality in theorem [ theorem:41 ] ( i ) or ( ii ) with respect to @xmath61 , one can determine an optimal value @xmath143 of @xmath61 for each given value of @xmath65 , which maximizes probability of detecting any false null or average power asymptotically . as seen from figure 4.1 ( left ) ,",
    "the chosen value @xmath143 of @xmath61 is decreasing in @xmath65 .",
    "note that @xmath144 , thus @xmath143 is roughly increasing in @xmath27 if @xmath0 is fixed and decreasing in @xmath0 if @xmath27 is fixed .",
    "for instance , suppose @xmath145 and @xmath146 , then @xmath147 . by checking figure 4.1 ( left ) ,",
    "the determined value @xmath143 of @xmath61 is about @xmath148 , which implies that about @xmath149 hypotheses are selected in the first stage for detection .",
    "based on the optimal value @xmath143 of @xmath61 , we can determine by theorem [ theorem:41 ] the upper bound of squared mean @xmath150 for our suggested two - stage bonferroni procedure , which constitutes a sharp detection threshold .",
    "when @xmath150 is larger than the bound , we can always detect @xmath151 .",
    "similarly , we can also determine by theorem [ theorem : bonpower ] the detection threshold of @xmath150 for the conventional bonferroni procedure .",
    "figure 4.1 ( right ) shows the detection thresholds of @xmath150 for these two procedures .",
    "as seen from figure 4.1 ( right ) , the detection thresholds of our suggested procedure are always lower than those of conventional bonferroni procedure for different values of @xmath65 , and their differences are increasingly larger with increasing @xmath65 .",
    "this implies that our suggested two - stage bonferroni procedure is more powerful than the conventional bonferorni procedure and its power improvement over the bonferroni procedure becomes increasingly larger with increasing @xmath65 .",
    "specifically , the detection threshold of our suggested procedure is almost linear in terms of @xmath65 with the slope being about @xmath152 and that of the conventional bonferroni procedure is an exponential function of @xmath65 .",
    "[ figure:4.1 ]     and the corresponding detection threshold ( right panel ) of squared mean @xmath150 in theorem [ theorem:41 ] for our proposed two - stage bonferroni procedure ( ts bonf . ) along with the detection threshold of @xmath150 in theorem [ theorem : bonpower ] for the conventional bonferroni procedure ( bonf . ) . ]",
    "the goal of this section is to show asymptotic control of the fwer is retained when @xmath6 are the same as unknown @xmath154 and @xmath154 is estimated . to this end , let @xmath155 denote an overall estimator of @xmath154 which satisfies @xmath156 actually , ( [ equation : hats ] ) can be weakened but it holds if we take the average or median of the @xmath0 sample variances computed from each of the @xmath0 samples . consider the modified procedure based on the selection set @xmath157 where @xmath158 and @xmath159 is the critical value used in selection when it is known that @xmath160 .",
    "the modified two - stage procedure is identical in the second stage in that , for each @xmath161 , @xmath29 is rejected if its corresponding @xmath23-statistic @xmath22 exceeds the @xmath162 quantile of the @xmath23-distribution with @xmath28 degrees of freedom , where @xmath163 denotes the number of selected hypotheses at the first stage .",
    "[ theorem : esigma ] assume assumption a.    \\(i ) for @xmath164 , the above modified two - stage procedure asymptotically controls the familywise error rate as @xmath165 .",
    "\\(ii ) for @xmath166 and @xmath80 , the above modified two - stage procedure asymptotically controls the familywise error rate as @xmath165 .",
    "in fact , the same is true if @xmath167~,\\ ] ] where @xmath168 and @xmath169 defined in ( [ equation : cstar ] ) .    the power analysis used to derive theorems [ theorem:31 ] and [ theorem:41 ] applies equally well to the above modified procedure when @xmath153 is estimated .",
    "of course , at the second stage , the detection probability analysis remains completely unchanged since there is no modification in the second stage . in the first stage ,",
    "the argument for selection can be used along with the assumption ( [ equation : hats ] ) to yield the same results , as the argument is basically the same .",
    "we now extend the two - stage method when the tests are dependent .",
    "the setup is similar to that described in section 2 .",
    "assume we have i.i.d .",
    "observations @xmath170 , where @xmath171 and the @xmath0 components of @xmath172 may be dependent . as before , @xmath173 is @xmath174 .",
    "( note that it is not necessary to assume @xmath172 is multivariate gaussian , but just that the one - dimensional marginal distributions are gaussian . )",
    "we firstly discuss the case of known @xmath153 . for convenience ,",
    "we still assume @xmath160 . the two - stage procedure is based on the same selection statistic @xmath21 and detection statistic @xmath22 as before .",
    "the two - stage procedure selects any @xmath29 for which @xmath175 and then rejects @xmath29 if also @xmath176 exceeds @xmath177 , where @xmath34 is the set of indices @xmath8 such that @xmath178 and @xmath35 is the number of selections at the first stage .",
    "let @xmath179 and @xmath180 be the set of indices of the selected true null hypotheses , i.e. , @xmath181 we make the following assumptions regarding @xmath182 and @xmath183 , in which the assumption regarding @xmath183 was already shown to hold under independence in lemma 3.2 .",
    "* assumption b1 : * @xmath184 as @xmath185 where @xmath186 is a fixed constant .    in assumption b1",
    ", @xmath187 corresponds to sparsity . by assumption b1 ,",
    "we have    @xmath188    so one can expect the following assumption b2 :    * assumption b2 : * @xmath189 as @xmath190 .",
    "based on ( [ equation : assumb1 ] ) , to show assumption b2 , one just needs @xmath191 which holds under weak dependence .",
    "[ theorem : dependence1 ] assume assumptions b1 and b2 .",
    "the two - stage procedure discussed in lemma [ lemma : control ] with @xmath179 asymptotically controls the familywise error rate at level @xmath38 .",
    "it is interesting to note that in theorem [ theorem : dependence1 ] , we do not make any assumption of dependence on false null statistics .",
    "only some weak dependence is imposed on true null statistics .    by checking the whole proof of theorem [ theorem : dependence1 ] , one can see that if the following assumption instead of b2 is imposed , @xmath192 theorem [ theorem : dependence1 ] still holds .",
    "[ remark : block depend]when the selection statistics @xmath21 are weakly dependent , assumption b2 is satisfied . in the following",
    ", we present such an example of block dependence satisfying assumption b2 .",
    "let @xmath193 for @xmath3 .",
    "suppose @xmath194 forming @xmath195 blocks of sizes @xmath196 each , which are reformulated as @xmath197 for @xmath198 blocks , are independent to each other , with @xmath199 , @xmath200 and @xmath201 as @xmath190 , where @xmath186 .",
    "note that @xmath202 in the following , we show that assumption b2 is satisfied under such block dependence . note that @xmath203 thus , by block independence of @xmath204 , we have @xmath205 we know that @xmath206 combining the above two inequalities , @xmath207 note that @xmath208 by chebychev s inequality , we have @xmath209 and thus assumption b2 is satisfied .     when @xmath6 are the same as unknown @xmath154 and @xmath154 is estimated , we consider the modified two - stage procedure discussed in theorem [ theorem : esigma ] . by using similar arguments as in the proof of theorem [ theorem : dependence1 ] , we can also show that asymptotic control of the fwer is retained for this procedure under dependence .    for any given @xmath210 and @xmath179 , define @xmath211 except for assumption b1 , we also make the following two assumptions regarding @xmath155 and @xmath212 :    * assumption b3 : * @xmath213 .",
    "* assumption b4 : * @xmath214 as @xmath185 where @xmath215 for some @xmath216 slowly .",
    "we should note that assumption b3 has been presented in section 5 and assumption b4 is a slight extension of assumption b2 .",
    "[ theorem : dependence2 ] assume assumptions b1 , b3 and b4 .",
    "the two - stage procedure discussed in theorem [ theorem : esigma ] asymptotically controls the familywise error rate at level @xmath38 .",
    "when the selection statistics @xmath21 are block dependent , if the overall estimate @xmath155 is chosen as @xmath217 we can similarly show that assumptions b3 and b4 are satisfied under block dependence by using the similar arguments as in the case of known variance where we showed in remark [ remark : block depend ] that assumption b2 is satisfied under block dependence .",
    "in this section , we perform a corresponding power analysis with some alternative methods .",
    "first , we consider the bonferroni method , which rejects @xmath29 if @xmath218 .",
    "we consider the power or rejection probability of @xmath29 when @xmath5 is the mean .",
    "[ theorem : bonpower ] assume assumption a. for the original bonferroni method ,    \\(i ) when @xmath219 , @xmath220    \\(ii ) when @xmath221 , @xmath222    in theorem [ theorem : bonpower ] , if @xmath85 , then the stated condition in ( i ) always holds , which implies @xmath77 is rejected by the bonferroni procedure with probability tending to one . on the other hand , the stated condition in ( ii ) holds for any large @xmath223 if @xmath65 is large enough , which implies @xmath77",
    "is rejected with probability tending to zero .    in the case of known variance , one can use a @xmath224-statistic with a normal quantile @xmath225 .",
    "similar to the proof of theorem [ theorem : bonpower ] , it can be shown that the threshold @xmath226 can be replaced by @xmath227 .      a common way ( skol et al . 2006 ;",
    "wasserman and roeder 2009 ) to achieve a reduction in the number of tests is to split the sample in two @xmath228 independent parts . the first part , based on @xmath229 observations is used to determine which hypotheses will be selected .",
    "then , those selected hypotheses are tested based on the independent set of @xmath230 observations .",
    "since the two subsamples are independent ( as we have been assuming all @xmath27 observations are i.i.d . ) , it is easy to control the fwer . indeed , suppose the first subsample produces a reduced set of hypotheses with indices @xmath34 , so that the number of selected hypotheses is @xmath231 . then , the bonferroni procedure applied to the remaining @xmath230 observations evidently controls the fwer . specifically , for @xmath232 , suppose @xmath233 denotes the @xmath23-statistic computed on the @xmath124th subsample of size @xmath234 for testing @xmath29 . here",
    ", @xmath29 is selected if @xmath235 , for some cutoff @xmath32 . here",
    ", we will take @xmath32 to be of the form @xmath236 for some @xmath62 .",
    "if @xmath231 denotes the number of @xmath237 satisfying the inequality so that @xmath29 is selected , then @xmath29 is rejected at the second stage if also @xmath238 for any cutoff @xmath32 used for selection , this procedure controls the fwer .",
    "we would like to determine the smallest value of @xmath239 where such a procedure has limiting power one .",
    "[ theorem : split ] assume assumption a. also assume @xmath240 and the sparsity condition ( [ equation : sparse ] ) . for the above split sample method ,    \\(i )",
    "when @xmath241 - 1 $ ] , @xmath220    \\(ii ) when @xmath242 - 1 $ ] , @xmath222    by theorem [ theorem : split ] , the detection threshold ( or rather its square ) of the split sample method is equal to @xmath243 - 1~,\\ ] ] which depends on @xmath65 , which we set as @xmath244 , a choice of @xmath61 , as well as the choice of @xmath245 to determine the split sample sizes .",
    "we want the threshold to be as small as possible . with",
    "@xmath65 fixed , minimizing over both @xmath246 and @xmath245 requires minimizing @xmath247 $ ] . if @xmath245 is fixed , the optimizing choice of @xmath246 is @xmath248 , in which case the threshold becomes @xmath249 , which is the same as the original bonferroni procedure .",
    "note that there are infinitely many optimizing combinations of @xmath245 and @xmath61 as long as @xmath248 .",
    "regardless , no claim can be made to an improvement over the bonferroni procedure .",
    "( on the other hand , we could also apply the split sample method and then apply holm method in the second stage , which if compared to the usual holm method based on the full data could offer an improvement because critical values now change more rapidly at each step . )",
    "in this section , we performed two simulation studies to evaluate the performances of our suggested two - stage bonferroni method as a high - dimensional global testing method and as an fwer controlling method .",
    "we performed a simulation study to compare the performance of our suggested modified two - stage bonferroni method ( see section 5 ) with those of several existing global testing methods with respect to type 1 error rate and power .",
    "the methods we chose for comparison include the conventional bonferroni test , simes test ( simes , 1986 ) , higher criterion method ( donoho and jin , 2004 , 2015 ) , and sample - split bonferroni test ( cox , 1975 ; skol et al , 2006 ) .",
    "each simulated data set is obtained by generating @xmath250 dependent normal random samples @xmath251 , with a common correlation @xmath252 and a sample size @xmath253 . among the 1,000 mean values @xmath5 , @xmath254 or @xmath255 are drawn from @xmath256 and the remaining are equal to 0 , where @xmath257 .",
    "the common variance @xmath154 is drawn from @xmath258 . for @xmath3 , consider using one - sample @xmath23-statistic for testing individual hypothesis @xmath259 against @xmath260 .",
    "we then use the aforementioned five global testing methods for testing the global hypothesis @xmath261 against @xmath262 at level @xmath263 . for our",
    "suggested modified two - stage bonferroni method , we use the sum of squares as the selection statistic for performing selection of the individual hypotheses .",
    "the selection threshold we chose is @xmath264 in section 5 , which roughly ensures @xmath265 of hypotheses to be selected . for the sample - split bonferroni test",
    ", we use one - sample @xmath23-statistics for both selection and testing , which are respectively constructed based on the first and second half samples . the selection threshold we chose",
    "is @xmath266 in section 7 .",
    "in addition , we always set @xmath267 in the simulations .     with values from @xmath254 to @xmath268 and equal variance @xmath269 .",
    "for the left and middle panels , all @xmath5 are equal to zero and for the right panel , @xmath255",
    "@xmath5 s are drawn from @xmath256 and the rest are equal to zero .",
    "in addition , @xmath250 , @xmath253 and @xmath263 . ]    the simulation is repeated for @xmath270 times . the type 1 error rate and power",
    "are both estimated as the proportions of simulations where @xmath261 is rejected when @xmath261 is respectively true and false . in figure 8.1 we compared the estimated type 1 error rates and powers of the aforementioned five global testing methods with respect to the common correlation . as seen from figure 8.1",
    ", our suggested modified two - stage bonferroni method always controls the type 1 error rate at level @xmath38 for all values of correlation while performing best in terms of power .",
    "however , for the higher criterion test , it completely loses the control of type 1 error rate even when the correlation is weak ; and even though for its inflated type 1 error rate , it is still less powerful than our suggested method .    in figure 8.2 we compared the estimated power of the aforementioned five methods under independence in the cases of equal and unequal variances with respect to @xmath271 with values from @xmath272 to @xmath273 . as seen from figure 8.2 , our suggested modified two - stage bonferroni method performs best under equal variance in terms of power and its power improvements over the existing four methods are always pretty large for different values of @xmath271 . under unequal variance ,",
    "our suggested modified two - stage bonferroni method still performs well compared to the existing methods , although the power improvements become smaller when the variability of variances becomes larger .",
    "( left panel ) and unequal variance with @xmath274 ( middle panel ) and @xmath275 ( right panel ) . among all these three panels ,",
    "@xmath255 @xmath5 s are drawn from @xmath256 with values of @xmath271 from @xmath272 to @xmath273 and the rest are equal to zero .",
    "in addition , @xmath250 , @xmath253 and @xmath263 . ]",
    "we also performed a simulation study to compare the performance of our suggested modified two - stage bonferroni method ( section 5 ) with those of several existing multiple testing methods with respect to the fwer control and average power .",
    "the methods we chose for comparison include conventional bonferroni procedure , hochberg procedure , and sample - split bonferroni procedure ( section 7 ) .",
    "each simulated data set is obtained by generating @xmath276 dependent normal random samples @xmath251 , with a common correlation @xmath252 and a sample size @xmath253 . among the 100 @xmath5 s , @xmath277",
    "are drawn from @xmath256 and the remaining are equal to 0 , where @xmath278 is the proportion of @xmath279 .",
    "the common variance @xmath154 is drawn from @xmath258 .",
    "for all of these four procedures , we use one - sample @xmath23-test statistics for testing the hypotheses @xmath259 against @xmath260 . for our",
    "suggested modified two - stage bonferroni method , we use the sum of squares as the selection statistic for performing selection of the tested hypotheses .",
    "the selection threshold we chose is @xmath280 , which roughly ensures about @xmath281 hypotheses to be selected . here , @xmath155 is the average of the sample variances of the @xmath0 samples and @xmath282 is the @xmath272 quantile of chi - square distribution with degrees of freedom @xmath27 .",
    "for the sample - split bonferroni procedure , we use one - sample @xmath23-statistics for performing selection of all of the @xmath283 hypotheses , which are constructed based on the first half sample with sample size @xmath284 . the selection threshold we chose",
    "is @xmath285 , the @xmath286 quantile of @xmath23-distribution with degrees of freedom @xmath229 , which also roughly ensures about @xmath281 hypotheses to be selected . for testing the selected hypotheses",
    ", we also use one - sample @xmath23-statistics , which are constructed based on the second half sample with sample size @xmath287 .",
    "the aforementioned four procedures are then applied to test @xmath29 against @xmath288 simultaneously for @xmath289 at level @xmath263 .",
    "the simulation is repeated for @xmath270 times .",
    "the fwer is estimated as the proportion of simulations where at least one true null hypothesis is falsely rejected and the average power is estimated as the average proportion of rejected false null hypothesis among all false nulls across simulations . in figure 8.3 we compared the estimated fwer and average power of these four procedures with respect to the proportion of false null hypotheses @xmath278 with values from @xmath254 to @xmath272 in the cases of @xmath290 ( upper panel ) or @xmath291 ( bottom panel ) . as seen from figure 8.3 ,",
    "our suggested modified two - stage bonferroni method performs best in terms of average power while controlling the fwer at level @xmath38 , and its power improvements over the existing three methods are decreasing with the increasing proportion of false nulls .     with @xmath290 ( upper panel ) or @xmath291 ( bottom panel ) and equal variance @xmath269 .",
    "for the mean values @xmath5 , @xmath292 @xmath5 s are equal to one and the rest are equal to zero . here , the value of @xmath278 is from @xmath254 to @xmath272 , @xmath276 , @xmath253 , and @xmath263 . ]    in figure 8.4 we compared the estimated fwer and average power of these four procedures with respect to the common correlation @xmath252 with values from @xmath254 to @xmath268 .",
    "we observe from figure 8.4 that for different values of correlation @xmath252 , our suggested modified two - stage bonferroni method always performs best in terms of average power while controlling the fwer at level @xmath38 .",
    "in addition , we also observe that the average powers of these methods are not affected by the correlation and the estimated fwers are basically decreasing in terms of the correlation .     with values from @xmath254 to @xmath268 and equal variance @xmath269 . for the mean values @xmath5 ,",
    "@xmath293 @xmath5 s are equal to one and the rest are equal to zero . in addition , @xmath276 , @xmath253 and @xmath263 . ]",
    "proof of lemma [ lemma : control ] : assume @xmath29 is true .",
    "then , we claim the detection statistic @xmath22 is independent of all the selection statistics @xmath294 . for the univariate normal model with mean 0 and unknown variance , the @xmath23-statistic @xmath22 is independent of @xmath21 by basu s theorem ( because @xmath22 is ancillary and @xmath21 is a complete sufficient statistic ) .",
    "hence , @xmath22 is independent of @xmath21 , and therefore independent of @xmath295 .",
    "let @xmath296 be the indices of the true null hypotheses .",
    "thus , the fwer is given by @xmath297 this probability , conditional on the selection statistics @xmath39 is @xmath298 which by bonferroni s inequality is bounded above by @xmath299 therefore , the unconditional probability is bounded above by @xmath38 , as required .",
    "proof of theorem [ theorem : holm ] : as in the proof of lemma [ lemma : control ] , compute the probability of at least one false rejection conditional on the selection statistics .",
    "let @xmath300 be the smallest ( or first ) @xmath8 for which @xmath48 is true and @xmath301 .",
    "such an event implies that the smallest @xmath1-value among the true null hypotheses which have been selected is less than or equal to @xmath302 .",
    "indeed , the largest possible value for @xmath300 ( leading to the largest possible critical value for the first true null hypothesis tested ) is given if , out of the @xmath35 selected hypotheses , all of the @xmath303 false null hypotheses are rejected first in the stepdown procedure , where @xmath304 is the set of indices of the false null hypotheses .",
    "this occurs when @xmath305 , in which case @xmath306 by bonferroni , the conditional probability is bounded above by @xmath38 because it is the conditional probability that the minimum of @xmath307 true null @xmath1-values is bounded above by @xmath302 .",
    "thus , the unconditional probability of fwer is bounded above by @xmath38 .     before proving lemma [ lemma : select ]",
    ", we will make use of the following lemmas .",
    "[ lemma : lm ] ( laurent and massart , 2000 ) . for every @xmath308 and every @xmath309",
    ", we have @xmath310    [ lemma : i ] ( inglot , 2010 ) . for every @xmath311 and",
    "every @xmath312 $ ] , we have @xmath313    proof of lemma [ lemma : select ] : to show ( i ) , it is enough to show that @xmath96 exceeds an upper bound to @xmath314 with probability tending to one . by lemma [ lemma : lm ] and the specification @xmath315 , we have : @xmath316 @xmath317 now , if @xmath96 is normalized to form @xmath318 , then by the central limit theorem , it follows that @xmath319 thus , it suffices to show that @xmath320 with probability tending to one , where @xmath321 but , @xmath322 @xmath323 by the assumption on @xmath79 .",
    "therefore , @xmath324 and so @xmath325 with probability tending to one .    to prove ( ii )",
    ", we argue similarly . by lemma [ lemma : i ] ,",
    "when @xmath27 is sufficiently large , we have @xmath326 therefore , it suffices to show @xmath327 with probability tending to 0 . in terms of @xmath318 , it suffices to show @xmath328 with probability tending to 0 , where @xmath329 but @xmath330 hence , @xmath331 and the result follows .",
    "proof of lemma [ lemma : cheb ] : for @xmath3 , let @xmath332 , where @xmath333 denotes the indicator function . recall that the number of selected hypotheses is @xmath35 ,",
    "so @xmath334 .",
    "note that , for any true null hypothesis @xmath29 , @xmath335 , in which case @xmath336 where @xmath337 then , if @xmath29 is true , @xmath338 and @xmath339 .",
    "in fact , since the chi - squared family of distributions ( with fixed degrees of freedom and varying noncentrality parameter ) has monotone likelihood ratio , its power function is increasing in the noncentrality parameter ; thus , @xmath340 regardless of whether or not @xmath29 is true .",
    "so , @xmath341 as stated in ( [ equation : selecta ] ) of the lemma .",
    "now , @xmath342 so , @xmath343 thus , @xmath344 as long as @xmath345 . combining ( [ equation : comb1 ] ) and ( [ equation : selecta ] ) yields @xmath346    using indicators again to approximate the variance of @xmath35 yields @xmath347 \\le   e ( | \\hat s_n | ) ~.\\ ] ] therefore , making use of ( [ equation : comb3 ] ) .",
    "@xmath348 thus , by chebychev s inequality , @xmath349 , yielding ( [ equation : cheb ] ) . combining ( [ equation : cheb ] ) and ( [ equation : comb3 ] ) yields ( [ equation : selectb ] ) .       in the second stage of the two - stage method , we need to be able to approximate the very upper tail quantiles of the normal and @xmath23 distributions .",
    "the approximation @xmath350 is well - known for large @xmath0 . in our application",
    ", we will apply this with random @xmath0 , and so some care must be taken to get good lower and upper bounds to the quantile .",
    "[ lemma : normalq ] for any fixed @xmath38 and any @xmath351 , the following inequalities hold for all large enough @xmath0 : @xmath352    in fact the approximations hold uniformly for @xmath353 $ ] for any @xmath354 and for all large enough @xmath0 .",
    "proof of lemma [ lemma : normalq ] : if @xmath355 denotes the standard normal density and @xmath356 , then the following inequalities are well - known ( see feller ( 1968 ) , lemma 2 in chapter vii ) : for any @xmath357 , @xmath358 it follows from the right inequality that @xmath359 as soon as @xmath360 .",
    "therefore , the @xmath361 quantile of the standard normal distribution must be bounded above by @xmath362 as soon as @xmath360 .",
    "the first inequality is similar .",
    "let @xmath363 be the cdf of student s @xmath23 with @xmath27 degrees of freedom , and @xmath364 be the cdf of @xmath365 .",
    "consider the equation @xmath366 and let @xmath367 be the solution of the equation .",
    "let @xmath368 and @xmath369 we will make use of the following result .",
    "[ lemma : fm ] ( fujikoshi and mukaihata , 1993 ) . for all @xmath370 ,",
    "we have    * @xmath371 * @xmath372    as before , let @xmath373 and @xmath374 denote the @xmath375 quantiles of @xmath365 and @xmath376 , respectively .",
    "then @xmath377    [ lemma : tbound ] fix any @xmath378 and @xmath379 .",
    "then , for all @xmath0 large enough , @xmath380^{1/2}\\ ] ] and @xmath381^{1/2}~.\\ ] ]    proof of lemma [ lemma : tbound ] : first , we show ( [ equation : boundr ] ) . by lemma [ lemma :",
    "fm ] , we have @xmath382 but since @xmath383 is an increasing function , we can replace @xmath384 by the upper bound @xmath385 provided for in lemma [ lemma : normalq ] , at least for all large @xmath0 .",
    "this gives the bound on the right side of ( [ equation : boundr ] ) .",
    "similarly , for all large @xmath0 , we have    @xmath386 which gives the lower bound in ( [ equation : boundl ] ) .",
    "proof of lemma [ lemma : detect ] : to prove ( i ) , detection occurs when @xmath387 exceeds @xmath388 , where @xmath35 is the number of selected hypotheses from the first stage . by lemma [ lemma : cheb ] , @xmath389 , and so by lemma [ lemma : tbound ] , @xmath390^{1/2}\\ ] ] with probability tending to one .",
    "hence , @xmath391 @xmath392^{1/2 }      \\ } + o(1)~,\\ ] ] where @xmath376 denotes a generic random variable having the @xmath23-distribution with @xmath393 degrees of freedom .",
    "the quantity inside the probability to the left of @xmath394 divided by @xmath395 tends in probability to @xmath396 , i.e. , @xmath397 but , using lemma [ lemma : cheb ] and assumption a , the quantity inside the probability to the right of @xmath394 divided by @xmath395 tends in probability to @xmath398 .",
    "hence , by slutsky s theorem , the probability will tend to one if @xmath399 .",
    "similarly , to prove ( ii ) , with probability tending to one we have @xmath400^{1/2}~.\\ ] ] call the expression on the right side @xmath401 .",
    "then , the detection probability can be bounded above as @xmath402 note that the left side inside the last probability divided by @xmath395 tends in probability to to @xmath396 , while the right side , @xmath401 divided by @xmath395 tends in probability to @xmath403 -1 } $ ] . hence , if for some @xmath404 , we have @xmath405 -1 } ~,\\ ] ] then the probability of detection tends to 0 . by continuity , if @xmath406 , then we can choose @xmath407 small enough so that ( [ equation : delta ] ) holds , and the result follows .",
    "proof of lemma [ lemma : cstar ] we first argue that for any @xmath408 and @xmath27 sufficiently large , @xmath409 where @xmath128 is a given positive constant satisfying @xmath410 . along the proof of theorem 4.1 in inglot ( 2010 ) ,",
    "to prove the above inequality , it is enough to show that @xmath411 where @xmath412 and @xmath413 .",
    "then , it is in turn enough to show the following inequality when @xmath27 is sufficiently large , @xmath414 but for given @xmath415 , @xmath416 is equivalent to @xmath417 , which in turn implies the inequality ( [ equation : old21 ] ) .",
    "therefore , ( [ equation : oldlemma4 ] ) holds if @xmath418 , where @xmath419 is defined in ( [ equation : ac ] ) .    specifically , if @xmath420 , under assumption a , we have @xmath421 .",
    "thus , for given @xmath422 and sufficiently large @xmath27 , as @xmath423 , ( [ equation : oldlemma4 ] ) holds .",
    "thus , as @xmath424 , ( i ) holds .    to prove ( ii ) , the proof is similar .",
    "when @xmath27 is sufficiently large , the lower bound of @xmath425 in lemma [ lemma : i ] can be improved as @xmath426 where @xmath427 .    by using the similar arguments as in the proof of theorem 5.2 of inglot ( 2010 ) and wherein letting @xmath428 , to prove the above inequality",
    ", it is enough to show that @xmath429 where @xmath430 .",
    "when @xmath27 is sufficiently large , we only need to show that @xmath431 which is equivalent to @xmath432 .",
    "therefore , when @xmath27 is sufficiently large , we have @xmath433 if @xmath434 .    specifically , if @xmath435 , by using a similar argument as above , we have @xmath436 for @xmath437 .",
    "[ lemma : tri ] let @xmath438 have the trinomial distribution based on @xmath27 trials and corresponding success probabilities @xmath439 .",
    "then , @xmath440    proof of lemma [ lemma : tri ] : since @xmath441 , it suffices to show @xmath442 the conditional distribution of @xmath443 given @xmath444 is @xmath128 is binomial based on @xmath445 trials and success probability @xmath446 .",
    "hence , @xmath447 the last sum is bounded above by one because if the sum included the index @xmath448 the sum would be the sum of binomial probabilities based on @xmath449 trials with success parameter @xmath450 .",
    "thus , @xmath451 ~\\ ] ] and so @xmath452 @xmath453    proof of theorem [ theorem : esigma ] : without loss of generality , assume @xmath160 .",
    "also note that the fwer is maximized when all null hypotheses are true .",
    "indeed , the number of hypotheses selected is an increasing function of @xmath454 , where @xmath5 is the mean of the @xmath8th sample ( since the non - central chi - squared distribution has monotone likelihood ratio in the non - centrality parameter ) . but increasing the number of selections only makes the fwer smaller since ( stochastically ) more hypotheses are tested at the second stage than just the true nulls .",
    "hence , we now assume all hypotheses are null .    for any @xmath455 , the event @xmath456 defined by @xmath457 has probability tending to one .",
    "let @xmath458 .",
    "for any @xmath32 , let @xmath459 be the selection set when it is known @xmath160 ; in particular , we will always take @xmath460 .",
    "then , with probability tending to one , @xmath461 and correspondingly the numbers of elements in these index sets satisfy @xmath462 ) and ( [ equation : subs2 ] ) , @xmath463 @xmath464 the point is that , conditional on all the @xmath21 , the sets @xmath465 are determined , and the @xmath23-statistics then remain conditionally independent ( but not so if we condition on @xmath466 ) .",
    "hence , by the bonferroni inequality , the last probability , conditional on the @xmath21 , is bounded above by @xmath467 . hence , to complete the argument , we must show @xmath468 let @xmath444 be the number of @xmath21 in @xmath469 and @xmath443 be the number @xmath470 . then , ( [ equation : show ] ) reduces to showing @xmath471 or equivalently @xmath472 by lemma [ lemma : tri ] , this last expression is bounded above by @xmath473 , and so we must show @xmath474 , where @xmath475 but , the denominator in ( [ equation : ratio ] ) satisfies @xmath476 and so it suffices to show @xmath477 the denominator in ( [ equation : ratio ] ) is , by construction , @xmath159 .",
    "the numerator involves an integration over @xmath478 , the chi - squared density with @xmath27 degrees of freedom .",
    "the mode of @xmath478 is @xmath479 .",
    "so , the integral can crudely be bounded above by @xmath480 , the density at the mode , multiplied by the length of the interval ( @xmath481 ) .",
    "but , @xmath482 which by stirling s formula is easily checked to be of order @xmath483 .",
    "hence , the left side of ( [ equation : ratio ] ) is bounded above by @xmath484 recalling that @xmath485 and @xmath486 shows the last expression is of order @xmath487 . for @xmath164 and @xmath455 slowly enough , this last expression tends to 0 as required .    for @xmath80 , one can improve the argument as follows .",
    "note that the chi - squared density is decreasing to the right of its mode . rather than using @xmath488",
    ", one can use @xmath489 with @xmath490 corresponding to ( or approximating ) the point in the interval @xmath491 closest to @xmath492 , i.e. , @xmath493 . note that @xmath494 for some @xmath495 ; thus , @xmath496 for all large @xmath27 .",
    "thus , we can bound the numerator in ( [ equation : nratio ] ) by the length of the interval , @xmath497 multiple by the density at the value @xmath498 of the chi - squared distribution with @xmath27 degrees of freedom .",
    "but , the chi - squared density evaluated at @xmath499 is equal to @xmath500^{\\frac{n}{2}-1 } e^ { - \\frac{1}{2 } n ( 1 + \\epsilon ) } \\ ] ] which by stirlings formula is of order @xmath501    hence , the expression ( [ equation : nratio ] ) is bounded above by order @xmath502 recalling that @xmath485 and @xmath486 shows the last expression is of order @xmath503 now , even for @xmath166 , this last expression ( [ equation : tired ] ) tends to 0 for @xmath455 sufficiently slowly , since @xmath504 .",
    "note ( [ equation : tired ] ) is equal to @xmath505\\ ] ] @xmath506   + o(1 ) \\}\\ ] ] hence , this last expression will tend to 0 ( with @xmath455 sufficiently slowly ) if @xmath507 but by ( [ equation : bigeps ] ) , we can take any @xmath271 satisfying @xmath508 therefore , if we let @xmath509 be the right side of ( [ equation : pester ] ) , then the result will follow for any @xmath61 satisfying ( [ equation : abc ] ) with @xmath510 replaced by @xmath509 , as claimed",
    ".     proof of theorem [ theorem : dependence1 ] : for every @xmath511 , let @xmath512 denote the event @xmath513 . under assumption b2 , we have @xmath514 thus , the fwer is given by @xmath515 here , the second inequality follows from independence of @xmath21 and @xmath22 when @xmath29 is true , and the second last expression follows from ( [ equation : assumb1 ] ) and ( [ equation : asymp ] ) .",
    "proof of theorem [ theorem : dependence2 ] : let @xmath215 for some @xmath216 slowly such that under assumption b3 , the event @xmath512 defined by @xmath516 has probability tending to one . for any @xmath511 ,",
    "let @xmath517 denote the event @xmath518 . under assumption b4 , the event @xmath517 has also probability tending to one .",
    "thus , @xmath519 we still use @xmath34 to denote the indices of selected hypotheses , i.e. , indices @xmath8 such that @xmath520 .",
    "thus , the fwer is given by @xmath521 here , the second inequality follows from independence of @xmath21 and @xmath22 under @xmath29 and the bonferroni inequality , and the second last expression follows from ( [ equation : limit ] ) , assumption b1 , and the proof of theorem 5.1 , in which it has been shown that @xmath522 which in turn implies @xmath523    proof of theorem [ theorem : bonpower ] : the rejection probability is @xmath524 where @xmath376 denotes a generic random variable having the @xmath23-distribution with @xmath28 degrees of freedom .",
    "but , @xmath525 moreover , by lemma [ lemma : tbound ] , @xmath526^{1/2}~.\\ ] ] hence , the limit of the rejection probability in ( [ equation : limi ] ) equals one or zero according to whether or not @xmath150 exceeds @xmath527 .",
    "proof of theorem [ theorem : split ] : we first show that @xmath29 is selected with probability 1 ( or 0 ) if @xmath31 exceeds ( or is less than ) @xmath528 .",
    "this is the probability @xmath529 @xmath530 where @xmath531 denotes a random variable having the @xmath23-distribution with @xmath532 degrees of freedom , and @xmath533 is the sample standard deviation for the @xmath8th component based on the first @xmath229 observations .",
    "but , @xmath534 and , by lemma [ lemma : tbound ] , @xmath535 and the first claim follows .",
    "the detection analysis is the same as for lemma [ lemma : detect ] , except that the number of selections @xmath35 is obtained differently .",
    "all that is needed is that @xmath349 .",
    "but the identical argument used to show this in lemma [ lemma : cheb ] applies as well .",
    "thus , using the same argument in lemma [ lemma : detect ] , but with @xmath27 replaced by @xmath536 gives that @xmath29 is detected or not according to as whether @xmath31 is greater or less than @xmath537 . combining this result with",
    "the first claim completes the proof .",
    "the research of the first author was supported in part by nsf grant dms-1309162 and the research of the second author was supported in part by nsf grant dms-1307973 .",
    "this work began during the first author s sabbatical stay at stanford university , and w.g .",
    "is thankful to stanford for hosting him ."
  ],
  "abstract_text": [
    "<S> when dealing with the problem of simultaneously testing a large number of null hypotheses , a natural testing strategy is to first reduce the number of tested hypotheses by some selection ( screening or filtering ) process , and then to simultaneously test the selected hypotheses . </S>",
    "<S> the main advantage of this strategy is to greatly reduce the severe effect of high dimensions . </S>",
    "<S> however , the first screening or selection stage must be properly accounted for in order to maintain some type of error control . in this paper </S>",
    "<S> , we will introduce a selection rule based on a selection statistic that is independent of the test statistic when the tested hypothesis is true . combining this selection rule and the conventional bonferroni procedure </S>",
    "<S> , we can develop a powerful and valid two - stage procedure . </S>",
    "<S> the introduced procedure has several nice properties : ( i ) it completely removes the selection effect ; ( ii ) it reduces the multiplicity effect ; ( iii ) it does not </S>",
    "<S>  waste \" data while carrying out both selection and testing . </S>",
    "<S> asymptotic power analysis and simulation studies illustrate that this proposed method can provide higher power compared to usual multiple testing methods while controlling the type 1 error rate . </S>",
    "<S> optimal selection thresholds are also derived based on our asymptotic analysis .    </S>",
    "<S> ams 1991 subject classifications . </S>",
    "<S> primary 62j15 , secondary 62g10    key words : screening , familywise error rate , filtering , high - dimensional , multiple testing </S>"
  ]
}