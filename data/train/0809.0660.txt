{
  "article_text": [
    "compressed sensing ( cs ) is a very recent field of fast growing interest and whose impact on concrete applications in coding and image acquisition is already remarkable . up",
    "to date informations on this new topic may be obtained from the website _ http://nuit - blanche.blogspot.com/_. the foundational paper is @xcite where the main problem considered was the one of reconstructing a signal from a few frequency measurements . since then , important contributions to the field have appeared ; see @xcite for a survey and references therein .      in mathematical terms , the problem can be stated as follows .",
    "let @xmath1 be a @xmath2-sparse vector in @xmath3 , i.e. a vector with no more than @xmath2 nonzero components .",
    "the observations are simply given by    @xmath4    where @xmath5 and @xmath6 small compared to @xmath7 with @xmath8 , and the goal is to recover @xmath1 exactly from these observations .",
    "the main challenges concern the construction of observation matrices @xmath9 which allow to recover @xmath1 with @xmath2 as large as possible for given values of @xmath7 and @xmath6 .",
    "the problem of compressed sensing can be solved unambiguously if there is no sparser solution to the linear system ( [ linsys ] ) than @xmath1 .",
    "then , recovery is obtained by simply finding the sparsest solution to ( [ linsys ] ) .",
    "if for any @xmath1 in @xmath3 we denote by @xmath10 the @xmath11-norm of @xmath1 , i.e. the cardinal of the set of indices of nonzero components of @xmath1 , the compressed sensing problem is equivalent to @xmath12 we denote by @xmath13 the solution of problem ( [ l0 ] ) and @xmath13 is called a decoder .",
    "is not the unique sparsest solution of ( [ l0 ] ) using this approach for recovery is of course possibly not pertinent .",
    "moreover , in such a case , this problem has several solutions with equal @xmath11-``norm '' and one may rather define @xmath13 as an arbitrary element of the solution set . ] thus , the cs problem may be viewed as a combinatorial optimization problem .",
    "moreover , the following lemma is well known .",
    "[ algcond ] ( see for instance @xcite ) if @xmath9 is any @xmath14 matrix and @xmath15 , then the following properties are equivalent :    \\i .",
    "the decoder @xmath16 satisfies @xmath17 , for all @xmath18 ,    \\ii . for any set of indices @xmath19 with @xmath20 , the matrix @xmath21 has rank @xmath22 where @xmath21 stands for the submatrix of @xmath9 composed of the columns indexed by @xmath19 only .      the main problem in using the decoder @xmath13 for given observations",
    "@xmath23 is that the optimization problem ( [ l0 ] ) is np - hard and can not reasonably be expected to be solved in polynomial time . in order to overcome this difficulty",
    ", the original decoder @xmath13 has to be replaced by simpler ones in terms of computational complexity . assuming that @xmath9 is given , two methods have been studied for solving the compressed sensing problem .",
    "the first one is the orthognal matching pursuit ( omp ) and the second one is the @xmath0-relaxation .",
    "both methods are not comparable since omp is a greedy algorithm with sublinear complexity and the @xmath0-relaxation offers usually better performances in terms of recovery at the price of a computational complexity equivalent to the one of linear programming . more precisely , the @xmath0 relaxation is given by @xmath24 in the following , we will denote by @xmath25 the solution of the @xmath0-relaxation ( [ l1 ] ) . from the computational viewpoint , this relaxation is of great interest since it can be solved in polynomial time . indeed , ( [ l1 ] ) is equivalent to the linear program @xmath26 the main subsequent problem induced by this choice of relaxation is to obtain easy to verify sufficient conditions on @xmath9 for the relaxation to be exact , i.e. to produce the sparsest solution to the underdetermined system ( [ linsys ] ) .",
    "a nice condition was given by candes , romberg and tao @xcite and is called the restricted isometry property . up to now",
    ", this condition could only be proved to hold with great probability in the case where @xmath9 is a subgaussian random matrix .",
    "several algorithmic approaches have also been recently proposed in order to garantee the exactness of the @xmath0 relaxation such as in @xcite and @xcite .",
    "the goal of our paper is different .",
    "its aim is to present a new method for solving the cs problem generalizing the original @xmath0-relaxation of ( @xcite ) and with much better performance in pratice as measured by success rate of recovery versus original sparsity @xmath2 .",
    "recall that the problem of exact reconstruction of sparse signals can be solved using @xmath16 and lemma [ algcond ] .",
    "let us start by writing down problem ( [ l0 ] ) , to which @xmath16 is the solution map , as the following equivalent problem @xmath27 subject to @xmath28 where @xmath29 denotes the vector of all ones . here since the sum of the @xmath30 s",
    "is maximized , the variable @xmath31 plays the role of an indicator function for the event that @xmath32 .",
    "this problem is clearly nonconvex due to the quadratic equality constraints @xmath33 .",
    "a simple way to construct a sdp relaxation is to homogenize the quadratic forms in the formulation at hand using a binary variable @xmath34 . indeed , by symmetry",
    ", it will suffice to impose @xmath35 since , if the relaxation turns out to be exact and a solution @xmath36 is recovered with @xmath37 , then , as the reader will be able to check at the end of this section , @xmath38 will also solve the relaxed problem .",
    "for instance , problem ( [ quad ] ) can be expressed as @xmath39 subject to @xmath40 for @xmath41 .",
    "if we choose to keep explicit all the constraints in problem ( [ quadhom ] ) , the lagrange function can be easily be written as @xmath42 where @xmath43 is the concatenation of @xmath44 , @xmath31 , @xmath1 into one vector , @xmath45 ( resp . @xmath46 and",
    "@xmath47 ) is the vector of lagrange multipliers associated to the constraints @xmath48 , @xmath49 ( resp .",
    "@xmath50 , @xmath49 , and @xmath51 , @xmath52 ) and where all the matrices @xmath53 , @xmath54 , @xmath52 , @xmath55 , @xmath49 and @xmath56 belong to @xmath57 , the set of symmetric @xmath58 real matrices and are defined by @xmath59 a_j= \\left [ \\begin{array}{ccc } 0 & 0_{1,n } & \\frac12 a_j^t \\\\ 0_{n,1 } & 0_{n , n } & 0_{n , n } \\\\",
    "\\frac12 a_j & 0_{n , n } & 0_{n , n }   \\end{array } \\right]\\ ] ] for @xmath52 , where @xmath60 is the @xmath61 row of @xmath9 , @xmath62 , e_i= \\left [ \\begin{array}{ccc } 0 & -e_i^t & 0_{1,n } \\\\",
    "-e_i & 2d(e_i ) & 0_{n , n } \\\\ 0_{n,1 } & 0_{n , n } & 0_{n , n }   \\end{array } \\right]\\ ] ] and @xmath63\\ ] ] for @xmath49 where @xmath64 is the vector with all components equal to zero except the @xmath65 which is set to one , @xmath29 is the vector of all ones , @xmath66 is the diagonal matrix with diagonal vector @xmath64 and where @xmath67 denotes the @xmath68 matrix of all zeros .",
    "the dual function is given by @xmath69 and thus @xmath70 with @xmath71 and where @xmath72 is the lwner ordering ( @xmath73 iff @xmath74 is positive semi - definite ) .",
    "therefore , the dual problem is given by @xmath75 which is in fact equivalent to the following semi - definite program @xmath76 subject to @xmath77 we can also try and formulate the dual of this semi - definite program which is called the bidual of the initial problem . this bidual problem",
    "is easily seen after some computations to be given by @xmath78 subject to @xmath79 @xmath80 @xmath81 now , if @xmath82 is an optimal solution with @xmath83 , then @xmath84\\big ) \\big(\\pm",
    "\\left [ \\begin{array}{c } z_0^ * \\\\ z^ * \\\\ x^ * \\end{array } \\right]\\big)^t\\ ] ] and it can be easily verified that all the constraints in ( [ quadhom ] ) are satisfied .",
    "moreover , we may additionally impose that @xmath85 .",
    ", multiply by @xmath86 the whole vector @xmath87 $ ] ] however , the following proposition ruins the hopes for the occurance of such an agreable situation .",
    "[ optrank ] if non empty , the solution set of the bidual problem ( [ bidual ] ) is not a singleton and it contains matrices with rank equal to @xmath88 .",
    "* proof*. consider the subspace @xmath89 of @xmath90 as the set of vectors whose @xmath91 first coordinates are equal to zero and such that the last @xmath7 coordinates form a vector in the kernel of @xmath9 . since we assumed that @xmath8 , we have that @xmath92 .",
    "assume that there exists a solution @xmath82 to ( [ bidual ] ) with rank less than or equal to @xmath93 .",
    "then , it is possible to find a vector @xmath43 in @xmath89 with @xmath94 . on the other hand , one can easily check that @xmath95 satisfies all the bidual constraints and has the same objective value as @xmath82 .",
    "thus , @xmath96 is also a solution of the bidual problem and @xmath97 . iterating the argument up to matrices of dimension equal to @xmath98",
    ", we obtain that the solution set contains matrices with rank equal to @xmath88 . to prove non uniqueness of the solution , for any solution matrix @xmath82 , set @xmath99 for",
    "any choice of @xmath43 in @xmath89 and @xmath100 is also a solution of the bidual problem . @xmath101      despite the powerfull lagrangian methodology behind its construction , the sdp relaxation of the problem has three major drawbacks :    * as implied by proposition [ optrank ] , the standard sdp relaxation scheme leads to solutions which naturally have rank greater than one which makes it hard to try and recover a nice primal candidate . moreover , even if the rank problem could be overcome in practice in the case where @xmath1 is sparse enough , by adding more ad hoc constraints in the sdp , finding the most natural way to do this seemed quite non trivial to us .",
    "* in the case where the sdp has a duality gap , proposing a primal suboptimal solution does not seem to be an easy task . *",
    "the computational cost of solving semi - definite programs is much greater than the cost of solving our naive relaxation , a fact which may be important in real applications .      in order to overcome the drawbacks of the sdp relaxation , we investigate another scheme which may look utopic at first sight .",
    "notice that one interesting variant of formulation ( [ quad ] ) could be the following in which the nonconvex complementarity constraints are merged into the unique constraint @xmath102 @xmath103 choosing to keep the constraints @xmath104 and @xmath105 implicit in ( [ prel1 ] ) , the lagrangian function is given by @xmath106 where @xmath107 is the diagonal matrix with diagonal vector equal to @xmath31 .",
    "the dual function ( with values in @xmath108 ) is defined by @xmath109 and the dual problem is @xmath110 the main problem with the dual problem ( [ dual ] ) is that the solutions to ( [ theta ] ) are as difficult to obtain as the solution of the original problem ( [ prel1 ] ) because of the nonconvexity of the lagrangian function @xmath111 .",
    "we now present a generalization of the @xmath0 relaxation which we call the alternating @xmath0 relaxation with better experimental performances than the standard @xmath0 relaxation and the sdp relaxation .      due to the difficulty of computing the dual function @xmath112 in the relaxation [ utop ]",
    ", the interest of this scheme seems at first to be of pure theoretical nature only . in this section",
    ", we propose a suboptimal but simple alternating minimization approach .",
    "when we restrict @xmath31 to the value @xmath113 , i.e. the vector of all ones , solving the problem @xmath114 gives exactly the solution @xmath25 of the @xmath0 relaxation . from this remark , and the lagrangian duality theory above ,",
    "it may be supected that a better relaxation can be obtained by trying to optimize the lagrangian even in a suboptimal manner .",
    "@xmath115 and @xmath116 @xmath117 @xmath118 @xmath119 @xmath120 @xmath121 @xmath122    output @xmath123 and @xmath124 .    at each step , knowing the value of @xmath125 implies that optimization with respect to @xmath126 can be equivalently restricted to the set of variables @xmath127 which are indexed by the @xmath128 s associated with the values of @xmath125 which are equal to one .",
    "thus , the choice of @xmath125 corresponds to adaptive support selection for the signal to recover .",
    "the following lemma states that @xmath125 is in fact the solution of a simple thresholding procedure .    [ 01 ] for all @xmath1 in @xmath3 , any solution @xmath31 of @xmath129^n } l(x , z , u)\\ ] ] satisfies that @xmath130 if @xmath131 , 0 if @xmath132 and @xmath133 $ ] otherwise .",
    "* proof*. problem ( [ rlxstp1 ] ) is clearly separable and the solution can be easily computed coordinatewise . @xmath101",
    "in this section , using monte carlo experiments , we compare our alternating @xmath0 approach to two recent methods proposed in the litterature : the reweighted @xmath0 of cands , wakin and boyd @xcite and the iteratively reweighted least - squares as proposed in @xcite .",
    "the problem size was chosen to be the same as in chartrand and yin s paper @xcite : @xmath134 , @xmath135 . for each sparsity @xmath2 level a hundred different @xmath2-sparse vectors @xmath1 were drawn as follows : the support was chosen uniformly on all support with cardinal @xmath2 and the nonzero components were drawn from the gaussian distribution @xmath136 .",
    "n , the observation matrix was obtained in two steps : first draw a @xmath137 matrix with i.i.d .",
    "gaussian @xmath138 entries and then normalize each column to 2 as in @xcite .",
    "the parameter @xmath139 , namely the lagrange multiplier for the complementarity constraint was tuned as follows : since on the one hand the natural breakdown point for @xmath140 equivalence , i.e. equivalence of using @xmath11 vs. @xmath0 minimization , lies around @xmath141 and on the other hand , the alternating @xmath0 is nothing but a successive thresholding algorithm due to lemma [ 01 ] , we decided to chose the smallest possible @xmath139 so that the @xmath142 largest components @xmath143 the first step of the alternating @xmath0 algorithm ( which is nothing but the plain @xmath0 decoder whatever the value of @xmath139 ) be over @xmath144 .",
    "notice that this value of @xmath139 is surely not the solution of the dual problem but our choice is at least motivated by reasonable deduction based on pratical observations whereas the tuning parameter in the other two algorithms is not known to enjoy such an intuitive and meaningful selection rule .",
    "we chose @xmath145 in these experiments .",
    "the numerical results for the irls and the reweighted @xmath0 were communicated to us by rick chartrand whom we greatly thank for his collaboration .",
    "comparison between the success rates the three methods is shown in figure 1 .",
    "our alternating @xmath0 method outperformed both the iteratively reweighted least squares and the reweighted @xmath0 methods for the given data size .",
    "as noted in @xcite , the irls and the reweighted @xmath0 enjoy nearly the same exact recovery success rates .",
    "* remark*. the reweighted @xmath0 and the reweighted ls both need a value of @xmath146 ( or even a sequence of such values as in @xcite ) which is hard to optimize ahead of time , whereas the value @xmath139 in the alternating @xmath0 is a lagrange multiplier , i.e. a dual variable . in the monte carlo experiments of the previous section",
    ", we decided to base our choice of @xmath139 on a simple an intuitive criterion suggested by the well known experimental behavior of the plain @xmath0 relaxation . on the other hand , it should be interesting to explore duality a bit further and perform experiments in the case where @xmath139 is approximately optimized ( using any derivative free procedure for instance ) based on our heuristic alternating @xmath0 approximation of the dual function @xmath112 .                    hiriart urruty , j .- b . and lemarchal , c. , convex analysis and minimization algorithms ii : advanced theory and bundle methods , springer- verlag , 1993 , 306 , grundlehren der mathematischen wissenschaften ."
  ],
  "abstract_text": [
    "<S> compressed sensing is a new methodology for constructing sensors which allow sparse signals to be efficiently recovered using only a small number of observations . </S>",
    "<S> the recovery problem can often be stated as the one of finding the solution of an underdetermined system of linear equations with the smallest possible support . the most studied relaxation of this hard combinatorial problem is the @xmath0-relaxation consisting of searching for solutions with smallest @xmath0-norm . </S>",
    "<S> in this short note , based on the ideas of lagrangian duality , we introduce an alternating @xmath0 relaxation for the recovery problem enjoying higher recovery rates in practice than the plain @xmath0 relaxation and the recent reweighted @xmath0 method of cands , wakin and boyd . </S>"
  ]
}