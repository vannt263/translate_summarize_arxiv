{
  "article_text": [
    "outliers are observations that do not follow the pattern of the majority of the data @xcite .",
    "outlier identification is a major part of data analysis for at least two reasons .",
    "first , because a few outliers , if left unchecked , will exert a disproportionate pull on estimated parameters , and we generally do not want our inferences to depend on such observations . in addition , we may want to find outliers to set them aside and study them as objects of interest in their own right . in any case , detecting outliers in settings involving more than two variables is difficult because we can not inspect the data visually and have to rely on algorithms instead .",
    "formally , this paper concerns itself with the simplest , most basic variant of the multivariate outlier detection problem .",
    "the general setting is that of a sample of @xmath0 observations @xmath1 ( with @xmath2 ) , at least @xmath3 of which are drawn from a multivariate elliptical distribution . the objective is to identify reliably the index of the remaining ones .",
    "a more complete treatment of this topic can be found in textbooks ( * ? ? ? * for example ) .    in this article",
    "we introduce pcs , a new procedure for finding multivariate outliers .",
    "we also detail fastpcs , a fast algorithm for computing it .",
    "the main output of fastpcs is an outlyingness index measuring how much each observation departs from the pattern set by the majority of the data .",
    "the pcs outlyingness index is affine equivariant ( meaning that the outlyingness ranking of the observations is not affected by a linear transformation of the data ) and can be computed efficiently for moderate values of @xmath4 and large values of @xmath0 .    to derive this index , fastpcs proceeds in two steps .",
    "first , it strives to select among many possible @xmath5-subsets of observations one devoid of outliers .",
    "then , the outlyingness index is simply the distance of each observation to this subset .    for easier outlier detection problems",
    ", we find that our approach produces results similar to state - of - the - art outlier detection algorithms .",
    "when considering more difficult cases however we find that the solution we propose leads to significantly better outcomes .    in the next section we motivate and define the pcs outlyingness and fastpcs .",
    "then , in section 3 we compare fastpcs to several competitors on synthetic data . in section 4",
    "we conduct a real data comparison . in section 5",
    ", we offer closing thoughts and discuss directions for further research .",
    "throughout this note , we will denote as @xmath6 a subset of the indexes @xmath7 ( the subscript @xmath8 indexing such sets will be used later on ) . for any @xmath6",
    "we denote its ( sample ) mean and covariance matrix as @xmath9 and we will write the squared mahalanobis distance of an observation @xmath10 as @xmath11    fastpcs starts selects among many such subsets an @xmath5-subset of observations that minimizes a criterion . other affine equivariant outlier detection algorithms that proceed in this way are the fastmve @xcite , the fastmcd @xcite and the sde @xcite . for all four procedures",
    "we will denote the chosen @xmath5-subset as @xmath12 .",
    "each of these algorithms relies on a different criterion to select the optimal @xmath5-subset .",
    "in all cases , there is a straightforward relationship between @xmath12 and the resulting outlyingness index .",
    "for example , for fastmve and fastmcd the outlyingness index is the vector of distances @xmath13 . for sde",
    "the outlyingness index is the vector of values of @xmath14 @xmath15 where @xmath16 is a set of @xmath17 directions orthogonal to hyperplanes spanned by a @xmath4-subset of @xmath18 and @xmath12 contains the observations with smallest values of @xmath14 .    in all cases ,",
    "if @xmath12 itself is contaminated , then the resulting outlyingness index can no longer be depended upon to find the outliers . for fastmve and fastmcd , @xmath12 is the subset of @xmath5 observations with smallest covariance matrix determinant and enclosed by the ellipsoid with smallest volume , respectively .",
    "given enough outliers , an adversary can easily confound these two characterizations ( for example , by placing outliers on a subspace of @xmath19 ) and ensure that a contaminated @xmath5-subset will always be chosen over @xmath5-subsets formed of genuine observations .",
    "it is not always recognized that the sde outlyingness index is also sensitive to the outliers .",
    "given enough outliers , they can always be placed in such a way that the denominator in equation @xmath20 will be smaller for some direction @xmath21 along which the numerator is smaller for outliers than for genuine observations ( for example , consider a direction @xmath21 close to the principal axis of the lighter , orange ellipse in the lower - left panel in figure  [ mcs : f1a ] ) .",
    "then , for many observations the maximum outlyingness will be attained at @xmath22 . repeated over a large number of such directions , this will cause the value of the outlyingness index in equation @xmath20 to be smaller for many outliers than for genuine observations .    consider the following example .",
    "the four panels in figure  [ mcs : f1a ] all depict the same 70 draws from a standard bivariate normal distribution together with 30 draws from a second normal distribution with smaller variance and centered at @xmath23 .",
    "light , orange ellipses in the first three plots show the @xmath24 obtained using the best subset found by the fastmcd , fastmve and sde .",
    "these were computed with the r package ` rrcov ` @xcite , using default settings ; 500 starting @xmath4-subsets ( for the first two ) , and 500 directions @xmath25 ( for sde ) ; and @xmath26 ( see section  [ mcs : s5 ] ) in all cases .        in all three cases , the fitted ellipses ( drawn in solid orange lines ) ,",
    "fail to adequately fit any @xmath5-subset of the data , including the subset they enclose .",
    "in particular , their centers of symmetry ( orange stars ) are not located in areas of highly concentrated observations . in all cases ,",
    "the model fitted on @xmath12 appears visually distinct from the distribution governing the good part of the data ( drawn as a dashed , dark blue ellipse ) .",
    "this is confirmed by the biases ( a dimensionless measure of dissimilarity between two subsets , see section  [ mcs : s3 ] ) of 7 , 5 and 3.5 for the three algorithms .",
    "the algorithm we propose differs from these methods in that it uses a new measure of multivariate congruence ( which we detail in the next section ) to select the optimal @xmath5-subset .",
    "the main advantage of this new characterization lies in its insensitivity to the outliers .",
    "as we argue below , this makes the outlyingness index derived from fastpcs both quantitatively and qualitatively more reliable .",
    "pcs looks for the @xmath5-subset of multivariate observations that is most _ congruent _ along many univariate projections . in this context",
    ", we measure the congruence of a given @xmath5-subset along a given projection by the size of its overlap with a second subset that is optimal ( in a sense we make precise below ) on that projection .",
    "the pcs criterion is based on the observation that a spatially cohesive @xmath5-subset will tend to be congruent with these optimal @xmath5-subsets , and a spatially disjoint one will not .    more precisely , denoting by @xmath27 a @xmath28 matrix formed of @xmath4 observations ( we detail below how we pick these @xmath4 observations ) , @xmath29 the hyperplane @xmath30 and @xmath31 the ( squared ) orthogonal distance of @xmath10 to @xmath29 : @xmath32 the set of @xmath5 observations with smallest @xmath33 will be denoted as @xmath34 . for a given subset @xmath6 and direction @xmath29",
    "we define the _ incongruence index _ of @xmath6 along @xmath29 as @xmath35this index is always positive and will have small value if the projection of the members of @xmath6 along @xmath29 greatly overlaps with the members of @xmath34 . to remove the dependence of equation on @xmath29 we measure the incongruence of @xmath6 by considering the average over many directions : @xmath36 where @xmath37 are all directions orthogonal to a hyperplane spanned by a @xmath4-subset of @xmath6 .",
    "we call the @xmath6 with smallest @xmath38 the _ projection congruent subset_. in essence , the @xmath39 index measures the spatial cohesion of an @xmath5-subset @xmath6 in terms of how much the projections of its members overlap with those of the @xmath34 over many projections . in practice",
    ", it would be too laborious to evaluate equation over all members of @xmath37 .",
    "a practical solution is to take the average over a sample of @xmath40 random directions @xmath41 instead .",
    "the @xmath39 index of a spatially disjoint @xmath5-subset tends to be higher than that of a spatially cohesive @xmath5-subset .",
    "this is because when @xmath6 forms a spatially cohesive set of observations , @xmath42 tends to be larger .",
    "this is illustrated on an example in figure [ mcs : f1b ] .",
    "both panels depict the same set of @xmath43 points .",
    "these points form two disjoint groups of observations .",
    "the main group contains 70 points and is located on the left - hand side .",
    "each panel illustrates the behavior of the @xmath39 index for a given @xmath5-subset of observations .",
    "@xmath44 ( left ) forms a set of spatially cohesive observations , all belonging to the main group .",
    "@xmath45 , in contrast , forms a spatially disjoint set of observations and contains members drawn from both groups .    for each @xmath6-subset , @xmath46",
    ", we drew two hyperplanes @xmath47 ( dark blue , dashed ) and @xmath48 ( light orange ) .",
    "the dark blue dots show the members of @xmath49 .",
    "similarly , light orange dots show the members of @xmath50 .",
    "the diamonds ( black squares ) show the members of @xmath51 ( @xmath52 ) that do not belong to @xmath6 .",
    "after just two projections , the number of non - overlapping observations ( i.e.\\ { @xmath53 ) is 8 ( @xmath54 ) and 18 ( @xmath55 ) respectively .",
    "as we increase the number of directions @xmath29 , this pattern repeats and the difference between a spatially cohesive and a disjoint subset grows steadily .     of spatially cohesive observations ( left ) and a subset @xmath45 of spatially disjoint observation ( right),title=\"fig:\",scaledwidth=49.0% ]    of spatially cohesive observations ( left ) and a subset @xmath45 of spatially disjoint observation ( right),title=\"fig:\",scaledwidth=49.0% ]",
    "the @xmath39 index measures the size of this overlap . for a direction @xmath29",
    ", the members of @xmath51 and @xmath52 not in @xmath6 ( shown as diamonds and black squares in figure [ mcs : f1b ] ) will decrease the denominator in equation @xmath56 without affecting the numerator , increasing the overall ratio .",
    "consequently , @xmath5-subsets formed of spatially disjoint groups of observations will have larger values of the @xmath39 index .",
    "crucially , the @xmath39 index characterizes a cohesive @xmath5-subset of observations independently of the spatial configuration of the outliers .",
    "for example , the pattern shown in figure [ mcs : f1b ] would still hold if the cluster of outliers were more concentrated .",
    "this is also illustrated in the fourth quadrant of figure [ mcs : f1a ] where the optimal @xmath5-subset found by fastpcs is not unduly attracted by members of the smaller cluster of observations located on the right .    in sections",
    "[ mcs : s3 ] and [ mcs : s4 ] , we show that this new characterization allows fastpcs to reliably select uncontaminated @xmath5-subsets .",
    "this includes many situations where other algorithms fail to do so . first though , the following section details the fastpcs algorithm .      to compute the pcs outlyingness index ,",
    "we propose the fastpcs algorithm @xmath57 .",
    "it combines ideas from fastmcd ( the use of random @xmath4-subsets as starting points and an iterative concentration step ) with some new ones . throughout",
    ", @xmath17 denotes the number of starting @xmath4-subsets .",
    "an important characteristic of our algorithm is that it can detect exact - fit situations : when @xmath5 or more observations lie exactly on a subspace , fastpcs will return the indexes of an @xmath5-subset of those observations and the fit given by the observations in @xmath12 will coincide with the subspace . since fastpcs is affine equivariant , this behavior suggests that it has maximal finite sample breakdown point , as discussed in ( * ? ? ?",
    "* remark 1 , pg 123 ) and @xcite .",
    "intuitively , this is because in any affine equivariant metric the @xmath58 observations not in the subspace do in effect lie arbitrarily far away from the ones that are .",
    "step @xmath59 increases the size of @xmath6 from @xmath60 to its final size @xmath3 in @xmath61 steps , rather than in one as is done in fastmcd .",
    "this improves the robustness of the algorithm when outliers are close to the good data .",
    "we find that increasing @xmath61 does not improve performance much if @xmath61 is greater than 3 and use @xmath62 as default .",
    "0.15 cm    ' '' ''    0.1 cm @xmath63    ' '' ''    for = @xmath54 to @xmath17 do : + @xmath64 : @xmath65 + @xmath59:for = @xmath66 to @xmath61 do : + @xmath67 + set @xmath68 + set @xmath69 + end for + @xmath70 + @xmath71 : compute @xmath72 + end for + keep @xmath12 , the subset @xmath6 with lowest @xmath38 .",
    "the final outlyingness index + of each observation is given by @xmath73 .",
    "-0.1 cm    ' '' ''    0.15 cm    empirically also , we found that small values for @xmath40 are sufficient to achieve good results and that we do not gain much by increasing @xmath40 above 25 , so we set @xmath74 as the default ( this is the value we use throughout ) .",
    "that such a small number of random directions suffice to reliably identify the outliers is remarkable .",
    "this is because fastpcs uses directions generated by @xmath4-subsets of @xmath6 .",
    "compare this to sde algorithm which needs a much larger number of projections to reliably find the outliers .",
    "this is because in sde the data is projected along directions given by hyperplanes passing through @xmath4 points drawn indiscriminately from the entire set of observations .",
    "consequently , when the contamination rate is high , most of these @xmath4-subsets will be contaminated , yielding directions that can end up almost parallel to each other .",
    "in contrast , our choice always ensures a wider spread of directions when @xmath6 is uncontaminated and thus yields better results .    like fastmcd and fastmve",
    ", fastpcs uses many random @xmath4-subsets as starting points",
    ". the number of initial @xmath4-subsets , @xmath17 , must be large enough to ensure that at least one of them is uncontaminated . for fastmcd and fastmve , for each starting @xmath4-subset , the computational complexity scales as @xmath75",
    "whereas for fastpcs it is @xmath76 .",
    "computing the sde outlyingness index also costs @xmath76 for each hyperplane and here also the number of such hyperplanes must be of order @xmath17 to ensure that at least one of them does not pass through any of the outliers .",
    "the value of @xmath17 ( and therefore the computational complexity of all four algorithms ) grows exponentially with @xmath4 .",
    "the actual run times will depend on implementation choices but in our experience fastmcd is slightly faster than fastpcs and both are noticeably slower than either fastmve or sde . in practice",
    "this means all four procedures become impractical for values of @xmath4 much larger than 25 .",
    "furthermore , all four procedures belong to the class of so called ` embarrassingly parallel ' algorithms , i.e. their time complexity scales as the inverse of the number of processors meaning that they are particularly well suited to benefit from modern computing environments . to enhance user experience",
    ", we implemented fastpcs in c-0.2excode wrapped in a portable ` r ` package @xcite distributed through ` cran ` ( package ` fastpcs ` ) .",
    "in this section we evaluate fastpcs numerically and contrast its performance to that of the sde , fastmcd and fastmve algorithms . the last three were computed using the ` r ` package ` rrcov ` with default settings ( except , respectively , for the number of random directions and starting subsets which for all algorithms we set according to equation @xmath77 ) .",
    "each algorithm returns a subset @xmath12 of @xmath5 observations with the smallest outlyingness index .",
    "our evaluation criteria are the bias and the misclassification rate of these @xmath5 observations .",
    "below , we briefly describe these .",
    "given a central model @xmath78 and an arbitrary contaminating distribution @xmath79 , consider the @xmath80-contaminated model @xmath81 for a subset of observations @xmath12 , the ( asymptotic ) bias measures the deviation of @xmath24 from the true @xmath82 .",
    "formally , denoting @xmath83 and @xmath84 , we have that for an affine equivariant scatter matrix @xmath85 , all the information about the bias is contained in the matrix @xmath86 or equivalently its condition number @xcite : @xmath87 where @xmath88 ( @xmath89 ) are the largest ( smallest ) eigenvalues of @xmath86 . evaluating",
    "the bias of @xmath24 is an empirical matter . for a given sample ,",
    "the bias will depend on the dimensionality of the data , the rate of contamination and the distance separating the outliers from the good part of the data .",
    "finally , the bias also depends on the spatial configuration of the outliers ( the choice of @xmath79 ) .",
    "fortunately , for affine equivariant algorithms the outlier configurations causing the largest biases are known and so we can focus on these cases .",
    "we can also compare the outlyingness indexes in terms of rate of contamination of their final @xmath5-subsets . denoting by @xmath90 the index set of the contaminated observations and @xmath91 the indicator function ,",
    "the misclassification rate is : @xmath92 this measure is always in @xmath93 $ ] , thus yielding results that are easier to compare across dimensions and rates of contamination .",
    "a value of 1 means that the @xmath94 contains all the outliers .",
    "the main difference with the bias criterion is that the misclassification rate does not account for how disruptive the outliers are .",
    "for example , when the distance separating the outliers from the good part of the data is small , it is possible for @xmath95 to be large without a commensurate increase in @xmath96 .",
    "we generate many contaminated datasets @xmath97 of size @xmath0 with @xmath98 where @xmath99 and @xmath100 are , respectively , the genuine and outlying part of the sample .",
    "the bias depends on the distance between the outliers and the genuine observations which we will measure by @xmath101 the bias also depends on the spatial configuration of @xmath100 . for affine equivariant algorithms , the worst - case configurations ( those causing the largest bias ) are known . in increasing order of difficulty",
    "these are :    * shift configuration .",
    "if we constrain the adversary to ( a ) @xmath102 and ( b ) place @xmath100 at a distance @xmath103 of @xmath99 , then , to maximize the bias , the adversary will set @xmath104 ( theorem 1 in @xcite ) and set @xmath105 in order to satisfy ( b ) .",
    "intuitively , this makes the components of the mixture the least distinguishable from one another . * point - mass configuration .",
    "if we omit the constraint ( a ) above but keep ( b ) , the adversary will place @xmath100 in a single point so @xmath106 ( theorem 2 in @xcite ) . *",
    "if we omit both constraints ( a ) and ( b ) , the adversary may set @xmath107 and choose @xmath108 to obtain a large bias .",
    "the barrow wheel contamination @xcite does this .",
    "we also considered other configurations such as radial outliers as well as cases where @xmath103 was set to extremely large values ( i.e. @xmath109 ) , but they posed little challenge for any of the algorithms , so these results are not shown .",
    "we can generate both the uncontaminated data @xmath99 and the contaminated data @xmath100 from the standard normal distribution since all methods under consideration are affine equivariant .",
    "for the shift and point configurations , we will also generate data from the standard cauchy distribution in order to quantify the sensitivity of each method to the tail index of the data .    for the shift and point configurations we generated the outliers by setting @xmath108 as either @xmath110 or @xmath111 ( @xmath110 denotes a rank @xmath4 diagonal matrix ) and set @xmath105 so that equation @xmath112 is satisfied .",
    "we generated the barrow wheel configuration using the ` robustx ` package @xcite with default parameters .",
    "the three configuration types are depicted in figure [ mcs : f0 ] for @xmath43 , @xmath113 , @xmath114 , @xmath115 and @xmath78 is the bivariate normal .",
    "the outlying observations are the lighter orange circles .",
    "the complete list of simulation parameters follows :    * the dimension @xmath4 is one of @xmath116 , * the sample size is @xmath117 , * the contamination fraction @xmath80 is one of @xmath118 , * the configuration of the outliers is either shift , point , or barrow wheel , * for shift and point contamination , the distance @xmath103 comes from the uniform distribution on ( 0,10 ) .",
    "the barrow wheel contamination does not depend on @xmath103 .",
    "* @xmath119 is a parameter determining the size of the subset of observations assumed to follow a model ( with @xmath120 . in section  [ mcs : s6a ] ( section  [ mcs : s6b ] ) we consider the case where we set @xmath121 ( @xmath122 ) . *",
    "the number of initial @xmath123-subsets @xmath17 is given by @xcite @xmath124 with @xmath125 so that the probability of getting at least one uncontaminated subset is always at least 99 percent .    in figures [ mcs : f1 ] to [ pcs : pm25 ]",
    "we display the bias and the misclassification rate ( left and right plots , respectively ) for discrete combinations of the dimension @xmath4 , and contamination rate @xmath80 . in all cases",
    ", we expect the outlier detection problem to become monotonically harder as we increase @xmath4 and @xmath80 , so not much information will be lost by considering a discrete grid of a few values for these parameters . for the barrow wheel ,",
    "@xmath4 and @xmath80 are the only parameters we have and so we can chart the results as trellises of boxplots @xcite .    for the shift and point contamination models ,",
    "the configurations also depend on the distance separating the data from the outliers . here , the effects of @xmath103 on the bias are harder to foresee : clearly nearby outliers will be harder to detect but misclassifying distant outliers will increase the bias more .",
    "therefore , we will test the algorithms for many values ( and chart the results as a function ) of @xmath103 . for both the bias and the misclassification rates curves , for each algorithm",
    ", a solid colored line will depict the median and a dotted line ( of the same color ) the 75th percentile . here",
    ", each panel will be based on 1000 simulations .",
    "the first part of the simulation study covers the case where there is no information about the extent to which the data is contaminated .",
    "then , we have to set the size of the subset of observations having positive weight to @xmath3 , corresponding to the lower bound of slightly more than the majority of the observations .",
    "in figure [ mcs : f1 ] we display the bias ( @xmath96 ) and the misclassification rate ( @xmath95 ) curves of each algorithm as a function of @xmath103 for different values of @xmath4 and @xmath80 for the shift configuration when @xmath78 is the standard normal .",
    "all the methods perform well until @xmath126 and @xmath127 . for larger values of @xmath4 and @xmath103 , the bias curves and misclassification rate of fastmve and sde clearly stand out for values of @xmath103 between 2 and 6 .",
    "eventually , as we move the outliers further away , sde and fastmve can again find them reliably .",
    ", @xmath128 , @xmath3 and normal - distributed observations shown as a function of @xmath103 .",
    "( dotdash lines ) , ( longdash lines ) , ( twodash lines ) , ( solid lines).,title=\"fig:\",scaledwidth=49.0% ] , @xmath128 , @xmath3 and normal - distributed observations shown as a function of @xmath103 .",
    "( dotdash lines ) , ( longdash lines ) , ( twodash lines ) , ( solid lines).,title=\"fig:\",scaledwidth=49.0% ]    when @xmath78 is cauchy ( figure [ mcs : f2 ] ) , fastmve performs significantly worse than the other algorithms starting already at @xmath129 and @xmath130 .",
    "eventually , ( @xmath126 and @xmath130 ) fastmcd also breaks away from @xmath131 .",
    "sde performs better until @xmath114 , where it is also noticeably affected by the thicker tails of the cauchy distribution and fails to identify the outliers .",
    "fastpcs maintains constant and low bias ( and misclassification rates ) throughout .    , @xmath128 , @xmath3 and cauchy - distributed observations shown as a function of @xmath103 .",
    "( dotdash lines ) , ( longdash lines ) , ( twodash lines ) , ( solid lines).,title=\"fig:\",scaledwidth=49.0% ] , @xmath128 , @xmath3 and cauchy - distributed observations shown as a function of @xmath103 .",
    "( dotdash lines ) , ( longdash lines ) , ( twodash lines ) , ( solid lines).,title=\"fig:\",scaledwidth=49.0% ]    in figure [ mcs : f3 ] , we show the results for the more difficult case of point - mass contamination when @xmath78 is the standard normal . starting at @xmath129 , the bias curves and misclassification rate of fastmcd become very high ( except for @xmath132 ) .",
    "starting at ( @xmath126 and @xmath133 ) all the algorithms except fastpcs fail to reliably find the outliers : looking at the misclassification rate , the optimal @xmath5-subsets for sde , fastmve and fastmcd even contain a higher contamination rate than @xmath80 .    the cauchy case shown in figure [ mcs : f4 ]",
    "is also interesting .",
    "it suggests , again , that fastmcd and fastmve are very sensitive to fat tails in the distribution of the good part of the data .",
    "the behavior of sde is roughly in line with figure [ mcs : f3 ] .",
    "again , we see that fastpcs is the only algorithm that can reliably find the outliers .",
    "furthermore , the misclassification rates reveal that the @xmath12 found by sde , fastmcd and fastmve often contains a proportion of outliers higher than @xmath80 .    , @xmath128 , @xmath3 and normal - distributed observations shown as a function of @xmath103 .",
    "( dotdash lines ) , ( longdash lines ) , ( twodash lines ) , ( solid lines).,title=\"fig:\",scaledwidth=49.0% ] , @xmath128 , @xmath3 and normal - distributed observations shown as a function of @xmath103 .",
    "( dotdash lines ) , ( longdash lines ) , ( twodash lines ) , ( solid lines).,title=\"fig:\",scaledwidth=49.0% ]    , @xmath128 , @xmath3 and cauchy - distributed observations shown as a function of @xmath103 .",
    "( dotdash lines ) , ( longdash lines ) , ( twodash lines ) , ( solid lines).,title=\"fig:\",scaledwidth=49.0% ] , @xmath128 , @xmath3 and cauchy - distributed observations shown as a function of @xmath103 .",
    "( dotdash lines ) , ( longdash lines ) , ( twodash lines ) , ( solid lines).,title=\"fig:\",scaledwidth=49.0% ]    in figure [ mcs : f5 ] we show the bias curves for the barrow wheel as boxplots .",
    "fastmcd performs badly compared to the other methods , and fastmve deteriorates for @xmath80 larger than 30 percent .",
    "it is at @xmath114 that fastpcs demonstrably exhibits superior performance , with sde producing results in between the other three algorithms .     and @xmath128 , @xmath3 .",
    ", , , ( far right).,title=\"fig:\",scaledwidth=49.0% ]   and @xmath128 , @xmath3 . , , , ( far right).,title=\"fig:\",scaledwidth=49.0% ]    across the tests , fastpcs consistently maintains low and stable biases and misclassification rates .",
    "furthermore , the performance of fastpcs is relatively insensitive to whether @xmath103 , @xmath80 and @xmath4 are low or high .",
    "additionally , in contrast with the other algorithms , fastpcs is unaffected by the thickness of the tails of @xmath78 . the consistent ability of fastpcs to detect outliers is a highly desirable feature , which we characterize as a form of qualitative robustness .      in this section ,",
    "we consider the case , important in practice , where the user can confidently place an upper bound on the rate of contamination of the sample .",
    "to fix ideas , we will set @xmath5 , the number of observations assumed to follow a model , to @xmath134 .",
    "for fastpcs , fastmve and fastmcd we adapt the algorithms by setting their respective @xmath135 parameter to 0.75 .",
    "sde does not have a corresponding parameter , so we take as member of @xmath12 the ( approximately ) @xmath136 observations with smallest outlyingness .",
    "furthermore , we reduce the number of starting subsets ( fastpcs , fastmcd , fastmve ) and random directions ( sde ) , by using equation @xmath77 , but this time setting @xmath137 . then ,",
    "as before , we measure the effect of our various configurations of outliers on the estimators ( but this time only considering values of @xmath138 ) .    in figures",
    "[ pcs : shift25 ] and  [ pcs : pm25 ] , we show the simulation results for shift and point - mass contamination .",
    "the results for normal - distributed observations are shown in the first two rows , while the last two rows show the results for observations drawn from the multivariate cauchy distribution .",
    "the shift contamination results in figure  [ pcs : shift25 ] show that when we increase the size of the active subsets , fastpcs still maintains a low bias and misclassification rate , but at equivalent rates of contamination , the other algorithms exhibit noticeably weaker performance than in figures  [ mcs : f1 ] and [ mcs : f2 ] .",
    "results in figure  [ pcs : pm25 ] illustrate that under point - mass contamination , fastpcs again reports good results .",
    "the other algorithms continue to exhibit larger biases and misclassification rates ( at equivalent rates of contamination ) than in figures [ mcs : f3 ] and [ mcs : f4 ] .",
    "we do not show the results for the barrow wheel because all the algorithms perform equivalently .    ,",
    "@xmath139 and @xmath128 .",
    "the first ( last ) two rows are for multivariate normal ( cauchy ) distributed r.v .",
    "( dotdash lines ) , ( longdash lines ) , ( twodash lines ) , ( solid lines).,title=\"fig:\",scaledwidth=49.0% ] , @xmath139 and @xmath128 .",
    "the first ( last ) two rows are for multivariate normal ( cauchy ) distributed r.v .",
    "( dotdash lines ) , ( longdash lines ) , ( twodash lines ) , ( solid lines).,title=\"fig:\",scaledwidth=49.0% ]    , @xmath139 and @xmath128 .",
    "the first ( last ) two rows are for multivariate normal ( cauchy ) distributed r.v .",
    "( dotdash lines ) , ( longdash lines ) , ( twodash lines ) , ( solid lines ) .",
    "the online version of this figure is in color.,title=\"fig:\",scaledwidth=49.0% ] , @xmath139 and @xmath128 .",
    "the first ( last ) two rows are for multivariate normal ( cauchy ) distributed r.v .",
    "( dotdash lines ) , ( longdash lines ) , ( twodash lines ) , ( solid lines ) .",
    "the online version of this figure is in color.,title=\"fig:\",scaledwidth=49.0% ]      for affine equivariant outlier detection algorithms there is a trade - off between bias and accuracy @xcite .",
    "this is especially problematic in small @xmath0 and low @xmath80 settings because then any reduction in bias can be partially undone by losses in small sample accuracy .",
    "for fastpcs in particular , this is visible in the first rows of figures [ mcs : f1 ] and [ mcs : f5 ] : in the misclassification plots , we see that for all algorithms the optimal subsets @xmath12 are uncontaminated by outliers . yet",
    ", the corresponding bias curves for fastpcs tend to be somewhat higher than for the other algorithms .    to improve accuracy in small samples without conceding too much in bias ,",
    "a solution is to add a so - called re - weighting step to the outlier detection algorithm .",
    "in essence , we replace @xmath12 by a larger subset @xmath140 , itself derived from @xmath12 .",
    "the motivation is that , typically , @xmath140 will include a greater share of the uncontaminated observations . over the years ,",
    "many such re - weighting procedures have been proposed @xcite .",
    "perhaps the simplest is the so called hard - thresholding re - weighting . given an optimal subset @xmath12 , the members of @xmath140 are : @xmath141 where the ratio on the rhs of the inequality is a scale factor we use to make @xmath142 consistent at the normal distribution .",
    "contrary to @xmath12 , the size of @xmath140 is not fixed in advance .",
    "this makes it difficult to compare algorithms on the basis of @xmath143 or @xmath144 evaluated at contaminated sub - samples @xmath145 .",
    "nonetheless , we can still compare them in terms of the bias of @xmath146 computed at uncontaminated data sets @xmath99 and for various sample sizes @xmath0 .",
    "in essence , we measure the accuracy of the various algorithms by considering their biases at uncontaminated datasets as a function of @xmath0 . in figure [ mcs : f6 ] , we show the results of doing this for @xmath147 ( shown as the abscissa ) and @xmath148 .",
    "we set @xmath149 observations as an upper limit because when @xmath150 fastmcd uses a nested sub - sampling scheme whereby larger datasets are divided unto non - overlapping sub - samples of at most 600 observations .     as a function of sample size .",
    "( dotdash lines ) , ( longdash lines ) , ( twodash lines ) , ( solid lines ) .",
    "the online version of this figure is in color.,scaledwidth=98.0% ]    figure [ mcs : f6 ] depicts the median ( solid ) and 75th percentile ( dotted ) accuracy curves of the algorithms for increasing @xmath0 when @xmath78 is normally distributed ( left ) and cauchy distributed ( right ) .",
    "each panel is based on 1,000 experiments . as expected , fastpcs is noticeably less accurate than the other algorithms in the normal case . nonetheless , we maintain that this difference is small compared to the reduction in bias achieved by fastpcs on contaminated samples .",
    "furthermore , this gap in performance depends on the distribution of the good data : when @xmath78 is cauchy the accuracy of fastpcs is similar to that of the other algorithms .",
    "in this section , we illustrate the behavior of fastpcs on a real data problem from the field of engineering : the concrete slump test data set @xcite .",
    "this dataset includes 103 data - points , each corresponding to a different type of concrete . for each observation",
    "we have 7 so - called input variables measuring , respectively , the quantity of cement ( kg / m3 ) , fly ash ( kg / m3 ) , blast furnace slag ( kg / m3 ) , water ( kg / m3 ) , super - plasticizer ( kg / m3 ) , coarse aggregate ( kg / m3 ) , and fine aggregate ( kg / m3 ) used to make the corresponding variety of concrete .",
    "additionally , for each observation , we have 3 so - called output variables measuring attributes of the resulting concrete . these are slump ( cm ) , flow ( cm ) and 28-day compressive strength ( mpa ) .",
    "this dataset actually contains two groups of observations collected over separate periods .",
    "the first 78 measurements pre - date the last 25 by several years .",
    "an interesting aspect of this dataset is that these two groups largely overlap on bivariate scatter - plots of the data , forming a seemingly homogeneous group of observations .",
    "appearances can be deceptive however : when considered along all variables jointly , the two groups are clearly distinct .",
    "for example , denoting @xmath151 ( @xmath152 ) the subset of 78 ( 25 ) members of the early ( latter ) batch of measurements , we find that the closest member of @xmath152 lies at a squared mahalanobis distance of over 760 with respect to @xmath153 . as a point of comparison , this is nearly 32 times larger than @xmath154 .",
    "we first ran the sde , fastmcd , fastmve and fastpcs algorithms on the raw dataset ( for the first three , we used the ` rrcov ` implementations with default parameters ) .",
    "we set the number of random directions in sde and the number of random @xmath4-subsets in fastmcd , fastmve and fastpcs to @xmath155 , as given in equation @xmath77 .",
    "to have comparable results , we will use for each estimator @xmath73 , the vector of statistical distances wrt @xmath12 .",
    "for the four estimators and the four variants of the concrete dataset . in each panel , the dark blue ( light orange )",
    "boxplot depicts @xmath73 for the members of @xmath151 ( @xmath152 ) .",
    "the online version of this figure is in color.,scaledwidth=100.0% ]    the mahalanobis outlyingness indexes are displayed in the first column of figure [ mcs : f7 ] , concrete ( i ) .",
    "each row corresponds to an estimator . for each estimator we drew two boxplots .",
    "the first one ( dark blue ) depicts the outlyingness index for the 78 members of @xmath151 and the second ( light orange ) boxplot for those 25 members of @xmath152 .",
    "all the algorithms are able to unambiguously separate the two subgroups of observations : the outlyingness values assigned to the members of the more recent batch of observations are notably larger than the outlyingness values assigned to any members of @xmath151 .    in a second experiment , concrete(ii )",
    ", we made the original outlier detection problem harder by narrowing the distance separating the outliers from the genuine observations .",
    "to do so , we pulled the 25 outliers towards the mean of the good part of the data by replacing the original outliers with 25 observations of the form @xmath156 for @xmath157 , @xmath158 .",
    "we denote @xmath159 to be these 25 members of @xmath160 for @xmath157 .",
    "the second column of figure [ mcs : f7 ] depicts the outlyingness indexes for this second experiment .",
    "again , for each algorithm , the first ( dark blue ) boxplots depict the values of the outlyingness indexes for the 78 members of @xmath151 and the second ( lighter , orange ) boxplot for the 25 members of @xmath159 .",
    "note that the members of @xmath159 still form a cluster that is genuinely separate from the main group of observations .",
    "for example , the closest member of @xmath159 lies at a squared mahalanobis distance of over 190 with respect to @xmath153 . for comparison",
    ", this is over 8 times larger than @xmath154 . here , the fastpcs and sde outlyingness indexes continue to clearly distinguish between the two groups .",
    "the outlyingness indexes derived from fastmve and fastmcd however fail to adequately flag the outliers .    in a third experiment , concrete(iii )",
    ", we made the original outlier detection problem harder still by increasing the contamination rate of the sample .",
    "this was done by adding an additional 25 outliers of the form @xmath161 for @xmath162 , @xmath158 .",
    "we denote @xmath163 the members of this third set of 50 outliers .",
    "the third column of figure [ mcs : f7 ] depicts the outlyingness indexes for this experiment . again for each algorithm , a ( dark ) blue boxplot pertains to the 78 members of @xmath151 and an lighter orange one to the 50 members of @xmath163 . for sde , fastmve and fastmcd",
    "we can see that increasing the contamination rate of the sample causes the outlyingness indexes of the two groups to overlap while the outlyingness index produced by fastpcs continues to make a clear distinction .",
    "remarkably , the subsets of @xmath5 observations with the smallest outlyingness chosen by the first three algorithms are primarily composed of outliers .    in a final experiment , concrete(iv )",
    ", we narrowed the distance between the good data and the outliers ( as in concrete(ii ) ) , and increased the contamination rate ( as in concrete(iii ) ) .",
    "the fourth column of figure [ mcs : f7 ] depicts the outlyingness indexes for this experiment , again with for each algorithm a ( darker ) blue boxplot for the 78 members of @xmath151 and an lighter , orange one for the 50 members of @xmath164 . here",
    "again , the outlyingness index of sde , fastmcd and fastmve fails to adequately flag the outliers .",
    "however , as with each of the previous experiments , fastpcs still assigns a larger index of outlyingness to the members of the outlying group .",
    "overall , we see that the results of the real data experiment confirm those of the simulations , at least qualitatively . when the contamination rates are small and the outliers well separated from the good part of the data , all outlier detection methods seem to perform equally well .",
    "as we consider more difficult outlier settings however , we see that the fastpcs outlyingness index is the only one that identifies the outliers reliably .",
    "in this article we introduced pcs , a new outlyingness index and fastpcs , a fast and affine equivariant algorithm for computing it . like many other outlier detection algorithms ,",
    "the performance of fastpcs hinges crucially on correctly identifying an @xmath5-subset of uncontaminated observations .",
    "our main contribution is to characterize this @xmath5-subset using a new measure of the congruence of a multivariate cloud of points .",
    "the main feature of this new characterization is that is was designed to be insensitive to the configuration of the outliers .    in our simulations , we have focused on configurations of outliers that are worst - case for affine - equivariant algorithms , and found that fastpcs behaves notably better than the other procedures we considered , often revealing outliers that would not have been identified by the other approaches . in practice , admittedly , contamination patterns will not always be as difficult as those we considered above and in many cases the different methods will , hopefully , concur . nevertheless , given that in practice we do not know the configuration of the outliers , as data analysts , we prefer to carry our inferences while planing for the worst contingencies .",
    "also through simulations , we found that the performance of fastpcs is not affected much by the tail index of the majority of the data .",
    "for example , fastpcs is capable of finding the outliers both when the majority of the data is distributed multivariate normal , or is drawn from a heavy - tailed distribution , such as the multivariate cauchy .    in this article",
    "we emphasized the practical aspects of pcs .",
    "a number of theoretical properties deserve further investigation . in particular",
    ", we suspect that pcs has maximum breakdown point .",
    "arguments supporting this conjecture are that the maximum biases are low and that the procedure has the exact fit property .",
    "00 deepayan , s. ( 2008 ) .",
    "lattice : multivariate data visualization with r. springer , new york .",
    "maronna r. a. , martin r. d. and yohai v. j. ( 2006 ) .",
    "robust statistics : theory and methods .",
    "wiley , new york .",
    "r core team ( 2012 ) .",
    "r : a language and environment for statistical computing .",
    "r foundation for statistical computing .",
    "vienna , austria .",
    "rocke d. m. and woodruff d. l. ( 1996 ) .",
    "identification of outliers in multivariate data .",
    "journal of the american statistical association , 91 , 10471061 .",
    "rousseeuw , p.j . and",
    "leroy , a.m. ( 1987 ) .",
    "robust regression and outlier detection .",
    "wiley , new york .",
    "rousseeuw , p. j. and van zomeren b.c .",
    "unmasking multivariate outliers and leverage points .",
    "journal of the american statistical association , vol .",
    "633 - 639 .",
    "rousseeuw , p. j. ( 1994 ) .",
    "unconventional features of positive - breakdown estimators .",
    "statistics and probability letters , 19 , 417431 .",
    "rousseeuw p. j. and van driessen k. ( 1999 ) . a fast algorithm for the minimum covariance determinant estimator .",
    "technometrics , 41 , 212223 .",
    "stahel w. ( 1981 ) .",
    "breakdown of covariance estimators .",
    "research report 31 , fachgrupp fr statistik , e.t.h .",
    "zrich . stahel w. and maechler m. ( 2009 ) .",
    "robustx : experimental extraneous extraordinary ... functionality for robust statistics .",
    "r package version 1.1 - 2 .",
    "todorov v. and filzmoser p. ( 2009 ) .",
    "an object - oriented framework for robust multivariate analysis .",
    "journal of statistical software , 32 , 147 .",
    "yeh , i. ( 2007 ) .",
    "modeling slump flow of concrete using second - order regressions and artificial neural networks .",
    "cement and concrete composites , vol.29 , no .",
    "6 , 474480 .",
    "yohai , v.j . and maronna , r.a .",
    "the maximum bias of robust covariances .",
    "communications in statistics  theory and methods , 19 , 39252933 ."
  ],
  "abstract_text": [
    "<S> the projection congruent subset ( pcs ) is a new method for finding multivariate outliers . like many other outlier detection procedures , pcs searches for a subset which minimizes a criterion . </S>",
    "<S> the difference is that the new criterion was designed to be insensitive to the outliers . </S>",
    "<S> pcs is supported by fastpcs , a fast and affine equivariant algorithm which is also detailed . </S>",
    "<S> both an extensive simulation study and a real data application from the field of engineering show that fastpcs performs better than its competitors . </S>"
  ]
}