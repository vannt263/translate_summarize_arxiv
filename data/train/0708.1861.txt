{
  "article_text": [
    "fifty years ago , jaynes @xcite gave the maximum entropy method ( maxent ) , based on the shannon entropy @xcite : @xmath13 where @xmath14 is the ( posterior ) probability of occurrence of the @xmath15th distinguishable state within a system , from @xmath16 such states.in the maxent method , one maximizes the shannon entropy of a system , subject to its constraints , to determine the `` least informative '' or `` maximally noncommittal '' probability distribution representing the system . from its inception ,",
    "maxent was advanced as a generic method of inference for the solution of indeterminate problems of all kinds , underpinned by information theory , not merely as an extension of mechanics @xcite .",
    "maxent was later extended into the maximum relative entropy , minimum divergence or minimum cross - entropy method ( minxent ) , involving extremization of the kullback - leibler measure @xcite : @xmath17 which allows for unequal prior probabilities @xmath18 . since that time , minxent and its subsidiary maxent",
    "have been successfully applied to the analysis of a vast number of phenomena , throughout most fields of human study ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , and can rightly be regarded as one of the most important of all human discoveries .",
    "it must be emphasised , however , that the cross - entropy and entropy concepts which underpin minxent and maxent are themselves subject to many different philosophical interpretations .",
    "dominant explanations include the axiomatic basis outlined by shannon @xcite , and the information - theoretic ( `` bits '' of information ) and coding basis , recognized by szilard @xcite and shannon @xcite ( c.f .",
    "these bases led jaynes , in particular , to consider the shannon and kullback - leibler functions to be the only logically consistent measures of uncertainty , and thus the only ones suitable for analysis .",
    "this view has been challenged by many researchers , on the grounds that the above two measures are too narrowly defined and/or inapplicable to many situations .",
    "for example , over the past 85 years , many alternative entropy and divergence functions have been introduced ( e.g. * ? ? ? * ; * ? ? ? * ; * ? ? ? * ; * ? ? ? * ; * ? ? ? * ; * ? ? ? * ; * ? ? ? * ; * ? ? ? * ; * ? ? ? * ; * ? ? ? * ; * ? ? ? * ; * ? ? ? * ; * ? ? ? * ; * ? ? ? * ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) ; in most cases , these are incompatible with the shannon and kullback - leibler functions , but have proved _ useful _ for the analysis of specific classes of systems .",
    "can such measures be explained by some broader philosophical framework ?",
    "how should we choose the `` correct '' cross - entropy or entropy function for a given problem ?",
    "the fact that such questions remain unanswered indicates the need for a unifying philosophical framework , which encompasses ( and _ explains _ ) such alternative entropy measures and their connections to information theory .",
    "this study examines one such framework :  the combinatorial ( or probabilistic ) basis of entropy , first given 130 years ago by boltzmann @xcite and subsequently promoted by planck @xcite .",
    "this involves the maximization of a governing probability distribution @xmath6 or weight @xmath3 of a system ; this can be viewed as a generalized principle of probabilistic inference , aptly described by vincze and grendar & grendar as the maximum probability ( `` maxprob '' ) principle @xcite .",
    "it also leads to generalized definitions of cross - entropy and entropy , based purely on probability theory @xcite . in this study , specific attention",
    "is paid to the origins of the governing distribution @xmath6 , including ( a ) frequentist - like models ( e.g.  ball - in - box or urn models ) ; ( b ) symmetry models ; ( c ) prior minxent models ; ( d ) kapur - kesavan inverse models ; and ( e ) game theoretic models .",
    "it is shown that the combinatorial basis is consistent with these different approaches , but is more soundly based and offers greater utility than traditional maxent / minxent based on the shannon and kullback - leibler functions .",
    "( 450,240 ) ( 0,0 )    owing to a tremendous confusion in terminology - especially amongst physicists - it is first necessary to rigorously define several important terms ( c.f .",
    "an _ entity _ is here taken to be a discrete particle , object or agent within a system , which acts separately but not necessarily independently of the other entities present .",
    "a _ system _ is a collection of entities with a defined boundary , subject to various constraints , which may or may not be open to the exchange of specified entities or substances with an external environment .",
    "the entity therefore constitutes the unit of analysis of a system .",
    "now consider a simple `` ball - in - box '' model of a system , shown in figure [ fig : defs ] , in which @xmath2 distinguishable entities ( balls ) are allocated to @xmath16 distinguishable non - degenerate states ( boxes ) .",
    "as shown :    @xmath19    a _ state _ refers to each different category or element of system ( e.g.  energy levels , sides of a die or alphabetic symbols ) .",
    "the states are therefore properties of , or associated with , each individual entity in the system .",
    "a _ configuration _ is a distinguishable permutation or pattern of entities amongst the states of a system ( a _ complexion _ ,",
    "_ microstate _ or _ sequence _ ) .",
    "a configuration is therefore a property of the system as a whole .",
    "a _ realization _ is each aggregated arrangement of entities amongst the states of a system , as specified by some rule , for example by the number of entities in each state ( a _ macrostate _ , _ outcome _ or _ type _ ) . in general",
    ", a realization will constitute a set of configurations , since several configurations could give the same realization ( see figure [ fig : defs ] ) .",
    "there is such confusion in and sloppy usage of the terms _ state _ , _ microstate _ and _ macrostate _ - severely impairing understanding - that the last two terms should be avoided . in the following , the states are indexed @xmath20 ( which may be multivariate ) ; @xmath21 denotes the number of entities in the @xmath15th state ; @xmath18 and @xmath22 respectively denote the prior and posterior probabilities of a entity being in the @xmath15th state ; and each realization in the asymptotic limits @xmath23 and @xmath24 , since @xmath25 discards information about the value of @xmath2 . ] is denoted @xmath26 .",
    "notwithstanding other philosophical differences with jaynes , the `` subjective bayesian '' definition of probabilities , as assignments based on what we know , is adopted here @xcite .",
    "for the analysis of probabilistic systems , it is possible to delineate a principle which stands out from all others : the _ maximum probability _ ( `` maxprob '' ) principle @xcite .",
    "this can be stated as :       _ `` a system can be represented by its realization of highest probability . '' _",
    "this seemingly trivial statement provides a powerful principle for _ probabilistic inference _ , which is independent of any information - theoretic considerations .",
    "this is critical , since in any contradiction between information theory and probability theory - for example , between the distributions inferred by each approach - probability theory must triumph .",
    "like minxent or maxent based on the kullback - leibler or shannon measures , maxprob is a method of inference ( inductive reasoning ) , which does not give certainty in its predictions . unlike them ,",
    "however , maxprob is founded solely on probability theory .",
    "indeed , maxprob does not depend upon any asymptotic limits ( a feature of the `` frequentist '' definition of probability , in which probabilities must correspond to measurable frequencies @xcite ) ; it can therefore be applied to systems containing finite numbers of entities @xcite .",
    "allied to maxprob is a generalized form of the second law of thermodynamics :       _ `` a system will tend towards its most probable realization . '' _",
    "this provides a purely probabilistic rationale for use of the maxprob principle , independent of thermodynamics . in effect , if we adopt maxprob as a principle for probabilistic inference , the above statement is its corresponding ergodic principle , which ( on average ) explains its success . of course - as expressed by jaynes @xcite - the concept of ergodicity is not needed for the purpose of inference , since in the absence of other information , we are fully justified in conducting inference without it .",
    "the maxprob principle also leads to the _ combinatorial definition _ of entropy , first given by boltzmann @xcite and planck @xcite .",
    "this can be written as : @xmath27 where @xmath3 is the number of ways in which a given realization can occur , referred to as its statistical weight .",
    "maximization of the entropy @xmath1 of a system , subject to its constraints , therefore selects the realization of highest weight @xmath3 ( the logarithmic function being a monotonic transformation , which does not alter the position of the extremum ) .",
    "eq .   can be extended to give generalized combinatorial ( or probabilistic ) definitions of cross - entropy and entropy @xcite : @xmath28 where @xmath29 is the probability of a given realization , subject to the prior probabilities @xmath30 , number of entities @xmath2 , number of states @xmath16 and background information @xmath31 ; @xmath7 is a convenient monotonic transformation function ; @xmath8 is a scaling parameter ; and @xmath9 is an arbitrary constant .",
    "this perspective is summarised in figure [ fig : flowchart ] . if @xmath6 or @xmath3 satisfy the multinomial distribution or weight",
    ": @xmath32 then by taking @xmath10 , @xmath11 and the asymptotic limits @xmath23 and @xmath24 ( the `` stirling approximation '' ) , @xmath12 and @xmath1 converge respectively to the kullback - leibler and shannon functions - @xcite .",
    "this provides a ( well - known ) justification for these functions , and their corresponding minxent and maxent principles , as a special case , independently of the arguments used in information theory .",
    "( 350,330 ) ( 0,0 )    in general , however , @xmath6 or @xmath3 need not be multinomial , nor may they approach an asymptotic limiting form . in such cases ,",
    "extremization of the cross - entropy or entropy defined by , subject to the constraints , gives the most probable ( maxprob ) realization of the system ( in the non - asymptotic case , due to the effect of quantization , extremization gives an `` attractor '' distribution which lies close to but not necessarily equal to the maxprob realization @xcite ) . in consequence ,",
    "the combinatorial definitions remain consistent with the rules of probability theory , whilst inference using the kullback - leibler or shannon measures may lead to inconsistencies .",
    "the combinatorial ( or probabilistic ) definitions are therefore more broadly applicable than those derived from information - theoretic considerations .    in the foregoing discussion , the astute reader will notice that there may be many different ways to classify the entities and states of a system , and hence to identify its configurations ; and many different ways to group the configurations into realizations .",
    "we are therefore led to the `` subjective '' ( or `` observer - dependent '' ) view of the entropy and cross - entropy concepts , a sentiment vocally defended by jaynes @xcite .",
    "this was succinctly expressed by tseng and caticha @xcite :       _ `` entropy is not a property of a system @xmath33 [ it ] is a property of our description of a system . ''",
    "the fact that the thermodynamic entropy @xmath34 is always defined in the same manner , allowing thermodynamicists to make consistent calculations , should not fool the reader into believing that the entropy concept is `` objective '' be devoted exclusively to the thermodynamic entropy , since it is a special case of - but is distinct from - the dimensionless shannon entropy . ] .",
    "we now consider the origins of the governing distribution @xmath6 or weight @xmath3 used in the combinatorial formulation . the problem of justifying a cross - entropy or entropy function is now replaced by a deeper problem , of how to justify its governing distribution .",
    "the ubiquity of the kullback - leibler and shannon measures , in many circumstances , therefore leads to the question : why the multinomial distribution ?",
    "this question , and the choice of @xmath6 , is examined from five different perspectives .    * ( a ) frequentist - like models *    in this approach , one simply asserts a governing distribution @xmath6 or weight @xmath3 as a probabilistic model of the system under consideration .",
    "one may have strong grounds , based on prior knowledge of a problem , for such an assertion ; in any case , we should have no `` fear of failure '' of this method ( in jaynes words ) , since if the model gives unsuccessful predictions , we have learnt that it is incorrect .",
    "many such models are available from classical probability theory , for example the _ ball - in - box _ models of the type represented in figure [ fig : defs ] .",
    "since these arose from frequentist studies , they can be termed `` frequentist - like '' models , although used here for the purpose of inference .    in the case",
    "discussed previously , in which distinguishable balls are allocated to distinguishable boxes in accordance with a set of constant prior probabilities ( see figure [ fig : defs ] ) , one obtains the multinomial distribution , and hence the kullback - leibler cross - entropy and shannon entropy functions in the stirling limits . however , different assumptions lead to different model distributions . if the asymptotic limits are not applied , then from , one obtains a non - asymptotic cross - entropy function ( c.f . * ? ? ?",
    "* ) : @xmath35 } \\bigr\\ } }   \\\\    \\end{split}\\end{aligned}\\ ] ] this is applicable to systems with finite ( small ) @xmath2 .",
    "minimisation of @xmath36 , subject to the usual constraints @xmath37 and @xmath38 , for @xmath39 , where @xmath40 is the @xmath41th function of each state @xmath15 and @xmath42 is its mathematical expectation , gives the `` most probable '' distribution ( c.f .",
    "* ) : @xmath43 \\label{eq : exact}\\ ] ] where @xmath44 is the inverse digamma function .",
    "eq .   can be viewed as the `` attractor '' for systems with finite @xmath2 , which differs from the attractor given by traditional minxent .",
    "if the states are considered to contain @xmath45 distinguishable , degenerate sub - states within each distinguishable state @xmath15 , then three cases have been examined historically : ( i ) distinguishable entities ; ( ii ) indistinguishable entities ; and ( iii ) indistinguishable entities , with a maximum of one entity in each state .",
    "the resulting distributions were given by brillouin @xcite as , respectively : @xmath46 where @xmath47 is the total degeneracy .",
    "the truncated weights and entropy functions corresponding to these distributions , referred to respectively as the maxwell - boltzmann , bose - einstein and fermi - dirac distributions respectively ( e.g * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , played an important role in the development of quantum theory . in the non - asymptotic case ,",
    "the resulting entropy functions appear to have profound information - theoretic consequences @xcite .",
    "recently , a quite different ball - in - box model was considered , in which distinguishable entities are allocated to indistinguishable , equally degenerate states . the statistical weight of each realization",
    "@xmath26 can be expressed as @xcite : @xmath48 where there are @xmath49 non - empty states amongst the @xmath16 states ; @xmath50 is the degeneracy of each state ; @xmath51 is a stirling number of the second kind ; and @xmath52 is the number of occurrences of integer @xmath53 in the set @xmath26 . the combinatorial entropy corresponding to , @xmath54 , does not appear to have a straightforward asymptotic form , except in the non - degenerate case @xmath55 with @xmath56 , when it reduces to the shannon entropy .    closely related to but distinct from ball - in - box models are _ urn models _ , in which a container ( urn ) is set up with a total of @xmath57 balls , made up of @xmath58 balls of each color @xmath15 .",
    "balls are then drawn from the urn in accordance with some sampling scheme , recorded and returned to the urn ( or the urn modified in some way ) , and the sampling repeated ( c.f . * ? ? ?",
    "* ; * ? ? ?",
    "the asymptotic limits of an infinitely large urn ( @xmath59 and @xmath60 ) , and an infinitely large ( smaller ) sample ( @xmath23 and @xmath24 ) , are usually applied .",
    "although quite different to the ball - in - box model of figure [ fig : defs ] , an urn model with simple replacement also yields the multinomial distribution @xcite .",
    "urn models involving the drawing of balls without replacement , or double replacement , lead respectively to the fermi - dirac and bose - einstein distributions @xcite .",
    "urn models also readily permit the construction of systems in which the prior probabilities are not independently and identical distributed ( non-_iid _ sampling ) : e.g.  the plya distribution , in which after every draw , the ball is returned , and @xmath61 balls of the same color are also added @xcite : @xmath62 substituting the _ initial _ prior probabilities @xmath63 and parameter @xmath64 , this gives analytic cross - entropy measures in the non - asymptotic and asymptotic cases @xcite .",
    "the resulting `` most probable '' distribution is intermediate between the bose - einstein and fermi - dirac distributions , with physical applications .    *",
    "( b ) symmetry - based arguments *    one may also choose a governing distribution on the basis of symmetry arguments ( related to the `` principle of insufficient reason '' ) .",
    "for a system made up of tosses of a coin , it is rational to consider the sampling to follow the binomial distribution , with equal prior probabilities of @xmath65 for each face , due to the symmetry of the states ( there being no information to suggest that one state should be preferred ) . alternatively ,",
    "as suggested by david blower at maxent07 , one can obtain a binomial distribution by the symmetry of all possible models in the model space ( assigning a uniform prior to the models , over the entire spectrum from an all - head to an all - tail model , there being no reason to prefer any model ) . applied to systems with more than two states , either argument leads to the multinomial distribution . in this respect ,",
    "the multinomial distribution plays a role somewhat analogous to a central limit theorem ( a `` central model theorem '' ) , a point which deserves greater mathematical attention ; this may be the reason for the ubiquity of the kullback - leibler and shannon measures . without symmetry , however , the argument breaks down , and one must adopt some other method to identify the governing distribution .    * ( c ) prior minxent models *    a third origin of the governing distribution @xmath6 is as a result of the application of minxent at a higher level , for example to the set of systems within which the actual system resides .",
    "for example , the multinomial distribution can be obtained by minxent based on the kullback - leibler cross - entropy , subject to a multinomial prior and mean constraints on each variate @xcite .",
    "this might then be imposed as a lower - level governing distribution .",
    "one can in fact envisage a hierarchy of governing and `` most probable '' distributions , at different levels of description . in a complex system ,",
    "in which there is bidirectional feedback , the result will be a mosaic of interconnected probabilistic models ( with thanks to the discussion by tony bell at maxent07 ) .    *",
    "( d ) kapur - kesavan inverse models *    the governing distribution @xmath6 can also be obtained by extension of the arguments of kapur and kesavan @xcite , in which one works backwards from an observed probability distribution @xmath66 , prior distribution @xmath30 ( if available ) and any constraints , to derive the measure of cross - entropy or entropy applicable to a system . by unravelling of the asymptotic limits , this",
    "could ( at least in principle ) be extended to determine the governing distribution of the system .",
    "this avenue of research has not been examined in detail , and deserves greater attention .    *",
    "( e ) game theoretic models *    the final method considered here is to derive the governing distribution of a system by analysis of a code - length game between the system ( `` nature '' ) and the observer @xcite . for a multivariate system of _ iid _ random variables , which take discrete values , this yields the multinomial distribution at game - theoretic equilibrium @xcite . as in case ( c ) , this could then be imposed as the governing distribution at a lower level of description .",
    "this study examines the maxprob principle , in which a system is represented by its distribution of highest probability .",
    "this can be interpreted as a generalized method of probabilistic inference , which does not provide certainty in its predictions , yet is always consistent with the rules of probability theory .",
    "in contrast , inference using the kullback - leibler cross - entropy or shannon entropy functions , in cases in which the governing distribution is not multinomial and/or does not satisfy the asymptotic limits , can lead to inconsistencies .",
    "the maxprob principle also gives rise to generalized combinatorial definitions of cross - entropy and entropy , an extension of the idea given by boltzmann 130 years ago .",
    "the cross - entropy or entropy can therefore be _ defined _ so that its extremization , subject to the constraints , gives the `` most probable '' ( `` maxprob '' ) realization of the system .",
    "this provides a purely probabilistic basis for maxent and minxent , which is independent of any information - theoretic justification .",
    "this work examines the origins of the governing distribution @xmath6 , including by ( a ) frequentist - like models ( e.g.  ball - in - box or urn models ) ; ( b ) symmetry models ; ( c ) prior minxent models ; ( d ) kapur - kesavan inverse models ; and ( e ) game theoretic models .",
    "it is shown that the combinatorial definition and maxprob are consistent with these different approaches , and the `` subjective bayesian '' definition of probability , yet is more broadly based and offers greater utility than traditional maxent / minxent based on the shannon and kullback - leibler functions .",
    "the author thanks the european commission for support as a marie curie incoming international fellow ; marian grendr , david blower , tony bell and flemming topse for specific arguments ( as detailed above ) and stimulating discussions ; and the organisers and participants of maxent07 .",
    "jaynes , phys .",
    "106 ( 1957 ) 620 .",
    "shannon , bell sys .",
    "j. 27 ( 1948 ) 379 ; 623 .",
    "jaynes , _ in _ k.w .",
    "ford ( ed . ) , brandeis university summer institute , lectures in theoretical physics , vol .",
    "3 , benjamin - cummings publ . co. ( 1963 ) 181 .",
    "jaynes , ieee trans .",
    "systems science and cybernetics ssc-4 ( 1968 ) 227 .",
    "jaynes , _ in _ r.d .",
    "levine , m. tribus ( eds . ) , the maximum entropy formalism , mit press , cambridge , ma ( 1978 ) 15 .",
    "jaynes , ( g.l . bretthorst , ed . ) probability theory : the logic of science , cambridge u.p . , cambridge , 2003 .",
    "s. kullback , r.a .",
    "leibler , annals math .",
    "( 1951 ) 79 .",
    "s. kullback , information theory and statistics , john wiley , ny , 1959 .",
    "levine , m. tribus ( eds . ) , the maximum entropy formalism , mit press , cambridge , ma , 1978 .",
    "kapur , maximum - entropy models in science and engineering , john wiley , ny , 1989 .",
    "kapur , h.k .",
    "kesevan , entropy optimization principles with applications , academic press , inc . , boston , ma , 1992 .",
    "l. szilard , zeitschrift fr physik 53 ( 1929 ) 840 .",
    "shore , r.w .",
    "johnson , ieee trans .",
    "information theory it-26(1 ) ( 1980 ) 26 .",
    "fisher , philos .",
    "royal soc .",
    "london a 222 ( 1922 ) 309 .",
    "fisher , proc .",
    "camb . philos .",
    "( 1925 ) 700 .",
    "bose , z. phys .",
    "26 ( 1924 ) 178 .",
    "a. einstein , sitzungsber .",
    "kl ( 1924 ) 261 .",
    "a. einstein , sitzungsber .",
    "kl ( 1925 ) 3 .",
    "e. fermi , z. phys .",
    "36 ( 1926 ) 902 .",
    "dirac , proc .",
    "( 1926 ) 661 .",
    "a. rnyi , proc .",
    "4th berkeley symp .",
    "stat . and prob . 1 ( 1961 ) 547",
    "sharma , d.p .",
    "mittal , j. math .",
    "( calcutta ) 10 ( 1975 ) 28 .",
    "sharma , d.p .",
    "mittal , j. combinat .",
    "sci . 2 ( 1977 ) 122 .",
    "c. tsallis , j. stat .",
    "52(1/2 ) ( 1988 ) 479 . c. tsallis , _ in _ s. abe , y. okamato ( eds . ) , nonextensive statistical mechanics and its applications , springer , berlin , ( 2001 ) 3 .",
    "g. kaniadakis , physica a 296(3 - 4 ) ( 2001 ) 405 .",
    "g. kaniadakis , phys . rev .",
    "e 66(5 ) ( 2002 ) 056125 .",
    "c. beck , e.g.d .",
    "cohen , physica a 322 ( 2003 ) 267 .",
    "niven , phys .",
    "lett . a 342(4 ) ( 2005 ) 286 . r.k .",
    "niven , physica a 365(1 ) ( 2006 ) 142 . l. boltzmann , wiener berichte , 76 ( 1877 ) 373 - 435 . m. planck , annalen der physik 4 ( 1901 ) 553 .",
    "i. vincze , progress in statistics , 2 ( 1974 ) 869 - 895 .",
    "m. grendr , jr . and",
    "m. grendr , _ in _ bayesian inference and maximum entropy methods in science and engineering , a. mohammad - djafari ( ed . ) , aip , melville ( 2001 ) 83 - 94 .",
    "r. k. niven , _ cond - mat/0512017 _ , 2005 - 2007 .",
    "tseng , a. caticha , yet another resolution of the gibbs paradox : an information theory approach , preprint ( 2002 ) .",
    "l. brillouin , annales de physique 7 ( 1927 ) 315 .",
    "l. brillouin , les statistiques quantiques et leurs applications , les presses universitaires de france , paris , 1930 .",
    "tolman , the principles of statistical mechanics , oxford univ . press ,",
    "london , 1938 .",
    "l. brillouin , j. appl .",
    "22(3 ) ( 1951 ) 338 .",
    "n. davidson , statistical mechanics , mcgraw - hill , ny , 1962 .",
    "niven , ctnext07 , catania , sicily , italy , july 2007 , in submission to aip .",
    "jensen , _ in _ s. kotz , n.l .",
    "johnson , encyclopedia of statistical sciences , 6 : 5200 .",
    "s. berg , urn models , _ in _ s. kotz , n.l .",
    "johnson , _ encyclopedia of statistical sciences _ , 9 : 424 .",
    "f. eggenberger , g. plya , ber die statistik verketter vorgnge z. angew .",
    ", 1 ( 1923 ) 279 - 289",
    ". h. s. steyn , proc .",
    "koninklijke nderlandse akademie van wetenschappen , ser .",
    "a , 54 ( 1951 ) 23 - 30 . n. l. johnson , s. kotz , n. balakrishnan , discrete multivariate distributions .",
    "new york : wiley , 1997 .",
    "m. grendar , r.k .",
    "cond - mat/0612697 _ , 2006 .",
    "f. topse , ieee trans .",
    "theory 48(8 ) ( 2002 ) 2368 .",
    "f. topse , physica a 340 ( 2004 ) 11 ."
  ],
  "abstract_text": [
    "<S> the combinatorial basis of entropy , given by boltzmann , can be written @xmath0 , where @xmath1 is the dimensionless entropy , @xmath2 is the number of entities and @xmath3 is number of ways in which a given realization of a system can occur ( its statistical weight ) . </S>",
    "<S> this can be broadened to give generalized combinatorial ( or probabilistic ) definitions of entropy and cross - entropy : @xmath4 and @xmath5 , where @xmath6 is the probability of a given realization , @xmath7 is a convenient transformation function , @xmath8 is a scaling parameter and @xmath9 an arbitrary constant . </S>",
    "<S> if @xmath3 or @xmath6 satisfy the multinomial weight or distribution , then using @xmath10 and @xmath11 , @xmath1 and @xmath12 asymptotically converge to the shannon and kullback - leibler functions . </S>",
    "<S> in general , however , @xmath3 or @xmath6 need not be multinomial , nor may they approach an asymptotic limit . in such cases , </S>",
    "<S> the entropy or cross - entropy function can be _ defined _ so that its extremization ( `` maxent '' or  minxent \" ) , subject to the constraints , gives the `` most probable '' ( `` maxprob '' ) realization of the system . </S>",
    "<S> this gives a probabilistic basis for maxent and minxent , independent of any information - theoretic justification .    </S>",
    "<S> this work examines the origins of the governing distribution @xmath6 . </S>",
    "<S> these include :  ( a ) frequentist - like models ; ( b ) symmetry models ; ( c ) prior minxent models ; ( d ) kapur - kesavan inverse models ; and ( e ) game theoretic models . </S>",
    "<S> the combinatorial definition and maxprob are consistent with these different approaches , and the notion of probabilistic inference , yet offer greater utility than traditional maxent / minxent based on the shannon and kullback - leibler functions .    </S>",
    "<S> = 1     address=(1 ) school of aerospace , civil and mechanical engineering , the university of new south wales at adfa , canberra , act , 2600 , australia . </S>",
    "<S> email : r.niven@adfa.edu.au , altaddress=(2 ) niels bohr institute , university of copenhagen , copenhagen  , denmark . </S>"
  ]
}