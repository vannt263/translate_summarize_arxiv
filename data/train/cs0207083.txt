{
  "article_text": [
    "non - monotonic reasoning , for example default logic  @xcite , models the intuitive process of making non - deductive inferences in the face of certain supportive but not conclusive evidence . given a default theory @xmath0",
    ", we can obtain its extensions by following a prescribed set of steps .",
    "however , on what grounds do we employ a particular default rule ? some writers would regard this as an inappropriate question , since they take as their goal the representation of human inference . to this",
    "end defaults represent rules that we take to be intuitively appropriate .",
    "but then when we apply these rules , we may be led to counterintuitive results  @xcite .",
    "the underlying principle seems to be circular : the original default rules are `` intuitively good '' at first glance , but when we discover that they do not give rise to the desired results , we tweak the rules until they give us those results .",
    "it seems that we have to know what results we want first before constructing the default theory , rather than having the default theory tell us what conclusions are warranted .",
    "this is precisely the reason why we need an independent measure of validity for default rules and default extensions .",
    "we think of nonmonotonic logic as sharing the normative character of other logics . from this point of view default rules",
    "require some defense .",
    "we will concentrate on default logic here , though much of what we have to say will apply to other nonmonotonic approaches as well .",
    "much of the work on nonmonotonic logic has concerned the syntactic manipulation of the nonmonotonic rules , rather than their basic justification .      for a _ default rule _",
    "@xmath1 , @xmath2 is the _",
    ", @xmath3 are the _ justifications _ , and @xmath4 is the _ consequent _ of @xmath5 . loosely speaking",
    ", the rule conveys the idea that if @xmath2 is provable , and @xmath6 are each not provable , then by default we conclude that @xmath4 is true .",
    "default theory _ is an ordered pair @xmath7 , where @xmath8 is a set of default rules and @xmath9 is a set of `` facts '' .",
    "a theory extended from @xmath9 by applying the default rules in @xmath8 is known as an _ extension _ of the default theory .",
    "consider the following canonical example .",
    "we have a default theory @xmath10 , where @xmath11    we get two extensions , one containing @xmath12 and the other containing @xmath13 . if we take `` @xmath14 '' to mean that @xmath15 is a bird , `` @xmath16 '' to mean that @xmath15 is a penguin , and `` @xmath17 '' to mean that @xmath15 flies , then we would like to reject the extension containing `` @xmath12 '' ( @xmath18 flies ) in favor of the extension containing `` @xmath13 '' ( @xmath18 does not fly ) .",
    "however , if we take `` @xmath16 '' to mean that @xmath15 is an animal instead and keep `` @xmath14 '' and `` @xmath17 '' the same , we would want to reverse our preference .",
    "now the extension containing `` @xmath12 '' ( @xmath18 flies ) seems better .",
    "note that each of the default rules involved in the example above is intuitively appealing when viewed by itself against our background knowledge : birds fly ; penguins do not fly ; and animals in general do not fly either .",
    "moreover , both instantiations ( penguins and animals ) are syntactically identical .",
    "thus , we can not base our decision to prefer one default rule over the other by simply looking at their syntactic structures .",
    "it is the interaction between the default rules and evidence that gives rise to the selective preference above .",
    "we have the evidence that `` @xmath18 is a bird '' .",
    "if in addition we also have `` @xmath18 is a penguin '' , we prefer the penguin rule . if instead we have `` @xmath18 is an animal '' , we prefer the bird rule .",
    "there are several approaches to circumventing this conceptual difficulty .",
    "the first is to revise the default theory so that the desired result is achieved  @xcite .",
    "we can amend the default rules by adding the exceptions as justifications , for example @xmath19 and @xmath20 . with this approach",
    "we have to constantly revise the default rules to take into account additional exceptions .",
    "we have little guidance in constructing the list of justifications except that the resulting default rule has to produce the `` right '' answer in the given situation .",
    "another approach is to establish some priority structure over the set of defaults .",
    "for example , we can refer to a specificity or inheritance hierarchy to determine which default rule should be used in case of a conflict  @xcite .",
    "the penguin rule is more specific than the bird rule , when both are applicable , and therefore we use the penguin rule and not the bird rule .",
    "however , conflicting rules do not always fit into neat hierarchies ( for example , adults are employed , students are not , how about adult students ?",
    "it is not obvious how we can extend the hierarchical structure without resorting to explicitly enumerating the priority relations between the default rules  @xcite .",
    "the third approach is to appeal to probabilistic analysis .",
    "defaults are interpreted as representing properties of conditional probabilities .",
    "for example , the conditional probability of @xmath18 being able to fly given that @xmath18 is a bird is `` high ''  @xcite or increases from the prior probability  @xcite , while the conditional probability of @xmath18 being able to fly given that @xmath18 is a penguin is `` low '' or decreases .",
    "this approach provides a probabilistic semantics for default rules , but in a way which does not represent the fact that the conclusions are _",
    "accepted_. the default conclusion is `` tweety flies , '' not `` probably tweety flies . ''",
    "this is in contrast to the spirit of nonmonotonic reasoning : default conclusions should be accepted as new facts , and we should be able to chain default rules and build upon the conclusions of previous default applications to obtain further conclusions .",
    "the justification of beliefs is a long standing issue in epistemology .",
    "there is not much that is problematic about the justification of beliefs obtained by deductive inference ( though there are plenty of problems that surround deduction  see @xcite , not to mention the voluminous literature on paraconsistent logic @xcite ) .",
    "the reason is that we can show that the ordinary rules of deduction lead from premises to conclusions that are true in every model in which the premises are true .",
    "this is exactly what is not true of ampliative inference , and it is what has led some writers ( e.g. , @xcite ) to deny that there _ is _ any such thing as a nonmonotonic logic .",
    "this has been disputed in @xcite .    but",
    "other kinds of justifications of beliefs have been proposed .",
    "isaac levi @xcite has argued for many years that the way to understand ampliative ( inductive , nonmonotonic ) argument is in terms of decision theory : we choose ( decide ) to accept a hypothesis in a given context provided that the expected epistemic utility of doing so in that context is greater than the expected utility of any other epistemic act , such as suspending belief totally , or accepting a stronger hypothesis .",
    "levi s approach employs a rich and detailed structure for acceptance , and allows drawing many important distinctions .",
    "this structure requires three things that make it less than perfect as a vehicle for ordinary nonmonotonic inferential systems .",
    "first , in keeping with a long tradition in pragmatism @xcite the context of inquiry must be tied to a specific problem : we need the answer to a question .",
    "second , the epistemic expectation of an answer is the expected value of the _ information _ contained in that answer .",
    "thus we need to presuppose an information measure on the language of our inquiry @xcite .",
    "third , we need to have available a credal or inductive probability , based on a measure ( or convex set of measures ) on the sentences of the language , in terms of which a conditional probability ( or convex set of conditional probabilities ) can be defined @xcite .",
    "it is our belief both that in some contexts in which we might wish to use nonmonotonic mechanisms , this overhead is unnecessary , and perhaps itself difficult to justify , and that we would like to be able to explicate the justification of inference in a less context dependent way .",
    "another approach that has attracted considerable attention in the philosophical community in recent years is that of `` reliabilism '' whose best known exponent is alvin goldman @xcite .",
    "according to this view , what justifies a belief is the fact that it is obtained by a `` reliable cognitive process ... '' @xcite of course there are a number of additional hedges to the view that are required for philosophical accuracy , and even with those hedges there remains a certain vagueness in the view .",
    "these details need not detain us , since we are seeking inspiration rather than philosophical precision .",
    "what does `` reliable '' mean ?",
    "we will construe reliability in terms of frequency or propensity to yield truth when applied .",
    "specifically , we will say that the belief @xmath21 is nonmonotonically justified by a default rule if the rule would frequently lead to truth and rarely to error , given what we know  given our background knowledge .",
    "a deductive argument is justified ( valid ) if its conclusion is true in every model of its premises .",
    "we will attempt to provide an analog of the justification of deductive rules : a default argument is justified if its conclusion is true in a high proportion of the relevant models in which its premises are true .",
    "to make this idea precise requires an excursion into model theory .",
    "we will suppose that the underlying object language is a first order language that does not involve such intensional predicates as `` know '' or `` believe . ''",
    "a number of nonmonotonic formalisms ( specifically autoepistemic logic  @xcite ) do involve such locutions within the object language , but they can be dispensed with in default logic .",
    "the default rule @xmath22 can be read in terms of the nonmembership of @xmath23 in a specified set of expressions @xmath24 . in original default logic",
    ", @xmath24 would just be an extension .",
    "there are a number of immediate problems associated with the idea of looking at the `` proportion '' of models .",
    "the least of them is choosing a _ level _ at which to regard the evidence as adequate .",
    "should we require that the proportion be 0.95 ? or 0.99 ? or 0.995 ?",
    "this is just the sort of question that arises in statistical hypothesis testing or in confidence interval estimation .",
    "we shall suppose that in a given context there is some agreed - upon level of security @xmath25 ; we will accept a conclusion if the proportion of models in which we could be committing an error is no greater than @xmath25 .",
    "this approach is to be contrasted with those of adams @xcite , pearl @xcite and bacchus et al @xcite .",
    "adams requires that for @xmath26 to be a reasonable consequence of the set of sentences @xmath27 , for _ any _ @xmath28 there must be a positive @xmath25 such that for every probability function , if the probability of every sentence in @xmath27 is greater than @xmath29 , then the probability of @xmath26 is at least @xmath30 @xcite .",
    "pearl s approach similarly involves quantification over possible probability functions .",
    "bacchus et al again take the degree of belief of a statement to be the limiting proportion of first order models in which the statement is true .",
    "all of these approaches involve matters that go well beyond what we may reasonably suppose to be available to us as empirical enquirers .",
    "our @xmath25 , on the other hand , serves much like the @xmath2 of statistical testing .",
    "we must restrict the number of models under consideration to a finite number so that the idea of looking at proportions makes sense .",
    "we will be taking account of statistical information , and to this end will want each model to have a finite domain . roughly speaking",
    ", we take as a model of our language one in which the domain of empirical individuals is of finite cardinality .",
    "this may be regarded as problematic ( it entails the falsity of `` every person has two parents and nobody is his own ancestor '' ) but with reasonable spatial and temporal bounding it can be rendered plausible .",
    "even so , to ensure that the set of models is finite we must restrict the empirical domain even further . not only",
    "must it consist of a finite set of physical entities , but this same set of physical entities @xmath31 must be taken to be the empirical domain of every model .",
    "we assume that it is possible to express statistical knowledge in this language .",
    "for example , if `` @xmath32 '' is the predicate `` is a bird '' and `` @xmath33 '' is `` can fly '' , we can express the fact that between 85% and 95% of birds fly by the formula @xmath34 . employing the notation of @xcite",
    "we write this as `` @xmath35 . ''",
    "this renders `` % '' a variable binding operator on 4-sequences of expressions : two formulas and two fractions .",
    "we distinguish , as do pearl and geffner @xcite between immediate evidence , represented by a finite set of sentences @xmath36 concerning particular facts ( to be distinguished from the general body of factual knowledge @xmath9 invoked by classical default logic ) , and a finitely axiomatizable set of sentences @xmath37 representing general background knowledge . what defaults are plausible depends , of course , on background knowledge .",
    "if it were not for what we take to be the typical ( or natural , or frequent ) behavior of birds , the world s best known example of a default rule would not be plausible .",
    "on the other hand no one has proposed the default rule @xmath38 .",
    "thus in general we will represent the set of default rules of a default theory as @xmath39 rather than @xmath8 , since we take them to be a function of our body of general knowledge @xmath37 .",
    "given an error tolerance @xmath25 , we will take a default rule to be _ @xmath25-valid _",
    "if , for every set of possible input sentences @xmath36 consistent with @xmath37 , the application of the rule to @xmath36 leads to a false conclusion in a proportion of at most @xmath25 of the relevant models .",
    "more precisely , a default rule is @xmath25-valid if and only if for every set of input sentences @xmath36 consistent with @xmath37 to which the rule is applicable , the proportion of models of @xmath40 in which the conclusion of the rule is false is no more than @xmath25 .    to fix our ideas ,",
    "let us begin with a simple example .",
    "suppose @xmath37 includes a statement to the effect that at least @xmath29 and not more than 1 - @xmath28 of birds fly and nothing else ; that is , `` @xmath41 . ''",
    "consider the rule @xmath42 .",
    "this rule is `` applicable '' to immediate evidence @xmath36 only if @xmath40 entails a sentence of the form @xmath43 and no corresponding sentence of the form @xmath44 .",
    "our models have a single domain @xmath31 of finite cardinality .",
    "we will write `` @xmath45 '' for the interpretation of @xmath21 in the model @xmath46 .",
    "the constraint imposed by @xmath37 is that for every model @xmath46 the proportion of objects in @xmath47 that are also in @xmath48 lies in @xmath49 $ ] .",
    "there are three cases .",
    "first , suppose @xmath40 does not entail a sentence of the form @xmath43 .",
    "then the rule is inapplicable .",
    "second , suppose that for some term @xmath18 , @xmath40 entails `` @xmath50 '' and also entails `` @xmath51 '' .",
    "the rule is again inapplicable , because it is blocked by the failure of a justification .",
    "third , suppose for some term @xmath18 , @xmath40 entails `` @xmath50 '' but not `` @xmath51 '' .",
    "then @xmath52 .",
    "there are @xmath53 interpretations of @xmath18 that make @xmath40 true ; of these at least @xmath54 make `` @xmath55 '' true .",
    "we have said nothing about interpreting the rest of the language , but however many interpretations there are ( we have seen to it that there are only a finite number ) the proportion that renders `` @xmath55 '' true will remain unchanged ; it will be at least @xmath54 .",
    "thus , given the background knowledge that we have posited , the rule is @xmath25-valid : if it is applicable it will lead to error no more than @xmath56 of the time .",
    "now let us consider a somewhat more complex example : suppose we know that typically birds fly , and that typically penguins do nt .",
    "if that is in our background knowledge @xmath37 , as well as `` @xmath57 '' , then the flying default becomes @xmath58 , and we also have the default @xmath59 if @xmath36 entails `` @xmath60 '' , only the second default is applicable . in no more than @xmath25 of the models of @xmath36 will @xmath18 fly , unless @xmath40 entails that @xmath18 can fly .",
    "another example : suppose @xmath37 contains vague information about the frequency with which red birds fly ( perhaps because we have encountered few red birds ) .",
    "say that we know the frequency to be between 0.50 and 1.0 .",
    "since the interval for birds in general @xmath49 $ ] is included in @xmath61 $ ] , this additional piece of information should not interfere with our inference of flying ability .",
    "there is no _ conflict _ between the two intervals , just less precision in one .",
    "the rule about birds in general can be applied to red birds . however ,",
    "if @xmath37 contains the knowledge that between 0.5 and @xmath62 of red birds fly , where @xmath62 is less than @xmath63 , then this information _ should _ interfere . in this case",
    "the general rule should be so construed that it does not apply to red birds . if @xmath64 is a red bird and not a penguin , no conclusion about flying ability is justified .",
    "we can arrange this by judiciously adding or deleting justifications in the general bird rule , in accordance with the statistical information in @xmath37 : in the first case we allow red birds ; in the second we must require that we do not know @xmath18 is red : `` @xmath65 '' must be among the justifications of the rule .",
    "this statistical approach provides exactly the normative guidance that is lacking in the ad - hoc approach of tweaking default rules in order to arrive at the `` intuitive '' results .",
    "more generally , we can give recipes for constructing @xmath25-valid defaults for conclusions of the form @xmath66 from background knowledge @xmath37 and immediate evidence @xmath36 .",
    "to be an @xmath67-sequence of terms .",
    "furthermore , any such conclusion can be taken to be an instance of the consequent of a statistical generalization , in virtue of the fact that statistical generalizations merely impose bounds .",
    "we are not imposing serious limitations on default rules . for details , see @xcite .",
    "] let @xmath37 contain @xmath68 and @xmath69 .",
    "we consider three cases :    1 .",
    "@xmath37 entails @xmath70 .",
    "there are three subcases according to the relation among @xmath71 , @xmath72 , @xmath73 , @xmath74 : 1 .",
    "( @xmath75 and @xmath76 ) or ( @xmath77 and @xmath78 ) + @xmath79 and @xmath80 are candidate defaults .",
    "2 .   @xmath75 and @xmath78 + @xmath81 is the only candidate default , since the justification @xmath82 of @xmath83 is inconsistent with the prerequisite @xmath84 and @xmath37 .",
    "3 .   @xmath77 and @xmath76 + @xmath79 and @xmath80 are candidate defaults .",
    "2 .   @xmath37 entails @xmath85 .",
    "this is symmetrical to case 1 .",
    "@xmath37 entails neither @xmath70 nor @xmath85 .",
    "again there are three subcases : 1 .",
    "( @xmath75 and @xmath76 ) or ( @xmath77 and @xmath78 ) + the candidate defaults are @xmath83 and @xmath86 .",
    "2 .   @xmath75 and @xmath78 + @xmath83 and @xmath81 are candidate default rules .",
    "@xmath77 and @xmath76 : this is symmetrical to case 3(b ) .",
    "having generated a list of candidate default rules based on our background knowledge @xmath37 , we delete those rules derived from statistics with a lower measure less than @xmath87 .",
    "the remainder is the set of defaults @xmath39 .",
    "we have not taken account of relations among default conclusions that may be entailed by @xmath37 .",
    "if @xmath37 contains @xmath88 then the default conclusion @xmath89 behaves just like the default conclusion @xmath90 .",
    "if @xmath37 contains @xmath91 , then since @xmath92 is equivalent to @xmath93 and @xmath94 is equivalent to @xmath95 we can make use of the obvious entailment relations .    soundness of a system of deductive logic requires that the conclusion of any inference be true in every model in which the premises are true .",
    "clearly nonmonotonic inference should not be sound .",
    "but there is a property that is _ like _ soundness that applies to default inference .",
    "it is the property that the conclusion is false in at most a fraction @xmath25 of the models of the premises @xmath96 .",
    "for every set of observations @xmath36 , if @xmath97 is applicable to @xmath36 , the proportion of models of @xmath40 in which the conclusion of @xmath5 is false is less than @xmath25 .",
    "the proof of this theorem is provided by the soundness theorem for evidential probability @xcite , since the rules for deriving defaults are a subset of the rules for computing evidential probabilities . @xmath98",
    "having determined which default rules are justified with respect to the background knowledge , the next step is to investigate the interaction between default rules in generating an extension .",
    "a default extension is a minimal deductively closed set that contains the given facts and the consequents of all applicable default rules . given an evidence set , we need to determine how to control the compound effects of multiple defaults in an extension .",
    "take for example , a default version  @xcite of the probabilistic lottery paradox  @xcite .",
    "there are @xmath67 species of birds , @xmath99 .",
    "we can say that penguins are atypical in that they can not fly ; hummingbirds are atypical in that they have very fine motor control ; parrots are atypical in that they could talk ; and so on .",
    "if we apply this train of thought to all @xmath67 species of birds , there is no typical bird left , as for each species there is always at least one aspect in which it is atypical .",
    "a parallel scenario is formulated below .",
    "[ ex : birds ] @xmath37 contains    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ @xmath100 +   + @xmath101 , for all @xmath102 +   + @xmath103 + _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    from @xmath37 we can derive @xmath67 @xmath104-valid default rules for @xmath39 : @xmath105 where @xmath104 is the maximum of @xmath106 .",
    "now consider the evidence set @xmath107 . in the original formulation of default logic",
    ", we would get @xmath67 extensions , each one containing one @xmath108 and the negations of all the other @xmath109 s .",
    "thus , for each extension , we would conclude that @xmath18 is a particular species of bird , which seems to be an over commitment , considering we have @xmath110 in @xmath37 .",
    "note that each of the @xmath67 default rules is @xmath25-valid when considered individually , but in an extension the rules interact to sanction a set of conclusions that when taken together seems implausible according to our knowledge of model frequencies .",
    "the definition of an extension dictates that we must keep applying rules until all `` applicable '' ones are exhausted .",
    "the `` applicability '' condition is based on maximizing logical strength : for @xmath111 , as long as @xmath2 is derivable , and the @xmath112 s are consistent with the extension , we must apply @xmath5 and add @xmath4 to the extension .",
    "thus , for each of the extensions above , we have to keep applying the rules until we have drawn @xmath113 conclusions : @xmath114 for all @xmath102",
    ". then the consistency requirement blocks the last default rule @xmath115 , as @xmath116 together with @xmath114 for all @xmath102 gives us @xmath108 , contradicting the @xmath112 of @xmath115 . from @xmath110 ,",
    "we know the proportion of models in which @xmath108 is true , and thus the proportion of models satisfying this extension , given @xmath36 , is at most @xmath117 , a small ratio .",
    "the validity criteria for individual default rules can be extended to extensions resulted from the application of a chain of default rules .",
    "we can think of the task of regulating the compound effect of multiple default rules as adjusting the set of relevant models by taking into account the default conclusions of all previously applied rules in the chain of reasoning .    one way to accomplish this is by _ sequential thresholding _",
    "the applicability condition of a default rule @xmath118 in an extension can be modified to take into account the validity of the rule .",
    "in addition to requiring that @xmath2 is provable and that none of @xmath6 are provable , we require that the default rule be `` above threshold '' , that is , the proportion of relevant models satisfying the consequent @xmath4 be greater than a threshold @xmath119 .",
    "the set of relevant models shrinks in a stepwise fashion .",
    "we start out with all the models satisfying the background knowledge and evidence we have . as default rules",
    "are applied sequentially , the consequent of the applied rule at each step is taken as true in all subsequent steps .",
    "the relevant models at a particular step are then those that are consistent with the given facts and all the consequents of the rules applied in the previous steps .",
    "a default rule , even if it is @xmath25-valid with respect to the background knowledge , would be blocked from application if it does not satisfy the thresholding criterion .    in  @xcite",
    ", the thresholding metric is based on a simple probability measure of possible worlds .",
    "we can easily extend this metric to employ the same measure as that used for evaluating the @xmath25-validity of default rules .",
    "reconsider example  [ ex : birds ] .",
    "let us take @xmath120 .",
    "we start out with the set @xmath121 of all models satisfying @xmath37 and @xmath36 . from @xmath122 we know that @xmath123 is above threshold , and it satisfies the other conditions for applicability .",
    "therefore we apply @xmath123 and conclude @xmath124 .",
    "now consider @xmath125 .",
    "the set @xmath126 of relevant models at this point is a subset of @xmath121 ; it contains only those models in @xmath121 that satisfy our new conclusion @xmath124 as well .",
    "we have eliminated the models in which @xmath127 is true . since @xmath128 , and @xmath129 , all the models eliminated satisfy @xmath50 , and none satisfies @xmath130 .",
    "thus , in @xmath126 , the number of models satisfying @xmath130 is the same as in @xmath121 .",
    "however , the number of models satisfying @xmath50 is lower in @xmath126 as a result of the addition of @xmath124 .",
    "this gives rise to a higher proportion of models satisfying @xmath130 in @xmath126 ( @xmath131 ) than in @xmath121 ( @xmath132 ) .",
    "if @xmath133 , @xmath125 is still above threshold after the application of @xmath123 , and we can apply it to obtain @xmath134 .",
    "otherwise , @xmath125 is below threshold , and we can not apply it even though it was above threshold before the application of @xmath123 .    after each step of applying a rule , the set of relevant models shrinks , and the proportion of @xmath108 of any unapplied rule @xmath115 increases . after a number of steps , all the remaining rules would be below threshold , and we thus obtain an extension containing only a portion of the conclusions that would otherwise be present in the non - thresholding version of the extension .",
    "note that the size of @xmath135 determines how much risk is tolerated in an extension .",
    "the higher the @xmath135 , the more of the rules can be applied and the longer they can stay above threshold .",
    "reiter s non - thresholding version corresponds to the case when @xmath136 ; that is , every rule whose associated proportion is above 0 is allowed , and logical consistency alone determines the rule s admissibility .",
    "we have developed a notion of validity for default inference based on model proportions .",
    "a rule is @xmath25-valid if the proportion of models in which the consequent of the rule is satisfied is greater than @xmath54 in the relevant models picked out by the background knowledge , the evidence , and the applicability conditions of the default rule . given a body of background knowledge @xmath37 , we can systematically generate candidate default rules and determine which ones are @xmath25-valid based on the statistical facts known in @xmath37 .",
    "conflicts between default rules stemming from multiple inheritance are resolved as a consequence of the validation process .",
    "the result is a set of @xmath25-valid default rules which are `` pre - compiled '' for a given background knowledge base , and can be reused for different evidence sets without change .",
    "this idea of evaluating the validity of a default rule using model proportions is extended to extensions generated by a combinations of rules .",
    "the compound effect is regulated by a sequential thresholding process , which blocks the rules whose associated model proportions with respect to the `` current '' ( shrinking ) set of models fall below a particular comfort threshold .",
    "this allows us to use a more reasonable `` closure condition '' for extension than the usual maximal logical strength : we can refrain from applying rules that would make the extension satisfiable in only a small set of models , even if the consequent of the rule is logically consistent with the extension .",
    "grounding the justification of default rules in model proportions provides a way to validate the rules empirically , and is a first step towards automating the learning of default rules from ( statistical ) data .",
    "one might ask why we need the default rules when we can reason with the statistics directly .",
    "default rules provide a succinct and more understandable characterization of the import of the data , as well as a smooth articulation of the information that may exist in the knowledge base .",
    "this work was supported by the national science foundation sts-9906128 , iis-0082928 , and nasa ncc2 - 1239 .",
    "fahiem bacchus , adam  j. grove , joesph  y. halpern , and daphne koller .",
    "statistical foundations for default reasoning . in _ proceedings of the thirteenth international joint conference on artificial intelligence _ , pages 563569 , 1993 .",
    "j.  f. horty , d.  s. touretzky , and r.  h. thomason . a clash of intuitions : the current state of nonmonotonic multiple inheritance systems . in _ proceedings of the tenth international joint conference on artificial intelligence _ ,",
    "pages 476482 , 1987 .",
    "judea pearl and hector geffner .",
    "a framework for reasoning with defaults . in henry",
    "e. kyburg , jr .",
    ", ronald  p. loui , and greg  n. carlson , editors , _ knowledge representation and defeasible reasoning _ , pages 6987 .",
    "kluwer , 1990 .",
    "david poole .",
    "what the lottery paradox tells us about default reasoning . in _ proceedings of the first international conference on principles of knowledge representation and reasoning _ ,",
    "pages 333340 , 1989 ."
  ],
  "abstract_text": [
    "<S> we seek to find normative criteria of adequacy for nonmonotonic logic similar to the criterion of validity for deductive logic . </S>",
    "<S> rather than stipulating that the conclusion of an inference be true in all models in which the premises are true , we require that the conclusion of a nonmonotonic inference be true in `` almost all '' models of a certain sort in which the premises are true . </S>",
    "<S> this `` certain sort '' specification picks out the models that are relevant to the inference , taking into account factors such as specificity and vagueness , and previous inferences . the frequencies characterizing the relevant models reflect known frequencies in our actual world . </S>",
    "<S> the criteria of adequacy for a default inference can be extended by thresholding to criteria of adequacy for an extension . </S>",
    "<S> we show that this avoids the implausibilities that might otherwise result from the chaining of default inferences . </S>",
    "<S> the model proportions , when construed in terms of frequencies , provide a verifiable grounding of default rules , and can become the basis for generating default rules from statistics .    </S>",
    "<S> 2ex _ keywords _ : probability , frequency , default logic </S>"
  ]
}