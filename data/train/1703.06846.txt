{
  "article_text": [
    "one of the key attributes fueling the success of deep learning is the ability of deep networks to compactly represent rich classes of functions .",
    "this phenomenon has drawn considerable attention from the theoretical machine learning community in recent years .",
    "the primary notion for formally reasoning about the representational abilities of different models is _ expressive efficiency_. given two network architectures  @xmath0 and  @xmath1 , with size parameters ( typically the width of layers across a network ) @xmath2  and  @xmath3 , we say that architecture  @xmath0 is expressively efficient w.r.t .",
    "architecture  @xmath1 if the following two conditions hold : _",
    "( i ) _  any function realized by  @xmath1 with size  @xmath3 can be realized ( or approximated ) by  @xmath0 with size @xmath4 ; _ ( ii ) _  there exist functions realized by  @xmath0 with size  @xmath2 that can not be realized ( or approximated ) by  @xmath1 unless its size meets @xmath5 for some super - linear function  @xmath6 .",
    "the nature of the function  @xmath6 in condition  _ ( ii ) _ determines the type of efficiency taking place  ",
    "if  @xmath6 is exponential then architecture  @xmath0 is said to be exponentially expressively efficient w.r.t .",
    "architecture  @xmath1 , and if  @xmath6 is polynomial so is the expressive efficiency of  @xmath0 over  @xmath1 .    to date ,",
    "works studying expressive efficiency in the context of deep learning ( e.g.  ) have focused on the architectural feature of depth , showing instances where deep networks are expressively efficient w.r.t .",
    "shallow ones . this theoretical focus is motivated by the vast empirical evidence supporting the importance of depth ( see @xcite for a survey of such results ) .",
    "however , it largely overlooks an additional architectural feature that in recent years is proving to have great impact on the performance of deep networks    _ connectivity_. nearly all state of the art networks these days ( e.g.  @xcite ) deviate from the simple feed - forward approach , running layers in parallel with various connectivity ( split / merge ) schemes . whether or not this relates to expressive efficiency remains to be an open question .",
    "a specific family of deep networks gaining increased attention in the deep learning community is that of _ dilated convolutional networks_. these models form the basis of the recent wavenet  ( @xcite ) and bytenet  ( @xcite ) architectures , which provide state of the art performance in audio and text processing tasks .",
    "dilated convolutional networks are typically applied to sequence data , and consist of multiple succeeding convolutional layers , each comprising non - contiguous filters with a different dilation ( distance between neighboring elements ) .",
    "the choice of dilations directly affects the space of functions that may be realized by a network , and while no choice is expressively efficient w.r.t .",
    "another , we show in this work that interconnecting networks with different dilations leads to expressive efficiency , and by this demonstrate that connectivity indeed bears the potential to enhance the expressiveness of deep networks .",
    "our analysis follows several recent works utilizing tensor decompositions for theoretical studies of deep learning ( see for example  @xcite ) , and in particular , builds on the equivalence between hierarchical tensor decompositions and convolutional networks established in   and  .",
    "we show that with dilated convolutional networks , the choice of dilations throughout a network corresponds to determination of the mode ( dimension ) tree underlying the respective decomposition .",
    "we then define the notion of a _ mixed tensor decomposition _ , which blends together multiple mode trees , effectively creating a large ensemble of hybrid trees formed from all possible combinations .",
    "mixed tensor decompositions correspond to _ mixed dilated convolutional networks _ , _",
    "i.e. _  mixtures formed by connecting intermediate layers of different dilated convolutional networks .",
    "this allows studying the expressive properties of such mixtures using mathematical machinery from the field of tensor analysis .",
    "we fully analyze a particular case of dilated convolutional arithmetic circuits , showing that a single connection between intermediate layers already leads to an almost quadratic expressive efficiency , which in large - scale settings typically makes the difference between a model that is practical and one that is not .",
    "an experiment on timit speech recognition dataset  ( @xcite ) demonstrates the gain brought forth by mixing different networks , showing that interconnectivity can indeed boost the performance of dilated convolutional networks .",
    "the remainder of the paper is organized as follows .",
    "[ sec : prelim ] provides preliminary background in the field of tensor analysis , and establishes notational conventions .",
    "[ sec : dcn ] presents dilated convolutional networks , and their correspondence to tensor decompositions . in sec .",
    "[ sec : mtd ] we define mixed tensor decompositions , and discuss their equivalence to mixed dilated convolutional networks . our analysis of expressive efficiency is given in sec .",
    "[ sec : analysis ] , followed the experiment in sec .  [",
    "sec : exp ] . finally , sec .",
    "[ sec : summary ] concludes .",
    "the constructions and analyses delivered in this paper rely on concepts from the field of tensor analysis .",
    "below we provide the minimal background required in order to follow our arguments .",
    "the core concept in tensor analysis is a _ tensor _ , which for our purposes may simply be thought of as a multi - dimensional array .",
    "the _ order _ of a tensor is defined to be the number of indexing entries in the array , which are referred to as _ modes_. the _ dimension _ of a tensor in a particular mode is defined as the number of values that may be taken by the index in that mode . for example , a @xmath7-by-@xmath8 matrix is a tensor of order  @xmath9 , _ i.e. _  it has two modes , with dimension  @xmath7 in mode  @xmath10 and dimension  @xmath8 in mode  @xmath9 . if @xmath11 is a tensor of order @xmath12 and dimension @xmath13 in each mode @xmath14 , the space of all configurations it can take is denoted , quite naturally , by @xmath15 .",
    "a fundamental operator in tensor analysis is the _ tensor product _ ( also known as _ outer product _ ) , which we denote by @xmath16 .",
    "it is an operator that intakes two tensors @xmath17 and @xmath18 ( orders @xmath19 and @xmath20 respectively ) , and returns a tensor @xmath21 ( order @xmath22 ) defined by : @xmath23 . in   a generalization of the tensor product",
    "is defined , by replacing multiplication with a general operator  @xmath24 .",
    "specifically , for a function @xmath25 that is commutative ( @xmath26 for all @xmath27 ) , the _ generalized tensor product _ , denoted  @xmath28 , is defined to be the operator that for input tensors @xmath17 and @xmath18 ( orders @xmath19 and @xmath20 respectively ) , returns the tensor @xmath29 ( order @xmath22 ) given by : @xmath30 .",
    "an additional operator we will make use of is _",
    "mode permutation_. let  @xmath11 be a tensor of order  @xmath12 , and let  @xmath31 be a permutation over  @xmath12 ( bijective mapping from @xmath32 to itself ) .",
    "the mode permutation of  @xmath11 w.r.t .",
    "@xmath31 , which by a slight abuse of notation is denoted  @xmath33 , is the order-@xmath12 tensor defined by : @xmath34 . in words ,",
    "@xmath33  is the tensor obtained by rearranging the modes of  @xmath11 in accordance with  @xmath31 .    when studying tensors , it is oftentimes useful to arrange them as matrices , a procedure referred to as",
    "_ matricization_. let  @xmath11 be a tensor of order  @xmath12 and dimension  @xmath13 in each mode @xmath14 , and let @xmath35 be a set of mode indexes , whose complement @xmath36 we denote by  @xmath37 .",
    "we may write @xmath38 where @xmath39 , and similarly @xmath40 where @xmath41 .",
    "the matricization of  @xmath11 w.r.t .",
    "@xmath42 , denoted @xmath43 , is the @xmath44-by-@xmath45 matrix holding the entries of  @xmath11 such that @xmath46 is placed in row index @xmath47 and column index @xmath48 .",
    "if  @xmath49 or @xmath50 , then by definition @xmath43 is a row or column ( respectively ) vector of dimension @xmath51 holding @xmath46 in entry @xmath52 .    to conclude this section , we hereinafter establish notational conventions that will accompany us throughout the paper .",
    "we denote tensors with uppercase calligraphic letters , _ e.g. _",
    "@xmath11 , and in some cases with the greek letters @xmath53 , @xmath54 or  @xmath55 .",
    "subscripts are used to refer to individual tensor entries , _ e.g. _",
    "@xmath56 , whereas superscripts indicate the location of a tensor in some annotated collection , for example @xmath57  stands for the @xmath58th tensor in the collection @xmath59 .",
    "vectors are typically denoted with boldface lowercase letters , _ e.g. _",
    "@xmath60 , where again subscripts refer to an individual entry ( _ e.g. _  @xmath61 ) , and superscripts to the identity of a vector within some annotated collection ( _ e.g. _  @xmath62 is the @xmath63th vector in the set @xmath64 ) .",
    "we use non - boldface lowercase or uppercase letters ( _ e.g. _  @xmath65 or  @xmath66 respectively ) to denote scalars , and in this case both subscripts and superscripts distinguish between objects in an annotated set ( _ e.g. _  @xmath67 ) . finally ,",
    "for a positive integer @xmath68 , we use  @xmath69 $ ] as shorthand for the set  @xmath32 .",
    "convolutional networks  ( @xcite ) are the cornerstone of modern deep learning , and have played a critical role in its resurgence . since the work of  @xcite , nearly all state of the art systems for image and video processing , in both academia and industry , are heavily based on convolutional networks ( see for example @xcite ) . in their basic form ,",
    "convolutional networks consist of successive layers , each comprising convolutions with multiple filters followed by point - wise activation ( non - linearity ) , which in turn is followed by spatial pooling ( decimation ) .",
    "recently , an alternative form of convolutional networks has emerged    _ dilated convolutional networks_. these models are obtained by removing spatial pooling and introducing non - contiguity to convolutional filters .",
    "although they have been used for more conventional image processing tasks ( _ e.g. _  @xcite ) , arguably the most significant merit of dilated convolutional networks is that they thrive in application domains previously outside the realm of convolutional networks .",
    "the wavenet model recently developed by google ( @xcite ) is based on dilated convolutions applied to raw audio , and provides state of the art text - to - speech results , as well as promising phoneme recognition ( speech classification ) performance .",
    "the following bytenet model ( @xcite ) applies dilated convolutional networks to raw textual characters , delivering state of the art character - level language modeling , as well as excellent character - level machine translation results at a fraction of the run time required by competing methods .",
    "taken together , these two developments demonstrate the ability of dilated convolutional networks to provide state of the art performance in sequence processing tasks .",
    "the dilated convolutional network architecture considered as baseline in this paper is the one underlying wavenet model , depicted in fig .",
    "[ fig : base_dcn ] .",
    "the input to the network is a sequence of vectors @xmath70)_t\\subset{{\\mathbb r}}^{r_0}$ ] , where @xmath71 is a natural time index . a size-@xmath9 convolutional layer with dilation-@xmath10 , _ i.e. _  with contiguous filters , maps this input into the hidden sequence @xmath72)_t\\subset{{\\mathbb r}}^{r_1}$ ] .",
    "specifically , entry  @xmath73 $ ] of  @xmath74 $ ] is obtained by applying the filter formed by @xmath75 to time points  @xmath76 of the input : @xmath77_{\\gamma}=g({\\left\\langle{{{\\mathbf a}}^{1,\\gamma,{\\text{i}}}},{{{\\mathbf x}}[t\\text{-}1]}\\right\\rangle},{\\left\\langle{{{\\mathbf a}}^{1,\\gamma,{\\text{ii}}}},{{{\\mathbf x}}[t]}\\right\\rangle})$ ]",
    ". for reasons that will shortly become apparent , we use @xmath24 here to denote the binary function combining two size-@xmath10 convolutions into a single size-@xmath9 convolution with non - linearity . different choices of  @xmath24 lead to different convolutional operators , for example @xmath78 leads to standard convolution followed by rectified linear activation ( _ relu _ ,  @xcite ) , whereas @xmath79 gives rise to what is known as a _ convolutional arithmetic circuit _  ( ) .",
    "following the first hidden layer , @xmath80  size-@xmath9 convolutional layers with increasing dilations are applied .",
    "specifically , for @xmath81 , hidden layer  @xmath65 maps the sequence @xmath82)_t\\subset{{\\mathbb r}}^{r_{l-1}}$ ] into @xmath83)_t\\subset{{\\mathbb r}}^{r_l}$ ] using filters with dilation-@xmath84 , _ i.e. _  with an internal temporal gap of @xmath85 points : @xmath86_{\\gamma}=g({\\left\\langle{{{\\mathbf a}}^{l,\\gamma,{\\text{i}}}},{{{\\mathbf h}}^{(l\\text{-}1)}[t\\text{-}2^{l\\text{-}1}]}\\right\\rangle},{\\left\\langle{{{\\mathbf a}}^{l,\\gamma,{\\text{ii}}}},{{{\\mathbf h}}^{(l\\text{-}1)}[t]}\\right\\rangle})$ ] .",
    "the last convolutional layer maps @xmath87)_t$ ] into network output sequence @xmath88)_t\\subset{{\\mathbb r}}^{r_l}$ ] using filters with dilation-@xmath89 : @xmath90_y = g({\\left\\langle{{{\\mathbf a}}^{l , y,{\\text{i}}}},{{{\\mathbf h}}^{(l\\text{-}1)}[t\\text{-}2^{l\\text{-}1}]}\\right\\rangle},{\\left\\langle{{{\\mathbf a}}^{l , y,{\\text{ii}}}},{{{\\mathbf h}}^{(l\\text{-}1)}[t]}\\right\\rangle})$ ] .",
    "altogether , the architectural parameters of the network are the number of convolutional layers  @xmath66 , the convolutional operator  @xmath24 , the input dimension  @xmath91 , the number of channels  @xmath92 for each hidden layer @xmath93 $ ] , and the output dimension  @xmath94 .",
    "the learnable parameters are the convolution weights @xmath95 for channel @xmath96 $ ] of layer  @xmath97 $ ] .",
    "our interest lies on the representational abilities of the network , _",
    "i.e. _  on the properties of the input - output mappings that may be realized by it . as illustrated in fig .",
    "[ fig : base_dcn ] , for some fixed time point  @xmath71 , @xmath98 $ ]    network output at time  @xmath71 , is a function of @xmath99\\ldots{{\\mathbf x}}[t]$ ]    network input over the last @xmath100 time points .",
    "taking into account the temporal stationarity of the network , and denoting  @xmath101 for brevity , we may write @xmath90_y = f_y({{\\mathbf x}}[t\\text{-}n\\text{+}1],\\ldots,{{\\mathbf x}}[t])$ ] for every @xmath102 $ ] , where the functions  @xmath103 are independent of the time index  @xmath71 .",
    "the latter functions , which obviously depend on the convolution weights @xmath104 , completely characterize the input - output mapping realized by the network .",
    "we will study these functions through the process of _ discretization_. namely , @xmath105    a function of  @xmath12 vector - variables ,",
    "will be represented by a lookup table ( tensor ) formed by varying each vector - variable over a finite number of possible values .",
    "obviously , the size of such a lookup table is exponential in  @xmath12 , thus treating it directly is intractable .",
    "however , as we shall see , the network admits a compact parameterization of lookup tables in terms of the convolution weights @xmath104 .",
    "this parameterization ( eq .  [ eq : base_decomp ] below ) entails an algebraic structure , and will be used to study the representational properties of the baseline dilated convolutional network .    for the discretization of  @xmath105",
    ", we choose a collection of vectors @xmath106 , and define the following tensor  @xmath57 of order  @xmath12 and dimension  @xmath107 in each mode : a^y_d_1  d_n:=f_y(v^(d_1), ",
    ",v^(d_n ) ) [ eq : grid_tensor ] the vectors @xmath108 are referred to as _ discretizers_. they generate the tensor  @xmath57 by assigning , in all possible combinations , the  @xmath12 vector - variables of the function  @xmath105 .",
    "we refer to  @xmath57 as the _ grid tensor _ of  @xmath105 , reflecting the fact that it holds function values over a discrete grid .",
    "the parameterization of  @xmath103 discretizations mentioned above is in fact a hierarchical decomposition of the grid tensors  @xmath109 .",
    "accordingly , and for the sake of highlighting correspondence to the baseline dilated convolutional network ( fig .",
    "[ fig : base_dcn ] ) , we refer to this parameterization as the _ baseline decomposition_. for conciseness , we defer the derivation of the baseline decomposition to app .",
    "[ app : base_decomp ] , and hereby lay out its final form : & & + & & _ = [ v^(1)_,  ,v^(m)_]^ + & & + & & _ = ( _ = 1^r_l-1 a_^l,,^l-1,2j-1 , ) _ g(_=1^r_l-1 a_^l,,^l-1,2j , ) + & & a^y=^l,1,y [ eq : base_decomp ] @xmath110  and  @xmath111 here stand for coordinate  @xmath112 of the convolution weights  @xmath113 and  @xmath114 respectively , while @xmath115  stands for coordinate  @xmath116 of the discretizer  @xmath117 .",
    "notice that the tensor products here are generalized ( see sec .",
    "[ sec : prelim ] )    based on the network s convolutional operator  @xmath24 .",
    "therefore , strictly speaking , the baseline decomposition is a _",
    "generalized tensor decomposition _",
    ", as defined in  .",
    "to conclude this subsection , we relate the material above to prior works in the literature , and highlight our contributions in the text to come .",
    "the first work to formalize the correspondence between convolutional networks and hierarchical tensor decompositions was  , in which only convolutional arithmetic circuits ( convolutional networks with product pooling and linear activations ) were considered .",
    "later generalized the correspondence to account for other types of convolutional networks ( _ e.g. _  ones with relu activation and max or average pooling ) as well . the baseline decomposition above ( eq .  [ eq : base_decomp ] )",
    "  a hierarchical tensor decomposition characterizing the baseline dilated convolutional network ( fig .",
    "[ fig : base_dcn ] ) , is essentially a direct outcome of the formulation presented in  .",
    "our contributions begin in the next subsection , where we establish a correspondence between hierarchical decompositions over general mode trees , and dilated convolutional networks with different dilations .",
    "thereafter , in sec .",
    "[ sec : mtd ] , we present the idea of blending together multiple mode trees in a single mixed decomposition , and show that this corresponds to interconnections of different dilated convolutional networks . subsequently , in sec .  [",
    "sec : analysis ] , we use the latter relation to demonstrate the expressive efficiency brought forth by the interconnections .      the baseline decomposition ( eq .  [ eq : base_decomp ] ) , corresponding to the baseline dilated convolutional network ( fig .",
    "[ fig : base_dcn ] ) , implicitly adheres to a tree structure    for every  @xmath63 , there exists a group of tensors  @xmath118 , formed through combinations of tensors from its `` child '' groups  @xmath119 and  @xmath119 . in this subsection",
    "we generalize the underlying tree structure , and show that the resulting decompositions capture networks with various dilations throughout their convolutional layers .",
    "we begin by defining a general ( binary ) tree over tensor modes :    [ def : tree ] let  @xmath68 .",
    "a _ _ binary mode tree _",
    "_ ) that correspond to networks with size-@xmath9 convolutions . we limit ourselves to this special case merely for simplicity of presentation .",
    "our formulation can easily be extended to account for convolutions of arbitrary size by considering mode trees that are not necessarily binary , and by modifying the decomposition in eq .",
    "[ eq : tree_decomp ] to take ( generalized ) tensor products between an arbitrary number of tensors ( not necessarily two ) . ] over  @xmath69 $ ] is a full binary tree in which :    * every node is labeled by a subset of  @xmath69 $ ] * there are exactly  @xmath12 leaves , labeled  @xmath120 * the root node is labeled  @xmath69 $ ] * the label of an interior ( non - leaf ) node is the union of the labels of its children    if  @xmath121 is a binary mode tree , we identify its nodes with their labels , _ i.e. _  with the corresponding subsets of  @xmath69 $ ] .",
    "the set of all interior nodes is denoted by  @xmath122}$ ] , the children of an interior node @xmath123 $ ] are denoted by  @xmath124 $ ] , and the parent of a non - root node @xmath123 $ ] is denoted by  @xmath125 .",
    "binary mode trees induce hierarchical decompositions of grid tensors .",
    "recall the definition of grid tensors in sec .",
    "[ sec : dcn : base ] ( eq .",
    "[ eq : grid_tensor ] ) , and let  @xmath121 be a binary mode tree over  @xmath69 $ ] . for every node",
    "@xmath123 $ ] in  @xmath121 , we define a collection of @xmath126-order tensors  @xmath127}$ ] , where  @xmath128 is some predetermined constant . tensors .",
    "] in addition , we also define , for each interior node  @xmath129 , two collections of weight vectors  ",
    "@xmath130}\\subset{{\\mathbb r}}^r$ ] and  @xmath131}\\subset{{\\mathbb r}}^r$ ] .",
    "the hierarchical grid tensor decomposition induced by  @xmath121 traverses through the tree in a depth - first fashion , assigning the tensors of node  @xmath132  ( @xmath133 ) through combinations of the tensors of its children ( @xmath134 and  @xmath135 ) .",
    "this is laid out formally in eq .",
    "[ eq : tree_decomp ] below , which we refer to as the _ tree decomposition_. & & + & & _ = [ v^(1)_,  ,v^(m)_]^ + & & + & & _ = ^(;t)((_=1^r a_^,,^c_(;t ) , ) _ g(_=1^r a_^,,^c_(;t ) , ) ) + & & a^y=^[n],y [ eq : tree_decomp ] as in the baseline decomposition ( eq .  [ eq : base_decomp ] ) , @xmath115 here stands for coordinate  @xmath116 of the discretizer  @xmath117 .",
    "the permutation  @xmath136 , for an interior node  @xmath129 , arranges the modes of the tensor  @xmath137 such that these comply with a sorted ordering of  @xmath132 .",
    "specifically , if we denote by @xmath138 the elements of @xmath139 $ ] , and by @xmath140 the elements of @xmath141 $ ] , the permutation @xmath142\\to[2^{{\\left\\lvert\\nu \\right\\rvert}}]$ ] is the one that sorts the tuple @xmath143 in ascending order .",
    "the final outcome of the decomposition , _",
    "i.e. _  the generated grid tensors  @xmath109 , are the tensors  @xmath144,\\gamma}\\}_{\\gamma}$ ] corresponding to the root of  @xmath121 .    compare the general tree decomposition in eq .",
    "[ eq : tree_decomp ] to the baseline decomposition in eq .",
    "[ eq : base_decomp ] .",
    "it is not difficult to see that the latter is a special case of the former .",
    "namely , it corresponds to a binary mode tree  @xmath121 that is perfect ( all leaves have the same depth  @xmath145 ) , and whose depth-@xmath65 nodes ( @xmath146 ) are @xmath147 $ ] for @xmath148 $ ] . is a scalar and  @xmath149 is a set , @xmath150  stands for the set obtained by adding  @xmath151 to each element in  @xmath149 . ]",
    "this implies that such a mode tree , when plugged into the tree decomposition ( eq .  [ eq : tree_decomp ] ) , provides a characterization of the baseline dilated convolutional network ( fig .",
    "[ fig : base_dcn ] ) , _ i.e. _  a network whose dilation in layer  @xmath65 is  @xmath152 ( see illustration in fig .  [",
    "fig : dilations_trees](a ) ) . if we were to choose a different mode tree , the corresponding dilated convolutional network would change .",
    "for example , assume that @xmath145  is even , and consider a perfect binary mode tree  @xmath121 whose depth-@xmath65 nodes ( @xmath146 ) are as follows :",
    "* even  @xmath65 : depth-@xmath65 nodes are @xmath147 $ ] for @xmath148 $ ] * odd  @xmath65 : depth-@xmath65 nodes are generated by splitting nodes of depth  @xmath153 , such that the first and third quadrants of a split node belong to one child , while the second and fourth belong to the other    in this case , the network characterized by the tree decomposition ( eq .  [ eq : tree_decomp ] ) is obtained by swapping dilations of even and odd layers in the baseline architecture , _ i.e. _  it has dilation in layer  @xmath65 of  @xmath154 if @xmath65  is even , and  @xmath155 if @xmath65  is odd ( see illustration in fig .  [",
    "fig : dilations_trees](b ) ) .        to conclude this subsection",
    ", we defined the notion of a tree over tensor modes ( def .",
    "[ def : tree ] ) , and laid out a corresponding hierarchical decomposition of grid tensors ( tree decomposition  ",
    ".  [ eq : tree_decomp ] ) .",
    "different choices of mode trees lead to decompositions characterizing networks with different dilations throughout their layers .",
    "the baseline decomposition ( eq .  [ eq : base_decomp ] ) , characterizing the baseline dilated convolutional network ( dilation  @xmath152 in layer  @xmath65    see fig .",
    "[ fig : base_dcn ] ) , is now merely a special case that corresponds to a particular choice of mode tree . in the next section ,",
    "we build on the constructions made here , and define mixed tensor decompositions blending together multiple mode trees .",
    "these decompositions will be shown to correspond to multiple dilated convolutional networks interconnected to one another .",
    "let  @xmath121 and  @xmath156 be two binary mode trees over  @xmath69 $ ] ( def .",
    "[ def : tree ] ) . consider the tree decomposition of grid tensors induced by  @xmath121 ( eq .  [ eq : tree_decomp ] ) .",
    "this decomposition iteratively assigns a group of tensors  @xmath157 for each node  @xmath132 in  @xmath121 , based on weight vectors  @xmath158 defined for each interior node  @xmath129 .",
    "the tree decomposition induced by  @xmath156 operates similarly , but for distinction we use  @xmath159 to denote the tensor group of node  @xmath160 , and  @xmath161 to denote the weights of interior node  @xmath162 .",
    "we will define a _",
    "mixed tensor decomposition _ , blending together the tree decompositions of  @xmath121 and  @xmath156 .",
    "the latter is obtained by choosing a collection of _ mixture nodes _  ",
    "these are nodes ( subsets of  @xmath69 $ ] ) that reside in the interior of both  @xmath121 and  @xmath156 , defining locations in the tree decompositions at which tensors will be exchanged . if  @xmath164 is chosen as the empty set , the mixed decomposition simply sums the output tensors generated by the tree decompositions of  @xmath121 and  @xmath156 ( @xmath144,y}\\}_y$ ]  and  @xmath165,y}\\}_y$ ] respectively ) . otherwise , the tree decompositions of  @xmath121 and  @xmath156 progress in parallel , until reaching a mixture node  @xmath166 , where they exchange half the tensors corresponding to that node ( half of  @xmath167 is exchanged for half of  @xmath168 ) .",
    "the process continues until all mixture nodes are visited and the root node ( of both trees )  @xmath69 $ ] is reached . at this point tensors ( @xmath144,y}\\}_y$ ]  and  @xmath165,y}\\}_y$ ] ) are summed and returned as output .",
    "the formal definition of the mixed decomposition is as follows : & & 1 :   + & & 2 :  ^\\{j } , = |^\\{j } , = [ v^(1)_,  ,v^(m)_]^ + & & 3 :   + & & 4 :   + & & 5 :  ^ , = ^(;t)((_=1^r a_^,,^c_(;t ) , ) _ g(_=1^r a_^,,^c_(;t ) , ) )   + & & 6 :   + & & 7 :  |^| , = ^(|;|t)((_=1^r |a_^|,,|^c_(|;|t ) , ) _",
    "g(_=1^r    & & 8 :   + & & 9 :  a^y = ^[n],y+|^[n],y [ eq : mix_decomp ] as in the basic tree decomposition ( eq .  [ eq : tree_decomp ] ) , the first step here ( lines  @xmath10-@xmath9 ) is to assign tensors corresponding to the leaf nodes ( @xmath120 ) via discretizers  @xmath108 .",
    "the outer loop in line  @xmath8 traverses  @xmath169 through mixture nodes and the root node in inclusion order , _",
    "i.e. _  such that a node ( subset of  @xmath69 $ ] ) is always reached after all nodes strictly contained in it .",
    "lines  @xmath7-@xmath170  ( respectively @xmath171-@xmath172 ) are the same as in the tree decomposition ( eq .  [ eq : tree_decomp ] ) , except that instead of running through the entire interior of  @xmath121 ( respectively  @xmath156 ) , they cover a segment of it .",
    "this segment continues where the previous left off , and comprises only nodes ( subsets of  @xmath69 $ ] ) contained in  @xmath169 ( including  @xmath169 itself ) .",
    "line  @xmath173 is where the mixing takes place    here",
    "half of the tensors corresponding to node  @xmath169 in the decomposition of  @xmath121  ( @xmath167 ) , are exchanged for half the tensors corresponding to  @xmath169 in the decomposition of  @xmath156  ( @xmath168 ) .",
    "finally , after  @xmath169 has reach the root  @xmath69 $ ] and the decompositions of  @xmath121 and  @xmath156 have concluded , line  @xmath174 sums the output tensors of these decompositions ( @xmath144,y}\\}_y$ ]  and  @xmath165,y}\\}_y$ ] respectively ) , to produce the grid tensors  @xmath109 .    in terms of computation and memory , the requirements posed by the mixed decomposition",
    "( eq .  [ eq : mix_decomp ] ) are virtually identical to those of running two separate tree decompositions ( eq .  [ eq : tree_decomp ] ) with  @xmath121 and  @xmath156 . specifically ,",
    "if the tree decompositions of  @xmath121 and  @xmath156 correspond to input - output mappings computed by the dilated convolutional networks  @xmath175 and  @xmath176 ( respectively ) , the mixed decomposition would correspond to the computation of a _",
    "mixed dilated convolutional network _",
    ", formed by summing the outputs of  @xmath175 and  @xmath176 , and interconnecting their intermediate layers .",
    "the choice of mixture nodes  @xmath164 in the mixed decomposition determines the locations at which networks  @xmath175 and  @xmath176 are interconnected , where an interconnection simply wires into  @xmath175 half the outputs of a convolutional layer in  @xmath176 , and vice versa .",
    "for example , suppose that  @xmath175 is the baseline dilated convolutional network ( dilation  @xmath152 in layer  @xmath65    see sec .",
    "[ sec : dcn : base ] ) , whereas  @xmath176 is the network obtained by swapping dilations of even and odd layers ( such that layer  @xmath65 has dilation  @xmath154 if @xmath65  is even , and  @xmath155 if @xmath65  is odd ) . the mode trees corresponding to these networks , illustrated in fig .  [ fig : dilations_trees ] ( for the case  @xmath177 ) , share interior nodes @xmath147 $ ] for @xmath178 , @xmath148 $ ] .",
    "we may therefore choose  @xmath164 to be all such nodes ( excluding root ) , and get a mixed decomposition that corresponds to a mixed network interconnecting all even layers of  @xmath175 and  @xmath176 .",
    "illustrations of such decomposition and network ( again , for the case  @xmath179 ) are given in fig .",
    "[ fig : mix_trees_dcn ] .",
    "the main advantage of the mixed decomposition ( eq .  [ eq : mix_decomp ] ) , and the reason for its definition , is that it leads to expressive efficiency .",
    "that is to say , the mixed dilated convolutional network , formed by interconnecting intermediate layers of networks with different dilations , can realize functions that without the interconnections would be expensive , or even impractical to implement .",
    "we theoretically support this in the next section , providing a complete proof for a special case of convolutional arithmetic circuits ( @xmath79 ) .",
    "as in sec .  [",
    "sec : mtd ] , let  @xmath175 and  @xmath180 be two dilated convolutional networks whose input - output mappings are characterized by the tree decomposition ( eq .  [ eq : tree_decomp ] ) with mode trees  @xmath121 and  @xmath156 respectively . consider the mixed decomposition ( eq .  [ eq : mix_decomp ] ) resulting from a particular choice of mixture nodes @xmath164 ( subset of the nodes interior to both  @xmath121 and  @xmath156 ) , and denote its corresponding mixed dilated convolutional network by  @xmath181 .",
    "we would like to show that  @xmath181 is expressively efficient w.r.t .",
    "@xmath175 and  @xmath180 , meaning : _",
    "( i ) _  any function realized by  @xmath175 or  @xmath180 can also be realized by  @xmath181 with no more than linear growth in network size ( number of channels in the convolutional layers ) ; _ ( ii ) _  there exist functions realizable by  @xmath181 that can not be realized by  @xmath175 or  @xmath180 ( or a summation thereof ) unless their size ( number of convolutional channels ) is allowed to grow super - linearly .",
    "we study the representational abilities of networks through their corresponding tensor decompositions , which as discussed in sec .",
    "[ sec : dcn ] , parameterize discretizations of input - output mappings ( grid tensors ) . before laying out the problem through the lens of tensor decompositions , a few remarks are in order :    *",
    "the number of channels in each layer of  @xmath175 or  @xmath180 corresponds to the constant  @xmath182 in the respective tree decomposition ( eq .  [ eq : tree_decomp ] with underlying mode tree  @xmath121 or  @xmath156 respectively ) .",
    "similarly , the number of channels in each layer of each interconnected network in  @xmath181 corresponds to  @xmath182 in the respective mixed decomposition ( eq .  [ eq : mix_decomp ] ) . in both the tree and",
    "mixed decompositions ,  @xmath182 , referred to hereafter as the _ size constant _ , stands for the number of tensors  @xmath157 ( respectively  @xmath159 ) held in each node  @xmath132 ( respectively  @xmath183 ) .",
    "we set this number uniformly across nodes , corresponding to uniformly sized layers across networks , merely for simplicity of presentation .",
    "our formulations and analysis can easily be adapted to account for varying layer sizes , by allowing different nodes in a decomposition to hold a different number of tensors . *",
    "an additional simplification we made relates to weight sharing . in both the tree and mixed decompositions",
    ", each interior node  @xmath132 ( respectively  @xmath183 ) has a separate set of weight vectors @xmath158 ( respectively  @xmath161 ) .",
    "this implies that in the corresponding networks , convolution filters may vary through time , _",
    "i.e. _  different weights may be used against different portions of a convolved sequence .",
    "the more commonplace setting of stationary filters ( standard convolutions ) is obtained by restricting different nodes in a decomposition to possess the same weights .",
    "we do not introduce such restrictions into our formulations , as they make little difference in terms of the analysis , but on the other hand significantly burden presentation .",
    "we are now in a position to formulate our expressive efficiency problem in terms of tensor decompositions .",
    "our objective is to address the following two propositions ( stated informally ) :    [ prop : tree_by_mix ] consider a tree decomposition ( eq .  [ eq : tree_decomp ] ) with underlying mode tree  @xmath121 or  @xmath156 and size constant  @xmath182 .",
    "this decomposition can be realized by a mixed decomposition of  @xmath121 and  @xmath156 ( eq .  [ eq : mix_decomp ] ) whose size constant is linear in  @xmath182 .",
    "[ prop : mix_by_tree ] consider a mixed decomposition of  @xmath121 and  @xmath156 ( eq .  [ eq : mix_decomp ] ) with size constant  @xmath182 .",
    "this decomposition can generate grid tensors  @xmath109 that can not be generated by tree decompositions of  @xmath121 or  @xmath156 ( eq .  [ eq : tree_decomp ] ) , or a summation of such , unless their size constant is super - linear in  @xmath182 .    before heading to a formal treatment of prop .",
    "[ prop : tree_by_mix ] and  [ prop : mix_by_tree ] above , we briefly convey the intuition behind our analysis .",
    "recall from sec .",
    "[ sec : mtd ] that the mixed decomposition ( eq .  [ eq : mix_decomp ] ) blends together tree decompositions ( eq .  [ eq : tree_decomp ] ) of different mode trees  @xmath121 and  @xmath156 , by traversing upwards through the trees , while exchanging tensors at each of a preselected set of mixture nodes .",
    "we may think of each mixture node as a decision point that can propagate upwards one of two computations    that carried out by  @xmath121 , or that carried out by  @xmath156 , where in both cases , the chosen computation is propagated upwards through both  @xmath121 and  @xmath156 .",
    "each combination of decisions across all mixture nodes gives rise to a computational path traversing between  @xmath121 and  @xmath156 , equivalent to a tree decomposition based on a _ hybrid mode tree _",
    "( see illustration in fig .",
    "[ fig : hybrid_trees ] ) .",
    "the number of possible hybrid trees is exponential in the number of mixture nodes , and thus a mixed decomposition is comparable to an exponential ensemble of tree decompositions .",
    "the original tree decompositions , based on  @xmath121 and  @xmath156 , are included in the ensemble , thus may easily be replicated by the mixed decomposition . on the other hand , many of the hybrid trees in the mixed decomposition are significantly different from  @xmath121 and  @xmath156 , requiring large size constants from tree decompositions of the latters .",
    "as a first step in formalizing the above intuition , we define the notion of a hybrid mode tree :    [ def : hybrid_tree ] let  @xmath121 and  @xmath156 be binary mode trees over  @xmath69 $ ] ( def .",
    "[ def : tree ] ) , and let  @xmath164 be a corresponding collection of mixture nodes , _",
    "i.e. _  a set of nodes ( subsets of  @xmath69 $ ] ) contained in the interior of both  @xmath121 and  @xmath156 .",
    "we say that  @xmath184 is a _ hybrid mode tree _ of  @xmath121 and  @xmath156 w.r.t .",
    "@xmath164 if it is a binary mode tree over  @xmath69 $ ] , whose interior may be generated by the following process : & & int(h)= + & & + & & s = int(t)2^\\ { } + & & |s = int(|t)2^\\ { } + & & int(h)=int(h)s  int(h)=int(h)|sin words , for every  @xmath169 that is either a mixture node or the root node , @xmath185  includes a _ segment _ from either  @xmath186 or  @xmath187 , where the segment comprises all descendants of  @xmath169 that are not descendants of any other mixture node ( see illustration in fig .",
    "[ fig : hybrid_trees ] ) .",
    "claim  [ claim : hybrid_tree_by_mix ] below states that with proper weight setting , a mixed decomposition of  @xmath121 and  @xmath156 ( eq .  [ eq : mix_decomp ] ) with size constant  @xmath182 can realize any tree decomposition ( eq .  [ eq : tree_decomp ] ) with size constant  @xmath188 whose underlying mode tree is a hybrid of  @xmath121 and  @xmath156 . since  @xmath121 and  @xmath156 are in particular hybrid mode trees of themselves , we obtain an affirmative answer to prop .  [ prop : tree_by_mix ] .    [ claim : hybrid_tree_by_mix ] let  @xmath121 and  @xmath156 be binary mode trees over  @xmath69 $ ] ( def .",
    "[ def : tree ] ) , and let  @xmath164 be a corresponding collection of mixture nodes ( a set of nodes contained in the interior of both  @xmath121 and  @xmath156 ) .",
    "consider a mixed decomposition of  @xmath121 and  @xmath156 w.r.t .",
    "@xmath164 ( eq .  [ eq : mix_decomp ] ) , and denote its size constant by  @xmath189 .",
    "let  @xmath184 be a hybrid mode tree of  @xmath121 and  @xmath156 w.r.t .",
    "@xmath164 ( def .",
    "[ def : hybrid_tree ] ) , and consider the respective tree decomposition ( eq .  [ eq : tree_decomp ] ) , with a size constant of  @xmath190 . for any setting of weights  @xmath191 leading to grid tensors  @xmath109 in this tree decomposition , there exists a setting of weights  @xmath191 and  @xmath192 in the mixed decomposition , independent of the discretizers  @xmath108 ( see sec .  [",
    "sec : dcn ] ) , that leads to the same grid tensors .",
    "see app .",
    "[ app : proofs : hybrid_tree_by_mix ] .",
    "claim  [ claim : hybrid_tree_by_mix ] not only addresses prop .",
    "[ prop : tree_by_mix ] , but also paves the way to a treatment of prop .  [ prop : mix_by_tree ] .",
    "in other words , not only does it imply that the mixed decomposition of  @xmath121 and  @xmath156 can realize their individual tree decompositions with a linear growth in size , but it also brings forth a strategy for proving that the converse does not hold , _",
    "i.e. _  that the tree decompositions of  @xmath121 and  @xmath156 can not realize their mixed decomposition without a super - linear growth in size .",
    "the aforementioned strategy is to find a hybrid mode tree  @xmath184 distinct enough from  @xmath121 and  @xmath156 , such that its tree decomposition , realized by the mixed decomposition according to claim  [ claim : hybrid_tree_by_mix ] , poses a significant challenge for the tree decompositions of  @xmath121 and  @xmath156 .",
    "hereinafter we pursue this line of reasoning , focusing on the particular case where the convolutional operator  @xmath24 is a simple product    @xmath79 . in this case",
    "the tree and mixed decompositions ( eq .  [ eq : tree_decomp ] and  [ eq : mix_decomp ] respectively ) are standard ( non - generalized ) tensor decompositions  ( @xmath193    see sec .  [ sec : prelim ] ) , and the corresponding dilated convolutional networks are convolutional arithmetic circuits .",
    "we focus on this special case since it allows the use of a plurality of algebraic tools for theoretical analysis , while at the same time corresponding to models showing promising results in practice ( see for example  ) .",
    "full treatment of additional cases , such as  @xmath194 , corresponding to networks with relu activation , is left for future work .    for establishing the difficulty experienced by the tree decompositions of  @xmath121 and  @xmath156 in replicating that of a hybrid tree  @xmath184",
    ", we analyze ranks of matricized grid tensors .",
    "specifically , we consider the tree decomposition ( eq .  [ eq : tree_decomp ] ) of a general mode tree , and derive upper and lower bounds on the ranks of generated grid tensors when these are subject to matricization w.r.t .  a general index set  @xmath195 $ ] ( see sec .",
    "[ sec : prelim ] ) .",
    "the bounds we derive ( theorem  [ theorem : tree_decomp_ranks ] below ) highly depend on both the underlying mode tree and the index set , and this allows finding index sets for which ranks tend to be higher with the hybrid mode tree  @xmath184 than they are with the original mode trees  @xmath121 and  @xmath156 .",
    "the only way for the latters to match ranks generated by the former is through a significant increase in the size constant  @xmath182 of their tree decompositions    precisely the sought after result .        to succinctly phrase our central theorem ,",
    "we define the notion of an index set tiled by a mode tree :    [ def : tiling ] let  @xmath121 be a binary mode tree over  @xmath69 $ ] ( def .",
    "[ def : tree ] ) , and let  @xmath195 $ ] be a non - empty set of indexes .",
    "a _ tiling _ of  @xmath42 by  @xmath121 is a collection of nodes in the tree , denoted  @xmath196 , which meets the following requirements :    * @xmath197 * @xmath198    in words , @xmath196  is a set of nodes in  @xmath121 whose disjoint union gives  @xmath42 , where each node is maximal , _ i.e. _  its parent in the tree is not a subset of  @xmath42 ( see illustration in fig .  [",
    "fig : tiling ] ) .",
    "it is not difficult to see that for any mode tree  @xmath121 and non - empty index set  @xmath42 , the tiling  @xmath196 always exists and is determined uniquely .",
    "as the theorem below states , this tiling , along with that of @xmath42 s complement ( @xmath199\\setminus{{\\mathcal i}}$ ] ) , characterizes the ranks of grid tensors generated by the tree decomposition of  @xmath121 when these are matricized w.r.t .",
    "@xmath42 .",
    "[ theorem : tree_decomp_ranks ] let  @xmath121 be a binary mode tree over  @xmath69 $ ] ( def .",
    "[ def : tree ] ) , and consider the corresponding tree decomposition ( eq .  [ eq : tree_decomp ] ) with discretizers @xmath108 spanning  @xmath200 .",
    "assume that  @xmath24 is the product operator ( @xmath79 ) , and suppose the generated grid tensors  @xmath109 are matricized ( see sec .  [",
    "sec : prelim ] ) w.r.t .",
    "an index set  @xmath195 $ ] ,  @xmath201 $ ] , whose complement we denote by  @xmath199\\setminus{{\\mathcal i}}$ ] .",
    "then , the ranks of the grid tensor matricizations  @xmath202 are :    * no greater than  @xmath203 * at least @xmath204 almost always , _",
    "i.e. _  for all configurations of weights  @xmath191 but a set of lebesgue measure zero    see app .",
    "[ app : proofs : tree_decomp_ranks ] .",
    "as stated previously , given two binary mode trees over  @xmath69 $ ] ( def .",
    "[ def : tree ] )  ",
    "@xmath121 and  @xmath156 , with a corresponding collection of mixture nodes  @xmath164 ( set of nodes interior to both  @xmath121 and  @xmath156 ) , the bounds in theorem  [ theorem : tree_decomp_ranks ] can be used to find an index set  @xmath195 $ ] and a hybrid mode tree  @xmath184 ( def .",
    "[ def : hybrid_tree ] ) , such that the tree decomposition ( eq .  [ eq : tree_decomp ] ) of  @xmath184 generates grid tensors whose ranks under matricization w.r.t .",
    "@xmath42 are much higher than those brought forth by the tree decompositions of  @xmath121 and  @xmath156 .",
    "consider our exemplar mode trees illustrated in fig .",
    "[ fig : dilations_trees ] .",
    "specifically , let  @xmath121 be the mode tree corresponding to the baseline dilated convolutional network ( dilation  @xmath152 in layer  @xmath97=[\\log_{2}n]$ ]    see sec .",
    "[ sec : dcn : base ] ) , and let  @xmath156 be the mode tree corresponding to the network obtained by swapping dilations of even and odd layers ( such that layer  @xmath65 has dilation  @xmath154 if @xmath65  is even , and  @xmath155 if @xmath65  is odd ) . as described in sec .  [",
    "sec : dcn : tree ] , @xmath121  is a perfect binary tree whose depth-@xmath65 nodes , @xmath146 , are @xmath147 $ ] for @xmath148 $ ] .",
    "@xmath156  is also perfect and has the same even - depth nodes , but its odd - depth nodes differ    they are generated by splitting parents into children holding non - contiguous quadrants .",
    "suppose we choose  @xmath164 to include the set of nodes in  @xmath121 and  @xmath156 whose depth is  @xmath205 , and consider the hybrid mode tree  @xmath184 formed by taking the segments ( see def .",
    "[ def : hybrid_tree ] ) of the first half of these nodes from  @xmath121 , and the rest of the tree from  @xmath156 .",
    "an illustration of  @xmath121 ,  @xmath156 and  @xmath184 in this setting , for the case  @xmath179 , is given in fig .",
    "[ fig : trees_tilings ] .",
    "now , let the index set  @xmath42 consist of every second index in  @xmath206 $ ] , and every second pair of indexes in  @xmath207 $ ] , _ i.e. _  @xmath208\\}\\cup\\{4k - k':k\\in[n/8],k'=2,3\\}$ ] . as illustrated in fig .",
    "[ fig : trees_tilings ] , the mode tree  @xmath121 tiles ( see def .",
    "[ def : tiling ] ) the lower half of  @xmath42 into singletons , and its upper half into pairs . the same applies to @xmath121 s  tiling of @xmath42 s  complement @xmath199\\setminus{{\\mathcal i}}$ ] .",
    "moreover , for every node in the former tiling  @xmath196 , there exists a sibling in the latter  @xmath209 ( and vice versa ) . by theorem  [ theorem : tree_decomp_ranks ] , this implies that the tree decomposition of  @xmath121 generates grid tensors whose matricizations w.r.t .",
    "@xmath42 have rank  @xmath210 .",
    "a similar situation occurs with the mode tree  @xmath156 , under which  @xmath42 and  @xmath37 are tiled into pairs in their lower halves and singletons in their top halves ( see illustration in fig .",
    "[ fig : trees_tilings ] ) .",
    "this also leads to matricized grid tensors of rank  @xmath210 . on the other hand , the hybrid mode tree  @xmath184 tiles  @xmath42 and  @xmath37 entirely into singletons ( see illustration in fig .  [",
    "fig : trees_tilings ] ) , leading ( by theorem  [ theorem : tree_decomp_ranks ] ) to grid tensor matricization ranks of  @xmath211 .",
    "this means that if we were to replicate grid tensors generated by the tree decomposition of  @xmath184 using those of  @xmath121 or  @xmath156 ( or a summation thereof ) , we would need to increase the size constant  @xmath182 super - linearly    by a power of  @xmath212 .",
    "the above example can be generalized , by considering swapping the dilations of more than two layers at once .",
    "in particular , if @xmath121  is the mode tree corresponding to the baseline dilated convolutional network ( dilation  @xmath152 in layer  @xmath65 ) , @xmath156  is the mode tree corresponding to the network obtained by swapping dilations of groups of  @xmath213 layers ( dilation  @xmath214 in layer  @xmath65 ) , and the set of mixture nodes includes all nodes of depth  @xmath215 , a hybrid mode tree  @xmath184 and an index set  @xmath42 can be found , such that the tree decomposition of  @xmath184 generates grid tensors whose ranks when matricized w.r.t .",
    "@xmath42 can only be matched by the tree decompositions of  @xmath121 and  @xmath156 if the latters size constant  @xmath182 is increased by a power of  @xmath216 . since the mixed decomposition of  @xmath121 and  @xmath156 ( eq .  [ eq : mix_decomp ]",
    ") can realize the tree decomposition of  @xmath184 with double the size constant ( claim  [ claim : hybrid_tree_by_mix ] ) , we conclude that it can , with size constant  @xmath217 , generate grid tensors whose matricization ranks require the tree decompositions of  @xmath121 and  @xmath156 to have size constant  @xmath218    super - linearly larger . therefore , in this particular setting , prop .  [",
    "prop : mix_by_tree ] holds and the mixed decomposition of  @xmath121 and  @xmath156 is indeed expressively efficient w.r.t .",
    "their tree decompositions . taking into account the fact that the mixed decomposition admits maximal matricization ranks almost always when  @xmath24 is the product operator ( see app .",
    "[ app : max_ranks ] ) , we formalize the result in network terms :    [ corollary : mix_by_tree ] let  @xmath175 be the baseline dilated convolutional network ( dilation  @xmath152 in layer  @xmath65    see sec .",
    "[ sec : dcn : base ] ) , and let  @xmath180 be the network obtained by swapping the dilations of groups of  @xmath213 layers ( dilation  @xmath214 in layer  @xmath65 )",
    ". denote by  @xmath181 the mixed dilated convolutional network obtained by summing the outputs of  @xmath175 and  @xmath180 , while interconnecting their  @xmath213th intermediate layer ( and possibly additional layers ) .",
    "assume the networks convolutional operator  @xmath24 is a product .",
    "then , besides a negligible set , all functions realized by  @xmath181 with  @xmath182 channels in the layers of each interconnected network , can not be realized by  @xmath175 or  @xmath180 ( or a summation thereof ) if the number of channels in each layer is less than  @xmath219 .",
    "corollary  [ corollary : mix_by_tree ] ( along with claim  [ claim : hybrid_tree_by_mix ] ) demonstrates that interconnecting intermediate layers of different dilated convolutional networks can bring forth expressive efficiency .",
    "that is to say , through cross - connections between networks , we are able to represent functions that would otherwise be expensive , or even impractical to implement .",
    "the lower bound in corollary  [ corollary : mix_by_tree ]  ",
    "@xmath219 , is essentially quadratic for any  @xmath220 .",
    "for example , if  @xmath221 and the number of channels  @xmath182 in each interconnected network is  @xmath222 , the lower bound would imply that in order to maintain representational abilities with an individual network ( or a summation of the networks ) , over  @xmath223 channels in each layer are required    far beyond acceptable practice in deep learning . in the next section",
    "we demonstrate empirically that this expressive advantage indeed translates to superior accuracies , _",
    "i.e. _  that interconnecting intermediate layers indeed boosts the performance of dilated convolutional networks .",
    "to assess the practical implications of the expressive efficiency brought forth by mixing dilated convolutional networks , a simple experiment was conducted .",
    "we trained a baseline dilated convolutional network  @xmath175 ( dilation  @xmath152 in layer  @xmath97 $ ]    see sec .  [",
    "sec : dcn : base ] ) , with architectural parameters similar to those used in wavenet ( @xcite ) , to classify individual phonemes in the timit acoustic speech corpus ( @xcite ) .",
    "in addition to this baseline model , we also trained the companion network  @xmath180 obtained by swapping dilations of even and odd layers ( such that layer  @xmath65 has dilation  @xmath154 if @xmath65  is even , and  @xmath155 if @xmath65  is odd ) .",
    "as discussed in sec .",
    "[ sec : mtd ] , the mode trees corresponding to these networks ( illustrated in fig .  [ fig : dilations_trees ] )  ",
    "@xmath121 and  @xmath156 , share interior nodes of even depth , thus any subset of those nodes may serve as mixture nodes for a mixed decomposition ( eq .  [ eq : mix_decomp ] ) .",
    "we evaluate mixed dilated convolutional networks  @xmath181 corresponding to different choices of mixture nodes ( see fig .",
    "[ fig : mix_trees_dcn ] for illustration of a particular case ) . specifically , we consider choices of the following form : @xmath224 varying the threshold yields mixed networks with a varying number of interconnections . in",
    "the extreme case  @xmath225 ( high threshold ) , @xmath181  simply sums the outputs of  @xmath175 and  @xmath180 .",
    "as the threshold decreases interconnections between hidden layers are added    starting from hidden layer  @xmath9 , then including hidden layer  @xmath7 , and so on .",
    "the intuition from our analysis ( sec .",
    "[ sec : analysis ] ) is that additional interconnections result in a larger number of hybrid mode trees , which in turn boosts the expressive power of the mixed dilated convolutional network . as fig .",
    "[ fig : exp ] shows , this intuition indeed complies with the results in practice    classification accuracy improves as we add interconnections between the networks , without any additional cost in terms of computation or model capacity .",
    "timit dataset is an acoustic - phonetic corpus comprising  @xmath226 sentences manually labeled at the phoneme level .",
    "we split the data into train and validation sets in accordance with  @xcite , and as advised by  @xcite , mapped the  @xmath227 possible phoneme labels into  @xmath228 and an additional `` garbage '' label .",
    "the task was then to classify individual phonemes into one of the latter categories .",
    "following wavenet , we used a baseline dilated convolutional network with relu activation ( @xmath194    see sec .",
    "[ sec : dcn : base ] ) , @xmath229  channels per layer , and input vectors of dimension  @xmath230 holding one - hot quantizations of the audio signal .",
    "the number of layers  @xmath66 was set to  @xmath231 , corresponding to an input window of  @xmath232 samples , spanning  @xmath233ms of audio signal    standard practice with timit dataset .",
    "the framework chosen for running the experiment was caffe toolbox ( @xcite ) , and we used adam optimizer ( @xcite ) for training ( with default hyper - parameters  ",
    "@xmath234 , @xmath235 , learning rate  @xmath236 ) .",
    "models were trained for  @xmath237 iterations with batch size  @xmath222 , and the learning rate was decreased by a factor of  @xmath238 after  @xmath239 of the iterations took place .",
    "weight decay was set to the standard value of  @xmath240 .",
    "besides the mixed dilated convolutional network  @xmath181 , we also evaluated the individual networks  @xmath175 and  @xmath180    both reached accuracies comparable to  @xmath181 in the case of  @xmath241 interconnections ( output summation only ) .",
    "in this paper we presented a study of the representational capacity of dilated convolutional networks , showing that interconnecting networks with different dilations can lead to expressive efficiency .",
    "in particular , we showed that even a single connection between intermediate layers can already lead to an almost quadratic expressive efficiency ( theorem  [ theorem : tree_decomp_ranks ] and corollary  [ corollary : mix_by_tree ] ) , which in large - scale settings typically makes the difference between a model that is practical and one that is not .",
    "we began with the dilated convolutional network underlying wavenet model ( fig .",
    "[ fig : base_dcn ] ) , referring to it as the `` baseline architecture '' , and couching it in a tensor algebraic setting ( eq .",
    "[ eq : base_decomp ] ) .",
    "the key for introducing tensors into the framework is a discretization of the network s input - output mapping    the  @xmath12 input vectors , that propagate through the network to form the output , are sampled from a pool of  @xmath107 `` templates '' , thereby creating a tensor with  @xmath242 entries , referred to as a `` grid tensor '' .",
    "the wavenet model is shown ( app .",
    "[ app : base_decomp ] ) to give rise to a hierarchical decomposition of grid tensors  ",
    ".  [ eq : base_decomp ] .    given that the tensor decomposition associated with the baseline architecture adheres to a specific tree structure , the generalization of the framework to an arbitrary tree follows quite naturally . if  @xmath121 represents a general binary mode tree ( as defined in def .",
    "[ def : tree ] ) , then eq .  [ eq : tree_decomp ] provides a tensor decomposition that captures various dilated convolutional networks , _",
    "i.e. _  networks with various dilation schemes . fig .",
    "[ fig : dilations_trees](b ) illustrates the type of dilation schemes we chose to focus on , obtained by swapping dilations in the scheme of the baseline architecture ( illustrated in fig .",
    "[ fig : dilations_trees](a ) ) .",
    "armed with a framework for describing dilated convolutional networks through mode trees and tensor decompositions , we next presented how two networks can be `` mixed '' .",
    "this is achieved by choosing a set of `` mixture nodes '' in the trees of both networks , and defining a `` mixed tensor decomposition '' ( eq .",
    "[ eq : mix_decomp ] ) that : _ ( i ) _  at each mixture node , exchanges tensors between the decompositions of the two networks ; _ ( ii ) _  at the root node , sums up the tensors from both decompositions . from a computational viewpoint , the mixing process amounts to `` rewiring '' intermediate layers between the two networks , and summing their outputs .",
    "accordingly , the requirements posed by the mixed network are virtually identical to those of running the two individual networks separately .",
    "the heart of our analysis is a theoretical study of the expressive efficiency brought forth by generating a mixed network  @xmath181 from two dilated convolutional networks  @xmath175 and  @xmath180 .",
    "establishing expressive efficiency requires proving two propositions : _",
    "( i ) _  any function realized by  @xmath175 or  @xmath180 can also be realized by  @xmath181 with no more than linear growth in network size ; _ ( ii ) _  there exist functions realizable by  @xmath181 that can not be realized by  @xmath175 or  @xmath180 ( or a summation thereof ) unless their size is allowed to grow super - linearly .",
    "we treat the first proposition in claim  [ claim : hybrid_tree_by_mix ] , and the second in theorem  [ theorem : tree_decomp_ranks ] .",
    "the latter is where the centrality of tensor algebra comes into play , as it is based entirely on ranks of tensor matricizations .",
    "the results of our work shed light on one of the most prominent architectural features of modern deep learning    connectivity .",
    "empirical evidence shows that running layers in parallel with various interconnection schemes yields improved performance .",
    "what our study shows , at least in the domain of dilated convolutional networks , is that these ideas are backed by theoretical principles , and in fact , provide a powerful boost to expressiveness .",
    "this work is supported by intel grant icri - ci # 9 - 2012 - 6133 , by isf center grant 1790/12 , and by the european research council ( theorydl project ) .",
    "nadav cohen is supported by a google doctoral fellowship in machine learning .",
    "in this appendix we derive the baseline decomposition ( eq .  [ eq : base_decomp ] )    a parameterization of grid tensors ( eq .  [ eq : grid_tensor ] ) discretizing input - output mappings of the baseline dilated convolutional network ( fig .",
    "[ fig : base_dcn ] ) . as discussed in sec .",
    "[ sec : dcn : base ] , @xmath98 $ ]    the network output at time  @xmath71 , is a function of @xmath243\\ldots{{\\mathbf x}}[t]$ ]    its input over the last @xmath101 time points .",
    "we would like to show that for any @xmath244 $ ] , entry @xmath245 of a tensor  @xmath57 generated by eq .",
    "[ eq : base_decomp ] , is equal to coordinate  @xmath58 of network output  @xmath98 $ ] under the following input assignment : @xmath243={{\\mathbf v}}^{(d_1)},\\ldots,{{\\mathbf x}}[t]={{\\mathbf v}}^{(d_n)}$ ] . to achieve this , we prove by induction that under the latter assignment , for every @xmath97\\cup\\{0\\}$ ] , @xmath246 $ ] and  @xmath96 $ ] , coordinate  @xmath116 of the network s depth-@xmath65 sequence ( input  @xmath70)_t$ ] for  @xmath247 ; hidden sequence  @xmath83)_t$ ] for  @xmath248 $ ] ; output  @xmath88)_t$ ] for  @xmath249 ) at time  @xmath250 ,",
    "is equal to entry  @xmath251 of the tensor  @xmath252 in the baseline decomposition ( eq .  [ eq : base_decomp ] ) .",
    "the desired result then follows from the case  @xmath253 .    when  @xmath247 , the inductive hypothesis is trivial    coordinate  @xmath116 of the input sequence at time  @xmath254 , _ i.e. _  @xmath255_\\gamma$ ] , is by definition of our assignment equal to  @xmath256    entry  @xmath257 of the tensor  @xmath258 ( see eq .",
    "[ eq : base_decomp ] ) .",
    "assume now that the inductive hypothesis holds whenever  @xmath259 , and consider the tensor  @xmath260 for some  @xmath261 $ ] and  @xmath262 $ ] .",
    "from the baseline decomposition ( eq .  [ eq : base_decomp ] ) : @xmath263 focusing on entry  @xmath264 of the left - hand side , while recalling the definition of the generalized tensor product  @xmath28 ( sec .",
    "[ sec : prelim ] ) , we may write : & ^k+1,j,_d_(j-1)2^k+1 + 1,  ,d_(j-1)2^k+1 + 2^k+1 = & + & g(_=1^r_k a_^k+1,,^k,2j-1,_d_(2j-2)2^k+1, ",
    ",d_(2j-2)2^k+2^k,_=1^r_k a_^k+1,,^k,2j,_d_(2j-1)2^k+1,  ,d_(2j-1)2^k+2^k ) & [ eq : base_decomp_entry ] by our inductive assumption : ^k,2j-1,_d_(2j-2)2^k+1,  ,d_(2j-2)2^k+2^k & = & h^(k)[t - n+(2j-1)2^k ]",
    "_ + ^k,2j,_d_(2j-1)2^k+1,  ,d_(2j-1)2^k+2^k & = & h^(k)[t - n+2j2^k ] _  where we overload notation in the case  @xmath265 , letting  @xmath266)_t$ ] stand for the input sequence  @xmath70)_t$ ] . plugging the latter into eq .",
    "[ eq : base_decomp_entry ] , we obtain : & ^k+1,j,_d_(j-1)2^k+1 + 1,  ,d_(j-1)2^k+1 +",
    "2^k+1 = & + & g ( , h^(k)[t - n+(2j-1)2^k ] , , h^(k)[t - n+2j2^k ] ) & by the definition of the baseline dilated convolutional network ( sec .",
    "[ sec : dcn : base ] ) , the latter expression is precisely equal to coordinate  @xmath116 of the sequence  @xmath267)_t$ ] ( or @xmath88)_t$ ]  if  @xmath268 ) at time  @xmath269 .",
    "this proves that our inductive hypothesis holds when  @xmath270 , and in general .",
    "we initiate the proof by introducing notations that will allow a more compact presentation .",
    "hereinafter , we let  @xmath271}$ ] stand for the weights in the tree decomposition of the hybrid mode tree  @xmath184 ( eq .  [ eq : tree_decomp ] with size constant  @xmath272 and underlying mode tree given by def .  [",
    "def : hybrid_tree ] ) .",
    "similarly , we use  @xmath273}$ ] and  @xmath274}$ ] to denote the weights , corresponding to  @xmath121 and  @xmath156 ( respectively ) , in the mixed decomposition ( eq .  [ eq : mix_decomp ] with size constant  @xmath275 ) . recall that by construction ( def .",
    "[ def : hybrid_tree ] ) , @xmath185    the interior of  @xmath184 , consists of different segments ( collections of nodes ) , each taken from either  @xmath186 or  @xmath187 .",
    "we define  @xmath276 to be the function indicating which tree an interior node in  @xmath184 came from . specifically , if the node  @xmath277 originated from  @xmath121 we have  @xmath278 , and on the other hand , if its source is  @xmath156 then  @xmath279 . by convention",
    ", feeding  @xmath280 with an argument outside  @xmath185 yields something that is different from both  @xmath121 and  @xmath156 .",
    "for example , if  @xmath277 is the root node , _",
    "i.e. _  @xmath281 $ ] , then  @xmath282    its parent in  @xmath184 , is undefined and we have  @xmath283 . similarly ,",
    "if the child  @xmath284 of  @xmath277 is a leaf , it is outside the domain of  @xmath280 and thus  @xmath285 .    given a particular setting of weights  @xmath286 for the tree decomposition of  @xmath184 , we would like to show that there exists a setting of weights  @xmath287 and  @xmath288 for the mixed decomposition of  @xmath121 and  @xmath156 , such that the latter generates grid tensors identical to those of the former . more precisely , for any collection of discretizers  @xmath289}$ ] fed into the tree decomposition of  @xmath184 , leading the latter to produce grid tensors  @xmath290}$ ] , we would like the mixed decomposition to be such that when fed with the padded discretizers  @xmath291^\\top\\in{{\\mathbb r}}^{r_{mix}}\\}_{i\\in[m]}$ ] , the first  @xmath190 grid tensors it generates are equal to  @xmath290}$ ] .",
    "we prove existence of the sought after weight setting constructively , by presenting an explicit procedure for assigning  @xmath287 and  @xmath288 based on  @xmath286 : & & + & & a^t,,,=a^t,,,=0 int(t ) , + & & a^|t,,,=a^|t,,,=0 int(|t ) , + & & + & & a^t(),,+r_mix , = \\ {    ll ^ & , t()=t(c_(;h ) ) + ^ & , t()t(c_(;h ) )    .",
    "+ & & a^t(),,+r_mix , = \\ {    ll ^ & , t()=t(c_(;h ) ) + ^ & , t()t(c_(;h ) )    .",
    "+ & & : + & & + & & [ eq : hybrid_tree_by_mix_assignment ] the idea behind this assignment is as follows .",
    "the computation corresponding to a node in the tree decomposition of  @xmath184 , is carried out , in the mixed decomposition of  @xmath121 and  @xmath156 , by the respective node in the respective source tree .",
    "that is to say , the computation of  @xmath277 in the tree decomposition is carried out by  @xmath292 in the mixed decomposition .",
    "@xmath292  uses half  ( @xmath190 ) of its weight vectors , and in each used weight vector , half  ( @xmath190 ) of the coordinates hold actual ( non - zero ) values    a copy of the respective weight from  @xmath277 . the choice of which weight vectors to use , and which coordinates to use in the active weight vectors , depends on the tree - transitioning scheme . if the parent of  @xmath132 in  @xmath184 came from the same tree as  @xmath132 , _ i.e. _  @xmath293 , @xmath292  in the mixed decomposition uses weight vectors with higher indexes ( @xmath294 $ ] ) , as these relate to tensors that are not exchanged ( see eq .",
    "[ eq : mix_decomp ] ) . on the other hand , if  @xmath283 , weight vectors with lower indexes ( @xmath295 $ ] ) are used , so that the computations ( tensors ) will be sent to the opposite tree .",
    "the analogous rationale holds for the children of  @xmath132 in  @xmath184 ( @xmath284  and  @xmath296 ) .",
    "if a child came from the same tree as  @xmath132 , upper coordinates of the corresponding weight vectors are used , so that computations ( tensors ) coming from the present tree are collected . on the other hand , if the child came from the opposite tree , lower coordinates are used and computations ( tensors ) from that tree are fetched .    altogether , the assignment in eq .",
    "[ eq : hybrid_tree_by_mix_assignment ] meets our requirements , and thus concludes the proof .",
    "@xmath297      since we are dealing with a single particular mode tree  @xmath121 , we omit it from our notations throughout the proof . specifically , we denote by  @xmath298 and  @xmath299 ( instead of  @xmath300 and  @xmath301 ) the children of an interior node  @xmath129 ; by  @xmath302 and  @xmath303 ( instead of  @xmath196 and  @xmath209 ) the tilings of  @xmath42 and  @xmath37 ( respectively ) w.r.t .",
    "@xmath121 ( see def .",
    "[ def : tiling ] ) ; and by  @xmath304 ( instead of  @xmath136 ) the permutation corresponding to  @xmath129 in the tree decomposition ( eq .  [ eq : tree_decomp ] ) .    the first stage of the proof is to derive a matricized form of the tree decomposition , shedding light into the manner in which grid tensor matricizations  @xmath202 are generated . as a preparatory step in this direction ,",
    "we define the notion of an _ index set reduction_. let  @xmath123 $ ] be a node in  @xmath121 , whose elements we denote by @xmath305 .",
    "the reduction of  @xmath42 onto  @xmath132 is defined as follows : i|_:=\\{j : i_ji } [ eq : reduction ] in words , it is the set of indexes corresponding to the intersection  @xmath306 inside  @xmath132 .",
    "besides index set reduction , an additional tool we will be using is the _ kronecker product _  ",
    "a matrix operator we denote by  @xmath307 . for two matrices @xmath308 and @xmath309 , @xmath310 is the matrix in @xmath311 holding @xmath312 in row index @xmath313 and column index @xmath314 .",
    "consider the central relation in the tree decomposition ( eq .  [ eq : tree_decomp ] ) , while noticing that  @xmath315 in our setting ( @xmath24  is the product operator  ",
    "see sec .",
    "[ sec : prelim ] ) : _ = ^()((_=1^r",
    "a_^,,^c _ ( ) , ) ( _ = 1^r a_^,,^c _ ( ) , ) ) [ eq : tree_decomp_main ] suppose we would like to matricize the tensor  @xmath137 w.r.t .",
    "the reduction  @xmath316 .",
    "if all elements of  @xmath298 were smaller than those of  @xmath299 , the permutation  @xmath304 would be the identity ( see sec .",
    "[ sec : dcn : tree ] ) , and the following matrix relation would hold : ^,_i| _ & = & _ = 1^r a_^,,^c_(),_i|_c _ ( ) _ = 1^r a_^,,^c_(),_i|_c _ ( ) + & = & ( _ = 1^r a_^,,^c_(),_i|_c _",
    "( ) ) ( _ = 1^r a_^,,^c_(),_i|_c _ ( ) ) in general however , elements in  @xmath298 could be greater than ones in  @xmath299 , and so eq .  [ eq : tree_decomp_main ] includes a tensor mode sorting via  @xmath304 . in matricized form , this amounts to rearranging rows and columns through appropriate permutation matrices  @xmath317 and  @xmath318 respectively : @xmath319 we thus arrive at the following matrix form of eq .",
    "[ eq : tree_decomp ] , referred to as the _",
    "matricized tree decomposition _ : & & + & & ^\\{j},_i|_\\{j } = ^_i|_\\{j }   + & & + & & ^,_i| _ = q^()((_=1^r a_^,,^c_(),_i|_c _ ( ) ) ( _ = 1^r a_^,,^c_(),_i|_c_()))|q^ ( )   + & & ^y_i=^[n],y_i|_[n ]   [ eq : mat_tree_decomp ]    next , we move on to the second stage of the proof , where we establish the upper bound stated in the theorem : rank^y_i  r^\\{(i ) , ( i^c ) } [ eq : tree_decomp_ranks_ub ] we begin by `` propagating outwards '' the permutation matrices  @xmath320)}$ ] and  @xmath321)}$ ] corresponding to the root node  @xmath69 $ ] in the matricized tree decomposition ( eq .  [ eq : mat_tree_decomp ] ) .",
    "namely , for every  @xmath322 $ ] , we replace the matrix  @xmath323,\\gamma}\\rrbracket}_{{{{\\mathcal i}}|_{[n]}}}$ ] by : @xmath324,\\gamma } : = \\left(\\sum_{\\alpha=1}^{r } a_\\alpha^{[n],\\gamma,{\\text{i}}}{\\llbracket\\phi^{c_{\\text{i}}([n]),\\alpha}\\rrbracket}_{{{{\\mathcal i}}|_{c_{\\text{i}}([n])}}}\\right ) \\odot \\left(\\sum_{\\alpha=1}^{r } a_\\alpha^{[n],\\gamma,{\\text{ii}}}{\\llbracket\\phi^{c_{\\text{ii}}([n]),\\alpha}\\rrbracket}_{{{{\\mathcal i}}|_{c_{\\text{ii}}([n])}}}\\right)\\ ] ] and accordingly move  @xmath320)}$ ] and  @xmath321)}$ ] to the assignments of  @xmath202 .",
    "this gives rise to the following decomposition : & & + & & ^\\{j},_i|_\\{j } = ^_i|_\\{j }   + & & + & & ^,_i| _",
    "= q^()((_=1^r a_^,,^c_(),_i|_c _ ( ) ) ( _ = 1^r a_^,,^c_(),_i|_c_()))|q^ ( )   + & & b^[n ] , = ( _ = 1^r a_^[n],,^c_([n]),_i|_c_([n ] ) ) ( _ = 1^r a_^[n],,^c_([n]),_i|_c_([n ] ) )   + & & ^y_i = q^([n])b^[n],y|q^([n ] )   consider now  @xmath325)$ ]    a child of the root node  @xmath69 $ ] , and suppose we would like to similarly propagate outwards its permutation matrices  @xmath326))}$ ] and  @xmath327))}$ ] .",
    "we may define , for every  @xmath322 $ ] : @xmath328),\\gamma } : = \\left(\\sum_{\\alpha=1}^{r } a_\\alpha^{c_{\\text{i}}([n]),\\gamma,{\\text{i}}}{\\llbracket\\phi^{c_{\\text{i}}(c_{\\text{i}}([n])),\\alpha}\\rrbracket}_{{{{\\mathcal i}}|_{c_{\\text{i}}(c_{\\text{i}}([n]))}}}\\right ) \\odot \\left(\\sum_{\\alpha=1}^{r } a_\\alpha^{c_{\\text{i}}([n]),\\gamma,{\\text{ii}}}{\\llbracket\\phi^{c_{\\text{ii}}(c_{\\text{i}}([n])),\\alpha}\\rrbracket}_{{{{\\mathcal i}}|_{c_{\\text{ii}}(c_{\\text{i}}([n]))}}}\\right)\\ ] ] which in turn implies : b^[n ] , & = & ( _ = 1^r a_^[n],,q^(c_([n]))b^c_([n]),|q^(c_([n ] ) ) ) ( _ = 1^r a_^[n],,^c_([n]),_i|_c_([n ] ) ) + & = & ( q^(c_([n]))(_=1^r a_^[n],,b^c_([n]),)|q^(c_([n ] ) ) ) ( _ = 1^r a_^[n],,^c_([n]),_i|_c_([n ] ) ) now , for any matrices  @xmath329 such that  @xmath330 and  @xmath331 are defined , the following equality holds :  @xmath332 ( see  @xcite for proof ) .",
    "we may therefore write : & b^[n ] , = & + & ( q^(c_([n]))i ) ( ( _ = 1^r a_^[n],,b^c_([n ] ) , ) ( _ = 1^r a_^[n],,^c_([n]),_i|_c_([n ] ) ) ) ( |q^(c_([n]))|i ) & where  @xmath333 and  @xmath334 are identity matrices of appropriate sizes . propagating outwards the matrices  @xmath326))}{\\odot}i$ ] and  @xmath327))}{\\odot}{\\bar{i}}$ ] ( while redefining  @xmath335,\\gamma}$ ] appropriately ) , we arrive at the following decomposition : & & + & & ^\\{j},_i|_\\{j } = ^_i|_\\{j }   + & & + & & ^,_i| _ = q^()((_=1^r a_^,,^c_(),_i|_c _ ( ) ) ( _ = 1^r a_^,,^c_(),_i|_c_()))|q^ ( )   + & & b^c_([n ] ) , = ( _ = 1^r a_^c_([n]),,^c_(c_([n])),_i|_c_(c_([n ] ) ) ) + & &  ( _ = 1^r a_^c_([n]),,^c_(c_([n])),_i|_c_(c_([n ] ) ) )   + & & b^[n ] , = ( _ = 1^r a_^[n],,b^c_([n ] ) , ) ( _ = 1^r a_^[n],,^c_([n]),_i|_c_([n ] ) )   + & & ^y_i= ( q^([n])(q^(c_([n]))i))b^[n],y((|q^(c_([n]))|i)|q^([n ] ) )   continuing this process , we propagate outwards the permutation matrices  @xmath317 and  @xmath318 of all nodes  @xmath132 in the tree that are not members of the tilings  @xmath302 or  @xmath303 ( see def .",
    "[ def : tiling ] ) , and are not descendants of such .",
    "this brings forth the following decomposition : & & + & & ^\\{j},_i|_\\{j } = ^_i|_\\{j }   + & & + & & ^,_i| _ = q^()((_=1^r a_^,,^c_(),_i|_c _ ( ) ) ( _ = 1^r a_^,,^c_(),_i|_c_()))|q^ ( )   + & & + & & b^ , = ^,_i| _   + & & + & & b^ , = ( _ = 1^r a_^,,b^c _ ( ) , ) ( _ = 1^r a_^,,b^c _ ( ) , )   + & & ^y_i = ab^[n],y|a   consider now a node  @xmath129 whose child belongs to a tiling    without loss of generality  @xmath298 belongs to  @xmath302 .",
    "notice that in this case  @xmath336 is a column vector for every  @xmath337 $ ] .",
    "we may thus define  @xmath338 to be the matrix whose  @xmath112th column is  @xmath336 , and get the following equalities : @xmath339 where again , @xmath333  is an appropriately sized identity matrix .",
    "this implies that we can propagate outwards  @xmath340 , just as we have done with permutation matrices . applying this procedure to all nodes in the tilings  @xmath302 and  @xmath303",
    ", we arrive at the decomposition below : & & + & & b^ , = e^ ( ) + & & + & & b^ , = ( e^())^ + & & + & & b^ , = ( _ = 1^r a_^,,b^c _ ( ) , ) ( _ = 1^r a_^,,b^c _ ( ) , ) + & & ^y_i = ab^[n],y|a   notice that for compactness in writing we made use of the fact that @xmath341 , where  @xmath342 , @xmath337 $ ] , is the vector in  @xmath200 holding  @xmath10 in entry  @xmath112 and  @xmath241 in the rest . note also that in this decomposition , as opposed to the previous ones , the matrices  @xmath0 and  @xmath343 are not global constants that depend only on  @xmath121 .",
    "rather , they also depend on  @xmath344 for tiling nodes @xmath345 , and thus are ultimately determined through a hidden computation that is not specified above .",
    "this hidden computation is outside our scope , as we are only interested in the size of the matrices  @xmath346,y}\\}_y$ ] .",
    "it is not difficult to see that this size is precisely  @xmath347-by-@xmath348 , meaning that the ranks of  @xmath346,y}\\}_y$ ] are no more than  @xmath349 .",
    "since these ranks are greater than or equal to those of  @xmath202 , the sought after upper bound ( eq .  [ eq : tree_decomp_ranks_ub ] ) indeed holds .    in the third and final stage of the proof",
    ", we establish the lower bound stated in the theorem , namely , that for all configurations of weights  @xmath191 but a set of lebesgue measure zero : rank^y_i  r^\\{(_1,_2)(i)(i^c ) :  } [ eq : tree_decomp_ranks_lb ] we reduce the problem in three successive steps :    * a tree decomposition ( eq .  [ eq : tree_decomp ] ) with a product operator  @xmath24 admits maximal matricization ranks almost always ( see app .",
    "[ app : max_ranks ] ) . therefore ,",
    "to prove that eq .",
    "[ eq : tree_decomp_ranks_lb ] holds for all weight settings but a set of lebesgue measure zero , it suffices to find a particular weight setting for which the inequality holds .",
    "* by assumption , the discretizers  @xmath350}$ ] span  @xmath200 . without loss of generality",
    ", assume that  @xmath351}$ ] are linearly independent , and consider the sub - tensors of  @xmath109 formed by restricting their indexes to the range  @xmath352 ( instead of  @xmath353 ) .",
    "the matricizations of these sub - tensors w.r.t .",
    "@xmath42 are sub - matrices of  @xmath202 , thus any lower bound on ranks of the former matricizations immediately translates to a lower bound on ranks of the latter . since the sub - tensors are precisely the grid tensors that would have been generated by the tree decomposition ( eq .  [ eq : tree_decomp ] ) had we omitted the trailing discretizers  @xmath350\\setminus[r]}$ ] , establishing eq .",
    "[ eq : tree_decomp_ranks_lb ] in the case  @xmath354 proves that it holds in general  ( @xmath355 ) . *",
    "bearing in mind that we assume  @xmath354 ( and linear independence of  @xmath351}$ ] ) , denote by  @xmath356 the  @xmath182-by-@xmath182 matrix holding  @xmath117 in its @xmath357th  row , _",
    "i.e. _  @xmath358^\\top$ ] . from the tree decomposition ( eq .  [ eq : tree_decomp ] ) it is evident that the discretizers affect generated grid tensors only through products of the form  @xmath359 or  @xmath360 , where  @xmath132 is a parent of a leaf node in  @xmath121 . since  @xmath356 is invertible ( @xmath351}$ ]  are linearly independent ) , its exact value has no effect on the class of representable grid tensors    any change it undergoes may be accounted for by the weights  @xmath361 and  @xmath362 that multiply it ( these weights do not appear elsewhere in the decomposition ) .",
    "accordingly , for establishing a lower bound on achievable grid tensor matricization ranks , the value of  @xmath356 is irrelevant ( so long as it is invertible ) , and we may assume , without loss of generality , that  @xmath356 is the identity matrix , _",
    "i.e. _  that  @xmath363 for all  @xmath364 $ ] .    taking into account the above reductions , our objective is to show that there exists a setting of weights @xmath191 , such that the following special case of the matricized tree decomposition ( eq .  [ eq : mat_tree_decomp ] ) generates matricizations meeting the lower bound in eq .",
    "[ eq : tree_decomp_ranks_lb ] : & & + & & ^\\{j},_i|_\\{j } = e^ ( ) + & & + & & ^\\{j},_i|_\\{j } = ( e^())^ + & & + & & ^,_i| _ = q^()((_=1^r a_^,,^c_(),_i|_c _ ( ) ) ( _ = 1^r a_^,,^c_(),_i|_c_()))|q^ ( )   + & & ^y_i=^[n],y_i|_[n ] similarly to the procedure carried out in the second stage of the proof ( establishing the upper bound in eq .",
    "[ eq : tree_decomp_ranks_ub ] ) , we now propagate outwards the permutation matrices  @xmath317 and  @xmath318 corresponding to all interior nodes  @xmath129 .",
    "this brings forth the following decomposition : & & + & & b^\\{j } , = e^ ( ) + & & + & & b^\\{j } , = ( e^())^ + & & + & & b^ , = ( _ = 1^r a_^,,b^c _ ( ) , ) ( _ = 1^r a_^,,b^c _ ( ) , ) + & & ^y_i = ab^[n],y|a   [ eq : tree_decomp_ranks_lb_reduce_decomp ] the matrices  @xmath0 and  @xmath343 in the assignments of  @xmath202 essentially collect all permutation matrices  @xmath365 and  @xmath366 ( respectively ) that have been propagated outwards .",
    "specifically ,  @xmath0 ( respectively  @xmath343 ) is a product of factors , each of the form  @xmath367 ( respectively  @xmath368 ) for a different interior node  @xmath132 and appropriately sized identity matrix .",
    "since permutation matrices are invertible , and since the kronecker product between two invertible matrices is invertible as well ( see  @xcite for proof ) , we conclude that the matrices  @xmath0 and  @xmath343 are invertible .",
    "therefore , for every  @xmath369 $ ] , the rank of  @xmath370 is equal to that of  @xmath335,y}$ ] .",
    "it thus suffices to find a setting of weights @xmath191 for which : rank(b^[n ] , )  r^\\{(_1,_2)(i)(i^c ) :  } [ eq : tree_decomp_ranks_lb_reduce ] disregard the trivial case where there exist siblings  @xmath371 and  @xmath372 of depth  @xmath10 , and  @xmath37 are the children of the root node  @xmath69 $ ] , and the maximal rank of  @xmath335,\\gamma}$ ] is  @xmath10 for every  @xmath322 $ ] . ] and consider the following weight setting :    * @xmath132  is a node in  @xmath302 or  @xmath303 , or a descendant of such : @xmath373\\ ] ] * @xmath132  has one child in  @xmath302 and the other in  @xmath303 : @xmath373\\ ] ] * @xmath132  is the root node  @xmath69 $ ] : @xmath374\\ ] ] * @xmath132  meets neither of the above ( @xmath375 and @xmath376 here denote the all - zero and all - one vectors in  @xmath200 , respectively ) : & a^,1 , = \\ { + ll 1 & , + e^(1 ) & , + . &",
    "+ & a^,1 , = \\ { + ll 1 & , + e^(1 ) & , + . &",
    "+ & a^ , , = a^ , , = 0\\{1 } &    plugging this into the decomposition in eq .",
    "[ eq : tree_decomp_ranks_lb_reduce_decomp ] , one readily sees that :    * for every  @xmath377 , @xmath378}$ ]  are indicator column vectors ( one entry holds  @xmath10 , the rest hold  @xmath241 ) such that  @xmath379 if  @xmath380 .",
    "the same holds for  @xmath381 , but with the vectors being rows . *",
    "if  @xmath132 has one child in  @xmath302 and the other in  @xmath303 , @xmath378}$ ]  are indicator matrices , where both the row and column indexes of the active entry do not repeat as  @xmath116 varies . *",
    "the matrices  @xmath346,\\gamma}\\}_{\\gamma\\in[r]}$ ] corresponding to the root node  @xmath69 $ ] are equal to one another , given by a joint kronecker product between all of the following : * * @xmath382  for every node  @xmath132 in either  @xmath302 or  @xmath303 which does not have a sibling in the other * * @xmath383 for every node  @xmath132 that has one child in  @xmath302 and the other in  @xmath303    according to the first observation above , @xmath382  has rank  @xmath10 for every  @xmath132 in  @xmath302 or  @xmath303 .",
    "the second observation implies that  @xmath384 has rank  @xmath182 for every node  @xmath132 that has one child in  @xmath302 and the other in  @xmath303 . in turn , and while taking into account the rank - multiplicative property of the kronecker product ( @xmath385    see  @xcite for proof ) , the third observation implies : @xmath386,\\gamma})=r^{{\\left\\lvert\\{(\\nu_1,\\nu_2)\\in\\theta({{\\mathcal i}})\\times\\theta({{\\mathcal i}}^c):~\\text{$\\nu_1 $ and $ \\nu_2 $ are siblings in~$t$}\\ } \\right\\rvert } } \\quad\\forall{\\gamma\\in[r]}\\ ] ] we thus have found weights  @xmath191 for which eq .",
    "[ eq : tree_decomp_ranks_lb_reduce ] holds .",
    "is such that there exist siblings  @xmath371 and  @xmath372 of depth  @xmath10 ( @xmath42  and  @xmath37 are the children of the root node  @xmath69 $ ] ) . in the latter case",
    "the lower bound in eq .",
    "[ eq : tree_decomp_ranks_lb_reduce ] can be met trivially . ]",
    "this establishes the sought after lower bound on matricization ranks ( eq .  [ eq : tree_decomp_ranks_lb ] ) , and completes the proof of the theorem .",
    "in the proof of theorem  [ theorem : tree_decomp_ranks ] ( app .",
    "[ app : proofs : tree_decomp_ranks ] ) , and in the derivation of corollary  [ corollary : mix_by_tree ] ( sec .",
    "[ sec : analysis ] ) , we made use of the fact that a tree or mixed decomposition ( eq .  [ eq : tree_decomp ] or  [ eq : mix_decomp ] respectively ) , with a product operator  @xmath24 , admits maximal matricization ranks almost always .",
    "that is to say , for any index set  @xmath195 $ ] , the ranks of generated grid tensors  @xmath109 when matricized w.r.t .",
    "@xmath42 , attain their maximum possible values ( which depend on both the decomposition and  @xmath42 ) for all configurations of weights ( @xmath191 for the tree decomposition , @xmath191 and  @xmath192 for the mixed decomposition ) but a set of lebesgue measure zero .",
    "hereinafter we justify this assertion .",
    "when equipped with the product operator ( @xmath79 ) , a tree or mixed decomposition generates grid tensors  @xmath109 whose entries are polynomials in the decomposition weights .",
    "therefore , for any index set  @xmath195 $ ] , the entries of the matricizations  @xmath202 are , too , polynomials in the decomposition weights . claim  [ claim : max_rank ] below implies that",
    "for a particular index  @xmath58 , the rank of  @xmath370  is maximal almost always , _",
    "i.e. _  for all weight settings but a set of measure zero .",
    "since the union of finitely many zero measure sets is itself a zero measure set ( see  @xcite for example ) , we conclude that the ranks of  @xmath202 are jointly maximal almost always , which is what we set out to prove .    [",
    "claim : max_rank ] let  @xmath387 , and consider a polynomial function mapping weights  @xmath388 to matrices  @xmath389 ( `` polynomial '' here means that all entries of  @xmath390 are polynomials in  @xmath391 ) .",
    "denote  @xmath392 , and consider the set @xmath393 .",
    "this set has lebesgue measure zero .",
    "we disregard the trivial case where  @xmath394 .",
    "let  @xmath395 be a point at which  @xmath396 is attained ( @xmath397 ) , and assume without loss of generality that the top - left @xmath398  minor of  @xmath399 , _ i.e. _  the determinant of  @xmath400 , is non - zero .",
    "the function  @xmath401 defined by  @xmath402 is a polynomial , which by construction does not vanish everywhere ( @xmath403 ) .",
    "the zero set of a polynomial is either the entire space , or a set of lebesgue measure zero ( see  @xcite for proof ) .",
    "therefore , the zero set of  @xmath404 has lebesgue measure zero .",
    "now , for every  @xmath405 : @xmath406 @xmath149  is thus contained in the zero set of  @xmath404 , and therefore too , has lebesgue measure zero ."
  ],
  "abstract_text": [
    "<S> expressive efficiency is a concept that allows formally reasoning about the representational capacity of deep network architectures . </S>",
    "<S> a network architecture is expressively efficient with respect to an alternative architecture if the latter must grow super - linearly in order to represent functions realized by the former . </S>",
    "<S> a well - known example is the exponential expressive efficiency of depth , namely , that in many cases shallow networks must grow exponentially large in order to represent functions realized by deep networks .    in this paper </S>",
    "<S> we study the expressive efficiency brought forth by the architectural feature of connectivity , motivated by the observation that nearly all state of the art networks these days employ elaborate connection schemes , running layers in parallel while splitting and merging them in various ways . </S>",
    "<S> a formal treatment of this question would shed light on the effectiveness of modern connectivity schemes , and in addition , could provide new tools for network design . </S>",
    "<S> we focus on dilated convolutional networks , a family of deep models gaining increased attention , underlying state of the art architectures like google s wavenet and bytenet . by introducing and studying the concept of mixed tensor decompositions </S>",
    "<S> , we prove that interconnecting dilated convolutional networks can lead to expressive efficiency . </S>",
    "<S> in particular , we show that a single connection between intermediate layers can already lead to an almost quadratic gap , which in large - scale settings typically makes the difference between a model that is practical and one that is not .    _ </S>",
    "<S> deep learning _ , _ expressive efficiency _ , _ dilated convolutions _ , _ tensor decompositions _ </S>"
  ]
}