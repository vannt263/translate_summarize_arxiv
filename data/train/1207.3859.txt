{
  "article_text": [
    "consider the estimation of a random vector @xmath0 from the measurement model illustrated in figure  [ fig : model ] .",
    "the random vector @xmath2 , which is assumed to have independent and identically distributed ( i.i.d . )",
    "components @xmath3 , is passed through a known linear transform that outputs @xmath4 .",
    "the components of @xmath1 are generated by a componentwise transfer function @xmath5 .",
    "this work addresses the cases where the distributions @xmath6 and @xmath5 have some unknown parameters , @xmath7 and @xmath8 , that must be learned in addition to the estimation of @xmath2 .",
    "such joint estimation and learning problems with linear transforms and componentwise nonlinearities arise in a range of applications , including empirical bayesian approaches to inverse problems in signal processing , linear regression and classification @xcite , and , more recently , bayesian compressed sensing for estimation of sparse vectors @xmath2 from underdetermined measurements@xcite . also , since the parameters in the output transfer function @xmath5 can model unknown nonlinearities , this problem formulation can be applied to the identification of linear - nonlinear cascade models of dynamical systems , in particular for neural spike responses @xcite .",
    "when the distributions @xmath6 and @xmath5 are known , or reasonably bounded , there are a number of methods available that can be used for the estimation of the unknown vector @xmath2 . in recent years",
    ", there has been significant interest in so - called approximate message passing ( amp ) and related methods based on gaussian approximations of loopy belief propagation ( lbp ) @xcite .",
    "these methods originate from cdma multiuser detection problems in @xcite , and have received considerable recent attention in the context of compressed sensing @xcite .",
    "see , also the survey article @xcite .",
    "the gaussian approximations used in amp are also closely related to standard expectation propagation techniques @xcite , but with additional simplifications that exploit the linear coupling between the variables @xmath2 and @xmath9 .",
    "the key benefits of amp methods are their computational simplicity , large domain of application , and , for certain large random @xmath10 , their exact asymptotic performance characterizations with testable conditions for optimality @xcite .",
    "this paper considers the so - called generalized amp ( gamp ) method of @xcite that extends the algorithm in @xcite to arbitrary output distributions @xmath5 ( many original formulations assumed additive white gaussian noise ( awgn ) measurements ) .",
    "however , although the current formulation of amp and gamp methods is attractive conceptually , in practice , one often does not know the prior and noise distributions exactly . to overcome this limitation , vila and schniter  @xcite and krzakala _ et al . _",
    "@xcite have recently proposed extension of amp and gamp based on expectation maximization ( em ) that enable joint learning of the parameters @xmath11 along with the estimation of the vector @xmath2 . while simulations indicate excellent performance , the analysis of these methods is difficult .",
    "this work provides a unifying analytic framework for such amp - based joint estimation and learning methods .",
    "the main contributions of this paper are as follows :    * generalization of the gamp method of @xcite to a class of algorithms we call _ adaptive gamp _ that enables joint estimation of the parameters @xmath7 and @xmath8 along with vector @xmath2",
    ". the methods are computationally fast and general with potentially large domain of application .",
    "in addition , the adaptive gamp methods include the em - gamp algorithms of @xcite as special cases .",
    "* exact characterization of the asymptotic behavior of adaptive gamp .",
    "we show that , similar to the analysis of the amp and gamp algorithms in @xcite , the componentwise asymptotic behavior of adaptive gamp can be described exactly by a simple scalar _ state evolution _ ( se ) equations .",
    "* demonstration of asymptotic consistency of adaptive gamp with maximum - likelihood ( ml ) parameter estimation .",
    "our main result shows that when the ml parameter estimation is computed exactly , the estimated parameters converge to the true values and the performance of adaptive gamp asymptotically coincides with the performance of the oracle gamp algorithm that knows the correct parameter values .",
    "remarkably , this result applies to essentially arbitrary parameterizations of the unknown distributions @xmath6 and @xmath5 , thus enabling provably consistent estimation on non - convex and nonlinear problems .",
    "* experimental evaluation of the algorithm for the problems of learning of sparse priors in compressed sensing and identification of linear - nonlinear cascade models in neural spiking processes .",
    "our simulations illustrate the performance gain of adaptive gamp and its asymptotic consistency .",
    "adaptive gamp thus provides a computationally - efficient method for a large class of joint estimation and learning problems with a simple , exact performance characterization and provable conditions for asymptotic consistency .",
    "as mentioned above , the adaptive gamp method proposed here can be seen as a generalization of the em methods in @xcite . in @xcite , the prior @xmath6",
    "is described by a generic @xmath12-term gaussian mixture ( gm ) whose parameters are identified by an em procedure @xcite . the  expectation \" or e - step is performed by gamp , which can approximately determine the marginal posterior distributions of the components @xmath13 given the observations @xmath14 and the current parameter estimates of the gm distribution @xmath6 .",
    "a related em - gamp algorithm has also appeared in @xcite for the case of certain sparse priors and awgn outputs .",
    "simulations in @xcite show remarkably good performance and computational speed for em - gamp over a wide class of distributions , particularly in the context of compressed sensing .",
    "also , using arguments from statistical physics , @xcite presents state evolution ( se ) equations for the joint evolution of the parameters and vector estimates and confirms them numerically .    as discussed in section  [ sec : em - gamp ] , em - gamp is a special case of adaptive gamp with a particular choice of the adaptation functions . therefore , one contribution of this paper is to provide a rigorous theoretical justification of the em - gamp methodology . in particular , the current work provides a rigorous justification of the se analysis in @xcite along with extensions to more general input and output channels and adaptation methods .",
    "however , the methodology in @xcite in other ways is more general in that it can also study  seeded \" or  spatially - coupled \" matrices as proposed in @xcite .",
    "an interesting open question is whether the analysis methods in this paper can be extended to these scenarios as well .",
    "an alternate method for joint learning and estimation has been presented in @xcite , which assumes that the distributions on the source and output channels are themselves described by graphical models with the parameters @xmath7 and @xmath8 appearing as unknown variables .",
    "the method in @xcite , called hybrid - gamp , iteratively combines standard loopy bp with amp methods .",
    "one avenue of future work is to see if methodology in this paper can be applied to analyze the hybrid - gamp methods as well .",
    "finally , it should be pointed out that while simultaneous recovery of unknown parameters is appealing conceptually , it is not a strict requirement . an alternate solution to",
    "the problem is to assume that the signal belongs to a known class of distributions and to minimize the maximal mean - squared error ( mse ) for the class .",
    "this minimax approach  @xcite was proposed for amp recovery of sparse signals in  @xcite .",
    "although minimax approach results in the estimators that are uniformly good over the entire class of distributions , there may be a significant gap between the mse achieved by the minimax approach and the oracle algorithm that knows the distribution exactly .",
    "indeed , this gap was the main justification of the em - gamp methods in @xcite . due to its asymptotic consistency ,",
    "adaptive gamp provably achieves the performance of the oracle algorithm .",
    "the paper is organized as follows : in section  [ sec : gampreview ] , we review the non - adaptive gamp and corresponding state evolution equations . in section  [ sec :",
    "gamp ] , we present adaptive gamp and describe ml parameter learning . in section  [ sec : convprop ]",
    ", we provide the main theorems characterizing asymptotic performance of adaptive gamp and demonstrating its consistency . in section  [ sec : numex ] , we provide numerical experiments demonstrating the applications of the method . section  [ sec : concl ] concludes the paper . a conference version of this paper has appeared in @xcite .",
    "this paper contains all the proofs , more detailed descriptions and additional simulations .",
    "before describing the adaptive gamp algorithm , it is useful to review the basic ( non - adaptive ) gamp algorithm of @xcite . consider the estimation problem in fig .",
    "[ fig : model ] where the componentwise distributions on the inputs and outputs have some parametric form , [ eq : pxzlam ] p_x(x|_x ) , p_y|z(y|z , _ z ) , where @xmath15 and @xmath16 represent parameters of the distributions and @xmath17 and @xmath18 some parameter sets .",
    "the gamp algorithm of @xcite can be seen as a class of methods for estimating the vectors @xmath2 and @xmath9 for the case when the parameters @xmath7 and @xmath8 are _ known_. in contrast , the adaptive gamp method that is discussed in section  [ sec : gamp ] enables _ joint _ estimation of the parameters @xmath7 and @xmath8 along with the vectors @xmath2 and @xmath9 . in order that to understand how the adaptation works , it is best to describe the basic gamp algorithm as a special case of the more general adaptive gamp procedure .",
    "the basic gamp algorithm corresponds to the special case of algorithm  [ algo : gamp ] when the _ adaptation functions _ @xmath19 and @xmath20 output fixed values [ eq : hfix ] h^t_z(^t,,^t_p ) = ^t_z , h^t_x(^t,^t_r ) = ^t_x , for some _ pre - computed _ sequence of parameters @xmath21 and @xmath22 . by ",
    "pre - computed \" , we mean that the values do not depend on the data through the vectors @xmath23 , @xmath24 , and @xmath25 . in the oracle scenario @xmath21 and @xmath22 are set to the true values of the parameters and do not change with the iteration number @xmath26 .    the _ estimation functions _ @xmath27 , @xmath28 and @xmath29 determine the estimates for the vectors @xmath2 and @xmath9 , given the parameter values @xmath30 and @xmath31 .",
    "as described in @xcite , there are two important sets of choices for the estimation functions , resulting in two variants of gamp :    * _ sum - product gamp : _ in this case , the estimation functions are selected so that gamp provides a gaussian approximation of sum - product loopy bp .",
    "the estimates @xmath32 and @xmath33 then represent approximations of the mmse estimates of the vectors @xmath2 and @xmath9 . *",
    "_ max - sum gamp : _ in this case , the estimation functions are selected so that gamp provides a quadratic approximation of max - sum loopy bp and @xmath32 and @xmath33 represent approximations of the map estimates .",
    "the estimation functions of the sum - product gamp are equivalent to scalar mmse estimation problems for the components of the vectors @xmath2 and @xmath9 observed in gaussian noise . for max - sum gamp , the estimation functions correspond to scalar map problems .",
    "thus , for both versions , the gamp method reduces the vector - valued mmse and map estimation problems to a sequence of scalar awgn problems combined with linear transforms by @xmath10 and @xmath34 .",
    "gamp is thus computationally simple , with each iteration involving only scalar nonlinear operations followed by linear transforms .",
    "the operations are similar in form to separable and proximal minimization methods widely used for such problems @xcite .",
    "appendix  [ sec : gampdetails ] reviews the equations for the sum - product gamp .",
    "more details , as well as the equations for max - sum gamp can be found in @xcite .",
    "in addition to its computational simplicity and generality , a key motivation of the gamp algorithm is that its asymptotic behavior can be precisely characterized when @xmath10 is a large i.i.d .",
    "gaussian transform .",
    "the asymptotic behavior is described by what is known as a _ state evolution _ ( se ) analysis . by now",
    ", there are a large number of se results for amp - related algorithms @xcite . here",
    ", we review the particular se analysis from @xcite which is based on the framework in @xcite .    [ as : gamp ] consider a sequence of random realizations of the gamp algorithm , indexed by the dimension @xmath35 , satisfying the following assumptions :    * [ as : gauss ] for each @xmath35 , the matrix @xmath36 has i.i.d .  components with @xmath37 and the dimension @xmath38 is a deterministic function of @xmath35 satisfying @xmath39 for some @xmath40 as @xmath41 . * [ as : px0 ] the input vectors @xmath2 and initial condition @xmath42 are deterministic sequences whose components converge empirically with bounded moments of order @xmath43 as [ eq : thetaxinit ] _",
    "n ( , ^0 ) ( x,^0 ) , to some random vector @xmath44 for some @xmath45 . for the precise definition of this form of convergence .",
    "* [ as : pyz ] the output vectors @xmath9 and @xmath1 are generated by [ eq : zax ] = , y_i = h(z_i , w_i ) i=1,  ,m , for some scalar function @xmath46 and vector @xmath47 representing an output disturbance .",
    "it is assumed that the output disturbance vector @xmath48 is deterministic , but empirically converges as [ eq : wlim ] _",
    "n w , where @xmath43 is as in assumption  [ as : gamp ] ( b ) and @xmath49 is some random variable .",
    "we let @xmath5 denote the conditional distribution of the random variable @xmath50 . *",
    "the estimation function @xmath51 and its derivative with respect to @xmath52 , is lipschitz continuous in @xmath52 at @xmath53 , where @xmath54 is a deterministic parameter from the se equations below .",
    "a similar assumptions holds for @xmath55 .",
    "* the adaptation functions @xmath19 and @xmath20 are set to for some deterministic sequence of parameters @xmath56 and @xmath57 .",
    "also , in the estimation steps in lines [ line : zhat ] , [ line : shat ] and [ line : xhat ] of algorithm  [ algo : gamp ] , the values of the @xmath58 and @xmath59 are replaced with the deterministic parameters @xmath60 and @xmath54 from the se equations defined below .",
    "assumption  [ algo : gamp](a ) simply states that we are considering large , gaussian i.i.d .",
    "matrices @xmath10 . assumptions ( b ) and ( c ) state that the input vector @xmath2 and output disturbance @xmath48 are modeled as deterministic , but whose empirical distributions asymptotically appear as i.i.d .",
    "this deterministic model is one of key features of bayati and montanari s analysis in @xcite .",
    "assumption ( d ) is a mild continuity condition .",
    "assumption ( e ) defines the restriction of adaptive gamp to the non - adaptive gamp algorithm .",
    "we will remove this final assumption later .",
    "note that , for now , there is no assumption that the  true \" distribution of @xmath61 or the true conditional distribution of @xmath62 given @xmath63 must belong to the class of distributions for any parameters @xmath7 and @xmath8 .",
    "the analysis can thus model the effects of model mismatch .",
    "now , given the above assumptions , define the sets of vectors    [ eq : thetaxz ] _",
    "x^t & : = & \\{(x_j , r_j^t,_j^1 ) , j=1,  ,n } , + _ z^t & : = & \\{(z_i,_i^t , y_i , p_i^t ) , i=1,  ,m } .",
    "the first vector set , @xmath64 , represents the components of the the  true , \" but unknown , input vector @xmath2 , its gamp estimate @xmath32 as well as @xmath25 .",
    "the second vector , @xmath65 , contains the components the  true , \" but unknown , output vector @xmath9 , its gamp estimate @xmath33 , as well as @xmath23 and the observed output @xmath14 .",
    "the sets @xmath64 and @xmath65 are implicitly functions of the dimension @xmath35 .",
    "the main result of @xcite shows that if we fix the iteration @xmath26 , and let @xmath66 , the asymptotic joint empirical distribution of the components of these two sets @xmath64 and @xmath65 converges to random vectors of the form [ eq : thetabarxz ] _ x^t : = ( x , r^t,^1 ) , _ z^t : = ( z,^t , y , p^t ) .",
    "we precisely state the nature of convergence momentarily ( see theorem  [ thm : stateevogamp ] ) . in",
    ", @xmath61 is the random variable in assumption  [ as : gamp](b ) , while @xmath67 and @xmath68 are given by    [ eq : rvxt ] @xmath69    for some deterministic constants @xmath70 , @xmath71 , and @xmath54 that are defined below .",
    "similarly , @xmath72 for some covariance matrix @xmath73 , and [ eq : yzpt ] y = h(z , w ) , ^t = g_z^t(p^t , y,_p^t,_z^t ) , where @xmath49 is the random variable in and @xmath73 contains deterministic constants .",
    "the deterministic constants @xmath70 , @xmath71 , @xmath54 and @xmath73 represent parameters of the distributions of @xmath74 and @xmath75 and depend on both the distributions of the input and outputs as well as the choice of the estimation and adaptation functions .",
    "the se equations provide a simple method for recursively computing these parameters .",
    "the equations are best described algorithmically as shown in algorithm  [ algo : se ] . in order that we do not repeat ourselves , in algorithm  [ algo : se ] , we have written the se equations for adaptive gamp : for non - adaptive gamp , the updates and can be ignored as the values of @xmath57 and @xmath56 are pre - computed .    with these definitions , we can state the main result from @xcite .",
    "[ thm : stateevogamp ] consider the random vectors @xmath64 and @xmath65 generated by the outputs of gamp under assumption [ as : gamp ] .",
    "let @xmath74 and @xmath75 be the random vectors in with the parameters determined by the se equations in algorithm  [ algo : se ] .",
    "then , for any fixed @xmath26 , the components of @xmath64 and @xmath65 converge empirically with bounded moments of order @xmath76 as [ eq : thetalimgamp ] _",
    "n _ x^t _ x^t , _",
    "z^t _ z^t . where @xmath74 and @xmath75 are given in .",
    "in addition , for any @xmath26 , the limits [ eq : taulimgamp ] _ n",
    "_ r^t = _ r^t , _",
    "p^t = _ p^t , also hold almost surely .",
    "the theorem shows that the behavior of any component of the vectors @xmath2 and @xmath9 and their gamp estimates @xmath32 and @xmath33 are distributed identically to a simple scalar equivalent system with random variables @xmath61 , @xmath63 , @xmath77 and @xmath78 .",
    "this scalar equivalent model appears in several analyses and can be thought of as a _ single - letter characterization _",
    "@xcite of the system . remarkably , this limiting property holds for essentially arbitrary distributions and estimation functions , even ones that arise from problems that are highly nonlinear or noncovex . from the single - letter characterization , one can compute the asymptotic value of essentially any componentwise performance metric such as mean - squared error or detection accuracy .",
    "similar single - letter characterizations can also be derived by arguments from statistical physics @xcite .",
    "[ algo : gamp ]    [ line : taup ] [ line : phat ] [ line : lamz ] [ line : zhat ] [ line : shat ] [ line : taus ]    [ line : taur ] [ line : rhat ] [ line : lamx ] [ line : xhat ] [ line : taux ]    as described in the previous section , the standard gamp algorithm of @xcite considers the case when the parameters @xmath7 and @xmath8 in the distributions in are known .",
    "the adaptive gamp method proposed in this paper , and shown in algorithm  [ algo : gamp ] , is an extension of the standard gamp procedure that enables simultaneous identification of the parameters @xmath7 and @xmath8 along with estimation of the vectors @xmath2 and @xmath9 . the key modification is the introduction of the two _ adaptation functions _ : @xmath79 and @xmath80 . in each iteration , these functions output estimates , @xmath81 and @xmath82 of the parameters based on the data @xmath23 , @xmath14 , @xmath25 , @xmath58 and @xmath59 .",
    "we saw the standard gamp method corresponds to the adaptation functions in which outputs fixed values @xmath57 and @xmath56 that do not depend on the data , and can be used when the true parameters are known . for the case when the true parameters are not known , we will see that a simple maximum likelihood ( ml ) can be used to estimate the parameters from the data .      to understand how to estimate parameters via the adaptation functions ,",
    "observe that from theorem  [ thm : stateevogamp ] , we know that the distribution of the components of @xmath25 are distributed identically to the scalar @xmath67 in .",
    "now , the distribution of @xmath67 only depends on three parameters  @xmath70 , @xmath71 and @xmath7 .",
    "it is thus natural to attempt to estimate these parameters from the empirical distribution of the components of @xmath25 .    to this end , let @xmath83 be the log likelihood [ eq : phixll ] _",
    "x(r,_x,_r,_r ) = p_r(r|_x,_r,_r ) , where the right - hand side is the probability density of a random variable @xmath84 with distribution @xmath85 then , at any iteration @xmath26 , we can attempt to perform a maximum - likelihood ( ml ) estimate + & & = _ _ x _ x _ ( _ r,_r ) s_x(_r^t ) \\ { _ j=1^n _ x(r^t_j,_x,_r,_r)}. [ eq : hxml ] here , the set @xmath86 is a set of possible values for the parameters @xmath87 .",
    "the set may depend on the measured variance @xmath59 .",
    "we will see the precise role of this set below .",
    "similarly , the joint distribution of the components of @xmath23 and @xmath14 are distributed according to the scalar @xmath88 which depend only on the parameters @xmath89 and @xmath8 .",
    "thus , we can define the likelihood @xmath90 where the right - hand side is the joint probability density of @xmath91 with distribution @xmath92 then , we can attempt to estimate @xmath8 via the ml estimate @xmath93 again , the set @xmath94 is a set of possible covariance matrices @xmath89 .",
    "it is useful to briefly compare the above ml parameter estimation with the em - gamp method proposed by vila and schniter in @xcite and krzakala _ et .",
    "_ in @xcite .",
    "both of these methods combine the bayesian amp @xcite or gamp algorithms @xcite with a standard em procedure @xcite as follows .",
    "first , the algorithms use the sum - product version of the amp / gamp algorithms , so that the outputs can provide an estimate of the posterior distributions on the components of @xmath2 given the current parameter estimates .",
    "specifically , at any iteration @xmath26 , define the distribution + & = & p_x(x_j|_x^1 ) .",
    "[ eq : phatpost ] for the sum - product amp or gamp algorithms , it is shown in @xcite that the se equations simplify so that @xmath95 and @xmath96 , if the parameters were selected correctly . therefore , from theorem  [ thm : stateevogamp ] , the conditional distribution @xmath97 should approximately match the distribution for large @xmath35 . if , in addition , we treat @xmath98 and @xmath59 as sufficient statistics for estimating @xmath13 given @xmath14 and @xmath10 , then @xmath99 can be treated as an approximation for the posterior distribution of @xmath13 given the current parameter estimate @xmath100 .",
    "some justification for this last step can be found in @xcite . using the approximation",
    ", we can approximately implement the em procedure to update the parameter estimate via a maximization + & : = & _ _ x _ x _ j=1^n , [ eq : lamem ] where the expectation is with respect to the distribution in . in @xcite ,",
    "the parameter update is performed only once every few iterations to allow @xmath101 to converge to the approximation of the posterior distribution of @xmath13 given the current parameter estimates . in @xcite , the parameter estimate is updated every iteration . a similar procedure can be performed for the estimation of @xmath8 .",
    "we thus see that the em - gamp procedures in @xcite and in @xcite are both special cases of the adaptive gamp algorithm in algorithm  [ algo : gamp ] with particular choices of the adaptation functions @xmath102 and @xmath103 . as a result",
    ", our analysis in theorem  [ thm : stateevo ] below can be applied to these algorithms to provide rigorous asymptotic characterizations of the em - gamp performance . however , at the current time , we can only prove the asymptotic consistency result , theorem  [ thm : consistent ] , for the ml adaptation functions and described above .",
    "that being said , it should be pointed out that em - gamp update is generally computationally much simpler than the ml updates in and .",
    "for example , when @xmath104 is an exponential family , the optimization in is convex .",
    "also , the optimizations in and require searches over additional parameters such as @xmath105 and @xmath106 .",
    "thus , an interesting avenue of future work is to apply the analysis result , theorem  [ thm : consistent ] below , to see if the em - gamp method or some similarly computationally simple technique can be developed which also provides asymptotic consistency .",
    "given the distributions in assumption  [ as : gamp ] , compute the sequence of parameters as follows :    * _ initialization _ set @xmath107 with [ eq : seinit ] _",
    "x^0 = ( x , ^0 ) , _",
    "x^0 = _ x^0 , where the expectation is over the random variables @xmath108 in assumption [ as : gamp](b ) and @xmath109 is the initial value in the gamp algorithm . * _ output node update : _ compute the variables associated with the output nodes + [ eq : outse ] compute the variables _ p^t & = & _ x^t , _ p^t = _ x^t , + _ z^t & = & h_z^t(p^t , y , _",
    "p^t ) , [ eq : lambarzse ] + _ r^t & = & -^-1 , [ eq : taubarrse ] + _ r^t & = & ( _ r^t)^2 , + _ r^t & = & _ r^t , [ eq : alphase ] + where the expectations are over the random variables @xmath72 and @xmath62 is given in . * _ input node update : _ compute + [ eq : inse ] _",
    "x^t & = & h_x^t(r^t,_r^t ) , [ eq : lambarxse ] + _ x^1 & = & _ r^t , + ^1_x & = & ( x,^1 ) , + where the expectations are over the random variables in .",
    "before proving the asymptotic consistency of the adaptive gamp method with ml adaptation , we first prove the following more general convergence result .    [ as : agamp ] consider the adaptive gamp algorithm running on a sequence of problems indexed by the dimension @xmath35 , satisfying the following assumptions :    * same as assumption  [ as : gamp](a ) to ( c ) with @xmath110 . * for every @xmath26 , *     [ thm : stateevo ] consider the random vectors @xmath64 and @xmath65 generated by the outputs of the adaptive gamp under assumption [ as : agamp ] .",
    "let @xmath74 and @xmath75 be the random vectors in with the parameters determined by the se equations in algorithm  [ algo : se ] .",
    "then , for any fixed @xmath26 , the components of @xmath64 and @xmath65 converge empirically with bounded moments of order @xmath110 as [ eq : thetalim ] _ n _ x^t _ x^t , _",
    "z^t _ z^t . where @xmath74 and @xmath75 are given in .",
    "in addition , for any @xmath26 , the limits    [ eq : taulamlim ] @xmath111    also hold almost surely .",
    "the result is a natural generalization of theorem  [ thm : stateevogamp ] and provides a simple extension of the se analysis to incorporate the adaptation .",
    "the se analysis applies to essentially arbitrary adaptation functions .",
    "it particular , it can be used to analyze both the behavior of the adaptive gamp algorithm with either ml and em - gamp adaptation functions in the previous section .",
    "the proof is straightforward and is based on a continuity argument also used in @xcite .",
    "we cam now use theorem  [ thm : stateevo ] to prove the asymptotic consistency of the adaptive gamp method with the ml parameter estimation described in section  [ sec : mladapt ] .",
    "the following two assumptions can be regarded as _ identifiability",
    "_ conditions .",
    "[ def : pxident ] consider a family of distributions , @xmath112 , a set @xmath113 of parameters @xmath114 of a gaussian channel and function @xmath83 .",
    "we say that @xmath104 is _ identifiable with gaussian outputs _ with parameter set @xmath113 and function @xmath115 if :    * the sets @xmath113 and @xmath17 are compact . * for any  true \" parameters @xmath116 , and @xmath117 , the maximization + & & , [ eq : lamxscore ] is well - defined , unique and returns the true value , @xmath118 .",
    "the expectation in is with respect to @xmath119 and @xmath120 .",
    "* for every @xmath7 and @xmath105 , @xmath106 , the function @xmath83 is pseudo - lipschitz continuous of order @xmath110 in @xmath52 .",
    "[ def : pzident ] consider a family of conditional distributions , @xmath121 generated by the mapping @xmath122 where @xmath123 is some random variable and @xmath124 is a scalar function .",
    "let @xmath125 be a set of covariance matrices @xmath89 and let @xmath126 be some function .",
    "we say that conditional distribution family @xmath127 is _ identifiable with gaussian inputs _ with covariance set @xmath125 and function @xmath128 if :    * the parameter sets @xmath125 and @xmath18 are compact . * for any  true \" parameter @xmath129 and true covariance @xmath130 ,",
    "the maximization + & & , [ eq : lamzscore ] is well - defined , unique and returns the true value , @xmath131 , the expectation in is with respect to @xmath132 and @xmath133 .",
    "* for every @xmath8 and @xmath89 , the function @xmath126 is pseudo - lipschitz continuous in @xmath134 of order @xmath110 .",
    "in addition , it is continuous in @xmath135 uniformly over @xmath136 and @xmath137 .    definitions [ def : pxident ] and [ def : pzident ] essentially require that the parameters @xmath7 and @xmath8 can be identified through a maximization . the functions @xmath115 and @xmath128 can be the log likelihood functions and , although we permit other functions as well , since the maximization may be computationally simpler .",
    "such functions are sometimes called _ pseudo - likelihoods_. the existence of a such a function is a mild condition . indeed ,",
    "if such a function does not exists , then the distributions on @xmath84 or @xmath138 must be the same for at least two different parameter values . in that case , one can not hope to identify the correct value from observations of the vectors @xmath25 or @xmath139 .",
    "[ as : agamp - ml ] let @xmath104 and @xmath140 be families of distributions and consider the adaptive gamp algorithm , algorithm [ algo : gamp ] , run on a sequence of problems , indexed by the dimension @xmath35 satisfying the following assumptions :    * same as assumption [ as : gamp](a ) to ( c ) with @xmath110 .",
    "in addition , the distributions for the vector @xmath61 is given by @xmath141 for some  true \" parameter @xmath116 and the conditional distribution of @xmath62 given @xmath63 is given by @xmath142 for some  true \" parameter @xmath129 .",
    "* same as assumption [ as : agamp](c ) . *",
    "the adaptation functions are set to and .",
    "[ thm : consistent ] consider the outputs of the adaptive gamp algorithm with ml adaptation as described in assumption [ as : agamp - ml ] .",
    "then , for any fixed @xmath26 ,    * the components of @xmath64 and @xmath65 in converge empirically with bounded moments of order @xmath110 as in and the limits hold almost surely . * in addition , if @xmath143 , and the family of distributions @xmath144 , @xmath15 is identifiable in gaussian noise with parameter set @xmath86 and pseudo - likelihood @xmath115 ( see definition  [ def : pxident ] ) , then [ eq : lamxcons ] _",
    "n ^t_x = _ x^t = ^*_x almost surely . *",
    "similarly , if @xmath145 for some @xmath26 , and the family of distributions @xmath146 , @xmath16 is identifiable with gaussian inputs with parameter set @xmath94 and pseudo - likelihood @xmath128 ( see definition  [ def : pzident ] ) then [ eq : lamzcons ] _",
    "n ^t_z = _ z^t = ^*_z almost surely .",
    "_ proof : _ see appendix [ sec : consistentpf ] .",
    "the theorem shows , remarkably , that for a very large class of the parameterized distributions , the adaptive gamp algorithm is able to asymptotically estimate the correct parameters .",
    "moreover , there is asymptotically no performance loss between the adaptive gamp algorithm and a corresponding oracle gamp algorithm that knows the correct parameters in the sense that the empirical distributions of the algorithm outputs are described by the same se equations .",
    "there are two key requirements : first , that the optimizations in and can be computed",
    ". these optimizations may be non - convex .",
    "secondly , that the optimizations can be performed are over sufficiently large sets of gaussian channel parameters @xmath113 and @xmath125 such that it can be guaranteed that the se equations eventually enter these sets . in the examples below , we will see ways to reduce the search space of gaussian channel parameters .",
    "recent results suggest that there is considerable value in learning of priors @xmath6 in the context of compressed sensing @xcite , which considers the estimation of sparse vectors @xmath2 from underdetermined measurements ( @xmath147 ) .",
    "it is known that estimators such as lasso offer certain optimal min - max performance over a large class of sparse distributions @xcite . however ,",
    "for many particular distributions , there is a potentially large performance gap between lasso and mmse estimator with the correct prior .",
    "this gap was the main motivation for @xcite which showed large gains of the em - gamp method due to its ability to learn the prior .    here",
    ", we illustrate the performance and asymptotic consistency of adaptive gamp in a simple compressed sensing example . specifically , we consider the estimation of a sparse vector @xmath0 from @xmath148 noisy measurements @xmath149 where the additive noise @xmath48 is random with i.i.d .",
    "entries @xmath150 . here",
    ", the  output \" channel is determined by the statistics on @xmath48 , which are assumed to be known to the estimator .",
    "so , there are no unknown parameters @xmath8 .    as a model for the sparse input vector @xmath2",
    ", we assumed the components are i.i.d .  with the gauss - bernoulli distribution , [ eq : gaussbern ] x_j ~\\ {    ll 0 &",
    "= 1- , + ( 0,_x^2 ) & =    . where @xmath151 represents the probability that the component is non - zero ( i.e.the vector s sparsity ratio ) and @xmath152 is the variance of the non - zero components .",
    "the parameters @xmath153 are treated as unknown .    in the adaptive gamp algorithm",
    ", we use the estimation functions @xmath154 , @xmath155 , and @xmath156 corresponding to the sum - product gamp algorithm . as described in appendix  [",
    "sec : gampdetails ] , for the sum - produce gamp the se equations simplify so that @xmath157 and @xmath96 .",
    "since the noise variance is known , the initial output noise variance @xmath158 obtained by adaptive gamp in algorithm  [ algo : gamp ] exactly matches that of oracle gamp .",
    "therefore , for @xmath159 , the parameters @xmath70 and @xmath71 do not need to be estimated , and   conveniently simplifies to @xmath160 where @xmath161 \\times [ 0 , + \\infty)$ ] . for iteration @xmath162 ,",
    "we rely on asymptotic consistency , and assume that the maximization   yields the correct parameter estimates , so that @xmath163",
    ". then , in principle , for @xmath162 adaptive gamp uses the correct parameter estimates and we expect it to match the performance of oracle gamp . in our implementation , we run em update   until convergence to approximate the ml adaptation  .    fig .",
    "[ fig : mseresults ] illustrates the performance of adaptive gamp on signals of length @xmath164 generated with the parameters @xmath165 .",
    "the performance of adaptive gamp is compared to that of lasso with mse optimal regularization parameter , and oracle gamp that knows the parameters of the prior exactly . for generating the graphs",
    ", we performed @xmath166 random trials by forming the measurement matrix @xmath10 from i.i.d .",
    "zero - mean gaussian random variables of variance @xmath167 . in figure",
    "[ fig : mseresults](a ) , we keep the variance of the noise fixed to @xmath168 and plot the average mse of the reconstruction against the measurement ratio @xmath169 . in figure",
    "[ fig : mseresults](b ) , we keep the measurement ratio fixed to @xmath170 and plot the average mse of the reconstruction against the noise variance @xmath171 . for completeness , we also provide the asymptotic mse values computed via se recursion .",
    "the results illustrate that gamp significantly outperforms lasso over the whole range of @xmath169 and @xmath171 .",
    "moreover , the results corroborate the consistency of adaptive gamp which achieves nearly identical quality of reconstruction with oracle gamp .",
    "the performance results indicate that adaptive gamp can be an effective method for estimation when the parameters of the problem are difficult to characterize and must be estimated from data .      as a second example , we consider the estimation of the linear - nonlinear - poisson ( lnp ) cascade model  @xcite .",
    "the model has been successfully used to characterize neural spike responses in early sensory pathways of the visual system . in the context of lnp cascade model ,",
    "the vector @xmath0 represents the linear filter , which models the linear receptive field of the neuron .",
    "amp techniques combined with the parameter estimation have been recently proposed for neural receptive field estimation and connectivity detection in @xcite .    as in section",
    "[ sec : gaussbern ] , we model @xmath2 as a gauss - bernoulli vector of unknown parameters @xmath172 . to obtain the measurements @xmath14",
    ", the vector @xmath4 is passed through a componentwise nonlinearity @xmath173 to result in u(z ) = .",
    "let the function @xmath174 denote a vector ( z ) = ( 1 , u(z ) , ",
    ", u(z)^r-1)^t .",
    "then , the final measurement vector @xmath14 is generated by a measurement channel with a conditional density of the form [ eq : pyzexp ] p_y|z(y_i|z_i,_z ) = ^-f(z_i ) , where @xmath175 denotes the nonlinearity given by @xmath176 adaptive gamp can now be used to also estimate vector of polynomial coefficients @xmath8 , which together with @xmath2 , completely characterizes the lnp system .    .",
    "this plots illustrates near consistency of adaptive gamp for large @xmath35.,width=321 ]    the estimation of @xmath8 is performed with ml estimator described in section  [ sec : mladapt ] .",
    "we assume that the mean and variance of the vector @xmath2 are known at iteration @xmath107 .",
    "this implies that for sum - product gamp the covariance @xmath177 is initially known and the optimization   simplifies to @xmath178 where @xmath179 .",
    "the estimation of @xmath7 is performed as in section  [ sec : gaussbern ] .",
    "as before , for iteration @xmath162 , we assume that the maximizations   and   yield correct parameter estimates @xmath163 and @xmath180 , respectively .",
    "thus we can conclude by induction that for @xmath162 the adaptive gamp algorithm should continue matching oracle gamp for large enough @xmath35 . in our simulations",
    ", we implemented   with a gradient ascend algorithm and run it until convergence .    in fig .",
    "[ fig : nlresults ] , we compare the reconstruction performance of adaptive gamp against the oracle version that knows the true parameters @xmath181 exactly .",
    "we consider the vector @xmath2 generated with true parameters @xmath182 .",
    "we consider the case @xmath183 and set the parameters of the output channel to @xmath184 $ ] .",
    "to illustrate the asymptotic consistency of the adaptive algorithm , we consider the signals of length @xmath185 and @xmath186 .",
    "we perform @xmath187 and @xmath188 random trials for long and short signals , respectively , and plot the average mse of the reconstruction against @xmath169 .",
    "as expected , for large @xmath35 , the performance of adaptive gamp is nearly identical ( within @xmath189 ) to that of oracle gamp .",
    "we have presented an adaptive gamp method for the estimation of i.i.d .",
    "vectors @xmath2 observed through a known linear transforms followed by an arbitrary , componentwise random transform .",
    "the procedure , which is a generalization of em - gamp methodology of @xcite that estimates both the vector @xmath2 as well as parameters in the source and componentwise output transform . in the case of large i.i.d .",
    "gaussian transforms , it is shown that the adaptive gamp method is provably asymptotically consistent in that the parameter estimates converge to the true values .",
    "this convergence result holds over a large class of models with essentially arbitrarily complex parameterizations .",
    "moreover , the algorithm is computationally efficient since it reduces the vector - valued estimation problem to a sequence of scalar estimation problems in gaussian noise .",
    "we believe that this method is applicable to a large class of linear - nonlinear models with provable guarantees can have applications in a wide range of problems .",
    "we have mentioned the use of the method for learning sparse priors in compressed sensing .",
    "future work will include learning of parameters of output functions as well as possible extensions to non - gaussian matrices .",
    "as described in @xcite , the sum - product estimation can be implemented with the estimation functions    [ eq : gsp ] g_x^t(r,_r,_x ) & : = & , [ eq : gxsp ] + g_z^t(p , y,_p,_z ) & : = & , [ eq : gzsp ] + g_s^t(p , y,_p,_z ) & : = & ( g_z^t(p , y,_p,_z ) - p ) ,    where the expectations are with respect to the scalar random variables    [ eq : gspdist ] @xmath190    the estimation functions correspond to scalar estimates of random variables in additive white gaussian noise ( awgn ) .",
    "a key result of @xcite is that , when the parameters are set to the true values ( i.e.  @xmath191 ) , the outputs @xmath32 and @xmath33 can be interpreted as sum products estimates of the conditional expectations @xmath192 and @xmath193 .",
    "the algorithm thus reduces the vector - valued estimation problem to a computationally simple sequence of scalar awgn estimation problems along with linear transforms .",
    "moreover , the se equations in algorithm  [ algo : se ] reduce to a particularly simple forms , where @xmath54 and @xmath71 in are given by    @xmath194,\\ ] ]    where the expectations are over the random variables @xmath72 and @xmath62 is given in .",
    "the covariance matrix @xmath73 has the form @xmath195",
    "\\beta\\tau_{x0 } - \\taubar_p^t & \\beta\\tau_{x0 } - \\taubar_p^t       \\\\[0.3em ]       \\end{bmatrix},\\ ] ] where @xmath196 is the variance of @xmath61 and @xmath40 is the asymptotic measurement ratio ( see assumption 1 for details ) .",
    "the scaling constant   becomes @xmath157 .",
    "the update rule for @xmath197 also simplifies to @xmath198,\\ ] ] where the expectation is over the random variables in .",
    "bayati and montanari s analysis in @xcite employs certain deterministic models on the vectors and then proves convergence properties of related empirical distributions . to apply the same analysis here",
    ", we need to review some of their definitions .",
    "we say a function @xmath199 is _ pseudo - lipschitz _ of order @xmath200 , if there exists an @xmath201 such for any @xmath2 , @xmath202 , @xmath203    now suppose that for each @xmath204 , @xmath205 is a set of vectors [ eq : vsetapp ] ^(n ) = \\{_i(n ) , i=1, ",
    ",(n ) } , where each element @xmath206 and @xmath207 is the number of elements in the set .",
    "thus , @xmath205 can itself be regarded as a vector with @xmath208 components .",
    "we say that @xmath205 _ empirically converges with bounded moments of order @xmath76 _ as @xmath209 to a random vector @xmath210 on @xmath211 if : for all pseudo - lipschitz continuous functions , @xmath212 , of order @xmath76 , @xmath213 when the nature of convergence is clear , we may write ( with some abuse of notation ) @xmath214 or @xmath215    finally , let @xmath216 be the set of probability distributions on @xmath211 with bounded @xmath76th moments , and suppose that @xmath217 is a functional @xmath216 to some topological space @xmath218 . given a set @xmath205 as in ,",
    "write @xmath219 for @xmath220 where @xmath221 is the empirical distribution on the components of @xmath222 . also , given a random vector @xmath210 with distribution @xmath223 write @xmath224 for @xmath225 . then",
    ", we will say that the functional @xmath226 is _ weakly pseudo - lipschitz continuous _ of order @xmath76 if @xmath227 where the limit on the right hand side is in the topology of @xmath218 .",
    "the proof follows along the adaptation argument of @xcite .",
    "we use the tilde superscript on quantities such as @xmath228 , and @xmath229 to denote values generated via a non - adaptive version of the gamp .",
    "the non - adaptive gamp algorithm has the same initial conditions as the adaptive algorithm ( i.e.  @xmath230 ) , but with @xmath82 and @xmath81 replaced by their deterministic limits @xmath231 and @xmath232 , respectively .",
    "that is , we replace lines [ line : zhat ] , [ line : shat ] and [ line : xhat ] with _ i^t & = & g_z^t(p_i^t , y_i,_p^t,_z^t ) , _",
    "i^t = g_s^t(p_i^t , y_i,_p^t,_z^t ) , + _ j^1 & = & g_x^t(r_j^t , _ r^t,_x^t ) .",
    "this non - adaptive algorithm is precisely the standard gamp method analyzed in @xcite .",
    "the results in that paper show that the outputs of the non - adaptive algorithm satisfy all the required limits from the se analysis .",
    "that is , @xmath233 where @xmath234 and @xmath235 are the sets generated by the non - adaptive gamp algorithm : @xmath236    the limits are now proven through a continuity argument that shows that the adaptive and non - adaptive quantities must asymptotically agree with one another . specifically , we will start by proving that the following limits holds almost surely for all @xmath237          the proof of the limits   and   is achieved by an induction on @xmath26 . although we only need to show the above limits for @xmath110 , most of the arguments hold for arbitrary @xmath45 .",
    "we thus present the general derivation where possible .    to begin the induction argument , first note that the non - adaptive algorithm has the same initial conditions as the adaptive algorithm .",
    "thus the limits   and   hold for @xmath159 and @xmath240 , respectively .",
    "we now proceed by induction .",
    "suppose that @xmath241 and the limits   and   hold for some @xmath26 and @xmath242 , respectively . since @xmath10 has i.i.d .",
    "components with zero mean and variance @xmath167 , it follows from the marenko - pastur theorem @xcite that that its @xmath243-norm operator norm is bounded .",
    "that is , there exists a constant @xmath244 such that [ eq : matrixbnd ] _",
    "n _ k c_a , _",
    "n ^_k c_a .",
    "this bound is the only part of the proof that specifically requires @xmath110 . from ,",
    "we obtain @xmath245 where ( a ) is due to the norm inequality @xmath246 . since @xmath247 , we have that for any positive numbers @xmath248 and @xmath249 [ eq : pbndtwo ] ( a + b)^k 2^k(a^k + b^k ) . applying the inequality into",
    ", we obtain @xmath250 now , since @xmath251 and @xmath252 are the outputs of the non - adaptive algorithm they satisfy the limits        to prove  , we first prove the empirical convergence of @xmath257 to @xmath88 . towards this end , let @xmath258 be any pseudo - lipschitz continuous function @xmath212 of order @xmath76 .",
    "then @xmath259\\right| } \\nonumber \\\\      & \\leq \\frac{1}{m } \\sum_{i = 1}^m \\left|\\phi(p_i^{t},y_i ) -          \\phi(\\tilde{p}_i^{t},y_i )   \\right| \\nonumber\\\\          & + \\left| \\frac{1}{m } \\sum_{i = 1}^m \\phi(\\tilde{p}_i^{t},y_i )           -\\exp\\left[\\phi(p^t , y)\\right]\\right|   \\nonumber \\\\      & { { \\overset{\\mathrm{(a)}}{\\leq } } } \\frac{l}{m } \\sum_{i = 1}^m \\left(1 + |p_i^t|^{k-1 } + |\\tilde{p}_i^t|^{k-1 }   + |y_i|^{k-1 } \\right)|p_i^t - \\tilde{p}_i^t| \\nonumber \\\\      & + \\left| \\frac{1}{m } \\sum_{i = 1}^m \\phi(\\tilde{p}_i^{t},y_i ) -\\exp\\left[\\phi(p^t , y)\\right]\\right| \\nonumber \\\\      & { { \\overset{\\mathrm{(b)}}{\\leq } } } lc \\delta_p^t+\\left| \\frac{1}{m } \\sum_{i = 1}^m \\phi(\\tilde{p}_i^{t},y_i ) -\\exp\\left[\\phi(p^t , y)\\right]\\right| .",
    "\\label{eq : phipybnd}\\end{aligned}\\ ] ] in ( a ) we use the fact that @xmath212 is pseudo - lipschitz , and in ( b ) we use hlder s inequality @xmath260 with @xmath261 and define @xmath262 as @xmath263^{k/(k-1 ) } } \\nonumber \\\\      & \\leq \\frac{1}{m}\\sum_{i=1}^m \\left (      1 + |p_i^t|^{k-1 } + |\\tilde{p}_i^t|^{k-1 }   + |y_i|^{k-1}\\right)^{k/(k-1 ) } \\nonumber \\\\      & \\leq \\mbox{const}\\x\\left [ 1 +      \\left(\\frac{1}{m}\\left\\|{\\mathbf{p}}^t\\right\\|_k^k\\right)^{\\frac{k-1}{k } } \\right . \\nonumber\\\\      & \\left.+ \\left(\\frac{1}{m}\\left\\|\\porc^t\\right\\|_k^k\\right)^{\\frac{k-1}{k } } +      \\left(\\frac{1}{m}\\left\\|{\\mathbf{y}}\\right\\|_k^k\\right)^{\\frac{k-1}{k } } \\right ] ,      \\label{eq : cpbnddef}\\end{aligned}\\ ] ] where the first step is from jensen s inequality .",
    "since @xmath264 satisfy the limits for the non - adaptive algorithm we have :    [ eq : ptildeybnd ] @xmath265 < \\infty \\\\      \\lim_{n \\rightarrow \\infty } \\frac{1}{m}\\|{\\mathbf{y}}\\|_k^k & =       \\lim_{n \\rightarrow \\infty } \\frac{1}{m}\\sum_{i = 1}^m |y_i|^k       = \\exp\\left[|y|^k\\right ] < \\infty\\end{aligned}\\ ] ]    also , from the induction hypothesis  , it follows that the adaptive output must satisfy the same limit @xmath266 < \\infty.\\ ] ] combining  , , ,  ,   we conclude that for all @xmath237 [ eq : pyconv ] _",
    "n ( ^t , ) ( p^t , y ) . the limit along with and the continuity condition on @xmath20 in assumption [ as : gamp](d ) prove the limit in  .",
    "the limit   together with continuity conditions on @xmath28 in assumptions [ as : gamp ] show that  ,   and   hold for @xmath26 .",
    "for example , to show  , we consider the limit @xmath267 of the following expression @xmath268 where at ( a ) we used the lipschitz continuity assumption .",
    "similar arguments can be used for   and  .    to show  , we proceed exactly as for  . due to the continuity assumptions on @xmath269 ,",
    "this limit in turn shows that   holds almost surely .",
    "then ,   and follow directly from the continuity of @xmath154 in assumptions [ as : gamp ] , together with   and  .",
    "we have thus shown that if the limits   and   hold for some @xmath26 , they hold for @xmath270 .",
    "thus , by induction they hold for all @xmath26 .    finally , to show  , let @xmath212 be any pseudo - lipschitz continuous function @xmath271 , and define @xmath272\\right|,\\end{aligned}\\ ] ] which",
    ", due to convergence of non - adaptive gamp , can be made arbitrarily small by choosing @xmath35 large enough .",
    "then , consider @xmath273\\right| \\nonumber\\\\ & \\leq \\epsilon_n^t + \\frac{1}{n } \\sum_{j = 1}^n \\left|\\phi(x_j , \\hat{r}_j^{t } , \\xhat_j^{\\tp1 } ) - \\phi(x_j , \\tilde{r}_j^{t } , \\tilde{x}_j^{\\tp1 } )   \\right| \\nonumber\\\\ & { { \\overset{\\mathrm{(a)}}{\\leq } } } \\epsilon_n^t + l\\|{\\mathbf{r}}^t - \\rorc^t\\|_1 + l\\|\\hat{{\\mathbf{x}}}^{\\tp1 } - \\xorc^{\\tp1}\\|_1 \\nonumber\\\\ & + \\frac{l^\\prime}{n}\\sum_{j = 1}^n \\left(|\\hat{r}_j^t|^{k-1 } + |\\tilde{r}_j^t|^{k-1 } \\right)(|\\hat{r}_j^t - \\tilde{r}_j^t| + |\\xhat_j^{\\tp1 } - \\tilde{x}_j^{\\tp1}| ) \\nonumber\\\\ & +   \\frac{l^\\prime}{n}\\sum_{j = 1}^n \\left(|\\xhat_j^{\\tp1}|^{k-1 } + |\\tilde{x}_j^{\\tp1}|^{k-1}\\right)(|\\hat{r}_j^t - \\tilde{r}_j^t| + |\\xhat_j^{\\tp1 } - \\tilde{x}_j^{\\tp1}| ) \\nonumber\\\\ & { { \\overset{\\mathrm{(b)}}{\\leq } } } \\epsilon_n^t + l\\left(\\delta_r^t\\right)^{\\frac{1}{k } } + l\\left(\\delta_x^t\\right)^{\\frac{1}{k}}\\nonumber\\\\ & + l^{\\prime}\\left(\\delta_r^t\\right)^{\\frac{1}{k}}\\left((\\tilde{m}_x^{\\tp1})^{\\frac{k-1}{k}}+(\\hat{m}_x^{\\tp1})^{\\frac{k-1}{k } } + ( \\tilde{m}_r^{t})^{\\frac{k-1}{k}}+(\\hat{m}_r^{t})^{\\frac{k-1}{k}}\\right)\\nonumber\\\\ & + l^{\\prime}\\left(\\delta_x^t\\right)^{\\frac{1}{k } } \\left((\\tilde{m}_x^{\\tp1})^{\\frac{k-1}{k}}+(\\hat{m}_x^{\\tp1})^{\\frac{k-1}{k } } + ( \\tilde{m}_r^{t})^{\\frac{k-1}{k}}+(\\hat{m}_r^{t})^{\\frac{k-1}{k}}\\right ) \\label{eq : xbound } \\ ] ] where @xmath12 , @xmath274 are constants independent of @xmath35 and @xmath275 in ( a ) we use the fact that @xmath212 is pseudo - lipshitz , in ( b ) we use @xmath276-norm equivalence @xmath277 and hlder s inequality @xmath260 with @xmath261 . by applying of  ,   and since",
    ", @xmath278 , @xmath279 , @xmath280 , and @xmath281 converge to a finite value we can obtain the first equation of   by taking @xmath41 .",
    "the second equation in   can be shown in a similar way .",
    "this proves the limits .    also , the first two limits in are a consequence of and .",
    "the second two limits follow from continuity assumptions in assumption [ as : gamp](e ) and the convergence of the empirical distributions in .",
    "this completes the proof .",
    "part ( a ) of theorem  [ thm : consistent ] is a direct application of the general result , theorem [ sec : stateevo ] . to apply the general result ,",
    "first observe that assumptions  [ as : agamp - ml](a ) and ( c ) immediately imply the corresponding items in assumptions [ as : agamp ] .",
    "so , we only need to verify the continuity condition in assumption [ as : agamp](b ) for the adaptation functions in and .",
    "we begin by proving the continuity of @xmath20 .",
    "fix @xmath26 , and let @xmath282 be a sequence of vectors and @xmath283 be a sequence of scalars such that [ eq : yptaulim ]",
    "_ n ( ^(n),^(n ) ) ( y , p^t ) _ n _ p^(n ) = ^t_p , where @xmath284 and @xmath285 are the outputs of the state evolution equations . for each @xmath35 , let [ eq : lamhatcons ] _ z^(n ) = h_z^t(^(n),^(n),_p^(n ) ) .",
    "we wish to show that @xmath286 , the true parameter .",
    "since @xmath287 and @xmath18 is compact , it suffices to show that , any limit point of any convergent subsequence is equal to @xmath288 .",
    "so , suppose that @xmath289 to some limit point @xmath290 on some subsequence @xmath291 .    from @xmath291 and the definition",
    "it follows that @xmath292 now , since @xmath293 and @xmath289 , we can apply the continuity condition in definition  [ def : pzident](c ) to obtain @xmath294 \\geq 0.\\end{aligned}\\ ] ] also , the limit and the fact that @xmath128 is psuedo - lipschitz continuous of order @xmath76 implies that [ eq : phizcompc ] .",
    "but , property ( b ) of definition  [ def : pzident ] shows that @xmath288 is the maxima of the right - hand side , so [ eq : phizcompd ] = . since , by definition  [ def : pzident](b ) , the maxima is unique , @xmath295 .",
    "since this limit point is the same for all convergent subsequences , we see that @xmath296 over the entire sequence . we have thus shown that given limits , the outputs of the adaptation function converge as @xmath297 thus , the continuity condition on @xmath103 in assumption [ as : agamp](b ) is satisfied .",
    "the analogous continuity condition on @xmath102 can be proven in a similar manner .",
    "so , it remains to show parts ( b ) and ( c ) of theorem  [ thm : consistent ] .",
    "we will only prove ( b ) ; the proof of ( c ) is similar .",
    "also , since we have already established , we only need to show that the output of the se equations matches the true parameter .",
    "that is , we need to show @xmath298 .",
    "this fact follows immediately from the selection of the adaptation functions : + & & _ _ x _ x _ ( _ r,_r ) s_x(_r^t ) + & & _ _ x _ x _ ( _ r,_r ) s_x(_r^t ) + & & + & & _ x^ * [ eq : lamxlim ] where ( a ) follows from the se equation ; ( b ) is the definition of the ml adaptation function @xmath299 when interpreted as a functional on a random variable @xmath67 ; ( c ) is the definition of the random variable @xmath67 in where @xmath300 ; and ( d ) follows from definition  [ def : pxident](b ) and the hypothesis that @xmath301 .",
    "thus , we have proven that @xmath298 , and this completes the proof of part ( b ) of theorem [ thm : consistent ] .",
    "the proof of part ( c ) is similar ."
  ],
  "abstract_text": [
    "<S> we consider the estimation of an i.i.d .  </S>",
    "<S> ( possibly non - gaussian ) vector @xmath0 from measurements @xmath1 obtained by a general cascade model consisting of a known linear transform followed by a probabilistic componentwise ( possibly nonlinear ) measurement channel . </S>",
    "<S> a novel method , called adaptive generalized approximate message passing ( adaptive gamp ) , that enables joint learning of the statistics of the prior and measurement channel along with estimation of the unknown vector @xmath2 is presented . </S>",
    "<S> the proposed algorithm is a generalization of a recently - developed em - gamp that uses expectation - maximization ( em ) iterations where the posteriors in the e - steps are computed via approximate message passing . </S>",
    "<S> the methodology can be applied to a large class of learning problems including the learning of sparse priors in compressed sensing or identification of linear - nonlinear cascade models in dynamical systems and neural spiking processes . </S>",
    "<S> we prove that for large i.i.d .  </S>",
    "<S> gaussian transform matrices the asymptotic componentwise behavior of the adaptive gamp algorithm is predicted by a simple set of scalar state evolution equations . in addition </S>",
    "<S> , we show that when a certain maximum - likelihood estimation can be performed in each step , the adaptive gamp method can yield asymptotically consistent parameter estimates , which implies that the algorithm achieves a reconstruction quality equivalent to the oracle algorithm that knows the correct parameter values . </S>",
    "<S> remarkably , this result applies to essentially arbitrary parametrizations of the unknown distributions , including ones that are nonlinear and non - gaussian . </S>",
    "<S> the adaptive gamp methodology thus provides a systematic , general and computationally efficient method applicable to a large range of complex linear - nonlinear models with provable guarantees . </S>"
  ]
}