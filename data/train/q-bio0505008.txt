{
  "article_text": [
    "noise is usually considered as having a corrupting effect on meaningful signals .",
    "there is however one well known counter example to this widespread belief ; the _ stochastic resonance _ ( sr ) phenomenon . in this case ,",
    "addition of a random interference signal ( noise ) to a weak , subthreshold stimuli , may enhance its detection .",
    "originally introduced in the framework of physics @xcite , stochastic resonance is now known to take place in several sensory systems from the cricket cercal sensory system @xcite , to crayfish mechanoreceptors @xcite , the somatosensory cortex of the cat @xcite and the human visual cortex @xcite in order to facilitate detection of weak signals ( for a review see @xcite ) .",
    "in addition there is evidence that stochastic resonance may be used not only at the level of sensory processing , but also in central cognitive processes . in a psychophysical study by usher and feingold ,",
    "the memory retrieval of arithmetical multiplication was found to be enhanced by the addition of noise @xcite .    in this article",
    ", we feed a simplified model of a cortical column with two time - varying input signals .",
    "we first show that for a broad set of computation based on those input signals , addition of a small amount of noise enhances the computational power of the system .",
    "the location where the noise strength is optimal lies approximately where the network reacts the strongest to a change in the mean input ( maximum susceptibility ) .",
    "we then set the connectivity to zero ( with appropriate scaling of the statistics of the input to the neurons ) .",
    "although the simplest task ( addition of both input signals ) can be achieved with a similar accuracy to that obtained with a connected network , a multiplicative task can only be solved if the population of neurons is connected .",
    "the stochastic resonance effect is thus seen to take place at the _ system level _",
    "@xcite rather than at the single cell level .",
    "it is an emergent property of the neuronal assemblies .",
    "we consider in our simulations networks of @xmath0 cells ( leaky integrate - and - fire neurons ) . the connectivity matrix is fixed and every neuron receives input from @xmath1 excitatory and @xmath2 inhibitory presynaptic neurons randomly chosen among the @xmath3 neurons in the network .",
    "the system is made out of @xmath4 excitatory and @xmath5 inhibitory neurons , reflecting the ratio of pyramidal cells to interneurons in cortical tissue .",
    "this excess of excitatory neurons is approximately balanced by the greater efficacy of synaptic transmission for inhibition ; in our model six times bigger than for excitation : @xmath6mv and @xmath7mv .",
    "this approximate balance between excitation and inhibition is thought to take place at a functional level in cortical areas @xcite .",
    "sparsely - connected networks of spiking neurons of this type have been fully described in terms of their dynamical behaviour @xcite .",
    "the dynamics of the leaky integrate - and - fire neurons is described by the following equation : @xmath8 where @xmath9 describes the membrane potential of neuron @xmath10 with respect to its equilibrium value @xmath11 , @xmath12ms corresponds to its effective membrane time constant and @xmath13 is the effective input resistance of the neuron stimulated by a total input @xmath14 .",
    "we add to this equation a threshold condition ; if the membrane potential of the neuron exceeds the critical value @xmath15mv , a spike is emitted and after a refractory period @xmath16ms , the integration start again from the reset potential @xmath17mv . every time the neuron @xmath10 receives an action potential from a presynaptic pyramidal cell @xmath18 ( resp .",
    "interneuron ) , its membrane potential is depolarized according to the value of the synaptic efficacy for excitation @xmath19 ( resp . for inhibition",
    ", the neuron is hyperpolarized by an amount @xmath20 ) .",
    "the input a neuron @xmath10 will get from within the network can thus be written as : @xmath21 where @xmath22 is the ensemble of presynaptic neurons , @xmath23 the time neuron @xmath18 fires its @xmath24th spike and @xmath25ms is a short transmission delay .",
    "we can now decompose the external stimulation @xmath26 into two contributions .",
    "first , we model all noise sources , ranging from synaptic bombardment from neurons outside the network to different sources of noise diversely located at the level of synaptic transmission , channel gating , ion concentrations , membrane conductance to name but a few .",
    "all these noise sources are grouped into a term @xmath27 with a mean depolarisation @xmath28 and a white noise component defined by its standard deviation @xmath29 .",
    "we can define the _ susceptibility _ @xmath30 of the network as the sensitivity of the population spiking rate @xmath31 upon a change in the mean depolarisation @xmath28 : @xmath32 .",
    "we constrained our simulation space to small mean depolarisation @xmath28 so that in absence of the noisy component @xmath33 , the network exhibits no spiking activity ( subthreshold regime ) .     and @xmath34 .",
    "in addition two randomly generated subpopulations , each composed of @xmath5 of the total number of neurons , receive test inputs @xmath35 and @xmath36 .",
    "the readout @xmath37 _ sees _ all neurons in the network and is trained to match a function of the test input @xmath38 and @xmath39 ( see the methods section for details ) .",
    ", width=309 ]    then , we inject two test signals @xmath35 and @xmath36 , each to @xmath5 randomly chosen neurons in the network .",
    "both inputs share the same statistical properties ; they are constant over a time interval @xmath40ms and then they switch to new randomly chosen values and remain constant for the next time interval @xmath41 . at each transition time , the new values are chosen uniformly over the interval @xmath42 $ ] .",
    "we want to know how performance in a series of computational tasks based on both test signals @xmath35 and @xmath36 are affected by the noise level .",
    "we adapt a paradigm introduced in the framework of liquid state machines @xcite or echo state networks @xcite : we consider a readout with a dynamics @xmath43 where the sums run over all firing times @xmath44 of all neurons in the network .",
    "@xmath45ms is a short synaptic time constant .",
    "the @xmath46 free parameters @xmath47 ( @xmath48 ) are chosen so as to minimize the signal reconstruction error between the readout and the target : @xmath49 ^ 2\\rangle$ ] . in our simulations we fixed @xmath50 so that the transient period after a transition has vanished .",
    "the set of functions @xmath51 that are under consideration in the present study are ; the addition @xmath52 , the multiplication @xmath53 which plays a crucial role in the transformation of object locations from retinal to body - centered coordinates @xcite and two polynomials of degree two @xmath54 and @xmath55 , which can be seen as the nonlinear xor paradigm @xcite .",
    "parameters were optimized using a first simulation ( learning set ) lasting 100 seconds ( 100000 time steps of simulation ) and were kept fixed afterwards .",
    "the performance measurements reported in this paper are then evaluated on a second simulation of 100 seconds ( test set ) .",
    "we compare our results to a simple two parameter readout .",
    "such a readout only adjusts to the mean of the time series @xmath56 .",
    "the reconstruction error for such a readout equals the variance @xmath57 of the time series @xmath56 .",
    "the performance with the full readout are therefore expressed as a gain ( in percent ) over the trivial prediction : @xmath58 , where @xmath59 is the error introduced above .    in the simulation of fig.[control ] , all connections in the network were removed . in order to compensate the loss of input @xmath60 , we changed the background input @xmath27 so that the neurons receive an input with the same statistical properties ( mean and variance ) .",
    "the adapted input in the network with  no connection  ( nc ) is thus : @xmath61 for the mean and @xmath62 for the variance term .",
    "@xmath31 is the mean population rate in the connected network ( see @xcite ) .",
    "simulation results were obtained using the simulation software nest .",
    "we want to know how noise influences the ability of a recurrent network of spiking neurons to process information and to perform a series of computational tasks .",
    "in particular we are interested in effects related to stochastic resonance .",
    "we therefore relate the optimal noise level ( where the system has a maximum performance ) to dynamical properties of the network . from that perspective , we analyze how performance in a series of computational tasks based on both test signals @xmath35 and @xmath36 are affected by the noise level . in a first series of simulations , we measure the gain over the trivial prediction for three different functions of the test signals ; @xmath52 , @xmath63 and @xmath64 ( see figure [ gradient ] , respectively top right , bottom left and bottom right graphs ) .",
    "note that the function @xmath64 can be seen as an implementation of the xor task , which can not be solved by a single layer neural network ( perceptron ) @xcite . in all three tasks ,",
    "the network exhibits stochastic resonance ; for a given mean depolarisation @xmath28 , there is a non - monotonic dependence upon the noise level .",
    "the maximum gain compared to trivial prediction reaches @xmath65 for the additive task and @xmath66 for polynomials of degree two .",
    "a comparison to the map of the susceptibility of the network @xmath67 ( see top left graph of figure [ gradient ] ) indicated that the tasks are solved best when the sensitivity of the network upon changes in the mean input is highest .",
    "addition of noise both increases the susceptibility of the network and the capacity of performing complex computation based on sparse inputs .     in ( hz / pa ) of the network as a function of the statistical properties of the external drive ( mean and variance ) .",
    "a high _ susceptibility _ ( light colors ) defines a network that is highly sensitive to a change in the mean of the drive .",
    "top right and bottom : gain ( in percent ) over the trivial prediction as a function of the mean and variance of the external drive ; for the simple additive task @xmath68 ( top right ) , for a first polynomial of degree two @xmath69 ( bottom left ) and a second polynomial of degree two @xmath70 ( bottom right ) .",
    "level curves are shown for the sake of clarity at @xmath71 and @xmath72 for the polynomials and at @xmath73 and @xmath74 for the addition .",
    "the system exhibits stochastic resonance for all three different tasks .",
    "in addition , the location where the performance peak approximately corresponds to the zone of high _ susceptibility_.,width=415 ]    in order to know whether stochastic resonance displayed in the network is an emergent property of the population of neuron @xcite or a single cell effect , we removed all connections within the network . in this second series of simulations ,",
    "we compare the performance achieved in networks with connectivity to networks with no connectivity .",
    "the latter networks receive adapted version of the mean input @xmath75 and of the noisy input @xmath76 so that every neurons is stimulated with a mean and variance equivalent to that of the connected network ( see methods ) .    in the first task ; the addition @xmath52 , the performance with and without connectivity are similar ( see figure [ control ] left ) .",
    "since the readout unit performs a weighted _ sum _ that runs over all neurons , it can capture the essence of this simple computation by summing the averaged response of the groups of neurons receiving input @xmath38 with the averaged response of those receiving @xmath39 .",
    "recurrent loops play here no significative role . in the second task",
    "we train the network to perform the multiplication of the two test signals @xmath53 .",
    "this arithmetical operation is thought to be essential to the brain in order to do coordinate transformation @xcite .",
    "the computation of a multiplication by a recurrent neural network was shown to be achievable in a model of the parietal cortex @xcite . in this multiplicative task @xmath53",
    ", the complex recurrent network outperforms the network with no connectivity ( see figure [ control ] right ) .",
    "in fact , in absence of connections within the population of neurons , multiplication can not be solved by the simple addition of noise .",
    "stochastic resonance displayed in the multiplicative task therefore takes place at a _ system _",
    "level rather than at the level of single neurons .    ; for the connected network ( solid ) and for the control network , when the connectivity is set to zero ( dashed ) .",
    "the unconnected collection of neurons is capable of solving this simple task with a similar accuracy as the randomly connected neural network .",
    "right : gain ( in percent ) over the trivial prediction for the multiplicative task @xmath78 ; for the reference network ( solid ) and when the connectivity is set to zero ( dashed ) . in absence of recurrence",
    ", the network is no longer able to sustain complex computations .",
    "stochastic resonance is thus a _ population - based _",
    "effect rather than a _ single - cell _ phenomenon.,width=309 ]",
    "complex networks of neurons fall in the class of non linear systems with a threshold ; systems that are known to exhibit stochastic resonance . from the experimental side , evidences have shown that the phenomenon helps in detecting sensory signals of small amplitude , and furthermore to favor high level cognitive processes such as arithmetical calculations .",
    "our model has revealed the presence of stochastic resonance in a series of neural - based computation . whereas simple additive transformations of input signals can be solved by a collection of independent neurons ,",
    "more complex computations need the massive recurrence typically observed in cortical tissue .",
    "such complex tasks include the xor problem , a nonlinear benchmark test , and the arithmetical multiplication the brain is likely to use in order to achieve coordinate transformation .",
    "stochastic resonance displayed is then an emergent property of the brain microcircuitry ."
  ],
  "abstract_text": [
    "<S> varied sensory systems use noise in order to enhance detection of weak signals . </S>",
    "<S> it has been conjectured in the literature that this effect , known as _ stochastic resonance _ , may take place in central cognitive processes such as the memory retrieval of arithmetical multiplication . </S>",
    "<S> we show in a simplified model of cortical tissue , that complex arithmetical calculations can be carried out and are enhanced in the presence of a stochastic background . </S>",
    "<S> the performance is shown to be positively correlated to the susceptibility of the network , defined as its sensitivity to a variation of the mean of its inputs . </S>",
    "<S> for nontrivial arithmetic tasks such as multiplication , stochastic resonance is an emergent property of the microcircuitry of the model network .    </S>",
    "<S> _ keywords : _ stochastic resonance , information processing , recurrent neural network +   + _ contact author : wulfram.gerstner@epfl.ch_ </S>"
  ]
}