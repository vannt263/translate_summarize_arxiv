{
  "article_text": [
    "big data analytics is a major topic in contemporary data management and machine learning research and practice .",
    "many platforms , e.g. , optiml  @xcite , graphlab  @xcite , systemml  @xcite , vowpal wabbit  @xcite , simsql  @xcite , glade  @xcite and libraries , e.g. , madlib @xcite , bismarck  @xcite , mllib  @xcite , mahout , have been proposed to provide support for distributed / parallel statistical analytics .",
    "we can categorize these solutions into general frameworks with machine learning support  mahout , spark s mllib , graphlab  dedicated machine learning systems  systemml , simsql , optiml , vowpal wabbit  and frameworks within databases  madlib , bismarck , glade . in this paper , we focus on the last category  frameworks for in - databse analytics .",
    "a common assumption across all these systems is that the number of model parameters or features is small enough to fit in memory .",
    "this is made explicit by the representation of the model as an in - memory array data structure .",
    "however , due to the explosive growth in data acquisition and the wide adoption of analytics methods , the current trend is to devise models with an ever - increasing number of features_big models_. a report on industrial machine learning cites models with 100 billion features ( 800 gb in size ) as early as 2012 .",
    "scientific applications such as ab initio nuclear structure calculations also generate extremely large models with billions of features  @xcite . while these are extreme cases , there are many realistic applications that require big models and",
    "are forced to limit the number of features they consider because of insufficient memory resources .",
    "we provide two such examples in the following .",
    "* example 1 : recommender systems . *",
    "a class of analytics models with highly - dimensional feature space are the ones in which the dimensionality grows with the number of observations .",
    "low - rank matrix factorization ( lmf ) is a typical example . in lmf ,",
    "the observations are a set of cells in a sparse @xmath0 matrix @xmath1 .",
    "the non - empty cells represent the users ratings of items in a certain category  such as songs or movies  with each row corresponding to a user and each column to an item . in general , every row is sparse since a typical user rates only a very limited set of items .",
    "each column is also sparse since only a restricted number of users rate an item .",
    "lmf seeks to decompose matrix @xmath1 into the product of two dense low - rank matrices @xmath2 and @xmath3 with dimensions @xmath4 and @xmath5 , respectively , where @xmath6 is the rank .",
    "the prediction accuracy increases with @xmath6 .",
    "the lmf features are matrices @xmath2 and @xmath3 which grow with the number of users @xmath7 and the number of items @xmath8 , respectively , and the rank @xmath6 .",
    "lmf is heavily used in recommender systems , e.g. , netflix , pandora , spotify .",
    "for example , spotify applies lmf for 24 million users and 20 million songs , which leads to 4.4 billion features at a relatively small rank of 100 .",
    "* example 2 : topic modeling . *",
    "n - grams are a common feature vector in topic modeling .",
    "they are extracted by considering sequences of 1-word tokens ( unigrams ) , 2-word tokens ( bigrams ) , up to n - word tokens ( n - grams ) from a fixed dictionary .",
    "the feature vector consists of the union of unigrams , bigrams , @xmath9 , n - grams .",
    "several analytics models can be applied over this feature space , including latent dirichlet allocation , logistic regression , and support vector machines .",
    "for the english wikipedia corpus , a feature vector with 25 billion unigrams and 218 billion bigrams can be constructed  @xcite .",
    "a similar scale can be observed in genomics where topic modeling is applied to genome sequence analysis .    * existing solutions . * the standard model representation across all the big data analytics systems we are aware of  in - database or not",
    " is a memory - resident container data structure , e.g. , ` vector ` or ` map ` .",
    "depending on the parallelization strategy , there can be one or more model instances in the system at the same time .",
    "hogwild !",
    "@xcite uses a single non - synchronized instance , while averaging techniques  @xcite replicate the model for each execution thread . at the scale of models introduced above",
    ", a memory - resident solution incurs prohibitive cost  if it is feasible at all . in reality ,",
    "in - database analytics frameworks can not handle much smaller models .",
    "for example , madlib and bismarck are built using the udf - uda functionality available in postgresql .",
    "the model is stored as an array attribute in a single - column table .",
    "postgresql imposes a hard constraint of 1 gb for the size of an attribute , effectively limiting the model size .",
    "high performance computing ( hpc ) libraries for efficient sparse linear algebra such as intel mkl , trilinos , cusparse , and cusp are optimized exclusively for in - memory processing , effectively requiring that both the training dataset and the model fit in memory simultaneously .",
    "two approaches are possible to cope with insufficient memory  secondary storage processing and distributed memory processing . in secondary storage processing",
    ", the model is split into partitions large enough to fit in memory and the goal is to optimize the access pattern in order to minimize the number of references to secondary storage .",
    "this principle applies between any two layers of the storage hierarchy  cache and memory , memory and disk ( or ssd ) , and texture memory and global memory of a gpu . while we are not aware of any secondary storage solution for data analytics , there have been several attempts to optimize the memory access pattern of the spmv kernel .",
    "however , they are targeted at minimizing the number of cache misses  @xcite or the number of accesses to the gpu global memory  @xcite  not the number of disk accesses .    in distributed memory processing ,",
    "the big model is partitioned across several machines , with each machine storing a sufficiently small model partition that fits in its local memory . since remote model access requires data transfer ,",
    "the objective in distributed processing is to minimize the communication between machines .",
    "this can not be easily achieved for the spmv kernel due to the non - clustered access pattern .",
    "distributed big data analytics systems built around the map - reduce computing paradigm and its generalizations , e.g. , hadoop and spark , require several rounds of repartitioning and aggregation due to their restrictive communication pattern .",
    "to the best of our knowledge , parameter server  @xcite is the only distributed memory analytics system capable of handling big models directly . in parameter server ,",
    "the big model can be transfered and replicated across servers .",
    "whenever a model entry is accessed , a copy is transferred over the network and replicated locally .",
    "modifications to the model are pushed back to the servers asynchronously .",
    "the communication has to be implemented explicitly and optimized accordingly .",
    "while parameter server supports big models , it does so at the cost of a significant investment in hardware and with considerable network traffic .",
    "our focus is on cost - effective single node solutions .",
    "* approach & contributions . * in this paper , we propose an in - database solution for big model analytics .",
    "the main idea is to offload the model to secondary storage and leverage database techniques for efficient training .",
    "the model is represented as a table rather than as an array attribute .",
    "this distinction in model representation changes fundamentally how in - database analytics tasks are carried out .",
    "we identify _ dot - product _ as the most critical operation affected by the change in model representation .",
    "our central contribution is the first _ dot - product join physical database operator _",
    "optimized to execute secondary storage array - relation dot - products effectively .",
    "dot - product join is a constrained instance of the spmv kernel  @xcite which is widely - studied across many computing areas , including hpc , architecture , and compilers .",
    "the paramount challenge we have to address is how to optimally schedule access to the dense relation  buffer management  based on the non - contiguous feature entries in the sparse arrays .",
    "the goal is to minimize the overall number of secondary storage accesses across all the sparse arrays .",
    "we prove that this problem is np - hard and propose a practical solution characterized by two technical contributions .",
    "the first contribution is to handle the sparse arrays in _ batches with variable size_determined dynamically at runtime .",
    "the second contribution is a _ reordering strategy _ for the arrays such that accesses to co - located entries in the dense relation can be shared .",
    "our specific contributions can be summarized as follows :    we investigate the big model analytics problem and identify dot - product as the critical operation in training generalized linear models ( section  [ sec : problem ] ) .",
    "we also establish a direct correspondence with the well - studied sparse matrix vector ( spmv ) multiplication problem .",
    "we present several alternatives for implementing dot - product in a relational database and discuss their relative advantages and drawbacks ( section  [ sec : baseline ] ) .",
    "we design the first array - relation dot - product join database operator targeted at secondary storage ( section  [ sec : dot - product ] ) .",
    "we prove that identifying the optimal access schedule for the dot - product join operator is np - hard ( section  [ ssec : reordering : np - hard ] ) and introduce two optimizations  dynamic batch processing and reordering  to make the operator practical .",
    "we devise three batch reordering heuristics ",
    "lsh , radix , and k - center ( section  [ sec : reordering ] )  inspired from optimizations to the spmv kernel and evaluate them thoroughly .",
    "we show how the dot - product join operator is integrated in the gradient descent optimization pipeline for training generalized linear models ( section  [ sec : dp - gd ] ) .",
    "we execute an extensive set of experiments that evaluate each sub - component of the operator and compare our overall solution with alternative dot - product implementations over synthetic and real data ( section  [ sec : experiments ] ) .",
    "the results show that dot - product join achieves an order of magnitude reduction in execution time over alternative in - database solutions .",
    "in this section , we provide a brief introduction to gradient descent as a general method to train a wide class of analytics models .",
    "then , we give two concrete examples  logistic regression and low - rank matrix factorization  that illustrate how gradient descent optimization works . from these examples ,",
    "we identify _ vector dot - product _ as the primary operation in gradient descent optimization .",
    "we argue that existing in - database solutions can not handle big model analytics because they do not support secondary storage dot - product .",
    "we provide a formal statement of the research problem studied in this paper and conclude with a discussion on the relationship with spmv .",
    "consider the following optimization problem with a linearly separable objective function : @xmath10 in which a @xmath11-dimensional vector @xmath12 , @xmath13 , known as the model , has to be found such that the objective function is minimized .",
    "the constants @xmath14 and @xmath15 , @xmath16 , correspond to the feature vector of the @xmath17 data example and its scalar label , respectively .",
    "gradient descent represents  by far  the most popular method to solve the class of optimization problems given in eq.([eq : optim - form ] ) .",
    "gradient descent is an iterative optimization algorithm that starts from an arbitrary model @xmath18 and computes new models @xmath19 , such that the objective function , a.k.a",
    ". , the loss , decreases at every step , i.e. , @xmath20 .",
    "the new models @xmath19 are determined by moving along the opposite @xmath21 gradient direction .",
    "formally , the @xmath21 gradient is a vector consisting of entries given by the partial derivative with respect to each dimension , i.e. , @xmath22 $ ] .",
    "the length of the move at a given iteration is known as the step size  denoted by @xmath23 . with these ,",
    "we can write the recursive equation characterizing the gradient descent method : @xmath24 in batch gradient descent ( bgd ) , the update equation is applied as is .",
    "this requires the exact computation of the gradient  over the entire training dataset . to increase",
    "the number of steps taken in one iteration , stochastic gradient descent ( sgd ) estimates the @xmath21 gradient from a subset of the training dataset .",
    "this allows for multiple steps to be taken in one sequential scan over the training dataset  as opposed to a single step in bgd .    in the following ,",
    "we provide two illustrative examples that show how gradient descent works for two popular analytics tasks  logistic regression ( lr ) and low - rank matrix factorization ( lmf )",
    ".    * logistic regression . *",
    "the lr objective function is : @xmath25 the model @xmath12 that minimizes the cumulative log - likelihood across all the training examples is the solution .",
    "the key operation in this formula is the dot - product between vectors @xmath12 and @xmath14 , i.e. , @xmath26 .",
    "this dot - product also appears in each component of the gradient : @xmath27    * low - rank matrix factorization . *",
    "the general form of the lmf objective function is : @xmath28 two low - rank matrices @xmath29 and @xmath30 , i.e. , the model , have to be found such that the sum of the cell - wise least - squares difference between the data matrix @xmath31 and the product @xmath32 is minimized .",
    "@xmath33 is the rank of matrices @xmath2 and @xmath3 .",
    "the lmf gradient as a function of row @xmath34 in matrix @xmath35 is shown in the following formula : @xmath36 there is such a dot - product between row @xmath37 and each column @xmath38 , i.e. , @xmath39 . however , only those dot - products for which there are non - zero cells @xmath40 in matrix @xmath1 have to be computed .",
    "a similar gradient formula can be written for column @xmath41 in @xmath42 .    from these examples ,",
    "we identify dot - product as the essential operation in gradient descent optimization . with almost no exception",
    ", dot - product appears across all analytics tasks . in lr and lmf",
    ", a dot - product has to be computed between the model @xmath12 and each example @xmath43 in the training dataset  at each iteration .",
    "formally , the dot - product of two @xmath11-dimensional vectors @xmath44 and @xmath45 is a scalar value computed as : @xmath46 this translates into a simple algorithm that iterates over the two vectors synchronously , computes the component - wise product , and adds it to a running sum ( algorithm  [ alg : dp - form ] ) . in most cases ,",
    "the @xmath47 function is a simple multiplication .",
    "however , in the case of lmf , where @xmath48 and @xmath49 are vectors themselves , @xmath47 is itself a dot - product .",
    "@xmath50    @xmath51 @xmath52 @xmath53      in this paper , we focus on _ big model dot - product_. several aspects make this particular form of dot - product an interesting and challenging problem at the same time .",
    "first , the vector corresponding to the model ",
    "say @xmath45  can not fit in the available memory .",
    "second , the vector corresponding to a training example ",
    "say @xmath44  is sparse and the number of non - zero entries is a very small fraction from the dimensionality of the model . in these conditions , algorithm _ dot - product _ becomes highly inefficient because it has to iterate over the complete model even though only a small number of entries contribute to the result .",
    "the third challenge is imposed by the requirement to support big model dot - product inside a relational database where there is no notion of order  the relational model is unordered . the only solution to implement algorithm _",
    "dot - product _ inside a database is through the udf - uda extensions which process a single tuple at a time .",
    "the operands @xmath44 and @xmath45 are declared as table attributes having ` array ` type , while the dot - product is implemented as a udf . for reference ,",
    "the existing in - database analytics alternatives treat the model as an in - memory uda state variable  @xcite .",
    "this is impossible in big model analytics .",
    "the research problem we study in this paper is _ how to design a database operator for big model dot - product_. given a dataset consisting of sparse training examples ( set of vectors ` u ` = @xmath54 in figure  [ fig : dp - operator ] ) and a dense model ( vector ` v ` in figure  [ fig : dp - operator ] ) , this operator computes the set of dot - products between the examples and the model ( vector ` dp ` in figure  [ fig : dp - operator ] ) optimally .",
    "an entry @xmath55 corresponds to the dot - product @xmath56 .",
    "following the notation in section  [ ssec : problem : grad - descent ] , @xmath57 corresponds to @xmath43 and vector ` v ` corresponds to @xmath12 .",
    "since the model ` v ` can not fit in - memory , optimality is measured by the total number of secondary storage accesses .",
    "this is a good indicator for the execution time , given the simple computation required in dot - product .",
    "in addition to the main objective , we include a functional constraint in the design_results have to be generated in a non - blocking fashion_. as soon as the dot - product @xmath58 corresponding to a training example @xmath57 is computed , it is made available to the calling operator / application .",
    "this requirement is essential to support sgd ",
    "the gradient descent solution used the most in practice .    * relationship between big model dot - product and spmv . *",
    "if we ignore the non - blocking functional constraint , the result vector ` dp ` is the product of sparse matrix ` u ` and vector ` v ` , i.e. , @xmath59 .",
    "this is the standard spmv kernel which is notorious for sustaining low fractions of peak performance and for which there is a plethora of algorithms proposed in the literature .",
    "however , none of these algorithms considers the case when vector ` v ` does not fit in memory .",
    "they focus instead on the higher levels of the storage hierarchy , i.e. , memory  cache  @xcite and global memory  texture memory in gpu  @xcite , respectively .",
    "while some of the solutions can be extended to disk - based spmv , they are applicable only to bgd optimization which is less often used in practice .",
    "in sgd , ` v ` is updated after computing the dot - product with every vector @xmath57 , i.e. , row of matrix ` u ` .",
    "this renders all the spmv algorithms inapplicable to big model dot - product .",
    "we propose novel algorithms for this constrained version of spmv .",
    "since this is a special instance of spmv , the proposed solution is applicable to the general spmv problem .",
    "in this section , we present two database solutions to the big model dot - product problem .",
    "first , we introduce a pure relational solution that treats the operands ` u ` and ` v ` as standard relations .",
    "second , we give an ` array`-relation solution that represents each of the vectors @xmath57 in ` u ` as an ` array ` data type  similar to existing in - database frameworks such as madlib and bismarck .",
    "the relational solution represents ` u ` and ` v ` as relations and uses standard relational algebra operators , e.g. , join , group - by , and aggregation  and their sql equivalent  to compute the dot - product . in order to represent vectors @xmath60 in relational format",
    ", we create a tuple for each non - zero dimension .",
    "the attributes of the tuple include the index and the actual value in the vector .",
    "moreover , since the components of a vector are represented as different tuples , we have to add another attribute identifying the vector , i.e. , _ tid_. the schema for the dense vector ` v ` is obtained following the same procedure .",
    "there is no need for a vector / tuple identifier _ tid _ , though , since ` v ` contains a single tuple , i.e. , the model :    u(index integer , value numeric , tid integer ) v(index integer , value numeric )    based on this representation , relation ` u ` corresponding to figure  [ fig : dp - operator ] contains 19 tuples .",
    "the tuples for @xmath61 are @xmath62 @xmath63 . relation ` v ` contains a tuple for each index .",
    "an alternative representation is a wide relation with an attribute corresponding to each dimension / index .",
    "however , tables with such a large number of attributes are not supported by any of the modern relational databases .",
    "the above representation is the standard procedure to map ordered vectors into non - ordered relations at the expense of data redundancy  the tuple identifier for sparse vectors and the index for dense vectors .",
    "the dot - product vector ` dp ` is computed in the relational data representation with the following standard _ join group - by aggregate _",
    "sql statement :    select u.tid , sum(u.value*v.value ) from u , v where u.index=v.index group by u.tid    this relational solution supports vectors of any size .",
    "however , it breaks the original vector representation since vectors are treated as tuples .",
    "moreover , the relational solution can not produce the result dot - product @xmath64 in a non - blocking manner due to the blocking nature of each operator in the query .",
    "this is not acceptable for sgd .      in",
    "the ` array`-relation join solution , the sparse structure of ` u ` is preserved by representing each vector @xmath57 with two ` array ` attributes , one for the non - zero index and another for the value :    u(index integer[],value numeric[],tid integer )    the vector identifier _",
    "tid _ is still required .",
    "this representation is the database equivalent of the coordinate representation  @xcite for sparse matrices and is possible only in database servers with type extension support , e.g. , postgresql .",
    "vector ` v ` is decomposed as in the relational solution .",
    "the sql query to compute dot - product ` dp ` in postgresql is :    select u.tid , sum(u.value[idx(u.index , v.index)]*v.value ) from u , v where v.index = any(u.index ) group by u.tid    there are several differences with respect to the relational solution .",
    "the equi - join predicate in ` where ` is replaced with an inclusion predicate ` v.index in u.index [ ] ` testing the occurrence of a scalar value in an array .",
    "this predicate can be evaluated efficiently with an index on ` v.index ` .",
    "the expression in the ` sum ` aggregate contains an indirection based on ` v.index ` which requires specialized support . while the size of the join is the same , the width of the tuples in ` array`-relation join is considerably larger ",
    "the size of the intermediate join result table is the size of u multiplied by the average number of non - zero entries across the vectors @xmath57  since the entire ` array ` attributes are replicated for every result tuple .",
    "this is necessary because the arrays are used in the ` sum ` .",
    "finally , the ` array`-relation join remains blocking , thus , it is still not applicable to sgd .",
    "in this section , we present the dot - product join operator for big model dot - product computation .",
    "essentially , dot - product join combines the advantages of the existing database solutions .",
    "it is an in - database operator over an ` array`-relation data representation that computes the result vector without blocking and without generating massive intermediate results . in this section",
    ", we present the requirements , challenges , and design decisions behind the dot - product join operator , while a thorough evaluation is given in section  [ sec : experiments ] .",
    "the dot - product join operator takes as input the operands ` u ` and ` v ` and generates the dot - product vector ` dp ` ( figure  [ fig : dp - operator ] ) . `",
    "u ` follows the ` array ` representation in which the index and value are stored as sparse ` array`-type attributes . only the non - zero entries are materialized . `",
    "v ` is stored in range - based partitioned relational format ",
    "the tuples ` ( index , value ) ` are partitioned into _ pages _ with fixed size and a page contains tuples with consecutive ` index ` values .",
    "the overall number of pages in ` v ` is @xmath65 .",
    "for example , with a page size of @xmath66 , vector ` v ` in figure  [ fig : dp - operator ] is partitioned into @xmath67 pages .",
    "page @xmath68 contains indexes @xmath69 , page @xmath66 indexes @xmath70 , and page @xmath67 indexes @xmath71 .",
    "the memory budget available to the dot - product join operator for storing ` v ` is @xmath1 pages , which is smaller than @xmath65this is a fundamental constraint .    without loss of generality",
    ", we assume that each vector @xmath72 accesses at most @xmath1 pages from ` v ` .",
    "this allows for atomic computation of entries in vector ` dp ` , i.e. , all the data required for the computation of the entry @xmath55 are memory - resident at a given time instant .",
    "moreover , each vector @xmath57 has to be accessed only once in order to compute its contribution to ` dp``u ` can be processed as a stream . as a result",
    ", the cost of big model dot - product computation is entirely determined by the number of secondary storage page accesses for ` v`the execution of algorithm _ dot - product _ is considerably faster than accessing a page from secondary storage .",
    "thus , the main challenge faced by the dot - product join operator is _ minimizing the number of secondary storage page accesses _ by judiciously using the memory budget @xmath1 for caching pages in ` v ` .",
    "a vector @xmath72 contains several non - zero entries .",
    "each of these entries requires a request to retrieve the value at the corresponding index in ` v ` .",
    "thus , the number of requests can become a significant bottleneck even when the requested index is memory - resident .",
    "_ reducing the number of requests _ for entries in ` v ` is a secondary challenge that has to be addressed by the dot - product join operator .    in order to illustrate these challenges ,",
    "we extend upon the example in figure  [ fig : dp - operator ] .",
    "the total number of requests to entries in ` v ` is @xmath73@xmath67 for @xmath61 , @xmath66 for @xmath74 , and so on .",
    "the corresponding number of page accesses to ` v ` when we iterate over ` u ` is @xmath75@xmath66 pages are accessed by each vector @xmath57 .",
    "this is also the number of requests when we group together requests to the same page . with a memory budget @xmath76 and lru cache replacement policy",
    ", the number of pages accessed from secondary storage reduces to @xmath77 .",
    "however , the number of requests remains as before , i.e. , @xmath75 .",
    "algorithm _ dot - product join operator _ contains a high - level presentation of the proposed operator for big model dot - product .",
    "intuitively , we push the aggregation inside the ` array`-relation join and apply a series of optimizations that address the challenges identified above  minimize the number of secondary storage accesses and the number of requests to entries in ` v ` . given `",
    "u ` , ` v ` , and memory budget @xmath1 , _ dot - product join operator _ iterates over the pages of ` u ` and executes a three - stage procedure at _ page - level_. the three stages are : optimization , batch execution , and dot - product computation .",
    "optimization minimizes the number of secondary storage accesses .",
    "batch execution reduces the number of requests to ` v ` entries .",
    "dot - product computation is a direct call to _ dot - product _ that computes an entry of the _ dp _ result .",
    "notice that generating intermediate join results and grouping are not required anymore since the entries in ` v ` corresponding to a vector @xmath57 are produced in a contiguous batch . in the following ,",
    "we give an overview of the optimization and batch execution stages .",
    "+ u ( index integer[],value numeric[],tid integer ) + v ( index integer , value numeric ) + memory budget @xmath1 dp ( tid integer , product numeric )    reorder vectors @xmath57 to cluster similar vectors together[alg : dot - product : reorder ] group vectors @xmath57 into batches @xmath78 that access at most @xmath1 pages from v[alg : dot - product : batching ]    collect pages @xmath79 accessed by vectors @xmath80 into a set @xmath81[alg : dot - product : batch-1 ] request access to pages in @xmath82[alg : dot - product : batch-2 ]    [ alg : dot - product : batch-3 ] dp @xmath83 _ dot - product_(@xmath57,v ) append ( @xmath84 , dp ) to dp [ alg : dot - product : batch-4 ]      the naive adoption of the ` array`-relation solution inside the dot - product join operator does not address the number of secondary storage accesses to pages in ` v ` .",
    "the database buffer manager is entirely responsible for handling secondary storage access .",
    "the standard policy to accomplish this is lru  the least recently accessed page is evicted from memory .",
    "this can result in a significantly suboptimal behavior , as shown in figure  [ fig : dp - flow ] . `",
    "u ` is the same as in figure  [ fig : dp - operator ] and @xmath76 . as discussed before ,",
    "the number of secondary storage page accesses is @xmath77 .",
    "essentially , every vector @xmath72 , except @xmath85 , requires a page replacement .",
    "this is because consecutive vectors access a different set of two pages .",
    "the net result is significant _ buffer or cache thrashing_a page is read into memory only to be evicted at the subsequent request .",
    "one can argue that the access pattern in this example is the worst - case for lru .",
    "the big model dot - product problem , however , exhibits this pattern pervasively because each dot - product computation is atomic . allowing partial dot - product computation",
    "incurs a different set of obstacles and does not satisfy the non - blocking requirement .",
    "thus , it is not a viable alternative for our problem .",
    "@xmath86 in figure  [ fig : dp - flow ] contains exactly the same vectors as ` u ` , reordered such that similar vectors ",
    "vectors with non - zero entries co - located in the same partition / page of ` v `  are clustered together .",
    "the number of secondary storage accesses corresponding to @xmath86 is only @xmath87 .",
    "the reason for this considerable reduction is that a ` v ` page  once read into memory ",
    "is used by several vectors @xmath57 that have non - zero indexes resident to the page .",
    "essentially , vectors share access to the common pages .",
    "* reordering . *",
    "the main task of the optimization stage is to _ identify the optimal reordering of vectors @xmath72 that minimizes the number of secondary storage accesses to ` v`_line  ( [ alg : dot - product : reorder ] ) in algorithm  [ alg : dp ] .",
    "we prove that this task is np - hard by a reduction from the minimum hamiltonian path problem .",
    "we propose three heuristic algorithms that find good reorderings in a fraction of the time  depending on the granularity at which the reordering is executed , e.g. , a page , several pages , or a portion of a page .",
    "the first algorithm is an lsh - based extension to nearest neighbor  the well - known approximation to minimum hamiltonian path .",
    "the second algorithm is partition - level radix sort .",
    "the third algorithm is standard k - center clustering applied at partition - level .",
    "we discuss these algorithms in section  [ sec : reordering ] .",
    "vector reordering is based on permuting a set of rows and columns of a sparse matrix in order to improve the performance of the spmv kernel .",
    "permuting is a common optimization technique used in sparse linear algebra .",
    "the most common implementation is the reverse cuthill - mckee algorithm ( rcm )  @xcite which is widely used for minimizing the maximum distance between the non - zeros and the diagonal of the matrix .",
    "since the rcm algorithm permutes both rows and columns , it incurs an unacceptable computational cost . to cope with this , vector reordering limits permuting only to the rows of the matrix .",
    "we show that even this simplified problem is np - hard and requires efficient heuristics .",
    "* batching . *",
    "the second task of the optimization stage is to _ reduce the number of page requests to the buffer manager_. even when the requested page is in the buffer , there is a non - negligible overhead in retrieving and passing the page to the dot - product join operator .",
    "a straightforward optimization is to group together requests made to indexes co - located in the same page . by applying this strategy to the example in figure  [ fig : dp - flow ] ,",
    "the number of requests in @xmath86 is reduced to @xmath75compared to @xmath73 in ` u ` .",
    "we take advantage of the reordering to group consecutive vectors into batches and make requests at batch - level  line  ( [ alg : dot - product : batching ] ) in algorithm  [ alg : dp ] .",
    "this strategy is beneficial because similar vectors are grouped together by the reordering .",
    "for example , the number of page requests corresponding to @xmath88 in figure  [ fig : dp - flow ] is only @xmath89@xmath66 for each batch .",
    "formally , we have to identify the batches that minimize the number of page requests to ` v ` given a fixed ordering of the vectors in ` u ` and a memory budget @xmath1 . without the ordering constraint , this is the standard bin packing problem  known to be np - hard . due to ordering , a simple greedy heuristic that iterates over the vectors and adds them to the current batch until the capacity constraint is not satisfied",
    "is guaranteed to achieve the optimal solution .",
    "the output is a set of batches with a variable number of vectors .",
    "given the batches @xmath78 extracted in the optimization stage , we compute the dot - products @xmath56 by making a single request to the buffer manager for all the pages accessed in the batch .",
    "it is guaranteed that these pages fit in memory at the same time .",
    "notice , though , that vectors @xmath57 are still processed one - by - one , thus , dot - product results are generated individually ( lines  ( [ alg : dot - product : batch-3])-([alg : dot - product : batch-4 ] ) in algorithm  [ alg : dp ] ) . while batch execution reduces the number of requests to the buffer manager , the requests consist of a set of pages  not a single page .",
    "it is important to emphasize that decomposing the set request into a series of independent single - page requests is sub - optimal .",
    "for example , consider ` u ` in figure  [ fig : dp - flow ] to consist of @xmath77 batches of a single vector each .",
    "vector @xmath90 has a request for page 1 and 2 , respectively .",
    "page 2 and 3 are in memory .",
    "if we treat the 2-page request as two single - page requests , page 2 is evicted to release space for page 1 , only to be immediately read back into memory . if the request is considered as a unit , page 3 is evicted .",
    "in order to support set requests , the functionality of the buffer manager has to be enhanced . instead of directly applying the replacement policy , e.g. , lru , there are two stages in handling a page set request .",
    "first , the pages requested that are memory - resident have to be pinned down such that the replacement policy does not consider them .",
    "second , the remaining requested pages and the non - pinned buffered pages are passed to the standard replacement policy . _ dot - product join operator _",
    "includes this functionality in lines ( [ alg : dot - product : batch-1])-([alg : dot - product : batch-2 ] ) , before the dot - product computation .",
    "in this section , we discuss strategies for reordering vectors @xmath72 that minimize the number of secondary storage accesses .",
    "first , we prove that finding the optimal reordering is np - hard .",
    "then , we introduce three scalable heuristics that cluster similar vectors .",
    "we formalize the reordering problem as follows .",
    "assume there are @xmath91 @xmath11-dimensional vectors @xmath54 .",
    "each vector @xmath57 contains @xmath92 page requests grouped into a set @xmath93 .",
    "given two consecutive vectors @xmath57 , @xmath94 and their corresponding page request sets @xmath95 , @xmath96 , the pages in the set difference @xmath97 potentially require secondary storage access . since we assume that all the pages in any set @xmath95 fit in memory , it is guaranteed that the pages in the set intersection @xmath98 are memory resident .",
    "pages in @xmath96 that are not in @xmath95 can be in memory if they have been accessed before and not evicted .",
    "let us denote the set difference between any two vectors @xmath57 and @xmath99 as @xmath100 .",
    "the cardinality of this set @xmath101 gives the number of potential secondary storage accesses .",
    "the goal of reordering is to identify the vector sequence @xmath102 that minimizes the cumulative cardinality @xmath103 of the set difference between all the pairs of consecutive vectors : @xmath104    [ thm : reordering - np - hard ] finding the vector sequence @xmath102 that minimizes the cumulative cardinality @xmath105 is np - hard .",
    "we provide a reduction of the minimum hamiltonian path in a weighted directed graph ",
    "a known np - complete problem  @xcite  to the reordering problem . given a weighted directed graph ",
    "the edges are directional and have a weight attached  the minimum hamiltonian path traverses all the vertexes once and the resulting path has minimum weight .",
    "we define a complete directed graph @xmath106 with @xmath91 vertexes and @xmath107 edges .",
    "a vertex corresponds to each vector @xmath57 .",
    "for any pair of vertexes , i.e. , vectors @xmath57 , @xmath99 , we introduce two edges @xmath108 , @xmath109 and @xmath110 , @xmath111 , respectively . the weight of an edge @xmath108 ,",
    "@xmath109 is given by the cardinality @xmath112 .",
    "notice that set difference is not commutative .",
    "this is the reason for having directed edges .",
    "a hamiltonian path in graph @xmath113 corresponds to a reordering because all the vectors are considered only once and all the orders are possible  there is an edge between any two vertexes .",
    "since the weight of an edge is defined as the cardinality @xmath101 , the weight of the minimum hamiltonian path corresponds to the minimum cumulative cardinality in eq .",
    "( [ eq : optim - order ] ) .",
    "several polynomial - time heuristic algorithms for computing an approximation to the minimum hamiltonian path over a complete directed graph exist in the literature  @xcite .",
    "nearest neighbor is a standard greedy algorithm that chooses the closest non - visited vertex as the next vertex .",
    "it has computational complexity of @xmath114 and non - constant approximation factor @xmath115 .",
    "the straightforward application of the nearest neighbor algorithm to our problem is too expensive because we have to materialize a complete directed graph .",
    "this takes significant time even for values of @xmath91 in the order of thousands  not to mention the required space .",
    "if we generate the graph on - the - fly , the weight to all the non - visited vertexes has to be computed .",
    "while this may seem more efficient , this still has an overall computational complexity of @xmath116 . due to these limitations of the nearest neighbor algorithm for complete graphs ,",
    "we explore more efficient heuristics that avoid considering all the possible pairs of vertexes .",
    "locality - sensitive hashing ( lsh )  @xcite is an efficient method to identify similar objects represented as high - dimensional sparse vectors .",
    "similarity between vectors is defined as their jaccard coefficient , i.e. , @xmath117 .",
    "the main idea is to build a hash index that groups similar vectors in the same bucket . given an input vector , the most similar vector is found by identifying the vector with the maximum jaccard coefficient between the vectors co - located in the same hash bucket . essentially , the search space is pruned from all the vectors in the set to the vectors in the hash bucket  typically considerably fewer .",
    "the complexity of lsh consists in finding a hash function that preserves the jaccard coefficient in mapping vectors to buckets  the probability of having two vectors in the same bucket approximates their jaccard coefficient .",
    "minwise hash functions  @xcite satisfy this property on expectation .",
    "given a set , a minwise hash function generates any permutation of its elements with equal probability .",
    "while such functions are hard to define , they can be approximated with a universal hash function  @xcite that has a very large domain , e.g. , @xmath118 . in order to increase the accuracy of correctly estimating the jaccard coefficient , @xmath7 such minwise hash functions",
    "are applied to a vector .",
    "their output corresponds to the signature of the vector which gives the bucket where the vector is hashed to .",
    "the value of @xmath7 is an important parameter controlling the number of vectors which are exhaustively compared to the input query vector .",
    "the larger @xmath7 is , the fewer vectors end - up in the same bucket . on the opposite , if @xmath119 , all the vectors that share a common value can be hashed to the same bucket . given a value for @xmath7 , banding is a method that controls the degree of tolerated similarity .",
    "the @xmath7-dimensional signature is divided into @xmath120 @xmath121-dimensional bands and a hash table is built independently for each of them .",
    "the input vector is compared with all the co - located vectors of at least one hash table . banding decreases the jaccard coefficient threshold acceptable for similarity , while increasing the probability that all the vectors that have a higher coefficient than the threshold are found .",
    "+ set of vectors @xmath54 with page requests + @xmath7 minwise hash functions grouped into @xmath120 bands reordered set of input vectors @xmath102    compute @xmath7-dimensional signature @xmath122 based on minwise hash functions and group into @xmath120 bands @xmath123 insert vector @xmath57 into hash table @xmath124 , @xmath125 , using minwise hash function of @xmath126    initialize @xmath127 with a random vector @xmath57 let @xmath128 be the set of vectors co - located in the same bucket with @xmath129 in hash table @xmath124 and not selected @xmath130 let @xmath131 be the vector in @xmath132 with the minimum set difference cardinality @xmath103 to the current vector @xmath129    _ compute lsh tables _ section in algorithm  [ alg : lsh ] summarizes the construction of the lsh index for the vector reordering problem . although we do not measure similarity using the jaccard coefficient , there is a strong correlation between set difference cardinality and the jaccard coefficient .",
    "intuitively , the higher the jaccard coefficient , the larger the intersection between two sets relative to their union .",
    "this translates into small set difference cardinality . since the goal of reordering is to cluster vectors with small differences , lsh places them into the same bucket with high probability .",
    "we compute the output vector reordering by executing the nearest neighbor heuristic over the lsh index ( section _ lsh - based nearest neighbor search _ in algorithm  [ alg : lsh ] ) .",
    "we believe this is a novel application of the lsh technique , typically used for point queries .",
    "the algorithm starts with a random vector .",
    "the next vector is selected from the vectors co - located in the same bucket across at least one other band of the lsh index .",
    "the process is repeated until all the vectors are selected .",
    "bands play a very important role in reordering because they allow for smooth bucket transition .",
    "this is not possible with a single band since there is no strict ordering between buckets .",
    "in this situation , choosing the next bucket involves inspecting all the other buckets of the hash table .",
    "in general , lsh reduces significantly the number of vector pairs for which the exact set difference has to be computed  only @xmath133 vector pairs are considered , i.e. , a constant number for each vector .",
    "thus , the overall complexity of _ lsh reordering _ ",
    "@xmath134  is dominated by the construction of the lsh index .",
    "we illustrate how lsh reordering works for the set of vectors @xmath135 depicted in figure  [ fig : dp - flow ] . to facilitate understanding , we set @xmath136 and @xmath137 , i.e. , two lsh indexes with 1-d signatures . since @xmath138",
    ", there are many conflicts in each bucket of the two bands .",
    "even though this does not reduce dramatically the number of vector pairs that require full comparison , it shows how bucket transition works .",
    "let the two minwise hash functions generate the following permutations : @xmath139 and @xmath140 , respectively .",
    "remember that the reordering is done at page level .",
    "the lsh index for the first band has two buckets , for key @xmath141 and key @xmath142 .",
    "the lsh index for the second band also has two buckets , for key @xmath143 and key @xmath144 .",
    "let @xmath61 be the random vector we start the nearest neighbor search from .",
    "the vectors considered at the first step of the algorithm are @xmath145 .",
    "all of them are contained in bucket with key @xmath66 of the first band . since",
    "@xmath90 and @xmath146 have set difference 0 to @xmath61 , one of them is selected as @xmath147 and the other as @xmath148 . at this moment , the bucket with key @xmath68 from the second band is exhausted and one of @xmath74 , @xmath149 , and @xmath150 is selected .",
    "independent of which one is selected , bucket transition occurs since the new vectors @xmath151 are co - located in bucket with key @xmath67 of the second band . by following the algorithm to termination , @xmath152 in figure  [ fig : dp - flow ] is a possible solution .",
    "sorting is a natural approach to generate a reordering of a set as long as a strict order relationship can be defined between any two elements of the set .",
    "the jaccard coefficient gives an order only with respect to a fixed reference , i.e. , given a reference vector we can quantify which of any other two vectors is smaller based on their jaccard coefficient with the reference .",
    "however , this is not sufficient to generate a complete reordering .",
    "it is possible to imagine a multitude of ordering strategies for a set of sparse @xmath11-dimensional vectors .",
    "the simplest solution is to consider the dimensions in some arbitrary order , e.g. , from left to right .",
    "vector @xmath61 is smaller than @xmath74 , i.e. , @xmath153 , for @xmath135 in figure  [ fig : dp - flow ] since it has a non - zero entry at index @xmath68 , while the first non - zero entry in @xmath74 is at index @xmath67 . in order to cluster similar vectors together , a better ordering",
    "is required .",
    "our strategy is to sort the dimensions according to the frequency at which they appear in the set of sparse vectors and compare vectors dimension - wise based on this order .",
    "this is exactly how radix sort  @xcite works , albeit without reordering the dimensions in descending order of their frequency .",
    "a similar idea is proposed for spmv on gpu in  @xcite .",
    "radix sort reordering _ depicts the entire process .",
    "the two stages  frequency computation and radix sort  are clearly delimited . since",
    "their complexity is @xmath154 , the overall is @xmath154 .",
    "set of vectors @xmath54 with page requests reordered set of input vectors @xmath102    compute page request frequency across vectors @xmath54 represent @xmath57 by a bitset of 0 s and 1 s where a 1 at index @xmath33 corresponds to the vector requesting page @xmath33 reorder the bitset in decreasing order of the page request frequency , i.e. , index 1 corresponds to the most frequent page    apply radix sort to the set of bitsets let @xmath129 be the vector corresponding to the bitset at position @xmath155 in the sorted order    we illustrate how radix sort reordering works for the set of vectors @xmath135 in figure  [ fig : dp - flow ] .",
    "the algorithm operates at page level .",
    "since page @xmath66 is the most frequent  it appears in @xmath89 vectors  it is the first considered .",
    "two partitions are generated : @xmath156 accesses page @xmath66 ; and @xmath157 does not .",
    "this is exactly @xmath152 in figure  [ fig : dp - flow ] .",
    "the frequency of page @xmath68 and page @xmath67 is @xmath158 , thus , any of them can be selected in the second iteration .",
    "assume that we brake the ties using the original index and we select page @xmath68 .",
    "each of the previous two partitions is further split into two . for example , @xmath159 and @xmath160 .",
    "the important thing to remark is that vectors accessing page @xmath68 split in the first iteration can not be grouped together anymore . if we follow the algorithm to completion , @xmath152 in figure  [ fig : dp - flow ] is generated .",
    "the intuition behind radix sort reordering is that  by considering pages in decreasing order of their frequency  the most requested page is accessed from secondary storage exactly once ; the second most accessed page at most twice ; and so on .",
    "this holds true because all the pages accessed by a vector fit together in memory .",
    "it is important to notice that  although the number of accesses increases  the request frequency decreases for pages considered at later iterations .",
    "this guarantees that the maximum number of accesses to a page is bounded by @xmath161 , where _ rank _ is the rank of the page and _ freq _ is the access frequency .",
    "essentially , radix sort reordering is a dimension - wise greedy heuristic .",
    "since the goal of reordering is to cluster similar vectors together , we include a standard k - center clustering  @xcite algorithm as a reference .",
    "the main challenge is that  similar to the jaccard coefficient  clustering does not impose a strict ordering between two vectors  only with respect to the common center .",
    "this is also true for centers .",
    "the stopping criterion for our hierarchical k - center clustering is when all the clusters have page requests that fit entirely in memory .",
    "as long as this does not hold , we invoke the algorithm recursively for the clusters that do not satisfy this requirement .",
    "the resulting clusters are ordered as follows .",
    "we start with a random cluster and select as the next cluster the one with the center having the minimum set difference cardinality .",
    "notice that reordering the vectors inside a cluster is not necessary since they all fit in memory .",
    "this procedure is depicted in algorithm  [ alg : k - center ] .",
    "it has complexity @xmath162 , where @xmath33 is the resulting number of clusters , i.e. , centers .",
    "set of vectors @xmath54 with page requests reordered set of input vectors @xmath102    initialize first set of @xmath33 centers with random vectors @xmath57 assign each vector to the center having the minimum set difference cardinality let @xmath163 be the set of vectors assigned to center @xmath164 , @xmath165 call _ k - center reordering _",
    "recursively for the sets @xmath163 with requests that do not fit in memory    reorder centers and their corresponding vectors      we propose three heuristic algorithms for the vector reordering problem .",
    "lsh and k - center cluster similar vectors together .",
    "lsh uses the jaccard coefficient to hash similar vectors to the same bucket of a hash table and then executes nearest neighbor search starting from a random vector .",
    "k - center clustering partitions the vectors recursively based on an increasing set of centers .",
    "both methods are limited by partial ordering between two vectors and incur overhead to define a strict ordering .",
    "moreover , they are randomized algorithms sensitive to the initialization and a handful of parameters .",
    "radix sort imposes a strict ordering at the expense of not considering the entire vector in clustering .",
    "we alleviate this problem by sorting the dimensions based on their access frequency .",
    "this bounds the total number of secondary storage accesses . in the experimental evaluation ( section  [ sec : experiments ] )",
    ", we compare these algorithms thoroughly in terms of execution time and reordering quality .",
    "in this section , we show how to integrate the dot - product join operator in gradient descent optimization for big model analytics .",
    "we discuss the benefits of the operator approach compared to the relational and ` array`-relation solutions presented in section  [ sec : baseline ] .",
    "figure  [ fig : sgd - integration ] depicts the gradient computation required in gradient descent optimization .",
    "vector dot - product is only a subroutine in this process .",
    "as we move from the relational solution to the proposed dot - product join operator , the query plan becomes considerably simpler .",
    "the relational solution consists of two parts . in the first part",
    ", the dot - product corresponding to a vector @xmath57 is computed . since",
    "vector components are represented as independent tuples with a common identifier , this requires a group - by on _",
    "tid_. however , this results in the loss of the vector components , required for gradient computation ( see eq .",
    "( [ eq : lr - gradient ] ) ) .",
    "thus , a second join group - by is necessary in order to compute each component of the gradient .",
    "+   +   +    as we show in the experimental evaluation , this turns out to be very inefficient .",
    "the ` array`-relation solution is able to discard the second join group - by because it groups vector components as an array attribute of a tuple .",
    "the proposed dot - product join operator goes one step further and pushes the group - by aggregation inside the join . while this idea has been introduced in  @xcite for bgd over normalized example data , the dot - product join operator considers joins between examples and the model and works for bgd and sgd alike  sgd requires only an additional selection . in  @xcite ,",
    "the model @xmath166 is small enough to fit in the state of the uda .",
    "the main benefit of pushing the group - by aggregation inside the join is that the temporary join result is not materialized  in memory or on secondary storage .",
    "the savings in storage can be several orders of magnitude , e.g. , with an average of @xmath167 non - zero indexes per vector @xmath57 , the temporary storage  if materialized  is 3 orders of magnitude the size of @xmath135 . by discarding the blocking group - by on _ tid _",
    ", the overall gradient computation becomes non - blocking since dot - product join is non - blocking .",
    "* sgd considerations . * in order to achieve faster convergence , sgd requires random example traversals . since dot - product join reorders the examples in order to cluster similar examples together",
    ", we expect this to have a negative effect on convergence . however , the reordering in dot - product join is only local  at page - level .",
    "thus , a simple strategy to eliminate the effect of reordering completely is to estimate the gradient at page - level  the number of steps in an iteration is equal to the number of pages .",
    "any intermediate scheme that trades - off convergence speed with secondary storage accesses can be imagined . to maximize convergence ,",
    "the data traversal orders across iterations have to be also random .",
    "this is easily achieved with lsh and k - center reordering  two randomized algorithms .",
    "a simple solution for radix is to randomize the order of pages across iterations .",
    "we provide experimental evidence that quantifies the impact of radix reordering  the strictest solution  on convergence .",
    "figure  [ fig : sgd - convergence ] depicts the behavior of the loss function when 10 sgd iterations are executed for lr and lmf models over two real datasets ( see section  [ sec : experiments ] for details ) .",
    "radix reordering does not degrade the convergence speed compared to a random data traversal  the model is updated after each example .",
    "moreover , for the lr model , radix reordering actually improves convergence .",
    "these results are in line with those presented in  @xcite , where only complete data sorting on the label has a negative impact on convergence .",
    "in the section , we first evaluate the effectiveness and efficiency of the three reordering heuristics .",
    "then , we apply the reordering heuristics to dot - product join and measure the effect of reordering and batching under different resource constraints . finally , we measure the end - to - end dot - product and gradient descent execution time as a function of the amount of memory available in the system .",
    "we also compare dot - product join with the baseline alternatives introduced in section  [ sec : baseline ] across several synthetic and real datasets .",
    "specifically , the experiments we design are targeted to answer the following questions :    how effective are the reordering heuristics and which one should be used under what circumstance ?",
    "how do reordering and batching affect the runtime and what is their overhead ?    what is the sensitivity of the dot - product join operator with respect to the available memory ?",
    "how does dot - product join compare with other solutions ?    what is the contribution of dot - product join within the overall big model gradient descent optimization ?",
    "* implementation .",
    "* we implement dot - product join as a new array - relation operator in glade  @xcite  a state - of - the - art parallel data processing system that executes analytics tasks expressed with the uda interface .",
    "glade has native support for the ` array ` data type .",
    "dot - product join is implemented as an optimized index join operator .",
    "it iterates over the pages in ` u ` . for each page",
    ", it applies the reordering and batching optimizations and then probes the entries in ` v ` at batch granularity .",
    "the dot - product corresponding to a vector is generated in a single pass over the vector and pipelined into the gradient uda .",
    "* we execute the experiments on a standard server running ubuntu 14.04 smp @xmath168-bit with linux kernel 3.13.0 - 43 .",
    "the server has 2 amd opteron 6128 series 8-core processors  16 cores ",
    "28 gb of memory , and 1 tb 7200 rpm sas hard - drive .",
    "each processor has 12 mb l3 cache , while each core has 128 kb l1 and 512 kb l2 local caches .",
    "the average disk bandwidth is 120 mb / s .    * methodology .",
    "* we perform all experiments at least 3 times and report the average value as the result . in the case of page - level results , we execute the experiments over the entire dataset  all the pages  and report the average value computed across the pages",
    ". we always enforce data to be read from disk in the first iteration by cleaning the file system buffers before execution .",
    "memory constraints are enforced by limiting the batch size , i.e. , the number of vectors @xmath57 that can be grouped together after reordering .",
    ".datasets used in the experiments . [ cols=\"<,>,>,>,>\",options=\"header \" , ]      our experiments identify radix sort as the fastest reordering heuristic and the only one scalable to large batches .",
    "the improvement over tuple - at - a - time processing with basic lru replacement is larger in this case .",
    "while lsh achieves the largest reduction in page misses , radix is not far behind .",
    "the reduction in dot - product computation execution time is as much as an order of magnitude when reordering and batching are combined . independently and",
    "when integrated in sgd , the dot - product join operator s performance degrades gracefully when the memory budget reduces below a threshold at which the model can not be buffered in memory . out of all the alternatives ,",
    "the dot - product join operator is the only solution that can handle big models in a scalable fashion .",
    "the relational implementation in glade is  in general ",
    "an order of magnitude or more slower than dot - product join , while the relational and ` array`-relation postgresql versions and scidb do not finish execution even after @xmath169 hours  except for small models .",
    "monetdb achieves efficient execution time for small and intermediate model sizes with a massive memory budget .",
    "however , it fails miserably on truly large models  the problem addressed by dot - product join .",
    "* in - database analytics .",
    "* there has been a sustained effort to add analytics functionality to traditional database servers over the past years .",
    "madlib  @xcite is a library of analytics algorithms built on top of postgresql .",
    "it includes gradient descent optimization functions for training generalized linear models such as lr and lmf  @xcite .",
    "this is realized through the udf - uda extensions existent in postgresql .",
    "the model is represented as the state of the uda .",
    "it is memory - resident during an iteration and materialized as an array attribute across iterations .",
    "this is not possible for big model analytics because the model can not fit in memory and postgresql has strict limitations on the maximum size of an attribute .",
    "glade  @xcite follows a similar approach .",
    "distributed learning frameworks such as mllib  @xcite and vowpal wabbit  @xcite represent the model as a program variable and allow the user to fully manage its materialization .",
    "thus , they can not handle big models directly .",
    "since the vectors are memory - resident , the dot - product is computed by a simple invocation of _ algorithm dot - product_. in the case of madlib , this is done inside the uda code .",
    "computing dot - product and other linear algebra operations as udas is shown to be more efficient than the relational and stored procedure solutions for low - dimensional vectors in  @xcite .",
    "slacid  @xcite is a solution to implement sparse matrices and linear algebra operations inside a column - oriented in - memory database in which the emphasis is on the storage format . adding support for efficient matrix operations in distributed frameworks such as hadoop  @xcite and spark  @xcite has been also investigated recently",
    "we consider a more general problem  dot - product is a sub - part of matrix multiplication  in a centralized environment .",
    "moreover , gradient descent optimization does not involve matrix operations  only dot - product .    * big model parallelism . *",
    "parameter server  @xcite is the first system that addresses the big model analytics problem .",
    "their approach is to partition the model across the distributed memory of multiple _",
    "parameter servers_in charge of managing the model .",
    "the training vectors @xmath135 are themselves partitioned over multiple workers .",
    "a worker iterates over its subset of vectors and  for each non - zero entry  makes a request to the corresponding parameter server to retrieve the model entry .",
    "once all the necessary model entries are received , the gradient is computed , the model is updated and then pushed back to the parameter servers . instead of partitioning the model across machines , we use secondary storage .",
    "minimizing the number of secondary storage accesses is the equivalent of minimizing network traffic in parameter server .",
    "thus , the optimizations we propose  reordering and batching  are applicable in a distributed memory environment .",
    "they are not part of parameter server . in strads",
    "@xcite , parameter servers are driving the computation , not the workers .",
    "the servers select the subset of the model to be updated in an iteration by each worker . in our case",
    ", this corresponds to ignoring some of the non - zero entries in a vector @xmath57 to further reduce i / o .",
    "we plan to explore strategies to select the optimal parameter subset in the future .",
    "* learning over normalized data . * the integration of relational join with gradient , i.e. , dot - product , computation has been studied in  @xcite . however , the assumption made in all these papers is that the vectors @xmath57 are vertically partitioned along their dimensions .",
    "a join is required to put them together before computing the dot - product . in  @xcite",
    ", the dot - product computation is pushed inside the join and only applicable to bgd .",
    "the dot - product join operator adopts the same idea .",
    "however , this operator has to compute the dot - product which is still evaluated inside a uda in  @xcite .",
    "the join is dropped altogether in  @xcite when similar convergence is obtained without considering the dimensions in an entire vertical partition . a solution particular to linear regression",
    "is shown to be efficient to compute when joining factorized tables in  @xcite . in all these solutions ,",
    "the model is small enough to fit entirely in memory .",
    "moreover , they work exclusively for bgd",
    ".    * joins .",
    "* dot - product join is a novel type of join operator between an array attribute and a relation .",
    "we are not aware of any other database operator with the same functionality . from a relational perspective ,",
    "dot - product join is most similar to index join  @xcite .",
    "however , for every vector @xmath57 , we have to probe the index on model @xmath166 many times .",
    "thus , the number of probes can be several orders of magnitude the size of @xmath135 .",
    "the proposed techniques are specifically targeted at this scenario .",
    "the batched key access join in mysql is identical to our batching optimization applied at vector level . however , it handles a single probe per tuple and its reordering is aimed at generating sequential storage access for @xmath166 .",
    "array joins  @xcite are a new class of join operators for array databases .",
    "while it is possible to view dot - product join as an array join operator , the main difference is that we consider a relational system and push the aggregation inside the join .",
    "this avoids the materialization of the intermediate join result",
    ".    * spmv kernel . * as we have already discussed in the paper , the dot - product join operator is a constrained formulation of the standard sparse matrix vector ( spmv ) multiplication problem .",
    "specifically , the constraint imposes the update of the vector after each multiplication with a row in the sparse matrix .",
    "this makes impossible the direct application of spmv kernels to big model analytics  beyond bgd .",
    "moreover , we consider the case when the vector size goes beyond the available memory .",
    "spmv is an exhaustively studied problem with applications to high performance computing , graph algorithms , and analytics .",
    "an extended discussion of the recent work on spmv is presented in  @xcite  on which we draw in our discussion . @xcite and",
    "@xcite propose optimizations for spmv in multicore architectures , while  @xcite and  @xcite optimize distributed spmv for large scale - free graphs with 2d partitioning to reduce communication between machines .",
    "array databases such as scidb  @xcite and rasdaman  @xcite support spmv as calls to optimized linear algebra libraries such as intel mkl and trilinos .",
    "there has also been preliminary research on accelerating matrix multiplication with gpus  @xcite and ssds  @xcite , showing that speedups are limited by i / o and setup costs .",
    "none of these works store the vector in secondary storage .",
    "in this paper , we propose a database - centric solution to handle big models , where the model is offloaded to secondary storage .",
    "we design and implement the first dot - product join physical database operator that is optimized to execute secondary storage array - relation dot - products effectively .",
    "we prove that identifying the optimal access schedule for this operator is np - hard .",
    "we propose dynamic batching and reordering techniques to minimize the overall number of secondary storage accesses .",
    "we design three reordering heuristics ",
    "lsh , radix , and k - center  and evaluate them in terms of execution time and reordering quality .",
    "we experimentally identify radix as the most scalable heuristic with significant improvement over basic lru .",
    "the dot - product join operator s performance degrades gracefully when the memory budget reduces . out of all the alternatives ,",
    "the dot - product join operator is the only solution that can handle big models in a scalable fashion .",
    "alternative solutions are  in general  an order of magnitude or more slower than dot - product join .",
    "while the focus of the paper is on vector dot - product in the context of gradient descent optimization , the proposed solution is general enough to be applied to any spmv kernel operating over highly - dimensional vectors . essentially , the dot - product join operator is the first spmv kernel that accesses the vector from secondary storage .",
    "previous solutions have  at most  considered accessing the sparse matrix out - of - memory . from a database perspective , dot - product join is the first join operator between an array and a relation .",
    "existing joins between an array attribute and a relation are tremendously ineffective .",
    "the experimental results in the paper confirm this .",
    "thus , we find dot - product join extremely relevant in the context of polystores where multiple storage representations are combined inside a single system .",
    "dot - product join eliminates the conversion between representations necessary to execute hybrid operations .    in future work",
    ", we plan to include data parallelism in the dot - product join operator by partitioning the array over multiple threads that update the model concurrently .",
    "while lack of parallelism may be seen as a limitation of the dot - product join operator  especially in a big data setting  we argue that addressing the sequential problem first is a mandatory requirement in order to design a truly scalable parallel solution .",
    "we also plan to investigate partition strategies from spmv kernels to improve the disk access pattern to the vector further .",
    "since ssd storage has improved dramatically over the last years , storing the matrix and the vector on ssds is another topic for future research .",
    "finally , we plan to expand the range of models covered by dot - product join beyond lr and lmf .",
    "as long as a dot - product operation is required between highly - dimensional vectors , the dot - product join operator is a solution to consider ."
  ],
  "abstract_text": [
    "<S> big model analytics tackles the training of massive models that go beyond the available memory of a single computing device , e.g. , cpu or gpu . </S>",
    "<S> it generalizes big data analytics which is targeted at how to train memory - resident models over out - of - memory training data . in this paper , we propose an in - database solution for big model analytics . </S>",
    "<S> we identify dot - product as the primary operation for training generalized linear models and introduce the first array - relation dot - product join database operator between a set of sparse arrays and a dense relation . </S>",
    "<S> this is a constrained formulation of the extensively studied sparse matrix vector multiplication ( spmv ) kernel . </S>",
    "<S> the paramount challenge in designing the dot - product join operator is how to optimally schedule access to the dense relation based on the non - contiguous entries in the sparse arrays . </S>",
    "<S> we prove that this problem is np - hard and propose a practical solution characterized by two technical contributions  </S>",
    "<S> dynamic batch processing and array reordering . </S>",
    "<S> we devise three heuristics  lsh , radix , and k - center  for array reordering and analyze them thoroughly . </S>",
    "<S> we execute extensive experiments over synthetic and real data that confirm the minimal overhead the operator incurs when sufficient memory is available and the graceful degradation it suffers as memory becomes scarce . </S>",
    "<S> moreover , dot - product join achieves an order of magnitude reduction in execution time over alternative in - database solutions . </S>"
  ]
}