{
  "article_text": [
    "deep neural networks ( dnns ) have been successful in modeling many aspects of natural language including word meanings @xcite@xcite , machine translation @xcite , syntactic parsing @xcite , language modeling @xcite and image captioning @xcite . given sufficient training data , dnns are highly accurate and can be trained end - to - end without the need for intermediate knowledge representations or explicit feature extraction . with recent interest in conversational user interfaces such as virtual assistants and chatbots ,",
    "the application of dnns to facilitate meaningful conversations is an area where more progress is needed .",
    "while sequence to sequence models based on recurrent neural networks ( rnns ) have shown initial promise in creating intelligible conversations @xcite , it has been noted that more work is needed for these models to fully capture larger aspects of human communication including conversational goals , personas , consistency , context , and word knowledge .    since discourse analysis considers language at the conversation - level , including its social and psychological context , it is a useful framework for guiding the extension of end - to - end neural conversational models .",
    "drawing on concepts from discourse analysis such as _ coherence _ and _ cohesion _",
    "@xcite , we can codify what makes conversations more intelligent in order to design more powerful neural models that reach beyond the sentence and utterance level .",
    "for example , by looking for features that indicate deixis , anaphora , and logical consequence in the machine - generated utterances we can benchmark the level of coherence and cohesion with the rest of the conversation , and then make improvements to models accordingly .    in the long run ,",
    "if neural models can encode the long - range structure of conversations , they may be able to express conversational discourse similar to the way the human brain does , without the need for explicitly building formal representations of discourse theory into the model .    to that end",
    ", we explore rnn - based sequence to sequence architectures that can capture long - range relationships between multiple utterances in conversations and look at their ability to exhibit discourse relationships . specifically , we look at 1 ) a baseline rnn encoder - decoder with attention mechanism and 2 ) a model with an additional discourse rnn that encodes a sequence of multiple utterances .",
    "our contributions are as follows :    * we examine two rnn models with attention mechanisms to model discourse relationships across different utterances that differ somewhat compared to what has been done before * we carefully construct controlled experiments to study the relative merits of different models on multi - turn conversations * we perform a sensitivity analysis on how the amount of context provided by previous utterances affects model performance * we quantify how neural conversational models display coherence by measuring the prevalence of specific syntactical features indicative of deixis , anaphora , and logical consequence .",
    "building on work done in machine translation , sequence to sequence models based on rnn encoder - decoders were initially applied to generate conversational outputs given a single previous message utterance as input@xcite@xcite . in @xcite",
    "several models were presented that included a `` context '' vector ( for example representing another previous utterance ) that was combined with the message utterance via various encoding strategies to initialize or bias a single decoder rnn .",
    "some models have also included an additional rnn tier to capture the context of conversations .",
    "for example , @xcite includes a hierarchical `` context rnn '' layer to summarize the state of a dialog , while @xcite includes an rnn `` intension network '' to model conversation intension for dialogs involving two participants speaking in turn .",
    "modeling the `` persona '' of the participants in a conversation by embedding each speaker into a @xmath0-dimensional embedding was shown to increase the consistency of conversations in @xcite .",
    "formal representations such as rhetorical structure theory ( rst ) @xcite have been developed to identify discourse structures in written text .",
    "discourse parsing of cue phrases @xcite and coherence modeling based on co - reference resolution of named - entities @xcite@xcite have been applied to tasks such as summarization and text generation .",
    "lexical chains @xcite and narrative event chains @xcite provide directed graph models of text coherence by looking at thesaurus relationships and subject - verb - temporal relationships , respectively .",
    "recurrent convolutional neural networks have been used to classify utterances into discourse speech - act labels @xcite and hierarchical lstm models have been evaluated for generating coherent paragraphs in text documents @xcite .    our aim is to develop end - to - end neural conversational models that exhibit awareness of discourse without needing a formal representation of discourse relationships .      since conversations are sequences of utterances and utterances are sequences of words , it is natural to use models based on an rnn encoder - decoder to predict the next utterance in the conversation given @xmath1 previous utterances as source input .",
    "we compare two types of models : * seq2seq+a * , which applies an attention mechanism directly to the encoder hidden states , and * nseq2seq+a * , which adds an additional rnn tier with its own attention mechanism to model discourse relationships between @xmath1 input utterances .    in both cases",
    "the rnn decoder predicts the output utterance and the rnn encoder reads the sequence of words in each input utterance .",
    "the encoder and decoder each have their own vocabulary embeddings .    as in @xcite",
    "we compute the attention vector at each decoder output time step @xmath2 given an input sequence @xmath3 using : @xmath4 where the vector @xmath5 and matrices @xmath6 , and @xmath7 are learned parameters .",
    "@xmath8 is the decoder state at time @xmath2 and is concatenated with @xmath9 to make predictions and inform the next time step . in * seq2seq+a * the @xmath10 are the hidden states of the encoder @xmath11 , and for * nseq2seq+a * they are the @xmath1 hidden states of the discourse rnn ( see fig . [ schematic ] . )",
    "therefore , in * seq2seq+a * the attention mechanism is applied at the word - level , while in * nseq2seq+a * attention is applied at the utterance - level .          as a baseline starting point",
    "we use an attention mechanism to help model the discourse by a straightforward adaptation of the rnn encoder - decoder conversational model discussed in @xcite .",
    "we join multiple source utterances using the _ eos _ symbol as a delimiter , and feed them into the encoder rnn as a single input sequence . as in @xcite",
    ", we reversed the order of the tokens in each of the individual utterances but preserved the order of the conversation turns .",
    "the attention mechanism is able to make connections to any of the words used in earlier utterances as the decoder generates each word in the output response .      since",
    "conversational threads are ordered sequences of utterances , it makes sense to extend an rnn encoder - decoder by adding another rnn tier to model the discourse as the turns of the conversation progress .",
    "given @xmath1 input utterances , the rnn encoder is applied to each utterance one at a time as shown in fig .",
    "[ schematic ] ( with tokens fed in reverse order . )",
    "the output of the encoder from each of the input utterances forms @xmath1 time step inputs for the discourse rnn .",
    "the attention mechanism is then applied to the @xmath1 hidden states of the discourse rnn and fed into the decoder rnn .",
    "we also considered a model where the output of the encoder is also combined with the output of the discourse rnn and fed into the attention decoder , but found the purely hierarchical architecture performed better .",
    "for each model we chose identical optimizers , hyperparameters , etc . in our experiments in order to isolate the impact of specific differences in the network architecture , also taking computation times and available gpu resources into account",
    ". it would be straightforward to perform a grid search to tune hyperparameters , try lstm cells , increase layers per rnn , etc . to further improve performance individually for each model beyond what we report here .    for each rnn",
    "we use one layer of gated recurrent units ( grus ) with 512 hidden cells .",
    "separate embeddings for the encoder and decoder , each with dimension 512 and vocabulary size of 40,000 , are trained on - the - fly without using predefined word vectors .",
    "we use a stochastic gradient descent ( sgd ) optimizer with @xmath12 norms clipped at @xmath13 , an initial learning rate of @xmath14 , and a learning rate decay factor of @xmath15 is applied when needed .",
    "we trained with mini - batches of 64 randomly selected examples , and ran training for approximately 10 epochs until validation set loss converged .",
    "we first present results comparing our neural discourse models trained on a large set of conversation threads based on the opensubtitles dataset @xcite .",
    "we then examine how our models are able to produce outputs that indicate enhanced coherence by searching for discourse markers .",
    "a large - scale dataset is important if we want to model all the variations and nuances of human language . from the opensubtitles",
    "corpus we created a training set and validation set with 3,642,856 and 911,128 conversation fragments , respectively and @xmath16 tokens , respectively ] .",
    "each conversation fragment consists of 10 utterances from the previous lines of the movie dialog leading up to a target utterance .",
    "the main limitation of the opensubtitles dataset is that it is derived from closed caption style subtitles , which can be noisy , do not include labels for which actors are speaking in turn , and do not show conversation boundaries from different scenes .",
    "we considered cleaner datasets such as the ubuntu dialog corpus @xcite , movie - dic dialog corpus @xcite , and subtle corpus @xcite but found they all contained orders of magnitude fewer conversations and/or many fewer turns per conversation on average .",
    "therefore , we found the size of the opensubtitles dataset outweighed the benefits of cleaner smaller datasets .",
    "this echoes a trend in neural networks where large noisy datasets tend to perform better than small clean datasets .",
    "the lack of a large - scale clean dataset of conversations is an open problem in the field .",
    "we compared models and performed a sensitivity analysis by varying the number of previous conversation turns fed into the encoder during training and evaluation .    [ cols=\"^,<,<\",options=\"header \" , ]",
    "we studied neural discourse models that can capture long distance relationships between features found in different utterances of a conversation .",
    "we found that a model with an additional discourse rnn outperforms the baseline rnn encoder - decoder with an attention mechanism .",
    "our results indicate that providing more context from previous utterances improves model performance up to a point .",
    "qualitative examples illustrate how the discourse rnn produces increased coherence and cohesion with the rest of the conversation , while quantitative results based on text mining of discourse markers show that the amount of deixis , anaphora , and logical consequence found in the decoder output can be sensitive to the size of the context window .    in future work",
    ", it will be interesting to train discourse models on even larger corpora and compare conversations in different domains . by examining the attention weights it should be possible to study what discourse markers the models are `` paying attention to '' and possibly provide a powerful new tool for analyzing discourse relationships . by applying multi - task sequence to sequence learning techniques as in @xcite",
    "we may be able to combine the conversational modeling task with other tasks such as discourse parsing and/or world knowledge modeling achieve better overall model performance .",
    "not just for conversations , neural discourse modeling could also be applied to written text documents in domains with strong patterns of discourse such as news , legal , healthcare .",
    "movie - dic : a movie dialogue corpus for research and development . in",
    "_ proceedings of the 50th annual meeting of the association for computational linguistics : short papers - volume 2 _ , association for computational linguistics , 2012 .",
    "p. blunsom and nal kalchbrenner .",
    "recurrent convolutional neural networks for discourse compositionality . in _ proceedings of the 2013 workshop on continuous vector space models and their compositionality _ , 2013 .",
    "j. li , mt luong , and d. jurafsky . a hierarchical neural autoencoder for paragraphs and documents . _ proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing _ , 2015 .",
    "a. sordoni , m. galley , m. auli , c. brockett , y. ji , m. mitchell , j.y .",
    "nie , j. gao , and b. dolan . a neural network approach to context - sensitive generation of conversational responses .",
    "_ arxiv preprint arxiv:1506.06714 _ , 2015 .",
    "j. tiedemann , news from opus - a collection of multilingual parallel corpora with tools and interfaces . in _",
    "n. nicolov and k. bontcheva and g. angelova and r. mitkov ( eds . ) recent advances in natural language processing ( vol v ) _ , pages 237 - 248 , john benjamins , amsterdam / philadelphia , 2009 ."
  ],
  "abstract_text": [
    "<S> deep neural networks have shown recent promise in many language - related tasks such as the modeling of conversations . </S>",
    "<S> we extend rnn - based sequence to sequence models to capture the long range discourse across many turns of conversation . </S>",
    "<S> we perform a sensitivity analysis on how much additional context affects performance , and provide quantitative and qualitative evidence that these models are able to capture discourse relationships across multiple utterances . </S>",
    "<S> our results quantifies how adding an additional rnn layer for modeling discourse improves the quality of output utterances and providing more of the previous conversation as input also improves performance . by searching the generated outputs for specific discourse markers </S>",
    "<S> we show how neural discourse models can exhibit increased coherence and cohesion in conversations . </S>"
  ]
}