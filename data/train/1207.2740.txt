{
  "article_text": [
    "in classical statistics , it is often assumed that the outcome of an experiment is precise and the uncertainty of observations is solely due to randomness . under this assumption ,",
    "numerical data are represented as collections of real numbers . in recent years , however , there has been increased interest in situations when exact outcomes of the experiment are very difficult or impossible to obtain , or to measure .",
    "the imprecise nature of the data thus collected is caused by various factors such as measurement errors , computational errors , loss or lack of information . under such circumstances and , in general , any other circumstances such as grouping and censoring ,",
    "when observations can not be pinned down to single numbers , data are better represented by intervals .",
    "practical examples include interval - valued stock prices , oil prices , temperature data , medical records , mechanical measurements , among many others .    in the statistical literature ,",
    "random intervals are most often studied in the framework of random sets , for which the probability - based theory has developed since the publication of the seminal book matheron ( 1975 ) .",
    "studies on the corresponding statistical methods to analyze set - valued data , while still at the early stage , have shown promising advances .",
    "see stoyan ( 1998 ) for a comprehensive review .",
    "specifically , to analyze interval - valued data , the earliest attempt probably dates back to 1990 , when diamond published his paper on the least squares fitting of compact set - valued data and considered interval - valued input and output as a special case ( see diamond ( 1990 ) ) . due to the embedding theorems started by brunn and minkowski and later refined by radstrm ( see radstrm ( 1952 ) ) and hrmander ( see hrmander ( 1954 ) ) , @xmath0 , the space of all nonempty compact convex subsets of @xmath1 , is embedded into the banach space of support functions .",
    "diamond ( 1990 ) defined an @xmath2 metric in this banach space of support functions , and found the regression coefficients by minimizing the @xmath2 metric of the sum of residuals .",
    "this idea was further studied in gil et al .",
    "( 2002 ) , where the @xmath2 metric was replaced by a generalized metric on the space of nonempty compact intervals , called `` w - distance '' , proposed earlier by krner ( 1998 ) .",
    "separately , billard and diday ( 2003 ) introduced the central tendency and dispersion measures and developed the symbolic interval data analysis based on those .",
    "( see also carvalho et al .",
    "( 2004 ) . )",
    "however , none of the existing literature considered distributions of the random intervals and the corresponding statistical methods .",
    "it is well known that normality plays an important role in classical statistics .",
    "but the normal distribution for random sets remained undefined for a long time , until the 1980s when the concept of normality was first introduced for compact convex random sets in the euclidean space by lyashenko ( 1983 ) .",
    "this concept is especially useful in deriving limit theorems for random sets .",
    "see , puri et al .",
    "( 1986 ) , norberg ( 1984 ) , among others .",
    "since a compact convex set in @xmath3 is a closed bounded interval , by the definition of lyashenko ( 1983 ) , a normal random interval is simply a gaussian displacement of a fixed closed bounded interval . from the point of view of statistics ,",
    "this is not enough to fully capture the randomness of a general random interval .    in this paper",
    ", we extend the definition of normality given by lyashenko ( 1983 ) and propose a normal hierarchical model for random intervals . with one more degree of freedom on `` shape '' , our model conveniently captures the entire randomness of random intervals via a few parameters .",
    "it is a natural extension from lyashenko ( 1983 ) yet a highly practical model accommodating a large class of random intervals . in particular , when the length of the random interval reduces to zero , it becomes the usual normal random variable .",
    "therefore , it can also be viewed as an extension of the classical normal distribution that accounts for the extra uncertainty added to the randomness .",
    "in addition , there are two interesting properties regarding our normal hierarchical model : 1 ) conditioning on the first hierarchy , it is exactly the normal random interval defined by lyashenko ( 1983 ) , which could be a very useful property in view of the limit theorems ; 2 ) with certain choices of the distributions , a linear combination of our normal hierarchical random intervals follows the same normal hierarchical distribution .",
    "an immediate consequence of the second property is the possibility of a factor model for multi - dimensional random intervals , as the `` factor '' will have the same distribution as the original intervals .    for random sets models , it is important , in the stage of parameter estimation , to take into account the geometric characteristics of the observations .",
    "for example , tanaka et al . ( 2008 ) proposed an approximate maximum likelihood estimation for parameters in the neyman - scott point processes based on the point pattern of the observation window .",
    "for another model , heinrich ( 1993 ) discussed several distance functions ( called `` contrast functions '' ) between the parametric and the empirical contact distribution function that are used towards parameter estimation for boolean models .",
    "bearing this in mind , to estimate the parameters of our normal hierarchical model , we propose a minimum contrast estimator ( mce ) based on the hitting function ( capacity functional ) that characterizes the distribution of a random interval by the hit - and - miss events of test sets .",
    "see matheron ( 1975 ) .",
    "in particular , we construct a contrast function based on the integral of a discrepancy function between the empirical and the parametric distribution measure .",
    "theoretically , we show that under certain conditions our mce satisfies a strong consistency and asymptotic normality .",
    "the simulation study is consistent with our theorems .",
    "we apply our model to analyze a daily temperature range data and , in this context , we have derived interesting and promising results .",
    "the use of an integral measure of probability discrepancy here is not new .",
    "for example , the integral probability metrics ( ipms ) , widely used as tools for statistical inferences , have been defined as the supremum of the absolute differences between expectations with respect to two probability measures .",
    "see , e.g. , zolotarev ( 1983 ) , mller ( 1997 ) , and sriperumbudur et al .",
    "( 2012 ) , for references . especially , the empirical estimation of ipms proposed by sriperumbudur et al .",
    "( 2012 ) drastically reduces the computational burden , thereby emphasizing the practical use of the ipms .",
    "this idea is potentially applicable to our mce and we expect similar reduction in computational intensity as for ipms .    the rest of the paper is organized as follows .",
    "section [ sec : model ] formally defines our normal hierarchical model and discusses its statistical properties .",
    "section [ sec : mce ] introduces a minimum contrast estimator for the model parameters , and presents its asymptotic properties .",
    "a simulation study is reported in section [ sec : simu ] , and a real data application is demonstrated in section [ sec : real ] .",
    "we give concluding remarks in section [ sec : conclu ] .",
    "proofs of the theorems are presented in section [ sec : proofs ] .",
    "useful lemmas and other proofs are deferred to the appendix .",
    "let @xmath4 be a probability space . denote by @xmath5 the collection of all non - empty compact subsets of @xmath6 .",
    "a random compact set is a borel measurable function @xmath7 , @xmath5 being equipped with the borel @xmath8-algebra induced by the hausdorff metric .",
    "if @xmath9 is convex for almost all @xmath10 , then @xmath11 is called a random compact convex set .",
    "( see molchanov ( 2005 ) , p.21 , p.102 . )",
    "denote by @xmath12 the collection of all compact convex subsets of @xmath6 . by theorem 1 of lyashenko ( 1983 ) , a compact convex random set @xmath11 in the euclidean space",
    "@xmath6 is gaussian if and only if @xmath11 can be represented as the minkowski sum of a fixed compact convex set @xmath13 and a @xmath14-dimensional normal random vector @xmath15 , i.e. @xmath16 as pointed out in lyashenko ( 1983 ) , gaussian random sets are especially useful in view of the limit theorems discussed earlier in lyashenko ( 1979 ) .",
    "that is , if the conditions in those theorems are satisfied and the limit exists , then it is gaussian in the sense of ( [ def_lsko ] ) .",
    "puri et al .",
    "( 1986 ) extended these results to separable banach spaces .    in the following",
    ", we will restrict ourselves to compact convex random sets in @xmath17 , that is , bounded closed random intervals .",
    "they will be called random intervals for ease of presentation .    according to ( [ def_lsko ] )",
    ", a random interval @xmath11 is gaussian if and only if a is representable in the form @xmath18 where @xmath19 is a fixed bounded closed interval and @xmath15 is a normal random variable .",
    "obviously , such a random interval is simply a gaussian displacement of a fixed interval , so it is not enough to fully capture the randomness of a general random interval .",
    "in order to model the randomness of both the location and the `` shape '' ( length ) , we propose the following normal hierarchical model for random intervals : @xmath20 where @xmath21 is another random variable and @xmath22 $ ] is a fixed interval in @xmath3 . here , the product @xmath23 is in the sense of scalar multiplication of a real number and a set .",
    "let @xmath24 denote the lebesgue measure of @xmath17 .",
    "then , @xmath25 that is , @xmath21 is the variable that models the length of @xmath11 .",
    "in particular , if @xmath26 , then a reduces to a normal random variable .",
    "obviously , @xmath15 and @xmath21 are `` location '' and `` shape '' variables .",
    "we assume that @xmath27 .",
    "then the normal hierarchical random interval is explicitly expressible as @xmath28.\\ ] ] the parameter @xmath29 is indeed unnecessary , as the difference @xmath30 can be absorbed by @xmath21 . as a result , @xmath31\\ ] ]",
    "compared to the  naive \" model @xmath32 $ ] , for which @xmath15 is precisely the center of the interval , ( [ mod - simple ] ) has an extra parameter @xmath33 .",
    "notice that the center of @xmath11 is @xmath34 , so @xmath33 controls the difference between @xmath15 and the center , and therefore is interpreted as modeling the uncertainty that the normal random variable @xmath15 is not necessarily the center .",
    "[ rmk:1 ] there are some existing works in the literature to model the randomness of intervals . for example , a random interval can be viewed as the  crisp \" version of the lr - fuzzy random variable , which is often used to model the randomness of imprecise intervals such as [ approximately 2 , approximately 5 ] .",
    "see krner ( 1997 ) for detailed descriptions .",
    "however , as far as the authors are aware , models with distribution assumptions for interval - valued data have not been studied yet .",
    "our normal hierarchical random interval is the first statistical approach that extends the concept of normality while modeling the full randomness of an interval .",
    "an interesting property of the normal hierarchical random interval is that its linear combination is still a normal hierarchical random interval .",
    "this is seen by simply observing that @xmath35 for arbitrary constants @xmath36 , where `` @xmath37 '' denotes the minkowski addition .",
    "this is very useful in developing a factor model for the analysis of multiple random intervals .",
    "especially , if we assume @xmath38 , then the `` factor '' @xmath39 has exactly the same distribution as the original random intervals .",
    "we will elaborate more on this issue in section [ sec : simu ] .    without loss of generality",
    ", we can assume in the model ( [ def : a_1])-([def : a_2 ] ) that @xmath40 .",
    "we will make this assumption throughout the rest of the paper .      according to the choquet theorem ( molchanov ( 2005 ) , p.10 ) ,",
    "the distribution of a random closed set ( and random compact convex set as a special case ) a , is completely characterized by the hitting function @xmath41 defined as : @xmath42 writing @xmath43 $ ] with @xmath44 , the normal hierarchical random interval in ( [ def : a_1])-([def : a_2 ] ) has the following hitting function : for @xmath45 $ ] : @xmath46)\\\\    & = & p([a , b]\\cap a\\neq\\emptyset)\\\\    & = & p([a , b]\\cap a\\neq\\emptyset,\\eta\\geq 0)+p([a , b]\\cap a\\neq\\emptyset,\\eta < 0)\\\\    & = & p(a-\\eta b_0\\leq\\epsilon\\leq b-\\eta a_0,\\eta\\geq 0)+p(a-\\eta a_0\\leq\\epsilon\\leq b-\\eta b_0,\\eta < 0).\\end{aligned}\\ ] ]    the expectation of a compact convex random set @xmath11 is defined by the aumann integral ( see aumann ( 1965 ) , artstein and vitale ( 1975 ) ) as @xmath47 in particular , the aumann expectation of a random interval @xmath11 is given by @xmath48,\\ ] ] where @xmath49 and @xmath50 are the interval ends .",
    "therefore , the aumann expectation of the normal hierarchical random interval @xmath11 is @xmath51i_{(\\eta\\geq 0)}+[b_0\\eta , a_0\\eta]i_{(\\eta<0)}\\right\\}\\\\      & = & e\\left[a_0\\eta i_{(\\eta\\geq 0)}+b_0\\eta i_{(\\eta<0)},b_0\\eta i_{(\\eta\\geq 0)}+a_0\\eta i_{(\\eta<0)}\\right]\\\\      & = & \\left[a_0e\\eta_{+}+b_0e\\eta_{-},b_0e\\eta_{+}+a_0e\\eta_{-}\\right],\\end{aligned}\\ ] ] where @xmath52 notice that @xmath53 can be interpreted as the positive part of @xmath21 , but @xmath54 is not the negative part of @xmath21 , as @xmath55 when @xmath56 .",
    "the variance of a compact convex random set @xmath11 in @xmath6 is defined via its support function . in the special case",
    "when @xmath57 , it is shown by straightforward calculations that @xmath58 or equivalently , @xmath59 where @xmath60 and @xmath61 denote the center and radius of a random interval @xmath11 .",
    "see krner ( 1995 ) . again , as we pointed out in remark [ rmk:1 ] , a random interval can be viewed as a special case of the lr - fuzzy random variable .",
    "therefore , formulae ( [ var-1 ] ) and ( [ var-2 ] ) coincide with the variance of the lr - fuzzy random variable , when letting the left and right spread both equal to 0 , i.e. , @xmath62 . see krner ( 1997 ) .",
    "for the normal hierarchical random interval @xmath11 , @xmath63",
    "^ 2\\\\    & = & e\\epsilon^2+a_0 ^ 2var(\\eta_{+})+b_0 ^ 2var(\\eta_{-})\\\\    & & + 2\\left(a_0e\\epsilon\\eta_{+}+b_0e\\epsilon\\eta_{-}-a_0b_0e\\eta_{+}e\\eta_{-}\\right),\\end{aligned}\\ ] ] and , analogously , @xmath64 the variance of @xmath11 is then found to be @xmath65\\\\    & & + ( a_0+b_0)e\\epsilon\\eta-2a_0b_0e\\eta_{+}\\eta_{-}.\\end{aligned}\\ ] ]    assuming @xmath27 , we have @xmath66 with @xmath40 .",
    "this formula certainly includes the special case of the  naive \" model @xmath32 $ ] , by letting @xmath67 and @xmath68 .",
    "it is more general because it also accounts for the covariance between  location \" and  length \" in calculating the total variance of the random interval , while the  naive \" model simply has @xmath69 .",
    "we study minimum contrast estimation ( mce ) of the parameters of the normal hierarchical random interval ( @xmath70)-(@xmath71 ) , as well as its asymptotic properties .",
    "since @xmath57 , from now on we let @xmath5 be the space of all non - empty compact subsets in @xmath3 restrictively , and let @xmath72 be the borel @xmath8-algebra on @xmath5 induced by the hausdorff metric .",
    "let @xmath12 denote the space of all non - empty compact convex subsets , i.e. , bounded closed intervals , in @xmath3 .",
    "as mentioned in the previous section , a random interval @xmath73 is a borel measurable function from a probability space @xmath4 to @xmath74 such that @xmath75 almost surely .    throughout this section , we assume observing a sample of i.i.d . random intervals @xmath76 .",
    "let @xmath77 denote a @xmath78 vector containing all the parameters in the model , which takes on a value from a parameter space @xmath79 . here",
    "@xmath80 is the number of parameters .",
    "let @xmath81 denote the true value of the parameter vector .",
    "denote by @xmath82)$ ] the hitting function of @xmath83 with parameter @xmath77 .    in order to introduce the mce , we will need some extra notations .",
    "let @xmath84 be a basic set and @xmath85 be a @xmath8-field over it .",
    "let @xmath86 denote a family of probability measures on ( * x*,@xmath85 ) and @xmath87 be a mapping from @xmath86 to some topologial space @xmath41 .",
    "@xmath88 denotes the parameter value pertaining to @xmath89 , @xmath90 .",
    "the classical definition of mce given in pfanzagl ( 1969 ) is quoted below .",
    "@xmath91 $ ] a family of @xmath85-measurable functions @xmath92 is a family of contrast functions if @xmath93<\\infty,\\ ] ] @xmath94 , and @xmath95<e_p\\left[f_t\\right],\\ ] ] @xmath96 .    in other words , a contrast function is a measurable function of the random variable(s ) whose expected value reaches its minimum under the probability measure that generates the random variable(s ) . from the view of probability , with the true parameters , a contrast function tends to have a smaller value than with other parameters . adopting notation from pfanzagl ( 1969 ) , we let @xmath86 denote a family of probability measures on ( @xmath97 ) and @xmath87 be a mapping from @xmath86 to some topologial space @xmath41 .",
    "similarly , @xmath88 denotes the parameter value pertaining to @xmath89 , @xmath98 . in a similar fashion to the contrast function in heinrich ( 1993 ) for boolean models ,",
    "we give our definition of contrast function for random intervals in the following .",
    "and then the mce is defined as the minimizer of the contrast function .",
    "[ def : cf ] a family of @xmath99-measurable functions @xmath100 : @xmath101 $ ] , @xmath102 , @xmath103 is a family of contrast functions for @xmath86 , if there exists a function @xmath104 : @xmath105 such that @xmath106 and @xmath107    [ def : mce ] a @xmath99-measurable function @xmath108 : @xmath109 , which depends on @xmath110 only , is called a minimum contrast estimator ( mce ) if @xmath111      we make the following assumptions to present the theoretical results in this section .",
    "[ aspt:1 ] @xmath112 is compact , and @xmath81 is an interior point of @xmath112 .",
    "[ aspt:2 ] the model is identifiable .",
    "[ aspt:3 ] @xmath113)$ ] is continuous with respect to @xmath77 .",
    "[ aspt:4 ] @xmath114)$ ] , @xmath115 , exist and are finite on a bounded region @xmath116 .",
    "[ aspt:5 ] @xmath117)$ ] , @xmath118)$ ] , and @xmath119)$ ] , @xmath120 , exist and are finite on @xmath121 for @xmath122 .",
    "assumptions 4 and 5 are essential to establish the asymptotic normality for the mce @xmath108 .",
    "they are rather mild and can be met by a large class of capacity functionals .",
    "for example , if @xmath121 is closed , then each @xmath123 with continuous up to third order partial derivatives satisfies both assumptions , as a continuous function on a compact region is always bounded .",
    "the following theorem gives sufficient conditions under which the minumum contrast estimator @xmath108 defined above is strongly consistent .",
    "[ thm : strong - consist ] let @xmath100 be a contrast function as in definition [ def : cf ] and let @xmath108 be the corresponding mce .",
    "under the hypothesis of assumption [ aspt:1 ] and in addition if @xmath100 is equicontinuous w.r.t .",
    "@xmath124 for all @xmath125 , then , @xmath126    let @xmath127\\in\\mathcal{k}_{\\mathcal{c}}$ ] .",
    "define an empirical estimator @xmath128;x(n))$ ] for @xmath129)$ ] as : @xmath130;x(n))=\\frac{\\ # \\left\\{x_i : [ a , b]\\cap x_i\\neq\\emptyset , i=1,\\cdots , n\\right\\}}{n}.\\ ] ] extending the contrast function defined in heinrich ( 1993 ) ( for parameters in the boolean model ) , we construct a family of functions : @xmath131)-\\hat{t}([a , b];x(n))\\right]^2w(a , b)\\mathrm{d}a\\mathrm{d}b,\\ ] ] for @xmath103 , where @xmath132 , and @xmath133 is a weight function on @xmath127 $ ] satisfying @xmath134 , @xmath135\\in\\mathcal{k}_\\mathcal{c}$ ] .",
    "we show in the next proposition that @xmath136 , @xmath137 defined in ( [ h_def ] ) is a family of contrast functions for @xmath77 .",
    "this , together with theorem [ thm : strong - consist ] , immediately yields the strong consistency of the associated mce .",
    "this result is summarized in corollary [ coro : consist ] .",
    "[ prop : cf ] suppose that assumption [ aspt:2 ] and assumption [ aspt:3 ] are satisfied .",
    "then @xmath138 , @xmath103 , as defined in ( [ h_def ] ) , is a family of contrast functions with limiting function @xmath139)-t_{\\boldsymbol{\\zeta}}([a , b])\\right]^2w(a , b)\\mathrm{d}a\\mathrm{d}b.\\ ] ] in addition , @xmath138 is equicontinuous w.r.t . @xmath124 .",
    "[ coro : consist ] suppose that assumption [ aspt:1 ] , assumption [ aspt:2 ] , and assumption [ aspt:3 ] are satisfied .",
    "let @xmath138 be defined as in ( [ h_def ] ) , and @xmath140 then @xmath141 as @xmath142 .",
    "next , we show the asymptotic normality for @xmath143 . as a preparation ,",
    "we first prove the following proposition .",
    "the central limit theorem for @xmath143 is then presented afterwards .",
    "[ prop : parh ] assume the conditions of lemma 1 ( in the appendix ) .",
    "define @xmath144^{t},\\nonumber\\ ] ] as the @xmath78 gradient vector of @xmath145 w.r.t .",
    "then , @xmath146   \\stackrel{\\mathcal{d}}{\\rightarrow}n\\left(0,\\xi\\right),\\nonumber\\ ] ] where @xmath147 is the @xmath148 symmetric matrix with the @xmath149 component @xmath150\\neq\\emptyset , x_1\\cap[c , d]\\neq\\emptyset\\right )    -t_{{\\boldsymbol}{\\theta}_0}\\left([a , b]\\right)t_{{\\boldsymbol}{\\theta}_0}\\left([c , d]\\right)\\right\\}\\nonumber\\\\    & & \\frac{\\partial t_{{\\boldsymbol}{\\theta}_0}}{\\partial\\theta_i}\\left([a , b]\\right )    \\frac{\\partial t_{{\\boldsymbol}{\\theta}_0}}{\\partial\\theta_j}\\left([c , d]\\right )    w(a , b)w(c , d)\\mathrm{d}a\\mathrm{d}b\\mathrm{d}c\\mathrm{d}d.\\label{def : xi}\\end{aligned}\\ ] ]    [ thm : clt ] let @xmath136 be defined in ( [ h_def ] ) and @xmath143 be defined in ( [ def : theta ] ) .",
    "assume the conditions of corollary [ coro : consist ] . if additionally assumption [ aspt:5 ] is satisfied ,",
    "then @xmath151 where @xmath152)w(a , b)\\mathrm{d}a\\mathrm{d}b$ ] , and @xmath147 is defined in ( [ def : xi ] ) .",
    "we carry out a small simulation to investigate the performance of the mce introduced in definition [ def : mce ] .",
    "assume , in the normal hierarchical model ( [ def : a_1])-([def : a_2 ] ) , that @xmath153 and @xmath154 the bivariate normal distribution conveniently takes care of the variances and covariance of the location variable @xmath15 and the shape variable @xmath21 .",
    "the removal of the freedom of @xmath29 is for model identifiability purposes ; it is seen that the hitting function @xmath155 is defined via @xmath156 and @xmath157 only . for the simulation , we assign the following parameter values : @xmath158      under the bivariate normal distribution assumption , the hitting function of our normal hierarchical model is found to be @xmath159)\\nonumber\\\\    & = & p(a-\\eta b_0\\leq\\epsilon\\leq b-\\eta a_0,\\eta\\geq 0)+p(a-\\eta a_0\\leq\\epsilon\\leq b-\\eta b_0,\\eta < 0)\\nonumber\\\\    & = & p\\left(\\epsilon\\leq b-\\eta a_0,\\eta\\geq 0\\right)-p\\left(\\epsilon < a-\\eta b_0,\\eta\\geq 0\\right)\\nonumber\\\\    & & + p\\left(\\epsilon\\leq b-\\eta b_0,\\eta < 0\\right)-p\\left(\\epsilon < a-\\eta a_0,\\eta<0\\right)\\nonumber\\\\    & = & p\\left(\\begin{bmatrix}1 & a_0\\\\ 0 & -1\\end{bmatrix } \\begin{bmatrix}\\epsilon\\\\ \\eta\\end{bmatrix}\\leq\\begin{bmatrix}b\\\\0\\end{bmatrix}\\right )    -p\\left(\\begin{bmatrix}1 & b_0\\\\ 0 & -1\\end{bmatrix } \\begin{bmatrix}\\epsilon\\\\ \\eta\\end{bmatrix}\\leq\\begin{bmatrix}a\\\\0\\end{bmatrix}\\right)\\nonumber\\\\    & & + p\\left(\\begin{bmatrix}1 & b_0\\\\ 0 & 1\\end{bmatrix } \\begin{bmatrix}\\epsilon\\\\ \\eta\\end{bmatrix}\\leq\\begin{bmatrix}b\\\\0\\end{bmatrix}\\right )    -p\\left(\\begin{bmatrix}1 & a_0\\\\ 0 & 1\\end{bmatrix } \\begin{bmatrix}\\epsilon\\\\ \\eta\\end{bmatrix}\\leq\\begin{bmatrix}a\\\\0\\end{bmatrix}\\right)\\nonumber\\\\    & = & \\phi\\left(\\begin{bmatrix}b\\\\0\\end{bmatrix } ; d_1\\begin{bmatrix}0\\\\ \\mu\\end{bmatrix } , d_1\\sigma d_1^{'}\\right )    -\\phi\\left(\\begin{bmatrix}a\\\\0\\end{bmatrix } ; d_2\\begin{bmatrix}0\\\\ \\mu\\end{bmatrix } , d_2\\sigma d_2^{'}\\right)\\nonumber\\\\    & & + \\phi\\left(\\begin{bmatrix}b\\\\0\\end{bmatrix } ; d_3\\begin{bmatrix}0\\\\ \\mu\\end{bmatrix } , d_3\\sigma d_3^{'}\\right )    -\\phi\\left(\\begin{bmatrix}a\\\\0\\end{bmatrix } ; d_4\\begin{bmatrix}0\\\\ \\mu\\end{bmatrix } , d_4\\sigma d_4^{'}\\right),\\label{eqn : hit - fct}\\end{aligned}\\ ] ] where @xmath160 is the bivariate normal cdf with mean @xmath161 and covariance @xmath162 , and @xmath163 after linear transformation of variables , the terms in formula ( [ eqn : hit - fct ] ) is calculated via the standard bivariate normal cdf . by absolute continuity , @xmath82)$ ] in this case",
    "is continuous and also infinitely continuously differentiable .",
    "therefore , all the assumptions are satisfied and the corresponding mce achieves the strong consistency and asymptotic normality .    according to the assigned parameter values given in ( [ eqn : par - val ] ) , @xmath164 .",
    "therefore the hitting function is well approximated by @xmath159)\\\\    & \\approx&p(a-\\eta b_0\\leq\\epsilon\\leq b-\\eta a_0,\\eta\\geq 0)\\\\    & \\approx&p(a-\\eta b_0\\leq\\epsilon\\leq b-\\eta a_0)\\\\    & = & p\\left (    \\begin{bmatrix}1 & a_0\\\\ -1 & -a_0 - 1\\end{bmatrix }    \\begin{bmatrix}\\epsilon\\\\ \\eta\\end{bmatrix}\\leq    \\begin{bmatrix}b\\\\-a\\end{bmatrix}\\right)\\\\    & = & \\phi\\left (    \\begin{bmatrix}b\\\\-a\\end{bmatrix } ;    d\\begin{bmatrix}0\\\\ \\mu\\end{bmatrix } , d\\sigma d^{'}\\right),\\end{aligned}\\ ] ] where @xmath165 we use this approximate hitting function to simplify computation in our simulation study .",
    "the model parameters can be estimated by the method of moments . in most cases",
    "it is reasonable to assume @xmath166 , and consequently , @xmath167 .",
    "so the moment estimates for @xmath168 and @xmath33 are approximately @xmath169 where @xmath170 and @xmath171 denote the sample means of @xmath50 and @xmath49 , respectively . denoting by @xmath60 the center of the random interval @xmath11",
    ", we further notice that @xmath172 . by the same approximation we have @xmath173 .",
    "define a random variable @xmath174 then , the moment estimate for @xmath175 is approximately given by the sample variance - covariance matrix of @xmath176 and @xmath177 , i.e. @xmath178      our simulation experiment is designed as follows : we first simulate an i.i.d",
    ". random sample of size @xmath179 from model ( [ def : a_1])-([def : a_2 ] ) with the assigned parameter values , then find the initial parameter values by ( [ mm-1])-([mm-3 ] ) based on the simulated sample , and lastly the initial values are updated to the mce using the function",
    "_ fminsearch.m _ in matlab 2011a .",
    "the process is repeated 10 times independently for each @xmath179 , and we let @xmath180 , successively , to study the consistency and efficiency of the mce s .    figure [ fig : sample_simu ] shows one random sample of 100 observations generated from the model .",
    "we show the average biases and standard errors of the estimates as functions of the sample size in figure [ fig : results_simu ] . here , the average bias and standard error of the estimates of @xmath175 are the @xmath2 norms of the average bias and standard error matrices , respectively .",
    "as expected from corollary [ coro : consist ] and theorem [ thm : clt ] , both the bias and the standard error reduce to 0 as sample size grows to infinity .",
    "the numerical results are summarized in table [ tab : mc_1 ] .",
    "finally , we point out that the choice of the region of integration @xmath181 is important .",
    "a larger @xmath181 usually leads to more accurate estimates , but could also result in more computational complexity .",
    "we do not investigate this issue in this paper .",
    "however , based on our simulation experience , an @xmath181 that covers most of the points @xmath182 such that @xmath127 $ ] hits some of the observed intervals , is a good choice as a rule of thumb . in our simulation ,",
    "@xmath183 $ ] , by ignoring the small probability @xmath184 .",
    "therefore , we choose @xmath185 , and the estimates are satisfactory .",
    "+     +    .average biases and standard errors of the mce s of the model parameters in the simulation study .",
    "[ cols= \" > , > , > , > , > , > , > , > , > \" , ]",
    "in this section , we apply our normal hierarchical model and minimum contrast estimator to analyze the daily temperature range data .",
    "we consider two data sets containing ten years of daily minimum and maximum temperatures in january , in granite falls , minnesota ( latitude 44.81241 , longitude 95.51389 ) from 1901 to 1910 , and from 2001 to 2010 , respectively .",
    "each data set , therefore , is constituted of 310 observations of the form : [ minimum temperature , maximum temperature ] .",
    "we obtained these data from the national weather service , and all observations are in fahrenheit .",
    "the plot of the data is shown in figure [ fig : real ] .",
    "the obvious correlations of the data play no roles here .",
    "+     +    same as in the simulation , we assume a bivariate normal distribution for @xmath186 and @xmath187 $ ] has length 1 . the initial parameter values are computed according to ( [ mm-1])-([mm-3 ] ) , and the weight function @xmath188 . the minimum contrast estimates for the model parameters",
    "are :    * data set 1 ( 1901 - 1910 ) : @xmath189 * data set 2 ( 2001 - 2010 ) : @xmath190    recall that the center and the length of the normal hierarchical random interval are @xmath191 and @xmath192(@xmath193 for the two considered data sets ) , respectively .",
    "therefore , they are assumed to follow normal distributions with means @xmath194 and @xmath168 , and variances @xmath195 and @xmath196 , respectively . to assess the goodness - of - fit",
    ", we compare the fitted normal distributions with the corresponding empirical distributions for both the center and the length of the two data sets .",
    "the results are shown in figure [ fig : pdf_plot ] .",
    "for the interval length of data 2 ( 2001 - 2010 ) , the fitted normal distribution is slightly more deviated from the empirical distribution , due to the skewness and heavy tail of the data .",
    "all the other three plots show very good fittings of our model to the data .",
    "+     +   +    denote by @xmath197 and @xmath198 respectively the random intervals from which the two data sets are drawn .",
    "the model fitted mean and variance for @xmath197 and @xmath198 are found to be : @xmath199 , \\widehat{\\text{var}}(a_1)=221.2313;\\\\ & & \\hat{\\text{e}}(a_2)=\\left[5.3335 , 25.8416\\right ] , \\widehat{\\text{var}}(a_2)=247.3275.\\end{aligned}\\ ] ] both mean and variance of the recent data are larger than those of the data 100 years ago .",
    "the two model fitted means are also shown on the data plots blue as the intervals between the solid horizontal lines in figure [ fig : real ] .",
    "in addition , the correlation coefficient of @xmath186 is @xmath200 for data set 1 and @xmath201 for data set 2 , suggesting a negative correlation between the location and the length for the january temperature range data in general .",
    "that is , colder days tend to have larger temperature ranges , and , this relationship is stronger in the more recent data .",
    "+ finally , we point out that some of the parameters can be easily estimated by simple traditional methods . for example , by averaging the two interval ends respectively , we get the moment estimates for the two means : @xmath202,\\\\ & & \\hat{\\text{e}}_{m}(a_2)=\\left[3.8323 , 23.6903\\right].\\end{aligned}\\ ] ] they are shown in figure [ fig : real ] as the intervals between the dashed horizontal lines , in comparison with our model fitted means .",
    "further , the sample correlations between the interval centers and lengths are computed as @xmath203 and @xmath204 for data sets 1 and 2 , respectively .",
    "these estimates can be viewed as a preliminary analysis . our model and",
    "the mce of the parameters refine it and provide a more systematic understanding of the data , by examining their geometric structure in the framework of random sets .",
    "in this paper we introduced a new model of random sets ( specifically for random intervals ) . in many practical situations data",
    "are not completely known , or are only known with some margins of error , and it is a very important issue to consider a model which extends normality for ordinary ( numerical ) data .",
    "our hierarchical normal model extends normality for point - valued random variables , and is quite flexible in the sense that it is well suited for both theoretical investigations and for simulations and real data analysis . to these goals",
    "we have defined a minimum contrast estimator for the model parameters , and we have proved its consistency and asymptotic normality .",
    "we carry out simulation experiments , and , finally we apply our model to a real data set ( daily temperature range data obtained from the national weather service ) .",
    "our approach is suitable for extensions to models in higher dimensions , e.g. , a factor model for multiple random intervals , or more general random sets , including possible extensions to spherical random sets .",
    "assume by contradiction that @xmath108 does not converge to @xmath205 almost surely .",
    "then , there exists an @xmath206 such that @xmath207 let @xmath208 and @xmath209 . by the compactness of @xmath210 , for every @xmath211",
    ", there exists a convergent subsequence @xmath212 of @xmath213 such that @xmath214 as @xmath215 .",
    "since @xmath81 is the true underlying parameter vector that generates @xmath110 , from definition [ def : cf ] , @xmath216 converges to @xmath217 almost surely , and any subsequence converges too .",
    "so we have @xmath218 on the other hand , almost surely , @xmath219 equation ( [ equicon ] ) follows from the equicontinuity of @xmath100 .    therefore , @xmath220 where @xmath221 and consequently @xmath222 . but from the assumptions , @xmath223 .",
    "this contradicts ( [ contra ] ) .",
    "hence the desired result follows .      from taylor",
    "s theorem , we have @xmath224 ^ 2   \\frac{\\partial h}{\\partial\\theta_i}\\left(x\\left(n\\right);{\\boldsymbol}{\\epsilon}_n\\right)\\nonumber\\\\   & = & \\frac{\\partial h}{\\partial\\theta_i}\\left(x\\left(n\\right);{\\boldsymbol}{\\theta}_0\\right)\\nonumber\\\\   & & + \\sum\\limits_{j=1}^{p}\\left(\\theta^h_{n , j}-\\theta_{0,j}\\right)\\left [   \\frac{\\partial^2h}{\\partial\\theta_j\\partial\\theta_i}\\left(x(n);{\\boldsymbol}{\\theta}_0\\right)+\\frac{1}{2 }   \\sum\\limits_{l=1}^{p}\\left(\\theta^h_{n , l}-\\theta_{0,l}\\right)\\frac{\\partial^3h }   { \\partial\\theta_l\\partial\\theta_j\\partial\\theta_i}\\left(x\\left(n\\right);{\\boldsymbol}{\\epsilon}_n\\right )   \\right],\\nonumber\\end{aligned}\\ ] ] for @xmath115 , where @xmath225 lies between @xmath81 and @xmath143 . writing the above equations in matrix form , we get @xmath226   \\left({\\boldsymbol}{\\theta}_n^h-{\\boldsymbol}{\\theta}_0\\right)\\nonumber\\\\   & & = 0\\label{thm3:eqn1}.\\end{aligned}\\ ] ] observe , by taking derivatives under the integral sign , that @xmath227 , @xmath228)-\\hat{t}([a , b];x(n))\\right]^2w(a , b)\\mathrm{d}a\\mathrm{d}b,\\nonumber\\\\   & = & \\frac{\\partial}{\\partial\\theta_j}2\\iint\\limits_{s}\\left[t_{\\boldsymbol{\\theta}}([a , b])-\\hat{t}([a , b];x(n))\\right ]   \\frac{\\partial t_{{\\boldsymbol}{\\theta}_0}}{\\partial\\theta_i}([a , b])w(a , b)\\mathrm{d}a\\mathrm{d}b,\\nonumber\\\\   & = & 2\\iint\\limits_{s}\\left[t_{\\boldsymbol{\\theta}}([a , b])-\\hat{t}([a , b];x(n))\\right ]   \\frac{\\partial^2t_{{\\boldsymbol}{\\theta}_0}}{\\partial\\theta_j\\partial\\theta_i}([a , b])w(a , b)\\mathrm{d}a\\mathrm{d}b\\nonumber\\\\   & & + 2\\iint\\limits_{s}\\left(\\frac{\\partial t_{{\\boldsymbol}{\\theta}_0}}{\\partial\\theta_j }   \\frac{\\partial t_{{\\boldsymbol}{\\theta}_0}}{\\partial\\theta_i}\\right)([a , b])w(a , b)\\mathrm{d}a\\mathrm{d}b\\nonumber\\\\   & : = & i+ii.\\nonumber\\end{aligned}\\ ] ] the first term is @xmath229\\right)-\\frac{1}{n}\\sum_{k=1}^{n}y_k\\left(a , b\\right)\\right )    \\frac{\\partial^2 t_{{\\boldsymbol}{\\theta}_0}}{\\partial\\theta_j\\partial\\theta_i}([a , b])w(a , b)\\mathrm{d}a\\mathrm{d}b\\nonumber\\\\    & = & \\frac{2}{n}\\sum_{k=1}^{n}\\iint\\limits_{s}\\left[t_{{\\boldsymbol}{\\theta}_0}\\left([a , b]\\right)-y_k\\left(a , b\\right)\\right ]    \\frac{\\partial^2 t_{{\\boldsymbol}{\\theta}_0}}{\\partial\\theta_j\\partial\\theta_i}([a , b])w(a , b)\\mathrm{d}a\\mathrm{d}b\\nonumber\\\\    & = & o_p(1),\\nonumber\\end{aligned}\\ ] ] according to the strong law of large numbers for i.i.d . random variables . therefore , @xmath230)w(a , b)\\mathrm{d}a\\mathrm{d}b,\\nonumber\\ ] ] @xmath227 . in matrix form ,",
    "@xmath231)w(a , b)\\mathrm{d}a\\mathrm{d}b.\\ ] ] observe again that @xmath232 , @xmath233)-\\hat{t}([a , b];x(n))\\right ]   \\frac{\\partial^3t_{{\\boldsymbol}{\\epsilon}_n}}{\\partial\\theta_j\\partial\\theta_k\\partial\\theta_l }   ( [ a , b])w(a , b)\\mathrm{d}a\\mathrm{d}b\\right|\\nonumber\\\\   & & + 2\\left|\\iint\\limits_{s}\\left[\\left(\\frac{\\partial t_{{\\boldsymbol}{\\epsilon}_n}}{\\partial\\theta_j }   \\frac{\\partial^2t_{{\\boldsymbol}{\\epsilon}_n}}{\\partial\\theta_k\\partial\\theta_l}\\right )   + \\left(\\frac{\\partial^2t_{{\\boldsymbol}{\\epsilon}_n}}{\\partial\\theta_j\\partial\\theta_k}\\frac{\\partial t_{{\\boldsymbol}{\\epsilon}_n } }   { \\partial\\theta_l}\\right )   + \\left(\\frac{\\partial^2t_{{\\boldsymbol}{\\epsilon}_n}}{\\partial\\theta_j\\partial\\theta_l}\\frac{\\partial t_{{\\boldsymbol}{\\epsilon}_n } }   { \\partial\\theta_k}\\right)\\right]([a , b])w(a , b)\\mathrm{d}a\\mathrm{d}b\\right|\\nonumber\\\\   & \\leq&4\\iint\\limits_{s}\\left|\\frac{\\partial^3t_{{\\boldsymbol}{\\epsilon}_n}}{\\partial\\theta_j\\partial\\theta_k\\partial\\theta_l }   ( [ a , b])w(a , b)\\mathrm{d}a\\mathrm{d}b\\right|\\nonumber\\\\   & & + 2\\left|\\iint\\limits_{s}\\left[\\left(\\frac{\\partial t_{{\\boldsymbol}{\\epsilon}_n}}{\\partial\\theta_j }   \\frac{\\partial^2t_{{\\boldsymbol}{\\epsilon}_n}}{\\partial\\theta_k\\partial\\theta_l}\\right )   + \\left(\\frac{\\partial^2t_{{\\boldsymbol}{\\epsilon}_n}}{\\partial\\theta_j\\partial\\theta_k}\\frac{\\partial t_{{\\boldsymbol}{\\epsilon}_n } }   { \\partial\\theta_l}\\right )   + \\left(\\frac{\\partial^2t_{{\\boldsymbol}{\\epsilon}_n}}{\\partial\\theta_j\\partial\\theta_l}\\frac{\\partial t_{{\\boldsymbol}{\\epsilon}_n } }   { \\partial\\theta_k}\\right)\\right]([a , b])w(a , b)\\mathrm{d}a\\mathrm{d}b\\right|\\nonumber\\\\   & : = & c_1({\\boldsymbol}{\\epsilon}_n)\\leq c_2,\\nonumber\\end{aligned}\\ ] ] @xmath234 , by the compactness of @xmath112 .",
    "this , together with the strong consistency of @xmath143 , gives @xmath235 @xmath236 .",
    "equivalently , in matrix form , @xmath237 by the multivariate slutsky s theorem , proposition [ prop : parh ] , together with equation ( [ thm3:eqn1 ] ) , ( [ thm3:eqn2 ] ) , and ( [ thm3:eqn3 ] ) , yields the desired result .",
    "+    3    and vitale , r.a .",
    "( 1975 ) . a strong law of large numbers for random compact sets .",
    "_ annals of probability _ , 5 , 879882 .",
    "integrals and set - valued functions . _",
    "journal of mathematical analysis and applications _",
    ", 12 , 112 .    and diday , e. ( 2003 ) . from the statistics of data to the statistics of knowledge",
    ": symbolic data analysis .",
    "_ journal of the american statistical association _ , 98 , 462 , review article .    ,",
    "neto , e.a.l . , and tenorio , c.p .",
    "( 2004 ) . a new method to fit a linear regression model for interval - valued data . _ lecture notes in computer science _ , 3238 , 295306 .",
    "least squares fitting of compact set - valued data . _ journal of mathematical analysis and applications _ , 147 , 531544 .    , lubiano , m.a .",
    ", montenegro , m. , and lopez , m.t .",
    "least squares fitting of an affine function and strength of association for interval - valued data .",
    "_ metrika _ , 56 , 97111 .",
    "asymptotic properties of minimum contrast estimators for parameters of boolean models .",
    "_ metrika _ , 40 , 6974 .",
    "sur la fonction dappui des ensembles convexes dans un espace localement convexe . _",
    "arkiv for mat . _ , 3 , 181186 .",
    "foundations of a theory of random sets .",
    "_ in stochastic geometry",
    "harding , e.f . and kendall , d.g .",
    "john wiley @xmath238 sons , new york .",
    "a variance of compact convex random sets .",
    "working paper , institut fr stochastik , bernhard - von - cotta - str .",
    "2 09599 freiberg .",
    "( 1997 ) . on the variance of fuzzy random variables .",
    "_ fuzzy sets and systems _ , 92 , 8393 .    and nther , w. ( 1998 ) . linear regression with random fuzzy variables : extended classical estimates , best linear estimates , least squares estimates .",
    "_ information sciences _ , 109 , 95118 .",
    "( 1979 ) . on limit theorems for sums of independent compact random subsets in the euclidean space .",
    "translated from _",
    "zapiski nauchnykh seminarov leningradskogo otdeleniya matematicheskogo instituta _ , 85 , 113128 .",
    "statistics of random compacts in euclidean space . translated from _",
    "zapiski nauchnykh seminarov leningradskogo otdeleniya matematicheskogo instituta _ , 98 , 115139 .",
    "_ lments pour une thorie des milieux poreux_. masson , paris .",
    "_ random sets and integral geometry_. john wiley @xmath238 sons , new york .",
    "( 2005 ) . _",
    "theory of random sets_. springer - verlag , london .",
    "integral probability metrics and their generating classes of functions .",
    "_ advances in applied probability _ , 29 , 429 - 443 .",
    "convergence and existence of random set distributions . _",
    "the annals of probability _ , 12 , 3 , 726732 .",
    "( 1969 ) . on the measurability and consistency of minimum contrast estimates .",
    "_ metrika _ , 14 , 249272 .    ,",
    "ralescu , d.a . , and ralescu , s.s .",
    "gaussian random sets in banach space .",
    "_ theory of probablity and its applications _",
    ", 31 , 526529 .    ( 1952 ) . an embedding theorem for spaces of convex sets .",
    "_ , 3 , 165169 .    ( 1993 ) .",
    "_ convex bodies : the brunn - minkowski theory_. cambridge university press , cambridge .",
    "on the empirical estimation of integral probability metrics . _ electronic journal of statistics _ , 6 , 1550 - 1599 .",
    "random sets : models and statistics . _ international statistical review _ , 66 , 1 , 1 - 27 .",
    "parameter estimation and model selection for neyman - scott point processes .",
    "_ biometrical journal _ , 50 , 43 - 57 .",
    "probability metrics .",
    "_ theory of probability and its applications _",
    ", 28 , 278 - 302 .",
    "notice that @xmath128;x(n))$ ] is the sample mean of i.i.d .",
    "random variables @xmath239 defined as : @xmath240\\neq\\emptyset , \\\\      0 , & \\text{otherwise}.    \\end{cases}.\\ ] ] therefore , an application of the strong law of large numbers in the classical case yields : @xmath241\\neq\\emptyset\\right )    = t_{\\boldsymbol{\\theta}_0}\\left([a , b]\\right),\\ \\text{as}\\ n\\to\\infty,\\ ] ] @xmath242 , and assuming @xmath205 is the true parameter value .",
    "that is , @xmath243;x(n)\\right)\\stackrel{a.s.}{\\rightarrow}t_{\\boldsymbol{\\theta}_0}\\left([a , b]\\right ) , \\nonumber\\ ] ] as @xmath244 .",
    "it follows immediately that @xmath245;x(n))-t_{\\boldsymbol{\\theta}_0}\\left([a , b]\\right)\\right]^2w(a , b)\\stackrel{a.s.}{\\rightarrow}0 .",
    "\\nonumber\\ ] ] notice that @xmath242 , @xmath246;x(n))-t_{\\boldsymbol{\\theta}_0}\\left([a , b]\\right)\\right]^2w(a , b)$ ] is uniformly bounded by @xmath247 . by the bounded convergence theorem , @xmath248;x(n))-t_{\\boldsymbol{\\theta}_0}\\left([a , b]\\right)\\right]^2w(a , b)\\mathrm{d}a\\mathrm{d}b    \\stackrel{a.s.}{\\rightarrow}\\iint\\limits_{s}0\\cdot \\mathrm{d}a\\mathrm{d}b=0 , \\nonumber\\ ] ] given any @xmath249 with finite lebesgue measure .",
    "this verifies that @xmath250 similarly , we also get @xmath251)-t_{\\boldsymbol{\\zeta}}([a , b])\\right]^2w(a , b)\\mathrm{d}a\\mathrm{d}b\\right\\}=1,\\ ] ] @xmath252 . equations ( [ eqn : n1 ] ) and ( [ eqn : n2 ] ) together imply @xmath253)-t_{\\boldsymbol{\\zeta}}([a , b])\\right]^2w(a , b)\\mathrm{d}a\\mathrm{d}b,\\    \\boldsymbol{\\theta } , \\boldsymbol{\\zeta}\\in\\theta.\\ ] ] by assumption [ aspt:2 ] , @xmath254)\\neq t_{\\boldsymbol{\\zeta}}([a , b])$ ] , for @xmath255 , except on a lebesgue set of measure 0 .",
    "this together with ( [ eqn : n ] ) gives @xmath256 which proves that @xmath138 , @xmath257 is a family of contrast functions . to see the equicontinuity of @xmath138 , notice that @xmath258 , we have @xmath259)-\\hat{t}([a , b];x(n))\\right)^2w(a , b)\\mathrm{d}a\\mathrm{d}b\\\\    & & -\\iint\\limits_{s}\\left(t_{\\boldsymbol{\\theta}_2}([a , b])-\\hat{t}([a , b];x(n))\\right)^2w(a , b)\\mathrm{d}a\\mathrm{d}b|\\\\    & = & |\\iint\\limits_{s}\\left(t_{\\boldsymbol{\\theta}_1}([a , b])-t_{\\boldsymbol{\\theta}_2}([a , b])\\right )    \\left(t_{\\boldsymbol{\\theta}_1}([a , b])+t_{\\boldsymbol{\\theta}_2}([a , b ] )    -2\\hat{t}([a , b];x(n))\\right)w(a , b)\\mathrm{d}a\\mathrm{d}b|\\\\    & \\leq&4c\\iint\\limits_{s}\\left|t_{\\boldsymbol{\\theta}_1}([a , b])-t_{\\boldsymbol{\\theta}_2}([a , b])\\right|\\mathrm{d}a\\mathrm{d}b,\\end{aligned}\\ ] ] since , by definition ( [ h_def ] ) , @xmath260 is uniformly bounded by @xmath261 , @xmath262 then the equicontinuity of @xmath138 follows from the continuity of @xmath254)$ ] .",
    "let @xmath138 be the contrast function defined in ( [ h_def ] ) . under the hypothesis of assumption [ aspt:4 ] , @xmath263    \\stackrel{\\mathcal{d}}{\\rightarrow }    n\\left(0,\\delta_i\\right),\\ \\text{as}\\ n\\to\\infty,\\ ] ] for @xmath115 , where @xmath264\\neq\\emptyset , x_1\\cap[c , d]\\neq\\emptyset\\right )    -t_{{\\boldsymbol}{\\theta}_0}\\left([a , b]\\right)t_{{\\boldsymbol}{\\theta}_0}\\left([c , d]\\right)\\right\\}\\nonumber\\\\    & & \\times\\frac{\\partial t_{{\\boldsymbol}{\\theta}_0}}{\\partial\\theta_i}\\left([a , b]\\right )    \\frac{\\partial t_{{\\boldsymbol}{\\theta}_0}}{\\partial\\theta_i}\\left([c , d]\\right )    w(a , b)w(c , d)\\mathrm{d}a\\mathrm{d}b\\mathrm{d}c\\mathrm{d}d.\\nonumber\\end{aligned}\\ ] ]    we will write @xmath265\\right)}{\\partial\\theta_i}= t_{{\\boldsymbol}{\\theta}_0}^i\\left(a , b\\right)$ ] to simplify notations . exchanging differentiation and integration by the bounded convergence theorem , we get @xmath266\\right)-\\hat{t}\\left([a , b];x(n)\\right)\\right)^2w(a , b)\\mathrm{d}a\\mathrm{d}b\\nonumber\\\\    & = & \\iint\\limits_{s}\\frac{\\partial}{\\partial\\theta_i }    \\left(t_{{\\boldsymbol}{\\theta}_0}\\left([a , b]\\right)-\\hat{t}\\left([a , b];x(n)\\right)\\right)^2w(a , b)\\mathrm{d}a\\mathrm{d}b\\nonumber\\\\    & = & \\iint\\limits_{s}2\\left(t_{{\\boldsymbol}{\\theta}_0}\\left([a , b]\\right)-\\hat{t}\\left([a , b];x(n)\\right)\\right )    t_{{\\boldsymbol}{\\theta}_0}^i\\left(a , b\\right)w(a , b)\\mathrm{d}a\\mathrm{d}b.\\nonumber\\end{aligned}\\ ] ] define @xmath267 as in ( [ y_def ] ) . then , @xmath268\\right)-\\frac{1}{n}\\sum_{k=1}^{n}y_k\\left(a , b\\right)\\right )    t_{{\\boldsymbol}{\\theta}_0}^i\\left(a , b\\right)w(a , b)\\mathrm{d}a\\mathrm{d}b\\nonumber\\\\    & = & \\frac{2}{n}\\iint\\limits_{s}\\sum_{k=1}^{n}\\left(t_{{\\boldsymbol}{\\theta}_0}\\left([a , b]\\right)-y_k\\left(a , b\\right)\\right )    t_{{\\boldsymbol}{\\theta}_0}^i\\left(a , b\\right)w(a , b)\\mathrm{d}a\\mathrm{d}b\\nonumber\\\\    & = & \\frac{1}{n}\\sum_{k=1}^{n}2\\iint\\limits_{s}\\left(t_{{\\boldsymbol}{\\theta}_0}\\left([a , b]\\right)-y_k\\left(a , b\\right)\\right )    t_{{\\boldsymbol}{\\theta}_0}^i\\left(a , b\\right)w(a , b)\\mathrm{d}a\\mathrm{d}b\\label{eqn : parh}\\\\    & : = & \\frac{1}{n}\\sum_{k=1}^{n}r_k.\\nonumber\\end{aligned}\\ ] ] notice that @xmath269 s are i.i.d .",
    "random variables : @xmath270 .",
    "+ let @xmath271 be a partition of @xmath181 , and @xmath272 be any point in @xmath273 , @xmath274 .",
    "let @xmath275 .",
    "denote by @xmath276 the area of @xmath273 . by the definition of the double integral , @xmath277\\right)-y_k\\left(a , b\\right)\\right )    t_{{\\boldsymbol}{\\theta}_0}^i\\left(a , b\\right)w(a , b)\\mathrm{d}a\\mathrm{d}b\\nonumber\\\\    & = & \\lim_{\\lambda\\rightarrow 0}\\left\\{\\sum_{j=1}^{m}\\left(t_{{\\boldsymbol}{\\theta}_0 }    \\left([a_j , b_j]\\right)-y_k\\left(a_j , b_j\\right)\\right )    t_{{\\boldsymbol}{\\theta}_0}^i\\left(a_j , b_j\\right)w(a_j , b_j)\\delta\\sigma_j\\right\\}.\\nonumber\\end{aligned}\\ ] ] therefore , by the lebesgue dominated convergence theorem , @xmath278\\right)-y_k\\left(a_j , b_j\\right)\\right )    t_{{\\boldsymbol}{\\theta}_0}^i\\left(a_j , b_j\\right)w(a_j , b_j)\\delta\\sigma_j\\right\\}\\\\    & = & 2\\lim_{\\lambda\\rightarrow 0}\\left\\{\\sum_{j=1}^{m}\\left[e\\left(t_{{\\boldsymbol}{\\theta}_0 }    \\left([a_j , b_j]\\right)-y_k\\left(a_j , b_j\\right)\\right)\\right ]    t_{{\\boldsymbol}{\\theta}_0}^i\\left(a_j , b_j\\right)w(a_j , b_j)\\delta\\sigma_j\\right\\}\\label{eqn_1}\\\\    & = & 2\\lim_{\\lambda\\rightarrow 0}\\left\\{\\sum_{j=1}^{m}0\\right\\}=0.\\end{aligned}\\ ] ]    moreover , @xmath279\\right)-y_k\\left(a_j , b_j\\right)\\right )    t_{{\\boldsymbol}{\\theta}_0}^i\\left(a_j , b_j\\right)w(a_j , b_j)\\delta\\sigma_j\\right\\}\\right\\}^2\\\\    & = & 4e\\lim_{\\lambda_1\\rightarrow 0}\\lim_{\\lambda_2\\rightarrow 0 }    \\left\\{\\sum_{j_1=1}^{m_1}\\left(t_{{\\boldsymbol}{\\theta}_0 }    \\left([a_{j_1},b_{j_1}]\\right)-y_k\\left(a_{j_1},b_{j_1}\\right)\\right )    t_{{\\boldsymbol}{\\theta}_0}^i\\left(a_{j_1},b_{j_1}\\right)w(a_{j_1},b_{j_1})\\delta\\sigma_{j_1}\\right\\}\\\\    & & \\left\\{\\sum_{j_2=1}^{m_2}\\left(t_{{\\boldsymbol}{\\theta}_0 }    \\left([a_{j_2},b_{j_2}]\\right)-y_k\\left(a_{j_2},b_{j_2}\\right)\\right )    t_{{\\boldsymbol}{\\theta}_0}^i\\left(a_{j_2},b_{j_2}\\right)w(a_{j_2},b_{j_2})\\delta\\sigma_{j_2}\\right\\}\\\\    & = & 4e\\lim_{\\lambda_1\\rightarrow 0}\\lim_{\\lambda_2\\rightarrow 0}\\sum_{j_1=1}^{m_1}\\sum_{j_2=1}^{m_2 }    \\left(t_{{\\boldsymbol}{\\theta}_0}\\left([a_{j_1},b_{j_1}]\\right)-y_k\\left(a_{j_1},b_{j_1}\\right)\\right )    \\left(t_{{\\boldsymbol}{\\theta}_0}\\left([a_{j_2},b_{j_2}]\\right)-y_k\\left(a_{j_2},b_{j_2}\\right)\\right)\\\\    & & t_{{\\boldsymbol}{\\theta}_0}^i\\left(a_{j_1},b_{j_1}\\right)t_{{\\boldsymbol}{\\theta}_0}^i\\left(a_{j_2},b_{j_2}\\right )    w(a_{j_1},b_{j_1})w(a_{j_2},b_{j_2})\\delta\\sigma_{j_1}\\delta\\sigma_{j_2}\\\\    & = & 4\\lim_{\\lambda_1\\rightarrow 0}\\lim_{\\lambda_2\\rightarrow 0}\\sum_{j_1=1}^{m_1}\\sum_{j_2=1}^{m_2 }    e\\left(t_{{\\boldsymbol}{\\theta}_0}\\left([a_{j_1},b_{j_1}]\\right)-y_k\\left(a_{j_1},b_{j_1}\\right)\\right )    \\left(t_{{\\boldsymbol}{\\theta}_0}\\left([a_{j_2},b_{j_2}]\\right)-y_k\\left(a_{j_2},b_{j_2}\\right)\\right)\\\\    & & t_{{\\boldsymbol}{\\theta}_0}^i\\left(a_{j_1},b_{j_1}\\right)t_{{\\boldsymbol}{\\theta}_0}^i\\left(a_{j_2},b_{j_2}\\right )    w(a_{j_1},b_{j_1})w(a_{j_2},b_{j_2})\\delta\\sigma_{j_1}\\delta\\sigma_{j_2}\\label{eqn_2}\\\\    & = & 4\\lim_{\\lambda_1\\rightarrow 0}\\lim_{\\lambda_2\\rightarrow 0}\\sum_{j_1=1}^{m_1}\\sum_{j_2=1}^{m_2 }    cov\\left(y_k\\left(a_{j_1},b_{j_1}\\right),y_k\\left(a_{j_2},b_{j_2}\\right)\\right)\\\\    & & t_{{\\boldsymbol}{\\theta}_0}^i\\left(a_{j_1},b_{j_1}\\right)t_{{\\boldsymbol}{\\theta}_0}^i\\left(a_{j_2},b_{j_2}\\right )    w(a_{j_1},b_{j_1})w(a_{j_2},b_{j_2})\\delta\\sigma_{j_1}\\delta\\sigma_{j_2}\\\\    & = & 4\\iiiint\\limits_{s\\times s}cov\\left(y_k\\left(a , b\\right),y_k\\left(c , d\\right)\\right )    t_{{\\boldsymbol}{\\theta}_0}^i\\left(a , b\\right)t_{{\\boldsymbol}{\\theta}_0}^i\\left(c , d\\right )    w(a , b)w(c , d)\\mathrm{d}a\\mathrm{d}b\\mathrm{d}c\\mathrm{d}d\\\\    & = & 4\\iiiint\\limits_{s\\times s}\\left\\{p\\left(x_k\\cap[a , b]\\neq\\emptyset , x_k\\cap[c , d]\\neq\\emptyset\\right )    -t_{{\\boldsymbol}{\\theta}_0}\\left([a , b]\\right)t_{{\\boldsymbol}{\\theta}_0}\\left([c , d]\\right)\\right\\}\\\\    & & t_{{\\boldsymbol}{\\theta}_0}^i\\left(a , b\\right)t_{{\\boldsymbol}{\\theta}_0}^i\\left(c , d\\right )    w(a , b)w(c , d)\\mathrm{d}a\\mathrm{d}b\\mathrm{d}c\\mathrm{d}d.\\end{aligned}\\ ] ] from the central limit theorem for i.i.d .",
    "random variables , the desired result follows .      by the cramr - wold device",
    ", it suffices to prove @xmath280 for arbitrary real numbers @xmath281 .",
    "it is easily seen from ( [ eqn : parh ] ) in the proof of lemma 1 that @xmath282\\right)-y_k\\left(a , b\\right)\\right )   \\frac{\\partial t_{{\\boldsymbol}{\\theta}_0}}{\\partial\\theta_i}\\left([a , b]\\right)w(a , b)\\mathrm{d}a\\mathrm{d}b\\right)\\nonumber\\\\   & : = & \\frac{1}{n}\\sum\\limits_{k=1}^{n}\\left(2\\sum\\limits_{i=1}^p\\lambda_iq_k^i\\right).\\nonumber\\end{aligned}\\ ] ] by lemma 1 , @xmath283 in view of the central limit theorem for i.i.d . random variables , ( [ prop1:target ] ) is reduced to proving @xmath284 by a similar argument as in lemma 1 , together with some algebraic calculations , we obtain @xmath285\\right)-y_k\\left(a , b\\right)\\right )   \\frac{\\partial t_{{\\boldsymbol}{\\theta}_0}}{\\partial\\theta_i}\\left([a , b]\\right)w(a , b)\\mathrm{d}a\\mathrm{d}b\\right)\\\\   & & \\left(\\iint\\limits_{s}\\left(t_{{\\boldsymbol}{\\theta}_0}\\left([a , b]\\right)-y_k\\left(a , b\\right)\\right )   \\frac{\\partial t_{{\\boldsymbol}{\\theta}_0}}{\\partial\\theta_j}\\left([a , b]\\right)w(a , b)\\mathrm{d}a\\mathrm{d}b\\right)\\\\   & = & 4\\sum\\limits_{1\\leq i , j\\leq p}\\lambda_i\\lambda_j\\iiiint\\limits_{s\\times s}\\left\\{p\\left(x_1\\cap[a , b]\\neq\\emptyset , x_1\\cap[c , d]\\neq\\emptyset\\right )    -t_{{\\boldsymbol}{\\theta}_0}\\left([a , b]\\right)t_{{\\boldsymbol}{\\theta}_0}\\left([c , d]\\right)\\right\\}\\\\    & & \\frac{\\partial t_{{\\boldsymbol}{\\theta}_0}}{\\partial\\theta_i}\\left([a , b]\\right )    \\frac{\\partial t_{{\\boldsymbol}{\\theta}_0}}{\\partial\\theta_j}\\left([c , d]\\right )    w(a , b)w(c , d)\\mathrm{d}a\\mathrm{d}b\\mathrm{d}c\\mathrm{d}d.\\end{aligned}\\ ] ] this validates ( [ prop1:target2 ] ) , and hence finishes the proof ."
  ],
  "abstract_text": [
    "<S> many statistical data are imprecise due to factors such as measurement errors , computation errors , and lack of information . in such cases , data are better represented by intervals rather than by single numbers . </S>",
    "<S> existing methods for analyzing interval - valued data include regressions in the metric space of intervals and symbolic data analysis , the latter being proposed in a more general setting . </S>",
    "<S> however , there has been a lack of literature on the parametric modeling and distribution - based inferences for interval - valued data . in an attempt to fill this gap , </S>",
    "<S> we extend the concept of normality for random sets by lyashenko and propose a normal hierarchical model for random intervals . in addition </S>",
    "<S> , we develop a minimum contrast estimator ( mce ) for the model parameters , which we show is both consistent and asymptotically normal . </S>",
    "<S> simulation studies support our theoretical findings , and show very promising results . </S>",
    "<S> finally , we successfully apply our model and mce to a real dataset . </S>"
  ]
}