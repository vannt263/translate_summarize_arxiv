{
  "article_text": [
    "inference problem , which aims at learning internal structure of complex systems from empirical output data , has been studied for a long time . among many models , inverse ising model , which is a basic pair - wise model in physics , has been widely studied for inference problem in many fields .",
    "a lot of study@xcite has been carried out on the ( static ) inverse ising problem , which asks to infer couplings and external fields of ising model from configurations sampled from boltzmann distribution .",
    "efficient algorithms based on mean - field approximations and message passing have been developed to address the inference task .",
    "recently it receives special attention in neural network reconstruction of retinas based on multi - electrode recordings@xcite , and gene - gene interaction reconstruction@xcite . in static inverse ising model ,",
    "empirical data are @xmath2 equilibrium configurations @xmath3 that sampled from boltzmann distribution @xmath4 , where energy @xmath5 is a function of couplings and external fields . when @xmath2 is very large , posterior distribution of @xmath6 peaks on the maximum point so that one can find the couplings by maximizing the log - likelihood @xmath7 , where @xmath8 denotes averaged energy .",
    "thanks to the convexity of log - likelihood , the exact method , so called boltzmann machine , computes exactly the correlations and magnetizations and match them with empirical ones .",
    "however , computing magnetizations and correlations exactly for large systems is unreachable , so many efforts have been paid to study approximate approaches based on different approximations , including na \" i ve mean - field approximation , tap equation , small correlation expansion and message passing schemes .",
    "boltzmann distribution is a strong restriction to system to be studied .",
    "many real systems like biological systems , especially with time - dependent external stimuli , do not have this good property .",
    "_ inverse kinetic ising model _ , which asks to reconstruct couplings and time - dependent external fields from dynamic output of real system , obviously meets wider needs in real system inference . in inverse kinetic ising model",
    ", exact reconstruction uses all empirical configurations in learning and time - complexity is proportional to the number of configurations .",
    "when number of configurations is large , one needs efficient approximate methods . in last years , na \" i ve mean - field and tap approaches have been adapted from static ising model in case of weak couplings@xcite . on densely connected fully asymmetric networks , an exact reconstruction method is proposed for asymmetric sk model@xcite .",
    "however , biological systems are often sparsely connected , so it should be valuable to adapt message passing@xcite method which work well in sparse - connected static ising model inference into kinetic ising model inference .",
    "another problem of existing mean - field methods is that they use approximations which predict quantities of time @xmath9 based on time @xmath0 , we term these approximations _ one - step approximations_. there are two cases that one - step approximations works well . in first case ,",
    "experimental data are drawn from stationary state where @xmath10 and in second case there is no feedback loops in system e.g. the fully asymmetric model .",
    "but in practice it is difficult to ensure these two conditions , so it should be useful to develop a method that use statistics at not only one time step before , but earlier time steps to do the construction . in what follows ,",
    "we show how to use bethe approximation in inference of kinetic ising model , in context of dynamic cavity method .",
    "this can be seen as an extension of belief - propagation inference in static inverse ising problems@xcite .",
    "the paper is organized as follows .",
    "firstly , in sec .",
    "[ sec : model ] , we give definition of kinetic ising model . in sec .",
    "[ sec : exact ] we introduce exact inference and existing mean - field approaches . in sec .",
    "[ sec : dc ] we discuss dynamical cavity methods and inference of kinetic ising model .",
    "[ sec : result ] contains some numerical results to compare performance of inference by dynamical cavity method with other mean - field approximations .",
    "the last section contains conclusions and some discussions .",
    "we consider random graphs with @xmath11 vertices and average connectivity @xmath12 . in this paper",
    "we focus on sparse graphs that @xmath13",
    ". connections of graphs are defined by matrix @xmath14 .",
    "@xmath15 means there is a directed edge from vertex @xmath16 to vertex @xmath17 and @xmath18 means there is no such edge . if @xmath19 , the edge between @xmath16 and @xmath17 is symmetric , otherwise it is asymmetric .",
    "we set each edge to be symmetric with probability @xmath20 and asymmetric with probability @xmath21 .",
    "obviously , with @xmath22 , graph is undirected and with @xmath23 , graph is fully asymmetric .    on each vertex @xmath16",
    "there is a spin @xmath24 associated , and on each edge @xmath25 there is a coupling @xmath26 associated . if @xmath27 , coupling @xmath26 is set to @xmath28 otherwise value of @xmath26 is generated randomly according to gaussian distribution with zero mean and variance @xmath29 .",
    "dynamics of spins are defined by configurations at different discrete time steps from time @xmath28 to time @xmath30 : @xmath31 . at each time step ,",
    "value of spin @xmath16 is updated based on configuration of last time according to @xmath32=\\frac{\\exp\\left[\\beta\\sigma_i{\\left(t+1\\right)}h_i(t)\\right]}{2\\cosh{\\beta h_i(t)}},\\ ] ] where @xmath33 denotes the inverse temperature and local field at time @xmath0 is expressed as @xmath34 with @xmath35 denoting external fields acting on spin @xmath16 at time @xmath0 .",
    "we consider in this paper the parallel dynamics , that all spins updated synchronously , evolution of configuration is written as @xmath36=\\prod_{i=1}^nw[{{\\sigma}_i{\\left(t+1\\right)}}|{\\boldsymbol\\sigma{\\left(t\\right)}}].\\ ] ] we argue that it would be not difficult to extend our approach from parallel dynamics to sequential dynamics following what has been done for mean - field methods in @xcite",
    ".    direct problem of kinetic ising model asks to predict dynamical behavior of spins at an arbitrary time @xmath0 , given couplings , external fields and initial state of network .",
    "calculating full distribution of system @xmath37 is a very difficult task , so most theories focused on computing macroscopic observables e.g. magnetization at a time @xmath38 and correlation at different times @xmath39 .",
    "this direct problem have been studied for long time especially in the context of attractor neural networks@xcite .",
    "many approaches has been proposed to study ensemble averaged macroscopic quantities , e.g. dynamical replica method@xcite , generating functional method@xcite and dynamical cavity method@xcite .",
    "inverse problem of kinetic ising model is the problem we would like to study in this paper . unlike direct problem , instead of couplings and external fields , experimental output of original model ( set of configurations )",
    "is given , one is asked to infer couplings and external fields from those experimental data .",
    "the experimental data are obtained by running parallel dynamics of ising model according to eq .",
    "(  [ eq_wi ] ) on graphs for @xmath40 realizations of time length @xmath30 paths(trajectories ) , denoted by @xmath41 with @xmath42 $ ] and @xmath43 $ ] . in this paper , we set @xmath30 to @xmath44 .",
    "given experimental configurations , probability of observing these samplings as a function of couplings is written as : @xmath45\\nonumber\\\\    & = & \\prod_{r=1}^r\\prod_{t=1}^{t-1}\\prod_{i=1}^n\\exp\\left [ \\beta\\sigma_i^r{\\left(t+1\\right)}h_i^r(t )    - \\log 2\\cosh\\beta h_i^r(t)\\right].\\end{aligned}\\ ] ] then , log - likelihood of observing data is defined as : @xmath46 .\\end{aligned}\\ ] ] to find the most - likely couplings and external fields , one needs to maximize the log - likelihood . at the maximum point of log - likelihood , by setting derivatives of @xmath47 with respect to @xmath26 and @xmath48 to zero , one has following equations : @xmath49}\\sigma^r_j{\\left(t\\right)}\\right>}_t\\right>}_r\\label{eq : equalj } , \\end{aligned}\\ ] ] where @xmath50 denotes taking average over time and @xmath51 denotes taking average over realizations .",
    "let us use @xmath52 and @xmath53 to denote experimental magnetization at time @xmath0 and correlation at time @xmath0 and @xmath1 respectively , and use @xmath54 and @xmath55 to denote magnetization and correlations predicted by ising model .",
    "( [ eq : equalm]),([eq : equalj ] ) show that at max - likelihood point , @xmath56 and @xmath55 are calculated using configurations at time @xmath0 , so we term it _ configurations based one - step reconstruction_. the reconstruction can be carried out by using gradient descent learning starting from an initial couplings and fields : @xmath57 where constant @xmath58 is learning rate . note that in above scheme , one has to scan all configurations to compute the average on every learning step .",
    "it is time consuming and not realistic when amount of experimental data is huge . to overcome this computational problem ,",
    "several mean - field approaches , e.g. na \" i ve mean - field , tap and simply mean - field method , are proposed .",
    "they use different approximations to compute macroscopic observables , say average magnetizations and correlations , and use those macroscopic observables , instead of all configurations , to do inference .",
    "na \" i ve mean - field method@xcite applies nave mean - field approximation @xmath59},\\ ] ] and compute one - step delayed correlation as @xmath60}.\\end{aligned}\\ ] ] as an extension of na \" i ve mean - field , tap method takes the expansion on couplings and outperforms na \" i ve tap method in case of weak couplings , the approximation is written as : @xmath61},\\ ] ] and delayed correlation is expressed as @xmath62}\\beta{\\left[1-{\\left[1-(m_i^j(t+1))^2\\right]}\\sum_kj_{ij}^2(1-m_k^{2}(t)\\right]}\\nonumber\\\\    & \\times&\\sum_k j_{ik}{\\left[c_{kj}^{}(t , t)-m_k^{}(t)m_j^{}(t)\\right]}.\\end{aligned}\\ ] ] another recently proposed approach , simply mean - field method@xcite , which assumes gaussian distribution of the local field of each spin , computes magnetization as @xmath63 and delayed correlation as @xmath64}\\nonumber\\\\    & \\times&\\sum_k j_{ik}{\\left[c_{kj}^{}(t , t)-m_k^{}(t)m_j^{}(t)\\right]}.\\end{aligned}\\ ] ] in above listed mean - field approximations , predictions of @xmath56 and @xmath55 are made by using magnetizations and correlations at time @xmath0 , this kind of reconstruction can be termed _ statistics based one - step reconstruction_. in some special cases , e.g. fully asymmetric networks , observables at time @xmath0 are sufficient to predict observables at time @xmath9 since effect of feedback loops are not present .",
    "but in general cases , to predict observables at time @xmath9 one needs to use not only observables at time @xmath0 but also at earlier time steps up to @xmath65 .",
    "dynamical cavity method , originally proposed in @xcite , together with dynamical replica analysis and generating functional analyses , are powerful tools in analysing dynamics of networks .",
    "the advantage of dynamical cavity method with respect to other two methods is that it can be applied on single instances to compute observables for every node .    from eq .",
    "( [ eq_wip ] ) , we can write the probability of observing a path as : @xmath66}})=\\prod_{t=0}^tw[{\\boldsymbol \\sigma(t+1})|{\\boldsymbol \\sigma(t)}]p({\\boldsymbol \\sigma(0)}),\\end{aligned}\\ ] ] where @xmath67 denotes initial probability distribution of spin configuration at time @xmath28 . it is difficult to study directly the full path distribution , so dynamical cavity method focuses on marginal probability of a path that spin @xmath16 evolves from time @xmath28 to time @xmath0 conditioning on the local fields @xmath68}$ ] : @xmath69}}|\\theta_i^{[0,t]})=\\sum _ { { \\boldsymbol \\sigma_{j\\neq i}^{[0,t]}}}p(\\{{\\boldsymbol \\sigma_i^{[0,t]}};{\\boldsymbol \\sigma_j^{[0,t]}}\\}|{\\boldsymbol \\theta^{[0,t]}}).\\ ] ] by applying bethe approximation , one can derive following iterative cavity equations ( we refer to literature @xcite for details of derivations ) : @xmath70}}|{\\boldsymbol \\theta_i^{[0,t-1]}}+j_{ki}{\\boldsymbol \\sigma_k^{[0,t-1]}})&=&\\sum_{{\\boldsymbol \\sigma_j^{[0,t-1 ] } } }   \\prod_{j\\in\\partial i\\backslash k } p_{j\\to i}({\\boldsymbol \\sigma_{j}^{[0,t-1]}}|{\\boldsymbol \\theta_{j}^{[0,t-1]}}+j_{ji}{\\boldsymbol \\sigma_i^{[0,t-1]}})\\nonumber\\\\&\\cdot &   \\prod_{t'=1}^{t-1 }    \\frac{e^{\\beta \\sigma_i^{t'}[\\sum_{j\\in\\partial i}j_{ij}\\sigma_{j}^{t'-1}+    \\theta_i^{t'-1 } ] } }    { 2\\cosh\\beta [ \\sum_{j\\in \\partial i}j_{ij}\\sigma_{j}^{t'-1 }    + \\theta_i^{t'-1}]}p_i(\\sigma_i^0),\\end{aligned}\\ ] ] where @xmath71}}|{\\boldsymbol \\theta_i^{[0,t-1]}}+j_{ki}{\\boldsymbol \\sigma_k^{[0,t-1]}})$ ] is cavity message denoting ( cavity ) marginal probability of a path that spin @xmath16 evolves from time @xmath28 to time @xmath0 with its neighbor spin @xmath72 removed from the graph .    above equation is exact on tree graphs and a good approximation on sparse graphs . after a fixed point of cavity equation",
    "is reached , observables e.g. marginal probabilities , magnetizations and correlations can be computed as functions of cavity messages . however , solving equation eq .",
    "( [ eq : cavity ] ) is very time - consuming for large @xmath0 because computational complexity is proportional to @xmath73 .",
    "so in practice one needs approximations to reduce computational complexity .",
    "here we adopt the most simple approximation , one - time approximation@xcite , which is also named time - factorization approximation@xcite : @xmath74}}|j_{ki}{\\boldsymbol \\sigma_k^{[0,t-1 ] } } ) = \\prod_{t'=0}^{t}p_{i\\to k}(\\sigma_i^{t'}|j_{ki}\\sigma_k^{t'-1}).\\ ] ] this approximation ignores correlations among different time steps before time @xmath75 , and makes summation over variables before time @xmath75 possible .",
    "then , from eq .",
    "( [ eq : cavity ] ) , one arrives at @xmath76p_i(\\sigma_i^{t-2}),\\end{aligned}\\ ] ] and expression of magnetizations and delayed correlations can be derived from above equation @xmath77p_i(\\sigma_i^{t-1})\\sigma_i(t+1)\\nonumber\\\\\\\\    c_{ij}^j(t+1,t)&=&\\sum_{{\\boldsymbol \\sigma_k^{t } } } \\sum_{{\\sigma_j^{t } } } \\prod_{k\\in \\partial i}p_{k\\to i}{\\left ( { \\sigma_k^{t } } |j_{ki}{\\sigma_i^{t-1}}\\right ) } p_{j\\to i}{\\left ( { \\sigma_j^{t } } |j_{ji}{\\sigma_i^{t-1}}\\right)}\\tanh{\\left(\\beta\\sum_{k\\in\\partial i}j_{ik}\\sigma_{k}^{t}\\right)}\\sigma_{j}^{t}\\label{eq : c2}.\\end{aligned}\\ ] ] with @xmath56 and @xmath55 obtained , a standard procedure to perform inference is using gradient descent to learn couplings from differences between experimental and predicted correlations , and learn external fields from differences between experimental and predicted correlations .",
    "@xmath78 where @xmath79}\\\\    \\delta c_{ij}&=&\\frac{1}{t-1}\\sum_{t=2}^t{\\left[c^{data}_{ij}(t , t-1)-",
    "c^{j}_{ij}(t , t-1)\\right]}.\\end{aligned}\\ ] ] note that there are two ways to do the inference using magnetizations and delayed correlations , one way as used in this paper is to learn couplings by gradient descent , and the other one is to solve directly the couplings by matching exactly the experimental data , which usually requires inversion of a correlation - based matrix@xcite .",
    "usually gradient descent learning works slower than matrix inversion method and the convergence of learning method may depend on initial couplings and learning rate .",
    "however , in matrix inversion method , sometimes it is difficult to find a set of couplings and external fields that exactly matches the noisy experimental data , while the learning method is able to converge to a fixed point close to the true experimental data ( as shown in fig .",
    "[ fig : con ] ) .",
    "in this section we make comparative analysis between dynamical cavity method and mean - field methods on inverse kinetic ising problem .",
    "before we compare performance of those methods , the first thing we are interested in is the convergence properties of gradient descent learning used in our scheme . in fig .",
    "[ fig : con ] we plot the evolution of average inference error at each time step @xmath80 and average difference of one - step delayed correlation @xmath81 given by dynamical inference in the gradient descent process . in fig .",
    "[ fig : con ] we recorded evolution of @xmath82 and @xmath83 in three gradient descent process on the same network with different coupling strengths .",
    "evolution of inference error @xmath82 and difference of one - step delayed correlations of dynamical cavity inference as function of iterating time for networks with @xmath84 , @xmath85 , @xmath23 and different coupling strength .",
    "number of realizations used in experimental data @xmath40 is @xmath86 .",
    "networks are fully asymmetric with @xmath87 and @xmath88 in top , middle and bottom panel respectively . ]",
    "we can see that in three panels of fig .",
    "[ fig : con ] , all correlation differences converge in the learning , but evolutions of @xmath82 are much different . in case of weak couplings(e.g .",
    "@xmath89 at left panel ) , @xmath82 decreases monotonously with difference of correlation till converges to the error level characterized by noise in experimental data . in case of @xmath90 ,",
    "@xmath82 decreases to a minimum value then start increasing and finally converges to a point larger than the minimum value . when @xmath91 increases to @xmath88 , @xmath82 keeps increasing after reaching the minimum point and finally goes beyond the initial error and diverges .",
    "coupling strength plays a role of inverse temperature , a larger coupling strength gives equivalently low temperature in the model and dynamics of the model becomes more difficult to predict due to stronger correlations .",
    "consequently , approximations we made in dynamical method turns worse with stronger couplings and results to worse convergence of gradient descent learning and larger inference error .",
    "we have compared convergence properties of gradient descent learning with other approximations on the same network(data not shown ) , results show that with na \" i ve mean - field , tap and simply mean - field approximations , @xmath82 stops convergence with couplings weaker than @xmath92 because they give even worse approximations than dynamical cavity method on larger couplings ( see also fig .",
    "[ fig : n100c5 ] ) .",
    "however exact reconstruction does not have this converging problem as it does not use approximations so inference error only depends on quality of data(e.g .",
    "number of realizations used in computing experimental data ) , and is not a function of coupling strength .    to evaluate performance of approximations ,",
    "especially how it is influenced by coupling strength , one method is to compare accuracy of approximations in direct problem , e.g. , by applying approximations on graphs with fixed couplings and external fields to compute magnetization and correlation and compare result with experimental data .",
    "error in correlations are characterized by eq .",
    "( [ eq : dcj ] ) and now @xmath83 is not a function of time @xmath0 since couplings are fixed .",
    "error of magnetization is defined in the same way by difference of magnetization given by approximations and that in the experimental data : @xmath93     difference of magnetizations @xmath94 , one - step delayed correlations @xmath83 and inference error @xmath82 given by na \" i ve mean - field , tap , simply mean - field and dynamical cavity method(dc ) of ising model with gaussian distributed couplings with zero mean and variance @xmath91 .",
    "@xmath95 axes denote coupling strength @xmath91 .",
    "size of the graph is @xmath84 , average degree is @xmath96 .",
    "fraction of symmetric edges @xmath23 in left @xmath97 figures , @xmath98 in middle @xmath97 figures and in the right ones @xmath99 .",
    "data are averaged over @xmath44 realizations .",
    "some data of inference errors are not present because of un - convergence of gradient descent learning . ]",
    "result are plotted in fig .",
    "[ fig : n100c5 ] , where graphs have same number of nodes and average connectivity but different degree of symmetry . note that number of samplings used in computing experiment data is large enough , and noise in experimental data can be ignored compared with error made in approximations , so a smaller @xmath94 and @xmath82 indicates a lower error made by the approximation .",
    "top and middle panels of fig .",
    "[ fig : n100c5 ] show that in direct problem , all approximations work better in asymmetric networks than in symmetric networks .",
    "this is because in asymmetric networks , feedback correlations are not present so correlation ignored by approximations are fewer than in symmetric networks . among all method , dynamical method works the best in this direct problem , which indicates that bethe approximations is more accurate than mean - field approximations in diluted networks .",
    "more over , tap method outperforms na \" i ve mean - field only with weak couplings , because the expansion is carried out with weak couplings , and it is more difficult to find a solution in eq .",
    "( [ eq : tapm ] ) with stronger couplings .    to evaluate performance of inference based on these approximations , we computed inference error @xmath82 given in eq .",
    "( [ eq : dj ] ) by different approximations on same set of graphs , results are plotted in bottom panel of fig .",
    "[ fig : n100c5 ] . tap works badly and stops converging on networks with very weak coupling strength , so i did not plot the inference error of tap .",
    "the missing points in the figure indicate that with value of that coupling strength , gradient descent learning does not converge .",
    "figures show that all methods perform worse when degree of symmetry increases , as in the direct problems . dynamical cavity method converges with larger @xmath91 and gives smaller inference error at same @xmath91 than na \" i ve mean - field and simply mean - field .",
    "simply mean - field outperforms na \" i ve mean - field only with weak couplings and stops converging with weaker couplings .",
    "one interesting point is that , with degree of symmetry of network increases , the difference of @xmath83 and @xmath82 between dynamical cavity method and simply mean - field becomes larger while the difference between na \" i ve mean - field and dynamical cavity becomes smaller .",
    "we saw from fig .",
    "[ fig : con ] and fig .",
    "[ fig : n100c5 ] that , performances of dynamical cavity method is much worse than exact reconstruction .",
    "this is because due to computational expenses , networks we tested here are rather small , so loop effect can not be ignored on these networks ( see e.g. @xcite ) .",
    "if we increase the sparsity of the graph , the error of dynamical cavity method should be decreased .",
    "inference error of dynamical cavity inference(dc ) , na \" i ve mean - field and exact method for networks with different system size @xmath11 .",
    "average connectivity of network is @xmath96 , @xmath23 , @xmath100 and data are averaged over @xmath44 realizations . ]    to illustrate at this point , in fig.[fig : diffn ] we plot inference errors of dynamical cavity method for networks with same average connectivity but different system sizes .",
    "the figure shows that with @xmath11 increasing , inference error approaches the error of exact inference .",
    "we expect that with @xmath101 , inference error of dynamical cavity method gives same result to that of exact reconstruction in fully asymmetric networks",
    ". note that in contrast to dynamical cavity method , mean - field method does not benefits from increasing system size because mean - field approximation does not become better with sparsity increases , and as a consequence , performance between dynamical cavity method and mean - field methods becomes larger for sparse networks with larger system sizes .",
    "in this paper we presented how to use dynamical cavity method to infer kinetic ising model , and compared the performance on sparse graphs with existing mean - field methods .",
    "results show that in sparse graphs dynamical cavity method works better than other mean - field approximations and performance increases with sparsity of graph increases . in computing cavity marginals",
    ", we use the simplest one - time approximation to simplify the cavity equations , and statistical quantities at time @xmath9 are computed from quantities at time @xmath0 and time @xmath1 , which is different from mean - field approximations which do predictions by using only quantities at time @xmath0 .",
    "we believe that dynamical cavity inference could benefit a lot from developing more efficient approximations in computing cavity equations to collect correlations between more time steps .",
    "the networks discussed in this paper have known topology .",
    "it is difficult to apply dynamical cavity method to networks with unknown structure since it is so expensive to run cavity equations on fully - connected networks . for inference problems which asks to reconstruct both network topology and coupling strengths",
    ", one can use another method e.g. simply mean - field to reconstruct the structure then refine the couplings by cavity method .",
    "the author would like to thank abolfazl ramezanpour and riccardo zecchina for discussing ."
  ],
  "abstract_text": [
    "<S> based on dynamical cavity method , we propose an approach to the inference of kinetic ising model , which asks to reconstruct couplings and external fields from given time - dependent output of original system . </S>",
    "<S> our approach gives an exact result on tree graphs and a good approximation on sparse graphs , it can be seen as an extension of belief propagation inference of static ising model to kinetic ising model . while existing mean field methods to the kinetic ising inference e.g. , na \" i ve mean - field , tap equation and simply mean - field , use approximations which calculate magnetizations and correlations at time @xmath0 from statistics of data at time @xmath1 , dynamical cavity method can use statistics of data at times earlier than @xmath1 to capture more correlations at different time steps . </S>",
    "<S> extensive numerical experiments show that our inference method is superior to existing mean - field approaches on diluted networks . </S>"
  ]
}