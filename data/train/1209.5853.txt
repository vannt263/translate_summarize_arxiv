{
  "article_text": [
    "evolutionary algorithms aim to optimize a ` fitness ' function that is either unknown or too complex to model directly .",
    "they allow domain experts to search for good or near - optimal solutions to numerous difficult real - world problems in areas ranging from medicine and finance to control and robotics .",
    "typically , three objectives have to be kept in mind when developing evolutionary algorithms  we want ( 1 ) robust performance ; ( 2 ) few ( potentially costly ) fitness evaluations ; ( 3 ) scalability with problem dimensionality .",
    "we recently introduced natural evolution strategies ( nes ; @xcite ) , a new class of evolutionary algorithms less ad - hoc than traditional evolutionary methods . here",
    "we propose a novel algorithm within this framework .",
    "it retains the theoretically well - founded nature of the original nes while addressing its shortcomings w.r.t .  the above objectives .",
    "nes algorithms maintain and iteratively update a multinormal mutation distribution .",
    "parameters are updated by estimating a _ natural evolution gradient _ ,",
    "i.e.  the natural gradient on the parameters of the mutation distribution , and following it towards better expected fitness .",
    "well - known advantages of natural gradient methods include isotropic convergence on ill - shaped fitness landscapes  @xcite .",
    "this avoids drawbacks of ` vanilla ' ( regular ) gradients which are prone to slow or premature convergence  @xcite .",
    "our algorithm calculates the natural evolution gradient using the _ exact _ fisher information matrix ( fim ) and the monte carlo - estimated gradient . in conjunction with the techniques of _",
    "optimal fitness baselines _ and _ fitness shaping _ this yields robust performance ( objective 1 ) .    to reduce the number of potentially costly evaluations ( objective 2 ) , we introduce _ importance mixing _ , a kind of steady - state enforcer which keeps the distribution of the new population conformed to the current mutation distribution .    to keep the computational cost manageable in higher problem dimensions ( objective 3 ) , we derive a novel , efficient algorithm for computing the inverse of the exact fisher information matrix ( previous methods were either inefficient or approximate ) .",
    "the resulting algorithm , _ efficient natural evolution strategies _ ( enes ) , is elegant , requires no additional heuristics and has few parameters that need tuning .",
    "it performs consistently well on both unimodal and multimodal benchmarks .",
    "first let us introduce the algorithm framework and the concept of evolution gradients .",
    "the objective is to maximize a @xmath0-dimensional unknown fitness function @xmath1 , while keeping the number of function evaluations  which are considered costly  as low as possible .",
    "the algorithm iteratively evaluates a population of size @xmath2 individuals @xmath3 generated from the mutation distribution @xmath4 .",
    "it then uses the fitness evaluations @xmath5 to adjust parameters @xmath6 of the mutation distribution .",
    "let @xmath7 , namely,@xmath8the core idea of our approach is to find , at each iteration , a small adjustment @xmath9 , such that the expected fitness @xmath10 is increased .",
    "the most straightforward approach is to set @xmath11 , where @xmath12 is the gradient on @xmath13 . using the ` log likelihood trick ' ,",
    "the gradient can be written as@xmath14the last term can be approximated using monte carlo:@xmath15where @xmath16 denotes the estimated evolution gradient .    in our algorithm",
    ", we assume that @xmath4 is a gaussian distribution with parameters @xmath17 , where @xmath18 represents the mean , and @xmath19 represents the cholesky decomposition of the covariance matrix @xmath20 , such that @xmath19 is upper triangular matrix and , @xmath21 denotes its inverse and @xmath22 denotes its transpose . ]",
    "the reason why we choose @xmath19 instead of @xmath20 as primary parameter is twofold .",
    "first , @xmath19 makes explicit the @xmath24 independent parameters determining the covariance matrix @xmath20 .",
    "second , the diagonal elements of @xmath19 are the square roots of the eigenvalues of @xmath20 , so @xmath25 is always positive semidefinite . in the rest of the text",
    ", we assume @xmath6 is column vector of dimension @xmath26 with elements in @xmath27 arranged as@xmath28 ^{\\top } \\text{.}\\]]here @xmath29 and @xmath30 ^{\\top } $ ] for @xmath31 , where @xmath32 ( @xmath33 ) denotes the @xmath34-th element of @xmath19 .",
    "now we compute@xmath35 where @xmath36 is assumed to be a @xmath37-dimensional column vector . the gradient w.r.t .",
    "@xmath18 is simply@xmath38the gradient w.r.t .",
    "@xmath32 ( @xmath33 ) is given by@xmath39where @xmath40 is the @xmath34-th element of@xmath41and @xmath42 is the kronecker delta function .    from @xmath36 ,",
    "the mutation gradient @xmath16 can be computed as @xmath43 , where @xmath44 $ ] , and @xmath45 ^{\\top } $ ] .",
    "we update @xmath6 by @xmath46 , where @xmath47 is an empirically tuned step size .",
    "vanilla gradient methods have been shown to converge slowly in fitness landscapes with ridges and plateaus .",
    "natural gradients  @xcite constitute a principled approach for dealing with such problems .",
    "the natural gradient , unlike the vanilla gradient , has the advantage of always pointing in the direction of the steepest ascent .",
    "furthermore , since the natural gradient is invariant w.r.t .",
    "the particular parameterization of the mutation distribution , it can cope with ill - shaped fitness landscapes and provides isotropic convergence properties , which prevents premature convergence on plateaus and avoids overaggressive steps on ridges  @xcite .    in this paper",
    ", we consider a special case of the natural gradient @xmath48 , defined as@xmath49where @xmath50 is an arbitrarily small constant and @xmath51 denotes the kullback - leibler divergence between distributions @xmath52 and @xmath53 .",
    "the constraints impose a geometry on @xmath6 which differs from the plain euclidean one . with @xmath54 ,",
    "the natural gradient @xmath48 satisfies the necessary condition @xmath55 , with @xmath56 being the fisher information matrix:@xmath57 \\text{.}\\ ] ]    if @xmath56 is invertible , which may not always be the case , the natural gradient can be uniquely identified by @xmath58 , or estimated from data using @xmath59 .",
    "the adjustment @xmath9 can then be computed by@xmath60    in the following sub - sections , we show that the fim can in fact be computed exactly , that it is invertible , and that there exists an efficient parameters , which is intractable for most practical problems . ] algorithm to compute the inverse of the fim .      in the original nes",
    "@xcite , we compute the natural evolution gradient using the empirical fisher information matrix , which is estimated from the current population .",
    "this approach has three important disadvantages .",
    "first , the empirical fim is not guaranteed to be invertible , which could result in unstable estimations .",
    "second , a large population size would be required to approximate the exact fim up to a reasonable precision .",
    "third , it is highly inefficient to invert the empirical fim , a matrix with @xmath61 elements .",
    "we circumvent these problems by computing the exact fim directly from mutation parameters @xmath6 , avoiding the potentially unstable and computationally costly method of estimating the empirical fim from a population which in turn was generated from @xmath6 .    in enes ,",
    "the mutation distribution is the gaussian defined by @xmath62 , the precise fim @xmath56 can be computed analytically .",
    "namely , the @xmath63-th element in @xmath56 is given by@xmath64where @xmath65 , @xmath66 denotes the @xmath67-th and @xmath2-th element in @xmath6 .",
    "let @xmath68 be the @xmath69 such that it appears at the @xmath70-th position in @xmath6 .",
    "first , notice that@xmath71and@xmath72so the upper left corner of the fim is @xmath73 , and @xmath56 has the following shape@xmath74 \\text{.}\\]]the next step is to compute @xmath75 .",
    "note that@xmath76 \\text{.}\\]]using the relation@xmath77and the properties of the trace , we get    @xmath78   \\\\ & & + \\operatorname*{tr}\\left [ \\frac{\\partial \\mathbf{a}}{\\partial a_{i_{m},j_{m}}}\\mathbf{c}^{-}\\frac{\\partial \\mathbf{a}^{\\top } } { \\partial a_{i_{n},j_{n}}}\\right ] \\text{.}\\end{aligned}\\ ] ]    computing the first term gives us@xmath79 = \\left ( \\mathbf{a}^{-}\\right ) _ { j_{n},i_{m}}\\left (   \\mathbf{a}^{-}\\right ) _ { j_{m},i_{n}}\\text{.}\\]]note that since @xmath19 is upper triangular , @xmath80 is also upper triangular , so the first summand is non - zero iff@xmath81 in this case , @xmath82 , so@xmath79 = a_{i_{m},i_{n}}^{-2}\\delta \\left ( i_{m},i_{n},j_{m},j_{n}\\right ) \\text{.}\\]]here @xmath83 is the generalized kronecker delta function , i.e. @xmath84 iff all four indices are the same .",
    "the second term is computed as@xmath85 = \\left ( \\mathbf{c}^{-}\\right ) _ { j_{n},j_{m}}\\delta \\left ( i_{n},i_{m}\\right ) \\text{.}\\]]therefore , we have@xmath86it can easily be proven that @xmath75 itself is a block diagonal matrix with @xmath0 blocks along the diagonal , with sizes ranging from @xmath0 to @xmath87 .",
    "therefore , the precise fim is given by@xmath88 \\text{,}\\]]with @xmath89 and block @xmath90 ( @xmath91 ) given by@xmath92 + \\mathbf{d}_{k}\\text{.}\\]]here @xmath93 is the lower - right square submatrix of @xmath73 with dimension @xmath94 , e.g.  @xmath95 , and @xmath96 .",
    "we prove that the fim given above is invertible if @xmath20 is invertible .",
    "@xmath90 @xmath97 being invertible follows from the fact that the submatrix @xmath93 on the main diagonal of a positive definite matrix @xmath73 must also be positive definite , and adding @xmath98 to the diagonal would not decrease any of its eigenvalues . also note that @xmath89 is invertible , so @xmath56 is invertible .",
    "it is worth pointing out that the block diagonal structure of @xmath56 partitions parameters @xmath6 into @xmath99 orthogonal groups @xmath100 , which suggests that we could modify each group of parameters without affecting other groups .",
    "we will need this intuition in the next section .",
    "the exact fim is a block diagonal matrix with @xmath99 blocks .",
    "normally , inverting the fim requires @xmath0 matrix inversions .",
    "however , we can explore the structure of each sub - block in order to make the inverse of @xmath56 more efficient , both in terms of time and space complexity .",
    "first , we realize that @xmath101 is simply a number , so its inversion is given by @xmath102 , and similarly @xmath103 . now , letting @xmath104 vary from @xmath105 to @xmath87 , we can compute @xmath106 and @xmath107 directly from @xmath108 . by block matrix",
    "inversion@xmath109 ^{-}=\\left [   \\begin{array}{cc } \\mathbf{q}_{1}^{- } & -\\mathbf{p}_{11}^{-}\\mathbf{p}_{12}\\mathbf{q}_{2}^{- } \\\\   -\\mathbf{q}_{2}^{-}\\mathbf{p}_{21}\\mathbf{p}_{11}^{- } & \\mathbf{q}_{2}^{-}\\end{array}\\right ] \\text{,}\\]]using@xmath110and the woodbury identity@xmath111 ^{- } \\\\ & = \\mathbf{p}_{22}^{-}-\\mathbf{p}_{22}^{-}\\mathbf{p}_{21}\\left [ -\\mathbf{p}_{11}+\\mathbf{p}_{21}^{\\top } \\mathbf{p}_{22}^{-}\\mathbf{p}_{21}\\right ] ^{-}\\mathbf{p}_{21}^{\\top } \\mathbf{p}_{22}^{-}\\text{,}\\end{aligned}\\]](also noting that in our case , @xmath112 is a number @xmath113 ) , we can state @xmath114this can be computed directly from @xmath115 , i.e.  @xmath108 . skipping the intermediate steps , we propose the following algorithm for computing @xmath106 and @xmath107 from @xmath108:@xmath116 \\text { , } \\\\",
    "\\mathbf{d}_{k}^{- } & = \\left [   \\begin{array}{cc } q & c\\mathbf{u}^{\\top } \\\\",
    "c\\mathbf{u}^{\\top } & \\mathbf{d}_{k+1}^{-}+q\\mathbf{uu}^{\\top } \\end{array}\\right ] \\text{.}\\end{aligned}\\]]here @xmath117 is the sub - vector in @xmath73 at column @xmath104 , and row @xmath118 to @xmath0 .",
    "a single update from @xmath108 to @xmath106 and @xmath107 requires @xmath119 floating point multiplications .",
    "the overall complexity of computing all sub - blocks @xmath106 , @xmath120 , is thus @xmath121 .",
    "the algorithm is efficient both in time and storage in the sense that , on one hand , there is no explicit matrix inversion , while on the other hand , the space for storing @xmath93 ( including @xmath90 , if not needed later ) can be reclaimed immediately after each iteration , which means that at most @xmath122 storage is required .",
    "note also that @xmath106 can be used directly to compute @xmath123 , using @xmath124 , where @xmath125   \\\\ & = \\left [ \\triangledown _ { \\theta ^{k}}\\ln p\\left ( \\mathbf{z}|\\theta \\right ) , \\ldots , \\triangledown _ { \\theta ^{k}}\\ln",
    "p\\left ( \\mathbf{z}|\\theta \\right ) \\right ] \\end{aligned}\\]]is the submatrix of @xmath126 w.r.t . the mutation gradient of @xmath127 .    to conclude , the algorithm given above efficiently computes the inverse of the exact fim , required for computing the natural mutation gradient .",
    "the concept of _ fitness baselines _ , first introduced in  @xcite , constitutes an efficient variance reduction method for estimating @xmath9 .",
    "however , baselines as found in  @xcite are suboptimal w.r.t .",
    "the variance of @xmath9 , since this fim  may not be invertible .",
    "it is difficult to formulate the variance of @xmath9 directly .",
    "however , since the exact fim is invertible and can be computed efficiently , we can in fact compute an optimal baseline for minimizing the variance of @xmath9 , given by@xmath128 \\right ) ^{\\top } \\\\ & & \\cdot \\left ( \\mathbf{f}^{-}\\triangledown _ { \\theta } ^{s}j-\\mathbb{e}\\left [ \\mathbf{f}^{-}\\triangledown _ { \\theta } ^{s}j\\right ] \\right ) ] \\text{,}\\end{aligned}\\]]where @xmath129 is the estimated evolution gradient , which is given by@xmath130 \\triangledown _ { \\theta } \\ln p\\left ( \\mathbf{z}_{i}|\\theta \\right ) \\text{.}\\]]the scalar @xmath131 is called the fitness baseline .",
    "adding @xmath131 does not affect the expectation of @xmath129 , since@xmath132 & = & \\triangledown _ { \\theta } \\int \\left ( f\\left ( \\mathbf{z}\\right ) -b\\right ) p\\left ( \\mathbf{z}|\\theta \\right ) d\\mathbf{z } \\\\ & = & \\triangledown _ { \\theta } \\int f\\left ( \\mathbf{z}\\right ) p\\left ( \\mathbf{z}|\\theta \\right ) d\\mathbf{z}\\text{.}\\end{aligned}\\]]however , the variance depends on the value of @xmath131 , i.e.@xmath133 \\\\ & & -2b\\mathbb{e}\\left [ \\left ( \\mathbf{f}^{-}\\mathbf{gf}\\right ) ^{\\top } \\left ( \\mathbf{f}^{-}\\mathbf{g1}\\right ) \\right ] + \\text{const.}\\end{aligned}\\]]here @xmath134 denotes a @xmath2-by-@xmath87 vector filled with @xmath87s .",
    "the optimal value of the baseline is@xmath135 } { \\mathbb{e}\\left [ \\left ( \\mathbf{f}^{-}\\mathbf{g1}\\right ) ^{\\top } \\left ( \\mathbf{f}^{-}\\mathbf{g1}\\right ) \\right ] } \\text{.}\\]]assuming individuals are i.i.d .",
    ", @xmath131 can be approximated from data by@xmath136    in order to further reduce the estimation covariance , we can utilize a parameter - specific baseline for each parameter @xmath137 individually , which is given by@xmath138 } { \\mathbb{e}\\left [ \\left ( \\mathbf{h}_{j}\\mathbf{g1}\\right ) \\left ( \\mathbf{h}_{j}\\mathbf{g1}\\right ) \\right ] } \\simeq \\frac{\\sum_{i=1}^{n}f\\left ( \\mathbf{z}_{i}\\right ) \\left ( \\mathbf{h}_{j}\\mathbf{g}\\left ( \\mathbf{z}_{i}\\right ) \\right ) ^{2}}{\\sum_{i=1}^{n}\\left ( \\mathbf{h}_{j}\\mathbf{g}\\left ( \\mathbf{z}_{i}\\right ) \\right ) ^{2}}\\text{.}\\]]here @xmath139 is the @xmath140-th row vector of @xmath141 .",
    "however , parameter - specific baseline values @xmath137 might reduce variance too much , which harms the performance of the algorithm . additionally , adopting different baseline values for",
    "correlated parameters may affect the underlying structure of the parameter space , rendering estimations unreliable . to address both of these problems , we follow the intuition that if the @xmath63-th element in the fim is @xmath142 , then parameters @xmath65 and @xmath66 are orthogonal and only weakly correlated . therefore , we propose using the _ block fitness baseline _ ,",
    "i.e.  a single baseline @xmath143 for each group of parameters @xmath144 , @xmath145",
    ". its formulation is given by@xmath146 } { \\mathbb{e}\\left [ \\left ( \\mathbf{f}_{k}^{-}\\mathbf{g}^{k}\\mathbf{1}\\right ) \\left ( \\mathbf{f}_{k}^{-}\\mathbf{g}^{k}\\mathbf{1}\\right ) \\right ] } \\\\ & \\simeq \\frac{\\sum_{i=1}^{n}f\\left ( \\mathbf{z}_{i}\\right ) \\left ( \\mathbf{f}_{k}^{-}\\mathbf{g}^{k}\\left ( \\mathbf{z}_{i}\\right ) \\right ) ^{\\top } \\left ( \\mathbf{f}_{k}^{-}\\mathbf{g}^{k}\\left ( \\mathbf{z}_{i}\\right ) \\right ) } { \\sum_{i=1}^{n}\\left ( \\mathbf{f}_{k}^{-}\\mathbf{g}^{k}\\left ( \\mathbf{z}_{i}\\right ) \\right ) ^{\\top } \\left ( \\mathbf{f}_{k}^{-}\\mathbf{g}^{k}\\left ( \\mathbf{z}_{i}\\right ) \\right ) } \\text{.}\\end{aligned}\\]]here @xmath106 denotes the inverse of the @xmath104-th diagonal block of @xmath141 , while @xmath147 and @xmath148 denote the submatrices corresponding to differentiation w.r.t .",
    "@xmath144 .    in a companion paper",
    "@xcite , we empirically investigated the convergence properties when using the various types of baseline .",
    "we found block fitness baselines to be very robust , whereas uniform and parameter - specific baselines sometimes led to premature convergence .",
    "the main complexity for computing the optimal fitness baseline pertains to the necessity of storing a potentially large gradient matrix @xmath126 , with dimension @xmath149 .",
    "the time complexity , in this case , is @xmath150 since we have to multiply each @xmath106 with @xmath147 . for large problem dimensions",
    ", the storage requirement may not be acceptable since @xmath2 also scales with @xmath0 .",
    "we solve this problem by introducing a time - space tradeoff which reduces the storage requirement to @xmath122 while keeping the time complexity unchanged .",
    "in particular , we first compute for each @xmath104 , a scalar @xmath151 , where @xmath152 is the @xmath104-th row vector of @xmath80 . then , for @xmath153 , we compute the vector @xmath154 , where @xmath155 is the submatrix of @xmath73 by taking rows @xmath104 to @xmath0 .",
    "the gradient @xmath156 can be computed from @xmath157 and @xmath158 , and used to compute @xmath159 directly .",
    "the storage for @xmath156 can be immediately reclaimed .",
    "finally , the complexity of computing @xmath156 for all @xmath160 is @xmath161 , so the total complexity of computing every element in @xmath126 would still be @xmath150 .",
    "at each generation , we evaluate @xmath2 new individuals generated from mutation distribution @xmath4 .",
    "however , since small updates ensure that the kl divergence between consecutive mutation distributions is generally small , most new individuals will fall in the high density area of the previous mutation distribution @xmath52 .",
    "this leads to redundant fitness evaluations in that same area .",
    "our solution to this problem is a new procedure called importance mixing , which aims to _ reuse _ fitness evaluations from the previous generation , while ensuring the updated population conforms to the new mutation distribution .",
    "importance mixing works in two steps : in the first step , rejection sampling is performed on the previous population , such that individual @xmath162 is accepted with probability@xmath163here @xmath164 $ ] is the _",
    "minimal refresh rate_. let @xmath165 be the number of individuals accepted in the first step . in the second step ,",
    "reverse rejection sampling is performed as follows : generate individuals from @xmath4 and accept @xmath162 with probability@xmath166until @xmath167 new individuals are accepted .",
    "the @xmath165 individuals from the old generation and @xmath167 newly accepted individuals together constitute the new population .",
    "note that only the fitnesses of the newly accepted individuals need to be evaluated .",
    "the advantage of using importance mixing is twofold : on the one hand , we reduce the number of fitness evaluations required in each generation , on the other hand , if we fix the number of newly evaluated fitnesses , then many more fitness evaluations can potentially be used to yield more reliable and accurate gradients .    the minimal refresh rate @xmath168 lower bounds the expected proportion of newly evaluated individuals @xmath169   $ ] , namely @xmath170 , with the equality holding iff @xmath171 .",
    "in particular , if @xmath172 , all individuals from the previous generation will be discarded , and if @xmath173 , @xmath174 depends only on the distance between @xmath4 and @xmath52 .",
    "normally we set @xmath168 to be a small positive number , e.g.  @xmath175 , to avoid too low an acceptance probability at the second step when @xmath176 .",
    "it can be proven that the updated population conforms to the mutation distribution @xmath4 . in the region where @xmath177 , the probability that an individual from previous generations appears in the new population is@xmath178the probability that an individual generated from the second step entering the population is @xmath179 , since@xmath180 so the probability of an individual entering the population is just @xmath181 in that region .",
    "the same result holds also for the region where @xmath182 .    in a companion paper  @xcite , we measured the usefulness of importance mixing , and found that it reduces the number of required fitness evaluations by a factor 5 .",
    "additionally , it reduced the algorithm s sensitivity to the population size .",
    "the computational complexity of importance mixing can be analyzed as follows .",
    "for each generated individual @xmath162 , the probability @xmath4 and @xmath183 need to be evaluated , requiring @xmath122 computations .",
    "the total number of individuals generated is bounded by @xmath184 in the worst case , and is close to @xmath2 on average .",
    "for problems with wildly fluctuating fitnesses , the gradient is disproportionately distorted by extreme fitness values , which can lead to premature convergence or numerical instability .",
    "to overcome this problem , we use _ fitness shaping _ , an order - preserving nonlinear fitness transformation function  @xcite .",
    "the choice of ( monotonically increasing ) fitness shaping function is arbitrary , and should therefore be considered to be one of the tuning parameters of the algorithm .",
    "we have empirically found that ranking - based shaping functions work best for various problems . the shaping function used for all experiments in this paper",
    "was fixed to @xmath185 for @xmath186 and @xmath187 for @xmath188 , where @xmath160 denotes the relative rank of @xmath189 in the population , scaled between @xmath190 .",
    "integrating all the algorithm elements introduced above , the efficient natural evolution strategy ( with block fitness baselines ) can be summarized as@xmath191note that vectors @xmath192 and @xmath158 in line 18 correspond to @xmath193 and @xmath194 , respectively .",
    "summing up the analysis from previous sections , the time complexity of processing a single generation is @xmath195 , while the space complexity is just @xmath196 , where @xmath197 comes from the need of storing the population . assuming that @xmath2 scales linearly with @xmath0 , our algorithms scales linearly in space and quadratically in time w.r.t .  the number of parameters , which is @xmath122 .",
    "this is a significant improvement over the original nes , whose complexity is @xmath61 in space and @xmath198 in time .",
    "implementations of enes are available in both python and matlab .",
    "[ fig : unimodal ]                the tunable parameters of efficient natural evolution strategies are comprised of the population size @xmath2 , the learning rate @xmath199 , the refresh rate @xmath200 and the fitness shaping function . in addition , three kinds of fitness baselines can be used .",
    "we empirically find a good and robust choice for the learning rate @xmath199 to be @xmath201 . on some ( but not all )",
    "benchmarks the performance can be further improved by more aggressive updates .",
    "therefore , the only parameter that needs tuning in practice is the population size , which is dependent on both the expected ruggedness of the fitness landscape and the problem dimensionality .",
    "we empirically validate our algorithm on 9 unimodal and 4 multimodal functions out of the set of standard benchmark functions from  @xcite and  @xcite , that are typically used in the literature , for comparison purposes and for competitions .",
    "we randomly choose the inital guess at average distance 1 from the optimum . in order to prevent potentially biased results ,",
    "we follow  @xcite and consistently transform ( by a combined rotation and translation ) the functions inputs , making the variables non - separable and avoiding trivial optima ( e.g.  at the origin ) .",
    "this immediately renders many other methods virtually useless , since they can not cope with correlated mutation directions .",
    "enes , however , is invariant under translation and rotation .",
    "in addition , the rank - based fitness shaping makes it invariant under order - preserving transformations of the fitness function .",
    "we ran enes on the set of unimodal benchmark functions with dimensions 5 , 15 and 50 with population sizes 50 , 250 and 1000 , respectively , using @xmath202 and a target precision of @xmath203 .",
    "figure  1 shows the average performance over 20 runs ( 5 runs for dimension 50 ) for each benchmark function .",
    "we left out the rosenbrock function on which enes is one order of magnitude slower than on the other functions ( e.g. 150,000 evaluations on dimension 15 ) .",
    "presumably this is due to the fact that the principal mutation direction is updated too slowly on complex curvatures .",
    "note that sharpr and parabr are unbounded functions , which explains the abrupt drop - off .",
    "[ fig : multimodal ]        for the experiments on the multimodal benchmark functions we varied the distance of the initial guess to the optimum between 0.1 and 1000 .",
    "those runs were performed on dimension 2 with a target precision of @xmath175 , since here the focus was on avoiding local maxima .",
    "we compare the results for population size 20 and 100 ( with @xmath202 ) .",
    "figure  2 shows , for all tested multimodal functions , the percentage of 100 runs where enes found the global optimum ( as opposed to it getting stuck in a local extremum ) conditioned on the distance from the initial guess to the optimum .",
    "[ fig : rastrigin ]        note that for ackley and griewank the success probability drops off sharply at a certain distance . for ackley",
    "this is due to the fitness landscapes providing very little global structure to exploit , whereas for giewank the reason is that the local optima are extremely large , which makes them virtually impossible to escape from , instead of @xmath204 .",
    "figure  3 shows the evolution path of a typical run on rastrigin , and the ellipses corresponding to the mutation distribution at different generations , illustrating how enes jumps over local optima to reach the global optimum .    for three functions",
    "we find that enes finds the global optimum reliably , even with a population size as small as 20 .",
    "for the other one , rastrigin , the global optimum is only reliably found when using a population size of 100 .",
    "unlike most evolutionary algorithms , enes boasts a relatively clean derivation from first principles . using a full multinormal mutation distribution and fitness shaping , the enes algorithm is invariant under translation and rotation and under order - preserving transformations of the fitness function .    comparing our empirical results to cma - es  @xcite , considered by many to be the ` industry standard ' of evolutionary computation , we find that enes is competitive but slower , especially on higher dimensions .",
    "however , enes is faster on diffpow for all dimensions . on multimodal benchmarks enes is competitive with cma - es as well , as compared to the results in  @xcite .",
    "our results collectively show that enes can compete with state of the art evolutionary algorithms on standard benchmarks .",
    "future work will also address the problems of automatically determining good population sizes and dynamically adapting the learning rate .",
    "moreover , we plan to investigate the possibility of combining our algorithm with other methods ( e.g.  estimation of distribution algorithms ) to accelerate the adaptation of covariance matrices , improving performance on fitness landscapes where directions of ridges and valleys change abruptly ( e.g. the rosenbrock benchmark ) .",
    "efficient nes is a novel alternative to conventional evolutionary algorithms , using a natural evolution gradient to adapt the mutation distribution . unlike previous natural gradient methods , enes _",
    "quickly _ calculates the inverse of the _ exact _ fisher information matrix .",
    "this increases robustness and accuracy of the evolution gradient estimation , even in higher - dimensional search spaces .",
    "importance mixing prevents unnecessary redundancy embodied by individuals from earlier generations .",
    "enes constitutes a competitive , theoretically well - founded and relatively simple method for artificial evolution .",
    "good results on standard benchmarks affirm the promise of this research direction .",
    "this research was funded by snf grants 200020 - 116674/1 , 200021 - 111968/1 and 200021 - 113364/1 ."
  ],
  "abstract_text": [
    "<S> efficient natural evolution strategies ( enes ) is a novel alternative to conventional evolutionary algorithms , using the natural gradient to adapt the mutation distribution . unlike previous methods based on natural gradients , </S>",
    "<S> enes uses a _ </S>",
    "<S> fast _ algorithm to calculate the inverse of the _ exact _ fisher information matrix , thus increasing both robustness and performance of its evolution gradient estimation , even in higher dimensions . </S>",
    "<S> additional novel aspects of enes include optimal fitness baselines and importance mixing ( a procedure for updating the population with very few fitness evaluations ) . </S>",
    "<S> the algorithm yields competitive results on both unimodal and multimodal benchmarks . </S>"
  ]
}