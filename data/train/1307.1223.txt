{
  "article_text": [
    "generating pseudo - random samples from a prescribed probability distribution is an important task in applied probability , statistics , computing , physics , and elsewhere .",
    "a classical approach is _ inverse transform sampling _ , in which pseudo - random samples @xmath0 are generated from a uniform distribution @xmath1 on @xmath2 $ ] and then transformed by @xmath3 , where @xmath4 is the cumulative distribution function ( cdf ) of a prescribed probability distribution .",
    "conventional wisdom is that the practical application of inverse transform sampling is limited . to quote @xcite :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ [ inverse transform sampling ] requires a complete approximation to [ the cdf ] regardless of the desired sample size , it does not generalize to multiple dimensions and it is less efficient than other approaches . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    similar comments are present in other computational statistics texts , for example , @xcite , @xcite , and @xcite .",
    "instead , other methods are advocated for low dimensions , such as slice sampling @xcite or rejection sampling @xcite . in higher dimensions ,",
    "algorithms concentrate on avoiding the curse of dimensionality , such as metropolis ",
    "hastings sampling @xcite .",
    "moreover , there are many other methods that are designed to exploit certain special properties : e.g. , ziggurat algorithm @xcite ( requires monotonicity ) and adaptive rejection sampling @xcite ( requires log - convexity ) .    in contradiction to conventional wisdom",
    ", we show that inverse transform sampling is computationally efficient and robust when combined with an adaptive chebyshev polynomial approximation scheme .",
    "approximation by chebyshev polynomials converges superalgebraically fast for smooth functions , and hence the cdf can be numerically computed to very high accuracy , i.e. , close to machine precision . rather than forming the inverse cdf  which is often nearly singular  we invert the cdf pointwise using a simple bijection method .",
    "this approach is robust  it is guaranteed to converge to a high accuracy  and fast , due to the simple representation of the cdf .",
    "furthermore , we extend our approach to probability distributions of two variables by exploiting recent developments in low rank function approximation to achieve a highly efficient inverse transform sampling scheme in 2d .",
    "numerical experiments confirm that this approach outperforms existing sampling routines for many black box distributions , see section  [ sec : numericalexamples ] .",
    "we use _ black box distribution _ to mean a continuous probability distribution that can only be evaluated pointwise . the only other piece of information we assume is an interval @xmath5 $ ] , or a rectangular domain @xmath5\\times [ c , d]$ ] , containing the numerical support of the distribution .",
    "a  implementation is publicly available from @xcite . as an example",
    ", the following syntax generates @xmath6 samples from a 2d distribution :    ....          [ x , y ] = sample(@(x , y ) exp(-x.^2 - 2.*y.^2).*(x - y).^2,[-3,3],[-3,3],2000 ) ;       ....    the algorithm scales the input function to integrate to one , and generates pseudo - random samples from the resulting probability distribution .",
    "let @xmath7 be a prescribed non - negative function supported on the interval @xmath5 $ ] .",
    "otherwise , if the support of @xmath8 is the whole real line and @xmath8 is rapidly decaying , an interval @xmath5 $ ] can be selected outside of which @xmath8 is negligible .",
    "the algorithm will draw samples that match the probability distribution @xmath9 up to a desired tolerance @xmath10 ( typically close to machine precision such as @xmath11 ) .",
    "first , we replace @xmath8 by a polynomial approximant @xmath12 on @xmath5 $ ] , which we numerically compute such that @xmath13 .",
    "that is , we construct a polynomial approximant @xmath12 of @xmath8 in the form @xmath14 ,   \\label{eq : chebexpansion}\\ ] ] where @xmath15 is the degree @xmath16 chebyshev polynomial . while the polynomial approximants could be represented in other polynomial bases",
    " such as the monomial basis  the chebyshev basis is a particularly good choice for numerical computation as the expansion coefficients @xmath17 can be stably calculated in @xmath18 operations @xcite .",
    "this is accomplished by applying the fast cosine transform to function evaluations of @xmath8 taken from a chebyshev grid in @xmath5 $ ] of size @xmath19 , i.e. , at the set of points @xmath20 we can adaptively select @xmath21 by sampling @xmath8 at chebyshev grids of size @xmath22 , @xmath23 , @xmath24 , and so on , until the tail of the coefficients @xmath25 has decayed to below machine precision relative to the absolute maximum of @xmath8 .",
    "in addition to the fast transform , chebyshev expansions are numerically stable @xcite , in contrast to the well - known numerical instability associated to interpolation at equispaced points @xcite .    instead of symbolically manipulating @xmath8 , which is not possible",
    "if @xmath8 is a black box distribution , we numerically manipulate @xmath12 to apply the inverse transform sampling approach .",
    "this rests on fast and well - established algorithms for evaluating and integrating polynomial interpolants of the form .",
    "( a convenient implementation of many of these algorithms is in the chebfun software system @xcite . )",
    "figure  [ fig : inversesamplingchebyshev ] summarizes the approach we are about to describe based on the inverse transform sampling and chebyshev approximation .        ' '' ''    * algorithm : inverse transform sampling with chebyshev approximation * + * input : * a non - negative function @xmath7 , a finite interval @xmath5 $ ] , and an integer @xmath26 . + * output : * random samples @xmath27 drawn from @xmath28 .",
    "+ construct an approximant @xmath12 of @xmath8 on @xmath5 $ ] .",
    "+ compute @xmath29 using .",
    "+ scale to a cdf , @xmath30 , if necessary .",
    "+ generate random samples @xmath0 from the uniform distribution on @xmath2 $ ] .",
    "+ * for @xmath31 * + solve @xmath32 for @xmath33 . + * end * +    with the polynomial approximation @xmath34 in hand , we construct the corresponding cumulative function , denoted by @xmath35 , by calculating the indefinite integral @xmath36 where the last equality comes from a change of variables with @xmath37 .",
    "fortunately , we have the following relation satisfied by chebyshev polynomials @xcite : @xmath38                        \\frac{1}{4}t_{2}(s ) , & k=1 , \\\\[3pt ]                                                      \\end{cases } \\label{eq : cumsumrelation}\\ ] ] where @xmath39 $ ] . therefore , using , we can compute coefficients @xmath40 in @xmath41 operations ( see , for example , @xcite ) such that @xmath42 .",
    "\\label{eq : chebcumsum}\\ ] ] at this stage we rescale @xmath35 so that it is a cdf : @xmath43 .",
    "once we have the approximant @xmath44 , given in , we generate @xmath26 pseudo - random samples @xmath45 from the uniform distribution on @xmath2 $ ] using a standard inbuilt pseudo - random number generator , and then solve the following rootfinding problems for @xmath27 : @xmath46 the rootfinding problems in can be solved by various standard methods such as the eigenvalues of a certain matrix @xcite , newton s method , the bisection method , or brent s method ( * ? ? ?",
    "* chapter 4 ) .    to achieve robustness",
    " guaranteed convergence , computation cost , and accuracy  we use the bisection method .",
    "the bisection method approximates the ( unique ) solution @xmath33 to @xmath47 by a sequence of intervals @xmath48 $ ] that contain @xmath33 , satisfying @xmath49 .",
    "the initial interval is the whole domain @xmath5 $ ] .",
    "each stage finds the midpoint @xmath50 of the interval @xmath48 $ ] , choosing the new interval based on whether @xmath51 is greater or less than @xmath52 .",
    "convergence occurs when @xmath53 , where @xmath54 .",
    "for example , it takes precisely 47 iterations to converge when @xmath55 and a further five iterations to converge to machine precision .",
    "since the cdf is represented as a chebyshev series it can be efficiently evaluated using clenshaw s method , which is an extension of horner s scheme to chebyshev series @xcite .",
    "the inverse transform sampling with chebyshev approximation is very efficient , as demonstrated in the numerical experiments of section [ sec : numericalexamples ] using the  implementation @xcite .",
    "we now extend the inverse transform sampling approach with chebyshev approximation to probability distributions of two variables . figure  [ fig : inversesamplingchebyshev2 ] summarizes our algorithm , which generates @xmath26 pseudo - random samples @xmath56 drawn from the probability distribution @xmath57 .",
    "the essential idea is to replace @xmath8 by a low rank approximant @xmath34 that can be manipulated in a computationally efficient manner .",
    "again , for convenience , if @xmath8 is a non - negative function that does not integrate to one then we automatically scale the function to a probability distribution .        ' '' ''    * algorithm : inverse transform sampling for @xmath58 * + * input : * a non - negative function @xmath59 , a finite domain @xmath5\\times [ c , d]$ ] , and an integer @xmath26 . + * output : * random samples @xmath56 from @xmath60 .",
    "+ construct a low rank approximant @xmath12 of @xmath8 on @xmath5\\times [ c , d]$ ] .",
    "+ compute @xmath61 .",
    "+ generate random samples @xmath27 from @xmath62 .",
    "+ * for @xmath63 * + generate a random sample @xmath64 from @xmath65 . +",
    "* end * +    we use a _ low rank approximation _ of @xmath59 .",
    "a rank-@xmath66 function is a function of two variables that can be written as the product of two univariate functions , i.e. , @xmath67 .",
    "moreover , a rank-@xmath16 function is a sum of @xmath16 rank-1 functions .",
    "we efficiently approximate a probability distribution @xmath8 by a rank-@xmath16 function by using gaussian elimination @xcite : @xmath68 where @xmath16 is adaptively chosen and @xmath69 and @xmath70 are polynomials of one variable represented in chebyshev expansions of the form . for simplicity",
    "we assume that @xmath69 and @xmath70 are polynomials of degree @xmath71 and @xmath21 , respectively , and then a function is considered to be of low rank if @xmath72 .    while most functions are mathematically of infinite rank  such as @xmath73  they can typically be approximated by low rank functions to high accuracy , especially if the function is smooth @xcite .",
    "it is well - known that the singular value decomposition can be used to compute optimal low rank approximations of matrices , and it can easily be extended for the approximation of functions . however , the gaussian elimination approach from @xcite is significantly faster and constructs near - optimal low rank function approximations @xcite .",
    "this algorithm is conveniently implemented in the recent software package chebfun2 @xcite , which we utilize to construct @xmath12 .",
    "furthermore , we emphasize that this low rank approximation process only requires pointwise evaluations of @xmath8 . with the low rank approximation @xmath34 , given in",
    ", we can efficiently perform the steps of the 2d inverse transform sampling algorithm .",
    "first , we approximate the marginal distribution of @xmath74 by integrating @xmath12 over the second variable : @xmath75 therefore , @xmath76 can be computed by applying a 1d quadrature rule , such as the clenshaw ",
    "curtis quadrature rule @xcite , to @xmath16 polynomials of one variable ( rather than anything inherently 2d ) .",
    "the resulting sum is a chebyshev expansion ( in @xmath77 ) , and we use the algorithm described in section  [ sec : inversesampling ] to generate @xmath26 pseudo - random samples @xmath78 from @xmath79 .    afterwards , we generate the corresponding @xmath80 by using a numerical approximation to the conditional distribution denoted by @xmath81 . to construct @xmath81 for each @xmath78",
    "we require evaluation , i.e. , @xmath82 where @xmath83 is precisely the normalization constant .",
    "fortunately , evaluation of @xmath84 is relatively cheap because of the low rank representation of @xmath12 .",
    "that is , @xmath85 and thus we only require the evaluation of @xmath16 polynomials of one variable ( again , nothing inherently 2d ) , which , as before , is accomplished using clenshaw s algorithm @xcite . the final task of generating the sample @xmath86 has been reduced to the 1d problem of drawing a sample from @xmath87 , which was solved in section  [ sec : inversesampling ] .",
    "in total we have generated @xmath26 pseudo - random samples @xmath88 from the probability distribution @xmath58 without an explicit expression for the cdf or its inverse .",
    "low rank approximation was important for computational efficiency , and the total computational cost for @xmath26 samples , including the low rank approximation of @xmath8 , is @xmath89 therefore , provided the probability distribution can be well - approximated by a low rank function , the inverse transform sampling approach in 2d has the same order of complexity  i.e. , linear in @xmath26 , @xmath71 and @xmath21  as the 1d approach described in section  [ sec : inversesampling ] .",
    "many probability distribution can be approximated by functions of low rank , as we see below .",
    "c | c || c | c | c c | c | c pdf & rank & its & ss & rs + @xmath90 &  & 0.29 & 3.01 & 0.55 @xmath91 &  & 0.15 & 2.87 & 0.24 @xmath92 &  & 0.21 &  & 0.11 @xmath93 &  & 0.67 & 6.76 & 6.89 @xmath94 & 2 & 1.34 &  & 2.46 @xmath95 & 3 & 0.54 &  & 6.41 @xmath96 & 16 & 10.90 &  & 9.43 @xmath97 & 51 & 19.03 &  & 7.12    in table  [ tab : samplingtimes ] , we compare the sampling times of our algorithms based on inverse transform sampling , the slice sampling ( slicesample ) routine in , and rejection sampling with a rectangular hat function bounded above by the exact maximum of the distribution .",
    "we consider the following univariate distributions :    1 .",
    "multimodal density : @xmath98 on @xmath99 $ ] , 2 .   the spectral density of a @xmath100 gaussian unitary ensemble @xcite : @xmath91 on @xmath101 $ ] , 3 .   compactly supported oscillatory density : @xmath92 on @xmath102 $ ] , 4 .",
    "concentrated sech density : @xmath93 on @xmath102 $ ] ,    and the following bivariate distributions :    1 .",
    "bimodal distribution : @xmath103 on @xmath104\\times [ -2,2]$ ] , 2 .",
    "quartic unitary ensemble eigenvalues @xcite : @xmath95 on @xmath105\\times [ -7,7]$ ] , 3 .   concentrated 2d sech density : @xmath96 on @xmath106 \\times [ -4,4]$ ] , 4 .",
    "butterfly density : @xmath107 on @xmath108 \\times [ -3,3]$ ] .",
    "figure  [ examples ] demonstrates our algorithm .",
    "figure  [ examples ] ( left ) shows a histogram of one hundred thousand pseudo - random samples generated from the multimodal density , and figure  [ examples ] ( right ) shows ten thousand pseudo - random samples from the butterfly density .",
    "we observe that the inverse transform sampling approach described in section  [ sec : inversesampling ] significantly outperforms the  implementation of slice sampling in 1d . in the worst case ,",
    "the computational cost is comparable to rejection sampling , and in two of the examples it outperforms rejection sampling by a factor of 10 .",
    "rejection sampling has practical limitations that inverse sampling avoids . finding the exact maximum of the distribution for the rejection sampling is computationally expensive , and replacing the exact maximum with a less accurate upper bound causes more samples to be rejected , which reduces its efficiency .",
    "furthermore , the computational cost of each sample is not bounded , and to emphasize this we consider generating pseudo - random samples from @xmath109 for varying @xmath110 on the interval",
    "@xmath99 $ ] . for large @xmath110",
    "the percentage of the rectangular hat functions area that is under the probability distribution is small , and the rejection sampling approach rejects a significant proportion of its samples , while for our approach the dominating computational cost stays roughly constant . in figure  [ vsrejection ] ( left )",
    "we compare the computational cost of rejection sampling and the approach described in section  [ sec : inversesampling ] , and we observe that the chebyshev expansion outperforms rejection sampling for @xmath111 .    of course , for a particular probability distribution a better hat function may be known , which would improve the efficiency of rejection sampling .",
    "however , in general accurate hat functions are not available and a rectangular hat function with a known upper bound is the practical choice .    in many applications ,",
    "the actual evaluation of the probability distribution is expensive .",
    "in such circumstances , another benefit of using polynomial approximation in the inverse transform sampling method is that once an approximation has been calculated the original distribution can be discarded , whereas rejection sampling evaluates the original distribution several times per sample .",
    "thus , in figure  [ vsrejection ] , we compare the number of function evaluations required to generate @xmath112 samples using rejection sampling versus the number of evaluations for our inverse transform sampling approach .",
    "the number of function evaluations are comparable at 50 samples , and hence if 500 pseudo - random samples were required then the rejection sampling approach would require about ten times the number of function evaluations .",
    "parallelization of the algorithm is important for extremely large sample sizes .",
    "this is easily accomplished using a polynomial approximation since the parameters used to represent the cdf can be stored in local memory and the task of generating @xmath26 pseudo - random samples can be divided up among an arbitrary number of processors or machines .",
    "however , the rejection sampling approach can not be parallelized with a black box distribution because evaluation would cause memory locking .",
    "we have shown that inverse transform sampling is efficient when employed with chebyshev polynomial approximation in one dimension , and with low rank function approximation in two dimensions .",
    "this allows for an automated and robust algorithm for generating pseudo - random samples from a large class of probability distributions .",
    "furthermore , the approach that we have described can be generalized to higher dimensions by exploiting low rank approximations of tensors .",
    "in fact , the inverse transform sampling approach is efficient for any class of probability distributions that can be numerically approximated , evaluated , and integrated .",
    "therefore , there are straightforward extensions to several other classes of distributions of one variable :    1 .   piecewise smooth probability distributions , utilizing piecewise polynomial approximation schemes and automatic edge detection @xcite , 2 .",
    "probability distribution functions with algebraic endpoint singularities , 3 .",
    "probability distributions with only algebraic decay at @xmath113 , by mapping to a bounded interval via @xmath114 .      in two dimensions and higher , approximating general piecewise smooth functions is a more difficult problem , and many fundamental algorithmic issues remain .",
    "however , if these constructive approximation questions are resolved the inverse transform sampling approach will be immediately applicable .    3 , _ hierarchical matrices : a means to efficiently solve elliptic boundary value problems _ , volume 63 of lecture notes in computational science and engineering ( lncse ) .",
    "verlag , 2008 . , _",
    "algorithms for minimization without derivatives _ , englewood cliffs , nj : prentice  hall , 1973 . , a note on the summation of chebyshev series , _ maths comp . _ , 9 ( 1955 ) , pp .",
    "118110 . ,",
    "a method for numerical integration on an automatic computer , _ numerische mathematik _ , 2 ( 1960 ) , pp .",
    "197205 . , _",
    "orthogonal polynomials and random matrices : a riemann ",
    "hilbert approach _ ,",
    "ams , 2000 .",
    ", _ random number generation and monte carlo methods _ , springer , 2003",
    ". , derivative - free adaptive rejection sampling for gibbs sampling , in _ bayesian statistics _ ( j. m. bernardo , j. o. berger , a. p. dawid , and a. f. m. smith , eds . ) , oxford univ .",
    "press , ( 1992 ) , pp .",
    "641649 . , adaptive rejection sampling for gibbs sampling , _ appl .",
    "statist . _",
    "41 ( 1992 ) pp .",
    "337348 . , _ computational statistics , 2nd edition _ ,",
    "wiley , 2013 .",
    ", the ziggurat method for generating random variables , _ j. stat",
    "_ , 5 ( 2000 ) pp .",
    ", _ the chebyshev polynomials _ , crc press , 2003 .",
    ", equations of state calculations by fast computing machines , _ j. chem .",
    "_ 21 ( 1953 ) , pp .",
    "10871092 . , slice sampling , _ annals stats",
    "_ , 31 ( 2003 ) , pp .",
    "705741 . , inverse transform sampling matlab implementation , https://github.com/dlfivefifty/inversetransformsampling . , piecewise - smooth chebfuns , _",
    "i m a j. numer .",
    "_ , 30 ( 2010 ) , pp",
    ".  898916 . , impossibility of fast stable approximation of analytic functions from equispaced samples , _ siam review _ , 53 ( 2011 ) pp .",
    "308318 . , _ computational methods in statistics and econometrics _ , crc pressi llc , 2004 . ,",
    "gaussian elimination as an iterative algorithm , _ siam news _ , march 2013 .",
    ", an extension of chebfun to two dimensions , ( 2013 ) , submitted . , _",
    "approximation theory and approximation practice _ , siam , 2013 .",
    ", chebfun version 4.2 , the chebfun development team , ( 2011 ) , http://www.maths.ox.ac.uk / chebfun/. , _ statistical methods in the atmospheric sciences _ , academic press , 2011 ."
  ],
  "abstract_text": [
    "<S> we develop a computationally efficient and robust algorithm for generating pseudo - random samples from a broad class of smooth probability distributions in one and two dimensions . </S>",
    "<S> the algorithm is based on inverse transform sampling with a polynomial approximation scheme using chebyshev polynomials , chebyshev grids , and low rank function approximation . </S>",
    "<S> numerical experiments demonstrate that our algorithm outperforms existing approaches . </S>"
  ]
}