{
  "article_text": [
    "developing learning algorithms for distributed compositional semantics of words has been a long - standing open problem at the intersection of language understanding and machine learning . in recent years",
    ", several approaches have been developed for learning composition operators that map word vectors to sentence vectors including recursive networks  @xcite , recurrent networks  @xcite , convolutional networks  @xcite and recursive - convolutional methods  @xcite among others .",
    "all of these methods produce sentence representations that are passed to a supervised task and depend on a class label in order to backpropagate through the composition weights .",
    "consequently , these methods learn high - quality sentence representations but are tuned only for their respective task .",
    "the paragraph vector of  @xcite is an alternative to the above models in that it can learn unsupervised sentence representations by introducing a distributed sentence indicator as part of a neural language model .",
    "the downside is at test time , inference needs to be performed to compute a new vector .",
    "in this paper we abstract away from the composition methods themselves and consider an alternative loss function that can be applied with any composition operator .",
    "we consider the following question : is there a task and a corresponding loss that will allow us to learn highly generic sentence representations ?",
    "we give evidence for this by proposing a model for learning high - quality sentence vectors without a particular supervised task in mind . using word vector learning as inspiration",
    ", we propose an objective function that abstracts the skip - gram model of  @xcite to the sentence level .",
    "that is , instead of using a word to predict its surrounding context , we instead encode a sentence to predict the sentences around it .",
    "thus , any composition operator can be substituted as a sentence encoder and only the objective function becomes modified .",
    "figure @xmath0 illustrates the model .",
    "we call our model * skip - thoughts * and vectors induced by our model are called * skip - thought vectors*.    our model depends on having a training corpus of contiguous text .",
    "we chose to use a large collection of novels , namely the bookcorpus dataset  @xcite for training our models .",
    "these are free books written by yet unpublished authors .",
    "the dataset has books in 16 different genres , e.g. , _ romance _",
    "( 2,865 books ) , _ fantasy _ ( 1,479 ) , _ science fiction _ ( 786 ) , _ teen _ ( 430 ) , etc . table  [ tab : bookdata ] highlights the summary statistics of the book corpus . along with narratives ,",
    "books contain dialogue , emotion and a wide range of interaction between characters .",
    "furthermore , with a large enough collection the training set is not biased towards any particular domain or application .",
    "table @xmath1 shows nearest neighbours of sentences from a model trained on the bookcorpus dataset .",
    "these results show that skip - thought vectors learn to accurately capture semantics and syntax of the sentences they encode .",
    "we evaluate our vectors in a newly proposed setting : after learning skip - thoughts , freeze the model and use the encoder as a generic feature extractor for arbitrary tasks . in our experiments",
    "we consider 8 tasks : semantic - relatedness , paraphrase detection , image - sentence ranking and 5 standard classification benchmarks . in these experiments ,",
    "we extract skip - thought vectors and train linear models to evaluate the representations directly , without any additional fine - tuning .",
    "as it turns out , skip - thoughts yield generic representations that perform robustly across all tasks considered .",
    "one difficulty that arises with such an experimental setup is being able to construct a large enough word vocabulary to encode arbitrary sentences .",
    "for example , a sentence from a wikipedia article might contain nouns that are highly unlikely to appear in our book vocabulary .",
    "we solve this problem by learning a mapping that transfers word representations from one model to another .",
    "using pre - trained word2vec representations learned with a continuous bag - of - words model  @xcite , we learn a linear mapping from a word in word2vec space to a word in the encoder s vocabulary space .",
    "the mapping is learned using all words that are shared between vocabularies . after training , any word that appears in word2vec",
    "can then get a vector in the encoder word embedding space .",
    "the skip - thoughts model . given a tuple ( @xmath2 ) of contiguous sentences , with @xmath3 the @xmath4-th sentence of a book ,",
    "the sentence @xmath3 is encoded and tries to reconstruct the previous sentence @xmath5 and next sentence @xmath6 . in this example , the input is the sentence triplet _",
    "i got back home .",
    "i could see the cat on the steps .",
    "this was strange .",
    "_ unattached arrows are connected to the encoder output .",
    "colors indicate which components share parameters .",
    "@xmath7eos@xmath8 is the end of sentence token . , scaledwidth=100.0% ]    .in each example , the first sentence is a query while the second sentence is its nearest neighbour .",
    "nearest neighbours were scored by cosine similarity from a random sample of 500,000 sentences from our corpus .",
    "[ cols= \" < \" , ]     on these tasks , properly tuned bag - of - words models have been shown to perform exceptionally well .",
    "in particular , the nb - svm of  @xcite is a fast and robust performer on these tasks .",
    "skip - thought vectors potentially give an alternative to these baselines being just as fast and easy to use . for an additional comparison ,",
    "we also see to what effect augmenting skip - thoughts with bigram naive bayes ( nb ) features improves performance .",
    "table@xmath9 presents our results . on most tasks ,",
    "skip - thoughts performs about as well as the bag - of - words baselines but fails to improve over methods whose sentence representations are learned directly for the task at hand .",
    "this indicates that for tasks like sentiment classification , tuning the representations , even on small datasets , are likely to perform better than learning a generic unsupervised sentence vector on much bigger datasets . finally , we observe that the skip - thoughts - nb combination is effective , particularly on mr .",
    "this results in a very strong new baseline for text classification : combine skip - thoughts with bag - of - words and train a linear model .",
    "[ fig : tsne ]    as a final experiment , we applied t - sne @xcite to skip - thought vectors extracted from trec , subj and sick datasets and the visualizations are shown in figure @xmath10 . for the sick visualization ,",
    "each point represents a sentence pair , computed using the concatenation of component - wise and absolute difference of features .",
    "remarkably , sentence pairs that are similar to each other are embedded next to other similar pairs . even without the use of relatedness labels ,",
    "skip - thought vectors learn to accurately capture this property .",
    "since our decoder is a neural language model , we can also generate from it .",
    "we can perform generation by conditioning on a sentence , generating a new sentence , concatenating the generated example to the previous text and continuing .",
    "since our model was trained on books , the generated samples reads like a novel , albeit a nonsensical one .",
    "below is a 20 sentence sample generated by our model :",
    "we evaluated the effectiveness of skip - thought vectors as an off - the - shelf sentence representation with linear classifiers across 8 tasks .",
    "many of the methods we compare against were only evaluated on 1 task .",
    "the fact that skip - thought vectors perform well on all tasks considered highlight the robustness of our representations .",
    "we believe our model for learning skip - thought vectors only scratches the surface of possible objectives .",
    "many variations have yet to be explored , including ( a ) deep encoders and decoders , ( b ) larger context windows , ( c ) encoding and decoding paragraphs , ( d ) other encoders , such as convnets .",
    "it is likely the case that more exploration of this space will result in even higher quality representations .",
    "we thank geoffrey hinton for suggesting the name skip - thoughts .",
    "we also thank felix hill , kelvin xu , kyunghyun cho and ilya sutskever for valuable comments and discussion .",
    "this work was supported by nserc , samsung , cifar , google and onr grant n00014 - 14 - 1 - 0232 ."
  ],
  "abstract_text": [
    "<S> we describe an approach for unsupervised learning of a generic , distributed sentence encoder . using the continuity of text from books , </S>",
    "<S> we train an encoder - decoder model that tries to reconstruct the surrounding sentences of an encoded passage . sentences that share semantic and syntactic properties are thus mapped to similar vector representations . </S>",
    "<S> we next introduce a simple vocabulary expansion method to encode words that were not seen as part of training , allowing us to expand our vocabulary to a million words . </S>",
    "<S> after training our model , we extract and evaluate our vectors with linear models on 8 tasks : semantic relatedness , paraphrase detection , image - sentence ranking , question - type classification and 4 benchmark sentiment and subjectivity datasets . </S>",
    "<S> the end result is an off - the - shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice . </S>",
    "<S> we will make our encoder publicly available . </S>"
  ]
}