{
  "article_text": [
    "missing sample recovery problem arises in many applications in the literature of signal processing @xcite .",
    "it is also known as inpainting in the context of audio and image processing .",
    "audio inpainting is investigated in @xcite , while image inpainting is discussed in @xcite and @xcite .",
    "the missing sample recovery problem is also applied in the field of wireless sensor network ( wsn ) @xcite where some of the spatial samples are missed .    among numerous algorithms for missing sample recovery and inpainting ,",
    "some of them exploit the sparsity of the signals @xcite . in this paper",
    ", we restrict ourselves to these sparse representation based algorithms . in this context",
    ", it is assumed that the signal is sparse in a domain such as discrete fourier transform ( dft ) , discrete cosine transform ( dct ) , discrete wavelet transform ( dwt ) or any other predefined or learned complete or overcomplete dictionary .",
    "the sparsity of the signal on a dictionary based representation , means that the vector of coefficients of the signal in the transform domain has many zeros ( or nearly zeros ) and only a few of its elements are nonzero . neglecting the insignificant ( zero ) coefficients , it is possible to reconstruct the signal with considerably low error .",
    "the sparsity of the signal gives us the ability to reconstruct it from very few random measurements far below the nyquist rate .",
    "this is well known as compressed sensing ( cs ) @xcite , which has had many applications in the past decade @xcite .",
    "the problem of reconstruction of the signal from a few random measurements is also known as sparse recovery .",
    "many algorithms are proposed for sparse recovery of signals in different applications in audio and image processing .    in a missing sample recovery problem",
    ", some samples of the signal are missed due to physical impairment , unavailability of measurements , or distortion and disturbances . in such cases",
    ", it is shown that the corrupted samples would better be omitted throughout the reconstruction process @xcite .",
    "thus the discarded samples may be considered as missed . even with these missing samples ,",
    "the signal can still be reconstructed , given the sparsifying basis or dictionary and the corresponding sparse coefficients .",
    "many algorithms and optimization problems are suggested to recover the sparse samples in this regard .",
    "the fundamental problem in a sparse recovery method is to maximize the sparsity which is principally stated in terms of the @xmath2-norm .",
    "there are a class of greedy algorithms for strictly sparse signal recovery based on @xmath2-norm minimization .",
    "these include matching pursuit ( mp ) @xcite , orthogonal matching pursuit ( omp ) @xcite , regularized omp ( romp ) @xcite , compressive sampling matching pursuit cosamp @xcite and generalized matching pursuit @xcite .",
    "there are also iterative methods based on majorization minimization technique proposed for approximate @xmath2-norm minimization using surrogate functions .",
    "the iterative hard thresholding ( iht ) algorithm @xcite is the first member of this class . there is also a modified version which uses adaptive thresholding named as iterative method with adaptive thresholding ( imat ) @xcite with different variants including imati and imatcs @xcite . a recent improved version of this method called inpmat is also proposed in @xcite .",
    "furthermore there is an approach for sparse approximation based on smoothed-@xmath2  ( sl0 ) norm minimization presented in @xcite .",
    "the @xmath2  minimization algorithms are mostly used in cases where the signal has exactly sparse support and the sparsity is known .",
    "but in many practical situations the sparsity is unknown or the signal is not strictly sparse but instead compressible , meaning that most coefficients are negligible ( despite being precisely zero ) compared to the significant elements . a good and a common alternative is to use the @xmath1-norm as the nearest convex approximation of @xmath2-norm .",
    "this approach is called @xmath1  minimization or the basis - pursuit method @xcite .",
    "there are many algorithms presented for @xmath1-norm minimization including iterative soft thresholding algorithm ( ista ) @xcite and the fast version fista @xcite , @xmath1  least sqaures ( l1-ls ) @xcite , sparse reconstruction by separable approximation ( sparsa ) @xcite , primal and dual augmented lagrangian methods ( palm and dalm)@xcite , iterative bayesian algorithm ( iba ) @xcite , sparse bayesian learning ( sbl ) @xcite and bayesian compressed sensing ( bcs ) @xcite .",
    "there are also more general @xmath3-norm minimization based algorithms available for solving the sparse recovery problem in the literature @xcite . for detailed survey on sparse recovery methods",
    ", one can refer to @xcite .",
    "in this paper we propose an alternative @xmath1  minimization method for sparse recovery of signals .",
    "specifically we consider the sparse recovery of image patches with missing samples which has application in image inpainting and restoration .",
    "we introduce a criterion for measuring the similarity between two image signals which is called convex similarity ( csim ) index . although it is derived from the structural similarity ( ssim ) index , the well - known perceptual quality assessment criterion @xcite , it has desirable mathematical features unlike its predecessor .",
    "in fact the advantage of the proposed index is its convexity and well - defined mathematical properties .",
    "these features result in simplified methods for solving the optimization problem involving this metric as the similarity index . in this paper",
    "we use this new index as fidelity criterion in our proposed optimization problem .",
    "similar to @xcite , an iterative algorithm is presented for @xmath1  minimization which uses alternating direction method of multipliers ( admm ) to solve the optimization problem .",
    "simulation results show the efficiency of the proposed method called csim minimization via augmented lagrangian method ( csim - alm ) compared to some popular existing algorithms .",
    "the problem of recovering an image with samples missed at random , is equivalent to random sampling reconstruction of the signal . this problem is also addressed in the literature as block loss restoration due to error in the transmission channel @xcite .",
    "it is also known as image inpainting especially in applications where the sampling mask is known and the objective is to fill in the gaps or remove occlusion or specific objects from the image @xcite .",
    "suppose @xmath4 is the vectorized image signal and @xmath5 is the sampling matrix by which the pattern of sampling of the image signal is determined .",
    "in other words @xmath6 is obtained by eliminating @xmath7 rows of the identity @xmath8 matrix corresponding to the index of the missing samples . the observed image signal with missing samples ,",
    "is also denoted by @xmath9 .",
    "there are many approaches for missing sample recovery of images or more specifically image inpainting including diffusion - based @xcite and exemplar - based @xcite methods . in this regard",
    "there are also a class of inpainting algorithms which use sparse representation for image restoration @xcite .",
    "if we assume that @xmath10 has approximately a sparse representation based on the atoms of a dictionary specified by the matrix @xmath11 , the regular optimization problem for sparse recovery of the missing samples is formulated as follows : @xmath12[c]{l } { \\vert \\vert}{\\mathbf{h}}{{\\bf{x}}}-{{\\bf{y}}}{\\vert \\vert}_2 ^ 2 \\leq \\epsilon_n \\\\ { { \\bf{x}}}={\\mathbf{d}}{{\\bf{s}}}\\end{ieeeeqnarraybox}\\right.\\ ] ] where @xmath13 denotes the sparse vector of representation coefficients and @xmath14 denotes the energy of the additive observation noise .",
    "the objective of the missing sample recovery problem is to reconstruct the original image @xmath10 based on observed remaining samples of the signal denoted by @xmath15 .",
    "this problem is also shown to be equivalent to : @xmath16 in @xcite there are iterative algorithms proposed for recovery of missing samples exploiting the sparsity of representation based on redundant ( over - complete ) dictionaries .",
    "these algorithms mostly use global reconstruction methods in which the whole image is considered to have sparse approximation and the iterations of these algorithms are performed globally to obtain the entire reconstructed image at each step .",
    "this approach can further be improved extending the image sparsity to local viewpoint . in other words instead of",
    "global restoration an image can be divided into small patches and the sparsity is promoted using local transforms or dictionaries . in this approach",
    "the image is partitioned into a set of @xmath17 overlapping patches of size @xmath18 denoted by @xmath19 and each patch @xmath20 is assumed to have sparse coefficients on a local transform basis @xmath11 .",
    "another improvement can also be achieved if we use adaptive dictionary instead of fixed @xmath11 .",
    "this dictionary can be learnt as in k - svd @xcite or be adopted to separate structures in an image called cartoon and texture as in mca algorithm @xcite . in @xcite a localized or patch - based algorithm for missing sample recovery is presented which uses an adaptive dictionary learning method .",
    "in fact this algorithm tries to solve the optimization problem below : @xmath21 where @xmath22 and @xmath23 denote the @xmath24th patch of the reconstructed image and the mask respectively and @xmath25 denotes its corresponding sparse coefficients on the basis of @xmath11 .",
    "this problem is alternately solved comprising of two steps namely sparse coding and dictionary learning . in the sparse coding step ,",
    "the dictionary is assumed fixed and the vector of sparse coefficients are obtained using sparse recovery methods like omp . in the dictionary learning step",
    "an approach like k - svd is used for adaptively learning a sparsifying dictionary based on existing sparse vectors . in this paper",
    "we propose an alternative method for sparse recovery of image signals which can be applied instead of omp in the sparse coding step of an adaptive dictionary learning method for image inpainting .",
    "we also use perceptual metrics for visually enhanced reconstruction of the missing samples of the image as fidelity criterion instead of @xmath0  norm . in the next section , we discuss more about the quality assessment metrics used for images .",
    "there are different criteria for image quality or in other words similarity assessment .",
    "the most popular fidelity metric for measuring the similarity between two signals @xmath26 is mse which is defined as @xmath27 .",
    "this criterion is widely used to measure the quality performance of an estimator which is subsequently used to recover a signal with missing samples .",
    "this is apparently because mse or equivalently @xmath0-norm is mathematically a well - defined function of the difference between the reference and the test signal .",
    "this function has desirable mathematical features such as convexity and differentiability which infers that it is simple and tractable to solve the optimization problem incurred by using this criterion as a penalty function .",
    "this optimization problem arises in any image recovery task such as denoising , deblurring and inpainting .",
    "nevertheless there are cases in which the mse criterion seems to be inefficient to accurately recover the original image signal especially in the presence of noise .",
    "one reason is that this fidelity metric is indifferent toward the error distribution , i.e. , the statistics of the error signal . for instance consider two scenarios in which a signal is corrupted .",
    "in the first case the reference signal is perturbed by noise and in the second , a constant amplitude is added to the original image .",
    "both corrupted images have the same mse distortion metric with respect to the original image signal , while the noisy image is definitely more visually deteriorated .",
    "thus there are a class of perceptual criteria introduced for measuring visual quality of images .",
    "the most popular perceptual metric is ssim which is defined as @xcite : @xmath28 where @xmath29 are constant and @xmath30 and @xmath31 denote the mean and the variance ( cross - covariance ) respectively .",
    "this function whose mathematical properties are discussed in @xcite , is non - convex and multi - modal implying that the problem of optimizing this criterion is hard to solve .",
    "another quality assessment metric is feature similarity ( fsim ) index @xcite , which is proposed based on the fact that human visual system ( hvs ) perceives the image quality mainly based on its low - level features . in @xcite , phase congruency ( pc ) and gradient magnitude ( gm )",
    "are considered as primary and secondary features used in characterizing the image local quality .",
    "moreover , there are other fidelity metrics in the literature such as an information theoretic index called visual information fidelity ( vif ) @xcite , multi - scale ssim ( ms - ssim ) @xcite , dynamic range independent measure ( drim ) @xcite , and tone - mapped quality index ( tmqi ) @xcite . for further investigation on subjective and objective quality assessment criteria",
    ", one can refer to @xcite .      as mentioned earlier ,",
    "ssim function is non - convex and multi - modal which results in hard optimization problems to solve .",
    "hence , in this paper , we define a simplified criterion derived from the numerator and the denominator terms appearing in .",
    "the proposed index named csim , is defined as follows : @xmath32 where @xmath33 and @xmath34 are positive constants .",
    "let us assume that @xmath35 ; this is to ensure biased sensitivity toward random disturbances or noise compared to uniform change .",
    "in fact unlike mse , the new criterion has noise - sensitive variation . in other words a constant change in the brightness level of the image",
    "does not alter this criterion as much as the case of noisy disturbance does .",
    "this is logically because constant change in the amplitude only affects the signal mean value .",
    "it does not change variance/ cross - covariance .",
    "thus as far as @xmath35 the function csim is only slightly influenced by mere brightness level change .",
    "the metric above also benefits some feasible mathematical features .",
    "for example it is a convex and a positive - definite function of @xmath10 or @xmath15 . in the following we state two remarks in this correspondence .",
    "[ theo:1 ] the fidelity criterion defined by the function in , is positive - definite .",
    "suppose @xmath36 , then @xmath37 note that we have used the unbiased estimate for variance and covariance .",
    "therefore @xmath38 where the equality is satisfied if and only if @xmath39 . on the other hand @xmath40",
    "hence , if we define the error as the difference between these two signals , i.e. , @xmath41 , the function in may be simplified as : @xmath42 where @xmath43 .",
    "now since @xmath44 , the function above is obviously positive - definite and equals zero if and only if @xmath45 or equivalently @xmath46    [ theo:2 ] the fidelity criterion defined by the function in is convex with respect to @xmath10 or @xmath15 .",
    "one can rewrite equation algebraically using the simplification as follows : @xmath47 where @xmath48 and @xmath49 is the identity matrix .",
    "therefore @xmath50 and @xmath51 is @xmath52 now since @xmath53 is a continuous and twice differentiable function of @xmath54 , to prove its convexity , it is sufficient to show that the hessian of @xmath53 or equivalently @xmath55 is positive - semidefinite .",
    "once the covexity of @xmath53 with respect to @xmath41 has been affirmed , it will also be concluded that @xmath56 is convex with respect to @xmath10 or @xmath15 .",
    "now the objective is to compute the eigenvalues of @xmath55 to approve the convexity of @xmath56 , but beforehand we need to express the following lemma :    [ lemm:1 ] let @xmath57 where @xmath58 . if the eigenvalues of @xmath59 are given as @xmath60 , then the eigenvalues of @xmath61 will be obtained using the following equation : @xmath62    the proof is trivial exploiting the definition of the eigenvalues and eigenvectors of @xmath63 .    now back to our discussion ,",
    "let us assume @xmath64 and @xmath65 . now using lemma [ lemm:1 ] we can obtain the eigenvalues of @xmath55 .",
    "but firstly we need to compute the eigenvalues of @xmath59 .",
    "since @xmath59 is symmetric and has a maximum rank of one , it can be concluded that there is only a single non - zero eigenvalue denoted by @xmath66 which satisfies : @xmath67 and all the remaining eigenvalues @xmath68 are zero .",
    "now if we set @xmath69 , then @xmath66 is given by : @xmath70 so : @xmath71[c]{l 's } 0 , & $ i",
    "< n$\\\\ \\frac{k_1}{n}-\\dfrac{k_2}{n-1 } , & $ i = n$\\end{ieeeeqnarraybox}\\right.\\ ] ] consequently the eigenvalues of the kernel matrix @xmath61 are obtained by : @xmath72[c]{l 's } \\frac{k_2}{n-1 } , & $ i <",
    "n$\\\\ \\frac{k_1}{n } , & $ i = n$\\end{ieeeeqnarraybox}\\right.\\ ] ] therefore the primary assumption @xmath73 is not a necessary condition to approve the convexity of @xmath56 and it only suffices for @xmath33 and @xmath34 to be non - negative .",
    "now the problem is how to find @xmath33 and @xmath34 so that the proposed criterion has utmost sensitivity to random perturbation compared to uniform change . assume a random binary signal with i.i.d .",
    "elements taking @xmath74 with equal probability , is added to the reference signal @xmath10 , i.e. : @xmath75 = a^2 { \\mathbf{i}}_n\\ ] ] consider another scenario in which @xmath10 is added by a deterministic signal @xmath76 with constant amplitude @xmath77 , i.e. , @xmath78 .",
    "now we define the ratio of sensitivity as : @xmath79}{{\\mathbb{e}}[{{\\mathrm{csim}}}({{\\bf{x}}},{{\\bf{y}}}_2 ) ] } = \\dfrac{{\\mathbb{e}}[{{\\mathrm{csim}}}({{\\bf{e}}}_1)]}{{\\mathbb{e}}[{{\\mathrm{csim}}}({{\\bf{e}}}_2 ) ] } = \\dfrac{{\\mathbb{e}}[{{\\bf{e}}}_1^t { \\mathbf{w}}{{\\bf{e}}}_1]}{{{\\bf{e}}}_2^t { \\mathbf{w}}{{\\bf{e}}}_2}\\ ] ]    [ rem:1 ] the sensitivity ratio is equal to @xmath80 .",
    "let @xmath81 and @xmath82 .",
    "since for a scalar variable @xmath83 , we have @xmath84 , we may write : @xmath85 & = \\mathrm{trace}({\\mathbb{e}}[{{\\bf{e}}}_1^t { \\mathbf{w}}{{\\bf{e}}}_1])={\\mathbb{e}}[\\mathrm{trace}({{\\bf{e}}}_1^t { \\mathbf{w}}{{\\bf{e}}}_1 ) ] \\nonumber \\\\ & = \\mathrm{trace}({\\mathbb{e}}[{{\\bf{e}}}_1 { { \\bf{e}}}_1^t ] { \\mathbf{w } } ) = a^2 \\mathrm{trace}({\\mathbf{w}})\\end{aligned}\\ ] ] similarly",
    ", we have : @xmath86    now for @xmath55 defined in , the sensitivity ratio would be simplified to : @xmath87 this relation states that the greater the ratio @xmath88 is , the more sensitive the proposed csim index will be toward noise .",
    "but there are other conditions which impose constraints on the eigenvalues of @xmath55 and accordingly the values of @xmath33 and @xmath34 .",
    "one is the condition number . consider the optimization problem below : @xmath89 since @xmath55 is hermitian and positive - definite , it is diagonalizable and thus , @xmath90 exists and is as follows : @xmath91 where @xmath92 and @xmath93 are obtained by : @xmath94 hence , we can rewrite as : @xmath95 where @xmath96 and @xmath97 . the solution to is obtained as @xmath98 . for to have a robust ( reliable ) solution",
    ", the matrix @xmath55 must not be ill - conditioned . now assuming @xmath99 , using",
    ", we can write : @xmath100 where @xmath101 denotes the condition number of @xmath55 and @xmath102 denotes the maximum value of condition number permitted .",
    "another constraint is imposed by adding @xmath1-norm penalty to the cost function in , i.e. : @xmath103 this cost function can be iteratively optimized using ista @xcite , and the estimated solution at iteration @xmath104 is obtained by : @xmath105 where @xmath106 denotes the soft - thresholding operator @xcite .",
    "a sufficient condition to insure converges to the solution of and is its unique minimizer , is that @xmath107 and the spectral norm of @xmath108 should be less than unity @xcite . since @xmath109 is invertible",
    ", the first condition is satisfied as far as @xmath107 .",
    "therefore : @xmath110 now since @xmath111 , if we assume @xmath112 , a sufficient condition could be expressed as : @xmath113 since a larger value of @xmath114 guarantees lower @xmath115 and consequently faster rate of convergence @xcite , to gain maximum @xmath116 , we choose : @xmath117 in the next sections we assume @xmath118 and let @xmath119 .",
    "as discussed earlier in section [ sec : problem ] most of the algorithms use @xmath0  norm as fidelity criterion for image reconstruction .",
    "but there are also inpainting methods based on local sparse representation which use perceptual image quality assessment metrics for recovery of the missing samples . in @xcite an exemplar - based method for image completion",
    "is proposed which uses adaptive dictionary learning for spare recovery of local image patches . in this algorithm , for each candidate patch the optimization problem below is solved in the sparse coding step :    @xmath120[c]{l }   { \\mathbf{h}}{{\\bf{x}}}={{\\bf{y}}}\\\\ { \\vert \\vert}{{\\bf{s}}}{\\vert \\vert}_0 \\leq t\\end{ieeeeqnarraybox}\\right.\\ ] ]    where for simplicity we have omitted the index of the patch @xmath10 and its corresponding sparse representation vector @xmath13 .",
    "this optimization problem is iteratively solved using a matching pursuit approach , i.e. , in each step the support of the sparse vector is retrieved and the coefficients are subsequently obtained solving unconstrained .",
    "this problem is non - convex and thus solved using time - consuming linear search methods . here",
    "we propose to use csim instead of ssim in the optimization problem .",
    "hence , to solve the missing sample recovery problem defined in ( [ eq_19 ] ) , we incorporate our proposed perceptual metric for reconstruction of the image samples .",
    "we do this by adding another constraint to this problem confining the csim of @xmath121 and @xmath15 to be less than a predefined value , i.e. @xmath122 .",
    "this is to insure reconstruction quality in terms of our proposed fidelity criterion .",
    "besides consider we have an estimate of the missed samples of @xmath10 which is denoted by @xmath123 where @xmath124 specifies the complement set of the sampling indices of @xmath5 .",
    "this estimate may be obtained by interpolating the signal @xmath10 ( with missed samples ) or in an exemplar inpainting method , it is actually the nearest exemplar of the corrupted patch of the image .",
    "hence , the constraint @xmath125 imposes some sort of fidelity criterion over the indices of the signal with unknown sample values . we will talk about this later in the method we propose for image inpainting . for",
    "now simply let @xmath126 and @xmath127 .",
    "now using lagrange multipliers theorem , the new problem is equivalent to : @xmath128 where @xmath129 and @xmath130 are chosen such that karush - kuhn - tucker ( kkt ) conditions are satisfied @xcite . note that since @xmath56 is convex and uni - modal finding the local minima of is sufficient to attain its global optimum . now introducing an auxiliary variable defined as @xmath131 the optimization problem would change to :    @xmath132[c]{l }",
    "{ { \\bf{x}}}={\\mathbf{d}}{{\\bf{s}}}\\\\ { { \\bf{z}}}={\\mathbf{h}}{{\\bf{x}}}\\end{ieeeeqnarraybox}\\right.\\ ] ]    the auxiliary variable is used to separate the optimization problem involving @xmath56 as the fidelity index .",
    "since the csim function is convex , it is guaranteed use the alternating direction method of multipliers ( admm ) @xcite to solve .",
    "hence the final cost function to be optimized is the augmented lagrangian function : @xmath133 the admm alternatively minimizes with respect to each variable while assuming the other variables fixed .",
    "hence at each iteration of the admm , the problem is split into three sub - problems as follows :      the augmented lagrangian cost function with respect to @xmath10 is a quadratic function .",
    "hence , the optimization sub - problem associating with @xmath10 at @xmath104-th iteration of the admm is : @xmath134 where @xmath135 and @xmath136 .",
    "the solution to this problem is simply obtained by differentiation and it is : @xmath137 now since @xmath138 is equivalent to consecutively projecting the extracted samples back to the initial higher dimensional space and repeating the sampling process , @xmath139 . hence ,",
    "using sherman - morrison - woodbury lemma @xcite , the solution to would be simplified to : @xmath140      the optimization sub - problem associating with @xmath13 is : @xmath141 assume @xmath142 , using the majorization minimization ( mm ) technique @xcite , we define a surrogate function similar to what is proposed in @xcite .",
    "@xmath143 since @xmath144 and @xmath145 , optimizing with respect to @xmath13 will reduce the initial cost function @xmath146 .",
    "hence by eliminating the unnecessary variables , the surrogate optimization problem is simplified to : @xmath147 where @xmath148 let us set @xmath149 , where @xmath104 denotes the iteration number .",
    "now the solution to is obtained using the soft - thresholding operator and @xmath150 is updated according to : @xmath151 now since @xmath152 is the minimizer of @xmath153 we have : @xmath154 where the first inequality comes from the fact that the surrogate function is the majorization of the original cost function .",
    "for this condition to be satisfied we use a backtracking procedure to choose the appropriate value of @xmath155 .",
    "this method as proposed in @xcite , solves the optimization problem and checks whether the solution @xmath156 satisfies @xmath157 . if true the value of @xmath152 is set to @xmath156 and if not , it multiplies the value of @xmath155 by a constant @xmath158 .",
    "hence it is an adaptive approach to specify the threshold @xmath159 .",
    "& & & & & & + & mse filter & csim filter & ssim filter & mse filter & csim filter & ssim filter + & & & & & & +    & psnr ( db ) & * 22.02624 & 19.59074 & 18.74218 & * 23.01379 & 20.51426 & 18.92838 + & ssim & 0.437667 & * 0.463604 & 0.445002 & 0.488934 & * 0.513346 & 0.50831 + & fsim & 0.725322 & * 0.752717 & 0.733245 & 0.753915 & * 0.774345 & 0.763619 + & time ( s ) & * 7.703125 & 8.671875 & 232.375 & * 11.32813 & 21.45313 & 210.5 + * * * * * * * *    & psnr ( db ) & * 23.47825 & 21.01571 & 19.62036 & * 24.47371 & 21.92508 & 19.82099 + & ssim & 0.497242 & * 0.520295 & 0.506187 & 0.54762 & 0.567756 & * 0.571409 + & fsim & 0.763082 & * 0.790042 & 0.777688 & 0.787491 & * 0.807698 & 0.807059 + & time ( s ) & * 6.984375 & 8.40625 & 201.5781 & * 10.48438 & 15.82813 & 206.9375 + * * * * * * * *    & psnr ( db ) & * 22.19703 & 20.21621 & 18.89702 & * 23.18561 & 21.16635 & 19.115 + & ssim & 0.517242 & * 0.537474 & 0.529405 & 0.577095 & * 0.593189 & 0.591923 + & fsim & 0.749932 & * 0.773378 & 0.75956 & 0.775784 & * 0.794461 & 0.784306 + & time ( s ) & * 7.46875 & 7.984375 & 228.375 & * 12.48438 & 15.54688 & 236.7344 + * * * * * * * *    & psnr ( db ) & * 22.05487 & 19.64165 & 18.80577 & * 23.03948 & 20.47174 & 19.07671 + & ssim & 0.387762 & * 0.396543 & 0.385529 & 0.439848 & * 0.455067 & 0.447596 + & fsim & 0.69685 & * 0.72486 & 0.695154 & 0.723184 & * 0.744779 & 0.72871 + & time ( s ) & * 8 & 9.78125 & 232.0625 & * 10.07813 & 15.40625 & 238.5 + * * * * * * * *    & psnr ( db ) & * 21.99298 & 19.3455 & 18.53909 & * 23.0564 & 20.35864 & 18.81843 + & ssim & 0.463839 & * 0.488692 & 0.475355 & 0.524988 & * 0.550085 & 0.543624 + & fsim & 0.765743 & * 0.793235 & 0.782742 & 0.790887 & * 0.81209 & 0.809912 + & time ( s ) & * 7.28125 & 8.078125 & 233.6094 & * 9.796875 & 15.54688 & 239.6875 + * * * * * * * *    & psnr ( db ) & * 23.09378 & 21.41167 & 18.93118 & * 23.93885 & 22.19459 & 19.13656 + & ssim & 0.531812 & 0.542148 & * 0.544879 & 0.578546 & 0.589889 & * 0.611208 + & fsim & 0.806133 & * 0.816203 & 0.810392 & 0.826742 & 0.831216 & * 0.834012 + & time ( s ) & * 7.578125 & 10.35938 & 232.1719 & * 9.859375 & 14.04688 & 236.625 + * * * * * * * *    [ table_1 ]     +      the sub - problem associating with @xmath160 is as follows : @xmath161 now substituting @xmath56 from , the resulting cost function will be in the quadratic form below : @xmath162     +    * set * @xmath163 , @xmath164 * initialize * @xmath165 , @xmath166 , @xmath167 , @xmath168 , @xmath169 .",
    "update @xmath170 update @xmath171 update @xmath172 and @xmath173 using and obtain @xmath174 by solving assuming @xmath149    set @xmath175 , update @xmath176 update @xmath177 update @xmath178 by solving using @xmath179 as in update @xmath180 and @xmath181 according to @xmath182    [ algorithm_1 ]    where @xmath183 and @xmath184 .",
    "the solution to is @xmath185 . to calculate the inverse of @xmath186 we use the matrix inverse lemma : @xmath187 where @xmath188 and @xmath189 .",
    "the final step of the admm is to update the lagrangian multipliers associated with the equality constraints .",
    "hence , we have : @xmath190    finally the proposed algorithm named as csim minimization via augmented lagrangian method ( csim - alm ) , is given in algorithm [ algorithm_1 ] .",
    "there are some points to be noticed .",
    "first of all we can use the method of fista @xcite to accelerate the rate of convergence of the iterative reconstruction of @xmath13 using . this way the proposed algorithm converges within the first few iterations .",
    "of course this step may be discarded by performing more iterations of the algorithm instead , to achieve similar reconstruction quality .",
    "another remark is that we use a variable ( adaptive ) regularizing parameter @xmath129 according to sparsa @xcite .",
    "+     & & & & & & + & mse filter & csim filter & ssim filter & mse filter & csim filter & ssim filter + & & & & & & +    & psnr ( db ) & 21.64793 & * 21.7143 & 21.5236 & 21.49739 & * 21.5663 & 21.547 + & ssim & 0.483457 & * 0.487075 & 0.48695 & 0.541758 & * 0.547965 & 0.545625 + & fsim & 0.737972 & * 0.743853 & 0.74152 & 0.760147 & * 0.762156 & 0.759996 + & time ( s ) & * 8.671875 & 8.84375 & 123.4844 & 12.90625 & * 12.5 & 126.6406 + * * * * * * * *    & psnr ( db ) & 23.4301 & * 23.51699 & 23.5393 & 23.27777 & 23.29914 & * 23.31125 + & ssim & 0.509773 & * 0.51945 & 0.516033 & 0.554572 & * 0.559539 & 0.55748 + & fsim & 0.76562 & * 0.770744 & 0.769893 & 0.779354 & * 0.780314 & 0.780307 + & time ( s ) & 8.875 & * 8.546875 & 121.7813 & 12.73438 & * 12.57813 & 125.0469 + * * * * * * * *    & psnr ( db ) & 22.07703 & * 22.09299 & 22.06136 & * 21.86944 & 21.86258 & 21.80966 + & ssim & 0.471681 & * 0.474548 & 0.47164 & 0.478401 & * 0.479339 & 0.473757 + & fsim & 0.757897 & * 0.759037 & 0.755325 & 0.760586 & * 0.763256 & 0.756748 + & time ( s ) & 9.09375 & * 8.609375 & 122.1875 & 13.15625 & * 12.51563 & 125.1719 + * * * * * * * *    & psnr ( db ) & 23.43906 & * 23.5731 & 23.52321 & 24.12948 & * 24.16865 & 24.10161 + & ssim & 0.386421 & * 0.394148 & 0.390097 & 0.476552 & * 0.48155 & 0.479555 + & fsim & 0.693577 & * 0.695039 & 0.694965 & 0.731814 & * 0.733746 & 0.732695 + & time ( s ) & 9.09375 & * 8.6875 & 124.75 & 13.20313 & * 12.64063 & 129.3281 + * * * * * * * *    & psnr ( db ) & 22.06733 & * 22.07957 & 22.05864 & 22.0714 & 22.06688 & * 22.10787 + & ssim & 0.382708 & 0.396937 & * 0.39702 & 0.453836 & * 0.457089 & 0.454604 + & fsim & 0.674499 & * 0.683098 & 0.681842 & 0.687971 & * 0.693188 & 0.689301 + & time ( s ) & * 8.921875 & 8.96875 & 125.5 & 13.14063 & * 12.75 & 123.0781 + * * * * * * * *    & psnr ( db ) & * 22.70483 & 22.67239 & 22.69083 & 22.37438 & 22.37977 & * 22.38362 + & ssim & 0.46712 & 0.467596 & * 0.470518 & * 0.458069 & 0.457024 & 0.454858 + & fsim & 0.758213 & 0.75606 & * 0.758453 & 0.730827 & * 0.732256 & 0.728856 + & time ( s ) & 9.25 & * 8.625 & 124.0313 & 13.42188 & * 12.29688 & 125.4844 + * * * * * * * *    [ table_2 ]",
    "in this part , we conduct an experiment to show the performance of the proposed quality assessment criterion compared to some popular criteria , namely mse , ssim and fsim .",
    "consider @xmath191 is the reference image signal and @xmath192 denotes the noisy observed image , i.e. , @xmath193 where @xmath194 denotes the noise signal which has gaussian distribution with zero mean and variance @xmath195 , @xmath196 .",
    "suppose that the image is divided into small patches of size @xmath197 .",
    "the problem is to find a linear denoising filter @xmath198 whose convolution with the @xmath24th patch of the image denoted by @xmath199 gives an estimate of the original patch signal denoted by @xmath200 .",
    "i.e. @xmath201 = \\sum_{k=0}^{m-1 } h_j[k ] y_j[i - k ] , \\quad i=0,1,\\ldots , n-1\\ ] ] after the small patches are denoised , the entire image is then reconstructed by superposition of the recovered patches .",
    "in this case we are given a clean estimate of the original signal @xmath10 .",
    "this estimate for instance , may be obtained by lowpass ( moving average ) filtering the noisy image signal in a natural application .",
    "but here we simply use the original image signal which is assumed to be known .",
    "this is in fact an artificial experiment to assess whether theoretically optimizing the proposed criterion is visually preferred to mse and ssim optimization or not .",
    "now if we assume @xmath202=0,\\,\\text{for}\\ , i<0 $ ] , we can restate the equation in algebraic form @xmath203 , where : @xmath204   & 0       & 0   & \\ldots & 0\\\\ y_j[1 ]   & y_j[0 ] & 0   & \\ldots & 0\\\\ \\vdots & \\vdots & \\vdots & \\ldots & \\vdots\\\\ y_j[n-1 ] & y_j[n-2 ] & y_j[n-3 ] & \\ldots & y_j[n - m ] \\end{pmatrix}\\ ] ] hence , in this case we have to solve the following optimization problem :    @xmath205    where @xmath53 denotes the performance metric we use as the fidelity criterion between the original and the reconstructed signal . if we use csim as the fidelity criterion , then the optimal filter @xmath206 will be obtained by solving the problem below : @xmath207 we use different approaches to find the optimal denoising filter .",
    "namely we use the mse , ssim and the csim optimal filters for denoising image patches with different levels of additive noise .",
    "the mse optimal filter is simply obtained by :    @xmath208    and the optimal filter based on ssim criterion is achieved by solving the optimization problem below : @xmath209    this optimization problem is non - convex , but using a linear search method it can be converted to a quasi - convex problem . the similar problem ( of course with sparsity constraint ) is solved in @xcite .",
    "now we use the optimized filters obtained by solving optimization methods introduced above to denoise image patches of size @xmath210 distorted with different noise levels .",
    "we then compare reconstruction quality of the denoising filters of different orders in terms of psnr , ssim and fsim .",
    "fig [ fig_1 ] shows the performance of the designed fir filters . for this part",
    "the image patches are raster scanned and converted into vectors of size @xmath211 .",
    "we also set the parameters of csim as @xmath212 .",
    "as shown in the figure the proposed metric provides some sort of reconstruction quality which stands between psnr and ssim .",
    "the denoised patches via csim filter have higher psnr than those having been denoised using the same order ssim filter .",
    "this is approximately the same based on fsim performance .",
    "the difference between these image quality assessment criteria is influenced by the noise level . at high snr ( low noise )",
    "the proposed metric is quite similar to mse or psnr , whereas at low snr it behaves much more like ssim .",
    "the fsim performance also confirms the superiority of the proposed criterion for image denoising compared to mse and ssim .",
    "this result can also be visually confirmed according to fig .",
    "[ fig_denoised ] which shows the denoising results for fir filters of order @xmath213 .",
    "table [ table_1 ] shows the restoration results for entire @xmath214 images obtained by separately dividing the image into overlapping @xmath215 patches and using the fir denoising filters for local reconstruction .",
    "it is clear that for low snr and the limited number of fir taps , the proposed method achieves higher reconstruction quality compared to mse and ssim methods . furthermore comparing the complexity or the running time of the denoising process of each algorithm , shows that the optimization of the proposed csim criterion , unlike ssim , is quite as fast and easy as mse because of its convexity .      in this scenario",
    "the original image signal @xmath10 is unknown and unavailable .",
    "in fact @xmath10 is the vector of spatial samples of a random process @xmath216 $ ] called the image signal , which is assumed to be ergodic and wss stationary .",
    "the clean image signal is then added by a white noise process @xmath217 $ ] whose samples are denoted by @xmath218 .",
    "the noise process is assumed to be also wss having gaussian distribution with zero mean and variance @xmath195 .",
    "the observed noisy image signal @xmath15 is thus modelled by the sum of theses two random processes . hence , in this case we are encountered stochastic signals and the problem of finding the equalizer filter , is indeed a linear estimation problem as discussed in @xcite .",
    "but here we assume stationariness within the spatial domain of each patch .",
    "in other words , the small patches of the clean and the noisy image signals denoted by @xmath22 and @xmath219 are considered stationary within their corresponding spatial domain .",
    "thus although all the patches are wss stationary processes of size @xmath220 , they have different distributions .",
    "hence , we confine ourselves to local patch denoising for estimation of @xmath206 .",
    "the usual fidelity criterion used for estimation is mse which leads to the so - called wiener - hopf equations : @xmath221-\\sum_{k=0}^{m-1 } h_j[k ] y_j[i - k ] \\big)^2\\big]\\\\ & = { \\mathbf{r}}_{y_j , y_j}^{-1 }   { { \\bf{r}}}_{x_j , y_j } \\nonumber\\end{aligned}\\ ] ] where @xmath222 and @xmath223 denote the auto - correlation matrix and the the vector of cross - correlation components respectively , i.e. , @xmath224 & = r_{y_j ,",
    "y_j}[k - l ] , \\quad k , l=0,\\ldots , m-1   \\nonumber \\\\ { { \\bf{r}}}_{x_j , y_j}[k ] & = r_{x_j , y_j}[k ] , \\quad k=0 , \\ldots , m-1\\end{aligned}\\ ] ] now assume the noise process is independent from @xmath10 and is distributed homogeneously through the whole image . using @xmath225 with @xmath226 and @xmath227=\\sigma_n^2 \\delta[k ]",
    "$ ] , it can be shown that @xcite @xmath228&=c_{x_j , x_j}[k]= c_{y_j , y_j}[k ] -\\sigma_n^2 \\delta[k ] \\nonumber \\\\ \\mu_{x_j } & = \\mu_{y_j}\\end{aligned}\\ ] ] we also have : @xmath229=c_{x_j , y_j}[k]+\\mu_{y_j}^2 , \\quad r_{y_j , y_j}[k]=c_{y_j , y_j}[k]+\\mu_{y_j}^2\\end{aligned}\\ ] ] instead of the mse criterion we may use csim or ssim in our estimation problem . if we use the statistical definition of csim , the optimization problem for finding the denoising filter @xmath206 would be : @xmath230 where @xmath231 is obtained by .",
    "now we have the following relations : @xmath232 y_j[i - k ] \\big ] = \\sum_{k=0}^{m-1 } h_j[k ] \\mu_{y_j}\\end{aligned}\\ ] ]    @xmath233-\\mu_{x_j})(\\hat{x}_j[i]-\\mu_{\\hat{x}_j } ) \\big ] \\\\ & = { \\mathbb{e}}\\big [   ( x_j[i]-\\mu_{x_j})\\big(\\sum_{k=0}^{m-1}h_j[k](y_j[i - k]-\\mu_{y_j } ) \\big ) \\big ]   \\nonumber \\\\ & = \\sum_{k=0}^{m-1}h_j[k ] c_{x_j , y_j}[k]={{\\bf{h}}}_j^t { { \\bf{c}}}_{x_j , y_j } \\nonumber \\end{aligned}\\ ] ]    now if we denote the covariance matrix of @xmath219 by @xmath234 , the optimization problem is simplified to :    @xmath235    this problem is quadratic in terms of @xmath206 . to solve this we use and we differentiate with respect to @xmath206 which gives : @xmath236 hence , @xmath237 to reduce the complexity of calculating the inverse above , having @xmath238 , we can use matrix inverse lemma .    now let us turn to ssim fidelity criterion .",
    "the ssim optimization problem for estimating the optimal equalization filter , tries to maximize the cost function below with respect to @xmath206 @xcite : @xmath239 this optimization problem is solved via conversion to a quasi - convex format using bi - sectional search method @xcite .    in practical simulations for denoising image patches",
    ", the covariance matrix @xmath234 is empirically obtained using unbiased estimation and we use equation and for the values of @xmath240 and @xmath241 .",
    "the simulation results for denoising with fir filters are given in table [ table_2 ] .",
    "it is clear that in the case where the original signals are unknown , the proposed csim metric is outperforming the other criteria in terms of estimation quality .",
    "it is even faster and performing better than mse and ssim in most cases .",
    "+     +      in this experiment we compare the quality performance of the proposed csim - alm method for recovery of missing samples of image patches with some popular sparse recovery algorithms .",
    "we use imatimat/ ] , l1-ls , dalmyang / software / l1benchmark/ ] , tvoptimization / l1/tval3/]@xcite , fista , sl0slzero/ ] , gomp and the method in @xcite which we call it ssim - based matching pursuit ( ssim - mp ) .      for simulations of this part",
    ", we extract @xmath210 patches of sample gray - scale images shown in fig [ fig_images ] .",
    "we then vectorize the patches using raster scanning and select 100 patch vectors of size @xmath211 denoted by @xmath242 at random . for each patch ,",
    "a binary ( @xmath243 ) random sampling matrix @xmath23 with size @xmath244 is generated and the observed image signal for each experiment is acquired by @xmath245 .",
    "the locations of @xmath246s in the rows of @xmath23 ( corresponding to @xmath247 sampling indices ) are chosen uniformly at random and the sampling ratio of the signal defined as @xmath248 varies between @xmath249 .",
    "we use complete ( @xmath250 ) and over - complete dct ( @xmath251 ) dictionaries for reconstruction and we assume sparse representation on the basis of the dct atoms .",
    "this in fact the case where compressible representation is presumed and the exact sparsity is unknown .",
    "hence , to use matching pursuit methods we consider the image patch signal is @xmath252 sparse . after the sparse recovery of missed samples ,",
    "we then average over random experiments ( random @xmath242s and random masks with same @xmath247 ) and plot the psnr , ssim and fsim of the reconstruction performance , versus the sampling rate .",
    "the parameters for sl0 and tv are set to their defaults .",
    "we have edited and accelerated the algorithm of fistayang / software / l1benchmark/ ] by removing unnecessary code lines and setting @xmath253 and @xmath254 .",
    "the values of the exponential threshold parameters in imat is set to @xmath255 and the value of the initial threshold to @xmath256 .",
    "the stopping criterion for dalm and l1-ls are set to their defaults meaning that the algorithms stop when the duality gap falls below a certain tolerance .",
    "the stopping criterion for the remaining algorithms including imat , fista , sl0 and csim - alm is set to the maximum iteration count which is 50 .",
    "the parameters for csim - alm are chosen as @xmath257 , @xmath258 , @xmath254 , @xmath253 , @xmath259 and @xmath260 . also similar to fista , the value of @xmath261 is set to @xmath262 .",
    "[ fig_2 ] shows the results of the sparse recovery from random samples .",
    "as depicted in these figures , the proposed csim - alm algorithm mostly outperforms the state of the art algorithms for sparse recovery via @xmath1-norm minimization specifically at @xmath263 .",
    "it mainly provides a better reconstruction quality compared to dalm and tv which commonly use the admm technique to solve the @xmath1  optimization problem .",
    "this superiority is specifically more apparent in terms of ssim performance which is shown in the second columns in fig .",
    "[ fig_2 ] .",
    "the proposed csim - alm algorithm also outperforms the sparse recovery method ssim - mp which is based on non - convex ssim maximization .",
    "furthermore the rate of convergence of csim - alm as shown in fig .",
    "[ fig_err ] is significantly faster than l1-ls and fista which uses the same method of acceleration for updating @xmath150 .",
    "the running time for some of these sparse recovery methods is also compared in fig .",
    "[ fig_time ] . the vertical axis in fig .",
    "[ fig_err ] is the relative error and the vertical axis in fig .",
    "[ fig_time ] represents the time each iteration takes in seconds .",
    "the horizontal axis in these figures also shows the number of iterations which is set to maximum 70 for all algorithms .",
    "since the initial sparse vector is not determined in this case and we only assume sparsity or more precisely compressibility in presentation of the image patches based on dct atoms , the relative error is defined as : @xmath264 where @xmath265 denotes the sparse vector recovered given the vector of observation samples @xmath266 since ssim - mp algorithm uses matching pursuit based on a given sparsity , it iterates until all the sparse components of the signal are recovered .",
    "hence , in fig . [ fig_err ] and fig .",
    "[ fig_time ] only 20 iterations of this algorithm ( corresponding with assumed sparsity of @xmath267 approximately ) are shown and beyond this limit the algorithm usually starts to diverge . although it may seem that this method which is based on matching pursuit via non - convex ssim maximization , yields the least reconstruction error within only few iterations , but looking at fig .",
    "[ fig_time ] it is clear that it takes much longer time than csim - alm to perform only the first iteration of ssim - mp . in fact ssim - mp is the most complex algorithm ( in our comparison ) because of its non - convex optimization behaviour .",
    "the least complex algorithms are sl0 , dalm and imat but they need more iterations to reach stable recovery performance compared to csim - alm . indeed according to fig .",
    "[ fig_err ] csim - alm reaches its minimum reconstruction error within roughly 10 - 15 iterations .",
    "this is much less than sl0 for high @xmath268 and dalm for low sampling rates .      in the second scenario",
    ", we consider the image signal @xmath242 is artificially sparse , i.e. , it is generated via multiplying a dictionary matrix @xmath269 by a strictly sparse vector @xmath270 , i.e. , @xmath271 .",
    "the @xmath272 non - zero elements of the random sparse signal @xmath273 are chosen from a gaussian distribution with @xmath274 and the locations of non - zero entries are also chosen uniformly at random . in this scenario",
    "we actually compare the reconstruction performance of the proposed algorithm for recovery of @xmath272-sparse signals with the algorithms introduced in the previous part .",
    "we use columns of discrete dct and walsh - hadamard matrices as the atoms of the dictionary @xmath11 .",
    "similar to the previous scenario , we investigate the missing sample recovery of the vectors @xmath242 by generating a sampling matrix @xmath23 as discussed before . fig .",
    "[ fig_ptc ] depicts the 2d plot of normalized mean square error ( nmse ) versus the sparsity @xmath275 and the undersampling @xmath276 ( or @xmath277 ) ratios .",
    "this plot shows the reconstruction nmse in a @xmath278 grid where @xmath279 .",
    "the nmse in this case where the initial sparse vector is knows , is defined as : @xmath280    where like before , @xmath265 denotes the recovered sparse vector given the observation @xmath281 .",
    "this plot is quite similar to the phase - transition curve ( ptc ) @xcite , but here instead of the successful recovery rate , we have plotted the nmse in two dimensions .",
    "furthermore , we have used dct and walsh - hadamard transform instead of the random projection ( gaussian ) matrix which is normally used in the ptc diagram . in fig .",
    "[ fig_ptc ] , the bluer the mesh color , the lower the nmse is and as the value of nmse rises ( poor recovery ) the color tends to yellow . in this experiment",
    "we consider recovery performance of strict sparse signals .",
    "the large area of the blue region indicates that the algorithm is more capable of recovering a sparse vector under different circumstances .",
    "it is actually proportional to the performance of the algorithm and as it is clear from fig .",
    "[ fig_ptc ] , the proposed csim - alm algorithm is more efficiently dealing with the signal reconstruction problem compared to fista and dalm for recovery of strict sparse signals via dct and walsh - hadamard dictionaries .",
    "in this paper , a new performance metric for image quality assessment is introduced which is a modified convex version of ssim and is called csim .",
    "this metric like mse , is well suited for mathematical manipulations and like ssim , has perceptual meanings .",
    "the proposed fidelity metric is used for solving the missing sample recovery problem based on sparse representation of the image patches .",
    "this sparse recovery method can subsequently be applied in the sparse coding step in a dictionary learning method which in turn may be used in image inpainting and restoration . in addition , an iterative admm - based algorithm is proposed to solve the optimization problem obtained from incorporating the new fidelity metric .",
    "the convexity of the optimization problem leads to an efficient iterative convergent algorithm .",
    "simulation results show the efficiency of the suggested new performance metric as well as the superiority of the proposed iterative algorithm over counterpart methods for missing sample recovery of images ."
  ],
  "abstract_text": [
    "<S> this paper investigates the problem of recovering missing samples using methods based on sparse representation adapted especially for image signals . instead of @xmath0-norm or mean square error ( mse ) , </S>",
    "<S> a new perceptual quality measure is used as the similarity criterion between the original and the reconstructed images . </S>",
    "<S> the proposed metric called convex similarity ( csim ) index is a modified version of the structural similarity ( ssim ) index which despite its predecessor , is convex and uni - modal . </S>",
    "<S> we also propose an iterative sparse recovery method based on a constrained @xmath1-norm minimization problem involving csim as the fidelity criterion . </S>",
    "<S> this optimization problem which is adopted for missing sample recovery of images is efficiently solved via an algorithm based on alternating direction method of multipliers ( admm ) . </S>",
    "<S> simulation results show the performance of the new similarity index as well as the proposed algorithm for missing sample recovery of test images .    missing sample , sparse recovery , admm , similarity index , image inpainting . </S>"
  ]
}