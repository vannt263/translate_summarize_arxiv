{
  "article_text": [
    "if a password , @xmath0 , is chosen at random from a finite set @xmath1 , how hard is it to guess @xmath0 ? if @xmath2 is known , then an optimal strategy is to guess passwords in decreasing order of probability .",
    "let @xmath3 denote the number of attempts required before correctly guessing @xmath4 , called @xmath5 s guesswork .",
    "massey @xcite proved that the shannon entropy of @xmath0 is a lower bound on the expected guesswork , @xmath6 , and that no general upper bound exists .",
    "this raised serious questions about the appropriateness of shannon entropy as a measure of complexity of a distribution with regards guesswork .",
    "as a corollary to stronger results , in this article we identify a large password relationship between the expectation of the logarithm of the guesswork and specific shannon entropy .",
    "arikan @xcite introduced an asymptotic regime for studying this problem by considering a sequence of passwords , @xmath7 , with @xmath8 chosen from @xmath9 with i.i.d .",
    "letters . again guessing potential passwords in decreasing order of probability for each @xmath10",
    ", he related the asymptotic fractional moments of the guesswork to the rnyi entropy of a single letter , @xmath11 for @xmath12 , where the right hand side is @xmath13 times the rnyi entropy of @xmath14 evaluated at @xmath15 .",
    "this result was subsequently extended by malone and sullivan @xcite to word sequences with letters chosen by a markov process and , further still , by pfister and sullivan @xcite to sophic shifts whose shift space satisfies an entropy condition and whose marginals possess a limit property .",
    "recently , using a distinct approach hanawal and sundaresan @xcite provided alternate sufficient conditions for the existence of the limit . in all cases ,",
    "the limit is identified in terms of the specific rnyi entropy @xmath16 where @xmath17 is the rnyi entropy of @xmath8 @xmath18    here we shall assume the existence of the limit on the left hand side of equation for all @xmath19 , its equality with @xmath13 times specific rnyi entropy , its differentiability with respect to @xmath13 in that range and a regularity condition on the probability of the most - likely word , that @xmath20 exists . from this , theorem [ thm : main ] deduces that the sequence @xmath21 satisfies a large deviation principle ( ldp ) ( e.g. @xcite ) with a rate function @xmath22 that must possess a specific form that will have a physical interpretation : @xmath22 is continuous where finite , can be linear on an interval @xmath23 $ ] , for some @xmath24 $ ] , and then must be strictly convex while finite on @xmath25 $ ] .",
    "in contrast to earlier results , corollary [ cor : curious ] to the ldp gives direct estimates on the guesswork distribution @xmath26 for large @xmath10 , suggesting the approximation @xmath27 as this calculation only involves the determination of @xmath22 , to approximately calculate the probability of the @xmath28 most likely word in words of length @xmath10 one does not have to identify the word itself , which would be computationally cumbersome , particularly for non - i.i.d .",
    "word sources .",
    "corollary [ cor : shannon ] to the ldp recovers a rle for shannon entropy in the asymptotic analysis of guesswork .",
    "it shows that the scaled expectation of the logarithm of the guesswork converges to specific shannon entropy @xmath29 where @xmath30",
    "consider the sequence of random variables @xmath21 . our starting point is the observation that the left hand side of is the scaled cumulant generating function ( scgf ) of this sequence :",
    "@xmath31 which is shown to exist for @xmath12 in @xcite@xcite and for @xmath19 in @xcite .    [ ass:1 ] for @xmath19 , the scgf @xmath32 exists , is equal to @xmath13 times the specific rnyi entropy , and has a continuous derivative in that range .",
    "we also assume the following regularity condition on the probability of the most likely word .",
    "[ ass:2 ] the limit @xmath33 exists in @xmath34 $ ] .",
    "this assumption is transparently true for words constructed of i.i.d . or markovian letters .",
    "we first show that the scgf exists everywhere .",
    "[ lem : flatscgf ] under assumptions [ ass:1 ] and [ ass:2 ] , for all @xmath35 @xmath36    let @xmath35 and note that @xmath37 taking @xmath38 with the first inequality and @xmath39 with the second while using the principle of the largest term , ( * ? ? ?",
    "* lemma 1.2.15 ) and usual estimates on the harmonic series , we have that @xmath40 for all @xmath35 .",
    "as @xmath41 is the limit of a sequence of convex functions and is finite everywhere , it is continuous and therefore @xmath42 .",
    "thus the scgf @xmath41 exists and is finite for all @xmath13 , with a potential discontinuity in its derivative at @xmath43 .",
    "this discontinuity , when it exists , will have a bearing on the nature of the rate function governing the ldp for @xmath21 . indeed , the following quantity will play a significant rle in our results : @xmath44 we will prove that the number of words with approximately equal highest probability is close to @xmath45 . in the special case where the @xmath7 are constructed of i.i.d .",
    "letters , this is exactly true and the veracity of the following lemma can be verified directly .",
    "[ lem : iid ] if @xmath7 are constructed of i.i.d .",
    "letters , then @xmath46 where @xmath47 indicates the number of elements in the set .",
    "this i.i.d .",
    "result does nt extend directly to the non - i.i.d .",
    "case and in general lemma [ lem : iid ] can only be used to establish a lower bound on @xmath48 : @xmath49 e.g ( * ? ? ?",
    "* theorem 24.5 ) .",
    "this lower bound can be loose , as can be seen with the following example .",
    "consider the sequence of distributions for some @xmath50 @xmath51 for each fixed @xmath10 there is one most likely word and we have @xmath52 on the right hand side of equation by lemma [ lem : iid ] .",
    "the left hand side , however , gives @xmath53 .",
    "regardless , this intuition guides our understanding of @xmath48 , but the formal statement of it approximately capturing the number of most likely words will transpire to be @xmath54 where @xmath55 is defined in equation .",
    "we define the candidate rate function as the legendre - fenchel transform of the scgf @xmath56\\\\",
    "\\sup_{\\alpha\\in\\r } \\{x\\alpha -\\lambda(\\alpha)\\ }           & \\text { if } x\\in(\\turn,\\log(m ) ] .",
    "\\end{cases}\\end{aligned}\\ ] ] the ldp can not be proved directly by baldi s version of the grtner - ellis theorem @xcite ( * ? ? ?",
    "* theorem 4.5.20 ) as @xmath22 does not have exposing hyper - planes for @xmath57 $ ] .",
    "instead we use a combination of that theorem with the methodology described in detail in @xcite where , as our random variables are bounded @xmath58 , in order to prove the ldp it suffices to show that the following exist in @xmath59 $ ] for all @xmath60 $ ] and equals @xmath61 : @xmath62 where @xmath63 .",
    "[ thm : main ] under assumptions [ ass:1 ] and [ ass:2 ] , the sequence @xmath21 satisfies a ldp with rate function @xmath22 .    to establish we have separate arguments depending on @xmath64 .",
    "we divide @xmath65 $ ] into two parts : @xmath66 $ ] and @xmath67 $ ] .",
    "baldi s upper bound holds for any @xmath68 $ ] .",
    "baldi s lower bound applies for any @xmath69 $ ] as @xmath22 is continuous and , as @xmath32 has a continuous derivative for @xmath19 , it only has a finite number of points without exposing hyper - planes in that region . for @xmath70 $ ] , however , we need an alternate lower bound .",
    "consider @xmath71 $ ] and define the sets @xmath72 letting @xmath73 denote the number of elements in each set .",
    "we have the bound @xmath74 as @xmath75 by baldi s upper bound , we have that @xmath76 thus to complete the argument , for the complementary lower bound we need to show that for any @xmath57 $ ] @xmath77 if @xmath78 for some @xmath79 , then for @xmath50 sufficiently small let @xmath80 be such that @xmath81 and @xmath82 . then by baldi s lower bound , which applies as @xmath83 $ ]",
    ", we have @xmath84 now @xmath85 where in the last line we have used the monotonicity of guesswork and the fact that @xmath86 . taking lower limits and using equation with @xmath87",
    ", we have that @xmath88 for all such @xmath89 . taking limits as @xmath90 and then limits as @xmath91 we have @xmath92 but @xmath93 so that @xmath94 as required .",
    "only one case remains : if @xmath95 for all @xmath79 , then we require an alternative argument to ensure that @xmath96 this situation happens if , in the limit , the distribution of words is near uniform on the set of all words with positive probability .",
    "thus define @xmath97 as @xmath95 for all @xmath79 , @xmath98 .",
    "to see @xmath99 , note that @xmath100 .",
    "as both @xmath32 and @xmath101 are finite and differentiable in a neighborhood of @xmath102 , by ( * ? ? ?",
    "* theorem 25.7 ) @xmath103 and @xmath104 .",
    "thus @xmath99 and , due to convexity , @xmath41 is linear with slope @xmath105 on @xmath106 $ ] .",
    "as @xmath107 , using lemma [ lem : flatscgf ] we have that @xmath108 .",
    "let @xmath109 and consider @xmath110 we shall assume that @xmath111 and show this results in a contradiction .",
    "let @xmath112 , then there exists @xmath113 such that @xmath114 for all @xmath115 , but this is strictly less than @xmath116 for @xmath10 sufficiently large and thus @xmath117 . finally , for @xmath118 , and @xmath50",
    ", note that we can decompose @xmath65 $ ] into three parts , @xmath119\\cup(\\lm-\\epsilon,\\lm+\\epsilon)\\cup[\\lm+\\epsilon,\\log(m)]$ ] , where the scaled probability of the guesswork being in either the first or last set is decaying , but @xmath120\\right)\\end{aligned}\\ ] ] and so the result follows from an application of the principle of the largest term .",
    "thus for any @xmath68 $ ] , @xmath121 and the ldp is proved .    in establishing the ldp ,",
    "we have shown that any rate function that governs such an ldp must have the form of a straight line in @xmath122 $ ] followed by a strictly convex function .",
    "the initial straight line comes from all words that are , in an asymptotic sense , of greatest likelihood .",
    "while the ldp is for the sequence @xmath21 , it can be used to develop the more valuable direct estimate of the distribution of each @xmath123 found in equation .",
    "the next corollary provides a rigorous statement , but an intuitive , non - rigorous argument for understanding the result therein is that from the ldp we have the approximation that for large @xmath10 @xmath124 as for large @xmath10 the distribution of @xmath125 and @xmath126 are ever closer to having densities , using the change of variables formula gives @xmath127 finally , the substitution @xmath128 gives the approximation in equation . to make this heuristic precise requires distinct means , explained in the following corollary .",
    "[ cor : curious ] recall the definition @xmath129 for any @xmath68 $ ] we have @xmath130    we show how to prove the upper bound as the lower bound follows using analogous arguments , as do the edge cases .",
    "let @xmath131 and @xmath50 be given .",
    "using the monotonicity of guesswork @xmath132 using the estimate found in theorem [ thm : main ] and the ldp provides an upper bound on the latter : @xmath133\\right)\\\\ & \\leq -\\inf_{x\\in [ x-3\\epsilon , x-\\epsilon ] } \\lambda^*(x).\\end{aligned}\\ ] ] thus @xmath134 } \\lambda^*(x).\\end{aligned}\\ ] ] thus the upper - bound follows taking @xmath90 and using the continuity when finite of @xmath22 .",
    "unpeeling limits , this corollary shows that when @xmath10 is large the probability of the @xmath28 most likely word is approximately @xmath135 , without the need to identify the word itself .",
    "this justifies the approximation in equation , whose complexity of evaluation does not depend on @xmath10 .",
    "we demonstrate its merit by example in section [ sec : examples ] .    before that , as a corollary to the ldp we find the following rle for the specific shannon entropy .",
    "thus , although massey established that for a given word length the shannon entropy is only a lower bound on the guesswork , for growing password length the specific shannon entropy determines the linear growth rate of the expectation of the logarithm of guesswork .",
    "[ cor : shannon ] under assumptions [ ass:1 ] and [ ass:2 ] , @xmath136 the specific shannon entropy .",
    "note that @xmath137 if and only if @xmath138 , by arguments found in the proof of theorem [ thm : main ] .",
    "the weak law then follows by concentration of measure , e.g. @xcite .",
    "i.i.d letters_.      .",
    "words constructed from i.i.d letters with @xmath139 . for @xmath144 and @xmath145 , comparison of @xmath146 times the logarithm of the probability of @xmath28 most likely word versus @xmath146 times the logarithm of @xmath147 , as well as the approximation @xmath148 versus @xmath64 . ]",
    "assume words are constructed of i.i.d .",
    "let @xmath14 take values in @xmath1 and assume @xmath149 if @xmath150 . then from @xcite and lemma [ lem : flatscgf ] we have that @xmath151 from lemma [ lem : iid ] we have that @xmath152 and no other values are possible .",
    "unless the distribution of @xmath14 is uniform , @xmath153 does not have a closed form for all @xmath64 , but is readily calculated numerically . with @xmath154 and @xmath140 , figure [ fig : fest ] compares the exact distribution @xmath155 versus @xmath3 with the approximation found in equation .",
    "as there are @xmath156 million words , the likelihood of any one word is tiny , but the quality of the approximation can clearly be seen . rescaling the guesswork and probabilities",
    "to make them comparable for distinct @xmath10 , figure [ fig : fconv ] illustrates the quality of the approximation as @xmath10 grows . by @xmath157",
    "there are @xmath158 times @xmath159 words and the underlying combinatorial complexities of the explicit calculation become immense , yet the complexity of calculating the approximation has not increased .        as an example of words constructed of correlated letters ,",
    "consider @xmath7 where the letters are chosen via a process a markov chain with transition matrix @xmath164 and some initial distribution on @xmath160 .",
    "define the matrix @xmath165 by @xmath166 , then by @xcite and lemma [ lem : flatscgf ] we have that @xmath167 where @xmath168 is the spectral radius operator . in the two letter alphabet case , with @xmath169 we have",
    "that @xmath170 equals @xmath171 as with the i.i.d .",
    "letters example , apart from in special cases , the rate function @xmath22 can not be calculated in closed form , but is readily evaluated numerically .",
    "regardless , we have the following , perhaps surprising , result on the exponential rate of growth of the size of the set of almost most likely words .",
    "this lemma can be proved by directly evaluating the derivative of @xmath32 with respect to @xmath13 .",
    "note that here @xmath174 definitely only describes the number of words of equal highest likelihood when @xmath10 is large as the initial distribution of the markov chain plays no rle in @xmath48 s evaluation .",
    "the case where @xmath175 occurs when @xmath176 .",
    "the most interesting case is when there are approximately @xmath177 approximately equally most likely words .",
    "this occurs if @xmath178 . for large @xmath10 , words of near - maximal probability",
    "have the form of a sequence of 1s , where a 2 can be inserted anywhere so long as there is a 1 between it and any other 2s .",
    "a further sub - exponential number of aberrations are allowed in any given sequence .",
    "for example , with an equiprobable initial distribution and @xmath179 there are @xmath180 most likely words ( 1111 , 1112 , 1121 , 1211 , 1212 , 2111 , 2121 , 2112 ) and @xmath181 .    figure [ fig : markrf ] gives plots of @xmath153 versus @xmath64 illustrating the full range of possible shapes that rate functions can take : linear , linear then strictly convex , or strictly convex , based on the transition matrices @xmath182 respectively ."
  ],
  "abstract_text": [
    "<S> how hard is it guess a password ? </S>",
    "<S> massey showed that the shannon entropy of the distribution from which the password is selected is a lower bound on the expected number of guesses , but one which is not tight in general . in a series of subsequent papers under ever less restrictive stochastic assumptions , an asymptotic relationship as password length grows between scaled moments of the guesswork and specific rnyi entropy </S>",
    "<S> was identified .    here </S>",
    "<S> we show that , when appropriately scaled , as the password length grows the logarithm of the guesswork satisfies a large deviation principle ( ldp ) , providing direct estimates of the guesswork distribution when passwords are long . </S>",
    "<S> the rate function governing the ldp possess a specific , restrictive form that encapsulates underlying structure in the nature of guesswork . returning to massey </S>",
    "<S> s original observation , a corollary to the ldp shows that expectation of the logarithm of the guesswork is the specific shannon entropy of the password selection process .    </S>",
    "<S> guesswork , rnyi entropy , shannon entropy , large deviations </S>"
  ]
}