{
  "article_text": [
    "the barrett - crane model of four - dimensional riemannian quantum gravity  @xcite has been of significant interest recently  @xcite .",
    "the model is discrete and well - defined , and the partition function for the perez - rovelli version has been rigorously shown to converge  @xcite for a fixed triangulation of spacetime .",
    "the riemannian model serves as a step along the way to understanding the less tractable but physically more realistic lorentzian version  @xcite .",
    "however , despite its simplicity , we are currently lacking explicit numerical computations of the partition function and of expectation values of observables in the riemannian model .",
    "these are necessary to test its large - scale behaviour and other physical properties .",
    "it has been shown  @xcite that the amplitudes in the barrett - crane model are always non - negative , and therefore that the expectation values of observables can be approximated using the metropolis algorithm .",
    "this greatly reduces the number of samples that must be taken , and thus the remaining obstacle is the time required to compute each sample .",
    "this paper presents a very efficient algorithm for doing these computations .",
    "the algorithm is used in  @xcite and  @xcite to understand the asymptotic behaviour of the @xmath0 symbols and the dependence of the partition function on a cutoff .    to explain further",
    ", we need to describe the barrett - crane model in more detail .",
    "it has been formulated by baez  @xcite as a discrete spin foam model , in which faces in the dual 2-skeleton of a fixed triangulation of spacetime are labeled by spins .",
    "the dual 2-skeleton consists of a dual vertex at the center of each 4-simplex of the triangulation , five dual edges incident to each dual vertex ( one for each tetrahedron in the boundary of the 4-simplex ) , and ten dual faces incident to each dual vertex ( one for each triangle in the boundary of the 4-simplex ) .",
    "baez notes that the partition function for this model is the sum , over all labelings of the dual faces by spins , of an expression that contains the product of a @xmath0 symbol for each dual vertex .",
    "a @xmath0 symbol , described in detail in section  [ se:10j ] , is a @xmath7spin network . roughly speaking ,",
    "a spin network is a graph whose vertices are labelled by tensors , and whose edges indicate how to contract these tensors .",
    "a spin network evaluates to a complex number in the way explained in section  [ se : elementary ] . in short ,",
    "the @xmath0 symbol is a function taking ten input spins and producing a complex number .",
    "it is at the heart of the calculation of the partition function , and thus an algorithm for calculating the @xmath0 symbols efficiently is quite important .    in section  [ se:10j ]",
    "we recall the definition of the @xmath0 symbol using spin networks .",
    "then in section  [ se : elementary ] we briefly describe the elementary algorithms for evaluating these spin networks , and give their running times and memory use .",
    "we conclude with section  [ se : ours ] , which presents our algorithms and their time and space needs .",
    "in the dual 2-skeleton of a triangulation of a 4-manifold , each dual vertex belongs to five dual edges , and each pair of these dual edges borders a dual face .",
    "a @xmath0 symbol is a @xmath7spin network with five vertices ( corresponding to the five dual edges ) and ten edges , one connecting each pair of vertices ( corresponding to the ten dual faces ) , with the edges labeled by spins . in the context of a @xmath7spin network ,",
    "a spin @xmath3 labeling an edge denotes the representation @xmath8 of @xmath9 , where @xmath3 is the spin-@xmath3 representation of @xmath10 .",
    "such representations are called `` balanced . ''",
    "we use the convention that spins are non - negative half - integers .",
    "here is a picture of the @xmath0 symbol , with the vertices numbered 0 through 4 , and the spins divided into two groups : @xmath11 are the spins on the edges joining vertex @xmath12 to vertex @xmath13 ( modulo 5 ) , and @xmath14 are the spins on the edges joining vertex @xmath12 to vertex @xmath15 ( modulo 5 ) . @xmath16",
    "the five vertices of the network are equal to barrett - crane intertwiners .",
    "these are the unique intertwiners ( up to a factor ) between four balanced representations of @xmath7with the property that their expansion as a sum of tensor products of trivalent @xmath10networks only contains balanced representations on the internal edge , regardless of which pairs of external edges are joined  @xcite .",
    "barrett and crane give the formula for these intertwiners in  @xcite : @xmath17 [ dr ]     -@-^>>{j_2 } [ ur ] [ dl ]      -@-^>>{j_3 } [ dl ] [ ur ]      -@-_>>{j_4 } [ dr ] [ ul ] } \\end{xy } } \\ , : = \\ , \\sum_l \\",
    ", \\delta_l \\ , { \\begin{xy }   \\xygraph{!{<1.6pc,0pc>:<0pc,1.8pc>::0 }     [ u(0.7 ) ] * { \\bullet }     -@-_>>{j_1 } [ ul ] [ dr ]     -@-^>>{j_2 } [ ur ] [ dl ]      -@-^{l } [ d(1.4 ) ] * { \\bullet }     -@-^>>{j_3 } [ dl ] [ ur ]      -@-_>>{j_4 } [ dr ] [ ul ] } \\end{xy } } \\quad { \\begin{xy }   \\xygraph{!{<1.6pc,0pc>:<0pc,1.8pc>::0 }     [ u(0.7 ) ] * { \\bullet }     -@-_>>{j_1 } [ ul ] [ dr ]     -@-^>>{j_2 } [ ur ] [ dl ]      -@-^{l } [ d(1.4 ) ] * { \\bullet }     -@-^>>{j_3 } [ dl ] [ ur ]      -@-_>>{j_4 } [ dr ] [ ul ] } \\end{xy } } \\ ] ] here the sum is over all admissible values of @xmath18 , i.e.  those that satisfy the clebsch - gordan condition for both @xmath10vertices .",
    "so @xmath18 ranges from @xmath19 to @xmath20 in integer steps .",
    "if the difference between these bounds is not an integer , the @xmath7vertex will be zero .",
    "when @xmath18 satisfies these conditions , there is a unique intertwiner up to normalization which can be used to label the trivalent @xmath10vertices .",
    "( these intertwiners are normalized so that the theta network in the numerator of equation  ( [ e : phi ] ) has value 1 . )",
    "@xmath21 is the value of a loop in the spin-@xmath18 representation , which is just @xmath22 , the superdimension of the representation .",
    "the uniqueness result of reisenberger  @xcite tells us that if we replace the vertical edges in the above definition by horizontal edges , the result differs at most by a constant factor .",
    "in fact , barrett and crane  @xcite stated that the two definitions give exactly the same @xmath7vertex , and yetter  @xcite has proved this .",
    "any closed spin network evaluates to a complex number , by contracting the tensors at the vertices according to the pairings specified by the edges .",
    "thus the @xmath0 symbol is a complex number .",
    "( in fact , one can show that it is always a real number . )    to avoid confusion , we want to make it clear that we are working with the `` classical '' ( non @xmath23-deformed ) evaluation of our spin networks .",
    "we will frequently reference the book  @xcite by kauffman and lins ; while it explicitly discusses the @xmath23-deformed version , the formulas we use apply to the classical evaluation as well .",
    "to set the context , we begin by explaining some elementary algorithms for computing the @xmath0 symbol .",
    "these algorithms all share the feature that they evaluate a spin network by choosing bases for the representations labelling the edges , computing the components of the tensors representing the intertwiners , and computing the contraction of the tensors in some way .",
    "they make no use of special features of these tensors , except for the vanishing property mentioned below .    the first three methods each have two versions , one which works directly with the @xmath7network  ( [ e:10j ] ) , and one that converts it into a five - fold sum over @xmath10networks , by expanding each barrett - crane intertwiner : @xmath24 here , @xmath25 is the spin labeling the new edge introduced by the expansion of the intertwiner at vertex @xmath12 , and it ranges in integer steps from @xmath26 to @xmath27 , where the vertex numbers are all to be interpreted modulo 5 .",
    "( if @xmath28 is not a non - negative integer , then the sum over @xmath29 is empty and the @xmath0 symbol is zero .",
    "in fact , if vertex @xmath12 is non - zero , then @xmath30 , @xmath31 , @xmath32 and @xmath33 must all differ by integers .",
    ") there are at most @xmath34 terms in each sum , where @xmath3 is the average of the ten spins .",
    "since the two decagonal networks are the same , one only needs to evaluate one of them and square the answer .",
    "the first elementary method is one we call * direct contraction*. one simply labels each edge in the spin network with a basis vector from the representation labelling the edge , and multiplies together the corresponding components of the tensors .",
    "then this is summed up over all labellings .",
    "in fact , one can restrict to a smaller set of labellings : the bases can be chosen so that for each choice of two basis vectors on two of the three edges meeting an @xmath10vertex , there is at most one choice of basis vector on the third edge giving a non - zero tensor component .",
    "the @xmath7vertices also have the property that the bases can be chosen so that when three of the basis vectors adjacent to a vertex are specified , the last one is determined .",
    "the second elementary method is * staged contraction*. in the @xmath7version of this method , one starts with the tensor at vertex 0 , contracts with the tensor at vertex 1 , and then vertex 4 , and then vertex 2 , and finally vertex 3 , again taking care to save space and time by using the vanishing properties of the tensors .",
    "similarly , one can iteratively contract the tensors in the decagonal @xmath10network . at intermediate stages",
    "one is storing tensors with a large number of components .",
    "the third elementary method is * 3cut*. here one takes a ray from the center of  ( [ e:10j ] ) and cuts the three edges it crosses .",
    "then one takes the trace of the operator this defines on the three - fold tensor product . in more detail ,",
    "one sums over basis vectors for the factors in this tensor product , computing the effect of the network on these basis vectors , and using the vanishing properties .",
    "the memory required for this method ( and the next ) is dominated by the memory needed to store the tensors themselves .",
    "the fourth and final elementary method is * 2cut*. this one only makes sense for the decagonal network , since one proceeds by taking a ray from the center of the decagon which crosses just two edges , cutting those two edges , and taking the trace of the resulting operator , using the vanishing properties .",
    "here is a table which gives an upper bound on the number of operations ( additions and multiplications ) that these algorithms use , and the amount of memory they require , as a function of a typical spin @xmath3 .",
    "the space requirements include the space to store the barrett - crane tensors .    [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]     in the last two rows , we represent the entries in a uniform way for either version by writing @xmath35 for the @xmath7version of each algorithm and @xmath36 for the @xmath10version",
    ". then we let @xmath37 and @xmath38 .",
    "this shows how they are related .",
    "for example , the @xmath10methods always get a factor of @xmath39 in time from the five loops coming from expanding the barrett - crane vertices . also , the @xmath7methods get powers of @xmath40 ( because @xmath41 ) , while the @xmath10methods get powers of @xmath3 ( because @xmath42 ) .",
    "the 2cut method has the best worst - case behaviour , in both space and time .    in the next section we present an algorithm which has running time @xmath43 and requires @xmath5 space , and give variants with running time @xmath44 and @xmath45 and which use a constant amount of space .",
    "the difference between @xmath39 and the running time , @xmath46 , for the best of the elementary methods is significant .",
    "for example , with all spins equal to 20 , our @xmath39 algorithm runs in under six minutes on a 300 mhz microprocessor .",
    "a back of the envelope calculation suggests that this would take about 30 years with a @xmath46 algorithm .",
    "in this section we describe a new method for computing @xmath0 symbols .",
    "the key feature of this method is that it does not proceed by computing the tensor components for the intertwiners .",
    "instead it uses recoupling to simplify the network to one that can be evaluated directly .",
    "thus this method makes use of special properties of the tensors that occur in the @xmath0 symbols .",
    "there are three versions of this method .",
    "we explain one of these in some detail , and briefly describe the variants at the appropriate points .",
    "we use the notation of equation  ( [ e:10j ] ) , and consider the expansion  ( [ e : decagonal ] ) as a sum of squares of decagonal networks .",
    "the decagonal networks can be deformed into the `` ladders '' shown below , where the vertices at the bottom of each ladder are to be identified with those at the top of the same ladder .",
    "any sign introduced by this deformation is produced twice and so can be ignored .",
    "@xmath47 to simplify these networks further , we recouple the sub - networks consisting of a horizontal edge and half of each vertical edge incident to its endpoints , rewriting them as sums of sub - networks with the same external edges , using the following recoupling formula for @xmath48 spin networks  ( * ? ? ?",
    "* ch .  7 ) : @xmath49 * { \\bullet }     -@-_>>{d } [ ul ] [ dr ]     -@-^>>{c } [ ur ] [ dl ]      -@-_{j } [ d(1.4 ) ] * { \\bullet }     -@-^>>{a } [ dl ] [ ur ]      -@-_>>{b } [ dr ] [ ul ] } \\end{xy } } = \\sum_{k } \\,\\ , { \\ensuremath{\\left\\{\\begin{matrix}a & b & k \\cr        c & d & j\\end{matrix}\\right\\ } } } { \\begin{xy }   \\xygraph{!{<1.6pc,0pc>:<0pc,1.8pc>::0 }     [ u(0.7 ) ] * { \\bullet }     -@-_>>{b } [ ul ] [ dr ]     -@-^>>{c } [ ur ] [ dl ]      -@-^{k } [ d(1.4 ) ] * { \\bullet }     -@-^>>{a } [ dl ] [ ur ]      -@-_>>{d } [ dr ] [ ul ] } \\end{xy } } \\ ] ] the horizontal and vertical networks appearing above are two different ways of writing intertwiners from one two - fold tensor product of irreducible representations to another ; both kinds of networks form bases for the space of intertwiners , and the @xmath50 symbols appearing in the formula are defined to be the change - of - basis coefficients .    at first glance , it might look as if this recoupling would introduce sums over ten new spins , labeling the ten new vertical edges .",
    "however , the total networks that result from the recoupling consist of chains of sub - networks shaped like @xmath51 * { \\bullet }     -@-_>>{a } [ u ] [ d ]     -@/_.6pc/_{b } [ dd ] [ uu ]     -@/^.6pc/^{c } [ dd ] * { \\bullet }     -@-^>>{d } [ d ] [ u ] } \\end{xy } } \\ ] ] by schur s lemma , such sub - networks will only be non - zero when the incoming and outgoing edges have identical spins .",
    "so the recoupled networks can be written as a sum over just two new spins , @xmath52 and @xmath53 .",
    "@xmath54 this sum is over all values such that every vertex in the diagram satisfies the clebsch - gordan condition , so both @xmath52 and @xmath53 will independently range in integer steps from @xmath55 to @xmath56 .",
    "here we use the fact that if the five barrett - crane vertices are non - zero , then as @xmath12 varies , the ten quantities @xmath57 and @xmath58 all differ by integers",
    ". indeed , @xmath59 modulo integers , and by the paragraph after equation  ( [ e : decagonal ] ) , @xmath60 modulo integers , where in the third step we use that vertex @xmath13 is non - zero .    at this point",
    "there is a choice which determines which version of the algorithm one obtains .",
    "if the sum over @xmath61 and @xmath62 is left inside the sum over the @xmath29 , then it can be written as the square of a sum over a single @xmath63 .",
    "as described below , each of the terms in this sum can be computed with @xmath34 operations and a constant amount of memory , where @xmath3 is the average of the ten spins .",
    "thus , this method produces an algorithm that runs in @xmath45 time and takes a constant amount of space .",
    "in general it turns out to be more efficient to make the sum over the @xmath63 s outermost , in order to reinterpret the sum over the @xmath25 as the trace of a matrix product . the range for @xmath52 and @xmath53 must encompass all potentially admissible values , and the range for each @xmath25 can then be adjusted for the current values of @xmath52 and @xmath53 .",
    "the original range for @xmath25 was from @xmath64 to @xmath65 ( see the paragraph after equation  ( [ e : decagonal ] ) ) , so the @xmath63 s can never be greater than @xmath66 without violating the triangle inequality at one of the vertices .",
    "the lower bound is given by @xmath67 , where the minimum breaks down into three cases : if @xmath68 , it is @xmath69 ; if @xmath70 , it is @xmath71 ; and if @xmath72 , it is either 0 or @xmath73 , depending on whether @xmath74 , or not .      by schur",
    "s lemma , each sub - network of the form  ( [ e : phi1 ] ) is equal to a multiple of the identity , so each of the recoupled ladders , with top and bottom edges joined , is simply a multiple of a loop .",
    "kauffman and lins  @xcite give the following formula , which can be checked by taking the trace of both sides : @xmath77 * { \\bullet }     -@-_>>{a } [ u ] [ d ]     -@/_.6pc/_{b } [ dd ] [ uu ]     -@/^.6pc/^{c } [ dd ] * { \\bullet }     -@-^>>{a } [ d ] [ u ] } \\end{xy } } = \\frac { \\begin{xy } \\xygraph{!{<0.8pc,0pc > : }     [ r ] * { \\bullet }     -@-_{b } [ ll ] [ rr ]     -@/_1.1pc/_{a } [ ll ] [ rr ]     -@/^1pc/^{c } [ ll ]      * { \\bullet } } \\end{xy } } { \\quad    \\begin{xy } * { \\xycircle<-.8pc,-.8pc > { } } , ( 5,-2 ) * { a }   \\end{xy } }   \\quad \\begin{xy } \\xygraph{!{<2pc,0pc > : }     [ u ]     -@-^{a } [ dd ] } \\end{xy}\\ ] ] the numerator of the fraction is written @xmath78 , and the denominator is @xmath79 , the superdimension of the representation .",
    "kauffman and lins also give a formula for the @xmath50 symbol in terms of tetrahedral and @xmath80 networks : @xmath81 } } \\delta_i }   { \\theta(a , d , i)\\,\\theta(b , c , i)}\\ ] ] equations  ( [ e : phi ] ) and  ( [ e : sixj ] ) allow us to write the following expression for the @xmath0 symbol : @xmath82 where @xmath83 } } \\ ,   { \\ensuremath{\\operatorname{tet}\\left[\\begin{matrix } l_k & j_{2,k } & m_2 \\cr l_{k+1 } & j_{2,k-1 } & j_{1,k}\\end{matrix}\\right ] } } }   { \\theta(j_{2,k},l_{k+1},m_1)\\,\\theta(j_{2,k},l_{k+1},m_2)}\\ ] ] the twists implicit in the identification of the top and bottom parts of each network introduce signs of @xmath84 and @xmath85 ; since @xmath86 , the product is @xmath87 .",
    "the loop values for the spin-@xmath52 and spin-@xmath53 representations have signs of @xmath88 and @xmath89 , but since @xmath90 , the product of these two signs is always unity .",
    "we have made use of the symmetries of the tetrahedral networks to put the coefficients in a uniform order for all terms .",
    "the sum over the @xmath25 in equation  ( [ e : tenj ] ) is the trace of the product of the five matrices @xmath91 . for each pair of values of @xmath52 and @xmath53",
    ", these matrices can be computed using closed formulas for the tetrahedral and @xmath80 networks given by kauffman and lins in  @xcite .",
    "the formula for the tetrahedral networks involves a sum with @xmath34 terms , so computing each matrix requires @xmath92 operations terms in the formula for the tetrahedral network contains factorials , which themselves require @xmath34 operations .",
    "however , with some care , the formula can be evaluated with a total of @xmath34 operations . in practice ,",
    "we precalculate the factorials , using @xmath34 space . ] .",
    "the trace of the matrix product can also be found in @xmath92 steps .",
    "there are two factors of @xmath3 coming from the sums over @xmath52 and @xmath53 , yielding an overall count of @xmath4 operations .",
    "this method requires @xmath5 space to store the matrices .    for some 10-tuples of spins ,",
    "if all the spins are multiplied by @xmath93 , the time required will scale at a lower power than @xmath94 .",
    "multiplying all the spins by a factor will increase the upper and lower bounds of all the sums linearly , but in cases where the two bounds are equal , the sum will consist of a single term , regardless of the scaling factor .",
    "when many of the upper and lower bounds coincide , the first variant of the algorithm , with worst case running time @xmath45 , in fact becomes faster than the @xmath43 version .",
    "thus one may wish to use the first variant for certain @xmath0 symbols .    for large spins",
    ", the memory usage can be a problem .",
    "for example , with spins of around 180 , storing each matrix @xmath91 requires about 1 gigabyte . in this case",
    ", one can recalculate the matrix entries as needed , resulting in @xmath44 time and @xmath95 space ( @xmath96 if factorials are cached ) .",
    "the formulas in  @xcite for the network evaluations are unnormalized .",
    "to normalize all the @xmath10intertwiners according to the convention that any @xmath80 network has a value of 1which is the convention used in the formula for the barrett - crane intertwiner  it is simpler to divide the matrix elements by the appropriate @xmath80 networks than to take the existing @xmath80 networks in equation  ( [ e : matrix ] ) to be unity and normalize the tetrahedral networks .",
    "including this normalization , the matrices become : @xmath97      we have not dealt explicitly with the @xmath23-deformed case , where the representations of @xmath10are replaced with representations of @xmath98 , but it is straightforward to adapt each stage of the development above , using the formulas in  @xcite for the @xmath23-deformed twist , loop , @xmath80 and tetrahedral networks .",
    "baez , an introduction to spin foam models of quantum gravity and bf theory , in _ geometry and quantum physics _ , edited by helmut gausterer and harald grosse , springer , berlin , 2000 .",
    "preprint available as gr - qc/9905087 ."
  ],
  "abstract_text": [
    "<S> the @xmath0 symbol is a spin network that appears in the partition function for the barrett - crane model of riemannian quantum gravity . </S>",
    "<S> elementary methods of calculating the @xmath0 symbol require @xmath1 or more operations and @xmath2 or more space , where @xmath3 is the average spin . </S>",
    "<S> we present an algorithm that computes the @xmath0 symbol using @xmath4 operations and @xmath5 space , and a variant that uses @xmath6 operations and a constant amount of space . </S>",
    "<S> an implementation has been made available on the web . </S>"
  ]
}