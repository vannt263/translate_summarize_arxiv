{
  "article_text": [
    "when a high - energy physics experiment enters the phase of data collection and analysis , the daily tasks of its postgraduate students are often centred not around the particle physics theories one is trying to test but rather on statistical methods .",
    "these methods are the tools needed to compare data with theory and quantify the extent to which one stands in agreement with the other .",
    "of course one must understand the physical basis of the models being tested and so the theoretical emphasis in postgraduate education is no doubt well founded .",
    "but with the increasing cost of hep experiments it has become important to exploit as much of the information as possible in the hard - won data , and to quantify as accurately as possible the inferences one draws when confronting the data with model predictions .    despite efforts to make the lectures self contained ,",
    "some familiarity with basic ideas of statistical data analysis is assumed .",
    "introductions to the subject can be found , for example , in the reviews of the particle data group  @xcite or in the texts  @xcite .    in these two lectures",
    "we will discuss two topics that are becoming increasingly important : bayesian statistics and multivariate methods .",
    "in section  [ sec : bayes ] we will review briefly the concept of probability and see how this is used differently in the frequentist and bayesian approaches . then in section  [ sec : linefit ] we will discuss a simple example , the fitting of a straight line to a set of measurements , in both the frequentist and bayesian approaches and compare different aspects of the two",
    ". this will include in section  [ sec : mcmc ] a brief description of markov chain monte carlo ( mcmc ) , one of the most important tools in bayesian computation .",
    "we generalize the treatment in section  [ sec : fitsys ] to include systematic errors .    in section  [ sec : multivariate ] we take up the general problem of how to distinguish between two classes of events , say , signal and background , on the basis of a set of characteristics measured for each event .",
    "we first describe how to quantify the performance of a classification method in the framework of a statistical test . although the neyman ",
    "pearson lemma indicates that this problem has an optimal solution using the likelihood ratio , this usually can not be used in practice and one is forced to seek other methods . in section  [ sec : bdt ] we look at a specific example of such a method , the boosted decision tree .",
    "using this example we describe several issues common to many classification methods , such as overtraining .",
    "finally , some conclusions are mentioned in section  [ sec : summary ] .",
    "in this section we look at the basic ideas of bayesian statistics and explore how these can be applied in particle physics .",
    "we will contrast these with the corresponding notions in frequentist statistics , and to make the treatment largely self contained , the main ideas of the frequentist approach will be summarized as well .",
    "we begin by defining probability with the axioms written down by kolmogorov  @xcite using the language of set theory .",
    "consider a set @xmath0 containing subsets @xmath1 .",
    "we define the probability @xmath2 as a real - valued function with the following properties :    1 .   for every subset @xmath3 in @xmath0 , @xmath4 ; 2 .   for disjoint subsets ( i.e. , @xmath5 ) , @xmath6 ; 3 .",
    "@xmath7 .",
    "in addition , we define the conditional probability @xmath8 ( read @xmath2 of @xmath3 given @xmath9 ) as    @xmath10    from this definition and using the fact that @xmath11 and @xmath12 are the same , we obtain _ bayes theorem _ ,    @xmath13    from the three axioms of probability and the definition of conditional probability , we can derive the _ law of total probability _",
    ",    @xmath14    for any subset @xmath9 and for disjoint @xmath15 with @xmath16 .",
    "this can be combined with bayes theorem  ( [ eq : bayesthm ] ) to give    @xmath17    where the subset @xmath3 could , for example , be one of the @xmath15 .",
    "the most commonly used interpretation of the subsets of the sample space are outcomes of a repeatable experiment .",
    "the probability @xmath18 is assigned a value equal to the limiting frequency of occurrence of @xmath3 .",
    "this interpretation forms the basis of _ frequentist statistics_.    the subsets of the sample space can also be interpreted as _ hypotheses _ ,",
    "i.e. , statements that are either true or false , such as `` the mass of the @xmath19 boson lies between 80.3 and 80.5 gev . '' in the frequency interpretation , such statements are either always or never true , i.e. , the corresponding probabilities would be 0 or 1 . using _ subjective probability _",
    ", however , @xmath18 is interpreted as the degree of belief that the hypothesis @xmath3 is true .",
    "subjective probability is used in _ bayesian _ ( as opposed to frequentist ) statistics .",
    "bayes theorem can be written    @xmath20    where ` theory ' represents some hypothesis and ` data ' is the outcome of the experiment . here",
    "@xmath21 is the _ prior _ probability for the theory , which reflects the experimenter s degree of belief before carrying out the measurement , and @xmath22 is the probability to have gotten the data actually obtained , given the theory , which is also called the _",
    "likelihood_.    bayesian statistics provides no fundamental rule for obtaining the prior probability ; this is necessarily subjective and may depend on previous measurements , theoretical prejudices , once this has been specified , however , eq .",
    "( [ eq : bayesthm3 ] ) tells how the probability for the theory must be modified in the light of the new data to give the _ posterior _ probability , @xmath23 .",
    "( [ eq : bayesthm3 ] ) is stated as a proportionality , the probability must be normalized by summing ( or integrating ) over all possible hypotheses .",
    "the difficult and subjective nature of encoding personal knowledge into priors has led to what is called _ objective bayesian statistics _",
    ", where prior probabilities are based not on an actual degree of belief but rather derived from formal rules .",
    "these give , for example , priors which are invariant under a transformation of parameters or which result in a maximum gain in information for a given set of measurements .",
    "for an extensive review see , for example , ref .",
    "@xcite .      in section  [ sec : linefit ]",
    "we look at the example of a simple fit in both the frequentist and bayesian frameworks .",
    "suppose we have independent data values @xmath24 , @xmath25 , that are each made at a given value @xmath26 of a control variable @xmath27 .",
    "suppose we model the @xmath24 as following a gaussian distribution with given standard deviations @xmath28 and mean values @xmath29 given by a function that we evaluate at the corresponding @xmath26 ,    @xmath30    we would like to determine values of the parameters @xmath31 and @xmath32 such that the model best describes the data .",
    "the ingredients of the analysis are illustrated in fig .",
    "[ fig : linefit](a ) .",
    "( 10.0,6.5 ) ( 0.5,0 )    ( 8.5,0 )    ( 0.,5.5)(a ) ( 15.5,5.5)(b )    now suppose the real goal of the analysis is only to estimate the parameter @xmath31 .",
    "the slope parameter @xmath32 must also be included in the model to obtain a good description of the data , but we are not interested in its value as such .",
    "we refer to @xmath31 as the parameter of interest , and @xmath32 as a _ nuisance parameter_. in the following sections we treat this problem using both the frequentist and bayesian approaches .",
    "our model states that the measurements are gaussian distributed , i.e. , the probability density function ( pdf ) for the @xmath33th measurement @xmath24 is    @xmath34    where @xmath35 .",
    "the _ likelihood function _ is the joint pdf for all of the @xmath24 , evaluated with the @xmath24 obtained and regarded as a function of the parameters .",
    "since we are assuming that the measurements are independent , the likelihood function is in this case given by the product    @xmath36    in the frequentist approach we construct estimators @xmath37 for the parameters @xmath38 , usually by finding the values that maximize the likelihood function .",
    "( we will write estimators for parameters with hats . ) in this case one can see from  ( [ eq : yilikelihood ] ) that this is equivalent to minimizing the quantity    @xmath39    where @xmath40 represents terms that do not depend on the parameters .",
    "thus for the case of independent gaussian measurements , the maximum likelihood ( ml ) estimators for the parameters coincide with those of the method of least squares ( ls ) .",
    "suppose first that the slope parameter @xmath32 is known exactly , and so it is not adjusted to maximize the likelihood ( or minimize the @xmath41 ) but rather held fixed . the quantity @xmath41 versus the single adjustable parameter @xmath31 would be as shown in fig .  [ fig : linefit](b ) , where the minimum indicates the value of the estimator @xmath42 .",
    "methods for obtaining the standard deviations of estimators  the statistical errors of our measured values  are described in many references such as  @xcite . here in the case of a single fitted parameter",
    "the rule boils down to moving the parameter away from the estimate until @xmath41 increases by one unit ( i.e. , @xmath43 decreases from its maximum by @xmath44 ) as indicated in the figure .",
    "it may be , however , that we do not know the value of the slope parameter @xmath32 , and so even though we do not care about its value in the final result , we are required to treat it as an adjustable parameter in the fit . minimizing @xmath45 results in the estimators @xmath46 , as indicated schematically in fig .",
    "[ fig : chi2contour](a ) .",
    "now the recipe to obtain the statistical errors , however , is not simply a matter of moving the parameter away from its estimated value until the @xmath41 goes up by one unit . here",
    "the standard deviations must be found from the tangent lines ( or in higher - dimensional problems , the tangent hyperplanes ) to the contour defined by @xmath47 , as shown in the figure .",
    "( 10.0,6.5 ) ( 0.5,0 )    ( 8.5,0 )    ( 0.,5.5)(a ) ( 15.5,5.5)(b )    the tilt of the contour in fig .",
    "[ fig : chi2contour](a ) reflects the correlation between the estimators @xmath42 and @xmath48 .",
    "a useful estimate for the inverse of the matrix of covariances @xmath49 $ ] can be found from the second derivative of the log - likelihood evaluated at its maximum ,    @xmath50    more information on how to extract the full covariance matrix from the contour can be found , for example , in refs .",
    "the point to note here is that the correlation between the estimators for the parameter of interest and the nuisance parameter has the result of inflating the standard deviations of both .",
    "that is , if @xmath32 were known exactly , then the distance one would have to move @xmath31 away from its estimated value to make the @xmath41 increase by one unit would be less , as one can see from the figure .",
    "so although we can improve the ability of a model to describe the data by including additional nuisance parameters , this comes at the price of increasing the statistical errors .",
    "this is an important theme which we will encounter often in data analysis .",
    "now consider the case where we have a prior measurement of @xmath32 .",
    "for example , we could have a measurement @xmath51 which we model as following a gaussian distribution centred about @xmath32 and having a given standard deviation @xmath52 . if this measurement is independent of the other @xmath24 values , then the full likelihood function",
    "is obtained simply by multiplying the original one by a gaussian , and so when we find the new @xmath41 from @xmath53 there is an additional term , namely ,    @xmath54    as shown in fig .",
    "[ fig : chi2contour](b ) , the new ( solid ) contour of @xmath55 is compressed relative to the old ( dashed ) one in the @xmath32 direction , and this compression has the effect of decreasing the error in @xmath31 as well . the lesson is : by better constraining nuisance parameters , one improves the statistical accuracy of the parameters of interest .      to treat the example above in the bayesian framework",
    ", we write bayes theorem  ( [ eq : bayesthm ] ) as    @xmath56    here @xmath35 symbolizes the hypothesis whose probability we want to determine .",
    "the likelihood @xmath57 is the probability to obtain the data @xmath58 given the hypothesis , and the prior probability @xmath59 represents our degree of belief about the parameters before seeing the outcome of the experiment . the posterior probability @xmath60 encapsulates all of our knowledge about @xmath38 when the data @xmath61 is combined with our prior beliefs .",
    "the denominator in  ( [ eq : bayesthmlinefit ] ) serves to normalize the posterior pdf to unit area .",
    "the likelihood @xmath57 is the same as the @xmath62 that we used in the frequentist approach above .",
    "the slightly different notation here simply emphasizes its role as the conditional probability for the data given the parameter .    to proceed we need to write down a prior probability density @xmath63 .",
    "this phase of a bayesian analysis , sometimes called the _ elicitation of expert opinion _",
    ", is in many ways the most problematic , as there are no universally accepted rules to follow . here",
    "we will explore some of the important issues that come up .",
    "in general , prior knowledge about one parameter might affect knowledge about the other , and if so this must be built into @xmath64 .",
    "often , however , one may regard the prior knowledge about the parameters as independent , in which case the density factorizes as    @xmath65    for purposes of the present example we will assume that this holds .    for the parameter of interest @xmath31 , it may be that we have essentially no prior information , so the density @xmath66 should be very broad .",
    "often one takes the limiting case of a broad distribution simply to be a constant , i.e. ,    @xmath67    now one apparent problem with eq .",
    "( [ eq : pit0 ] ) is that it is not normalizable to unit area , and so does not appear to be a valid probability density .",
    "it is said to be an _ improper prior_. the prior always appears in bayes theorem multiplied by the likelihood , however , and as long as this falls off quickly enough as a function of the parameters , then the resulting posterior probability density can be normalized to unit area .",
    "a further problem with uniform priors is that if the prior pdf is flat in @xmath38 , then it is not flat for a nonlinear function of @xmath38 , and so a different parametrization of the problem would lead in general to a non - equivalent posterior pdf .    for the special case of a constant prior , one can see from bayes theorem  ( [ eq : bayesthmlinefit ] ) that the posterior is proportional to the likelihood , and therefore the mode ( peak position ) of the posterior is equal to the ml estimator .",
    "the posterior mode , however , will change in general upon a transformation of parameter .",
    "a summary statistic other than the mode may be used as the bayesian estimator , such as the median , which is invariant under a monotonic parameter transformation . but this will not in general coincide with the ml estimator .    for",
    "the prior @xmath68 , let us assume that our prior knowledge about this parameter includes the earlier measurement @xmath51 , which we modelled as a gaussian distributed variable centred about @xmath32 with standard deviation @xmath52 .",
    "if we had taken , even prior to that measurement , a constant prior for @xmath32 , then the `` intermediate - state '' prior that we have before looking at the @xmath24 is simply this flat prior times the gaussian likelihood , i.e. , a gaussian prior in @xmath32 :    @xmath69    putting all of these ingredients into bayes theorem gives    @xmath70    where @xmath71 represents the constant prior in @xmath31 and the equation has been written as a proportionality with the understanding that the final posterior pdf should be normalized to unit area .",
    "what bayes theorem gives us is the full joint pdf @xmath72 for both the parameter of interest @xmath31 as well as the nuisance parameter @xmath32 .",
    "to find the pdf for the parameter of interest only , we simply integrate ( marginalize ) the joint pdf , i.e. ,    @xmath73    in this example , it turns out that we can do the integral in closed form .",
    "we find a gaussian posterior ,    @xmath74    where @xmath42 is in fact the same as the ml ( or ls ) estimator found above with the frequentist approach , and @xmath75 is the same as the standard deviation of that estimator @xmath76 .",
    "so we find something that looks just like the frequentist answer , although here the interpretation of the result is different .",
    "the posterior pdf @xmath77 gives our degree of belief about the location of the parameter in the light of the data .",
    "we will see below how the bayesian approach can , however , lead to results that differ both in interpretation as well as in numerical value from what would be obtained in a frequentist calculation .",
    "first , however , we need to pause for a short digression on bayesian computation .      in most real bayesian calculations ,",
    "the marginalization integrals can not be carried out in closed form , and if the number of nuisance parameters is too large then they can also be difficult to compute with standard monte carlo methods . however ,",
    "_ markov chain monte carlo _ ( mcmc ) has become the most important tool for computing integrals of this type and has revolutionized bayesian computation . in - depth",
    "treatments of mcmc can be found , for example , in the texts by robert and casella  @xcite , liu  @xcite , and the review by neal  @xcite .",
    "the basic idea behind using mcmc to marginalize the joint pdf @xmath78 is to sample points @xmath79 according to the posterior pdf but then only to look at the distribution of the component of interest , @xmath31 .",
    "a simple and widely applicable mcmc method is the metropolis - hastings algorithm , which allows one to generate multidimensional points @xmath38 distributed according to a target pdf that is proportional to a given function @xmath60 , which here will represent our posterior pdf .",
    "it is not necessary to have @xmath60 normalized to unit area , which is useful in bayesian statistics , as posterior probability densities are often determined only up to an unknown normalization constant , as is the case in our example .    to generate points that follow @xmath60 ,",
    "one first needs a proposal pdf @xmath80 , which can be ( almost ) any pdf from which independent random values @xmath38 can be generated , and which contains as a parameter another point in the same space @xmath81 .",
    "for example , a multivariate gaussian centred about @xmath81 can be used .",
    "beginning at an arbitrary starting point @xmath81 , the hastings algorithm iterates the following steps :    1 .",
    "generate a value @xmath38 using the proposal density @xmath80 ; 2 .",
    "form the hastings test ratio , @xmath82 $ ] ; 3 .",
    "generate a value @xmath83 uniformly distributed in @xmath84 $ ] ; 4 .   if @xmath85 , take @xmath86 .",
    "otherwise , repeat the old point , i.e. , @xmath87 .    if one takes the proposal density to be symmetric in @xmath38 and @xmath81 ,",
    "then this is the _ metropolis_hastings algorithm , and",
    "the test ratio becomes @xmath88 $ ] .",
    "that is , if the proposed @xmath38 is at a value of probability higher than @xmath81 , the step is taken .",
    "if the proposed step is rejected , hop in place .",
    "methods for assessing and optimizing the performance of the algorithm are discussed , for example , in refs .",
    "one can , for example , examine the autocorrelation as a function of the lag @xmath89 , i.e. , the correlation of a sampled point with one @xmath89 steps removed .",
    "this should decrease as quickly as possible for increasing @xmath89 .",
    "generally one chooses the proposal density so as to optimize some quality measure such as the autocorrelation . for certain problems",
    "it has been shown that one achieves optimal performance when the acceptance fraction , that is , the fraction of points with @xmath90 , is around 40% .",
    "this can be adjusted by varying the width of the proposal density .",
    "for example , one can use for the proposal pdf a multivariate gaussian with the same covariance matrix as that of the target pdf , but scaled by a constant .    for our example above ,",
    "mcmc was used to generate points according to the posterior pdf @xmath91 by using a gaussian proposal density .",
    "the result is shown in fig .",
    "[ fig : mcmc ] .",
    "( 10.0,5 ) ( 0.,0 )    ( 5.3,0 )    ( 10.8,0 )    ( 4.3,4.)(a ) ( 9.6,4.)(b ) ( 14.9,4.)(c )    from the @xmath92 points in the scatter plot in fig .",
    "[ fig : mcmc](a ) we simply look at the distribution of the parameter of interest , @xmath31 [ fig .  [ fig : mcmc](b ) ] . the standard deviation of this distribution is what we would report as the statistical error in our measurement of @xmath31 .",
    "the distribution of the nuisance parameter @xmath32 from fig .",
    "[ fig : mcmc](c ) is not directly needed , although it may be of interest in some other context where that parameter is deemed interesting .",
    "in fact one can go beyond simply summarizing the width of the distributions with the a statistic such as the standard deviation .",
    "the full form of the posterior distribution of @xmath31 contains useful information about where the parameter s true value is likely to be . in this example",
    "the distributions will in fact turn out to be gaussian , but in a more complex analysis there could be non - gaussian tails and this information can be relevant in drawing conclusions from the result .",
    "the posterior distribution of @xmath31 obtained above encapsulates all of the analyst s knowledge about the parameter in the light of the data , given that the prior beliefs were reflected by the density @xmath63 .",
    "a different analyst with different prior beliefs would in general obtain a different posterior pdf .",
    "we would like the result of a bayesian analysis to be of value to the broader scientific community , not only to those that share the prior beliefs of the analyst .",
    "and therefore it is important in a bayesian analysis to show by how much the posterior probabilities would change upon some reasonable variation in the prior .",
    "this is sometimes called the _ sensitivity analysis _ and is an important part of any bayesian calculation .    in the example",
    "above , we can imagine a situation where there was no prior measurement @xmath51 of the parameter @xmath32 , but rather a theorist had told us that , based on considerations of symmetry , consistency , aesthetics , etc .",
    ", @xmath32 was `` almost certainly '' positive , and had a magnitude `` probably less than 0.1 or so '' .",
    "when pressed to be precise , the theorist sketches a curve roughly resembling an exponential with a mean of 0.1 .",
    "so we can express this prior as    @xmath93    with @xmath94 .",
    "we can substitute this prior into bayes theorem  ( [ eq : bthmlf ] ) to obtain the joint pdf for @xmath31 and @xmath32 , and then marginalize to find the pdf for @xmath31 . doing this numerically with mcmc results in the posterior distributions shown in fig .",
    "[ fig : piexp](a ) .",
    "( 10.0,6.5 ) ( 0.5,0 )    ( 8.5,0 )    ( 0.,5.5)(a ) ( 15.5,5.5)(b )    now the theorist who proposed this prior for @xmath32 may feel reluctant to be pinned down , and so it is important to recall ( and to reassure the theorist about ) the `` if - then '' nature of a bayesian analysis .",
    "one does not have to be absolutely certain about the prior in eq .",
    "( [ eq : pi1exp ] ) .",
    "rather , bayes theorem simply says that _ if _ one were to have these prior beliefs , _ then _ we obtain certain posterior beliefs in the light of the data .    one simple way to vary the prior here is to try different values of the mean @xmath95 , as shown in fig .",
    "[ fig : piexp](a ) .",
    "we see here the same basic feature as shown already in the frequentist analysis , namely , that when one increases the precision about the nuisance parameter , @xmath32 , then the knowledge about the parameter of interest , @xmath31 , is improved .    alternatively ( or in addition )",
    "we may try different functional forms for the prior , as shown in fig .",
    "[ fig : piexp](b ) . in this case using a uniform distribution for @xmath68 with @xmath96 or gaussian with @xmath97 truncated for @xmath98 both give results similar to the exponential with a mean of @xmath99 .",
    "so one concludes that the result is relatively insensitive to the detailed nature of the tails of @xmath68 .",
    "we can now generalize the example of section  [ sec : linefit ] to explore some further aspects of a bayesian analysis .",
    "let us suppose that we are given a set of @xmath100 measurements as above , but now in addition to the statistical errors we also are given systematic errors .",
    "that is , we are given @xmath101 for @xmath102 where the measurements as before are each carried out for a specified value of a control variable @xmath27 .",
    "more generally , instead of having @xmath103 it may be that the set of measurements comes with an @xmath104 covariance matrix @xmath105 corresponding to the statistical errors and another matrix @xmath106 for the systematic ones . here",
    "the square roots of the diagonal elements give the errors for each measurement , and the off - diagonal elements provide information on how they are correlated .    as before we assume some functional form @xmath107 for the expectation values of the @xmath24 .",
    "this could be the linear model of eq .",
    "( [ eq : muofx ] ) or something more general , but in any case it depends on a vector of unknown parameters @xmath38 . in this example , however , we will allow that the model is not perfect , but rather could have a systematic bias .",
    "that is , we write that the true expectation value of the @xmath33th measurement can be written    @xmath108 = \\mu(x_i ; \\boldvec{\\theta } ) + b_i \\;,\\ ] ]    where @xmath109 represents the bias .",
    "the @xmath109 can be viewed as the systematic errors of the model , present even when the parameters @xmath38 are adjusted to give the best description of the data .",
    "we do not know the values of the @xmath109 . if we did , we would account for them in the model and they would no longer be biases .",
    "we do not in fact know that their values are nonzero , but we are allowing for the possibility that they could be .",
    "the reported systematic errors are intended as a quantitative measure of how large we expect the biases to be .",
    "as before , the goal is to make inferences about the parameters @xmath38 ; some of these may be of direct interest and others may be nuisance parameters . in section  [ sec : genfreq ]",
    "we will try to do this using the frequentist approach , and in section  [ sec : genbay ] we will use the bayesian method .      if we adopt the frequentist approach , we need to write down a likelihood function such as eq .",
    "( [ eq : yilikelihood ] ) , but here we know in advance that the model @xmath107 is not expected to be fully accurate .",
    "furthermore it is not clear how to insert the systematic errors . often , perhaps without a clear justification ,",
    "one simply adds the statistical and systematic errors in quadrature , or in the case where one has the covariance matrices @xmath105 and @xmath106 , they are summed to give a sort of ` full ' covariance matrix :    @xmath110    one might then use this in a multivariate gaussian likelihood function , or equivalently it could be used to construct the @xmath41 ,    @xmath111    which is then minimized to find the ls estimators for @xmath38 . in eq .",
    "( [ eq : chi2multi ] ) the vector @xmath58 should be understood as a column vector , @xmath112 is the corresponding vector of model values , and the superscript @xmath113 represents the transpose ( row ) vector . minimizing this @xmath41 gives the generalized ls estimators @xmath37 , and the usual procedures can be applied to find their covariances , which now in some sense include the systematics .",
    "but in what sense is there any formal justification for adding the covariance matrices in eq .",
    "( [ eq : fullcov ] ) ?",
    "next we will treat this problem in the bayesian framework and see that there is indeed some reason behind this recipe , but with limitations , and further we will see how to get around these limitations .      in the corresponding bayesian analysis ,",
    "one treats the statistical errors as given by @xmath105 as reflecting the distribution of the data @xmath61 in the likelihood .",
    "the systematic errors , through @xmath106 , reflect the width of the prior probabilities for the bias parameters @xmath109 .",
    "that is , we take    @xmath114 \\ ; , \\\\*[0.2 cm ] \\label{eq : pib } \\pi_b(\\boldvec{b } ) & \\propto &   \\exp \\left [ - \\begin{matrix } \\frac{1}{2 } \\end{matrix } \\boldvec{b}^t v_{\\rm sys}^{-1 } \\boldvec{b } \\right ] \\ ; ,   \\quad   \\quad \\pi_{\\theta}(\\boldvec{\\theta } ) = \\mbox{const . }",
    "\\ ; , \\\\*[0.2 cm ]   \\label{eq : bayesthetab } p(\\boldvec{\\theta } , \\boldvec{b } | \\boldvec{y } ) &   \\propto & l(\\boldvec{y } | \\boldvec{\\theta } , \\boldvec{b } ) \\pi_{\\theta}(\\boldvec{\\theta } ) \\pi_{b}(\\boldvec{b } ) \\;,\\end{aligned}\\ ] ]    where in  ( [ eq : bayesthetab ] ) , bayes theorem is used to obtain the joint probability for the parameters of interest , @xmath38 , and also the biases @xmath115 . to obtain the probability for @xmath38 we integrate ( marginalize ) over @xmath115 ,    @xmath116",
    "one finds that the mode of @xmath117 is at the same position as the least - squares estimates , and its covariance will be the same as obtained from the frequentist analysis where the full covariance matrix was given by the sum @xmath118 .",
    "so this can be taken in effect as the formal justification for the addition in quadrature of statistical and systematic errors in a least - squares fit .",
    "if one stays with the prior probabilities used above , the bayesian and least - squares approaches deliver essentially the same result . an advantage of the bayesian framework , however , is that it allows one to refine the assessment of the systematic uncertainties as expressed through the prior probabilities .",
    "for example , the least - squares fit including systematic errors is equivalent to the assumption of a gaussian prior for the biases .",
    "a more realistic prior would take into account the experimenter s own uncertainty in assigning the systematic error , i.e. , the ` error on the error ' .",
    "suppose , for example , that the @xmath33th measurement is characterized by a reported systematic uncertainty @xmath119 and an unreported factor @xmath120 , such that the prior for the bias @xmath109 is    @xmath121 \\pi_s(s_i ) \\",
    ", ds_i \\;.\\ ] ]    here the ` error on the error ' is encapsulated in the prior for the factor @xmath122 , @xmath123 . for this",
    "we can take whatever function is deemed appropriate . for some types of systematic error",
    "it could be close to the ideal case of a delta function centred about unity .",
    "many reported systematics are , however , at best rough guesses , and one could easily imagine a function @xmath124 with a mean of unity but a standard deviation of , say , @xmath125 or more . here",
    "we show examples using a gamma distribution for @xmath123 , which results in substantially longer tails for the prior @xmath126 than those of the gaussian .",
    "this can be seen in fig .",
    "[ fig : pib ] , which shows @xmath127 for different values of the standard deviation of @xmath124 , @xmath128 .",
    "related studies using an inverse gamma distribution can be found in refs .",
    "@xcite , which have the advantage that the posterior pdf can be written down in closed form .",
    "( 10.0,6 ) ( 1.,-0.2 )    ( 9.0,.8 )    ( 6,4)[b ]    using a prior for the biases with tails longer than those of a gaussian results in a reduced sensitivity to outliers , which arise when an experimenter overlooks an important source of systematic uncertainty in the estimated error of a measurement . as a simple test of this ,",
    "consider the sample data shown in fig .",
    "[ fig : outlier](a ) .",
    "suppose these represent four independent measurements of the same quantity , here a parameter called @xmath129 , and the goal is to combine the measurements to provide a single estimate of @xmath129 .",
    "that is , we are effectively fitting a horizontal line to the set of measured @xmath130 values , where the control variable @xmath27 is just a label for the measurements .    in this example , suppose that each measurement @xmath24 , @xmath131 , is modelled as gaussian distributed about @xmath129 , having a standard deviation @xmath132 , and furthermore each measurement has a systematic uncertainty @xmath133 , which here is taken to refer to the standard deviation of the gaussian component of the prior @xmath134 .",
    "this is then folded together with @xmath135 to get the full prior for @xmath109 using eq .",
    "( [ eq : pieoe ] ) , and the joint prior for the vector of bias parameters is simply the product of the corresponding terms , as the systematic errors here are treated as being independent .",
    "these ingredients are then assembled according to the recipe of eqs .",
    "( [ eq : equivbayes])([eq : margprob ] ) to produce the posterior pdf for @xmath129 , @xmath136 .    results of the exercise are shown in fig .",
    "[ fig : outlier ] . in fig .",
    "[ fig : outlier](a ) , the four measurements @xmath24 are reasonably consistent with each other . figure  [ fig : outlier](b ) shows the corresponding posterior @xmath136 for two values of @xmath137 , which reflect differing degrees of non - gaussian tails in the prior for the bias parameters , @xmath134 . for @xmath138 ,",
    "the prior for the bias is exactly gaussian , whereas for @xmath139 , the non - gaussian tails are considerably longer , as can be seen from the corresponding curves in fig .",
    "[ fig : pib ] .",
    "the posterior pdfs for both cases are almost identical , as can be see in fig .",
    "[ fig : outlier](a ) .",
    "determining the mean and standard deviation of the posterior for each gives @xmath140 for the case of @xmath141 , and @xmath142 for @xmath139 . so assuming a 50% `` error on the error '' here one only inflates the error of the averaged result by a small amount .",
    "( 10.0,8.5 ) ( 1.5,5.5 )    ( 8.5,5.5 )    ( 0.5,7.5)(a ) ( 15.,7.5)(b ) ( 1.5,0 )    ( 8.5,0 )    ( 0.5,3.5)(c ) ( 15.,3.5)(d )    now consider the case where one of the measured values is substantially different from the other three , as shown in fig .",
    "[ fig : outlier](c ) . here using the same priors for the bias parameters",
    "results in the posteriors shown in fig .",
    "[ fig : outlier](d ) .",
    "the posterior means and standard deviations are @xmath143 for the case of @xmath141 , and @xmath144 for @xmath145 .    when we assume a purely gaussian prior for the bias ( @xmath146 ) , the presence of the outlier has in fact no effect on the width of the posterior .",
    "this is rather counter - intuitive and results from our assumption of a gaussian likelihood for the data and a gaussian prior for the bias parameters .",
    "the posterior mean is however pulled substantially higher than the three other measurements , which are clustered around @xmath147 . if the priors @xmath134 have longer tails , as occurs when we take @xmath139 , then the posterior is broader , and furthermore it is pulled less far by the outlier , as can be seen in fig .",
    "[ fig : outlier](d ) .",
    "the fact is that the width of the posterior distribution , which effectively tells us the uncertainty on the parameter of interest @xmath129 , becomes coupled to the internal consistency of the data .",
    "in contrast , in the ( frequentist ) least - squares method , or in the bayesian approach using a gaussian prior for the bias parameters , the final uncertainty on the parameter of interest is unaffected by the presence of outliers . and in many cases of practical interest , it would be in fact appropriate to conclude that the presence of outliers should indeed increase one s uncertainty about the final parameter estimates . the example shown here",
    "can be generalized to cover a wide variety of model uncertainties by including prior probabilities for an enlarged set of model parameters .      in these lectures",
    "we have seen how bayesian methods can be used in parameter estimation , and this has also given us the opportunity to discuss some aspects of bayesian computation , including the important tool of markov chain monte carlo .",
    "although bayesian and frequentist methods may often deliver results that agree numerically , there is an important difference in their interpretation .",
    "furthermore , bayesian methods allow one to incorporate prior information that may be based not on other measurements but rather on theoretical arguments or purely subjective considerations . and as these considerations may not find universal agreement , it is important to investigate how the results of a bayesian analysis would change for a reasonable variation of the prior probabilities .",
    "it is important to keep in mind that in the bayesian approach , all information about the parameters is encapsulated in the posterior probabilities .",
    "so if the analyst also wants to set upper limits or determine intervals that cover the parameter with a specified probability , then this is a straightforward matter of finding the parameter limits such that the integrated posterior pdf has the desired probability content .",
    "a discussion of bayesian methods to the important problem of setting upper limits on a poisson parameter is covered in ref .",
    "@xcite and references therein ; we will not have time in these lectures to go into that question here .",
    "we will also unfortunately not have time to explore bayesian model selection .",
    "this allows one to quantify the degree to which the the data prefer one model over the other using a quantity called the bayes factor .",
    "these have not yet been widely used in particle physics but should be kept in mind as providing important complementary information to the corresponding outputs of frequentist hypothesis testing such as @xmath148-values .",
    "a brief description of bayes factors can be found in ref .",
    "@xcite and a more in - depth treatment is given in ref .",
    "in the second part of these lectures we will take a look at the important topic of multivariate analysis .",
    "in - depth information on this topic can be found in the textbooks  @xcite . in a particle physics context",
    ", multivariate methods are often used when selecting events of a certain type using some potentially large number of measurable characteristics for each event . the basic framework we will use to examine these methods is that of a frequentist hypothesis test .",
    "the fundamental unit of data in a particle physics experiment is the ` event ' , which in most cases corresponds to a single particle collision . in some cases",
    "it could be instead a decay , and the picture does not change much if we look , say , at individual particles or tracks .",
    "but to be concrete let us suppose that we want to search for events from proton  proton collisions at the lhc that correspond to some interesting ` signal ' process , such as supersymmetry .",
    "when running at full intensity , the lhc should produce close to a billion events per second .",
    "after a quick sifting , the data from around 200 per second are recorded for further study , resulting in more than a billion events per year .",
    "but only a tiny fraction of these are of potential interest .",
    "if one of the speculative theories such as supersymmetry turns out to be realized in nature , then this will result in a subset of events having characteristic features , and the susy events will simply be mixed in randomly with a much larger number of standard model events .",
    "the relevant distinguishing features depend on what new physics nature chooses to reveal , but one might see , for example , high @xmath149 jets , leptons , missing energy .    unfortunately , background processes ( e.g. , standard model events ) can often mimic these features and one will not be able to say with certainty that a given event shows a clear evidence for something new such as supersymmetry .",
    "for example , even standard model events can contain neutrinos which also escape undetected .",
    "the typical amount and pattern of missing energy in these events differs on average , however , from what a susy event would give , and so a statistical analysis can be applied to test whether something besides standard model events is present .    in a typical analysis",
    "there is a class of event we are interested in finding ( signal ) , and these , if they exist at all , are mixed in with the rest of the events ( background ) .",
    "the data for each event is some collection of numbers @xmath150 representing particle energies , momenta , etc .",
    "we will refer to these as the _ input variables _ of the problem . and",
    "the probabilities are joint densities for @xmath151 given the signal ( s ) or background ( b ) hypotheses : @xmath152 and @xmath153 .",
    "to illustrate the general problem , consider the scatterplots shown in fig .",
    "[ fig : scatter ] .",
    "these show the distribution of two variables , @xmath154 and @xmath155 , which represent two out of a potentially large number of quantities measured for each event .",
    "the blue circles could represent the sought after signal events , and the red triangles the background . in each of the three figures",
    "there is a decision boundary representing a possible way of classifying the events .",
    "( 10.0,5 ) ( 0.,0 )    ( 5.3,0 )    ( 10.8,0 )    ( 4.4,4.1)(a ) ( 9.6,4.1)(b ) ( 15.1,4.1)(c )    figure  [ fig : scatter](a ) represents what is commonly called the ` cut - based ' approach .",
    "one selects signal events by requiring @xmath156 and @xmath157 for some suitably chosen cut values @xmath158 and @xmath159 .",
    "if @xmath154 and @xmath155 represent quantities for which one has some intuitive understanding , then this can help guide one s choice of the cut values .",
    "another possible decision boundary is made with a diagonal cut as shown in fig .",
    "[ fig : scatter](b ) .",
    "one can show that for certain problems a linear boundary has optimal properties , but in the example here , because of the curved nature of the distributions , neither the cut - based nor the linear solution is as good as the nonlinear boundary shown in fig .  [",
    "fig : scatter](c ) .",
    "the decision boundary is a surface in the @xmath100-dimensional space of input variables , which can be represented by an equation of the form @xmath160 , where @xmath161 is some constant .",
    "we accept events as corresponding to the signal hypothesis if they are on one side of the boundary , e.g. , @xmath162 could represent the acceptance region and @xmath163 could be the rejection region .",
    "equivalently we can use the function @xmath164 as a scalar _",
    "test statistic_. once its functional form is specified , we can determine the pdfs of @xmath164 under both the signal and background hypotheses , @xmath165 and @xmath166 .",
    "the decision boundary is now effectively a single cut on the scalar variable @xmath130 , as illustrated in fig .",
    "[ fig : teststat ] .",
    "( 10.0,5 ) ( 1.5,0 . )",
    "( 9.0,.8 )    ( 6,4)[b ]    to quantify how good the event selection is , we can define the _ efficiency _ with which one selects events of a given type as the probability that an event will fall in the acceptance region .",
    "that is , the signal and background efficiencies are    @xmath167 \\varepsilon_{\\rm b } & = & p ( \\mbox{accept event } | \\mbox{b } )   = \\int_{\\rm a } f(\\boldvec{x } | \\mbox{b } ) \\ , d \\boldvec{x }   = \\int_{-\\infty}^{y_{\\rm cut } } p(y | \\mbox{b } ) \\ , dy \\ ; , \\end{aligned}\\ ] ]    where the region of integration a represents the acceptance region .    dividing the space of input variables into two regions where one accepts or rejects the signal hypothesis is essentially the language of a frequentist statistical test .",
    "if we regard background as the ` null hypothesis ' , then the background efficiency is the same as what in a statistical context would be called the significance level of the test , or the rate of ` type - i error ' .",
    "viewing the signal process as the alternative , the signal efficiency is then what a statistician would call the power of the test ; it is the probability to reject the background hypothesis if in fact the signal hypothesis is true .",
    "equivalently , this is one minus the rate of ` type - ii error ' .",
    "the use of a statistical test to distinguish between two classes of events ( signal and background ) , comes up in different ways .",
    "sometimes both event classes are known to exist , and the goal is to select one class ( signal ) for further study .",
    "for example , proton  proton collisions leading to the production of top quarks are a well - established process . by selecting these events one can carry out precise measurements of the top quark s properties such as its mass . in other cases , the signal process could represent an extension to the standard model , say , supersymmetry , whose existence is not yet established , and the goal of the analysis is to see if one can do this . rejecting the standard model with a sufficiently high significance level amounts to discovering something new , and of course one hopes that the newly revealed phenomena will provide important insights into how nature behaves .",
    "what the physicist would like to have is a test with maximal power with respect to a broad class of alternative hypotheses . for two specific signal and background hypotheses , it turns out that there is a well defined optimal solution to our problem .",
    "the _ neyman  pearson _",
    "lemma states that one obtains the maximum power relative for the signal hypothesis for a given significance level ( background efficiency ) by defining the acceptance region such that , for @xmath151 inside the region , the _ likelihood ratio _",
    ", i.e. , the ratio of pdfs for signal and background ,    @xmath168    is greater than or equal to a given constant , and it is less than this constant everywhere outside the acceptance region .",
    "this is equivalent to the statement that the ratio  ( [ eq : lratio ] ) represents the test statistic with which one obtains the highest signal efficiency for a given background efficiency , or equivalently , for a given signal purity .    in principle",
    "the signal and background theories should allow us to work out the required functions @xmath152 and @xmath169 , but in practice the calculations are too difficult and we do not have explicit formulae for these .",
    "what we have instead of @xmath152 and latexmath:[$f(\\boldvec{x }    sample @xmath151 to produce simulated signal and background events . because of the multivariate nature of the data , where @xmath151 may contain at least several or perhaps even hundreds of components , it is a nontrivial problem to construct a test with a power approaching that of the likelihood ratio .    in the usual case",
    "where the likelihood ratio  ( [ eq : lratio ] ) can not be used explicitly , there exists a variety of other multivariate classifiers that effectively separate different types of events .",
    "methods often used in hep include _ neural networks _ or _ fisher discriminants_. recently , further classification methods from machine learning have been applied in hep analyses ; these include _ probability density estimation ( pde ) _ techniques , _ kernel - based pde _ ( _ kde _ or _ parzen window _ ) , _ support vector machines _ , and _ decision trees_. techniques such as ` boosting ' and ` bagging ' can be applied to combine a number of classifiers into a stronger one with greater stability with respect to fluctuations in the training data .",
    "descriptions of these methods can be found , for example , in the textbooks  @xcite and in proceedings of the phystat conference series  @xcite .",
    "software for hep includes the tmva  @xcite and statpatternrecognition  @xcite packages , although support for the latter has unfortunately been discontinued .    as we will not have the time to examine all of the methods mentioned above , in the following section we look at a specific example of a classifier to illustrate some of the main ideas of a multivariate analysis : the boosted decision tree ( bdt ) .",
    "boosted decision trees exploit relatively recent developments in machine learning and have gained significant popularity in hep .",
    "first in section  [ sec : dectree ] we describe the basic idea of a decision tree , and then in section  [ sec : boosting ] we will say how the the technique of ` boosting ' can be used to improve its performance .",
    "a decision tree is defined by a collection of successive cuts on the set of input variables . to determine the appropriate cuts ,",
    "one begins with a sample of @xmath171 training events which are known to be either signal or background , e.g. , from monte carlo .",
    "the set of @xmath100 input variables measured for each event constitutes a vector @xmath172 .",
    "thus we have @xmath171 instances of @xmath151 , @xmath173 , as well as the corresponding @xmath171 true class labels @xmath174 .",
    "it is convenient to assign numerical values to the labels so that , e.g. , @xmath175 corresponds to signal and @xmath176 for background .",
    "in addition we will assume that each event can be assigned a weight , @xmath177 , with @xmath178 .",
    "for any subset of the events and for a set of weights , the signal fraction ( purity ) is taken to be    @xmath179    where s and b refer to the signal and background event types .",
    "the weights are not strictly speaking necessary for a decision tree , but will be used in connection with boosting in section  [ sec : boosting ] . for",
    "a decision tree without boosting we can simply take all the weights to be equal .    to quantify the degree of separation achieved by a classifier for a selected subset of the events one can use , for example ,",
    "the _ gini coefficient _  @xcite , which historically has been used as a measure of dispersion in economics and is defined as    @xmath180    the gini coefficient is zero if the selected sample is either pure signal or background .",
    "another measure is simply the misclassification rate ,    @xmath181    the idea behind a decision tree is illustrated in fig .",
    "[ fig : miniboonebdt ] , from an analysis by the miniboone neutrino oscillation experiment at fermilab  @xcite .",
    "( 10.0,6 ) ( .5,-2 )    ( 9.0,.8 )    ( 6,4)[b ]    one starts with the entire sample of training events in the root node , shown in the figure with 52 signal and 48 background events .",
    "out of all of the possible input variables in the vector @xmath151 , one finds the component that provides the best separation between signal and background by use of a single cut .",
    "this requires a definition of what constitutes ` best separation ' , and there are a number of reasonable choices .",
    "for example , for a cut that splits a set of events @xmath182 into two subsets @xmath183 and @xmath184 , one can define the degree of separation through the weighted change in the gini coefficients ,    @xmath185    where    @xmath186    and similarly for @xmath187 and @xmath188 .",
    "alternatively one may use a quantity similar to  ( [ eq : delta ] ) but with the misclassification rate  ( [ eq : misclassrate ] ) , for example , instead of the gini coefficient .",
    "more possibilities can be found in ref .",
    "@xcite .    for whatever chosen measure of degree of separation , @xmath189 ,",
    "one finds the cut on the variable amongst the components of @xmath151 that maximizes it . in the example of the miniboone experiment shown in fig .",
    "[ fig : miniboonebdt ] , this happened to be a cut on the number of pmt hits with a value of 100 .",
    "this splits the training sample into the two daughter nodes shown in the figure , one of which is enhanced in signal and the other in background events .    the algorithm requires a stopping rule based , for example , on the number of events in a node or the misclassification rate .",
    "if , for example , the number of events or the misclassification rate in a given node falls below a certain threshold , then this is defined as a terminal node or ` leaf ' .",
    "it is classified as a signal or background leaf based on its predominant event type . in fig .",
    "[ fig : miniboonebdt ] , for example , the node after the cut on pmt hits with 4 signal and 37 background events is classified as a terminal background node .    for nodes that have not yet reached the stopping criterion , one iterates the procedure and finds , as before , the variable that provides the best separation with a single cut . in fig .",
    "[ fig : miniboonebdt ] this is an energy cut of @xmath190 .",
    "the steps are continued until all nodes reach the stopping criterion .",
    "the resulting set of cuts effectively divides the @xmath151 space into two regions : signal and background . to provide a numerical output for the classifier we can define    @xmath191                 -1 & \\quad \\boldvec{x } \\mbox { in background region } .",
    "\\end{array }          \\right.\\ ] ]    equation  ( [ eq : ftree ] ) defines a decision tree classifier . in this form , these tend to be very sensitive to statistical fluctuations in the training data .",
    "one can easily see why this is , for example , if two of the components of @xmath151 have similar discriminating power between signal and background .",
    "for a given training sample , one variable may be found to give the best degree of separation and is chosen to make the cut , and this affects the entire further structure of the tree . in a different statistically independent sample of training events",
    ", the other variable may be found to be better , and the resulting tree could look very different .",
    "boosting is a technique that can decrease the sensitivity of a classifier to such fluctuations , and we describe this in the following section .",
    "boosting is a general method of creating a set of classifiers which can be combined to give a new classifier that is more stable and has a smaller misclassification rate than any individual one .",
    "it is often applied to decision trees , precisely because they suffer from sensitivity to statistical fluctuations in the training sample , but the technique can be applied to any classifier .    let us suppose as above that we have a sample of @xmath171 training events , i.e. , @xmath171 instances of the data vector , @xmath192 , and @xmath171 true class labels @xmath193 , with @xmath175 for signal and @xmath194 for background .",
    "also as above assume we have @xmath171 weights @xmath195 , where the superscript @xmath196 refers to the fact that this is the first training set .",
    "we initially set the weights equal and normalized such that    @xmath197    the idea behind boosting is to create from the initial sample , a series of further training samples which differ from the initial one in that the weights will be changed according to a specific rule .",
    "a number of boosting algorithms have been developed , and these differ primarily in the rule used to update the weights",
    ". we will describe the adaboost algorithm of freund and schapire  @xcite , as it was one of the first such algorithms and its properties have been well studied .",
    "one begins with the initial training sample and from it derives a classifier .",
    "we have in mind here a decision tree , but it could be any type of classifier for where the training employs the event weights . the resulting function @xmath198 will have a certain misclassification rate @xmath199 . in general for the @xmath89th classifier ( i.e. , based on the @xmath89th training sample ) , we can write the error rate as    @xmath200    where @xmath201 if the boolean expression @xmath202 is true , and is zero otherwise .",
    "we then assign a score to the classifier based on its error rate . for the adaboost algorithm",
    "this is    @xmath203    which is positive as long as the error rate is lower than 50% , i.e , .",
    "the classifier does better than random guessing .",
    "having carried out these steps for the initial training sample , we define the second training sample by updating the weights .",
    "more generally , the weights for step @xmath204 are found from those for step @xmath89 by    @xmath205    where the factor @xmath206 is chosen so that the sum of the updated weights is equal to unity .",
    "note that if an event is incorrectly classified , then the true class label @xmath24 and the value @xmath207 have opposite signs , and thus the new weights are greater than the old ones .",
    "correctly classified events have their weights decreased .",
    "this means that the updated training set will pay more attention in the next iteration to those events that were not correctly classified , the idea being that it should try harder to get it right the next time around .",
    "after @xmath208 iterations of this procedure one has classifiers @xmath209 , each with a certain error rate and score based on eqs .",
    "( [ eq : errorrate ] ) and  ( [ eq : alphak ] ) . in the case of decision trees , the set of new trees",
    "is called a _",
    "forest_. from these one defines an averaged classifier as    @xmath210    equation  ( [ eq : boostave ] ) defines a boosted decision tree ( or more generally , a boosted version of whatever classifier was used ) .",
    "one of the important questions to be addressed is how many boosting iterations to use .",
    "one can show that for a sufficiently large number of iterations , a boosted decision tree will eventually classify all of the events in the training sample correctly .",
    "similar behaviour is found with any classification method where one can control to an arbitrary degree the flexibility of the decision boundary .",
    "the user can arrange it so that the boundary twists and turns so as to get all of the events on the right side .",
    "in the case of a neural network , for example , one can increase the number of hidden layers , or the number of nodes in the hidden layers ; for a support vector machine , one can adjust the width of the kernel function and the regularization parameter to increase the flexibility of the boundary .",
    "an example is shown in fig .",
    "[ fig : overtraining](a ) , where an extremely flexible classifier has managed to enclose all of the signal events and exclude all of the background .",
    "( 10.0,6.5 ) ( 0.5,0 )    ( 8.5,0 )    ( 0.,5.5)(a ) ( 15.5,5.5)(b )    of course if we were now to take the decision boundary shown in fig .",
    "[ fig : overtraining](a ) and apply it to a statistically independent data sample , there is no reason to believe that the contortions that led to such good performance on the training sample will still work .",
    "this can be seen in fig .",
    "[ fig : overtraining](b ) , which shows the same boundary with a new data sample . in this case",
    "the classifier is said to be _",
    "its error rate calculated from the same set of events used to train the classifier underestimates the rate on a statistically independent sample .    to deal with overtraining , one estimates the misclassification rate not only with the training data sample but also with a statistically independent test sample .",
    "we can then plot these rates as a function of the parameters that regulate the flexibility of the decision boundary , e.g. , the number of boosting iterations used to form the bdt . for a small number of iterations , one will find in general that the error rates for both samples drop . the error rate based on the training sample will continue to drop , eventually reaching zero .",
    "but at some point the error rate from the test sample will cease to decrease and in general will increase .",
    "one chooses the architecture of the classifier ( number of boosting iterations , number of nodes or layers in a neural network , etc . ) to minimize the error rate on the test sample .    as the test sample is used to choose between a number of competing architectures based on the minimum observed error rate , this in fact gives a biased estimate of the true error rate . in principle",
    "one should use a third validation sample to obtain an unbiased estimate of the error rate . in many cases",
    "the bias is small and this last step is omitted , but one should be aware of its potential existence .    in some applications ,",
    "the training data is relatively inexpensive ; one simply generates more events with monte carlo .",
    "but often event generation can take a prohibitively long time and one may be reluctant to use only a fraction of the events for training and the other half for testing . in such cases , procedures such as _ cross validation _ ( see , e.g. , refs .",
    "@xcite ) can be used where the available events are partitioned in a number of different ways into training and test samples and the results averaged .",
    "boosted decision trees have become increasingly popular in particle physics in recent years .",
    "one of their advantages is that they are relatively insensitive to the number of input variables used in the data vector @xmath151 .",
    "components that provide little or no separation between signal and background are rarely chosen as for the cut that provides separation , i.e. , to split the tree , and thus they are effectively ignored .",
    "decision trees have no difficulty in dealing with different types of data ; these can be real , integer , or they can simply be labels for which there is no natural ordering ( categorical data ) . furthermore , boosted decision trees are surprisingly insensitive to overtraining .",
    "that is , although the error rate on the test sample will not decrease to zero as one increases the number of boosting iterations ( as is the case for the training sample ) , it tends not to increase .",
    "further discussion of this point can be found in ref .",
    "@xcite .",
    "the boosted decision tree is an example of a relatively modern development in machine learning that has attracted substantial attention in hep .",
    "support vector machines ( svms ) represent another such development and will no doubt also find further application in particle physics ; further discussion on svms can be found in refs .",
    "@xcite and references therein .",
    "linear classifiers and neural networks will no doubt continue to play an important role , as will probability density estimation methods used to approximate the likelihood ratio .",
    "multivariate methods have the advantage of exploiting as much information as possible out of all of the quantities measured for each event . in an environment of competition between experiments",
    ", this can be a natural motivation to use them .",
    "some caution should be exercised , however , before placing too much faith in the performance of a complicated classifier , to say nothing of a combination of complicated classifiers .",
    "these may have decision boundaries that indeed exploit nonlinear features of the training data , often based on monte carlo .",
    "but if these features have never been verified experimentally , then they may or may not be present in the real data .",
    "there is thus the risk of , say , underestimating the rate of background events present in a region where one looks for signal , which could lead to a spurious discovery .",
    "simpler classifiers are not immune to such dangers either , but in such cases the problems may be easier to control and mitigate .",
    "one should therefore keep in mind the following quote , often heard in the multivariate analysis community :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ keep it simple . as simple as possible .",
    "not any simpler . _",
    " a.  einstein _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    to this we can add the more modern variant ,    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ if you believe in something you do nt understand , you suffer ,  _",
    "stevie wonder _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    having made the requisite warnings , however , it seems clear that multivariate methods will play an important role in the discoveries we hope to make at the lhc .",
    "one can easily imagine , for example , that 5-sigma evidence for new physics from a highly performant , and complicated , classifier would be regarded by the community with some scepticism .",
    "but if this is backed up by , say , 4-sigma significance from a simpler , more transparent analysis , then the conclusion would be more easily accepted , and the team that pursues both approaches may well win the race .",
    "in these lectures we have looked at two topics in statistics , bayesian methods and multivariate analysis , which will play an important role in particle physics in the coming years .",
    "bayesian methods provide important tools for analysing systematic uncertainties , where prior information may be available that does not necessarily stem solely from other measurements , but rather from theoretical arguments or other indirect means .",
    "the bayesian framework allows one to investigate how the posterior probabilities change upon variation of the prior probabilities . through this type of sensitivity analysis ,",
    "a bayesian result becomes valuable to the broader scientific community .    as experiments become more expensive and the competition more intense",
    ", one will always be looking for ways to exploit as much information as possible from the data .",
    "multivariate methods provide a means to achieve this , and advanced tools such as boosted decision trees have in recent years become widely used . and",
    "while their use will no doubt increase as the lhc experiments mature , one should keep in mind that a simple analysis also has its advantages .",
    "as one studies the advanced multivariate techniques , however , their properties become more apparent and the community will surely find ways of using them so as to maximize the benefits without excessive risk .",
    "i wish to convey my thanks to the students and organizers of the 2009 european school of high - energy physics in bautzen for a highly stimulating environment .",
    "the friendly atmosphere and lively discussions created a truly enjoyable and productive school .",
    "probabilistic inference using markov chain monte carlo methods _ , technical report crg - tr-93 - 1 , dept .  of computer science , university of toronto , available from www.cs.toronto.edu/~radford/res-mcmc.html .",
    "b.  roe _ et al .",
    "_ , boosted decision trees as an alternative to artificial neural networks for particle identification , _ nucl .",
    "methods phys .",
    "_ * a543 * ( 2005 ) 577584 ; h.j .",
    "yang , b.  roe and j.  zhu , studies of boosted decision trees for miniboone particle identification , _ nucl .",
    "methods phys .",
    "_ * a555 * ( 2005 ) 370385 ."
  ],
  "abstract_text": [
    "<S> these lectures concern two topics that are becoming increasingly important in the analysis of high energy physics ( hep ) data : bayesian statistics and multivariate methods . in the bayesian approach </S>",
    "<S> we extend the interpretation of probability to cover not only the frequency of repeatable outcomes but also to include a degree of belief . in this way we are able to associate probability with a hypothesis and thus to answer directly questions that can not be addressed easily with traditional frequentist methods . in multivariate analysis </S>",
    "<S> we try to exploit as much information as possible from the characteristics that we measure for each event to distinguish between event types . </S>",
    "<S> in particular we will look at a method that has gained popularity in hep in recent years : the boosted decision tree ( bdt ) . </S>"
  ]
}