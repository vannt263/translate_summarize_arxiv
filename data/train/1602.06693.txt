{
  "article_text": [
    "kernel machines , in enabling flexible feature space representations of data , comprise a broad and important class of tools throughout machine learning and statistics ; prominent examples include support vector machines @xcite and gaussian processes ( gps ) @xcite . at the core of most kernel machines is the need to solve linear systems involving the gram matrix @xmath0 , where the kernel function @xmath1 , parameterized by @xmath2 , implicitly specifies the feature space representation of data points @xmath3 . because @xmath4 grows with the number of data points @xmath5 , a fundamental computational bottleneck exists : storing @xmath4 is @xmath6 , and solving a linear system with @xmath4 is @xmath7 . as the need for large - scale kernel machines grows ,",
    "much work has been directed towards this scaling issue .",
    "standard approaches to kernel machines involve a factorization ( typically cholesky ) of @xmath4 , which is efficient and exact but maintains the quadratic storage and cubic runtime costs .",
    "this cost is particularly acute when adapting ( or learning ) hyperparameters @xmath2 of the kernel function , as @xmath4 must then be factorized afresh for each @xmath2 . to alleviate this burden , numerous works have turned to approximate methods @xcite or methods that exploit structure in the kernel  @xcite .",
    "approximate methods can achieve attractive scaling , often through the use of low - rank approximations to @xmath4 , but they can incur a potentially severe loss of accuracy .",
    "an alternative to factorization is found in the conjugate gradient method ( cg ) , which is used to directly solve linear systems via a sequence of matrix - vector products .",
    "any kernel structure can then be exploited to enable fast multiplications , driving similarly attractive runtime improvements , and eliminating the storage burden ( neither @xmath4 nor its factors need be represented in memory ) .",
    "unfortunately , in the absence of special structure that accelerates multiplications , cg performs no better than @xmath7 in the worst case , and in practice finite numerical precision often results in a degradation of runtime performance compared to factorization approaches .    throughout optimization , the typical approach to the slow convergence of cg",
    "is to apply preconditioners to improve the geometry of the linear system being solved @xcite . while preconditioning has been explored in domains such as spatial statistics  @xcite , the application of preconditioning to kernel matrices in machine learning has received little attention . here",
    "we design and study preconditioned conjugate gradient methods ( pcg ) for use in kernel machines , and provide a full exploration of the use of approximations of @xmath4 as preconditioners .",
    "our contributions are as follows .",
    "( i ) extending the work in @xcite , we apply a broad range of kernel matrix approximations as preconditioners .",
    "interestingly , this step allows us to exploit the important developments of _ approximate _ kernel machines to accelerate the _",
    "exact _ computation that pcg offers .",
    "( ii ) as a motivating example used throughout the paper , we analyze and provide a general framework to both learn kernel parameters and make predictions in gps .",
    "( iii ) we extend stochastic gradient learning for gps @xcite to allow any likelihood that factorizes over the data points by developing an unbiased estimate of the gradient of the approximate log - marginal likelihood .",
    "we demonstrate this contribution in making the first use of pcg for gp classification .",
    "( iv ) we evaluate datasets over a range of problem size and dimensionality . because pcg is exact in the limit of iterations ( unlike approximate techniques ) , we demonstrate a tradeoff between accuracy and computational effort that improves beyond state - of - the - art approximation and factorization approaches . in all",
    ", we show that pcg , with a thoughtful choice of preconditioner , is a competitive strategy which is possibly even superior than existing approximation and cg - based techniques for solving general kernel machines .",
    "gaussian processes ( gps ) are the fundamental building block of many probabilistic kernel machines that can be applied in a large variety of modeling scenarios @xcite . throughout the paper",
    ", we will denote by @xmath8 a set of @xmath5 input vectors and use @xmath9 for the corresponding labels .",
    "gps are formally defined as collections of random variables characterized by the property that any finite number of them is jointly gaussian distributed .",
    "the specification of a kernel function determines the covariance structure of such random variables @xmath10 in this work we focus in particular on the popular radial basis function ( rbf ) kernel @xmath11,\\ ] ] where @xmath2 represents the collection of the kernel parameters @xmath12 and @xmath13 . defining @xmath14 and @xmath15 , and assuming a zero mean gp",
    ", we have @xmath16 where @xmath4 is the @xmath17 gram matrix with elements @xmath18 .",
    "note that the kernel above and many popular kernels in machine learning give rise to dense kernel matrices .",
    "observations are then modeled through a transformation @xmath19 of a set of gp - distributed latent variables , specifying the model @xmath20      the success of nonparametric models based on kernels hinges on the adaptation of kernel parameters @xmath2 .",
    "the motivation for preconditioning begins with an inspection of the log - marginal likelihood of gp models with prior @xmath21 . in gaussian processes with a gaussian likelihood @xmath22",
    ", we have analytic forms for @xmath23 = -\\frac{1}{2 } \\log\\left(|k_{{\\mathbf{y}}}|\\right ) - \\frac{1}{2 } { \\mathbf{y}}^{{\\top } } k_{{\\mathbf{y}}}^{-1 } { \\mathbf{y}}+ \\mathrm{const,}\\ ] ] and its derivatives with respect to kernel parameters @xmath24 , @xmath25 where @xmath26 .",
    "the traditional approach involves factorizing the kernel matrix @xmath27 using the cholesky algorithm @xcite which costs @xmath7 operations .",
    "after that , all other operations cost @xmath6 except for the trace term in the calculation of @xmath28 which once again requires @xmath7 operations .",
    "similar computations are required for computing mean and variance predictions for test data  @xcite .",
    "note that the solution of a linear system is required for computing the variance at every test point .",
    "this approach is not viable for large @xmath5 and , consequently , many approaches have been proposed to approximate these computations , thus leading to approximate optimal values for @xmath2 and approximate predictions . here",
    "we investigate the possibility of avoiding approximations altogether , by arguing that for parameter optimization it is sufficient to obtain an unbiased estimate of the gradient @xmath28 . in particular , when such an estimate is available , it is possible to employ stochastic gradient optimization that has strong theoretical guarantees @xcite . in the case of gps , the problematic terms in eq .",
    "[ eq : gradient : exact ] are the solution of the linear system @xmath29 and the trace term . in this work",
    "we make use of a stochastic linear algebra result that allows for an approximation of the trace term , @xmath30 where the @xmath31 vectors @xmath32 have components drawn from @xmath33 with probability @xmath34 .",
    "verifying that this is an unbiased estimate of the trace term is straightforward considering that @xmath35 @xcite .",
    "this result shows that all it takes to calculate stochastic gradients is the ability to efficiently solve linear systems .",
    "linear systems can be iteratively solved using _ conjugate gradient _ ( cg )  @xcite .",
    "the advantage of this formulation is that we can attempt to optimize kernel parameters using stochastic gradient optimization without having to store @xmath36 and , given that the most expensive operation is now multiplying the kernel matrix by vectors , only @xmath6 computations are required",
    ". however , it is well known that the convergence of the cg algorithm depends on the condition number @xmath37 ( ratio of largest to smallest eigenvalues ) , so the suitability of this approach may also be curtailed if @xmath36 is badly conditioned . to this end , a well - known approach for improving the conditioning of a matrix , which in turn accelerates convergence , is _",
    "preconditioning_. this necessitates the introduction of a preconditioning matrix , @xmath38 , which should be chosen in such a way that @xmath39 approximates the identity matrix , @xmath40 .",
    "intuitively , this can be obtained by setting @xmath41 ; however , given that in preconditioned cg ( pcg ) we are required to solve linear systems involving @xmath38 , this choice would be no easier than solving the original system .",
    "thus we must choose @xmath38 which approximates @xmath36 as closely as possible , but which can also be easily inverted .",
    "the pcg algorithm is shown in algorithm  [ alg : pcg ] .",
    "data _ x _ , vector @xmath42 , convergence threshold @xmath43 , initial vector * * x**@xmath44 , maximum no . of iterations _",
    "@xmath45 @xmath46 @xmath47 return * x * = * * x**@xmath48 @xmath49 @xmath50",
    "@xmath51    [ alg : pcg ]      when the likelihood @xmath52 is not gaussian , it is no longer possible to analytically integrate out latent variables .",
    "instead , techniques such as gaussian approximations ( see , e.g. , @xcite ) and methods attempting to characterize the full posterior @xmath53 @xcite may be required . among the various schemes to recover tractability in the case of models with a non - gaussian likelihood",
    ", we choose the laplace approximation , as we can formulate it in a way that only requires the solution of linear systems .",
    "the gp models we consider assume that the likelihood factorizes across all data points @xmath54 .",
    "the use of cg for computing the laplace approximation has been proposed elsewhere  @xcite , but we make the first use of preconditioning and stochastic gradient estimation within the laplace approximation to compute stochastic gradients for non - conjugate models .",
    "defining @xmath55 $ ] ( a diagonal matrix ) , carrying out the laplace approximation algorithm , computing its derivatives wrt @xmath2 , and making predictions , all possess the same computational bottleneck : the solution of linear systems involving the matrix @xmath56  @xcite . for a given @xmath2",
    ", each iteration of the laplace approximation algorithm requires solving one linear system involving @xmath57 and two matrix - vector multiplications involving @xmath4 ; the linear system involving @xmath57 can be solved using cg or pcg .",
    "the laplace approximation yields the mode @xmath58 of the posterior over latent variables and offers an approximate log - marginal likelihood in the form : @xmath59 = -\\frac{1}{2 } \\log|b| - \\frac{1}{2 } \\hat{{\\mathbf{f}}}^{{\\top } } k^{-1 } \\hat{{\\mathbf{f } } } + \\log[p({\\mathbf{y}}\\mid \\hat{{\\mathbf{f}}})]\\ ] ] which poses the same computational challenges as the regression case .",
    "once again , we therefore seek an alternative way to learn kernel parameters by stochastic gradient optimization based on computing unbiased estimates of the gradient of the approximate log - marginal likelihood .",
    "this is complicated further by the inclusion of an additional `` implicit '' term accounting for the change in the solution given by the laplace approximation for a change in @xmath2 .",
    "the full derivation of the gradient is rather lengthy and is deferred to the supplementary material .",
    "nonetheless , it is worth noting that the calculation of the exact gradient involves trace terms similar to the regression case that can not be computed for large @xmath5 , and we unbiasedly estimate these using the stochastic approximation of the trace .",
    "here we consider choices for kernel preconditioners , and for the sake of clarity we focus on preconditioners for @xmath36 . unless stated otherwise , we shall consider standard _ left _ preconditioning , whereby the original problem of solving @xmath60 is transformed by applying a preconditioner , @xmath38 , to both sides of this equation .",
    "this formulation may thus be expressed as @xmath61      the nystrm method was originally proposed to approximate the eigendecomposition of kernel matrices  @xcite ; as a result , it offers a way to obtain a low rank approximation of @xmath4 .",
    "this method selects a subset of @xmath62 data ( inducing points ) collected in the set @xmath63 which are intended for approximating the spectrum of @xmath4 .",
    "the resulting approximation is @xmath64 where @xmath65 denotes the evaluation of the kernel function over the inducing points , and @xmath66 denotes the evaluation of the kernel function between the input points and the inducing points .",
    "the resulting preconditioner @xmath67 can be inverted using the matrix inversion lemma @xmath68{\\mathbf{v}},\\ ] ] which has @xmath69 complexity .",
    "the use of a subset of data for approximating a gp kernel has also been utilized in the fully and partially independent training conditional approaches ( fitc and pitc , respectively ) for approximating gp regression  @xcite . in the former case",
    ", the prior covariance of the approximation can be written as follows : @xmath70 as the name implies , this formulation enforces that the latent variables associated with @xmath63 are taken to be completely conditionally independent . on the other hand ,",
    "the pitc method extends on this approach by enforcing that although inducing points assigned to a designated block are conditionally dependent on each other , there is no dependence between points placed in different blocks : @xmath71 for the fitc preconditioner , the diagonal resulting from the training conditional can be added to the diagonal noise matrix , and the inversion lemma can be invoked as for the nystrm case .",
    "meanwhile , for the pitc preconditioner , the noise diagonal can be added to the block diagonal matrix , which can then be inverted block - by - block .",
    "once again , matrix inversion can then be carried out as before , where the inverted block diagonal matrix takes the place of @xmath72 in the original formulation .",
    "this group of preconditioners relies on approximations to @xmath4 that factorize as @xmath73 .",
    "we shall consider different ways of determining @xmath74 such that @xmath38 can be inverted at a lower cost than the original kernel matrix @xmath4 .",
    "once again , this enables us to employ the matrix inversion lemma , and express the linear system : @xmath75 ^-1 [ i - ( i + ^ ) ^-1 ^ ] .",
    "@xmath76 we now review a few methods to approximate the kernel matrix @xmath4 in the form @xmath77 .",
    "the spectral approach uses random fourier features for deriving a sparse approximation of a gp @xcite .",
    "this approach for gps was introduced in  @xcite , and relies on the assumption that stationary kernel functions can be represented as the fourier transform of non - negative measures .",
    "as such , the elements of @xmath4 can be approximated as follows : @xmath78.\\ ] ] in the equation above , the vectors @xmath79 denote the _ spectral points _ ( or _ frequencies _ ) which in the case of the rbf kernel can be sampled from @xmath80 , where @xmath81 $ ] . to the best of our knowledge ,",
    "this is the first time such an approximation has been considered for the purpose of preconditioning kernel matrices .",
    "another factorization approach that we consider in this work is the partial singular value decomposition ( svd ) method  @xcite .",
    "the svd method factorizes the original kernel matrix @xmath4 into @xmath82 , where @xmath83 is a unitary matrix and @xmath84 is a diagonal matrix of singular values . here",
    ", we shall consider a variation of this technique called _ randomized truncated _",
    "svd  @xcite , which constructs an approximate low rank svd factorization of @xmath4 using random sampling to accelerate computations .      some recent work on approximating gps has exploited the fast computation of kronecker matrix - vector multiplications when inputs are located on a cartesian grid  @xcite . unfortunately , not all datasets meet this requirement , thus limiting the widespread application of kronecker inference .",
    "to this end , ski  @xcite is an approximation technique which exploits the benefits of the kronecker product without imposing any requirements on the structure of the training data . in particular ,",
    "a grid of inducing points , @xmath63 , is constructed , and the covariance between the training data and @xmath63 is then represented as @xmath85 in this formulation , @xmath86 denotes a sparse interpolation matrix for assigning weights to the elements of @xmath65 . in this manner ,",
    "a preconditioner exploiting kronecker structure can be constructed as @xmath87 .",
    "if we consider @xmath88 , we can rewrite the ( inverse ) preconditioner as @xmath89 since this can no longer be solved directly , we solve this ( inner - loop ) linear system using the cg algorithm ( all within one iteration of the outer - loop pcg ) .",
    "for badly conditioned systems , although the complexity of the required matrix - vector multiplications is now much less than @xmath90 , the number of iterations to solve linear systems involving the preconditioner is potentially very large , and could diminish the benefits of preconditioning .",
    "an alternative to using a single subset of data involves constructing local gps over segments of the original data  @xcite .",
    "an example of such an approach is the _ block jacobi _ approximation , whereby the preconditioner is constructed by taking a block diagonal of @xmath4 and discarding all other elements in the kernel matrix . in this manner",
    ", covariance is only expressed for points within the same block , as @xmath91 the inverse of this block diagonal matrix is computationally cheap ( also block diagonal ) .",
    "however , given that a substantial amount of information contained in the original covariance matrix is ignored , this choice is intrinsically a rather crude approach .",
    "an appealing feature shared by the aforementioned preconditioners ( aside from ski ) is that their structure enables us to directly solve @xmath92 .",
    "an alternative technique for constructing a preconditioner involves adding a positive regularization parameter , @xmath93 , to the original kernel matrix , such that @xmath94  @xcite .",
    "this follows from the fact that adding noise to the diagonal of @xmath36 makes it better - conditioned , and the condition number is expected to decrease further as @xmath95 increases .",
    "nonetheless , for the purpose of preconditioning , this parameter should be tuned in such a way that @xmath38 remains a sensible approximation of @xmath36 .",
    "as opposed to the previous preconditioners , this is an instance of _ right _ preconditioning , which has the following general form @xmath96    given that it is no longer possible to evaluate @xmath92 analytically , this linear system is solved yet again using cg , such that a linear system of equations is solved at every outer iteration of the pcg algorithm . due to the potential loss of accuracy incurred while solving the inner linear systems , a variation of the standard pcg algorithm , referred to as",
    "_ flexible _",
    "pcg  @xcite , is used instead . using this approach ,",
    "a re - orthogonalization step is introduced such that the search directions remain orthogonal even when the inner system is not solved to high precision .",
    "c c c m1 cm * concrete dataset * & & & + & & &    c @xmath97 +    coordinates ( 0,0 ) ;     + & & &    m1 cm +    coordinates ( 0,0 ) ;     +    in this section , we provide an empirical exploration of these preconditioners in a practical setting .",
    "we begin by considering three datasets for regression from the uci repository @xcite , namely the concrete dataset ( @xmath98 ) , the power plant dataset ( @xmath99 ) , and the protein dataset ( @xmath100 ) . in particular",
    ", we evaluate the convergence in solving @xmath101 using iterative methods , where @xmath102 denotes the labels of the designated dataset , and @xmath36 is constructed using different configurations of kernel parameters .    with this experiment , we aim to assess the quality of different preconditioners based on how many matrix - vector products they require , which , for most approaches , corresponds to the number of iterations taken by pcg to converge .",
    "the convergence threshold is set to @xmath103 so as to roughly accept an average error of @xmath104 on each element of the solution .    for every variation",
    ", we set the parameters of the preconditioners so as to have a complexity lower than the @xmath6 cost associated with matrix - vector products ; by doing so , we can assume that the latter computations are the dominant cost for large @xmath5 .",
    "in particular , for nystrm - type methods , we set @xmath105 inducing points , so that when we invert the preconditioner using the matrix inversion lemma , the cost is in @xmath106 .",
    "similarly , for the spectral preconditioner , we set @xmath105 random features . for the ski preconditioner ,",
    "we take an equal number of elements on the grid for each dimension ; under this assumption , kronecker products have @xmath107 cost @xcite , and we set the size of the grid so that the complexity of applying the preconditioner matches @xmath108 , so as to be consistent with the other preconditioners . for the regularized approach ,",
    "each iteration needed to apply the preconditioner requires one matrix - vector product , and we add this to the overall count of such computations . for this preconditioner",
    ", we add a diagonal offset @xmath95 to the original matrix , equivalent to two orders of magnitude greater than the noise of the process .",
    "in general , although the complexity of pcg is indeed no different from that of cg , we emphasize that experiencing a 2-fold or 5-fold ( in some cases even an order of magnitude ) improvement can be very substantial when plain cg takes very long to converge or when the dataset is large .",
    "we focus on an isotropic rbf variant of the kernel in eq .",
    "[ fig : comparison : preconditioners ] , fixing the marginal variance @xmath12 to one .",
    "we vary the length - scale parameter @xmath109 and the noise variance @xmath110 in @xmath111 scale . the top part of fig .",
    "[ fig : comparison : preconditioners ] shows the number of iterations that the standard cg algorithm takes , where we have capped the number of iterations to 100,000 .",
    "the bottom part of the figure reports the improvement offered by various preconditioners measured as @xmath112 it is worth noting that when both cg and pcg fail to converge within the upper bound , the improvement will be marked as 0 , i.e. neither a gain or a loss within the given bound .",
    "the results plotted in fig .  [ fig : comparison : preconditioners ] indicate that the low - rank preconditioners ( pitc , fitc and nystrm ) achieve significant reductions in the number of iterations for each dataset , and all approaches work best when the lengthscale is longer , characterising smoother processes .",
    "in contrast , preconditioning seems to be less effective when the lengthscale is shorter , corresponding to a kernel matrix that is more sparse .",
    "however , for cases yielding positive results , the improvement is often in the range of an order of magnitude , which can be substantial when a large number of iterations is required by the cg algorithm .",
    "the results also confirm that , as alluded to in the previous section , block jacobi preconditioning is generally a poor preconditioner , particularly when the corresponding kernel matrix is dense .",
    "the only minor improvements were observed when cg itself converges quickly , in which case preconditioning serves very little purpose either way .    the regularization approach with flexible conjugate gradient does not appear to be effective in any case either , particularly due to the substantial amount of iterations required for solving an inner system at every iteration of the pcg algorithm .",
    "this implies that introducing additional small jitter to the diagonal does not necessarily make the system much easier to solve , whilst adding an overly large offset would negatively impact convergence of the outer algorithm .",
    "one could assume that tuning the value of this parameter could result in slightly better results ; however , preliminary experiments in this regard yielded only minor improvements .",
    "the results for ski preconditioning are similarly discouraging at face value .",
    "when the matrix @xmath36 is very badly conditioned , an excessive number of inner iterations are required for every iteration of outer pcg .",
    "this greatly increases the duration of solving such systems , and as a result , this method was not included in the comparison for the protein dataset , where it was evident that preconditioning the matrix in this manner would not yield satisfactory improvements . notwithstanding that these experiments depict a negative view of ski preconditioning",
    ", it must be said that we assumed a fairly simplistic interpolation procedure in our experiments , where each data point was mapped to nearest grid location .",
    "the size of the constructed grid is also hindered considerably by the constraint imposed by our upper bound on complexity .",
    "conversely , more sophisticated interpolation strategies or even grid formulation procedures could possibly speed up the convergence of cg for the inner systems . in line with this thought , however , one could argue that the preconditioner would no longer be straightforward to construct , which goes against our innate preference towards easily derived preconditioners .",
    "cccc & & & + & & & +    -0.2 in    one of the primary objectives of this work is to reformulate gp regression and classification in such a way that preconditioning can be effectively exploited . in section  [ sec :",
    "gps ] , we demonstrated how preconditioning can indeed be applied to gp regression problems , and also proposed a novel way of rewriting gp classification in terms of solving linear systems ( where preconditioning can thus be employed ) .",
    "we can now evaluate how the proposed preconditioned gp techniques compare to other state of the art methods .    to this end",
    ", in this section , we empirically report on the generalization ability of gps as a function of the time taken to optimize parameters @xmath2 and compute predictions .",
    "in particular , for each of the methods featured in our comparison , we iteratively run the optimization of kernel parameters for a few iterations and predict on unseen data , and assess how prediction accuracy varies over time for different methods .    the analysis provided in this report is inspired by @xcite , although we do not propose an _ approximate _ method to learn gp kernel parameters .",
    "instead , we put forward a means of accelerating the optimization of kernel parameters _ _ without any approximation__. given the predictive mean and variance for the @xmath113 test points , say @xmath114 and @xmath115 , we report two error measures , namely the root mean square error , @xmath116 along with the negative log - likelihood on the test data , @xmath117 , $ ] where @xmath118 denotes the label of the @xmath119th of @xmath113 data points . for classification , instead of",
    "the rmse we report the error rate of the classifier .",
    "we can make use of stochastic gradients for gp models to optimize kernel parameters using off - the - shelf stochastic gradient optimization algorithms . in order to reduce the number of parameters to tune ,",
    "we employ adagrad  @xcite  an optimization algorithm having a single step - size parameter . for the purpose of this experiment , we do not attempt to optimize this parameter , since this would require additional computations .",
    "nonetheless , our experience with training gp models indicates that the choice of this parameter is not critical : we set the step - size to one .",
    "[ fig : error_vs_time ] shows the two error measures over time for a selection of approaches . in the figure ,",
    "pcg and cg refer to stochastic gradient optimization of kernel parameters using adagrad , where linear systems are solved with pcg and cg , respectively . in view of the results obtained in our comparison of preconditioners , we decide to proceed with the nystrm preconditioning method .",
    "furthermore , we construct the preconditioner with @xmath120 points randomly selected from the input data at each iteration , such that the overall complexity of the pcg method matches plain cg . for these methods ,",
    "stochastic estimates of trace terms are carried out using @xmath121 random vectors .",
    "the baseline chol method refers to the optimization of kernel parameters using the l - bfgs algorithm , where the exact log - marginal likelihood and its gradient are calculated using the full cholesky decomposition of @xmath36 or @xmath57 .    alongside these approaches for optimizing kernel parameters without approximation",
    ", we also evaluate the performance of approximate gp methods .",
    "for this experiment , we chose to compare against approximations found in the software package gpstuff @xcite , namely the fully and partial independent training conditional approaches ( fitc , pitc ) , and the sparse variational gp ( var ) @xcite . in order to match the computational cost of cg / pcg , which is in @xmath6 , we set the number of inducing points for the approximate methods to be @xmath122 .",
    "all methods are initialized from the same set of kernel parameters , and the curves are averaged over @xmath123 folds ( @xmath124 for the protein and eeg datasets ) .",
    "for the sake of integrity , we ran each method in the comparison individually on a workstation with intel xeon e5 - 2630 cpu having 16 cores and 128 gb ram .",
    "we also ensured that all methods reported in the comparison used optimized linear algebra routines exploiting the multi - core architecture .",
    "this diligence for ensuring fairness gives credence to our assumption that the timings are not affected by external factors other than the actual implementation of the algorithms .",
    "the cg , pcg and chol approaches have been implemented in r ; the fact that the approximate methods were implemented in a different environment ( gpstuff is written in matlab / octave ) and by a different developer may cast some doubt on the correctness of directly comparing results .",
    "however , we believe that the key point emerging from this comparison is that preconditioning feasibly enables the use of iterative approaches for optimization of kernel parameters in gps , and the results are competitive with those achieved using popular gp software packages .    for the reported experiments , it was possible to store the kernel matrix @xmath4 for all datasets , making it possible to compare methods against the baseline gp where computations use cholesky decompositions .",
    "we stress , however , that iterative approaches based on cg / pcg can be implemented without the need to store @xmath4 , whereas this is not possible for approaches that attempt to factorize @xmath4 exactly .",
    "it is also worth noting that for the cg / pcg approach , calculating the log - likelihood on test data requires solving one linear system for each test point ; this clearly penalizes the speed of these methods given the set - up of the experiment , where predictions are carried out every fixed number of iterations .",
    "careful attention to numerical properties is essential in scaling machine learning to large and realistic datasets .",
    "here we have introduced the use of preconditioning to the implementation of kernel machines , specifically , prediction and learning of kernel parameters for gps .",
    "our novel scheme permits the use of any likelihood that factorizes over the data points , allowing us to tackle both regression and classification .",
    "we have shown robust performance improvements , in both accuracy and computational cost , over a host of state - of - the - art approximation methods for kernel machines .",
    "notably , our method is exact in the limit of iterations , unlike approximate alternatives .",
    "we have also shown that the use of pcg is competitive with exact cholesky decomposition in modestly sized datasets , when the cholesky factors can be feasibly computed . when data and thus the kernel matrix grow large enough , cholesky factorization becomes unfeasible , leaving pcg as the optimal choice .",
    "one of the key features of a pcg implementation is that it does not require storage of any @xmath125 objects .",
    "we plan to extend our implementation to compute the elements of @xmath4 on the fly in one case , and in another case store @xmath4 in a distributed fashion ( e.g. in tensorflow / spark ) .",
    "furthermore , while we have focused on solving linear systems , we can also use preconditioning for other iterative algorithms involving the @xmath4 matrix , e.g. , those to solve @xmath126 and @xmath127 @xcite , as is often useful in estimating marginal likelihoods for probabilistic kernel models like gps .",
    "kc and mf are grateful to pietro michiardi and daniele venzano for assisting the completion of this work by providing additional computational resources for running the experiments .",
    "jpc acknowledges support from the sloan foundation , the simons foundation ( scgb#325171 and scgb#325233 ) , and the grossman center at columbia university .",
    "32 [ 1]#1 [ 1]`#1 ` urlstyle [ 1]doi : # 1    anitescu , m. , chen , j. , and wang , l. .",
    "_ siam journal on scientific computing _ , 340 ( 1):0 a240a262 , 2012 .",
    "ashby , s.  f. and falgout , r.  d. a parallel multigrid preconditioned conjugate gradient algorithm for groundwater flow simulations . _ nuclear science and engineering _ , 1240 ( 1):0 145159 , 1996 .",
    "asuncion , a. and newman , d.  j. machine learning repository , 2007 .",
    "url http://archive.ics.uci.edu/ml .",
    "candela , j.  q. and rasmussen , c.  e. .",
    "_ journal of machine learning research _ , 6:0 19391959 , 2005 .",
    "chalupka , k. , williams , c. k.  i. , and murray , i. a framework for evaluating approximation methods for gaussian process regression .",
    "_ journal of machine learning research _ , 140 ( 1):0 333350 , 2013 .",
    "chen , j. , anitescu , m. , and saad , y. .",
    "_ siam journal on scientific computing _ , 330 ( 1):0 195222 , 2011 .",
    "chen , k. _ matrix preconditioning techniques and applications_. cambridge monographs on applied and computational mathematics .",
    "cambridge university press , 2005 .",
    "davies , a. _ effective implementation of gaussian process regression for machine learning_. phd thesis , university of cambridge , 2014 .",
    "duchi , j. , hazan , e. , and singer , y. . _ journal of machine learning research _ , 12:0 21212159 , 2011 .",
    "filippone , m. and engler , r. enabling scalable stochastic gradient - based inference for gaussian processes by employing the unbiased linear system solver ( ulisse ) . in blei , d. and bach , f. ( eds . ) , _ proceedings of the 32nd international conference on machine learning _ , volume  37 of _ jmlr proceedings _ , pp .   10151024 , 2015 .",
    "filippone , m. , zhong , m. , and girolami , m. a comparative evaluation of stochastic - based inference methods for gaussian process models .",
    "_ machine learning _",
    ", 930 ( 1):0 93114 , 2013 .",
    "flaxman , s. , wilson , a. , neill , d. , nickisch , h. , and smola , a. fast kronecker inference in gaussian processes with non - gaussian likelihoods . in blei ,",
    "d. and bach , f. ( eds . ) , _ proceedings of the 32nd international conference on machine learning _ , volume  37 of _ jmlr proceedings _ , pp .   607616 , 2015 .",
    "gibbs , m.  n. _ bayesian gaussian processes for regression and classification_. phd thesis , university of cambridge , 1997 .",
    "gilboa , e. , saatci , y. , and cunningham , j.  p. . _ ieee transactions on pattern analysis and machine intelligence _ , 370 ( 2):0 424436 , 2015 .",
    "golub , g.  h. and van  loan , c.  f. _ matrix computations_. the johns hopkins university press , 3rd edition , 1996 .",
    "halko , n. , martinsson , p.  g. , and tropp , j.  a. .",
    "_ siam review _ , 530 ( 2):0 217288 , 2011 .    kuss , m. and rasmussen , c.  e. .",
    "_ journal of machine learning research _ , 6:0 16791704 , 2005 .",
    "lzaro - gredilla , m. , quinonero - candela , j. , rasmussen , c.  e. , and figueiras - vidal , a.  r. .",
    "_ journal of machine learning research _ , 11:0 18651881 , 2010 .",
    "murray , i. , adams , r.  p. , and mackay , d. j.  c. elliptical slice sampling . in teh ,",
    "y.  w. and titterington , d.  m. ( eds . ) , _ proceedings of the thirteenth international conference on artificial intelligence and statistics _ , volume  9 of _ jmlr proceedings _ , pp .   541548 , 2010 .",
    "nickisch , h. and rasmussen , c.  e. .",
    "_ journal of machine learning research _ , 9:0 20352078 , 2008 .",
    "notay , y. . _ siam journal on scientific computing _ , 220 ( 4):0 14441460 , 2000 .    rahimi , a. and recht , b. random features for large - scale kernel machines .",
    "in platt , j.  c. , koller , d. , singer , y. , and roweis , s.  t. ( eds . ) , _ advances in neural information processing systems 20 _ , pp . 11771184 , 2008 .",
    "rasmussen , c.  e. and williams , c. _ gaussian processes for machine learning_. mit press , 2006 .",
    "robbins , h. and monro , s. .",
    "_ the annals of mathematical statistics _ , 22:0 400407 , 1951 .",
    "schlkopf , b. and smola , a.  j. _ learning with kernels : support vector machines , regularization , optimization , and beyond_. mit press , cambridge , ma , usa , 2001 .",
    "snelson , e. and ghahramani , z. . in meila , m. and shen , x. ( eds . ) ,",
    "_ proceedings of the 11th international conference on artificial intelligence and statistics _ , volume  2 of _ jmlr proceedings _ , pp .   524531 , 2007 .",
    "srinivasan , b.  v. , hu , q. , gumerov , n.  a. , murtugudde , r. , and duraiswami , r. , 2014 .",
    "arxiv:1408.1237 .",
    "stein , m.  l. , chen , j. , and anitescu , m. difference filter preconditioning for large covariance matrices .",
    "_ siam journal on matrix analysis applications _",
    ", 330 ( 1):0 5272 , 2012 .",
    "titsias , m.  k. . in dyk , d.  a. and welling , m. ( eds . ) , _ proceedings of the 12th international conference on artificial intelligence and statistics _ , volume  5 of _ jmlr proceedings _ , pp .   567574 , 2009 .",
    "vanhatalo , j. , riihimki , j. , hartikainen , j. , jylnki , p. , tolvanen , v. , and vehtari , a. stuff : bayesian modeling with gaussian processes .",
    "_ journal of machine learning research _ , 140 ( 1):0 11751179 , 2013 .",
    "williams , c. k.  i. and seeger , m.  w. using the nystrm method to speed up kernel machines . in leen , t.  k. , dietterich , t.  g. , and tresp , v. ( eds . ) , _ advances in neural information processing systems 13 _ , pp .   682688 , 2000 .",
    "wilson , a. and nickisch , h. . in blei , d. and bach , f. ( eds . ) , _ proceedings of the 32nd international conference on machine learning _ ,",
    "volume  37 of _ jmlr proceedings _ , pp .   17751784 , 2015 .",
    "in fig .  [ fig : error_vs_time : supplement ] we report some of the runs that we did not include in the main text for lack of space .",
    "the figure reports plots on the error vs.  time for the same regression cases considered in the main text but with an isotropic kernel , and results on the concrete dataset with isotropic and ard kernels .",
    "cc & + & +    -0.2 in",
    "in this section we report the derivations of the quantities needed to compute an unbiased estimate of the log - marginal likelihood given by the laplace approximation for gp models with non - gaussian likelihood functions . throughout this section",
    ", we assume a factorizing likelihood @xmath128 and we specialize the equations to the probit likelihood @xmath129 where @xmath74 denotes the cumulative function of the gaussian density .",
    "the latent variables @xmath130 are given a zero mean gp prior @xmath131 .    for a given value of the hyperparameters @xmath2 , define @xmath132 + \\log[p({\\mathbf{f}}\\mid { \\boldsymbol{\\theta } } ) ] + \\mathrm{const.}\\ ] ] as the logarithm of the posterior density over @xmath130 . performing a laplace approximation amounts in defining a gaussian @xmath133 , such that @xmath134 as it is not possible to directly solve the maximization problem in equation  [ eq : fhat ] , an iterative procedure based on the following newton - raphson formula is usually employed , @xmath135 starting from some initial @xmath130 until convergence .",
    "the gradient and the hessian of the log of the target density are @xmath136 - k^{-1 } { \\mathbf{f}}\\text{\\quad and } \\ ] ] @xmath137 - k^{-1 } = -w - k^{-1},\\ ] ] where we have defined @xmath55 $ ] , which is diagonal because the likelihood factorizes over observations .",
    "note that if @xmath138 $ ] is concave , such as in probit classification , @xmath139 has a unique maximum .",
    "we can rewrite the inverse of the negative hessian using the matrix inversion lemma : @xmath141 where @xmath142 this means that each iteration becomes : @xmath143).\\ ] ] we can define @xmath144)$ ] and rewrite this expression as : @xmath145 from this , we see that at convergence @xmath146 as we will see later , the definition of @xmath147 is useful for the calculation of the gradient and for predictions .",
    "proceeding with the calculations from right to left we see that in order to complete a newton - raphson iteration the expensive operations are : ( i ) carry out one matrix - vector multiplication @xmath148 , ( ii ) solve a linear system involving the @xmath57 matrix , and ( iii ) carry out one matrix - vector multiplication involving @xmath4 and the vector in the parenthesis . calculating @xmath149 and performing any multiplications of @xmath150 with vectors cost @xmath151 .",
    "all these operations can be carried out without the need to store @xmath4 or any other @xmath17 matrices . the linear system in ( ii )",
    "can be solved using the cg algorithm that involves repeatedly multiplying @xmath57 ( and therefore @xmath4 ) with vectors .",
    "the laplace approximation yields an approximate log - marginal likelihood in the following form : @xmath59 = -\\frac{1}{2 } \\log|b| - \\frac{1}{2 } \\hat{{\\mathbf{f}}}^{{\\top } } k^{-1 } \\hat{{\\mathbf{f } } } + \\log[p({\\mathbf{y}}\\mid \\hat{{\\mathbf{f}}})]\\ ] ] handy relationships that we will be using in the remainder of this section are : @xmath160 @xmath161    the gradient of the log - marginal likelihood with respect to the kernel parameters @xmath2 requires differentiating the terms that explicitly depend on @xmath2 and those that implicitly depend on it because a change in the parameters reflects in a change in @xmath58 . denoting by @xmath28 the @xmath119th component of the gradient of @xmath162}{\\partial \\theta_i}$ ]",
    ", we obtain @xmath163 \\right]^{{\\top } } \\frac{\\partial \\hat{{\\mathbf{f}}}}{\\partial \\theta_i } \\end{aligned}\\ ] ]    the trace term can not be computed exactly for large @xmath5 so we propose a stochastic estimate : @xmath164 } =   - \\frac{1}{2 n_{{\\mathbf{r } } } } \\sum_{i=1}^{n_{{\\mathbf{r } } } }   ( { \\mathbf{r}}^{(i)})^{{\\top } } b^{-1 } \\frac{\\partial b}{\\partial \\theta_i } { \\mathbf{r}}^{(i)}.\\ ] ] by noticing that the derivative of @xmath57 is @xmath165 , this simplifies to @xmath166 so we need to solve @xmath31 linear systems involving @xmath57 .",
    "data @xmath152 , labels @xmath102 , @xmath58 , @xmath147 @xmath168 for @xmath169 compute first term of @xmath170 compute second term of @xmath170 @xmath171 for @xmath169 compute @xmath172 @xmath173)$ ] compute third term of @xmath170 @xmath174      the last ( implicit ) term in the last equation can be simplified by noticing that : @xmath175 = \\psi(\\hat{{\\mathbf{f } } } ) - \\frac{1}{2 } \\log|b|\\ ] ] and that the derivative of the first term wrt @xmath58 is zero because @xmath58 maximizes @xmath176 . therefore : @xmath177 \\right]^{{\\top } } \\frac{\\partial \\hat{{\\mathbf{f}}}}{\\partial \\theta_i } = - \\frac{1}{2 } \\left[\\nabla_{\\hat{{\\mathbf{f } } } }   \\log|b| \\right]^{{\\top } } \\frac{\\partial \\hat{{\\mathbf{f}}}}{\\partial \\theta_i}\\ ] ] the components of @xmath178 $ ] can be obtained by considering the identity @xmath179 , so differentiating @xmath180 wrt the components of @xmath58 becomes : @xmath181 we can rewrite this by gathering @xmath4 inside the inverse and , due to the inversion of the matrix product , @xmath4 cancels out : @xmath182 we notice here that the resulting trace contains the inverse of the same matrix needed in the iterations of the laplace approximation and that the matrix @xmath183 is zero everywhere except in the @xmath184th diagonal element where it attains the value : @xmath185}{\\partial ( \\hat{{\\mathbf{f}}})^3_j}\\ ] ] for this reason , it would be possible to simplify the trace term as the product between the @xmath184th diagonal element of @xmath186 and @xmath187}{\\partial ( \\hat{{\\mathbf{f}}})^3_j}$ ] .",
    "bearing in mind that we need @xmath5 of these quantities , we could define @xmath188\\right]\\ ] ] @xmath189}{\\partial ( \\hat{{\\mathbf{f}}})^3_j}\\ ] ] and rewrite @xmath190 = - \\frac{1}{2 } d { \\mathbf{d}}\\ ] ] which is the standard way to proceed when computing the gradient of the approximate log - marginal likelihood using the laplace approximation @xcite",
    ". however , this would be difficult to compute exactly for large @xmath5 , as this would require inverting @xmath191 first and then compute its diagonal . using the matrix inversion lemma would not simplify things as there would still be an inverse of @xmath57 to compute explicitly .",
    "we therefore aim for a stochastic estimate of this term starting from : @xmath192   \\right ) \\nonumber \\\\\\end{aligned}\\ ] ] where we have introduced the @xmath193 vectors with the property @xmath194 = i$ ] .",
    "so an unbiased estimate of the trace for each component of @xmath58 is : @xmath195 } \\nonumber \\\\ & = & \\frac{1}{n_{{\\mathbf{r } } } } \\sum_{i=1}^{n_{{\\mathbf{r } } } }   ( { \\mathbf{r}}^{(i)})^{{\\top } } ( k^{-1 } + w)^{-1 } \\frac{\\partial w}{\\partial ( \\hat{{\\mathbf{f}}})_j } { \\mathbf{r}}^{(i ) }    \\nonumber \\\\\\end{aligned}\\ ] ] which requires solving @xmath31 linear systems involving the @xmath57 matrix : @xmath196    the derivative of @xmath58 wrt @xmath24 can be obtained by differentiating the expression @xmath197 $ ] : @xmath198 + k \\nabla_{\\hat{{\\mathbf{f } } } } \\nabla_{\\hat{{\\mathbf{f } } } } \\log[p({\\mathbf{y}}\\mid \\hat{{\\mathbf{f } } } ) ] \\frac{\\partial \\hat{{\\mathbf{f}}}}{\\partial \\theta_i}\\ ] ] given that @xmath199 = -w$ ] we can rewrite : @xmath200\\ ] ] which yields : @xmath201\\ ] ]            to obtain an approximate predictive distribution , conditioned on a value of the hyperparameters @xmath2 , we can compute : @xmath210 given the properties of multivariate normal variables , @xmath211 is distributed as @xmath212 with @xmath213 and @xmath214 .",
    "approximating @xmath215 with a gaussian @xmath216 makes it possible to analytically perform integration with respect to @xmath130 in eq .",
    "[ eq : approx : integration : f ] .",
    "in particular , the integration with respect to @xmath130 yields @xmath217 with @xmath218 and @xmath219 these quantities can be rewritten as : @xmath220 and @xmath221 this shows that the mean is cheap to compute , whereas the variance requires solving another linear system involving @xmath57 for each test point .",
    "when a low rank approximation of the matrix @xmath4 is available , say @xmath73 , the inverse of the preconditioner can be rewritten as : @xmath223 by using the matrix inversion lemma we obtain : @xmath224 similarly to the gp regression case , the application of this preconditioner is in @xmath69 , where @xmath225 is the rank of @xmath74 ."
  ],
  "abstract_text": [
    "<S> the computational and storage complexity of kernel machines presents the primary barrier to their scaling to large , modern , datasets . a common way to tackle </S>",
    "<S> the scalability issue is to use the conjugate gradient algorithm , which relieves the constraints on both storage ( the kernel matrix need not be stored ) and computation ( both stochastic gradients and parallelization can be used ) . </S>",
    "<S> even so , conjugate gradient is not without its own issues : the conditioning of kernel matrices is often such that conjugate gradients will have poor convergence in practice . </S>",
    "<S> preconditioning is a common approach to alleviating this issue . </S>",
    "<S> here we propose preconditioned conjugate gradients for kernel machines , and develop a broad range of preconditioners particularly useful for kernel matrices . </S>",
    "<S> we describe a scalable approach to both solving kernel machines and learning their hyperparameters . </S>",
    "<S> we show this approach is exact in the limit of iterations and outperforms state - of - the - art approximations for a given computational budget . </S>"
  ]
}