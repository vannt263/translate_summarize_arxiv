{
  "article_text": [
    "there exists a substantial body of literature on transmission schemes for memoryless channels with noiseless feedback .",
    "horstein  @xcite proposed a simple sequential transmission scheme which is capacity - achieving and provides larger error exponents than traditional fixed - length block - coding for discrete memoryless channels ( dmcs ) .",
    "similarly , schalkwijk and kailath  @xcite showed that capacity and a double exponentially decreasing error probability can be achieved by a simple sequential transmission scheme for the additive white gaussian noise ( awgn ) channel with average power constraint .",
    "a remarkable result was derived by burnashev in  @xcite , where error exponent matching upper and lower bounds were derived for dmcs with feedback and variable - length codes .",
    "the error exponent has a simple form @xmath0 , where @xmath1 is the average rate , @xmath2 is the channel capacity and @xmath3 is the maximum divergence that can be obtained in the channel for a binary hypothesis testing problem .",
    "recently , shayevitz and feder  @xcite identified an underlying principle shared by the aforementioned schemes and introduced a simple encoding scheme , namely the posterior matching ( pm ) scheme for general memoryless channels and showed that it achieves capacity .",
    "the above transmission schemes can be contrasted to those inspired by the work in  @xcite where a variable - length transmission scheme was proposed and its error exponent was found to achieve the burnashev upper bound .",
    "this scheme ( and others inspired by it ) is not explicit in the sense that it assumes that some unspecified capacity - achieving codes are used in the `` communication '' stage of the transmission .    for channels with memory and noiseless feedback",
    ", there exists a rich literature on the capacity characterization  @xcite .",
    "recently , the capacity of the trapdoor channel was found in closed form in  @xcite , and extended to a subset of chemical channels in  @xcite , while the capacity of the binary unit memory channel on the output was found in closed form in  @xcite .",
    "a number of `` explicit '' transmission schemes have been recently studied in the literature  @xcite but no results on error exponents are reported . in the case of channels with memory and feedback",
    ", an error exponent analysis is performed in  @xcite for fixed length coding .",
    "the only work that studies error exponents for variable - length codes for channels with memory and feedback is  @xcite where the authors consider a finite state channels with channel state known causally to both the transmitter and the receiver .",
    "the transmission scheme presented therein is inspired by that of  @xcite and as a result it is based on an otherwise unspecified capacity - achieving code for this channel .    in this work , we consider channels with memory and feedback , and propose and analyze variable - length transmission schemes .",
    "we specifically look at unifilar channels since for this family , the capacity has been characterized in an elegant way through the use of markov decision processes ( mdps )  @xcite .",
    "we consider two transmission schemes with increasing degree of complexity .",
    "the first one is a single - stage sequential transmission scheme , similar to the one proposed in  @xcite .",
    "the encoding is a time - invariant function that depends on a summary of the available common information between the transmitter and the receiver in the form of two @xmath4-dimensional vectors : one is the vector of current states conditioned on each message and the other is the posterior probability mass function of the message given the observation ( with @xmath4 being the number of messages ) .",
    "the second one is a two - stage transmission scheme where the first stage is as described above , and the second stage , which is entered when one of the messages is sufficiently reliable , is resolving a binary hypothesis testing problem much like the original scheme of burnashev . following the hints from our upper bound analysis in the companion paper  @xcite , the second stage employs a more sophisticated transmission scheme ( compared to that for dmcs ) in order to achieve the error - exponent upper bound .",
    "the analysis assumes the presence of some common randomness shared by the transmitter and receiver , and is based on the study of the log - likelihood ratio of the transmitted message posterior belief , and in particular on the study of its multi - step drift .",
    "our results are derived under an additional assumption / conjecture on the concentration of an appropriately defined markov process .",
    "although this part is currently unresolved , we provide strong evidence supporting this assumption through numerical evaluations .",
    "simulation results for the trapdoor , chemical , and other binary input / state / output unifilar channels confirm that the bounds are tight compared to the upper bounds derived in  @xcite .    the main difference between our work and that in  @xcite is that for unfilar channels , the channel state is not observed at the receiver . in addition , in this work , an `` explicit '' transmission scheme is proposed and analyzed .    the remaining part of this paper is organized as follows . in section  [ sec : model ] , we describe the channel model for the unifilar channel and its capacity characterization . in section  [ sec : singlestage ]",
    ", we propose and analyze a single - stage transmission scheme with common randomness . in section",
    "[ sec : twostage ] , we propose and analyze a two - stage transmission scheme with common randomness .",
    "section  [ sec : example ] presents numerical evidence for the performance of the proposed schemes for several unifilar channels .",
    "final conclusions are given in section  [ sec : conclusions ] .",
    "* all proofs are omitted due to space limitations .",
    "they can be found on the extended version of the paper  @xcite . *",
    "consider a family of finite - state point - to - point channels with inputs @xmath5 , output @xmath6 and state @xmath7 at time @xmath8 , with all alphabets being finite and @xmath9 known to both the transmitter and the receiver .",
    "the channel conditional probability is @xmath10 for a given stochastic kernel @xmath11 and deterministic function @xmath12 , where @xmath13 denotes the space of all probability measure on @xmath14 , and @xmath15 is the kronecker delta function .",
    "this family of channels is referred to as unifilar channels  @xcite .",
    "the authors in  @xcite have derived the capacity @xmath2 in the form of @xmath16 the capacity can be written as an optimal reward per unit time of an appropriately defined mdp  @xcite . for channels with ergodic behavior , the capacity has a single - letter expression @xmath17 where @xmath18 is the posterior belief on the current state given @xmath19 at time @xmath8 and the mutual information is evaluated using the distribution @xmath20 where @xmath21 is the stationary distribution of the markov chain @xmath22 with transition kernel @xmath23 for an appropriately defined update function  @xcite . where the update function @xmath24 is defined through the recursion @xmath25 in this paper , we restrict our attention to such channels with strictly positive @xmath26 for any @xmath27 and ergodic behavior so that the above capacity characterization is indeed valid .",
    "in this section , we adapt burnashev s scheme  @xcite to unifilar channels . to introduce randomness into our encoding strategy , in addition to the structure described in section  [ sec : model ] , the transmitter and the receiver have access to a common set of random variables @xmath28 in a causal way .",
    "let @xmath29 and @xmath30 be the message to be transmitted and the target error probability . in this system ,",
    "the transmitter receives perfect feedback of the output with unit delay and determines the input @xmath31 with ( deterministic ) encoding strategies @xmath32 at time @xmath8 .",
    "observe that the encoding strategies are deterministic and this is a crucial part of the analysis , since it allows the receiver to have an estimate of the current state and input for any hypothesized message .",
    "the common random variables are utilized to induce the appropriate maximizing input distribution .",
    "define the filtration @xmath33 and define the posterior probability of the massage , @xmath34 as @xmath35 we now specify in more detail the encoding strategy .",
    "suppose we are given a collection of input distributions @xmath36 that maximize  .",
    "we define the vector @xmath37 , where @xmath38 is the state at time @xmath8 conditioning on @xmath39 and thus can be written as @xmath40 since deterministic strategies are used .",
    "we further define for any @xmath41 and @xmath42 the quantities @xmath43 which are almost surely equal to @xmath44 and @xmath45 , respectively .",
    "the common random variables @xmath46^{|\\cs|}\\}$ ] are generated as @xmath47 where @xmath48 denotes the uniform distribution .",
    "the transmission scheme is a generalization of burnashev s scheme  @xcite and also very similar to the pm scheme  @xcite .",
    "first , based on the quantity @xmath49 , and @xmath50 the conditional distribution on the state is evaluated as in  .",
    "then the input signal @xmath51 is generated exactly as in dmc from @xmath52 to  match \" the input distribution @xmath53 .",
    "figure  [ fig : encodingstrategy ] illustrates the encoding strategy for binary unifilar channels with @xmath54 . at that situation , @xmath51 is deterministic when @xmath55 , or @xmath56 . when @xmath57 , the output @xmath51 depends on @xmath58 . if @xmath59 , then @xmath60 .",
    "otherwise the transmitter sends @xmath61 .",
    "if there exists an @xmath62 such that    @xmath63    then @xmath64 , otherwise @xmath65 we can write @xmath66 . for any @xmath67 , the quantities @xmath68 and @xmath69 can be updated as    @xmath70    which can be concisely written as    @xmath71    and @xmath72 can be generated according to  .",
    "regarding decoding , the receiver estimates the message by @xmath73 at time @xmath8 .",
    "there is a stopping time @xmath74 which is the first time that @xmath75 is greater than @xmath76 , where @xmath77 is a pre - specified system parameter ( the target probability of error ) . at this time transmission stops and",
    "the decoded message is declared to be @xmath78 .",
    "the average transmission rate of this system is defined as @xmath79}$ ] . the drift analysis provided below shows that @xmath74 is almost surely finite and therefore @xmath80 .",
    "the error probability of this scheme is given by @xmath81 where ( a ) is due to that @xmath73 and @xmath82 .",
    "the drift analysis provided below shows that @xmath74 is almost surely finite and therefore @xmath80 . inspired by burnashev",
    "s method , we analyze the log - likelihood ratio @xmath83 at time @xmath8 , defined by @xmath84 and stopping time @xmath85 given by @xmath86 by definition we have @xmath87 almost surely . to derive a lower bound on error - exponent",
    ", we would like to have an upper bound on @xmath74 . in the following",
    "we derive an upper bound on @xmath85 instead of @xmath74 since @xmath85 is almost surely greater than @xmath74 and directly connected with the message @xmath88 .",
    "we first analyze the drift of @xmath83 w.r.t @xmath89 and then apply the optional sampling theorem to a proposed submartingale , in order to derive the desired result . a major difference between unifilar channels and dmcs",
    "is the presence of memory . in order to capture channel memory",
    ", we eventually need to analyze multi - step instead of one - step drift of @xmath83 .",
    "[ lemma : onestepdrift ] the one - step drift of @xmath90 w.r.t .",
    "@xmath89 is lower - bounded by @xmath91 \\geqslant    i(\\hat{b}_{t-1}),\\ ] ] where @xmath92 is given by @xmath93    please see extended version of the paper  @xcite . see appendix  [ proof : onestepdrift ] .",
    "the following remark is crucial in the subsequent development .",
    "first , unlike the case in dmc where the one - step drift is shown to be greater than the channel capacity , here the one step drift is a random variable .",
    "this is exactly the reason we consider multi - step drift analysis : under an ergodicity assumption , the arithmetic mean of these random variables will converge almost surely to their mean .",
    "this raises the question of what the mean of the process @xmath94 is .",
    "if the process @xmath95 had the same statistics as those of the markov chain @xmath96 defined in  , then convergence to @xmath2 would be guaranteed with rate independent of the parameter @xmath97 .",
    "however , the two processes have different statistics .",
    "this is because of the introduction of common randomness !",
    "indeed , @xmath98 , while @xmath99 and they are related according to @xmath100 $ ] .",
    "in fact , @xmath95 is not a markov chain , but is measurable w.r.t .",
    "the state of the markov chain @xmath101 as shown in  .",
    "we conjecture that @xmath95 converges to its steady state independently of @xmath97 .",
    "we argue that this is so since the first part of the state @xmath102 has a finite state space and its transition matrix is very sparse and only depends loosely on @xmath34 .",
    "this implies ( from random matrix theory ) that @xmath103 converges exponentially fast to its steady state , independently of @xmath104 and thus independently of @xmath97 .",
    "furthermore , this induces a steady - state distribution on @xmath105 that is exactly that of @xmath106 .",
    "our results presented in section  [ sec : example ] provide strong support for this conjecture .",
    "based on the above discussion we have @xmath107 furthermore , due to the assumption that the transition kernel @xmath26 positive for any @xmath108 , the quantity @xmath109 can be shown to be upper bounded by a certain constant @xmath110 as in  ( * ? ? ? * lemma  4 ) .",
    "now we have the result of drift analysis of our system .",
    "the relation between the drift and the stopping time , @xmath85 is given in  ( * ? ? ?",
    "* lemma , p.  230 ) . combining the above results",
    ", we are ready to state a lower bound on the error exponent .",
    "the relation between the drift and the stopping time , @xmath85 is given through the following result    ( * ? ? ?",
    "* lemma , p.  230 ) [ fact : easysubmartingale ] let @xmath111 be a submartingale w.r.t . @xmath111 which has the following properties    @xmath112 & \\geqslant k_1 \\qquad ( k_1>0 )   \\\\     and define a stopping time @xmath113 . then @xmath114 \\leqslant \\frac{b - z_0 + k_3}{k_1}.\\ ] ]    combining the above results , we are ready to state a lower bound on the error exponent .",
    "[ thm : singlestagescheme ] with @xmath115 messages and target error probability @xmath77 , given any @xmath116 we have @xmath117 } \\geqslant \\tilde{c } ( 1-\\frac{\\overline{r}}{\\tilde{c } } ) + u(\\epsilon , k,\\overline{r},\\tilde{c},c_2),\\end{aligned}\\ ] ] where @xmath118 .",
    "please see extended version of the paper  @xcite .",
    "see appendix  [ proof : singlestagescheme ] .",
    "following the ideas from  @xcite , we now consider a two - stage scheme that has the potential to achieve the upper bounds derived in  @xcite .",
    "suppose we are given the optimizing strategies relating to the mdp discussed in  @xcite .",
    "in particular we are given a policy @xmath119 and a policy @xmath120 .",
    "for now we fix a threshold @xmath121 that will be specified in the analysis .",
    "the transmission starts at stage one which is exactly the one described in the previous section .",
    "if @xmath122 exceeds the threshold @xmath121 , this implies the receiver has very high confidence that a certain message is the transmitted one .",
    "therefore the transmitter enters the second stage that determines whether the estimated message of the receiver is the true message , which is a binary hypothesis testing problem .",
    "define @xmath123 whenever @xmath124 and @xmath125 .",
    "now we specify our encoding strategy .",
    "let @xmath126 be the hypothesis that the estimation at the receiver is correct ( i.e. , @xmath127 ) , and @xmath128 be the opposite .",
    "we define the quantities @xmath129 and @xmath130 similarly to stage - one related quantities , with the only difference being that they represent posterior beliefs conditioned on @xmath128 @xmath131 under @xmath126 , the input signal is @xmath132 $ ] . under @xmath128 ,",
    "the input signal @xmath51 is generated in a similar fashion as in stage one ( as illustrated in  fig .",
    "[ fig : encodingstrategy ] ) , expect that now , the message distribution @xmath133 is used ( instead of @xmath134 ) and the input distribution @xmath135(\\cdot|s^w_t)$ ] is to be  matched \" ( instead of @xmath136 ) .",
    "we use @xmath137 and @xmath138 to denote the encoding functions for @xmath126 and @xmath128 respectively , where we make explicit the dependence on the common random variable @xmath139 used in @xmath128 . with this encoding strategy",
    ", we can update @xmath140 and @xmath141 by @xmath142 @xmath143 where @xmath144 is given by @xmath145 and @xmath146 , @xmath147 can be updated according to  , and  , respectively .",
    "we conclude the description of the encoding scheme by noting that if during stage two the posterior belief of message @xmath148 drops below the threshold then the system reverts to stage one .",
    "the analysis follows the same structure as in the first stage : we first analyze the one - step drift of @xmath83 under both hypotheses and then , arguing in the same way we did above , we consider the asymptotic behaviour of the @xmath149-step drift .",
    "[ lemma : twostagemultistepdrift ] for any @xmath150 , there exist an @xmath151 such that    @xmath152 \\geqslant n(\\tilde{c}_1-\\epsilon ) \\quad   \\text{if } l_t   \\geqslant \\log\\frac{p_0}{1-p_0}\\\\ e[l_{t+n}-l_t|\\mathcal{f}_t ] \\geqslant n(\\tilde{c}-\\epsilon ) \\quad   \\text{if }",
    "l_t   < \\log\\frac{p_0}{1-p_0}.\\end{aligned}\\ ] ]    please see extended version of the paper  @xcite .",
    "see appendix  [ proof : secondstagemultistepdrift ] .",
    "lemma  [ lemma : twostagemultistepdrift ] shows that in the second stage , the likelihood ratio grows faster than in the first stage if the estimation at the receiver is correct .",
    "even if the estimation is wrong , the likelihood ratio maintains the increasing rate as in the first stage . for this to be true",
    "we have assumed that @xmath153 .",
    "if this is not the case then an alternative scheme can be proposed as in  @xcite .",
    "we omit this description due to space limitations . as in the previous section ,",
    "the connection between the drift and the stopping time is given by  ( * ? ? ?",
    "* lemma , p.50 ) . as in the previous section",
    ", the following result connects the drift and the stopping time .",
    "* lemma , p.  50 ) [ fact:2ndstagemartingale ] suppose @xmath154 is a process with the following properties    @xmath112 & \\geqslant    k_1   \\qquad \\text{if } z_t < 0 , k_1 > 0 \\\\ e[z_{t+1}-z_t|f_t ] & \\geqslant k_2   \\qquad \\text{if }   z_t \\geqslant 0 , k_2 >",
    "k_1\\\\     z_0 & < 0.\\end{aligned}\\ ] ]    define a stopping time @xmath113 .",
    "then we have @xmath114 \\leqslant \\frac{b}{k_2 } + \\frac{|z_0|}{k_1 } + d(k_1,k_2,k_3),\\ ] ] where @xmath155 is a bounded constant .",
    "the final result is given in the following proposition .",
    "[ thm : twostagescheme ] with @xmath115 messages and target error probability @xmath77 , given any @xmath116 we have @xmath156 } \\geqslant \\tilde{c}_1(1-\\frac{\\overline{r}}{\\tilde{c } } ) + u(\\epsilon , k , pe,\\tilde{c},\\tilde{c}_1,c_2),\\ ] ] where @xmath157 .",
    "please see extended version of the paper  @xcite .",
    "see appendix  [ proof : twostagescheme ] .",
    "in this section , we provide simulation results for the error exponents achieved by the two proposed transmission schemes for some binary input / output / state unifilar channels . due to space limitations",
    ", we present results only for a symmetric unifilar channel , denoted as channel @xmath158 , where @xmath159 , and @xmath160 . * a number of additional results",
    "are presented in the extended version of the paper  @xcite*. we consider the trapdoor channel ( denoted as channel @xmath161 ) , chemical channel ( denoted as channel @xmath162 ) , and a symmetric unifilar channel ( denoted as channel @xmath158 ) .",
    "all of these channels have @xmath160 and kernel @xmath163 characterized as shown in table  [ t : q ] .",
    ".kernel definition for binary unifilar channels [ cols=\"^,^,^,^,^\",options=\"header \" , ]     [ t : q ]    we simulated a system with message length @xmath164 ( bits ) and target error rates @xmath165 . in each simulation",
    "sufficient experiments were run to have a convergent average rate , since the error probability is guaranteed to be bellow the target .",
    "infinite precision arithmetic was used in all evaluations through the  gnu multiple precision arithmetic library \" ( gmp ) .",
    "the results are shown in fig .",
    "[ fig : symmetric ] .",
    "each curve in the figure corresponds to a value of @xmath97 .",
    "all two - stage schemes were run with the policy @xmath166 and @xmath167 , which was confirmed to be optimal in the mdp of  @xcite .",
    "the results are shown in fig .",
    "[ fig : trapdoor ] , fig .",
    "[ fig : chemical ] , fig .",
    "[ fig : symmetric ] , for the three channels , respectively .",
    "each curve in these figures corresponds to a value of @xmath97 .",
    "all two - stage schemes were run with the policy @xmath166 and @xmath167 , which was confirmed to be optimal in the mdp of  @xcite .                also shown on",
    "the same figures are the error exponent upper bounds obtained in  @xcite , as well as the pairs of parameters @xmath168 for each channel .",
    "we make two main observations regarding these results .",
    "the first observation is that for the trapdoor channels there is strong evidence that the error exponents are infinite .",
    "this is consistent with the findings in  @xcite where a zero - error capacity - achieving scheme is proposed , and the discussion in  @xcite regarding channels with zeros in their transition kernels .",
    "interestingly enough , the infinite exponent is achieved even with a single - stage receiver .",
    "similar comments are valid for the chemical channel .",
    "the second observation is the remarkable agreement between simulation results of the proposed ( two - stage ) scheme and the upper bound derived in  @xcite for channel c. the analysis of the one - stage scheme seems to be quite conservative and the true drift is higher than the lower bound @xmath2 .",
    "these results represent very strong evidence for the validity of the conjecture stated earlier .",
    "we propose two variable - length transmission schemes for unifilar channels with noiseless feedback .",
    "their error exponent is analyzed by generalizing the techniques of burnashev  @xcite .",
    "both schemes have a posterior matching flavor ; the latter does posterior matching even at the second stage where a binary hypothesis is resolved , which is a unique feature of this encoding scheme compared to its dmc counterpart .",
    "the analysis assumes the presence of some common randomness shared by the transmitter and receiver , and is based on the study of the log - likelihood ratio of the transmitted message posterior belief , and in particular on the study of its multi - step drift .",
    "the theoretical results hinge on an additional assumption we make about the induced markov chain @xmath169 , i.e. , that its first component converges fast to its steady state and the corresponding empirical distribution @xmath170 has the same statistics ( asymptotically ) as those of the markov chain @xmath171 . simulation results for the trapdoor , chemical , and other binary input / state / output unifilar channels provide strong support towards the validity of this assumption and show remarkable tightness compared to the upper bounds derived in the companion paper  @xcite .    we conclude by noting that the techniques used in this work can be applied to channels with markov states and inter - symbol interference ( isi ) where the state is observed at the receiver and with unit delay at the transmitter .",
    "@xmath172\\nonumber \\\\   & = \\log(1-\\pi_{t-1}(w ) ) + e[\\log \\frac{\\pi_t(w)}{(1-\\pi_t(w))\\pi_{t-1}(w ) } |w,\\mathcal{f}_{t-1}]\\nonumber \\\\   & =   \\log(1-\\pi_{t-1}(w ) ) + e[\\log \\frac{q(y_t|x_t , s_t)}{p(y_t|v_t,\\mathcal{f}_{t-1})-q(y_t|x_t , s_t)\\pi_{t-1}(w ) } |w,\\mathcal{f}_{t-1}].\\end{aligned}\\ ] ]    we look into @xmath173 @xmath174 and after taking expectations @xmath175\\nonumber \\\\   & = e[\\sum_{x , s } q(y|x , s ) \\sum_{i=1}^m \\delta_{e(i,\\underline{s}_t,\\pi_{t-1},v^{s^i_t}_t)}(x)\\delta_{s^i_t}(s)\\pi_{t-1}(i)|w , x_t , s_t , v^{t-1},y^{t-1},s_1 ] \\nonumber \\\\ & = \\sum_{x , s } q(y|x ,",
    "s ) \\sum_{i=1}^m e[\\delta_{e(i,\\underline{s}_t,\\pi_{t-1},v^{s^i_t}_t)}(x)|w , x_t , s_t , \\mathcal{f}_{t-1}]\\delta_{s^i_t}(s)\\pi_{t-1}(i ) \\nonumber \\\\ & = \\sum_{x , s } q(y|x , s ) \\sum_{i=1}^m \\overline{e}(x|i,\\underline{s}_t,\\pi_{t-1})\\delta_{s^i_t}(s)\\pi_{t-1}(i )   \\nonumber \\\\ & = \\sum_{x , s } q(y|x , s ) ( p_x(x|s,\\hat{b}_{t-1})+\\delta_{x_t}(x)\\delta_{s_t}(s)\\pi_{t-1}(w)-\\overline{e}(x|w,\\underline{s}_t,\\pi_{t-1})\\delta_{s_t}(s)\\pi_{t-1}(w))\\nonumber \\\\ & = p(y_t = y | \\hat{b}_{t-1 } ) + q(y|x_t , s_t)\\pi_{t-1}(w ) - \\pi_{t-1}(w)\\sum_{x}q(y|x , s_t)\\overline{e}(x|w,\\underline{s}_t,\\pi_{t-1}),\\end{aligned}\\ ] ] where @xmath176 is given by @xmath177.\\end{aligned}\\ ] ] therefore , by convexity of @xmath178 , @xmath179 \\nonumber \\\\ & \\geqslant \\log(1-\\pi_{t-1}(w ) ) +   e[\\log \\frac{q(y_t|x_t , s_t)}{p(y_t|\\hat{b}_{t-1})-\\pi_{t-1}(w)\\sum_{x}q(y_t|x , s_t)\\overline{e}(x|w,\\pi_{t-1},s_t ) } |w,\\mathcal{f}_{t-1 } ] .\\end{aligned}\\ ] ] looking further into the last term in the above inequality we get @xmath180 \\nonumber \\\\ & = e [ \\log   \\frac{q(y_t|x_t , s_t)}{p(y_t|\\hat{b}_{t-1 } ) } + \\log\\frac{p(y_t|\\hat{b}_{t-1})}{p(y_t|\\hat{b}_{t-1})-\\pi_{t-1}(w)\\sum_{x}q(y_t|x , s_t)\\overline{e}(x|w,\\pi_{t-1},s_t ) } |w,\\mathcal{f}_{t-1 } ] \\nonumber \\\\ & \\geqslant e [ \\log   \\frac{q(y_t|x_t , s_t)}{p(y_t|\\hat{b}_{t-1})}|w,\\mathcal{f}_{t-1 } ] + e[\\log\\frac{1}{1-\\pi_{t-1}(w)}|w,\\mathcal{f}_{t-1 } ] , \\end{aligned}\\ ] ] where the last equation is due to the convexity of @xmath181 . combining and , we get @xmath182 & = e[e[l_t - l_{t-1}|w,\\mathcal{f}_{t-1}]|\\mathcal{f}_{t-1}]\\nonumber \\\\ & \\geqslant e[\\log   \\frac{q(y_t|x_t , s_t)}{p(y_t|\\hat{b}_{t-1})}| \\mathcal{f}_{t-1 } ] \\nonumber \\\\ & = i(\\hat{b}_{t-1}).\\end{aligned}\\ ] ]",
    "we now define a stopping time @xmath187 w.r.t to @xmath188 by @xmath189 . by definition",
    "we have @xmath190 almost surely .",
    "applying fact  [ fact : easysubmartingale ] to @xmath111 , we have @xmath191}{n}\\leqslant e[\\tilde{t } ] \\leqslant \\frac{\\log\\frac{1-pe}{pe}-k+nc_2}{n(c-\\epsilon)},\\end{aligned}\\ ] ] which after rearranging of terms becomes @xmath192      @xmath193&=   e[\\log\\frac{\\pi_{t+1}(\\hat{w})}{1-\\pi_{t+1}(\\hat{w})}-\\log \\frac{\\pi_{t}(\\hat{w})}{1-\\pi_{t}((\\hat{w})}|\\mathcal{f}_{t},h_0]\\nonumber \\\\ & = e[\\log \\frac{q(y_{t+1}|e^0(s^{\\hat{w}}_{t+1},\\hat{b}^1_{t}),s^{\\hat{w}}_{t+1})}{\\sum_{i\\neq \\hat{w } } q(y_{t+1}|e^1(s^{\\hat{w}}_{t+1},\\hat{b}^1_{t},i , s^i_{t+1},v_{t+1}),s^i_{t+1})\\frac{\\pi_{t}(i)}{1-\\pi_{t}(\\hat{w})}}|\\mathcal{f}_{t},h_0 ] \\nonumber \\\\ & = e[\\log \\frac{q(y_{t+1}|e^0(s^{\\hat{w}}_{t+1},\\hat{b}^1_{t}),s^{\\hat{w}}_{t+1})}{\\sum_{x , s}q(y_{t+1}|x , s)\\sum_{i\\neq \\hat{w}}1_{\\{e^1(s^{\\hat{w}}_{t+1},\\hat{b}^1_{t},i , s^i_{t+1},v_{t+1})=x\\}}\\pi^1_{t}(i|s)\\hat{b}^1_{t}(s)}|\\mathcal{f}_{t},h_0]\\nonumber \\\\ & = e[e[\\log \\frac{q(y_{t+1}|e^0(s^{\\hat{w}}_{t+1},\\hat{b}^1_{t}),s^{\\hat{w}}_{t+1})}{\\sum_{x , s}q(y_{t+1}|x , s)\\sum_{i\\neq \\hat{w}}1_{\\{e^1(s^{\\hat{w}}_{t+1},\\hat{b}^1_{t},i , s^i_{t+1},v_{t+1})=x\\}}\\pi^1_{t}(i|s)\\hat{b}^1_{t}(s)}|y_{t+1},\\mathcal{f}_{t},h_0]|\\mathcal{f}_{t},h_0 ] \\nonumber \\\\ & \\overset{(a)}{\\geqslant } e[\\log \\frac{q(y_{t+1}|e^0(s^{\\hat{w}}_{t+1},\\hat{b}^1_{t}),s^{\\hat{w}}_{t+1})}{\\sum_{x , s}q(y_{t+1}|x , s)e[\\sum_{i\\neq \\hat{w}}1_{\\{e^1(s^{\\hat{w}}_{t+1},\\hat{b}^1_{t},i , s^i_{t+1},v_{t+1})=x\\}}\\pi^1_{t}(i|s)|y_{t+1},\\mathcal{f}_{t},h_0]\\hat{b}^1_{t}(s)}|\\mathcal{f}_{t},h_0 ] \\nonumber \\\\ & \\overset{(b)}{= }   e[\\log \\frac{q(y_{t+1}|e^0(s^{\\hat{w}}_{t+1},\\hat{b}^1_{t}),s^{\\hat{w}}_{t+1})}{\\sum_{x , s}q(y_{t+1}|x , s)x^1[s^{\\hat{w}}_{t+1},\\hat{b}^1_{t}](x|s)\\hat{b}^1_{t}(s)}|\\mathcal{f}_{t},h_0 ] \\nonumber \\\\ & = e[\\log \\frac{q(y_{t+1}|x^0[s^{\\hat{w}}_{t+1},\\hat{b}^1_{t})],s^{\\hat{w}}_{t+1})}{\\sum_{x , s}q(y_{t+1}|x , s)x^1[s^{\\hat{w}}_{t+1},\\hat{b}^1_{t}](x|s)\\hat{b}^1_{t}(s)}|\\mathcal{f}_{t},h_0]\\nonumber \\\\ & = e [ r(s^{\\hat{w}}_{t+1},\\hat{b}^1_{t},x^0_{t}[s^{\\hat{w}}_{t+1},\\hat{b}^1_{t}],x^1_{t+1}[s^{\\hat{w}}_{t+1},\\hat{b}^1_{t}])|\\mathcal{f}_{t},h_0],\\end{aligned}\\ ] ]    where ( a ) is due to the convexity of @xmath194 , ( b ) is due to our posterior matching scheme under @xmath128 hypothesis , and @xmath195 is given by @xmath196 we realize that this is exactly the instantaneous reward received by the markov chain with state @xmath197 . considering the @xmath149-step drift , we have @xmath198 & = \\sum_{i = t}^{i = t+n-1 } e[l_{i+1}-l_i|\\mathcal{f}_{t},h_0]\\nonumber \\\\ & = \\sum_{i = t}^{i = t+n-1 } e[l_{i+1}-l_i|\\mathcal{f}_{t},h_0 ] \\nonumber \\\\ & = \\sum_{i = t}^{i = t+n-1 } e [ e[l_{i+1}-l_i|\\mathcal{f}_{i},h_0   ] |\\mathcal{f}_{t},h_0 ] \\nonumber \\\\ & \\overset{(a)}{\\geqslant }   \\sum_{i = t}^{i = t+n-1 } e[e[\\log \\frac{q(y_{i+1}|x_0[s^{\\hat{w}}_{i+1},\\hat{b}^1_{i}],s^{\\hat{w}}_{i+1})}{\\sum_{x , s}q(y_{i+1}|x , s)x^1[s^{\\hat{w}}_{i+1},\\hat{b}^1_i(s)])(x|s)\\hat{b}^1_{i}(s)}|\\mathcal{f}_{i},h_0]|\\mathcal{f}_{t},h_0 ] \\nonumber \\\\ & = \\sum_{i = t}^{i = t+n-1 } e[r(s^{\\hat{w}}_{i+1},\\hat{b}^1_{i},x^0_{i+1}[s^{\\hat{w}}_{i+1},\\hat{b}^1_{i}],x^1_{i+1}[s^{\\hat{w}}_{i+1},\\hat{b}^1_{i}])|\\mathcal{f}_t , h_0],\\end{aligned}\\ ] ] where ( a ) is due to  .",
    "thus , the multi - step drift corresponds to the total average reward in the aforementioned markov chain .",
    "this total reward relates to the mdp problem discussed in  @xcite .",
    "thus , under the assumptions stated in the discussion after lemma  [ lemma : onestepdrift ] , the corresponding per - unit - time reward converges to @xmath199 almost surely as @xmath200 .",
    "thus , given any @xmath116 , there exist a @xmath201 such that @xmath202 \\geqslant n_1(\\tilde{c}_1-\\epsilon).\\ ] ] now we consider the drift of @xmath90 w.r.t .",
    "@xmath203 under @xmath204 hypothesis . given any @xmath205 , @xmath206 \\nonumber \\\\ & = e[\\log\\frac{\\prod_{j = t+1}^{t+n}q(y_j|e^1(s^{\\hat{w}}_j,\\hat{b}^1_{j-1},w , s^w_j , v_j),s^w_j)(1-\\pi_{t}(w ) ) } { \\sum_{i\\neq w,\\hat{w } }   \\prod_{j = t+1}^{t+n}q(y_j|e^1(s^{\\hat{w}}_j,\\hat{b}^1_{j-1},i , s^i_j , v_j),s^i_j)\\pi_t(i)+ \\prod_{j = t+1}^{t+n}q(y_j|e^0(s^{\\hat{w}}_j,\\hat{b}^1_{j-1}),s^{\\hat{w}}_j)\\pi_t(\\hat{w } )    } |\\mathcal{f}_t , h_1].\\end{aligned}\\ ] ] as @xmath207",
    ", the right - hand side of   becomes @xmath208\\nonumber \\\\ & = e[\\sum_{j = t+1}^{t+n}\\log\\frac{q(y_j|e^1(s^{\\hat{w}}_j,\\hat{b}^1_{j-1},w , s^w_j , v_j),s^w_j)}{q(y_j|e^0(s^{\\hat{w}}_j,\\hat{b}^1_{j-1}),s^{\\hat{w}}_j)}|\\mathcal{f}_t , h_1 ] \\nonumber \\\\ & = \\sum_{j = t+1}^{t+n}e[\\log\\frac{q(y_j|e^1(s^{\\hat{w}}_j,\\hat{b}^1_{j-1},w , s^w_j , v_j),s^w_j)}{q(y_j|e^0(s^{\\hat{w}}_j,\\hat{b}^1_{j-1}),s^{\\hat{w}}_j)}|\\mathcal{f}_t , h_1 ] \\nonumber \\\\ & = \\sum_{j = t+1}^{t+n}e[e[\\log\\frac{q(y_j|e^1(s^{\\hat{w}}_j,\\hat{b}^1_{j-1},w , s^w_j , v_j),s^w_j)}{q(y_j|e^0(s^{\\hat{w}}_j,\\hat{b}^1_{j-1}),s^{\\hat{w}}_j)}|\\mathcal{f}_{j-1},h_1]|\\mathcal{f}_t , h_1].\\end{aligned}\\ ] ] furthermore , @xmath209\\nonumber \\\\ & = \\sum_y\\sum_{i\\neq \\hat{w}}\\int_{v_j}p(dv_j)q(y|e^1(s^{\\hat{w}},\\hat{b}^1_{j-1},i , s^i_j , v_j),s^i_j)p(w = i|\\mathcal{f}_{j-1},h_1)\\log\\frac{q(y|e^1(s^{\\hat{w}}_j,\\hat{b}^1_{j-1},i , s^i_j , v_j),s^i_j)}{q(y_j|e^0(s^{\\hat{w}}_j,\\hat{b}^1_{j-1}),s^{\\hat{w}}_j ) } \\nonumber \\\\ & = \\sum_{x , s}\\sum_y\\sum_{i\\neq \\hat{w}}\\int_{v_j}p(dv_j)q(y|x , s)1_{\\{e^1(s^{\\hat{w}},\\hat{b}^1_{j-1},i , s^i_j , v_j)=x\\}}1_{\\{s^i_j = s\\}}\\pi^1_{j-1}(i)\\log\\frac{q(y|x , s)}{q(y_j|e^0(s^{\\hat{w}}_j,\\hat{b}^1_{j-1}),s^{\\hat{w}}_j ) } \\nonumber \\\\ & = \\sum_{x , s}\\sum_y q(y|x , s)\\log\\frac{q(y|x , s)}{q(y_j|e^0(s^{\\hat{w}}_j,\\hat{b}^1_{j-1}),s^{\\hat{w}}_j)}\\left[\\sum_{i\\neq \\hat{w}}\\int_{v_j}p(dv_j)1_{\\{e^1(s^{\\hat{w}},\\hat{b}^1_{j-1},i , s^i_j , v_j)=x\\}}1_{\\{s^i_j = s\\}}\\pi^1_{j-1}(i)\\right]\\nonumber \\\\ & = \\sum_{x , s}\\left[\\sum_y q(y|x , s)\\log\\frac{q(y|x , s)}{q(y_j|e^0(s^{\\hat{w}}_j,\\hat{b}^1_{j-1}),s^{\\hat{w}}_j)}\\right]x^1[s^{\\hat{w}}_j,\\hat{b}^1_{j-1}](x|s)\\hat{b}^1_{j-1}(s)\\nonumber \\\\ & = \\sum_{x , s}x^1[s^{\\hat{w}}_j,\\hat{b}^1_{j-1}](x|s)\\hat{b}^1_{j-1}(s ) d\\left(q(\\cdot|x , s)\\|q(\\cdot|e^0(s^{\\hat{w}}_j,\\hat{b}^1_{j-1}),s^{\\hat{w}}_j ) \\right ) \\nonumber \\\\ & = r^*({\\underline{s}}_j,\\pi_{j-1}).\\end{aligned}\\ ] ] we realize that this is also some instantaneous reward received by the markov chain with state @xmath197 .",
    "thus , under the assumptions stated in the discussion after lemma  [ lemma : onestepdrift ] , the corresponding per - unit - time reward converges quantity converges to @xmath210 almost surely as @xmath200 .",
    "therefore , under hypothesis @xmath128 , for any @xmath150 , there exists @xmath211 such that @xmath202 \\geqslant n_2(\\tilde{c}^*_1-\\epsilon).\\ ] ] therefore for any @xmath150 , we have @xmath212 such that @xmath152 \\geqslant n(\\tilde{c}_1-\\epsilon ) \\qquad   \\text{if } l_t   \\geqslant \\log\\frac{p_0}{1-p_0}\\\\ e[l_{t+n}-l_t|\\mathcal{f}_t ] \\geqslant n(\\tilde{c}^*_1-\\epsilon ) \\qquad   \\text{if } l_t   < \\log\\frac{p_0}{1-p_0}.\\end{aligned}\\ ] ]      define a process @xmath184 and filtration @xmath185 .",
    "we have @xmath213 & \\geqslant n(\\tilde{c}-\\epsilon ) \\nonumber \\\\",
    "e[z_{t+1}- z_t|\\mathcal{f'}_{t } ] & \\geqslant    n(\\tilde{c}_1-\\epsilon ) \\qquad \\text{if } z_t > \\log\\frac{1-p_0}{p_0 } \\nonumber \\\\",
    "define a stopping time @xmath187 w.r.t to @xmath214 by @xmath215 by definition we have @xmath190 almost surely .",
    "apply fact  [ fact:2ndstagemartingale ] to @xmath216 , we have an upper bound on the expectation of the stopping time @xmath187 .",
    "@xmath191}{n } \\leqslant e[\\tilde{t } ] \\leqslant \\frac{\\log\\frac{1-pe}{pe}}{\\tilde{c}_1-\\epsilon } + \\frac{k}{n(\\tilde{c}-\\epsilon)}+d(n\\tilde{c},nc_1,nc_2).\\end{aligned}\\ ] ] rearranging the above inequality , we get @xmath217 } \\geqslant \\tilde{c}_1(1-\\frac{\\overline{r}}{\\tilde{c } } ) + \\frac{\\tilde{c}-\\tilde{c}_1}{\\tilde{c}(\\tilde{c}-\\epsilon)}\\epsilon -\\frac{nd(n\\tilde{c},n\\tilde{c}_1,nc_2)}{k/\\overline{r } } -\\frac{\\log(1-pe)}{k/\\overline{r}}- \\epsilon.\\end{aligned}\\ ] ]"
  ],
  "abstract_text": [
    "<S> the reliability function of memoryless channels with noiseless feedback and variable - length coding has been found to be a linear function of the average rate in the classic work of burnashev . in this work </S>",
    "<S> we consider unifilar channels with noiseless feedback and study specific transmission schemes , the performance of which provides lower bounds for the channel reliability function . in unifilar channels </S>",
    "<S> the channel state evolves in a deterministic fashion based on the previous state , input , and output , and is known to the transmitter but is unknown to the receiver . </S>",
    "<S> we consider two transmission schemes with increasing degree of complexity . </S>",
    "<S> the first one is a single - stage sequential transmission scheme , where both transmitter and receiver summarize their common information in an m - dimensional vector with elements in the state space of the unifilar channel and an m - dimensional probability mass function , with m being the number of messages . </S>",
    "<S> the second one is a two - stage transmission scheme where the first stage is as described above , and the second stage , which is entered when one of the messages is sufficiently reliable , is resolving a binary hypothesis testing problem . </S>",
    "<S> this stage is different from the one employed by burnashev for discrete memoryless channels . </S>",
    "<S> the analysis assumes the presence of some common randomness shared by the transmitter and receiver , and is based on the study of the log - likelihood ratio of the transmitted message posterior belief , and in particular on the study of its multi - step drift . </S>",
    "<S> simulation results for the trapdoor , chemical , and other binary input / state / output unifilar channels confirm that the bounds are tight compared to the upper bounds derived in a companion paper . </S>"
  ]
}