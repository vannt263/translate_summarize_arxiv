{
  "article_text": [
    "knowing the redshifts of extragalactic objects is vital for understanding their physical properties as well as for statistical cosmology measurements .",
    "the primary method employed by researchers to measure galaxy redshifts is spectroscopy ; however , spectroscopy is available for only a small fraction of all imaged galaxies , on the order of 1% @xcite .",
    "this low percentage has necessitated a search for alternative methods of redshift estimation .",
    "much progress has been made with photometric redshift estimation methods , which primarily rely on the colors of galaxies to estimate redshifts @xcite . while such methods are used frequently , they face inherent limitations , resulting in redshift estimates with large associated error values .",
    "consequently , methods of redshift estimation using the angular clustering of galaxies have been proposed and explored @xcite .",
    "because these methods require only the angular positions of galaxies , they are considered distinct from photometric redshift methods per the working definition argued by @xcite .",
    "the `` clustering - based '' redshift estimation methods rely on the spatial correlations and directly use the angular two - point correlation function @xmath0 , which expresses the excess probability of finding a galaxy at an angular separation @xmath1 from another galaxy .",
    "while these methods are promising , they propose only redshift distributions and thus are agnostic to the redshifts of specific galaxies .",
    "therefore , a natural question arising from clustering - based redshift estimation is whether galaxy clustering can be used not only to produce redshift distributions of galaxy samples but also to determine the redshifts of individual galaxies .    in this paper",
    ", we propose a new method that uses combinatorial optimization techniques applied to the angular two - point correlation function to divide galaxy samples into separate redshift bins , thereby constraining the redshifts of individual galaxies .",
    "creating thin slices not only is possible but potentially makes the optimization faster .",
    "ultimately , the final method is expected to balance the statistical noise and the optimization cost .    here",
    "we explore the simplest case .",
    "we aim to partition a set of ( simulated ) galaxies into two subsamples such that their correlation functions match the observations",
    ". this case would be particularly useful in creating a hard boundary between overlapping photo - z redshift bins .",
    "if it works , the optimization can be repeated iteratively in order to separate a galaxy sample into narrow redshift slices .",
    "of course , even this simplest case presents computational challenges .",
    "[ line - through]*naively , for a sample of size @xmath2 , @xmath3 possible partitions would have to be considered in order to find the optimal partition , which is prohibitively expensive even for dozens of objects .",
    "* we present a formalism using integer linear programming that , when implemented , makes this optimization tractable for statistically relevant sample sizes , .    in section [ sec : formalism ]",
    ", we provide a detailed description of our method , including a derivation of the relevant formalism . in section [ sec : results ] , we present results from in section [ sec : applicability ] , we discuss the applicability of this method .",
    "before introducing our formalism , let us introduce the notation that will be used throughout the rest of this paper .",
    "let @xmath4 and @xmath5 be two datasets with the same sky coverage , and let @xmath6 be a random dataset with the same sky coverage as @xmath4 and @xmath5 . in accordance with @xcite we define @xmath7 as the set of all unordered pairs of galaxies in @xmath4 , and we define @xmath8 as the set of all unordered pairs of galaxies such that one galaxy is from @xmath4 and the other is from @xmath5 .",
    "we also define @xmath9 and @xmath10 analogously .",
    "because the correlation function is estimated over a set of @xmath2 bins ( i.e. , intervals ) of angular separation , we must introduce notation related to pair counts within these bins .",
    "we define @xmath11 as the set of unordered data - data pairs such that the angular separation between the members of the pair is in bin @xmath12 , @xmath13 .",
    "the corresponding terms for @xmath14 , @xmath10 , etc . , are defined analogously .",
    "lastly , we define @xmath15 as the size of @xmath4 , @xmath16 as the size of @xmath11 , etc .    in this paper , we develop our formalism using two estimators of correlation functions given two samples with the same sky coverage : the natural estimator and the @xcite estimator .",
    "the natural estimator is the least expensive of the correlation function estimators to compute and hence expected to yield the simplest optimization formulas . on the other hand ,",
    "the landy - szalay estimator has been shown to be the most accurate estimator by @xcite . using the natural estimator , the cross - correlation and autocorrelation function estimates of the two samples",
    "@xmath4 and @xmath5 in bin @xmath12 are given by : @xmath17 @xmath18 @xmath19 in the main body of the paper , we focus on these equations but provide the alternative method for the landy - szalay estimator in appendix  [ sec : ls ] .",
    "one of the most powerful tools in discrete optimization is _ integer linear programming ( ilp)_. _ linear programming _ is an optimization model in which one seeks to optimize a linear function of finitely many variables , subject to linear inequality constraints on the variables .",
    "integer linear programming generalizes this by only admitting solutions where some ( or all ) of the variables are constrained to take only integer values .",
    "such models are an indispensable tool for optimizing over discrete solution spaces , and state - of - the - art software codes have been developed to handle numerous large scale problems arising in technological , scientific , and economic applications .    in order to build an integer linear programming model ,",
    "we must build a model consisting of a cost function and constraints , where our cost function and constraints are constructed such that the minimization of our cost function yields the optimal solution to our problem .",
    "we define our optimal solution as a partition of our sample @xmath20 into two subsamples @xmath21 and its complement @xmath22 such that : @xmath23\\ ] ] is minimized , where @xmath24 , @xmath25 , and @xmath26 are our correlation function estimates in bin @xmath12 , as calculated using our choice of estimator , and @xmath27 and @xmath28 are target values for @xmath24 , @xmath25 , and @xmath26 , respectively .",
    "informally , this function is minimized when @xmath29 , @xmath30 , and @xmath31 are pulled as close to our target values @xmath27 and @xmath28 as possible , across all bins @xmath12 .    the purpose of the following formalism is to translate equation [ eq : tot ] into an integer linear programming model .",
    "the construction of such a model requires translating all unknowns into variables , creating a cost function using a linear combination of these variables , and adding constraints to the model to enforce certain relationships between the variables .",
    "we must also specify which variables are integer or continuous . in current software",
    ", one can even specify if an integer variable is binary , i.e. , takes only values @xmath32 or @xmath33 .",
    "our integer variables will , in fact , be binary variables to accommodate the problem of classifying objects into two redshift slices .",
    "to simplify the optimization further , we fix the size of @xmath21 and its complement , @xmath34 and @xmath35 , to prevent runaway solutions in which one subsample contains a large majority of the galaxies in @xmath20 .",
    "moreover , this enables us to keep the cost function and constraints linear .",
    "consequently , we treat @xmath34 and @xmath35 as constants throughout .",
    "this , however , poses no real threat to generality because often good initial estimates are available , and further optimization can be performed along the size dimension if needed .",
    "we begin by defining variables for each of our galaxies .",
    "we introduce the binary variable @xmath36 that encodes whether a galaxy @xmath37 is a member of @xmath21 or @xmath22 : @xmath38 these variables serve as the bridge between the cost function and the partitioning of @xmath20 : we will construct the cost function in such a way that minimizing it sets each @xmath36 to either 0 or 1 and thus assigns each galaxy to @xmath21 or @xmath22 according to the optimal partition .",
    "we now add a constraint to our model in order to enforce that @xmath34 must be fixed to a pre - determined positive integer by using the fact that @xmath34 is precisely the sum of @xmath39 that evaluate to @xmath33 : @xmath40 thus , we have fixed @xmath34 , and because , where @xmath41 is fixed , we have also fixed @xmath35 .",
    "next , we introduce variables for unordered pairs of galaxies in @xmath20 that encode whether the galaxies in each pair are from the different subsamples . for each unordered pair of galaxies",
    ", we define the binary variable @xmath42 as follows : @xmath43 where @xmath42 is symmetric in @xmath37 and @xmath44 .",
    "significantly , @xmath42 can be expressed in terms of the boolean ( xor hereafter ) of the @xmath36 and @xmath45 variables , @xmath46 where each xor is encoded through four linear constraints of @xmath47 and @xmath42 that we add to our model as @xmath48 we reiterate that these constraints establish the relationship between @xmath49 s and @xmath50 s , as given in their definitions in and .    before proceeding , we introduce more notation - related summations over pair counts .",
    "we use the summation notation @xmath51 to denote summing over all unordered pairs of galaxies in @xmath20 such that the angular separation between @xmath37 and @xmath44 falls into bin @xmath12 .",
    "we begin by translating the natural estimator @xmath52 to its linear programming equivalent in this section and generalize to the landy - szalay estimator @xmath53 in the appendix .      using the natural estimator",
    ", we first translate @xmath31 into its cost function equivalent , @xmath54 . for cross - correlation",
    ", we seek to minimize : @xmath55 where @xmath26 is an estimate of cross - correlation in bin @xmath12 .",
    "we can express @xmath26 in terms of previously - defined quantities : @xmath56 because @xmath6 is fixed , @xmath57 and @xmath58 are constants .",
    "furthermore , @xmath34 and @xmath35 are fixed .",
    "thus , we can combine these constants into a single weight for each bin : @xmath59    we therefore seek to minimize @xmath60 where @xmath61 is the only non - constant term within the minimization for each bin @xmath12 .",
    "we can now reformulate this expression using our previously - defined binary variables @xmath42 .",
    "@xmath62 is precisely equal to the number of unordered pairs @xmath63 of galaxies in @xmath20 such that @xmath37 and @xmath44 are in different subsamples and the angular separation between @xmath37 and @xmath44 falls into bin @xmath12 ; thus , @xmath64    because we are summing over all unordered pairs in @xmath65 , as opposed to just unordered pairs in @xmath66 , we have eliminated any dependence on the partitioning of @xmath20 except in the variables themselves .",
    "the expression that we seek to minimize for cross - correlation optimization now simplifies to : @xmath67 because each @xmath42 is the xor of @xmath36 and @xmath45 , we have expressed cross - correlation optimization entirely in terms of binary variables for each galaxy , and minimization of the above expression will assign each galaxy to either @xmath21 or @xmath22 according to the optimal partition of @xmath20 .",
    "the expression in is not a linear function of the @xmath42 variables due to the absolute value function .",
    "however , this expression can be modeled by a linear function by introducing auxiliary continuous variables @xmath68 for each bin @xmath12 and relating them to the @xmath42 variables as follows .",
    "@xmath69 where for each @xmath68 , we add the following two constraints : @xmath70 we have now incorporated cross - correlation optimization into our linear programming model .",
    "the key insight here is that any solution that minimizes the expression in   must satisfy one of the inequalities in   at equality depending on which left hand side is larger , and thus @xmath71 in any optimum solution minimizing  .",
    "we mention here the conscious choice of using the @xmath72 norm in  , as opposed to the @xmath73 norm : the @xmath72 norm leads to a formulation with a linear objective like   and linear constraints like , as opposed to the @xmath73 norm which would give a convex quadratic objective . as per folk wisdom in integer programming ,",
    "we prefer the linear formulation , and hence we go with the @xmath72 norm .",
    "next , we formalize autocorrelation optimization using the natural estimator . in this paper , we formalize two different approaches : combining the autocorrelations of @xmath21 and @xmath22 in such a way that the autocorrelation target values @xmath74 and @xmath75 of @xmath21 and @xmath22 , respectively , are set to be equal , and implementing separate target values for the autocorrelation of @xmath21 and @xmath22 , thereby allowing @xmath74 and @xmath75 to be potentially distinct for any bin @xmath12 .",
    "although the latter method is advantageous because it allows for independent target values , the downside is that one needs more variables in the integer linear program to model this , compared to the former method in which no new variables need to be introduced into our model .",
    "we introduce the former method next and introduce the latter method in the appendix in [ sec : independent ] .      to derive a parameterized autocorrelation for the two samples , let us consider the variable @xmath76 defined as @xmath77 from the definition we can see that . although @xmath76 is agnostic as to which sample @xmath37 and @xmath44 belong",
    ", it does encode whether the pair contributes to an autocorrelation calculation .",
    "we can naturally extend the notion of autocorrelation for @xmath21 and @xmath22 into a combined autocorrelation given by : @xmath78 where @xmath79 is the weighted average of @xmath24 and @xmath25 : @xmath80 with @xmath81 in this combined autocorrelation model , we seek to minimize @xmath82 where @xmath74 is the target value for the combined autocorrelation in bin @xmath12 .",
    "we introduce the weight @xmath83 to replace constants in @xmath84 : @xmath85    furthermore , we can express entirely in terms of variables that have already been introduced because this sum is precisely equal to the sum of unordered pairs @xmath86 of galaxies in @xmath20 such that @xmath37 and @xmath44 are in the same subsample and the angular separation between @xmath37 and @xmath44 falls into bin @xmath12 : @xmath87 our expression takes the form : @xmath88 - ( 1 + \\alpha_i ) \\bigg|\\ ] ]    as with cross - correlation optimization , the final step is to eliminate absolute values from the cost function . for each bin @xmath12 , we add a continuous variable @xmath89 .",
    "the portion of the cost function @xmath90 corresponding to combined autocorrelation optimization takes its final form : @xmath91 where for each @xmath89 , we add the following two constraints : @xmath92 - ( 1 + \\alpha_i ) & \\le \\psi_i\\\\   \\bigg [ b_i \\sum_{(u ,",
    "v ) \\in vv_i } ( 1 - y_{uv } ) \\bigg ] - ( 1 + \\alpha_i ) & \\le - \\psi_i    \\end{split}\\ ] ] we can now express our entire model using combined autocorrelation and the natural estimator .",
    "the model consists of the cost function : @xmath93 and all associated constraints .",
    "this model can be generalized to the landy - szalay estimator by modifying the cost function slightly ; we provide the details in the appendix in [ sec : ls ] .",
    "furthermore , in all of the above formalism , we have used a uniform weighting across all bins for cross - correlation and autocorrelation in the cost function for notational simplicity .",
    "we note that one can easily weight each bin individually .",
    "to test the effectiveness of the integer linear programming method described in section [ sec : formalism ] , we used the python interface for gurobi , a solver available for both academic and commercial use .",
    "we ran all tests on a dell poweredge r815 machine with 512 gb of ram and 4 amd opteron 6272 processors .",
    "the machine ran scientific linux 7 , python 2.7.5 , and gurobi 6.0.4 .",
    "it is worth noting that none of the following tests required gurobi to use more than 1% of ram at any given time ; therefore , our machine s specifications far exceed the specifications necessary to reproduce the following results .",
    "for the two datasets @xmath21 and @xmath22 , we used two mock catalogs , each 1 square degree in size and each consisting of 10,000 galaxies , generated [ line - through]*by sbastien heinis * using a cox point process , as described in heinis et al .",
    "the samples were produced using different random seeds , and consequently , they behaved as uncorrelated samples , meaning that the theoretical cross - correlation was 0 . in both samples , we selected the thin redshift cut of , the densest redshift slice in both samples , in order to select samples with strong angular autocorrelation signals .",
    "these cuts left slightly over 1000 galaxies per catalog .",
    "we then randomly selected 2000 galaxies in total from these two samples , yielding @xmath21 and @xmath22 such that , for our random catalog , we generated a 1 square degree poisson sample consisting of 200,000 galaxies .",
    "all bins for cross - correlation and autocorrelation were given equal weights in the cost function according to the formalism derived in section [ sec : formalism ] .",
    "furthermore , we chose to use binning schemes with an equal number of galaxy pairs per bin across all bins . with this choice ,",
    "the poisson noise was equal across all bins , unlike binning schemes set by uniform increments in angular separation .",
    "test the effectiveness of the optimization itself , we fed gurobi ground - truth target values by pre - computing all @xmath74 s and @xmath28 s using the real partition of @xmath20 into @xmath21 and @xmath22 ; we also fixed @xmath34 and @xmath35 according to equation [ eq : ssize ] using the real partition of v. given the ground - truth as target values , we then tested    1 .   the time required for gurobi to complete the optimization 2 .",
    "the fraction of galaxies assigned correctly    accordingly , these are the main metrics by which we compare different optimization runs in this section .    in the limit of sufficiently few pairs per bin , the optimization is computationally feasible and recovers the ground - truth solution exactly .",
    "for instance , with a binning scheme of @xmath94 pairs per bin and @xmath95 bins , the optimization recovers the ground - truth partition in @xmath96 seconds .",
    "however , in this case of only 5 pairs per bin , the optimization almost certainly couples to the noise of the ground - truth correlation function target values .",
    "this motivated an analysis of a binning scheme of 100 pairs per bin , which improves upon the poisson noise per bin while still retaining resolution in the autocorrelation signal at short scales by having sufficiently narrow bins .    in order to reduce computational load and make the optimization feasible , we fixed a fraction of the total galaxy population before beginning the optimization by adding extra constraints to our model that if and if for a fraction of galaxies ; including a fixed sample is reasonable because a spectroscopic sample could be used as a fixed population in an application of this method to real data . in order to explore the optimization with these settings , we ran a series of tests using gurobi with @xmath97 of galaxies fixed and a binning scheme of 100 pairs per bin . .    three distinct regimes",
    "the first regime occurs in the limit of few bins , or short maximum angular scales , in which there are many solutions with 0 cost .",
    "because there are so many optimal solutions , gurobi is capable of finding one such optimal solution in a short amount of time ; however , for this same reason , this solution behaves only slightly better than random in assigning the unfixed galaxies correctly .",
    "it is worth noting that even in this regime , the amount of time required to complete optimization increases exponentially as a function of number of bins .",
    "this increase in runtime is expected because increasing the number of bins increases the number of target values and thus decreases the number of solutions with 0 cost .",
    "the second regime occurs in the limit of many bins , in which the optimization is communicated a sufficient amount of information about the ground - truth solution through the @xmath74 s and @xmath28 s that it is able to find the ground - truth solution in of order a minute .",
    "consequently , gurobi assigns 100% of the unfixed galaxies correctly .",
    "the third regime is the peak in the amount of time required for the optimization to complete , located between the other two regimes .",
    "as seen in , the full features of the peak can not be determined due to the exponential growth in runtime .",
    "the points correspond to optimization runs that were terminated before completion after @xmath98 seconds , the maximum allowed time in our study ; the fraction of galaxies correctly assigned corresponds to that of the optimal solution found before termination .",
    "this peak is the result of a trade - off between the other two regimes : there are relatively few solutions with 0 cost , and they can not be found quickly by gurobi ; however , there are not enough bin target values to constrain the ground - truth solution immediately .    in figure",
    "[ fig:100leftright ] , for a binning scheme with 200 pairs per bin .",
    "the same three characteristic regimes are present ; however , the second and third regimes begin at shorter maximum angular binning scales for the binning scheme of 100 pairs per bin than for the binning scheme of 200 pairs per bin .",
    "this is a consequence of the fact that for a given maximum angular binning scale , the binning scheme of 100 pairs per bin is fed twice the number of @xmath74 s and @xmath28 s than the binning scheme of 200 pairs per bin and thus has twice the information .",
    "the results presented in figure [ fig:100leftright ] are highly sensitive to the fraction of galaxies fixed before optimization .",
    "for example , in the regime of many bins , the optimization requires an order of magnitude more time to complete the optimization for 75% of galaxies fixed than for 80% of galaxies fixed ; this optimization does not complete within @xmath98 seconds for 70% of galaxies fixed .",
    "reducing the total number of galaxies to @xmath99 , the optimization requires a lower percentage of galaxies to be fixed in order to recover the ground truth solution in the limit of many bins .",
    "fixing 65% of galaxies and using 100 edges per bin and 600 bins , the optimization completes in 195 seconds . lowering the percentage fixed to 60%",
    ", the optimization completes in 695 seconds .",
    "thus , reducing the total number of galaxies to @xmath99 still allows for @xmath100 unfixed galaxies to be assigned , confirming that the percentage of fixed galaxies necessary for the optimization to complete within a given time frame is largely dependent on the total number of galaxies in @xmath20 .",
    "the results presented in section [ sec : results ] reveal that in the appropriate regimes , the optimization is computationally feasible when ground - truth values are fed in as the target values . in any real application of this method , the ground - truth values of the @xmath74 s and @xmath28 s would only be known approximately , and fiducial values are good approximations in the case of many pairs in the bins .",
    "the determination of the appropriate fiducial values is a question of physics rather than of linear programming , and because we have chosen to test only the optimization itself in this paper , so far we have omitted the exploration of the effects of inputting fiducial values on the optimization .",
    "we test the optimization s response to inexact target values by perturbing the @xmath74 s toward values taken from a power law fit of the combined autocorrelation function and perturbing the @xmath28 s toward @xmath32 , the expected cross - correlation .",
    "we accomplish this by using interpolation and setting the target values by varying the interpolation parameter @xmath101 $ ] according to the following equations : @xmath102\\ ] ] and @xmath103 where @xmath104 is the ground - truth combined autocorrelation value in bin @xmath12 , @xmath105 is the ground - truth cross - correlation value in bin @xmath12 , and @xmath106 is the value of the power - law fit in bin @xmath12 .",
    "as seen in figure [ fig : corrfn ] .",
    "this power law fit was used only for the purposes of perturbing the ground - truth combined autocorrelation function by small amounts , as seen in figure [ fig : interpolation ] . in figure",
    "[ fig : interpolation ] , we present the time required for the optimization to complete when varying @xmath107 up to 0.04 for 80% of galaxies fixed , 200 pairs per bin , and 300 bins . by varying @xmath107 only by small amounts ,",
    "we lessen the impact of an incorrect power law fit .",
    "furthermore , these binning settings were chosen because the optimization recovered the exact ground - truth partition in 44 seconds given these settings and ground - truth target values .",
    "for all runs in this figure , the optimization exactly recovered the ground - truth solution .",
    "the fact that the optimization still completes and recovers the real partition when given inexact target values for for , the optimization does not complete within @xmath98 seconds ,    the observed phenomenon of an increase in runtime with @xmath107 is possibly due to the fact that the model might not have a perfect solution and the cost function could become shallow , which leads to a large number of  nearly optimal \" solutions .",
    "these solutions must be pruned by the solver to find the global minimum conclusively .",
    "however , the pruning methodologies of ip solvers often need to spend a significant amount of time to discard the ",
    "nearly optimal \" solutions , even though they have arrived at a stage of the optimization where all the solutions that are being considered have values that are very close to the true optimal value . to finally discover the true optimal value by weeding through the large number of `` nearly optimal '' ones occupies the bulk of the time for",
    "the solver in such situations ( see chapter 2 of  @xcite for a discussion of these pruning strategies for integer linear programming ) .    by interpolating a power law fit",
    ", we have in effect pulled the @xmath74 target values to a slightly less - noisy correlation function . in order to resolve fully the question of whether the optimization is finding the ground - truth solution by coupling to noise",
    ", @xmath107 would have to be driven closer to 1 ; of course , in doing so , the target values would become more dependent on the fiducial values , and physically - correct fiducial values would have to be chosen , .",
    "however , a visual representation of the noise in each bin of the ground - truth combined autocorrelation function is nonetheless instructive and can be seen in figure [ fig : corrfn ] .",
    "our ultimate goal is to use this procedure in conjunction with photometric redshift results to sort galaxies into thin redshift slices using only photometric data .",
    "the true redshift distribution of photo - z bins will have overlapping tails due to the uncertainties in the estimation . starting with designations from the photo - z catalog",
    ", one can apply our procedure to create a sharper boundary between the bins .",
    "repeating this procedure will yield much improved redshifts . .",
    "we have presented a novel integer linear programming method that enables a galaxy sample to be partitioned into two subsamples such that the angular two - point cross- and autocorrelations of the subsamples are optimized to pre - determined target values .",
    "our approach is the first application of integer linear programming to , which is expected to find other applications as we explore large statistical samples .",
    "we tested this optimization method using mock catalogs and gurobi , an optimization solver , and verified that this optimization technique is not only feasible in certain regimes but also provides good solutions , .",
    "this is due to the formulation of the problem using only linear equations .",
    "we explored the applicability of this method and have described how it could be used in the future to estimate the redshifts of individual galaxies using only their celestial coordinates .",
    "evidently , much more remains to be explored with the applicability of this method , a significant portion of which relates to fiducial values .",
    "for instance , the question of the extent to which the optimization is coupling to noise in the correlation function can only be resolved by analyzing runs with fiducial target values .",
    "furthermore , much would be learned from re - generating figure [ fig:100leftright ] using fiducial values as target values once the best method for selecting fiducial values is identified .",
    "other variables such as the optimal binning scheme and maximum angular binning separation for the autocorrelation and cross - correlation when using fiducial values would also have to be explored . ideally ,",
    "for real application , the number of galaxies per sample could be pushed higher , and the fraction of galaxies fixed could be pushed lower ; given the rapid improvement in linear programming algorithms and the increase in computing power , these limitations may very well resolve themselves with time .",
    "additions to the outlined method could be explored in order to improve runtimes and accuracy .",
    "some of these are mathematical and others physical issues .",
    "\\(1 ) in a real application of this method , the optimization could be terminated when the relative gap between the upper and lower bounds of the optimization is below a threshold value , instead of forcing this gap to reach zero in order for the optimization to complete and find the true _ mathematically _ optimal solution . in this case , the solver may report a `` nearly optimal '' solution of the optimization model , as opposed to the true _ mathematical _ optimum of the model .",
    "nevertheless , we feel that from a physical standpoint , the `` nearly optimal '' solutions may be more meaningful than the mathematical optimal which could be influenced by noise .",
    "such a strategy would also make sense when we use fiducial values instead of values from a simulation and would help to deal with the explosion in running time as discussed in section  [ sec : toward_fiducial ] .",
    "\\(2 ) other than through the @xmath74 s , the power law nature of the autocorrelation function is never explicitly leveraged ; it could potentially be exploited in a greedy algorithm , for example , by fixing pairs of shortest angular separations to the same subsample before beginning optimization .",
    "the autocorrelation signal could also potentially be leveraged to a greater extent by using a binning scheme set by uniform increments in angular binning separation , which could provide more bins at the shortest angular scales .",
    "\\(3 ) it is possible that subsampling the pairs in each bin could decrease runtime without sacrificing accuracy .",
    "considering that the true underlying variables are the classes to which the objects belong ( @xmath36 ) , a subset of the pairs ( @xmath42 ) could provide enough constraints at a reduced computational cost .",
    "the sampling , however , will probably have to be carefully constructed to optimize performance .",
    "the authors would like to thank sbastien heinis for providing mock catalogs and brice mnard for helpful discussions .",
    "lance joseph helped with computational resources .",
    "b.l . was supported by the 2015 herchel smith - harvard undergraduate science research fellowship .",
    "a.b . gratefully acknowledges partial support from nsf grant cmmi1452820 .",
    "we formalize the independent autocorrelation method for the natural estimator and provide the formalism for the landy - szalay estimator .",
    "the mathematical notation for these are somewhat more complicated but the complexity of the algorithm does not increase .",
    "here , we introduce the formalism for autocorrelation optimization with the natural estimator in which target values @xmath74 and @xmath75 can be set independently , as opposed to the combined autocorrelation method described in [ sec : combined ] .",
    "the independence of @xmath74 and @xmath75 comes at the expense of model complexity : we must introduce the variables @xmath108 and @xmath109 and associated constraints for each unordered pair , where neither @xmath108 nor @xmath109 can be expressed in terms of @xmath42 .",
    "therefore , this method effectively triples the number of variables in our model in comparison to the combined autocorrelation model .",
    "we designate the cost function equivalent of the autocorrelation of @xmath21 and @xmath22 using the natural estimator as @xmath110 and @xmath111 , respectively .    in order to formalize the autocorrelation of @xmath21",
    ", we introduce the variable @xmath108 . for each unordered pair , we define @xmath108 as follows : @xmath112    in addition , to formalize the autocorrelation of @xmath22 , we introduce the analogous variable @xmath109 . for each unordered pair , we define @xmath109 as follows : @xmath113    just as with @xmath50 s , we must add constraints to relate @xmath49 s to @xmath114 s and @xmath115 s :    @xmath116    @xmath117    for the autocorrelation of @xmath21 , we minimize : @xmath118 and for the autocorrelation of @xmath22 , we minimize : @xmath119    thus , @xmath120 takes the final form of : @xmath121 where for each @xmath89 , we add the following two constraints : @xmath122 and @xmath123 takes the final form of : @xmath124 where for each @xmath89 , we add the following two constraints : @xmath125    in order to convert @xmath110 and @xmath126 into their final forms , we must eliminate the absolute values using the method described in sections  [ sec : cross ] and [ sec : combined ] . in this formalization using independent autocorrelations for @xmath21 and @xmath22 and using the natural estimator , the full model consists of the cost function : @xmath127 and all of the associated constraints .",
    "here , we introduce the formalism for optimization with the landy - szalay estimator . using the landy - szalay estimator , the autocorrelation and cross - correlation function estimates of two samples @xmath4 and @xmath5 in bin @xmath12 are given by : @xmath128 @xmath129 @xmath130 in building the ilp model for the natural estimator , we have already modeled the first terms in all three expressions  ,  , and .",
    "thus , we only need to translate the second terms in these expressions .",
    "fortunately , they can be expressed entirely in terms of constants and @xmath36 s .",
    "we begin with the @xmath131 term . in and",
    ", this term in the @xmath12th bin is defined as : @xmath132 furthermore , we know that @xmath133 is defined as the number of unordered pairs such that the angular separation between @xmath37 and @xmath44 lies in bin @xmath12 . defining @xmath134 for a given as the number of ordered pairs such that and the angular separation between @xmath37 and @xmath114 lies in bin @xmath12 , we can re - express @xmath135 : @xmath136 we can in turn express this in terms of our variables @xmath36 : @xmath137    by generalizing to summing over all galaxies in @xmath20 as opposed to just galaxies in @xmath21 , we have eliminated any dependence on the partitioning of @xmath20 except in the variables themselves .",
    "furthermore , for a given galaxy , @xmath134 is a constant that can be pre - computed before optimization .",
    "we now define the weight @xmath138 to absorb all of these constants : @xmath139 thus , in the @xmath12th bin , the term involving @xmath131 becomes : @xmath140 we can define the term involving @xmath141 in the @xmath12th bin analogously : @xmath142 where : @xmath143 thus , referring to , , , we can now express @xmath31 , @xmath29 , and @xmath30 , respectively , in terms of our binary variables : @xmath144 where @xmath145 has been defined in equation [ eq : a ] , and @xmath146 and @xmath147 have been defined in equations [ eq : as ] and [ eq : asbar ] , respectively .",
    "the cost function equivalents of @xmath31 , @xmath29 , and @xmath30 , given by @xmath54 , @xmath110 , and @xmath111 , respectively , can now be converted to their final forms by eliminating the absolute values using the method described in sections  [ sec : cross ] and [ sec : combined ] .",
    "thus , our full model consists of the cost function : @xmath148 and all associated constraints .",
    "+    bentez , n. 2000 , apj , 536 , 571 benjamin , j. , van waerbeke , l. , mnard , b. , & kilbinger , m. 2010 , mnras , 408 , 1168 brammer , g. , van dokkum , p. , & coppi , p. 2008 , apj , 686 , 1503 budavri , t. , szalay , a. , connolly , a. , csabai , i. , & dickinson , m. 2000 , aj , 120 , 1588 budavri , t. , csabai , i. , szalay , a. , connolly , a. , & szokoly , g. 2001 , aj , 122 , 1163 budavri , t. 2008 , apj , 695 , 747 conforti , m. , cornujols , g. , zambelli , g. , integer programming , _ graduate texts in mathematics , springer - verlag _ , 2015 .",
    "connolly , a. , csabai , i. , szalay , a. , koo , d. , kron , r. , & munn , j. 1995 , aj , 110 , 2655 feldmann , r. , et  al .",
    "2006 , mnras , 372 , 565 gurobi optimization , inc . , 2015 ,",
    "gurobi optimizer reference manual , http://www.gurobi.com heinis , s. , budavri , t. , & szalay , a. 2009 , apj , 705 , 739 kerscher , m. , szapudi , i. , & szalay , a. 2000 , apj , 535 , l13 koo , d. 1985 , aj , 90 , 418 koo , d. 1999 , in asp conf .",
    "191 , photometric redshifts and high redshift galaxies , ed .",
    "weymann , r. , storrie - lombardi , l. , sawicki , m. , & brunner , r. ( san francisco , ca : asp ) , 3 landy , s. d. , & szalay , a. s. 1993 , apj , 412 , 64 mnard , b. , scranton , r. , schmidt , s. , morrison , c. , jeong , d. , budavri , t. , & rahman , m. 2013 , arxiv e - prints , arxiv:1303.4722 newman , j. 2008 , apj , 684 , 88 rahman , m. , mnard , b. , & scranton , r. 2015a , arxiv e - prints , arxiv:1508.03046 rahman , m. , mnard , b. , scranton , r. , schmidt , s. , & morrison , c. 2015b , mnras , 447 , 3500 rahman , m. , mendez , a.  j. , mnard , b. , et al .",
    "2016 , , 460 , 163 schmidt , s. j. , mnard , b. , scranton , r. , morrison , c. , & mcbride , c. k. 2013 , mnras , 431 , 3307 schmidt , s. j. , mnard , b. , scranton , r. , et al .",
    "2015 , mnras , 446 , 2696"
  ],
  "abstract_text": [
    "<S> we propose a new method of the redshifts of individual extragalactic sources based on . </S>",
    "<S> techniques from integer linear programming are utilized to optimize simultaneously for the angular two - point cross- and autocorrelation functions . </S>",
    "<S> our novel formalism introduced here not only transforms the otherwise hopelessly expensive , brute - force combinatorial search into a linear system with integer constraints but also is readily implementable in off - the - shelf solvers . </S>",
    "<S> we adopt and use python to build the cost function dynamically . </S>",
    "<S> the preliminary results on simulated data show for future applications to sky surveys by complementing and enhancing photometric redshift estimators . </S>",
    "<S> our approach is the first </S>"
  ]
}