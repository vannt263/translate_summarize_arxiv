{
  "article_text": [
    "the configuration interaction approach to computational nuclear physics casts the schrdinger equation for the nuclear many - body bound state problem as a matrix eigenvalue problem @xcite .",
    "the many - body hamiltonian is approximated by a finite matrix whose eigenvalues and eigenvectors correspond to the bound state energies and wavefunctions .",
    "the wavefunctions can then be used to calculate other observables .",
    "typically only the lowest eigenvalues and eigenvectors of this matrix are of interest .",
    "the hamiltonian matrices required to accurately calculate nuclear properties can be very large , with dimension in excess of @xmath11 , and @xmath12 or more nonzero matrix elements @xcite . calculations of this magnitude can only be performed on supercomputers , with parallel codes like many - fermion dynamics for nuclei ( mfdn ) @xcite , a hybrid mpi / openmp software package written in fortran and c.    the hamiltonian matrices are very sparse , but their sparsity structure is nontrivial ; locating and calculating the nonzero matrix elements takes a significant fraction of the overall runtime in mfdn , on the order of @xmath13 ",
    "@xmath14 for some representative cases .",
    "the matrix construction stage contains a number of parallelizable steps , however , as each nonzero element can be calculated independently .",
    "this problem structure is a promising target for acceleration on simd - style coprocessors like gpus .",
    "we present an investigation of gpu acceleration in the matrix construction stage of mfdn .",
    "gpu accelerators pair a large number of cores with a communal block of memory .",
    "they have much less memory per core and can not easily handle more complex program logic , but are capable of running many calculations in parallel .",
    "the cuda framework @xcite from nvidia provides a high - level api for accessing gpu functionality , allowing gpus to be programmed in languages like c and fortran .",
    "code to be executed on the gpu is written in a function called the kernel .",
    "the cpu code can then invoke the kernel , specifying a number of cores on which to run simultaneously .",
    "the kernel invocation specifies thread count in a two - level hierarchy : threads are grouped together into blocks , and blocks are grouped into a grid .",
    "each thread has its own small , private , local memory , and each thread in a block can access the shared memory of that block .",
    "every thread in the grid has access to the global memory , which can be in the @xmath15 gb ",
    "@xmath16 gb range .",
    "the user calls cuda allocation and copy functions to move data to the global memory of the gpu , invokes the kernel and waits for completion , and then uses copy functions to retrieve the results of the calculation .",
    "the kernel invocation will often request more threads than the gpu has cores . in this case , the gpu has a scheduler to stream blocks to cores as they become available .",
    "mfdn is a hybrid mpi / openmp parallel software package written in fortran and c for ab initio nuclear physics calculations .",
    "mfdn generates a many - body nuclear hamiltonian matrix for the nucleus in question and uses the lanczos algorithm to extract the lowest eigenvalues and eigenvectors .",
    "the many - body matrix is stored in memory on core ; this strategy limits the sizes of the matrices that can be used , but is much faster than accessing the matrix from disk .",
    "the matrix is symmetric , so only half of it is generated and stored @xcite .",
    "mfdn runs in several stages . after the various indexing systems are set up to specify the many - body basis ,",
    "the many - body hamiltonian matrix is constructed .",
    "the nonzero elements are then located , calculated , and stored .",
    "elements in the many - body matrix are built up as linear combinations of kinetic energy and nuclear interaction terms .",
    "mfdn reads the kinetic energy and 2-body and 3-body potentials from file , and uses them to calculate elements of the many - body hamiltonian .",
    "the many - body matrix is distributed among mpi processes in a way that produces a roughly uniform distribution of nonzero elements .",
    "once the matrix is generated , mfdn obtains the lowest eigenvalues and eigenvectors with the lanczos algorithm , an iterative algorithm that relies on successive matrix - vector multiplications and orthogonalizations .",
    "the lanczos algorithm requires many iterations , and is the most computationally - intensive stage .",
    "efficient multicore approaches have been implemented in @xcite .",
    "performance with respect to non - uniform memory access ( numa ) architecture in supercomputer nodes has been studied in @xcite .",
    "when the lanczos algorithm has completed , mfdn uses the eigenvalues and eigenvectors to calculate other observables , which can then be compared to experiment .    despite being computationally intensive ,",
    "the lanczos algorithm stage is not an easy target for gpu acceleration ; it is memory - bound , and also can not be easily broken down into gpu - parallelizable pieces . in the matrix construction stage , however , each many - body matrix element can be calculated independently .",
    "furthermore , in the current implementation of mfdn , each 3-body matrix element that is needed in the many - body matrix must be obtained by performing a change of basis on the input 3-body potential .",
    "this part of the code is very computationally intensive , and we implement gpu acceleration at the level of this basis transformation .",
    "one method for storing the @xmath17-body input interaction matrix is to use the coupled-@xmath18 basis , which adds , or `` couples '' the angular momenta of the three single particle states ( spss ) together into one total angular momentum for the @xmath17-body state .",
    "isospin , a quantum number that has to do with whether a nucleon is a proton or a neutron , is similarly coupled .",
    "this basis exploits the rotational symmetry of the interaction to reduce the amount of information that must be stored .",
    "however , for constructing the many - body matrix elements , we need 3-body interaction matrix elements in an @xmath19-scheme basis ; that is , we need to `` decouple '' these coupled-@xmath18 matrix elements every time we need a 3-body matrix element in the construction of the many - body matrix .",
    "storing the 3-body interaction matrix in @xmath19-scheme would be more efficient for the calculations , but requires much more memory : in one representative case , a @xmath17-body interaction is @xmath20 gb in the @xmath19-scheme basis , but only @xmath15 gb in the coupled-@xmath18 basis .",
    "substantial memory savings can thus be achieved by storing the input matrix in - core in the coupled-@xmath18 basis , and calculating @xmath19-scheme elements individually as they are required by mfdn @xcite .",
    "when mfdn requires a @xmath17-body input interaction matrix element , then , it must convert that element from the coupled-@xmath18 basis . from linear algebra ,",
    "basis transformations of matrices are of the form @xmath21 , where @xmath22 is a matrix of projections from one basis to the other .",
    "figure [ matrix_multiply ] shows a high - level illustration of this transformation . as in any basis transformation , an element in the new basis is a linear combination of elements from the old basis , weighted by the projections in @xmath22 .    in the coupled-@xmath18 to @xmath19-scheme transformation",
    ", @xmath22 is developed from a series of angular momentum and isospin coupling coefficients .",
    "the matrices @xmath23 , @xmath22 , and @xmath24 are never actually constructed in their entirety .",
    "mfdn requests @xmath17-body elements from @xmath23 one - at - a - time , and elements of @xmath22 are developed as needed for each request ; figure [ matrix_multiply_black ] illustrates the calculation of a single element from @xmath25 .    in principle",
    "an element of @xmath25 is a linear combination of all elements from @xmath23 .",
    "many elements from @xmath23 do not contribute , however , because of orthogonality relations that manifest as zeroes in @xmath23 and @xmath22 .",
    "this sparsity structure is highly predictable and can be exploited . @xmath23 and @xmath25",
    "can be divided into blocks in a way that allows any element in a block in @xmath23 to be constructed entirely from elements in the corresponding block in @xmath25 .",
    "furthermore , all the nonzero elements within that block can be iterated over with a set of nested loops over coupled angular momentum and isospin values .",
    "only these potentially - nonzero elements are stored , and they are arranged in the order that the nested loops will reference them .",
    "the basis conversion routine , then , consists of locating the start of the correct block , going through the nested angular momentum and isospin loops , and adding the elements of @xmath23 one after the other , each weighted by coupling coefficients calculated from the corresponding angular momentum and isospin values . in practice , because the relevant isospin space is so small , the isospin coupling coefficients are precalculated with several conditionals , and the isospin loops are unrolled into a single weighted summation .",
    "the core of the calculation , then , is a set of three nested loops over coupled angular momentum values .",
    "is transformed from the coupled-@xmath18 basis to the @xmath19-scheme basis through multiplication with @xmath22 and @xmath24.,scaledwidth=80.0% ]     can be calculated from the indicated elements in @xmath23 , @xmath22 , and @xmath24 . in practice ,",
    "not all the indicated elements are used , as orthogonality relations dictate that many of them are zeroes.,scaledwidth=80.0% ]      the bounds of the inner angular momentum decoupling loop depend on the positions in the outer loops , and the bounds of the outer loops depend on which @xmath17-body element is being calculated .",
    "the loop structure is thus irregular .",
    "this irregularity makes it difficult to map gpu threads to parts of the problem when dedicating more than one thread to the calculation of a single @xmath17-body element .",
    "we parallelized with one @xmath17-body element calculation per gpu thread , invoking the kernel across many @xmath17-body element calculations at once .",
    "we used cuda to interface with the gpu .",
    "we put the nested loop structure into the kernel without modifying it significantly , and added a wrapper function to transfer a chunk of @xmath17-body element requests to the gpu , invoke the kernel , and transfer the results back .",
    "mfdn and the decoupling code flatten the sps quantum numbers into a single linear sps index , so a @xmath17-body element request takes the form of a set of six sps indices .",
    "we also flattened several arrays that were multidimensional in the cpu - only code so that they could be transferred to and referenced on the gpu more quickly .    before integrating this gpu acceleration strategy into mfdn",
    ", we applied it to a standalone version of the basis transformation code to test performance and act as an intermediate step ; we observed a @xmath26x to @xmath27x speedup compared to a multithreaded cpu implementation running on eight cores @xcite .",
    "in the matrix construction stage of mfdn , the many - body hamiltonian is divided into chunks of elements with similar quantum numbers , and these chunks are apportioned across mpi processes .",
    "each process then splits into openmp threads to count , locate , and calculate the nonzero many - body elements .",
    "elements are stored per mpi process in a single array , with a list of column pointers to split them into columns and a secondary array to denote their locations in their columns ( compressed row format , or csr ) .",
    "the nonzero elements are located in one of two ways .",
    "if a block is denser , all elements in it are iterated through and tested for being nonzero .",
    "if the block is sparser , all combinations of quantum numbers that could yield nonzero elements are iterated through .",
    "mfdn runs through nonzero elements twice during the many - body matrix generation : once to count the nonzero matrix elements so the appropriate arrays can be allocated , and once to calculate them .      during the construction of the many - body matrix",
    ", mfdn uses a recursive loop to calculate the nonzero many - body matrix elements , frequently requesting 3-body matrix elements in @xmath19-scheme . in the cpu - only version of mfdn ,",
    "the decoupling code calculates @xmath17-body elements one - by - one , as they are requested .",
    "the gpu version of the decoupling code requires a large block of simultaneous requests to be efficient , so the sequential requests in the cpu code are not ideal . to bridge the gap between mfdn and the gpu decoupling code , we use buffers to store lists of @xmath17-body element requests so large `` chunks '' of requests can be sent to the gpu at once .",
    "each openmp thread has its own buffer allocated to store requests . on receiving a request , the cpu part of the decoupling subroutine stores the request in the buffer and returns @xmath28 .",
    "in the cpu - only version , the returned value is added directly to the many - body element under calculation ; we must thus also store which many - body element the request pertains to so that it can be added to the correct many - body element when the calculation finishes on the gpu . furthermore , the @xmath17-body element is added with a specific phase , which must also be stored with the request .",
    "larger buffers are more efficient because the overhead associated with the single cuda memory copy and kernel invocation is split over more elements .",
    "tests with the standalone code indicate diminishing returns after around 20,000 elements @xcite on the supercomputer dirac at the national energy research scientific computing center .",
    "hence , we use here buffers of approximately this size , although further testing may be required to optimize buffer size for the integrated code and for different hardware .",
    "each openmp thread starts in the so - called `` accumulating mode '' while passing element requests to its buffer until it is full .",
    "then , the thread sends the buffer to the gpu and switches into the so - called `` non - accumulating mode '' . in this mode",
    ", the decoupling code runs as in the cpu - only version of mfdn , calculating @xmath17-body elements on the cpu at request and returning them ; this allows the cpu to continue work while the gpu , which may be shared among many openmp threads , is busy .",
    "the thread checks periodically for a completed chunk from the gpu .",
    "when it receives the chunk , it iterates through the returned three - body elements in the chunk , multiplies them by the stored phases , and adds them to the array of many - body elements at the stored locations .",
    "it then switches back into accumulating mode , and the cycle begins again . at the end of the many - body matrix construction phase",
    ", all the @xmath17-body contributions have been added in , either from the gpu calculations or directly from the cpu decoupling code .",
    "we allow the gpu to decide which requests to calculate first .",
    "each openmp thread is given its own stream to access the gpu , so at any given time the gpu will have a number of requests open . with this scheme , a single gpu may end up being accessed by multiple mpi processes .",
    "this is not possible on all systems , and incurs a performance penalty when it is possible .",
    "one alternative is to restrict the number of mpi processes to the number of nodes used ( one per node ) .",
    "the resulting mpi / openmp divisions , however , may not be ideal for the numa structure of the node , which may incur a performance penalty due to data locality and thread contention issues  @xcite .",
    "another alternative is to gpu - accelerate only some mpi processes , leaving the others to run as in the cpu - only version .",
    "however , using this alternative would derange the almost perfect load - balancing , which is a prominent design feature of the cpu - only version of mfdn , and thus , diminish the benefit of the acceleration . given these considerations ,",
    "we opt to accept the performance penalty from multiple mpi processes per gpu on systems where that is possible , and restrict the number of mpi processes to the number of nodes on other systems .",
    "the decoupling code requires the use of a six - dimensional index array , which relates quantum numbers to locations in the input coupled-@xmath18 interaction matrix .",
    "this array is highly jagged : the length along any particular dimension is not constant , but rather depends on the position along the other dimensions . in c ,",
    "jagged arrays are represented as trees of pointers , wherein the `` root '' pointer points to an array of pointers , each of which point to further arrays of pointers , and thus until the `` leaves '' , which hold the actual data of the array .",
    "generating such a structure requires a prodigious number of allocations , as each array at each level must be allocated . on the cpu",
    "this is not an issue , but allocations on the gpu must first be requested from the cpu , adding a significant transfer time penalty .",
    "our first attempt did not take this inefficiency into account , and over @xmath29 of the matrix construction time was spent generating the index array on the gpu .    to address this problem",
    ", we generated the entire index array in a contiguous block on the cpu , producing a structure wherein the pointers were correct relative to each other .",
    "we then applied a constant offset to all the pointers in the index array so that their absolute coordinates would be correct for a contiguous block allocated on the gpu .",
    "the entire pointer structure could then be copied into that block with one copy , allowing the index structure to be created on the gpu with a single gpu allocate and gpu copy . with this improvement",
    "the index array creation time becomes negligible in the overall matrix construction .",
    "we present results from the doe supercomputer titan at the oak ridge national laboratory .",
    "titan is a cray xk7 supercomputer with 18,688 physical compute nodes , each of which has one 16-core 2.2 ghz amd opteron 6274 processor and 32 gb of ram .",
    "each node is divided into two numa domains , and nodes are served in groups of two by gemini high - speed interconnect routers .",
    "additionally , each node has one nvidia k20 tesla gpu accelerator with 6 gb of memory .",
    "we use the number of non - zeroes in the many - body matrix as a measure of problem size , and test at a variety of problem sizes .",
    "the number of nonzeroes is determined by the choice of nucleus and truncation , so it is difficult to provide a smooth spectrum .",
    "different problem sizes can require vastly different numbers of cores to store the many - body matrix , so we do not test all problems sizes on the same configuration ; for each problem size , we allow the many - body matrix to take up half of the total memory , and choose the smallest configuration that satisfies that requirement .",
    "we implement and test gpu acceleration in mfdn version 14 , and compare against cpu performance using an unmodified build of version 14 .",
    "our primary results are summarized in figures [ construction_time ] and [ speedup ] : we see a speedup of 2.2x  ",
    "2.7x in the matrix construction stage .",
    "there is no immediately apparent pattern in the dependence of speedup on problem size .",
    "the choices of nuclei and truncation parameters required to generate the spectrum of problem sizes are somewhat haphazard , so it is possible that any problem size dependence has become entangled with dependence on those parameters . despite the individually varying speedup , the range of speedups appears to stay roughly the same ; thus the gpu acceleration appears to scale well for the problem sizes investigated .",
    "the speedup over the entire run is a more ambiguous quantity .",
    "the time taken in the mfdn diagonalization stage depends on how many eigenvalues are required and the accuracy to which they are required to converge .",
    "the speedup over the entire run , which depends on the relative times of the matrix construction and diagonalization stages , therefore depends on these parameters also . for the representative parameter choices used in the matrix construction speedup calculations ,",
    "the overall speedup is in the 1.2x  1.4x range .",
    "we have modified the matrix construction stage of mfdn to run partly on the gpu .",
    "the current mfdn implementation stores the matrix elements of the @xmath17-body input interaction in the compressed coupled-@xmath18 basis in - core .",
    "the conversion of these elements back to @xmath19-scheme for use with mfdn is highly parallelizable ; we implement this basis transformation on the gpu .    initial timing results with the gpu - accelerated mfdn code are promising .",
    "we have achieved a consistent speedup in the two- to three - fold range for the matrix construction stage , and our speedup scales smoothly to larger problem sizes , at least for those investigated in this paper .",
    "it may be possible , though more difficult , to leverage gpu acceleration in the diagonalization stage , or at a higher level in the matrix construction stage ; such improvements are left for future consideration .",
    "this work was supported in part by iowa state university under the contract de - ac02 - 07ch11358 with the u.s .",
    "department of energy ( doe ) , by the u.s .",
    "doe under the grants desc0008485 ( scidac / nuclei ) , and de - fg02 - 87er40371 ( division of nuclear physics ) , by the director , office of science , division of mathematical , information , and computational sciences of the u.s .",
    "department of energy under contract number de - ac02 - 05ch11231 , and in part by the national science foundation grant nsf / oci-0941434 , 0904782 , 1047772 . this research was also supported ( in part ) by the dfg through sfb 634 and by hic for fair .",
    "this research used resources of the oak ridge leadership computing facility at the oak ridge national laboratory , which is supported by the office of science of the u.s .",
    "department of energy under contract no .",
    "de - ac05 - 00or22725 , and resources of the national energy research scientific computing center , which is supported by the office of science of the u.s .",
    "department of energy under contract no .",
    "de - ac02 - 05ch11231 .",
    "h. m. aktulga , c. yang , e. g. ng , p. maris and j. p. vary , _ topology - aware mappings for large - scale eigenvalue problems _ , in _ euro - par 2012 parallel processing _ , eds . c. kaklamanis , t. s. papatheodorou and p. g. spirakis , lncs * 7484 * , 830 ( 2012 ) .",
    "h. m. aktulga , c. yang , e. g. ng , p. maris and j. p. vary , _ improving the scalability of a symmetric iterative eigensolver for multi - core platforms _ , concurrency computat . :",
    "doi : 10.1002/cpe.3129 ( 2013 , in press ) ."
  ],
  "abstract_text": [
    "<S> this paper describes some applications of gpu acceleration in ab initio nuclear structure calculations . </S>",
    "<S> specifically , we discuss gpu acceleration of the software package mfdn , a parallel nuclear structure eigensolver . </S>",
    "<S> we modify the matrix construction stage to run partly on the gpu . on the titan supercomputer at the oak ridge leadership computing facility , this produces a speedup of approximately @xmath0x  </S>",
    "<S> @xmath1x for the matrix construction stage and @xmath2x  </S>",
    "<S> @xmath3x for the entire run . + </S>",
    "<S> * keywords : * _ configuration interaction ; no - core shell model ; ab initio nuclear structure ; gpu acceleration ; titan supercomputer _    * *    accelerating ab initio nuclear physics calculations with gpus     +    @xmath4__department of physics and astronomy , iowa state university , ames , ia 50011 , usa _ </S>",
    "<S> _ + @xmath5__department of electrical and computer engineering , iowa state university , ames , ia 50011 , usa _ _ + @xmath6__ames laboratory , ames </S>",
    "<S> , ia 50011 , usa _ _ </S>",
    "<S> + @xmath7__department of modeling , simulation and visualization engineering , old dominion university , norfolk , va 23529 , usa _ _ </S>",
    "<S> + @xmath8__institut fr kernphysik - theoriezentrum , technische universitt darmstadt , 64289 darmstadt , germany _ _ </S>",
    "<S> + @xmath9__departments of biomedical informatics , electrical and computer engineering , and computer science and engineering , ohio state university , columbus , oh 43210 , usa _ _ </S>",
    "<S> + @xmath10__department of computer science , university of north carolina at charlotte , charlotte , nc 28223 , usa _ _ </S>"
  ]
}