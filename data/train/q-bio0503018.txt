{
  "article_text": [
    "large - scale comprehensive protein - protein interaction data , which have become available recently , open the possibility of deriving new information about proteins from their associations in the interaction graph . in the following ,",
    "we discuss and compare several probabilistic methods for predicting protein functions from the functions of neighboring proteins in the interaction graph .",
    "in particular , we compare two recently published methods that are based on markov random fields @xcite with a prediction based on a machine - learning appproach using maximum - likelihood parameter estimation .",
    "it turns out that all three approaches can be considered different versions of each other using different approximations . the main difference between the markov random field ( mrf ) and",
    "the machine - learning methods is that the former apprach takes a global look at the network , while the latter considers each networks node as an independent training example .",
    "however , in the mean - field approximation required to make the mrf approach numerically tractable , it is reduced to considering each node independently . the local enrichment - method considered in @xcite",
    "can then be interpreted as another approximation which enables us to make predictions directly from observer frequencies , bypassing the numerical minimization step required in the more general machine - learning approach .",
    "we also extend these methods by considering a non - linear generalization for the probability distribution in the machine - learning approach , and by taking larger neighborhoods in the network into account .",
    "finally , we compare the performance of these methods to a standard supper vector machine .",
    "we consider a network specified by a graph whose nodes are proteins and whose undirected vertices indicate interactions between the proteins .",
    "each node is assigned one of a set of protein functions . in a machine - learning approach to prediction",
    ", this assignment follows a simple probability function depending on the protein functions in the network neighborhood of each node and parametrized by a small set of parameters .",
    "the learning problem is to estimate these parameters from a given sample of assignments .",
    "the prediction can then be performed by evaluating the probability distribution using these parameters .",
    "assume we only consider a single protein function at a time .",
    "node assignments can then be chosen binary , @xmath0 , with @xmath1 indicating that a node has the function under consideration . in the simplest case ,",
    "the probability that a node @xmath2 has assignment @xmath3 depends only its immediate neighbors , and since all vertices of the graph are equal , it can only depend on the number of neighbors @xmath4 , and the number of active neighbors @xmath5 .",
    "borrowing from statistical mechanics , we write the probability using a potential @xmath6 @xmath7 where the partition sum @xmath8 is a normalizing factor .",
    "this equation basically expresses that the log - probabilities of @xmath3 are proportional to the potential @xmath6 . in a lowest - order approximation",
    ", we can choose a linear function for the potential : @xmath9 later , we will extend this approach to more general functions .",
    "the parameters @xmath10 can be estimated from a set of training samples @xmath11 by maximum - likelihood estimation . in this approach , they are chosen to maximize the joint probability @xmath12 of the training data , or equivalently , to minimize its negative logarithm @xmath13 \\quad.\\ ] ] taking the partial derivative w.r.t .  to a parameter",
    "gives the equation @xmath14 the first term in the bracket is the expectation value of @xmath15 in the neighborhood @xmath16 under the probability distributions parametrized by @xmath17 : @xmath18 at the extremum , the derivative vanishes and we have the simple relation @xmath19 thus , in the maximum - likelihood model , the parameters are adjusted so that the average expectation values of the derivatives of the potential are equal to the averages observed in the training data .",
    "using eq .",
    "[ eq:1 ] , this gives the set of three equations .",
    "@xmath20 where the expectation value of @xmath3 in the environment @xmath16 and in the model parametrized by @xmath10 is given by @xmath21 only in the simplest case , @xmath22 , this equation can be solved analytically , leading to the relation : @xmath23 in the general case , we solve these equations numerically using a conjugate - gradient method by explicitly minimizing the joint probability @xmath24 .      an alternative approach to prediction starts out from considering a given network and the protein function assignments as a whole and assigning a score based on how well the network and the function assignments agree . in the approach of @xcite , each link contributes to this score with a gain @xmath25 or @xmath26 , resp .",
    ", if both nodes at the ends of the link have the same function @xmath27 or @xmath1 , and a penalty @xmath24 if they have different function assignments . assuming again that the log - probabilities are proportional to the scores , this induces a probability distribution over all joint function assignments @xmath28 given by @xmath29 where now the normalization factor is calculated by summing over all possible joint function assignments of the nodes .",
    "the scoring function @xmath30 can be expressed as @xmath31 with the parameters @xmath32 in terms of statistical mechanics , this describes a ferromagnetic system where the inverse temperature is determined by @xmath33 and an external field by @xmath34 and @xmath35 .    again",
    ", maximum - likelihood parameter estimation is performed by finding a set of parameters @xmath36 such that the probability of the @xmath5 sample configurations @xmath37 is maximized : @xmath38 the logarithm of the partition sum appearing in the second term can be related to the entropy by @xmath39 the quantity @xmath40 is the thermodynamical free energy .",
    "maximum likelihood parameters estimation therefore corresponds to choosing the parameters such that the energy of the given configuration is minimized while the free energy of the system as a whole is maximized : @xmath41 unfortunately , this requires the calculation of both the internal energy , @xmath42 , and the entropy , @xmath43 , of the system and thus more or less a complete solution of the system .",
    "this can be avoided by employing the _",
    "mean field _",
    "approximation , in which the probability distribution @xmath44 is replaced by a trial distribution @xmath45 as a product of single - variable distributions @xmath46 which can be completely parametrized by the expectation values @xmath47 using @xmath48 optimum values for the parameters @xmath49 can then be estimated by minimizing the kl entropy of @xmath45 vs.  the true distribution @xmath44 .",
    "interestingly , this approximation removes the distinguishing feature of the network approach , namely that the neighborhood structure ( in the sense of neghbors of neighbors ) is taken into account .",
    "the resulting equations are very similar to the machine - learning equations in which neighbors are treated as unrelated .",
    "the binomial - neighborhood approach @xcite is a simpler approach in which the probability distribution @xmath50 is chosen in such a way that it can be directly derived from observed frequencies without the minimization process typical for maximum - likelihood approaches .",
    "it is based on the assumption that the distribution of active neighbors @xmath51 of a node @xmath2 follows a binomial distribution whose single probability @xmath52 depends on whether the node @xmath2 is active or not : @xmath53 and correspondingly for @xmath54 using a single probability @xmath55 .",
    "this is the assumption of _ local enrichment _ ,",
    "i.e.  that the probability @xmath56 to find an active node around another active node is larger than the probability @xmath55 to find an active node around an inactive node . using bayes theorem , we can use this to calculate the probability distribution of @xmath57 : @xmath58 where @xmath59 is the overall probability of observing an active node , and @xmath60 the resulting probability distribution can be written as @xmath61 with @xmath62 this can be easily rewritten in the same form as ( [ eq:2 ] ) @xmath63\\ ] ] the first term in the potential has the same form as ( [ eq:3 ] ) and adjusts the overall number of positive sites ; the two other terms constitute a bones for having positive neighbors ( proportional to @xmath51 ) and a penalty for having negative neighbors ( proportional to @xmath64 ) .",
    "this approach evidently gives a conditional probability distribution @xmath65 of the same for as the one in the machine - learning approach .",
    "however , the coefficient in the potential can be directly calculated from the observed frequencies @xmath66 , @xmath55 , and @xmath56 .",
    "this is only possible because we made here the assumption that the probability distribution @xmath67 is binomial .",
    "the machine - learning approach is more flexible in that in does not have to make this assumption and yields a true maximum - likelihood estimate even for distributions that deviate greatly from binomial form .",
    "in particular , the binomial distribution implies that the neighbors of a node behave statistically independent , which might be violated in a densely connected network , where we would expect clusters to form .",
    "to compare the different prediction methods , we chose the mips protein - protein interaction database for _ saccharomyces cerevisiae _ @xcite and the go - slim database of protein function assignments from the gene ontology consortium @xcite .",
    "the latter is a slimmed - down subset of the full gene ontology assignments comprising 32 different processes , 21 functions , and 22 cell compartments .",
    "we focused here on the process assignments as these were expected to correspond most closely to the interaction network .",
    "we compared four methods :    1 .   the binomial neighborhood enrichment from sec .",
    "[ sec : bin ] , 2 .",
    "the machine - learning maximum - likelihood method from sec .",
    "[ sec : ml ] using a linear potential ( [ eq:1 ] ) 3 .   the same method with an extended non - linear potential , and 4 .   a standard support vector machine @xcite .    for the probabilistic methods , we first looked at the single - function prediction problem in which the system is presented with a binary assignment expressing",
    "which proteins are known to have a given function , and then makes a prediction for an unknown protein based on the number of neighbors that have this function .",
    "-axis , and the number of neighbors having the funtion of interested on the @xmath68-axis .",
    "the numbers indicate the total incidence of the situation , while the shading expresses how frequently the central node had the function of interest in that situation .",
    "the lines are the decision boundaries for the binomial method and the linear and polynomal machine - learning methods .",
    "the shading is the prediction region from the svm . ]    in this case , the local environment of a node can be described by two numbers : @xmath69 , the number of neighbors , and @xmath70 , the number of neighbors that have the function assignment under consideration .",
    "the content of the training data set can be characterized by a glyph plot such as in fig .",
    "[ fig:1 ] .    after learning the training data ,",
    "the probabilistic method has inferred a probability distribution that yields , for each pair @xmath71 , a probability @xmath72 which is then utilized for predictions .",
    "the 50%-level of this probability , which determines the prediction in a binary system , is indicated in fig .",
    "[ fig:1 ] by green lines .",
    "the three probabilistic predictors in fig .",
    "[ fig:1 ] yield similar results that differ rarely by more than one box .",
    "the main difference is that the binomial predictor is restricted to a straight line , while the linear and non - linear maximum - likelihood predictors can accomodate a little turn .",
    "linear and non - linear predictors differ only minimally .",
    "[ fig:2 ]    finally the prediction from a support vector machine that was trained on the same single - function data set is indicated by a shaded area marking all those @xmath71 for which the svm returned a positive prediction .",
    "the border of this area very closely follows the linear and non - linear m.l .  predictors .    fig .",
    "[ fig:2 ] shows a sensitivity - specificity curve using five - fold cross validation for single - function prediction using the probabilistic predictors .",
    "again , all three curves follow each other quite closely , with a slight edge for the nonlinear m.l .  predictor .",
    "the preceding discussion applied to the problem of single function prediction . to perform full prediction",
    ", we generated each of the three predictors separately for each function and chose , for each protein with an unknown function , the prediction with the largest probability .",
    "for simplicity , this approach does not take into account possible correlations between different protein functions .",
    "however , such correlations were taken into account for the support vector machine , which generated a full set of cross - predictors ( predicting function @xmath2 with neighbors of type @xmath70 ) .",
    "[ fig:3 ]    in the probabilistic case , each predictor does not only provide us with a yes - no decision , but also with a probability for the prediction .",
    "we can use the information to restrict the predictions to highly probable ones .",
    "[ fig:3 ] shows the accuracy of the prediction as a function of how many predictions are made with different cut - offs in the predicted probability . again",
    ", all three curves closely follow each other , with maybe a small but unsignificant edge of the linear m.l .  predictor .",
    "the predictions from all predictors including the svm were similar , and combining them would not have improved predictive accuracy .",
    ".prediction accuracy in five - fold cross validation for the yeast data set .",
    "[ cols=\"<,<,<\",options=\"header \" , ]     finally , the success rates for all predictors are shown in table [ tab:1 ] using five - fold cross - validation on a data set of 2014 unique function assignments for the yeast proteome .",
    "it turns out that all four methods perform closely , with success rates between 30 and 33% .",
    "this compares to the null - hypothesis of prediction in a randomized network , in which we would have a success rate of 11% for these data .",
    "the protein - protein interaction data therefore roughly triples the prediction success over a random network .",
    "however , all methods , from the simple , counting - based binomial classifier to the full support vector machine , perform similarly .",
    "we also extended our methods to take larger neighborhoods ( second and higher - order neighbors ) into account , but failed to substantially improve predictive power .    finally , we also performed protein function prediction on a recently published protein - interaction network for _ drosophila melanogaster _",
    "@xcite , with similar results .",
    "we compared different probabilistic approaches to predicting protein functions in protein interaction networks . under closer analysis , the different markov random field methods in the literature",
    "can be related to a basic machine - learning approach with maximum - likelihood parameter estimation . using real data , they exhibit similar performance , with simple methods performing as well as more complex ones .",
    "this might indicate limits on the functional information contained in protein - protein interaction networks .",
    "a standard support vector machine gave similar result , though it was equipped with more information , namely the frequencies of all function classes in the neighborhood .",
    "the additional information did neither improve nor harm predictive performance .",
    "9999 s. letovsky , s. kasif , bioinformatics * 19 * , suppl . 1 , i197 ( 2003 ) . m. deng , t. chen , f. sun , in : proceedings , recomb 03 , 7th international conference on research in computational molecular biology , p.  95",
    ", acm press , new york , ny ( 2003 ) .",
    "l. giot _ et .",
    "science * 302 * , 1727 ( 2003 ) .",
    "p. uetz _ et .",
    "nature * 403 * , 623 ( 2000 ) . h. w. mewes _ et .",
    "nucleic acids research * 32 * , d41 ( 2004 ) .",
    "the gene ontology consortium , nucleic acids res * 32 * , d258 ( 2004 ) .",
    "chang , c .- j .",
    "lin , libsvm : a library for support vector machines , 2001 .",
    "software available at * http://www.csie.ntu.edu.tw/  cjlin / libsvm *      uetz p , giot l , cagney g , mansfield ta , judson rs , knight jr , lockshon d , narayan v , srinivasan m , pochart p , qureshi - emili a , li y , godwin b , conover d , kalbfleisch t , vijayadamodar g , yang m , johnston m , fields s , rothberg jm ."
  ],
  "abstract_text": [
    "<S> we discuss probabilistic methods for predicting protein functions from protein - protein interaction networks . </S>",
    "<S> previous work based on markov randon fields is extended and compared to a general machine - learning theoretic approach . using actual protein interaction networks for yeast from the mips database and </S>",
    "<S> go - slim function assignments , we compare the predictions of the different probabilistic methods and of a standard support vector machine . </S>",
    "<S> it turns out that , with the currently available networks , the simple methods based on counting frequencies perform as well as the more sophisticated approaches . </S>"
  ]
}