{
  "article_text": [
    "both written and spoken languages have complex grammatical structure that dictates how information can be communicated from one individual to another . recently , complex network theory has become one of the tools used to study the structure and dynamics of languages  @xcite .",
    "the study of _ semantic networks _ is an interdisciplinary endeavor spanning many fields , including statistical physics , information theory , linguistics , and cognitive science .",
    "semantic networks can refer to experimentally testable organic models of functioning in the brain  @xcite as well as statistical models of connections and patterns in texts .",
    "although the two may in fact be related , it is particularly the latter form of semantic network that interests us here ; such networks have been constructed from lexicons , dictionaries , prose and poetic fictional literature , and thesaurus networks  @xcite .",
    "dorogovtsev and mendes have even provided an analytical model for the time - evolution of a language network  @xcite .",
    "most of these approaches study the topology of a single network , sometimes composed of many samples in order to better understand the structure of language .",
    "however , in the same way that the overall structure and behavior of a language network is important , so too is the use of a language for particular forms of communication important . specifically , there are many questions that could be asked about substructures that arise in a language , such as what is the difference between networks that are composed of samples from different literary styles ?",
    "can we use complex network theory to distinguish between different types of literature ?",
    "what is the critical number of words in a piece of literature at which these differences become apparent to the reader , and how might we mathematically model this kind of pattern recognition ?",
    "this article addresses these questions .",
    "prior to approaches using semantic networks , there are numerous studies doing either the classification of text into a variety of subject area , or , closer to the present study , the classification into genres .",
    "text classification is of particular interest in query and retrieval systems , since restricting to texts that are concerned with the same topics as the query can improve efficiency and correctness .",
    "typically , some features are extracted from the text , such as lists of words or sets of adjacent word stems and their frequencies , sometimes with meanings or qualities attached",
    ". then a training set of texts with known classifications is used with some methodology such as linear maps to a small set or just one value , with cutoff values experimentally determined to give the best classification .",
    "other methodologies include bayesian independence probabilities , neural networks , etc .",
    "then these classification methods are applied to new texts , and their effectiveness measured .",
    "useful surveys of the contexts , methods and literature in this area are @xcite .",
    "effectiveness varies , as does the complexity of methods and amount of specialized knowledge embedded in them . because the number of categories tends to be somewhat large ( the often used reuters collection having nearly 100 , and the ohsumed collection has more than 10,000 , with multiple categories applicable to each text ( see @xcite ) , accuracy rates are not directly comparable to a binary genre classification into fact or fiction as in the present article .    the narrower field of automated genre classification has a now extensive literature as well .",
    "similar considerations apply as for more general text classifications .",
    "search and retrieval remain applications of interest , as well as the automated organizing of digital libraries .",
    "this is distinct from the classification of texts by subject , as texts on the same subject may be in different genres . in @xcite",
    "we see an account of various classifications , including into two categories of informative and imaginative , which in 500 texts had error rates of 4% and 5% respectively .",
    "discriminant analysis was run against the brown corpus of english text samples of equal lengths on a score of features , such as counts of word lengths , first person pronouns , sentences , etc .",
    "the training and test sets were the same , and no attempt to estimate estimation variances was made .",
    "when applied to a larger number of sub - genres , the error rates went up .",
    "the larger number of features adds to the effectiveness of the discriminant analysis .",
    "so this is similar in some respects to the present article .",
    "this article may then be considered as a proof of concept that measures derived from complex networks have validity as well as those of simple counts or with language or subject specific knowledge encoded in the methods .",
    "recall that previous studies explicitly using networks did so with a single network .",
    "this article departs from those approaches . in lieu of a single network composed of many literature samples , we chose to represent each literature sample as its own undirected , unweighted network . from these networks",
    ", we were able to compare the power law behavior of degree , clustering , and mean geodesic distance , all established useful measures in complex network theory .",
    "complex network theory has studied many aspects of human behavior , such as mobility patterns  @xcite . here",
    "we study on the basic human activity of storytelling .",
    "we show that complex network theory can be used to distinguish between storytelling in different forms : in novels , where the stories are fictional , and in news stories , where the stories are non - fictional .",
    "our article is outlined as follows . in sec .",
    "[ sec : model ] we describe the semantic network model that was used to create complex networks from our text samples , providing an explicit example based on a quote from feynman . section  [ sec : analysis ] details the different kinds of behavior the networks exhibited and our method of exploiting this behavior in order to distinguish between two modes of storytelling .",
    "section  [ sec : conclusions ] lists our conclusions as well as outlook for future studies .",
    "words and the meanings that they represent are foundational to the grammar of a language , but without a conceptual structure within which to arrange them there can be no communication of coherent thought .",
    "thus the meaning of a single word is equally important as understanding the context of the sentence , paragraph , extended text , or story in which that word appears .",
    "although conjugation , the use of singular or plural , and other grammatical forms contribute to the understandability and elegance of a story , nevertheless the essence of the story can be communicated in pidgin english or by a child , without proper use of all grammatical rules .",
    "these observations form the qualitative basis for our model .    using a variation of the model proposed by cancho and sole  @xcite",
    ", we constructed an unweighted , undirected semantic network model that take a single text sample as a network .",
    "each vertex in the network represents all occurrences of a unique word in the text .",
    "thus a text of @xmath8 words has in general @xmath0 vertices , where @xmath9 and usually @xmath10 ; for example , all appearances of `` and '' in a text would count as a single vertex .",
    "two vertices are assigned a link or edge between them if the words that they represent appear within @xmath1 words of each other in the text sample .",
    "we call @xmath1 the _ word distance_. since our semantic network is unweighted , once two vertices have an edge , further edges appearing from other occurrences of the same two vertices are not counted . to avoid focusing on grammatical forms and extract storytelling forms as independent of grammatical usage as possible , we allow a single vertex to not only represent all occurrences of its assigned word , but to also represent all lexical forms of that word .",
    "for example , `` eat '' , `` eats '' , and `` eaten '' would be represented by the same vertex in our model .",
    "contractions were considered to be two different words , and all occurrences of `` s '' were excluded because of the difficulty in distinguishing between the possessive form of an `` s '' word and the contraction of the word with `` is '' .    of course , more complicated semantic network models are certainly possible .",
    "for instance , one could construct a weighted network .",
    "however , we sought the simplest possible model which could distinguish between fictional and non - fictional written storytelling .    in order to illustrate our model",
    "we will analyze a quote by richard feynman : `` to those who do not know mathematics it is difficult to get across a real feeling as to the beauty , the deepest beauty , of nature ... '' if we choose @xmath11 then fig . [ toynetwork ] displays the resulting network .",
    "notice how the word `` beauty '' is assigned to only one vertex .",
    "that vertex is then connected to the @xmath12 nearest neighbors of both instances of `` beauty '' .",
    "these @xmath12 neighbors are `` to '' , `` the '' , `` the '' , and `` deepest '' , from the first occurrence and ` the' , `` deepest '' , `` of '' , and `` nature '' . one can see also that there is only one edge between `` the '' and `` beauty '' and no self edges on `` beauty '' .",
    "our model does not allow self loops nor does it allow multiply connected pairs of vertices in order to ensure that the resulting adjacency matrix is indeed unweighted .",
    "any and all punctuation is also ignored in the text sample .",
    "this results in connections across commas , periods , and other forms of punctuation .",
    "thus our model considers the overall connectivity of a text independent of individual sentences .",
    ".[toynetwork],width=321 ]      the main two network measures that we used were the degree of a vertex and the clustering coefficient of a vertex .",
    "the degree of a vertex is the number of edges associated to the vertex , i.e. , its number of connections . in an unweighted adjacency matrix ,",
    "the degree ( @xmath13 ) of the @xmath14th vertex can be calculated by summing over the @xmath14th column of the adjacency matrix @xmath15 .",
    "@xmath16 where @xmath0 is the number of vertices in the network .",
    "the clustering coefficient @xmath17 of a vertex describes how well connected the vertex is to the rest of the network by summing over all closed paths involving two other vertices : @xmath18 the clustering coefficient is normalized between 0 and 1 , with 1 being fully connected and 0 being not connected at all .    in order to extract fundamental features from degree and cluster , we consider the distribution of values over each data set , i.e. , each text sample . for degree",
    "we found @xmath2 , the number of vertices having degree @xmath13 , to be the most useful quantity to consider .",
    "then we fit two separate power laws to @xmath2 for @xmath19 and @xmath20 , as we discuss in more detail in sec .",
    "[ sec : analysis ] . the particular distribution chosen and the power law fits",
    "were determined after different types of curves had been fit to various distributions . for example , we also fitted both exponentials and poisson distributions to @xmath2 , and we considered a single power law fit without the break at @xmath21 ; however , all were poor fits for the data set according to their reduced @xmath22 .",
    "for the clustering we found the distribution @xmath3 to be most useful .    with our network model , the linguistic significance of the degree distribution @xmath2 can be interpreted as follows .",
    "the degree distribution can be understood as the fraction of words in the network which appear a certain number of times . because a word gains an upper bound of @xmath12 connections every time that it appears in the text sample , @xmath23 is a lower bound on the number of times a word is present in the text , where @xmath13 is the degree of a vertex .",
    "a word that appears more than once has a finite probability of being next to a word to which it is already connected , thus @xmath24 may be smaller than the total number of instances of a word .",
    "different network measures can mean very different things in separate types of networks .",
    "for semantic networks , the clustering coefficient of a word can be interpreted as a measure of the breadth of the word s usage or the number of times it occurs within a set phrase . if a word occurs frequently within a certain phrase , then it will tend to have a higher clustering coefficient than a word such as `` a '' , `` an '' , or `` the '' , which are all used very often without regard to any specific phrase .",
    "the higher clustering coefficient value arises from the fact that a word will have more highly connected neighbors due to the multiple instances of the entire phrase in the text sample .",
    "this can be highly affected by the word distance @xmath1 of the network .",
    "phrase neighbors will not be as well connected to each other in an @xmath25 network as they would be in an @xmath11 or higher network .",
    "this could also be described in terms of a hierarchy .",
    "phrases represent modular structures in the network which have higher clustering than the network average  @xcite .",
    "articles such as `` a '' , `` an '' , and `` the '' will tend to not be related to any specific hierarchy of terms , but may be found in many .",
    "another factor that could affect a word s clustering coefficient is the range of terms that can commonly be associated with the word .",
    "for instance , the word `` biological '' can be applied to a narrower range of terms than the word `` blue '' . a house , car , bird , pencil , etc .",
    "can be blue , while fewer of those could be considered to be biological .",
    "this specificity inherent in the meaning of a term as it applies to a field or group of other terms can increase the clustering coefficient of that word . if a term has a narrower range of meaning or a smaller subset of words which it can be related to then it will have a higher clustering coefficient , whereas words that can be used more interchangeably will tend to have lower clustering coefficients .",
    "this relationship is encapsulated in the power - law @xmath3 , the degree dependence of the clustering coefficient .",
    "the range of meaning of a word translates into the word s ability to appear within multiple word hierarchies in the network . in the previous example , `` biological '' can relate to a smaller hierarchy , ( given that specific set of control terms ) than the word `` blue '' .",
    "again , this tendency can be affected by the chosen word distance .",
    "if the parameter @xmath1 grows , the clustering coefficient of a vertex will be less affected by the previously mentioned literary nuances , and more affected by the number of times that it appears in the sample .    as an example of another measure we considered ,",
    "consider the mean geodesic path of the network , also called the closeness , defined as the average shortest path between any two vertices .",
    "the mean geodesic distance @xmath26 of the network is  @xcite : @xmath27 where @xmath28 is the geodesic distance between two vertices @xmath14 and @xmath29 .",
    "when @xmath26 is on the order of @xmath30 then the network is said to have the small world effect  @xcite .",
    "we will discuss @xmath26 as an alternate measure in sec .",
    "[ sec : analysis ] .",
    "to explore storytelling we chose two particular fictional and non - fictional literary forms , novels and news stories .",
    "we contrast our data sets to more obviously distinguishable literary forms such as poetry and prose  @xcite , which have significant differences in structure and often in grammar as well .",
    "our initial analysis was not performed on the entire length of each sample , but was instead limited to between 500 - 1000 words .",
    "subsequently we more finely controlled the size of the networks to determine the number of words required for our analysis to distinguish between the different storytelling modes .",
    "the word networks that we constructed displayed power - laws in their degree distributions , as discussed in sec .",
    "[ sec : model ] , of form @xmath31 where @xmath2 is the number of occurrences of degree @xmath13 within a single text sample , @xmath32 is a proportionality constant , @xmath33 is the power - law coefficient  @xcite , and @xmath34 refers to two distinct regions .",
    "we found that the semantic networks exhibited two distinct power - law regions , as can be seen in fig .",
    "[ degree ] .",
    "the division between the two typically occurred at @xmath4 , where @xmath0 is the total number of words in the network .    .",
    "note the break at @xmath35 necessitating separate power law fits in regions @xmath36 and @xmath37.[degree],width=321 ]    this result is similar to that of cancho and sole , who also found two power - law regimes in their network composed of the british national corpus .",
    "however , our large @xmath13 region was always found to have a weaker power law than our small @xmath13 region , @xmath38 , whereas cancho and sole found the opposite .",
    "this is due to the difference in the size of the networks being used . where their corpus network had a maximum degree on the order of @xmath39 , ours had degree on the order of @xmath40 , which is due to the restricted size of our analysis .",
    "given the size of the texts that were analyzed , degrees @xmath13 above about @xmath41 had dropped to counts that were nearly constantly 1 . even though it is plausible by zipf s law that for sufficiently large samples a single power law would hold farther past this point",
    ", the integral nature of the degree frequencies makes a smooth fit between these halves problematic .",
    "in fact , a natural break at @xmath42 is typical of small world networks : high degree words for @xmath20 represent common words which form the common framework of the english language , while low degree words below @xmath41 are rare words .",
    "we expect that common words like `` and , '' `` the '' , etc .",
    "are used quite differently than specialized words like `` biological . ''",
    "the clustering coefficient distribution was found via various fits to be best represented exhibit by a single power law of form @xmath43 where @xmath3 represents the set of all vertices @xmath14 with clustering coefficient @xmath44 and degree @xmath45 . in eqs .",
    "[ eqn : degreepowerlaw]-[eqn : clusteringpowerlaw ] one can normalize @xmath46 ; however , as we will only utilize the exponents normalization is irrelevant for our analysis .",
    ", here there is only a single power law evident.[cluster],width=321 ]    there are many parameters and values that can be chosen to represent and analyze networks  @xcite . for our simplest possible semantic network",
    "we sought the minimal set of measures needed to distinguish between fictional and non - fictional written storytelling , and found the power law exponents @xmath47 to be optimal .",
    "measures that were tried and discarded in attempting to find a minimal set included magnitudes of the power law fits , small word lengths , average sentence lengths and average distance between verbs .",
    "additional parameters in the linear discriminant analysis would have strengthened the accuracy at the expense of simplicity .",
    "further more detailed analysis is warranted in the future . but",
    "a principal component analysis would not take advantage of the known classifications , and hence may give misleading conclusions as to the importance of different parameters .",
    "figure  [ exponentspace ] shows a distribution over all data sets , with each point representing all three exponents for a novel or a news article .",
    "the plane represents the division that fisher s linear discriminant , discussed later in this section , induces on the data set .     and",
    "@xmath48 from the degree distribution , and @xmath49 from the clustering coefficient[exponentspace ] , together with the best planar separation.,width=321 ]    because small values of @xmath1 , such as the @xmath7 that was found to be optimal , induces oscillations in the degree probability as a function of the degree @xmath13 out to @xmath1 away from the @xmath13s that are divisible by @xmath12 , binning the degrees on the order of every @xmath12 removes this signal that is an artifact of how the network is constructed rather than of distinctions in types of language being employed , and is therefore reasonable . using a cumulative distribution instead of the one used here would not remove this oscillation .",
    "these oscillations could be more closely analyzed and subtracted out in the future , which would remove the need to bin . before fitting a curve",
    ", the degree distributions were binned so as to smooth them .",
    "the data values in certain ranges were averaged .",
    "this technique allows us to look at different scales of data sets .",
    "the bin width was set equal to @xmath12 ; this bins the distributions so that the probabilities represented were the probabilities of selecting a vertex out of the network whose assigned word appears at least @xmath24 times in the text sample .",
    "this representation of the degree distributions is useful because the values of @xmath2 for @xmath13 that is not an integer multiple of @xmath12 are lower as compared to those for integer multiples of @xmath12 .",
    "the discrepancy between these values arises because non - integer multiples of @xmath13 correspond to words that appear next to the same word multiple times in the text sample .",
    "for example , in an @xmath11 network , the probability of having a vertex with a degree of 5 is lower than the value expected by the power - law because in order to have a degree of 5 , a word must appear twice in the sample and , in its second appearance , be next to three of its four previous neighbors .",
    "this event is one of lower probability when compared with the probability of appearing next to four completely new words in a second appearance .",
    "once the characteristic values of @xmath50 were calculated , we needed a method to distinguish between the two groups of data . at first , we averaged the data points together to produce the center of each distribution . a new data point could then be categorized as a novel or news article depending upon which center was geometrically closest to the new data point .",
    "the approach could then be tested by taking data points of known category and running the algorithm to see if the data was placed correctly .",
    "this approach achieved results no better than blindly classifying the text samples , a 50 - 50 accuracy rate , as can be guessed from a careful consideration of fig .",
    "[ exponentspace ] .",
    "when this method proved ineffective , we turned to fisher s linear discriminant in order to distinguish between the two groups  @xcite .",
    "the linear discriminant transforms a multivariate observation of a group into a univariate one in order to be able to classify data points into different groups .",
    "the single value @xmath51 representing a multi - dimensional data point @xmath52 can be found by using @xmath53 where @xmath54 and @xmath55 are the column vectors representing the average of data groups 1 and 2 , respectively , and @xmath56 is the inverse of @xmath57 , which is a matrix defined as @xmath58^{t } \\nonumber \\\\ & & + \\sum_{j=1}^{n_{2 } } ( \\vec{x}_{2j}-\\overline{x}_{2 } ) [ ( \\vec{x}_{2j}-\\overline{x}_{2})]^{t } \\big\\},\\end{aligned}\\ ] ] where @xmath59 and @xmath60 are the individual column vectors representing individual data points in groups 1 and 2 , respectively , and @xmath61 and @xmath62 are the number of data points in their respective groups . in order to distinguish between two groups of distinct data , one must first define the univariate midpoint @xmath63 between the two .",
    "@xmath64 to perform the analysis , two sets of control data are required in order to define the two groups . when these groups have been selected",
    ", one can then calculate the univariate value of a data point not in either the control group .",
    "if @xmath65 then the data point belongs in group 1 , otherwise it belongs in group 2 .    in figure  [ exponentspace ] ,",
    "the scatterplot has been rotated to best exhibit the separation between the two datasets , and the plane @xmath66 ( where @xmath67 has been normalized by the dispersion of each variable ) has been included that separates the points based on fisher s linear discriminant analysis . in figure",
    "[ fischerlda ] , the distribution of the univariate values for each of the two groups of data is given separately , and combined in the center , together with the dividing value of @xmath68 .",
    "we can see that these two distributions are mounded , with different means and variances , and are sufficiently distinct that this dividing point has predictive value .",
    "we gathered a total of 400 random novels from www.gutenberg.org and 400 random news stories from www.npr.org . in each category , 200 randomly selected samples were used as a control data set , and fisher s linear discriminant was used on the other 200 samples . for the news stories we focused on reporting on current events as the best example of non - fictional storytelling ; for novels we took only modern writing , i.e. , 20th century , so as to remove effects of archaic language .",
    "an accuracy of classification for each category was calculated based upon the number of correct classifications made by the discriminant analysis divided by the total number of text samples in that category .",
    "figure  [ accuracy ] displays the accuracy of the linear discriminant as a function of word distance @xmath1 .",
    "the greatest accuracy that we calculated was @xmath5 for the correct classification of novels and @xmath6 for the classification of news stories .",
    "this accuracy was achieved at @xmath7 in our semantic network model .",
    "these values are comparable to those found by @xcite in their disambiguation of poetry and prose by similar methods .",
    "we take these values as the peak accuracies not only because the novels have the largest accuracy at this connectivity , but also because the relative distance between the accuracies of the novels and the news stories is smallest .",
    "not only one , but both of the accuracies must be larger than approximately 60% in order for that particular value of @xmath1 to be considered a better method than just randomly choosing a category for an unknown literature sample . at all stages of our analysis proper methods for a blind study",
    "were implemented in our code .",
    "used in the semantic network model .",
    "the curves are a guide to the eye ; error bars are obtained from the bootstrapping method.[accuracy],width=321 ]    these results came from an analysis that only took into consideration the two exponents @xmath69 from the degree distribution and the exponent @xmath70 from the clustering coefficient distribution .",
    "we considered a number of other possibilities , of which we describe one here as follows .",
    "we ran an analysis that used the mean geodesic path ( or small world length ) @xmath26 in addition to the three exponents .",
    "this analysis yielded results that agreed to within 0.01% of the original analysis .",
    "the reason for this is that all of the mean geodesic paths were very similar .",
    "we found that all texts were small world networks , meaning that the mean geodesic path of each of the networks was @xmath71 .",
    "since @xmath0 was similar in magnitude for all of the text samples the mean geodesic path for each of them was similar as well .",
    "the @xmath25 semantic networks displayed the second highest accuracy .",
    "this can be interpreted to mean that the majority of the information implied by the presence of a word is contained within the connection to that word s nearest neighbors , which is indeed true .",
    "after @xmath7 , the accuracy begins to drop , as one can observe in the @xmath72 study in fig .",
    "[ accuracy ] . as the word distance of the network increases",
    ", the number of false connections also increases . that is to say ,",
    "as the connectivity grows , the number of meaningless or incorrect connections occurs .",
    "though there are many words in a sample that could be considered to be connected to words relatively far away , such as words in a distantly placed adverbial or adjectival clause , most words ca nt be considered to have such connections . moreover",
    ", the more connected a network becomes the closer it gets to being a maximally or fully connected network , whose topology does not consist of power - laws , and can not be significantly distinguished from another fully connected network .",
    "in addition to testing the accuracy of different word distances , we also checked how accurate the analysis became when the text samples were shortened .",
    "the human mind can quickly determine fictional vs. non - fictional story - telling as represented by novels and news stories , typically within a few lines .",
    "we tested our semantic network model using different lengths of text to see how well it distinguished between the two control groups .",
    "the model was tested with text sample lengths of 50 to 400 words , with the connectivity held fixed at @xmath7 .",
    "[ lengths ] summarizes the results . as the plot shows ,",
    "the accuracies of classification begin near 50@xmath73 and then improve starting at 75 words . at 200 words ,",
    "the networks have approximately reached their highest accuracy .",
    "if this is compared with the comprehension of an adult , the result is reasonable . by the time a person has read about one paragraph of a text ,",
    "the fictional aspect of a story is clear .",
    "we clarify that the number of vertices @xmath0 is usually much less than the number of words @xmath8 .     by the relative sizes of the @xmath74 coefficients in the equation of the plane of separation , we see that the largest contributor to accurate distinguishing of these two genres comes from @xmath75 , the degree distribution power law exponent for the larger regime of degrees . interpreting this observation",
    "calls for additional study , as these larger degree values are infrequent , often occurring only once .",
    "but one can roughly say that news articles generally have a higher value for @xmath75 , and this is associated with a steeper drop - off in frequencies of the degrees of the nodes for the larger degrees .",
    "this may come about if articles have a larger number of high degree words , which is to say that they have more repeated or common words .",
    "this rings true as fiction writers consciously try to vary their vocabulary , while writers of news articles have collections of stock words and phrases .",
    "this hypothesis could be tested by independent analysis of these texts and their word frequencies .",
    "however one attempt to distinguish these genres by word frequencies is detailed below with negligible accuracy .    in order to achieve an estimate of the error inherent to our analysis methods for our semantic network ,",
    "a bootstrapping method was used on the discriminant analysis .",
    "this approach is an iterative method used to deduce the size of a distribution of a random variable @xmath76 strictly from numerical data without any knowledge of the theoretical distribution of @xmath76  @xcite .",
    "the bootstrap method takes a random sample of size @xmath77 of the total number of observations of @xmath76 , performs the desired analysis , takes another random sample from the observations of @xmath76 , and repeats the entire process until it has a set of results from the desired calculation from which one can calculate the mean and standard deviation .",
    "we used twice the standard deviation as the error for each of our data sets and used the mean value as the reported value . with our discriminant analysis ,",
    "the bootstrapping algorithm randomly chose 200 of the 400 text samples of each group ( news stories and novels ) and calculated the accuracy of classification for each .",
    "we chose to use 200 random samplings in our error analysis .",
    "this provided us with a list of accuracies from which we could calculate the mean and the standard deviation .",
    "the mean and standard deviations of the bootstrap method converged with increasing iterations .",
    "all error bars in figures are obtained via the bootstrap approach .    to compare this method with that of a simple statistic that does not make use of the complex network theory measures , we analyze , for example , the word frequencies in articles and fiction . after breaking the chosen text segment into words",
    ", we find the probability of each , and rank them from most frequent down .",
    "an established empirical result , colloquially known as zipf s law , has that this distribution is well approximated by a power law @xcite .",
    "if we fit a power - law to this data ( and the fit is visually and by norm measure reasonable ) , and use the same discrimination technique as above together with the bootstrapping algorithm , we obtain an accuracy of @xmath78 for the correct classification of novels and @xmath79 for the classification of news stories . the worse - than - chance showing for fiction",
    "is a consequence of the empirical observation from this sample that the distribution of the fitted exponents from the power - law are approximately symmetrical for articles while they are positively skewed for fiction . using the average of the medians rather than the means in the discrimination improves the accuracy for fiction to slightly over chance , while decreasing the accuracy for articles to closer to chance .",
    "other choices of a discrimination value could be made to different effects .",
    "but this particular simple statistic is noticeably inferior in distinguishing fiction and non - fiction as compared with the complex network based measure highlighted in this article .",
    "we have shown that a combination of established complex network theory measures and multivariate statistical analysis can be used to distinguish between different types of storytelling , fictional and non - fictional . specifically in our analysis , an unweighted ,",
    "undirected network model was used to distinguish between novels and news stories .",
    "fisher s linear discriminant was used to classify random samples of known novels and news stories . in this way we were able to calculate an accuracy of classification for our model using the linear discriminant and calculate error bars based on an iterative bootstrap algorithm .",
    "we considered the simplest possible semantic network model , an undirected unweighted network which assigns vertices to independent words independent of their conjugation , pluralization , or other grammatical structure , and assigns edges whenever two words are within @xmath1 places of each other .",
    "the ensuing networks constructed from novels and news stories have collectively different values for the exponents in the power - laws of both their degree distributions and their clustering coefficients .",
    "these exponents can be used to classify the two literature types . with the use of the linear discriminant",
    ", one can test to see if a story of unknown literature type is fiction ( a twentieth century novel ) or non - fiction ( a news story on current events ) . this same method can also be applied to text samples of known category in order to calculate the accuracy with which the model can distinguish between the two types of storytelling .",
    "the highest value for the accuracy of classification for the two literature types was found at a word distance of @xmath7 , giving accuracies of @xmath80 and @xmath81 for the novels and news stories respectively .",
    "categories could be distinguished beginning at about 75 words and were maximally accurate at about 200 words .",
    "a less coarse semantic network model might lead to substantially improved accuracy in future studies .",
    "a weighted or directed network approach may be able to provide more meaningful structure in the placement or relationships of words .",
    "other tools could also be used in the analysis such as community detection algorithms in order to further enhance the boundaries between novels and the news stories .",
    "we thank william c. navidi for discussions of statistical analysis and norman eisley for helping us recognize that storytelling is as fundamental a human behavior as , for example , social networks or mobility .",
    "f.  sebastiani , acm comput .",
    "surv . * 34 * , 1 ( 2002 ) .",
    "f.  sebastiani , proceedings of asai-99 * 1 * , 7 ( 1999 ) .",
    "y.  yang , information retrieval * 1 * , 69 ( 1999 ) .",
    "j.  karlgren and d.  cutting , proceedings of the 15th conference on computational linguistics * 21 * 1071 ( 1994 ) .",
    "g.  k.  zipf , _ the psycho - biology of language : an introduction to dynamic philology _ ( houghton mifflin , new york , 1935 ) ."
  ],
  "abstract_text": [
    "<S> we establish concrete mathematical criteria to distinguish between different kinds of written storytelling , fictional and non - fictional . </S>",
    "<S> specifically , we constructed a semantic network from both novels and news stories , with @xmath0 independent words as vertices or nodes , and edges or links allotted to words occurring within @xmath1 places of a given vertex ; we call @xmath1 the word distance . </S>",
    "<S> we then used measures from complex network theory to distinguish between news and fiction , studying the minimal text length needed as well as the optimized word distance @xmath1 . </S>",
    "<S> the literature samples were found to be most effectively represented by their corresponding power laws over degree distribution @xmath2 and clustering coefficient @xmath3 ; we also studied the mean geodesic distance , and found all our texts were small - world networks . </S>",
    "<S> we observed a natural break - point at @xmath4 where the power law in the degree distribution changed , leading to separate power law fit for the bulk and the tail of @xmath2 . </S>",
    "<S> our linear discriminant analysis yielded a @xmath5 accuracy for the correct classification of novels and @xmath6 for news stories . </S>",
    "<S> we found an optimal word distance of @xmath7 and a minimum text length of 100 to 200 words @xmath0 . </S>"
  ]
}