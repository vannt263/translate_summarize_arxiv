{
  "article_text": [
    "in this paper we analyze a family of abstract , mathematical models which have been used to illustrate _",
    "highly optimized tolerance _ ( hot )  @xcite , a mechanism for complexity based on robustness tradeoffs in systems subject to uncertain environments .",
    "hot systems abound in nature and modern technology , and are complex and highly structured .",
    "they arrive at optimized `` or organized '' states through deliberate design or biological evolution , and exhibit robust , yet fragile ( ryf ) characteristics , the essence of hot .",
    "that is , they are robust to normal or common perturbations , yet may be extremely fragile to rare perturbations or design flaws , even if the perturbations are small and seemingly innocuous .",
    "recently , hot has been investigated in the context of a variety of specific applications , including the internet  @xcite , the electric power grid  @xcite , wildfires  @xcite , and biological networks  @xcite .",
    "typically , these studies involve a combination of simple abstract , analytically tractable representations , which focus on fundamental tradeoffs and derivations of the power laws , with detailed , high - resolution simulation models , aimed at pinpointing specific system and model fragilities . here",
    "we focus specifically on the abstract models which have been used to describe hot .",
    "we compare discrete and continuum models in a common framework , and clarify the approximations that are made and the ranges of applicability of the models .",
    "this forces us to address certain fundamental issues in probability and statistics , including distinctions between discrete and continuous distributions , and properties associated with mixtures of distributions .",
    "one key success of hot is to offer an alternative perspective on the origins and ubiquity of complexity , and particularly power laws .",
    "mathematically , heavy tailed distributions ( e.g.  power laws ) often require special care because of the broad range of spatial and temporal scales over which data is sampled @xcite . in many cases , conventional assumptions and methodologies associated with modeling and data analysis",
    "are misleading and/or break down .",
    "one of the goals of this paper is to illustrate how such problems can arise , and to approach them in a manner which is mathematically rigorous .",
    "hot has been compared to earlier work emphasizing emergent complexity , where power laws arise from minimal tuning , on an otherwise random substrate . in emergent complexity power laws",
    "are associated with fractals and self - similarity @xcite . in many studies ,",
    "hot illustrates the differences between organized and emergent complexity by using percolation forest fire models from physics @xcite , but including a minimal form of optimization ( intended to capture design or evolution ) and robustness tradeoffs @xcite .",
    "this produces power laws ( in better agreement with data ) that arise from highly organized and self - dissimilar structures , the opposite of self - similarity .",
    "all of the abstract hot models follow the same basic mechanistic description involving optimization of tradeoffs in an uncertain environment .",
    "each begins with a @xmath4-dimensional substrate representing the system .",
    "each event ( e.g. , a power outage or fire ) is triggered by some small perturbation or spark ( typically chosen from a nonuniform distribution ) which initiates a cascading failure , resulting in loss of some portion of the substrate .",
    "all of the models considered here assume the loss ( or cost ) associated with an event scales linearly with the event size .",
    "alternative cost functions give power laws in cost , not necessarily raw event size  @xcite .",
    "thus cost functions that heavily weight large events can lead to truncation of the power laws  @xcite .    in hot ,",
    "resources are allocated to create barriers limiting propagation of the cascading failure events in a manner which optimizes the cost function ( minimizing loss or maximizing yield ) .",
    "there is a limited number of resources available , and this constraint is modeled in one of two ways .",
    "the first method places a fixed limit on the total resources available .",
    "the second weights resource use alongside other costs or losses , which are associated with the events themselves , by including an explicit resource term in the cost function . here",
    "the key issue is to account explicitly for resource use .",
    "the specific form of the constraint does not play a significant role in determining the size distribution .    in hot , optimization of the resource allocations subject to the constraint represents design and/or evolutionary tradeoffs in systems faced with a spectrum of disturbances .",
    "because resources are constrained and often sparse or expensive , optimal solutions make efficient use of the resources available , resulting in hot states characterized by structured , compact , @xmath4-dimensional regions surrounded by @xmath6-dimensional barriers . in addition , for a broad class of distributions of disturbances ( e.g. gaussian , exponential , and cauchy ) , minimization of the average loss results in heavy - tailed , power law distributions in the sizes of the events . newman _",
    "et al . _",
    "@xcite emphasize however that the specific exponents characterizing the decay of the power law distribution in hot models can be different .",
    "in this paper we focus on three models for hot which are among the simplest , and most analytically tractable examples .",
    "table  [ tbl : compare ] summarizes their basic properties , which will be described in detail in the following sections . in each case ,    probability @xmath7 : represents uncertainty in the environment .",
    "loss @xmath8 : represents the volume or size associated with an individual event , which is directly proportional to the cost of that event .",
    "resources @xmath9 : provide mechanisms to limit losses .",
    "constraints : are imposed on the resources .",
    "optimization : of the resource assignments subject to constraints leads to the hot state .",
    "power laws : in the cumulative event distributions , @xmath10 vs. @xmath8 , are characteristic of these optimal solutions .",
    "all of these models are motivated by studies of the hot version of the percolation forest fire model @xcite .",
    "the most well studied of these are the continuum model  @xcite , generalized by newman et .",
    "@xcite , and the probability loss resource ( plr )  @xcite model .",
    "their abstractions differ in subtle , yet important ways , leading to differences in the predictions .",
    "the continuum model aims to describe the continuum limit of the hot percolation forest fire model  @xcite , building on lattice models from statistical physics  @xcite , and introducing a mean - field - like analysis of the continuum limit . in the continuum model",
    "all aspects of the system are described as smoothly varying functions on the substrate .",
    "the plr model is a generalization of shannon source coding theory  @xcite from information theory  @xcite , perhaps the simplest design model in engineering .",
    "the plr model begins with discrete event categories @xmath11 , each of which has a characteristic probability , resource allocation , and resulting loss . like the continuum model , the cuts model  @xcite",
    "can be thought of as the limiting description of a lattice model as the lattice size becomes infinite .",
    "the cuts model represents space continuously ( like the continuum model ) but divides it into discrete regions ( like the plr model ) using sharp barriers , i.e.  cuts .",
    ".the hot continuum , plr , and cuts models predict power laws based on optimal allocation of limited resources to minimize loss in an uncertain environment .",
    "different assumptions in the continuum and plr models lead to different exponents in the dense and sparse resource regimes , both of which can arise as ( opposite ) limits of the cuts model . the plr model can be extended to the dense resource limit ( section v ) , where it agrees with the continuum and cuts model .",
    "the plr cumulative probability @xmath10 assumes densely sampled data ( section iii ) . to increase readability ,",
    "constant factors are set to unity in the equations for yield @xmath12 which is optimized .",
    "[ cols=\"<,<,<,<\",options=\"header \" , ]     a key distinction between the models is their predictions for power law exponents .",
    "the continuum model predicts a power law in @xmath10 with exponent @xmath13 , while plr predicts an exponent of @xmath3 for the same distribution of sparks ( assuming densely sampled data ) .",
    "we show the first two models match solutions in different limits of the cuts model with an exponential distribution of sparks . in @xmath14",
    "the cuts model predicts an exponent of @xmath15 in the limit of small events and @xmath16 in the large - event limit .",
    "thus the cuts model captures the power laws predicted by the other two models in the limit of small ( continuum ) and large ( plr ) events .",
    "analysis of the cuts model provides a unifying picture for all the models , and a concrete illustration of how certain key approximations made in the first two models can break down .",
    "we show that when the plr and cuts models have sufficiently similar assumptions , their results agree as expected . in the dense resource regime limit ( described by the continuum model ) , all three models agree .",
    "the cuts model also illustrates how the exponent describing small events departs from this dense resource limit as the density of resources and barriers becomes lower .    in the remaining sections of this paper ,",
    "we first summarize results for the continuum ( section ii ) and plr ( section iii ) models , with special attention to derivation of the power laws , and specific features which will be useful for comparing models .",
    "we also discuss mathematical subtleties which can arise in taking continuum limits in systems with sharp barriers , as well as mathematical issues which can arise in comparing continuum vs.  discrete models and distributions , and distributions composed of finite mixtures of probability distributions .",
    "the next three sections comprise the bulk of the new analytical results in this paper .",
    "in section iv we review and extend the cuts model and in section v we compare it to the other models .",
    "we show that the event size distribution for the plr and cuts model agrees when their assumptions are forced to be similar .",
    "they also both agree with the continuum model in the limit of dense resources and small event sizes . for the cuts model with a power law distribution of sparks ,",
    "the small event limit is described by a power law in which the exponent depends on the distribution of sparks , ranging from the limiting value of @xmath15 ( which we obtain for an exponential spark density ) for an infinitely steep power law , to @xmath16 ( sparse resource limit , and in agreement with plr ) when @xmath17 .",
    "furthermore , for both exponential and power law distributions of sparks , we find that the event size distribution for the cuts model agrees with the plr model in the limit of large event sizes , where the distribution is clearly discrete . in this case",
    "the agreement between models depends on the assumption of a sufficiently well sampled data set , which would only arise in the cuts and plr models due to mixtures .",
    "in section vi we return to the original hot lattice model , and illustrate a subtle pathology which arises in the continuum limit of the lattice model in the absence of an explicit resource cost or constraint . in section vii",
    "we conclude with a discussion of our results , and the relevance of the different resource regimes in the context of observed data .",
    "the continuum model was first suggested as an approximate limiting description of the hot forest fire percolation lattice model by carlson and doyle  @xcite .",
    "it was later studied alongside large lattice model simulations by newman et al .",
    "the definition of the model most conveniently begins with the lattice model , which we will return to in section vi . strictly speaking",
    ", the continuum model is an approximation to the lattice model based on scaling arguments .",
    "it captures the power laws observed in the hot lattice model in the limit of large , finite lattice sizes , and allows the size distribution to be calculated analytically .",
    "consider a @xmath18dimensional space , with positions labeled by the @xmath18dimensional vector @xmath19 ( these are discrete sites on a hypercubic lattice , each labeled by @xmath4 integer indices @xmath20 , with @xmath21 , where @xmath22 is the number of sites along each axis of the lattice ) . in the percolation lattice model ,",
    "each position ( site ) is either occupied by a tree , or vacant ( firebreak ) .",
    "environmental uncertainty is represented by the probability @xmath23 that a spark lands at site @xmath19 .",
    "a spark ignites a fire that spreads throughout the nearest neighbor connected cluster of trees in all @xmath4 directions , but terminates at firebreaks .",
    "the resulting fire size is the total number of sites in the burned patch , @xmath24 , and the value of @xmath24 is clearly constant within each contiguous patch .",
    "a sample lattice configuration in the special case @xmath14 is illustrated in figure  [ 1d_lattice ] .",
    "occupied sites ( black ) are trees and unoccupied sites ( white ) are firebreaks .",
    "event sizes @xmath25 correspond to the number of occupied spaces between firebreaks , or _",
    "cuts _ , labeled @xmath26 .",
    "hot configurations optimize the layout of vacant and occupied sites to maximize yield @xmath12 , defined to be the average number of occupied sites which remain after a single spark lands on the lattice ( averaging over the spark distribution @xmath23 ) . for small lattices ,",
    "it is possible to compute the globally optimal solution  @xcite .",
    "however , for large lattices the solution becomes computationally intractable ( and not especially informative ) .",
    "instead , a wide variety of constrained optimization schemes have been investigated  @xcite , all leading to similar results .",
    "firebreaks are concentrated in regions of high spark probability , so that only small fires occur in regions of the lattice where sparks are common , while large fires occur in regions where sparks are rare .",
    "the specialized , patterned hot configurations reflect patterns in the perturbing environment .",
    "this is in sharp contrast to the traditional forest fire percolation model studied in statistical physics  @xcite , where configurations are essentially random , aside from a tuned , or self - organized \" average critical density @xcite .",
    "the contrasts between the hot and self - organized critical lattice models are discussed in detail in @xcite , and will not be our emphasis here .",
    "the hot lattice model was the first model introduced to illustrate the hot mechanism , and is pedagogically useful in illustrating the emergence of @xmath27-barriers on the @xmath18dimensional substrate , as well as the high concentrations of barriers in regions where perturbations are common .",
    "all of the other models considered here retain these key features , but each explicitly accounts for the cost of resources in a different way .",
    "more importantly , each makes different approximations in representing continuum versus discrete spatial features of the lattice model , which lead to the different predictions for the event size distribution .    in the continuum model",
    "the integer @xmath28 components of the @xmath18dimensional vector positions @xmath19 are replaced in the limit @xmath29 by real valued components . the occupied ( tree ) and vacant ( firebreak ) lattice sites",
    "are replaced by a resource density @xmath30 , representing the local density of firebreaks .",
    "a function @xmath24 represents the size of the loss which occurs when a spark lands at position @xmath19 . a key approximation relative to the original lattice model",
    "is clearly made in the continuum model , which represents @xmath30 and @xmath31 as continuous functions .",
    "the idea is to use a scaling relation , motivated by the lattice model , to mimic the manner in which higher resource densities lead to smaller fires in a given region , without accounting in detail for the specific configuration .    to derive the distribution of fire sizes for the continuum model",
    ", we follow the elegant derivation of newman et al .",
    "the size of a firebreak surrounding a given patch @xmath24 is : @xmath32 where @xmath33 is a geometric factor of order unity that depends on the shape of the patch .",
    "it is in eq .",
    "( [ r_of_x ] ) that the dimensional relationship between resource and loss is captured . in the continuum model",
    "the total resource use is given by @xmath34 where the integral is over the @xmath18dimensional substrate .",
    "in the continuum model , this cost enters explicitly into the yield function . normalizing @xmath12 by the total volume of the substrate ( i.e. @xmath35 corresponds to a fully occupied forest , with no fires or firebreaks ) , and averaging over the distribution of sparks @xmath23 , we write the expected yield as @xmath36 where @xmath37 is the cost per unit area ( or generally , @xmath4-dimensional volume ) of forest , and @xmath38 is the cost per unit length ( or @xmath6-dimensional volume ) of firebreaks .",
    "this yield function is motivated by tradeoffs inherent in the original lattice model , where the resources are empty sites , and the cost of firebreaks is the yield penalty in initial density associated with creation of vacancies . however , unlike the lattice model , it includes a nonvanishing resource term explicitly in the yield function , and allows the constants @xmath38 and @xmath37 to scale differently with dimension @xmath4 .",
    "this fortuitously omits a pathology which results from the difference in scaling between the compact , @xmath18dimensional clusters of trees , and the @xmath27-dimensional firebreaks which arises in the lattice model as @xmath29 .",
    "we discuss this in more detail in section vi .    the optimal allocation of resources @xmath30 maximizes the expected yield .",
    "optimizing resources is equivalent to optimizing over event sizes because they are explicitly related via eq .",
    "( [ r_of_x ] ) . to obtain the solution",
    ", we assume that @xmath24 is a continuous function of the ignition site @xmath19 , and set the functional derivative @xmath39 equal to zero .",
    "this leads to @xmath40 where c is a constant that depends on @xmath38,@xmath37 , and @xmath33 .",
    "a schematic solution in @xmath14 is illustrated in figure [ l_of_x_continuum ] .",
    "it is important to note that the continuum model departs from the original lattice model in representing @xmath24 as a continuous function of @xmath41 . for a given configuration in the lattice model",
    ", @xmath24 assumes a constant , finite value for each contiguous cluster of occupied sites .",
    "therefore @xmath31 in that case , is piecewise constant .",
    "the continuum model represents @xmath42 as continuous over the entire space , leading to eq .",
    "( [ cont_l ] ) .",
    "it is the only one of the models we consider which builds in this assumption .",
    "it is also possible to calculate the probability density @xmath43 of fire sizes for the continuum model .",
    "again , assuming @xmath24 is continuous we obtain  @xcite : @xmath44 c^{\\prime } l^{-(2 + 1/d)}\\end{aligned}\\ ] ] where @xmath45 is a constant that depends on @xmath4,@xmath37,@xmath38 , and @xmath33 .",
    "thoroughly investigated the behavior of @xmath43 and found that the scaling behavior is dominated by the factor of @xmath46 , while the factor @xmath47 generates at most logarithmic corrections for a broad class of probability distributions @xmath48",
    "@xcite .    since the probability density @xmath43 is continuous , the cumulative distribution of events of size greater than or equal to @xmath8 , @xmath49 , is proportional to @xmath50 .",
    "therefore , for a one dimensional substrate , the continuum model predicts a slope of @xmath15 for the cumulative distribution of events . table  [ tbl : compare ] summarizes the properties of this model .",
    "the plr ( probability loss resource ) hot model is a generalization of shannon source coding theory for data compression @xcite , the simplest , most elegant design theory in engineering .",
    "it is the simplest model illustrating hot @xcite , and is based on optimal allocation of limited resources , with an explicit , fixed cap on the total resources available .",
    "it retains a dimension - dependent relationship between resources ( @xmath27-dimensions ) and loss ( @xmath4-dimensions ) , but otherwise replaces the explicit spatial variable @xmath19 with a more abstract notion of event categories @xmath11 .",
    "the idea is to group similar conditions , from the common to the rare , into a category , represented by the relative probabilities @xmath51 .",
    "the plr objective is to allocate resources in a manner which maximizes yield @xmath12 averaged over a spectrum of possible events : @xmath52 here @xmath37 is a constant , and @xmath11 , @xmath53 , indexes the finite and discrete set of probabilities @xmath51 , assumed to be in descending order , with corresponding loss @xmath25 .",
    "normalized , the cumulative @xmath54 is the rank order divided by the total number of events in a data set , from which corresponding values of @xmath51 may be deduced .",
    "we will interpret the @xmath51 as probabilities , so @xmath12 is average yield , but in general the @xmath51 could be any weights assigned to create a cost function",
    ".    the probability @xmath51 of each category is fixed , and a total resource allocation @xmath55 is made to the event category @xmath11 , resulting in events of size @xmath25 for the category ( i.e. @xmath55 is the total resource allocation to all the events in event category @xmath11 ) .",
    "the @xmath55 are chosen to minimize the average event size , averaged over the spectrum of possible conditions @xmath56 .",
    "the only interaction between events is that the sum of all resources is limited by @xmath57 .",
    "this means that any reasonable design will devote more resources to the categories of common events so that they yield small losses , leaving relatively few resources for rare events .    unlike explicitly spatial lattice models",
    ", the plr model presumes a mean - field - like independence of events .",
    "however , a lattice abstraction ( which should not be interpreted as a literal gridding of the forest ) can be used to derive the relationship @xmath58 between resource allocation and loss for the event categories @xmath56 .",
    "imagine a large , finite @xmath18dimensional lattice which is an abstraction of a space representing a single condition category @xmath11 .",
    "the lattice is of length @xmath59 on each side , and the total volume @xmath60 serves as the large scale cutoff , i.e. the size of the largest possible event .",
    "the value of @xmath51 is the total probability of hitting any part of the lattice for category @xmath11 , and the probability of hitting any one of the cells within category @xmath11 is equal .",
    "resources @xmath55 represent the total allocation of vacant sites within the @xmath61 category .    because the spark distribution @xmath51 is uniform within each category , the optimal use of resources ( vacant sites ) defines a collection of equally spaced @xmath27-dimensional surfaces , one lattice spacing wide in the remaining dimension , on an otherwise occupied lattice .",
    "this defines a set of compact , contiguous cells , all of equal size @xmath25 , for category @xmath11 .",
    "for example , in @xmath14 , the barriers correspond to a single unoccupied site between contiguous occupied sites of equal length .",
    "this is similar to the lattice shown in figure  [ 1d_lattice ] , except the occupied regions @xmath62 would all have the same length .",
    "suppose a resource allocation of size @xmath63 ( number of vacancies ) is made to category @xmath11 , arranged as @xmath64 equally spaced cuts , spanning the full length of the lattice @xmath59 in each dimension @xmath4 .",
    "then the event size @xmath25 for category @xmath11 is @xmath65 .",
    "eliminating @xmath64 yields a relationship between event size @xmath25 and the resource allocation @xmath55 , which scales like @xmath66 . here",
    "@xmath59 is simply the constant subregion lattice length scale , and the key result is the dimensional relationship between resource allocation ( to the event category as a whole ) and the corresponding characteristic loss size for that category .",
    "this process is illustrated for @xmath14 in figure [ plr_nc ] .",
    "three event categories ( @xmath67 ) are shown , in order of descending probability @xmath68 here the constant subregion size @xmath59 is the identical horizontal length of the line segment associated with each region .",
    "the vertical height of each box reflects the probabilities @xmath51 ( and is not related to any spatial dimension or length scale ) .",
    "resources @xmath55 are allocated to each region , with @xmath69 , and divide each region into line segments @xmath25 equal size , with @xmath70 .    incorporating a cutoff at small event sizes , and normalizing so that @xmath71 with f(1)=0 , in @xmath18dimensions we write @xmath72 which incorporates the scaling determined above , and uniquely determines @xmath73 up to the parameter @xmath74 . as in the original shannon theory , we relax the constraint that the @xmath55 take integer values .",
    "this is an extremely simple and tractable model with essentially only one parameter , the dimension of the substrate , where events are characterized by @xmath18dimensional , compact regions , enclosed by @xmath27-dimensional perimeters .",
    "given a fixed resource budget @xmath75 , the goal outlined in eq .",
    "( [ jdef ] ) is to optimize the division of resources @xmath55 to maximize yield , by minimizing the expected loss @xmath76 , subject to the resource vs. loss relationship in eq .",
    "( [ resource ] ) .",
    "this is accomplished using standard constrained optimization methods ( lagrange multipliers ) . setting the gradient of @xmath77 equal to zero yields @xmath78 , which equalizes the expected marginal loss and can be solved for the @xmath55 . then the optimal @xmath79 saturates the resource constraint with @xmath80 , @xmath81 , yielding @xmath82 so that @xmath83 , \\label{l}\\ ] ] and @xmath84 . \\label{jopt}\\ ] ] inverting ( [ l ] ) yields a relationship between the event type and corresponding probability : @xmath85 where @xmath3 and @xmath86 is a constant ( which depends on @xmath74 , @xmath4 , and @xmath75 in eq .",
    "( [ resource ] ) ) which sets the small size scale in the resource vs. loss relationship . for simplicity",
    ", we will assume throughout that @xmath86 is sufficiently small that we can neglect any small size cutoff .",
    "the plr model is defined in terms of noncumulative probabilities @xmath51 , but to reliably compare with data it is necessary to use cumulative distributions . since @xmath87 ( eq .",
    "( [ pdist ] ) ) , the naive expectation is that the cumulative distribution @xmath88 . however , this is not necessarily the case for discrete data sets , where cumulative distributions are attained by summing , rather than integrating the density .",
    "in fact , in the discrete case , the cumulative distribution can be steeper , shallower , or have the same decay properties as the density , depending on how densely the data is sampled . thus ,",
    "unlike the case of a continuous probability density , there is no general relationship between discrete probability distributions and their noncumulative densities .",
    "we can not simply assume that since @xmath89 is a power law with slope @xmath90 , that @xmath91 is a power law , let alone with slope @xmath92 .",
    "this issue is fundamental in the theory of discrete probability distributions , and also arises for the cuts model ( section iv ) , which is also inherently discrete ) , and in comparing plr with the continuum and cuts models ( section v ) .",
    "furthermore , in making comparisons with data , use of the density , rather than the distribution does not solve the problem .",
    "use of the cumulative distribution is in fact preferable , because it avoids statistical anomalies associated with binning .",
    "the cumulative distribution simply corresponds to a normalized plot of the ranked ( by size ) order of events in a catalog , which does not introduce any statistical biases .",
    "although the plr model can be used to generate a cumulative event probability function @xmath10 which is inherently discrete , most data sets exhibiting power laws in the cumulative event probability as a function of size are sufficiently dense to exhibit a fairly convincing unit difference in slope between the density and the cumulative distribution .",
    "this leads us to determine circumstances under which the naive expectation of unit difference in the exponent between the cumulative distribution and the noncumulative density is in fact correct .",
    "this requires sampling in the data set which is sufficiently dense that integration of the density to obtain the cumulative distribution is a good approximation to computing the discrete sum .",
    "one possible explanation is to hypothesize that most data sets are _ mixtures _ from many different systems , or the same system averaged over long times .",
    "thus a complete treatment of how to assess whether data is consistent with a plr mechanism ultimately requires a treatment of mixtures @xcite .",
    "the simplest scenario corresponds to a mixture of discrete power law distributions with the same exponent .",
    "this generates a power law with that same exponent , but possibly different short- and large - scale cutoffs , and provides a simple and unambiguous way to connect the plr @xmath87 with @xmath88 .",
    "this scenario assumes sufficient data up to some cutoff size @xmath59 , binned with fixed @xmath93 , to treat the resulting @xmath94 as binned samples from a continuous density .",
    "then we can define @xmath95 which in the limit of large data sets approximates a continuous @xmath10 satisfying @xmath96 leading to the exponent @xmath16 in @xmath14 .",
    "table  [ tbl : compare ] assumes these properties of the prl model .",
    "note , however , that when the prl model is used and the @xmath25 are not densely sampled , then the above calculations for the cumulative distribution need not hold .",
    "the cuts model  @xcite is a simple , analytic model that helps clarify the discrepancy between the power law exponents predicted by the continuum and plr models .",
    "we focus on @xmath14 for this case and the comparisons .",
    "higher dimensional generalizations of the cuts model are possible , but correspond to constrained optimization schemes ( e.g. the grid design problem in @xcite ) or choices of @xmath23 with special symmetries . as we show below",
    ", the other two models as formulated above agree with the cuts model in ( different ) asymptotic regimes .",
    "we also use the cuts model to formulate an extension of the plr model that describes the dense resource limit , where all three models agree .    like the continuum model",
    ", the cuts model is naturally understood as a continuum limit of a percolation lattice model , but it is a variant of percolation which includes an explicit constraint on the resources , as in plr .",
    "the cuts model removes the assumption that the event sizes @xmath97 are nearly continuous ( an approximation made in the continuum model ) , which makes it possible to span both the dense and sparse resource regimes in a single formulation of the model .",
    "consider a percolation forest fire lattice model in one dimension .",
    "resources are vacancies that act as dividers or _ cuts _ between connected clusters of occupied sites .",
    "an example of this is shown for @xmath14 in figure  [ 1d_lattice ] .",
    "if we take a continuum limit by rescaling into a finite interval and taking the number of lattice sites to infinity , the the cuts become infinitesimally thin , zero dimensional dividers between continuous connected regions of unit density .",
    "the cuts model is defined on position space @xmath98 , @xmath99 \\subset \\re$ ] , where @xmath100 is the large - scale cutoff .",
    "a discrete set of zero - dimensional cuts divide the axis into a set of separate one - dimensional line segments .",
    "the model imposes the constraint that the maximum number of cuts is a natural number @xmath22 .",
    "analogous to the plr model s explicit constraint on total resources ( @xmath101 ) , optimal solutions make full use of all available resources ( @xmath80 in plr and @xmath102 cuts=@xmath22 in cuts ) .",
    "events are triggered ( sparked ) according to a spatial probability function @xmath103 as in the continuum model , propagating along the connected cluster , between adjacent cuts .",
    "the position of the @xmath11th cut is labeled @xmath26 and @xmath104 is at @xmath105 .",
    "the cut positions define discrete line segments @xmath25 and the corresponding event probabilities @xmath51 : @xmath106 in other words , the cuts map the continuous spatial function @xmath103 defined on @xmath107 \\subset \\re $ ] into a discrete set of events with probability @xmath51 given by the cumulative probability of sparking the segment of length @xmath25 between adjacent cuts .",
    "this mapping is illustrated in figure  [ cuts_nc ] .",
    "carlson and doyle  @xcite maximized the yield function @xmath108 with respect to the cut positions .",
    "note that this is the same yield function used in the plr model .",
    "they found an iterative solution for the optimal cut positions in the continuum limit : @xmath109 unfortunately , analytic solutions to this equation involve transcendental functions even if there is a simple functional form for @xmath103 .",
    "the problem simplifies if we consider a slightly modified cost function , replacing @xmath110 in ( [ cuts_yield ] ) , with @xmath111 , where @xmath112 is the probability of events of index greater than @xmath11 , @xmath113 .",
    "this cost function can be naturally motivated in many cases , such as web layout  @xcite .",
    "furthermore , as we show below , results obtained for the power laws using this modified cost function are equivalent to the original cost function in the small and large size asymptotic regimes .    with the modified cost function , we can define the yield as @xmath114 and optimize the yield with respect to the cut positions @xmath115 by setting @xmath116 . using the definitions from eq .",
    "( [ cuts_def ] ) , the following iterative equations hold for the optimal cut positions : @xmath117 this equation is simpler to iterate than eq .",
    "( [ cuts_recur1 ] ) .",
    "its solutions are no longer transcendental functions , and optimal @xmath25 for general @xmath103 can easily be found using simple numerical techniques .",
    "note that the number of cuts @xmath22 does not appear explicitly in the recursion relation .",
    "instead , the equation requires two initial cut positions , @xmath26 and @xmath118(which is the lower limit of integration for the integral defined as @xmath51 ) .",
    "these initial cut positions define a length scale , @xmath119 .",
    "this length scale together with the large - scale cutoff , @xmath100 determine the total number of cuts , @xmath22 .",
    "therefore , choosing two initial cut positions is equivalent to specifying @xmath22 for a fixed @xmath100 .      to solve the recursion equation analytically we first choose @xmath120 , which leads to an especially simple solution : @xmath121 as with the other two models , we are interested in the probability distribution of event sizes @xmath122 and the cumulative probability distribution @xmath91 . in this case ,",
    "solving for @xmath91 is transparent ; @xmath123 we substitute @xmath103 into eq .",
    "( [ cuts_recur2 ] ) to find a recursion relation for the optimal region sizes : @xmath124 notice that the event sizes increase exponentially as @xmath25 becomes large .",
    "we use @xmath25 to construct the function @xmath97 which is defined as the event size @xmath25 when a spark hits site @xmath98 .",
    "this function is piecewise constant between cuts , as illustrated in figure  [ lx_figure ] . for large @xmath98 ,",
    "the function exhibits large discontinuities . for small @xmath98 , while still discrete , it approaches a continuous function .",
    "the slope of @xmath91 on a log - log plot can be easily calculated in limiting cases by substituting eq .",
    "( [ l_iter ] ) into eq .",
    "( [ cuts_2 ] ) , dividing @xmath125 by @xmath126 , taylor expanding , and dropping higher order terms .",
    "the limiting case describing the large event sizes , with sparse resource allocations , is discussed in  @xcite . following the derivation there :",
    "@xmath127    the opposite limiting case , describing small events , and high resource densities , can also be calculated .",
    "we find : @xmath128    we can also investigate the asymptotic behavior for small and large events in this model numerically by choosing two initial cut positions @xmath26 and @xmath118 and then iterating eq .",
    "( [ cuts_recur2 ] ) backwards and forwards . the cumulative probability @xmath91 ( large circles ) vs. the event size @xmath25 is shown in figure  [ cuts_figure](a ) and ( b ) , and the limiting power law behaviors in the small and large event size limit derived analytically are apparent . notice that there are only a few points in the slope = -1 regime in figure  [ cuts_figure](a ) .",
    "this is because the event sizes are increasing exponentially as shown in eq .",
    "( [ l_iter ] ) .",
    "we can populate the tail of this distribution by combining many data sets with slightly different initial cut positions , @xmath26 , @xmath118 , and the results are shown in figure  [ cuts_mixture ] .",
    "this models a mixture of data from systems with the same number of resources @xmath22 but different large scale cutoffs , @xmath100 .",
    "figure  [ cuts_figure](a ) and ( b ) also show the probability density @xmath122 ( small squares ) vs. the event size @xmath25 . for small @xmath8 ,",
    "the cumulative probability has a steeper slope than the probability density , while for large @xmath8 the points are very nearly the same and have the same slope .",
    "this occurs because the probability density for the cuts model is inherently discrete .",
    "as discussed in section iii , if a probability density is smooth and continuous , the corresponding cumulative probability can be found by integrating the density .",
    "for example , if @xmath103 is a power law with exponent @xmath129 , then the cumulative probability is a power law with exponent @xmath130 , as we intuitively expect .",
    "this simple , intuitive result also applies when data consists of a set of discrete probabilities @xmath51 which are sufficiently dense that we can use them to derive a continuous probability distribution as we did in the plr model eq .",
    "( [ cum2 ] ) . however , in figure  [ cuts_figure ] the discrete probabilities @xmath51 are not dense , and the relationship between @xmath10 and @xmath51 is not the same as in the continuous case . as @xmath25 becomes large , eq .",
    "( [ l_iter ] ) indicates @xmath131 , and @xmath132 .",
    "the cumulative probability distribution becomes the same as the probability density in the tail : @xmath133 this asymptotic behavior is verified in figure  [ cuts_figure](a ) and figure  [ cuts_mixture ] .",
    "we can also analytically solve for the optimal cut positions in the case of a power law distribution of sparks : @xmath134 @xcite .",
    "using the same procedure as in the exponential case , we find the following results for the discrete probabilities and the corresponding event sizes : @xmath135 so that @xmath136 for @xmath137 , the slope of @xmath10 vs. @xmath8 on a log - log plot approaches @xmath138 as @xmath8 becomes small .",
    "as @xmath8 becomes large , the slope approaches @xmath139 .",
    "these asymptotic relationships are derived in appendix a. in addition , as @xmath38 approaches infinity the initial probability density @xmath140 decays faster than any power law .",
    "notice that in the limit @xmath141 we recover @xmath142 as the exponent for the cumulative probability distribution , which is exactly the same as the exponential result .",
    "we also investigate the event size distribution for power law @xmath103 by solving the recursion relation in eq .",
    "( [ cuts_recur_power ] ) numerically .",
    "figure  [ cuts_power ] shows the cumulative ( large circles ) and noncumulative ( small squares ) event size distributions for a power law spark distribution with @xmath143 ( i.e. @xmath144 ) . for small @xmath8",
    "this leads to a cumulative probability distributions of event sizes that has a shallower slope than the corresponding data for the exponential spark density ( figure [ cuts_figure ] ) but still a steeper slope than for larger events .",
    "the slope of the cumulative distribution is close to the analytically calculated asymptotic value of @xmath145 in the small event limit ( appendix a ) . for large @xmath8",
    "the slope is approximately @xmath139 . the corresponding data for the case @xmath146 ( @xmath17 ) has slope @xmath139 for the entire range of event sizes .",
    "additionally , solutions of the cuts model obtained for a power law distribution of sparks has the feature that the large event sizes @xmath25 increase at a slower rate than in the corresponding exponential solution .",
    "therefore we are able to see more points in the tail of figure  [ cuts_power ] and easily confirm the slope @xmath139 that we derive analytically ( appendix a ) .",
    "we next make more direct comparisons between the continuum , plr , and cuts models . despite the apparent differences ,",
    "we show that there are a variety of cases where one model can be used to approximate another . in these cases",
    "the resulting power laws match .",
    "however , in doing this we face several challenges :    * the plr and continuum models use the expected event size as the cost function : @xmath147 ( yield function @xmath12 in eq .",
    "( [ cuts_yield ] ) ) .",
    "the cuts model is most easily solved analytically for the cumulative cost function @xmath148 ( yield function @xmath149 in eq .",
    "( [ cuts_yield2 ] ) ) , where @xmath150 . *",
    "the continuum and cuts models specify a probability density @xmath103 which is a continuous function of the spatial position @xmath98 , while the plr model specifies condition categories @xmath11 with discrete probabilities @xmath51 which have no _ a priori _ association with a position @xmath98 . *",
    "the cuts and plr models specify a set of discrete probabilities @xmath51 and corresponding set of discrete event sizes @xmath25 , while the continuum model uses only continuous @xmath103 and @xmath97 . *",
    "the cumulative distribution @xmath10 is an analytic function of the probability density @xmath43 only if the density is a _ continuous _ function of event size .",
    "if instead the density @xmath43 ( or @xmath151 ) is discrete , as it is for the cuts and plr models , there is no universal analytic relationship between the cumulative and noncumulative distributions .",
    "we address all these issues in the subsections that follow .",
    "to reconcile the cost functions of the different models we can either find solutions to a `` @xmath152-cuts model '' which uses the original cost function @xmath152 , or we can adapt the plr model to use the modified cost function @xmath153 .",
    "the recursion relation for the cuts model with the cost function @xmath152 , eq .",
    "( [ cuts_recur1 ] ) is more difficult to solve , but fortunately we can determine the asymptotic behavior of this `` @xmath152-cuts model '' without solving those equations .",
    "this is because the asymptotic results in the simple exponential `` @xmath153-cuts model '' ( eqs .",
    "( [ cuts_limit1 ] ) and ( [ cuts_limit2 ] ) ) are valid for both cost functions @xmath152 and @xmath154 . in particular , the optimal solutions @xmath155 are asymptotically equal for the two costs ( @xmath153 , @xmath152 ) in the limits @xmath156 and @xmath157 . proof of this result is given in appendix b. this implies that our results for the cuts model can be directly compared to the results for the plr and continuum models in these limiting cases , as shown in table i. alternatively , we can modify the plr model to use the same cost function as the `` @xmath153-cuts model '' .",
    "this is particularly simple if the probability distribution of sparks @xmath103 is exponential , since cumulative and noncumulative exponential distributions are proportional to each other .",
    "to compare plr to other models , we also must decide how to associate the discrete probabilities @xmath51 in the plr model with positions @xmath98 . in the plr model , we derive scaling relations between resources and event sizes by imagining that each event category @xmath11 is associated with a region of the _ same _ total length @xmath59 , inside of which the probability is a uniform @xmath51 as illustrated in figure [ plr_nc ] .",
    "( the length @xmath59 is later divided up into optimal event sizes @xmath25 . )",
    "this procedure is discussed in section iii .    to construct a mapping from the event categories to the real axis",
    ", we can use this length @xmath59 to derive a right - continuous piecewise constant probability function @xmath103 on the real line , as illustrated in figure [ plr_of_x ] .",
    "we order the @xmath51 so that they are monotonically nonincreasing , associate each category @xmath11 with a length @xmath59 , and place the categories adjacent to one another on the real line .",
    "then @xmath158 whenever @xmath159",
    ". we can then use plr formalism to calculate the optimal event sizes @xmath25 within each category .",
    "this defines a event size function @xmath97 which describes the size of the loss which occurs when a spark hits position @xmath98 .",
    "we define @xmath160 whenever @xmath161 .",
    "note that @xmath59 is the maximum possible event size in plr and the large - scale cutoff @xmath100 is defined by @xmath59 and the number of event categories @xmath162 : @xmath163 .    intuitively , here it is helpful to think of the plr model as a coarse - grained version of the cuts model .",
    "the piecewise constant spark probability density @xmath164 can be viewed as an approximation to some underlying continuous probability density @xmath165 which has been averaged to produce a constant value over each interval of length @xmath59 . as",
    "@xmath59 becomes smaller , plr becomes a better approximation to the cuts model with a continuous @xmath103 .",
    "the cuts and plr models can compared in many regimes because they both produce inherently discrete event size distributions .",
    "the spatial mapping of event categories to spatial positions , and the approximation of a continuous @xmath166 ( for cuts ) by a piecewise continuous @xmath164 composed from the @xmath51 s lead to excellent agreement between the two models for a wide range of @xmath103 .",
    "for the cuts model we choose a continuous probability density : @xmath167 for the plr model we choose a probability density which is piecewise constant on intervals of length @xmath59 : @xmath168 and chose these densities so that @xmath165 matches @xmath164 at the mid - point of each interval .",
    "the density @xmath169 can be thought of as a coarse - grained average of @xmath170 .",
    "graphs of these functions are shown in figure  [ comp](a ) .",
    "we can trivially modify the plr model to use the same cost function @xmath153 because cumulative and noncumulative exponential distributions are proportional to each other , implying @xmath171 .",
    "next we use the plr model to find the optimal @xmath25 , and thus @xmath151 and @xmath172 for the spark probability density @xmath164 .",
    "we take @xmath173 and a large scale cutoff @xmath100 which is @xmath174 times larger than @xmath59 .",
    "the cumulative probability @xmath172 vs. event size @xmath25 ( large circles ) is shown in figure  [ comp](b ) .",
    "note that @xmath172 has a exponent of @xmath142 , which is exactly the same as the exponent for the noncumulative probability @xmath151 .",
    "again , this is due to the discrete nature of @xmath151 and the exponential @xmath103 which is approximated by the piecewise constant @xmath164 .    to obtain the corresponding solution for the event size distribution of the cuts model",
    ", we use the recursion relation ( eq .",
    "( [ cuts_recur2 ] ) ) to compute the optimal @xmath25 , @xmath122 and @xmath172 for the continuous , exponential @xmath175 .",
    "we choose the initial cut positions based on our solution for the largest event obtained for the corresponding plr model above .",
    "in other words , we take @xmath176 ( the endpoint of the interval on the real axis for the mapping of the plr categories into position space ) and @xmath177 , where @xmath178 is the largest event size in the plr model .",
    "we then iterate the recursion relation eq .",
    "( [ cuts_recur2 ] ) backwards until we reach the cut at position @xmath179 .",
    "the cumulative probability @xmath172 vs. event size @xmath25 ( small squares ) is shown in figure  [ comp](b ) .",
    "@xmath172 has a exponent of @xmath142 in agreement with plr for the same cost function @xmath154 , and the corresponding spark distributions @xmath165 and @xmath164 .",
    "thus the cumulative probabilities for the cuts and plr models are remarkably similar .",
    "this indicates that even outside the asymptotic regime ( @xmath157 ) , the cuts model and the plr model match for an exponential spark probability density .",
    "note that in this example we are still in the regime where the cuts model solution @xmath172 vs. @xmath25 has a slope of @xmath142 on a log - log plot  that is , the dense resource regime .",
    "we next compare the continuum model , which has a continuous event size function @xmath97 , with the cuts and plr models which both have a piecewise constant @xmath97 , corresponding to the discrete @xmath25 for these models ( and the spatial mapping , in the case of plr ) .",
    "the continuum model can not be extended outside of the dense resource regime , because it builds in the assumption of a continuous event size function @xmath97 .",
    "interestingly , all three models can be made to agree in the dense resource limit .",
    "for the plr and cuts models , these correspond to regimes in which the piecewise constant function @xmath97 becomes nearly continuous .",
    "we begin by comparing the continuum model to the cuts model .",
    "the cuts model predicts that for small event sizes ( and thus dense resource allocations ) , the function @xmath97 will be close to continuous ( as shown in figure  [ lx_figure ] ) .",
    "we showed earlier in eq .",
    "( [ cuts_limit2 ] ) that in the limit @xmath180 , the @xmath153-cuts model predicts a power law with exponent @xmath142 . in appendix b ,",
    "we show that the solution for the cuts model with the modified cost function @xmath154 is the same as the solution for the cuts model with the original cost function @xmath152 in this limit . therefore that the cuts model matches the continuum model in the limit @xmath181 , when the two models have the same cost function @xmath152 .",
    "note that even though @xmath97 is approaching a continuous function , @xmath103 remains discrete , so that the cumulative distribution of events @xmath10 is in fact a steeper power law than the density in this regime , as illustrated numerically in figure [ cuts_figure ] .",
    "next , to compare the continuum model to plr , we note that in plr , @xmath97 becomes close to continuous when the category size @xmath59 becomes small and the event sizes @xmath25 become very small .",
    "formally , this corresponds to the limit @xmath182 with @xmath183 for every @xmath25 and every @xmath59 . as for the cuts model ,",
    "this is a case where the discrete plr model produces a nearly continuous event size function @xmath97 , although , the event size probability density @xmath151 remains sufficiently discrete that computing the cumulative distribution @xmath172 does not simply correspond to a unit increase in the exponent , and instead we must be cautious and do additional work to compute the cumulative exponent , we did for the plr model in eq .",
    "( [ cuts_limit2 ] ) and appendix a.    in @xmath14 plr predicts that the discrete event size probability density is @xmath184 , regardless of the density at which points in that density are sampled .",
    "furthermore the plr model begins with the @xmath51 as input ( solving for the @xmath25 by optimizing resource allocations ) , so we must work with the density first , then solve for the cumulative distribution , . because the @xmath51 and @xmath25 are discrete , there is no simple relationship between the density @xmath151 and the cumulative distribution @xmath172 .",
    "naively , one might expect the cumulative probability to be the integral of the probability density and guess @xmath185 . as we have stated previously ,",
    "this is emphatically not the case .",
    "figure  [ plr_small ] is a numerical simulation of plr for a piecewise constant @xmath103 ( using the mapping of event categories into spatial positions , each of length @xmath59 given by the width over which @xmath103 is constant ) , which is defined so that the left - hand end - point of each interval has a value which fits an exponential density function .",
    "this figure shows @xmath186 , as predicted , yet @xmath187 as well . in other words ,",
    "the cumulative and noncumulative probabilities on a log - log plot both have a slope of @xmath142 .",
    "in fact , it is straightforward to show analytically that the cumulative slope matches the noncumulative slope in this case .",
    "the plr probabilities @xmath188 are given as exponentially distributed : @xmath189 , where @xmath59 is the category size , which is subdivided into regions of size @xmath25 .",
    "the large scale cutoff @xmath100 is @xmath190 , where @xmath162 is the number of categories .",
    "we can calculate the cumulative probability : @xmath191 where we have used the fact that because @xmath182 the reciprocal of the norm , @xmath192 , approaches zero and we can approximate the sum as an integral .",
    "we also drop the term proportional to @xmath193 , which is much smaller than @xmath194 .",
    "thus the cumulative distribution is proportional to the noncumulative distribution in this limit , and the continuum , cuts , and plr models all match in the regime where resources are dense and event sizes are small .",
    "a final question is whether the plr model is similar to the cuts model in the limit of very large event sizes , where the cuts model predicts @xmath172 has a exponent of @xmath139 . as we mentioned earlier , in one dimension",
    "the plr model predicts @xmath184 for every @xmath25 and every @xmath59 .",
    "however , cumulative distributions which result from discrete probability densities can have any one of a large class of shapes and exponents . for plr to predict a cumulative slope @xmath185 ( i.e. the same as the cuts model for large events ) , the discrete plr points",
    "@xmath122 must be sufficiently dense so that the summation of those points approximates an integral .",
    "this occurs when the @xmath25 increase very slowly , or equivalently if the spark probability density @xmath103 is very heavy tailed . for spark probability densities @xmath103 ( such as the exponential ) which drop off quickly , the @xmath25 increase rapidly ( see eq .",
    "( [ cuts_recur2 ] ) ) and plr will not predict a slope @xmath195 for an individual optimized system .",
    "interestingly , most data from complex systems like forest fires  @xcite and web traffic  @xcite are sufficiently dense that an integral approximation is reasonable .",
    "cumulative slopes of @xmath196 are consistent with the plr model when interpreted as in section iii .",
    "as previously mentioned , this might best be explained by viewing these data sets as _ mixtures _ of data from systems which are individually optimized . in this case a probability density with a sparsely populated tail ( such as fig .",
    "[ cuts_figure](a ) ) might be mixed with similar data so that the tail becomes densely populated .",
    "this is precisely what is done in for the cuts model in figure  [ cuts_mixture ] and here the mixture power law retains @xmath197 .",
    "thus it is possible that mixtures of plr models could be made consistent with the cuts model in the limit of large event sizes .",
    "however , because plr makes analytic predictions only for noncumulative probability densities @xmath151 , in the absence of a more thorough analysis of mixtures of plr solution , we can draw no further general conclusions about the behavior of cumulative probabilities @xmath172 for large @xmath25 in this paper .",
    "instead , we reserve this issue for a more detailed analysis in @xcite .",
    "abstract forest fire models have arisen as paradigms in complex systems theory , initially for the soc mechanism @xcite and later also for hot @xcite .",
    "inspiration for soc comes from statistical physics , where lattice models have played a central role in theoretical explorations of large scale consequences of local interactions @xcite .",
    "hot is motivated by biology and engineering , where lattice models are a less natural starting point .",
    "nonetheless , in an effort to clarify comparisons between the mechanisms , and because of their pedagogical explanatory power , study of hot also began with lattice models .",
    "however , in the limit of large lattices , the hot lattice model can become somewhat pathological , which led to the alternative hot models analyzed in this paper . in this section",
    "we discuss the nature of this pathology .",
    "it arises in what corresponds to a natural limit for percolation in statistical physics that goes awry in the analogous hot model , because the difference in scaling between the @xmath18dimensional contiguous regions , and the @xmath198dimensional barriers .",
    "soc builds on the concept of criticality in statistical physics .",
    "the percolation phase transition is associated with a critical density of occupied sites , at which a connected cluster of nearest neighbor occupied sites first spans the lattice ( say , from top to bottom ) in the limit of infinite lattice size . infinitesimally above the critical density , the infinite cluster exists with probability converging to unity as the lattice size diverges .",
    "simultaneously the probability any given site is connected to the infinite cluster converges to zero .",
    "this occurs because the infinite cluster is a fractal .",
    "an immediate consequence of the fact that the fractal dimension is less than the lattice dimension is that removal of the infinite cluster ( i.e. in the largest possible fire ) does not alter the lattice density even though the cluster is system - spanning ( i.e. would stretch across the entire forest ) . at the critical density , and only at the critical density ,",
    "the distribution of cluster sizes in the ensemble is described by a power law .",
    "in statistical physics power law predictions are typically sharpened by taking the limit of infinite lattice size . however , in attempting this for the hot lattice model a problem emerges , that makes the large lattice limit ill - posed .",
    "this also reveals more clearly an intrinsic flaw in the lattice model when it comes to modeling mechanisms and costs associated with suppression of fires and other cascading events in highly designed or evolved systems @xcite .",
    "consider the lattice model in @xmath199 .",
    "in both the hot and soc lattice model a firebreak forms when any unbroken chain of empty lattice sites isolates a connected cluster , even if the chain is only one lattice spacing wide . in soc ( and criticality ) the underlying randomness with which configurations are generated , and the symmetry between vacant and occupied sites , results in a critical density of 0.4 ( 0.59 ) in @xmath199 which is bounded away from unity , so that a finite fraction of the lattice is devoted to both clusters and firebreaks in the limit of infinite size .",
    "in other words , the size of the firebreaks scales in the same way as the size of the connected clusters .",
    "however , in the hot version , simple optimization of yield ( number of trees remaining after a single spark , averaged over the spark distribution ) leads to macroscopic , compact clusters of trees separated by narrow ( one lattice spacing wide ) , efficient ( linear ) firebreaks .",
    "thus in the limit of large lattices the cost in density and yield associated with each firebreak becomes vanishingly small .    to visualize how the cost of firebreaks becomes negligible for large lattices and why this is a problem , consider large @xmath200 lattices as @xmath29 .",
    "a vertical line of empty sites extending from top to bottom on the lattice involves @xmath22 sites , and so the cost in lattice density associated with making those sites vacant is @xmath201 .",
    "this cut divides an otherwise fully occupied lattice into two separate regions ( left and right of the firebreak ) . in the limit @xmath29 , the cost in density of the cut is zero , even though the division of the lattice into two separate regions is preserved .",
    "similarly , a collection of equally spaced vertical and horizontal cuts on an otherwise occupied lattice results in a gridded configuration dividing the lattice into square regions of equal size , each outlined by a firebreak one lattice spacing wide on each of the four sides . for this configuration ,",
    "all fires are of equal size ( the area of the contiguous square ) . for a finite lattice",
    "such a solution could only be optimal for a spatially uniform distribution of sparks .",
    "however , in the limit @xmath29 an infinite family of such solutions all achieve the maximum yield of unity .",
    "all that is required is that the cuts be positioned far enough apart that the grid of firebreaks consume zero density , yet close enough together that the density cost associated with a fire in any individual square of contiguous occupied sites is also zero .",
    "this is achieved whenever the spacing between grid lines scales like @xmath202 with @xmath203 .",
    "this produces a yield of unburnt trees that is asymptotically perfect ( i.e. approaches unity ) for the entire forest for any distribution of sparks , with infinitesimal fire sizes .",
    "it is straightforward to generalize this argument to higher dimensions , because it relies only on the fact that the barriers scale differently ( like @xmath204 ) compared to the the compact regions ( like @xmath4 ) .",
    "unrealistically , a literal interpretation of the lattice model suggests that with proper management and minimal cost , essentially all fires could be eliminated@xcite . while this form of the hot lattice model is useful pedagogically as it exhibits such striking differences from the soc version , it has too many flaws to be taken literally as a model of real forest fires because the costs of resources for suppression are not accounted for properly .",
    "while there is a natural duality between vacant and occupied sites in the models of statistical physics , in hot models vacancies are resources which define boundaries that scale differently than the bulk substrate . for specific applications ,",
    "resources are rarely ( if ever ) simply the absence of substrate .",
    "even firebreaks constructed on forest land ( e.g. roads ) are not simply the absence of trees , but are cut and maintained at significant economic expense .",
    "the abstract hot models studied in this paper correct the pathology of the original hot lattice model by explicitly accounting for resource use .",
    "the plr and cuts models do this through an explicit cap on the total resources available .",
    "the continuum model does this through inclusion of an explicit resource cost term in the yield function .",
    "several preliminary calculations suggest that at least within a range of functional representations , the specific manner in which resources are accounted for is not a crucial factor in determining the exponent in the power law for these models .",
    "for example , a more general cost - benefit term describing resource use can replace the explicit cap on resources in the plr model , at the expense of analytical tractability of the model , but with no significant change in the exponent .",
    "analogously , the cuts model ( in the limit of small event sizes ) and the continuum model can lead to the same power law exponent , in spite of the fact that they account for the cost of resources in different ways .",
    "the key feature in determining the size distribution for a given model is that we optimize , while measuring the cost ( or loss ) in terms of the average event size .",
    "alternative formulations of the continuum model @xcite have considered alternative cost or utility functions , which clearly can lead to modifications in the event size distribution .",
    "for example , if the cost function puts a large penalty for events greater than a given size , then more resources will be devoted to large events , at the expense of more smaller events , and a great average size .",
    "such considerations are clearly relevant in cases such as finance and economics , where risk - seeking and risk - averse strategies come into play .    compared with models based on criticality , the power laws predicted by all of the hot models are much steeper , and",
    "have the opposite trends with dimensionality .",
    "in criticality the exponents become smaller for lower dimensional problems .",
    "this is the opposite of the trends observed in data @xcite , which typically exhibit steeper power laws for lower dimensional problems , as in hot .",
    "it is worth noting ( especially given our focus on @xmath14 ) that while percolation in @xmath14 has the ( trivial ) critical density of unity the only way connectivity can arise across a one dimensional lattice is for every site to be occupied the configurations and size distribution ( not a power law in @xmath14 ) which arise in random percolation in the neighborhood of the critical density even in that case are completely unlike those that arise in the corresponding one - dimensional hot lattices . in criticality ,",
    "the placement of vacancies is random , whereas in hot the specific placement of vacancies is dictated by optimization .    in models based on criticality ,",
    "the self - similar , fractal event shapes , reflect a mechanism which is intrinsically scale - free , producing a single exponent , spanning all scales .",
    "in contrast , in hot models heavy - tailed distributions arise from optimization on a macroscopic scale .",
    "compact regions predicted by hot are not fractal or self - similar and there is no reason to expect that small scale events will _ a priori _ be described by the same power law as large scale events .",
    "the cuts model is a clear example in which we do observe a heavy - tailed event size distribution , with asymptotically different power law exponents as we vary the scale .",
    "this model highlights the essential difference between the dense and sparse resource regimes , which in the original formulations of the continuum and plr models emerge from the distinction between inherently continuum and discrete fields describing probabilities , resources , and losses . in the continuum case , it is simply not possible to capture features which could arise as a consequence of discrete , sharp , well - separated boundaries the sparse resource regime .",
    "thus the continuum model agrees with the cuts model only in the limit that the cuts ( which are sharp and discrete ) are placed asymptotically close together , i.e. the dense resource limit . on the other hand , the plr model , which assumes discrete event categories ,",
    "can in principle capture both the dense resource limit and the sparse resource regime , though the latter will need additional treatment because of the intrinsic role that mixtures play in real data . in this paper",
    "we explored the plr model in the limit of dense resources , by taking the length scales of the system @xmath59 and the event sizes @xmath205 simultaneously to zero . in this limit",
    ", the plr model can capture the the continuous , spatial spark distribution @xmath103 , though plr ( and cuts ) remain intrinsically discrete .    based on this analysis",
    ", it may appear that the cuts model is the clear winner , simultaneously capturing the full range of behaviors seen in the other two , and this would be true if we only considered @xmath14 .",
    "however , in order to generalize the cuts model beyond @xmath14 it is necessary to constrain the optimization procedure .",
    "for example , in @xcite this was done by specifying a grid design . in many cases ,",
    "such a constrained design may not be desirable , and the abstractions of the other models may be preferred . the continuum and plr models are both easily formulated in arbitrary dimension @xmath4 , but with different predictions for the exponents .",
    "as we ve shown here , the plr model can be extended to the dense resource regime , where it agrees with the predictions of the continuum model .",
    "the reverse is not the case . in that sense ,",
    "the continuum model is less flexible .",
    "furthermore , the plr model has been far more successful in capturing statistics of event size distributions , assuming data sets are dense enough to be described as continuous distributions ( e.g. assuming they are mixtures @xcite ) .",
    "examples which have been studied include world wide web traffic , forest fires , and power outages @xcite .    in comparison",
    ", we do not yet have any clear examples where the predictions of the continuum model have been shown to apply .",
    "perhaps the reason behind this lies in the fact that data is almost exclusively collected for large events in the sparse resource regime . in regimes where resources are abundant",
    ", one may simply choose not to optimize .",
    "small file downloads , fires , and outages are rarely monitored , and small scale cutoffs , whether deliberately imposed for convenience or arising from an inherent physical mechanism , tend to prevent detailed statistical analysis of this regime . in any case ,",
    "statistical distributions remain only a starting point for understanding mechanisms for complexity and modeling system failure .",
    "success arises from the study of simple models when their predictions capture aspects of the system which can be described and quantified at a relatively low resolution . from this initial success",
    ", they can inspire a sequence of higher resolution models and observations to understand and anticipate detailed mechanisms for cascading failure in natural and technological systems .",
    "in this appendix we derive the slope of @xmath91 on a log - log plot for a cuts model where the initial probability density is described by a power law , @xmath206 .",
    "first we use the cuts model to find an analytic description for the set of discrete probabilities @xmath51 and event sizes @xmath25 : @xmath207    we also recall the definitions for the cuts positions @xmath26 and the cumulative probabilities @xmath208 :    @xmath209    the slope of @xmath91 on a log - log plot can be calculated in limiting cases by dividing @xmath210 by @xmath126 , taylor expanding and dropping higher order terms .",
    "@xmath211 - \\log l_i}\\end{aligned}\\ ] ]    now we will assume that @xmath25 is small compared to @xmath212 and we will derive terms which can be taylor expanded to first order in @xmath213 .",
    "we first evaluate numerator of eq .",
    "( [ power_step2 ] ) :    @xmath214    now we evaluate the denominator :    @xmath215 - \\log l_i } \\nonumber \\\\ & = &   -\\log a - \\log l_i + ( a+1 ) \\log ( { c_{i-1}}+ l_i ) - a \\log { c_{i-1}}\\nonumber \\\\ & &    + \\log\\left(1-(1 + \\frac{l_i}{{c_{i-1}}})^{-a}\\right ) \\nonumber \\\\ & = & -\\log(a l_i ) + ( a+1)\\log { c_{i-1}}+ ( a+1 ) \\log\\left(1 + \\frac{l_i}{{c_{i-1}}}\\right ) \\nonumber \\\\ & &     -   a \\log { c_{i-1}}+ \\log\\left(1-\\left(1 + \\frac{l_i}{{c_{i-1}}}\\right)^{-a}\\right ) \\label{denominator}\\end{aligned}\\ ] ]    we assume @xmath216 and use the binomial expansion on the last term .",
    "then eq .",
    "( [ denominator ] ) becomes : @xmath215 - \\log l_i } \\nonumber \\\\ & = & \\log\\left(\\frac{{c_{i-1}}}{a l_i}\\right ) + ( a + 1)\\log \\left(1 + \\frac{l_i}{{c_{i-1}}}\\right ) \\nonumber \\\\ & &   + \\log\\left(1 - \\left(1- \\frac{a l_i}{{c_{i-1 } } } + \\frac{a(a+1)}{2}(\\frac{l_i}{{c_{i-1}}})^{2}\\right)\\right ) \\nonumber \\\\ & = & \\log\\left(\\frac{{c_{i-1}}}{a l_i}\\right ) + ( a + 1)\\log \\left(1 + \\frac{l_i}{{c_{i-1}}}\\right ) + \\nonumber \\\\ & & \\log\\left(\\frac{a l_i}{{c_{i-1}}}\\left(1 - \\frac{(a+1)}{2}(\\frac{l_i}{{c_{i-1}}}\\right)\\right ) \\nonumber \\\\ & = & ( a+1)\\log\\left(1 + \\frac{l_i}{{c_{i-1}}}\\right ) + \\log\\left(1 - \\frac{(a+1)l_i}{2{c_{i-1}}}\\right ) \\label{denominator2}\\end{aligned}\\ ] ] inserting the numerator and denominator back into eq .",
    "( [ power_step2 ] ) we have : @xmath217 now we use the taylor expansion @xmath218 : @xmath219 @xmath220 this is the slope of @xmath91 on a log - log plot in the limit where @xmath8 becomes small .",
    "now we will look in the opposite limit , where @xmath8 becomes large .",
    "we first show that @xmath221 implies @xmath222 if @xmath223 . using the definition for @xmath224 in eq .",
    "( [ cuts_recur_power_2 ] ) we derive a recursion relation for @xmath225 .    by definition",
    ": @xmath226 we note that @xmath227 will always be less than 1 .",
    "therefore we can use the binomial expansion and write out the terms to lowest order in @xmath228 : @xmath229 now we note that if we assume @xmath230 for large @xmath11 we can drop all terms of order @xmath231 .",
    "also , for @xmath223 the first term in the denominator will be much larger than the other terms , and we drop all the other terms in the denominator . then we have : @xmath232 we can then find the ratio of consecutive terms : @xmath233 because @xmath234 for all @xmath227 , we see that our assumption that @xmath235 was indeed valid , and that the sequence goes to zero as @xmath11 approaches infinity .",
    "we also note that if @xmath236 , we can no longer assume that the first term in the denominator in eq .",
    "( [ g_middle ] ) is much larger than 1 .",
    "in fact , as @xmath237 the first term in the denominator approaches 1 , and it is not true that @xmath238 approaches zero for large @xmath25 .",
    "we now solve eq .",
    "( [ power_step2 ] ) for terms which we can taylor expand to first order in @xmath238 .",
    "first we simplify the numerator : @xmath239 where in the last line we have used the taylor expansion @xmath240 .",
    "as @xmath238 approaches @xmath241 , @xmath242 becomes large and negative , and the terms inside the braces in eq .",
    "( [ numerator2 ] ) become negligible .",
    "therefore the numerator in this limit is : @xmath243 we simplify the denominator of eq .",
    "( [ power_step2 ] ) .",
    "@xmath215 - \\log l_i } \\nonumber \\\\ & = & -\\log(a l_i ) - ( a+1 ) \\log \\left(\\frac{{c_{i-1}}}{l_i } + 1 \\right ) \\nonumber \\\\ & & + ( a+1 ) \\log l_i - a \\log l_i \\nonumber \\\\ & & + \\log \\left ( \\left ( \\frac{{c_{i-1}}}{l_i } \\right)^{-a } - \\left(\\frac{{c_{i-1}}}{l_i } + 1 \\right)^{-a}\\right ) \\label{denominator3}\\end{aligned}\\ ] ] again we use the binomial expansion to approximate the last term in the denominator : @xmath244 where in the last line we have used that @xmath245 .",
    "then the denominator ( eq .  [ denominator3 ] ) becomes @xmath215 - \\log l_i } \\nonumber \\\\ & = & - \\log a + ( a+1)\\log \\left(\\frac{{c_{i-1}}}{l_i } + 1 \\right ) + \\log \\left",
    "( \\left ( \\frac{{c_{i-1}}}{l_i } \\right)^{-a } \\right ) \\nonumber \\\\ & = & - a \\log \\left ( \\frac{{c_{i-1}}}{l_i } \\right ) + ( a+1)\\log \\left(\\frac{{c_{i-1}}}{l_i } + 1 \\right ) - \\log a \\nonumber \\\\",
    "& = & - a \\log \\left ( \\frac{{c_{i-1}}}{l_i } \\right ) + \\nonumber \\\\ & & \\left\\ { -(a+1)\\left [ \\frac{{c_{i-1}}}{l_i } + { \\cal o } \\left ( \\frac{{c_{i-1}}}{l_i } \\right)^{2 } \\right ] - \\log a \\right\\ } \\label{denominator4}\\end{aligned}\\ ] ] where in the last line we used the taylor expansion for @xmath246 .",
    "again we see that for any finite @xmath38 the terms inside the braces in the last line of eq .",
    "( [ denominator4 ] ) become negligible compared to @xmath247 in the limit that @xmath238 becomes small . in this limit",
    "the denominator can be approximated as : @xmath248 dividing eq .",
    "( [ numerator_last ] ) by eq .",
    "( [ denominator5 ] ) , we see that for small @xmath238 , the slope of @xmath10 on a log - log plot is : @xmath249 we have shown that if @xmath38 , the exponent for the power law spark distribution , is greater than 1 , then @xmath238 becomes small as @xmath25 becomes large . in this case",
    "we have shown @xmath139 to be the asymptote of the exponent of @xmath10 for large @xmath8 .",
    "we are interested in comparing the optimal solutions for two different cost functions ( equivalently , for two different yield functions @xmath12 in eq .",
    "( [ cuts_yield ] ) and @xmath149 in eq .",
    "( [ cuts_yield2 ] ) ) . the first cost function @xmath250 equates",
    "cost with expected event size , and is used in plr and continuum models .",
    "the second cost function @xmath251 equates cost with expected _ transferred _ event size and is used in the cuts model .",
    "this situation arises when the frequency with which an event is `` transferred '' is equal to the cumulative probability of all larger events .",
    "one example is sequentially linked web files . though @xmath153 is less intuitive than @xmath152 , it has the very nice property that one can analytically solve for the optimal event sizes @xmath25 given cost function @xmath153 . in most situations , however , we are really interested in optimizing the original cost function @xmath152 .    in this section",
    "we will show that the optimal solutions @xmath155 are the _ same _ for either definition of cost ( @xmath153 , @xmath152 ) in the limits @xmath156 and @xmath157 .",
    "this allows us to directly compare analytic results from the cuts model with results from continuum and plr models in limiting cases .            comparing eq .",
    "( [ cuts_recur2_app ] ) and  ( [ cuts_recur1_app ] ) , we see they give the same result if the bracketed term in eq .  ( [ cuts_recur1_app ] ) , @xmath254 , is much smaller than @xmath51 .",
    "first we will show this is the the case in the limit @xmath157 .                        the position @xmath26 approaches the midpoint of the interval [ @xmath118,@xmath258 because @xmath264 . as the length of the interval goes to 0",
    ", the value of @xmath103 at the midpoint , @xmath265 , approaches the average value of @xmath103 over the interval .",
    "therefore the left hand side of eq .",
    "( [ cuts_last_app ] ) _ is _ negligible compared to the right - hand side . in the limit @xmath180 , @xmath266 is much smaller than @xmath51 .                      _",
    "acknowledgements : _ this work was supported by the david and lucile packard foundation , nsf grant no .",
    "dmr-9813752 , the james s. mcdonnell foundation , and the institute for collaborative biotechnologies through grant daad19 - 03-d-0004 from the u.s . army research office .",
    "m.m . was supported by a national science foundation graduate research fellowship ."
  ],
  "abstract_text": [
    "<S> power law cumulative frequency @xmath0 vs.  event size @xmath1 distributions @xmath2 are frequently cited as evidence for complexity and serve as a starting point for linking theoretical models and mechanisms with observed data . </S>",
    "<S> systems exhibiting this behavior present fundamental mathematical challenges in probability and statistics . </S>",
    "<S> the broad span of length and time scales associated with heavy tailed processes often require special sensitivity to distinctions between discrete and continuous phenomena . a discrete highly optimized tolerance ( hot ) model , referred to as the probability , loss , resource ( plr ) model , gives the exponent @xmath3 as a function of the dimension @xmath4 of the underlying substrate in the sparse resource regime . </S>",
    "<S> this agrees well with data for wildfires , web file sizes , and electric power outages . </S>",
    "<S> however , another hot model , based on a continuous ( dense ) distribution of resources , predicts @xmath5 . in this paper </S>",
    "<S> we describe and analyze a third model , the cuts model , which exhibits both behaviors but in different regimes . </S>",
    "<S> we use the cuts model to show all three models agree in the dense resource limit . in the sparse resource regime , </S>",
    "<S> the continuum model breaks down , but in this case , the cuts and plr models are described by the same exponent . </S>"
  ]
}