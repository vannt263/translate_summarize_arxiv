{
  "article_text": [
    "with advancing technology comes the need to extract information from increasingly high - dimensional data , whereas the number of samples is often limited .",
    "dimension reduction techniques and models incorporating sparsity become important solution strategies .",
    "partial least squares regression ( pls - r ) combines dimensionality reduction and prediction using a latent variable model .",
    "it was first developed for regression analysis in chemometrics @xcite , and has been successfully applied to many different areas , including sensory science and , more recently , genetics @xcite .",
    "moreover , pls - r algorithm is designed precisely to operate with high dimensional data .",
    "since the first proposed algorithm does not require matrix inversion nor diagonalization but deflation to find the latent components , it can be applied to problems with large numbers of variables .",
    "the latent components reduce the dimension by constructing linear combinations of the predictors , which successfully solved the collinearity problems in chemometrics @xcite .",
    "however , the linear combinations are built on all the predictors .",
    "the resulting pls - r model tends to overfit when the number of predictors increases for a fixed number of samples .",
    "therefore , variable selection becomes essential for pls - r in high - dimensional sample - limited problems .",
    "it not only avoids over - fitting , but also provides more accurate predictors and yields more interpretable estimates .",
    "for this reason sparse pls - r was developed by h. chun and s. keles @xcite .",
    "the sparse pls - r algorithm performs variable selection and dimension reduction simultaneously using an @xmath0 type variable selection penalty .",
    "however , the @xmath0 penalty used in @xcite penalizes each variable in each component independently and this can result in different sets of variables being selected for each pls component leading to an excessively large number of variables .",
    "in this work we propose a global criterion for pls that changes the sequential optimization for a k component model in statistically inspired modification of pls ( simpls ) @xcite into a unified optimization formulation , which we refer to as global simpls .",
    "this enables us to perform global variable selection , which penalizes the total number of variables across all pls components .",
    "we formulate pls - r with global sparsity as a variational optimization problem with the objective function equal to the global simpls criterion plus a mixed norm sparsity penalty on the weight matrix .",
    "the mixed norm sparsity penalty is the @xmath0 norm of the @xmath1 norm on the weights corresponding to the same variable used over all the pls components .",
    "the proposed global penalty encourages the selected variables to be shared among all the @xmath2 pls components .",
    "a novel augmented lagrangian method is proposed to solve the optimization problem , which enables us to obtain the global simpls components and to perform joint variable selection simultaneously .",
    "a greedy algorithm is proposed to overcome the computation difficulties in the iterations , and soft thresholding for sparsity occurs naturally as part of the iterative solution .",
    "experiments show that our approach to pls regression attains better or as good performance ( lower mean squared error , mse ) with many fewer selected predictor variables .",
    "these experiments include a chemometric data set , and a human viral challenge study dataset , in addition to numerical simulations .",
    "we review the developments in pls - r for both univariate and multivariate responses in section [ sec : pls overview ] , in which we discuss different objective functions that have been proposed for pls - r , particularly the statistically inspired modification of pls ( simpls ) proposed by de jong @xcite . in section [ sec : mix norm ] , we formulate the jointly sparse global simpls - r by proposing a new criterion that jointly optimizes over @xmath2 weight vectors ( components ) and imposing a mixed norm sparsity penalty to select variables jointly .",
    "the algorithmic implementation is discussed in section [ sec : algorithm ] with simulation experiments presented in section [ sec : experiment ] .",
    "the proposed jointly sparse global simpls regression is applied to two applications : ( 1 ) chemometrics in section [ sec : application1 ] and ( 2 ) predictive health studies in section [ sec : application2 ] .",
    "section [ sec : conclusion ] concludes the paper .",
    "partial least squares ( pls ) methods embrace a suite of data analysis techniques based on algorithms belonging to the pls family .",
    "these algorithms consist of various extensions of the nonlinear estimation by iterative partial least squares ( nipals ) algorithm that was proposed by herman wold @xcite as an alternative algorithm for implementing principal component analysis ( pca ) @xcite .",
    "the nipals approach was slightly modified by svante wold , and harald martens , in order to obtain a regularized component based regression tool , known as pls regression ( pls - r ) @xcite .",
    "suppose that the data consists of @xmath3 samples of @xmath4 independent variables @xmath5 and @xmath6 dependent variables ( responses ) @xmath7 . in standard pls regression",
    "the aim is to define orthogonal latent components in @xmath8 , and then use such latent components as predictors for @xmath9 in an ordinary least squares framework .",
    "the x weights used to compute the latent components can be specified by using iterative algorithms belonging to the nipals family or by a sequence of eigen - decompositions .",
    "the general underlying model is @xmath10 and @xmath11 , where @xmath12 is the latent component matrix , @xmath13 and @xmath14 are the loading matrices , @xmath2 is the number of components , @xmath15 and @xmath16 are the residual terms .",
    "the latent components in @xmath17 $ ] are linear combinations of the independent variables , hence pls can be viewed as a dimensional reduction technique , reducing the dimension from @xmath4 to @xmath2 .",
    "the latent components should be orthogonal to each other either by construction as in nipals @xcite or via constrained optimizations as in simpls @xcite .",
    "this allows pls to build a parsimonious model for high dimensional data with collinearity @xcite .      we assume , without loss of generality , that all the variables have been centered in a pre - processing step . for univariate @xmath9 ,",
    "i.e @xmath18 , pls regression , also often denoted as pls1 , successively finds @xmath19 weights @xmath20 $ ] as the solution to the constrained optimization @xmath21 where @xmath22 is the matrix of the residuals ( i.e. , the deflated matrix ) from the regression of the @xmath19-variables on the first @xmath23 latent components , and @xmath24 .",
    "these weights are then used to find the latent components @xmath25 $ ] .",
    "such components can be also expressed in terms of original variables ( instead of deflated variables ) , i.e. , as @xmath26 , where @xmath27 is the matrix containing the weights to be applied to the original variables in order to exactly obtain the latent components @xcite .    for a fixed number of components , the response variable @xmath9 is predicted in an ordinary least squares regression model , where the latent components play the role of the exogenous variables , @xmath28 this provides the regression coefficients @xmath29 for the model @xmath30 .",
    "depending on the number of selected latent components the length @xmath31 of the vector of pls coefficients changes .",
    "in particular , de jong @xcite had shown that the sequence of these coefficient vectors has lengths that are strictly increasing as the number of components increases .",
    "this sequence converges to the ordinary least squares coefficient vector and the maximum number of latent components obtainable equals the rank of the @xmath19 matrix .",
    "thus , by using a number of latent components @xmath32 , pls - r performs a dimension reduction by shrinking the @xmath19 matrix .",
    "hence , pls - r is a suitable tool for problems for which the data contains many more variables @xmath4 than observations @xmath3 .    the objective function in ( [ pls ] )",
    "can be interpreted as maximizing the squared covariance between @xmath9 and the latent component : @xmath33 . because the response @xmath9 has been taken into account to formulate the latent matrix , pls usually has better performance in prediction problems than principle component analysis ( pca ) does .",
    "this is one of the main differences between pls - r and pca @xcite .",
    "similarly to univariate response pls - r , multivariate response pls - r selects latent components in @xmath34 and @xmath35 , i.e. , @xmath36 and @xmath37 , such that the covariance between @xmath36 and @xmath37 is maximized . for a specific component ,",
    "the sets of weights @xmath38 and @xmath39 are obtained by solving @xmath40 where @xmath41 , @xmath42 , and @xmath22 and @xmath43 are the deflated matrices associated with @xmath19 and @xmath9 .",
    "notice that the optimal solution @xmath44 should be proportional to @xmath45 .",
    "therefore , the optimization in ( [ pls2crit ] ) is equivalent to @xmath46 for each component , the solution to this criterion can be obtained by using a so called pls2 algorithm .",
    "a detailed description of the iterative algorithm as presented by hskuldsson @xcite is in algorithm [ nipals ] .    in 1993",
    "de jong proposed a variant of the pls2 algorithm , called statistically inspired modification of pls ( simpls ) , which calculates the pls latent components directly as linear combinations of the original variables @xcite .",
    "the simpls was first developed as the solution to an optimization problem @xmath47 ter braak and de jong @xcite provided a detailed comparison between the objective functions for pls2 in ( [ pls2 ] ) and simpls in ( [ simpls ] ) and showed that the successive weight vectors @xmath48 can be derived either from the deflated data matrices or the original variables in pls2 and simpls respectively .",
    "let @xmath49 be the moore - penrose inverse of @xmath50 $ ] .",
    "the pls2 algorithm ( algorithm [ nipals ] ) is equivalent to solving the optimization @xmath51 @xmath52 both nipals and simpls have the same objective function but each is maximized under a different normalization constraint .",
    "nipals and simpls are equivalent when y is univariate , but provide slightly different weight vectors in multivariate scenarios .",
    "the performance depends on the nature of the data , but simpls appears easier to interpret since it does not involve deflation of the data set @xcite .",
    "we develop our globally sparse pls - r based on the simpls optimization formulation .",
    "one approach to sparse pls - r is to add the @xmath53 norm of the weight vector , a sparsity inducing penalty , to ( [ simpls ] ) .",
    "the solution for the first component would be obtained by solving @xmath54 the addition of the @xmath0 norm is similar to scotlass ( simplified component lasso technique ) , the sparse pca proposed by jolliffe @xcite . however , the solution of scotlass is not sufficiently sparse , and the same issue remains in ( [ pls_keles ] ) .",
    "chun and keles @xcite reformulated the problem , promoting the exact zero property by imposing the @xmath0 penalty on a surrogate of the weight vector instead of the original weight vector @xcite .",
    "for the first component , they solve the following optimization by alternating between updating @xmath55 and updating @xmath56 ( block coordinate descent ) .",
    "@xmath57 @xmath58 allen _ et .",
    "al _ proposed a general framework for regularized pls - r @xcite .",
    "@xmath59 in which @xmath60 is the cross - product matrix @xmath61 , and the regularization function @xmath62 is a convex penalty function .",
    "the formulation is a relaxation of simpls with penalties being applied to the weight vectors , and can be viewed as a generalization of @xcite .    as mentioned in the introduction",
    ", these formulations ( @xcite ) penalize the variables in each pls component independently .",
    "this paper proposes an alternative in which variables are penalized simultaneously over all components .",
    "first , we define the global weight matrix , consisting of the @xmath2 weight vectors , as    @xmath63 = \\left [ { \\begin{array}{*{20}c }     { \\begin{array}{*{20}c }      -   & { { \\bf w}'_{(1 ) } } &   -    \\\\ \\end{array } }   \\\\     { \\begin{array}{*{20}c }      -   & { { \\bf w}'_{(2 ) } } &   -    \\\\ \\end{array } }   \\\\     { \\begin{array}{*{20}c }     { } &   \\vdots   & { }   \\\\ \\end{array } }   \\\\     { \\begin{array}{*{20}c }      -   & { { \\bf w}'_{(p ) } } &   -    \\\\",
    "\\end{array } }   \\\\ \\end{array } } \\right].\\ ] ] notice that the elements in a particular row of w , i.e. , @xmath64 , are all associated with the same predictor variable @xmath65 .",
    "therefore , rows of zeros correspond to variables that are not selected .",
    "to illustrate the drawbacks of penalizing each variable in each component independently , as in @xcite , suppose that each entry in @xmath27 is selected independently with probability @xmath66 .",
    "the probability that the @xmath67 variable is not selected becomes @xmath68 , and the probability that all the variables are selected by at least one weight vector is @xmath69^p$ ] , which increases as the number of weight vectors @xmath2 increases .",
    "this suggests that for large @xmath2 the local variable selection approach of @xcite may not lead to an overall sparse and parsimonious pls - r model . in such cases a group sparsity constraint",
    "can be employed to limit the overall number of selected variables .",
    "the jointly sparse global simpls regression variable selection problem is to find the top @xmath2 weight vectors that best relate @xmath19 to @xmath9 , while using limited number of variables .",
    "this is a subset selection problem that is equivalent to adding a constraint on the @xmath70 norm of the vector consisting of the norms of the rows of @xmath27 , i.e , the number of nonzero rows in @xmath27 . for concreteness",
    "we use the @xmath1 norm for the rows .",
    "this leads to the optimization problem    @xmath71    in which @xmath72.\\ ] ]    the objective function ( [ global_pls ] ) , which we refer to as global simpls , is the sum of the objective functions ( [ simpls ] ) in the first @xmath2 iterations of simpls . instead of the sequential greedy solution in pls2 algorithm",
    ", the proposed jointly sparse global simpls regression solves for the @xmath2 weight vectors simultaneously .",
    "we introduce the @xmath73 factor to the objective function to interpret it as an empirical covariance .",
    "given the complexity of this combinatorial problem , as in standard optimization practice , we relax the @xmath70 norm optimization to a mixed norm structured sparsity penalty @xcite . @xmath74",
    "the @xmath1 norm of each row of @xmath27 promotes grouping entries in @xmath27 that relate to the same predictor variable , whereas the @xmath0 norm promotes a small number of groups , as in ( [ pls_keles ] ) .",
    "constrained eigen - decomposition and group variable selection are each well - studied problems for which efficient algorithms have been developed .",
    "we propose to solve the optimization ( [ global_pls_relax ] ) by augmented lagrangian methods , which allows one to solve ( [ global_pls_relax ] ) by variable splitting iterations . augmented lagrangian methods introduce a new variable @xmath60 , constrained such that @xmath75 , such that the row vectors @xmath76 of @xmath60 obey the same structural pattern as the rows of @xmath27 : @xmath77 the optimization ( [ global_pls_al ] ) can be solved by replacing the constrained problem by an unconstrained one with an additional penalty on the frobenius norm of the difference @xmath78 .",
    "this penalized optimization can be iteratively solved by an alternating direction method of multipliers ( admm ) algorithm @xcite , a block coordinate descent method that alternates between optimizing over @xmath27 and over @xmath60 ( see algorithm [ pls_algorithm ] ) .",
    "we initialize algorithm [ pls_algorithm ] with @xmath79 equal to the solution of simpls , and @xmath80 equal to the zero matrix .",
    "setting the parameter @xmath81 is nontrivial @xcite , and is hand - tuned for fastest convergence in some applications @xcite .",
    "once the algorithm converges , the final pls regression coefficients are obtained by applying simpls regression on the selected variables keeping the same number of components @xmath2 .",
    "the optimization over @xmath27 can be further simplified to a secular equation problem , whereas the optimization over @xmath60 can be shown to reduce to a soft thresholding operation .",
    "the algorithm iterates until the stopping criterion based on the norm of the residuals @xmath82 is satisfied , for some given tolerance @xmath83 .",
    "as described later in the experimental comparisons section , the parameters @xmath84 and @xmath2 are decided by cross validation .",
    "set @xmath85 , choose @xmath86 , @xmath79 , @xmath80    [ [ optimization - over - w ] ] optimization over @xmath27 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the following optimization in algorithm [ pls_algorithm ] is a nonconvex quadratically constrained quadratic program ( qcqp ) .",
    "@xmath87 @xmath88    we propose solving for the @xmath2 vectors in @xmath27 successively by a greedy approach .",
    "let @xmath89 and @xmath90 be the columns of the matrices @xmath60 and @xmath91 , and @xmath92 .",
    "the optimization over @xmath27 becomes @xmath93    let @xmath94 be an orthonormal basis for the orthogonal complement of @xmath95 .",
    "the optimization ( [ pls_greedy ] ) can be solved by the method of lagrange multipliers .",
    "the solution is @xmath96 , in which @xmath97 , @xmath98 and @xmath99 is the minimum solution that satisfies @xmath100 .",
    "let @xmath101 , then the optimization ( [ pls_greedy ] ) can be written as @xmath102 since we assume that @xmath55 is a linear combination of the basis vectors in @xmath94 , the orthogonality conditions in ( [ pls_greedy ] ) are automatically satisfied . hence these conditions have been dropped in the new formulation . then using lagrange multipliers",
    ", we can show that the solution takes the form as stated above .",
    "suppose there are two solutions of @xmath99 that satisfy @xmath100 , corresponding to two pairs of solutions to the optimization , @xmath103 and @xmath104 .",
    "since @xmath105 , @xmath106 @xmath107 by multiplying ( [ min_alpha1 ] ) by @xmath108 , and ( [ min_alpha2 ] ) by @xmath109 , then subtracting the two new equations , we have @xmath110 on the other hand , by multiplying ( [ min_alpha1 ] ) by @xmath109 , and ( [ min_alpha2 ] ) by @xmath108 , and subtracting the two new equations , we have @xmath111 given ( [ min_alpha3 ] ) and ( [ min_alpha4 ] ) , it can be shown that @xmath112 hence , one should select the minimum among all the feasible @xmath99 s .",
    "the equation @xmath100 is a secular equation , a well studied problem in constrained eigenvalue decomposition @xcite .",
    "the more general problem of least squares with a quadratic constraint was discussed in @xcite .",
    "we can diagonalize the matrix @xmath113 as @xmath114 , in which @xmath91 is diagonal with eigenvalues @xmath115 in decreasing order on the diagonal , and the columns of @xmath116 are the corresponding eigenvectors .",
    "define    @xmath117u'{\\bf b}.\\ ] ] let @xmath118 , then @xmath119 , and hence @xmath120 is a secular equation .",
    "@xmath121 increases strictly as @xmath99 increases from @xmath122 to @xmath123 , since @xmath124 is positive for @xmath125 .",
    "moreover , given the limits @xmath126 @xmath127 we can conclude that there is exactly one solution @xmath128 to the equation @xmath120 , @xcite . an iterative algorithm ( algorithm [ sec ] )",
    "is used to solve @xmath120 starting from a point to the left of the smallest eigenvalue @xmath123 @xcite .",
    "notice that calculating @xmath129 involves inverting a @xmath130 matrix , but @xmath113 has rank at most @xmath6 .",
    "we can reduce the computational burden by the use of woodbury matrix identity , @xmath131 the new format only requires inverting a @xmath132 matrix , and in most applications , the number of responses @xmath6 is much less than the number of predictors @xmath4 .",
    "furthermore , @xmath94 is involved in @xmath121 in the form of @xmath133 , in which @xmath134 is an orthonormal basis for @xmath95 .",
    "@xmath134 can be constructed by gram - schmidt process as the algorithm successively finds the weight vectors @xmath135 .",
    "set @xmath136 , choose @xmath137    [ [ optimization - over - m ] ] optimization over @xmath60 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the optimization over @xmath60 has a closed form solution .",
    "let @xmath138 , and @xmath139 denote the @xmath140 row of @xmath141 , then each row of @xmath60 is given as @xmath142 _",
    "+   \\frac{{{\\bf   \\delta}_{(j ) } } } { { ||{\\bf   \\delta}_{(j ) } ||}}$ ] , in which @xmath143_+=max\\{z,0\\}$ ] .",
    "+   + convergence analysis of admm can be found in @xcite .",
    "in particular , it has been shown that admm converges linearly for strongly convex objective functions @xcite . although the convergence is based on strong convexity assumptions , admm has been widely applied in practice @xcite , even to nonconvex problems @xcite .",
    "the proposed jointly sparse global simpls regression is one example of these nonconvex applications .",
    "we implement the simulation models in @xcite .",
    "there are four models all following @xmath144 , in which the number of observation is @xmath145 , and the dimension is @xmath146 .",
    "full details of the models are given in table [ table : exp_model ] .",
    "we compare five different methods : the standard pls regression ( denoted as pls in the following comparison tables ) , pls generalized linear regression proposed by bastien et al .",
    "@xcite , @xmath0 penalized pls regression @xcite ( denoted as @xmath0 spls ) , lasso @xcite and the jointly sparse global simpls regression ( denoted as @xmath147 spls ) .",
    "all the methods select their parameters by ten fold cross - validation , except for the pls generalized linear regression , which stops including an additional component if the new component is not significant .",
    "the parameter @xmath81 in the jointly sparse global simpls - r is fixed to 2000 , and updated in each iteration by a scaling factor @xmath148 .",
    "the experiments on real data in section [ sec : application1 ] and [ sec : application2 ] are using the same setting of @xmath81 .",
    "two i.i.d sets are generated for each trial : one as the training set and one as the test set .",
    "ten trials are conducted for each model , and the averaged results are listed in table [ table : simulation1 ] .    in most of the simulations , we observe that the proposed jointly sparse global simpls - r performs as good or better than other methods in terms of the prediction mse . in particular , the number of variables and the number of components chosen in jointly sparse global simpls - r are usually less than the @xmath0 penalized pls - r .",
    "we also calculate the @xmath149 for each method on the training data to measure the variation explained . the standard pls regression and the pls generalized linear regression proposed by bastien et al .",
    "@xcite both have @xmath149 close to @xmath150 , but the performance in terms of mse is not ideal in the first three models for these methods .",
    "this may suggest that these models overfit the data .",
    "in addition to the averaged performance , the p - values of one sided paired t - test suggest that jointly sparse global simpls - r reduces model complexity significantly from those in the standard pls regression and the pls generalized linear regression .",
    "lasso achieves low complexity in terms of the number of variables , but the mse is high compared to jointly sparse global simpls - r and the @xmath0 penalized pls - r .",
    "the cross validation time for jointly sparse global simpls regression is long , searching over a two - dimensional grid of the number of components @xmath2 and the regularization parameter @xmath84 to minimize mse .",
    "however , the performance in terms of prediction mse improves , and the model complexity in terms of the number of variables and the number of components both decreases compared with other methods in most simulations .",
    "* 1c   + * model 1 * + @xmath151 +   +   + * model 2 * + @xmath152 +   +   + * model 3 * + @xmath153 +   +   + * model 4 * + @xmath154 +   +     * 10c + & & & & & & +   + & 1 .",
    "bastien & 3 .",
    "@xmath0 spls & 4 .",
    "lasso & 5 .",
    "@xmath147 spls & ( 5,1 ) & ( 5,2 ) & ( 5,3 ) & ( 5,4 ) +   +   + * model 1 * + number of comp .",
    "& 1.4 & 5 & 1.9 & na & 1.4 & @xmath155 & @xmath156 & @xmath157 & na + number of variables & 5000 & 1129.4 & 246.5 & 40.7 & 276.1 & @xmath158 & @xmath159 & @xmath160 & @xmath161 + mse & 3.14 & 2.98 & 3.00 & 3.23 & 2.82 & @xmath162 & @xmath163 & @xmath164 & @xmath165 + @xmath149 & 0.98 & 1 & 0.71 & 0.59 & 0.83 + time cv & 101.47 & 0 & 43.49 & 51.59 & 11414 + time analysis & 0.89 & 121.91 & 0.05 & 0.04 & 6.40 + time prediction & 0.010 & 0.011 & 0.002 & 0.04 & 0.002 + total time & 102.37 & 121.92 & 43.54 & 51.67 & 11420 +   +   + * model 2 * + number of comp .",
    "& 2 & 5 & 2.3 & na & 1.1 & @xmath166 & @xmath167 & @xmath168 & na + number of variables & 5000 & 1158.4 & 273.4 & 15.8 & 171.7 & @xmath169 & @xmath170 & @xmath171 & @xmath172 + mse & 3.18 & 2.99 & 2.93 & 3.09 & 2.69 & @xmath173 & @xmath174 & @xmath175 & @xmath176 + @xmath149 & 0.98 & 1 & 0.79 & 0.39 & 0.75 + time cv & 100.51 & 0 & 43.03 & 53.97 & 11420 + time analysis & 1.28 & 122.69 & 0.06 & 0.04 & 5.72 + time prediction & 0.010 & 0.011 & 0.002 & 0.039 & 0.001 + total time & 101.80 & 122.70 & 43.09 & 54.05 & 11426 +   +   + * model 3 * + number of comp . & 1.4 & 5 & 1.4 & na & 1.5 & @xmath177 & @xmath178 & @xmath179 & na + number of variables & 5000 & 1156.4 & 89.2 & 41.3 & 60.5 & @xmath180 & @xmath181 & @xmath182 & @xmath183 + mse & 1.82 & 1.48 & 1.27 & 1.48 & 1.25 & @xmath184 & @xmath185 & @xmath186 & @xmath187 + @xmath149 & 0.98 & 1 & 0.77 & 0.75 & 0.73 + time cv & 102.61 & 0 & 43.84 & 49.45 & 11295 + time analysis & 1.03 & 126.08 & 0.04 & 0.04 & 5.48 + time prediction & 0.01 & 0.01 & 0.001 & 0.039 & 0.001 + total time & 103.65 & 126.09 & 43.88 & 49.53 & 11300 +   +   + * model 4 * + number of comp . & 2 & 5 & 2.6 & na & 2.1 & @xmath188 & @xmath189 & @xmath190 & na + number of variables & 5000 & 1118.8 & 1260.8 & 9.4 & 1180.5 & @xmath191 & @xmath192 & @xmath193 & @xmath194 + mse & 2.15 & 2.29 & 2.41 & 2.14 & 2.36 & @xmath195 & @xmath196 & @xmath197 & @xmath198 + @xmath149 & 1 & 1 & 0.78 & 0.19 & 0.91 + time cv & 98.16 & 0 & 44.31 & 50.52 & 12051 + time analysis & 1.55 & 123.79 & 0.10 & 0.04 & 7.97 + time prediction & 0.010 & 0.011 & 0.007 & 0.042 & 0.004 + total time & 99.73 & 123.8 & 44.41 & 50.60 & 12059 +   +",
    "in this section we show experimental results obtained by comparing standard pls - r , @xmath0 penalized pls - r @xcite ( denoted as @xmath0 spls in the performance table ) , and our proposed jointly sparse global simpls - r ( denoted as @xmath147 spls in the performance table ) .",
    "all the methods have been applied on the octane data set ( see @xcite ) .",
    "the octane data is a real data set consisting of 39 gasoline samples for which the digitized octane spectra have been recorded at 225 wavelengths ( in nm ) .",
    "the aim is to predict the octane number , a key measurement of the physical properties of gasoline , using the spectra as predictors .",
    "this is of major interest in real applications , because the conventional procedure to calculate the octane number is time consuming and involves expensive and maintenance - intensive equipment as well as skilled labor .",
    "the experiments are composed of 150 trials . in each trial",
    "we randomly split the 39 samples into 26 training samples and 13 test samples .",
    "the regularization parameter @xmath84 and number of components @xmath2 are selected by 2-fold cross validation on the training set .",
    "the averaged results over the 150 trials are shown in table [ exp ] .",
    "we further show the variable selection frequencies for the sparse pls methods over the 150 trials superimposed on the octane data in fig . [",
    "fig : select_freq_pls ] ( b ) and ( c ) . in chemometrics , the rule of thumb is to look for variables that have large amplitudes in first derivatives with respect to wavelength .",
    "notice that both @xmath0 penalized pls - r and jointly sparse global simpls - r have selected variables around 1200 and 1350 nm , and the selected region in the latter case is more confined .",
    "box and whisker plots for comparing the mse , number of selected variables , and number of components of these three pls formulations are shown in fig .",
    "[ fig : select_freq_pls ] ( a ) .",
    "comparing our proposed jointly sparse global simpls regression with standard pls - r and @xmath0 penalized pls - r @xcite , we show that jointly sparse global simpls - r attains better performance in terms of mse , the number of predictors , and the number of components . besides , the model complexity in jointly sparse global simpls - r is significantly lower than both standard pls - r and @xmath0 penalized pls - r , given the p - values of one sided paired t - test .",
    "* 6c   + & & & & +   + & 1 .",
    "@xmath0 spls & 3 .",
    "@xmath147 spls & ( 3,1 ) & ( 3,2 ) +   +    number of comp . & 5.5 & 4.5 & 3.8 & @xmath199 & 0.0027 + number of var . & 225 & 87.3 & 38.5 & @xmath200 & @xmath201 + mse & 0.0564 & 0.0509 & 0.0481 & 0.0032 & 0.1575 +     +   +   +",
    "in this section we apply the jointly sparse global simpls regression to 4 types of predictive health challenge studies involving the h3n2 , the h1n1 , the hrv , and the rsv viruses . in these challenge studies , publicly available from the ncbi - geo website , serial peripheral blood samples were acquired from a population of subjects inoculated with live flu viruses @xcite .",
    "the prediction task in these experiments is to predict the symptom scores based on gene expression of 12023 genes .",
    "there were 10 symptom scores , i.e. , runny nose , stuffy nose , sneezing , sore throat , earache , malaise , cough , shortness of breath , headache , and myalgia , documented over time .",
    "the symptoms are self - reported scores , ranging from 0 to 3 .",
    "we linearly interpolate the gene expressions to match them with the sampling time of the symptom reports .",
    "we compare the jointly sparse global simpls - r with standard pls - r and @xmath0 penalized pls - r by leaving one subject out as the test set , and the rest as the training set .",
    "the process is repeated until all subjects have been treated as the test set .",
    "the number of components for all methods and the regularization parameter in jointly sparse global simpls - r are selected by 2-fold cross validation to minimize the sum of the mse of the responses .",
    "since each subject has multiple samples , we perform the cross validation by splitting by subjects , i.e. , no samples from the same subject will appear in both training and tuning sets .",
    "we restrict the responses to the first 3 symptoms , which are the upper respiratory symptoms , and the results are shown in table [ table : plssymptom3detail ] . in",
    "most of the cases , the proposed jointly sparse global simpls - r method outperforms the standard pls - r and @xmath0 penalized pls - r in terms of prediction mse , number of components , number of genes .",
    "as can be seen in table [ table : plssymptom3detail ] , the number of selected variables decreases significantly by applying the @xmath147 mixed norm sparsity penalty to the pls - r objective function .",
    "thus the proposed pls - r method is able to construct a more parsimonious predictor relative to the other pls - r methods having similar accuracy .",
    "the pls - r method can also be viewed as an exploratory data analysis tool for constructing low dimensional descriptors of the independent variables and response variables .",
    "specifically , the general underlying matrix factorization model @xmath10 and @xmath11 , with latent component @xmath26 , provides a factor analysis model for the independent and response variables @xmath19 and @xmath9 .",
    "@xmath202 , @xmath62 and @xmath203 can be interpreted in a similar manner as the singular vectors of pca .",
    "however , different from pca that does not account for the response variables , @xmath202 , @xmath62 and @xmath203 contain information about both the independent variables and the response variables .",
    "the factor analysis interpretation of the underlying pls model is that @xmath202 is a latent score matrix and @xmath62 , @xmath203 are latent factor loading matrices that associate @xmath202 with the independent variables and the response variables via the approximate matrix factorizations @xmath204 and @xmath205 , respectively .",
    "the correlations between the latent component @xmath202 and the sum of the 3 upper respiratory symptoms are reported in table [ table : mf ] , which also shows results for classic matrix factorization methods including non - negative matrix factorization ( nmf ) @xcite and bayesian linear unmixing ( blu ) @xcite , previously applied to this dataset , for comparison .",
    "notice the sparse pls - r methods achieve higher correlation , as expected .",
    "remarkably , the proposed jointly sparse global simpls - r achieves this higher degree of correlation with many fewer components and variables than the nmf and blu methods .",
    "this experiment demonstrates that jointly sparse global simpls - r can be used as a factor analysis method to find the hidden molecular factors that best relate to the response .",
    "* 6c   +   +   + & & & & +   + & 1 .",
    "@xmath0 spls & 3 .",
    "@xmath147 spls & ( 3,1 ) & ( 3,2 ) +   +   + * h1n1 * + number of comp .",
    "& 2.8 & 2.3 & 2.4 & 0.1163 & 0.3322 + number of genes & 12023 & 3624.1 & 3575.8 & @xmath206 & 0.4842",
    "+ overall mse & 0.599 & 0.603 & 0.591 & 0.2890 & 0.1094 + runny nose mse & 0.167 & 0.165 & 0.167 + stuffy nose mse & 0.281 & 0.282 & 0.269 + sneezing mse & 0.151 & 0.157 & 0.155 +   + * h3n2 * + number of comp . & 3.2 & 2.5 & 1.9 & 0.0030 & 0.0863 + number of genes & 12023 & 3944.5 & 1721.5 & @xmath207 & 0.0601 + overall mse & 0.623 & 0.622 & 0.609 & 0.3073 & 0.2530 + runny nose mse & 0.186 & 0.174 & 0.173 + stuffy nose mse & 0.277 & 0.284 & 0.272 + sneezing mse & 0.160 & 0.164 & 0.165 +   + * hrv * + number of comp .",
    "& 2.8 & 2.3 & 2.2 & 0.0484 & 0.3773 + number of genes & 12023 & 2193.2 & 1779.1 & @xmath208 & 0.3522 + overall mse & 0.628 & 0.607 & 0.603 & 0.2020 & 0.4490 + runny nose mse & 0.243 & 0.226 & 0.232 + stuffy nose mse & 0.324 & 0.323 & 0.314 + sneezing mse & 0.062 & 0.058 & 0.057 +   + * rsv * + number of comp . &",
    "3.2 & 2.3 & 2.4 & 0.0198 & 0.4103 + number of genes & 12023 & 2445.4 & 3889.8 & @xmath209 & 0.1472 + overall mse & 0.866 & 0.920 & 0.855 & 0.3318 & 0.0567 + runny nose mse & 0.312 & 0.327 & 0.312 + stuffy nose mse & 0.412 & 0.448 & 0.397 + sneezing mse & 0.143 & 0.145 & 0.145 +     * 11c & + factor & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 +   + * h1n1 * + pls & 0.47 & 0.38 & 0.38 + @xmath0 spls & 0.52 & 0.37 + @xmath147 spls & 0.57 & 0.35 + nmf & 0.32 & 0.45 & 0.04 + blu & 0.27 & 0.19 & 0.02 & 0.51 & 0.16 & 0.06 +   + * h3n2 * + pls & 0.67 & 0.42 & 0.33 + @xmath0 spls & 0.73 & 0.33 & 0.33 + @xmath210 spls & 0.71 & 0.33 + nmf & 0.62 & 0.70 & 0.10 + blu & 0.54 & 0.73 & 0.26 & 0.33 & 0.01 & 0.02 & 0.28 & 0.05 & 0.00 +   + * hrv * + pls & 0.45 & 0.43 & 0.35 + @xmath0 spls & 0.52 & 0.38 + @xmath147 spls & 0.53 & 0.42 + nmf & 0.02 & 0.22 & 0.19 + blu & 0.11 & 0.04 & 0.18 & 0.33 & 0.01 & 0.02 & 0.28 & 0.05 & 0.00 +   + * rsv * + pls & 0.66 & 0.35 & 0.35 + @xmath0 spls & 0.70 & 0.34 + @xmath147 spls & 0.69 & 0.39 + nmf & 0.41 & 0.13 & 0.16 & 0.01 & 0.31 & 0.02 & 0.11 & 0.11 & 0.01 + blu & 0.01 & 0.02 & 0.23 & 0.20 & 0.03 & 0.68 & 0.12 +    c +",
    "the formulation of the global simpls objective function with an added group sparsity penalty greatly reduces the number of variables used to predict the response .",
    "this suggests that when multiple components are desired , the variable selection technique should take into account the sparsity structure for the same variables among all the components .",
    "our proposed jointly sparse global simpls regression algorithm is able to achieve as good or better performance with fewer predictor variables and fewer components as compared to competing methods .",
    "it is thus useful for performing dimension reduction and variable selection simultaneously in applications with large dimensional data but comparatively few samples ( @xmath211 ) .",
    "the jointly sparse global simpls regression objective function is minimized using augmented lagrangian techniques and , in particular , the admm algorithm .",
    "the admm algorithm splits the optimization into an eigen - decomposition problem and a soft - thresholding that enforces sparsity constraints .",
    "the general framework is extendable to more complicated regularization and can thus be tailored for other pls - type applications , e.g. , positivity constraints or smoothness penalties .",
    "for example , in the chemometric application , the data is smooth over the wavelengths and we can apply wavelet shrinkage on the data or include a total variation regularization to encourage smoothness .",
    "the sparsity constraints can be imposed on the wavelet coefficients if wavelet shrinkage is applied , or together with total variation regularization .",
    "the equivalence of soft wavelet shrinkage and total variation regularization was discussed in @xcite .",
    "one can also consider imposing sparsity structures on the weights corresponding to the same components , adding @xmath0 penalty within the groups , or total variation regularization , depending on the applications .",
    "the decoupling property of the admm algorithm allows one to extend the jointly sparse global simpls regression to these various regularizations .",
    "wold , s. , martens , h. , and wold , h. ( 1983 ) .",
    "the multivariate calibration problem in chemistry solved by the pls method .",
    "_ proceedings of the conference on matrix pencils .",
    "lectures notes in mathematics _ , 286 - 293 .",
    "sjstrm , m. , and wold , s. , and lindberg , w. , and persson , j. , and martens , h. ( 1983 ) .",
    "a multivariate calibration problem in analytical chemistry solved by partial least - squares models in latent variables .",
    "_ analytica chimica acta _ * 150 * , 61 - 70 .",
    "chun , h. , ballard , d. h. , cho , j. , and zhao , h. ( 2011 ) .",
    "identification of association between disease and multiple markers via sparse partial least squares regression .",
    "_ genetic epidemiology _ , * 35*(6 ) , 479 - 486 .",
    "wold , s. , ruhe , a. , wold , h. , and dunn iii , w. j. ( 1984 ) .",
    "the collinearity problem in linear regression . the partial least squares ( pls ) approach to generalized inverses .",
    "_ siam journal on scientific and statistical computing _ , * 5*(3 ) , 735 - 743 .",
    "chun , h. , and kele , s. ( 2010 ) .",
    "sparse partial least squares regression for simultaneous dimension reduction and variable selection .",
    "_ journal of the royal statistical society : series b ( statistical methodology ) _ , * 72 * , 3 - 25 .",
    "allen , g. i. , peterson , c. , vannucci , m. , and maleti - savati , m. ( 2013 ) .",
    "regularized partial least squares with an application to nmr spectroscopy .",
    "_ statistical analysis and data mining _ , * 6*(4 ) , 302 - 314 .",
    "beck , a. , ben - tal , a. , and teboulle , m. ( 2006 ) . finding a global optimal solution for a quadratically constrained fractional quadratic problem with applications to the regularized total least squares .",
    "_ siam journal on matrix analysis and applications _ , * 28*(2 ) , 425 - 445 .",
    "afonso , m. v. , bioucas - dias , j. m. , and figueiredo , m. a. ( 2011 ) . an augmented lagrangian approach to the constrained optimization formulation of imaging inverse problems .",
    "_ image processing , ieee transactions on _ , * 20*(3 ) , 681 - 695 .",
    "boyd , s. , parikh , n. , chu , e. , peleato , b. , and eckstein , j. ( 2011 ) . distributed optimization and statistical learning via the alternating direction method of multipliers . _ foundations and trends in machine learning _ , * 3*(1 ) , 1 - 122 .",
    "masoum , s. , bouveresse , d. j. r. , vercauteren , j. , jalali - heravi , m. , and rutledge , d. n. ( 2006 ) .",
    "discrimination of wines based on 2d nmr spectra using learning vector quantization neural networks and partial least squares discriminant analysis .",
    "_ analytica chimica acta _",
    ", * 558*(1 ) , 144 - 149 .",
    "zaas , a. k. , chen , m. , varkey , j. , veldman , t. , hero , a. o. , lucas , j. , huang , y. , turner , r. , gilbert , a. , lambkin - williams , r. , ien , n. c. , nicholson , b. , kingsmore , s. , carin , l. , woods , c. w. , and ginsburg , g. s. ( 2009 ) .",
    "gene expression signatures diagnose influenza and other symptomatic respiratory viral infections in humans .",
    "_ cell host and microbe _ , * 6*(3 ) , 207 - 217 .",
    "huang , y. , zaas , a. k. , rao , a. , dobigeon , n. , woolf , p. j. , veldman , t. , ien , n. c. , mcclain , m. t. , varkey , j. b. , nicholson , b. , carin , l. , kingsmore , s. , woods , c. w. , ginsburg , g. s. , hero , a. o. ( 2011 ) .",
    "temporal dynamics of host molecular responses differentiate symptomatic and asymptomatic influenza a infection . _",
    "plos genetics _ ,",
    "* 7*(8 ) , e1002234 .",
    "woods . , c. w. , mcclain , m. t. , chen , m. , zaas , a. k. , nicholson , b. p. , varkey , j. , veldman , t. , kingsmore , s. f. , huang , y. , lambkin - williams , r. , gilbert , a. g. , hero , a. o. , ramsburg , e. , glickman , s. , lucas1 , j. e. , carin , l. , and ginsburg , g. s. ( 2013 ) . a host transcriptional signature for presymptomatic detection of infection in humans exposed to influenza h1n1 or h3n2 .",
    "_ plos one _ 8.1 : e52198 .",
    "zaas , a. k. , burke , t. , chen , m. , mcclain , m. , nicholson , b. , veldman , t. , tsalik , e. l. , fowler , v. , rivers , e. p. , otero , r. , kingsmore , s. f. , voora , d. , lucas , j. , hero , a. o. , carin , l. , woods , c. w. , and ginsburg , g. s. ( 2013 ) .",
    "a host - based rt - pcr gene expression signature to identify acute respiratory viral infection .",
    "_ science translational medicine _ ,",
    "* 5 * , 203ra126 .",
    "dobigeon , n. , moussaoui , s. , coulon , m. , tourneret , j. y. , and hero , a. o. ( 2009 ) .",
    "joint bayesian endmember extraction and linear unmixing for hyperspectral imagery . _ signal processing , ieee transactions on _ , * 57*(11 ) , 4355 - 4368 .",
    "steidl , g. , weickert , j. , brox , t. , mrzek , p. , and welk , m. ( 2004 ) . on the equivalence of soft wavelet shrinkage , total variation diffusion , total variation regularization , and sides .",
    "_ siam journal on numerical analysis _ , * 42*(2 ) , 686 - 713 ."
  ],
  "abstract_text": [
    "<S> partial least squares ( pls ) regression combines dimensionality reduction and prediction using a latent variable model . since partial least squares regression ( pls - r ) does not require matrix inversion or diagonalization , it can be applied to problems with large numbers of variables . as predictor dimension increases , </S>",
    "<S> variable selection becomes essential to avoid over - fitting , to provide more accurate predictors and to yield more interpretable parameters . </S>",
    "<S> we propose a global variable selection approach that penalizes the total number of variables across all pls components . </S>",
    "<S> put another way , the proposed global penalty encourages the selected variables to be shared among the pls components . </S>",
    "<S> we formulate pls - r with joint sparsity as a variational optimization problem with objective function equal to a novel global simpls criterion plus a mixed norm sparsity penalty on the weight matrix . </S>",
    "<S> the mixed norm sparsity penalty is the @xmath0 norm of the @xmath1 norm on the weights corresponding to the same variable used over all the pls components . </S>",
    "<S> a novel augmented lagrangian method is proposed to solve the optimization problem and soft thresholding for sparsity occurs naturally as part of the iterative solution . </S>",
    "<S> experiments show that the modified pls - r attains better or as good performance with many fewer selected predictor variables . </S>"
  ]
}