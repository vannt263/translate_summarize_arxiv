{
  "article_text": [
    "in the field of decsion and inference making hypothesis testing plays a crucial role .",
    "a parameter can be estimated looking at the obtained samples , either by a single number ( point estimation ) or by an entire interval of plausible values ( confidence interval ) . however , in most cases the object is not to estimate a parameter but to decide which of the contradictory claims about the parameter is correct or more likely .",
    "the theory of decision making from a set of contradictory claims is known as hypothesis testing .      in case of simple hypothesis testing problem",
    ", there are two contradictory hypothesis .",
    "the established one or the null hypothesis and one that contradicts the established theory , the alternative hypothesis .",
    "the first is to formulate a test statistics which is a function of random samples .",
    "now , given the sample instant one gets an outcome of the test statistic .",
    "the distribution of the test statistic under null hypothesis is known and is based on the assumptioins made in the probabilistic model .",
    "based on the outcome of the test statistic a desicion is made under a pre - decided decision rule . + in decision making two types of error might arise-    * type - i error- + wrongly rejecting the null hypothesis is termed as the type - i error(false positive ) .",
    "i.e. @xmath0 * type - ii error- + wrongly accepting null hypothesis is termed as tpe - ii error or false negative .",
    "i.e. @xmath1    now one would like to keep both these error as small as possible",
    ". however , unfortunately these errors are inversely related to each other i.e. if one is decreased the other is increased .",
    "so there has to be a trade off between these two errors . here for obvious reasons type - i errors are delicately handeled as they are more important and should be taken care of .",
    "so , the basic idea is to fix a significance level i.e. the tolerance level of type - i error and minimize the type - ii error .",
    "the probability of type - i error called the significance level or level of test denoted by @xmath2 .",
    "however , in applications this level of @xmath3 is somewhat arbitrary and .",
    "thus , in practice one might have different conclusions for different values of @xmath3 .",
    "thus , the concept of p - value or attained significance comes into play .",
    "it is defined as the probability of the given data under the null hypothesis . in simpler words",
    ", it signifies if the null hypothesis was true then how likely we would observe the sample data . in other words",
    "it is the smallest level attained significance level , i.e. the smallest level of significance for which the null hypothesis would be rejected .",
    "the smaller the p - value is the stronger the evidence behind rejecting the null hypothesis .",
    "an alternative approach to this p - value would be come up with a rejection region @xmath4based on the pre - defined significance level and if the outcome of the test statistic @xmath5 then we reject the null hypothesis else we keep the null hypothesis .",
    "now , if one chooses high threshold then it is more likely to attain higher power but prone to false positive there has to be a trade - off between high power and false negative .",
    "multiple hypothesis testing is quite common in practice and often encountered .",
    "an example would be : suppose one has several forecasting strategies to compare with the benchmark and wishes to identify which works better than the existing benchmark .    in order to test several hypothesis simultaneously the first idea would be to test each hypothesis separately w.r.t some significance level @xmath3 . however keeping in mind the multiplicity",
    "this might not be a good idea to pursue .",
    "for example , if we have a significance level 0.05 one would expect 5 type - i errors in 100 samples even if all the test are not significant .",
    "now , if we have 20 hypothesis to test then it turns out to be @xmath6 considering the hypothesis to be independent with each other and this is quite unacceptable that even if all the hypothesis are insignificant still the probability of making a type - i error is 64 out of 100 and as a matter of fact mostly in practical cases the number of hypothesis to be tested is in general much larger than 20 and unfortunately due to the chance this error rate keeps on going up .",
    "the idea is to come up with an adjustment of @xmath3 so that the error rate remains below the desired significance level . to deal with this multiplicity problem classical multiple comparison",
    "invoke the familywise error rate(fwer ) defined as the probability of one or more false alarms .",
    "fwer = pr(@xmath7pr(@xmath8when @xmath9is true ) + bonferroni inewuality or the union bound suggests    pr(@xmath10)@xmath11 @xmath12 + thus , as a natural extension of this well known union bound it can be concluded that if each hypothesis is tested at the level @xmath3 then fwer@xmath13 thus to get around this problem and ensuring fwer @xmath14 for the family of m tests the bonferroni correction suggests to set the significance level to @xmath15 thus the threshold is now selected as @xmath16 instead of @xmath17 and thus fwer@xmath14 as can be seen that this bonferroni correction scheme is qquite conservative . here though we benifit from the independent assumptions but in practice",
    "this is hardly the case and keeping in mind the correlation structure of the tests , it can be concluded that this bonferroni correction could be extremely conservative as it leads to a high number of false negativeness .",
    "the per comparison error rate ( pcer ) approach that almost ignores the multiplicity problem alltogether is defined as pcer = e(@xmath18 ) ; where v is the number of type - i error s made or the number of false alarms and m is the total number of hypothesis being tested.testing each hypothesis at the significance level @xmath3 ensures pcer@xmath14      the authors pointed out some of the practical disadvantages of fwer and pcer and the necessity of an alternative and more powerful procedure as follows-    * most of the methods of fwer controlling mcps assume multiple - treatment type problem and thus concerns multivariate normal distribution which in practice is not always the case .",
    "* fwer tends to have lower power than the pcer at levels conventional to single - comparison problems . *",
    "fwer as can be seen from the definition might reject when atleast one is significant .",
    "this is likely when several methods are compared with a baseline and the best will be selected .",
    "however , in many cases viz . in various treatment group / control group various aspects of the treatment",
    "are compared and it ca nt be concluded that the existing treatment is errornous even if some of the null hypothesis are rejected .",
    "thus , in these cases fwer based error control strategies are not suitable .",
    "thus the authors have proposed a alternative approach to deal with the mcp frameworks .",
    "for that a new type of error measure has been invoked and termed as `` false discovery ratio(fdr)''.suppose m be total number of hypothesis being tested,@xmath19be the number of true null hypothesis . out of these @xmath19true null hypothesis u are declared non - significant and v are declared significant ( false alarm ) . and out of the @xmath20 alternative hypothesis s",
    "are rightly identified as significant and t are misclassified as non - significant(type - ii error / flase negative ) .",
    "however , u , s , v , t are unobservable random variables and the statistician observes only total number of significant outcomes r = v+s and total declared insignificant m - r = u+t .    the authors define fdr as the proportion of errors committed by falsely rejecting null hypothesis which can be transletted into the term q=@xmath21 .",
    "q=0 when v+s=0 due to the fact in this case no error can be committed",
    ". however , q is an unobserved quantity as v , s are both unobservable .",
    "thus , fdr is defined as-    fdr=@xmath22    two important aspects are pointed out here -    * when all the null hypothesis are true then s=0 so , fdr = e(q)=pr(v@xmath231 ) + which is exactly equal to fwer thus , in it has been claimed that controlling fdr automatically controls fwer .",
    "* when all hypothesis are not null ( more common case ) then fdr@xmath11fwer as v@xmath11r thus , pr(v@xmath24 thus it has been shown that the fdr is often more powerful than fwer .",
    "the algorithm proposed by the authors in support of their claims can be in short broken in three step concisely to control fdr at level @xmath3    * test the hypothesis @xmath25 based on the corresponding p - values @xmath26 * order the p - values @xmath27 * rank the hypothesis according to their corresponding p - values . *",
    "find the test with highest rank j for which the p value @xmath28 * declare the top j tests 1,2, .... j with @xmath28 as significant .",
    "the authors have reported this as a theorem and provided a comprehensive proof to claim that this method will always lead to controlled fdr at @xmath3    the paper uses an example of treatment procedures pertaining to cardiac patients . with the help of a few example",
    "it shows that in case of inference problem where the comparison is not between different methods rather the comparison is between different features the bh fdr method finds out significant results when the fwer based method fail to do so .",
    "the author s also put forward a different way to formulate their proposed algorithm .",
    "it has been claimed that the proposed algorithm can be formulated as a constraint optimization problem .",
    "it has been suggested that it is same as to choose @xmath29 that maximizes the number of rejections , @xmath30 at the level subject to the constraint @xmath31 the author s also claim the method is more powerful as compared to classical fwer based methods with a set of experiments where both the algorithms were tested and the proposed algorithm resulted in much better results .",
    "this paper by benjamini and hochberg et.al . seems to open a new dimension in the study of multiple comparison test . in order to undestand the effect or the real contribution of the paper in the cotext of multiple comparison test we should think about the preceding works and only then one would be able to evaluate the novelty of the paper .",
    "when one conducts a multiple comparison test with a fixed level of significance @xmath3 for all the experiments and carry out the comparison it is pretty straightforward to notice that the probability of comminting a type - i error hence the probability of getting significant result goes up just by chance due to the law of probability . as such if there are n experiments being compared then the + pr(atleast one type",
    "- i error)=1-@xmath32 + which is not desired .",
    "thus there needed to be a modificatio in @xmath3 to control this fwer .",
    "thus , looking at this the first thing that comes to the the mind is to reduce the @xmath3 but in order to do so it would give rise to a reduction in power .",
    "thus , pretty much as one would try straightaway is to reduce the fwer without much loss in power thus , a trade off between them is desired .",
    "sidak et.al . came up with the first form of correction known as dunn - sidak procedure [ 2 ] .",
    "they proposed a modified value of the tolerance level . according to there proposal if one chooses the significance level to be @xmath33 for each experiment that will ensure the fwer to be controlled at @xmath34 .",
    "later bonferroni came up with a correction which was just the first order approximation of the infinite sequence of proposed dann - sidak method and the popular bonferroni correction turns out to be using @xmath35 for each of the comparison tolerance level in order to achieve fwer controlled at tolerance level @xmath34 .",
    "however , both these boferroni and sidak procedures seem to be very conservative thus less powerful .",
    "the next significant work was done by holm s. et .",
    "they proposed to rank the p values in ascending order and test the i th p - value @xmath36 against the level @xmath37 to keep th fwer controlled at @xmath3 .",
    "this method turned out to be more powerful the existing bonferroni method .",
    "now , the authors of this paper have proposed a modification of the holm s method and also proposed a new controlling error rate called false discovery rate ( fdr) and arranges p - values in increasing order and tests i th p vaue @xmath36 with @xmath38 where m is the number of hypothesis being tested.then rejecting all @xmath39 @xmath40 such that @xmath41 and this will ensure that fdr is controlled at the level @xmath42 .",
    "it also proves that controlling fdr automatically controls the fwer .",
    "+ thus , judging from the point of novelty , it seems like an extension to the holm s method to control fwer and follows the similar procedure .",
    "however the introduction of the fdr as an error measurment criterion is very novel and significant as it automatically takes care of fwer control and also additionaly less conservative and thus leading to more powerful detection . + in section 4.2 the author s have reported relative study between the current method and the existing bonferroni and holm - hochberg method ( 1988 ) and they have reported that this proposed method leads to uniformly most powerful among them and its performance is significantly better than the other methods as the number of hypothesis tested are increased .",
    "however some technical and non - technical advice to improve the quality and readability of the paper-    * the quality of the diagrams should be improved for better readability ( 300 dpi ) . *",
    "it proves significant improvement as compared to the existing procedure however , it does nt mention if this is the best one can achieve . as in",
    ", it would be nice to look into if there is a bound on the power that can be achieved while keeping the fdr level controlled at the desired level .",
    "* more simulation study should be added to investigate how does the method performs at different fdr levels with respect to sparsity . * more rigorous analysis should have been made in order to observe if the trend really persists as the number of hypothesis increases to higher levels like m=500 or 1000",
    ". it would be nice to look into a long term power vs. number of hypothesis analysis and see how powerful the method remains at that high number of hypothesis .",
    "overall , the paper provides nice intuitive development of the multiple comparison scheme and goes beyond the preceding work .",
    "the idea of fdr introduced here looks very novel and thus opens up a new line of research in this field .",
    "further investigation and extension over this method should follow from this paper .",
    "yoav benjamini , yosef hochberg , controlling the false discover rate : a practical and powerful approach to multiple testing , journal of royal statistical society , series b(methodological ) , vol.57,no.1(1995),289 - 300 z sidak , rectangular confidence regions for the means of multivariate normal distributions , journal of the american statistical association , 1967 holm s.[1979],a simple sequencially rejective multiple test procedure .",
    "scand j. statist,6,65 - 70 ."
  ],
  "abstract_text": [
    "<S> the paper titled controlling the false discovery rate - a practical and powerful approach to multiple testing by benjamini et . </S>",
    "<S> al.[1 ] proposes a new framework of controlling the false discovery rate in a multiple hypothesis testing problem . </S>",
    "<S> it has been claimed that the procedure proposed in the paper results in a substantial gain in power more applicable in case of problems which call for false discovery rate ( fdr ) control rather than familywise error rate ( fwer ) . </S>",
    "<S> the proposed method uses a simple bonferroni type procedure for fdr control . </S>"
  ]
}