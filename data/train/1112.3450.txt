{
  "article_text": [
    "there has been much work on penalized methods for variable selection and estimation in high - dimensional regression models .",
    "several important methods have been proposed .",
    "examples include estimators based on the bridge penalty [ @xcite ] , the @xmath1 penalty or the least absolute shrinkage and selection operator [ lasso , @xcite , @xcite ] , the smoothly clipped absolute deviation ( scad ) penalty [ @xcite , @xcite ] and the minimum concave penalty [ mcp , @xcite ] .",
    "these methods are able to do estimation and automatic variable selection simultaneously and provide a  computationally feasible way for variable selection in high - dimensional settings .",
    "much progress has been made in understanding the theoretical properties of these methods .",
    "efficient algorithms have also been developed for implementing these methods .",
    "a common feature of the methods mentioned above is the independence between the penalty and the correlation among predictors .",
    "this can lead to unsatisfactory selection results , especially in @xmath0 settings .",
    "for example , as pointed out by @xcite , the lasso tends to only select one variable among a group of highly correlated variables ; and its prediction performance may not be as good as the ridge regression if there exists high correlation among predictors . to overcome these limitations , @xcite proposed the elastic net ( enet ) method , which uses a combination of the @xmath1 and @xmath2 penalties .",
    "selection properties of the enet and adaptive enet have also been studied by jia and yu ( @xcite ) and @xcite .",
    "@xcite proposed the oscar ( octagonal shrinkage and clustering algorithm for regression ) approach , which uses a  combination of the @xmath1 norm and a pairwise @xmath3 norm for the coefficients .",
    "huang et al . ( @xcite ) proposed the mnet method , which uses a combination of the mcp and @xmath2 penalties .",
    "the mnet estimator is equal to the oracle ridge estimator with high probability under certain conditions .",
    "these methods are effective in dealing with certain types of collinearity among predictors and has the useful grouping property of selecting and dropping highly correlated predictors together .",
    "still , these combination penalties do not use any specific information on the correlation pattern among the predictors .",
    "@xcite proposed a network - constrained regularization procedure for variable selection and estimation in linear regression models , where the predictors are genomic data measured on genetic networks . @xcite",
    "considered the general problem of regression analysis when predictors are measured on an undirected graph , which is assumed to be known a  priori .",
    "they called their method a graph - constrained estimation procedure or grace .",
    "the grace penalty is a combination of the @xmath1 penalty and a  penalty that is the laplacian quadratic associated with the graph . because the grace uses the @xmath1 penalty for selection and sparsity , it has the same drawbacks as the enet discussed above .",
    "in addition , the full knowledge of the graphical structure for the predictors is usually not available , especially in high - dimensional problems .",
    "@xcite proposed the weighted fusion method , which also uses a combination of the @xmath1 penalty and a quadratic form that can incorporate information among correlated variables for estimation and variable selection .",
    "@xcite studied a form of correlation based penalty , which can be considered a special case of the general quadratic penalty . but this approach does not do variable selection .",
    "the authors proposed a blockwise boosting procedure in combination with the correlation based penalty for variable selection .",
    "@xcite studied the theoretical properties of the smoothed - lasso and other @xmath4-penalized methods in @xmath0 models .",
    "@xcite studied a grouped penalty based on the @xmath5-norm for @xmath6 that smoothes the regression coefficients over a network . in particular , when @xmath7 and after appropriate rescaling of the regression coefficients , this group @xmath5 penalty simplifies to the group lasso [ yuan and lin ( @xcite ) ] with the nodes in the network as groups .",
    "this method is capable of group selection , but it does not do individual variable selection . also , because the group @xmath5 penalty is convex for @xmath6 , it does not lead to consistent variable selection , even at the group level .",
    "we propose a new penalized method for variable selection and estimation in sparse , high - dimensional settings that takes into account certain correlation patterns among predictors .",
    "we consider a combination of the mcp and laplacian quadratic as the penalty function .",
    "we call the proposed approach the sparse laplacian shrinkage ( sls ) method .",
    "the sls uses the mcp to promote sparsity and laplacian quadratic penalty to encourage smoothness among coefficients associated with the correlated predictors .",
    "an important advantage of the mcp over the @xmath8 penalty is that it leads to estimators that are nearly unbiased and achieve selection consistency under weaker conditions [ @xcite ] .",
    "the contributions of this paper are as follows .",
    "* first , unlike the existing methods that use an @xmath1 penalty for selection and a ridge penalty or a general @xmath2 penalty for dealing with correlated predictors , we use the mcp to achieve nearly unbiased selection and proposed a concrete class of quadratics , the laplacians , for incorporating correlation patterns among predictors in a local fashion .",
    "in particular , we suggest to employ the approaches for network analysis for specifying the laplacians .",
    "this provides an implementable strategy for incorporating correlation structures in high - dimensional data analysis . * second , we prove that the sls estimator is sign consistent and equal to the oracle laplacian shrinkage estimator under reasonable conditions .",
    "this result holds for a large class of laplacian quadratics .",
    "an important aspect of this result is that it allows the number of predictors to be larger than the sample size .",
    "in contrast , the works of @xcite and @xcite do not contain such results in @xmath0 models .",
    "the selection consistency result of @xcite requires certain strong assumptions on the magnitude of the smallest regression coefficient ( their assumption c ) and on the correlation between important and unimportant predictors ( their assumption d ) , in addition to a  variant of the restricted eigenvalue condition ( their assumption b ) . in comparison ,",
    "our assumption involving the magnitude of the regression coefficients is weaker and we use a sparse riese condition instead of imposing restriction on the correlations among predictors .",
    "in addition , our selection results are stronger in that the sls estimator is not only sign consistent , but also equal to the oracle laplacian shrinkage estimator with high probability . in general ,",
    "similar results are not available with the use of the @xmath1 penalty .",
    "* third , we show that the sls method is potentially capable of incorporating correlation structure in the analysis without incurring extra bias . the enet and",
    "the more general @xmath9 methods in general introduces extra bias due to the quadratic penalty , in addition to the bias resulting from the @xmath1 penalty . to the best of our knowledge",
    ", this point has not been discussed in the existing literature .",
    "we also demonstrate that the sls has certain local smoothing property with respect to the graphical structure of the predictors . * fourth , unlike in the grace method , the sls does not assume that the graphical structure for the predictors is known a priori .",
    "the sls uses the existing data to construct the graph laplacian or to augment partial knowledge of the graph structure .",
    "* fifth , our simulation studies demonstrate that the sls method outperforms the @xmath1 penalty plus a quadratic penalty approach as studied in @xcite and @xcite . in our simulation examples , the sls in general has smaller empirical false discovery rates with comparable false negative rates .",
    "it also has smaller prediction errors .",
    "this paper is organized as follows . in section [ sec2 ] ,",
    "we define the sls estimator . in section [ sec3 ]",
    "we discuss ways to construct graph laplacian , or equivalently , its corresponding adjacency matrix . in section [ sec4 ] , we study the selection properties of the sls estimators . in section [ sec5 ] , we investigate the properties of laplacian shrinkage . in section [ sec6 ] , we describe a coordinate descent algorithm for computing the sls estimators , present simulation results and an application of the sls method to a microarray gene expression dataset .",
    "discussions of the proposed method and results are given in section  [ sec7 ] .",
    "proofs for the oracle properties of the sls and other technical details are provided in the .",
    "consider the linear regression model @xmath10 with @xmath11 observations and @xmath12 potential predictors , where @xmath13 is the vector of @xmath11 response variables , @xmath14 is the @xmath15th predictor , @xmath16 is the @xmath15th regression coefficient and @xmath17 is the vector of random errors . let @xmath18 be the @xmath19 design matrix . throughout",
    ", we assume that the response and predictors are centered and the predictors are standardized so that @xmath20 . for @xmath21 with @xmath22 and @xmath23 ,",
    "we propose the penalized least squares criterion @xmath24\\\\[-8pt ] & & { } + \\frac{1}{2 } { \\lambda}_2 \\sum_{1\\le j < k \\le p } \\vert a_{jk}\\vert(b_j - s_{jk}b_k)^2,\\nonumber\\end{aligned}\\ ] ] where denotes the @xmath2 norm , @xmath25 is the mcp with penalty parameter @xmath26 and regularization parameter @xmath27 , @xmath28 measures the strength of the connection between @xmath29 and @xmath30 , and @xmath31 is the sign of @xmath32 , with @xmath33 or @xmath34 , respectively , for @xmath35 or @xmath36 .",
    "the two penalty terms in  ( [ slsdefa ] ) play different roles .",
    "the first term promotes sparsity in the estimated model .",
    "the second term encourages smoothness of the estimated coefficients of the connected predictors .",
    "we can associate the quadratic form in this term with the laplacian for a suitably defined undirected weighted graph for the predictors .",
    "see the description below . for any given @xmath37 ,",
    "the sls estimator  is @xmath38    the sls uses the mcp , defined as @xmath39 where for any @xmath40 , @xmath41 is the nonnegative part of @xmath42 , that is , @xmath43 .",
    "the mcp can be easily understood by considering its derivative , @xmath44 we observe that the mcp begins by applying the same level of penalization as the @xmath1 penalty , but continuously reduces that level to 0 for @xmath45 .",
    "the regularization parameter @xmath27 controls the degree of concavity .",
    "larger values of @xmath27 make @xmath25 less concave . by sliding the value of @xmath27 from 1 to @xmath46 ,",
    "the mcp provides a continuum of penalties with the hard - threshold penalty as @xmath47 and the convex @xmath8 penalty at @xmath48 .",
    "detailed discussion of mcp can be found in @xcite .",
    "the sls also allows the use of different penalties than the mcp for @xmath25 , including the scad [ @xcite , @xcite ] and other quadratic splines . because the mcp minimizes the maximum concavity measure and has the simplest form among nearly unbiased penalties in this family , we choose it as the default penalty for the sls . further discussion of the mcp and its comparison with the lasso and scad can be found in @xcite and @xcite .",
    "we express the nonnegative quadratic form in the second penalty term in  ( [ slsdefa ] ) using a positive semi - definite matrix @xmath49 , which satisfies @xmath50 for simplicity , we confine our discussion to the symmetric case where @xmath51 .",
    "since the diagonal elements @xmath52 do not appear in the quadratic form , we can define them any way we like for convenience .",
    "let @xmath53 and @xmath54 , where @xmath55 .",
    "we have @xmath56 therefore , @xmath57 .",
    "this matrix is associated with a labeled weighted graph @xmath58 with vertex set @xmath59 and edge set @xmath60 . here",
    "the @xmath28 is the weight of edge @xmath61 and @xmath62 is the degree of vertex @xmath15 . the @xmath62 is also called the connectivity of vertex @xmath15 .",
    "the matrix @xmath49 is called the laplacian of @xmath63 and @xmath64 its signed adjacency matrix [ @xcite ] .",
    "the edge @xmath61 is labeled with the `` @xmath65 '' or `` @xmath66 '' sign , but its weight @xmath28 is always nonnegative .",
    "we use a labeled graph to accommodate the case where two predictors can have a nonzero adjacency coefficient but are negatively correlated .",
    "note that the usual adjacency matrix can be considered a special case of signed adjacency matrix when all @xmath67 . for simplicity",
    ", we will use the term adjacency matrix below .",
    "we usually require that the adjacency matrix to be sparse in the sense that many of its entries are zero or nearly zero . with a sparse adjacency matrix ,",
    "the main characteristic of the shrinkage induced by the laplacian penalty is that it occurs locally for the coefficients associated with the predictors connected in the graph .",
    "intuitively , this can be seen by writing @xmath68 thus for @xmath69 , the laplacian penalty shrinks @xmath70 toward zero for @xmath71 .",
    "this can also be considered as a type of local smoothing on the graph  @xmath63 associated with the adjacency matrix @xmath64 . in comparison ,",
    "the shrinkage induced by the ridge penalty used in the enet is global in that it shrinks all the coefficients toward zero , regardless of the correlation structure among the predictors .",
    "we will discuss the laplacian shrinkage in more detail in section [ sec5 ] .    using the matrix notation , the sls criterion ( [ slsdefa ] )",
    "can be written as @xmath72 here the laplacian is not normalized , meaning that the weight @xmath62 is not standardized to 1 . in problems where",
    "predictors should be treated without preference with respect to connectivity , we can first normalized the laplacian @xmath73 with @xmath74 and use the criterion @xmath75 technically , a normalized laplacian @xmath76 can be considered a special case of a  general @xmath49 .",
    "we only consider the sls estimator based on the criterion  ( [ slsdefb ] ) when studying its properties . in network analysis of gene expression data , genes with large connectivity also tend to have important biological functions [ @xcite ] .",
    "therefore , it is prudent to provide more protection for such genes in the selection process .",
    "in this section , we describe several simple forms of adjacency measures proposed by @xcite , which have have been successfully used in network analysis of gene expression data . the adjacency measure is often defined based on the notion of dissimilarity or similarity .",
    "a basic and widely used dissimilarity measure is the euclidean distance .",
    "based on this distance , we can define adjacency coefficient as @xmath77 where @xmath78 .",
    "a simple adjacency function is the threshold function @xmath79 .",
    "then @xmath80    it is convenient to express @xmath32 in terms of the pearson s correlation coefficient @xmath81 between @xmath29 and @xmath30 , where @xmath82 . for predictors that are standardized with @xmath83 , we have @xmath84 thus in terms of correlation coefficients",
    ", we can write .",
    "we determine the value of @xmath85 based on the fisher transformation @xmath86 .",
    "if the correlation between  @xmath29 and  @xmath30 is zero , @xmath87 is approximately distributed as @xmath88 .",
    "we can use this to determine a threshold @xmath89 for @xmath87 . the corresponding threshold for @xmath81",
    "is @xmath90 .",
    "we note that here we use the fisher transformation to change the scale of the correlation coefficients from @xmath91 $ ] to the normal scale for determining the threshold value @xmath85 , so that the adjacency matrix is relatively sparse .",
    "we are not trying to test the significance of correlation coefficients .",
    "the adjacency coefficient in ( [ daa ] ) is defined based on a dissimilarity measure .",
    "adjacency coefficient can also be defined based on similarity measures .",
    "an often used similarity measure is pearson s correlation coefficient  @xmath81 .",
    "other correlation measures such as spearman s correlation can also be used .",
    "let @xmath92 here @xmath85 can be determined using the fisher transformation as above .    with the power adjacency function considered in @xcite ,",
    "@xmath93 here @xmath94 and can be determined by , for example , the scale - free topology criterion .",
    "a variation of the above power adjacency function is @xmath95    for the adjacency matrices given above , ( i ) and ( ii ) use dichotomized measures , whereas ( iii ) and ( iv ) use continuous measures . under ( i ) and  ( iii ) , two covariates are either positively or not connected / correlated .",
    "in contrast , under ( ii ) and ( iv ) , two covariates are allowed to be negatively connected / correlated .",
    "there are many other ways for constructing an adjacency matrix .",
    "for example , a popular adjacency measure in cluster analysis is @xmath96 for @xmath97 .",
    "the resulting adjacency matrix @xmath98 $ ] is the gram matrix associated with the gaussian kernel . for discrete covariates , the pearson correlation coefficient can still be used as a measure of correlation or association between two discrete predictors or between a discrete predictor and a continuous one .",
    "for example , for single nucleotide polymorphism data , pearson s correlation coefficient is often used as a measure of linkage disequilibrium ( i.e. , association ) between two markers .",
    "other measures , such as odds ratio or measure of association based on contingency table can also be used for @xmath81 .",
    "we note that how to construct the adjacency matrix is problem specific .",
    "different applications may require different adjacency matrices . since construction of adjacency matrix is not the focus of the present paper",
    ", we will only consider the use of the four adjacency matrices described above in our numerical studies in section [ sec6 ] .",
    "in this section , we study the theoretical properties of the sls estimator .",
    "let the true value of the regression coefficient be @xmath99 .",
    "denote @xmath100 , which is the set of indices of nonzero coefficients .",
    "let @xmath101 be the cardinality of @xmath102 .",
    "define @xmath103 this is the oracle laplacian shrinkage estimator on the set @xmath102 .",
    "theorems [ thma ] and [ thmb ] below provide sufficient conditions under which @xmath104 or .",
    "thus , under those conditions , the sls estimator is sign consistent and equal to @xmath105 with high probability .",
    "we need the following notation in stating our results .",
    "let @xmath106 .",
    "for any @xmath107 , vectors @xmath108 , the design matrix @xmath109 and @xmath110 , define @xmath111 for example , @xmath112 and @xmath113 .",
    "let @xmath114 denote the cardinality of @xmath115 .",
    "let @xmath116 be the smallest eigenvalue of @xmath117 .",
    "we use the following constants to bound the bias of the laplacian : @xmath118\\\\[-8pt ] c_2&=&\\|\\{\\sigma_{{\\mathcal{o}}^c,\\mathcal{o}}({\\lambda}_2 ) \\sigma_\\mathcal{o}^{-1}({\\lambda}_2)l_\\mathcal{o}-l_{{\\mathcal{o}}^c,\\mathcal { o}}\\}{\\bolds\\beta}^o_\\mathcal{o}\\|_\\infty.\\nonumber\\end{aligned}\\ ] ]    we make the following sub - gaussian assumption on the error terms in  ( [ rega ] ) .",
    "[ conda ] for a certain constant @xmath119 , @xmath120      we first consider the case where @xmath121 is positive definite .",
    "since ( [ ora ] ) is the minimizer of the laplacian restricted to the support @xmath122 , it can be explicitly written as @xmath123 provided that @xmath124 is invertible .",
    "its expectation @xmath125 , considered as a target of the sls estimator , must satisfy @xmath126    [ condb ] ( i ) @xmath127 with @xmath128 in ( [ slsdefa ] ) .",
    "\\(ii ) the penalty levels satisfy @xmath129 with @xmath130 in ( [ c1c2 ] ) .",
    "\\(iii ) with @xmath131 being the diagonal elements of @xmath132 , @xmath133    define @xmath134 .",
    "if @xmath102 is an empty set , that is , when all the regression coefficients are zero , we set @xmath135 .    [ thma ] suppose conditions [ conda ] and [ condb ] hold .",
    "if @xmath137 instead of condition [ condb ] , then @xmath138 here note that @xmath139 and @xmath116 are all allowed to depend on @xmath11 .",
    "the probability bound on the selection error in theorem [ thma ] is nonasymptotic .",
    "if the conditions of theorem [ thma ] hold with @xmath140 , then ( [ thma-1 ] ) implies selection consistency of the sls estimator and ( [ thma-2 ] ) implies sign consistency .",
    "the conditions are mild .",
    "condition [ conda ] concerns the tail probabilities of the error distribution and is satisfied if the errors are normally distributed .",
    "condition [ condb](i ) ensures that the sls criterion is strictly convex so that the solution is unique .",
    "the oracle estimator @xmath141 is biased due to the laplacian shrinkage .",
    "condition [ condb](ii ) requires a penalty level @xmath26 to prevent this bias and noise to cause false selection of variables in @xmath142 .",
    "condition [ condb](iii ) requires that the nonzero coefficients not be too small in order for the sls estimator to be able to distinguish nonzero from zero coefficients .    in theorem [ thma ]",
    ", we only require @xmath143 , or equivalently , @xmath144  to be positive definite .",
    "the matrix @xmath145 can be singular .",
    "this can be seen  as follows .",
    "the adjacency matrix partitions the graph into disconnected cliques  @xmath146 , @xmath147 , for some @xmath148 .",
    "let node @xmath149 be a ( representative ) member of @xmath146 .",
    "a  node @xmath150 belongs to the same clique @xmath146 iff ( if and only if ) @xmath151 through a certain chain @xmath152 .",
    "define @xmath153 , where @xmath154 is the cardinality of  @xmath146 .",
    "the matrix @xmath144 is positive definite iff @xmath155 implies @xmath156 .",
    "since @xmath157 implies @xmath158 , @xmath144 is positive definite iff the vectors @xmath159 are linearly independent .",
    "this does not require @xmath160 .",
    "in other words , theorem [ thma ] is applicable to @xmath161 problems as long as the vectors @xmath159 are linearly independent .",
    "when @xmath162 is singular , theorem [ thma ] is not applicable . in this case ,",
    "further conditions are required for the oracle property to hold .",
    "the key condition needed is the sparse reisz condition , or src [ @xcite ] , in ( [ srca ] ) below .",
    "it restricts the spectrum of diagonal subblocks of @xmath163 up to a certain dimension .",
    "let @xmath164 be a matrix satisfying @xmath165 and @xmath166 be a vector satisfying @xmath167 .",
    "define @xmath168 since @xmath169 , the two penalized loss functions have the same set of local minimizers . for the penalized loss ( [ tm ] ) with the data @xmath170 ,",
    "let @xmath171 where the map @xmath172 defines the mc@xmath65 estimator [ @xcite ] with data @xmath173 and penalty level @xmath26 .",
    "it was shown in @xcite that @xmath174 depends on @xmath173 only through @xmath175 and @xmath176 , so that different choices of @xmath177 and @xmath178 are allowed .",
    "one way is to pick @xmath179 and @xmath180 .",
    "another way is to pick @xmath181 and @xmath182 of smaller dimensions , where @xmath183 is the moore ",
    "penrose inverse of @xmath184 .",
    "[ condc ] ( i ) for an integer @xmath185 and spectrum bounds @xmath186 , @xmath187\\\\[-8pt ] & & \\eqntext{\\forall b \\mbox { with } |b\\cup{\\mathcal{o}}| \\le d^ * , \\|{\\mathbf{u}}_b\\|=1,}\\end{aligned}\\ ] ] with @xmath188 , @xmath189 in ( [ slsdefa ] ) , and @xmath190 .",
    "\\(ii ) with @xmath191 , @xmath192 ( iii ) with @xmath131 being the diagonal elements of @xmath132 , @xmath193    [ thmb ] suppose conditions [ conda ] and [ condc ] hold .",
    "let @xmath194 be as in ( [ mc+ ] ) .",
    "then @xmath195 if @xmath196 instead of condition  [ condc ] , then @xmath197 here note that @xmath12 , @xmath198 , @xmath199 , @xmath185 , @xmath200 , @xmath201 , @xmath202 and @xmath203 are all allowed to depend on  @xmath11 , including the case @xmath204 as long as the conditions hold as stated .",
    "the statements in also hold for all local minimizers @xmath205 of ( [ slsdefb ] ) or  ( [ tm ] ) satisfying @xmath206 .",
    "if the conditions of theorem [ thmb ] hold with @xmath140 , then ( [ thmb-1 ] ) implies selection consistency of the sls estimator and ( [ thmb-2 ] ) implies sign consistency .",
    "condition [ condc ] , designed to handle the noncovexity of the penalized loss , is a weaker version of condition [ condb ] in the sense of allowing singular @xmath163 .",
    "the src ( [ srca ] ) , depending on @xmath109 or @xmath177 only through the regularized gram matrix @xmath207 , ensures that the model is identifiable in a lower @xmath185-dimensional space . when @xmath161 , the smallest singular value of @xmath109 is always zero .",
    "however , the requirement @xmath208 only concerns @xmath209 diagonal submatrices of @xmath163 , not the gram matrix @xmath145 of the design matrix @xmath109 .",
    "we can have @xmath210 but still require @xmath211 as in ( [ srca ] ) .",
    "since @xmath212 , @xmath185 , @xmath200 , @xmath202 and @xmath203 can depend on @xmath11 , we allow the case @xmath204 as long as conditions [ conda ] and [ condc ] hold as stated .",
    "thus , we allow @xmath0 but require that the model is sparse , in the sense that the number of nonzero coefficients @xmath199 is smaller than @xmath213 .",
    "for example , if @xmath214 for a small @xmath215 and @xmath216 , then we require @xmath217 or greater , @xmath218 and @xmath219 or greater .",
    "so all these quantities can depend on @xmath11 , as long as the other requirements are met in condition  [ condc ] .    by examining the conditions [ condc](ii ) and [ condc](iii ) , for standardized predictors with @xmath220",
    ", we can have @xmath221 or @xmath222 as long as condition [ condc](ii ) is satisfied . as in @xcite , under a somewhat stronger version of condition [ condc ] , theorem [ thmb ] can be extended to quadratic spline concave penalties satisfying @xmath223 with a penalty function satisfying @xmath224 at @xmath225 and @xmath226 for @xmath227 .    also , comparing our results with the selection consistency results of @xcite on the smoothed @xmath228-penalized methods , our conditions tend to be weaker .",
    "notably , @xcite require an condition on the gram matrix which assumes that the correlations between the truly relevant variables and those which are not are small .",
    "no such assumption is required for our selection consistency results .",
    "in addition , our selection results are stronger in the sense that the sls estimator is not only sign consistent , but also equal to the oracle laplacian shrinkage estimator with high probability . in general ,",
    "similar results are not available with the use of the @xmath1 penalty for sparsity .",
    "theorem [ thmb ] shows that the sls estimator automatically adapts to the sparseness of the @xmath12-dimensional model and the denseness of a _ true _ submodel . from a sparse @xmath12-model",
    ", it correctly selects the true underlying model @xmath102 .",
    "this underlying model is a dense model in the sense that all its coefficients are nonzero . in this dense model",
    ", the sls estimator behaves like the oracle laplacian shrinkage estimator in ( [ ora ] ) . as in the convex penalized loss setting , here the results do not require a correct specification of a population correlation structure of the predictors .",
    "there are two natural questions concerning the sls .",
    "first , what are the benefits from introducing the laplacian penalty ?",
    "second , what kind of laplacian @xmath49 constitutes a reasonable choice ?",
    "since the sls estimator is equal to the oracle laplacian estimator with high probability by theorem [ thma ] or [ thmb ] , these questions can be answered by examining the oracle laplacian shrinkage estimator ( [ ora ] ) , whose nonzero part is @xmath229 without the laplacian , that is , when @xmath230 , it becomes the least squares ( ls ) estimator @xmath231 if some of the predictors in @xmath232 are highly correlated or @xmath233 , the ls estimator @xmath234 is not stable or unique . in comparison ,",
    "as discussed below theorem  [ thma ] , @xmath235 can be a full rank matrix under a  reasonable condition , even if the predictors in @xmath232 are highly correlated or @xmath236 .",
    "for the second question , we examine the bias of @xmath237 . since the bias of the target vector ( [ target ] ) is @xmath238",
    ", @xmath237 is unbiased iff @xmath239 .",
    "therefore , in terms of bias reduction , a laplacian @xmath49 is most appropriate if the condition @xmath239 is satisfied .",
    "we shall say that a laplacian @xmath49 is unbiased if @xmath239 .",
    "it follows from the discussion at the end of section [ sec41 ] that @xmath239 if @xmath240 , where @xmath149 is a representative member of the clique @xmath241 and @xmath242 .    with an unbiased laplacian ,",
    "the mean square error of @xmath237 is @xmath243 the mean square error of @xmath244 is @xmath245 we always have @xmath246 for @xmath69 .",
    "therefore , an unbiased laplacian reduces variance without incurring any bias on the estimator .",
    "the results in section [ sec4 ] show that the sls estimator is equal to the oracle laplacian shrinkage estimator with probability tending to one under certain conditions . in addition",
    ", an unbiased laplacian reduces variance but does not increase bias .",
    "therefore , to study the shrinkage effect of the laplacian penalty on @xmath247 , we can consider the oracle estimator @xmath248 . to simplify the notation and without causing confusion , in this section , we study some other basic properties of the laplacian shrinkage and compare it with the ridge shrinkage .",
    "the laplacian shrinkage estimator is defined as @xmath249    the following proposition shows that the laplacian penalty shrinks a  coefficient toward the center of all the coefficients connected to it .    [ propa ] let @xmath250 .",
    "@xmath251    @xmath252    note that @xmath253 is a signed weighted average of the @xmath254 s connected to @xmath255 , since @xmath256 .",
    "part ( i ) of proposition [ propa ] provides an upper bound on the difference between @xmath255 and the center of all the coefficients connected to it .",
    "when @xmath257 , this difference converges to zero . for standardized @xmath258 , part ( ii ) implies that the difference between the centered @xmath255 and @xmath254 converges to zero when @xmath259 .",
    "when there are certain local structures in the adjacency matrix @xmath64 , shrinkage occurs at the local level . as an example",
    ", we consider the adjacency matrix based on partition of the predictors into @xmath260-balls defined in ( [ daa ] ) .",
    "correspondingly , the index set @xmath261 is divided into disjoint neighborhoods / cliques @xmath262 .",
    "we consider the normalized laplacian @xmath263 , where @xmath264 is a @xmath265 identity matrix and @xmath266 with @xmath267 . here",
    "let @xmath269 .",
    "we can write the objective function as @xmath270 for the laplacian shrinkage estimator based on this criterion , we have the following grouping properties .",
    "[ propb ] for any @xmath271 , @xmath272    let @xmath273 be the average of the estimates in @xmath146 .",
    "for any @xmath274 and @xmath275 , @xmath276 , @xmath277    this proposition characterizes the smoothing effect and grouping property of the laplacian penalty in ( [ na ] ) .",
    "consider the case @xmath278 . part ( i ) implies that , for @xmath15 and @xmath150 in the same neighborhood and @xmath69 , the difference @xmath279 if @xmath280 . part ( ii ) implies that , for @xmath15 and @xmath150 in different neighborhoods and @xmath69 , the difference between the centered @xmath255 and @xmath254 converges to zero if @xmath280 .",
    "we now compare the laplacian shrinkage and ridge shrinkage .",
    "the discussion at the end of section [ sec4 ] about the requirement for the unbiasedness of laplacian can be put in a wider context when a general positive definite or semidefinite matrix @xmath281 is used in the place of @xmath49 .",
    "this wider context includes the laplacian shrinkage and ridge shrinkage as special cases .",
    "specifically , let @xmath282 for @xmath283 , @xmath284 becomes the mnet estimator [ huang et al .",
    "( @xcite ) ] . with some modifications on the conditions in theorem [ thma ] or theorem [ thmb ] ,",
    "it can be shown that @xmath285 is equal to the oracle estimator defined as @xmath286 then in a way similar to the discussion in section [ sec4 ] , @xmath285 is nearly unbiased iff @xmath287 .",
    "therefore , for @xmath288 , @xmath289 must be a rank deficient matrix , which in turn implies that @xmath281 must be rank deficient .",
    "note that any laplacian @xmath49 is rank deficient .",
    "this rank deficiency requirement excludes the ridge penalty with . for the ridge penalty to yield an unbiased estimator",
    ", it must hold that @xmath290 in the underlying model .",
    "we now give a simple example that illustrates the basic characteristics of laplacian shrinkage and its differences from ridge shrinkage .",
    "[ ashrinka ]    consider a linear regression model with two predictors satisfying @xmath291 , @xmath292 .",
    "the laplacian shrinkage and ridge estimators are defined as @xmath293 and @xmath294 denote @xmath295 , @xmath296 and @xmath297 .",
    "the laplacian shrinkage estimates are @xmath298 let @xmath299 where @xmath300 is the ordinary least squares ( ols ) estimator for the bivariate regression , @xmath301 is the ols estimator that assumes the two coefficients are equal , that is , it minimizes @xmath302 .",
    "let @xmath303 .",
    "after some simple algebra , we have @xmath304 and @xmath305 thus , for any fixed @xmath306 , @xmath307 is a weighted average of @xmath308 and @xmath301 with the weights depending on @xmath306 . when @xmath309 , @xmath310 therefore , the laplacian penalty shrinks the ols estimates toward a common value , which is the ols estimate assuming equal regression coefficients .",
    "now consider the ridge regression estimator .",
    "we have @xmath311 the ridge estimator converges to zero as @xmath309 . for it to converge to a  nontrivial solution , we need to rescale it by a factor of @xmath312 .",
    "let @xmath313 .",
    "let @xmath314 and @xmath315 . because @xmath316 and @xmath317 , @xmath318 and @xmath319 are also the ols estimators of univariate regressions of @xmath320 on @xmath321 and @xmath320 on @xmath322 , respectively .",
    "we can write @xmath323 where @xmath324 . note that @xmath325 .",
    "thus , @xmath326 is a weighted average of the ols and the univariate regression estimators .",
    "the ridge penalty shrinks the ( rescaled ) ridge estimates toward individual univariate regression estimates .",
    "we use a coordinate descent algorithm to compute the sls estimate .",
    "this algorithm optimizes a target function with respect to a single parameter at a time and iteratively cycles through all parameters until convergence .",
    "this algorithm was originally proposed for criterions with convex penalties such as lasso [ @xcite , @xcite , @xcite , wu and lange ( @xcite ) ] .",
    "it has been proposed to calculate the mcp estimates [ @xcite ]",
    ". detailed steps of this algorithm for computing the sls estimates can be found in the technical report accompanying this paper [ huang et al .",
    "( @xcite ) ] .    in simulation studies , we consider the following ways of defining the adjacency measure .",
    "( n.1 ) @xmath327 and @xmath328 . here",
    "the cutoff @xmath85 is computed as 3.09 using the approach described in section [ sec3 ] with a @xmath12-value of @xmath329 ; ( n.2 ) @xmath330 and @xmath331 . here",
    "the cutoff @xmath85 is computed as 3.29 using the approach described in section [ sec3 ] with a @xmath12-value of @xmath329 ; ( n.3 ) @xmath332 and @xmath333 .",
    "we set @xmath334 , which satisfies the scale - free topology criteria [ @xcite ] ; ( n.4 ) @xmath335 and @xmath331 .",
    "we set @xmath336 .",
    "the penalty levels @xmath26 and @xmath306 are selected using @xmath337-fold cross validation . in our numerical study , we set @xmath338 . to reduce computational cost , we search over the discrete grid of @xmath339 . for comparison , we also consider the mcp estimate and the approach proposed in @xcite ; referred to as d ",
    "j hereafter .",
    "both the sls and mcp involve the regularization parameter @xmath27 . for mcp",
    ", @xcite suggested using @xmath340 for standardized covariates .",
    "the average @xmath341 value of this choice is 2.69 in his simulation studies .",
    "the simulation studies in breheny and huang ( @xcite ) suggest that @xmath342 is a reasonable choice .",
    "we have experimented with different @xmath198 values and reached the same conclusion .",
    "therefore , we set @xmath342 .",
    "we set @xmath343 and @xmath344 . among the 500 covariates ,",
    "there are 100 clusters , each with size 5 .",
    "we consider two different correlation structures .",
    "( i )  covariates in different clusters are independent , whereas covariates @xmath345 and  @xmath15 within the same cluster have correlation coefficients @xmath346 ; and  ( ii )  covariates @xmath345 and @xmath15 have correlation coefficients @xmath346 . under structure",
    "i , zero and nonzero effects are independent , whereas under structure ii , they are correlated .",
    "covariates have marginal normal distributions with mean zero and variance one .",
    "we consider different levels of correlation with @xmath347 . among the 500 covariates ,",
    "the first 25 ( 5  clusters ) have nonzero regression coefficients .",
    "we consider the following scenarios for nonzero coefficients : ( a )  all the nonzero coefficients are equal to 0.5 ; and ( b )  the nonzero coefficients are randomly generated from the uniform distribution on @xmath348 $ ] . in  ( a ) , the laplacian matrices satisfy the unbiasedness property @xmath349 discussed in section [ sec4 ] .",
    "we have experienced with other levels of nonzero regression coefficients and reached similar conclusions .",
    "we examine the accuracy of identifying nonzero covariate effects and the prediction performance . for this purpose , for each simulated dataset , we simulate an independent testing dataset with sample size 100 .",
    "we conduct cross validation ( for tuning parameter selection ) and estimation using the training set only .",
    "we then make prediction for subjects in the testing set and compute the pmse ( prediction mean squared error ) .",
    "we simulate 500 replicates and present the summary statistics in table  [ table1 ] .",
    "we can see that the mcp performs satisfactorily when the correlation is small .",
    "however , when the correlation is high , it may miss a considerable number of true positives and have large prediction errors . the d ",
    "j approach , which can also accommodate the correlation structure , is able to identify all the true positives .",
    "however , it also identifies a large number of false positives , causing by the over - selection of the lasso penalty .",
    "the proposed sls approach outperforms the mcp and d  j methods in the sense that it has smaller empirical false discovery rates with comparable false negative rates .",
    "it also has significantly smaller prediction errors .",
    "= =      in the study reported in @xcite , f1 animals were intercrossed and 120 twelve - week - old male offspring were selected for tissue harvesting from the eyes and microarray analysis using the affymetric genechip rat genome 230 2.0 array .",
    "the intensity values were normalized using the rma [ robust multi - chip averaging , bolstad et al .",
    "( @xcite ) , @xcite ] method to obtain summary expression values for each probe set .",
    "gene expression levels were analyzed on a logarithmic scale .",
    "for the probe sets on the array , we first excluded those that were not expressed in the eye or that lacked sufficient variation .",
    "the definition of expressed was based on the empirical distribution of rma normalized values . for",
    "a probe set to be considered expressed , the maximum expression value observed for that probe among the 120  f2 rats was required to be greater than the 25th percentile of the entire set of rma expression values . for a probe to be considered `` sufficiently variable , '' it had to exhibit at least 2-fold variation in expression level among the 120 f2 animals .",
    "we are interested in finding the genes whose expression are most variable and correlated with that of gene trim32 .",
    "this gene was recently found to cause bardet ",
    "biedl syndrome [ @xcite ] , which is a genetically heterogeneous disease of multiple organ systems including the retina .",
    "one approach to find the genes related to trim32 is to use regression analysis . since it is expected that the number of genes associated with gene trim32 is small and since we are mainly interested in genes whose expression values across samples are most variable , we conduct the following initial screening . we compute the variances of gene expressions and select the top 1000 .",
    "we then standardize gene expressions to have zero mean and unit variance .",
    "we analyze data using the mcp , d  j , and proposed approach . in cross validation , we set @xmath338 .",
    "the numbers of genes identified are mcp : 23 , d  j : 31 ( n.1 ) , 41 ( n.2 ) , 34 ( n.3 ) , 30 ( n.4 ) , sls : 25 ( n.1 ) , 26 ( n.2 ) , 16  ( n.3 ) and 17 ( n.4 ) , respectively .",
    "more detailed results are available from the authors . different approaches and different ways of defining the adjacency measure lead to the identification of different genes .",
    "as expected , the sls identifies shorter lists of genes than the d ",
    "j , which may lead to more parsimonious models and more focused hypothesis for confirmation .",
    "as the proposed approach pays special attention to the correlation among genes , we also compute the median of the absolute values of correlations among the identified genes , which are mcp : 0.171 , d ",
    "j : 0.201 ( n.1 ) , 0.207",
    "( n.2 ) , 0.215 ( n.3 ) , 0.206 ( n.4 ) , sls : 0.247 ( n.1 ) , 0.208 ( n.2 ) , 0.228 ( n.3 ) , 0.212 ( n.4 ) .",
    "the d  j and sls , which incorporate correlation in the penalty , identify genes that are more strongly correlated than the mcp .",
    "the sls identified genes have slightly higher correlations than those identified by d  j .    unlike in simulation study , we are not able to evaluate true and false  positives .",
    "this limitation is shared by most existing studies .",
    "we use the following @xmath337-fold ( @xmath338 ) cross validation based approach to evaluate prediction .",
    "( a )  randomly split data into @xmath337-subsets with equal sizes ; ( b )  remove one subset from data ; ( c )  conduct cross validation and estimation using the rest @xmath350 subsets ; ( d )  make prediction for the one removed subset ; ( e )  repeat steps ( b)(d ) over all subsets and compute the prediction error .",
    "the sums of squared prediction errors are mcp : 1.876 ; d  j : 1.951 ( n.1 ) , 1.694 ( n.2 ) , 1.534 ( n.3 ) and 1.528 ( n.4 ) ; sls : 1.842 ( n.1 ) , 1.687 ( n.2 ) , 1.378 ( n.3 ) and 1.441 ( n.4 ) , respectively .",
    "the sls has smaller cross validated prediction errors , which may indirectly suggest better selection properties .",
    "in this article , we propose the sls method for variable selection and estimation in high - dimensional data analysis . the most important feature of the sls is that it explicitly incorporates the graph / network structure in predictors into the variable selection procedure through the laplacian quadratic .",
    "it provides a systematic framework for connecting penalized methods for consistent variable selection and those for network and correlation analysis . as can be seen from the methodological development ,",
    "the application of the sls variable selection is relatively independent of the graph / network construction .",
    "thus , although graph / network construction is of significant importance , it is not the focus of this study and not thoroughly pursued .",
    "an important feature of the sls method is that it incorporates the correlation patterns of the predictors into variable selection through the laplacian quadratic .",
    "we have considered two simple approaches for determining the laplacian based on dissimilarity and similarity measures .",
    "our simulation studies demonstrate that incorporating correlation patterns improves selection results and prediction performance .",
    "our theoretical results on the selection properties of the sls are applicable to a general class of laplacians and do not require the underlying graph for the predictors to be correctly specified .",
    "we provide sufficient conditions under which the sls estimator possesses an oracle property , meaning that it is sign consistent and equal to the oracle laplacian shrinkage estimator with high probability .",
    "we also study the grouping properties of the sls estimator .",
    "our results show that the sls is adaptive to the sparseness of the original @xmath12-dimensional model with @xmath0 and the denseness of the underlying @xmath199-dimensional model , where @xmath351 is the number of nonzero coefficients .",
    "the asymptotic rates of the penalty parameters are derived . however , as in many recent studies , it is not clear whether the penalty parameters selected using cross validation or other procedures can match the asymptotic rate .",
    "this is an important and challenging problem that requires further investigation , but is beyond the scope of the current paper .",
    "our numerical study shows a satisfactory finite - sample performance of the sls .",
    "particularly , we note that the cross validation selected tuning parameters seem sufficient for our simulated data .",
    "we are only able to experiment with four different adjacency measures .",
    "it is not our intention to draw conclusions on different ways of defining adjacency .",
    "more adjacency measures are hence not explored .",
    "we have focused on the linear regression model in this article .",
    "however , the sls method can be applied to general linear regression models .",
    "specifically , for general linear models , the sls criterion can be formulated as @xmath352 where @xmath353 is a given loss function .",
    "for instance , for generalized linear models such as logistic regression , we can take @xmath353 to be the negative log - likelihood function . for cox regression",
    ", we can use the negative partial likelihood as the loss function .",
    "computationally , for loss functions other than least squares , the coordinate descent algorithm can be applied iteratively to quadratic approximations to the loss function . however",
    ", further work is needed to study theoretical properties of the sls estimators for general linear models .",
    "there is a large literature on the analysis of network data and much work has also been done on estimating sparse covariance matrices in high - dimensional settings .",
    "see , for example , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , among others",
    ". it would be useful to study ways to incorporate these methods and results into the proposed sls approach . in some problems such as genomic data analysis",
    ", partial external information may also be available on the graphical structure of some genes used as predictors in the model",
    ". it would be interesting to consider approaches for combining external information on the graphical structure with existing data in constructing the laplacian quadratic penalty .",
    "in this appendix , we give proofs of theorems [ thma ] and [ thmb ] and propositions  [ propa ] and  [ propb ] .",
    "proof of theorem [ thma ] since @xmath354 , the criterion ( [ slsdefa ] ) is strictly convex and its minimizer is unique .",
    "let @xmath355 , @xmath356 and @xmath357 since @xmath358 , @xmath359 does not depend on @xmath360 .",
    "thus , @xmath247 is the minimizer of @xmath361 .",
    "since @xmath362 gives @xmath363 , the kkt conditions hold for @xmath364 at @xmath365 in the intersection of the events @xmath366 let @xmath367 with @xmath368 .",
    "since @xmath369 and both  @xmath370 and @xmath371 are supported in @xmath102 , @xmath372 which describes the effect of the bias of @xmath105 on the gradient in the linear model @xmath373 .",
    "since @xmath374 , we have @xmath375 .",
    "since @xmath376 , ( [ bias-1 ] ) gives @xmath377 since @xmath125 , @xmath378 can be written as @xmath379 , @xmath380 , where @xmath381 and @xmath382 are the diagonal elements of @xmath383 .",
    "thus , @xmath384 since @xmath385 , the sub - gaussian condition  [ conda ] yields @xmath386 the proof of ( [ thma-1 ] ) is complete , since @xmath387 for all @xmath388 in @xmath389 .    for the proof of ( [ thma-2 ] )",
    ", we have @xmath390 due to @xmath391 it follows that the condition on @xmath392 implies condition [ condb](iii ) with @xmath393 in @xmath389 .",
    "proof of theorem [ thmb ] for @xmath394 and vectors @xmath395 in the range of @xmath177 , define @xmath396\\\\[-8pt ] & & \\qquad= \\max\\biggl\\{\\frac{\\|({\\widetilde{p}}_b-{\\widetilde{p}}_{{{\\mathcal{o}}}}){\\mathbf{v}}\\|_2}{(m n)^{1/2}}\\dvtx { \\mathcal{o}}\\subseteq b\\subseteq\\{1,\\ldots .",
    "p\\ } , |b| = m+|{\\mathcal{o}}| \\biggr\\},\\nonumber\\end{aligned}\\ ] ] where @xmath397 . here",
    "@xmath398 depends on @xmath306 through @xmath399 .",
    "since @xmath194 is the mc+ estimator based on data @xmath170 at penalty level @xmath26 and ( [ srca ] ) holds for @xmath400 , the proof of theorem 5 in @xcite gives @xmath365 in the event @xmath401 , where @xmath402 is as in ( [ kktb ] ) and @xmath403 note that @xmath404 in @xcite is identified with @xmath405 , @xmath406 here .",
    "let @xmath367 with @xmath368 . since @xmath369 , ( [ bias-1 ] ) still holds with @xmath375 . since @xmath407 , ( [ bias-1 ] ) still gives ( [ omega1 ] ) . a slight modification of the argument for ( [ omega2 ] ) yields @xmath408\\\\[-8pt ] & & \\hspace*{137.6pt}\\ge \\sigma\\sqrt{2\\log(|\\mathcal{o}|/{\\epsilon})}\\bigr\\}.\\nonumber\\end{aligned}\\ ] ] for @xmath409 , we have @xmath410 and @xmath411 .",
    "thus , by ( [ tzeta ] ) @xmath412 since @xmath413 , this gives @xmath414 since @xmath415 , ( [ omega1 ] ) , ( [ omega2a ] ) , ( [ omega3 ] ) and condition [ conda ] imply @xmath416 the proof of ( [ thmb-1 ] ) is complete , since @xmath387 for all @xmath388 in @xmath389 .",
    "we omit the proof of ( [ thmb-2 ] ) since it is identical to that of ( [ thma-2 ] ) .",
    "proof of proposition [ propa ] the @xmath417 satisfies @xmath418 therefore , by cauchy  schwarz and using @xmath419 , we have @xmath420 now because @xmath421 , we have @xmath422 .",
    "this proves part ( i ) .    for part ( ii ) , note that we have @xmath423 thus @xmath424 part ( ii ) follows .",
    "proof of proposition [ propb ] the @xmath417 must satisfy @xmath425 taking the difference between the @xmath15th and @xmath150th equations in ( [ neq1 ] ) for , we get @xmath426 therefore , @xmath427 part ( i ) follows from this inequality .",
    "define @xmath428 .",
    "this is the average of the elements in @xmath429 . for any @xmath274 and @xmath430",
    ", we have @xmath431 thus , part ( ii ) follows .",
    "this completes the proof of proposition [ propb ] .",
    "we wish to thank two anonymous referees , the associate editor and editor for their helpful comments which led to considerable improvements in the presentation of the paper ."
  ],
  "abstract_text": [
    "<S> we propose a new penalized method for variable selection and estimation that explicitly incorporates the correlation patterns among predictors . </S>",
    "<S> this method is based on a combination of the minimax concave penalty and laplacian quadratic associated with a graph as the penalty function . </S>",
    "<S> we call it the sparse laplacian shrinkage ( sls ) method . </S>",
    "<S> the sls uses the minimax concave penalty for encouraging sparsity and laplacian quadratic penalty for promoting smoothness among coefficients associated with the correlated predictors . </S>",
    "<S> the sls has a generalized grouping property with respect to the graph represented by the laplacian quadratic . </S>",
    "<S> we show that the sls possesses an oracle property in the sense that it is selection consistent and equal to the oracle laplacian shrinkage estimator with high probability . </S>",
    "<S> this result holds in sparse , high - dimensional settings with @xmath0 under reasonable conditions . </S>",
    "<S> we derive a coordinate descent algorithm for computing the sls estimates . </S>",
    "<S> simulation studies are conducted to evaluate the performance of the sls method and a real data example is used to illustrate its application .    ,    ,    and    .    . </S>"
  ]
}