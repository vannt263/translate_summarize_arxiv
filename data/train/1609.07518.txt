{
  "article_text": [
    "modeling large - scale networks has been stirring much developments in various fields such as machine learning and system identification .",
    "the assembly of numerous systems interacting with one another arises in fields such as biology , e.g with the brain neurons in @xcite , optics with the atmospheric turbulence , and many others . due to the large size of input - output data batches , identifying locally",
    "the behavior of the network is a major challenge that has been mainly addressed by using prior knowledge on how the subsystems , or nodes , are connected to one another .",
    "one common assumption is sparsity and relies on the fact that each node is connected to a limited number of other nodes with respect to the network s size .",
    "other well - studied structures include interconnected one - dimensional strings of subsystems in @xcite , or clusters of different subsystems with known connection patterns , named as alpha - heterogeneous in @xcite .",
    "however the links between the subsystems in the network might not be known beforehand . in the so - called sparse plus low rank networks",
    "-@xcite- , a few latent variables relate most of the measured nodes from which few of them influence each other .",
    "model identification remains computationally challenging to handle the combination of these two matrices structure . in @xcite",
    "very general graphs are studied , whose weighted adjacency matrix is approximated with kronecker products of a so - called initiator matrix .",
    "such an operation replicates the network structure associated to the initiator matrix to higher dimensions .",
    "therefore this matrix embeds all the required information on the network to construct it at higher scales .",
    "it is moreover shown in @xcite that any network is approximated with such a structure after having ordered the nodes in the most adequate way . in this paper",
    "we address the identification of ( 2d ) spatial - temporal dynamical models of the vector - auto - regressive ( var ) form .",
    "the coefficient - matrices of this model are parametrized as sums of kronecker products .",
    "@xcite establishes the equivalence between expressing a matrix as a sum containing few kronecker products and a low - rank approximation of a reshuffled matrix .",
    "the latter has been studied in @xcite for function - related matrices , in which more general existence theorems are derived . moreover ,",
    "when the matrix exhibits multiple symmetries or is block - toeplitz with toeplitz - blocks , it is guaranteed that such a low rank approximation of the reshuffled matrix exists , see @xcite and @xcite .",
    "more than only enjoying the storage of a reduced number of entries , such a structure enables fast computations thanks to the very pleasant algebra of the kronecker product , @xcite . for a matrix written as @xmath0 with @xmath1 ,",
    "matrix - matrix multiplication and inversion both require @xmath2 instead of @xmath3 for the unstructured case .    for a given labeling of the network ,",
    "the coefficient - matrices in the varx model may be represented with as few terms as possible in the kronecker sum while still guaranteeing a given level of performance with respect to the standard least squares solution with unstructured coefficient matrices .",
    "a major challenge in the estimation is the computational efficiency .",
    "we address this problem by parametrizing only the factor matrices , e.g @xmath4 , which gives rise to a bilinear least squares problem .",
    "the estimation problem belongs to the class of multi - convex optimization : fixing all variables but one yields a convex problem , @xcite .",
    "such a formulation shares similarities with the identification of hammerstein systems , see e.g @xcite for which a two - stage algorithm is proposed .",
    "the contribution of this work includes the formulation of a kronecker - based varx model for networks with unknown communication links .",
    "the estimation problem is solved using both a non - iterative three - stage algorithm and an iterative alternating least squares , for which we have extended a convergence proof to our case .",
    "thirdly , the missing sensor data can be retrieved by formulating a multi - linear least squares .",
    "this paper is organized as follows .",
    "in the second section the class of _ kronecker networks _ is defined .",
    "the third section formulates the kronecker varx identification framework while the fourth section describes the non - iterative overparametrized algorithm to estimate the factor matrices with minimum computational complexity .",
    "an alternative is proposed in section v within the framework of multi - linear least squares .",
    "section vi describes the missing data case .",
    "this covers the case of non - rectangular measurement grids .",
    "we study how additional structure can be considered on the factor matrices in section vii .",
    "last , section viii is dedicated to numerical experiments .",
    "_ notations . _",
    "the vectorization operator for a matrix @xmath5 is @xmath6 .",
    "@xmath7 reshapes the vector @xmath8 into a matrix whose size will be clear from the context .",
    "the kronecker product between two matrices @xmath9 is denoted by @xmath10 . , the 2-norm of a vector @xmath8 is written as @xmath11 .",
    "the number of non - zero - elements in a vector @xmath8 is @xmath12 while the sum in absolute value is @xmath13 .",
    "@xmath14 is the largest eigenvalue of the positive - semidefinite matrix @xmath5 .",
    "the nuclear norm of @xmath5 , denoted with @xmath15 , represents the sum of the singular values of @xmath5 .",
    "[ @xcite ] [ def : def1 ] let @xmath5 be a @xmath16 block matrix with blocks @xmath17 in @xmath18 , given as : @xmath19   \\in \\mathbb{r}^{m_1 m_2 \\times n_1 n_2}\\ ] ] then the re - shuffle operator @xmath20 is defined as : @xmath21   \\in \\mathbb{r}^{m_1 n_1 \\times m_2 n_2}\\ ] ]    reshuffling @xmath22 to form @xmath5 back is defined with the operator @xmath23 , i.e @xmath24 .",
    "[ @xcite ] [ lem : kron ] let @xmath5 be defined as in definition  [ def : def1 ] , and let @xmath25 , with @xmath26 . then",
    ": @xmath27    the operation in lemma  [ lem : kron ] can also be reversed by the definition of the inverse vec operator @xmath28 .",
    "[ @xcite ] [ lem : skron ] let @xmath5 be defined as in definition  [ def : def1 ] , and let an svd of @xmath20 be given as : @xmath29 and let @xmath30 , @xmath31 : @xmath32    the integer @xmath33 is called the _ kronecker rank _ of @xmath5 w.r.t .",
    "the chosen block partitioning of @xmath5 as given in definition  [ def : def1 ] .",
    "[ def : adecomp ] let @xmath34 be a @xmath35 pattern matrix .",
    "define @xmath36 ( with @xmath37 ) and @xmath38}$ ] as an @xmath35 diagonal matrix which contains @xmath39 in the diagonal entries of indices from @xmath40 to @xmath41 ( included ) and @xmath42 elsewhere , then an @xmath43-decomposible matrix ( for a given @xmath43 ) is a matrix of the following kind : @xmath44 }   \\otimes m_a^{(i ) } ) + \\sum_{i=1}^\\alpha ( i_{[\\beta_{i-1}+1:\\beta_i ] }   p \\otimes m_b^{(i ) } ) \\ ] ] the matrices @xmath45 are the diagonal blocks of @xmath46 , while the matrices @xmath47 constitute the off - diagonal blocks , according to the structure of @xmath48 .    for @xmath49 ( and @xmath50 ) , these matrices are simply called _",
    "decomposable _ matrices .",
    "the class of @xmath43-decomposable matrices will be denoted by @xmath51 , with for @xmath52 just the symbol @xmath53 will be used .    as a generalization of this class of structured matrices ,",
    "we define next the class of sums of kronecker product matrices .",
    "[ def : skron ] the class of sums of kronecker product matrices , denoted by @xmath54 , contains matrices of the following kind : @xmath55 with @xmath56 and @xmath57",
    "the sensor readings at time instance @xmath58 are stored in the matrix @xmath59 as : @xmath60\\ ] ] with @xmath61 . in this paper",
    "we will consider that the ( temporal ) dynamics of this array of sensors is governed by the following var(x ) model : @xmath62 with @xmath63 zero - mean white noise with covariance matrix @xmath64 .",
    "the coefficient matrices @xmath65 and @xmath66 in the varx ( we will restrict for simplicity to the ar - case ) model , in general are highly structured . here",
    "we will consider these coefficient matrices to be in the matrix sets @xmath67 or @xmath54 .",
    "we will consider the case they belong to the set @xmath54 , and for the moment only focus on the coefficient matrices @xmath65 . to address an identification problem",
    "we will parametrize these coefficient matrices as : @xmath68 with the vectors @xmath69 and @xmath70 parametrizing the matrices @xmath71 and @xmath72 in an affine manner .",
    "if we consider the term @xmath73 as a temporally white sequence @xmath74 , then the arx model can be written as , @xmath75 using the following kronecker rule , for matrices @xmath76 of compatible dimensions such that the product @xmath77 exists , @xmath78 we can write the arx model as , @xmath79 with @xmath80 .",
    "this can also be written explicitly as , @xmath81 \\bigl(i_{r_i } \\otimes s(k - i)\\bigr ) \\left [ \\begin{matrix }   m(b_i^{(1 ) } ) \\\\ \\vdots \\\\",
    "m(b_i^{(p ) } ) \\end{matrix } \\right ] \\nonumber \\\\ & &     + v(k)\\end{aligned}\\ ] ] the ar(x ) models , or are called _ kronecker arx network models _ , or briefly _ kronecker arx models _ ( krarx ) ( pronounced as `` _ _ quarks _ _ '' ) .",
    "given the model structure of krarx models , the problem of identifying these models from measurement sequences @xmath82 is fourfold :    1 .",
    "the temporal order index @xmath83 .",
    "the spatial order index @xmath84 for each coefficient matrix .",
    "3 .   the parametrization of the matrices @xmath71 and @xmath72 .",
    "an example of a parametrization of the matrices @xmath71 and @xmath72 is ( block ) toeplitz .",
    "the estimation of the parameter vectors @xmath69 , @xmath70 up to an ambiguity transformation .",
    "this requires the specification of a cost function .",
    "an example of such a cost function using the model is the following least squares cost function ,    @xmath85    for data batches with @xmath86 temporal samples . by the selection of the parameter @xmath83 and particular choices of the parametrizations in step 3 above , various special cases of restricting the coefficient matrices @xmath65 in to particular sets ( such as @xmath67 or @xmath54 ) can be considered .",
    "further constraints to the ( least - squares ) cost function , such as in , might be introduced to look for sparsity in the parametrization vectors @xmath69 and @xmath70 .",
    "an important challenge of the parameter estimation problem is the _",
    "computational efficiency_. the covariance matrix estimation in high dimensional spaces has already been addressed in @xcite and is not considered further on in this document .",
    "consider the krarx model then we can define the matrix @xmath87 with @xmath88 and write this model as : @xmath89 according to and the definition of the re - shuffling operator @xmath90 we have , @xmath91 therefore a way to find the spatial order index ( assuming it is the same for all @xmath87 ) is via the kronecker rank .",
    "let this be denoted by @xmath33 , then we write , @xmath92 and with the definition of @xmath93 , @xmath94 , the coefficient matrix @xmath87 can be written as , @xmath95 not knowing the kronecker rank , a possible way to retrieve this parameter and the coefficient matrices @xmath87 for a given temporal order @xmath83 from the data is via the following multi - criteria cost function : @xmath96 let the estimated coefficient matrices be denoted by @xmath97 , then subsequently an svd of the matrices @xmath98 provides estimates of the terms @xmath99 , @xmath100 and @xmath101 in the kronecker products in .",
    "it should be remarked that this way of formulating the identification problem does not require a parametrization of the matrices @xmath71 and @xmath72 as stipulated in the third step of the fourfold generic identification problem formulation outlined in section  [ sec_idep ] .",
    "since the rank operator in the cost function turns this cost function into a non - convex optimization problem , the nuclear norm can be used to convexify this problem .",
    "this would then yield the following problem formulation , @xmath102 where an additional weighting parameter @xmath103 is introduced .",
    "the nuclear norm regularization on @xmath83 matrices of size @xmath104 is prohibitive especially when handling large datasets .",
    "it would indeed imply solving e.g a alternating direction method of multipliers algorithm ( admm ) with a singular - value decomposition on large matrices at each iteration or expensive matrices inversions scaling up to @xmath3 .",
    "however we see in the following section that some more efficient computations can be performed by parallelizing the optimization problem .      in this paragraph",
    "we consider @xmath105 for the sake of clarity .",
    "the least squares term in the cost function can be addressed row by row .",
    "the @xmath106-th line of @xmath107 , where @xmath108 , and @xmath109 integers , is denoted with @xmath110 .",
    "using `` standard '' matlab notation to select part of a matrix , the matrix @xmath111 and @xmath112 are related as : @xmath113 to further clarify this notation , we refer to figure 1 for a display of the different matrices in the above relation .     and @xmath114.,scaledwidth=40.0% ]",
    "we state the following lemma :    if @xmath115 , then for all @xmath116 .",
    "let the pca decomposition of @xmath114 be such that : @xmath117 where @xmath118 . from , @xmath119 .",
    "the vectors @xmath120 are not necessarily full column rank , therefore @xmath121 .",
    "the reverse implication is not true .",
    "denote an upper - bound on the rank of @xmath114 by @xmath122 . by assuming that the pca decomposition of @xmath123 is given with @xmath124 and @xmath125 , a low - rank matrix @xmath114 is built using .",
    "we describe the algorithm in the following lines .",
    "let the measurement at position @xmath126 in the data matrix @xmath59 correspond to the @xmath106-th entry of the vector @xmath127 . then a rank - constrained least squares optimization",
    "is formulated to estimate the matrix @xmath128 without over - fitting : @xmath129 which is solved using the relaxed problem : @xmath130 a pca of the low - rank matrix @xmath131 yields the following decomposition : @xmath132 where @xmath133 .",
    "this decomposition is unique up to a ( non - singular ) ambiguity transformation @xmath134 : @xmath135 therefore such a pca can not be performed for @xmath136 independent well - chosen rows , as it would yield @xmath136 different ambiguity transformations .",
    "there remains @xmath137 matrices of size @xmath138 to estimate the full factor matrices @xmath139 .",
    "the latter have to be consistent with the estimation in and consider the same ambiguity transformation . therefore ,",
    "if is solved e.g for @xmath140 , then for all @xmath141 $ ] , we solve the constrained least - squares optimization : @xmath142 and for all @xmath106 such that @xmath143 , where @xmath144 $ ] : @xmath145 these @xmath137 least squares can be performed in parallel , each of which corresponds to one sensor location as can be visualized in figure 2 .",
    "choosing the sensor location in position @xmath146 in is not unique .     which data to consider in order to estimate the coefficient - matrices with minimum computational complexity .",
    "blue entry : minimization of type - .",
    "green entry : minimization of type .",
    "yellow entry : minimization of type .,scaledwidth=20.0% ]    the three - step algorithm has been discussed for the case @xmath147 . in most cases ,",
    "the kronecker rank is not known _ a priori _ and it has to be detected with cross - validation .",
    "therefore we analyze how to deal with higher kronecker ranks , e.g @xmath148 . in the previous paragraph ,",
    "the kronecker rank was limited by the size of the submatrices @xmath149 .",
    "therefore a submatrix of size @xmath150 shall be selected such that the pca in is then carried out on a rank - deficient matrix .",
    "for example , the output data associated with the set of indices @xmath151 conveys enough information to retrieve the factor matrices in this case .",
    "figure 3 illustrates the sensor locations .",
    "the regularized least squares in is then extended into : @xmath152 where the rank - deficient matrix is : @xmath153 similarly , the pca is performed on @xmath154 and the least squares in and are formulated for 4 neighboring points .     and @xmath155 .",
    "the color code is the same as in figure 2.,scaledwidth=20.0% ]    the computational complexity is reduced exploiting the kronecker structure , and is attractive for being non - iterative and parallelizable to a large extent .",
    "although it is possible to enforce some additional structure , e.g block - banded with banded - blocks , the framework is nonetheless less elegant than the alternating least squares minimization that we propose in the next paragraph to embed more structure on the factor matrices .",
    "this approach is formulated based on the representation .",
    "this forms has the advantage that constraints on the parametrizations of the matrices @xmath71 and @xmath72 can be more easily taken into consideration .",
    "for example the parametrization may enforce the matrices to have a ( block ) toeplitz structure with entries in a particularly chosen set , e.g. @xmath156 .",
    "denote this set for the parameter vectors @xmath69 , @xmath70 resp .",
    "by @xmath157 and @xmath158 and let us assume @xmath83 and @xmath33 to be given .",
    "denote moreover : @xmath159   \\\\",
    "\\tilde m_{b_i } & : = & \\left [ \\begin{matrix }   m(b_i^{(1 ) } ) \\\\ \\vdots \\\\",
    "m(b_i^{(p ) } ) \\end{matrix } \\right ]   \\end{aligned}\\ ] ] then we have the following bi - convex optimization problem @xmath160 here again we consider @xmath161 for the sake of clarity .",
    "the krarx model then reads : @xmath162 such that @xmath163 .",
    "the @xmath164-th column of @xmath165 , resp .",
    "@xmath166 , is denoted with @xmath167 , resp .",
    "@xmath84 . given an initial guess of the matrix @xmath165 denoted as @xmath168 ,",
    "the following three steps are performed :    1 .   _",
    "step 1 : _ a minimization step over the columns of the matrix @xmath166 : @xmath169 2 .",
    "_ step 2 : _ the estimates @xmath170 are normalized : @xmath171 3 .",
    "_ step 3 : _ a minimization step over the columns of the matrix @xmath165 : @xmath172      in this section we study the convergence of the alternating least squares repeating the three steps in - .",
    "the proof relies on the work in @xcite that establishes the result when @xmath173 are vectors .",
    "therefore we review here the main theorems of the proof , and highlight the changes in appendix .",
    "we first reformulate the equation . it can be shown that : @xmath174 where @xmath175 @xmath176 is defined similarly as @xmath177 .",
    "@xcite analyse the convergence of the alternating least squares solution using the contraction mapping theorem , @xcite .",
    "let @xmath178 denote the iteration counter .",
    "let the initial estimate of @xmath179 be denoted by @xmath180 and denote the results of the subsequents three steps ( 1 to 3 ) be denoted resp .",
    "@xmath181 then a _ functional _ representation of the three steps reads : @xmath182 these equations can be expressed using a single operator @xmath183 mapping the estimate @xmath180 to @xmath184 : @xmath185    _ [ the contraction mapping theorem , @xcite ] _ let @xmath186 be a non - empty complete metric space where @xmath187 is a metric on @xmath5 .",
    "let @xmath188 be a contraction mapping on @xmath5 , i.e. , there is a nonnegative real number @xmath189 such that @xmath190 , for all @xmath191 .",
    "then the map @xmath192 admits one and only one fixed point @xmath193 which means @xmath194 .",
    "furthermore , this fixed point can be found from the convergence of an iterative sequence defined by @xmath195 for @xmath196 with an arbitrary starting point @xmath197 in @xmath5 .",
    "we start by defining the following inner product .",
    "[ def : x ] let @xmath198 and denote their columns with @xmath199 . for two matrices @xmath200 of conformable sizes , such that @xmath201 , similarly for @xmath202 , the inner product on @xmath203 is _ defined _ with : @xmath204    the matrices @xmath205 have the structure of the matrices @xmath200 in the above definition .",
    "[ lem10 ] for the matrix @xmath206 and the defined inner product in definition  [ def : x ] , the quantity @xmath207 is a norm on @xmath203 .",
    "we define now two sets , associated to each of the variables @xmath166 and @xmath165 : @xmath208 @xmath209 are the estimates of resp .",
    "@xmath210 .",
    "if the following statements are true :    * @xmath211 : the noise components in @xmath212 are independent identically distributed ( i.i.d ) with zero - mean and finite temporal and spatial variance .",
    "* @xmath213 : the matrix @xmath214 is full column rank , which corresponds to the temporal persistency of excitation .",
    "* @xmath215 : either @xmath216 or @xmath217 is known for all @xmath164 , and the first non - zero entry of @xmath218 is strictly positive . *",
    "@xmath219 : the initial estimate @xmath220 is non - zero .    then , the map @xmath221 is a contraction on @xmath222 and has a unique fixed point @xmath223 when @xmath224 , that corresponds to the true parameters .",
    "this theorem proves that , whatever the ( non - zero ) initial conditions are , the alternating least squares in equations - converges to a global minimum when @xmath224 .",
    "that global minimum corresponds to the true parameters of the krarx model .",
    "the analysis focuses on one alternating step because the cost is identical for the other minimization step .",
    "computing @xmath225 costs @xmath226 . at each iteration ,",
    "the least - squares estimate of @xmath165 is computed as @xmath227 .",
    "the matrix - matrix multiplication is done in parallel by sending to @xmath228 cores the vector @xmath229 , each of which processes @xmath230 with @xmath231 .",
    "a cholesky factorization is then computed with @xmath232 , hence enabling faster inversion with gaussian pivoting on triangular matrices .",
    "denote @xmath233 . the global cost for computing",
    "the matrix - matrix multiplication @xmath234 is @xmath235 , which can be reduced by distributing the computations over different cores .",
    "last , @xmath236 requires @xmath237 .",
    "a total of @xmath238 operations are needed to solve one alternating minimization .",
    "hence the als requires @xmath239 flops , where @xmath240 is the total number of iterations .",
    "solving the unstructured least squares requires @xmath3 .",
    "more importantly , part of the bottleneck resides into matrix - matrix multiplications for which graphical processing units ( gpu ) are efficient computing tools .",
    "we extend here the framework considered in that collects the sensor data in a matrix .",
    "let us assume that there are a few missing entries in the map @xmath59 , @xmath241 .",
    "for example , the data might be collected on a circular array , which implies that the blocks in the matrices do nt share the same size .",
    "consequently , the coefficient - matrices in the lifted varx model do not retain the kronecker structure .",
    "therefore we embed the network in a rectangular enveloppe , with the added entries considered as unknowns . in this paragraph",
    "we study the estimation of the factor matrices as well as the missing entries by working on the rectangular embedding .",
    "denote the set of known , resp .",
    "unknown , entries in @xmath242 with @xmath243 , resp . @xmath244 .",
    "the global least squares boils down to : @xmath245 the estimation problem still belongs to the class of multi - convex problems , @xcite .",
    "the alternating minimization algorithm consists of minimizing each set of variables while the others are fixed and iterate until convergence .",
    "for example , we start by initializing @xmath246 and @xmath247 , for all @xmath248 , and optimize over @xmath167",
    ". eventually this will provide with estimates of the unknown sensor measurements for all @xmath249 .",
    "the parametrization of the factor matrices based on additional knowledge of the network may help either to reduce the computational complexity of the model identification step , or to cast the model into a structure useful for future use , e.g control .",
    "the first category include banded , symmetric , toeplitz and circulant patterns whereas the second contains e.g sparse or sequentially semi - separable ( sss ) matrices .",
    "exploring such structures on the factor matrices is very attractive numerically as the problem size reduces further .",
    "the block - toeplitz toeplitz - blocks ( bttb ) structure arises e.g when modeling 2d homogeneous spatially - invariant phenomena on a rectangular grid .",
    "the kronecker and bttb structures are related , but not equivalent .",
    "an insight is given in the following lemma .",
    "let @xmath250 .",
    "* if @xmath5 is symmetric block - toeplitz , then @xmath5 has a kronecker rank of @xmath136 . *",
    "if @xmath5 has a kronecker rank of 1 , it does nt _ in general _ imply neither that @xmath5 is block - toeplitz nor has toeplitz - blocks .",
    "enforcing the coefficient matrices to be toeplitz fastens up the identification : at each alternating step , the unknown toeplitz is embedded into a circulant matrix which is then diagonalized using the discrete fast fourier transform .",
    "the graphs corresponding to real - networks are sparse , @xcite .",
    "we assume that a node is connected to a limited number of other nodes in the network relatively to the network s size , and therefore we seek to induce sparsity in the matrices @xmath71 and @xmath72 .",
    "when the influence of neighboring nodes decay with the distance , a multi - banded structure is equivalent to a banded structure of each factor matrix .",
    "when the graph does nt exhibit such regularity , one other possibility is to induce zero - elements in the estimated parameter vectors @xmath69 , @xmath70 .",
    "let @xmath251 denote the zero - norm , then using again @xmath103 as regularization parameter , the following constrained optimization problem results , @xmath252 with @xmath253\\ ] ] in order to preserve the biconvex nature the zero norm is replaced by the @xmath39-norm as follows , @xmath254 we can not guarantee theoretically the convergence of this alternating sparse least squares because the proof of theorem 6 in appendix relies on a closed - form expression of each update .",
    "optimization of sparse regularized least squares as in has been widely studied in the literature , see e.g @xcite .",
    "the so - called sss structure enables standard matrix computations ( @xmath255 ) to be done in linear computational complexity with respect to the matrix size .",
    "for example , inverting a matrix @xmath256 written as @xmath257 in which both @xmath258 , @xmath259 have a sss structure requires @xmath260 operations instead of @xmath3 .",
    "low - rank off - diagonal blocks of the factor matrices that are sought after can be enforced via nuclear norm regularization , see @xcite for more details .",
    "the proposed _ quarks identification _ method is now illustrated with a real - life example of a network with unknown communication links . the atmospheric turbulence is a stochastic process that has been modeled via state - space and var models or simplified into diagonal var models , @xcite .",
    "when a lightbeam with a flat wavefront passes through a turbulent medium , the wavefront gets distorted .",
    "the spatial covariance of the wavefront is not sparse and hence there exists multiple connections from a subsystem to the other in the network . in this paper",
    "the turbulence is modeled with a state - space model following the method in @xcite for one single layer .",
    "the quarks identification is performed with the als algorithm , and the number of iterations is limited to 10 .",
    "the variance accounted for ( vaf ) between two signals @xmath261 and @xmath262 is defined with : @xmath263 two signals with a vaf equal to @xmath264 are identical .",
    "the performance criteria are both the vaf and relative root - mean - square - error ( rmse ) between the signals @xmath265 and @xmath266 .",
    "an array of @xmath267 phase points is considered .",
    "the identification set contains @xmath268 temporal measurements . in figure 4 and 5 the coefficient matrix @xmath269",
    "is displayed when identifying a varx model with @xmath105 resp .",
    "for the case of estimating the varx coefficient matrices via least - squares with @xmath270 sparsity regularization and with krarx model . in figure 6",
    "is plotted the relative rmse in boxplots for different methods these different methods .",
    "the patterns of coefficient - matrices identified differ from the method used , and although the sparsity constraint minimizes the number of non - zero entries , it remains detrimental to the overall performance quality with respect to the number of parameters needed to construct the matrix itself .",
    "let us define a measure that we call _ model complexity _ as the number of non - zero entries needed to construct the @xmath83 coefficient - matrices .",
    "for example , the complexity of a krarx model is at most @xmath271 -only the non - zero elements of the factor matrices- , while it reaches a total of @xmath272 for the full least squares estimation .",
    "it is illustrated in figure 7 that displays the vaf with respect to the 0-norm of the entries needed to construct the full coefficient matrix .",
    "the vaf obtained with the sparse identification decreases with increasing regularization parameter .",
    "exact _ number of non - zero entry decreases only for high prior on sparsity , for which the vaf is already 0 .         model with a kronecker structure , @xmath273 .",
    "entries in log10 . ]    .",
    "sp2 corresponds to sparser model than sp1 . ]",
    "this paragraph investigates how the kronecker rank of the coefficient - matrices evolves with increasing network size .",
    "a total of 7500 points is considered in the identification batch .",
    "10 different experiments were carried out and the kronecker rank of a var1 model ranged from 1 to 4 .",
    "figure 8 displays the relative rmse as a function of the total number of phase points , i.e @xmath228 .",
    "the relative rmse decreases with increasing network size for all methods which is due to the fact that the frozen flow assumption shifts the turbulence phase from one column at every time sample and the shifted values -easily predictable- constitute the biggest part of the matrix . the ratio of new wavefront entries at each time step to the total number of phase points in the screen is equal to @xmath274 , which is the trend observed in figure 8 .",
    "moreover , we observe that the gap between the least squares solution and the kronecker estimation does widen with increasing number of points but still keeps a very reasonable relative rmse .",
    "figure 9 displays the number of entries requires to represent the model for both the standard least - squares solution and the kronecker - based representation .",
    "in this paper is defined the class of kronecker networks for which the arx modeling part is investigated .",
    "each coefficient - matrix of the var model is approximated with a sum of few kronecker matrices which offers high data compression for large networks .",
    "estimating in least - squares sense the data matrices give rise to a bilinear problem which is addressed using two methods : first a three - stage non - iterative method is derived , then a iterative alternating least squares whose convergence was proved with a non - zero initial guess .",
    "this second framework provides an elegant way of dealing with missing sensor data and adding further information on the factor matrices , e.g toeplitz , banded , sparse .",
    "numerical examples on atmospheric turbulence data demonstrates the high compression capabilities of this model as well as its scalability for larger networks .",
    "while the ordering of the nodes in the network is crucial for an efficient kronecker - representation of the coefficient matrix in the varx model , it may follow the intuition in some cases such as the one presented in the example .",
    "the stability of the kvarx model identified has not been presented in this paper and is subject of current investigations .",
    "the proof contains 4 points .",
    "1 .   positiveness : @xmath275 is positive because @xmath276 and the 2-norm for vectors @xmath277 are both positive .",
    "if @xmath278 , and the matrix @xmath214 is full rank , then @xmath279 and @xmath8 is zero .",
    "this implies that @xmath280 and therefore also the terms @xmath281 for all @xmath164 .",
    "+ for the converse , we introduce a partitioning of @xmath214 such that @xmath282 .",
    "@xmath283 each block - column @xmath65 is full column rank ( persistency of excitation ) . hence @xmath284 , and @xmath285 .",
    "3 .   let @xmath286 .",
    "@xmath287 hence , @xmath288 4 .   triangular inequality .",
    "@xmath289          1 .",
    "positiveness : @xmath275 is positive because @xmath276 and the 2-norm for vectors @xmath277 are both positive .",
    "if @xmath278 , and the matrix @xmath214 is full rank , then @xmath279 and @xmath8 is zero .",
    "this implies that @xmath280 and therefore also the terms @xmath281 for all @xmath164 .",
    "+ for the converse , we introduce a partitioning of @xmath214 such that @xmath282 .",
    "@xmath283 each block - column @xmath65 is full column rank ( persistency of excitation ) .",
    "hence @xmath284 , and @xmath285 .",
    "3 .   let @xmath286 .",
    "@xmath287 hence , @xmath288 4 .   triangular inequality .",
    "@xmath289    @xmath277 is therefore a norm .",
    "the following lemma corresponds to lemma @xmath291 in @xcite .",
    "it is used to prove that @xmath192 is a contraction mapping .",
    "it is derived straightforwardly from the definition of the norm above .",
    "let @xmath292 be defined with : @xmath293 . under assumption a2",
    ", the magnitude of the directional derivative of @xmath294 along a direction vector @xmath295 attains its maximum when @xmath295 is in the same directions as @xmath296 .",
    "it is shown that : @xmath297 key in this lemma is therefore to evaluate the term @xmath298 , which we get _ from the definition _ of the norm : @xmath299 the proof of the lemma is therefore unchanged compared with lemma @xmath291 of @xcite : the contour planes of @xmath294 are concentric spheres , and hence the gradient is in the radial direction .",
    "the proof consists of first proving that @xmath192 is an operator mapping @xmath222 to @xmath222 as @xmath224 .",
    "then the operator @xmath183 is a contraction mapping on @xmath222 , and last that the unique fixed point is equal to the true parameters @xmath300 .",
    "note that three norms are used .",
    "when @xmath8 is a vector , @xmath11 denotes the 2-norm .",
    "when @xmath301 , @xmath302 is the norm related to the inner product from definition 4 .",
    "else , the induced norm is used . as preliminary",
    ", we recall the inequality @xmath303 .",
    "we have : @xmath304 therefore : @xmath305 we start with the right - hand side term : @xmath306 @xmath176 has a finite spatial and temporal variance , hence @xmath307 is finite . since @xmath308 , @xmath309 therefore , @xmath310 @xmath311 is equal to @xmath39 if and only if @xmath312 .",
    "else , use can be made of cauchy - schwarz inequality and the fact that @xmath166 is in @xmath313 to prove that it is inferior to 1 .",
    "we conclude this first part with : @xmath314 therefore , @xmath183 goes from @xmath222 to @xmath222 .",
    "let us now study the contraction mapping , and determine an upper bound on @xmath315 .",
    "we have , for @xmath224 : @xmath316 multiplying by @xmath317 on both left sides gives : @xmath318 the right - hand side term reads : @xmath319 and hence , for all @xmath320 : @xmath321 @xmath322 being full column rank , we have : @xmath323 therefore , since @xmath324 , @xmath325 we have that @xmath326 , which then enables to prove that @xmath327 .",
    "a similar reasoning holds to prove that @xmath328 .",
    "we now introduce the quantity @xmath329 .",
    "we have : @xmath330 where @xmath331 is the composition between the vectorized operator and @xmath183 . from @xmath332 ,",
    "we decompose : @xmath333 let us start the analysis with @xmath334 , and by introducing a deviation from @xmath33 , denoted with @xmath335 : @xmath336 when @xmath337 , the difference is approximated with : @xmath338 therefore , @xmath339 where @xmath340 .",
    "moreover , @xmath341 from the update rule @xmath342 . the relationship between the norms of @xmath335 and @xmath296 ( which are vectors in the same direction ) is @xmath343 .",
    "hence : @xmath344 again using cauchy - schwarz and the fact @xmath345 .",
    "it proves that @xmath192 is a contraction map on @xmath222 .",
    "c. , jackson , k. , veran , j - p . ,",
    "andersen , d. , lardire , o. and bradley , c. static and predictive tomographic reconstruction for wide - field multi - object adaptive optics systems .",
    ", volume 31 , 2014 ."
  ],
  "abstract_text": [
    "<S> in this paper we propose a kronecker - based modeling of large networks with unknown interconnection links . </S>",
    "<S> the class of kronecker networks is defined for which we formulate a vector autoregressive model . </S>",
    "<S> its coefficient - matrices are decomposed into a sum of kronecker products . when the network is labeled such that the number of terms in the sum is small compared to the size of the matrix , exploiting this kronecker structure leads to high data compression . </S>",
    "<S> two algorithms were designed for an efficient estimation of the coefficient - matrices , namely a non - iterative and overparametrized algorithm as well as an alternating least squares minimization . </S>",
    "<S> we prove that the latter always converges to the true parameters for non - zero initial conditions . </S>",
    "<S> this framework moreover allows for a convenient integration of more structure ( e.g sparse , banded , toeplitz ) on smaller - size matrices . </S>",
    "<S> numerical examples on atmospheric turbulence data has shown comparable performances with the unstructured least - squares estimation while the number of parameters is growing _ only linearly _ w.r.t . the number of nodes instead of quadratically in the full unstructured matrix case .    </S>",
    "<S> network modeling , large - scale systems , kronecker product , low - rank approximation , multi - convex minimization . </S>"
  ]
}