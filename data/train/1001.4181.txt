{
  "article_text": [
    "in zero - delay source coding , the reconstruction of each input sample must take place at the same time instant the corresponding input sample has been encoded .",
    "zero - delay source coding is desirable in many applications , e.g. , in real - time applications where one can not afford to have large delays  @xcite , or in systems involving feedback , in which the current input depends on the previous outputs  @xcite .",
    "a weaker notion closely related to the principle behind zero - delay codes is that of causal source coding , wherein the reproduction of the present source sample _ depends _ only on the present and past source samples but not on the future source samples  @xcite .",
    "this notion does not preclude the use of non - causal entropy coding , and thus it does not guarantee zero - delay reconstruction .",
    "nevertheless , any zero - delay source code must also be causal .",
    "it is known that , in general , causal codes can not achieve the _ rate - distortion function _ ( rdf ) @xmath5 of the source , which is the _ optimal performance theoretically attainable _ ( opta ) in the absence of causality constraints  @xcite .",
    "however , it is in general not known how close to @xmath5 one can get when restricting attention to the class of causal or zero - delay source codes , except , for causal codes , when dealing with memory - less sources  @xcite , stationary sources at high resolution  @xcite , or first - order gauss - markov sources under a per - sample mse distortion metric  @xcite .    for the case of memory - less sources , it was shown by neuhoff and gilbert that the optimum rate - distortion performance of causal source codes is achieved by time - sharing at most two memory - less scalar quantizers ( followed by entropy coders )  @xcite .",
    "in this case , the rate loss due to causality was shown to be given by the space - filling loss of the quantizers , i.e. the loss is at most @xmath12 ( @xmath13 0.254 ) bits / sample .",
    "for the case of gaussian stationary sources with memory and mse distortion , gorbunov and pinsker showed that the information theoretic causal rdf , here denoted by @xmath0 and to be defined formally in section  [ sec : prelimi ] , tends to shannon s rdf as the distortion goes to zero  @xcite .",
    "the possible gap between the opta of causal source codes and this information - theoretic causal rdf was not assessed . on the other hand , for arbitrary stationary sources with finite differential entropy and under high - resolution conditions ,",
    "it was shown in  @xcite that the rate - loss of causal codes ( i.e , the difference between their opta and shannon s rdf ) is at most the space - filling loss of a uniform scalar quantizer . with the exception of memory - less sources and first - order gauss - markov sources ,",
    "the `` price '' of causality at general rate regimes for other stationary sources remains an open problem .",
    "however , it is known that for any source , the mutual information rates across an _ additive white gaussian noise _ ( awgn ) channel and across a scalar ecdq channel do not exceed @xmath5 by more than @xmath6 and @xmath14 bits per sample , respectively  @xcite ,  @xcite",
    ". this immediately yields the bounds @xmath15 and @xmath16 .    in causal source coding",
    "it is generally difficult to provide a constructive proof of achievability since shannon s random codebook construction , which relies upon jointly encoding long sequences of source symbols , is not directly applicable even in the case of memory - less sources .",
    "thus , even if one could obtain an outer bound for the achievable region based on an information theoretic rdf , finding the inner bound , i.e. , the opta , would still remain a challenge .",
    "there exist other results related to the information - theoretic causal rdf , in which achievability is not addressed .",
    "the minimum sum rate necessary to sequentially encode and decode two scalar correlated random variables under a coupled fidelity criterion was studied in  @xcite . a closed - form expression for this minimum rate",
    "is given in  ( * ? ? ?",
    "* theorem  4 ) for the special case of a squared error distortion measure and a per - variable ( as opposed to a sum or average ) distortion constraint . in  @xcite , the minimum rate for causally",
    "encoding and decoding source samples ( under per - sample or average distortion constraints ) was given the name  _ sequential rate - distortion function _",
    "( srdf ) . under a per - sample mse distortion constraint @xmath17",
    ", it was also shown in  @xcite that for a first - order gauss - markov source @xmath18 , where @xmath19 is a zero - mean white gaussian process with variance @xmath20 , the information theoretic srdf . ]",
    "@xmath21 takes the form @xmath22 for all @xmath23 .",
    "no expressions are known for @xmath21 for higher - order gauss - markov sources .",
    "also , with the exception of memory - less gaussian sources , @xmath0 , with its average mse distortion constraint ( weaker than a per - sample mse constraint ) , has not been characterized .    in this paper , we improve the existing inner and outer rate - distortion bounds for causal and for zero - delay source coding of zero - mean gaussian stationary sources and average mse distortion .",
    "we start by showing that , for any zero - mean gaussian source with bounded differential entropy rate , the causal opta exceeds @xmath0 by less than approximately @xmath24 bits / sample . then we revisit the srdf problem for first - order gauss - markov sources under a per - sample distortion constraint schedule and find the explicit expression for the corresponding rdf by means of an alternative , constructive derivation .",
    "this expression , which turns out to differ from the one found in  ( * ? ? ? * bottom of  p.  186 ) , allows us to show that for first - order gauss - markov sources , the information - theoretic causal rdf @xmath0 for an average ( as opposed to per - sample ) distortion measure coincides with  . in order to upper bound @xmath0 for general gaussian stationary sources ,",
    "we introduce the information - theoretic causal rdf when the distortion is jointly stationary with the source and denote it by @xmath3 .",
    "we then derive three closed - form upper bounding functions to the rate - loss @xmath25 , which can be applied to any stationary gaussian random process .",
    "two of these bounds are , at all rates , strictly tighter than the best previously known general bound of @xmath6 bits / sample . since , by definition , @xmath26 , we have that @xmath27 and thus all four three bounding functions also upper bound the gap @xmath28 .",
    "as we shall see , equality holds in @xmath29 if @xmath0 could be realized by a test channel with distortion jointly stationary with the source , which seems a reasonable conjecture for stationary sources .",
    "we do not provide a closed - form expression for @xmath3 ( except for first - order gauss - markov sources ) , and thus the upper bound on the _ right - hand - side _ ( rhs ) of   ( the tightest bound discussed in this paper ) is not evaluated analytically for the general case .",
    "however , we propose an iterative procedure that can be implemented numerically and which allows one to evaluate @xmath3 , for any source _ power spectral density _ ( psd ) and @xmath23 , with any desired accuracy . this procedure is based upon the iterative optimization of causal pre- , post- and feedback - filters around an awgn channel .",
    "a key result in this paper ( and its second main contribution ) is showing that such filter optimization problem is convex in the frequency responses of all the filters .",
    "this guarantees that the mutual information rate between source and reconstruction yielded by our iterative procedure converges monotonically to @xmath3 as the number of iterations and the order of the filters tend to infinity .",
    "this equivalence between the solution to a convex filter design optimization problem and @xmath30 avoids the troublesome minimization over mutual informations , thus making it possible to actually compute @xmath30 in practice , for general gaussian stationary sources .",
    "we then make the link between @xmath3 and the opta of causal and zero - delay codes . more precisely ,",
    "when the awgn channel is replaced by a subtractively dithered uniform scalar quantizer followed by memory - less entropy coding , the filters obtained with the iterative procedure yield a causal source coding system whose operational rate is below @xmath31  bits / sample .",
    "if the entropy coder in this system is restricted to encode quantized values individually ( as opposed to long sequences of them ) , then this system achieves zero - delay operation with an operational rate below @xmath32  bits / sample .",
    "this directly translates into an upper bound to the opta of zero - delay source codes , namely @xmath10 .",
    "to illustrate our results , we present an example for a zero - mean and a zero - mean gaussian source , for which we evaluate the closed - form bounds and obtain an approximation of @xmath30 numerically by applying the iterative procedure proposed herein .",
    "this paper is organized as follows : in section  [ sec : prelimi ] , we review some preliminary notions .",
    "we prove in section  [ sec : upr_bonds_for : rop_from_rit ] that the opta for gaussian sources does not exceed the information - theoretic rdf by more than approximately 0.254 bits per sample .",
    "section  [ sec : rcit(d)_1st_order ] contains the derivation of a closed - form expression for @xmath0 for first - order gauss - markov sources . in section",
    "[ sec : analytic_bounds ] we formally introduce @xmath3 and derive the three closed - form upper bounding functions for the information - theoretic rate - loss of causality .",
    "section  [ sec : convexity ] presents the iterative procedure to calculate @xmath3 , after presenting the proof of convexity that guarantees its convergence .",
    "the two examples are provided in section  [ sec : example ] .",
    "finally , section  [ sec : concl ] draws conclusions .",
    "( most of the proofs of our results are given in sections  [ sec : proof_italwaysbounds ] to  [ sec : proof_jsc()_is_convex ] . )",
    "@xmath33 and @xmath34 denote , respectively , the set of real numbers and the set of non - negative real numbers . @xmath35 and @xmath36 denote , respectively , the sets of integers and positive integers .",
    "we use non - italic lower case letters , such as @xmath37 , to denote scalar random variables , and boldface lower - case and upper - case letters to denote vectors and matrices , respectively .",
    "we use @xmath38 , @xmath39 and @xmath40 to denote the moore - penrose pseudo - inverse , the column span and the null space of the matrix @xmath41 , respectively .",
    "the expectation operator is denoted by @xmath42 .",
    "the notation @xmath43 refers to the variance of @xmath37 .",
    "the notation @xmath44 describes a one - sided random process , which may also be written simply as @xmath45 .",
    "we write @xmath46 to refer to the sequence @xmath47 .",
    "the psd of a wide - sense stationary process @xmath45 is denoted by @xmath48 .",
    "notice that @xmath49 .",
    "for any two functions @xmath50 , @xmath51 , we write the standard squared norm and inner product as @xmath52 and @xmath53 , respectively , where  @xmath54 denotes complex conjugation . for one - sided random processes @xmath45 and @xmath55 , the term @xmath56 denotes the mutual information rate between @xmath45 and @xmath55 , provided the limit exists .",
    "similarly , for a stationary random process @xmath45 , @xmath57 denotes the differential entropy rate of @xmath45 .",
    "a source _ encoder - decoder _ ( ed ) pair encodes a source @xmath58 into binary symbols , from which a reconstruction @xmath59 of @xmath44 is generated .",
    "the end - to - end effect of any ed pair can be described by a series of _ reproduction functions _ @xmath60 , such that , for every @xmath61 , @xmath62 where we write @xmath63 as a short notation for @xmath64 .",
    "following  @xcite , we say that an ed pair is _ causal _ if and only if it satisfies the following definition  @xcite :    [ def : causal_sc ] an ed pair is said to be causal if and only if its reproduction functions are such that @xmath65    it also follows from definition  [ def : causal_sc ] that an ed pair is causal if and only if the following markov chain holds for every possible random input process @xmath45 : @xmath66 it is worth noting that if the reproducing functions are random , then this equivalent causality constraint must require that   is satisfied for each realization of the reproducing functions @xmath60 .",
    "let   @xmath67 be the total number of bits that the decoder has received when it generates the output subsequence @xmath68 .",
    "define @xmath69 as the random binary sequence that contains the bits that the decoder has received when @xmath68 is generated .",
    "notice that @xmath70 is , in general , a function of all source samples , since the binary coding may be non - causal , i.e. , @xmath68 may be generated only after the decoder has received enough bits to reproduce @xmath71 , with @xmath72 .",
    "we highlight the fact that even though @xmath73 may contain bits which depend on samples @xmath74 with @xmath75 , the random sequences @xmath76 and @xmath68 may still satisfy  , i.e. , the ed pair can still be causal .",
    "notice also that @xmath67 is a random variable , which depends on @xmath76 , the functions @xmath77 and on the manner in which the source is encoded into the binary sequence sent to the decoder .    for further analysis",
    ", we define the _ average operational rate _ of an ed pair as  @xcite @xmath78 in the sequel , we focus only on the mse as the distortion measure .",
    "accordingly , we define the _ average distortion _ associated with an ed pair as : @xmath79 the above notions allow us to define the operational causal rdf as follows :    the operational causal rate - distortion function for a source @xmath45 is defined as  @xcite : @xmath80    we note that the operational causal rate distortion function defined above corresponds to the opta of all causal ed pairs . in order to find a meaningful information - theoretical counterpart of @xmath1 , we note from  ( * ? ? ?",
    "* theorem  5.3.1 ) that @xmath81 also , from the data processing inequality  @xcite , it follows immediately that @xmath82 where the last inequality turns into equality for a causal ed pair , since in that case   holds .",
    "thus , combining  ,   and  , @xmath83 this lower bound motivates the study of an information - theoretic causal rate distortion function , as defined below .",
    "[ def : causalrdf ] _ the information - theoretic causal rate - distortion function for a source @xmath45 , with respect to the average mse distortion measure , is defined as @xmath84 where the infimum is over all processes @xmath55 such that @xmath85 and such that   holds .",
    "_    the above definition is a special case of the non - anticipative epsilon - entropy introduced by pinsker and gorbunov , which was shown to converge to shannon s rdf , for gaussian stationary sources and in the limit as the rate goes to infinity  @xcite .    in the non - causal case",
    ", it is known that for any source and for any single - letter distortion measure , the opta equals the information - theoretic rdf  @xcite .",
    "unfortunately , such a strong equivalence between the opta and the information - theoretic rdf does not seem to be possible in the causal case ( i.e. , for @xmath0 ) .",
    "( one exception is if one is to jointly and causally encode an asymptotically large number of parallel gaussian sources , in which case @xmath0 can be shown to coincide with the opta of causal codes . ) nevertheless , as outlined in section  [ sec : intro ] , it is possible to obtain lower and upper bounds to the opta of causal codes from @xmath0 . indeed , and to begin with , since @xmath86",
    ", it follows directly from   and   that @xmath87 the last inequality in   is strict , in general , and becomes equality when the source is white or when the rate tends to infinity .",
    "also , as it will be shown in section  [ sec : upr_bonds_for : rop_from_rit ] , for gaussian sources @xmath1 does not exceed @xmath0 by more than approximately @xmath24 bits / sample , and thus an upper bound to @xmath1 can be obtained from @xmath0 .    for completeness , and for future reference",
    ", we recall that for any mse distortion @xmath23 , the rdf for a stationary gaussian source with psd @xmath88 is equal to the associated information - theoretic rdf , given by the `` reverse water - filling '' equations  @xcite    [ eq : r(d)waterfill ] @xmath89    although in general it is not known by how much @xmath0 exceeds @xmath5 , for gaussian stationary sources one can readily find an upper bound for @xmath0 in the quadratic gaussian rdf for source - uncorrelated distortion , defined as  @xcite @xmath90 where the infimum is taken over all output processes @xmath55 consistent with mse@xmath91 and such that the reconstruction error @xmath92 is uncorrelated with the source .",
    "more precisely , it is shown in  @xcite that this rdf , given by    [ eq : r^perp ] @xmath93\\nonumber\\end{aligned}\\ ] ] wherein @xmath94 is the only scalar that satisfies @xmath95    can be realized causally .    more generally , it is known that , for any source , the mutual information across an awgn channel ( which satisfies  ) introducing noise with variance  @xmath17 , say @xmath96 , exceeds shannon s rdf @xmath5 by at most @xmath6 bits / sample , see , e.g.  @xcite .",
    "thus , we have : @xmath97 until now it has been an open question whether a bound tighter than   can be obtained for sources with memory and at general rate regimes  @xcite . in sections  [ sec : rcit(d)_1st_order ] , [ sec : analytic_bounds ] and  [ sec : convexity ] , we show that for for gaussian sources this is indeed the case . but",
    "before focusing on upper bounds for @xmath0 , its operational importance will be established by showing in the following section that , for gaussian sources , the opta does not exceed @xmath0 by more than approximately @xmath24 bits / sample .",
    "in this section we show that , for any gaussian source @xmath45 and @xmath100 , an upper bound to @xmath98 can be readily obtained from @xmath0 by adding ( approximately ) @xmath24 bits per sample to @xmath0 .",
    "this result is first formally stated and proved for finite subsequences of any gaussian source .",
    "then , it is extended to gaussian stationary processes .",
    "we start with two definitions .",
    "the _ causal information theoretic rdf _ for a zero - mean gaussian random vector of length @xmath101 is defined as @xmath102 where the infimum is taken over all output vectors satisfying the causality constraint @xmath103 and the distortion constraint @xmath104    the _ operational causal rdf _ for a zero - mean gaussian random vector of length @xmath101 is defined as @xmath105    we will also need the following result  ( * ? ? ? * lemma  1 ) :    [ lem : zisgaussian ] let @xmath106 .",
    "let @xmath107 and @xmath108 be two random vectors with zero mean and the same covariance matrix , i.e. , @xmath109 , and having the same cross - covariance matrix with respect to @xmath110 , that is , @xmath111 . if @xmath112 and @xmath110 are jointly gaussian , and if @xmath113 has any distribution , then @xmath114\\nonumber\\end{aligned}\\ ] ] if furthermore @xmath115 , then equality is achieved in   if and only if @xmath116 with @xmath113 and @xmath110 being jointly gaussian .",
    "notice that if one applies lemma  [ lem : zisgaussian ] to a reconstruction error with which the output sequence satisfies the causality constraint  , then the gaussian version of the same reconstruction error will also produce an output causally related with the input .",
    "more precisely , if a given reconstruction error @xmath117 satisfies  , then , for all @xmath118 , it holds that @xmath119 .",
    "since @xmath120 have the same joint second - order statistics as @xmath121 , it follows that @xmath122 .",
    "this , together with the fact that @xmath123 is jointly gaussian with @xmath124 , implies that also the reconstructed sequence @xmath125 satisfies the causality constraint  .",
    "we are now in the position to state the first main result of this section :    [ lem : italwaysbounds ] for any zero - mean gaussian random vector source of length @xmath101 having bounded differential entropy , and for every @xmath23 , @xmath126    the proof of lemma  [ lem : italwaysbounds ] is presented in section  [ sec : proof_italwaysbounds ] .",
    "the result stated in lemma  [ lem : italwaysbounds ] for gaussian random vector sources is extended to gaussian stationary processes in the following theorem ( the second main result of this section ) :    [ thm : italwaysboundsproc ] for a zero - mean gaussian stationary source @xmath45 , and @xmath23 , @xmath127    the proof of theorem  [ thm : italwaysboundsproc ] can be found in section  [ sec : proof_italwaysboundsproc ] .",
    "the fact that @xmath128 for gaussian sources allows one to find upper bounds to the opta of causal codes by explicitly finding or upper bounding @xmath0 .",
    "this is accomplished in the following sections .",
    "in this section we will find @xmath0 when the source is a first - order gauss - markov process .",
    "more precisely , we will show that the information - theoretic causal rdf @xmath0 , which is associated with an average distortion constraint , coincides with the expression for the srdf on the rhs of   obtained in  @xcite for a per - sample distortion constraint .",
    "to do so , and to provide also a constructive method of realizing the srdf as well as @xmath0 , we will start by stating an alternative derivation of the srdf defined in  @xcite .    before proceeding",
    ", it will be convenient to introduce some additional notation . for any process",
    "@xmath45 , we write @xmath129 , @xmath130 , to denote the random column vector @xmath131^{t}$ ] and adopt the shorter notation @xmath132 .",
    "for any two random vectors @xmath129 , @xmath133 , we define @xmath134 , @xmath135 .",
    "it was already stated in lemma  [ lem : zisgaussian ] that the reconstruction process @xmath136 which realizes mutual information for any given mse distortion constraint , must be jointly gaussian with the source .",
    "this holds in particular for a realization of the srdf with distortion schedule @xmath137 . in the next theorem",
    "we will obtain an explicit expression for this rdf and prove that in its realization , the sample distortions @xmath138 equal the _ effective distortions _ @xmath139 , defined as    [ eq : sigsqz_must_equal ] @xmath140    moreover , it will be shown that the unique second - order statistics of this realization are given by the following recursive algorithm :    @xmath141 + figure  [ fig : matrices ] illustrates the operation of the above recursive procedure .",
    "after @xmath142 iterations , the covariance sub - matrices @xmath143 , @xmath144 have been found . at the @xmath145-th iteration ,",
    "step  @xmath146 is responsible of revealing the partial rows and columns indicated by number  @xmath146 in the figure .",
    "the above results are formally stated in the following theorem , which also gives an exact expression for the srdf of first - order gauss - markov sources .",
    "[ thm : longone ] let @xmath147 be a first - order gauss - markov source of the form @xmath148 where @xmath149 and the innovations @xmath150 are independent zero - mean gaussian random variables with variances @xmath151 and @xmath152 , respectively .",
    "then , the sequential rate distortion function ( srdf ) for @xmath147 under distortion schedule @xmath153 is given by @xmath154 where the _ effective distortions _ @xmath139 are defined in  .",
    "the unique second - order statistics of a realization of @xmath0 for this source are obtained by the recursive algorithm described in procedure  1 .",
    "the proof of this theorem can be found in section  [ sec : proof_longone ] .",
    "the expression for the srdf with per - sample distortion constraints in   differs from the one found in  @xcite for the source   with @xmath155 , @xmath156 , which in our notation reads @xmath157 wherein @xmath158 and @xmath159 .",
    "the difference lies in that the logarithms in   contain the _ effective _ distortions @xmath139 , whereas   uses the distortion constraints @xmath153 themselves .",
    "it is likely that the author of  @xcite , on page 186 , intended these distortion constraints to be the effective distortions , i.e. , that @xmath160 , for every @xmath161",
    ". however , on  ( * ? ? ?",
    "* definition  5.3.5 on p.  147 ) , the srdf under a distortion schedule is defined as the infimum of a mutual information rate subject to the constraints @xmath162 . under the latter interpretation ,",
    "nothing precludes one from choosing an arbitrarily large value for , say , @xmath163 , yielding an arbitrarily large value for the second term in the summation on the rhs of  , which is , of course , inadequate .",
    "we are now in a position to find the expression for @xmath0 for first - order gauss - markov sources .",
    "this is done in the following theorem , whose proof is contained in section  [ sec : proof_shortone ] .",
    "[ thm : shortone ] for a stationary gaussian process @xmath164 where @xmath19 is an i.i.d .",
    "sequence of zero - mean gaussian random variables with variance @xmath20 , @xmath165 with  @xmath166 , the information - theoretic causal rdf is given by @xmath167    the technique applied to prove theorems  [ thm : longone ] and  [ thm : shortone ] does not seem to be extendable to gauss - markov processes of order greater than  1 . in the sequel , we will find upper bounds to @xmath0 for arbitrary ( any order ) stationary gaussian sources .",
    "in order to upper bound the difference between @xmath0 and @xmath5 for arbitrary stationary gaussian sources , we will start this section by defining an upper bounding function for @xmath0 , denoted by @xmath30 .",
    "we will then derive three closed - form upper bounding functions to the rate - loss @xmath25 , applicable to any gaussian stationary process .",
    "two of these bounds are strictly smaller than @xmath6 bit / sample for all distortions @xmath168 .",
    "we begin with the following definition :    [ def : stat_causalrdf ] for a stationary source  @xmath45 , the information - theoretic causal stationary rate - distortion function @xmath30 is defined as @xmath169 where the infimum is over all processes @xmath55 such that :    * @xmath85 , * the reconstruction error @xmath170 is jointly stationary with the source , and * markov chain   holds .",
    "next we derive three closed - form upper bounding functions to @xmath171 that are applicable to arbitrary zero - mean stationary gaussian sources with finite differential entropy rate .",
    "this result is stated in the following theorem , proved in section  [ sec : proof_bouds_b ] :    [ thm : bouds_b ] _ let @xmath45 be a zero - mean gaussian stationary source with psd @xmath88 with bounded differential entropy rate and variance @xmath43 .",
    "let @xmath5 denote shannon s rdf for @xmath45 ( given by  ) , and let @xmath172 denote the quadratic gaussian rdf for source - uncorrelated distortions for the source @xmath45 defined in  .",
    "let @xmath0 denote the information - theoretic causal rdf ( see definition  [ def : causalrdf ] ) .",
    "then , for all @xmath173 , @xmath174 where @xmath175\\frac{s_{\\rvax}\\ejw    } { d}\\right)d\\w - r(d)\\label{eq : b3 } \\\\",
    "b_{3}(d ) & \\eq % \\begin{cases } % %   % % \\frac{1}{2}\\log_{2 } \\left(1 + d\\varsigma_{\\rvax}^{2 }",
    "\\right )    % %   %     \\frac{1}{2 } %     \\log_{2 } %     \\left ( %     ( d+\\vareps ) %     \\varsigma_{\\rvax}^{\\vareps } + [ 1-\\tfrac{d}{\\sigsq_{\\rvax}}]\\frac{d+\\vareps}{d } %     \\right ) % %   % \\ ; \\textrm{bits / sample }    & , \\textrm {   % if } d\\leq \\left ( \\varsigma_{\\rvax}^{2 } \\right)^{-1},\\\\ % 0.5 \\ ; \\textrm{bits / sample } & , \\textrm {   if } % \\left ( \\varsigma_{\\rvax}^{2}\\right)^{-1}<d<\\frac{\\sigsq_{\\rvax}}{2 } , % \\\\ % % % \\frac{1}{2}\\log_{2}\\left ( \\frac{\\sigsq_{\\rvax}}{d}\\right ) \\ ; \\textrm{bits / sample } & , \\textrm {   if }   % \\frac{\\sigsq_{\\rvax}}{2}\\leq d\\leq \\sigsq_{\\rvax}. % \\end{cases } \\min\\set {   \\frac{1}{2}\\log_{2 }     \\left((1+\\tfrac{\\vareps}{d } )     \\left [   1 + ( \\varsigma_{\\rvax}^{\\vareps } -\\tfrac{1}{\\sigsq_{\\rvax}})d \\right]\\right )     \\,,\\ ,     0.5     \\,,\\ ,     \\frac{1}{2}\\log_{2}\\left ( \\frac{\\sigsq_{\\rvax}}{d}\\right ) } ,",
    "\\label{eq : b4 } % \\ ; \\textrm { bits / sample},\\end{aligned}\\ ] ] where @xmath176 with @xmath177 being any non - negative scalar with which   exists and such that @xmath178 .",
    "_    notice that @xmath179 is independent of @xmath5 , being therefore numerically simpler to evaluate than the other bounding functions introduced in theorem  [ thm : bouds_b ] .",
    "however , as @xmath17 is decreased away from @xmath43 and approaches @xmath180 , @xmath179 becomes very loose .",
    "in fact , it can be seen from   that for @xmath181 , the gap between @xmath0 and @xmath5 is actually upper bounded by @xmath182 , which is of course tighter than @xmath179 , but requires one to evaluate @xmath5 .",
    "it is easy to see that time - sharing between two causal realizations with distortions @xmath163 , @xmath183 and rates @xmath184 , @xmath185 yields an output process which satisfies causality with a rate - distortion pair corresponding to the linear combination of @xmath184 , @xmath185 .",
    "thus , in some cases one could get a bound tighter than @xmath186 by considering the boundary of the convex hull of the region above @xmath187 and then subtracting @xmath5",
    ". however , such bound would be much more involved to compute , since it requires to evaluate not only @xmath5 , but also the already mentioned convex hull .",
    "it is also worth noting that the first term within the @xmath188 operator on the rhs of   becomes smaller when @xmath189 is reduced .",
    "this difference , which from jensen s inequality is always non - negative , could be taken as a measure of the `` non - flatness '' of the psd of @xmath45 ( specially when @xmath190 ) . indeed , as @xmath45 approaches a white process , @xmath186 tends to zero .",
    "it can be seen from   that @xmath30 provides the tightest upper bound for the information - theoretic rdf among all bounds presented so far .",
    "although it does not seem to be feasible to obtain a closed - form expression for @xmath30 , we show in the next section how to get arbitrarily close to it .",
    "in this section we present an iterative procedure that allows one to calculate @xmath30 with arbitrary accuracy , for any @xmath23 .",
    "in addition , we will see that this procedure yields a characterization of the filters in a dithered feedback quantizer  @xcite that achieve an operational rate which is upper bounded by @xmath191  [ bits / sample ] .      to derive the results mentioned above , we will work on a scheme consisting of an awgn channel and a set of causal filters , as depicted in fig .",
    "[ fig : causalfq ] .    in this scheme , the source @xmath45 is gaussian and stationary , with psd @xmath88 , and is assumed to have finite differential entropy rate . in fig .",
    "[ fig : causalfq ] , the noise @xmath192 is a zero - mean gaussian process with i.i.d .",
    "samples , independent of @xmath45 .",
    "thus , between @xmath193 and @xmath194 lies the awgn channel @xmath195 .",
    "the filter @xmath196 is stable and strictly causal , i.e. , it has at least a one sample delay .",
    "the filters @xmath197 and @xmath198 are causal and stable .",
    "the idea , to be developed in the remainder of this section , is to first show that with the filters that minimize the variance of the reconstruction error for a fixed ratio @xmath199 , the system of fig .",
    "[ fig : causalfq ] attains a mutual information rate between source and reconstruction equal to @xmath30 , with a reconstruction mse equal to @xmath17 .",
    "we will then show that finding such filters is a convex optimization problem , which naturally suggests an iterative procedure to solve it .    in order to analyze the system in fig .",
    "[ fig : causalfq ] , and for notational convenience , we define @xmath200 we also restrict the filters @xmath197 and @xmath198 to satisfy the `` perfect reconstruction '' condition @xmath201 thus , @xmath202 \\rvan(k),\\end{aligned}\\ ] ] see fig .",
    "[ fig : causalfq ] .",
    "therefore , @xmath203 is the signal transfer function of the system .",
    "the perfect reconstruction condition   induces a division of roles in the system , which will later translate into a convenient parametrization of the optimization problem associated with it . on the one hand , because of  , the net effect of the awgn channel and the filters @xmath197 , @xmath198 and @xmath196 is to introduce ( coloured ) gaussian stationary additive noise , namely @xmath204 , independent of the source .",
    "the psd of this noise , @xmath205 , is given by @xmath206 the diagram in figure  [ fig : roles ] shows how the signal transfer function @xmath203 and the noise transfer function @xmath207 act upon @xmath45 and @xmath192 to yield the output process .",
    "on the other hand , by looking at fig .",
    "[ fig : causalfq ] one can see that @xmath203 plays also the role of a de - noising filter , which can be utilized to reduce additive noise at the expense of introducing linear distortion .",
    "more precisely , @xmath203 acts upon the gaussian stationary source @xmath45 corrupted by additive gaussian stationary noise with psd @xmath208 . from   and",
    "[ fig : causalfq ] , the mse is given by @xmath209 where @xmath210 and @xmath211 on the rhs of  , the first term is the variance of the additive , source independent , gaussian noise .",
    "the second term corresponds to the error due to linear distortion , that is , from the deviation of @xmath212 from a unit gain . since we will be interested in minimizing @xmath213 , for any given @xmath196 and @xmath203 , the filters @xmath197 and @xmath198 in fig .",
    "[ fig : causalfq ] are chosen so as to minimize  @xmath214 in  , while still satisfying  . from the viewpoint of the subsystem comprised of the filters @xmath197 , @xmath198 and @xmath196 and the awgn channel , @xmath203 acts as an error frequency weighting filter ,",
    "thus , for any @xmath196 and @xmath203 , the filters @xmath197 and @xmath198 that minimize @xmath214 are those characterized in  ( * ? ? ?",
    "* prop .  1 ) , by setting @xmath215 in  (",
    "* eq .  ( 20b ) ) equal to @xmath203 . with the minimizer filters in  @xcite , the variance of the source - independent error term is given by @xmath216 on the other hand , the filter @xmath196 needs to be strictly causal and stable . as a consequence",
    ", it holds that @xmath217 which follows from jensen s formula  @xcite ( see also the bode integral theorem in , e.g. ,  @xcite ) .",
    "thus , from   and  , if one wishes to minimize the reconstruction mse by choosing appropriate _ causal _ filters in the system in fig .  [",
    "fig : causalfq ] for a given value of @xmath218 , one needs to solve the following optimization problem :    [ opprob : fw ] for any given @xmath219 , and for any given @xmath220 , find the frequency response @xmath212 and the frequency response magnitude @xmath221 that    @xmath222    where @xmath223 denotes the space of all frequency responses that can be realized with causal filters .",
    "now we can establish the equivalence between solving optimization problem  [ opprob : fw ] and finding @xmath30 .",
    "[ lem : filters_realize_rc ] for any @xmath220 and @xmath219 , if the filters @xmath224 , @xmath225 , and @xmath226 solve optimization problem  [ opprob : fw ] and yield distortion @xmath227 , then @xmath228    from the above lemma , whose proof can be found in section  [ sec : proof_filters_realize_rc ] , one can find @xmath30 either by solving the minimization in definition  [ def : stat_causalrdf ] or by solving optimization problem  [ opprob : fw ] . in the following , we will pursue the latter approach . as we shall see , our formulation of optimization problem  [ opprob : fw ] provides a convenient parametrization of its decision variables .",
    "in fact , it makes it possible to establish the convexity of the cost functional defined in   with respect to the set of all causal frequency responses involved .",
    "that result can be obtained directly from the following key lemma , proved in section  [ sec : proof_jsc()_is_convex ] :    [ lem : jsc(fg)_is_convex ] define the sets of functions @xmath229 where @xmath218 is some positive constant .",
    "then , for any @xmath230 and @xmath220 , the cost functional @xmath231 , defined as @xmath232 is strictly convex in @xmath233 and @xmath234 .",
    "we can now prove the convexity of optimization problem  [ opprob : fw ] .",
    "[ lem : jsc()_is_convex ] for all @xmath235 and for all @xmath220 , optimization problem  [ opprob : fw ] is convex .    with the change of variables @xmath236 and @xmath237 in  ,",
    "we obtain @xmath238 , see  . with this",
    ", optimization problem  [ opprob : fw ] amounts to finding the functions @xmath233 and @xmath234 that    [ eq : opprob_fg ] @xmath239    where @xmath240 clearly , the space of frequency responses associated with causal transfer functions , @xmath223 , is a convex set .",
    "this implies that @xmath241 is a convex set .",
    "in addition , @xmath242 is also a convex set , and from lemma  [ lem : jsc(fg)_is_convex ] , @xmath243 is a convex functional",
    ". therefore , the optimization problem stated in  , and thus optimization problem  [ opprob : fw ] , are convex .",
    "this completes the proof .",
    "lemma  [ lem : jsc()_is_convex ] and the parametrization in optimization problem  [ opprob : fw ] allow one to define an iterative algorithm that , as will be shown later , yields the information - theoretic causal rdf .",
    "such algorithm is embodied in iterative procedure  2 :    @xmath141 + [ anchor ]    @xmath141    notice that after solving step 3 in the first iteration of procedure  2 , the mse is comprised of only additive noise independent of the source . introduced in  @xcite ( see also  ) . ]",
    "step 4 then reduces the mse by attenuating source - independent noise at the expense of introducing linear distortion .",
    "each step reduces the mse until a local ( or global ) minimum of the mse is obtained . based upon the convexity of optimization problem  [ opprob : fw ] , the following theorem , which is the main technical result in this section , guarantees convergence to the global minimum of the mse , say @xmath17 , for a given end - to - end mutual information . since all the filters in optimization problem  [ opprob : fw ] are causal , the mutual information achieved at this global minimum is equal to @xmath30 .",
    "[ thm : convergence ] iterative procedure  2 converges monotonically to the unique @xmath233 and @xmath244 that realize @xmath30 .",
    "more precisely , letting @xmath245 denote the mse obtained after the @xmath246-th iteration of iterative procedure  2 aimed at a target rate @xmath247 , we have that@xmath248 and @xmath249    the result follows directly from the fact that optimization problem  [ opprob : fw ] is strictly convex in @xmath233 and @xmath244 , which was shown in lemma  [ lem : jsc(fg)_is_convex ] , and from lemma  [ lem : filters_realize_rc ] .",
    "the above theorem states that the stationary information - theoretic causal rdf can be obtained by using iterative procedure  2 . in practice",
    ", this means that an approximation arbitrarily close to @xmath30 for a given @xmath17 can be obtained if sufficient iterations of the procedure are carried out .",
    "the feasibility of running iterative procedure  2 depends on being able to solve each of the minimization sub - problems involved in steps  3 and  4 .",
    "we next show how these sub - problems can be solved .      if @xmath212 is given , the minimization problem in step  3 of iterative procedure  2 is equivalent to solving a feedback quantizer design problem with the constraint @xmath250 and with error weighting filter @xmath212 . therefore , the solution to step  3 is given in closed form by  ( * ? ? ?",
    "( 20 ) ,  ( 29 ) and ( 31b ) ) , where @xmath215 in  ( * ? ? ?",
    "* eq .  ( 20b ) ) is replaced by @xmath203 .",
    "the latter equations of  @xcite characterize the frequency response magnitudes of the optimal @xmath197 , @xmath198 and @xmath251 given @xmath203 .",
    "the existence of rational transfer functions @xmath197 , @xmath198 and @xmath196 arbitrarily close ( in an @xmath252 sense ) to such frequency response magnitudes is also shown in  @xcite .      finding the causal frequency response @xmath253 that minimizes @xmath213 for a given @xmath233 is equivalent to solving @xmath254 for a given @xmath233 , where @xmath241 is as defined in  . since @xmath241 and @xmath255 are convex ,   is a convex optimization problem . as such , its global solution",
    "can always be found iteratively . in particular , if @xmath203 is constrained to be an @xmath256-th order fir filter with impulse response @xmath257 , such that @xmath258 , where @xmath259 denotes the discrete - time fourier transform , then @xmath260 is a convex functional .",
    "the latter follows directly from the convexity of @xmath255 and the linearity of  @xmath259 . as a consequence",
    ", one can solve the minimization problem in step 4 , to any degree of accuracy , by minimizing @xmath261 over the values of the impulse response of @xmath212 , using standard convex optimization methods ( see , e.g ,  @xcite ) .",
    "this approach also has the benefit of being amenable to numerical computation .",
    "it is interesting to note that if the order of the de - noising filter @xmath203 were not a priori restricted , then , after iterative procedure  2 has converged to @xmath30 , the obtained @xmath203 is the causal wiener filter ( i.e. , the mmse causal estimator ) for the noisy signal that comes out of the perfect reconstruction system that precedes @xmath203 .",
    "notice also that one can get the system in fig .",
    "[ fig : filters ] to yield a realization of shannon s @xmath5 using iterative procedure  1 by simply allowing @xmath203 to be non - causal .",
    "this would yield a system equivalent to the one that was obtained analytically in  @xcite .",
    "an important observation is that one could not obtain a realization of @xmath0 from such a system in one step by simply replacing @xmath203 ( a non - causal wiener filter ) by the mmse causal estimator ( that is , a causal wiener filter ) . to see this",
    ", it suffices to notice that , in doing so , the frequency response magnitude of @xmath203 would change . as a consequence",
    ", the previously matched filters @xmath197 , @xmath198 and @xmath196 would no longer be optimal for @xmath203 .",
    "one would then have to change @xmath197 , and then @xmath203 again , and so on , thus having to carry out infinitely many recursive optimization steps .",
    "however , a causally truncated version of the non causal wiener filter @xmath203 that realizes shannon s rdf could be used as an alternative starting guess in step  2 of the iterative procedure .      if the awgn channel in the system of fig .",
    "[ fig : causalfq ] is replaced by a _",
    "subtractively dithered uniform scalar quantizer _ ( sdusq ) , as shown in fig .",
    "[ fig : sdusq_with_filters ] ,    then instead of the noise @xmath192 we will have an i.i.d .",
    "process independent of @xmath45 , whose samples are uniformly distributed over the quantization interval  @xcite .",
    "the dither signal , denoted by @xmath262 , is an i.i.d .",
    "sequence of uniformly distributed random variables , independent of the source .",
    "let @xmath263 be the quantized output of the sdusq .",
    "denote the resulting input and the output to the quantizer , before adding and after subtracting the dither , respectively , as @xmath264 and @xmath265 , and let @xmath266 be the quantization noise introduced by the sdusq .",
    "notice that the elements of @xmath267 are independent , both mutually and from the source @xmath45 .",
    "however , unlike @xmath268 and @xmath269 , the processes @xmath264 and @xmath270 are not gaussian , since they contain samples of the uniformly distributed process @xmath267 .",
    "we then have the following :    [ thm : causal_bound_with_ecdq ] if the scheme shown in fig .",
    "[ fig : sdusq_with_filters ] uses the filters yielded by iterative procedure  2 , and if long sequences of the quantized output of this system are entropy coded conditioned to the dither values in a memoryless fashion , then an operational rate @xmath271 satisfying @xmath272 is achieved causally while attaining a reconstruction mse equal to @xmath17 .    if memoryless entropy coding is applied to long sequences of symbols conditioning the probabilities to dither values , then then operational rate equals the conditional entropy @xmath273 . for this entropy ,",
    "the following holds in the system shown in fig .",
    "[ fig : sdusq_with_filters ] : @xmath274 where @xmath275 denotes the entropy of @xmath276 conditioned to the @xmath145-th value of the dither signal . in the above",
    ",  @xmath29 follows from  ( * ? ? ?",
    "* theorem  1 ) . in turn",
    ",  @xmath277 stems from the well known result @xmath278 , where @xmath279 denotes the kullback - leibler distance , see , e.g. ,  @xcite .",
    "the inequality in the last line of   is strict since the distribution of @xmath280 is not gaussian .",
    "the result follows directly by combining   with lemma  [ lem : filters_realize_rc ] and theorem  [ thm : convergence ] .    in view of theorem  [ thm : causal_bound_with_ecdq ] , and since any ed pair using an sdusq and lti filters yields a reconstruction error jointly stationary with the source , it follows that the operational rate - distortion performance of the feedback quantizer thus obtained is within @xmath281 bits / sample from the best performance achievable by any ed pair within this class .",
    "[ rem : highrate ] when the rate goes to infinity , so does @xmath218 . in that limiting case",
    ", the transfer function @xmath203 tends to unity , and it follows from  @xcite that the optimal filters asymptotically satisfy @xmath282 , @xmath283 , @xmath284 .",
    "moreover , when @xmath285 , the system of fig .",
    "[ fig : sdusq_with_filters ] achieves @xmath1 which , in this asymptotic regime , coincides with @xmath286 , with @xmath30 tending to @xmath5 .",
    "if the requirement of zero - delay , which is stronger than that of causality , was to be satisfied , then it would not be possible to apply entropy coding to long sequences of quantized samples .",
    "this would entail an excess bit - rate not greater than @xmath288 bit per sample , see , e.g. ,  ( * ? ? ?",
    "* section  5.4 ) .",
    "consequently , we have the following result :    [ thm : zd_bound_with_ecdq ] the opta of zero - delay codes , say @xmath289 , can be upper bounded by the operational rate of the scheme of fig .  [",
    "fig : sdusq_with_filters ] when each quantized output value is entropy - coded independently , conditioned to the current dither value .",
    "thus @xmath290    the @xmath24 bits per sample in  , commonly referred to as the `` space - filling loss '' of scalar quantization , can be reduced by using vector quantization  @xcite .",
    "vector quantization could be applied while preserving causality ( and without introducing delay ) if the samples of the source were @xmath291-dimensional vectors .",
    "this would also allow for the use of entropy coding over @xmath291-dimensional vectors of quantized samples , which reduces the extra @xmath288 bit / sample at the end of   to @xmath292 bits / sample , see  ( * ? ? ?",
    "* theorem  5.4.2 ) .",
    "it is worth noting that lemma  [ lem : filters_realize_rc ] and the above analysis reveals an interesting fact : the rate loss due to causality for gaussian sources with memory , that is , the difference between the opta of causal codes and @xmath5 , is upper bounded by the sum of two terms .",
    "the first term is @xmath24 bits / sample , and results from the space filling loss associated with scalar quantization , as was also pointed out in  @xcite for the high resolution situation .",
    "this term is associated only with the _ encoder_. for a scalar gaussian stationary source , such excess rate can only be avoided by jointly quantizing blocks of consecutive source samples ( vector quantization ) , i.e. , by allowing for non - causal encoding ( or by encoding several parallel sources ) .",
    "the second term can be attributed to the reduced de - noising capabilities of causal filters , compared to those of non - causal ( or smoothing ) filters .",
    "the contribution of the causal filtering aspect to the total rate - loss is indeed @xmath171 .",
    "this latter gap can also be associated with the performance loss of causal _",
    "decoding_.    as a final remark , we note that the architecture of fig .",
    "[ fig : causalfq ] , which allowed us to pose the search of @xmath293 as a convex optimization problem , is by no means the only scheme capable of achieving the upper bounds   and  . for instance",
    ", it can be shown that the same performance can be attained removing either @xmath197 or @xmath196 in the system of fig .",
    "[ fig : causalfq ] , provided an entropy coder with infinite memory is used .",
    "indeed , the theoretical optimality ( among causal codes ) of the differential pulse code modulation ( dpcm ) architecture , with predictive feedback and causal mmse estimation at the decoding end , has been shown in a different setting  @xcite .",
    "to illustrate the upper bounds presented in the previous sections , we here evaluate @xmath294 , @xmath295 , and @xmath179 , and calculate an approximation of @xmath30 via iterative procedure  2 , for two gaussian zero - mean ar-1 and ar-2 sources .",
    "these sources were generated by the recursion@xmath296 where the elements of the process @xmath297 are i.i.d .",
    "zero - mean unit - variance gaussian random variables .",
    "iterative procedure  2 was carried out by restricting @xmath203 to be an 8-tap fir filter .",
    "for each of the target rates considered , the procedure was stopped after four complete iterations .",
    "the first - order source ( source  1 ) was chosen by setting the values of the coefficients in   to be @xmath298 , @xmath299 .",
    "this amounts to zero - mean , unit variance white gaussian noise filtered through the colouring transfer function @xmath300 .",
    "the second - order source ( source  2 ) consisted of zero - mean , unit variance white gaussian noise filtered through the colouring transfer function @xmath301 $ ] .",
    "the resulting upper bounds for source  1 and source  2 are shown in figs .",
    "[ fig : bounds_09 ] and  [ fig : bounds_0901 ] , respectively .    as predicted by   and  ,",
    "all the upper bounds for @xmath0 derived in section  [ sec : analytic_bounds ] converge to @xmath5 in the limit of both large and small distortions ( that is , when @xmath302 and @xmath303 , respectively ) .    for both sources ,",
    "the gap between @xmath30 and @xmath5 is significantly smaller than @xmath6 bits / sample , for all rates at which @xmath30 was evaluated .",
    "indeed , this gap is smaller than @xmath304 bit / sample for both sources .",
    "for the first - order source , the magnitude of the coefficients of the fir filter @xmath203 obtained decays rapidly with coefficient index .",
    "for example , when running five cycles of iterative procedure  2 , using a 10th order fir filter for  @xmath203 , for source  1 at @xmath305 bits / sample , the obtained @xmath203 was @xmath306 such fast decay of the impulse response of @xmath203 suggests that , at least for ar-1 sources , there is little to be gained by letting @xmath203 be an fir filter of larger order .",
    "( it is worth noting that , in the iterative procedure , the initial guess for @xmath203 is a unit scalar gain . )",
    "the frequency response magnitude of @xmath203 is plotted in fig .",
    "[ fig : filters ] , together with @xmath219 and the resulting frequency response magnitude @xmath307 after four iterations on source  1 for a target rate of @xmath308 bits / sample .    ,",
    "@xmath307 and @xmath309 of an approximate realization of @xmath30 for a gaussian stationary source with psd @xmath310 when the rate is @xmath311 [ bit / sample ] , using the system shown in fig .",
    "[ fig : causalfq ] .",
    "these frequency responses were obtained after four iterations of iterative procedure  1 , with filter @xmath203 being fir with 8 taps . ]",
    "notice that for source  1 , after four iterations of iterative procedure  1 , the obtained values for @xmath30 are almost identical to @xmath0 , evaluated according to  .",
    "this suggests that iterative procedure  2 has fast convergence .",
    "for example , when applying four iterations of iterative procedure  2 to source  1 with a target rate of @xmath311 bits / sample , the distortions obtained after each iteration were @xmath312 , @xmath313 , @xmath314 and @xmath314 , respectively . for the same source with a target rate of @xmath315 bits / sample , the distortion took the values @xmath316 , @xmath317 , @xmath317 , and @xmath318 as the iterations proceeded .",
    "a similar behaviour is observed for other target rates , and for other choices of @xmath319 in   as well .",
    "thus , at least for ar-1 sources , one gets close to the global optimum @xmath30 after just three iterations .",
    "in this paper we have obtained expressions and upper bounds to the causal and zero - delay rate distortion function for gaussian stationary sources and mse as the distortion measure .",
    "we first showed that for gaussian sources with bounded differential entropy rate , the causal opta does not exceed the information - theoretic rdf by more than approximately  @xmath24 bits / sample .",
    "after that , we derived an explicit expression for the information - theoretic rdf under per - sample mse distortion constraints using a constructive method .",
    "this result was then utilized for obtaining a closed - form formula for the causal information - theoretic rdf @xmath0 of first - order gauss - markov sources under an average mse distortion constraint .",
    "we then derived three closed - form upper bounding functions to the difference between @xmath0 and shannon s rdf .",
    "two of these bounding functions are tighter than the previously best known bound of @xmath6 bits / sample , at all rates .",
    "we also provided a tighter fourth upper bound to @xmath320 , named @xmath3 , that is constructive .",
    "more precisely , we provide a practical scheme that attains this bound , based on a noise - shaped predictive coder consisting of an awgn channel surrounded by pre- , post- , and feedback filters . for a given source spectral density and desired distortion , the design of the filters is convex in their frequency responses .",
    "we proposed an iterative algorithm , which is guaranteed to converge to the optimal set of unique filters .",
    "moreover , the mutual information obtained across the awgn channel , converges monotonically to @xmath3 .",
    "thus , one avoids having to solve the more complicated minimization of the mutual information over all possible conditional distributions satisfying the distortion constraint . to achieve the upper bounds on the operational coding rates ,",
    "one may simply replace the awgn channel by a subtractively - dithered scalar quantizer and using memoryless entropy coding conditioned to the dither values .",
    "we will first show that @xmath321 can be realized by a vector awgn channel between two square matrices .",
    "it was already established in lemma  [ lem : zisgaussian ] that an output @xmath322 corresponds to a realization of @xmath323 only if it is jointly gaussian with the source @xmath110 . from this gaussianity condition ,",
    "the mmse estimator of @xmath322 from @xmath110 , say @xmath324 , is given by @xmath325 where the inverse of @xmath326 exists from the fact that @xmath110 has bounded differential entropy .",
    "it is clear from   and the joint gaussianity between @xmath110 and @xmath322 that the causality condition is satisfied if and only if the matrix @xmath327 on the other hand , the distortion constraint   can be expressed as @xmath328 from the definition of @xmath323 , for every @xmath329 , there exists an output vector @xmath322 jointly gaussian with @xmath110 such that @xmath330 and @xmath331 satisfy  ,   and @xmath332 we will now describe a simple scheme which is capable of reproducing the joint statistics between @xmath110 and any given @xmath322 jointly gaussian with @xmath110 satisfying  ,   and  .",
    "suppose @xmath110 is first multiplied by a matrix @xmath333 yielding the random vector @xmath334 .",
    "then a vector with gaussian i.i.d .",
    "entries with unit variance , independent from @xmath110 , say @xmath335 , is added to @xmath336 , to yield the random vector @xmath337 .",
    "finally , this result is multiplied by a matrix @xmath338 to yield the output @xmath339 on the other hand , the joint second - order statistics between @xmath322 and @xmath110 are fully characterized by the matrices @xmath340 it can be seen from these equations that all that is needed for the system described above to reproduce any given pair of covariance matrices @xmath330 , @xmath331 is that the matrices @xmath341 and @xmath342 satisfy @xmath343 thus , @xmath344 can be chosen , for example , as the lower - triangular matrix in a cholesky factorization of @xmath345 . with this , a tentative solution for @xmath41 could be obtained as @xmath346 , which would satisfy   if and only if @xmath347 . the latter holds if and only if @xmath348 ( recall that @xmath326 is non - singular since @xmath110 has bounded differential entropy ) .",
    "we will now show that this condition actually holds by using a contradiction argument .",
    "suppose @xmath349 .",
    "since @xmath350 , the former supposition is equivalent to @xmath351 . if this were the case , then there would exist @xmath352 such that @xmath353 and @xmath354 .",
    "the latter , combined with  , would imply @xmath355 .",
    "one could then construct the scalar random variable @xmath356 , which would have non - zero variance .",
    "the mse of predicting @xmath357 from @xmath110 is given by @xmath358 from this , and in view of the fact that @xmath357 is gaussian with non - zero variance , we conclude that @xmath359 would be unbounded .",
    "however , by construction , the markov chain @xmath360 holds , and therefore by the data processing inequality we would have that @xmath361 , implying that @xmath362 is unbounded too .",
    "this contradicts the assumption that @xmath322 is a realization of @xmath321 , leading to the conclusion that @xmath363 .",
    "therefore , the choice @xmath364 is guaranteed to satisfy  , and thus for every @xmath329 , there exist matrices @xmath344 and @xmath41 which yield an output vector satisfying  ,   and",
    ". on the other hand , we have that @xmath365 the first equality follows from the data - processing inequality and the fact that @xmath336 is obtained deterministically from @xmath110 .",
    "the second equality stems from  , which implies that @xmath366 .",
    "the latter means that @xmath344 is invertible along all the directions in which @xmath336 has energy , which together with the fact that @xmath367 is i.i.d . and independent of @xmath368 implies @xmath369 . therefore , if @xmath341 and @xmath342 yield an output @xmath322 such that @xmath370 , then @xmath371 .    finally ,",
    "if we keep the @xmath41 and @xmath344 satisfying the above conditions and replace the noise @xmath367 by the vector of noise samples @xmath372 with unit variance introduced by @xmath101 independently operating subtractively - dithered uniform scalar quantizers ( sduqs )  @xcite , with their outputs being jointly entropy - coded conditioned to the dither , then the operational data rate @xmath373 would be upper bounded by  @xcite @xmath374 where @xmath375 is the output of the ecdq channel . since the distortion yielded by the sduqs is the same as that obtained with the original gaussian channel , we conclude that @xmath376 given that the above holds for any @xmath329 and since @xmath377 is defined as an infimum , we conclude that @xmath378 , which completes the proof .",
    "we will start by showing that @xmath379 first , following exactly the same proof as in lemma  [ lem : inflingeqliminf ] in the appendix , it is straightforward to show that @xmath380 now , consider the following family of encoding / decoding schemes .",
    "for some positive integer @xmath101 , the entire source sequence is encoded in blocks of @xmath101 contiguous samples .",
    "encoding and decoding of each block is independent of the encoding and decoding of any other block . as in the scheme described in the second part of the proof of lemma  [ lem : italwaysbounds ] , each block is encoded and decoded utilizing @xmath101 parallel and independent sdusqs , with their outputs jointly entropy coded conditioned to the dither values , and using with the optimal pre- and post - processing matrices .",
    "for such an ed pair , and from  , the operational rate after @xmath145 samples have been reconstructed is @xmath381 where @xmath382 denotes rounding to the nearest larger integer ( since the @xmath145-th sample is reconstructed only after @xmath383 blocks of length @xmath101 are decoded ) . on the other hand , since the variance of each reconstruction error sample can not be larger than the variance of the source , we have that the average distortion associated with the first @xmath145 samples is upper bounded as @xmath384 where @xmath385 denotes rounding to the nearest smaller integer . therefore , for any finite @xmath101 , the average distortion of this scheme equals @xmath17 when @xmath386 ( i.e. , when we consider the entire source process ) . also ,",
    "from   and  , letting @xmath386 we conclude that @xmath387 if @xmath388 exists , then , for every @xmath329 , there exists a finite @xmath389 such that @xmath390 therefore , every @xmath329 , there exists a finite @xmath389 such that @xmath391 since @xmath1 is defined as an infimum among all causal codes ( which , in particular , means @xmath101 can be chosen larger than @xmath392 for any @xmath329 ) , it readily follows from  ,  , lemma  [ lem : italwaysbounds ] and lemma  [ lem : limusp ] , that @xmath393 completing the proof .",
    "from lemma  [ lem : zisgaussian ] , for any given reconstruction - error covariance matrix , the mutual information is minimized if and only if the output is jointly gaussian with the source .",
    "in addition , for any given mutual information between @xmath124 and a jointly gaussian output @xmath136 , the variance of every reconstruction error sample @xmath394 is minimized if and only if @xmath395 is the estimation error resulting from estimating @xmath396 from @xmath397 , that is , if and only if @xmath398 which for gaussian vectors implies @xmath395 and @xmath397 are independent , and therefore @xmath399 thus , hereafter we restrict the analysis to output processes jointly gaussian with and causally related to @xmath124 which also satisfy  . for any such output process ,",
    "say , @xmath136 , the following holds : @xmath400 \\\\ & = \\frac { h(\\rvax(1 ) ) -   h(\\rvaz(1 ) ) } { \\ell } + \\frac{1}{\\ell } \\sumfromto{k=2}{\\ell } \\left[h(a_{k-1}\\rvax(k-1)+\\xi(k-1)|\\rvay^{k-1 } ) -   h(\\rvaz(k))\\label{eq : first } \\right ] \\\\ & % = \\frac{1}{2\\ell } \\ln\\left(\\frac{\\sigsq_{\\rvax(1)}}{\\sigsq_{\\rvaz(1 ) } } \\right ) + \\frac{1}{\\ell } \\sumfromto{k=2}{\\ell } \\left[h(-a_{k-1}\\rvaz(k-1)+\\xi(k-1 ) ) -   h(\\rvaz(k ) ) \\right]\\label{eq : refirst } \\\\ & = \\frac{1}{2\\ell } \\ln\\left(\\frac{\\sigsq_{\\rvax(1)}}{\\sigsq_{\\rvaz(1 ) } } \\right ) + \\frac{1}{2\\ell } \\sumfromto{k=2}{\\ell } \\ln\\left (   \\frac{a_{k-1}^{2 } \\sigsq_{\\rvaz(k-1 ) } + \\sigsq_{\\xi(k-1 ) }   } { \\sigsq_{\\rvaz(k ) } } \\right)\\label{eq : jane'saddiction}\\end{aligned}\\ ] ] in the above ,   follows because @xmath136 depends causally upon @xmath124 . in turn , inequality   is due to the fact that @xmath401 , and thus equality holds in   if and only if the following markov chain is satisfied : @xmath402 finally ,   and   follow because @xmath136 satisfies   for all @xmath161 .",
    "thus , the mutual information @xmath403 of every output @xmath136 that is a candidate to constitute a realization of @xmath404 is lower bounded by the rhs of  , which in turn depends only on the error variances @xmath405 associated with @xmath136 .",
    "we shall now see that this lower bound is minimized by a unique set of error variances , and then show that the resulting bound is achievable while having these error variances . revisiting   and  , we have that @xmath406/\\sigsq_{\\rvaz(k ) } ) = h(\\rvax(k)|\\rvay^{k-1 } ) - h(\\rvax(k)|\\rvay^{k } ) \\geq 0 $ ] and @xmath407 .",
    "therefore , in a realization of @xmath408 , it holds that    [ eq : subeq_eff_sigmas ] @xmath409    with this , and since the right - hand side of   decreases when any error variance  @xmath410 increases , the minimum value of the right - hand side of   subject to the constraints @xmath411 is attained when these variances satisfy @xmath412 , for @xmath161 ( see  ) .",
    "therefore , for all outputs @xmath136 causally related to and jointly gaussian with @xmath124 satisfying the distortion constraints , it holds that @xmath413 with equality if and only if @xmath136 satisfies  ,   and  .",
    "now we will show that for any distortion schedule @xmath153 , the output @xmath136 yielded by the recursive algorithm of procedure  1 is such that @xmath403 equals the lower bound  , thus being a realization of @xmath404 .",
    "we will first demonstrate that @xmath55 satisfies the causality markov chain @xmath414 and the conditions   ( mmse ) , and   ( source s past independence ) which are necessary and sufficient to attain equality in  .",
    "[ [ causality - condition ] ] causality condition   + + + + + + + + + + + + + + + + + + + +    let @xmath415 .",
    "suppose @xmath416 satisfies causality .",
    "then , since @xmath417 , it follows from   that the top - left square submatrix @xmath418 of @xmath41 is lower triangular , being given by @xmath419 then step  2 of the algorithm is equivalent to @xmath420 this means that the top  @xmath421 entries in the @xmath145-th column of @xmath422 depend only on the entries of @xmath423 above its @xmath145-th row .",
    "recalling that @xmath417 , we conclude that @xmath41 is also lower triangular , and thus @xmath424 also satisfies causality .",
    "notice that for any given @xmath425 and @xmath143 satisfying causality up to sample @xmath142 , the vector @xmath426 yielded by step  2 is the only vector consistent with @xmath427 satisfying causality up to the @xmath145-th sample .",
    "[ [ mmse - condition ] ] mmse condition   + + + + + + + + + + + + + + +    step  1 guarantees that   is satisfied for @xmath428 .",
    "steps 3 , 4 and 5 mean that @xmath429 for all @xmath430 .",
    "therefore , the reconstruction vector @xmath431 yielded by the above algorithm satisfies   for all @xmath161 .",
    "[ [ sources - past - independence ] ] source s past independence   + + + + + + + + + + + + + + + + + + + + + + + + + + +    since all variables are jointly gaussian , condition   is equivalent to @xmath432 for all @xmath433 . on the other hand , @xmath434 }   \\left ( \\begin{matrix }   \\bk_{\\rvey^{1}_{k-1 } }                   & \\expe{\\rvax_{k}\\rvey^{1}_{k-1}}\\\\   \\expe{\\rvax_{k}\\rvey^{1}_{k-1}}^{t }     & \\expe{\\rvax_{k}^{2 } } \\end{matrix } \\right)^{-1 } \\left [ \\begin{matrix }   \\rvey^{1}_{k-1}\\\\   \\rvax_{k } \\end{matrix } \\right].\\end{aligned}\\ ] ] from steps 1 ,  3 and  4 it follows that @xmath435 } =   \\expe{\\rvay_{k}(\\rvey^{1}_{k})^{t } }   =   \\expe{\\rvax_{k}(\\rvey^{1}_{k})^{t } } $ ] .",
    "substitution of this into   and the result into   leads directly to  .",
    "thus ,   is satisfied for all @xmath161 .",
    "since the above algorithm yields an output which satisfies  ,   and  , for all @xmath161 , this output attains equality in  , thus being a realization of @xmath436 .",
    "notice that once the distortions @xmath139 are given , each step in the recursive algorithm yields the only variances and covariances that satisfy  ,   and  .",
    "therefore , for any given distortion schedule @xmath153 , the latter algorithm yields the unique output that realizes @xmath437 .",
    "this completes the proof .",
    "consider the first  @xmath101 samples of input and output .",
    "the average distortion constraint here takes the form @xmath438 then , @xmath439 where the last inequality follows from jensen s inequality and the fact that @xmath440 is a convex function of @xmath441 .",
    "equality is achieved if and only if all distortions @xmath410 equal some common value for all @xmath442 .",
    "given that the rhs of   is minimized when constraint   is active ( i.e. , by making @xmath443 ) , we can attain equality in   and minimize its rhs by picking @xmath444 for this choice to be feasible , the distortion @xmath410 must satisfy  , which translates into the constraint @xmath445 thus , substituting   into  , we obtain @xmath446 in view of  , as @xmath447 , the value of @xmath448 that infimizes   remains bounded .",
    "therefore , @xmath449 finally , from lemma  [ lem : limusp ] in the appendix , we conclude that @xmath0 equals the rhs of  , completing the proof .",
    "the first inequality in   follows directly from definitions  [ def : causalrdf ] and  [ def : stat_causalrdf ] . for a plain awgn channel with noise variance @xmath450 ,",
    "the mutual information between source and reconstruction is @xmath451 on the other hand , by definition , the mutual information across a test channel that realizes @xmath172 with distortion @xmath452 satisfies  @xcite : @xmath453 in both cases the end - to - end distortion can be reduced by placing a scalar gain after the test channel .",
    "the optimal ( minimum mse ) gain is @xmath454 . the mutual information from the source to the signal before the scalar gain is the same as that between the source an the signal after it .",
    "however , now the resulting end - to - end distortion is @xmath455 .",
    "therefore , for a given end - to - end distortion @xmath17 , the distortion between the source and the signal before the optimal scalar gain is @xmath456 which implies that the mutual informations across the @xmath457 channel and the awgn channel when the optimal scalar gain is used are given by @xmath458 and @xmath459 , respectively .",
    "we then have that    [ eq : boudsineqs ] @xmath460\\frac{s_{\\rvax}\\ejw } { d } \\right)d\\w - r(d ) = b_{2}(d ) .",
    "\\label{eq : aca } % \\\\ & % % \\overset{(a ) } % < % \\frac{1}{4\\pi}\\intfromto{-\\pi}{\\pi}\\log_{2}\\left(1+\\frac{s_{\\rvax}\\ejw}{d } \\right)d\\w % - % \\frac{1}{4\\pi}\\intfromto{-\\pi}{\\pi}\\log_{2}\\left ( \\frac{s_{\\rvax}\\ejw}{d}\\right)d\\w % \\label{eq : ineqa } % \\\\ & % % \\overset{\\hphantom{(a ) } } % = % \\frac{1}{4\\pi}\\intfromto{-\\pi}{\\pi}\\log_{2}\\left(1+\\frac{d}{s_{\\rvax}\\ejw } \\right)d\\w\\nonumber % \\\\ & % % \\overset{(b ) } % \\leq % \\frac{1}{2 } % \\log_{2}\\left(1 + d \\varsigma_{\\rvax}^{2 } \\right)\\label{eq : b4a_proof}.\\end{aligned}\\ ] ]    to obtain the first function within the @xmath188 operator on the rhs of  , we notice from   that , since @xmath461 , the rdf for a gaussian stationary source with psd @xmath462 , @xmath463 , say @xmath464 , will equal the value @xmath5 given by   when the `` water level '' @xmath465 takes the same value as in  .",
    "hence , denoting by @xmath466 the distortion obtained in   when @xmath467 is substituted by @xmath468 , we find that @xmath469    on the other hand , @xmath470 with this , and starting from  , we have the following : @xmath471\\frac{s_{\\rvax}\\ejw}{d } \\right)d\\w    -r(d)\\nonumber    \\\\ &    \\leq     \\frac{1}{4\\pi}\\intfromto{-\\pi}{\\pi}\\log_{2}\\left(1+[1-\\tfrac{d}{\\sigsq_{\\rvax}}]\\frac{s^{\\vareps}_{\\rvax}\\ejw}{d } \\right)d\\w    -    \\frac{1}{4\\pi}\\intfromto{-\\pi}{\\pi}\\log_{2}\\left ( \\frac{s^{\\vareps}\\ejw}{d^{\\vareps}}\\right)d\\w     \\label{eq : intlog - intlog }    \\\\ &    =     \\frac{1}{4\\pi }     \\intfromto{-\\pi}{\\pi}\\log_{2 }     \\left (     \\frac{d^{\\vareps}}{s^{\\vareps}\\ejw}+[1-\\tfrac{d}{\\sigsq_{\\rvax}}]\\frac{d^{\\vareps}}{d }      \\right)d\\w     \\\\ &     \\leq      \\frac{1}{4\\pi }     \\intfromto{-\\pi}{\\pi}\\log_{2 }     \\left (     \\frac{d+\\vareps}{s^{\\vareps}\\ejw}+[1-\\tfrac{d}{\\sigsq_{\\rvax}}]\\frac{d+\\vareps}{d }     \\right)d\\w\\label{eq : greater }     \\\\ &     \\leq      \\frac{1}{2 }     \\log_{2 }     \\left (     ( d+\\vareps )    \\varsigma_{\\rvax}^{\\vareps } + [ 1-\\tfrac{d}{\\sigsq_{\\rvax}}]\\frac{d+\\vareps}{d }     \\right),\\label{eq:38a }    \\end{aligned}\\ ] ] where   follows from  ,   and   and by noting that @xmath472 ,   stems from  , and   follows from jensen s inequality .",
    "notice that the rhs of   equals the first term on the rhs of  .",
    "the middle term on the rhs of   follows directly from  .",
    "finally , for distortions close to @xmath43 , a bound tighter than   can be obtained from   as follows    [ eq : more_bound_ineqs ] @xmath473s_{\\rvax}\\ejw}{\\sigsq _ { \\rvax}d}\\right)d\\w   - r(d ) \\nonumber \\\\ & % \\overset{(a ) } { < } \\label{eq : b4a } < \\frac{1}{4\\pi}\\intfromto{-\\pi}{\\pi}\\log_{2}\\left(1+\\frac{[\\sigsq_{\\rvax}-d]s_{\\rvax}\\ejw}{\\sigsq _ { \\rvax}d}\\right)d\\w \\\\ & % \\overset{(b)}{\\leq } \\label{eq : b4b } \\leq \\frac{1}{2}\\log_{2}\\left(1 + \\frac{\\sigsq_{\\rvax } -d}{d } \\right ) = \\frac{1}{2}\\log_{2}\\left(\\frac{\\sigsq_{\\rvax}}{d } \\right),\\end{aligned}\\ ] ]    which is precisely the third term on the rhs of  . in the above ,   holds trivially since @xmath474 , and   follows from jensen s inequality .",
    "therefore , equality holds in   if and only if @xmath45 is white .",
    "the validity of the chain of inequalities in   follows directly from   and  .",
    "this completes the proof .",
    "the idea of the proof is to first show that if the distortion @xmath213 equals @xmath23 , then @xmath475    immediately afterward we prove that , despite the distortion and causality constraints , the scheme in fig .  [",
    "fig : causalfq ] has enough degrees of freedom to turn all the above inequalities into equalities .",
    "that means that if we are able to globally infimize @xmath218 over the filters of the system while satisfying the distortion and causality constraints , then that infimum , say @xmath476 , must satisfy @xmath477 .",
    "we now proceed to demonstrate the validity of   and to state the conditions under which equalities are achieved .",
    "the first equality in   follows from the fact that @xmath192 is a gaussian i.i.d .",
    "inequality  @xmath29 stems from the following : @xmath478 where @xmath479 is the signal at the output of @xmath197 , see fig .",
    "[ fig : causalfq ] . in the above ,   follows from the fact that @xmath192 and @xmath45 are independent and from the fact that @xmath196 is strictly causal . as a consequence ,",
    "@xmath480 is independent of @xmath193 , for all @xmath61 .",
    "inequality   holds from the property @xmath481 , with equality if and only if @xmath441 and @xmath482 are independent , i.e. , if and only if @xmath269 is white .",
    "similarly ,   holds since the samples of @xmath192 are independent . by noting that @xmath483 is a linear combination of @xmath46 and @xmath484",
    ", it follows immediately that @xmath480 is independent from @xmath483 upon knowledge of @xmath484 , which leads to  . on the other hand ,",
    "stems from the fact that @xmath485 .",
    "equality in   holds from the fact that , if @xmath486 is known , then @xmath487 can be obtained deterministically from @xmath488 , and vice - versa , see fig .",
    "[ fig : causalfq ] .",
    "equality   follows from the fact that there exists no feedback from @xmath269 to @xmath479 , and thus the markov chain @xmath489 holds . on the other hand , @xmath490 , with equality if and only if @xmath491 is invertible for all frequencies @xmath492 for which @xmath493 .",
    "finally ,   follows directly from the data processing inequality , with equality if and only if @xmath494 is invertible for all frequencies @xmath492 for which @xmath495 .",
    "since @xmath30 is by definition an infimum , it follows that , for every @xmath329 , there exists an output process @xmath496 jointly gaussian with @xmath45 , satisfying the causality and distortion constraints and such that @xmath497 .",
    "such output can be characterized by its noise psd , say @xmath498 , and its signal transfer function , say @xmath499 , by using the model in fig .",
    "[ fig : roles ] .    therefore , all that is needed for the system in fig .",
    "[ fig : causalfq ] to achieve @xmath500 is to yield the required noise psd @xmath498 , the required signal transfer function @xmath499 , a white @xmath269 and satisfy @xmath501 . to summarize and to restate the latter more precisely :    [ eq : allconditions ] @xmath502    all these equations are to be satisfied @xmath503 .",
    "we have chosen @xmath504 in   for simplicity and because , as we shall see next , we have enough degrees of freedom to do so without compromising rate / distortion performance . solving the system of equations formed by  ,   and",
    "we obtain    [ eq : lasexplicitas ] @xmath505    it is only left to be shown that there exist causal , stable and minimum - phase transfer functions @xmath198 , @xmath506 and @xmath197 such that their squared magnitudes equal their right - hand sides in  . to do so , we will make use of the paley - wiener theorem ( theorem  [ thm : paley - wiener ] in the appendix ) .    to begin with , we notice from fig .",
    "[ fig : roles ] , and since @xmath507 is independent of @xmath45 , that @xmath508 where   follows from  . since @xmath30 is bounded ,",
    "so is @xmath509 , and thus we conclude from the paley - wiener theorem that there exists a stable , causal and minimum - phase transfer function @xmath506 satisfying  .",
    "also , from the fact that the first sample of the impulse response of @xmath506 is  @xmath288 and as a consequence of @xmath506 being minimum - phase , we conclude that @xmath510 ( see , e.g. ,  @xcite ) .",
    "therefore , @xmath511    next , we notice that since @xmath203 is stable and causal , then there exists a causal , stable and minimum phase transfer function @xmath512 such that @xmath513 , forall @xmath514 .",
    "from the paley - wiener theorem , it follows that @xmath515 which implies that @xmath516 on the other hand , from  , @xmath517 and recalling that @xmath518 , it follows that @xmath519 is bounded from below . in view of",
    ", we conclude that @xmath520 .",
    "now , since @xmath521 , we can apply lemma  [ lem : parapaleywiener ] ( see appendix ) to obtain that @xmath522 substitution of the rhs of the second equation of   into the above , together with the paley - wiener theorem , yields that there exists a causal , stable and minimum phase transfer function @xmath523 such that @xmath524 and thus @xmath198 can be chosen to be the causal , stable and minimum - phase transfer function @xmath525 which allows us to choose a stable , causal and minimum - phase @xmath526 .",
    "therefore , for every @xmath329 , there exists causal , stable and minimum phase transfer functions @xmath197 , @xmath198 and @xmath251 that satisfy  , attaining equalities throughout and therefore yielding a value of @xmath218 which satisfies  .",
    "this completes the proof .",
    "strict convexity exists if and only if the inequality @xmath527\\jsc(p_{2 } ) >   \\jsc(\\lambda p_{1}+[1-\\lambda ] p_{2 } ) , \\fspace\\forall \\lambda\\in(0,1),\\label{eq : jp1_jp2_convexity}\\end{aligned}\\ ] ] holds for any two pairs @xmath528 and @xmath529 satisfying @xmath530 we will first prove the validity of   for pairs @xmath531 and @xmath532 which also satisfy @xmath533g_{2}(\\w ) } > 0,\\,\\forallwinpipi , \\forall \\lambda\\in[0,1],\\end{aligned}\\ ] ] but are otherwise arbitrary . for any given",
    "@xmath534 $ ] , define the pair @xmath535 ( f_2,g_2).\\ ] ] upon defining the functions @xmath536 any pair along the `` line '' between @xmath537 and @xmath538 can be written in terms of a single scalar parameter @xmath539 via @xmath540 where @xmath541 $ ] .",
    "define the functions    [ eq : nsp_and_dsp ] @xmath542    where @xmath543 denotes the real part of @xmath441 .",
    "substitution of   into   allows one to write the latter as @xmath544 where @xmath545 we next show that   holds by showing that @xmath546 for every @xmath534 $ ] . for this purpose , we first take the derivative of @xmath547 with respect to @xmath539 . denoting the derivatives of the functions @xmath548 and @xmath549 with respect to @xmath539 by @xmath550 and @xmath551 , respectively , we have that @xmath552 differentiating again , one arrives to @xmath553 from  , we have that @xmath554 where @xmath555 see  , and where @xmath556 notice that  @xmath557 and @xmath558 in   are well defined since we are considering pairs @xmath531 and @xmath532 for which   holds .",
    "substitution of   into   yields @xmath559 where @xmath29 and @xmath277 follow from  ,   and from the fact that @xmath560 .",
    "the strict inequality in   stems from the fact that @xmath561 .",
    "the latter follows directly from   and  .",
    "therefore   holds for any two pairs @xmath562 satisfying  .",
    "we will show now that   also holds for pairs @xmath563 which do not satisfy  .",
    "the idea is to construct another pair , say @xmath564 , @xmath565 , `` close '' to @xmath531 , @xmath532 and meeting  , and then show that strict convexity along the straight line between @xmath564 and @xmath565 implies strict convexity along the straight line between @xmath531 and @xmath532 . for this purpose , define , for any given pairs @xmath566 ,",
    "@xmath567 , the family of functions @xmath568g_{2}(\\w ) = 0 \\textrm { for some } \\lambda\\in(0,1),\\\\ 0        & , \\textrm { in any other case}. \\end{cases}\\end{aligned}\\ ] ] where @xmath569 is a scalar parameter .",
    "the functions @xmath570 defined above exhibit the property ( to be exploited below ) that @xmath571 + [ 1-\\lambda ] \\big[g_{2}(\\w ) + h_{\\delta}(\\w ) \\big ] } > 0 , \\fspace \\forall g_{1},g_{2}\\in\\gset , \\ ; \\forall \\delta>0,\\,\\forall \\lambda\\in(0,1).\\end{aligned}\\ ] ] upon introducing the notation @xmath572 and @xmath573 , it follows directly from   that @xmath574 satisfy   for _ all _ pairs @xmath575 .",
    "notice also that @xmath576 on the other hand , it is easy to show that @xmath577 is uniformly continuous at @xmath578p_{2}$ ] for any pairs @xmath575 and for all @xmath534 $ ] .",
    "in view of  , uniform continuity of @xmath577 means that , for every @xmath329 , there exists @xmath579 such that @xmath580p_{2 } , \\,\\forall \\lambda\\in(0,1).\\end{aligned}\\ ] ] the fact that @xmath564 and @xmath565 satisfy   implies that @xmath564 , @xmath565 also satisfy the strict - convexity condition  .",
    "therefore , for each @xmath581 , there exists @xmath582 such that @xmath583\\jsc(p_{2}^{\\delta } ) -   \\jsc(\\lambda p_{1}^{\\delta } + [ 1-\\lambda]p_{2}^{\\delta } ) > \\vareps_{2}(\\lambda)>0 , \\fspace \\forall \\lambda\\in(0,1).\\end{aligned}\\ ] ] then , from   and  , @xmath584\\jsc(p_{2 } ) & \\geq \\lambda \\jsc(p_{1}^{\\delta } ) + [ 1-\\lambda]\\jsc(p_{2}^{\\delta } ) - 2\\vareps % \\\\ & \\geq   \\jsc(\\lambda p_{1}^{\\delta } + [ 1-\\lambda]p_{2}^{\\delta } ) + \\vareps_{2}(\\lambda ) - 2\\vareps \\\\ & \\geq   \\jsc(\\lambda p_{1 } + [ 1-\\lambda]p_{2 } ) + \\vareps_{2}(\\lambda ) - 3\\vareps.\\end{aligned}\\ ] ] since @xmath585 can be chosen arbitrarily small , and in particular , strictly smaller than @xmath586 , it follows that   also holds for all pairs @xmath587 not satisfying  .",
    "this completes the proof .",
    "[ lem : inflingeqliminf ] for any zero - mean gaussian stationary source @xmath45 and @xmath23 , @xmath588    suppose   does not hold , i.e. , that @xmath589 for some @xmath590 .",
    "the definition of @xmath0 in   means that , @xmath591 , there exists @xmath592 such that @xmath593 combining this inequality with   we arrive to @xmath594 since @xmath595 can be chosen to be arbitrarily small , it can always be chosen so that @xmath596 , which contradicts  . therefore   holds .",
    "[ lem : limusp ] let @xmath597 where @xmath598 denotes the space of all random processes causally related to @xmath45 .",
    "let @xmath599 then , for any first - order gauss - markov source , the following holds : @xmath600    in lemma  [ lem : inflingeqliminf ] in the appendix it is shown that @xmath601 so all we need to demonstrate is that @xmath602 . to do this , we simply observe from theorem  [ thm : longone ] that if we construct an output process @xmath55 by using the recursive algorithm of that theorem , with the choice @xmath603 , for all @xmath604 , then this output process is such that @xmath605 equals @xmath606 . therefore , @xmath607 , concluding the proof .",
    "[ prop : anyfiltercanbewiener ] let @xmath608 be a gaussian random vector source with covariance matrix @xmath326",
    ". a reconstruction gaussian random vector @xmath322 satisfies @xmath609 if and only if @xmath610      [ lem : tr = tr ] let @xmath614 , with @xmath615 , be a gaussian random source vector with covariance matrix @xmath326 . a reconstruction gaussian random vector @xmath322 satisfies @xmath616 if and only if @xmath617_{j , k } = \\left[\\bk_{\\rvey\\rvex}\\right]_{j , k},\\fspace \\forall j\\leq k , \\;j , k=1,2,\\ldots n.\\end{aligned}\\ ] ]    let us first introduce the notation @xmath618 , denoting the top - left submatrix of any given square matrix @xmath619 , with @xmath620 . from proposition  [ prop : anyfiltercanbewiener ] , it immediately follows that , for every @xmath621 , @xmath622 which is equivalent to  .",
    "lemma  [ lem : tr = tr ] implies that , if the reconstruction @xmath322 is the output of a causal wiener filter applied to the noisy source @xmath623 for some noise vector @xmath624 ( a condition equivalent to  ) , then @xmath330 and @xmath331 have identical entries on and above their main diagonals .",
    "[ thm : paley - wiener ] let @xmath625 be a non - negative function defined on @xmath626 $ ] .",
    "there exists a unique stable , causal and minimum phase transfer function @xmath627 such that @xmath628 if and only if to exist .",
    "however , from  ( * ? ? ?",
    "* note  2 , p.  228 ) and the discrete - continuous equivalence in  @xcite it follow that   is also necessary . ]",
    "@xmath629                        m.  pinsker and a.  gorbunov , `` epsilon - entropy with delay for small mean - square reproduction error , '' _ probl .",
    "_ , vol .  23 , pp . 9195 , 1987 , translation from problemi peredachi informatsii , vol .  23 , no .  2 , pp",
    "38 , april - june 1987 .",
    "a.  gorbunov and m.  pinsker , `` asymptotic behavior of nonanticipative epsilon - entropy for gaussian processes , '' _ probl .",
    "_ , vol .  27 , no .  4 , pp . 361365 , 1991 , translation from problemi peredachi informatsii , vol .  27 , no .  4 , pp .  100104 , october - december 1991 .            m.  s. derpich , j.  stergaard , and g.  c. goodwin , `` the quadratic gaussian rate - distortion function for source uncorrelated distortions , '' in _ proc .   data compression conf .",
    "_ , snowbird , ut , march 2008 , pp",
    ". 7382 .",
    "m.  s. derpich , e.  i. silva , d.  e. quevedo , and g.  c. goodwin , `` on optimal perfect reconstruction feedback quantizers , '' _ ieee trans . signal process .",
    "_ , vol .",
    "56 , no . 8 , part 2 , pp . 38713890 , august 2008 ."
  ],
  "abstract_text": [
    "<S> we improve the existing achievable rate regions for causal and for zero - delay source coding of stationary gaussian sources under an average mean squared error ( mse ) distortion measure . to begin with , we find a closed - form expression for the information - theoretic causal rate - distortion function ( rdf ) under such distortion measure , denoted by @xmath0 , for first - order gauss - markov processes . </S>",
    "<S> @xmath0 is a lower bound to the optimal performance theoretically attainable ( opta ) by any causal source code , namely @xmath1 . </S>",
    "<S> we show that , for gaussian sources , the latter can also be upper bounded as @xmath2  bits / sample . in order to analyze @xmath0 for arbitrary zero - mean gaussian stationary sources , we introduce @xmath3 , the information - theoretic causal rdf when the reconstruction error is jointly stationary with the source . based upon @xmath3 </S>",
    "<S> , we derive three closed - form upper bounds to the additive rate loss defined as @xmath4 , where @xmath5 denotes shannon s rdf . </S>",
    "<S> two of these bounds are strictly smaller than @xmath6 bits / sample at all rates . </S>",
    "<S> these bounds differ from one another in their tightness and ease of evaluation ; the tighter the bound , the more involved its evaluation . </S>",
    "<S> we then show that , for any source spectral density and any positive distortion @xmath7 , @xmath3 can be realized by an awgn channel surrounded by a unique set of causal and feedback filters . </S>",
    "<S> we show that finding such filters constitutes a convex optimization problem . in order to solve the latter , we propose an iterative optimization procedure that yields the optimal filters and </S>",
    "<S> is guaranteed to converge to @xmath3 . </S>",
    "<S> finally , by establishing a connection to feedback quantization we design a causal and a zero - delay coding scheme which , for gaussian sources , achieves an operational rate lower than @xmath8 and @xmath9 bits / sample , respectively . </S>",
    "<S> this implies that the opta among all _ zero - delay _ source codes , denoted by @xmath10 , is upper bounded as @xmath11 bits / sample .    </S>",
    "<S> causality , rate - distortion theory , entropy coded dithered quantization , noise - shaping , differential pulse - code modulation ( dpcm ) , sequential coding , convex optimization . </S>"
  ]
}