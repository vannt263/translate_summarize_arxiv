{
  "article_text": [
    "character - level models are attracting increasing interest . we group them into three classes .",
    "( i ) * character - level models of words * derive a word representation from the character string , but they are symbolic in that they need text segmented into tokens as input .",
    "( ii ) * bag - of - ngram models * discard the order of character ngrams on the assumption that relevant order information is coded in the ngrams , so the order of ngrams can be neglected .",
    "( iii ) * end - to - end models * learn a separate model on the raw character ( or byte ) input for each task ; these models estimate task - specific parameters , but no representation of text that would be usable across tasks is computed .    turning to group ( iii ) first",
    ", one premise of this paper is that text representations are needed in nlp . a large body of work on the use of word embeddings like senna @xcite",
    ", word2vec @xcite , hlbl @xcite and glove @xcite demonstrates that a generic text representation , trained in an unsupervised fashion on large corpora , is useful for many nlp applications .",
    "thus , we take the view that end - to - end learning without any representation learning is not a good approach for nlp .    in this paper , we separate * training * and * utilization * of the text representation model .",
    "we use `` training '' to refer to the method by which the model is learned and `` utilization '' to refer to the application of the model to a piece of text to compute a representation of the text . in many text",
    "representation models , utilization is trivial .",
    "for example , for word embedding models , utilization amounts to a simple lookup of a word to get its precomputed embedding .",
    "however , for the models we consider , utilization is not trivial and we will discuss different approaches",
    ".    both training and utilization can be either _ symbolic _ or _",
    "nonsymbolic_. we define a symbolic approach as one that is based on tokenization , i.e. , a segmentation of the text into tokens .",
    "symbol identifiers ( i.e. , tokens ) can have internal structure , e.g. , if a tokenizer recognizes tokens like `` to and fro '' and `` london - based '' that contain delimiters or if a token is morphologically analyzed downstream .",
    "we define a nonsymbolic approach as one that is tokenization - free , i.e. , no assumption is made that there are segmentation boundaries and that each segment ( e.g. , a word ) should be represented ( e.g. , by a word embedding ) in a way that is independent of the representations ( e.g. , word embeddings ) of neighboring segments .",
    "methods for training text representation models that require tokenized text include word embedding models like senna , hlbl , word2vec and glove and character - level models like fasttext skipgram @xcite .",
    "examples of nonsymbolic text representation utilization models are bag - of - ngram models that compute the representation of a text as the sum of the embeddings of all character ngrams occurring in it , e.g. , charagram @xcite ) .",
    "charagram is an example of a mixed training - utilization model : training requires tokenized text ( words and phrases ) , utilization is nonsymbolic .",
    "the model is trained on words and phrases , i.e. , tokens that are the output of tokenization , but it can be applied to any untokenized character sequence .",
    "we make two contributions in this paper .",
    "first , we propose the first generic method for training a text representation model without the need for tokenization and address the challenging sparseness issues that make this a difficult problem .",
    "second , we propose the first nonsymbolic utilization method that fully represents sequence information  in contrast to utilization methods like bag - of - ngrams that discard sequence information that is not directly encoded in the character ngrams themselves .    we show that our models perform better than prior work on an information extraction and a text denoising task .",
    "give two motivations for their work on character - level models .",
    "first , segmentation/*tokenization algorithms make many mistakes * and are brittle : `` we do not have a perfect word segmentation algorithm for any one language '' .",
    "tokenization errors then propagate throughout the nlp pipeline , potentially doing great damage .",
    "second , there is currently * no general solution for morphology * in statistical nlp . for many languages , high - coverage and high - quality morphological resources are not available . even for well resourced languages , problems like ambiguity make morphological processing difficult ; e.g. ,",
    "`` rung '' is either the singular of a noun meaning `` part of a ladder '' or the past participle of `` to ring '' . in many languages , e.g. , in german , syncretism ,",
    "a particular type of systematic morphological ambiguity , is pervasive .",
    "thus , there is no simple morphological processing method that would produce a representation in which all inflected forms of `` to ring '' are marked as having a common lemma ; and no such method in which an unseen form like `` aromatizing '' is reliably analyzed as a form of `` aromatize '' whereas an unseen form like `` antitrafficking '' is reliably analyzed as the compound `` anti+trafficking '' .    of course",
    ", it is an open question whether nonsymbolic methods can perform better than morphological analysis , but the foregoing discussion motivates us to investigate them .    focus on problems with processing the tokens that are output by text segmentation algorithms . equally important",
    "is the problem that * tokenization fails to capture structure across multiple tokens*. the job of dealing with multi - token structure is often given to downstream components of the pipeline , e.g. , components that recognize multiwords and named entitites in english or in fact any word in a language like chinese that uses no overt delimiters .",
    "however , there is no linguistic or computational reason in principle why we should treat the recognition of a unit like `` electromechanical '' ( containing no space ) as fundamentally different from the recognition of a unit like `` electrical engineering '' ( containing a space ) .",
    "character - level models offer the potential of uniform treatment of linguistic units that are relevant for nlp .",
    "many text representation learning algorithms can be understood as estimating the parameters of the model from a unit - context matrix @xmath0 where each row corresponds to a unit @xmath1 , each column to a context @xmath2 and each cell @xmath3 is a quantity that measures how strongly @xmath1 and @xmath2 are associated .",
    "for example , the skipgram model is closely related to an svd factorization of a positive pointwise mutual information matrix @xcite ; in this case , both units and contexts are words .",
    "text representation learning algorithms explicitly formalized as matrix factorization include @xcite , but there may not be a big difference between methods that explicitly factorize and those that approximate the factorization through iterative methods ( e.g. , @xcite ) ; see also @xcite .",
    "our goal in this paper is not to develop new parameter estimation methods , e.g. , a new matrix factorization algorithm .",
    "instead , we will focus on defining the unit - context matrix in such a way that no symbolic assumption has to be made .",
    "this unit - context matrix can then be processed by any existing or still to be invented algorithm .",
    "how can we define units and contexts without relying on segmentation boundaries ?",
    "in initial experiments , we simply generated all character ngrams of length up to @xmath4 ( where @xmath4 is a parameter ) , including character ngrams that cross token boundaries ; i.e. , no segmentation is needed .",
    "we then used an objective similar to the skipgram objective for learning ngram embeddings , i.e. , we used an iterative training procedure each instance of which consisted of selecting an ngram @xmath5 and trying to predict an ngram @xmath6 in its context based on @xmath5 .",
    "however , this does not work well because many training instances consist of pairs @xmath7 in which @xmath5 and @xmath6 overlap , e.g. , one is a subsequence of the other .",
    "so the objective encourages trivial predictions of ngrams that have high string similarity with the input and nothing interesting is learned .    in this paper",
    ", we propose an alternative way of defining units and contexts that supports well - performing nonsymbolic text representation learning : * multiple random segmentation*. a pointer moves through the training corpus .",
    "the current position @xmath8 of the pointer defines the left boundary of the next segment .",
    "the length @xmath9 of the next move is uniformly sampled from @xmath10 $ ] where @xmath11 and @xmath4 are the minimum and maximum segment lengths .",
    "the right boundary of the segment is then @xmath12 .",
    "thus , the segment just generated is @xmath13 , the subsequence of the corpus between ( and including ) positions @xmath8 and @xmath12 .",
    "the pointer is positioned at @xmath14 , the next segment is sampled and so on .    the corpus is segmented this way @xmath15 times ( where @xmath15 is a parameter ) and the @xmath15 random segmentations are concatenated .",
    "the unit - context matrix is derived form this concatenated corpus .",
    "multiple random segmentation has two advantages .",
    "first , there is no redundancy since , in any given random segmentation , two ngrams do not overlap and are not subsequences of each other .",
    "second , a single random segmentation would only cover a small part of the space of possible ngrams .",
    "for example , a random segmentation of `` a rose is a rose is a rose '' might be `` [ a ros][e is a ros][e is][a rose ] '' .",
    "this segmentation does not contain the segment `` rose '' and this part of the corpus can then not be exploited to learn a good embedding for the fourgram `` rose '' .",
    "however , if we produce multiple random segmentations of the corpus ( and concatenate them ) , then it is likely that this part of the corpus does give rise to the segment `` rose '' in one of the segmentations and can contribute information to learning a good embedding for `` rose '' .",
    "nonsymbolic representation learning does not preprocess the training corpus by means of tokenization and considers many ngrams that would be ignored in tokenized approaches because they span token boundaries . as a result ,",
    "the number of ngrams is much larger and sparseness is greatly increased .",
    "shows that the number of ngrams that occur in a corpus is an order of magnitude larger for tokenization - free approaches than for tokenization - based approaches .",
    ", @xmath16 ) occur in the first @xmath17 bytes of the english wikipedia for symbolic ( tokenization - based ) vs.  nonsymbolic ( tokenization - free ) processing .",
    "the number of ngrams is an order of magnitude larger in the nonsymbolic approach .",
    ", scaledwidth=49.0% ]    we will see below that this sparseness impacts performance of nonsymbolic text representation negatively .",
    "we address sparseness by defining ngram equivalence classes .",
    "all ngrams in an equivalence class will receive the same embedding .    the learning procedure for learning equivalence classes",
    "is based on two premises .",
    "* permutation premise .",
    "* let @xmath18 be the alphabet of a language , i.e. , its set of characters , @xmath19 a permutation on @xmath18 , @xmath0 a corpus and @xmath20 the transformed corpus after the permutation has been applied to @xmath0 .",
    "for example , if @xmath21 , then all characters `` a '' in @xmath0 are replaced with `` e '' in @xmath20 .",
    "* the learning procedure should learn identical equivalence classes on @xmath0 and @xmath20 .",
    "* so , if @xmath22 after runing the learning procedure on @xmath0 , then @xmath23 after running the learning procedure on @xmath20 .",
    "this premise is motivated by our desire to come up with a general method that does not rely on specific properties of a language or genre ; e.g. , the premise rules out exploiting the fact through feature engineering that in many languages and genres , `` c '' and `` c '' are related .",
    "such a relationship has to be learned from the data .",
    "* form - meaning homomorphism premise . *",
    "the relationship between form and meaning is mostly arbitrary , but there are substructures of the ngram space and the embedding space that are systematically related by homomorphism . in this paper ,",
    "* we will assume that the following homomorphism holds : * @xmath24 where @xmath25 iff @xmath26 for a string transduction @xmath27 and @xmath28 iff @xmath29 .",
    "we will propose a learning procedure for the transduction below . as a simple example for a meaning - preserving string operation consider `` delete a space at the beginning of an ngram '' .",
    "for the transductor @xmath27 we will learn below , we have : @xmath30 .",
    "this is a good example of what a transducer can do because we can reasonably assume that the embeddings of `` @mercedes '' and `` mercedes '' are the same .",
    "we define @xmath31 as `` closeness ''  not as identity  because of estimation noise when embeddings are learned .",
    "we assume that there are no true synonyms and therefore the direction @xmath32 also holds .",
    "for example , `` car '' and `` automobile '' are considered synonyms , but we assume that their embeddings are different ( e.g. , because `` car '' has the literary sense `` chariot '' that `` automobile '' does not have ) . if they were identical , then the homomorphism would not hold since `` car '' and `` automobile '' can not be converted into each other by any plausible meaning - preserving string operation .    *",
    "learning procedure .",
    "* to learn the transductor automatically , we define three templates that transform one ngram into another : ( i ) replace character @xmath33 with character @xmath34 , ( ii ) delete character @xmath33 if its immediate predecessor is character @xmath34 , ( iii ) delete character @xmath33 if its immediate successor is character @xmath34 .",
    "the learning procedure takes a set of ngrams and their embeddings as input .",
    "it then exhaustively searches for all pairs of ngrams , for all pairs of characters @xmath33/@xmath34 , for each of the three templates .",
    "when two matching embeddings exist , we compute their cosine .",
    "for example , for the operation `` delete space before m '' , an ngram pair from our embeddings that matches is `` @mercedes '' / `` mercedes '' and we compute the cosine of these two embeddings . as the characteristic statistic of an operation",
    "we take the average of all cosines ; e.g. , for `` delete space before m '' the average cosine is 0.7435 .",
    "we then rank operations according to average cosine and take the first @xmath35 as the definition of the transductor where @xmath35 is a parameter . for characters that are replaced by each other ( e.g. , 1 , 2 , 3 in ) ,",
    "we compute the eqivalence class and then replace the learned operations with ones that replace a character by the canonical member of its equivalence class ( e.g. , 2 @xmath36 1 , 3 @xmath36 1 ) .",
    "we run experiments on @xmath0 , a 3 gigabyte english wikipedia corpus . based on the permutation premise ( )",
    ", we first apply a randomly generated permutation @xmath19 to @xmath0 and then conduct all experiments on @xmath20 , the permuted form of @xmath0 .",
    "we train three text representation models : fasttext , nonsymbolic and transduced .",
    "we use a dimensionality of 200 throughout this paper for all models .",
    "* fasttext .",
    "* we train fasttext skipgram @xcite , with default parameters , on @xmath20 .",
    "* nonsymbolic .",
    "* we train word2vec skipgram on a multiple random segmentation corpus that is generated from @xmath20 .",
    "as we discussed in , our focus in this paper is not to develop new methods for implicit matrix factorization .",
    "we therefore selected word2vec because it is a well understood and widely used method .",
    "we use default parameters , except that we train for only one iteration since we found this was sufficient .",
    "parameters for the segmentation were @xmath37 ( number of copies ) and @xmath38 , @xmath39 ( min / max length of character ngrams ) .",
    "the average length of words in this type of setting is between 7 and 8 , the average length of ngrams of the model we learned with word2vec is 6.9 , which is roughly comparable .",
    "if we used the fasttext maximum @xmath40 for our model , this would clearly not be a fair comparison .    for the nonsymbolic experiments ( i.e. , not for the experiments with fasttext )",
    ", we replace white spaces in @xmath0 with `` @ '' .",
    "the segmenter then simply marks segment boundaries by white space .",
    "this trick avoids the need for any change to the implementation of word2vec and we can run it as is .",
    "* transduced .",
    "* we repeat experiment nonsymbolic with a version of the multiple random segmentation corpus that has been transformed by the transductor @xmath27 .    to learn @xmath27",
    ", we first run the learning procedure described in on the ngram embeddings that are the output of nonsymbolic .",
    "we then rank all operations by the average cosine of the input embedding and output embedding of all pairs that match the operation ( see ) and select the top @xmath41 to define the transductor .",
    "shows a selection of the @xmath35 operations .",
    "the two uppercase / lowercase conversions shown in the table were the only ones that were learned ( we had hoped for more ) .",
    "the postdeletion rule `` ml '' @xmath36 `` m '' was learned because of `` html '' @xmath36 `` htm ''",
    "type ngram pairs .",
    "this is good for `` html '' , but dangerous as a general rule .",
    "we inspected all 200 rules and , with a few exceptions like `` html '' @xmath36 `` htm '' , they looked reasonable to us .",
    "@xmath27 applies all 200 operations .",
    "* evaluation .",
    "* we evaluate the three models on an entity typing task based on an entity dataset released by in which each entity has been assigned one or more types from a set of 50 types .",
    "for example , the entity `` harrison ford '' has the types `` actor '' , `` celebrity '' and `` award winner '' among others .",
    "we extract mentions from facc @xcite if an entity has a mention there or we use the freebase name as the mention otherwise .",
    "this gives us a data set of 54,334 , 6085 and 6747 mentions in train , dev and test , respectively .",
    "each mention is annotated with the types that its entity has been assigned by .",
    "we will release this dataset at time of publication .",
    ".evaluation results for named entity typing [ cols=\"<,<,<,<\",options=\"header \" , ]",
    "our motivation for multiple segmentation is that we want good coverage of the space of possible segmentations .",
    "an alternative approach would be to attempt to find a single optimal segmentation according to some optimality criterion .",
    "our intuition is that in many cases * overlapping segments contain complementary information*. gives an example for this intuition .",
    "there is a clear difference in semantics between `` historic exchange rates '' and `` floating exchange rates '' and this is captured by the low similarity of the ngrams ` ic@exchang ` and ` ing@exchan ` .",
    "also , the meaning of `` historic '' and `` floating '' is noncompositional : these two words take on a specialized meaning in the context of exchange rates . the same is true for `` rates '' : its meaning is not its general meaning in the compound `` exchange rates '' .",
    "thus , we need a representation that contains overlapping segments , so that `` historic '' / `` floating '' and `` exchange '' can disambiguate each other in the first part of the compound and `` exchange '' and `` rates '' can disambiguate each other in the second part of the compound .",
    "position embeddings may at first seem to stand in opposition to phrase and sentence embeddings .",
    "for many nlp tasks , we need a fixed length  as opposed to a variable length",
    " representation of a longer sequence .",
    "for example , in most if not all models that are commonly employed for sentiment analysis , we need a fixed - length representation to classify a sentence as having positive or negative polarity .    to see that position embeddings are perfectly compatible with fixed - length embeddings of phrases and sentences , observe first that , in principle , there is * no difference between word embeddings and position embeddings * in this respect .",
    "take a sequence that consists of , say , 6 words and 29 characters and assume our embedding dimensionality is 100 in both cases .",
    "if we use word embeddings , then the initial representation of the sentence is of size @xmath42 .",
    "if we use position embeddings , then the initial representation of the sentence is of size @xmath43 . in both cases ,",
    "we need a ( potentially complex ) model that reduces the variable length sequence into a fixed length vector at some intermediate stage and then classifies this vector as positive or negative . for example , both word and position embeddings can be used as the input to an rnn ( e.g. , an lstm ) whose final hidden unit activations are a fixed length vector of this type .    so assessing position embeddings is not a question of variable - length vs.  fixed - length representations .",
    "word embeddings give rise to variable - length representations too .",
    "the question is solely whether the position - embedding representation is a more effective representation than other types of representation for learning robust generalizations when solving nlp tasks .    a more specific form of this argument concerns architectures that compute fixed - length representations of subsequences on intermediate levels , e.g. , cnns .",
    "the difference between a position - embedding - based cnn and a word - embedding - based cnn is that the former has access to a * vastly increased range of subsequences * , including substrings of words ( making it easier to learn that `` exchange '' and `` exchanges '' are related ) and character strings that span words ( making it easier to learn that the phrase `` exchange rate '' is noncompositional ) . here ,",
    "the questions are : ( i ) how many if any of these additional subsequences made available by position embeddings are useful and ( ii ) is the increased level of noise due to many useless subsequences and the loss of efficiency due to the increased universe of subsequences worth the information gained by adding a few useful subsequences .",
    "we attempt to provide a comprehensive review of recent literature , but have no space to discuss individual papers .",
    "we therefore group related work into clusters and tag each cluster .",
    "the reader can find the relevant papers by scanning the reference section for papers with the tag of interest .",
    "our main goal in this paper is to overcome the limitations of tokenization both in training , i.e. , estimating the parameters of a text representation model , and in utilization , i.e. , applying it to computing the representation of a new text .",
    "most prior work on character - level models has been based on tokenization or , more generally , has addressed tasks that involve processing of words .",
    "this class of paper is tagged * tokenization - based - model*. we also include here models that learn one embedding per chinese character ( which is conceptually and architecturally similar to word embeddings ) and models that learn character sequences that are phrases ( e.g. , named entities ) .",
    "the criterion for inclusion in this group is not that segmentation is identical to standard tokenization ; instead , the criterion is that a segmentation is computed and each segment boundary is a traditional token boundary .",
    "note that many rule - based tokenizers produce tokens that contain spaces ( e.g. , ) .",
    "there have been two main approaches that are not tokenization - based .",
    "the first one learns character ngram embeddings ( from tokenized text or short text segments in all prior work that we have reviewed ) , but then computes the representation of new text segments without the need for tokenization by summing the embeddings of all occurring ngrams .",
    "we have shown above that important sequence information is lost in this approach .",
    "this class of paper is tagged * bag - of - ngram - model*. we also include sparse representation spaces whose dimensionality is the number of ngrams , i.e. , non - embedding approaches .",
    "many methods compute token representations in a bag - of - ngram fashion .",
    "we only tag papers as bag - of - ngram that go beyond word and token representations .",
    "end - to - end approaches to speech and natural language processing @xcite are the second main tokenization - free approach . they dispense with intermediate representations entirely .",
    "this class of papers is tagged * end - to - end - model*.    our premise is that text representations are needed in nlp . a large body of work on word embeddings like senna @xcite , word2vec @xcite , hlbl @xcite and glove @xcite demonstrates that a generic text representation , trained in an unsupervised fashion on large corpora , is useful for solving nlp tasks .",
    "thus , we take the view that end - to - end learning without any representation learning is not a good approach for nlp ; note that this argument does not apply to variants of end - to - end learning that use text representations , e.g. , pretrained word embeddings , that are then modified in an end - to - end setup ( e.g. , @xcite ) .",
    "our work bears some similarity to the autoencoder introduced by , but the authors do not couch their approach as representation learning and do not analyze the properties of their representation beyond showing that it performs well on a number of tasks .",
    "the hidden states of a character language model can also be interpreted as a nonsymbolic text representation .",
    "there seems little work that has exploited this , but see @xcite .",
    "we took the idea of random segmentation from work on biological sequences @xcite .",
    "biological sequences have no delimiters , so they provide a good model if one believes that delimiter - based segmentation is problematic for text .",
    "we have introduced the first generic text representation model that is completely nonsymbolic , i.e. , it does not require the availability of a segmentation or tokenization method that attempts to identify words or other symbolic units in text .",
    "this is true for the training of the model as well as for applying it when computing the representation of a new text .",
    "we showed that our model performs better than prior work on an information extraction and a text denoising task .    * future work . *",
    "write : `` it is worth noting that noise is often added  to images   and speech where the added noise does not fundamentally alter the input , but rather blurs it .",
    "[ bytes allow us to achieve ] something like blurring with text . ''",
    "it is not clear whether performing this blurring on the byte level is useful .",
    "for example , if each of the bytes in a phrase like `` historic exchange rates '' is blurred independently , then it is unlikely that the noise generated is helpful in , say , training by providing good training examples in parts of the space that would otherwise be unexplored .",
    "`` blurring '' is not a in contrast , the text representation we have introduced in this paper can be blurred in a way that is completely analogous to images and speech .",
    "each embedding of a position is a vector that can be smoothly changed in every direction .",
    "we have showed that the similarity in this space gives rise to natural variation .",
    "we plan to exploit this property in future work .",
    "the most important challenge that we need to address is how to use nonsymbolic text representation for tasks that are word - based like part - of - speech tagging .",
    "this may seem like a contradiction at first , but have shown how character - based methods can be used for `` symbolic '' tasks .",
    "we are currently working on creating an analogous evaluation for our nonsymbolic text representation .",
    "ehsaneddin asgari and mohammad r.  k. mofrad .",
    "2016 . comparing fifty natural languages and twelve genetic languages using word embedding language divergence ( weld ) as a quantitative measure of language distance . in _ workshop on multilingual and cross - lingual methods in nlp _ , pages 6574 .",
    "dzmitry bahdanau , jan chorowski , dmitriy serdyuk , philemon brakel , and yoshua bengio . 2016 .",
    "end - to - end attention - based large vocabulary speech recognition . in _",
    "ieee international conference on acoustics , speech and signal processing _ , pages 49454949 . * end - to - end - model*.    timothy baldwin and marco lui .",
    "2010 . language identification : the long and the short of the matter . in _ conference of the north american chapter of the association for computational linguistics / human language technologies _ , pages 229237 .",
    "* bag - of - ngram - model*.    miguel ballesteros , chris dyer , and noah  a. smith .",
    "2015 . improved transition - based parsing by modeling characters instead of words with lstms . in _ conference on empirical methods in natural language processing_. * tokenization - based - model*.            william chan , navdeep jaitly , quoc  v. le , and oriol vinyals . 2016 .",
    "listen , attend and spell : a neural network for large vocabulary conversational speech recognition . in _ ieee international conference on acoustics , speech and signal processing _ , pages 49604964 . * end - to - end - model*.      xinxiong chen , lei xu , zhiyuan liu , maosong sun , and huan - bo luan . 2015 .",
    "joint learning of character and word embeddings . in _",
    "international joint conference on artificial intelligence _ , pages 12361242 .",
    "* tokenization - based - model*.          junyoung chung , kyunghyun cho , and yoshua bengio",
    ". 2016 . a character - level decoder without explicit segmentation for neural machine translation . in",
    "_ annual meeting of the association for computational linguistics_. * end - to - end - model*.    alexander clark . 2003 . combining distributional and morphological information for part of speech induction . in _ conference of the european chapter of the association for computational linguistics _",
    ", pages 5966 .",
    "* tokenization - based - model*.        ryan cotterell and hinrich schtze . 2015 .",
    "multitask learning of morphological word - embeddings . in _ conference of the north american chapter of the association for computational linguistics / human language technologies_. * tokenization - based - model*.    ryan cotterell , tim vieira , and hinrich schtze .",
    "2016 . a joint model of orthography and morphological segmentation . in _ conference of the north american chapter of the association for computational linguistics / human language technologies_. * tokenization - based - model*.          li  dong , furu wei , hong sun , ming zhou , and ke  xu .",
    "2015 . a hybrid neural model for type classification of entity mentions .",
    "in _ proceedings of the twenty - fourth international joint conference on artificial intelligence , ijcai 2015 , buenos aires , argentina , july 25 - 31 , 2015 _ , pages 12431249 .",
    "* tokenization - based - model*.    ccero  nogueira dos santos and maira gatti .",
    "deep convolutional neural networks for sentiment analysis of short texts . in _ international conference on computational linguistics _ , pages 6978 . * tokenization - based - model*.      ccero  nogueira dos santos and bianca zadrozny . 2014 .",
    "learning character - level representations for part - of - speech tagging . in _ international conference on machine learning _ , pages 18181826 .",
    "* tokenization - based - model*.      kilian evang , valerio basile , grzegorz chrupala , and johan bos .",
    "elephant : sequence labeling for word and sentence segmentation . in",
    "_ conference on empirical methods in natural language processing _ , pages 14221426 .",
    "florian eyben , martin wllmer , bjrn  w. schuller , and alex graves .",
    "2009 . from speech to letters - using a novel neural network architecture for grapheme based asr . in _ ieee workshop on automatic speech recognition & understanding ( asru )",
    "_ , pages 376380 . *",
    "end - to - end - model*.    asli eyecioglu and bill keller .",
    "2016 . at semeval-2016 task 1 : sentence representation with character n - gram embeddings for semantic textual similarity .",
    "semeval-2016 : the 10th international workshop on semantic evaluation _ , pages 13201324 .",
    "* bag - of - ngram - model*.    manaal faruqui , yulia tsvetkov , graham neubig , and chris dyer .",
    "morphological inflection generation using character sequence to sequence learning . in _ conference of the north american chapter of the association for computational linguistics / human language technologies_. * tokenization - based - model*.            e.  dario gutirrez , roger levy , and benjamin bergen .",
    "finding non - arbitrary form - meaning systematicity using string - metric learning for kernel regression . in _ annual meeting of the association for computational linguistics_. * tokenization - based - model*.    qi  han , junfei guo , and hinrich schtze .",
    "codex : combining an svm classifier and character n - gram language models for sentiment analysis on twitter text . in _",
    "second joint conference on lexical and computational semantics ( * sem ) , volume 2 : proceedings of the seventh international workshop on semantic evaluation ( semeval 2013 ) _ , pages 520524 .",
    "* end - to - end - model*.      po - sen huang , xiaodong he , jianfeng gao , li  deng , alex acero , and larry heck .",
    "learning deep structured semantic models for web search using clickthrough data . in _ acm international conference on information and knowledge management _ , pages 23332338 .",
    "* tokenization - based - model*.        katharina kann and hinrich schtze .",
    "2016b . single - model encoder - decoder with explicit morphological representation for reinflection . in _ annual meeting of the association for computational linguistics_. * tokenization - based - model*.    katharina kann , ryan cotterell , and hinrich schtze .",
    "2016 . neural morphological analysis : encoding - decoding canonical segments . in _ conference on empirical methods in natural language processing_.",
    "* tokenization - based - model*.    ronald  m. kaplan .",
    "a method for tokenizing text . in mickael",
    "suominen , antti arppe , anu airola , orvokki heinmki , matti miestamo , urho mtt , jussi niemi , kari  k. pitknen , and kaius sinnemki , editors , _ a man of measure .",
    "festschrift in honour of fred karlsson in his 60th birthday _ , pages 5766 .",
    "linguistic association of finland .",
    "kimmo kettunen , paul mcnamee , and feza baskaya .",
    "2010 . using syllables as indexing terms in full - text information retrieval . in _ human language technologies - the baltic perspective - proceedings of the fourth international conference baltic hlt 2010 , riga , latvia , october 7 - 8 , 2010 _ , pages 225232 .",
    "* bag - of - ngram - model*.    yoon kim , yacine jernite , david sontag , and alexander  m. rush .",
    "character - aware neural language models . in _",
    "aaai conference on artificial intelligence _ ,",
    "pages 27412749 .",
    "* tokenization - based - model*.    dan klein , joseph smarr , huy nguyen , and christopher  d. manning .",
    "2003 . named entity recognition with character - level models . in _ computational natural language learning _ , pages 180183 .",
    "* tokenization - based - model*.    wanqiu kou , fang li , and timothy baldwin .",
    "automatic labelling of topic models using word vectors and letter trigram vectors . in _ asia information retrieval societies conference ( airs ) _ , pages 253264 . * bag - of - ngram - model*.    guillaume lample , miguel ballesteros , sandeep subramanian , kazuya kawakami , and chris dyer . 2016 .",
    "neural architectures for named entity recognition . in _ conference of the north american chapter of the association for computational linguistics / human language technologies_.",
    "* tokenization - based - model*.        wang ling , tiago lus , lus marujo , ramn  fernandez astudillo , silvio amir , chris dyer , alan  w black , and isabel trancoso .",
    "finding function in form : compositional character models for open vocabulary word representation . in _ conference on empirical methods in natural language processing_. * tokenization - based - model*.      minh - thang luong and christopher  d. manning .",
    "2016 . achieving open vocabulary neural machine translation with hybrid word - character models . in _ annual meeting of the association for computational linguistics_. * tokenization - based - model*.    minh - thang luong , richard socher , and christopher  d. manning",
    "better word representations with recursive neural networks for morphology . in _",
    "computational natural language learning_. * tokenization - based - model*.    thang luong , ilya sutskever , quoc  v. le , oriol vinyals , and wojciech zaremba .",
    "2015 . addressing the rare word problem in neural machine translation . in _ annual meeting of the association for computational linguistics",
    "_ , pages 1119 .",
    "* tokenization - based - model*.            tomas mikolov , ilya sutskever , kai chen , gregory  s. corrado , and jeffrey dean . 2013 . distributed representations of words and phrases and their compositionality . in _ advances in neural information processing systems _",
    ", pages 31113119 .",
    "jeffrey pennington , richard socher , and christopher  d. manning .",
    "glove : global vectors for word representation . in _ conference on empirical methods in natural language processing _ , pages 15321543",
    ".    barbara plank , anders sgaard , and yoav goldberg . 2016 .",
    "multilingual part - of - speech tagging with bidirectional long short - term memory models and auxiliary loss . in _ annual meeting of the association for computational linguistics_. * tokenization - based - model*.    yanjun qi , sujatha  g. das , ronan collobert , and jason weston .",
    "deep learning for character - based information extraction . in _",
    "european conference on information retrieval _ , pages 668674 .",
    "* tokenization - based - model*.    pushpendre rastogi , benjamin van durme , and raman arora .",
    "multiview lsa : representation learning via generalized cca . in _",
    "north american chapter of the association for computational linguistics _ , pages 556566 .",
    "pushpendre rastogi , ryan cotterell , and jason eisner . 2016 .",
    "weighting finite - state transductions with neural context . in _ conference of the north american chapter of the association for computational linguistics / human language technologies _ , pages 623633 . * tokenization - based - model*.      rico sennrich , barry haddow , and alexandra birch .",
    "neural machine translation of rare words with subword units . in _ annual meeting of the association for computational linguistics_.",
    "* tokenization - based - model*.    m.  ali  basha shaik , amr  el - desoky mousa , ralf schlter , and hermann ney . 2013 .",
    "feature - rich sub - lexical language models using a maximum entropy approach for german lvcsr . in _",
    "annual conference of the international speech communication association _ , pages 34043408 .",
    "* tokenization - based - model*.    henning sperr , jan niehues , and alex waibel .",
    "letter n - gram - based input encoding for continuous space language models . in _",
    "workshop on continuous vector space models and their compositionality _ , pages 3039 , august . * tokenization - based - model*.    karl stratos , michael collins , and daniel  j. hsu . 2015 .",
    "model - based word embeddings from decompositions of count matrices . in _ annual meeting of the association for computational linguistics _ , pages 12821291 .",
    "jrg tiedemann and preslav nakov .",
    "2013 . analyzing the use of character - level translation with sparse and noisy datasets . in _ recent advances in natural language processing , ranlp 2013 , 9 - 11 september , 2013 , hissar , bulgaria _ , pages 676684 .",
    "* end - to - end - model*.          ekaterina vylomova , trevor cohn , xuanli he , and gholamreza haffari .",
    "word representation models for morphologically rich languages in neural machine translation .",
    ", abs/1606.04217 . * tokenization - based - model*.      john wieting , mohit bansal , kevin gimpel , and karen livescu .",
    "charagram : embedding words and sentences via character n - grams . in _ conference on empirical methods in natural language processing_.",
    "* bag - of - ngram - model*.    y.  wu , m.  schuster , z.  chen , q.  v. le , m.  norouzi , w.  macherey , m.  krikun , y.  cao , q.  gao , k.  macherey , j.  klingner , a.  shah , m.  johnson , x.  liu ,  .",
    "kaiser , s.  gouws , y.  kato , t.  kudo , h.  kazawa , k.  stevens , g.  kurian , n.  patil , w.  wang , c.  young , j.  smith , j.  riesa , a.  rudnick , o.  vinyals , g.  corrado , m.  hughes , and j.  dean . 2016 .",
    "google s neural machine translation system : bridging the gap between human and machine translation . .",
    "* end - to - end - model*.      ruobing xie , zhiyuan liu , jia jia , huanbo luan , and maosong sun .",
    "representation learning of knowledge graphs with entity descriptions . in _",
    "aaai conference on artificial intelligence _ ,",
    "pages 26592665 .",
    "xiang zhang , junbo zhao , and yann lecun .",
    "character - level convolutional networks for text classification . in _ advances in neural information processing systems _ , pages 649657 .",
    "* end - to - end - model*."
  ],
  "abstract_text": [
    "<S> we introduce the first generic text representation model that is completely nonsymbolic , i.e. , it does not require the availability of a segmentation or tokenization method that attempts to identify words or other symbolic units in text . </S>",
    "<S> this applies to training the parameters of the model on a training corpus as well as to applying it when computing the representation of a new text . </S>",
    "<S> we show that our model performs better than prior work on an information extraction and a text denoising task . </S>"
  ]
}