{
  "article_text": [
    "interactive data exploration is a prerequisite in model design .",
    "it requires the analyst to execute a series of exploratory queries in order to find patterns or relationships in the data . in the big data context , it is likely that the entire process is time - consuming even for the fastest parallel database systems given the size of the data and the sequential nature of exploration  the next query to be asked is always dependent on the previous .",
    "online aggregation  @xcite aims at reducing the duration of the exploration process by allowing the analyst to rule out the non - informative queries early in the execution . to make this possible , an estimate to the final result of the query with progressively narrower confidence bounds is continuously returned to the analyst . when the confidence bounds become tight enough  typically early in the processing  the analyst can decide to stop the execution and focus on the next query .",
    "although introduced in the late nineties , it is hard to find any commercial system that supports online aggregation even today  @xcite . in our opinion ,",
    "there are multiple reasons that hindered adoption given that multiple academic prototypes  @xcite proved the feasibility of the approach .",
    "the first argument underlines the negative effect online aggregation has on normal query execution , with increases of at least @xmath1 being common  @xcite .",
    "this is regarded as unacceptable in the commercial world who focus instead on improving the performance of traditional database systems .",
    "the second argument addresses the lack of a unified approach to express estimation models .",
    "the authors went multiple times through the process of designing a new estimation model in previous projects  @xcite .",
    "each time , major changes to the overall system architecture and a significant amount of implementation were required .",
    "these obstructed the ultimate goal of designing a better estimation model .",
    "the final argument against online aggregation adoption we mention is the requirement to re - implement the data processing system from ground up in order to support estimation .",
    "pf - ola ( * * p**arallel * * f**ramework for * * o**n**l**ine * * a**ggregation ) overpasses these limitations and brings online aggregation closer to broader adoption , especially for big data interactive exploration . pf - ola is a shared nothing parallel system executing complex analytical queries over terabytes of data very efficiently .",
    "estimates and corresponding confidence bounds for the query result are computed during the entire query execution process without incurring any noticeable overhead .",
    "thus , a user executing a long - running query starts getting estimates with provable guarantees almost instantly after query processing starts .",
    "as the query progresses , the width of the confidence bounds shrinks progressively , converging to the true result when the query is complete .",
    "as far as we know , pf - ola is the first system that incurs virtually no overhead on top of the query execution time corresponding to the non - interactive execution .",
    "this is due to the extensive use of parallelism at all levels of the system  including storage , inside a single node , and across all the nodes  and to a judicious overlapping of query execution and estimation . at the same time , neither the estimator accuracy nor the convergence rate are negatively impacted , but rather the convergence receives a significant boost from the parallel discovery of result tuples .",
    "this results in fast and accurate estimations even for highly selective queries with very low result cardinality  the needle in the haystack problem  or in the case of skewed data .    a second aspect that differentiates pf - ola from other online aggregation systems",
    "is the unified approach to express estimation models . in pf - ola , the estimation is closely intertwined with the actual computation , with both clearly separated from query execution . any computation is expressed as a user - defined aggregate ( uda )  @xcite .",
    "the user is responsible for implementing a standard interface while the framework takes care of all the issues related to parallel execution and data flow . adding online estimation to the computation is a matter of extending the uda state and interface with constructs corresponding to the estimator . in order to apply a different estimation model to a computation",
    ", a corresponding uda has to be implemented .",
    "no changes have to be made to the implementation of the computation or to the implementation of the framework .",
    "this provides tremendous flexibility in designing a variety of estimation models .    to verify the expressiveness of the framework and",
    "test the performance , we design an asynchronous sampling estimator specifically targeted at parallel online aggregation . the estimator is defined over multiple data partitions which can be independently sampled in parallel .",
    "this allows for accurate estimates to be computed even when there is considerable variation between the sampling rate across partitions .",
    "we analyze the properties of the estimator and compare it with two other sampling - based estimators proposed for parallel online aggregation  a synchronous estimator  @xcite and a stratified sampling estimator  @xcite .",
    "all these estimators are expressed using the extended uda interface and executed without any changes to the framework , thus proving the generality of our approach .    the complete system re - implementation",
    "is avoided in pf - ola through the use of the generic uda mechanism to express both computation as well as estimation .",
    "the framework defines only the execution strategy without imposing any limitation on the actual computation . as long as the user - provided computation and estimation model can be expressed using the generic mechanism ,",
    "there is no need to change the pf - ola implementation . and , as shown in  @xcite , the complexity of the tasks that can be expressed with the uda mechanism ranges from simple and group - by aggregations to clustering and convex optimization problems applied in machine learning .",
    "our main contributions can be summarized as follows :    we design the first framework for parallel online aggregation that incurs virtually no overhead on top of the actual execution . estimates and corresponding confidence bounds are continuously computed based on samples extracted from data during the entire processing .    we define a generic interface to express estimation models by extending the well - known uda mechanism .",
    "the user is required to represent the model using a pre - defined set of methods while the framework handles all the execution details in a parallel environment .",
    "we implement the online aggregation framework in a highly - parallel processing system for the execution of arbitrary jobs .",
    "the result is an extremely efficient prototype for big data analytics with support for online aggregation .",
    "we propose a novel asynchronous sampling estimator for parallel online aggregation that we implement and execute inside the framework .",
    "we provide statistical analysis , verify the correctness , and show the superior performance when compared with two other estimators previously proposed in the literature .",
    "we run an extensive set of experiments to benchmark the estimator . when executed by the framework over a massive @xmath0 tpc - h instance  @xcite",
    ", the estimator provides accurate confidence bounds early in the execution even when the cardinality of the result is seven orders of magnitude smaller than the dataset size or when data are skewed without incurring any noteworthy overhead on top of the normal execution .",
    "moreover , the estimator exhibits high resilience in the wake of processing node delays and failures .",
    "[ [ roadmap . ] ] roadmap .",
    "+ + + + + + + +    in the remainder of the paper , we first introduce a set of preliminary concepts in section  [ sec : prelim ] .",
    "parallel online aggregation is formalized in section  [ sec : par - online - agg ] which contains a detailed presentation of our proposed estimator and a thorough comparison with existent estimators .",
    "the design of the framework and the implementation details are discussed in section  [ sec : pf - ola ] .",
    "example estimators and their implementation in pf - ola are presented in section  [ sec : est - examples ] , while section  [ sec : empirical ] contains the empirical evaluation of the framework and of the proposed estimator .",
    "related work is discussed in section  [ sec : rel - work ] .",
    "we conclude by summarizing the main findings of this paper and providing future directions in section  [ sec : conclusions ] .",
    "we consider aggregate computation in a parallel cluster environment consisting of multiple processing nodes .",
    "each processing node has a multi - core processor consisting of one or more cpus , thus introducing an additional level of parallelism .",
    "data are partitioned into fixed size chunks that are stored across the processing nodes .",
    "parallel aggregation is supported by processing multiple chunks at the same time both across nodes as well as across the cores inside a node .",
    "we focus on the computation of general ` select - project - join ` ( spj ) queries having the following sql form  @xcite : @xmath2 where @xmath3 is the concatenation operator , ` f ` is an arbitrary _ associative decomposable aggregate function _",
    "@xcite over the tuple created by concatenating @xmath4 and @xmath5 , and ` p ` is some boolean predicate that can embed selection and join predicates .",
    "the class of associative decomposable aggregate functions , i.e. , functions that are associative and commutative , is fairly extensive and includes the majority of standard sql aggregate functions .",
    "associative decomposable aggregates allow for the maximum degree of parallelism in their evaluation since the computation is independent of the order in which data inside a chunk are processed as well as of the order of the chunks , while partial aggregates computed over different chunks can be combined together straightforwardly .",
    "while the paper does not explicitly discuss aggregate functions other than ` sum ` , functions such as ` count ` , ` average ` , ` std dev ` , and ` variance ` can all be handled easily ",
    "they are all associative decomposable .",
    "for example , ` count ` is a special case of ` sum ` where ` f`(@xmath6 ) = 1 for any tuple , while ` average ` can be computed as the ratio of ` sum ` and ` count ` .",
    "` group by ` queries can also be handled using the methods in this paper by simply treating each group as a separate query and running all queries simultaneously ; then all of the estimates are presented to the user .",
    "for each group , a version of ` p ` is used that accepts only tuples from that particular group .",
    "aggregate evaluation takes two forms in parallel databases .",
    "they differ in how the partial aggregates computed for each chunk are combined together . in the centralized approach",
    ", all the partial aggregates are sent to a common node ",
    "the coordinator  that is further aggregating them to produce the final result . as an intermediate step",
    ", local aggregates can be first combined together and only then sent to the coordinator . in the parallel approach ,",
    "the nodes are first organized into an aggregation tree .",
    "each node is responsible for aggregating its local data and the data of its children .",
    "the process is executed level by level starting from the leaves , with the final result computed at the root of the tree .",
    "the benefit of the parallel approach is that it also parallelizes the aggregation of the partial results across all the nodes rather than burdening a single node ( with data and computation ) .",
    "the drawback is that in the case of a node failure it is likely that more data are lost .",
    "notice that these techniques are equally applicable inside a processing node , at the level of a multi - core processor .",
    "pf - ola supports both centralized and parallel aggregation at the level of the entire cluster as well as inside each node .",
    "the strategy to be applied is determined dynamically for each query . moreover ,",
    "when online aggregation is executed simultaneously with the normal query execution , the aggregation strategy is chosen individually for each of the tasks .",
    "thus , for example , it is possible to have the query executed with parallel aggregation , while the estimation is centralized .",
    "the idea in online aggregation is to compute only an estimate of the aggregate result based on a sample of the data  @xcite . in order to provide any useful information though",
    ", the estimate is required to be accurate and statistically significant .",
    "different from one - time estimation  @xcite that might produce very inaccurate estimates for arbitrary queries , online aggregation is an iterative process in which a series of estimators with improving accuracy are generated .",
    "this is accomplished by including more data in estimation , i.e. , increasing the sample size , from one iteration to another .",
    "the end - user can decide to run a subsequent iteration based on the accuracy of the estimator .",
    "although the time to execute the entire process is expected to be much shorter than computing the aggregate over the entire dataset , this is not guaranteed , especially when the number of iterations is large .",
    "other issues with _ iterative online aggregation _",
    "@xcite regard the choice of the sample size at each iteration and reusing the work done from one iteration to the following .",
    "an alternative that avoids these problems altogether is to completely _ overlap query processing with estimation _  @xcite .",
    "as more data are processed towards computing the final aggregate , the accuracy of the estimator improves accordingly . for this to be true",
    "though , data are required to be processed in a statistically meaningful order , i.e. , random order , to allow for the definition and analysis of the estimator .",
    "this is typically realized by randomizing data during the loading process .",
    "the drawback of the overlapped approach is that the same query is essentially executed twice  once towards the final aggregate and once for computing the estimator . as a result",
    ", the total execution time in the overlapped case is expected to be higher when compared to the time it takes to execute each task separately .",
    "pf - ola is designed as an online aggregation system which overlaps query execution with estimation .",
    "the motivation for this choice is the ever increasing number of cores available on modern cpus . since i / o is the bottleneck in database processing , the additional computation power is not utilized unless concurrent tasks are found and executed . given that the estimation process requires access to the same data as normal processing , estimation is a natural candidate for overlapped execution . while it is straightforward to execute these two processes concurrently , the challenge is how to realize this such that the normal query execution time is not increased at all",
    " this should always be possible as long as there are non - utilized cores on the processing node .",
    "we show how pf - ola achieves this goal by carefully scheduling access to shared data across the two tasks .",
    "an online aggregation system provides estimates and confidence bounds during the entire query execution process .",
    "as more data are processed , the accuracy of the estimator increases while the confidence bounds shrink progressively , converging to the actual query result when the entire data have been processed .",
    "there are multiple aspects that have to be considered in the design of a parallel online aggregation system .",
    "first , a mechanism that allows for the computation of partial aggregates has to be devised .",
    "second , a parallel sampling strategy to extract samples from data over which partial aggregates are computed has to be designed .",
    "each sampling strategy leads to the definition of an estimator for the query result , estimator that has to be analyzed in order to derive confidence bounds .",
    "we discuss in details each of these aspects for the overlapped online aggregation approach in this section .",
    "then , in section  [ sec : pf - ola ] , we show how everything is implemented in the pf - ola framework .",
    "the first requirement in any online aggregation system is a mechanism to compute partial aggregates over some portion of the data .",
    "partial aggregates are typically a superset of the query result since they have to contain additional data required for estimation .",
    "the partial aggregation mechanism can take two forms .",
    "we can fix the subset of the data used in partial aggregation and execute a normal query .",
    "or we can interfere with aggregate computation over the entire dataset to extract partial results before the computation is completed .",
    "the first alternative corresponds to iterative online aggregation , while the second to overlapped execution .",
    "partial aggregation in a parallel setting raises some interesting questions . for iterative online aggregation , the size and location of the data subset used to compute the partial aggregate have to be determined .",
    "it is common practice to take the same amount of data from each node in order to achieve load balancing . or to have each node process a subset proportional to its data as a fraction from the entire dataset .",
    "notice though that it is not necessary to take data from all the nodes . in the extreme case ,",
    "the subset considered for partial aggregation can be taken from a single node .",
    "once the data subset at each node is determined , parallel aggregation proceeds normally , using either the centralized or parallel strategy . in the case of overlapped execution , a second process that simply aggregates the current results at each node has to be triggered whenever a partial aggregate is computed",
    ". the aggregation strategy can be the same or different from the strategy used for computing the final result .",
    "centralized aggregation might be more suitable though due to the reduced interference .",
    "the amount of data each node contributes to the result is determined only by the processing speed of the node . since the work done for partial aggregation",
    "is also part of computing the final aggregate , it is important to reuse the result so that the overall execution time is not increased unnecessarily .      in order to provide any information on the final result , partial aggregates have to be statistically significant .",
    "it has to be possible to define and analyze estimators for the final result using partial aggregates .",
    "online aggregation imposes an additional requirement .",
    "the accuracy of the estimator has to improve when more data are used in the computation of partial aggregates . in the extreme case of using the entire dataset to compute the partial aggregate , the estimator collapses on the final result .",
    "the net effect of these requirements is that the data subset on which the partial aggregate is computed can not be arbitrarily chosen . since sampling satisfies these requirements , the standard approach in online aggregation is to choose the subset used for partial aggregation as a random sample from the data .",
    "[ [ centralized - sampling . ] ] centralized sampling .",
    "+ + + + + + + + + + + + + + + + + + + + +    thus , an important decision that has to be taken when designing an online aggregation system is how to generate random samples . according to the literature  @xcite",
    ", there are two methods to generate samples from the data in a centralized setting .",
    "the first method is based on using an index that provides the random order in which to access the data .",
    "while it does not require any pre - processing , this method is highly inefficient due to the large number of random accesses to the disk .",
    "the second method is based on the idea of storing data in random order on disk such that a sequential scan returns random samples at any position .",
    "although this method requires considerable pre - processing at loading time to permute data randomly , it is the preferred randomization method in online aggregation systems  @xcite since the cost is paid only once and it can be amortized over the execution of multiple queries  the indexing method incurs additional cost for each query . as a result , pf - ola implements a parallel version of the random shuffling method .",
    "[ [ sampling - synopses . ] ] sampling synopses .",
    "+ + + + + + + + + + + + + + + + + +    it is important to make the distinction between the runtime sampling methods used in online aggregation and estimation based on static samples taken offline  @xcite , i.e. , sampling synopses . in the later case ,",
    "a sample of fixed size is taken only once and all subsequent queries are answered using the sample .",
    "this is typically faster than executing sampling at runtime , during query processing .",
    "the problem is that there are queries that can not be answered from the sample accurately enough , for example , highly selective queries .",
    "the only solution in this case is to extract a larger sample entirely from scratch which is prohibitively expensive .",
    "the sampling methods for online aggregation described previously avoid this problem altogether due to their incremental design that degenerates in a sample consisting of the entire dataset in the worst case .",
    "[ [ sample - size . ] ] sample size .",
    "+ + + + + + + + + + + +    determining the correct sample size to allow for accurate estimations is an utterly important problem in the case of sampling synopses and iterative online aggregation .",
    "if the sample size is not large enough , the entire sampling process has to be repeated , with unacceptable performance consequences .",
    "while there are methods that guide the selection of the sample size for a given accuracy in the case of a single query , they require estimating the variance of the query estimator  an even more complicated problem . in the case of overlapped online aggregation , choosing the sample size is not a problem at all since the entire dataset is processed in order to compute the correct result .",
    "the only condition that has to be satisfied is that the data seen up to any point during processing represent a sample from the entire dataset .",
    "as more data are processed towards computing the query result , the sample size increases automatically .",
    "both runtime sampling methods discussed previously satisfy this property .",
    "[ [ stratified - sampling . ] ] stratified sampling .",
    "+ + + + + + + + + + + + + + + + + + + +    there are multiple alternatives to obtain a sample from a partitioned dataset  the case in a parallel setting .",
    "the straightforward solution is to consider each partition independently and to apply centralized sampling algorithms inside the partition ( figure  [ fig : paragg - centr ] ) .",
    "this type of sampling is known as _ stratified sampling _",
    "while stratified sampling generates a random sample for each partition , it is not guaranteed that when putting all the local samples together the resulting subset is a random sample from the entire data . for this to be the case",
    ", it is required that the probability of a tuple to be in the sample is the same across all the partitions .",
    "the immediate solution to this problem is to take local samples that are proportional with the partition size .",
    "* input : * number of nodes @xmath7 ; random hash function @xmath8 + * output : * partition @xmath9 of @xmath10    let @xmath11 add @xmath12 to set @xmath13 in partition @xmath14    [ [ global - randomization . ] ] global randomization .",
    "+ + + + + + + + + + + + + + + + + + + + +    a somehow more complicated solution is to make sure that a tuple can reside at any position in any partition_global randomization _ ( figure  [ fig : paragg - tree ] ) .",
    "this can be achieved by randomly shuffling the data across all the nodes  as a direct extension of the similar centralized approach .",
    "the global randomization process consists of two stages , each executed in parallel at every node . in the first stage ( algorithm  [ alg : random - split ] ) , each node partitions the local data into sets corresponding to all the other nodes in the environment .",
    "the assignment of an item to a partition is based on a random hash function which requires as argument a random value independent of the item in order to randomize the data .",
    "the index of the item in the dataset ( round - robin partitioning ) might be a good choice as long as the set assignment does not follow a predictable pattern .",
    "an even better choice is a random value generated for the item .    *",
    "input : * set @xmath15 of data fragments from all @xmath7 nodes ; random number generator _",
    "rng _ + * output : * random permutation of @xmath16    let @xmath17__rng__@xmath18 be a random number for item @xmath19 sort @xmath16 in the increasing order of @xmath20    in the second stage of the randomization process ( algorithm  [ alg : random - permute ] ) , each node generates a random permutation of the data received from all the other nodes  random shuffling .",
    "this is required in order to separate the items received from the same origin .",
    "the standard method for random shuffling consists in generating a random value for each item in the dataset and then sorting the items according to these random values .",
    "notice that using the random values from the origin node is not guaranteed to produce a random permutation across all the sets .",
    "the main benefit provided by global randomization is that it simplifies the complexity of the sampling process in a highly - parallel asynchronous environment .",
    "this in turn allows for compact estimators to be defined and analyzed  a single estimator across the entire dataset .",
    "it also supports more efficient sampling algorithms that require a reduced level of synchronization , as is the case with our estimator.moreover , global randomization has another important characteristic for online aggregation  it allows for incremental sampling .",
    "what this essentially means is that in order to generate a sample of a larger size starting from a given sample is enough to obtain a sample of the remaining size .",
    "it is not even required that the two samples are taken from the same partition since random shuffling guarantees that a sample taken from a partition is actually a sample from the entire dataset .",
    "equivalently , to get a sample from a partitioned dataset after random shuffling , it is not necessary to get a sample from each partition .    while random shuffling in a centralized environment is a time - consuming process executed in addition to data loading , global randomization in a parallel setting is a standard hash - based partitioning process executed as part of data loading . due to the benefits provided for workload balancing and for join processing , hash - based partitioning",
    "is heavily used in parallel data processing even without online aggregation .",
    "thus , we argue that global randomization for parallel online aggregation is part of the data loading process and it comes at no cost with respect to sampling .      while designing sampling estimators for online aggregation in a centralized environment is a well - studied problem , it is not so clear how these estimators can be extended to a highly - parallel asynchronous system with data partitioned across nodes . to our knowledge , there are two solutions to this problem proposed in the literature . in the first solution ,",
    "a sample over the entire dataset is built from local samples taken independently at each partition .",
    "an estimator over the constructed sample is then defined .",
    "we name this approach _",
    "single estimator_. in the single estimator approach , the fundamental question is how to generate a single random sample of the entire dataset from samples extracted at the partition level .",
    "the strategy proposed in  @xcite requires synchronization between all the sampling processes executed at partition level in order to guarantee that the same fraction of the data is sampled at each partition . to implement this strategy , serialized access to a common resource",
    "is required for each item processed .",
    "this results in a factor of four increase in execution time when estimation is active ( see the experimental evaluation section ) .    in the second solution , which we name _ multiple estimators _ ( figure  [ fig : paragg - centr ] ) ,",
    "an estimator is defined for each partition . as in stratified sampling theory  @xcite ,",
    "these estimators are then combined into a single estimator over the entire dataset .",
    "the solution proposed in  @xcite follows this approach .",
    "the main problem with the multiple estimators strategy is that the final result computation and the estimation are separated processes with different states that require more complicated implementation .",
    "we propose an asynchronous sampling estimator specifically targeted at parallel online aggregation that combines the advantages of the existing strategies .",
    "we define our estimator as in the single estimator solution , but without the requirement for synchronization across the partition - level sampling processes which can be executed independently ( figure  [ fig : paragg - tree ] ) .",
    "this results in much better execution time . when compared to the multiple estimators",
    "approach , our estimator has a much simpler implementation since there is complete overlap between execution and estimation . in this section",
    ", we analyze the properties of the estimator and compare it with the two estimators it inherits from .",
    "then , in section  [ sec : est - examples ] we provide insights into the actual implementation in pf - ola , while in section  [ sec : empirical ] we present experimental results to evaluate the accuracy of the estimator and the runtime performance of the estimation .      to design estimators for the parallel aggregation problem",
    "we first introduce a generic sampling estimator for the centralized case .",
    "this is a standard estimator based on sampling without replacement  @xcite that is adequate for online aggregation since it provides progressively increasing accuracy .",
    "we define the estimator for the simplified case of aggregating over a single table and then show how it can be generalized to ` group by ` and general spj queries ( equation  [ eq : prob - def ] ) in section  [ sec : est - examples ] .",
    "consider the dataset @xmath10 to have a single partition sorted in random order .",
    "the number of items in @xmath10 ( size of @xmath10 ) is @xmath21 .",
    "while sequentially scanning @xmath10 , any subset @xmath22 represents a random sample of size @xmath23 taken without replacement from @xmath10 .",
    "we define an estimator for the aggregate as follows : @xmath24 where @xmath25 has the properties given in lemma  [ lema : gen - est - moments ] :    [ lema : gen - est - moments ] @xmath25 is an unbiased estimator for the aggregation problem , i.e. , @xmath26 } = \\sum_{d \\in d , \\texttt{p}(d ) } \\texttt{f}(d)$ ] , where @xmath26}$ ] is the expectation of @xmath25 .",
    "the variance of @xmath25 is equal to : @xmath27 \\\\",
    "\\end{split}\\ ] ]    it is important to notice the factor @xmath28 in the variance numerator which makes the variance to decrease while the size of the sample increases . when the sample is the entire dataset , the variance becomes zero , thus the estimator is equal to the exact query result . the standard approach to derive confidence bounds  @xcite is to assume a normal distribution for estimator @xmath25 with the first two frequency moments given by @xmath26}$ ] and @xmath29 .",
    "the actual bounds are subsequently computed at the required confidence level from the cumulative distribution function ( cdf ) of the normal distribution .",
    "since the width of the confidence bounds is proportional with the variance , a decrease in the variance makes the confidence bounds to shrink",
    ".    a closer look at the variance formula in equation  [ eq : gen - est - var ] reveals the dependency on the entire dataset @xmath10 through the two sums over all the items @xmath30 that satisfy the selection predicate ` p ` .",
    "unfortunately , when executing the query we have access only to the sampled data .",
    "thus , we need to compute the variance from the sample .",
    "we do this by defining a variance estimator , @xmath31 , with the following properties :    [ lema : gen - est - var - est ] the estimator @xmath32 \\\\",
    "\\end{split}\\ ] ] is an unbiased estimator for the variance in equation  [ eq : gen - est - var ] .",
    "having the two estimators @xmath25 and @xmath31 computed over the sample @xmath33 , we are in the position to provide the confidence bounds required by online aggregation in a centralized environment .",
    "the next step is to extend the generic estimators to the parallel setting of the pf - ola framework where data is partitioned across multiple processing nodes .      in the pf - ola framework ,",
    "the dataset @xmath10 is partitioned across @xmath7 processing nodes , i.e. , @xmath34 .",
    "a sample @xmath35 , @xmath36 , is taken independently at each node .",
    "these samples are then put together in a sample @xmath37 over the entire dataset @xmath10 . to guarantee that @xmath33 is indeed a sample from @xmath10 , in the case of the synchronized estimator in  @xcite it",
    "is enforced that the sample ratio @xmath38 is the same across all the nodes . for the estimator we propose ,",
    "we let the nodes run independently and only during the partial aggregation stage we combine the samples from all the nodes as @xmath33 .",
    "thus , nodes operate asynchronously at different speed and produce samples with different size . the global randomization guarantees though that the combined sample @xmath33 is indeed a sample over the entire dataset . as a result , the generic sampling estimator in equation  ( [ eq : gen - est ] ) can be directly applied to this distributed setting without any modifications .      for multiple estimators",
    ", the aggregate @xmath39 can be decomposed as @xmath40 , with each node computing the sum over the local partition in the first stage followed by summing - up the local results to get the overall result in the second stage .",
    "an estimator @xmath41 is defined for each partition based on the generic sampling estimator in equation  ( [ eq : gen - est ] ) .",
    "we can then immediately infer that the sum of the estimators @xmath42 , @xmath43 , is an unbiased estimator for the query result and derive the variance @xmath44 if the sampling process across partitions is independent . since the samples are taken independently from each data partition , local randomization of the data at each processing node is sufficient for the analysis to hold .",
    "we propose an estimator for parallel online aggregation based on the _ single estimator _",
    "the main difference is that our estimator is completely asynchronous and allows fully parallel evaluation .",
    "we show how it can be derived and analyzed starting from a generic sampling estimator for centralized settings .",
    "the implementation in pf - ola for three different aggregation problems of various complexity is presented in section  [ sec : est - examples ] .",
    "we conclude with a detailed comparison with a stratified sampling estimator ( or _ multiple estimators _ ) along multiple dimensions :    _ data randomization .",
    "_ while the multiple estimators approach requires only local randomization , the single estimator approach requires global randomization across all the nodes in the system .",
    "although this might seem a demanding requirement , the randomization process can be entirely overlapped with data loading as part of hash - based data partitioning .    _ dataset information . _",
    "multiple estimators requires each node to have knowledge of the local partition cardinality , i.e. , @xmath45 .",
    "single estimator needs only full cardinality information , i.e. , @xmath21 , where the estimation is invoked .    _ accuracy . _",
    "according to the stratified sampling theory , multiple estimators provides better accuracy when the size of the sample at each node is proportional with the local dataset size ( but not a requirement )  @xcite .",
    "this is not true in the general case though with the variance of the estimators being entirely determined by the samples at hand .",
    "given that pf - ola is a highly asynchronous framework , this optimal condition is hard to enforce .    _ convergence rate . _ as with accuracy , it is not possible to characterize the relative convergence rate of the two methods in the general case .",
    "nonetheless , we can argue that multiple estimators is more sensitive to discrepancies in processing across the nodes since the effect on variance is only local .",
    "consider for example the case when one variance is considerably smaller than the others .",
    "its effect on the overall variance is asymptotically limited by the fraction it represents from the overall variance rather than the overall variance .",
    "_ fault tolerance . _",
    "the effect of node failure on the estimation process is catastrophic for multiple estimators . if one node can not be accessed , it is impossible to compute the estimator and provide bounds since the corresponding variance is infinite . for single estimator ,",
    "the variance decrease stops at a higher value than zero .",
    "this results in bounds that do not collapse on the true result even when all the available data is processed .",
    "online aggregation in pf - ola is a process consisting of multiple stages . in the first stage ,",
    "data are stored in random order on disk .",
    "this is an offline process executed at load time .",
    "once data are available , we can start executing aggregate queries .",
    "a query specifies the input data and the computation to be executed .",
    "every computation , including estimation , is expressed as a uda  the central abstraction in the pf - ola framework .",
    "the user application submits the query to a coordinator  a designated node managing the execution .",
    "it is the job of the coordinator to send the query further to the worker nodes , coordinate the execution , and return the result to the user application once the query is done .",
    "the result of a query is always a uda containing the final state of the aggregate ",
    "pf - ola takes udas as input and produces a uda as output by implementing the uda evaluation mechanism . when executing the query in non - interactive mode , the user application blocks until the final uda arrives from the coordinator . when running in interactive mode with online aggregation",
    "enabled , the user application emits requests to the coordinator asking for the current state of the computation .",
    "evidently , the returned uda has an incomplete state since it is computed only over a portion of the data .",
    "the user application can use this partial uda in many different ways .",
    "computing online estimators and confidence bounds is only one of them .    to our knowledge ,",
    "pf - ola is the first online aggregation system in which the estimation is driven by the user application rather than the system .",
    "the main reason for this design choice is our goal to allow maximum asynchrony between the nodes in the system and to minimize the number of communication messages .",
    "a query always starts executing in non - interactive mode .",
    "partial results are extracted asynchronously based only on user application requests .",
    "it is clear that generating partial results interferes with the actual computation of the query result . in the case of aggregate computation",
    "though , this is equivalent to early aggregation which is executed as part of the final aggregation nonetheless . given this high - level description of the framework , we concentrate our attention on the following two important questions :    * how to enhance the uda interface with estimation functionality without modifying the execution model ? * how to optimally overlap the estimation process with query execution ?",
    "udas  @xcite represent a mechanism to extend the functionality of a database with application - specific aggregate operators similar in nature to user - defined data types ( udt ) and user - defined functions ( udf )  @xcite .",
    "a uda is typically implemented as a class with a standard interface defining the following four methods  @xcite : ` init ` , ` accumulate ` , ` merge ` , and ` terminate ` .",
    "these methods operate on the ` state ` of the aggregate which is also part of the class . while the interface is standard , the user has complete freedom when defining the ` state ` and implementing the methods .",
    "the execution engine ( runtime ) computes the aggregate by scanning the input relation and calling the interface methods as follows . `",
    "init ` is called to initialize the state before the actual computation starts . `",
    "accumulate ` takes as input a tuple from the input relation and updates the state of the aggregate according to the user - defined code . `",
    "terminate ` is called after all the tuples are processed in order to finalize the computation of the aggregate . `",
    "merge ` is not part of the original specification  @xcite and is intended for use when the input relation is partitioned and multiple udas are used to compute the aggregate ( one for each partition ) .",
    "it takes as parameters two udas and it merges their states into the state of an output uda . in the end , all the udas are merged into a single one upon which ` terminate ` is called .",
    "a graphical depiction of the entire execution process is shown in figure  [ fig : uda ] .    in order to provide online estimates",
    ", it is clear that additional data need to be stored in the uda ` state ` .",
    "the exact data are determined by the actual estimator .",
    "since users implement the uda , they have complete control over what data go in the ` state ` .",
    "what users do not have control over though is the estimation process , driven by the uda interface and the invocation mechanism .",
    "this imposes restrictions on the set of estimators that can be implemented using the standard uda interface .",
    "thus , the uda interface also needs to be extended with additional functions . our goal is to limit the modifications to the uda interface and to the overall invocation process while still allowing for any estimator to be implemented in the pf - ola framework .",
    "one of the main contributions made in this paper is the design of extensions to the uda interface for online aggregation and the subsequent implementation in pf - ola .",
    "the first extension handles the communication problem .",
    "since udas are transferred between nodes and from the coordinator to the user application , a mechanism that does this transparently is required .",
    "given that the uda state is defined by the user , it is the writer of the uda who is in the position to specify what needs to be transferred in order to re - create an equivalent uda in the memory space of another process .",
    "thus , it is natural to apply the same principles at the core of uda and extend the uda interface with methods to ` serialize / deserialize ` the uda state .",
    "it is the job of the uda creator to implement these methods correctly and the responsibility of the framework to invoke them transparently .",
    "the second extension is specifically targeted at estimation modeling for online aggregation . to support estimation",
    ", the uda state needs to be enriched with additional data on top of the original aggregate .",
    "although it is desirable to have a perfect overlap between the final result computation and estimation , this is typically not possible . in the few situations when it is possible , no additional changes to the uda interface",
    "are required . for the majority of the cases",
    "though , the uda interface needs to be extended in order to distinguish between the final result and a partial result used for estimation . as we shall see in section  [ sec : est - examples ] , there are at least two methods that need to be added : ` estimatorterminate ` and ` estimatormerge ` . `",
    "estimatorterminate ` computes a local estimator at each node .",
    "it is invoked after merging the local udas during the estimation process . `",
    "estimatormerge ` is called to put together in a single uda the estimators computed at each node by ` estimatorterminate ` .",
    "it is invoked with udas originating at different nodes .",
    "it is important to notice that ` estimatorterminate ` is an intra - node method while ` estimatormerge ` is inter - node .",
    "it is possible to further separate the estimation from aggregate computation and have an intra - node ` estimatormerge ` and an inter - node ` estimatorterminate ` .",
    "while this adds more flexibility and might be required for particular estimation models , we have not encountered such a situation .    the third extension we add to the uda interface is the ` estimate ` method .",
    "it is invoked by the user application on the uda returned by the framework as a result of an estimation request .",
    "the complexity of this method can range from printing the uda state to complex statistical models . in the case of online aggregation , ` estimate ` computes an estimator for the aggregate result and corresponding confidence bounds . as with the other methods , the only restriction on ` estimate ` is that it can only access the uda state .    .extended uda interface . [ cols=\"<,<\",options=\"header \" , ]     to clarify the surprisingly low execution time",
    ", we analyze in detail the time it takes to execute the group - by small query . remember that we have tpc - h scale @xmath46 data on each node . `",
    "lineitem ` has @xmath47 tuples for this instance .",
    "this corresponds to @xmath48 tuples per disk .",
    "datapath uses columnar storage , thus it reads only the columns required by the query . in this case",
    "it reads @xmath49 columns , summing - up to @xmath50 bytes per tuple and @xmath51 per disk .",
    "given the @xmath52 disk bandwidth , the theoretical execution time is @xmath53 seconds .",
    "this comes close to the actual execution time of @xmath54 seconds , but is not exactly the same . here",
    "are the reasons .",
    "we monitored the disk bandwidth during the execution and it was only @xmath55 . according to ` iostat ` the disk was fully utilized , thus the execution was i / o - bound . with this bandwidth , the execution time is @xmath56 seconds .",
    "we already knew that the difference is taken by the time to setup the query , thus everything makes perfect sense .",
    "it is possible to run similar analyses for the other queries .",
    "more parameters such as the time to merge two glas , the time to serialize / deserialize the gla state , and the time to transfer the data between the nodes might need to be accounted for though .    although the scaleup of pf - ola can be inferred from the accuracy figures  the execution time corresponding to all the configurations is the same  figure  [ fig : scaleup ] shows explicitly the time it takes to run each task on 1 , 2 , 4 , and 8 nodes when the size of the data increases with a corresponding factor .",
    "since the execution time remains constant , this confirms that pf - ola scales - up linearly .",
    "the main findings of the experimental study are :    the study proves the expressiveness of the pf - ola framework .",
    "two estimation models are created for three different tasks , each consisting of two queries .",
    "they are all implemented as glas with the extended uda interface and executed successfully by the framework .",
    "the proposed single estimator has similar ttu and identical or better accuracy than multiple estimators .",
    "pf - ola is able to provide accurate estimations for the query result early in the execution even in the most difficult scenarios when only a handful of tuples are part of the result or when data are skewed .",
    "the asynchronous single estimator we propose is highly insensitive to processing node delays and failure .",
    "this is the signature characteristic when compared to the multiple estimators solution .",
    "no significant overhead is incurred by online aggregation for any of the queries .",
    "the execution is always i / o - bound .",
    "pf - ola has perfectly linear scaleup as the data and the processing resources increase proportionally .",
    "the correctness of the estimators is confirmed through monte carlo simulations .",
    "there is a plethora of work on online aggregation published in the database literature  @xcite starting with the seminal paper by hellerstein et al .",
    "we can broadly categorize this body of work into system design  @xcite , online join algorithms  @xcite , online algorithms for estimations other than join  @xcite , and methods to derive confidence bounds  @xcite .",
    "all of this work is targeted at single - node centralized environments .",
    "the parallel online aggregation literature is not as rich though .",
    "we identified only a relatively small number of research papers that are closely related to our work .",
    "we discuss them in details in the following .",
    "luo et al .",
    "@xcite extend the centralized ripple join algorithms  @xcite to a parallel setting .",
    "the proposed parallel hash ripple join algorithm is a non - blocking version of the parallel hybrid hash join algorithm that allows for estimates to the final query result to be computed . a stratified sampling estimator  @xcite is defined to compute the result estimate while confidence bounds can not always be derived .",
    "we implement a similar stratified sampling estimator in pf - ola and compare it with the estimator we propose .",
    "our focus is on analyzing the properties of the two estimators along a larger set of dimensions , including robustness , which is not discussed at all in  @xcite .",
    "moreover , the prototype system implementing the specific parallel hash ripple join algorithm is very particular to the proposed estimator .",
    "there is no common framework proposed for general parallel online aggregation .",
    "wu et al .",
    "@xcite extend online aggregation to distributed point - to - point ( p2p ) networks .",
    "they introduce a synchronized sampling estimator over partitioned data that requires data movement from storage nodes to processing nodes .",
    "we also implement this estimator in pf - ola and show the poor performance it achieves in a highly - parallel asynchronous system . in subsequent work  @xcite , wu et al .",
    "tackle online aggregation over multiple queries .",
    "they emphasize the benefits of global randomization as a sample generating method .",
    "recently , online aggregation in map - reduce emerged as a popular research area .",
    "this is mostly motivated by the poor performance of the map - reduce implementation in hadoop  @xcite , which makes online aggregation a necessity rather than a luxury in the context of big data .",
    "hadoop online ( hop )  @xcite is an extension to the hadoop framework which allows for partial aggregates to be extracted during execution using pipelining between operators .",
    "stock hadoop contains only blocking operators that materialize the intermediate results for fault - tolerance . as explained in section  [ ssec : par - online - partial - agg ] , partial result extraction is only the basic requirement for online aggregation .",
    "sampling and estimation provide significance to the partial results .",
    "hop is limited only to partial result extraction .",
    "there is no formal sampling or estimation involved .",
    "thus , hop is not an online aggregation system .",
    "it is a partial aggregation system .",
    "it is the work in  @xcite where hop is elevated to a real online aggregation system by providing an estimation mechanism .",
    "the proposed solution is a bayesian framework to handle the correlation between the time to process a data partition and the result it generates .",
    "this is required because chunks are treated as black boxes that only produce an aggregate .",
    "there is no information on what operation was performed to generate the aggregate or on the content of the chunk .",
    "based on the aggregates produced by the processed chunks and the time it took to schedule and process the chunk , a prediction is made for the aggregates in the chunks not scheduled yet  the processed chunks are an independent and identically distributed ( iid ) sample from the entire chunk population .",
    "the bayesian model is continuously updated as more chunks are processed .",
    "this results in more accurate estimates as more data are processed .",
    "although this estimation model is not based on sampling , it can still be expressed as a gla using the extended uda interface .",
    "we plan to do this in future work to further validate the expressiveness of the pf - ola framework .",
    "blinkdb  @xcite stores pre - computed samples of different sizes on disk .",
    "this requires a significant amount of additional storage on top of the original data which might be a problem if the dataset is massive .",
    "moreover , the time to compute the samples  even if executed offline  might be prohibitive .",
    "similar to iterative online aggregation , a query is evaluated on a chosen sample and an estimate is produced .",
    "if the accuracy is not satisfying , a subsequent query can be executed on a larger sample  in the worst case , the query is executed on the entire dataset .",
    "while this allows for discrete estimates of increasing accuracy , it can not support continuous estimation .",
    "determining the correct sample to execute the query on is a complicated problem that requires estimating the variance of the result .",
    "none of these problems arise in pf - ola as long as global randomization is executed on the data .",
    "this is a considerably less time - consuming process than taking samples of progressively increasing sizes .",
    "a somehow similar idea is used in earl  @xcite where a single sample is pre - computed .",
    "bootstrapping is then used to extract multiple samples from the pre - computed sample and compute estimates .",
    "the sample sizes are increased dynamically to provide better accuracy .",
    "different from blinkdb , if the large sample in earl has to be re - computed , this is done dynamically at runtime .",
    "different estimation algorithms are proposed in each reference we mention .",
    "no common framework for estimation exists .",
    "pf - ola provides a common framework to model a much larger class of estimation models . in terms of performance ,",
    "all the estimation methods incur considerable overhead .",
    "the only exception is pr - join  @xcite which combines a non - blocking join algorithm with temporary storage on solid - state drives ( ssd ) to produce the result tuples much faster , thus increasing the convergence rate .",
    "it is a centralized algorithm though .",
    "we present pf - ola , the first framework for parallel online aggregation that does not incur any noticeable overhead on top of the actual computation .",
    "the extensive use of parallelism at all levels of the system and the sound overlapping between computation and estimation make this possible .",
    "pf - ola provides an abstract interface enhancing the well - known uda to express any estimation model .",
    "the framework handles all the execution details in a parallel environment allowing the analyst to focus on estimation .",
    "we design a novel asynchronous sampling estimator for parallel online aggregation over partitioned data .",
    "while achieving similar accuracy to existing estimators , our estimator is much more robust to node delays and failures . to verify the capabilities of the framework",
    ", we compare our estimator with two existing estimators by implementing and executing them in the framework .",
    "the results confirm the ability of the framework to execute the estimators without incurring any remarkable overhead and to provide tight confidence bounds early in the execution for highly selective queries , skewed data , and other pathological cases .",
    "the reason for this is the extremely efficient tuple discovery mechanism that takes advantage of multi - node and multi - threaded parallelism .",
    "we plan to address some of the limitations of the framework and extend its capabilities in future work .",
    "we have already started to investigate how to provide online estimates for asynchronous parallel joins when none of the two relations fits in memory .",
    "we plan to incorporate other estimation methods than sampling in the framework , for example bayesian statistics and bootstrapping .",
    "our long - term goal is to provide online estimates for any computation without incurring any overhead .",
    "we believe this is possible given the amount of parallelism available in modern processors ."
  ],
  "abstract_text": [
    "<S> online aggregation provides estimates to the final result of a computation during the actual processing . </S>",
    "<S> the user can stop the computation as soon as the estimate is accurate enough , typically early in the execution . </S>",
    "<S> this allows for the interactive data exploration of the largest datasets .    in this paper </S>",
    "<S> we introduce the first framework for parallel online aggregation in which the estimation virtually does not incur any overhead on top of the actual execution . </S>",
    "<S> we define a generic interface to express any estimation model that abstracts completely the execution details . </S>",
    "<S> we design a novel estimator specifically targeted at parallel online aggregation . </S>",
    "<S> when executed by the framework over a massive @xmath0 tpc - h instance , the estimator provides accurate confidence bounds early in the execution even when the cardinality of the final result is seven orders of magnitude smaller than the dataset size and without incurring overhead . </S>"
  ]
}