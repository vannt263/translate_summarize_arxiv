{
  "article_text": [
    "inference on population genetic models such as coalescent trees is one representative example of cases when statistical analyses like bayesian inference can not operate because the likelihood function associated with the data is not completely known , i.e.  can not be computed in a manageable time @xcite .",
    "the fundamental reason for this impossibility is that the statistical model associated with coalescent data needs to integrate over trees of extreme complexity .    in such settings , traditional approximation tools based on monte carlo simulation @xcite from the bayesian posterior distribution",
    "are unavailable for all practical purposes . indeed , due to the complexity of the latent structures defining the likelihood ( such as the coalescent tree ) , simulation of those structures is too unstable to be trusted to bring a reliable approximation in a manageable time .",
    "such complex models call for a practical if cruder approximation method , the abc methodology being a serious contender , where abc stands for _ approximate bayesian computation_. @xcite and @xcite introduced abc methods as a rejection technique bypassing the computation of the likelihood function via a simulation from the corresponding distribution . for recent reviews on abc , see @xcite and @xcite .",
    "the wide and successful array of applications based on implementations of abc in genomics and ecology is covered by @xcite , while the number of publications relying on this technique runs in the hundreds .",
    "@xcite describe the use of model choice based on abc for distinguishing between different mutation models .",
    "the intuition behind the method is that the average abc acceptance rate associated with a given model is proportional to the marginal likelihood corresponding to this approximative model , when identical summary statistics , distance , and tolerance level are used for all models . in practice ,",
    "an estimate of the ratio of marginal likelihoods is given by the ratio of observed acceptance rates .",
    "using bayes formula , estimates of the posterior probabilities are straightforward to derive .",
    "this approach has been widely used in the literature ( see , e.g. , @xcite , @xcite , and @xcite , @xcite ) . note that @xcite is particularly influencial for the conclusion it derives from the abc analysis : the focus of this _ science _ paper is the european invasion of the western corn rootworm , which is north america s most destructive corn pest . because this pest was initially introduced in central europe , it was believed that subsequent outbreaks in western europe originated from this area .",
    "based on this abc model choice analysis of the genetic variability of the rootworm , the authors conclude that this belief is false : there have been at least three independent introductions from north america during the past two decades .",
    "an improvement to the above estimate is due to @xcite , thanks to a regression regularisation . in this approach .",
    "model indices are processed as categorical variables in a formal multinomial ( polychotomous ) regression .",
    "for instance , when comparing two models , this leads to a standard logistic regression .",
    "rejection - based approaches were lately introduced by @xcite , @xcite and @xcite , in a monte carlo perspective simulating model indices as well as model parameters .",
    "those more recent extensions are already widely in use by the population genetics community , as exemplified by @xcite , or @xcite .",
    "another illustration of the popularity of this approach is given by the availability of three three softwares implementing an abc model choice methodology :    * abc - sysbio , developped by the theoretical systems biology group at imperial college london , which implements a smc - based abc for inference in system biology , including model - choice @xcite .",
    "* diyabc , developped by the centre de biologie et de gestion des populations , at inra montpellier , which implements a regularised abc - mc algorithm on population history using molecular markers @xcite . * popabc , developped by the school of biological sciences at the university of reading , which implements a regular abc - mc algorithm for genealogical simulation @xcite .",
    "@xcite process via abc the specific case of gibbs random fields with missing normalising constants .",
    "they establish that exact bayesian model selection can be implemented in this setting , deriving this result from the property that the concatenation of the sufficient statistics across models is also sufficient for model comparison . in a subsequent paper , @xcite",
    "advocate the role of abc approximations in general bayesian model choice .",
    "the issue of sufficiency is covered in this paper , with a generic cross - model sufficiency completion leading the authors to validate the method in full generality , including in - sufficient cases .    in this paper",
    ", we argue that abc is a valid approximation method for conducting bayesian inference in complex stochastic models , barring the limitation that it can not discriminate between those complex stochastic models when based on summary statistics .",
    "in essence , we highlight the fact that , since abc is conducting model choice based on in - sufficient statistics , the resulting inference is flawed in that the loss of information is severe to the point of inconsistency , namely that the abc model selection can not recover the proper model , even with an infinite amount of observation and computation . we demonstrate this inconsistency in the limiting ( and more favourable ) case of sufficient statistics .",
    "the conclusion of the current paper are thus quite negative in that we consider that conducting testing or model comparison using abc does not carry any reliable weight of evidence and therefore should not be trusted .",
    "more empirical measures such as those proposed in @xcite and @xcite seem to be the only possibility at the current time for conducting model comparison .",
    "we are therefore at odds with the positive conclusion found in @xcite , as discussed below .",
    "we stress here that , while @xcite repeatedly expressed reservations about the formal validity of the abc approach in statistical testing , those criticisms were addressed at the bayesian paradigm _ per se _ rather than at the approximation method .",
    "quite clearly , templeton s criticisms got rebutted in @xcite and are not relevant for the current paper .",
    "the plan of the paper is as follows : in section [ genesis ] , we recall the basics of abc as well as its justification ; section [ theramones ] exposes why a bayes factor based on an abc approximation is not converging to the true bayes factor as the computational effort increases ; section [ theclash ] explains the specificity of mrfs in this regard , while section [ siouxie ] illustrates the potential for divergence in examples .",
    "sectoion [ rollingstones ] concludes the paper .",
    "the setting in which abc operates is the approximation of the simulation from the posterior distribution @xmath0 when both distributions associated with @xmath1 and @xmath2 can be simulated .",
    "the first abc algorithm was introduced by @xcite in a genetic setting , as follows : given a sample @xmath3 from a sample space @xmath4 ,    generate @xmath5 from the prior distribution @xmath6 generate @xmath7 from the likelihood @xmath8 set @xmath9 ,    the parameters of the abc algorithm are the statistic @xmath10 , the distance @xmath11 , and the tolerance level @xmath12 .",
    "the approximation of the posterior distribution provided by the algorithm is that it samples from the marginal in @xmath13 of the joint distribution @xmath14 where @xmath15 denotes the indicator function of the set @xmath16 and where @xmath17 the basic justification of the abc approximation is that , when using a sufficient statistic @xmath10 and a small ( enough ) tolerance @xmath18 , we have @xmath19 the ( correct ) posterior distribution @xmath20 being the limit as @xmath18 goes to zero of @xmath21 .    in practice ,",
    "the statistic @xmath10 is not sufficient and the approximation then converges to @xmath22 .",
    "this fact is appreciated by users in the field who see this loss of information as an unvoidable price to pay for the access to computable quantities . while acknowledging the gain brought by abc in handling bayesian inference in complex models , we will demonstrate below that the loss due to the abc approximation may be arbitrary in the specific setting of bayesian model choice and testing , whether or not @xmath10 is sufficient .",
    "testing and model choice constitute a highly specific domain of bayesian analysis that involves conceptual and computational complexification since several models are simultaneously considered @xcite . given that both inferential problems are processed the same way in a bayesian perspective , we will only mention model choice in the remainder of the paper , but the reader must bear in mind that we cover testing as a particular case .",
    "the standard tool on which a bayesian approach relies is the evidence @xcite , also called the marginal likelihood , @xmath23 that leads to the bayes factor for comparing the evidences brought by the data on models with likelihoods @xmath24 and @xmath25 , @xmath26 as detailed in the bayesian literature @xcite , this ratio provides an absolute criterion for model comparison that is naturally penalised for model complexity @xcite and whose first order approximation is the bayesian information criterion ( bic ) .",
    "given that this issue is fundamental to our point , we recall that bayesian model choice proceeds by creating a probability structure across models ( or likelihoods ) .",
    "namely , in addition to the parameters associated with each model , a bayesian inference introduces the model index @xmath27 as an extra parameter .",
    "it is associated with its own prior distribution , @xmath28 ( @xmath29 ) , while the prior distribution on the parameter is conditional on the value @xmath30 of the model index , denoted by @xmath31 and defined on the parameter space @xmath32 .",
    "the choice between those models is then driven by the posterior distribution of @xmath27 , @xmath33 where @xmath34 denotes the marginal likelihood of @xmath3 for model @xmath35 .    while this distribution is well - defined and straightforward to interpret , it offers a challenging computational conundrum in bayesian analysis .",
    "moreover , the solutions found in the literature @xcite do not handle the case when the likelihood is not available and abc represents the almost unique alternative .",
    "as exposed in e.g. @xcite , @xcite , and @xcite , once @xmath27 is incorporated within the parameters , the abc approximation to the posterior follows from the same principles as regular abc .",
    "the corresponding implementation is as follows , using for the tolerance region a statistic @xmath36 that is the concatenation of the summary statistics used for all models ( with an obvious elimination of duplicates ) .",
    "generate @xmath30 from the prior @xmath28 generate @xmath37 from the prior @xmath38 generate @xmath7 from the model @xmath39 set @xmath40 and @xmath41    the abc estimate of the posterior probability @xmath42 is then the frequency of acceptances from model @xmath30 in the above simulation @xmath43 this also corresponds to the frequency of simulated pseudo - dataset from model @xmath30 that are closer to the data @xmath3 than the tolerance @xmath18 . in order to improve the estimation by smoothing , @xcite follow the rationale that motivated the use of a local linear regression in @xcite and",
    "rely on a weighted polychotomous logistic regression to estimate @xmath42 .",
    "this modelling is implemented in the diyabc software .",
    "most perspectives on abc do not question the role of the abc distance nor of the statistic @xmath44 in model choice settings .",
    "there is however a much stronger discrepancy between the genuine bayes factor / posterior probability and the approximations resulting from abc .",
    "the abc approximation to a bayes factor , @xmath45 say , resulting from algorithm [ algo : abcmoc ] is @xmath46 an alternative representation is given by @xmath47 where the pairs @xmath48 are simulated from the ( joint ) prior and @xmath49 is the total number of simulations that are necessary for @xmath50 acceptances in algorithm [ algo : abcmoc ] . in order to study the limiting behaviour of this approximation , we first let @xmath49 go to infinity .",
    "( for simplification purposes and without loss of generality , we choose a uniform prior on the model index . )",
    "the limit of @xmath51 is then @xmath52 }                               { \\mathbb{p}[\\mathcal{m}=2,\\rho\\{{\\boldsymbol{\\eta}}({\\mathbf{z}}),{\\boldsymbol{\\eta}}({\\mathbf{y}})\\ } \\le \\epsilon]}\\\\                       & = & \\dfrac{\\int \\mathbb{i}_{\\rho\\{{\\boldsymbol{\\eta}}({\\mathbf{z}}),{\\boldsymbol{\\eta}}({\\mathbf{y}})\\ } \\le \\epsilon }   \\pi_1({\\boldsymbol{\\theta}}_1)f_1({\\mathbf{z}}|{\\boldsymbol{\\theta}}_1)\\,\\text{d}{\\mathbf{z}}\\,\\text{d}{\\boldsymbol{\\theta}}_1 }                               { \\int \\mathbb{i}_{\\rho\\{{\\boldsymbol{\\eta}}({\\mathbf{z}}),{\\boldsymbol{\\eta}}({\\mathbf{y}})\\ } \\le \\epsilon }   \\pi_2({\\boldsymbol{\\theta}}_2)f_2({\\mathbf{z}}|{\\boldsymbol{\\theta}}_2)\\,\\text{d}{\\mathbf{z}}\\,\\text{d}{\\boldsymbol{\\theta}}_2}\\\\                       & = & \\dfrac{\\int \\mathbb{i}_{\\rho\\{{\\boldsymbol{\\eta}},{\\boldsymbol{\\eta}}({\\mathbf{y}})\\ } \\le \\epsilon }   \\pi_1({\\boldsymbol{\\theta}}_1)f_1^{{\\boldsymbol{\\eta}}}({\\boldsymbol{\\eta}}|{\\boldsymbol{\\theta}}_1)\\,\\text{d}{\\boldsymbol{\\eta}}\\,\\text{d}{\\boldsymbol{\\theta}}_1 }                               { \\int \\mathbb{i}_{\\rho\\{{\\boldsymbol{\\eta}},{\\boldsymbol{\\eta}}({\\mathbf{y}})\\ } \\le \\epsilon }   \\pi_2({\\boldsymbol{\\theta}}_2)f_2^{{\\boldsymbol{\\eta}}}({\\boldsymbol{\\eta}}|{\\boldsymbol{\\theta}}_2)\\,\\text{d}{\\boldsymbol{\\eta}}\\,\\text{d}{\\boldsymbol{\\theta}}_2}\\,,\\end{aligned}\\ ] ] where @xmath53 and @xmath54 denote the distributions of @xmath55 when @xmath56 and @xmath57 , respectively . by lhospital formula ,",
    "if we let @xmath18 go to zero , the above converges to @xmath58 which is precisely and exactly the bayes factor for testing model @xmath59 versus model @xmath60 based on the sole observation of @xmath61 .",
    "this result is completely coherent with the current perspective on abc , namely that the inference derived from the ideal abc output when @xmath62 only uses the information contained in @xmath61 .",
    "thus , in the limiting case , i.e.  when the abc algorithm uses an infinite computing power , the abc odds ratio does not take into account the features of the data besides the value of @xmath61 , which is why the limiting bayes factor only depends on the distributions of @xmath44 under both models .",
    "in contrast with point estimation  where using a sufficient statistic has no impact on the inference in the limiting case , the loss of information resulting from considering solely @xmath44 seriously impacts the resulting inference on which model is best supported by the data .",
    "indeed , as exhibited in a special case by @xcite , the information contained in @xmath61 is almost always smaller than the information contained in @xmath3 and this even in the case @xmath61 is a sufficient statistic for _ both models_. in other words , _",
    "@xmath61 being sufficient for both @xmath63 and @xmath64 does not usually imply that @xmath61 is sufficient for @xmath65 . _ to see why this is the case , consider the most favourable case , namely when @xmath61 is a sufficient statistic for both models .",
    "we then have by the factorisation theorem @xcite that @xmath66 , therefore that @xmath67 therefore , unless @xmath68 , the two bayes factors differ by this ratio , @xmath69 , which is only equal to one in a very small number of known cases .",
    "this decomposition is a straightforward proof that a model - wise sufficient statistic is usually not sufficient across models , i.e.  for model comparison .",
    "an immediate corollary is that the abc - mc approximation does not converge to the exact bayes factor .",
    "the discrepancy between the limiting abc inference and the genuine bayesian inference does not completely come as a surprise , because abc is indeed an approximation method .",
    "users of abc algorithms are therefore prepared for some degree of imprecision in their final answer , a point stressed by @xcite or @xcite when they qualify abc as exact inference on a wrong model .",
    "however , the magnitude of the difference between @xmath70 and @xmath71 expressed by is such that there is no direct connection between both answers . in a general",
    "setting , if @xmath44 has the same dimension as one component of the @xmath72 components of @xmath3 , the ratio @xmath69 is equivalent to a density ratio for a sample of size @xmath73 , hence it can be arbitrarily small or arbitrarily large when @xmath72 grows . on the opposite",
    ", the bayes factor @xmath74 is based on what is equivalent to a single observation , hence does not necessarily converge with @xmath72 , as shown by the poisson and normal examples below .",
    "the conclusion derived from one bayes factor may therefore completely differ from the conclusion derived from other one and there is no possibility of a generic agreement between both , or even of a manageable correction factor .    for this reason ,",
    "we conclude that the abc approach can not be used for testing nor for model choice , with the exception of gibbs random fields as explained in the next section . in all cases when @xmath69 is different from one and impossible to approximate , no inference on the true bayes factor",
    "can be made based on the abc - mc approximation without further information on the ratio @xmath69 , which is most often unavailable .",
    "we note that @xcite also derived this relation between both bayes factors in their formula ( 18 ) but surprisingly concluded on advocating the use of abc in complex models , where there are no sufficient statistics .",
    "we disagree with this perspective for reasons that will be made clear in the following sections .",
    "@xcite showed that , for gibbs random fields and in particular for potts models , when the goal is to compare several neighbourhood structures , the computation of the posterior probabilities of the models / structures under competition can be operated by likelihood - free simulation techniques , in the sense that there exists a converging approximation to the true bayes factor .",
    "the reason for this property is that , in the above ratio , @xmath68 in this special model .    indeed , if we consider a gibbs random field given by the likelihood function @xmath75where @xmath3 is a vector of dimension @xmath72 taking values over the finite set @xmath76 ( possibly a lattice ) , @xmath77 is the potential function defining the random field , taking values in @xmath78 , @xmath79 is the associated parameter , and @xmath80 is the corresponding normalising constant , the potential function @xmath10 is a sufficient statistic for the model . for instance , in potts models , the sufficient statistic is the number of neighbours , @xmath81 associated with a neighbourhood structure denoted by @xmath82 ( meaning that @xmath83 and @xmath84 are neighbours ) .",
    "the property that validates an abc resolution for the comparison of gibbs random fields is that , due to their specific structure , there exists a sufficient statistic vector that runs across models and which allows for an exact ( when @xmath62 ) simulation from the posterior probabilities of the models .",
    "more specifically , consider @xmath85 gibbs random fields in competition , each one being associated with a potential function @xmath86 @xmath87 , i.e.  with corresponding likelihood @xmath88 where @xmath89 and @xmath90 is the unknown normalising constant .",
    "a bayesian analysis operates on the extended parameter space @xmath91 that includes both the model index @xmath27 and the corresponding parameter space @xmath32 .",
    "the inferential target is thus the model posterior probability @xmath92 i.e.  the marginal in @xmath27 of the posterior distribution on @xmath93 given @xmath3 .",
    "each model has its own sufficient statistic @xmath94 .",
    "then , for _ each _ model , the vector of statistics @xmath95 is clearly sufficient ; furthermore @xcite exposed the fact that @xmath44 is also sufficient for the joint parameter @xmath93 . that this concatenation of sufficient statistics is jointly sufficient across models",
    "is a property that is rather specific to gibbs random field models , at least from a practical perspective ( see below ) .",
    "figure [ figgrrr ] shows an experiment from @xcite concluding rightly at the agreement between the exact bayes factor and an abc approximation .",
    "comparison between the true bayes factor and the abc approximation in a markov model selection of @xcite , based on @xmath96 simulated sequences and @xmath97 proposals from the prior .",
    "the solid / red line is the diagonal . _",
    "( source : @xcite . ) _ ]    @xcite point out that this specific property of gibbs random fields can be extended to any exponential family ( hence to any setting enjoying sufficient statistics , see e.g.  @xcite ) .",
    "their argument is an encompassing property : by including all sufficient statistics and all dominating measure statistics in an encompassing model , models under comparison become submodels of the encompassing model .",
    "they then conclude that the concatenation of those statistics is jointly sufficient across models . while this encompassing principle holds in full generality , in particular when comparing models that are already embedded , we think it leads to a biased perspective about the merits of abc for model choice : in practice , complex models do not enjoy sufficient statistics ( if only because they are not exponential families ) . as demonstrated in the next section",
    ", there is more than a mere loss of information due to the use of insufficient statistics and looking at what happens in the limiting case when one is relying on a common sufficient statistic is a formal study that brings light on the potentially huge discrepancy between the abc - based bayes factor and the true bayes factor . to study a solution to the problem in the formal case of the exponential families does not help in the understanding of the discrepancy in non - exponential models .",
    "the difficulty with the arbitrary discrepancy between @xmath70 and @xmath74 is that it is impossible to evaluate in a general setting , while there is no reason to expect a reasonable agreement between both quantities .",
    "a first illustration was produced by @xcite in the setting of @xmath98 time series : a simulation experiment showed that , when comparing an @xmath99 with an @xmath100 model , the abc approximation to the bayes factor was stable ( around @xmath101 ) as @xmath18 decreases , remaining far from the true bayes factor @xmath102 for an @xmath99 simulated sample , while the approximation was @xmath103 against a true value of @xmath104 in the case of a simulated @xmath100 sample .",
    "as a first illustration of the discrepancy due to the use of a sufficient statistic , consider the simple case when a sample @xmath105 could come from either a poisson @xmath106 distribution or from a geometric @xmath107 distribution , already introduced in @xcite as a counter - example to gibbs random fields and later reprocessed in @xcite to support their sufficiency argument . in this setting ,",
    "the sum @xmath108 is a sufficient statistic for both models but not across models .",
    "the distribution of the sample given @xmath109 is a multinomial @xmath110 distribution when the data is poisson , since @xmath109 is then a poisson @xmath111 variable , while it is the uniform distribution with constant probability @xmath112 in the geometric case , since @xmath109 is then a negative binomial @xmath113 variable .",
    "the discrepancy ratio is therefore @xmath114 when simulating @xmath72 poisson or geometric variables and using prior distributions @xmath115 on the respective models , the exact bayes factor can be evaluated and the range and distribution of the discrepancy are therefore available .",
    "figure [ fig : poisneg ] gives the range of @xmath70 versus @xmath74 , showing that @xmath74 is in this case absolutely un - related with @xmath70 : the values produced by both approaches simply have nothing in common .",
    "as noted above , the approximation @xmath74 based on the sufficient statistic @xmath109 is producing figures of the magnitude of a _ single _ observation , while the true bayes factor is of the order of the sample size .",
    "comparison between the true log - bayes factor _",
    "( first axis ) _ for the comparison of a poisson model versus a negative binomial model and of the log - bayes factor based on the sufficient statistic @xmath116 _",
    "( second axis ) _",
    ", for poisson _ ( left ) _ and negative binomial _ ( left ) _ samples of size @xmath117 , based on @xmath118 replications . ]",
    "the discrepancy between both bayes factors is in fact increasing with the sample size , as shown by the following result :    consider performing model selection between model 1 : @xmath106 with prior distribution @xmath119 equal to an @xmath120 distribution and model 2 : @xmath107 with a uniform prior distribution @xmath121 when the observed data @xmath3 consists of iid observations with @xmath122 = \\theta_0 > 0 $ ] .",
    "then @xmath123 is the minimal sufficient statistic for both models and the bayes factor based on the sufficient statistic @xmath124 , @xmath74 , satisfies @xmath125    therefore , the bayes factor based on the sufficient statistic @xmath124 is _ not _ consistent ; it converges to a non - zero , finite value almost surely .    under model 1",
    ", we have @xmath126 , with corresponding likelihood @xmath127 the marginal likelihood of @xmath109 under the prior @xmath128 is then @xmath129 under model 2 , the sufficient statistic has a negative binomial distribution , @xmath130 and thus @xmath131 the corresponding marginal likelihood under the prior @xmath121 is @xmath132 therefore from and , the bayes factor based on the sufficient statistic is given by @xmath133 since the @xmath134 s are iid with mean @xmath135 , the law of large numbers implies that @xmath136 almost surely , thus @xmath137 since @xmath138 .",
    "furthermore , @xmath139 thus from  we deduce that @xmath140 proving the result .    in this specific setting , @xcite show that adding @xmath141 to the sufficient statistic @xmath109 induces a statistic @xmath142 that is sufficient across both models .",
    "while this is a mathematically correct observation , we think it is not helpful for the understanding of the behaviour of abc - model choice in realistic settings : outside toy examples as the one above and well - structured although complex exponential families like gibbs random fields , it is not possible to come up with completion mechanisms that ensure sufficiency across models and it is therefore more fruitful to consider the diverging behaviour of the abc approximation as given , rather than attempting at solving the problem .      first , note that , given a one - dimensional sufficient statistic @xmath143 , the functions @xmath144 and @xmath145 can on principle be anything .",
    "for instance , @xmath146 and @xmath147 is a possible model . in other words , by a reparameterisation of the models , we could observe @xmath148 with @xmath149 this independently of the distributions of @xmath109 under both models .",
    "( this means that we can find two competing models where the distributions of @xmath109 are not connected with @xmath150 nor with @xmath151 . ) because they depend on the choice of those distributions , the true bayes factor and the abc - bayes factor are unrelated and may as well diverge from one another .",
    "admitedly , this construct is artificial in that there is no clear statistical setting when this could occur , but the construct is both mathematically valid and informative about the lack of control over the diverging factor @xmath69 .",
    "if we look at a fully normal @xmath152 setting , we have @xmath153 hence @xmath154 if we reparameterise the observations into @xmath155 , we do get @xmath156 ^ 2   \\big/2 \\right\\}\\end{aligned}\\ ] ] since the jacobian is @xmath59 .",
    "hence @xmath157 ^ 2   /2 \\right\\ } \\sigma^{-n}\\ ] ]    considering both models @xmath158 the discrepancy ratio is then given by @xmath159 ^ 2   /2 \\right\\ } \\sigma_1^{-n+1 } } { \\exp\\left\\{-\\sigma_2^{-2 } \\sum_{i=1}^{n-1 } ( y_i-\\bar y)^2/2 -      \\sigma_2^{-2 } \\left [ \\sum_{i=1}^{n-1 } ( y_i-\\bar y ) \\right]^2   /2",
    "\\right\\ } \\sigma_2^{-n+1 } } \\\\ & \\quad= \\dfrac{\\sigma_2^{n-1}}{\\sigma_1^{n-1}}\\ , \\exp\\left\\{\\dfrac{\\sigma_2^{-2}-\\sigma_1^{-2}}{2}\\left(\\sum_{i=1}^{n-1 } ( y_i-\\bar y)^2 + \\left [ \\sum_{i=1}^{n-1 } ( y_i-\\bar y ) \\right]^2",
    "\\right)\\right\\ } \\end{aligned}\\ ] ] and is connected with the lack of consistency of the bayes factor :    consider performing model selection between model 1 : @xmath160 and model 2 : @xmath161 , @xmath150 and @xmath151 being given , with prior distributions @xmath162 equal to a @xmath163 distribution and when the observed data @xmath3 consists of iid observations with finite mean and variance .",
    "then @xmath123 is the minimal sufficient statistic for both models and the bayes factor based on the sufficient statistic @xmath124 , @xmath74 , satisfies @xmath164    the marginal likelihood associated with @xmath124 and the prior @xmath165 is @xmath166 hence leading to the bayes factor @xmath167 which indeeds converges to @xmath59 as @xmath72 goes to infinity .",
    "figure [ fig : twonormal ] illustrates the behaviour of the discrepancy ratio when @xmath168 and @xmath169 , for datasets of size @xmath170 simulated according to both models .",
    "the discrepancy ( expressed on a log scale ) is once again dramatic , in concordance with the above lemma .",
    "empirical distributions of the log discrepancy @xmath171 for datasets of size @xmath170 simulated from @xmath160 _",
    "( left ) _ and @xmath161 _ ( right ) _ distributions when @xmath168 and @xmath169 , based on @xmath172 replications and a flat prior . ]",
    "if we now turn to an alternative choice of sufficient statistic , using the pair @xmath173 with @xmath174 we follow the solution of @xcite . using a conjugate prior @xmath165 , the true bayes factor is given by @xmath175 and it is equal to the bayes factor based on the corresponding distributions of the pair @xmath173 in the respective models .",
    "again , we do not think this coincidence brings the proper light on the behaviour of the abc approximations in realistic settings .",
    "since its introduction by @xcite and @xcite , abc has been extensively used in several areas involving complex likelihoods , primarily in population genetics . in those domains ,",
    "abc has been used both for point estimation and testing of hypotheses . in realistic settings , with the exception of gibbs random fields that satisfy a resilience property with respect to their sufficient statistics , the conclusions drawn on model comparison can not alas be trusted _ per se _ but require further analyses as to the pertinence of the ( abc ) bayes factor based on the summary statistics .",
    "this paper has only examined in details the case when the summary statistics are sufficient for both models , while practical situations imply the use of in - sufficient statistics , and further research is needed for the latter case . however",
    ", this practical situation implies a wider loss of information compared with the exact inferential approach , hence a wider discrepancy between the exact bayes factor and the quantity produced by an abc approximation .",
    "it thus appears to us an urgent duty to warn the community about the dangers of this approximation , especially when considering the rapidly increasing number of applications using abc for conducting model choice and hypothesis testing . as a final ( and negative ) point ,",
    "we unfortunately do not see an immediate and generic alternative for the approximation of bayes factors because importance sampling techniques are suffering from the same difficulty , namely they only depend on the summary statistics .    as a final remark",
    ", we note that @xcite advocate the use of full allelic distributions in an abc framework , instead of resorting to summary statistics .",
    "they show that it is possible to apply abc using allele frequencies to draw inferences in cases where it is difficult to select a set of suitable summary statistics ( and when the complexity of the model or the size of dataset makes it computationally prohibitive to use full - likelihood methods ) . in such settings , were we to consider a model choice problem , the divergence exhibited in the current paper would not occur because the measure of distance does not rely on a reduction of the sample .",
    "the first two authors work has been partly supported by the agence nationale de la recherche ( anr , 212 , rue de bercy 75012 paris ) through the 2009 - 2012 project emile , directed by jean - marie cornuet ."
  ],
  "abstract_text": [
    "<S> approximate bayesian computation ( abc ) , also known as likelihood - free methods , have become a favourite tool for the analysis of complex stochastic models , primarily in population genetics but also in financial analyses . </S>",
    "<S> we advocated in @xcite the use of abc for bayesian model choice in the specific case of gibbs random fields ( grf ) , relying on a sufficiency property mainly enjoyed by grfs to show that the approach was legitimate . despite having </S>",
    "<S> previously suggested the use of abc for model choice in a wider range of models in the diy abc software @xcite , we present theoretical evidence that the general use of abc for model choice is fraught with danger in the sense that no amount of computation , however large , can guarantee a proper approximation of the posterior probabilities of the models under comparison .    * keywords * : likelihood - free methods , bayes factor , diyabc , bayesian model choice , sufficiency . </S>"
  ]
}