{
  "article_text": [
    "a central challenge in computer vision is the assignment of a semantic class label to every pixel in an image , a task known as semantic segmentation .",
    "a common strategy for semantic segmentation is to use pixel - level classifiers such as random forests ( rf ) @xcite , which have the advantage of being easy to train and performing well on a wide range of tasks , even in the face of little training data .",
    "the use of stacked classifiers , such as in auto - context @xcite , has been shown to improve performance on many tasks such as object - class segmentation @xcite , facade segmentation @xcite , and brain segmentation @xcite . however , this strategy has the limitation that the individual classifiers are trained greedily .",
    "recently , numerous groups have explored the use of convolutional neural networks ( cnns ) for semantic segmentation  @xcite , which has the advantage that it enables `` end - to - end learning '' of all model parameters .",
    "this trend is largely inspired by the success of deep cnns on high - level computer vision tasks , such as image classification @xcite and object detection @xcite .",
    "however , training a deep cnn requires substantial experience and large amounts of labeled data , or availability of a pre - trained cnn for a similar task @xcite .",
    "thus , there currently exists a divide between stacked classifiers and deep cnns .",
    "we propose an alternative solution , exploiting the fundamental connection between decision trees ( dt ) and nns @xcite to bridge the gap between stacked classifiers and deep cnns .",
    "this provides a novel approach with the strengths of stacked classifiers , namely robustness to limited training data , and the end - end - learning capacity of nns .",
    "figure  [ fig : pipeline ] depicts our proposed pipeline .",
    "* contributions . *",
    "we make the following contributions :    \\1 .",
    "we show that a stacked rf with contextual features is a special case of a deep cnn with sparse convolutional kernels .",
    "we apply this successfully to semantic segmentation .",
    "we describe an exact mapping of a stacked rf to our sparse , deep cnn .",
    "we utilize this mapping to initialize the cnn from a greedily trained stacked rf .",
    "this is important in the case of limited training samples .",
    "we show that this leads to superior results compared to alternative strategies .",
    "we describe an approximate mapping of our sparse , deep cnn back to a stacked rf .",
    "we show that this improves the performance of a greedily trained stacked rf .",
    "\\4 . due to our special cnn architecture",
    "we are able to gain new insights of the activation pattern of internal layers , with respect to semantic labels .",
    "in particular , we observe that the common smoothing strategy in stacked rfs is naturally learned by our cnn .",
    "[ fig : pipeline ]",
    "our work relates to ( i ) global optimization of rf classifiers , ( ii ) mapping rf classifiers to neural networks , ( iii ) feature learning in stacked rf models , ( iv ) applying cnns to the task of semantic segmentation , and ( v ) training cnns with limited labeled data .",
    "we cover these areas in turn .    *",
    "global optimization of rfs .",
    "* the limitations of traditional greedy rf construction  @xcite have been addressed by numerous works . in  @xcite",
    ", the authors learn a dt by the standard greedy construction , followed by a process they call `` fuzzification '' , replacing all threshold split decisions with smooth sigmoid functions that they interpret as partial or `` fuzzy '' inheritance by the daughter nodes .",
    "they develop a back - propagation algorithm , which begins in the leaves and propagates up one layer at time to the root node , re - optimizing all split parameters of the dt . in",
    "@xcite , they learn to combine the predictions from each dt so that the complementary information between multiple trees is optimally exploited . they identify a suitable loss function , and after training a standard rf , they retrain the distributions stored in the leaves , and prune the dts to accomplish compression and avoid overfitting .",
    "however , @xcite does not retrain the parameters of the internal split nodes of individual dts , whereas  @xcite does not retrain the combination of trees in the forest .",
    "conceptually , our approach does both .    *",
    "mapping rfs to nns . * in both  @xcite and @xcite ,",
    "rfs were initially trained in a greedy fashion , and then later refined .",
    "an alternative but related approach is to map the greedily trained rf to an nn with two hidden layers , and use this as a smart initialization for subsequent parameter refinement by back - propagation @xcite .",
    "this effectively `` fuzzifies '' threshold split decisions , and simultaneously enables training with respect to a final loss function on the output of the nn .",
    "hence as opposed to @xcite and @xcite , all model parameters are learned simultaneously in an end - to - end fashion .",
    "additional advantages are that ( i ) back - propagation has been widely studied in this form , and ( ii ) back - propagation is highly parallelized , and only needs to propagate over 2 hidden layers , compared to all tree levels as in @xcite",
    ".    our work builds upon @xcite : we extend their approach to a deep cnn , inspired by the auto - context algorithm @xcite , for the purpose of semantic segmentation .",
    "furthermore , we propose an approximate algorithm for mapping the trained cnn back to a rf with axis - aligned threshold split functions , for fast inference at test time .    * feature learning in a rf framework . * the auto - context algorithm  @xcite attempts to capture pixel interdependencies in the learning process by iteratively learning a pixel - wise classifier , using the prediction of nearby pixels from the previous iteration as features .",
    "this process is closely related to feature learning , due to the introduction of new features during the learning process .",
    "numerous works have generalized the initial approach of auto - context .",
    "in entangled random forests ( erfs )  @xcite , spatial dependencies are captured by `` entanglement features '' in each dt , without the need for stacking .",
    "geodesic forests  @xcite additionally introduce image - aware geodesic smoothing to the class distributions , to be used as features by deeper nodes in the dt",
    ". however , despite the fact that erfs use a _ soft _ sigmoid split function to obtain max - margin behaviour with a small number of trees , these approaches are still limited by greedy parameter optimization .    in a more traditional approach to feature learning , neural decision forests  @xcite mix rfs and nns by using multi - layer perceptrons ( mlp ) as soft split functions , to jointly tackle the problem of data representation and discriminative learning .",
    "this approach can obtain superior results with smaller trees , at the cost of more complicated split functions ; however , the mlps in each split node are trained independently of each other .",
    "this limitation is addressed in  @xcite , which trains the entire system end - to - end .",
    "however , they adopt a mixed framework , with both differentiable rfs and cnns , that are trained in an alternating fashion , and applied to image classification . in contrast",
    ", we map to the cnn framework , which enables optimization with popular back - propagation algorithm , and apply to the task of semantic segmentation .    * cnns for semantic segmentation . * while cnns have proven very successful for high - level vision tasks , such as image classification , they are less popular for the task of dense semantic segmentation , due to their in - built spatial invariance .",
    "cnns can be applied in a tile - based manner @xcite ; however , this leads to pixel - independent predictions , which require additional measures to ensure spatial consistency @xcite . in @xcite ,",
    "the authors extend the tile - based approach to `` whole - image - at - a - time '' processing , in their fully convolutional network ( fcn ) .",
    "they address the coarse - graining effect of the cnn by upsampling the feature maps in deconvolution layers , and combining fine - grained and coarse - grained features during prediction .",
    "this approach , combining down - sampling with subsequent up - sampling , is necessary to maintain a large receptive field without increasing the size of the convolution kernels , which otherwise become difficult to learn .",
    "a variant of fcn called u - net was recently proposed in  @xcite . in @xcite , they minimize coarse - graining by skipping multiple sub - sampling layers and avoid introducing additional parameters by using sparse convolutional kernels in the layers with large receptive fields . they additionally post - process by a fully connected crf . in @xcite , they address coarse - graining by expressing mean - field inference in a dense crf as a recurrent neural network ( rnn ) , and concatenating this rnn behind a fcn , for end - to - end training of all parameters .",
    "notably , they demonstrate a significant boost in performance on the pascal voc 2012 segmentation benchmark .    in our work we propose a new cnn architecture for semantic segmentation .",
    "contrary to the previous approaches , we avoid coarse - graining effects , which arise in large part due to pre - training a cnn for _ image classification _ on data provided by the imagenet large scale visual recognition challenge ( ilsvrc ) .",
    "instead , we pre - train a stacked rf on a small set of densely labeled data .",
    "our approach is related to the use of sparse kernels in @xcite ; however , we learn the non - zero element(s ) of very sparse convolutional kernels during greedy construction of an rf stack .",
    "one advantage of this approach is that since the kernels have a very large receptive field , we do not need max - pooling and deconvolution layers , as _ e_._g_.in the fcn .",
    "additionally , in our approach the sparsity of the kernels can be specified by the number of features used in each rf split node , independently of the size of the receptive field .    * training cnns with limited labelled data .",
    "* cnns provide a powerful tool for feature learning ; however , their performance relies on a large set of labeled training data .",
    "unsupervised pre - training has been used successfully to leverage small labeled training sets @xcite ; however , fully supervised training on large data sets still gives higher performance .",
    "alternatively , transfer learning makes use of _",
    "e.g. , _ pseudo - tasks @xcite , or surrogate training data @xcite .",
    "more recent practice is to train a cnn on a large training set , and then fine tune the parameters on the target data @xcite .",
    "however , this requires a closely related task with a large labeled data set , such as ilsvrc .",
    "another strategy to address the dependency on training data , is to expand a small labeled training set through data augmentation @xcite .",
    "alternatively , one can use _ companion _ objective functions at each hidden layer , as a form of regularization during training @xcite .",
    "however , this may in principle interfere with the deep network s ability to learn the optimal internal representations , as noted by the authors .",
    "we propose a novel strategy for addressing the challenge of training deep cnns given limited training data .",
    "similar in spirit to @xcite , we employ greedy supervised pre - training , yet in a complementary model , namely the popular auto - context model .",
    "we then map the resulting auto - context model onto a deep cnn , and refine all weights using back - propagation .",
    "in section  [ subsec : welbl ] , we review the algorithm for mapping an rf onto an nn with two hidden layers  @xcite . in section  [ subsec :",
    "rfvscnn ] , we introduce the relationship between rfs with contextual features and cnns . in section  [ subsec : forward - map ] , we describe our main contribution , namely how to map a stack of rfs onto a deep cnn . in section  [ subsec : back - map ] , we describe our second contribution , namely an algorithm for mapping our deep cnn back onto the original rf stack , with updated parameters .      in the following , we review the existing works  @xcite .",
    "a decision tree consists of a set of split nodes , @xmath0 , and leaf nodes , @xmath1 .",
    "each split node @xmath2 processes the subset @xmath3 of the feature space @xmath4 that reaches it .",
    "usually , @xmath5 , where @xmath6 is the number of features .",
    "let @xmath7 and @xmath8 denote the left and right _ child node _ of a split node @xmath2 . a split node @xmath2 partitions the set @xmath3 into two sets @xmath9 and @xmath10 by means of a _",
    "split decision_. for dts using axis - aligned split decisions , the split is performed on the basis of a single feature whose index we denote by @xmath11 , and a respective threshold denoted as @xmath12 : @xmath13 .    for each leaf",
    "node @xmath14 , there exists a unique path from root node @xmath15 to leaf @xmath14 , @xmath16 , with @xmath17 and @xmath18 .",
    "thus , leaf membership can be expressed as follows :    @xmath19    each leaf node @xmath14 stores votes for the semantic class labels , @xmath20 , where @xmath21 is the number of classes . for a feature vector @xmath22",
    ", we denote the unique leaf of the tree that has @xmath23 as @xmath24 .",
    "the prediction of a dt for feature vector @xmath22 to be of class @xmath25 is given by : @xmath26 using this notation , we now describe how to map a dt to a feed - forward nn , with two hidden layers . conceptually , the nn separates the task of evaluating the split nodes and evaluating leaf membership into the first and second hidden layers , respectively .",
    "see figure [ fig : forward - map_tree ] for a sketch of the following description .    [",
    "cols=\"^,^,^ \" , ]     * insights . * in figure",
    "[ fig : zfish : activationlayers ] we discuss insights on the internal activation layers of this network .",
    "ccc    , @xmath27 ) for somite @xmath28 .",
    "notice that the activation from the cnn appears smoothed along the direction of the foreground classes compared to the noisier output of the stacked rf .",
    "best viewed in colour.,title=\"fig:\",scaledwidth=30.0% ] & , @xmath27 ) for somite @xmath28 .",
    "notice that the activation from the cnn appears smoothed along the direction of the foreground classes compared to the noisier output of the stacked rf .",
    "best viewed in colour.,title=\"fig:\",scaledwidth=30.0% ] +",
    "we have exploited a new mapping between stacked rfs and deep cnns , and demonstrated the practical benefits of this mapping for semantic segmentation .",
    "this is particularly important when dealing with limited amount of training data .",
    "in contrast to common cnn architectures , our specific architecture produces internal activation images , one for each class , which are of the same dimension as the input image .",
    "this enables us to gain insights on the semantic behaviour of the internal layers .",
    "there are many exciting avenues for future research . in the short term",
    ", we plan to refine the input convolution filters , which are currently fixed , during back - propagation .",
    "another refinement is to incorporate drop - out regularization during training , which should lead to better generalization performance as has been shown for traditional cnn architectures .",
    "also , the approximate mapping from a cnn architecture back to stacked rfs , and related test - time efficient architectures , may be further improved . in the midterm",
    "we are excited about extending our architecture and also merging it with existing cnn architectures .",
    "since our internal activation images are directly interpretable , it is straight forward to incorporate differentiable model layers .",
    "it will be interesting to see how our specialized cnn behaves as part of a larger cnn network , for instance by placing it directly after the feature extraction layers of a traditional cnn .",
    "in section  [ subsubsec : kinect ] , we describe the training parameters used to train the stacked rf and deep cnn for the kinect example . in section  [ subsubsec :",
    "zfish ] , we describe the training parameters used to train the stacked rf and deep cnn for the zebrafish example .",
    "we also describe the parameters used for training the equivalent deep cnn with random weight initialization",
    ".        * stacked rf . *",
    "we trained a two - level stacked rf , with the following forest parameters at every level : 10 trees , maximum depth 12 , stop node splitting if less than 25 samples .",
    "we selected 20 samples per class per image for training , and used the standard scale invariant offset features from @xcite , with standard deviation , @xmath29 = 50 in each dimension .",
    "each split node selected the best from a random sample of 100 such features .",
    "* we mapped the rf stack to a deep cnn with 5 hidden layers , as described in section 3.3 . for efficient training , the initialization parameters @xmath30 were reduced such that the network could transmit a strong gradient via back - propagation .",
    "however , softening these parameters moves the deep cnn further from its initialization by the equivalent stacked rf .",
    "we evaluated a range of initialization parameters and found @xmath31 , @xmath32 , @xmath33 to be a good compromise .",
    "we trained the cnn using back - propagation and stochastic gradient descent ( sgd ) , with a cross - entropy loss function . during back - propagation",
    ", we maintained the sparse connectivity from rf initialization , allowing only the weights on pre - existing edges to change , corresponding to the _ sparse _ training scheme from  @xcite .    since the network is designed for whole - image inputs , we first cropped the training images around the region of foreground pixels , and then down - sampled them by 25x .",
    "learning rate , @xmath34 , was set such that for the @xmath35 iteration of sgd , @xmath36 with hyper - parameters @xmath37 and @xmath38 iterations .",
    "momentum , @xmath39 , was set according to the following schedule : @xmath40 , where @xmath41 @xcite .",
    "* stacked rf . *",
    "we trained a three - level rf stack , with the following forest parameters at every level : 16 trees , maximum depth 12 , stop node splitting if less than 25 samples .",
    "features were extracted from the images using a standard filter bank , and then normalized to zero mean , unit variance .",
    "the number of random features tested in each node was set to the square root of the total number of input features .",
    "for each randomly selected feature , 10 additional contextual features were also considered , with x and y offsets within a 129x129 pixel window .",
    "training samples were generated by sub - sampling the training images 3x in each dimension and then randomly selecting @xmath42 of these samples for training .    *",
    "* we mapped the rf stack to a deep cnn with 8 hidden layers .",
    "the cnn was initialized and trained exactly as for the kinect example , with the following exeptions : ( i ) we used a class - balanced cross - entropy loss function , ( ii ) training samples were generated by sub - sampling the training images 9x in each dimension .",
    "( iii ) learning rate parameters were as follows : @xmath37 and @xmath43 iterations .",
    "( iv ) momentum was initialized to @xmath44 , and increased to @xmath45 after @xmath46 iterations .",
    "we observed convergence after only 1 - 2 passes through the training data , similar to what was reported by @xcite .    *",
    "cnn from random initialization . *",
    "as discussed in section 4.2 of the paper , for comparison to the rf - initialized weights described above , we also trained cnns with the same architecture , but with random weight initialization .",
    "weights were initialized according to a gaussian distribution with zero mean and standard deviation , @xmath47 .",
    "we applied a similar sgd training routine , and re - tuned the hyper - parameters as follows : @xmath48 , @xmath49 iterations , momentum was initialized to 0.4 and increased to 0.99 after 96 iterations .",
    "larger step - sizes failed to train .",
    "networks were trained for 2500 iterations .    * fully convolutional network . * as discussed in section 4.2 of the paper , we also compared our method with the fully convolutional network ( fcn )  @xcite .",
    "this network was downloaded from caffe s model zoo , and initialized with weights fine - tuned from the ilsvrc - trained vgg-16 model .",
    "we trained all layers of the network using sgd with a learning rate of @xmath50 , momentum of @xmath51 and weight decay of @xmath52 .",
    "see figure  [ fig : fcn](b ) for an example of the resulting segmentation ."
  ],
  "abstract_text": [
    "<S> we consider the task of pixel - wise semantic segmentation given a small set of labeled training images . among two of the most popular techniques to address this task </S>",
    "<S> are random forests ( rf ) and neural networks ( nn ) . </S>",
    "<S> the main contribution of this work is to explore the relationship between two special forms of these techniques : stacked rfs and deep convolutional neural networks ( cnn ) . </S>",
    "<S> we show that there exists a mapping from stacked rf to deep cnn , and an approximate mapping back . </S>",
    "<S> this insight gives two major practical benefits : firstly , deep cnns can be intelligently constructed and initialized , which is crucial when dealing with a limited amount of training data . </S>",
    "<S> secondly , it can be utilized to create a new stacked rf with improved performance . </S>",
    "<S> furthermore , this mapping yields a new cnn architecture , that is well suited for pixel - wise semantic labeling . </S>",
    "<S> we experimentally verify these practical benefits for two different application scenarios in computer vision and biology , where the layout of parts is important : kinect - based body part labeling from depth images , and somite segmentation in microscopy images of developing zebrafish . </S>"
  ]
}