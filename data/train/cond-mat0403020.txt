{
  "article_text": [
    "in general , artificial neural networks have been widely applied to memorize and retrieve information . during the last number of years",
    "there has been considerable interest in neural networks with multi - state neurons using the framework of statistical mechanics , which deals with large systems of stochastically interacting microscopic elements ( see , e.g. , @xcite and references cited therein ) . in these models ,",
    "the firing states of the neurons or their membrane potentials are the microscopic dynamical variables . basically , compared to models with two - state ( = binary ) neurons , such models can function as associative memories for grey - toned or colored patterns @xcite and/or allow for a more complicated internal structure of the recall process , e.g. , a distinction between the exact location and the details of a picture in pattern recognition and the analogous problem of retrieval focusing in the framework of cognitive neuroscience @xcite , a combination of information retrieval based on skills and based on specific facts or data @xcite .    different types of multi - state neurons can be distinguished according to the symmetry of the interactions between the different states .",
    "here we are primarily interested in the so - called @xmath0-ising neuron , the states of which can be represented by scalars , and the interaction between two neurons can then be written as a function of the product of these scalars .",
    "so , the @xmath0-states of the neuron can be ordered like a ladder between a minimum and a maximum value , usually taken to be @xmath1 and @xmath2 .",
    "special cases are @xmath3 , i.e. , the well - known hopfield model @xcite and @xmath4 , i.e. , networks with analogue or graded response neurons @xcite .    in analogy to the hopfield model , the multi - state neuron models we discuss here have their immediate counterpart in random magnetic systems ( = spin - glasses ) ( cfr . , e.g. , @xcite and @xcite ) , but with couplings defined in terms of embedded patterns through a learning rule .",
    "since one of the aims of these networks is to find back the embedded patterns as attractors of the recall process , they are also interesting from the point of view of dynamical systems .    this close relation with spin - glass systems means that the methods and techniques used to study the latter have been successfully applied to these network models .",
    "in particular , it also means that concepts like temperature , fluctuations , disorder , _ noise _ , stochasticity  play a crucial role . in the literature",
    "it is well - known ( see , e.g. ,  @xcite ) that noise can have rather surprising and counterintuitive effects in the behavior of dynamical systems .",
    "it has been shown many times that noise can have a constructive rather than a destructive role .",
    "relevant examples  @xcite of this fact are the phenomenon of stochastic resonance , noise induced ordering transitions , noise induced disordering phase transitions and an increase of the maximal information content with dilution in some neural network models  @xcite . in principle , these ordering effects seem to be related to the multiplicative character of the fluctuations , as compared to the disordering role of additive fluctuations . but things are not so simple because there is an interplay between additive and multiplicative noise terms .",
    "moreover , there are several types of internal fluctuations , e.g. , thermal fluctuations introduced through a random term , quite often assumed to be gaussian distributed with zero mean and uncorrelated at different times .",
    "these internal fluctuations are described by an additive noise term , i.e. , a random term that does not depend on the variable under consideration .",
    "this is not a necessary character of internal fluctuations . in some systems",
    "these can also be described by a multiplicative noise which is coupled to the state of the system . as a natural extension of the concept of internal fluctuations , external noise are those fluctuations that are not of thermal origin .    in the @xmath0-ising neural network models we are considering here",
    ", the following noise can be characterized .",
    "first , the neurons are stochastic such that the analogue of temperature is introduced .",
    "this enables us , as hinted at already above , to use the techniques of statistical mechanical mean - field theory , and ultimately to compute , e.g. , the storage capacity of the network @xcite .",
    "the zero - temperature limit will always reduce our system to a deterministic hopfield or multi - state @xmath0-ising network .",
    "the meaning of this stochastic behavior is to model that neurons fire with variable strength , that there are delays in synapses , that there are random fluctuations in the release of transmitters  .",
    "briefly , we model this internal noise by thermal fluctuations .    secondly , in single pattern recall with many  a fraction of the size of the network ",
    "stored patterns there is the generally nontrivial interference noise due to the other patterns .",
    "this noise has been treated , e.g. , by statistical neurodynamics @xcite or functional integration methods @xcite .",
    "thirdly , in the case of the hopfield model the hebb learning rule has been generalized in @xcite in order to bring the model closer to natural systems . in particular",
    ", two types of noise terms have been added .",
    "the first one , an additive external contribution which is independent of the learning algorithm , and assumed to be gaussian distributed , is relaxing the hypothesis that the entire synaptic efficacy is coming from the learning process .",
    "the second one , a random multiplicative factor of order @xmath5 , represents a static disruption of the learning process .",
    "an important example of the latter is random dilution of the network by the pruning or dying of synapses , relaxing the unrealistic condition that every neuron is connected to every other one .",
    "the effects of both these static fluctuations on the recall process in the hopfield model have been estimated using equilibrium mean - field theory statistical mechanics .",
    "technically speaking , the use of this method allows for symmetric dilution only , because the detailed balance principle , i.e. , absence of microscopic probability currents in the stationary state , is needed to define an energy function .",
    "an additional remark is that non - linear updating of the synapses is allowed in that work .",
    "it has been shown that all these effects can be represented by an additive static gaussian noise in the learning rule and that the model is robust against the interference of this static noise .    in this contribution",
    "we extend the work of sompolinsky @xcite in different directions .",
    "since we use both replica mean - field theory equilibrium methods and non - equilibrium functional integration techniques , the assumption of symmetric couplings is not required such that we can treat all forms of dilution .",
    "moreover , we allow a fraction of the couplings to be anti - hebbian @xcite",
    ". we can also have sequential and parallel updating of the neurons , and we examine the effect of these noise terms in @xmath0-ising networks .",
    "the main results are that for the forms of dilution we have examined the effects can be represented by additive noise in the learning rule and a scaling factor proportional to the average amount of anti - hebbian couplings .",
    "the diluted networks are robust under these effects .",
    "the rest of this paper is organized as follows . in section 2",
    "we introduce the @xmath0-ising neural network model and the types of dilution we are interested in .",
    "section 3 treats the statics of the model , where detailed balance requires , as we will explain , symmetric dilution . in section 4",
    "the dynamics of the network is studied allowing for a general form of dilution .",
    "section 5 presents some concluding remarks .",
    "consider a neural network consisting of @xmath6 neurons which can take values @xmath7 from a discrete set @xmath8 .",
    "the @xmath9 patterns to be stored in this network are supposed to be a collection of independent and identically distributed random variables ( i.i.d.r.v . ) , @xmath10 , @xmath11 , with zero mean , @xmath12 , and variance @xmath13 .",
    "the latter is a measure for the activity of the patterns .",
    "we remark that for simplicity we have taken these variables @xmath14 equidistant and we have also taken the patterns and the neurons out of the same set of variables , but this is no essential restriction . given the configuration @xmath15 , the local field in neuron @xmath16 equals @xmath17 with @xmath18 the synaptic coupling from neuron @xmath19 to neuron @xmath16 .",
    "all neurons are updated sequentially or in parallel through the spin - flip dynamics defined by the transition probabilities @xmath20 }          { \\sum_{s \\in { \\cal s } } \\exp [ - \\beta \\epsilon_i                                     ( s|\\bsigma_{n } ( t))]}\\ , .",
    "\\label{eq : trans}\\ ] ] the configuration @xmath21 is chosen as input . here",
    "the energy potential @xmath22 $ ] is defined by @xmath23=                  -\\frac{1}{2}[h_i({\\bsigma}_{n}(t))s - bs^2 ]           \\ , , \\label{eq : energy}\\ ] ] where @xmath24 is the gain parameter of the system .",
    "the zero temperature limit @xmath25 of this dynamics is given by the updating rule @xmath26              = \\epsilon_i[s_k|{\\bsigma}_{n } ( t ) ] \\,.\\ ] ] this updating rule ( [ eq : enpot ] ) is equivalent to using a gain function @xmath27 , @xmath28\\end{aligned}\\ ] ] with @xmath29 and @xmath30 and @xmath31 the heaviside function .",
    "for finite @xmath0 , this gain function @xmath27 looks like a staircase with @xmath0 steps .",
    "the gain parameter @xmath32 controls the average slope of @xmath27 and , hence , suppresses or enhances the role of the states around zero .",
    "it is clear that the @xmath18 explicitly depend on the architecture .",
    "we are interested in architectures with variable dilution and we also want to allow a fraction of the couplings to be anti - hebbian .",
    "we realize this by choosing the couplings according to the hebb rule multiplied with a factor @xmath33 @xmath34 with the @xmath35 chosen to be i.i.d.r.v and obeying , in general , a distribution of the form @xmath36=",
    "c_1\\delta_{x,1}+c_2\\delta_{x,-1}+(1-c_1-c_2)\\delta_{x,0 }      \\label{nd}\\ ] ] with @xmath37 . in order to allow for variable symmetry as well , we define a joint - probability distribution for @xmath38 ( @xmath39 )",
    "@xmath40=      & \\quad\\left(c_1 ^ 2+\\frac{u+2v+w}{4}\\right )       \\delta_{x,1}\\delta_{y,1}+\\left(c_2 ^ 2+\\frac{u-2v+w}{4}\\right )        \\delta_{x,-1}\\delta_{y,-1 }                          \\nonumber\\\\      & + \\left(c_1(1-c_1-c_2)-\\frac{v+w}{2}\\right )     \\left(\\delta_{x,1}\\delta_{y,0}+\\delta_{x,0}\\delta_{y,1}\\right )                            \\nonumber\\\\      & + \\left(c_2(1-c_1-c_2)+\\frac{v - w}{2}\\right )     \\left(\\delta_{x,0}\\delta_{y,-1}+\\delta_{x,-1}\\delta_{y,0}\\right )                           \\nonumber\\\\      & + \\left(c_1c_2-\\frac{u - w}{4}\\right )     \\left(\\delta_{x,1}\\delta_{y,-1}+\\delta_{x,-1}\\delta_{y,1}\\right )                          \\nonumber\\\\       & + \\left((1-c_1-c_2)^2+w\\right)\\delta_{x,0}\\delta_{y,0 }     \\end{aligned}\\ ] ] with @xmath41 we note that these expressions are symmetric under the change @xmath42 . + these distributions generalize the following cases of random dilution frequently discussed in the literature ( see , e.g. , @xcite )    * symmetric dilution where @xmath43 ( sd ) : due to the symmetry , @xmath44 , @xmath45 and @xmath46 , yielding for eq .",
    "( [ toch ] ) @xmath47=       c_1\\delta_{x,1}\\delta_{y,1}+c_2\\delta_{x,-1}\\delta_{y,-1}+          \\left(1-c\\right)\\delta_{x,0}\\delta_{y,0}\\ ] ] whereby in most cases @xmath48 is taken to be zero , indicating that there are no anti - hebbian couplings mixed in explicitly . * asymmetric dilution with @xmath49 and @xmath50 ( ad ) : in this case @xmath51 and the joint - probability distribution eq .",
    "( [ toch ] ) becomes @xmath52=      & ( u+c^2)\\delta_{x,1}\\delta_{y,1 }         + ( c(1-c)-u)(\\delta_{x,1}\\delta_{y,0}+\\delta_{x,0}\\delta_{y,1 } )                     \\nonumber\\\\       & + ( 1+u-2c+c^2)\\delta_{x,0}\\delta_{y,0}\\ , .\\end{aligned}\\ ] ]    the meaning of the variable dilution as introduced in eq.([eq : jd])-([uvw ] ) can best be understood from the theory of random graphs with @xmath6 nodes and @xmath9 the probability that any two of them are connected ( see , e.g. , @xcite ) .",
    "the number of connections a node has for @xmath53 ( i.e. , in the thermodynamic limit ) goes to a poisson distribution @xmath54 telling us that @xmath55 is the average number of connections per node . in order to indicate what range of dilution we allow for ,",
    "we look at the diameter of the random graph , @xmath56 , i.e. , the maximum distance between any pair of nodes .",
    "this diameter is concentrated around @xmath57 the diameter is clearly 1 in the case that there is an edge with probability @xmath58 and , hence , we have a fully connected graph .",
    "the diameter diverges when @xmath59 .",
    "given that @xmath9 scales with @xmath6 like @xmath60 , several regimes containing different types of subgraphs can be distinguished as a function of @xmath61 @xcite .",
    "in particular , it can be shown that the precise point above which the system becomes completely connected ( such that it is always possible to find a path between any two nodes ) is @xmath62 .",
    "looking at eq .",
    "( [ nd ] ) we find that this describes precisely a random graph with @xmath63 being the probability to have an edge .",
    "it follows that the average number of connections per neuron is @xmath64 . since our @xmath0-ising network is taken to be a mean - field system characterized by an extensive number of long - range interactions we need to have that @xmath65 tends to infinity for @xmath66 and all @xmath67 .",
    "this implies that @xmath68 such that @xmath69 for @xmath70 and the complete connectivity of our model is still guaranteed . in this way , the diameter of our network is @xmath71 for @xmath72 and @xmath73 when @xmath70 , and the average number of connections is given by @xmath74 .",
    "the limit @xmath75 , i.e. , the so - called extremely diluted limit has now a simple interpretation : each neuron has an infinite number of neighboring neurons but such that the average distance between any two neurons tends to infinity .",
    "all this is graphically illustrated in figure 1 .",
    "( fully connected ) ; top right : @xmath76 ( some couplings are cut ) ; bottom left : @xmath70 ( extreme dilution , tree - like structures ) ; and bottom right : not all sites are connected (= finite connectivity).,title=\"fig : \" ] @xmath77@xmath77 ( fully connected ) ; top right : @xmath76 ( some couplings are cut ) ; bottom left : @xmath70 ( extreme dilution , tree - like structures ) ; and bottom right : not all sites are connected (= finite connectivity).,title=\"fig : \" ] +   ( fully connected ) ; top right : @xmath76 ( some couplings are cut ) ; bottom left : @xmath70 ( extreme dilution , tree - like structures ) ; and bottom right : not all sites are connected (= finite connectivity).,title=\"fig : \" ] @xmath77@xmath77 ( fully connected ) ; top right : @xmath76 ( some couplings are cut ) ; bottom left : @xmath70 ( extreme dilution , tree - like structures ) ; and bottom right : not all sites are connected (= finite connectivity).,title=\"fig : \" ]    we remark that in this way we can understand that eq .",
    "( [ eq : jd ] ) has a well - defined meaning .",
    "there is a factor @xmath64 in the denominator which always tends to infinity in the thermodynamic limit @xmath78 , whatever the value of @xmath67 .",
    "finally , we note that a distribution for the @xmath33 can be chosen that does not support complete connectivity ( see figure 1 bottom right ) , e.g. , by taking @xmath79 , with @xmath80 a fixed number independent of @xmath6 .",
    "the system then consists of disjoint clusters of different sizes ( = finite connectivity ) @xcite .",
    "these models are much more difficult to handle and are outside the scope of the present work .",
    "it is well - known that networks with symmetric couplings @xmath18 obey the detailed balance principle .",
    "systems with detailed balance can be described by standard equilibrium statistical mechanics making use of a hamiltonian .",
    "the @xmath0-ising neural network we have defined in section 2 is of a mean - field type with couplings ( [ eq : jd ] ) of infinite range and restricting ourselves to symmetric dilution @xmath43 ( cfr . , eq .",
    "( [ sd ] ) ) the couplings @xmath81 remain symmetric .",
    "the long time behavior of this network for sequential updating of the neurons is then governed by the following hamiltonian @xmath82 for parallel updating of the neurons a hamiltonian is defined in terms of a two - spin representation @xcite as @xmath83 the thermodynamic properties of the system are then determined from the free energy using the standard techniques of replica mean - field theory @xcite .",
    "it is outside the scope of this contribution to go into the very details of these techniques but we indicate how the dilution is going to affect the calculations and the results .",
    "the free energy is given as the logarithm of the partition function averaged over the disorder .",
    "this average is done using the replica technique such that we can write @xmath84 where @xmath85 is the hamiltonian of each replica of the system given above and @xmath86 is the replica index .",
    "next , we first average this partition function over the dilution . at this point",
    "we remark that we consider the case of sequential updating .",
    "parallel updating does not contain any additional difficulties @xcite .",
    "furthermore , we do not write explicitly the term proportional to @xmath32 in the hamiltonian since it does not contain the @xmath33 such that it can be inserted at any point of the calculation . due to",
    "the central limit theorem we can easily see that @xmath87 , so that we can expand @xmath88 \\ , .\\end{aligned}\\ ] ] the average over the dilution variables @xmath33 with @xmath89 denoted by @xmath90 is then straightforward @xmath91                        \\nonumber\\\\     & \\simeq\\underset{{\\bsigma}}{\\mbox{tr } }       \\exp{\\left[\\frac{\\beta}{2}\\,\\,\\frac{c_1-c_2}{c }          \\sum_\\alpha\\sum_{i , j\\neq i}j_{ij }    \\sigma_i^\\alpha\\sigma_j^\\alpha               + \\frac{\\beta^2}{4}\\,\\,\\frac{c-(c_1-c_2)^2}{c^2 }          \\sum_{i , j \\neq i}(j_{ij})^2     \\left(\\sum_\\alpha\\sigma_i^\\alpha\\sigma_j^\\alpha\\right)^2           \\right]}\\end{aligned}\\ ] ] where we have used that @xmath92 . finally , noting that @xmath93 as @xmath53 , with @xmath94 the finite loading capacity , we write the replicated partition function averaged over the dilution as @xmath95 }                            \\ , .\\ ] ] using a hubbard - stratonovich transformation this can be further expressed as @xmath96 }      \\right\\rangle_{{\\bf d } }      \\label{dilav}\\ ] ] with @xmath97 indicating the average over the @xmath98 , a set of i.i.d.r.v . for @xmath89 and symmetric with @xmath99 .",
    "they obey a gaussian distribution with mean @xmath100 and variance @xmath101 with @xmath102 .",
    "this shows that the symmetric dilution ( [ sd ] ) , being a form of multiplicative noise , introduces an effective hamiltonian where the learning rule now contains an additive noise term plus a scaling of the hebbian part @xmath103 the scaling factor expresses the influence of explicitly allowing an average amount @xmath104 $ ] of anti - hebbian couplings . for @xmath50",
    "the scaling term is @xmath58 and in this case the expression ( [ jste ] ) agrees with the results of sompolinsky @xcite for the hopfield model @xmath3 and with the results of theumann and erichsen @xcite for the symmetrically diluted @xmath0-ising model .",
    "an analogous calculation can be performed for parallel updating of the neurons leading precisely to eq .",
    "( [ dilav ] ) with @xmath105 replaced by @xmath106 and with the factor @xmath107 removed . in all these calculations we have only used the first and second moments of the probability distribution for the @xmath33 .",
    "this is due to the mean - field character of the network we have treated .",
    "therefore , one can easily extend this result to any ( symmetric ) multiplicative noise such that @xmath108 with obvious meaning of the superscripts @xmath109 and @xmath110 , with @xmath111 a gaussian with mean zero and variance @xmath58 , and with @xmath112 and @xmath113 characterizing the multiplicative noise .    to close this section",
    "we remark that when we are interested in the further calculation of the free energy , e.g. , in order to obtain the equilibrium fixed - point equations and the loading capacity we have to average the partition function ( [ dilav ] ) over the pattern distribution employing the standard techniques .",
    "we refer to the literature for the final results @xcite .",
    "for asymmetric dilution ( recall , e.g. , ( [ ad ] ) ) the system we have defined does not obey detailed balance . in this case",
    "we have to resort to techniques used in non - equilibrium statistical mechanics .",
    "the method we use to study the effect of general dilution in the dynamics is the generating functional approach @xcite .",
    "the idea of this approach is to look at the probability to find a certain microscopic path in time .",
    "the basic tool to study the statistics of these paths is the generating function @xmath114\\big\\rangle_{{\\bf c } }                         \\big\\rangle_{{\\bxi } }            = \\sum_{{\\bsigma}(0)} ... \\sum_{{\\bsigma}(t ) }      \\exp{\\left(-i\\sum_{s=0}^t\\sum_{i=1}^n(\\phi_i(s)\\sigma_i(s))\\right ) }        \\big\\langle\\big\\langle \\mbox{p}[{\\bsigma}(0) ... {\\bsigma}(t ) ]             \\big\\rangle_{{\\bf c}}\\big\\rangle_{{\\bxi}}\\ ] ] where the dilution and pattern averages are denoted by @xmath115 and where @xmath116 $ ] is the probability to have a certain path in phase space @xmath117 = \\mbox{p}[{\\bsigma}(0 ) ] \\prod_{t'=0}^{t-1}w[{\\bsigma}(t')|{\\bsigma}(t'-1)]\\ ] ] with @xmath118 $ ] the transition probabilities from @xmath119 to @xmath120 . for the @xmath0-ising network with parallel updating they read @xmath121=",
    "\\prod_{i=1}^n\\frac{\\exp{\\left(\\beta\\sigma_i(t')\\sum_j          j_{ij}^c\\sigma_j(t'-1)- \\beta b\\sigma_i^2(t')\\right ) } }          { \\underset{\\sigma}{\\textrm{tr } }       \\exp{\\left(\\beta\\sigma\\sum_jj_{ij}^c\\sigma_j(t'-1)-\\beta                  b\\sigma^2 \\right ) } } \\ , .\\ ] ] we remark again that sequential updating can be treated in an analogous way .",
    "furthermore , we note that one can obtain all the order parameters of the system through derivation of the generating function , e.g. , the overlap between the network configuration and an embedded pattern is given by @xmath122 }   { \\partial \\phi_i(t ) }         \\big\\rangle_{{\\bf c}}\\big\\rangle_{{\\bxi } } \\ , .\\ ] ] introducing the local fields @xmath123 and their conjugates @xmath124 , we arrive at @xmath125\\big\\rangle_{{\\bf c } }         \\big\\rangle_{{\\bxi}}&=     \\int\\{d{\\bf h}\\}\\{d\\hat{{\\bf h}}\\ }     \\sum_{{\\bsigma}(0)} ...",
    "\\sum_{{\\bsigma}(t ) }     \\mbox{p}[{\\bsigma}(0)]\\prod_{s>0}^t\\prod_i\\mbox{p}[\\sigma_i(s)|         h_i(s-1),b ]                        \\nonumber\\\\    & = \\exp{\\left(n\\mathcal{f}[{\\bsigma},\\hat{{\\bf h}}]\\right ) }",
    "\\prod_i\\prod_{s=0}^t\\exp{\\left(i\\hat{h}_i(s)h_i(s)-i\\phi_i(s )             \\sigma_i(s)\\right)}\\end{aligned}\\ ] ] where the disorder and dilution are put in one term @xmath126=",
    "\\frac{1}{n}\\log{\\left ( \\bigg\\langle\\bigg\\langle      \\exp{\\left[-i\\sum_i\\sum_{s=0}^t\\hat{h}_i(s )          \\sum_jj_{ij}^c\\sigma_j(s)\\right ] }                 \\bigg\\rangle_{{\\bf c}}\\bigg\\rangle_{{\\bxi}}\\right ) } \\ , .\\ ] ] we do not report the whole treatment of the generating function here but refer to , e.g. , @xcite for more details on the method .",
    "we do discuss in detail , however , the dilution average since this is precisely the subject of our study .",
    "as in the statics we use that the diluted couplings @xmath81 are of order @xmath127 and that the @xmath92 .",
    "we note that , in principle , diagonal coupling terms usually taken to be of the form @xmath128 can be present , but they are taken out separately because they do not need to be averaged over . introducing then @xmath129 and using the fact that the distribution for the asymmetric dilution eq .",
    "( [ toch ] ) is i.i.d.r.v . for @xmath89 we can write @xmath130 }                   \\big\\rangle_{(c_{ij},c_{ji } ) }",
    "\\nonumber\\\\      & = \\prod_{i < j}\\big\\langle       1-\\frac{i}{c}j_{ij}\\left(c_{ij}b_{ij}+c_{ji}b_{ji}\\right )          -\\frac{1}{2c^2}j_{ij}^2\\left(c_{ij}b_{ij}+c_{ji}b_{ji}\\right)^2          + \\mathcal{o}((cn)^{-3/2})\\big\\rangle_{(c_{ij},c_{ji } ) }                   \\nonumber\\\\      & = \\prod_{i , j\\neq i}\\exp{\\left[-i\\,\\,\\frac{c_1-c_2}{c}j_{ij}b_{ij }        -\\frac{s}{4c^2}j_{ij}^2(b_{ij}+b_{ji})^2-          \\frac{u - s}{2c^2}j_{ij}^2b_{ij}b_{ji}\\right ] }                      \\nonumber\\\\       & = \\prod_{i , j\\neq i}\\exp{\\left[-i\\,\\,\\frac{\\sqrt{c - s}}{c}j_{ij}b_{ij }            -\\frac{s}{2c^2}j_{ij}^2(w_+b_{ij}+ w_-b_{ji})^2\\right]}\\end{aligned}\\ ] ] where we have introduced a number of shorthand notations @xmath131 we remark that the parameter @xmath132 is a measure for the symmetry in the @xmath33 , and takes values in the range @xmath133 $ ] ; @xmath134 is complete symmetry , @xmath135 is complete antisymmetry . finally , a hubbard - stratonovich transformation can be done as before leading to @xmath136)b_{ij}\\right ] } \\bigg\\rangle_{{\\bf d}}\\ ] ] where the @xmath98 are a set of i.i.d.r.v . for @xmath89 , symmetric , @xmath137 , and obeying a gaussian probability distribution with @xmath100 and @xmath138 .",
    "this shows that also general asymmetric dilution can be written as additive noise in the learning rule with a scaling of the hebbian part @xmath139 where now @xmath111 .",
    "we note explicitly that the parameters @xmath140 and @xmath141 ( recall eq .",
    "( [ uvw ] ) ) do not play a role in the calculation .",
    "only the mean @xmath67 , the variance @xmath142 and the covariance @xmath143 of the probability distribution for the random dilution are important .",
    "again , for @xmath50 the scaling factor expressing the explicit mixing in of anti - hebbian couplings is @xmath58 and in the symmetric case ( sd ) , @xmath134 in addition , such that the additive noise reduces to @xmath144 ( see eq .",
    "( [ jste ] ) ) .    as in the statics ,",
    "the calculation of the dynamics can be pursued by doing the average over the patterns and expressions for the overlap , correlation functions and response functions can be obtained .",
    "this is beyond the purpose of the present contribution and will be presented in @xcite . finally , the remarks made before that sequential dynamics can be treated similarly and that the effective dynamics is formally the same , boil down to the fact that the explicit form of the transition rates is not needed to derive the effective path average .",
    "only the initial conditions need to factorize over the site index @xmath16 and this is a characteristic property of mean - field systems .    using the generating functional approach it is clear that the effect of random dilution on the learning rule in neural networks based on other types of multi - state neurons @xcite , e.g. , blume - emery - griffiths neurons , potts neurons , ashkin - teller neurons can be examined in an analogous way .",
    "in this work we have examined the effects of general random dilution , which can be considered as a static disruption of the learning process and , hence , as a form of multiplicative noise in the hebbian learning rule , on the statics and dynamics of @xmath0-ising multi - state neural networks .",
    "a fraction of the couplings is explicitly taken to be anti - hebbian . both sequential updating and parallel updating of the neurons are allowed .     as a function of the loading capacity @xmath86 for the @xmath145 ising model with uniform patterns ( @xmath146 ) , @xmath147 , @xmath148 , and symmetric dilution with @xmath50 and @xmath149 , @xmath150 and @xmath151 .",
    "]    it is shown , using replica mean - field theory , that for symmetric dilution the effect on the learning rule appears as additive gaussian noise together with a scaling of the hebbian part .",
    "this scaling is a measure for the average amount of anti - hebbian couplings and becomes @xmath58 when no such couplings are present .",
    "this extends previous results in the literature .",
    "moreover , for general dilution , including asymmetric forms , a similar result is obtained using the generating functional approach employed in studies of the dynamics of disordered systems .",
    "the additive noise is determined as a function of the mean , the variance and the covariance of the probability distribution characterizing the dilution .",
    "we conjecture that this result is valid for any network of mean - field type .",
    "although this is beyond the scope of the present work it is relevant to remark that it can be shown that the type of multi - state networks studied here are robust against the interference of static noise coming from random dilution ( cfr .",
    ", e.g. , @xcite ) in the sense that the quality of the retrieval properties is affected very little , unless the amount of dilution is very high . as an illustration of this fact",
    "we show in fig .",
    "2 the information content , being the product of the loading capacity and the mutual information , of a @xmath145-ising neural network with parameters as indicated in the figure caption for several amounts of symmetric dilution @xmath67 . for more details on this",
    "we refer to @xcite .",
    "the fact that the effect of random dilution can be expressed as additive noise in the learning rule makes the analytical calculations on these networks easier and more transparent and can be of help in the non - trivial numerical simulations of diluted systems .",
    "we thank rubem erichsen jr . for informative discussions .",
    "this work has been supported in part by the fund of scientific research , flanders - belgium .",
    "d. boll , `` multi - state neural networks based upon spin - glasses : a biased overview '' , in _ advances in condensed matter and statistical mechanics _ , eds . s. korutcheva and r. cuerno , ( commack , ny : nova science publishers ) , to appear , 2004 t. tadaki and j. inoue , `` multi - state image restoration by transmission of bit - composed data '' , _ phys .",
    "_ e * 65 * , 016101 , 2001 j. inoue and d.m .",
    "carlucci , `` image restoration using the @xmath0-ising spin - glass '' , _ phys .",
    "e _ * 64 * , 036121 , 2001 .",
    "schacter , k.a .",
    "norman and w. koutstaal , `` the cognitive neuroscience of constructive memory '' , _ annu .",
    "psychol . _ * 49 * , 289 , 1998 h. komatsu and y. ideura , `` relationship between color , shape and pattern selectivity of neurons in the inferior temporal cortex of the monkey '' , _ j. neurophysiol .",
    "_ * 70 * , 677 , 1993 l.r .",
    "squire , `` mechanisms of memory '' , _ science _ * 232 * , 1612 , 1986 j.j .",
    "hopfield , `` neural networks and physical systems with emergent collective computational abilities '' , _ proc .",
    "usa _ * 79 * , 2554 , 1982 j.j .",
    "hopfield , `` neurons with graded response have collective computational properties like those of two - state neurons '' , _ proc .",
    "usa _ * 81 * , 3088 , 1984 c.m .",
    "marcus and r.m .",
    "westerfelt , `` dynamics of iterated - map networks '' , _ phys .",
    "a _ * 40 * , 501 , 1989 c.m .",
    "marcus , f.m .",
    "waugh and r.m .",
    "westerfelt , `` associative memory in an analog iterated - map neural network '' , _ phys .",
    "a _ * 41 * , 3355 , 1990 d.  sherrington and s.  kirkpatrick , `` solvable model of a spin - glass '' , _ phys .",
    "_ * 35 * , 1792 , 1972 s.k .",
    "ghatak and d.  sherrington , `` crystal field effects in a general s - ising spin - glass '' , _ j. phys .",
    "c : solid state physics _ * 10 * , 3149 , 1977 j. garca - ojalvo and j.m .",
    "sancho , _ noise in spatially extended systems _ , springer , ny , 1999 d. boll , r. erichsen jr and t. verbeiren , `` parallel versus sequential dynamics of @xmath0-ising neural networks '' , in preparation s.  amari and k.  maginu , `` statistical neurodynamics of associative memory '' , _ neural networks _ * 1 * , 63 , 1988 h. nishimori , _ statistical physics of spin glasses and information processing _ , oxford univ .",
    "press , 2001 e.d .",
    "siggia , p.c .",
    "martin and h.a . rose , `` statistical dynamics of classical systems '' , _ phys .",
    "a _ * 8 * , 423 , 1973 c. de dominicis , `` dynamics as a substitute for replicas in systems with quenched random impurities ''",
    "_ j. phys .",
    "a : math . gen . _ * 18 * , 4913 , 1978 a.c.c .",
    "coolen , `` statistical mechanics of recurrent neural networks ii - dynamics '' , in _ handbook of biological physics vol 4 _ , ed . by f.",
    "moss and s. gielen , elsevier science , 597 , 2001 h. sompolinsky , `` neural networks with nonlinear synapses and a static noise '' , _ phys .",
    "rev . a _ * 34 * , 2571 , 1986 j.j .",
    "hopfield , d.i . feinstein and r.g .",
    "palmer , `` unlearning has a stabilizing effect in collective memories '' , _ nature _ * 304 * , 158 , 1983 b. bollobs , _ modern graph theory _ ,",
    "springer , new york , 1998 b. wemmenhove and a.c.c .",
    "coolen , `` finite connectivity attractor neural networks '' , _ j. phys .",
    "_ , * 36 * , 9617 , 2003 .",
    "p. peretto , `` collective properties of a neural network : a statistical physics approach '' , _ biol .",
    "_ , * 50 * , 51 , 1984 l. viana and a.j",
    ". bray , `` phase diagrams for dilute spin glasses '' , _ j. phys .",
    "c : solid state physics _ , * 18 * , 3037 , 1985 m. mzard , g. parisi and m.a .",
    "virasoro , _ spin glass theory and beyond _ , world scientific , singapore , 1987 w.k .",
    "theumann and r. erichsen jr .",
    ", `` retrieval behavior and thermodynamic properties of symmetrically diluted @xmath0-ising neural networks '' , _ phys .",
    "e _ * 64 * , 061902 , 2001"
  ],
  "abstract_text": [
    "<S> the effects of a variable amount of random dilution of the synaptic couplings in @xmath0-ising multi - state neural networks with hebbian learning are examined . a fraction of the couplings is explicitly allowed to be anti - hebbian . </S>",
    "<S> random dilution represents the dying or pruning of synapses and , hence , a static disruption of the learning process which can be considered as a form of multiplicative noise in the learning rule . </S>",
    "<S> both parallel and sequential updating of the neurons can be treated . </S>",
    "<S> symmetric dilution in the statics of the network is studied using the mean - field theory approach of statistical mechanics . </S>",
    "<S> general dilution , including asymmetric pruning of the couplings , is examined using the generating functional ( path integral ) approach of disordered systems . </S>",
    "<S> it is shown that random dilution acts as additive gaussian noise in the hebbian learning rule with a mean zero and a variance depending on the connectivity of the network and on the symmetry . </S>",
    "<S> furthermore , a scaling factor appears that essentially measures the average amount of anti - hebbian couplings . + </S>",
    "<S> * keywords : * neural networks , multi - state neurons , stochastic dynamics , thermodynamics , dilution , additive noise , multiplicative noise . </S>"
  ]
}