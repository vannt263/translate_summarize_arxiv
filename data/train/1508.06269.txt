{
  "article_text": [
    "there are many practical scenarios where strategic players with different sets of observations are involved in a time - evolving dynamical process such that their actions influence each others payoffs .",
    "such scenarios include repeated online advertisement auctions , wireless resource sharing , competing sellers and energy markets . in the case of repeated online advertisement auctions , advertisers place bids for locations on a website to sell a product .",
    "these bids are based on the value of that product , which is privately observed by an advertiser and past actions of everybody else , which are observed publicaly .",
    "each advertiser s goal is to maximize its reward , which depends on the value of the products and on the actions taken by everybody else .",
    "a similar scenario can be considered for wireless resource sharing where players are allocated channels that interfere with each other .",
    "each player privately observes its channel gain and takes actions , which may be the choice of modulation and coding scheme and also the transmission power .",
    "the reward here is the rate each player gets at time @xmath1 , which is a function of everyone s channel gain and actions .",
    "consider another scenario where different sellers compete to sell different but related goods which are complementary , substitutable or in general , with externalities .",
    "the true value of the goods is private information of a seller who , at each stage , takes an action to stock some amount of goods for sale .",
    "her profit is based on some market mechanism ( say through walrasian prices ) based on the true value of all the goods and their availability in the market , which depends on the actions of the other sellers .",
    "each seller wants to maximize her own profit .",
    "finally , a similar scenario also exists for energy markets where different suppliers ( to their different end consumers ) bid their estimated power outputs to an independent system operator ( iso ) that forms the market mechanism to determine the prices assessed to the different suppliers .",
    "each supplier wants to maximize its returns , which depend on its cost of production of energy , which is their private information , and the market - determined prices which depend on all the bids .",
    "such dynamical systems with strategic players are modeled as dynamic games . in dynamic games with perfect and symmetric information , subgame perfect equilibrium ( spe ) is an appropriate equilibrium concept @xcite , @xcite , @xcite and there is a backward recursive algorithm to find all subgame perfect equilibria of such games .",
    "maskin and tirole in @xcite introduced the concept of markov perfect equilibrium ( mpe ) for dynamic games with perfect and symmetric information where equilibrium strategies are dependent on some payoff relevant state of the system rather than on the entire history .",
    "however , for games with asymmetric information , since players have different information sets in each period , they need to form a belief on the information sets of other players , based upon which they predict their strategies . as a result , spe or mpe",
    "are not appropriate equilibrium concepts for such setting .",
    "there are several notions of equilibrium for such games , such as perfect bayesian equilibrium ( pbe ) , sequential equilibrium , trembling hand equilibrium @xcite .",
    "each of these notions of equilibrium consists of a strategy and a belief profile of all players .",
    "the equilibrium strategies are optimal given the beliefs and the beliefs are derived from the equilibrium strategy profile and using bayes rule",
    "( whenever possible ) , with some equilibrium concepts requiring further refinements . due to this circular argument of beliefs being consistent with strategies , which are in turn optimal given the beliefs , finding such equilibria is a difficult task .",
    "moreover , strategies are function of histories , which belong to an ever - expanding space , and thus the space of optimization also becomes computationally intractable .",
    "there is no known methodology to find such equilibria for general dynamic games with asymmetric information .    in this paper",
    ", we consider a model where players observe their types privately and publicly observe the actions taken by other players at the end of each period .",
    "their instantaneous rewards depend on everyones types and actions .",
    "we provide a two - step algorithm involving a backward recursion followed by a forward recursion to construct a class of pbe for the dynamic game in consideration , which we call _ structured perfect bayesian equilibria _ ( spbe ) . in these equilibria ,",
    "players strategies are based on their type and a set of beliefs on each type which is common to all players and lie in a time - invariant space .",
    "these beliefs on players types form independent controlled markov processes that together summarize the common information history and are updated individually and sequentially , based on corresponding agents actions and ( partial ) strategies .",
    "the algorithm works as follows . in a backward recursive way , for each stage",
    ", the algorithm finds an equilibrium strategy function for all possible beliefs on types of the players which involves solving a fixed point equation on the space of probability simplexes .",
    "then , the equilibrium strategies and beliefs are obtained through forward recursion by operating on the function obtained in the backward step .",
    "the sbpes that are developed in this paper are analogous to the mpes for dynamic games with perfect information in the sense that players choose their actions based on beliefs that depend on common information and have markovian dynamics , where actions of a players are now partial functions from their private information to their action sets .",
    "related literature on this topic include @xcite and @xcite .",
    "nayyar et al . in @xcite",
    "consider a model of dynamic games with asymmetric information .",
    "there is an underlying controlled markov process where players jointly observe part of the process and also make some observations privately .",
    "it is shown in @xcite that the considered game with asymmetric information , under certain assumptions , can be transformed to another game with symmetric information .",
    "once this is established , a backward recursive algorithm is provided to find mpe of the transformed game , which are equivalently nash equilibria of the transformed symmetric information game . for this strong equivalence to hold , authors in @xcite make a critical assumption in their model : based on the common information , a player s posterior beliefs about the system state and about other players information are independent of the strategies used by the players in the past .",
    "our model is different from the model considered in @xcite .",
    "we assume that the underlying state of the system has independent components , each constituting the type of a player .",
    "however , we do not make any assumption regarding update of beliefs and allow the common information based belief state to depend on players strategies .",
    "ouyang et al . in @xcite",
    "consider a dynamic oligopoly game with @xmath0 strategic sellers of different goods and @xmath2 strategic buyers .",
    "each seller privately observes the valuation of their good , which is assumed to have independent markovian dynamics , thus resulting in a dynamic game of asymmetric information . in each period ,",
    "sellers post prices for their goods and buyers make decisions regarding buying the goods . then a public signal indicating buyers experience is revealed which depends on sellers valuation of the goods .",
    "authors in @xcite consider a policy - dependent common information based belief state based on which they define the concept of common information based equilibria .",
    "they show that for any given update function of this belief state , which is consistent with strategies of the players , if all other players play actions based on this common belief and their private information , then player @xmath3 faces a markov decision process ( mdp ) with respect to its action with state as common belief and its type . for every prior distribution",
    ", this defines a fixed point equation on belief update functions and strategies of all players .",
    "they provide necessary and sufficient conditions for common information based strategy profile and belief update functions to constitute pbe of the game ; however they do no provide a systematic way to find such equilibria .",
    "in addition , because of the special structure of the reward function , the problem admits a degenerate solution where agents strategies do not depend on their private information and therefore no signaling takes place .",
    "this allows existence of myopic , type - independent equilibrium policies ( although other equilibria may also exist ) .",
    "the paper is organized as follows . in section  [ sec :",
    "model ] , we present our model . in section  [ sec : structuralresults ] we present structural results that serve as motivation for spbe . in section  [ sec : result ] we present the main result by providing a two - step backward - forward recursive algorithm to construct a strategy profile and a sequence of beliefs and show that it is a pbe of the dynamic game considered . as an illustration , we apply this algorithm on a discrete version of an example from @xcite on repeated public good game in section  [ sec : example ] .",
    "we conclude in section  [ sec : conc ] .",
    "all proofs are presented in appendices .",
    "we use uppercase letters for random variables and lowercase for their realizations . for any variable ,",
    "subscripts represent time indices and superscripts represent player identities .",
    "we use notation @xmath4 to represent all players other than player @xmath3 i.e. @xmath5 .",
    "we use notation @xmath6 to represent vector @xmath7 when @xmath8 or an empty vector if @xmath9 .",
    "we use @xmath10 to mean @xmath11 .",
    "we remove superscripts or subscripts if we want to represent the whole vector , for example @xmath12 represents @xmath13 . in a similar vein , for any collection of sets @xmath14",
    ", we denote @xmath15 by @xmath16 .",
    "we denote the indicator function of any set @xmath17 by @xmath18 . for any finite set @xmath19",
    ", @xmath20 represents the space of probability measures on @xmath19 and @xmath21 represents its cardinality .",
    "we denote by @xmath22 ( or @xmath23 ) the probability measure generated by ( or expectation with respect to ) strategy profile @xmath24 .",
    "we denote the set of real numbers by @xmath25 . for a probabilistic strategy profile of players @xmath26 where probability of action @xmath27 conditioned on @xmath28 is given by @xmath29",
    ", we use the short hand notation @xmath30 to represent @xmath31 .",
    "all equalities and inequalities involving random variables are to be interpreted in the _ a.s .",
    "we consider a discrete - time dynamical system with @xmath0 strategic players in the set @xmath32 , over a time horizon @xmath33 and with perfect recall .",
    "there is a dynamic state of the system @xmath34 , where @xmath35 is the type of player @xmath3 at time @xmath1 which is perfectly observed and is its private information .",
    "types of the players evolve as conditionally independent , controlled markov processes such that    @xmath36    where @xmath37 are known kernels .",
    "player @xmath3 at time @xmath1 takes action @xmath38 on observing @xmath39 , which is common information among players , and @xmath40 which it observes privately .",
    "the sets @xmath41 are assumed to be finite .",
    "let @xmath42 be a probabilistic strategy of player @xmath3 where @xmath43 such that player @xmath3 plays action @xmath44 according to @xmath45 .",
    "let @xmath46 be a strategy profile of all players . at the end of interval @xmath1 , player",
    "@xmath3 receives an instantaneous reward @xmath47 .",
    "the objective of player @xmath3 is to maximize its total expected reward    @xmath48 with all players being strategic , this problem is modeled as a dynamic game @xmath49 with imperfect and asymmetric information , and with simultaneous moves .",
    "in this section we present structural results for the considered dynamical process that serve as a motivation for finding spbe of the underlying game @xmath49 .",
    "specifically , we define a belief state based on common information history and show that any reward profile that can be obtained through a general strategy profile can also be obtained through strategies that depend on this belief state and player s current type which is its private information .",
    "these structural results are inspired by the analysis of decentralized team problems , which serve as guiding principles to design our equilibrium strategies .",
    "while these structural results provide intuition and the required notation , they are not directly used in the proofs for finding spbes , later , in section  [ sec : result ] .    at any time @xmath1 ,",
    "player @xmath3 has information @xmath50 where @xmath39 is the common information among players , and @xmath51 is the private information of player @xmath3 .",
    "since @xmath50 increases with time , any strategy of the form @xmath52 becomes unwieldy .",
    "thus it is desirable to have an information state in a time - invariant space that succinctly summarizes @xmath50 and that can be sequentially updated .",
    "we first show in fact  [ fact : g2s ] that given common information @xmath39 and its current type @xmath53 , player @xmath3 can discard its type history @xmath54 and play a strategy of the form @xmath55 .",
    "then in fact  [ fact : l1 ] , we show that @xmath39 can be summarized through a belief @xmath56 , defined as follows . for any strategy profile @xmath24 , belief @xmath57 on @xmath58 , @xmath59 , is defined as @xmath60 .",
    "we also define the marginals @xmath61 .    for player @xmath3 ,",
    "we use notation @xmath24 to denote a general policy of type @xmath62 , notation @xmath63 where @xmath64 to denote a policy of the form @xmath65 and notation @xmath66 where @xmath67 to denote a policy of the form @xmath68 .",
    "it should be noted that since @xmath56 is a function of random variables @xmath39 , @xmath66 policy is a special type of @xmath63 policy , which in turn , is a special type of @xmath24 policy .    using the agent - by - agent approach  @xcite ,",
    "we show in fact  [ fact : g2s ] that any expected reward profile of the players that can be achieved by any general strategy profile @xmath24 can also be achieved by a strategy profile @xmath63 .",
    "given a fixed strategy @xmath69 of all players other than player @xmath3 and for any strategy @xmath70 of player @xmath3 , there exists a strategy @xmath71 of player @xmath3 such that @xmath72 which implies @xmath73.[fact : g2s ]    see appendix [ app : d ] .",
    "since any @xmath71 policy is also a @xmath70 type policy , the above fact can be iterated over all players which implies that for any @xmath24 policy profile there exists an @xmath63 policy profile that achieves the same reward profile i.e. @xmath74 .",
    "policies of types @xmath63 still have increasing domain due to increasing common information , @xmath39 . in order to summarize this information , we take an equivalent view of the system dynamics through a common agent , as taken by nayyar et al . in @xcite .",
    "the common agent approach is a general approach that has been used extensively for dynamic team problems @xcite . using this approach ,",
    "the problem can be equivalently described as follows : player @xmath3 at time @xmath1 observes @xmath39 and takes action @xmath75 , where @xmath76 is a partial ( stochastic ) function from its private information @xmath53 to @xmath27 of the form @xmath77 .",
    "these actions are generated through some policy @xmath78 , @xmath79 , that operates on the common information @xmath39 so that @xmath80 $ ] .",
    "then any policy of the form @xmath81 is equivalent to @xmath82 ( \\cdot|x_t^i)$ ] @xcite .",
    "we call a player @xmath3 s policy through common agent to be of type @xmath83 if its actions @xmath84 are taken as @xmath85 $ ] .",
    "we call a player @xmath3 s policy through common agent to be of type @xmath86 where @xmath87 , if its actions @xmath84 are taken as @xmath88 $ ]",
    ". a policy of type @xmath86 is also a policy of type @xmath83 .",
    "there is a one - to - one correspondence between policies of type @xmath71 and of type @xmath83 and between policies of type @xmath89 and of type @xmath86 .    in the following fact , we show that the space of profiles of type @xmath63 is outcome - equivalent to the space of profiles of type @xmath66 .",
    "[ fact : l1 ] for any given strategy profile @xmath63 of all players , there exists a strategy profile @xmath66 such that @xmath90 which implies @xmath91 .",
    "furthermore @xmath56 can be factorized as @xmath92 where each @xmath93 can be updated through an update function @xmath94 where @xmath95 is independent of @xmath63.[fact : s2 m ]    see appendix [ app : a ] .",
    "the above two facts show that any reward profile that can be generated through policy profile of type @xmath24 can also be generated through policy profile of type @xmath66 .",
    "it should be noted that the construction of @xmath71 , as in ( [ eq : defsi ] ) , depends only on @xmath70 , while the construction of @xmath89 depends on the whole policy profile @xmath24 and not just on @xmath70 , since construction of @xmath86 depends on @xmath96 in ( [ eq : defmi ] ) .",
    "thus any unilateral deviation of player @xmath3 in @xmath24 policy profile does not necessarily translate to unilateral deviation of player @xmath3 in the corresponding @xmath66 policy profile .",
    "therefore @xmath24 being an equilibrium of the game ( in some appropriate notion ) does not necessitate the corresponding @xmath66 also being an equilibrium .",
    "as shown in the previous facts , due to the independence of types and their evolution as independent controlled markov processes , for any strategy of the players , joint beliefs on types can be factorized as product of their marginals i.e. @xmath97 . since in this paper",
    ", we only deal with such joint beliefs , to accentuate this independence structure , we define @xmath98 as vector of marginal beliefs where @xmath99 . in the rest of the paper , we will use @xmath100 instead of @xmath56 whenever appropriate , where of course @xmath56 can be constructed from @xmath100 .",
    "similarly , we define vector of belief updates as @xmath101 .",
    "we also change the notation of policies of type @xmath66 as @xmath102 and common agent s policies of type @xmath103 as @xmath104 .",
    "we end this section by noting that finding general pbes of type @xmath24 of the game @xmath49 would be a desirable goal , but due to the space of strategies growing exponentially with time , that would be computationally intractable .",
    "however fact  1 suggests that strategies of type @xmath66 form a class that is rich in the sense that they achieve every possible reward profile . since these strategies are functions of beliefs @xmath56 that lie in a time - invariant space and are easily updatable , equilibria of this type are potential candidates for computation through backward recursion .",
    "in this paper our goal is to devise an algorithm to find structured equilibria of type @xmath66 of the dynamic game @xmath49 .",
    "any history of this game at which players take action is of the form @xmath105 .",
    "let @xmath106 be the set of such histories of the game at time @xmath1 when players take action , @xmath107 be the set of all possible such histories . at any time @xmath1 player",
    "@xmath3 observes @xmath108 and all players together have @xmath109 as common history .",
    "let @xmath110 be the set of observed histories of player @xmath3 at time @xmath1 and @xmath111 be the set of common histories at time @xmath1 .",
    "an appropriate concept of equilibrium for such games is pbe @xcite , which consists of a pair @xmath112 of strategy profile @xmath113 where @xmath114 and a belief profile @xmath115 where @xmath116 that satisfy sequential rationality so that @xmath117    @xmath118}\\left\\ { \\sum_{n = t}^t r^i(x_n , a_n)\\big\\lvert   h^i_t\\right\\ } & \\geq { \\mathbb{e}}^{{\\beta}^{i } \\beta^{*,-i},\\ , \\mu^*[h_t^i]}\\left\\ { \\sum_{n = t}^t r^i(x_n , a_n)\\big\\lvert   h^i_t\\right\\ } , \\;\\ ; \\;\\ ;    \\label{eq : seqeq}\\end{aligned}\\ ] ] and the beliefs satisfy some consistency conditions as described in  @xcite . in general , a belief for player @xmath3 at time @xmath1 , @xmath119 is defined on history @xmath120 given its private history @xmath108 .",
    "here player @xmath3 s private history @xmath121 consists of a public part @xmath122 and a private part @xmath40 . at any time @xmath1 ,",
    "the relevant uncertainty player @xmath3 has is about other players type @xmath123 . in our setting ,",
    "due to independence of types , player @xmath3 s current type @xmath53 does not provide any information about @xmath123 as will be shown later .",
    "for this reason we consider beliefs that are functions of each agent s history @xmath124 only through the common history @xmath125 .",
    "hence , for each agent @xmath3 , its belief for each history @xmath126 is derived from a common belief @xmath127 $ ] which itself factorizes into a product of marginals @xmath128 $ ] , as will be shown later .",
    "thus we can sufficiently use the system of beliefs , @xmath129 with @xmath130 , with the understanding that agent @xmath3 s belief on @xmath123 is @xmath131(x_t^{-i})=\\prod_{j\\neq i } \\mu^{*,j}_t[a_{1:t-1}](x_t^j)$ ] . under the above structure , all consistency conditions that are required for pbes  @xcite are automatically satisfied .",
    "structural results from section  [ sec : structuralresults ] provide us motivation to study equilibria of the form @xmath132 , which are equivalent to policy profiles of the form @xmath133 ( a^i_t|x_t^i))_{i\\in { \\mathcal{n}}}$ ] and have the advantage of being defined on a time - invariant space .",
    "in this section , we define an equilibrium generating function @xmath134 , where @xmath135 and a sequence of functions @xmath136 , where @xmath137 , in a backward recursive way , as follows .",
    "* initialize @xmath138 , @xmath139 * for @xmath140 , let @xmath141 $ ] be generated as follows .",
    "set @xmath142 $ ] , where @xmath143 is the solution , if it exists s instantaneous reward does not depend on its private type @xmath53 , the fixed point equation always has a type - independent solution @xmath144 since it degenerates to a best - response - like equation .",
    "] , of the following equation , @xmath145 , @xmath146 + where expectation in ( [ eq : m_fp ] ) is with respect to random variables @xmath147 through the measure + @xmath148 and @xmath149 is defined in the proof of fact  [ fact : l1 ] and in particular claim  [ claim : c1 ] .",
    "+ furthermore , set @xmath150    it should be noted that in ( [ eq : m_fp ] ) , @xmath151 is not the outcome of the maximization operation as in a best response equation similar to that of a bayesian nash equilibrium . rather ( [ eq : m_fp ] ) has characteristics of a fixed point equation",
    "this is because the maximizer @xmath152 appears in both , the left - hand - side and the right - hand - side of the equation .",
    "this distinct construction allows the maximization operation to be done with respect to the variable @xmath153 for every @xmath53 separately as opposed to be done with respect to the whole function @xmath154 , and is pivotal in the construction .    to highlight the significance of structure of ( [ eq : m_fp ] )",
    ", we contrast it with two alternate incorrect constructions .    * following the common information approach as in decentralized team problems  @xcite , instead of ( [ eq : m_fp ] ) , suppose @xmath84 were constructed as equilibrium on common agents actions @xmath155 , i.e. for a fixed @xmath156 , @xmath157 + it should be noted that in ( [ eq : fp2 ] ) , the argument of the maximization operation , @xmath75 , appears both , in generation of action @xmath44 and in the update of the belief @xmath56 .",
    "moreover , ( [ eq : fp2 ] ) is not conditioned on @xmath53 , the private information of player @xmath3 , similar to the case in the corresponding team problem .",
    "this is because the common agent who does not observe the private information of the player @xmath3 , averages out that information .",
    "while this averaging of private information works for the team problem whose objective is to maximize the total expected reward , for the case with strategic players , it is incompatible with the sequential rationality condition in ( [ eq : seqeq ] ) , which requires conditioning on the entire history @xmath158 and not just the common information @xmath39 .",
    "+ if the private information is also conditioned on , the construction still remains invalid , as discussed next .",
    "* instead of ( [ eq : m_fp ] ) , suppose @xmath84 were constructed as best response of player @xmath3 to other players actions @xmath159 , similar to a standard bayesian nash equilibrium .",
    "for a fixed @xmath160 , @xmath161 then @xmath162 would be a function of @xmath163 and @xmath164 through a best response relation @xmath165 , where @xmath166 is appropriately defined from ( [ eq : fp3 ] ) .",
    "consequently , every component of the solution of the fixed point equation @xmath167 , if it existed , would be a function of the whole type profile @xmath168 , resulting in a mapping @xmath169 $ ] .",
    "since player @xmath3 only observes its own type @xmath53 , it would not be able to implement the corresponding @xmath152 and therefore the construction would be invalid .",
    "as discussed above , a pair of strategy and belief profile @xmath170 is a pbe if it satisfies ( [ eq : seqeq ] ) .",
    "based on @xmath103 defined above in ( [ eq : vt+1])([eq : vdef ] ) , we now construct a set of strategies @xmath171 and beliefs @xmath172 for the game",
    "@xmath49 in a forward recursive way , as follows , beliefs at time @xmath1 are functions of each agent s history @xmath124 only through the common history @xmath125 and are the same for all agents . ] . as before",
    ", we will use the notation @xmath173 : = ( \\mu_t^{*,i}[a_{1:t-1}])_{i\\in { \\mathcal{n}}}$ ] where @xmath174 $ ] can be constructed from @xmath173 $ ] as @xmath174(x_t ) = \\prod_{i=1}^n\\mu_t^{*,i}[a_{1:t-1}](x_t^i)\\ ; \\forall a_{1:t-1}\\in \\mathcal{h}_t^c$ ] where @xmath175 $ ] is a belief on @xmath53 .",
    "* initialize at time @xmath176 , @xmath177(x_1 ) & : = \\prod_{i=1}^n q_1^i(x_1^i ) .",
    "\\label{eq : mu*def0}\\end{aligned}\\ ] ] * for @xmath178 @xmath179(a^i_{t}|x_{t}^i ) \\label{eq : beta*def}\\end{aligned}\\ ] ] and @xmath180 & : = \\bar{f}(\\mu_t^{*,i}[a_{1:t-1 } ] , \\theta_t^i[\\underline{\\mu}_t^*[a_{1:t-1 } ] ] , a_t ) \\label{eq : mu*def}\\end{aligned}\\ ] ]    where @xmath181 is defined in the proof of fact  [ fact : l1 ] and in particular claim  [ claim : c1 ] .",
    "we now state our main result .",
    "[ thm : main ] a strategy and belief profile @xmath112 , constructed through backward / forward recursion algorithm described in section  [ sec : result ] is a pbe of the game , i.e. @xmath182 , @xmath183 } \\left\\ { \\sum_{n = t}^t r^i(x_n , a_n ) \\big\\lvert   a_{1:t-1 } , x_{1:t}^i \\right\\ } \\geq   { \\mathbb{e}}^{\\beta_{t : t}^{i }",
    "\\beta_{t : t}^{*,-i},\\ , \\mu_{t}^{*}[a_{1:t-1 } ] } \\left\\ { \\sum_{n = t}^t r^i(x_n , a_n ) \\big\\lvert   a_{1:t-1 } , x_{1:t}^i \\right\\}. \\label{eq : prop}\\end{aligned}\\ ] ]    see appendix  [ app : b ]    an intuitive explanation for why all players are able to use a common belief is the following .",
    "the sequence of beliefs defined above serve two purposes .",
    "first , for any player @xmath3 , it puts a belief on @xmath123 to compute an expectation on the current and future rewards .",
    "secondly , it predicts the actions of the other players since their strategies are functions of these beliefs . since for",
    "any strategy profile , @xmath53 is conditionally independent of @xmath123 given the common history @xmath39 and since other players do not observe @xmath53 , knowledge of @xmath53 does not affect this belief and thus in our definition , all players can use the same belief @xmath172 which is independent of their private information .",
    "independence of types is a crucial assumption in proving the above result , which manifests itself in lemma  [ lemma:3 ] in appendix  [ app : lemmas ] , used in the proof of theorem  [ thm : main ] .",
    "this is because , at equilibrium , player @xmath3 s reward - to - go at time @xmath1 , conditioned on its type @xmath53 , depends on its strategy at time @xmath1 , @xmath184 , only through its action @xmath27 and is independent of the corresponding partial function @xmath185 . in other words ,",
    "given @xmath53 and @xmath27 , player @xmath3 s reward - to - go is independent of @xmath184 .",
    "we discuss this in more detail below .    at equilibrium ,",
    "all players observe past actions @xmath39 and update their belief @xmath56 , which is the same as @xmath174 $ ] , through the equilibrium strategy profile @xmath171 .",
    "now suppose at time @xmath1 , player @xmath3 decides to unilaterally deviate to @xmath186 at time @xmath1 for some history @xmath39 keeping the rest of its strategy the same .",
    "then other players still update their beliefs @xmath187 same as before and take their actions through equilibrium strategy @xmath188 operated on @xmath56 and @xmath123 , whereas player @xmath3 forms a new belief @xmath189 on @xmath168 which depends on strategy profile @xmath190 .",
    "thus at time @xmath1 player @xmath3 would need both the beliefs @xmath191 to compute its expected future reward ; @xmath192 to predict other players actions and @xmath189 to form a true belief on @xmath168 based on its information .",
    "as it turns out , due to independence of types , @xmath189 does not provide additional information to player @xmath3 to compute its future expected reward and thus it can be discarded .",
    "intuitively , this is so because the belief on type @xmath193 , @xmath194 is a function of strategy and action of player @xmath193 till time @xmath1 ( as shown in claim  1 in the proof of theorem  [ thm : main ] in appendix  [ app : b ] ) ; thus @xmath195 .",
    "now since player @xmath3 already observes its type @xmath53 , its belief @xmath196 on @xmath53 does not provide any additional information to player @xmath3 , and thus @xmath56 ( which is the same as @xmath174 $ ] ) sufficiently computes future expected reward for player @xmath3 .",
    "also @xmath192 is updated from @xmath56 , @xmath197 and @xmath198 , and is independent of @xmath186 given @xmath27 .",
    "this implies player @xmath3 can use the equilibrium strategy @xmath199 to update its future belief , as used in ( [ eq : m_fp ] ) .",
    "then by construction of @xmath103 and specifically due to ( [ eq : m_fp ] ) , player @xmath3 does not gain by unilaterally deviating at time @xmath1 keeping the remainder of its strategy the same .",
    "finally , we note that in the two - step backward - forward algorithm described above , once the equilibrium generating function @xmath103 is defined through backward recursion , the spbes can be generated through forward recursion for any prior distribution @xmath200 on types @xmath201 . since , in comparison to the backward recursion ,",
    "the forward recursive part of the algorithm is computationally insignificant , the algorithm computes spbes for different prior distributions at the same time .    in the next section ,",
    "we discuss an example to illustrate the methodology described above for the construction of spbes .",
    "we consider a discrete version of example  8.3 from ( * ? ? ?",
    "* ch.8 ) , which is an instance of a repeated public good game .",
    "there are two players who play a two period game . in each period @xmath1 , they simultaneously decide whether to contribute to the period @xmath1 public good , which is a binary decision @xmath202 for player @xmath203 . before the start of period 2 , both players know the actions taken by them in period 1 . for both periods ,",
    "each player gets reward 1 if at least one of them contributed and 0 if none does .",
    "player @xmath3 s cost of contributing is @xmath204 which is its private information .",
    "both players believe that @xmath205s are drawn independently and identically with probability distribution @xmath200 with support @xmath206 ; @xmath207 , such that @xmath208 where @xmath209 .",
    "this example is similar to our model where @xmath210 and reward for player @xmath3 in period @xmath1 is @xmath211    we will use the backward recursive algorithm , defined in section  [ sec : result ] , to find an spbe of this game . for period @xmath212 and for @xmath203 , the partial functions @xmath75 can equivalently be defined through scalars @xmath213 and @xmath214 such that @xmath215 , @xmath216 and @xmath217 , @xmath218 , where @xmath219 $ ] .",
    "henceforth , we will use @xmath213 and @xmath214 interchangeably with the corresponding @xmath75 .    for @xmath220 and for any fixed @xmath221 , where @xmath222 $ ] represents a probability measure on the event @xmath223 ,",
    "player @xmath3 s reward is    @xmath224    let @xmath225 $ ] and equivalently @xmath226 $ ] be defined through the following fixed point equation , which is equivalent to ( [ eq : m_fp ] ) .",
    "for @xmath203    @xmath227    since @xmath228 , @xmath229 achieves the maximum in ( [ eq : ext2h ] ) . thus ( [ eq : ext2l])([eq : ext2h ] ) can be reduced to , @xmath230 @xmath231 this implies , @xmath232    the fixed point equation ( [ eq : p2eq ] ) has the following solutions ,    1 .",
    "@xmath233 for @xmath234 , \\pi_2 ^ 2 \\leq x^l$ ] * @xmath235 * @xmath236 * @xmath237 * @xmath238 .",
    "@xmath239 for @xmath240 $ ] * @xmath241 * @xmath242 * @xmath243 * @xmath244 .",
    "3 .   @xmath245 for @xmath246 * @xmath241 * @xmath236 * @xmath237 * @xmath247 .",
    "4 .   @xmath248 for @xmath249 $ ] where @xmath250 $ ] * @xmath241 * @xmath251 * @xmath237 * @xmath244 .",
    "5 .   @xmath252 for @xmath253 , \\pi_2 ^ 2 = x^l$ ] where @xmath254 $ ] * @xmath241 * @xmath236 * @xmath237 * @xmath255 . 6 .",
    "@xmath256 for @xmath257 * @xmath241 * @xmath258 * @xmath237 * @xmath259 .    figure  [ fig1 ] shows these solutions in the space of @xmath260 .    ) , height=336 ]    thus for any @xmath261 , there can exist multiple equilibria and correspondingly multiple @xmath262 $ ] can be defined . for any particular @xmath263 , at @xmath176 ,",
    "the fixed point equation that needs to be solved is of the form , @xmath264    @xmath265    where @xmath266 and    @xmath267    if the denominators in ( [ eq : f_0])([eq : f_1 ] ) are strictly positive , else @xmath268 as in the proof of fact  [ fact : l1 ] , and in particular claim  [ claim : c1 ] .",
    "a solution of the fixed point equation in ( [ eq : ext1l])-([eq : ext1h ] ) defines @xmath269 $ ] .",
    "using one such @xmath103 defined as follows , we find an spbe of the game for @xmath270 .",
    "we use @xmath262 $ ] as one possible set of solutions of ( [ eq : p2eq ] ) , shown in figure  [ fig : exfig2 ] and described below ,    @xmath271 = ( \\tilde{p}^{1l}_2,\\tilde{p}^{2l}_2 , \\tilde{p}^{1h}_2,\\tilde{p}^{2h}_2 )   = \\lb { ( \\frac{1-x^l}{1-\\pi_2 ^ 1},\\frac{1-x^l}{1-\\pi_2 ^ 2},0,0 ) \\;\\;\\;\\;\\hfill \\pi_2 ^ 1 \\in[0 , x^l ) , \\pi_2 ^ 2 \\in [ 0,x^l ) \\text { } \\\\ ( 1,0,0,0 )",
    "\\;\\;\\;\\hfill \\pi_2 ^ 1 \\in[0 , x^l ] , \\pi_2 ^ 2 \\in [ x^l,1 ] \\text { } \\\\ ( 0,1,0,0)\\;\\;\\;\\hfill \\pi_2 ^ 1 \\in[x^l,1 ] , \\pi_2 ^ 2 \\in[0 , x^l ] \\text { } \\\\ ( 1,1,0,0)\\;\\;\\;\\hfill \\pi_2 ^ 1\\in(x^l , 1 ] , \\pi_2 ^ 2 \\in ( x^l , 1 ] .",
    "} \\end{aligned}\\ ] ]    $ ] described in ( [ eq : theta2]),height=336 ]    then , through iteration on the fixed point equation ( [ eq : ext1l])-([eq : ext1h ] ) and using the aforementioned @xmath262 $ ] , we numerically find ( and analytically verify ) that @xmath269 = ( \\tilde{p}^{1l}_1,\\tilde{p}^{2l}_1 , \\tilde{p}^{1h}_1,\\tilde{p}^{2h}_1 ) = ( 0,1,0,0)$ ] is a fixed point .",
    "thus    @xmath272    with beliefs @xmath273 = ( q,1 ) , \\mu^*_2[01 ] = ( q,0 ) , \\mu^*_2[10]=(q,1 ) , \\mu^*_2[11 ] = ( q,0 ) $ ] and @xmath274 $ ] is an spbe of the game . in this equilibrium ,",
    "player 2 at time @xmath176 , contributes according to her type whereas player 1 never contributes , thus player 2 reveals her private information through her action whereas player 1 does not .",
    "since @xmath263 is symmetric , there also exists an ( antisymmetric ) equilibrium where at time @xmath176 , players strategies reverse i.e. player 2 never contributes and player 1 contributes according to her type .",
    "we also obtain a symmetric equilibrium where @xmath269 = ( \\frac{1-x^l}{(1-q)(1+x^l)},\\frac{1-x^l}{(1-q)(1+x^l)},0,0)$ ] as a fixed point when @xmath275 , resulting in beliefs @xmath273 = ( p , p ) , \\mu^*_2[01 ] = ( p,0 ) , \\mu^*_2[10]=(0,p ) , \\mu^*_2[11 ] = ( 0,0 ) $ ] where @xmath276 .",
    "in this paper , we study a class of dynamic games with asymmetric information where player @xmath3 observes its true private type @xmath53 and together with other players , observe past actions of everybody else .",
    "the types of the players evolve as conditionally independent , controlled markov processes , conditioned on players current actions .",
    "we present a two - step backward - forward recursive algorithm to find spbe of this game , where equilibrium strategies are function of a markov belief state @xmath56 , which depends on the common information , and current private types of the players .",
    "the backward recursive part of this algorithm defines an equilibrium generating function .",
    "each period in backward recursion involves solving a fixed point equation on the space of probability simplexes for every possible belief on types . then using this function , equilibrium strategies and beliefs",
    "are defined through a forward recursion .    in this paper",
    "we consider perfectly observable , independent dynamic types of the players .",
    "future work includes considering types of players where players do not perfectly observe their types , rather they make noisy observations . in general , this methodology opens the door for finding pbes for many applications , analytically or numerically , which was not feasible before .",
    "one such case would be dynamic lqg games where types evolve linearly with gaussian noise and players incur quadratic cost .",
    "the authors wish to acknowledge vijay subramanian for his contribution to the paper .",
    "achilleas anastasopoulos wishes to acknowledge ashutosh nayyar for the fruitful discussion and criticism of an early draft of this work presented during the ita 2012 conference .",
    "we prove this fact in the following steps .",
    "* in claim  [ claim : condind ] , we prove that for any policy profile @xmath24 and @xmath277 , @xmath51 for @xmath278 are conditionally independent given the common information @xmath279 .",
    "* in claim  [ claim : b2 ] , using claim  [ claim : condind ] , we prove that for every fixed strategy @xmath69 of the players @xmath280 , @xmath281 is a controlled markov process for player @xmath3 . * for a given policy @xmath24 , we define a policy @xmath71 of player @xmath3 from @xmath24 as @xmath282 .",
    "* in claim  [ claim : b3 ] , we prove that the dynamics of this controlled markov process @xmath283 under @xmath284 are same as under @xmath24 i.e. @xmath285 .",
    "* in claim  [ claim : b4 ] , we prove that w.r.t .",
    "random variables @xmath286 , @xmath53 is sufficient for player @xmath3 s private information history @xmath51 i.e. @xmath287 . * from ( c ) ,",
    "( d ) and ( e ) we then prove the result of the fact that @xmath288 .",
    "+    for any policy profile @xmath24 and @xmath289 , @xmath290 [ claim : condind ]    @xmath291    for a fixed @xmath69 , @xmath292 is a controlled markov process with state @xmath293 and control action @xmath27 . [",
    "claim : b2 ]    @xmath294    where ( [ eq : xiicmp1 ] ) follows from claim  [ claim : condind ] since @xmath295 is conditionally independent of @xmath51 given @xmath39 and the corresponding probability is only a function of @xmath69 .    for any given policy profile @xmath24",
    ", we construct a policy @xmath71 in the following way ,    @xmath296    where dependence of ( [ eq : sdef1 ] ) on only @xmath70 is due to claim  [ claim : condind ] .",
    "the dynamics of the markov process @xmath297 under @xmath284 are the same as under @xmath24 i.e. @xmath298 [ claim : b3 ]    we prove this by induction .",
    "clearly , @xmath299 now suppose ( [ eq : d0 ] ) is true for @xmath300 which also implies that the marginals @xmath301 .",
    "then    @xmath302    where ( [ eq : d2 ] ) is true from induction hypothesis , definition of @xmath71 in ( [ eq : defsi ] ) and since @xmath292 is a controlled markov process as proved in claim  [ claim : b2 ] and its update kernel does not depend on policy @xmath70.this completes the induction step .    for any policy @xmath24 , @xmath303 [",
    "claim : b4 ]    @xmath304 now    @xmath305    where ( [ eq : d3 ] ) follows from claim  [ claim : condind ] .",
    "hence    @xmath306    finally ,    @xmath307    where ( [ eq : d6 ] ) follows from ( [ eq : d5 ] ) in claim  [ claim : b4 ] and ( [ eq : d8 ] ) from ( [ eq : d0 ] ) in claim  [ claim : b3 ] .",
    "for this proof we will assume the common agents strategies to be probabilistic as opposed to being deterministic , as was the case in section  [ sec : structuralresults ] .",
    "this means actions of the common agent , @xmath75 s are generated probabilistically from @xmath83 as @xmath308 , as opposed to being deterministically generated as @xmath80 $ ] , as before .",
    "these two are equivalent ways of generating actions @xmath27 from @xmath39 and @xmath53 .",
    "we avoid using the probabilistic strategies of common agent throughout the main text for ease of exposition and because it conceptually does not affect the results .",
    "we prove this fact in the following steps .",
    "we view this problem from the perspective of a common agent .",
    "let @xmath96 be the coordinator s policy corresponding to policy profile @xmath24 .",
    "let @xmath309 .",
    "* in claim  [ claim : c1 ] , we show that @xmath56 can be factorized as @xmath92 where each @xmath93 can be updated through an update function @xmath310 and @xmath181 is independent of common agent s policy @xmath96 .",
    "* in claim  [ claim : c2 ] , we prove that @xmath311 is a controlled markov process .",
    "* we construct a policy profile @xmath103 from @xmath24 such that @xmath312 .",
    "* in claim  [ claim : c3 ] , we prove that dynamics of this markov process @xmath311 under @xmath103 is same as under @xmath96 i.e. @xmath313 .",
    "* in claim  [ claim : c4 ] , we prove that with respect to random variables @xmath314 , @xmath56 can summarize common information @xmath39 i.e. @xmath315 . * from ( c ) , ( d ) and ( e ) we that prove the result of the fact that @xmath316 which is equivalent to @xmath317 , where @xmath66 is the policy profile of players corresponding to @xmath103 .",
    "+    @xmath56 can be factorized as @xmath92 where each @xmath93 can be updated through an update function @xmath318 and @xmath95 is independent of common agent s policy @xmath96 .",
    "we also say @xmath319 .",
    "[ claim : c1 ]    we prove this by induction . since @xmath320 , the base case is verified .",
    "now suppose @xmath321 .",
    "then ,    [ eq : piupdate ] @xmath322    where ( [ eq : c1a ] ) follows from induction hypothesis .",
    "it is assumed in ( [ eq : c1b])-([eq : c1a ] ) that the denominator is not 0 .",
    "if denominator corresponding to any @xmath75 is zero , we define @xmath323 where @xmath192 still satisfies ( [ eq : pi_prod ] ) .",
    "thus @xmath324 and @xmath325 where @xmath181 and @xmath149 are appropriately defined from above .",
    "@xmath311 is a controlled markov process with state @xmath326 and control action @xmath327 [ claim : c2 ]    @xmath328    for any given policy profile @xmath96 , we construct policy profile @xmath103 in the following way .",
    "@xmath329    @xmath330 [ claim : c3 ]    we prove this by induction . for @xmath176 ,",
    "@xmath331 now suppose @xmath332 is true for @xmath1 , then    @xmath333    where ( [ eq : g2 ] ) is true from induction hypothesis , definition of @xmath334 in ( [ eq : defmi ] ) and since @xmath311 is a controlled markov process as proved in claim  [ claim : c2 ] and thus its update kernel does not depend on policy @xmath335 .",
    "this completes the induction step .    for any policy @xmath335 , @xmath336 [",
    "claim : c4 ]    @xmath337    finally ,    @xmath338    where ( [ eq : g6 ] ) follows from ( [ eq : g5 ] ) , ( [ eq : g7 ] ) is change of variable and ( [ eq : g8 ] ) from ( [ eq : g0 ] ) .",
    "we prove ( [ eq : prop ] ) using induction and from results in lemma  [ lemma:2 ] , [ lemma:3 ] and [ lemma:1 ] proved in appendix  [ app : lemmas ] .    for base case at @xmath339 , @xmath340 @xmath341 }",
    "\\left\\ {   r^i(x_t , a_t ) \\big\\lvert a_{1:t-1 } , x_{1:t}^i \\right\\}&=v^i_t(\\underline{\\mu}_t^*[a_{1:t-1 } ] , x_t^i )   \\label{eq : t2a}\\\\ & \\geq { \\mathbb{e}}^{\\beta_{t}^{i } \\beta_{t}^{*,-i},\\ , \\mu_{t}^{*}[a_{1:t-1 } ] } \\left\\ { r^i(x_t , a_t ) \\big\\lvert a_{1:t-1 } , x_{1:t}^i \\right\\}.   \\label{eq : t2}\\end{aligned}\\ ] ]        @xmath344 } \\left\\ { \\sum_{n = t+1}^t r^i(x_n , a_n ) \\big\\lvert a_{1:t } , x_{1:t+1}^i \\right\\ } \\\\   & \\geq { \\mathbb{e}}^{\\beta_{t+1:t}^{i } \\beta_{t+1:t}^{*,-i},\\ , \\mu_{t+1}^ { * } [ a_{1:t } ] } \\left\\ { \\sum_{n = t+1}^t r^i(x_n , a_n ) \\big\\lvert   a_{1:t } , x_{1:t+1}^i \\right\\}. \\label{eq : propindhyp}\\end{aligned}\\ ] ]    then @xmath345 , we have @xmath346 } \\left\\ { \\sum_{n = t}^t r^i(x_n , a_n ) \\big\\lvert a_{1:t-1 } , x_{1:t}^i \\right\\ } \\nonumber \\\\ & = v^i_t(\\underline{\\mu}^*_t[a_{1:t-1 } ] , x_t^i)\\label{eq : t1}\\\\ & \\geq { \\mathbb{e}}^{\\beta_t^i \\beta_t^{*,-i } , \\,\\mu_t^*[a_{1:t-1 } ] } \\left\\ { r^i(x_t , a_t ) + v_{t+1}^i ( \\underline{\\mu}^*_{t+1}[a_{1:t-1}a_t ] , x_{t+1}^i ) \\big\\lvert a_{1:t-1 } , x_{1:t}^i \\right\\ }   \\label{eq : t3}\\\\ & = { \\mathbb{e}}^{\\beta_t^i \\beta_t^{*,-i } , \\,\\mu_t^*[a_{1:t-1 } ] } \\left\\ { r^i(x_t , a_t ) + \\right . \\nonumber \\\\ & \\hspace{3cm}\\left . { \\mathbb{e}}^{\\beta_{t+1:t}^{*,i } \\beta_{t+1:t}^{*,-i},\\ , \\mu_{t+1}^ { * } [ a_{1:t-1},a_t ] } \\left\\ { \\sum_{n = t+1}^t r^i(x_n , a_n ) \\big\\lvert a_{1:t-1},a_t , x_{1:t+1}^i \\right\\ }   \\big\\vert a_{1:t-1 } , x_{1:t}^i \\right\\ }   \\label{eq : t3b}\\\\ & \\geq { \\mathbb{e}}^{\\beta_t^i \\beta_t^{*,-i } , \\,\\mu_t^*[a_{1:t-1 } ] } \\left\\ { r^i(x_t , a_t ) + \\right.\\nonumber \\\\ & \\hspace{3 cm } \\left.{\\mathbb{e}}^{\\beta_{t+1:t}^{i } \\beta_{t+1:t}^{*,-i } \\mu_{t+1}^{*}[a_{1:t-1},a_t ] } \\left\\ { \\sum_{n = t+1}^t r^i(x_n , a_n ) \\big\\lvert a_{1:t-1},a_t , x_{1:t}^i , x_{t+1}^i\\right\\ } \\big\\vert a_{1:t-1 } , x_{1:t}^i \\right\\ }   \\label{eq : t4 } \\\\ & = { \\mathbb{e}}^{\\beta_t^i \\beta_t^{*,-i } , \\ ,   \\mu_t^*[a_{1:t-1 } ] } \\left\\ { r^i(x_t , a_t ) +   { \\mathbb{e}}^{\\beta_{t : t}^{i } \\beta_{t : t}^{*,-i } \\mu_{t}^{*}[a_{1:t-1 } ] } \\left\\ { \\sum_{n = t+1}^t r^i(x_n , a_n ) \\big\\lvert a_{1:t-1},a_t , x_{1:t}^i , x_{t+1}^i\\right\\ } \\big\\vert a_{1:t-1 } , x_{1:t}^i \\right\\ }   \\label{eq : t5}\\\\ & = { \\mathbb{e}}^{\\beta_{t : t}^{i } \\beta_{t : t}^{*,-i}\\ , \\mu_{t}^{*}[a_{1:t-1 } ] } \\left\\ { \\sum_{n = t}^t r^i(x_n , a_n ) \\big\\lvert a_{1:t-1 } ,   x_{1:t}^i \\right\\ }   \\label{eq : t6},\\end{aligned}\\ ] ]    where ( [ eq : t1 ] ) follows from lemma  [ lemma:1 ] , ( [ eq : t3 ] ) follows from lemma  [ lemma:2 ] , ( [ eq : t3b ] ) follows from lemma  [ lemma:1 ] , ( [ eq : t4 ] ) follows from induction hypothesis in ( [ eq : propindhyp ] ) and ( [ eq : t5 ] ) follows from lemma  [ lemma:3 ]",
    ". moreover , construction of @xmath103 in ( [ eq : m_fp ] ) , and consequently definition of @xmath171 in ( [ eq : beta*def ] ) are pivotal for ( [ eq : t5 ] ) to follow from ( [ eq : t4 ] ) .",
    "we note that @xmath172 satisfies the consistency condition of  @xcite from the fact that ( a ) for all @xmath1 and for every common history @xmath39 , all players use the same belief @xmath174 $ ] on @xmath168 and ( b ) the belief @xmath347 can be factorized as @xmath174 = \\prod_{i=1}^n \\mu_t^{*,{i}}[a_{1:t-1 } ] \\ ; \\forall a_{1:t-1 } \\in \\mathcal{h}_t^c$ ] where @xmath348 is updated through bayes rule ( @xmath181 ) as in claim  [ claim : c1 ] in appendix  [ app : a ] .",
    "[ lemma:2 ] @xmath349 @xmath350 , x_t^i ) \\geq { \\mathbb{e}}^{\\beta_t^i \\beta_t^{*,-i},\\ , \\mu_t^*[a_{1:t-1 } ] } \\left\\ { r^i(x_t , a_t ) + v_{t+1}^i ( f(\\underline{\\mu}_t^*[a_{1:t-1 } ] , \\beta_t^*(\\cdot|a_{1:t-1},\\cdot ) , a_t ) , x_{t+1}^i ) \\big\\lvert   a_{1:t-1 } , x_{1:t}^i \\right\\}.\\label{eq : lemma2}\\end{aligned}\\ ] ]      suppose the claim is not true for @xmath1 .",
    "this implies @xmath351 such that @xmath352 } \\left\\ { r^i(x_t , a_t ) + v_{t+1}^i ( f(\\underline{\\mu}_t^*[\\hat{a}_{1:t-1 } ] , \\beta_t^*(\\cdot|\\hat{a}_{1:t-1},\\cdot ) , a_t ) , x_{t+1}^i ) \\big\\lvert \\hat{a}_{1:t-1},\\hat{x}_{1:t}^i \\right\\ } > v_t^i(\\underline{\\mu}_t^*[\\hat{a}_{1:t-1 } ] , \\hat{x}_{t}^i).\\label{eq : e8}\\end{aligned}\\ ] ] we will show that this leads to a contradiction . construct @xmath353",
    "@xmath355 , \\hat{x}_t^i)\\\\ & = \\max_{\\gamma^i_t(\\cdot|\\hat{x}_t^i ) } { \\mathbb{e}}^{\\gamma^i_t(\\cdot|\\hat{x}_t^i ) \\beta_t^{*,-i } , \\ , \\mu_t^*[\\hat{a}_{1:t-1 } ] } \\left\\ { r^i(\\hat{x}_t^ix_t^{-i},a_t ) + v_{t+1}^i ( f(\\underline{\\mu}_t^*[\\hat{a}_{1:t-1 } ] , \\beta_t^{*}(\\cdot|\\hat{a}_{1:t-1},\\cdot ) ,",
    "a_t ) , x_{t+1}^i ) \\big\\lvert   \\hat{x}_{t}^i \\right\\ } , \\label{eq : e11}\\\\ & \\geq{\\mathbb{e}}^{\\hat{\\gamma}_t^i(\\cdot|\\hat{x}_t^i ) \\beta_t^{*,-i},\\,\\mu_t^*[\\hat{a}_{1:t-1 } ] } \\left\\ { r^i(x_t , a_t ) + v_{t+1}^i ( f(\\underline{\\mu}_t^*[\\hat{a}_{1:t-1 } ] , \\beta_t^{*}(\\cdot|\\hat{a}_{1:t-1},\\cdot ) , a_t ) , { x}_{t+1}^i ) \\big\\lvert \\hat{x}_{t}^i \\right\\ }   \\nonumber \\\\ & = \\sum_{x_t^{-i},a_t , x_{t+1 } }    \\left\\ { r^i(\\hat{x}_t^ix_t^{-i},a_t ) + v_{t+1}^i ( f(\\underline{\\mu}_t^*[\\hat{a}_{1:t-1 } ] , \\beta_t^{*}(\\cdot|\\hat{a}_{1:t-1},\\cdot ) , a_t ) , x_{t+1}^i)\\right\\}\\times \\nonumber \\\\   & \\hspace{50pt } \\mu_t^{*,-i}[\\hat{a}_{1:t-1 } ] ( x_t^{-i } ) \\hat{\\gamma}_t^i(a^i_t|\\hat{x}_t^i ) \\beta_t^{*,-i}(a_t^{-i}|\\hat{a}_{1:t-1 } , x_t^{-i})q_t^i(x_{t+1}^i|\\hat{x}_t^i , a_t )   \\\\ & = \\sum_{x_t^{-i},a_t , x_{t+1 } }   \\left\\ { r^i(\\hat{x}_t^ix_t^{-i},a_t ) + v_{t+1}^i ( f(\\underline{\\mu}_t^*[\\hat{a}_{1:t-1 } ] , \\beta_t^{*}(\\cdot|\\hat{a}_{1:t-1},\\cdot ) , a_t ) , x_{t+1}^i)\\right\\}\\times \\nonumber \\\\ & \\hspace{50pt } \\mu_t^{*,-i}[\\hat{a}_{1:t-1}](x_t^{-i } ) \\hat{\\beta}^i_t(a_t^i|\\hat{a}_{1:t-1 } , \\hat{x}_{1:t}^i ) \\beta_t^{*,-i}(a_t^{-i}|\\hat{a}_{1:t-1 } , x_t^{-i})q_t^i(x_{t+1}^i|\\hat{x}_t^i , a_t ) \\label{eq : e9}\\\\ & = { \\mathbb{e}}^{\\hat{\\beta}_t^i \\beta_t^{*,-i } , \\mu_t^*[\\hat{a}_{1:t-1 } ] } \\left\\ { r^i(\\hat{x}_t^ix_t^{-i},a_t ) + v_{t+1}^i ( f(\\underline{\\mu}_t^*[\\hat{a}_{1:t-1 } ] , \\beta_t^{*}(\\cdot|\\hat{a}_{1:t-1},\\cdot ) , a_t ) , x_{t+1}^i ) \\big\\lvert \\hat{a}_{1:t-1 } ,   \\hat{x}_{1:t}^i \\right\\ }   \\\\ & > v_t^i(\\underline{\\mu}_t^*[\\hat{a}_{1:t-1 } ] , \\hat{x}_{t}^i ) \\label{eq : e10 } \\end{aligned}\\ ] ] where ( [ eq : e11 ] ) follows from definition of @xmath356 in ( [ eq : vdef ] ) , ( [ eq : e9 ] ) follows from definition of @xmath357 and ( [ eq : e10 ] ) follows from ( [ eq : e8 ] ) .",
    "however this leads to a contradiction .",
    "[ lemma:3 ] @xmath358 and @xmath359 @xmath360 }   \\left\\ { \\sum_{n = t+1}^t r^i(x_n , a_n ) \\big\\lvert   a_{1:t } , x_{1:t+1}^i \\right\\ } & = { \\mathbb{e}}^{\\beta^i_{t+1:t } \\beta^{*,-i}_{t+1:t},\\ , \\mu_{t+1}^{*}[a_{1:t } ] }   \\left\\ { \\sum_{n = t+1}^t r^i(x_n , a_n ) \\big\\lvert a_{1:t } , x_{1:t+1}^i \\right\\}. \\label{eq : f1}\\end{aligned}\\ ] ] thus the above quantities do not depend on @xmath184 .",
    "essentially this claim stands on the fact that @xmath361 $ ] can be updated from @xmath362 , \\beta_t^{*,-i}$ ] and @xmath198 , as @xmath361 = \\prod_{j\\neq i } \\bar{f}(\\mu_t^{*,-i}[a_{1:t-1 } ] , \\beta_t^{*,-i } , a_t)$ ] as in claim  [ claim : c1 ] .",
    "since the above expectations involve random variables @xmath363 , we consider @xmath364 } ( x_{t+1}^{-i } , a_{t+1:t } , x_{t+2:t } \\big\\lvert   a_{1:t } , x_{1:t+1}^i ) $ ] .",
    "@xmath365 } ( x_{t+1}^{-i } , a_{t+1:t } , x_{t+2:t}\\big\\lvert   a_{1:t } , x_{1:t+1}^i ) \\nonumber \\\\ & = \\frac{\\sum_{x_t^{-i}}p^{\\beta^i_{t : t } \\beta^{*,-i}_{t : t},\\ , \\mu_{t}^{*}[a_{1:t-1 } ] } ( x_t^{-i } , a_t , x_{t+1 } , a_{t+1:t } , x_{t+2:t } \\big\\lvert a_{1:t-1 } , x_{1:t}^i )   } { \\sum_{\\tilde{x}_{t}^{-i } } p^{\\beta^i_{t : t } \\beta^{*,-i}_{t : t},\\ , \\mu_{t}^{*}[a_{1:t-1 } ] } ( \\tilde{x}_t^{-i } , a_t , x_{t+1}^i\\big\\lvert   a_{1:t-1 } , x_{1:t}^i ) }   \\label{eq : f2}\\end{aligned}\\ ] ] we consider the numerator and the denominator separately . the numerator in ( [ eq : f2 ] )",
    "is given by @xmath366 } ( x_t^{-i } \\big\\lvert   a_{1:t-1 } , x_{1:t}^i ) \\beta_t^{i}(a_t^{i}|a_{1:t-1 } , x_{1:t}^{i } ) \\beta_t^{*,-i}(a_t^{-i}|a_{1:t-1 } , x_t^{-i})q(x_{t+1}|x_t , a_t)\\nonumber \\\\ & \\hspace{19pt}p^{\\beta^i_{t : t } \\beta^{*,-i}_{t : t},\\ , \\mu_{t}^{*}[a_{1:t-1 } ] } ( a_{t+1:t } , x_{t+2:t}| a_{1:t } , x_{1:t-1}^i , x_{t : t+1 } ) \\\\ = & \\sum_{x_t^{-i}}\\mu_t^{*,-i}[a_{1:t-1}](x_t^{-i})\\beta_t^{i}(a_t^{i}|a_{1:t-1 } , x_{1:t}^{i } ) \\beta_t^{*,-i}(a_t^{-i}|a_{1:t-1 } , x_t^{-i } ) q^i(x^i_{t+1}|x^i_t , a_t)\\nonumber \\\\ & \\hspace{19pt}q^{-i}(x^{-i}_{t+1}|x^{-i}_t , a_t)p^{\\beta^i_{t+1:t } \\beta^{*,-i}_{t+1:t},\\ , \\mu_{t+1}^{*}[a_{1:t } ] } ( a_{t+1:t } , x_{t+2:t}| a_{1:t } , x_{1:t}^i , x_{t+1})\\label{eq : nr2}\\end{aligned}\\ ] ] where ( [ eq : nr2 ] ) follows from the conditional independence of types given common information , as shown in claim  [ claim : condind ] , and the fact that probability on @xmath367 given @xmath368 $ ] depends on @xmath369 $ ] through @xmath370 .",
    "similarly , the denominator in ( [ eq : f2 ] ) is given by        @xmath374(x_t^{-i})\\beta_t^{*,-i}(a_t^{-i}|a_{1:t-1 } , x_t^{-i } ) q_{t+1}^{-i}(x^{-i}_{t+1}|x^{-i}_t , a_t)}{\\sum_{\\tilde{x}_{t}^{-i } } \\mu_t^{*,-i}[a_{1:t-1}](\\tilde{x}_t^{-i } ) \\beta_t^{*,-i}(a_t^{-i}|a_{1:t-1 } , \\tilde{x}_t^{-i } ) } \\times \\nonumber \\\\ & p^{\\beta^i_{t+1:t } \\beta^{*,-i}_{t+1:t},\\ , \\mu_{t+1}^{*}[a_{1:t } ] } ( a_{t+1:t } , x_{t+2:t}| a_{1:t } , x_{1:t}^i , x_{t+1})\\\\ = & \\mu_{t+1}^{*,-i}[a_{1:t}](x_{t+1}^{-i } ) p^{\\beta^i_{t+1:t } \\beta^{*,-i}_{t+1:t},\\ , \\mu_{t+1}^{*}[a_{1:t } ] } ( a_{t+1:t } , x_{t+2:t}| a_{1:t } , x_{1:t}^i , x_{t+1})\\label{eq : f6}\\\\ = & p^{\\beta_{t+1:t}^ { i } \\beta_{t+1:t}^ { * , -i},\\ , \\mu_{t+1}^{*}[a_{1:t } ] }   ( x_{t+1}^{-i } , a_{t+1:t},x_{t+2:t } | a_{1:t } , x_{1:t+1}^i ) , \\end{aligned}\\ ] ]      [ lemma:1 ] @xmath375 , @xmath376 , x_t^i ) = { \\mathbb{e}}^{\\beta_{t : t}^{*,i } \\beta_{t : t}^{*,-i},\\mu_{t}^{*}[a_{1:t-1 } ] } \\left\\ { \\sum_{n = t}^t r^i(x_n , a_n ) \\big\\lvert   a_{1:t-1 } , x_{1:t}^i \\right\\ } .\\end{aligned}\\ ] ]    we prove the lemma by induction .",
    "for @xmath339 , @xmath377 } \\left\\ {   r^i(x_t , a_t ) \\big\\lvert a_{1:t-1 } ,   x_{1:t}^i \\right\\}\\nonumber \\\\   & = \\sum_{x_t^{-i } a_t } r^i(x_t , a_t)\\mu_{t}^{*}[a_{1:t-1}](x_t^{-i } ) \\beta_{t}^{*,i}(a_t^i|a_{1:t-1},x_{t}^i ) \\beta_{t}^{*,-i}(a_t^{-i}|a_{1:t-1 } , x_{t}^{-i})\\\\     & = v^i_t(\\underline{\\mu}^*_t[a_{1:t-1 } ] , x_t^i ) \\label{eq : c1},\\end{aligned}\\ ] ]",
    "suppose the claim is true for @xmath342 , i.e. , @xmath358 @xmath379 , x_{t+1}^i ) = { \\mathbb{e}}^{\\beta_{t+1:t}^{*,i } \\beta_{t+1:t}^{*,-i},\\ , \\mu_{t+1}^{*}[a_{1:t } ] } \\left\\ { \\sum_{n = t+1}^t r^i(x_n , a_n ) \\big\\lvert a_{1:t } , x_{1:t+1}^i \\right\\ } \\label{eq : cindhyp}.\\end{aligned}\\ ] ] then @xmath375 , we have    @xmath380 } \\left\\ { \\sum_{n = t}^t r^i(x_n , a_n ) \\big\\lvert   a_{1:t-1 } , x_{1:t}^i \\right\\ } \\nonumber \\\\ & =   { \\mathbb{e}}^{\\beta_{t : t}^{*,i } \\beta_{t : t}^{*,-i } , \\,\\mu_{t}^{*}[a_{1:t-1 } ] } \\left\\{r^i(x_t , a_t ) + \\right.\\nonumber \\\\ & \\hspace{3cm}\\left .",
    "{ \\mathbb{e}}^{\\beta_{t : t}^{*,i } \\beta_{t : t}^{*,-i } , \\,\\mu_{t}^{*}[a_{1:t-1 } ] } \\left\\ { \\sum_{n = t+1}^t r^i(x_n , a_n)\\big\\lvert a_{1:t-1 } ,   a_t , x_{1:t}^i , x_{t+1}^i\\right\\ } \\big\\lvert a_{1:t-1 } ,   x_{1:t}^i \\right\\ } \\label{eq : c2}\\\\ & =   { \\mathbb{e}}^{\\beta_{t : t}^{*,i } \\beta_{t : t}^{*,-i } , \\,\\mu_{t}^{*}[a_{1:t-1 } ] } \\left\\{r^i(x_t , a_t ) + \\right.\\nonumber \\\\ & \\hspace{3cm}\\left .   { \\mathbb{e}}^{\\beta_{t+1:t}^{*,i } \\beta_{t+1:t}^{*,-i},\\ , \\mu_{t+1}^{*}[a_{1:t-1},a_t ] } \\left\\ { \\sum_{n = t+1}^t r^i(x_n , a_n)\\big\\lvert a_{1:t-1},a_t , x_{1:t}^i , x_{t+1}^i\\right\\ } \\big\\lvert a_{1:t-1 } , x_{1:t}^i \\right\\ } \\label{eq : c3}\\\\ & =   { \\mathbb{e}}^{\\beta_{t : t}^{*,i } \\beta_{t : t}^{*,-i } , \\,\\mu_{t}^{*}[a_{1:t-1 } ] } \\left\\{r^i(x_t , a_t ) +   v^i_{t+1}(\\underline{\\mu}^*_{t+1}[a_{1:t-1}a_t ] , x_{t+1}^i ) \\big\\lvert   a_{1:t-1 } , x_{1:t}^i \\right\\ } \\label{eq : c4}\\\\ & =   { \\mathbb{e}}^{\\beta_{t}^{*,i } \\beta_{t}^{*,-i } , \\,\\mu_{t}^{*}[a_{1:t-1 } ] } \\left\\{r^i(x_t , a_t ) +   v^i_{t+1}(\\underline{\\mu}^*_{t+1}[a_{1:t-1}a_t ] , x_{t+1}^i ) \\big\\lvert   a_{1:t-1 } , x_{1:t}^i \\right\\ } \\label{eq : c5}\\\\ & = v^i_{t}(\\underline{\\mu}^*_{t}[a_{1:t-1 } ] , x_t^i ) \\label{eq : c6},\\end{aligned}\\ ] ]    where ( [ eq : c3 ] ) follows from lemma  [ lemma:3 ] in appendix  [ app : lemmas ] , ( [ eq : c4 ] ) follows from the induction hypothesis in ( [ eq : cindhyp ] ) , ( [ eq : c5 ] ) follows because the random variables involved in expectation , @xmath381 do not depend on @xmath382 and ( [ eq : c6 ] ) follows from the definition of @xmath199 in the forward recursion in  ( [ eq : beta*def ] ) , the definition of @xmath383 in ( [ eq : mu*def ] ) and the definition of @xmath356 in ( [ eq : vdef ] ) ."
  ],
  "abstract_text": [
    "<S> we consider a finite horizon dynamic game with @xmath0 selfish players who observe their types privately and take actions , which are publicly observed . </S>",
    "<S> players types evolve as conditionally independent markov processes , conditioned on their current actions . </S>",
    "<S> their actions and types jointly determine their instantaneous rewards . </S>",
    "<S> since each player has a different information set , this is a dynamic game with asymmetric information and there is no known methodology to find perfect bayesian equilibria ( pbe ) for such games in general . in this paper , we develop a methodology to obtain a class of pbe using a belief state based on players common information . </S>",
    "<S> we first show that any expected reward profile that can be achieved by any general strategy profile can also be achieved by a policy based on players private information and this belief state . with this structural result as our motivation , we develop our main result that provides a two - step backward - forward recursive algorithm to find a class of pbe of this game that are based on this belief state . </S>",
    "<S> we refer to such equilibria as _ structured bayesian perfect equilibria _ </S>",
    "<S> ( spbe ) . </S>",
    "<S> the backward recursive part of this algorithm defines an equilibrium generating function . </S>",
    "<S> each period in the backward recursion involves solving a fixed point equation on the space of probability simplexes for every possible belief on types . using this function , equilibrium strategies and beliefs </S>",
    "<S> are generated through a forward recursion . </S>"
  ]
}