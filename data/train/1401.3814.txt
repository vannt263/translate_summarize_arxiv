{
  "article_text": [
    "information geometry established by amari and nagaoka @xcite is an elegant method for statistical inference .",
    "this method provides us a very general approach to statistical parameter estimation . under this framework",
    ", we easily find that the efficient estimator can be given with less calculation complexity for exponential families and a curved exponential families under the independent and identical distributed case .",
    "therefore , we can expect a similar structure in the markov chains .",
    "the preceding studies @xcite introduced the concept of exponential families of transition matrices",
    ". however , in their definition , although the maximum likelihood estimator has the asymptotic efficiency , i.e. , attains the cramr - rao bound asymptotically , the maximum likelihood estimator is not necessarily calculated with less calculation complexity .",
    "that is , the maximum likelihood estimator has a complex form so that it requires long calculation time in their model .",
    "further , it is quite difficult to calculate the cramr - rao bound even with the asymptotic first order coefficient because these papers focused only on the limit of the inverse of the fisher information . from a practical viewpoint , it is needed to calculate the asymptotic first order coefficient .",
    "so , it is strongly required to resolve these two problems for the estimation of markovian process , i.e. , ( 1 ) to give an asymptotically efficient estimator with small calculation and ( 2 ) to derive a formula for the asymptotic cramr - rao bound with small calculation .",
    "the purpose of this paper is giving the answers for these two problems .",
    "for this purpose , we notice another type of exponential family of transition matrices by nakagawa and kanaya @xcite and nagaoka @xcite .",
    "they defined the fisher information matrix in their sense . on the other hand , for the estimation of the probability distribution , the class of curved exponential families plays an important role as a wider class of distribution families than the class of exponential families .",
    "that is , when the unknown distribution belongs to a curved exponential family , the asymptotic efficient estimator can be treated in the information - geometrical framework .",
    "therefore , to deal with these problems in a wider class of families of transition matrices , we introduce a curved exponential family of transition matrices as a subset of an exponential family of transition matrices in the sense of @xcite .",
    "since any exponential family of transition matrices is a curved exponential family , the class of curved exponential families is a larger class of families of transition matrices than the class of exponential families .",
    "especially , any smooth subset of transition matrices on a finite - size system forms a curved exponential family of transition matrices .",
    "our purpose is resolving the above two problems for a curved exponential family as well as for an exponential family .",
    "since any smooth parametric subfamily of transition matrices on a finite - size system forms a curved exponential family , our treatment for curved exponential families has a wide applicability for the estimation of markovian process .",
    "this is reason why we adopted the definition of an exponential family by @xcite .",
    "firstly , we show that , for an exponential family of transition matrices in the sense of @xcite , an estimator of a simple form asymptotically attains the cramr - rao bound , which is given as the inverse of fisher information matrix .",
    "that is , the estimator for the expectation parameter is asymptotically efficient and is written as the sample mean of @xmath0-observations .",
    "since it requires only a small amount of calculation , the problem ( 1 ) is resolved .",
    "additionally , the problem ( 2 ) is also resolved for an exponential family of transition matrices because fisher information matrix is computable .    to show the above items",
    ", we discuss the behavior of the sample mean of @xmath0 observations . indeed , while the existing papers @xcite derived the form of the asymptotic variance , this paper shows that the asymptotic variance can be written by using the second derivative of the potential function of the generated exponential family .",
    "using this relation , we show that the sample mean asymptotically attains the cramr - rao bound for the expectation parameter .",
    "next , we define the fisher information matrix for a curved exponential family with a computable form .",
    "then , using a transition matrix version of the pythagorean theorem , we give an asymptotically efficient estimator for a curved exponential family , in which , the estimator is given as a function of the above estimator in the larger exponential family .",
    "since the asymptotic mean square error is the inverse of the fisher information matrix , the problems ( 1 ) and ( 2 ) are resolved jointly . in the above way",
    ", we resolve the problems that were unsolved in existing papers @xcite .",
    "further , during this derivation , we also obtain a notable evaluation for variance of sample mean as a by product , which is summarized in subsection [ s2 - 1 ] .    for the above discussion",
    ", we need the description of an exponential family of transition matrices . since the information geometrical structure for probability distributions plays important roles in several topics in information theory as well as statistics , it is better to describe the information geometry of transition matrices so that it can be easily applied to these topics .",
    "in fact , the authors applied it to finite - length evaluations of the tail probability , the error probability in simple hypothesis testing , source coding , channel coding , and random number generation in markov chain as well as the estimation error of parametric family of transition matrices @xcite .",
    "thus , we revisit the exponential family of transition matrices @xcite in a manner consistent with the above purpose by using bregmann divergence @xcite .",
    "in particular , the relative rnyi entropy for transition matrices plays an important role in the finite - length analysis ; we define the relative entropy for transition matrices so that it is a special case of the relative rnyi entropy , which is different from the definitions in the literatures @xcite .",
    "although some of results in this paper have been already stated in @xcite ( without detailed proof ) , we restate those results and give proofs since the logical order of arguments are different from @xcite and we want to keep the paper self - contained .",
    "in particular , although the paper @xcite is written with differential geometrical terminologies , e.g. , christoffel symbols , this paper is written only with terminologies of convex functions and linear algebra .",
    "the remaining of this paper is organized as follows .",
    "section [ s2 ] gives the brief summary of obtained results , which is crucial for understanding the structure of this paper . in section [ s4 ]",
    ", we define the relative entropy and the relative rnyi entropy between two transition matrices in section [ s4 - 5 ] , we revisit an exponential family of transition matrices and its properties . in section [ s5 ] ,",
    "we focus on the joint distribution when a transition matrix is given as an element of a one - parameter exponential family and the input distribution is given as the stationary distribution .",
    "then , we characterize the quantities given in sections [ s4 ] and [ s4 - 5 ] by using the joint distribution . in section [ s6 ] ,",
    "we proceed to the @xmath0 observation markov process when the initial distribution is the stationary distribution .",
    "then , we show that the sample mean of the generator is an unbiased and asymptotically efficient estimator under a one - parameter exponential family . in section [ s7 ] ,",
    "we proceed to the @xmath0 observation markov process when the initial distribution is a non - stationary distribution .",
    "we show a similar fact in this case .",
    "section [ s8 ] extends a part of these results to the multi - parameter case and the case of a curved exponential family . in appendix",
    ", we address the relations with existing results by nakagawa and kanaya @xcite , nagaoka@xcite , and natarajan @xcite .",
    "here , we prepare notations and definitions . for two given transition matrices @xmath1 and @xmath2 over @xmath3 and @xmath4 , we define @xmath5 , @xmath6 , and @xmath7 . for a given distribution @xmath8 on @xmath3 and a transition matrix @xmath9 from @xmath3 to @xmath4 ,",
    "we define @xmath10 and @xmath11 .",
    "a non - negative matrix @xmath1 is called _ irreducible _ when for each @xmath12 , there exists a natural number @xmath13 such that @xmath14 @xcite . an irreducible matrix @xmath1 is called _ ergodic _ when there are no input @xmath15 and no integer @xmath16 such that @xmath17 unless @xmath13 is divisible by @xmath16 @xcite .",
    "the irreducibility and the ergodicity depend only on the support @xmath18 for a non - negative matrix @xmath1 over @xmath3 .",
    "hence , we say that @xmath19 is _ irreducible _ and _ ergodic _ when a non - negative matrix @xmath1 is irreducible and ergodic , respectively . indeed ,",
    "when a subset of @xmath19 is irreducible and ergodic , the set @xmath19 is also irreducible and ergodic , respectively .",
    "it is known that the output distribution @xmath20 converges to the stationary distribution of @xmath1 for a given ergodic transition matrix @xmath1 @xcite .",
    "although the main result is asymptotic estimation for an exponential family and a curved exponential family , we also have additional results as subsections [ s2 - 1 ] and [ s2 - 3 ] .",
    "assume that the random variable @xmath21 obeys the markov process with the irreducible and ergodic transition matrix @xmath22 . in this paper , for an arbitrary two - input function @xmath23",
    ", we focus on the sample mean @xmath24 where @xmath25 , and @xmath26 .",
    "this is because a two - input function @xmath23 is closely related to an exponential family of transition matrices .",
    "indeed , the simple sample mean can be treated in this formulation by choosing @xmath23 as @xmath27 or @xmath15 .",
    "since the function @xmath23 can be chosen arbitrary , the following discussion can handle the sample mean of the hidden markov process .",
    "then , the expectation @xmath28 $ ] and the variance @xmath29 $ ] are characterized as follows .",
    "we denote the normalized perron - frobenius eigenvector of @xmath22 by @xmath30 and define the limiting expectation @xmath31:= \\sum_{x , x'}g(x , x')w(x|x ' ) p_w(x')$ ] .",
    "we denote the perron - frobenius eigenvalue of @xmath32 by @xmath33 and define the cumulant generating function @xmath34 .",
    "then , when the transition matrix @xmath1 is irreducible and ergodic , the relation @xmath35 & \\to \\mathsf{e}[g(x , x ' ) ] \\end{aligned}\\ ] ] is known . in sections [ s6 ] and [ s7 ] of this paper , we show @xmath36 & \\to \\frac{d^2 \\phi}{d \\theta^2}(0)\\end{aligned}\\ ] ] while existing papers @xcite characterized the asymptotic variance by using the fundamental matrix .",
    "( see ( * ? ? ?",
    "* section 6 ) . )    in particular , when the initial distribution is the stationary distribution @xmath30 , we have @xmath28=   \\mathsf{e}[g(x , x')]$ ] . then , in section [ s6 ] , using a constant @xmath37 , we show that @xmath38 \\le    \\frac{d^2 \\phi}{d \\theta^2}(0 ) ( 1+\\frac{c}{\\sqrt{n}})^2\\end{aligned}\\ ] ] for the stationary case . the concrete form of @xmath37 is also given in section [ s6 ] .",
    "this analysis is obtained via evaluations of fisher information given in sections [ s5 ] , [ s6 ] , and [ s7 ] .",
    "firstly , for simplicity , we summarize our obtained results for the one - parameter case while this paper addresses a multi - parameter exponential family . in section [ s4 - 5 ] , for a given two - input function @xmath23 and an irreducible and ergodic transition matrix @xmath1 , we define the potential function @xmath39 and exponential family of transition matrices @xmath40 with the generator @xmath23 .",
    "we also define its fisher information matrix @xmath41 and the expectation parameter @xmath42 . then",
    ", we focus on the distribution family of markov chains generated by the family of transition matrices @xmath43 with arbitrary initial distributions .",
    "we show that the fisher information of the expectation parameter under the distribution family is asymptotically equal to @xmath44 even for the non - stationary case in section [ s7 ] .",
    "then , we show that the random variable @xmath45 is the asymptotically efficient estimator , i.e. , the mean square error is @xmath46 . in section [ s6 ] ,",
    "we give more detailed analysis for the stationary case .",
    "to derive the results in sections [ s6 ] and [ s7 ] , we prepare evaluations of fisher information in section [ s5 ] .",
    "now , we address the multi - parameter case . in section [ s4 - 5 ] , we also define a multi - parameter exponential family @xmath47 of transition matrices , and show the pythagorean theorem .",
    "then , we show the asymptotic efficiency of the sample mean in the multi - parameter case in subsections [ s8 - 1 ] and [ s8 - 2 ] .",
    "we also show that the set of all positive transition matrices on a finite - size system forms an exponential family in example [ 1 - 3 - 8 ] .",
    "further , we define a curved exponential family of transition matrices , and give its asymptotically efficient estimator in subsection [ s8 - 3 ] .",
    "since any smooth parametric family of transition matrices on a finite - size system forms a curved exponential family , this result has a wide applicability .",
    "these results require the technical preparations given in sections [ s4 ] , [ s4 - 5 ] , and [ s5 ] .      in this paper , given two transition matrices @xmath1 and @xmath9 , we define the relative entropy @xmath48 and the relative rnyi entropy @xmath49 in section [ s4 ] . in subsection",
    "[ s8 - 3 ] , the relative entropy @xmath48 plays a crucial role in our estimator in a curved exponential family .",
    "we also show that the fisher information is given as the limits of the relative entropy and the relative rnyi entropy , which plays important roles in the proof of the asymptotic efficiency of our estimator in a curved exponential family in subsection [ s8 - 3 ] .",
    "also , as discussed in @xcite , the relative rnyi entropy @xmath49 plays a central role in simple hypothesis testing as well as the relative entropy @xmath48 .",
    "further , these information quantities play an central role in random number generation , data compression , and channel coding @xcite . in section [ s4 ] , we also give their properties that are useful in the above applications .",
    "for these applications , we need to address the relative entropy @xmath48 and the relative rnyi entropy @xmath49 in a unified way .",
    "more precisely , the relative entropy @xmath48 is needed to be defined as the limit of the relative rnyi entropy @xmath49 .",
    "indeed , the existing paper @xcite defined the relative entropy @xmath48 in a different way .",
    "however , the definition by @xcite can not yield the definition of the relative rnyi entropy in a unified way .",
    "appendix [ as1 ] summarizes the detailed relation between the results in this part and existing results .",
    "in this section , in order to investigate geometric structure for transition matrices , we define the relative entropy and the relative rnyi entropy . for this purpose",
    "we prepare the following lemma , which is shown after lemma [ l11 ] .",
    "consider an irreducible transition matrix @xmath1 over @xmath3 and a real - valued function @xmath50 on @xmath51 .",
    "define @xmath39 as the logarithm of the perron - frobenius eigenvalue of the matrix : @xmath52 then , the function @xmath39 is convex .",
    "further , the following conditions are equivalent .    * no real - valued function @xmath53 on @xmath3 satisfies that @xmath54 for any @xmath55 with a constant @xmath56 . *",
    "the function @xmath39 is strictly convex , i.e. , @xmath57 for any @xmath58 .",
    "* @xmath59 .    using lemma [ l1 ] ,",
    "given two distinct transition matrices @xmath1 and @xmath9 , we define the relative entropy @xmath60 and the relative rnyi entropy @xmath61 as follows . for this purpose , we denote the logarithm of the perron - frobenius eigenvalue of the matrix @xmath62 by @xmath63 under the condition given below . when @xmath64 and @xmath19 is irreducible , we define @xmath65 for @xmath66 .",
    "the relative rnyi entropy @xmath61 with @xmath67 is defined by ( [ 1 - 10 ] ) when @xmath68 is irreducible , which is a weaker assumption .",
    "when @xmath68 is irreducible and the condition @xmath64 does not hold , the relative entropy @xmath60 and the relative rnyi entropy @xmath61 with @xmath66 are regarded as the infinity .",
    "note that the limit @xmath69 equals @xmath70 .",
    "when @xmath64 and @xmath19 is irreducible , the function @xmath71 satisfies the condition for the function @xmath50 in lemma [ l1 ] because @xmath1 and @xmath9 are distinct . hence , the function @xmath72 is strictly convex .",
    "so , the relative rnyi entropy @xmath61 is strictly monotone increasing with respect to @xmath73 .    from the property of perron - frobenius eigenvalue",
    ", we immediately obtain the following lemma .    given two transition matrices @xmath74 and @xmath75 ( @xmath2 and @xmath76 ) on @xmath77 ( @xmath78 ) , respectively , we have @xmath79 for @xmath80 .",
    "transition matrices @xmath81 , @xmath82 , and @xmath1 satisfy @xmath83 for @xmath84 .    can be directly shown from lemma [ l1 - 14 ] given latter .",
    "the proof of will be given after .",
    "in the following , we treat only irreducible transition matrices . hence , an irreducible transition matrix is simply called a transition matrix .",
    "we define an exponential family for transition matrices .",
    "we focus on a transition matrix @xmath22 from @xmath3 to @xmath3 .",
    "then , a set of real - valued functions @xmath85 on @xmath51 is called _ linearly independent _ under the transition matrix @xmath22 when any linear non - zero combination of @xmath85 satisfies the condition in lemma [ l1 ] . for @xmath86 and linearly independent functions @xmath85 , we define the matrix @xmath87 from @xmath3 to @xmath3 in the following way .",
    "@xmath88 using the perron - frobenius eigenvalue @xmath89 of @xmath90 , we define the potential function @xmath91 .",
    "note that , since the value @xmath92 generally depends on @xmath15 , we can not make a transition matrix by simply multiplying a constant with the matrix @xmath90 . to make a transition matrix from the matrix @xmath90 , we recall that a non - negative matrix @xmath9 from @xmath3 to @xmath3 is a transition matrix if and only if the vector @xmath93 is an eigenvector of the transpose @xmath94 . in order to resolve this problem , we focus on the structure of the matrix @xmath90 .",
    "we denote the perron - frobenius eigenvectors of @xmath90 and its transpose @xmath95 by @xmath96 and @xmath97 .",
    "then , similar to ( * ? ? ?",
    "* ( 16 ) ) ( * ? ? ?",
    "* ( 2 ) ) , we define the matrix @xmath87 as @xmath98 the matrix @xmath87 is a transition matrix because the vector @xmath93 is an eigenvector of the transpose @xmath99 .",
    "the stationary distribution of the given transition matrix @xmath100 is the perron - frobenius normalized eigenvector of the transition matrix @xmath100 , which is given as @xmath101 because @xmath102 in the following , we call the family of transition matrices @xmath103 an _ exponential family _ of transition matrices generated by @xmath1 with the generator @xmath104 .",
    "since the generator @xmath104 is linearly independent , due to lemma [ l1 ] , @xmath105 is strictly positive for an arbitrary non - zero vector @xmath106 .",
    "that is , the hesse matrix @xmath107=[\\frac{\\partial^2 \\phi}{\\partial \\theta^i\\partial \\theta^j}]_{i , j}$ ] is non - negative .    using the potential function @xmath39 , we discuss several concepts for transition matrices based on lemma [ l1 ] , formally .",
    "we call the parameter @xmath108 the _ natural parameter _ , and the parameter @xmath109 the _ expectation parameter_. for @xmath110 , we define @xmath111 as @xmath112 .    for a given transition matrix @xmath1 , we define a linear subspace @xmath113 of the space @xmath114 of all two - input functions as the set of functions @xmath115 . then , we obtain the following lemma .",
    "the following are equivalent for the generator @xmath85 and the transition matrix @xmath1 .    *",
    "the set of functions @xmath85 are linearly independent in the quotient space @xmath116 .",
    "* the map @xmath117 is one - to - one . *",
    "the hesse matrix @xmath107 $ ] is strictly positive for any @xmath118 , which implies the strict convexity of the potential function @xmath119 . *",
    "the hesse matrix @xmath107|_{\\vec{\\theta}=0}$ ] is strictly positive . *",
    "the parametrization @xmath120 is faithful for any @xmath118 .    applying lemma [ l1 ] to @xmath121 for an arbitrary non - zero vector @xmath122",
    ", we obtain the equivalence among ( 1 ) , ( 3 ) , and ( 4 ) . ( 3 ) @xmath123 ( 2 ) is trivial .",
    "now , we show ( 2 ) @xmath123 ( 1 ) by showing the contraposition . if ( 1 ) does not holds .",
    "there exists a non - zero vector @xmath122 such that @xmath124 .",
    "hence , we have @xmath125 .",
    "hence , ( 2 ) does not hold .",
    "now , we show ( 1 ) @xmath123 ( 5 ) by showing the contraposition . when @xmath126 , considering the logarithm , there exist a function @xmath53 and a constant @xmath127 such that @xmath128 for @xmath55 .",
    "now , we show ( 5 ) @xmath123 ( 1 ) by showing the contraposition .",
    "if a set of real - valued functions @xmath85 on @xmath51 is not linearly independent , there exist a function @xmath53 and a constant @xmath37 such that @xmath129 . in this case , choosing @xmath130 and @xmath131 , @xmath132 and @xmath133 are the perron - frobenius eigenvector and eigenvalue of the transition matrix @xmath134 .",
    "then , we have @xmath126 .",
    "now , we introduce the notation @xmath135 is a transition matrix and @xmath136 .",
    "any element @xmath137 can be written as @xmath138 by using an element @xmath139 because of @xmath140 .",
    "hence , if and only if the set of two - input functions @xmath85 form a basis of the quotient space @xmath116 , the set @xmath141 coincides with the exponential family generated by @xmath1 with the generator @xmath85 .",
    "this fact shows that @xmath141 is an exponential family .",
    "in particular , when @xmath1 is a positive transition matrix , the subspace @xmath113 does not depend on @xmath1 and is abbreviated to @xmath142 . in this case",
    ", @xmath141 is the set of positive transition matrices .",
    "then , it does not depend on @xmath1 , and is abbreviated to @xmath143 .",
    "we define the fisher information matrix for the natural parameter by the hesse matrix @xmath107 : = [ \\frac{\\partial^2 \\phi}{\\partial \\theta^i\\partial \\theta^j}(\\vec{\\theta})]_{i , j}$ ] .",
    "the fisher information matrix for the expectation parameter is given as @xmath107^{-1}$ ] .",
    "further , for fixed values @xmath144 , we call the subset @xmath145 an exponential subfamily of @xmath146 .",
    "the following are examples of an exponential family .",
    "now , we assume that @xmath147 and @xmath1 is a positive transition matrix , i.e. , @xmath148 .",
    "define @xmath149 for @xmath150 and @xmath151 .",
    "then , the @xmath152 functions @xmath153 form a basis of the quotient space @xmath154 .",
    "therefore , the set of positive transition matrices forms an exponential family with the above choice of @xmath153 .    for a given subset @xmath155 for @xmath147",
    ", we choose a transition matrix @xmath1 whose support is @xmath156 . define the subset @xmath157 as @xmath158 is not minimum integer satisfying @xmath159 for a fixed @xmath160 .",
    "we define @xmath149 for @xmath161 .",
    "then , the set @xmath162 is an exponential family generated by @xmath163 .",
    "however , the set @xmath162 is not an exponential subfamily of the set of positive transition matrices because it is not included in the set of positive transition matrices .",
    "the above - defined exponential families contain exponential families of distributions as follows . for a given exponential family of distributions @xmath164 on @xmath3 with the generator @xmath165",
    ", we define the transition matrix @xmath22 as @xmath166 and the generator @xmath23 as @xmath165 .",
    "then , the exponential family @xmath167 is @xmath168 .",
    "the given potential function and the given expectation parameter ( defined in the next subsection ) are the same as those in the case with the exponential family of distributions @xmath169 .",
    "the papers @xcite called a family of transition matrices @xmath170 an exponential family when @xmath171 has the form @xmath172 the papers @xcite extended the above definition to the continuous - time case . however , our exponential family is written as @xcite @xmath173 by choosing @xmath174 and @xmath175 as @xmath176 and @xmath177 , respectively .",
    "so , the traditional definition ( [ 1 - 3 ] ) is different from ours .",
    "the advantage of our model over their model is explained in remark [ rem8 - 4 ] .      in the following ,",
    "we assume that the functions @xmath85 satisfies the condition of lemma [ l1 - 14 - 2 ] . for fixed values @xmath178 , we call the subset",
    "@xmath179 a _ mixture subfamily of @xmath146_. given a transition matrix @xmath1 , real - valued functions @xmath180 on @xmath181 , and real numbers @xmath182 , we say that the set @xmath183 is a _ mixture family on @xmath184 generated by the constraints @xmath185_. note that a mixture family on @xmath184 does not necessarily contain @xmath1 because its definition depends on the real numbers @xmath182 . when @xmath1 is a positive transition matrix , it is simply called a _ mixture family generated by the constraints @xmath185 _ because @xmath141 is the set of positive transition matrices . for a given transition matrix @xmath1 and two mixture families @xmath186 and @xmath187 on @xmath184 , the intersection @xmath188 is also a mixture family on @xmath184 .",
    "the intersection of the mixture family on @xmath184 generated by the constraints @xmath189 and the exponential family @xmath141 is the mixture subfamily @xmath190 of the exponential family @xmath141 .",
    "lemma [ l5 - 1 ] will be shown after lemma [ lemma : markov - derivative - expectation ] in section [ s5 ] . here",
    ", we give examples for mixture families .    a transition matrix @xmath1 on @xmath191",
    "is called _ non - hidden _ for @xmath77 when @xmath192 does not depend on @xmath193 . for a transition matrix @xmath1 on @xmath194 ,",
    "the set @xmath195 is non - hidden for @xmath77 on @xmath196 is a mixture family on @xmath197 .",
    "hence , the set @xmath198 is also a mixture family on @xmath184 .",
    "the set of bi - stochastic matrices on @xmath147 forms a mixture family as follows . for a permutation @xmath199",
    ", we define the transition matrix @xmath200",
    ". then , we focus on the set @xmath201 of transpositions @xmath202 and the subset @xmath203 of cyclic permutations with length @xmath204 defined by @xmath205 . then , @xmath206 . as will be shown in appendix [ as2 ] ,",
    "the set of bi - stochastic matrices on @xmath147 is parametrized as @xmath207 , where @xmath208 we define the functions @xmath209 as will be shown in appendix [ as2 ] , the set @xmath210 is linearly independent .",
    "then , the matrix @xmath211 given as follows is invertible : @xmath212 then , using the inverse matrix @xmath213 , we can define the functions @xmath214 as the dual basis in the following way : @xmath215 which implies that @xmath216 hence , the set of functions @xmath217 is linearly independent .",
    "we can employ the mixture parameter under the above set of functions .",
    "since the stationary distribution of @xmath218 is the uniform distribution and @xmath219 the transition matrix @xmath220 is the expectation parameter @xmath221 .",
    "that is , the set of bi - stochastic matrices on @xmath77 is the mixture family generated by the constraints @xmath222 .",
    "the relative entropy and the relative rnyi entropies are characterized by using the potential function @xmath119 as follows .",
    "two transition matrices @xmath100 and @xmath223 satisfies @xmath224    let @xmath63 be the logarithm of the perron - frobenius eigenvalue of the matrix @xmath225 .",
    "then , we have @xmath226 .",
    "hence , we obtain ( [ 1 - 2 ] ) . taking the limit @xmath227",
    ", we obtain ( [ 1 - 1 ] ) .",
    "the fisher information matrix @xmath107 $ ] can be characterized by the limits of the relative entropy and relative rnyi entropy as follows . that is , taking the limits in and in lemma [ l7 ]",
    ", we can show the following lemma .    for @xmath106 , we have @xmath228_{i , j}c^i c^j \\\\ \\label{27 - 21 } \\lim_{t \\to 0 } \\frac{2}{t^2}d_{1+s}({w}_{\\vec{\\theta } } \\| { w}_{\\vec{\\theta}+\\vec{c}t } ) = & \\lim_{t \\to 0 } \\frac{2}{t^2}d_{1+s}({w}_{\\vec{\\theta}+\\vec{c}t } \\| { w}_{\\vec{\\theta } } ) = ( 1+s)\\sum_{i , j}\\mathsf{h}_{\\vec{\\theta } } [ \\phi]_{i , j}c^i c^j .\\end{aligned}\\ ] ]    the right hand side of ( [ 1 - 1 ] )",
    "can be regarded as the bregmann divergence @xcite , which does not require christoffel symbols calculation .",
    "since the derivations by @xcite more directly explain the relation between the convex function @xmath119 and these properties , we refer the paper @xcite for these properties .",
    "] of the strictly convex function @xmath119 . in the following ,",
    "we derive several properties of the relative entropy by using bregmann divergence .",
    "that is , the following properties follow only from the strong convexity of @xmath119 and the properties of bregmann divergence .    using ( * ? ? ?",
    "* ( 40 ) ) , we have another expression of @xmath229 as @xmath230 where @xmath231 is defined as legendre transform of @xmath232 as @xmath233 since @xmath231 is convex as well as @xmath119 , we have the following lemma .",
    "\\(1 ) for a fixed @xmath118 , the maps @xmath234 and @xmath235 are convex for @xmath236 .",
    "( 2 ) for a fixed @xmath237 , the map @xmath238 is convex .      it is known that bregmann divergence satisfies the pythagorean theorem for ( * ? ? ?",
    "* ( 34 ) ) .",
    "applying this fact , we have the following proposition as the pythagorean theorem .",
    "( nagaoka ( * ? ? ?",
    "* ( 23 ) ) ) we focus on two points @xmath239 and @xmath240 .",
    "we choose the exponential subfamily of @xmath146 whose natural parameters @xmath241 are fixed to @xmath242 , and the mixture subfamily of @xmath146 whose expectation parameters @xmath243 are fixed to @xmath244 .",
    "let @xmath245 be the natural parameter of the intersection of these two subfamilies of @xmath146 .",
    "that is , @xmath246 for @xmath247 and @xmath248 for @xmath249 .",
    "then , we have @xmath250    indeed ,",
    "nagaoka @xcite showed ( [ 5 - 1 ] ) in a more general form by showing the dually flat structure @xcite via christoffel symbols calculation . using ( [ 5 - 1 ] ) and lemma [ l5 - 1 ]",
    ", we obtain the following corollary .    given a transition matrix @xmath9 and a mixture family @xmath251 on @xmath252 with constraints @xmath253",
    ", we define @xmath254 .",
    "\\(1 ) any transition matrix @xmath255 satisfies @xmath256 .",
    "\\(2 ) the transition matrix @xmath257 is the intersection of the mixture family @xmath251 on @xmath252 and the exponential family generated by @xmath9 and the generator @xmath258 .",
    "first , we notice that the exponential family @xmath259 contains @xmath9 and includes @xmath251 .",
    "choose an element @xmath260 in the intersection of the mixture family @xmath251 on @xmath252 and the exponential family @xmath261 generated by @xmath9 and the generator @xmath258 .",
    "we apply ( [ 5 - 1 ] ) to the mixture family @xmath251 and the exponential family @xmath261 .",
    "then , any transition matrix @xmath255 satisfies that @xmath262 . since @xmath263 except for @xmath264 , we have @xmath265 , which implies that @xmath266 , i.e. , ( 2 ) .",
    "hence , we obtain ( 1 ) .",
    "similarly , we have another version of the above corollary .",
    "given a transition matrix @xmath1 and an exponential family @xmath267 with the generator @xmath85 , we define @xmath268 .",
    "assume that @xmath269 .",
    "\\(1 ) any transition matrix @xmath270 satisfies @xmath271 .",
    "\\(2 ) the transition matrix @xmath272 is the intersection of the exponential family @xmath146 and the mixture family on @xmath184 with the constraints @xmath185 .",
    "we choose transition matrices @xmath75 and @xmath76 on @xmath77 and @xmath78 , respectively .",
    "we also choose a transition matrix @xmath1 on @xmath191 whose support is @xmath273 .",
    "when a set of two - input functions @xmath274 forms a basis of @xmath275 , the exponential family generated by @xmath276 with the generator @xmath274 is @xmath277 .",
    "when a set of two - input functions @xmath278 forms a basis of @xmath279 , the exponential family generated by @xmath276 with the generator @xmath280 is @xmath281 .",
    "hence , when a transition matrix @xmath1 belongs to a mixture family with the constraints @xmath282 , the intersection between the exponential family and the mixture family consists of one points , which is denoted by @xmath283 . applying ( [ 5 - 1 ] ) , we obtain @xmath284 in particular , when @xmath1 is non - hidden for @xmath77 ( for the definition , see example [ ex5 ] . ) , @xmath74 satisfies the same constraint @xmath285 because the stationary distribution @xmath286 is the marginal distribution of the stationary distribution @xmath287 .",
    "hence , @xmath288 .",
    "thus , @xmath289 can be regarded as a marginalization of a transition matrix @xmath1 that is not necessarily non - hidden .",
    "in the previous section , we formally defined several information quantities from the convex function @xmath119 in the multi - parameter case . in this section",
    ", we consider the relation with the structure of probabilities in the one - parameter case .",
    "that is , we will see how the information quantities reflect the conventional information quantities . for this purpose , we assume that the input distribution is the stationary distribution of the given transition matrix .    since the stationary distribution of the given transition matrix @xmath290 is @xmath291 given in , we can define the joint distribution @xmath292 on @xmath51 .",
    "now , we focus on the probability distribution family @xmath293 , and denote the expectation and the variance under the distribution @xmath294 by @xmath295 and @xmath296 . these are simplified to @xmath297 and @xmath298 when @xmath299 .",
    "( ( * ? ? ?",
    "* theorem 4 ) , ( * ? ? ?",
    "* ( 28 ) ) ) for @xmath300 , we have @xmath301   = \\sum_{x , x^\\prime } \\overline{p}^1_\\theta(x ) w_\\theta(x|x^\\prime ) g(x , x^\\prime).\\end{aligned}\\ ] ]    the lemma shows the reason why we call the parameter @xmath302 the expectation parameter .    from the definition of @xmath303 ,",
    "we have @xmath304 taking the average of the both hand sides with respect to the distribution @xmath294 , we have @xmath305    lemma [ lemma : markov - derivative - expectation ] shows lemma [ l5 - 1 ] as follows .",
    "lemma [ l5 - 1 ] in this proof , we consider the multi - parameter case .",
    "replacing the derivative by the partial derivative in lemma [ lemma : markov - derivative - expectation ] , we have @xmath306 choose the generator @xmath307 of the mixture family on @xmath184 .",
    "there exist two - input functions @xmath308 such that the set of two - input functions @xmath309 form a basis of @xmath116 .",
    "hence , due to ( [ 1 - 3 - 9 ] ) , we see that the intersection of the mixture family on @xmath184 generated by the constraints @xmath189 and the exponential family @xmath141 is the mixture subfamily @xmath190 of the exponential family @xmath141 .",
    "now , we introduce the conditional relative entropy for transition matrices @xmath1 and @xmath9 from @xmath3 to @xmath4 and a distribution @xmath8 on @xmath3 as follows .",
    "@xmath310 where the relative entropy between two distributions @xmath8 and @xmath311 is defined in the conventional way as @xmath312 hence , the relative entropy defined in the previous section is characterized as follows ( * ? ? ?",
    "* ( 24 ) ) .",
    "@xmath313 where @xmath314 follows from the fact that @xmath315 .    since the map @xmath316 is convex for a given @xmath58 , guarantees .      using the fisher information @xmath317 of the family @xmath318 of stationary distributions ,",
    "we discuss the fisher information @xmath319 of the family @xmath320 of joint distributions in the following lemma .",
    "the fisher information @xmath319 can be written as @xmath321    the second derivative @xmath322 is calculated as @xmath323.\\label{27 - 11}\\end{aligned}\\ ] ] in particular , when @xmath299 , @xmath324 + 2 \\sum_{x , x ' } w(x|x ' ) g(x , x ' ) \\frac{d \\overline{p}^2_{\\theta}(x')}{d \\theta}\\bigr|_{\\theta=0}. \\label{27 - 12}\\end{aligned}\\ ] ]    proofs of lemmas [ l11 ] and [ l11 - 2 ] are given in appendix [ as3 ] .",
    "further , the quantity @xmath325 has another form ( * ? ? ?",
    "* theorem 6.6 ) .",
    "using lemma [ l11 - 2 ] , we can show lemma [ l1 ] as follows .",
    "lemma [ l1 ] due to ( [ 27 - 11 ] ) , the non - negativity of variance implies that @xmath39 is convex .",
    "since condition ( 2 ) trivially implies condition ( 3 ) , it is enough to show that condition ( 1 ) implies condition ( 2 ) and condition ( 3 ) implies condition ( 1 ) .",
    "assume condition ( 1 ) .",
    "then , the random variable @xmath326 is not a constant on @xmath19 .",
    "hence , the variance in ( [ 27 - 11 ] ) is strictly greater than zero , which implies condition ( 2 ) .",
    "conversely , we assume that condition ( 1 ) does not hold , i.e. , @xmath327 for any @xmath55 with a constant @xmath328 .",
    "then , we can find that the perron - frobenius eigenvalue of @xmath329 is @xmath330 and its right eigenvector is @xmath331 .",
    "thus , we have @xmath332 , i.e. , condition ( 3 ) does not hold .",
    "hence , condition ( 3 ) implies condition ( 1 ) .",
    "similar to the previous section , this section also discusses the one - parameter case with the stationary initial distribution @xmath291 .",
    "now , we consider the distribution @xmath333 on @xmath334 , which is defined as @xmath335 we also define the random variable @xmath336 for @xmath337 . in this section , we denote the expectation and the variance under the distribution @xmath338 by @xmath295 and @xmath296 .",
    "then , the cumulant generating function @xmath339 $ ] satisfies @xmath340 = n \\eta(\\theta ) .",
    "\\label{25 - 7}\\end{aligned}\\ ] ]    now , we calculate information quantities .",
    "similar to lemma [ l11 ] , the fisher information can be calculated as follows .",
    "the fisher information @xmath341 of the family @xmath342 can be written as @xmath343    the proof can be done in the same way as lemma [ l11 ] .",
    "the conditional relative entropy is characterized by the bregman divergence defined by the convex function @xmath39 as follows .",
    "@xmath344      the relation implies that @xmath345 is an unbiased estimator for the parameter @xmath302 .",
    "the variance of @xmath346 is evaluated as follows .",
    "the inequalities @xmath347   \\le n \\frac{d^2 \\phi}{d\\theta^2 } ( \\theta ) ( 1 + 2\\sqrt{\\frac{\\hat{\\mathsf{v}}_\\theta } { n \\frac{d^2 \\phi}{d\\theta^2 } ( \\theta)}})^2 \\label{25 - 5}\\end{aligned}\\ ] ] hold , where @xmath348 = \\sum_{x } \\overline{p}^1_{\\theta}(x ) ( \\frac{d}{d\\theta } \\log \\overline{p}^3_{\\theta}(x))^2 $ ] .    hence , we obtain @xmath349   = \\frac{\\mathsf{v}_\\theta [ g^n(x^{n+1 } ) ] } { n^2 } = \\frac{\\frac{d^2\\phi}{d\\theta^2 } ( \\theta)}{n } + o(\\frac{1}{n\\sqrt{n } } ) .",
    "\\label{25 - 8}\\end{aligned}\\ ] ]    the fisher information @xmath350 for the expectation parameter @xmath302 of the family @xmath351 is @xmath352 that is , the lower bound of the variance of the unbiased estimator given by cramr - rao inequality is @xmath353 .",
    "hence , any unbiased estimator @xmath354 for the expectation parameter @xmath302 satisfies @xmath355   \\ge \\frac{\\frac{d^2\\phi}{d\\theta^2 } ( \\theta ) } { n(1 + \\frac{j_{\\theta}^1}{n\\frac{d^2\\phi}{d\\theta^2 } ( \\theta ) } ) } = \\frac{\\frac{d^2\\phi}{d\\theta^2 } ( \\theta)}{n } -\\frac{j_{\\theta}^1}{n^2}+ o(\\frac{1}{n^2 } ) \\label{25 - 6f}.\\end{aligned}\\ ] ] the relation ( [ 25 - 8 ] ) shows that the unbiased estimator @xmath345 realizes the optimal performance with the order @xmath356 .",
    "lemma [ l20b ] the combination of ( [ 25 - 1 ] ) and ( [ 25 - 2 ] ) implies that @xmath357 is the variance of @xmath358 $ ] under the distribution @xmath359 in the two - observation case . in the @xmath0-observation case , using lemma [ l11 - 11 ] , we can similarly show that @xmath360 is the variance of @xmath361 $ ] under the distribution @xmath362 .",
    "now , we define the @xmath363-norm of the random variable @xmath364 as @xmath365",
    ". then , we have @xmath366 } + 2\\sqrt{\\hat{\\mathsf{v}}_\\theta},\\end{aligned}\\ ] ] which implies @xmath367 $ ] .",
    "then , we obtain the first inequality because @xmath368 .",
    "similarly , since @xmath369 , we obtain the second inequality because @xmath370 .",
    "similar to the previous section , this section also discusses the one - parameter case .",
    "now , we consider the non - stationary case . since the convergence to the stationary distribution is required , we assume that the transition matrices @xmath371 are ergodic as well as irreducible .",
    "then , we fix an arbitrary initial distributions @xmath164 on @xmath3 such that the @xmath164 is distribution is smoothly parameterized by the parameter @xmath58 . in this section",
    ", we assume that @xmath303 is the exponential family generated by the generator @xmath23 and the random variable @xmath26 is subject to @xmath372 with the unknown parameter @xmath58 .",
    "then , we denote the expectation and the variance under the distribution @xmath372 by @xmath295 and @xmath296 . in this general case",
    ", the relation ( [ 25 - 6 ] ) does not hold . in stead of these relations , as is shown in ( * ? ? ?",
    "* lemma 5.4 ) , we have @xmath373    for a function @xmath374 on @xmath375 , we define the random variable @xmath376 .",
    "when we use the random variable @xmath377 as an estimator of the parameter @xmath378 , the error is measured by the mean square error : @xmath379:= \\mathsf{e}_\\theta [ ( \\frac{\\tilde{g}^n ( x^{n+1})}{n}-\\eta(\\theta))^2 ] .\\end{aligned}\\ ] ] then , we have @xmath380= \\mathsf{e}_\\theta [ g^n(x^{n+1})]+ \\mathsf{e}_\\theta [ h(x_1)]$ ] . in the following discussion , we employ the norm @xmath381}$ ] for a function @xmath53 on @xmath382 . using the triangle inequality for this norm",
    ", we have @xmath383}-\\sqrt{\\mathsf{v}_\\theta [ h(x_1 ) ] } \\le   \\sqrt{\\mathsf{v}_\\theta[\\tilde{g}^n(x^{n+1 } ) ] } \\nonumber \\\\ \\le & \\sqrt{\\mathsf{v}_\\theta [ g^n(x^{n+1})]}+\\sqrt{\\mathsf{v}_\\theta [ h(x_1 ) ] } , \\label{11 - 11 - 2 } \\\\ & \\sqrt{\\mathsf{e}_\\theta [ ( \\frac{g^n ( x^{n+1})}{n } - \\mathsf{e}_\\theta [ \\frac{g^n ( x^{n+1})}{n}])^2 ] } - \\sqrt{\\mathsf{e}_\\theta [ ( \\frac{h ( x_1)}{n}+   \\mathsf{e}_\\theta [ \\frac{g^n ( x^{n+1})}{n } ] -\\eta(\\theta ) ) ^2 ] } \\nonumber \\\\ \\le & \\sqrt{\\mathsf{e}_\\theta [ ( \\frac{\\tilde{g}^n ( x^{n+1})}{n}-\\eta(\\theta))^2 ] } \\nonumber \\\\ \\le & \\sqrt{\\mathsf{e}_\\theta [ ( \\frac{g^n ( x^{n+1})}{n } - \\mathsf{e}_\\theta [ \\frac{g^n ( x^{n+1})}{n}])^2 ] } + \\sqrt{\\mathsf{e}_\\theta [ ( \\frac{h ( x_1)}{n}+   \\mathsf{e}_\\theta [ \\frac{g^n ( x^{n+1})}{n } ] -\\eta(\\theta ) ) ^2]}. \\label{11 - 11 - 3}\\end{aligned}\\ ] ] it is known that the expectation of @xmath346 and the variance of @xmath384 converge to those under the stationary distribution @xcite . hence , due to ( [ 25 - 7 ] ) and ( [ 25 - 8 ] ) , we have @xmath385   = \\lim_{n\\to \\infty } \\mathsf{e}_\\theta [ \\frac{g^n(x^{n+1})}{n } ]   = \\eta(\\theta ) = \\frac { d\\phi}{d \\theta}(\\theta ) , \\label{27 - 15}\\\\ & \\lim_{n\\to \\infty } n \\mathsf{mse}_\\theta [ \\frac{\\tilde{g}^n ( x^{n+1})}{n } ] \\stackrel{(a)}{= }    \\lim_{n\\to \\infty } \\mathsf{v}_\\theta [ \\frac{\\tilde{g}^n(x^{n+1})}{\\sqrt{n } } ]   \\nonumber   \\\\ \\stackrel{(b)}{= }    & \\lim_{n\\to \\infty } \\mathsf{v}_\\theta [ \\frac{g^n(x^{n+1})}{\\sqrt{n } } ]   =   \\frac{d^2\\phi}{d\\theta^2 } ( \\theta ) , \\label{27 - 15b}\\end{aligned}\\ ] ] where @xmath314 and @xmath386 follow from and , respectively .",
    "the relation ( [ 27 - 15 ] ) shows that the estimator @xmath387 is asymptotically unbiased for the parameter @xmath302 .",
    "the mean square error is @xmath388 , which implies ( [ 1 - 15 - 1 ] ) .",
    "further , it is shown that the random variable @xmath389 asymptotically obeys the gaussian distribution with the variance @xmath390 at @xmath299 ( * ? ? ?",
    "* corollary 6.2 ) . replacing @xmath391 by @xmath371 ,",
    "we find that the random variable @xmath392 asymptotically obeys the gaussian distribution with the variance @xmath41 .    next ,",
    "for the family @xmath393 , we consider the fisher information @xmath394 for the natural parameter @xmath58 and the fisher information @xmath395 for the expectation parameter @xmath302 .",
    "the limit of the fisher information @xmath394 for the natural parameter @xmath58 is characterized as @xmath396 hence , the limit of the fisher information @xmath395 for the expectation parameter @xmath302 is characterized as @xmath397 .",
    "lemma [ l29 - 1 ] implies that the lower bound of the cramr - rao inequality is @xmath398 .",
    "therefore , the estimator @xmath387 attains the lower bound by the cramr - rao inequality with the order @xmath356 .",
    "that is , the estimator @xmath387 is asymptotically efficient .",
    "lemma [ l29 - 1 ] similar to ( [ 25 - 2 ] ) , we have @xmath399 + j_{\\theta}^1    \\\\ = &   \\| - n \\frac{d \\phi}{d \\theta}(\\theta ) + \\frac{d}{d\\theta } \\log \\overline{p}^3_{\\theta}(x_{n+1 } ) - \\frac{d}{d\\theta } \\log \\overline{p}^3_{\\theta}(x_1 ) + g^n(x^{n+1 } ) \\|_2 ^ 2 +   j_{\\theta}^1 .",
    "\\label{29 - 2}\\end{aligned}\\ ] ] since ( [ 27 - 15 ] ) and ( [ 27 - 15b ] ) yield that @xmath400 , we have @xmath401 the combination of ( [ 29 - 2 ] ) and ( [ 29 - 3 ] ) yields that @xmath402 .",
    "similarly , the opposite inequality can be shown by replacing the role of ( [ 29 - 3 ] ) by the following inequality .",
    "@xmath403 hence , we obtain ( [ 29 - 1 ] ) . since @xmath404 , ( [ 29 - 1 ] ) implies @xmath397 .",
    "assume that @xmath47 is a multi - parameter exponential family of transition matrices with @xmath86 with the generator @xmath85 .",
    "then , we assume that the initial distribution is the stationary distribution @xmath405 on @xmath3 of @xmath47 and the random variable @xmath26 is subject to @xmath406 with the unknown parameter @xmath407 . in this subsection",
    ", we denote the expectation and the variance under the distribution @xmath406 by @xmath408 and @xmath409 .",
    "similar to ( [ 25 - 7 ] ) , using @xmath410_{j}$ ] , we can show that @xmath411   = \\vec{\\eta}(\\vec{\\theta } ) , \\label{25 - 7c}\\end{aligned}\\ ] ] which implies that @xmath412 is an unbiased estimator of the expectation parameter @xmath413 .",
    "we denote the covariance matrix of @xmath412 by @xmath414 $ ] .",
    "we also denote the covariance matrix of @xmath415_j$ ] by @xmath416 .",
    "the matrix inequalities @xmath417 ( 1 - 2\\sqrt{\\frac{\\| \\mathsf{h}_{\\vec{\\theta } } [ \\phi]^{-\\frac{1}{2 } } \\hat{\\mathsf{cov}}_\\theta    \\mathsf{h}_{\\vec{\\theta } } [ \\phi]^{-\\frac{1}{2}}\\| } { n } } ) ^2 \\le \\mathsf{cov}_\\theta [ \\vec{g}^n(x^{n+1 } ) ] \\\\",
    "\\le & n \\mathsf{h}_{\\vec{\\theta } } [ \\phi ] ( 1 + 2\\sqrt{\\frac{\\| \\mathsf{h}_{\\vec{\\theta } } [ \\phi]^{-\\frac{1}{2 } } \\hat{\\mathsf{cov}}_\\theta    \\mathsf{h}_{\\vec{\\theta } } [ \\phi]^{-\\frac{1}{2}}\\| } { n } } ) ^2   \\label{25 - 5c}\\end{aligned}\\ ] ] hold , where the matrix inequality is defined by the positive semi - definiteness .",
    "first , we fix a real unit vector @xmath418_j$ ] . applying ( [ 25 - 5 ] ) to the random variable @xmath419",
    ", we obtain @xmath420 \\vec{a } ( 1 - 2\\sqrt{\\frac { \\vec{a}^t \\hat{\\mathsf{cov}}_\\theta   \\vec{a } } { n \\vec{a}^t \\mathsf{h}_{\\vec{\\theta } } [ \\phi]\\vec{a } } } ) ^2 \\le & \\vec{a}^t \\mathsf{cov}_\\theta [ \\vec{g}^n(x^{n+1})]\\vec{a } \\\\",
    "\\le & n \\vec{a}^t \\mathsf{h}_{\\vec{\\theta } } [ \\phi ] \\vec{a } ( 1 + 2\\sqrt{\\frac { \\vec{a}^t \\hat{\\mathsf{cov}}_\\theta   \\vec{a } } { n \\vec{a}^t \\mathsf{h}_{\\vec{\\theta } } [ \\phi]\\vec{a } } } ) ^2 .",
    "\\label{5 - 11}\\end{aligned}\\ ] ] since @xmath421\\vec{a } } \\le   \\| \\mathsf{h}_{\\vec{\\theta } } [ \\phi]^{-\\frac{1}{2 } } \\hat{\\mathsf{cov}}_\\theta    \\mathsf{h}_{\\vec{\\theta } } [ \\phi]^{-\\frac{1}{2}}\\|$ ] , ( [ 5 - 11 ] ) implies ( [ 25 - 5c ] ) .",
    "lemma [ l5 - 2 ] yields that @xmath422   = \\frac{\\mathsf{cov}_{\\vec{\\theta } } [ \\vec{g}^n(x^{n+1 } ) ] } { n^2 } = \\frac { \\mathsf{h}_{\\vec{\\theta}}[\\phi]}{n } + o(\\frac{1}{n } ) .",
    "\\label{25 - 8c}\\end{aligned}\\ ] ]    now , we denote the fisher information matrix of the distribution family @xmath423 by @xmath424 .",
    "the fisher information matrix @xmath425 for the expectation parameter @xmath426 of the distribution family @xmath427 is @xmath428_{i , j}^t)^{-1 } { j}^{n+1}_{\\vec{\\theta } }   ( [ \\frac{\\partial \\eta_i(\\vec{\\theta})}{\\partial \\theta_j}]_{i , j})^{-1 } = \\mathsf{h}_{\\vec{\\theta}}[\\phi]^{-1 } ( n   \\mathsf{h}_{\\vec{\\theta}}[\\phi ] + j_{\\vec{\\theta}}^1 ) \\mathsf{h}_{\\vec{\\theta}}[\\phi]^{-1 } \\\\ = & \\mathsf{h}_{\\vec{\\theta}}[\\phi]^{-\\frac{1}{2 } }",
    "( n   i+   \\mathsf{h}_{\\vec{\\theta}}[\\phi]^{-\\frac{1}{2 } } j_{\\vec{\\theta}}^1 \\mathsf{h}_{\\vec{\\theta}}[\\phi]^{-\\frac{1}{2 } } ) \\mathsf{h}_{\\vec{\\theta}}[\\phi]^{-\\frac{1}{2}}.\\end{aligned}\\ ] ] that is , the lower bound of the variance of the unbiased estimator given by cramr - rao inequality is @xmath429^{\\frac{1}{2 } } ( 1   i+ \\frac{1}{n } \\mathsf{h}_{\\vec{\\theta}}[\\phi]^{-\\frac{1}{2 } } j_{\\vec{\\theta}}^1 \\mathsf{h}_{\\vec{\\theta}}[\\phi]^{-\\frac{1}{2 } } ) ^{-1 } \\mathsf{h}_{\\vec{\\theta}}[\\phi]^{\\frac{1}{2}}$ ] , i.e. , the cramr - rao inequality is given as @xmath430   \\ge & \\frac{1}{n } \\mathsf{h}_{\\vec{\\theta}}[\\phi]^{\\frac{1}{2 } } ( i+ \\frac{1}{n } \\mathsf{h}_{\\vec{\\theta}}[\\phi]^{-\\frac{1}{2 } } j_{\\vec{\\theta}}^1 \\mathsf{h}_{\\vec{\\theta}}[\\phi]^{-\\frac{1}{2 } } ) ^{-1 } \\mathsf{h}_{\\vec{\\theta}}[\\phi]^{\\frac{1}{2 } } \\\\",
    "= & \\frac{1}{n } \\mathsf{h}_{\\vec{\\theta}}[\\phi]+ o(\\frac{1}{n^2 } ) .",
    "\\label{25 - 6c}\\end{aligned}\\ ] ] the relation ( [ 25 - 8c ] ) shows that the unbiased estimator @xmath431 realizes the optimal performance with the order @xmath356 .",
    "therefore , we obtain an asymptotically efficient estimator for the expectation parameter . to estimate the natural parameter , we need to solve the equation @xmath432 for @xmath118 .",
    "since the function @xmath119 is strictly convex , @xmath433 can be derived by the maximization of the concave function as @xmath434 the calculation complexity does not depend on the number @xmath13 of data .",
    "hence , when the number @xmath435 of parameters is not so large , the natural parameter can be estimated efficiently even with a large number @xmath13 of data .",
    "however , the conventional algorithm for the maximization of the concave function @xcite requires the calculation of the derivative .",
    "since the convex function @xmath119 is given as the logarithm of the perron - frobenius eigenvalue of the matrix @xmath436 , the calculation of the derivative is not so easy . to overcome this kind of difficulty , we can employ derivative - free optimization algorithms @xcite represented by nelder - mead method @xcite .",
    "a derivative - free optimization algorithm maximizes a concave function without calculating the derivative only with calculating the outcomes with several inputs .",
    "in particular , it is expected that such an algorithm enables us to numerically derive @xmath433 for a given @xmath426 .",
    "next , similar to section [ s7 ] , we consider the non - stationary case and assume that the transition matrices @xmath47 are ergodic as well as irreducible .",
    "then , we fix an arbitrary initial distributions @xmath437 on @xmath3 such that the distribution @xmath437 is smoothly parameterized by the natural parameter @xmath118 .",
    "this assumption contains the special case when the distribution @xmath437 does not depend on the parameter @xmath118 .    in this subsection",
    ", we denote the expectation , the variance , and the covariance matrix under the distribution @xmath438 by @xmath408 , @xmath409 , and @xmath439 .",
    "then , we employ the random variable @xmath440 .",
    "when we use the random variable @xmath441 as an estimator of the parameter @xmath118 , the error is measured by the mean square error matrix : @xmath442_{i , j}:= \\mathsf{e}_\\theta [ ( \\frac{{g}_i^n ( x^{n+1})}{n}-\\eta_i(\\vec{\\theta } ) ) ( \\frac{{g}_j^n ( x^{n+1})}{n}-\\eta_j(\\vec{\\theta } ) ) ] .\\end{aligned}\\ ] ] similar to ( [ 27 - 15 ] ) , we can show that @xmath443   = & \\vec{\\eta}(\\vec{\\theta } ) = [ \\frac { \\partial \\phi}{\\partial   \\theta^j}(\\vec{\\theta})]_j   \\label{27 - 15c}.\\end{aligned}\\ ] ] for any vector @xmath444 , the application of ( [ 27 - 15b ] ) to @xmath445 implies that @xmath446 \\vec{c } =   \\lim_{n\\to \\infty }   n   \\vec{c}^t \\mathsf{cov}_{\\vec{\\theta } } [ \\frac{\\vec{g}^n(x^{n+1})}{n } ]   \\vec{c } =   \\vec{c}^t \\mathsf{h}_{\\vec{\\theta } } [ \\phi ] \\vec{c},\\end{aligned}\\ ] ] which implies the following theorem .",
    "@xmath447 =   \\lim_{n\\to \\infty }   n   \\mathsf{cov}_{\\vec{\\theta } } [ \\frac{\\vec{g}^n(x^{n+1})}{n } ]   =   \\mathsf{h}_{\\vec{\\theta } } [ \\phi ] .",
    "\\label{27 - 15bc}\\end{aligned}\\ ] ]    the relation ( [ 27 - 15c ] ) shows that the estimator @xmath431 is asymptotically unbiased for the expectation parameter @xmath426 .",
    "the above theorem implies that the mean square error is @xmath448   + o(\\frac{1}{n})$ ] .",
    "next , for the family @xmath449 , we consider the fisher information matrix @xmath450 for the natural parameter @xmath118 and the fisher information matrix @xmath451 for the expectation parameter @xmath426 .",
    "the limit of the fisher information matrix @xmath450 for the natural parameter @xmath118 is characterized as @xmath452 $ ] .",
    "hence , the limit of the fisher information matrix @xmath451 for the expectation parameter @xmath426 is characterized as @xmath453^{-1}$ ] .",
    "we fix a real unit vector @xmath418_j$ ] .",
    "the application of the relation ( [ 29 - 1 ] ) to @xmath445 yields that @xmath454 \\vec{a}$ ] , which implies @xmath452 $ ] . since @xmath455_{i , j}$ ] is @xmath107 $ ] , we obtain @xmath453^{-1}$ ] .",
    "lemma [ l29 - 1c ] implies that the lower bound of the cramr - rao inequality is @xmath456 + o(\\frac{1}{n})$ ] .",
    "therefore , the estimator @xmath431 attains the lower bound by the cramr - rao inequality with the order @xmath356 .",
    "that is , the estimator @xmath431 is asymptotically efficient .",
    "similar to the one - parameter case , we can show that the random variable @xmath457 converges to the gaussian distribution with the covariance matrix @xmath458 $ ] .",
    "next , we proceed to estimation with multi - parameter curved exponential family . a @xmath459-parameter subset @xmath460 of an exponential family @xmath461 of transition matrices is called a _ curved exponential family _ of transition matrices .",
    "for example , a mixture family defined in subsection [ s4 - 5 - 2 ] is also a curved exponential family . as explained in example [ 1 - 3 - 8 ] , the set of all positive transition matrices on a finite - size system forms an exponential family .",
    "hence , any smooth subfamily of transition matrices on a finite - size system forms a curved exponential family .",
    "then , we define the fisher information matrix @xmath462 as the metric of the submanifold .",
    "assume that the jacobian matrix @xmath463_{i , j}$ ] has the rank @xmath459 .",
    "when the potential function of the exponential family is @xmath464 , the fisher information matrix is written as @xmath465^{-1 } a$ ] because the fisher information matrix for the expectation parameter @xmath302 at @xmath466 is @xmath467 .    in the following ,",
    "we assume that the exponential family @xmath146 is generated by @xmath468 .",
    "given @xmath0 observations @xmath469 , as fig .",
    "[ f1 ] , we define the estimator @xmath470 for the curved exponential family @xmath471 .",
    "then , similar to the case of a curved exponential family of probability distributions ( * ? ? ?",
    "* section 4.4 ) , we can show that the estimator @xmath472 is asymptotically efficient .",
    "that is , the mean square error matrix is asymptotically approximated to @xmath473^{-1}+ o(\\frac{1}{n})$ ] as follows .",
    "the random variable @xmath474 asymptotically obeys the gaussian distribution with the covariance matrix @xmath475^{-1}$ ] .",
    "then , the mean square error matrix of our estimator @xmath472 is asymptotically approximated to @xmath473^{-1}+ o(\\frac{1}{n})$ ] .",
    "the random variable @xmath476 asymptotically obeys the gaussian distribution with the covariance matrix @xmath477 $ ] , where @xmath478 .",
    "since the neighborhood of @xmath472 in @xmath471 can be approximated to the tangent space at the true point @xmath479 , due to corollary [ c15 - 1 ] , the point @xmath480 can be approximately regarded as the projection to the tangent space at @xmath479 from the observed point @xmath481 .    to see the asymptotic variance of the random variable @xmath474",
    ", we choose a @xmath482 matrix @xmath483 and a @xmath484 matrix @xmath485 such that the @xmath486 matrix @xmath487 satisfies that @xmath488^{-1 } b = i \\quad   \\hbox { and } \\quad b_2^t",
    "\\mathsf{h}_{\\vec{\\theta}_{\\rm crv}(\\vec{\\xi}_o)}[\\phi]^{-1 } a = 0 . \\label{11 - 18 - 1 } \\end{aligned}\\ ] ] then , @xmath489^{-1 } a$ ] is invertible .",
    "so , @xmath490^{-1 } a = $ ]    @xmath491^{-1 } a)^t",
    "b_1^t \\mathsf{h}_{\\vec{\\theta}_{\\rm crv}(\\vec{\\xi}_o)}[\\phi]^{-1 } b_1 ( b_1^t \\mathsf{h}_{\\vec{\\theta}_{\\rm crv}(\\vec{\\xi}_o)}[\\phi]^{-1 } a ) $ ]    @xmath492^{-1 } a)^t ( b_1^t \\mathsf{h}_{\\vec{\\theta}_{\\rm crv}(\\vec{\\xi}_o)}[\\phi]^{-1 } a)$ ] .",
    "now , we introduce the new parameter @xmath493 under which , the metric is given as cartesian inner product .",
    "hence , the covariance matrix of the estimator @xmath494 for the parameter @xmath495 is the matrix @xmath496 .",
    "we denote the vector @xmath497 by @xmath498 . since the parameter @xmath499 is approximately identified with the element of the tangent space",
    ", we have @xmath500 .",
    "hence , implies that @xmath501^{-1 } a ( \\vec{\\xi}-\\vec{\\xi}_o )   = b_1^t   \\mathsf{h}_{\\vec{\\theta}_{\\rm crv}(\\vec{\\xi}_o)}[\\phi]^{-1 } b   ( \\vec{\\tau}(\\vec{\\eta}(\\vec{\\xi}))-\\vec{\\tau}(\\vec{\\eta}_o ) ) \\\\ = & b_1^t   \\mathsf{h}_{\\vec{\\theta}_{\\rm crv}(\\vec{\\xi}_o)}[\\phi]^{-1 }   b_1   ( \\vec{\\tau}'(\\vec{\\eta}(\\vec{\\xi}))-\\vec{\\tau}'(\\vec{\\eta}_o ) ) = \\vec{\\tau}'(\\vec{\\eta}(\\vec{\\xi}))-\\vec{\\tau}'(\\vec{\\eta}_o).\\end{aligned}\\ ] ] thus , @xmath502^{-1 } a)^{-1 } ( \\vec{\\tau}'(\\vec{\\eta}(\\vec{\\xi}))-\\vec{\\tau}'(\\vec{\\eta}_o)).\\end{aligned}\\ ] ] in this approximation , our estimator @xmath472 for @xmath499 is characterized as    @xmath503^{-1 } a)^{-1 } b^{-1}(\\vec{g}^n(x^{n+1})/n-\\vec{\\eta}_o)+\\vec{\\xi}_o$ ] .",
    "thus , the covariance matrix of our estimator is @xmath504^{-1 } a)^{-1 } \\frac{i}{n } ( ( b_1^t \\mathsf{h}_{\\vec{\\theta}_{\\rm crv}(\\vec{\\xi}_o)}[\\phi]^{-1 } a)^t)^{-1 } \\\\",
    "= & \\frac{1}{n}((b_1^t \\mathsf{h}_{\\vec{\\theta}_{\\rm crv}(\\vec{\\xi}_o)}[\\phi]^{-1 } a)^t ( b_1^t \\mathsf{h}_{\\vec{\\theta}_{\\rm crv}(\\vec{\\xi}_o)}[\\phi]^{-1 } a ) ) ^{-1 } \\\\ = & \\frac{1}{n}(a^t \\mathsf{h}_{\\vec{\\theta}_{\\rm crv}(\\vec{\\xi}_o)}[\\phi]^{-1 } a)^{-1 } = \\frac{1}{n } \\tilde{\\mathsf{h}}_{\\vec{\\xi}_o}[\\phi]^{-1}.\\end{aligned}\\ ] ] that is , the random variable @xmath474 asymptotically obeys the gaussian distribution with the covariance matrix @xmath475^{-1}$ ] .",
    "therefore , the mean square error matrix of our estimator @xmath472 is asymptotically approximated to @xmath473^{-1}+ o(\\frac{1}{n})$ ] .",
    "the papers @xcite showed that the maximum likelihood estimator ( mle ) is asymptotically efficient in the exponential family with their definition ( [ 1 - 3 ] ) . since the definition ( [ 1 - 3 ] ) is different from ours ( [ 5 - 6 ] ) , the results in this section are different from theirs .",
    "further , since our asymptotically efficient estimator is given as the sample mean of @xmath50 , the required calculation amount is smaller than theirs . even in the case of a curved exponential family , the pythagorean theorem ( [ 5 - 1 ] )",
    "enables us to calculate our asymptotically efficient estimator with small amount of calculation .",
    "however , their mle does not have so simple form because their exponential family does not have such a geometrical structure , e.g. , expectation parameter and the pythagorean theorem , etc .",
    "hence , it requires large calculation amounts .",
    "indeed , when the matrix entries of the transition matrix is to be estimated , the literature @xcite showed that the sample mean is the same as the maximum likelihood estimator .",
    "however , this fact holds only for such a specific parameter , and can not be applied to the parameter estimation of our exponential family , in general .",
    "our method can be applied to any parameter of an exponential family in our sense .      in this subsection , we consider how to calculate our estimator @xmath472 .",
    "this calculation depends on the type of parametrization of the transition matrix @xmath505 .",
    "we can consider two cases as follows .",
    "( 1 ) : :    the entries of the transition matrix    @xmath505 are calculated    directly from @xmath499 with small calculation    complexity . ( 2 ) : :    the entries of the transition matrix    @xmath505 are calculated by    via the calculation of    @xmath506 . in this case , the    calculation of these entries has large calculation complexity .    for example ,",
    "example [ 1 - 3 - 8c ] belongs to case ( 1 ) because @xmath218 is directly calculated from the parameter @xmath426 .    in the calculation of the estimator @xmath472 ,",
    "first , we obtain the estimate @xmath507 of the larger exponential family @xmath146 with the expectation parameter .",
    "then , we calculate its natural parameter @xmath237 by the method given in the end of subsection [ s8 - 1 ] .",
    "the following steps depend on the above case . in case",
    "( 1 ) , we can implement the minimization by employing the final expression in with small calculation complexity due to the following reason . the final expression in needs only the entries of the transition",
    "matrices @xmath508 and @xmath505 and the perron - frobenius eigenvector of @xmath509 . in this case , it is enough to calculate the perron - frobenius eigenvalue the perron - frobenius eigenvector of @xmath509 only at the first step . at each step of the minimization",
    ", we do not have any difficult calculation .",
    "therefore , the final expression in brings us an easy implementation of the minimization in case ( 1 ) .",
    "however , in case ( 2 ) , it is better to employ instead of the final expression in due to the following reason .",
    "when the final expression in is employed , the calculation of the transition matrix @xmath505 requires the calculations of the perron - frobenius eigenvalue and the perron - frobenius eigenvector of the matrix given in as in . to calculate the rhs of",
    ", we need to calculate the partial derivative @xmath510 and the perron - frobenius eigenvalues @xmath511 and @xmath512 .",
    "fortunately , the partial derivative @xmath510 coincides with the expectation parameter @xmath507 , which is firstly obtained .",
    "also , it is enough to calculate the perron - frobenius eigenvalue @xmath511 only once . hence , at each step of the minimization , we need to calculate only the perron - frobenius eigenvalue @xmath512 , i.e. , we do not need to calculate the perron - frobenius vector .",
    "therefore , requires less calculation complexity than the final expression in in case ( 2 ) .",
    "we have revisited the information geometrical structure ( the exponential family , the natural parameter , the expectation parameter , relative entropy , relative rnyi entropy , fisher information matrix , and the pythagorean theorem ) of transition matrices by using the convex function @xmath39 defined by the perron - frobenius eigenvalue of the matrix @xmath90 defined by ( [ 5 - 6 ] ) .",
    "then , we have shown that the sample mean of the generating function is an asymptotically efficient estimator for the expectation parameters in the exponential family of transition matrices . combining this property and the pythagorean theorem , we have given an asymptotically efficient estimator for a curved exponential family of transition matrices . as a consequence",
    ", we have characterized the asymptotic variance of the sample mean in the markovian chain by using the second derivative of the convex function @xmath39 .    in this paper , we have assumed that our system consists of finite elements .",
    "indeed , the existing papers @xcite reported several difficulties to evaluate the variance of the sample mean in the continuous probability space even with the discrete time markov chain .",
    "so , it is remained to extend the obtained results to the continuous case .",
    "however , this assumption is assumed only for describing the conditional distribution by a matrix .",
    "we do not use the finiteness of the cardinality of the probability space explicitly .",
    "therefore , it seems that there is no essential obstacle for extension to the continuous case under a proper regularity condition .",
    "this extension will enable us to handle several gaussian markovian chains in a simple way .",
    "further , the obtained version of the pythagorean theorem will be helpful for the hierarchy of exponential families of transition matrices .",
    "for an example , a hierarchy of exponential families can be constructed by changing the degree of markovian chain , it might be interesting to investigate this example .",
    "the authors are grateful for dr .",
    "wataru kumagai to informing the references @xcite .",
    "mh is partially supported by a mext grant - in - aid for scientific research ( a ) no .",
    "mh is also partially supported by the national institute of information and communication technology ( nict ) , japan .",
    "sw is partially supported by jsps postdoctoral fellowships for research abroad .",
    "the centre for quantum technologies is funded by the singapore ministry of education and the national research foundation as part of the research centres of excellence programme .",
    "as mentioned in introduction , some of results in this paper for relative entropy and exponential family have been already stated in @xcite ( without detailed proof ) and we restate those results and give proofs to keep the paper self - contained . for deeper understanding ,",
    "we summarize the relation with those papers in this appendix .",
    "our definition ( [ 1 - 10 ] ) for the relative entropy @xmath48 has the following relation with those by @xcite .",
    "natarajan @xcite and nakagawa and kanaya @xcite defined the relative entropy @xmath48 by the final term of ( [ eq : markov - divergence - cdf - relation ] ) .",
    "however , nagaoka @xcite defined the relative entropy @xmath48 by ( [ 1 - 1 ] ) and showed the equivalence with the final term of ( [ eq : markov - divergence - cdf - relation ] ) .",
    "if we consider only the relative entropy @xmath48 , the definition by the final term of ( [ eq : markov - divergence - cdf - relation ] ) is natural .",
    "however , the relative rnyi entropy @xmath49 can not define in the same way .",
    "hence , in order to treat the relative entropy @xmath48 and the relative rnyi entropy @xmath49 in a unified way , we adopt the definition ( [ 1 - 10 ] ) for the relative entropy @xmath48 instead of the final term of ( [ eq : markov - divergence - cdf - relation ] ) .",
    "our definition clarifies the relation between the relative entropy @xmath48 and the relative rnyi entropy @xmath49 , which is helpful when we apply these quantities to simple hypothesis testing @xcite , random number generation , data compression , and channel coding @xcite in markov chain .",
    "next , we address the convexity of the function @xmath119 . nakagawa and kanaya ( * ? ? ?",
    "* section iii ) and nagaoka @xcite showed the convexity @xmath119 in their respective cases .",
    "nagaoka @xcite also showed the equivalence between ( 1 ) and ( 5 ) in lemma [ l1 - 14 - 2 ] .",
    "however , they did not clearly consider the relation with the other conditions in lemma [ l1 - 14 - 2 ] .",
    "in fact , these equivalence relations are essential for the condition of a generator of an exponential family and also for applications to finite - length evaluations of the tail probability , the error probability in simple hypothesis testing @xcite , source coding , channel coding , and random number generation @xcite in markov chain .",
    "now , we proceed to the definition of an exponential family for transition matrices .",
    "our logical order of arguments in this definition is different from that by nagaoka @xcite and nakagawa and kanaya @xcite .",
    "we firstly define the potential function @xmath119 from a given transition matrix @xmath1 and a given generator @xmath85 then , we give the parametric transition matrices although their papers @xcite gave the parametric transition matrices firstly . the potential function @xmath119 for a transition matrix @xmath1 and a generator @xmath85 produces several information quantities , which play the central roles when we apply the exponential family for transition matrices to finite - length evaluations of the tail probability and the above applications @xcite in markov chain .",
    "to characterize these information quantities , we employ an exponential family of transition matrices .",
    "so , our logical order adapts such an application .",
    "further , this paper introduces a mixture family while the existing papers @xcite did not define a mixture family .",
    "indeed , kontoyiannis and meyn ( * ? ? ? * ( 11 ) ) gave a one - parameter family of transition matrices with the same logical order .",
    "however , they did not use the terminology `` exponential family '' and did not show the convexity of the potential function @xmath119 .",
    "ito and amari @xcite discussed the geometrical structure of an exponential family of transition matrices only for @xmath513 in the same definition as ours .",
    "however , they did not treat this set as an exponential family of transition matrices .",
    "our formula ( [ 5 - 1 ] ) in pythagorean theorem ( proposition [ p1 - 15 ] ) has the following relation with nakagawa and kanaya @xcite .",
    "nakagawa and kanaya ( * ? ? ?",
    "* lemma 5 ) showed ( [ 5 - 1 ] ) with @xmath514 .",
    "hence , our relation ( [ 5 - 1 ] ) can be regarded as a generalization of nakagawa and kanaya ( * ? ? ?",
    "* lemma 5 ) . indeed , the motivation of nakagawa and kanaya ( * ? ? ?",
    "* lemma 5 ) is related to the exponent of simple hypothesis testing .",
    "that is , their purpose is to show the relation @xmath515 however , the multi - parametric extension ( [ 5 - 1 ] ) is essential for estimation in a curved exponential family , which is discussed in subsection [ s8 - 3 ] .",
    "to discuss example [ 1 - 3 - 8c ] in the detail , we investigate the set of bi - stochastic matrices on @xmath147 .",
    "first , we divide the linear space of @xmath516 matrices into two linear spaces : @xmath517 in the following , any two - input function @xmath23 is regarded as an @xmath516 matrix . for an arbitrary non - identical permutation @xmath518",
    ", the function @xmath519 belongs to @xmath520 .",
    "the function @xmath468 belongs to @xmath521 .",
    "also , when a function @xmath374 satisfies @xmath522 with a constant @xmath127 and a vector @xmath523 , the function @xmath374 belongs to @xmath521 .",
    "any non - zero linear combination of @xmath524 can not be written by the above function @xmath374 .",
    "thus , to show the linear independence of the set of functions @xmath525 , it is enough to show the following lemma .",
    "the set @xmath526 is linearly independent in the linear space @xmath520 .",
    "the number of elements of the set @xmath525 is @xmath527 , which equals the dimension of @xmath520 .",
    "so , the set @xmath525 spans the linear space @xmath520 . for any bi - stochastic matrix @xmath1",
    ", we have @xmath528 . hence , @xmath529 can be written as a linear combination of @xmath526 , i.e. , @xmath530 . therefore , @xmath531 .",
    "lemma [ l1 - 11 - 2 ] now , we prepare notations . for a two - input function @xmath50",
    ", we define the symmetric matrix @xmath532_{x , x'}:= g(x , x ' ) + g(x',x)$ ] and the anti - symmetric matrix @xmath533_{x , x'}:= g(x , x ' ) -g(x',x)$ ] .    due to the constraint for @xmath520 ,",
    "the diagonal entries of an element of @xmath534 are determined by other entries .",
    "fixed @xmath535 , only the matrix @xmath536 $ ] has a non - zero @xmath537-th entry among the set @xmath538\\}_{(i , j)\\in t}$ ] .",
    "hence , the set @xmath538\\}_{(i , j)\\in t}$ ] is linearly independent in the linear space @xmath539 $ ] .    due to the constraint for @xmath520 , the @xmath540-th entry and @xmath541-th entry of an element of @xmath542",
    "are determined by other entries .",
    "fixed @xmath543 , only the matrix @xmath544 $ ] has a non - zero @xmath537-th entry among the set @xmath545\\}_{(0,i , j)\\in h}$ ] .",
    "hence , the set @xmath545\\}_{(0,i , j)\\in h}$ ] is linearly independent in the linear space @xmath546 $ ] . therefore , the set @xmath547 is linearly independent in the linear space @xmath520 . since @xmath548=0 $ ] for @xmath549 , the set @xmath550 is linearly independent in the linear space @xmath520",
    "the fisher information @xmath319 can be written as @xmath551   \\\\ = & \\sum_{x , x ' } w_{\\theta } \\times \\overline{p}^1_{\\theta}(x , x ' )   [ -\\frac{d^2}{d\\theta^2 } \\log w_{\\theta } ( x|x ' )   -\\frac{d^2}{d\\theta^2 } \\log \\overline{p}^1_{\\theta}(x ' ) ] \\nonumber \\\\ = & \\sum_{x , x ' } w_{\\theta } \\times \\overline{p}^1_{\\theta}(x , x ' )   [ -\\frac{d^2}{d\\theta^2 } \\log w_{\\theta } ( x|x ' ) ] + \\sum_{x ' } \\overline{p}^1_{\\theta}(x ' )   [ -\\frac{d^2}{d\\theta^2 } \\log \\overline{p}^1_{\\theta}(x ' ) ] \\nonumber \\\\ = & \\sum_{x , x ' } w_{\\theta } \\times \\overline{p}^1_{\\theta}(x , x ' )   [ -\\frac{d^2}{d\\theta^2 } \\log w_{\\theta } ( x|x ' ) ] + j_{\\theta}^1\\nonumber \\\\ = & \\sum_{x , x ' } w_{\\theta } \\times \\overline{p}^1_{\\theta}(x , x ' )   \\bigl[-\\frac{d^2}{d\\theta^2 } \\log \\frac{1}{\\lambda_{\\theta } } -\\frac{d^2}{d\\theta^2 } \\log \\frac{\\overline{p}^3_{\\theta}(x)}{\\overline{p}^3_{\\theta}(x ' ) } -\\frac{d^2}{d\\theta^2 } \\log w(x|x ' ) \\nonumber \\\\ & \\hspace{5ex}-\\frac{d^2}{d\\theta^2 } \\theta g(x , x')\\bigr ] + j_{\\theta}^1\\nonumber \\\\ \\stackrel{(a)}{= } & \\sum_{x , x ' } w_{\\theta } \\times \\overline{p}^1_{\\theta}(x , x ' )   [ -\\frac{d^2}{d\\theta^2 } \\log \\frac{1}{\\lambda_{\\theta } } ] + j_{\\theta}^1   =   \\frac{d^2\\phi}{d\\theta^2 } ( \\theta ) + j_{\\theta}^1 ,   \\label{25 - 1}\\end{aligned}\\ ] ] where @xmath314 follows from the relation @xmath552 , which is shown by the following fact : the expectations of @xmath553 and @xmath554 are the same because the marginal distributions of @xmath555 and @xmath556 are the same . hence , we obtain ( [ 27 - 10 ] ) .",
    "the fisher information @xmath319 is also written as @xmath557 \\nonumber \\\\ = & \\sum_{x , x ' } w_{\\theta } \\times \\overline{p}^1_{\\theta}(x , x ' )   \\biggl [ \\bigl ( \\frac{d}{d\\theta } \\log w_{\\theta } ( x|x')\\bigr)^2 \\biggl ] + \\sum_{x ' } \\overline{p}^1_{\\theta}(x ' )   \\bigl(\\frac{d}{d\\theta } \\log \\overline{p}^1_{\\theta}(x ' ) \\bigr)^2 \\biggr ] \\nonumber \\\\ & + 2\\sum_{x , x ' }",
    "\\bigl(\\frac{d}{d\\theta } \\log w_{\\theta } ( x|x ' ) \\bigr ) w_{\\theta}(x|x ' )   \\bigl(\\frac{d}{d\\theta } \\log \\overline{p}^1_{\\theta}(x ' ) \\bigr )   \\overline{p}^1_{\\theta}(x ' )   \\nonumber \\\\ = & \\sum_{x , x ' } w_{\\theta } \\times \\overline{p}^1_{\\theta}(x , x ' )   \\bigl[\\frac{d}{d\\theta } \\log w_{\\theta } ( x|x')\\bigr]^2 +   j_{\\theta}^1 \\nonumber \\\\ = & \\sum_{x , x ' } w_{\\theta } \\times \\overline{p}^1_{\\theta}(x , x ' )   \\bigl[- \\frac{d \\phi}{d \\theta}(\\theta ) + \\frac{d}{d\\theta } \\log \\overline{p}^3_{\\theta}(x ) - \\frac{d}{d\\theta } \\log \\overline{p}^3_{\\theta}(x ' ) + g(x , x')\\bigr]^2 +   j_{\\theta}^1 .",
    "\\label{25 - 2}\\end{aligned}\\ ] ] combining ( [ 27 - 10 ] ) and ( [ 25 - 2 ] ) , we have @xmath558 > 0,\\end{aligned}\\ ] ] which implies ( [ 27 - 11 ] ) . since @xmath559",
    "we have another expression of @xmath322 as follows .",
    "@xmath560 -(\\frac{d}{d\\theta } \\phi(\\theta))^2.\\end{aligned}\\ ] ] when @xmath299 , @xmath561 -\\eta(0)^2 \\nonumber",
    "\\\\ = & \\mathsf{v}_0 [ g(x , x ' ) ] + 2 \\sum_{x , x ' } w(x|x ' ) g(x , x ' ) \\frac{d \\overline{p}^2_{\\theta}(x')}{d \\theta}\\bigr|_{\\theta=0}\\end{aligned}\\ ] ] because @xmath562 and @xmath563 $ ] .",
    "hence , we obtain ( [ 27 - 12 ] ) .",
    "we show the twice - differentiablity of @xmath39 , @xmath564 and @xmath565 .",
    "first , focus on the @xmath566-parameter case .",
    "now , we define the function @xmath567 with the identity matrix @xmath568 .",
    "since @xmath569 is the unique solution of @xmath570 and the function @xmath571 is twice - differentiable , the implicit function theorem guarantees that @xmath33 is twice - differentiable .",
    "hence , @xmath39 is also twice - differentiable .",
    "next , we show that the twice - differentiablity of @xmath564 and @xmath565 , which are normalized eigenvector with positive entries of @xmath572 and @xmath573 .",
    "now , we define the vector - valued function @xmath574 and the function @xmath575 .",
    "since @xmath565 is the unique solution of @xmath576 and @xmath577 and the functions @xmath578 and @xmath579 are twice - differentiable , the implicit function theorem guarantees that @xmath580 is twice - differentiable . replacing the role of @xmath581 by that of @xmath573",
    ", we can show the twice - differentiablity of @xmath565 .",
    "these discussions can be extended to the case when @xmath58 is a @xmath435-dimensional parameter .",
    "h. nagaoka , `` the exponential family of markov chains and its information geometry '' proceedings of the 28th symposium on information theory and its applications ( sita2005 ) , okinawa , japan , nov .",
    "20 - 23 , ( 2005 ) .",
    "v. t. stefanov , `` explicit limit results for minimal sufficient statistics and maximum likelihood estimators in some markov processes : exponential families approach , '' _ the annals of statistics _ , vol . 23 , no . 4 , 1073 - 1101 ( 1995 ) .",
    "l. bregman , `` the relaxation method of finding a common point of convex sets and its application to the solution of problems in convex programming , '' _ comput .",
    ", vol . 7 , pp .",
    "200 - 217 , 1967 ."
  ],
  "abstract_text": [
    "<S> we consider the parameter estimation of markov chain when the unknown transition matrix belongs to an exponential family of transition matrices . </S>",
    "<S> then , we show that the sample mean of the generator of the exponential family is an asymptotically efficient estimator . </S>",
    "<S> further , we also define a curved exponential family of transition matrices . using a transition matrix version of the pythagorean theorem </S>",
    "<S> , we give an asymptotically efficient estimator for a curved exponential family . </S>"
  ]
}