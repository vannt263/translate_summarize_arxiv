{
  "article_text": [
    "the subject of cosmic microwave background ( cmb ) power spectrum estimation has been a very active research field for many years now , and the combined effort from the scientific community has resulted in a number of qualitatively different methods . broadly , one may classify these methods into three groups , namely maximum likelihood methods @xcite , pseudo-@xmath2 methods @xcite , and specialized methods @xcite . in general the maximum likelihood methods are more accurate than the ( often monte carlo based ) pseudo-@xmath2 methods , but they are usually so at a prohibitive computational cost . and",
    "even these methods can only return very approximate summaries of the error bars , since exploring the likelihood away from the peak is practically impossible .",
    "further , the specialized methods are usually only applicable following rather restrictive assumptions . for general experiments we have up",
    "to now been left with the rather uncomfortable choice between the optimal but prohibitively expensive , and the feasible but approximate .    in this context , a method based on monte carlo markov chains and gibbs sampling was very recently developed by jewell , levin & anderson ( 2004 ) and wandelt , larson & lakshminarayanan ( 2004 ) which may change this picture .",
    "the fundamental idea behind this method is to solve the power spectrum estimation problem by establishing its posterior probability distribution through sampling , rather than by direct solution of the corresponding optimization problem . in its most general form",
    ", the scaling of this monte carlo method equals that of the map making process , which is to be compared to the typical @xmath3 scaling for traditional maximum likelihood estimators @xcite , @xmath4 being the number of pixels in the map .",
    "further , for an experiment with spherically symmetric beams and uncorrelated noise , one may work directly with maps instead of time - ordered data , in which case the scaling reduces to that of a spherical harmonics transform , namely @xmath5 , using the healpix pixelization .",
    "until now , the only application of this method to cosmological data was the analysis of the low - resolution _",
    "cobe_-dmr data , presented by @xcite . in the following",
    "we demonstrate the practicality of this method for current and future experiments , as we for the first time apply it to a large data set , namely the _ wilkinson microwave anisotropy probe _ ( _ wmap _ ) data @xcite .",
    "this data set consists of eight cosmologically important frequency bands , each with about three million pixels , and it is therefore an excellent test bed for any new algorithm . we have developed two independent implementations of the algorithm , one called _",
    "_ commander _ _ ( `` commander is an optimal monte - carlo markov chain driven estimator '' ) and the other called _ _ magic _ _ ( `` magic allows global inference of covariance '' ) @xcite .",
    "we have tested these implementations extensively , and found that they produce statistically identical results .",
    "the goals of the present paper are twofold .",
    "first , we prepare for the actual _ wmap _ analysis by developing a number of sophistications to the gibbs sampling algorithms necessary to facilitate a high - resolution analysis .",
    "second , we apply the computer codes to simulated data in order to verify that the codes perform as expected , and to build up an intuitive understanding of issues such as the markov chain correlation length vs.  the signal - to - noise ratio , multipole coupling vs.  sky coverage , and sufficient sampling vs.overall cpu time .",
    "a proper understanding of these questions is crucial in order to optimize real - world analyses .",
    "the scientific results from the _ wmap _ analysis are reported in a companion letter by @xcite .",
    "this paper is a natural extension of the work presented by @xcite and @xcite , and we will in the following frequently refer to those papers .",
    "further , we do not attempt to re - establish the motivation behind the gibbs sampling approach here , but refer the interested reader to those papers for details and proofs . in the present paper",
    "we simply summarize the operational steps of the algorithm , and specialize the discussion to the problems encountered when analyzing the first - year _ wmap _ data .",
    "we now define some notation .",
    "the data are given in the form of @xmath6 sky maps ( also called `` bands '' or `` channels '' ) @xmath7 where @xmath8 runs over the bands .",
    "@xmath9 is the vector of observed pixel values on the sky , @xmath10 is the matrix corresponding to beam convolution , @xmath11 is the true sky vector , and @xmath12 is instrumental noise .",
    "as mentioned above , our main scientific goal of the current work is to analyze the first - year _ wmap _ data , and for that reason we assume the beam to be azimuthally symmetric @xcite .",
    "the beam convolution @xmath13 may therefore be computed in harmonic space by a straightforward multiplication of the corresponding legendre components @xmath14 . in order to simplify the notation",
    ", we incorporate the pixel window function into @xmath14 .",
    "further , for the _ wmap _ data , it is reasonable to approximate the noise as uncorrelated , but non - uniform , so that the real space noise covariance matrix can be written as @xmath15 , where @xmath16 is the noise standard deviation of the @xmath17th pixel of the @xmath18th sky map .",
    "in fact , we explicitly demonstrate the validity of this assumption in section [ sec : highres_sim ] , by first analyzing simulations including white noise and then correlated noise , showing that they are statistically consistent for the levels of correlated noise present in the _ wmap _ data .",
    "finally , we assume the cmb fluctuations to be gaussian and isotropic , and the signal covariance matrix therefore simplifies considerably , @xmath19 .      the idea behind the gibbs sampling power spectrum estimation technique is to draw samples from the probability density @xmath20 .",
    "the properties of this density can then be summarized in terms of any preferred statistic , such as its multivariate mean or mode .",
    "however , one of the major strengths of the gibbs sampling approach is that it allows for a global , optimal analysis , and it should therefore not be considered as yet another maximum likelihood technique , although it certainly is able to produce such an estimate .",
    "while direct sampling from the probability density @xmath20 is difficult , it is in fact possible to sample from the joint density @xmath21 , and then marginalize over the signal @xmath11 .",
    "this is feasible because the theory of gibbs sampling tells us that if it is possible to sample from the _ conditional _ densities @xmath22 and @xmath23 , then the two following equations will , after an initial burn - in period , converge to being samples from the joint density @xmath21 : @xmath24 thus , given some initial power spectrum and the data , we may iterate these two relations , discard the first few pre - convergence samples ( if necessary ) , and then use the remaining samples to construct whatever statistic we prefer for the power spectrum . one further advantage of this approach is that we probe the joint distribution , and we may therefore quantify joint uncertainties . and , in the process , we also obtain a wiener filtered map which may be useful for other studies .",
    "drawing a power spectrum @xmath25 given a sky map @xmath26 is trivial ( see , e.g. , wandelt et al .",
    "2004 ) . given the power spectrum of the signal map ( often written on the form @xmath27 ) , one draws @xmath28 gaussian random variates @xmath29 with zero mean and unit variance , and form the sum @xmath30    on the other hand , drawing a sky map @xmath26 given the data and an assumed power spectrum , is certainly not trivial .",
    "again , we refer the interested reader to the above - mentioned papers for justification of the following procedure , and here we only review the operational steps .",
    "the map sampling process is performed in two steps , the first being to solve the following equation for the so - called mean field map @xmath31 , @xmath32\\mathbf{c}^{1/2}\\biggr)\\mathbf{c}^{-1/2}\\mathbf{x } %   = \\\\ = \\mathbf{c}^{1/2 } \\sum_{k=1}^{n } \\mathbf{a}_k^{\\textrm{t } } %   \\mathbf{n}^{-1}_k \\mathbf{r}_k^{\\textrm{s}}. \\left(\\mathbf{c}^{-1 } + \\left[\\sum_{k=1}^{n }    \\mathbf{a}_k^{\\textrm{t } } \\mathbf{n}^{-1}_k    \\mathbf{a}_k\\right]\\right)\\,\\mathbf{x }    = \\sum_{k=1}^{n } \\mathbf{a}_k^{\\textrm{t } }    \\mathbf{n}^{-1}_k \\mathbf{r}_k^{\\textrm{s}}. % \\left(\\mathbf{c}^{-1 } + \\mathbf{a}^{\\textrm{t } } \\mathbf{n}^{-1 } %   \\mathbf{a}\\right)\\mathbf{x } % = \\mathbf{a}^{\\textrm{t } } % \\mathbf{n}^{-1 } \\mathbf{r}^{\\textrm{s}}. \\end{split } \\label{eq : meanfield}\\ ] ] here @xmath33 is the residual signal map .",
    "the reason for introducing this notation will become clearer when additional components are introduced into the sampling chain .",
    "any new component we may wish to include in the analysis will simply be subtracted from the data , to form an actual residual map from which the mean field map is computed .",
    "the mean field map is a generalized wiener filtered map , and as such is biased . to construct an unbiased sample",
    "one must therefore add a fluctuation map @xmath34 with properties such that the sum of the two fields is a sample from the distribution of the correct mean and covariance .",
    "the appropriate equation for this fluctuation map is @xmath32\\mathbf{c}^{1/2}\\biggr)\\,\\mathbf{c}^{-1/2}\\mathbf{y } %   = \\\\",
    "= \\mathbf{\\omega}_0 + \\mathbf{c}^{1/2 } \\sum_{k=1}^{n } %   \\mathbf{a}_k^{\\textrm{t } } \\mathbf{n}^{-1/2}_k \\mathbf{\\omega}_k , \\left(\\mathbf{c}^{-1 } + \\left[\\sum_{k=1}^{n }    \\mathbf{a}_k^{\\textrm{t } } \\mathbf{n}^{-1}_k    \\mathbf{a}_k\\right]\\right)\\,\\mathbf{y }    = \\quad\\quad\\quad\\quad\\quad\\quad \\\\",
    "\\quad\\quad\\quad\\quad\\quad\\quad = \\mathbf{c}^{-1/2}\\mathbf{\\omega}_0 + \\sum_{k=1}^{n }    \\mathbf{a}_k^{\\textrm{t } } \\mathbf{n}^{-1/2}_k \\mathbf{\\omega}_k , % \\left(\\mathbf{c}^{-1 } + \\mathbf{a}^{\\textrm{t } } \\mathbf{n}^{-1 } %   \\mathbf{a}\\right)\\,\\mathbf{y } = \\mathbf{c}^{-1/2}\\mathbf{\\omega}_0 + %   \\mathbf{a}^{\\textrm{t } } \\mathbf{n}^{-1/2 } \\mathbf{\\omega } , \\end{split } \\label{eq : fluctmap}\\ ] ] where @xmath35 are gaussian white noise maps of zero mean and unit variance .",
    "examples of such maps are shown in figure [ fig : maps ] , and the corresponding power spectra are shown in figure [ fig : ex_spectra ] .",
    "however , in practice the two equations are solved simultaneously by solving for the sum of @xmath31 and @xmath34 , in order to reduce the total cpu time .",
    "finally , we point out that even though the gibbs sampling technique is a bayesian method , a frequentist view may be taken by choosing a uniform prior . in that case",
    ", the procedure reduces to simply exploring the joint likelihood , and frequentist concepts such as the maximum likelihood estimate may be established .",
    "one of the most elegant features of this formalism is its ability to incorporate virtually any real - world complication , as discussed by @xcite and @xcite .",
    "a few examples of this flexibility are applications to @xmath36 noise , asymmetric beams , non - cosmological foregrounds , or arbitrary sky coverage . however , in this paper we include only the effects of the mono- and dipole contributions ( which may be thought of as foregrounds ) and that of partial sky coverage , given that our main scientific goal is to analyze the fairly well - behaved _ wmap _ data .",
    "the question regarding mono- and dipole contributions has gained renewed importance during the previous year , given the very active debate concerning the quadrupole seen in the _ wmap _ data .",
    "this quadrupole appears to be small compared to the best - fit cosmological model @xcite , and several authors have considered what this may imply in terms of new physics .",
    "however , the exact significance of this anomaly is difficult to assess for several reasons , but mainly because of uncertainties in the foreground subtraction process @xcite .",
    "methodology issues for estimating the lowest multipole amplitudes have also been pointed out @xcite .",
    "strongly related to both these issues is the fact that non - cosmological mono- and dipole contributions may couple into the other low - order modes through incomplete sky coverage .",
    "the most common way of handling this latter problem is to fit a mono- and dipole to the incomplete sky , including internal coupling caused by the sky cut , and then simply subtract the resulting best - fit components from the data .",
    "however , this procedure neglects the noise correlations that are introduced by removing any fitted templates .",
    "the gibbs sampling framework allows a statistically more consistent approach : rather than directly subtracting the fitted mono- and dipoles from the data , one may marginalize over them through sampling , and thus recognize the inherent uncertainties involved .    as always in bayesian analyses",
    ", one has to choose a prior , and the most natural choice in this case is a uniform prior .",
    "this corresponds to saying that we do not know anything about these components . for analytic computations and proofs",
    ", however , it is more convenient to define this as a gaussian with infinite variance , which is just a different way of parameterizing a uniform prior .",
    "it should be noted that a uniform prior does not mean that these components are unrestricted , but , rather , it simply means that their values are determined by the data alone .",
    "again , general formalisms for handling this type of problem were described by @xcite and @xcite , and we will only repeat the operational steps here , in a notation suitable for our purposes .",
    "let us first define a @xmath37 template matrix @xmath38 containing the four real spherical harmonics in pixel space , @xmath39 where @xmath40 and @xmath41 note that @xmath38 is a projection matrix onto the subspace spanned by the corresponding templates .",
    "next we define a vector of template amplitudes @xmath42 , letting the amplitudes be different for each channel , since we have no reason to assume that these components are frequency independent .",
    "thus , the mono- and dipole contribution to the @xmath18th channel is @xmath43 .",
    "we now want to sample from the conditional distribution @xmath44 , and this is done ( assuming the infinite variance prior ) by solving the following equation , @xmath45 where the mono- and dipole residual map is @xmath46 , and @xmath47.\\ ] ] here , @xmath48 are white noise maps of vanishing mean and unit variance .",
    "the next step in traditional gibbs sampling would now be to sample from the conditional density @xmath49 , where @xmath50 are the parameters of the probability distribution describing the mono- and dipoles .",
    "however , since we have chosen a very special prior , namely one with infinite variance , this distribution does not change , and no sampling is required .",
    "including the mono- and dipole components in the gibbs sampling chain , it now reads @xmath51 the first step is computed as described in the previous paragraphs , and the second step is computed by equation [ eq : meanfield ] , with the slight modification that the mono- and dipole contributions now are subtracted from the data , @xmath52 .",
    "perhaps the single most important complication in any cmb analysis is proper treatment of foregrounds . with amplitudes up to several thousand times the cmb amplitude",
    ", galactic foregrounds will necessarily compromise any cosmological result unless corrected and accounted for .",
    "unfortunately , there is currently a critical shortage of robust component separation ( or even just foreground removal ) methods , and the only reliable approach at the time of writing is simply to mask out the most contaminated regions of the sky . on the bright side",
    ", the flexibility in specifying foreground models that can be implemented in the gibbs sampling approach offers an attractive avenue for progress .",
    "this will be explored further in future publications .",
    "the gibbs sampling approach supports two fundamentally different methods for removing parts of the sky by means of a mask .",
    "first , the most straightforward option from a conceptual point of view is simply to set the inverse noise matrix to zero at all pixels within the mask .",
    "this corresponds to saying that the noise level of these pixels is infinite , and therefore that the data are completely non - informative .",
    "no other modifications of the equations are necessary .",
    "this is the solution chosen for the commander implementation .",
    "however , this approach carries a considerable cost in the form of a poorly conditioned coefficient matrix @xmath53 , which , as we will discuss at greater length in the next section , results in slow convergence for the conjugate gradient algorithm , and increased overall expense for the gibbs sampling . recognizing this fact , an alternative approach was chosen for the magic implementation , namely to introduce a new foreground component into the gibbs sampling chain .",
    "let us recall the general sampling equation for the foreground component @xcite , @xmath54 here @xmath55 is the covariance matrix for the foreground prior , @xmath56 is the residual map after removal of the signal estimate and any other foregrounds already sampled by the algorithm , and @xmath57 are vectors of uniform gaussian variates . finally , @xmath58 is the unconvolved foreground sample .    for each pixel in the masked region , mainly the galaxy but also some point sources , we do not know the foreground contribution .",
    "the maximally uninformative foreground prior for these pixels has infinite variance .",
    "it corresponds to a complete lack of _ a priori _ knowledge of the foregrounds in the mask . by specifying maximal ignorance of the foreground",
    "we allow the algorithm to determine the level of the foreground in these pixels which is supported by the data .",
    "substituting this foreground prior into equation [ foreground ] creates a method to numerically marginalize over the unknown foreground contribution in the masked pixels .    in the limit of infinite variance",
    ", this sampling equation simplifies to @xmath59 in the masked region and @xmath60 outside .",
    "this is easy to compute and avoids the use of the conjugate gradient solver , hence saving computational time .    with the introduction of this foreground component",
    ", the full gibbs chain reads @xmath61 again , the only modifications in the two middle steps is a subtraction of the foreground components from the corresponding residual maps , @xmath62 and @xmath63 .    as mentioned earlier , the main advantage of this approach is that the uniform properties of the coefficient matrix @xmath64 are conserved , leading to a faster convergence for the conjugate gradient solver , often reducing the number of iterations by a factor of three . on the other hand ,",
    "there is also a slight disadvantage in that the correlations between consecutive gibbs samples are stronger , since information is carried over from sample to sample through the foreground component .",
    "however , this is more than compensated by the rapid cg convergence .",
    "we will return to these issues later .",
    "as described in section [ sec : basic_gibbs ] , equations [ eq : meanfield ] and [ eq : fluctmap ] are the very heart of the gibbs sampling method , and its feasibility is directly connected to our ability to solve those equations . for a low - resolution experiment such as _ cobe_-dmr , which comprises a few thousand pixels or multipole components ,",
    "the system may be solved directly , for instance through cholesky decomposition . however , for a high - resolution experiment such as _ wmap _ , with eight cosmologically important maps of each several millions pixels , more sophisticated algorithms must be employed , and the most efficient method currently available for positive - definite matrices is the conjugate gradient ( cg ) method @xcite . for a truly excellent review of this algorithm ,",
    "see @xcite .",
    "the general problem is to solve a system of linear equations , @xmath65 where the coefficient matrix @xmath64 is very large . in the case of the first - year _ wmap _ data",
    ", @xmath64 corresponds to a system of three million equations in pixel space , and a system of several hundred thousands equations in harmonic space ( depending on the @xmath66 of choice ; see section [ sec : correlations ] for a discussion on how to choose an appropriate @xmath66 ) .",
    "further , this coefficient matrix is in general not sparse in either real space due to complicated signal correlations , or in harmonic space due to complicated noise correlations .",
    "however , favorable sparsity patterns may be obtained for special scanning strategies and sky cuts @xcite .",
    "for this reason , the sheer size of the problem poses a real problem , and for many applications one may find that the system described above is ill - conditioned .",
    "for instance , the solution vector @xmath31 contains elements of very different magnitudes , and therefore round - off errors can easily compromise the results .",
    "it is therefore numerically advantageous to rewrite equations [ eq : meanfield ] and [ eq : fluctmap ] as follows , @xmath67\\mathbf{c}^{1/2}\\right)\\,\\left(\\mathbf{c}^{-1/2}\\mathbf{x}\\right )     = \\\\",
    "= \\mathbf{c}^{1/2 } \\sum_{k=1}^{n } \\mathbf{a}_k^{\\textrm{t } }    \\mathbf{n}^{-1}_k \\mathbf{r}_k^{\\textrm{s } } \\end{split } \\\\",
    "\\begin{split } \\left(\\mathbf{1 } + \\mathbf{c}^{1/2}\\left[\\sum_{k=1}^{n }    \\mathbf{a}_k^{\\textrm{t } } \\mathbf{n}^{-1}_k    \\mathbf{a}_k\\right]\\mathbf{c}^{1/2}\\right)\\,\\left(\\mathbf{c}^{-1/2}\\mathbf{y}\\right )    = \\\\",
    "= \\mathbf{\\omega}_0 + \\mathbf{c}^{1/2}\\sum_{k=1}^{n }    \\mathbf{a}_k^{\\textrm{t } } \\mathbf{n}^{-1/2}_k \\mathbf{\\omega}_k .",
    "\\end{split } \\label{eq : rewritten_eq}\\end{aligned}\\ ] ] the coefficient matrix @xmath68 is now much better behaved , and all elements of the solution vector @xmath69 have unity variance .",
    "note also that the diagonal elements of @xmath64 are now simply the signal - to - noise ratios of the corresponding mode .",
    "we choose to work in harmonic space in the following for several reasons .",
    "first , in this space it is easy to limit the size of the problem according to the signal - to - noise ratio of the data by choosing an appropriate @xmath66 . in pixel space one",
    "is always forced to work with vectors of length @xmath4 .",
    "second , given the form of equations [ eq : meanfield ] and [ eq : fluctmap ] , two spherical harmonics transforms are eliminated by operating in harmonic space in the first place , thereby reducing the total cpu time by a factor of two .",
    "finally , since we are mainly interested in the power spectrum , an harmonic space based convergence criterion for the cg search seems more natural than a pixel space based criterion .",
    "one of the main advantages of the cg algorithm is that it does not require inversion of the coefficient matrix , and we do not even need to store it .",
    "all we need is the ability to multiply @xmath64 with a given vector @xmath70 , and solving a preconditioning equation .",
    "we first consider the matrix multiplication operation . in our setting ,",
    "for which @xmath68 , this is done in a step - wise fashion .",
    "first we multiply each component @xmath71 of the input vector by @xmath72 ( where @xmath73 is the product of the beam and pixel window functions ) , and then we perform an inverse spherical harmonic transform into real space . here",
    "we multiply with the inverse noise matrix , @xmath74 , under the assumption of uncorrelated noise",
    ". then we perform an ordinary spherical harmonic transform of the vector into harmonic space , where we again multiply with the beam and square root of the power spectrum",
    ". finally we add the original vector .",
    "thus , multiplication of @xmath64 is computationally equivalent to two spherical harmonic transforms , and memory requirements are virtually negligible bytes , or on the order of 1 gb for @xmath75 , @xmath76 being the healpix resolution parameter , which corresponds directly to the number of pixels in the map through the relation @xmath77 . ] .",
    "the efficiency of the cg algorithm is highly dependent on our ability to construct a good preconditioner ( e.g. , oh et al .",
    "1999 ) , and two preconditioners have been proposed for this problem so far , both approximating @xmath78 in harmonic space .",
    "first , under the assumption of white , but non - uniform , noise , the inverse real - space noise covariance matrix may be written as a simple inverse noise rms map , @xmath79 , which again may be expanded into spherical harmonics , @xmath80 the inverse noise matrix in spherical harmonic space is then @xcite @xmath81^{1/2 } \\\\ & \\times \\left ( \\begin{array}{ccc } \\ell_1 & \\ell_2 & \\ell_3 \\\\ 0 & 0 & 0 \\end{array } \\right ) \\left ( \\begin{array}{ccc } \\ell_1 & \\ell_2 & \\ell_3 \\\\ m_1 & -m_2 & m_3 \\end{array } \\right ) \\end{split}\\ ] ]    a very simple preconditioner may therefore be defined in terms of the diagonal elements only , @xmath82^{-1}.\\ ] ] while satisfactory for the simplest applications , we find that it takes about 300 iterations to solve for the _ wmap _ data consisting of all eight cosmologically interesting bands with this preconditioner ( applying the kp2 mask directly ) , making the total solution of the problem very expensive .    by considering the overall structure of the inverse noise matrix , @xcite proposed to use a block - diagonal matrix .",
    "in the limit of perfect azimuthal symmetry of both the galactic cut and the noise distribution , @xmath83 is orthogonal with respect to @xmath84 , and therefore it makes sense to also include all elements having @xmath85 up to some arbitrary limit @xmath86 . at higher @xmath84 s",
    ", the diagonal preconditioner is used .",
    "@xcite claims to achieve convergence in six iterations with this preconditioner for properties corresponding to the two - year _ wmap _ data , but , unfortunately , we have not yet been able to reproduce this performance . from our experiments",
    "it seems the combination of a highly non - symmetric kp2 cut , 700 resolved point source cuts , and a noise distribution tilted with respect to the galactic plane introduces significant couplings between different @xmath84 s .    in figure",
    "[ fig : sn_matrices ] we have plotted the coefficient matrices corresponding to the first - year _ wmap _ data in two different orderings , both @xmath87-major and @xmath84-major ( see caption for details ) , and with and without application of the kp2 mask . in the limit of uniform noise and no galactic cut ,",
    "these matrices would all be diagonal , and convergence would be reached in one single cg iteration using even the diagonal preconditioner .    however , as seen in the top two panels of figure [ fig : sn_matrices ] , adding non - uniform noise to the problem introduces significant coupling between different modes , which again leads to poorer cg performance . in the left panel",
    ", we see that the largest absolute values are found at low @xmath87 s , which of course is not very surprising , considering that these matrices are a measure of the signal - to - noise ratio . in the right panel",
    "we see the same matrix organized as @xmath84-major , and in the limit of azimuthal symmetry , this would be a strictly block - diagonal matrix with very small block elements . the preconditioner proposed by @xcite consists of the inverses of those blocks .",
    "but , as we see , there are many off - diagonal elements in this matrix , and , indeed , the dominant elements actually seem to be components for which @xmath88 . however , if we had computed these quantities in the ecliptic frame , rather than in the galactic , then the matrix is likely to be dominated by the @xmath89 elements , and possibly even by the @xmath90 elements .",
    "the bottom two panels show a similar set of matrices , but in this case the kp2 mask has been applied to the sky . and , as mentioned in section [ sec : incomplete_sky ] , this has the highly undesirable effect of magnifying the off - diagonal elements through mode - to - mode coupling considerably .",
    "unfortunately , neither of these matrices have a very dominant symmetry structure , and it is therefore difficult to establish an optimal preconditioner .    nevertheless , based on the structures seen in the lower left panel in figure [ fig : sn_matrices ] a third alternative was chosen for the commander implementation . rather than including only the diagonal elements , or only @xmath89 elements as @xcite",
    "do , we include _ all _ elements up to some arbitrary @xmath66 ( typically @xmath9170 for _ wmap _ ) , and at higher @xmath87 s we include only the diagonal elements .",
    "the required memory requirements for this matrix scales as @xmath92 , and are thus quite expensive , but in practice , the real limitation is the cpu time required for its cholesky decomposition ( which scales as @xmath93 $ ] ) rather than memory requirements for its storage . for @xmath94 ,",
    "the memory requirements are 52 mb and the cpu time for cholesky decomposition is on the order of one or two minutes .",
    "obviously , the latter number must be compared to the cpu time it takes to perform one cg iteration and the number of iterations saved . and yet , even with this rather expensive preconditioner , we find that the cg search converges in about 60 iterations for the combined first - year _ wmap _ data and a norm - based fractional convergence criterion of @xmath95 .",
    "thus , our performance is not as impressive as the six iterations achieved by @xcite",
    ". work on this issue is still on - going , and a hybrid of all three variants may prove to be the ultimate solution .",
    "in contrast to the commander implementation , magic does not apply a sky cut directly , but instead it introduces a new random field into the sampling chain .",
    "the appropriate coefficient matrix is therefore the one shown in the upper left panel of figure [ fig : sn_matrices ] .",
    "this choice has a very positive effect in terms of cg performance , and one routinely achieves convergence within 20 iterations using just the simple diagonal preconditioner for a first - year _ wmap _ type experiment .",
    "however , as we will see later , the cost for this performance comes in the form of a slightly longer correlation length in the markov chain , and therefore fewer independent samples .",
    "the main limitation for the gibbs sampling method is cpu time . even though the scaling of the method is equivalent to that of a spherical harmonics transform for a _ wmap _ type analysis",
    ", one has to perform this operation many times , and the total prefactor of the algorithm is therefore large .",
    "specifically , the number of spherical harmonic transforms to produce one gibbs sample is two times the number of cg iterations , times the number of frequency bands .",
    "the total number of transforms for computing one sample from an eight - band _ wmap _ data set is then typically on the order of 1000 for the commander approach ( reaching convergence in 60 iterations ) and 350 for the magic approach ( reaching convergence in 20 iterations ) . knowing that one harmonic transform takes about 5 seconds for @xmath1 and @xmath96 , the total cpu time required for one single gibbs sample is therefore on the order of one or two hours for commander and half an hour for magic . obviously , parallelization is essential to produce a sufficient number of samples .",
    "two fundamentally different approaches may be taken in this respect .",
    "either one may choose to run one single markov chain and parallelize the spherical harmonic transforms internally .",
    "since the healpix routines operate on pixel rings of constant latitude , this can be done quite efficiently by letting each processor compute its own ring",
    ". nevertheless , optimal speed - up will not be achieved , and the implementation will be somewhat complicated .",
    "the other approach is to take advantage of the fact that this method is truly a monte carlo method , and one can therefore let each processor run its own markov chain .",
    "the most important advantages of this approach are optimal speed - up and the possibility to initialize each chain with a different first guess .",
    "as we will see in the next section , consecutive gibbs samples in the markov chain are highly correlated in the low signal - to - noise regime , and producing a larger number of independent samples is therefore quite expensive .",
    "if we have a rough approximation of the true spectrum and its uncertainties ( as we usually do , through a master type analysis ; hivon et al.2002 ) , we can partially remedy this problem by initializing each markov chain with an independent power spectrum .",
    "the major drawback of this latter parallelization scheme , however , is that each markov chain will necessarily be quite short , perhaps only twenty to fifty samples .",
    "this problem is due to the fact that most current super - computer facilities have a maximum wall - clock time limit of 24 to 72 hours , and therefore the maximum length of one chain is on the same order of magnitude .",
    "of course , one may store intermediate results and restart the computations after every cycle , but this only increases the total length by a factor of a few , not by hundreds .",
    "we have chosen a combination of external and internal parallelization in our implementation , by recognizing the fact that we will in general be analyzing multi - frequency data sets consisting of @xmath97 maps .",
    "we may therefore let @xmath97 processors work on the same markov chain , each processor transforming one band .",
    "thus , optimal speed - up is not compromised , while the length of the chains is increased by the same factor . in future versions",
    "we will also implement fully internal parallelization in the healpix routines map2alm and alm2map , to have the option of focusing all the computational resources into one single chain .",
    "in this section we apply the commander and magic codes to simulated data sets for which all components are perfectly known .",
    "the goals are two - fold .",
    "firstly , we wish to demonstrate that the codes produce results consistent with theoretical expectations , and secondly , we seek to gain insight on what limitations of the algorithm we can expect to meet in real - world applications , when cpu time is limited .",
    "a number of different simulations are analyzed in the following sections , each designed to highlight some specific feature .",
    "first , in order to establish the asymptotic behavior of the algorithm , we study a data set of smaller size than the full - resolution _ wmap _ data .",
    "specifically , we construct a data set at intermediate resolution ( @xmath0 ; 196608 pixels ) , for which the cpu time per sample is on the order of 10 seconds .",
    "thus , cpu time is not a dominating problem , and we can establish the markov chain correlation lengths and power spectrum correlation matrix to great accuracy .",
    "the burn - in time is also considered .",
    "finally , we make two simulations at full _ wmap _",
    "resolution in order to confirm that the overall results from the low - resolution analysis carry naturally over to higher resolutions .",
    "this time the cpu cost is the limiting factor , and the main goal of this section is in fact to demonstrate that the gibbs sampling method is able to handle even large data sets , such as the _ wmap _ data .",
    "this analysis mimics the analysis of the first - year _ wmap _ data presented by @xcite , in that it is run on a super - computer with many short , parallel chains .",
    "the only difference between the two runs is that either white or correlated noise are added to the cmb simulations .",
    "this way we test whether the assumption of white noise may compromise the scientific results in the presence of small , but non - negligible , noise correlations .",
    "we find that this is not a significant problem for the first - year _ wmap _ data .",
    "the main goal of the low - resolution simulations is to study the asymptotic behavior of the method when the number of independent samples is very high . on the one hand , this allows us to verify that the codes work as expected without worrying about errors introduced because of a limited number of samples , and on the other , essential quantities such as the markov chain correlation length and the power spectrum correlation matrix may be established to a high degree of accuracy .    in order to facilitate such long - chain analyses , we study maps with relatively low resolution , @xmath0 , but with properties corresponding to a consistently down - scaled _ wmap_-type experiment .",
    "specifically , we generate a cmb sky from the best - fit _ wmap _ running index spectrum , and convolve this sky with modified version of the _ wmap _ beams .",
    "the beams are made four times wider by replacing their original legendre transform with @xmath98 .",
    "the noise components are generated by degrading the original _ wmap _ noise rms maps , where @xmath99 is the average sensitivity of the various bands , and @xmath100 is the number of observations for each pixel @xmath101 . ] to @xmath102 by simple averaging over pixels in the healpix nested organization .",
    "thus , the noise per low - resolution pixel in our simulated maps is about the same as that for each high - resolution pixel in the full - sized _ wmap _ data .",
    "the signal - to - noise ratio is therefore downscaled to the appropriate resolution , a fact which will be important when studying the relationship between the correlation length and the signal - to - noise ratio .",
    "we also want to study the effect of residual mono- and dipoles on the cosmological power spectrum , and we therefore add a random mono- and dipole contribution with an artificially large amplitude ( on the order of tens to a hundred mk ) to the signal plus noise map .",
    "the reconstructed values are then later compared with the exact input values .",
    "finally , we generate a degraded mask to match this resolution , based on the _ wmap _ kp2 mask as defined by @xcite .",
    "this mask is downgraded to @xmath102 by requiring that all high - resolution sub - pixel within a @xmath102 pixel ( again , in the healpix nested organization ) are included by the original mask .",
    "thus , this mask is very slightly expanded compared to the actual kp2 mask .      in the first test ,",
    "we apply the commander and magic codes to one single band from the data set described above , namely to the v1 band .",
    "commander was run for 100000 samples , while magic was run for 4000 , with the main goal of comparing the codes , verifying that they produce identical output .",
    "the results from this exercise are shown in figure [ fig : sim_hist ] .",
    "the black probability densities show the commander results , while the red histograms show the magic results .",
    "the agreement is striking , and this is a strong confirmation that the codes work as expected , and that the minor differences in implementational details discussed earlier do not affect the scientific results .",
    "the dashed lines show the true , underlying cmb power spectrum value , which should theoretically coincide with the peaks of the histograms , in the limit of full sky coverage and no noise . at low @xmath87 s",
    ", we see that this is indeed the case .",
    "here it is also worth recalling that we added artificially large mono- and dipole components to the simulations ( several orders of magnitudes larger than what is realistic ) , and this does still not compromise the results .    in figure",
    "[ fig : sim_spectrum ] we have plotted the full spectrum computed from the 100000 sample run .",
    "the input spectrum is marked in red , the ensemble - averaged spectrum in blue , and the maximum likelihood solution from the gibbs sampler in black .",
    "the gray bands indicate 1 and @xmath103 confidence regions for the power spectrum .    at low @xmath87 s",
    "the discrepancy between the estimated and input spectra is primarily due to the galactic cut , while at high @xmath87 s it is primarily due to noise . in particular",
    ", we see that the maximum likelihood estimate actually drops to zero for many of the low signal - to - noise ratio modes , which , again , is the expected behavior for a maximum likelihood estimator in the noise - dominated regime .    in figure",
    "[ fig : covarmat ] we plot two different correlation matrices , each on the form @xmath104 the averages are taken over the 100000 samples in the markov chain described above .",
    "the left panel shows the correlation matrix of the sampled power spectra , which are basically uncorrelated by construction , while the right panel shows the correlation matrix of the power spectra , @xmath105 , computed from the sampled maps , @xmath11 .",
    "the latter matrix is related to the correlation matrix of the maximum likelihood power spectrum found by maximizing the posterior , and mainly describes mode - mode coupling due to the cut sky on these scales .",
    "finally , in figure [ fig : md_results ] we plot the distributions of the mono- and dipole samples and compare them to the input values , marked by dashed , vertical lines . although there certainly is a discrepancy between the distribution modes and the input values , the overall rms values are very small , on the order of 5@xmath106 , and consistent with the fluctuation level expected for a single realization .",
    "further , there is some coupling between the mono- and dipole modes due to the galactic cut , which could be important .",
    "however , since our sole interest in these components lies in removing them , rather than estimating them , this is not an important problem for our purposes .",
    "in fact , given the very small impact of these very large mono- and dipole components , we feel confident that the cosmological low-@xmath87 spectrum is not compromised by mono- and dipole issues .",
    "we now turn to the issues of convergence , correlation length and burn - in time , all of which must be thoroughly understood in order to design and optimize a real - world analysis properly .",
    "the problem can be plainly stated as follows : how many gibbs samples do we need to estimate the power spectrum with sufficient accuracy in order to be limited by non - algorithmic issues ? as we will see , the answer depends intimately on which angular scales we wish to consider , a conclusion which is most easily seen by going back to the gibbs sampling scheme .",
    "the algorithm works as follows : first we assume some arbitrary ( but hopefully reasonable ) power spectrum , and compute a wiener - filtered map based on that spectrum .",
    "then we add a fluctuation term which replaces the power lost both to noise and to the galactic cut .",
    "the sum of those two terms mimics a full - sky , noiseless map with a power spectrum determined by the data in the high signal - to - noise regime , and by the assumed power spectrum in the low signal - to - noise regime . from this full - sky power spectrum",
    "we then draw a new spectrum , which subsequently is taken as the input spectrum for the next gibbs iteration .",
    "the crucial point is that the random step size in the final stage is determined by the cosmic variance alone .",
    "our goal is to probe the full probability distribution which includes both noise _ and _ cosmic variance . in the high signal - to - noise regime , the difference does not matter .",
    "sequential gibbs samples are therefore for all practical purposes uncorrelated .",
    "the opposite is true in the low signal - to - noise regime : since the distance between the two samples is determined by the cosmic variance , while the full distribution is dominated by the much larger noise variance , two sequential samples will be strongly correlated .",
    "this problem is a severe limitation for the gibbs sampling technique in its current formulation .",
    "it makes it very expensive to probe the low signal - to - noise regime completely .",
    "the gibbs sampling technique is only a special case of the more general metropolis - hastings framework .",
    "other sampling schemes may be devised which break the correlation between neighboring samples",
    ". this will be the topic of a future publication , and for now our main goal is to quantify this effect , rather than eliminate or minimize it .",
    "we take advantage of the low - resolution simulations in order to quantify these correlations .",
    "specifically , we consider the power spectrum values at constant @xmath87 in the markov chain as independent functions , and study the correlations in these chains as a function of @xmath87 .",
    "the statistic we choose for this study is a simple auto - correlation function , @xmath107 here @xmath108 is the distance in the chain measured in number of iterations .",
    "such functions are plotted in figure [ fig : sim_corlength_a ] for six different @xmath87 s , computed from a new commander chain consisting of 3800 samples , including all eight bands .",
    "as expected , the correlations become stronger as @xmath87 increases , or , equivalently , as the signal - to - noise ratio decreases . in this particular case ,",
    "the signal - to - noise ratio is unity at approximately @xmath109 , and therefore the spectrum is limited by cosmic variance at smaller @xmath87 s .",
    "this translates into a very short correlation length for @xmath110 in this case , and consequently into a high efficiency in terms of independent samples . on the other extreme ,",
    "the correlation length at @xmath111 is very , very long , and with only 3800 samples in the chain , we have only a very few independent samples from which to form our power spectrum estimate",
    ".    we can take this exercise one step further and define a typical correlation scale for each @xmath87 , by computing the scale at which the correlation function drops under , say , 0.1 . in figure",
    "[ fig : sim_corlength_b ] we have plotted this correlation length directly as a function of the signal - to - noise ratio , and from this plot there seems to be a well - defined relationship between these two quantities .",
    "in fact , we will use this relation to estimate how many samples we need in the actual _ wmap _ analysis later on .",
    "for now we note that with 3800 samples , as in the above case , we have about 200 independent samples at a signal - to - noise ratio of 0.6 , which corresponds to @xmath112 . in other words , it would be rather optimistic to believe in the power spectrum based on these samples at @xmath87 s higher than , say , 110 .",
    "another lesson to be learned from these plots is that the correlation length increases very rapidly with @xmath87 , once entering the low signal - to - noise regime .",
    "this is an important point to realize when desiging a new analysis : probing the low signal - to - noise regime with the current implementation of the gibbs sampling algorithm is extremely expensive",
    ". it may therefore often be desirable to limit @xmath66 to the @xmath87 corresponding to a signal - to - noise ratio of , say , 0.5 or 0.25 . the saved cpu time . ]",
    "may then be spent on producing more independent samples in the high and intermediate signal - to - noise regimes . however , truncating the system this way does modify the global solution , and care must therefore be taken with respect to the highest @xmath87 s . in general , the larger the sky cut , the more high-@xmath87 modes will have to be discared from the final power spectrum , since mode - mode couplings spread the sharp @xmath87-space cut - off into a wide range of multipoles . in practice , it is convenient to pre - define some range of @xmath87 s of interest , and then increase @xmath66 until that range becomes stable .    in figure",
    "[ fig : sim_corlength_c ] we have plotted the ratio of the magic correlation length to the commander correlation length , and here it is seen , as noted earlier , that the magic correlation length is typically a factor of 1.52 longer at low @xmath87 s , resulting in a smaller number of independent samples of the same factor . of course , this is both caused and made up by the fact that magic handles the incomplete sky coverage differently than commander .",
    "since magic obtains convergence in the cg search roughly three times faster than commander ( using a very crude preconditioner ) , the codes do perform quite similarly in terms of total cpu time per independent sample .    finally , we turn to the issue of burn - in time .",
    "although the theory of gibbs sampling guarantees us that the samples will converge toward being samples from the joint distribution density , it does not tell us when such convergence is obtained , and this must therefore be established by experiments .",
    "we study this issue through a simple exercise : once again we utilize the simulated data described above , but this time we choose a first power spectrum guess which is exactly three times larger than the true spectrum",
    ". then we run the algorithms for a number of iterations , and plot the power spectrum samples as a function of iteration count , @xmath17 .",
    "the results from this exercise are shown in figure [ fig : burn_in ] , in the form of @xmath113 note that the spectra have been averaged with a window width of @xmath114 , making it easier to see the overall trends .",
    "the conclusion to be drawn from this plot seems clear : a poor initial guess can invalidate a large number of samples , and , in particular , a weak estimate of the low signal - to - noise regime is very expensive to correct .",
    "this can potentially pose a serious threat to our main parallelization scheme , which is based on many independent short chains , rather than one long chain .",
    "for this reason , the gibbs sampling approach in its current form may not be particularly well suited as the only estimator for a new experiment . a faster method , such as master @xcite , is therefore suggested to provide an initial guess for the gibbs samplers .",
    "once an approximate power spectrum is established , the gibbs sampling process is already within the appropriate range , and only a few samples need to be discarded , if any at all .",
    "however , we do not need to rely blindly on the first guess , since a poorly chosen starting point would lead to a systematic drift in the gibbs chains which should be easily detectable .",
    "in this section we turn to high - resolution simulations , and undertake a full - scale _ wmap_-type analysis .",
    "the simulations in this case are prepared in the same way as in the low - resolution case , except with full - scale input data , and no inclusion of mono- and dipole components .",
    "the main limitation in this case is cpu time , and extremely long chains are simply not feasible .",
    "instead , we run many independent chains in parallel , each producing only a small number of samples , as discussed in section [ sec : parallelization ] .",
    "the analysis is designed to match the analysis of the first - year _ wmap _ data presented by @xcite .",
    "specifically , we generate a random sky with the healpix utility synfast , and convolve this sky with the beams corresponding to each of the eight _ wmap _ bands ( q12 , v12 , w14 ) .",
    "next we add either white noise ( with the appropriate @xmath115 patterns for each band ) or correlated noise ( as generated by the _ wmap _ team ) to these cmb maps .",
    "finally , the _ wmap _ kp2 mask @xcite which excludes point sources is imposed on the data , leaving 85% of the sky available for analysis . at this stage , the gibbs sampler is run over 12 independently initialized chains for 60 iterations , for a total of 720 samples .",
    "we point out that the numbers of observations per pixel , @xmath115 , in the correlated noise files supplied by the _ wmap _",
    "team do not match perfectly those of the observed map files , and unless the appropriate @xmath115 patterns are used in each case , a noise excess at @xmath116 is observed . the white noise level , however , are identical for the two patterns , and so this difference does not have a significant impact on the results , as long as one is aware of the difference .    in figure",
    "[ fig : high_res_spec ] we have plotted the power spectra from the multiple - chain analysis , including white noise in the left panel and correlated noise in the right panel .",
    "overall , we see that the agreement between the realization specific spectrum ( red line ) and the maximum likelihood solution ( black line ) found by the gibbs sampler is quite good , and there is no detectable bias in any parts of the spectrum .",
    "next , in figure [ fig : wn_vs_cn_hist ] we plot a few selected histograms of the power spectra , comparing the white ( black histograms ) and correlated ( red histograms ) noise results more directly . as we see , the agreement is generally very good , and in particular , the three upper panels clearly demonstrate that the low level of correlated noise present in the _ wmap _ data do not compromise the low-@xmath87 spectrum .    at higher @xmath87 s",
    ", a small shift may be seen between the two distributions , which is most likely due to the fact that the noise realizations are different .",
    "we made similar plots for neighboring multipoles , finding that the absolute levels of discrepancy seen in figure [ fig : wn_vs_cn_hist ] are quite typical for these angular scales , and the signs of the shifts are random .",
    "thus , the differences does not seem to be indicative of a systematic bias .    by studying simulated data",
    ", we have thus explicitly demonstrated that the gibbs sampling technique is able to analyze the mega - pixel _ wmap _ data set properly , and that neither correlated noise nor incomplete sky coverage compromise the scientific results significantly .",
    "all in all , the feasibility of this approach with respect to current and future data sets has been firmly established .",
    "we have implemented two independent versions of the gibbs sampling technique introduced by @xcite and @xcite , and tested the performance and behavior of the codes thoroughly . in particular , we have explicitly verified that the two implementations produce identical output , despite a few algorithmic differences , demonstrating that these algorithmic choices do not affect the scientific results .",
    "further , we applied the codes to simulated data with controlled properties , and found the output to agree very well with the theoretical expectations . in doing so",
    ", we also demonstrated the feasibility of the method for high - resolution applications .",
    "one of the main goals of these simulations was to build up intuition about the phenomenological behavior of the gibbs sampling algorithm , focusing in particular on issues such as the correlation length of the markov chains , and the burn - in and convergence time . through these experiments",
    ", we found that the signal - to - noise ratio is by far the most constraining factor to the algorithm in its current form .",
    "the step size between two consecutive samples is determined by the cosmic variance alone , while the overall posterior density incorporates noise uncertainty as well .",
    "thus , in the low signal - to - noise regime subsequent samples are highly correlated , and the effective number of independent samples is dramatically reduced . in its current formulation ,",
    "the method is therefore most efficient at scales for which the signal - to - noise ratio is higher than , say , 0.5 , or perhaps up to @xmath117400 for the first - year _ wmap _ data . on the other hand ,",
    "the gibbs sampler is only a special case from a more general framework , and other sampling schemes may be introduced in order to break these correlations",
    ". this will be the topic of a future publication .",
    "perhaps the single most appealing feature of the gibbs sampling approach , is its ability to incorporate most real - world complications in a statistically consistent manner . in this paper",
    "we have demonstrated how to handle incomplete sky coverage and unknown mono- and dipole contributions , which are the most important point for the analysis of the first - year _ wmap _ data , but future extensions will also include polarization , more sophisticated treatment of foregrounds , internal sampling over cosmological parameters , inclusion of asymmetric beams , and statistically consistent handling of @xmath36 noise .",
    "in fact , the gibbs sampling approach is not simply a maximum likelihood method , but rather a machinery facilitating an optimal , global analysis . needless to say ,",
    "the computational challenges are considerable , but with a scaling equivalent to that of map making ( which has to be performed in any approach currently proposed ) , this method may just be able to do the job .",
    "a second goal of this paper was to prepare for the actual analysis of the _ wmap _ data , by applying the algorithm to simulated data with similar properties .",
    "specifically , we showed that the estimated power spectrum is unbiased , and that even the lowest - order multipoles are not compromised by the either the galactic cut , given that the foreground correction method presented by @xcite is adequate , or by the ( low levels of ) correlated noise present in the data .",
    "thus , no further sophistications beyond those presented in this paper seem necessary in order to perform a valid bayesian analysis of the first - year _ wmap_. the scientific results from this analysis are presented in a companion letter by @xcite .",
    "we acknowledge use of the healpix software ( grski , hivon & wandelt 1998 ) and analysis package for deriving the results in this paper .",
    "we also acknowledge use of the legacy archive for microwave background data analysis ( lambda ) .",
    "h.  k.  e.  and p.  b.  l.  acknowledge financial support from the research council of norway , including a ph .",
    "d.  studentship for h.  k.  e. this work has also received support from the research council of norway ( programme for supercomputing ) through a grant of computing time .",
    "this work was partially performed at the jet propulsion laboratory , california institute of technology , under a contract with the national aeronautics and space administration .",
    "this work was partially supported by an ncsa faculty fellowship for b.  d.  w. this research used resources of the national energy research scientific computing center , which is supported by the office of science of the u.s .",
    "department of energy under contract no .",
    "de - ac03 - 76sf00098 .",
    "bennett , c.  l.  et al .   2003a , , 148 , 1 bennett , c.  l.  et al .   2003b , , 148 , 97 bond , j.  r. , jaffe , a.  h. , & knox , l. 1998 , , 57 2117 borrill , j. 1999 , , 59 , 027302 challinor , a.  d. , mortlock , d.  j. , van leeuwen , f. , lasenby , a.  n. , hobson , m.  p. , ashdown , m.  a.  j. , & efstathiou , g.  p. 2002",
    ", mnras , 331 , 994 de oliveira - costa , a. , tegmark , m. , zaldarriaga , m. , & hamilton , a. 2004 , , 69 , 063516 dor , o. , knox , l. , & peel , a. 2001 , , 64 , 083001 efstathiou , g. 2003a , , 343 l95 efstathiou , g. 2003b , , 346 , l26 eriksen , h.  k. , banday , a.  j. , grski , k.  m. , & lilje , p.b .",
    "2004 , , in press , [ astro - ph/0403098 ] grski , k.  m. 1994 , 430 , l85 grski , k.  m. 1997 , preprint [ astro - ph/9701191 ] grski , k. m. , hivon , e. , & wandelt , b. d. , 1999 , in evolution of large - scale structure : from recombination to garching , ed .",
    "j. banday , r. k. sheth , & l. n. da costa ( garching , germany : european southern observatory ) , 37 golub , g. h. & van loan , c. f. 1996 , matrix computations , the johns hopkins university press    hansen , f. k. , & grski , k. m. 2003 , , 343 , 559 hansen , f. k. , grski , k. m. , & hivon , e. 2002 , , 336 , 1304 hinshaw , g.  et al .",
    "2003 , , 148 , 135 hivon , e. , g ' orski , k.  m. , netterfield , c.  b. , crill , b.  p. , prunet , s. , & hansen , f.  2002 , , 567 , 2 jewell , j. , levin , s. , & anderson , c. h. 2004 , , 609 , 1 odwyer , i. j. et al .",
    "2004 , submitted [ astro - ph/0407027 ] oh , s. p. , spergel , d. n. , & hinshaw , g. 1999 , , 510 551 page , l. , et al .",
    "2003 , , 148 , 39 shewchuk , j. r. 1994 , http://www.cs.cmu.edu/@xmath118quake-papers/painless-conjugate-gradient.ps    slosar , a. , & seljak , u. 2004 , , in press [ astro - ph/0404567 ] spergel , d.  n. et al .",
    "2003 , , 148 , 175 tegmark , m. 1997 , , 55 , 5895 van leeuwen , f. et al .",
    "2002 , , 331 , 975 wandelt , b. d. 2003 , econf , c030908 , welt001 , [ astro - ph/0401623 ] wandelt , b. d. , & hansen , f. k. 2003 , , 67 , 023001 wandelt , b. d. , hivon , e. , & grski , k. m. 2001 , , 64 , 083003 wandelt , b. d. , larson , d. l. , & lakshminarayanan , a. 2004 , [ astro - ph/0310080 ]"
  ],
  "abstract_text": [
    "<S> we revisit a recently introduced power spectrum estimation technique based on gibbs sampling , with the goal of applying it to the high - resolution _ wmap _ data . in order to facilitate this analysis , </S>",
    "<S> a number of sophistications have to be introduced , each of which is discussed in detail . </S>",
    "<S> we have implemented two independent versions of the algorithm to cross - check the computer codes , and to verify that a particular solution to any given problem does not affect the scientific results . </S>",
    "<S> we then apply these programs to simulated data with known properties at intermediate ( @xmath0 ) and high ( @xmath1 ) resolutions , to study effects such as incomplete sky coverage and white vs.  correlated noise . from these simulations </S>",
    "<S> we also establish the markov chain correlation length as a function of signal - to - noise ratio , and give a few comments on the properties of the correlation matrices involved . </S>",
    "<S> parallelization issues are also discussed , with emphasis on real - world limitations imposed by current super - computer facilities . </S>",
    "<S> the scientific results from the analysis of the first - year _ wmap _ data are presented in a companion letter . </S>"
  ]
}