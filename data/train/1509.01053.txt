{
  "article_text": [
    "when solving classification problems in a supervised or semi - supervised fashion , it is always necessary to somehow label samples from the training data .",
    "incorporating these labeled examples into the training process enables the model to assign a class label either directly to an unknown example or to the implicit category that the example belongs to .",
    "a common approach is applying labels to all or to a subset of the training examples prior to training .",
    "this process can be very time - consuming , especially if there are many examples and a large subset of them should be labeled . using semi - supervised learning ,",
    "it is possible to train a sufficient model while having only a subset of the training data enriched with labels @xcite .",
    "however , it is still necessary to label some samples prior to training .",
    "+ this paper presents an alternative approach for the classification of images , which works in the reverse order : first , train a generative model of the data and afterwards apply labels to samples from the trained model .",
    "similar ideas have been pursued in the field of face recognition , e.g. by @xcite using unsupervised clustering prior to a manual labeling task , however , we want take a more general approach .",
    "reversing the order has some advantages over the classical way : first , it is possible to label more examples in a shorter period of time by showing the human labeler a constantly changing stream of model samples .",
    "second , it is possible to prevent the user from manually labeling examples similar to the ones that the model can already firmly classify . by trying to maximize the additional information in each new training example this aspect is similar to _ active learning_/_selective sampling _ proposed in @xcite .",
    "+ there are some caveats to this approach : if , at the time of the training , there is no label information , the parametrization of the training process must rely on metrics like the reconstruction error .",
    "also , the samples generated by the model must be human - interpretable in order to perform the labeling .",
    "+ using the mnist data set of handwritten digits , we show that the post - labeling approach is competitive to a semi - supervised training scenario .",
    "semi - supervised training is a hybrid of supervised and unsupervised training @xcite . in unsupervised training settings ,",
    "we try to find interesting structures in the a set @xmath0 consisting of @xmath1 training examples @xmath2 without explicitly assigning classes or labels to those structures , e.g. for clustering or statistical density estimation . in",
    "a supervised setting , we are interested in finding a mapping of a variable @xmath3 to another variable @xmath4 in a training set consisting of example pairs @xmath5 , i.e. we are facing a classification task . in order to solve a supervised learning problem , it is possible to use discriminative or generative models . a discriminative model tries to directly learn the relationship between @xmath6 and @xmath7 , often by trying to directly estimate the conditional probability @xmath8 of a label given a data point .",
    "using a generative model , the approach is more related to the unsupervised case : by learning the structure of the data in @xmath0 , generative models try to estimate the class - conditional probability @xmath9 or the joint probability @xmath10 and retain the conditional probability @xmath8 using bayes rule @xcite . given a well - trained generative model , it is therefore often possible to draw samples from the model that resemble training data , as they come from the same probability distribution .",
    "recently , a class of generative models called restricted boltzmann machines has been widely used for discrimination tasks such as digit classification @xcite , phone recognition @xcite or document classification @xcite .",
    "restricted boltzmann machines are stochastic , energy - based neural network models @xcite .",
    "an rbm consists of a visible layer @xmath11 and a hidden layer @xmath12 , connected by weights @xmath13 from each visible neuron @xmath14 to each hidden neuron @xmath15 , forming a bipartite graph .",
    "they can be trained to model the joint distribution of the data , which is presented to the visible layer , and the hidden neurons by adjusting the weights @xmath13 and biases @xmath16 and @xmath17 .",
    "the neurons of the hidden layer are often referred to as _ feature detectors _ , as they tend to model features and patterns occurring in the data , thus capturing the structure in the training data .",
    "the probability @xmath18 that a hidden neuron @xmath15 is active depends on the activation of the visible units @xmath14 and the bias of the hidden neuron @xmath19 , thus @xmath20 , with @xmath21 being the logistic function @xmath22 .",
    "the probability @xmath23 that a visible unit @xmath14 is active given the hidden layer activations @xmath15 is , in turn , equal to @xmath24 , with @xmath16 being the bias of neuron @xmath14 .",
    "calculating @xmath25 and @xmath26 is therefore easy and efficient .",
    "+ the energy function    @xmath27    defined on the rbm associates a scalar energy value for each configuration of visible neurons @xmath11 and hidden neurons @xmath12 .",
    "the probability @xmath28 of a joint configuration is proportional to its energy :    @xmath29    it is now possible to marginalize over all hidden configurations to obtain the probability @xmath30 of a visible vector ( see @xcite for details ) .      to train an rbm on a data set ,",
    "it is necessary to increase the probability (= lower the energy ) of training data vectors and decrease the probability of configurations that do not resemble training data .",
    "this can be done by updating the weights @xmath13 following the log likelihood gradient @xmath31 .",
    "it can be shown that this partial derivative is a sum of two terms usually referred to as the positive and negative gradient , which is why the training algorithm is called _ contrastive divergence _ ( cd ) @xcite .",
    "the resulting update rule for the weights is    @xmath32    with @xmath33 being the expected value that @xmath14 and @xmath15 are active simultaneously .",
    "the first term ( positive gradient ) is calculated after initializing the visible layer with a data vector from the training set and subsequently activating @xmath12 given @xmath11 .",
    "the second term ( negative gradient ) is calculated when the model is running freely , that is after a potentially infinite number of gibbs sampling steps @xmath34 . as the negative gradient is intractable , it is often approximated using only @xmath35 steps of sampling after initializing the visible neurons with data ( cd - n ) . in practice , this approximation works pretty well ( see e.g. @xcite ) .",
    "+ to learn a labeled data set , we simply extend the visible layer to also capture label data ( e.g. a one - hot vector representing the label classes ) and add an extra set of label weights @xmath36 connecting the @xmath37 labels to the hidden neurons .",
    "the learning rule for the label weights and biases remains unchanged .",
    "figure [ model_ablauf ] compares the steps of the standard approach to train a classification rbm with the post - labeling approach pursued in this paper .",
    "the standard approach first collects training data and then manually applies labels to the data , or to a subset of the data .",
    "afterwards , a ( semi- ) supervised model is trained on labeled data , simultaneously learning both the regular weights @xmath13 , connecting the visible neurons to the features , and label weights @xmath36 , connecting the label neurons the features .",
    "+ with post - labeling , we change the order : after collecting data , we train an rbm in an unsupervised fashion on the unlabeled data , thus only updating the regular weights @xmath13 .",
    "afterwards , we let the model generate samples and apply labels to those samples .",
    "we then use the labeled samples to update the label weights @xmath36 in a supervised way .    0.2 in        -0.2 in      we used the mnist database of handwritten digits for our experiments @xcite .",
    "the data set contains 60,000 labeled training examples and 10,000 labeled test examples of 28 * 28 pixel images of handwritten digits in ten classes .",
    "when performing the semi - supervised or unsupervised learning tasks , we remove the labels .",
    "we perform the post - labeling tests on a restricted boltzmann machine with 784 ( = 28 * 28 ) visible neurons @xmath14 and 225 hidden neurons @xmath15 ( feature detectors ) . in order to validate the competitiveness of the post - labeling approach ,",
    "we compare it to an rbm of the same size - with the visible layer extended by @xmath38 label neurons - trained on labeled data in a supervised ( all data labeled ) or semi - supervised ( only a subset labeled ) fashion . during the initial training , the post - labeling rbm thus only has one set of weights @xmath13 , whereas the classic rbm has a second set of label weights @xmath36 . + we train both models networks using the training algorithm cd-10 and 50,000 images from the training set .",
    "the remaining 10,000 examples are held out in order to find feasible parameters ( such as the learning rate ) for the supervised model .",
    "] we use the reconstruction error ( sum of squared pixel - wise differences between data and one - step reconstruction ) to measure the training progress of the unsupervised model trained on data without labels .",
    "this is one of the main caveats of the post - labeling method : the reconstruction error can be misleading , especially when learning parameters are adapted during the training @xcite .",
    "also , the reconstruction errors between different learning algorithms can differ without giving a proper hint to model quality .",
    "the goal of the post - labeling phase is to find proper label weights @xmath36 . for this purpose",
    ", we developed a gui that shows samples from the model to a human labeler , who can activate the corresponding class using the keyboard or mouse ( see fig .",
    "[ screenshot_labeltrainer_gui ] ) .",
    "we initialize the visible layer with a randomly chosen ( unlabeled ) image from the training set and then let the model perform repeated gibbs sampling between the visible and the hidden layer of the underlying rbm .",
    "this leads to a slight deformation of the shown image in each sampling step , while the model traverses along a low - energy ravine in the energy landscape .",
    "if the model produces good reconstructions , the user observes slowly changing samples that belong to the same class , and potentially class transitions ( see figure [ samples_merged ] ) .",
    "the displayed image is constantly updated at a speed of approx .",
    "6 frames / second , which is adjustable in the gui .",
    "the user s task is to activate the corresponding class as soon as the observed image firmly resembles one of the classes .",
    "the selected class label stays active until the user presses the `` unsure '' button or another class button .",
    "this leads to a high number of labeled samples , as the display resembles a video of `` morphing '' digits .",
    "after 30 gibbs iterations , the visible neurons are initialized with the next random image from the training set . +    0.2 in        -0.2 in    0.2 in    ) .",
    "the first row shows a constantly changing eight ( which might transition into a three in one of the subsequent images ) , the second row shows a transition from a nine to a seven . ]",
    "-0.2 in      there are two possibilities for training the label weights @xmath36 .",
    "the first is to perform online learning during the post - labeling phase .",
    "whenever a label is activated by the user , we update the label weights proportional to an approximation of the positive and the negative gradient at the same time . in this case , the positive gradient is @xmath39 , with @xmath40 being the @xmath37th class label activation ( as given by the user ) and @xmath15 the probability of the @xmath41th feature being active .",
    "the negative gradient approximation is @xmath42 with @xmath43 being the probability of the @xmath37th label being active ( as reconstructed by @xmath12 ) .",
    "thus , we strengthen connections from active features to the correct label and penalize connections from active features to the potentially wrong , reconstructed label .",
    "the biases are updated accordingly .",
    "we activate online learning in the gui by default .",
    "alternatively , it is possible to train the label weights @xmath36 in an offline fashion after the manual labeling of model samples in the gui .",
    "we save all frames labeled in the gui and used them to train the label weights @xmath36 using standard cd-1 .",
    "again the update for the label weights is proportional to @xmath44 .",
    "the only difference to the online learning is that we can cycle through the labeled training set multiple times , thus the negative gradient may change during the course of the training , resulting in a better approximation .",
    "the weights @xmath13 remain unchanged during the learning phase .",
    "it is possible to improve the ease of use of the labeling gui and the resulting labeling quality using a few tweaks .",
    "first , we can automatically control the speed of the image stream that is presented the user .",
    "after a few minutes of training , the model already assings a reasonably high probability to the correct class for `` common '' samples ( online learning is activated ) . on the contrary , if the current sample is visually distant from the previously labeled samples , the model does nt assign a high probability to any label - it is unsure which label to pick for this example .",
    "thus , it is possible to decrease the display speed for samples that seem unknown , thus allowing the user to make a more precise pick of the label ( especially on class transitions ) .",
    "we implemented this tweak in the gui as `` autospeed '' and activated it by default ( see fig .",
    "[ screenshot_labeltrainer_gui ] ) .",
    "+ analogously , it is possible to bias the choice of samples from the training set to initialize the image ( active sampling ) .",
    "if the probability for a label is very high ( @xmath4580% ) the gui can directly skip the example and try the next one .",
    "although this approach channels the user s attention to samples where the model is still unsure , it deprives the learning process of the chance to detect confident misclassifications .",
    "thus this technique should nt be used right away but only after some training .",
    "we implemented this `` do nt show if sure '' concept in the gui and asked users to activate it after the first five minutes of training .",
    "+ we also added the possibility to automatically undo the last five update steps if the user changes his opinion on a displayed image ( class changes and changes from a class to _ unsure _ ) .",
    "initial tests showed that when running on higher speeds , the reaction time of a user usually allows some wrong labels to slip in in case of a class transition or image degradation .",
    "+ if the reconstructions of the model are too stable to produce a constantly changing stream , it is possible to implement a set of `` fast weights '' as in @xcite .",
    "those fast weights can add a temporary penalty to the areas of low energy just visited , thus forcing the model to wander around .",
    "we did nt implement this tweak as of now .",
    "we test both the rbm trained with the standard ( semi- ) supervised approach as well as the post - labeling rbm using the mnist test set with 10,000 labeled images",
    ". + figure [ results_labeled ] shows the resulting test set error rate of the rbm trained using the standard approach .",
    "having only 500 of 50,000 images labeled results in a classification error of approx 14% .",
    "on increasing the number of labeled images , the error rate drops quickly and reaches its minimum of approx .",
    "4% on a fully labeled training set . + figure [ results_postlabeling_on_and_offline ] shows the test set error of the rbm trained using the post - labeling approach .",
    "both online learning and offline learning results show high initial error rates and a fast drop on increasing gui time .",
    "however , the classification error of epoch - wise offline learning is constantly smaller .",
    "it reaches a performance of around 6.2% error after 4200 seconds of labeling model samples .",
    "+ although our goal is to compare ( semi- ) supervised and post - labeling approach , we do not plot the results in a single figure because they do not share a common x axis . in order to compare the results",
    ", we have to make an estimation on the time required to label static images .",
    "test showed that 1.5 - 2 seconds per labeled image is a realistic labeling rate . given this labeling rate , the standard and the post - labeling approach show similar error rates given the labeling time .",
    "when spending 2,000 seconds on labeling , both approaches show a test set error around 8% .",
    "accordingly , the error rates for 4,000 seconds labeling time are around 6.5% . +    0.2 in        -0.2 in    0.2 in        -0.2 in      the results shown above",
    "are biased in two ways .",
    "first , our initial parameter choice for the the unsupervised model was influenced by our background knowledge from previous supervised tests with the mnist data set . on a genuinely new training set",
    ", we would nt possess such knowledge and would have to rely on the reconstruction error only ( see section [ models ] ) . on the other hand ,",
    "our results are biased by the fact that we use the labels of the official test set , which almost certainly come from a different distribution than the ones given by our labelers during training ( consider the ambiguity of sevens and ones or fours and nines , given the cultural background ) .",
    "if all labels ( test and training ) origin from the same distribution , the test error rate will most probably be lower .",
    "the displayed results of the supervised model can profit from this fact , as opposed to the results of the post - labeling model . + it is not known whether the mnist labels were double - checked in order to get error - free labels ( at least for the non - ambiguous cases ) . if there is more than one labeling pass , the required time increases accordingly in the standard approach .",
    "the results show that the post - labeling approach is , in gereral , competitive to the standard approach in terms of the resulting classification quality .",
    "it is likely that , by following the low - energy ravines , the model displays samples that resemble a class , but are not part of the training data .",
    "these samples can then be labeled by the gui user .",
    "+ on the other hand , the post - labeling approach has a number of drawbacks . as mentioned above , the initial unsupervised training must rely on metrics such as the reconstruction error . also , the quality of the labeled model samples is not as high as the quality of labeled real - world examples .",
    "as the displayed image is constantly changing , there are almost certainly some mislabeled or low - quality samples .",
    "nevertheless it should be possible to use the labeled samples as a whole to train the label weights of a different model than the one they originated from , as most of them genuineley represent the classes .",
    "+ another drawback of this approach is that it is crucial to have meaningful reconstructions of the original input .",
    "they have to be clearly distinguishable from one another by a human observer , and more or less stable on repeated gibbs sampling .",
    "especially when dealing with real - world ( and thus real - valued ) images , this sets a high standard for the unsupervised model .",
    "the approach is , however , independent of the model type and can , e.g. , be used with higher - order boltzmann machines to model covariances in the dataset to better model real - world images @xcite .",
    "+ the approach can , in principle , be combined with classical semi - supervised learning , e.g. by initializing the label learning procedure with some labeled images in the training set or to get a better understanding of parameter settings by using a small labeled validation set .",
    "we proposed a different approach for training a classification model . using the mnist set of handwritten digits",
    ", we showed that it is feasible to train an rbm on unlabeled data first and subsequently label model samples using a gui .",
    "this approach presents an alternative to semi - supervised learning , but does not reach the classification performance of a model trained on fully labeled data given the tested labeling times .",
    "an interesting question for further research is whether it is possible to also improve the model quality with respect to the data using the post - labeling gui .",
    "that is , to capture user input during the interactive learning phase ( such as `` i see only noise '' ) to improve the quality of the weights @xmath13 connecting the visible and the hidden neurons .                  geoffrey hinton and ruslan salakhutdinov .",
    "discovering binary codes for documents by learning deep generative models .",
    "_ topics in cognitive science _ , 30 ( 1):0 7491 , 2011 .",
    "issn 1756 - 8765 .",
    "doi : 10.1111/j.1756 - 8765.2010.01109.x .",
    "url http://dx.doi.org/10.1111/j.1756-8765.2010.01109.x .",
    "p.  smolensky .",
    "_ parallel distributed processing : explorations in the microstructure of cognition _ , volume  1 , chapter information processing in dynamical systems : foundations of harmony theory , pages 194281 . mit press , cambridge , ma , usa , 1986 .",
    "yuandong tian , wei liu , rong xiao , fang wen , and xiaoou tang .",
    "a face annotation framework with partial clustering and interactive labeling . in _ ieee conference on computer vision and pattern recognition _ , 2010 .",
    "tijmen tieleman .",
    "training restricted boltzmann machines using approximations to the likelihood gradient . in _ proceedings of the 25th international conference on machine learning _ , icml 08 , pages 10641071 , new york , ny , usa , 2008 .",
    "isbn 978 - 1 - 60558 - 205 - 4 .",
    "doi : http://doi.acm.org/10.1145/1390156.1390290 .",
    "url http://doi.acm.org/10.1145/1390156.1390290 .",
    "tijmen tieleman and geoffrey hinton .",
    "using fast weights to improve persistent contrastive divergence . in _ proceedings of the 26th annual international conference on machine learning _ , icml 09 , pages 10331040 , new york , ny , usa , 2009 .",
    "isbn 978 - 1 - 60558 - 516 - 1 .",
    "doi : http://doi.acm.org/10.1145/1553374.1553506 .",
    "url http://doi.acm.org/10.1145/1553374.1553506 ."
  ],
  "abstract_text": [
    "<S> we propose an alternative method for training a classification model . using the mnist set of handwritten digits and restricted boltzmann machines , it is possible to reach a classification performance competitive to semi - supervised learning if we first train a model in an unsupervised fashion on unlabeled data only , and then manually add labels to model samples instead of training data samples with the help of a gui . </S>",
    "<S> this approach can benefit from the fact that model samples can be presented to the human labeler in a video - like fashion , resulting in a higher number of labeled examples . </S>",
    "<S> also , after some initial training , hard - to - classify examples can be distinguished from easy ones automatically , saving manual work .         * </S>",
    "<S> technical report in information systems + and business administration * +    ( 1,0)380    * johannes gutenberg - university mainz * + department of information systems and business administration + d-55128 mainz / germany + phone + 49 6131 39 22734 , fax + 49 6131 39 22185 + e - mail : sekretariat[at]wi.bwl.uni - mainz.de + internet : http://wi.bwl.uni-mainz.de + </S>"
  ]
}