{
  "article_text": [
    "in the history of using computer simulation as a research tool to study the physics of turbulence , the dominant approach has been to use spectral methods . direct numerical simulation ( dns )",
    "was introduced as a means to check the validity of turbulence theories _ directly _ from the equations of fluid dynamics @xcite .",
    "the idea that important features of turbulence are _ universal _ encouraged researchers to study the simplest of geometries , a periodic cubic volume of homogeneous , isotropic turbulent fluid . in this case , the simplicity and efficiency of a fourier - spectral method can not be matched .",
    "the largest direct numerical simulation of isotropic turbulence was conducted by ishihara et al .",
    "@xcite using @xmath0 grid points at a maximum taylor - microscale reynolds number @xmath1 .",
    "this record - breaking computation was done on _ earth simulator_a large vector machine with crossbar switch interconnect that can efficiently perform large - scale fft .",
    "the successive generations of supercomputers have not been so fft - friendly , and this record has not been surpassed even though the peak performance of supercomputers has increased nearly 50-fold since then .",
    "the record was matched for the first time in the u.s . by donzis et al .",
    "@xcite , running on 16 thousand cpu  cores of the ranger supercomputer in texas , a linux - cluster supercomputer with 16-core nodes .",
    "future high - performance computing systems will have ever more nodes , and ever more cores per node , but will probably not be equipped with the bandwidth required by many popular algorithms to transfer the necessary data at the optimal rate .",
    "this situation is detrimental to parallel scalability .",
    "therefore , it is becoming increasingly important to consider alternative algorithms that may achieve better sustained performance on these extremely parallel machines of the future .    in most standard methods of incompressible cfd",
    ", the greatest fraction of the calculation runtime is spent solving a poisson equation .",
    "equations of this type can be efficiently solved by means of an fft - based algorithm , a sparse linear solver , or a fast multipole method ( fmm ) @xcite . for the sake of our argument",
    ", we will not differentiate between fft - based poisson solvers and pseudo - spectral methods because they both rely on fft .",
    "the fast multipole method has not gained popularity due to the fact that it is substantially slower  depending on implementations , at least an order of magnitude slower  than fftand multigrid solvers when compared using a small cpu  cluster .",
    "we aim to show with our ongoing research that the relative performance of fmmimproves as one scales to large numbers of processes using gpu  acceleration .",
    "the highly scalable nature of the fmmalgorithm , among other features , makes it a top contender in the algorithmic toolbox for exascale systems .",
    "one point of evidence for this argument is given by the gordon bell prize winner of 2010 , which achieved 0.7 petaflop / s with an fmmalgorithm on 200k cores of the jaguar supercomputer at oak ridge national laboratory @xcite .",
    "in the previous year , fmmalso figured prominently at the supercomputing conference , with a paper among the finalists for the best paper award @xcite and the gordon bell prize in the price / performance category going to work with hierarchical @xmath2-body methods on gpu  architecture @xcite .",
    "the fmmalgorithm is well adapted to the architectural features of gpus , which is an important consideration given that gpus are likely to be a dominant player as we move towards exascale .",
    "the work presented in 2009winner in the price / performance category in great measure thanks to the ingenious and resourceful system design using gaming hardware  reported ( at the time of the conference ) 80 teraflop / s @xcite . that work progressed to an honorable mention in the 2010 list of awardees , with a performance of 104 teraflop / s @xcite .    at the level of the present work , where we present a 1.08 petaflop / s ( single precision ) calculation of homogeneous isotropic turbulence , the fmmmoves firmly into the arena of _ petascale _ gpu  computing .",
    "the significance of this advance is that we are now in a range where the fmmalgorithm shows its true capability .",
    "the excellent scalability of fmmusing over 4000 gpus is an advantage over the dominant fft - based algorithms .",
    "showcasing the fmmin the simulation of homogeneous isotropic turbulence is especially fitting , given that a years - old record there remains unchallenged .",
    "we not only match the grid size of the world record , but also demonstrate that using the fmmas the underlying algorithm enables isotropic turbulence simulations that scale to thousands of gpus . given the severe bottleneck imposed by the all - to - all communication pattern of the fftalgorithm ,",
    "this is not possible with pseudo - spectral methods in current hardware .",
    "the fmmis used here as the numerical engine in a vortex particle method , which is an approach to solve the navier - stokes equations using a the vorticity formulation of momentum conservation and a particle discretization of vorticity .",
    "this is not a standard approach for the simulation of turbulence and the vortex method is not yet trusted for this application .",
    "for this reason , we have made efforts to document the validation of our vortex - method code and , in a separate publication @xcite , compared with a trusted spectral - method code .",
    "we looked at various turbulence statistics , including higher - order velocity derivatives , and performed a parameter study for the relevant numerical parameters of the vortex method .",
    "that work provides evidence that the vortex method is an adequate tool for direct numerical simulation of fluid turbulence , while in the present work we focus on the performance aspects . for completeness , this sections gives a brief overview of the numerical methods .",
    "the vortex method @xcite is a particle - based approach for fluid dynamics simulations .",
    "the particle discretization results in the continuum physics being solved as an @xmath2-body problem .",
    "therefore , the hierarchical @xmath2-body methods that extract the full potential of gpus can be used for the simulation of turbulence . unlike other particle - based solvers for fluid dynamics , e.g. ,",
    "smoothed particle hydrodynamics @xcite , the vortex method is especially well suited for computing turbulent flows , because the vortex interactions seen in turbulent flows are precisely what it calculates with the vortex particles . in the vortex method ,",
    "the navier - stokes equation is solved in the velocity - vorticity formulation , using a particle discretization of the vorticity field .",
    "the velocity is calculated using the following equation , representing the biot - savart law of fluid dynamics : @xmath3 here , @xmath4 is the strength of vortex particles , @xmath5 is the green s function for the laplace equation and @xmath6 is the cutoff function , with @xmath7 the distance between the interacting particles , and @xmath8 the standard deviation of the gaussian function .",
    "the navier - stokes system is solved by a simultaneous update of the particle positions to account for convection , of the particle strengths to account for vortex stretching , and of the particle width to account for diffusion .",
    "the equation used to calculate the stretching term , @xmath9 , is : @xmath10 which was obtained by substituting the biot - savart equation ( [ eq : biotsavart ] ) for @xmath11 , and using the discrete form of the vorticity .",
    "finally , the diffusion update is calculated according to @xmath12 we perform a radial basis function interpolation for reinitialized gaussian distributions to ensure the convergence of the diffusion calculation @xcite",
    ". equations ( [ eq : biotsavart ] ) and ( [ eq : stretching ] ) are @xmath2-body interactions , and are evaluated using the fmm .",
    "we use a highly parallel fmmlibrary for gpus developed in our group , called ` exafmm ` , which is available under an open - source license and is described further below .      for the purpose of comparing with a spectral method",
    ", we used a code for homogeneous isotropic turbulence developed and used at the center for turbulence research of stanford university @xcite .",
    "the code is called ` hit3d ` and is available freely for download .",
    "it uses a spectral galerkin method in primitive - variable formulation with pseudo - spectral methods to compute the convolution sums . for the fft , it relies on the ` fftw ` library and it provides parallel capability for cpu  clusters using mpi .",
    "parallelization of the fftis accomplished by a domain - decomposition approach , illustrated by the schematic of figure [ fig : spectral_fft ] .",
    "domain decomposition is applied in two spatial directions , resulting in first `` slabs '' then `` pencil '' sub - domains that are assigned to each mpiprocess .",
    "the initial conditions for our runs were generated by ` hit3d ` .",
    "the vortex method used the same initial condition by first calculating the vorticity field in physical space , and then using radial basis function interpolation to obtain the vortex strengths .",
    "this is different from our other publication , where we studied the accuracy of turbulence simulations using the fmm - based vortex method on gpus , looking at high - order turbulence statistics @xcite . for that case ,",
    "the initial condition provided by ` hit3d ` , which has a fully developed energy spectrum , was not suitable for our validation exercise looking at the time evolution of the velocity derivative skewness and flatness .",
    "for this reason , we constructed initial conditions in fourier space as a solenoidal isotropic velocity field with random phases and a prescribed energy spectrum .",
    "this initial velocity field had a gaussian distribution and satisfied the incompressibility condition .",
    "it is common that algorithms with low complexity ( sparse linear algebra , fft ) have low arithmetic intensity , while algorithms with high arithmetic intensity ( dense linear algebra ) tend to have high complexity .",
    "the fmmpossesses a rare combination of @xmath13 complexity and an arithmetic intensity that is even higher than dgemm @xcite .",
    "although this may seem like a great combination , it also implies that there is a large constant in front of the @xmath13 scaling , which results in a larger time - to - solution compared to other @xmath13 or @xmath14 methods like multigrid methods and fft .",
    "however , as arithmetic operations become cheaper compared to data movement in terms of both cost and energy , the large asymptotic constant of the _ arithmetic _ complexity becomes less of a concern .",
    "the calculation of velocity by means of the biot - savart equation ( [ eq : biotsavart ] ) and the computation of the stretching term ( [ eq : stretching ] ) both result in an @xmath2-body problem , which has a complexity of @xmath15 for @xmath2 particles , if solved directly .",
    "the fast multipole method @xcite reduces the complexity to @xmath13 by clustering source and target particles , and using series expansions that are valid far ( multipole expansion ) or near ( local expansion ) a point .",
    "the far / near relationships between points in the domain and the interactions between clusters are determined by means of a tree structure .",
    "the domain is hierarchically divided , and different sub - domains are associated with branches in the tree , a graphical representation of which is shown in figure [ fig : kernels ] .",
    "the algorithm proceeds as follows .",
    "first , the strengths of the particles ( e.g. , charges or masses ) are transformed to multipole expansions at the leaf cells of the tree ( known as the particle - to - multipole , or pmkernel ) .",
    "then , the multipole expansions of the smaller cells are translated to the center of larger cells in the tree hierarchy recursively and added ( multipole - to - multipole , or mmkernel ) .",
    "subsequently , the multipole expansions are transformed into local expansions for all pairs of well - separated cells ( multipole - to - local , or mlkernel ) , and then to local expansions at the center of smaller cells recursively ( local - to - local , or llkernel ) .",
    "finally , the local expansions at leaf cells are used to compute the effect of the far field on each target particle .",
    "since mloperations can only be performed for well - separated cells , the direct neighbors at the finest level of the tree interact directly via the original equation ( particle - to - particle , or ppkernel ) .",
    "the current implementation of the ` exafmm ` code uses expansions in spherical harmonics of the laplace green s function @xcite .",
    "details of the extension of laplace kernels to biot - savart and stretching kernels and of the implementation on gpus can be found in previous publications @xcite .      when parallelizing hierarchical @xmath2-body algorithms , the fact that the particle distribution is dynamic makes it impossible to precompute the decomposition and to balance the work - load or communication _ a priori_. warren and salmon @xcite developed a parallel algorithm for decomposing the domain into recursive subdomains using the method of orthogonal recursive bisection ( orb ) .",
    "the resulting process can be thought of as a binary tree , which splits the domain into subdomains with equal number of particles at every bisection .",
    "another popular technique for partitioning tree structures is to use morton ordering @xcite , where bits representing the particle coordinates are interleaved to form a unique key that maps to each cell in the tree . following the morton index monotonically",
    "will take the form of a space - filling curve in the shape of a  z \" . partitioning the list of morton indices equally assures that each partition will contain an equal number of cells , regardless of the particle distribution .",
    "for an adaptive tree , a nave implementation of the morton ordering could result in large communication costs if the  z \" is split in the wrong place , as illustrated in figure [ fig : partitioning ] .",
    "sundar et al .",
    "@xcite proposed a bottom - up coarsening strategy that ensures a clean partition at the coarse level while using morton ordering . on the other hand , an orbalways partitions the domain into a well balanced binary tree , since the number of particles is always equal on each side of the bisection",
    "therefore , orbis advantageous from the point of view of load - balancing .",
    "the main difference between using morton ordering and using an orbis the shape of the resulting subdomains and the corresponding tree structure that connects them , as shown in figure [ fig : partitioning ] .",
    "morton ordering is done on cubic octrees and the corresponding tree structure can become highly non - uniform depending on the particle distribution .",
    "conversely , an orbalways creates a balanced tree structure but the sub - domain shapes are rectangular and non - uniform .    a difficulty when applying orbwithin the framework of conventional fmmis that the construction of cell - cell interaction lists depends on the cells being cubic ( and not rectangular ) .",
    "the dual tree traversal described in the following subsection allows the use of rectangular cells , while enabling cell - cell interactions in the fmmwith minimum complications . by using orbalong with the dual tree traversal",
    ", we obtain a partitioning+traversal mechanism that allows perfect load balancing for highly non - uniform particle distributions , while retaining the @xmath13 complexity of the fmm .",
    "another advantage of this technique is that global indexing of cells is no longer necessary .",
    "having a global index becomes an issue when solving problems of large size .",
    "the maximum depth of octrees that a 64-bit integer can handle is 21 levels ( @xmath16 ) . for highly adaptive trees with small number of particles per cell",
    ", this limit will be reached quite easily , resulting in integer - overflow with a global index . circumventing this problem by using multiple integers for index storage will require much more work when sorting ,",
    "so this is not a desirable solution .",
    "the dual tree traversal allows the entire fmmto be performed without the use of global indices , and is an effective solution to this problem .",
    "our current partitioning scheme is an extension of the orb , which allows multi - sections instead of bisections . bisecting",
    "the domain involves the calculation of the median of the particle distribution for a given direction , and doing this recursively in orthogonal directions @xmath17 is what constitutes the  orthogonal recursive bisection \" . therefore the extension from bisection to multi - section",
    "can be achieved by simply providing a mechanism to search for something other than the median .",
    "we developed a parallel version of the  @xmath18-element \" algorithm .",
    "finding the  @xmath18-element \" is much faster than any sorting algorithm , so our technique is much faster than any method that requires sorting . always searching for the @xmath19-th element will reduce to the original algorithm based on bisections , but searching for",
    "the @xmath20-th element will enable the domain to be split between 3 and 4 processes , for example .",
    "therefore , our recursive multisection allows efficient partitioning when the number of processes is not a power of two .",
    "the dual tree traversal enables cell - cell interactions in the @xmath14 barnes - hut treecode @xcite framework , thus turning it into a @xmath13 algorithm @xcite .",
    "we give a detailed explanation of the dual tree traversal in a previous publication that focused on hybrid treecode & fmmtechniques @xcite .",
    "it has several advantages compared to explicit construction of cell - cell interaction lists , the more common approach in the fmmframework .",
    "firstly , it is the simplest and most general way to perform the task of finding all combinations of cell - cell interactions .",
    "it is simple in the sense that it does not contain multiple loops for finding  the parent s neighbor s children that are non - neighbors of the current cell , \" as is the case with the explicit construction .",
    "it is general in the sense that the property `` well - separated '' can be defined flexibly , instead of the rigid definition of  non - neighboring cells \" used traditionally in the fmm .",
    "it is common in treecodes to define the well - separated property more precisely and adaptively by introducing the concept of multipole acceptance criterion ( mac ) @xcite .",
    "we give a graphical description of the different types of macused in our method in figure [ fig : mac ] .",
    "the simplest macis the one defined for barnes - hut treecodes , where @xmath21 is the size of the source cell and @xmath22 is the distance from the target particle to the center of mass of the source cell ; @xmath23 , called the opening angle , is the parameter that is used to flexibly control the definition of the well - separated property . in the fmmframework ,",
    "a macis defined between two cells ( rather than a cell and a particle ) , where @xmath24 is the size of the target cell and @xmath22 is the distance between the center of mass of the target cell and the center of mass of the source cell .",
    "a flexible definition of the well - separated property can not be implemented easily in the traditional fmmframework , where the cell - cell interaction lists are constructed explicitly .",
    "this is because one must first provide a list of candidates to apply the macto , and increasing the neighbor search domain to @xmath25 neighbors instead of @xmath26 neighbors and applying the macon them is not an efficient solution .",
    "the dual tree traversal is a natural solution to this problem , since the list of candidates for the cell - cell interaction is inherited directly from the parent .",
    "this  inheritance of cell - cell interaction candidates \" is lacking from conventional fmm , and can be provided by the dual tree traversal while simplifying the implementation at the same time .",
    "furthermore , since the dual tree traversal naturally allows interaction between cells at different levels of the tree , it automatically finds the pair of cells that appear in @xmath27-lists for adaptive trees @xcite , but with much greater flexibility ( coming from the mac ) and no overhead of generating the lists ( since it does nt generate any ) .",
    "another advantage of the dual tree traversal is that it enables the use of non - cubic cells . as we have explained in section [ sse : orb ] , this allows using orbpartitioning in the fmmframework , which has superior load balancing properties compared to morton ordering and removes the dependence on global indexing . as mentioned above ,",
    "global morton indices are a problem when using millions of cores and the tree depth exceeds 21 levels causing the morton index to overflow from the 64-bit integer . the combination of the dual tree traversal and orbis an elegant solution to this problem as well .",
    "we describe the dual tree traversal in algorithm",
    "[ al : evaluate ] , which calls an internal routine for the interaction of a pair of cells , given in algorithm [ al : interact ] .",
    "first , a pair of cells is pushed to a stack .",
    "it is most convenient to start from the root cell although there is no possibility that two root cells will interact . for every step in the while - loop",
    ", a pair of cells is popped from the stack and the larger cell is subdivided . then",
    ", algorithm [ al : interact ] is called to perform either particle - particle ( pp ) or multipole - local ( ml ) interactions .",
    "if the cells in the pair are too close and either of the cells has children , the pair is pushed to the stack and will be handled later .",
    "a more detailed explanation is given in a previous publication focusing on hybrid treecode & fmmtechniques @xcite .",
    "a = b = rootcell push pair(a , b ) into a stack pop stack to get a pair(a , b ) interact(a , b ) interact(a , b )    evaluate multipole - local ( m2l ) evaluate particle - particle ( p2p ) evaluate multipole - local ( m2l ) push pair(a , b ) into a stack      the partitioning techniques discussed in section [ sse : orb ] will assign a local portion of the global tree to each process .",
    "however , the fmmevaluation requires information from all parts of the global tree , and the multipole expansions on remote processes must be communicated . unlike standard domain - decomposition methods , where only a thin halo needs to be communicated , the fmmrequires a global halo .",
    "fortunately , this global halo becomes exponentially coarser as the distance from the target cell increases , as shown in figure [ fig : periodic ] .",
    "therefore , the data required to be communicated from far partitions is very small ( but never zero ) .",
    "this results in a non - homogeneous ` alltoallv`-type communication of subsets of the global tree .",
    "once all the subsets of the global tree are communicated between the processes , one can locally reconstruct a tree structure that contains all the information required for the evaluation in the local process .",
    "this reconstructed tree is called the _ local essential tree _ ( let ) , and it is a ( significantly smaller ) subset of the global tree that contains all the information that is necessary to perform the evaluation .",
    "salmon and warren @xcite introduced two key techniques in this area : one for finding which cell data to send , and the other for communicating the data efficiently .",
    "the determination of which data to send is tricky , since each process can not see what the adaptive tree structure looks like on remote processes .",
    "the solution proposed by salmon and warren @xcite is to use a conservative estimate and communicate a larger portion of the tree than is exactly required .",
    "this conservative estimate is obtained by means of a special macdescribed in figure [ fig : mac ] as the letmac .",
    "there , the distance @xmath22 is defined as the distance between the center of mass of the source cell and the edge of the partition on the remote process .",
    "a formula to calculate @xmath22 can be given as the following expression , where we assume element - wise boolean operations over array elements ( giving zero for false , and 1 for true ) and element - wise multiplication and summation : @xmath28    here , @xmath29 and @xmath30 are the minimum and maximum coordinate values for all particles in the target partition , while @xmath31 is the center of mass of the source cell .",
    "this definition of the letmaccorresponds to assuming the case where the target cell is located at the edge of the remote partition .",
    "we also assume that the target cell is of the same size as the source cell ( @xmath32 ) , so @xmath33 becomes @xmath34 .",
    "these assumptions generally hold quite well and the required part of the tree is sent to the remote process most of the time . however , we must ensure that the fmmcode still works for extreme cases where necessary information fails to be sent . with this end in view",
    ", we have added a conditional statement in the interaction calculation ( algorithm",
    "[ al : interact ] ) as a further precaution for anomalous cases . this way , the traversal will perform the mltranslation with the smallest cell that is available",
    ". this cell may be too large to satisfy the fmmmacso there will be a small penalty on the accuracy , but since the occurrence of such a case is so rare , it does not affect the overall result .",
    "a schematic showing how the partitioning and communication techniques are used in the vortex method is given in figure [ fig : flow_chart ] .",
    "first , the domain is partitioned using the recursive multisection described in section [ sse : orb ] .",
    "then the fmmkernels for the local tree are evaluated while the letdata is being communicated in a separate openmp section .",
    "after the letdata is communicated , the fmmkernels are evaluated again for the remaining parts of the let .",
    "subsequently , the position , vortex strength @xmath35 , and core radius @xmath8 of the vortex particles are updated locally .",
    "this information is communicated in the next stage when the letis exchanged .",
    "in addition , the lagrangian vortex method needs to reinitialize the particle positions to maintain sufficient overlap of the smooth particles . for this procedure",
    ", we can reuse the same tree structure since the particles are reinitialized to the same position every time .",
    "therefore , the partitioning is performed only once at the beginning of this simulation .",
    "the fmmwas originally devised to solve potential problems with free - field boundary conditions .",
    "the method can be extended to handle periodic boundary conditions by placing periodic images around the original domain and using multipole expansions to approximate their influence @xcite , as illustrated in figure [ fig : periodic ] .",
    "when a sufficient number of periodic images are placed , the error caused by using a finite number of periodic images becomes smaller than the approximation error of the fmmitself @xcite .",
    "this approach to extend the fmmto periodic domains adds negligible computational overhead to the original fmm , for two reasons .",
    "first , distant domains are clustered into larger and larger cells , so the extra cost of considering a layer of periodic images is constant , while the number of images accounted for grows exponentially .",
    "the second reason is that only the sources need to be duplicated and the target points exist only in the original domain .",
    "since the work load for considering the periodicity is independent of the number of particles , it becomes negligible as the problem size increases .",
    "an earlier study showed that periodic boundary conditions add approximately 3% to the calculation time for a million particles @xcite .",
    "the fmmconsists of six different computational kernels , as illustrated on figure [ fig : kernels ] . in the ` exafmm`code , all of these kernels are evaluated on gpu  devices using cuda . out of the six kernels , a great majority of the runtime is spent executing ppand ml .",
    "we use a batch evaluation for these two kernels , since there is no data dependency between them : they are evaluated in one batch after the tree traversal is completed .",
    "this batch evaluation can be broken into multiple calls to the gpu  device , depending on its storage capacity and the data size . with this approach , we are able to handle problem sizes of up to 100 million particles on a single gpu , if the memory on the host machine is large enough @xcite .    as an example of the usage of thread blocks in the gpu  execution model , we show in figure [ fig : m2l_gpu ] an illustration of the mlkernel on gpus .",
    "each coefficient of the multipole / local expansion is mapped to a thread on the gpu  and each target cell is mapped to a thread block , while each source cell is loaded to shared memory and evaluated sequentially .",
    "all other kernels are mapped to the threads and thread blocks in a similar manner .",
    "more details regarding the gpu  implementation of fmmkernels can be found in chapter 9 of the _ gpu gems emerald edition _",
    "book @xcite , with accompanying open - source codes .",
    "in this section , we present results from large - scale simulations of homogeneous isotropic turbulence using the fmm - based vortex particle method , up to a problem size of @xmath0 computational points .",
    "this is still the largest mesh - size for which dnsof turbulence has been published , even though this scale of simulation was achieved 10 years ago . as we discussed in the introduction ,",
    "one of the reasons for this is the difficulty in scaling the fftalgorithm beyond a few thousand processes .",
    "the current simulations with a vortex method were checked for correctness by comparing the kinetic energy spectrum with that obtained using a trusted spectral - method code ( described in  [ s : spectral ] ) .",
    "the focus here is not on the physics , however , but on demonstrating large - scale simulation of turbulence using the fmmas a numerical engine and reporting on performance aspects using gpu  hardware .",
    "the performance is described via the results of weak scaling tests using between 1 and 4096 processes with @xmath36 ( 16.8 million ) particles per process .",
    "each process offloads to one gpu  to speed - up the fmm , making it even more challenging to obtain good parallel efficiency ( it is obviously harder to scale faster code ) . despite this",
    ", a parallel efficiency of 74% was obtained for the fmm - based vortex method on weak scaling between 1 and 4096 processes , with the full application code .",
    "the largest calculation used @xmath37 billion particles , which as far as we know is the largest vortex - method calculation to date .",
    "previous noteworthy results by other authors reported calculations with up to 6 billion particles @xcite , which we surpass here by an order of magnitude .",
    "the calculations reported here were run on the tsubame-2.0system during spring and fall of 2011 , thanks to guest access provided by the grand challenge program of tsubame-2.0 .",
    "this system has 1408 nodes , each equipped with two six - core intel xeon  x5670 ( formerly westmere - ep ) 2.93ghz processors , three nvidia  m2050 gpus , 54 gb of ram ( 96 gb on 41 nodes ) , and 120 gb of local ssd storage ( 240 gb on 41 nodes ) .",
    "computing nodes are interconnected with the infiniband device grid director 4700 developed by voltaire inc .",
    ", with non - blocking and full bisectional bandwidth .",
    "each node has @xmath38 gbps bandwidth , and the bisection bandwidth of the system is over 200 tbps .",
    "the total number of  m2050 gpus in the system is 4224 , and the peak performance of the entire system is 2.4 petaflop / s .",
    "we set up simulations of decaying , homogeneous isotropic turbulence using an initial taylor - scale reynolds number of @xmath39 .",
    "given that we had guest access to the full tsubame-2.0system for a very brief period of time ( only a few hours ) to produce the scalability results , we opted for a lower reynolds number than previous turbulence simulations of this size using spectral methods .",
    "there is limited experience using vortex methods for dnsof turbulence , but previous work has suggested that higher resolutions are needed than when using the spectral method at the same reynolds number .",
    "we thus decided to be conservative and ensure that we obtained usable results from these one - off runs .",
    "the calculation domain is a box of size @xmath40 ^ 3 $ ] with periodic boundary conditions in all directions , using @xmath41 periodic images in each dimension in the periodic fmm(see figure [ fig : periodic ] ) . to achieve the best accuracy from the fmmfor the present application , the order of multipole expansions",
    "was set to @xmath42 ; this may be a conservative value , but given our limited allocation in the tsubame-2.0system , we did not have the luxury of tuning the simulations for this parameter .",
    "the fmmkernels are run in single precision on the gpu which may raise some concerns .",
    "we are able to achieve double - precision accuracy using single - precision computations in the fmmkernels by means of two techniques .",
    "the multipole - expansion coefficients have exponentially smaller magnitude with increasing order of expansion ; therefore , by adding them from higher- to lower - order terms , we can prevent small numbers being added to the large numbers .",
    "this preserves the significant digits in the final sum .",
    "the second technique consists of normalizing the expansion coefficients to reduce the dynamic range of the variables , allowing the use of single - precision variables to get a double - precision result .",
    "thus , we are able to achieve sufficient accuracy to reproduce the turbulence statistics ( as shown below ) and obtain the same results that would be obtained using double precision ( given that the fmmerror is larger than 6 significant digits anyway ) .      the isosurface of the second invariant of the velocity gradient tensor is shown in figure [ fig : isosurface ] .",
    "this is a snapshot at the early stages of the simulation and we do not observe any large coherent structures . in order to take a closer look at the quantitative aspects of the vortex simulation , in figure [ fig : spectrum ] we compare the kinetic energy spectrum with that of the spectral method , where @xmath43 is the eddy turnover time .",
    "the energy spectrum of the vortex method is obtained by calculating the velocity field on a uniform lattice , which is induced by the lagrangian vortex particles .",
    "the capability of ` exafmm ` to calculate for different sources and targets enabled such operations .",
    "we have excellent quantitative agreement between the vortex method and spectral method and we conclude that our fmm - based particle method is capable of simulating turbulence of this scale correctly .",
    "we turn our attention to the performance of these calculation in the next section .",
    ", obtained with the vortex method and spectral method .",
    "notice that the vertical axis goes down to @xmath44 , which is many times smaller than in similar plots presented by other authors . ]",
    "we had two very short windows of time in which we were able to run weak scaling tests , one with half the system and the other with almost the full tsubame-2.0system .",
    "the larger scaling tests used @xmath36 particles per process , on 1 to 4096 processes and executing three mpiprocesses per node , each process assigned to a gpu  card within the node .",
    "the results of the weak scaling test are shown in figure [ fig : weak_scaling ] in the form of total runtime of the fmm , and timing breakdown for different phases in the computation .",
    "the label ` near - field evaluation ' corresponds to the ppkernel evaluation illustrated in figure [ fig : kernels ] , and the label ` far - field evaluation ' corresponds to the sum of all the other kernel evaluations , i.e. , the far field .",
    "the ` mpicommunication ' is overlapped with the fmmevaluation ( see figure [ fig : flow_chart ] ) , so the plot shows only the amount of time that communication exceeds the local portion of the ` near - field ' and ` far - field ' evaluation . in this way , the total height of each bar correctly represents the total wall - clock time of each calculation .",
    "note that particle updates in the vortex - method calculation take less than 0.01% in all runs and thus were invisible in the bar plots , so we ve left this computing stage out of the labels .",
    "the ` gpu  buffering ' label corresponds to the time it takes to form a queue of tasks and corresponding data buffer to be transferred to the gpu , which is a significant amount of time .",
    "we have found this buffering to be necessary in order to achieve high efficiency in the ppand mlevaluation and fmmevaluation on gpus .",
    "moreover , this part of the computation scales perfectly , and does not affect the scalability of the fmm .",
    "the parts that do affect the scalability are the tree construction and mpicommunication . actually , the tree construction also involves mpicommunications for the partitioning , so the parallel efficiency in weak scaling is fully determined by mpicommunications .",
    "figure [ fig : weak_scaling ] shows that the current fmmis able to almost completely hide the communication time up to 512 gpus .",
    "it may be worth noting that the @xmath2-d - hypercube - type communication of the let@xcite turned out to be slower on tsubame-2.0than a simple call to ` mpi_alltoallv ` for sending the entire letat once .",
    "this is a consequence of the network topology of tsubame-2.0(with a dual - qdr infiniband link to each node and non - blocking full - bisection fat - tree interconnect ) and also of the relatively small number of mpiprocesses .",
    "the results shown in figure [ fig : weak_scaling ] are those obtained with ` mpi_alltoallv ` and not the hypercube - type communication .",
    "we performed a corresponding weak scalability test for the spectral method , increasing the problem size from @xmath36 on one process to @xmath0 on 4096 processes .",
    "we used three mpiprocesses per node to match the condition of the vortex method runs , but there is no gpu  acceleration in this case . matching",
    "the number of mpiprocesses per node should give both methods an equal advantage / handicap for the bandwidth per process .",
    "note that using gpus for the fftwithin the spectral - method code is unlikely to provide any benefits , because performance improvements of ` cufft ` over ` fftw ` would be canceled out by data transfer between host and device ] and inter - node communications in parallel runs .",
    "figure [ fig : compare_scaling ] shows the parallel efficiency obtained with the two methods , under these conditions .",
    "the parallel efficiency of the fmm - based vortex method is 74% when going from one to 4096 gpus , while the parallel efficiency of the spectral method is 14% when going from one to 4096 cpus .",
    "the bottleneck of the spectral method is the all - to - all communication needed for transposing the slabs into pencils as shown in figure [ fig : spectral_fft ] .",
    "even though this may not be the best implementation of a parallel fft , the difference in the scalability between the spectral method and fmm - based vortex method is considerable .",
    "the actual calculation time is in the same order of magnitude for both methods at 4096 processes : it was 108 seconds per time step for the vortex method and 154 seconds per time step for the spectral method .",
    "therefore , the superior scalability of the fmmhas merely closed the gap with fft , being barely @xmath45 faster at this scale .",
    "however , we anticipate that this trend will affect the choice of algorithm in the future , as we move to architectures with higher degree of parallelism .",
    "the scaling test with half the tsubame-2.0system was done several months before and with a different revision of the code , with many changes having been incorporated since then .",
    "we include the results here for completeness ; see figure [ fig : compare_scaling_old ] . in this case",
    ", the number of particles per process is much smaller , at 4 million ( compared to 16.8 million particles per process in the larger test ) and we scale from 4 to 2048 processes .",
    "the parallel efficiency of the fmm - based vortex method is 72% when going from 4 to 2048 gpus , while the parallel efficiency of the spectral method was 14% .",
    "when we compare the parallel efficiency with the 1 to 4096 gpucase , we see that the 4 to 2048 case is scaling relatively poorly .",
    "this is due to the number of particles per process being roughly 1/4 in the 4 to 2048 case , and also the improvement in the ` exafmm ` code during the half year gap between the two runs .",
    "the run time per time step in this case was 27 seconds for the vortex method and 20 seconds for the spectral method to compute a domain with @xmath46 points .",
    "note that if we read from the plot in figure 2 of donzis et al .",
    "@xcite , their spectral - method calculation on a @xmath46 grid using 2048 cores of the ranger supercomputer takes them about 20 seconds per time step .",
    "since this is the same timing we obtained with the ` hit3d ` code , we are satisfied that this code provides a credible reference point for the scalability of spectral dns codes .",
    "the calculation in the fmmis mostly dominated by the floating point operations in the particle - particle interactions , while all other parts are a minor contribution in terms of flop / s ( although not negligible in terms of runtime ) .",
    "we will thus consider in the estimate of sustained performance only the operations executed by the ppkernels .",
    "two separate equations are being calculated for the particle - particle interactions : the biot - savart equation ( [ eq : biotsavart ] ) and the stretching equation ( [ eq : stretching ] ) . the number of floating point operations required by these two kernels is summarized in table [ tab : flops ] .",
    ".floating point operations per ppinteraction . [ cols=\"^,^,^\",options=\"header \" , ]     the approximate number of flop / s for one step of the vortex method calculation of isotropic turbulence is obtained by the following equation .",
    "@xmath47    thus , the current fmm - based vortex method achieved a sustained performance of 1.08 petaflop / s ( single precision ) on 4096 gpus of tsubame-2.0 .",
    "the authors of the ` exafmm`code have a consistent policy of making science codes available openly , in the interest of reproducibility .",
    "the entire code that was used to obtain the present results is available from https://bitbucket.org/exafmm/exafmm .",
    "the revision number used for the results presented in this paper is 191 for the large - scale tests up to 4096 gpus .",
    "documentation and links to other publications are found in the project homepage at http://exafmm.org/. figure [ fig : compare_scaling ] , its plotting script and datasets are available online and usage is licensed under cc - by-3.0@xcite .",
    "we acknowledge the use of the ` hit3d ` pseudo - spectral dns code for isotropic turbulence , and appreciate greatly their authors for their open - source policy ; the code is available via google code at http://code.google.com / p / hit3d/.",
    "this work represents several milestones .",
    "although the fmmalgorithm has been taken to petascale before ( notably , with the 2010 gordon bell prize winner ) , the present work represents the first time that this is done on gpu  architecture .",
    "also , to our knowledge , the present work is the largest direct numerical simulation with vortex methods to date , with 69 billion particles used in the cubic volume ; this is an order of magnitude larger than the previously reported record .",
    "yet another significant event is reaching a range where the highly scalable fmmstarts showing advantage over fft - based algorithms . with a 1.08 petaflop / s ( single precision )",
    "calculation of isotropic turbulence in a @xmath0 box , using 4096 gpus , we are within reach of a turning point .",
    "the combination of application , algorithm , and hardware used are also notable",
    ".    the real challenge in exascale computing will be the optimization of data movement .",
    "when we compare the data movement of fmmagainst other fast algorithms like multigrid and fft , we see that the fmmhas a potential advantage .",
    "compared to fft , both multigrid and fmmhave an advantage in the asymptotic complexity of the global communication .",
    "the hierarchical nature of multigrid and fmmresults in @xmath48 communication complexity where @xmath49 is the number of processes . on the other hand , fftrequires two global - transpose communications between @xmath50 processes , and has communication complexity of @xmath51 .",
    "when @xmath49 is in the order of millions , it seems unrealistic to expect that an affordable network can compensate for this large gap in the communication complexity . although it is not the focus of the present article , we would like to briefly note that an advantage of fmmover multigrid is obtained from differences in the synchronization patterns .",
    "for example , increasing the desired accuracy in iterative solvers using multigrid will result in more iterations , hence more global synchronizations .",
    "conversely , increasing the accuracy in fmminvolves increasing the number of multipole expansions , which results in even higher arithmetic intensity in the inner kernels while the number of global synchronizations remains the same . as the amount of concurrency increases , bulk - synchronous execution",
    "/ communication models are reaching their limit .",
    "thus , fmmprovides a new possibility to reduce the amount of communication and synchronization in these inherently  global \" problems .",
    "finally , we would like to point out that the fmmcan be used to solve the poisson equation directly , or as a preconditioner for an iterative solver .",
    "therefore , we are not concerned at this point about the fact that vortex methods may still be comparatively inefficient for the simulation of fluid turbulence , where spectral methods will continue to dominate in the foreseeable future .",
    "this does not detract from the conclusions about the efficiency of the fmmitself , which is the object of our study . in future work",
    ", we would like to demonstrate the efficiency of fmmby using it as a poisson solver or preconditioner in the framework of more standard finite difference / volume / element methods .",
    "there , the comparison against fftand multigrid methods should be of interest to a broader spectrum of the cfdcommunity .",
    "computing time in the tsubame-2.0system was made possible by the grand challenge program of tsubame-2.0 .",
    "the current work was partially supported by the core research for the evolution science and technology ( crest ) of the japan science and technology corporation ( jst ) .",
    "lab acknowledges funding from nsf grant oci-0946441 , onr grant # n00014 - 11 - 1 - 0356 and nsf career award oci-1149784 .",
    "lab is also grateful for the support from nvidiacorp .  via an academic partnership award ( aug .",
    "2011 ) .",
    "s.  g. chumakov , j.  larsson , c.  schmitt , and h.  pitsch . lag - modeling approach for dissipation terms in large eddy simulation .",
    "annual research briefs , center for turbulence research , 2009 .",
    "http://www.stanford.edu/group/ctr/resbriefs/arb09.html .",
    "t.  hamada , t.  narumi , r.  yokota , k.  yasuoka , k.  nitadori , and m.  taiji .",
    "42 tflops hierarchical n - body simulations on gpus with applications in both astrophysics and turbulence . in _",
    "sc 09 : proceedings of the conference on high performance computing networking , storage and analysis _ , pages 112 .",
    "acm , 2009 .",
    "t.  hamada and k.  nitadori .",
    "190 tflops astrophysical @xmath2-body simulation on a cluster of gpus . in",
    "_ high performance computing , networking , storage and analysis ( sc ) , 2010 international conference for _ , pages 19 , nov .",
    "t.  ishihara , y.  kaneda , m.  yokokawa , k.  itakura , and a.  uno .",
    "small - scale statistics in high - resolution direct numerical simulation of turbulence : reynolds number dependence of one - point velocity gradient statistics . , 592:335366 , 2007 .",
    "christophe  g. lambert , thomas  a. darden , and john a.  board jr . a multipole - based algorithm for efficient calculation of forces and potentials in macroscopic periodic assemblies of particles .",
    ", 126(2):274285 , 1996 .",
    "i.  lashuk , a.  chandramowlishwaran , h.  langston , t.  nguyen , r.  sampath , a.  shringarpure , r.  vuduc , l.  ying , d.  zorin , and g.  biros . a massively parallel adaptive fast - multipole method on heterogeneous architectures . in _ proceedings of the conference on high performance computing networking , storage and analysis , sc 09 _ ,",
    "pages 112 , portland , oregon , november 2009 .",
    "a.  rahimian , i.  lashuk , s.  veerapaneni , a.  chandramowlishwaran , d.  malhotra , l.  moon , r.  sampath , a.  shringarpure , j.  vetter , r.  vuduc , d.  zorin , and g.  biros .",
    "petascale direct numerical simulation of blood flow on 200k cores and heterogeneous architectures . in _ proceedings of the 2010 acm / ieee international conference for high performance computing , networking , storage and analysis _ , sc 10 , pages 111 .",
    "ieee computer society , 2010 .",
    "m.  s. warren and j.  k. salmon .",
    "astrophysical @xmath2-body simulations using hierarchical tree data structures . in _ proceedings of the 1992 acm / ieee conference on supercomputing _ , pages 570576 , los alamitos , ca , usa , 1992 .",
    "ieee computer society press .",
    "m.  yokokawa , k.  itakura , a.  uno , t.  ishihara , and y.  kaneda .",
    "16.4-tflops direct numerical simulation of turbulence by a fourier spectral method on the earth simulator . in _ supercomputing ,",
    "acm / ieee 2002 conference _ , pages 5050 .",
    "ieee , 2002 .",
    "rio yokota and l.  a. barba .",
    "treecode and fast multipole method for @xmath2-body simulation with cuda . in wen - mei hwu ,",
    "editor , _ gpu computing gems emerald edition _ , chapter  9 , pages 113132 .",
    "elsevier/ morgan kaufman , 2011 .",
    "preprint on http://arxiv.org/abs/1010.1482[arxiv:1010.1482 ] .",
    "rio yokota and l.  a. barba .",
    "-based vortex method for simulation of isotropic turbulence on gpus , compared with a spectral method . to appear in _ comput .  & fluids _ , preprint on http://arxiv.org/abs/1110.2921[arxiv:1110.2921 ] , 2012 .",
    "rio yokota , jaydeep  p. bardhan , matthew  g. knepley , l.  a. barba , and tsuyoshi hamada .",
    "biomolecular electrostatics using a fast multipole bem on up to 512 gpus and a billion unknowns . , 182(6):12711283 , 2011 ."
  ],
  "abstract_text": [
    "<S> this paper reports large - scale direct numerical simulations of homogeneous - isotropic fluid turbulence , achieving sustained performance of 1.08 petaflop / s on gpu  hardware using single precision . </S>",
    "<S> the simulations use a vortex particle method to solve the navier - stokes equations , with a highly parallel fast multipole method ( fmm ) as numerical engine , and match the current record in mesh size for this application , a cube of @xmath0 computational points solved with a spectral method . the standard numerical approach used in this field </S>",
    "<S> is the pseudo - spectral method , relying on the fftalgorithm as numerical engine . </S>",
    "<S> the particle - based simulations presented in this paper quantitatively match the kinetic energy spectrum obtained with a pseudo - spectral method , using a trusted code . in terms of parallel performance , </S>",
    "<S> weak scaling results show the fmm - based vortex method achieving 74% parallel efficiency on 4096 processes ( one gpu  per mpiprocess , 3 gpus per node of the tsubame-2.0system ) . </S>",
    "<S> the fft - based spectral method is able to achieve just 14% parallel efficiency on the same number of mpiprocesses ( using only cpu  cores ) , due to the all - to - all communication pattern of the fftalgorithm . </S>",
    "<S> the calculation time for one time step was 108 seconds for the vortex method and 154 seconds for the spectral method , under these conditions . </S>",
    "<S> computing with 69 billion particles , this work exceeds by an order of magnitude the largest vortex - method calculations to date .    </S>",
    "<S> isotropic turbulence , fast multipole method , integral equations , gpu </S>"
  ]
}