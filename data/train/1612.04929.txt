{
  "article_text": [
    "the techniques and algorithms used to optimize the control of quantum systems @xcite and those underlying the field of deep neural networks @xcite share a number of common elements .",
    "both areas heavily use linear algebra operations combined with gradient descent optimization .",
    "thus , advanced hardware and software technology recently emerging from the rapid development of machine learning also paves the way for a significant boost of optimal quantum control techniques .",
    "a crucial factor for recent impressive progress in machine learning has been the leveraging of massive parallelism native to graphics processing units ( gpus ) @xcite .",
    "specifically , gpus are extremely efficient in multiplying very large matrices @xcite . such multiplications also form a central step in the simulation and optimal control of quantum systems . exploiting this advantageous feature of gpus",
    ", we achieve significant speed improvements in optimizing control schemes for systems at the current frontiers of experimental quantum computation .",
    "as the number of qubits in these experiments is increasing @xcite , it becomes increasingly important to take advantage of optimal control techniques .",
    "moreover , recent advances in commercially available electronics  e.g. , arbitrary waveform generators enabling base - band synthesis of the entire microwave spectrum @xcite  afford new capabilities which quantum optimal control is uniquely well - suited to harness .",
    "there have been numerous theoretical developments of numerical and analytical methods for quantum optimal control ( see ref .   for a recent review ) .",
    "the algorithms involved are predominantly based on gradient methods , such as realized in gradient ascent pulse engineering ( grape ) @xcite , krotov algorithms @xcite or monotonically convergent algorithms @xcite , and are available in several open - source packages , including qutip @xcite , dynamo @xcite , spinach @xcite , and simpson @xcite .",
    "quantum optimal control has been remarkably successful in determining optimized pulse sequences @xcite , designing high - fidelity quantum gates @xcite , and preparing entangled states @xcite .",
    "optimal control is a versatile concept which can be applied to a vast variety of quantum systems . while the general goal remains universal",
    " minimizing deviations from the intended target unitary or state  the experimentally available controls and their limitations are specific to each physical system .",
    "examples of such constraints include fixed maximum amplitudes of control pulses @xcite , maximum overall power of control signals @xcite , and limited time resolution of arbitrary waveform generators @xcite .",
    "further , finite coherence of quantum systems motivates minimizing the overall time needed for reaching the intended state or unitary ( time - optimal control ) @xcite . in certain cases , steering the quantum system among an optimal path ( time - dependent target ) may be desired @xcite . incorporating new constraints in the optimization process",
    "often requires the analytical derivation and implementation of additional contributions to the gradient calculation , and may necessitate significant effort to deploy on large computer clusters .",
    "this issue can greatly impede the ability to quickly develop control strategies for new problems .    to overcome these obstacles ,",
    "we have implemented a quantum optimal control scheme that incorporates constraints via automatic differentiation @xcite and utilizes gpus for boosting computational efficiency .",
    "specifically , automatic differentiation handles the updating of gradient calculations in the backward propagation algorithm @xcite , and thus eliminates the need to hard - code additional gradient contributions from constraints . for the actual optimal control applications we present in this paper , we find that the computational speed - up from utilizing gpus becomes significant for hilbert space sizes exceeding dimensions of the order of one hundred , see fig .  [",
    "fig : benchmark ] .",
    "together , these features allow a quick turnaround for varying optimization constraints and system parameters , rendering this approach invaluable for the study of quantum optimal control .",
    "in this paper , we describe the implementation of automatic differentiation , demonstrate its application to quantum optimal control of example systems relevant to quantum computing and quantum optics , and discuss the performance gains achieved by utilizing gpus .",
    "we briefly review the essential idea of quantum optimal control and introduce the notation used throughout our paper .",
    "we consider the general setting of a quantum system with intrinsic hamiltonian @xmath0 and a set of external control fields @xmath1 acting on the system via control operators @xmath2 .",
    "the resulting system hamiltonian is given by @xmath3 .",
    "optimal control theory aims to minimize deviations from a target state or target unitary by appropriate adjustments of the control fields @xmath4 . to implement this optimization ,",
    "the time interval of interest is discretized into a large number @xmath5 of sufficiently small time steps @xmath6 .",
    "denoting intermediate times by @xmath7 , the hamiltonian at time @xmath8 takes on the form @xmath9 the control fields subject to optimization now form a set @xmath10 of @xmath11 real numbers .",
    "the quantum evolution from the initial time @xmath12 to time @xmath8 is described by a propagator @xmath13 , decomposed according to @xmath14 where @xmath15 is the propagator for the short time interval @xmath16 $ ] .",
    "( here and in the following , we set @xmath17",
    ". ) evolution of a select initial state @xmath18 from @xmath19 to @xmath20 then takes the usual form @xmath21 in the decomposition of @xmath13 , each short - time propagator @xmath22 can be evaluated exactly by matrix exponentiation or approximated by an appropriate series expansion .",
    "optimization of the discretized control fields @xmath23 can be formulated as the minimization of a cost function @xmath24 where @xmath25 .",
    "table [ table : cost_functions ] shows some of the most important cost function contributions used for quantum optimal control . the total cost function is a linear combination of these cost functions , @xmath26 .",
    "the weight factors @xmath27 must be determined empirically , and depend on the specific problem and experimental realization at hand . in the following ,",
    "we discuss these relevant cost function contributions .",
    "the first cost contribution , @xmath28 , is the primary tool for realizing a target unitary @xmath29 , such as a single or multi - qubit gate .",
    "cost is incurred for deviations between the target unitary and the realized unitary @xmath30 at a given final time @xmath31 . for a system with hilbert space dimension @xmath32 ,",
    "its expression @xmath33 @xcite represents the infidelity obtained from the trace distance between the target unitary and the realized unitary .",
    "minimizing this cost function is the principle goal of the quantum control problem .",
    "the second cost contribution has its primary use in quantum state preparation .",
    "its expression @xmath34 measures the distance between a desired target state @xmath35 and the state @xmath36 realized at the final time @xmath31 , as obtained from evolution of a given initial state @xmath37 .",
    "in addition , generalizing @xmath38 to multiple initial and target states is useful for performing a unitary @xmath29 which is only defined on some subspace @xmath39 of the modeled hilbert space .",
    "such restriction to a selected subspace is of practical importance whenever a desired unitary is to be implemented within some computational subspace only , as is common for quantum computation applications .",
    "there , evolution of higher excited states or auxiliary systems outside the computational subspace is immaterial .",
    "optimal control , then , can be achieved by simultaneous evolution of a set of initial states @xmath40 ( @xmath41 ) that forms a basis of @xmath39 .",
    "optimal control fields are obtained from minimizing the composite state infidelity @xmath42 relative to the desired target states @xmath43 .",
    "( here , @xmath44 is the projector onto subspace @xmath39 . )",
    "this strategy of simultaneous state evolution is significantly more memory efficient than evolving the full unitary and calculating the gate infidelity restricted to @xmath39 , given by @xmath45 .",
    "the simultaneous state evolution can be easily parallelized on distributed system , enabling control optimization for large - scale quantum systems .",
    ".relevant contributions to cost functions for quantum optimal control .",
    "names of contributions indicate the quantity to be _",
    "minimized_. [ cols=\"<,^,^\",options=\"header \" , ]     like many optimization problems , quantum optimal control is typically underconstrained .",
    "this freedom can be exploited in useful ways .",
    "adding further constraints can improve overall convergence properties , and is often crucial for obtaining control fields which are consistent with specific experimental capabilities and limitations .",
    "control fields must be realizable in the lab , should be robust to noise , and avoid large control amplitudes and rapid variations based on signal output specifications of instruments employed in experiments .",
    "exceedingly strong control fields may also be problematic due to heat dissipation which may , for instance , raise the temperature inside a dilution refrigerator .",
    "these points motivate the consideration of additional cost function contributions in the following .",
    "one such contribution , @xmath46 suppresses large control - field amplitudes globally , and is commonly employed in quantum optimal control studies @xcite .",
    "( the generalization to more fine - grained suppression of individual control fields is straightforward to implement as well . ) penalizing the @xmath47 norm of the control fields favors solutions with low amplitudes .",
    "it also tends to spread relevant control fields over the entire allowed time window .",
    "while @xmath48 constitutes a `` soft '' penalty on control - field amplitudes , one may also apply a trigonometric mapping to the amplitudes to effect a hard constraint strictly enforcing fixed maximum amplitudes @xcite .",
    "the fourth type of contribution to the cost function , @xmath49 , penalizes rapid variations of control fields by suppressing their ( discretized ) time derivatives @xcite .",
    "the resulting smoothening of signals is of paramount practical importance , since any instrument generating a control field has a finite impulse response .",
    "if needed , contributions analogous to @xmath50 which suppress higher derivatives or other aspects of the time dependence of fields can be constructed .",
    "together , limiting the control amplitudes and their time variation filters out high - frequency `` noise '' from control fields , which is an otherwise common result of less - constrained optimization .",
    "smoother control fields also have the advantage that essential control patterns can potentially be recognized and given a meaningful interpretation .",
    "the contribution @xmath51 to the cost function has the effect of suppressing occupation of a select `` forbidden '' state @xmath52 ( or a set of such states , upon summation ) throughout the evolution .",
    "the inclusion of this contribution addresses an important issue ubiquitous for systems with hilbert spaces of large or infinite dimension .",
    "in this situation , truncation of hilbert space is needed or inevitable due to computer memory limitations .",
    "( note that this need even arises for a single harmonic oscillator . ) whenever the evolution generated by optimal control algorithms explores highly excited states , truncation introduces a false non - linearity which can misguide the optimization .",
    "including additional states can , in principle , mitigate this problem , but is generally computationally very expensive . an independent physics motivation for avoiding occupation of highly - excited states",
    "consists of spontaneous relaxation in realistic systems : high - energy states are often more lossy ( as is usually the case , e.g. , for superconducting qubits ) , and possibly more difficult to model .",
    "active penalization of such states therefore has the two - fold benefit of keeping hilbert space size at bay , and reducing unwanted fidelity loss from increased relaxation . to address these challenges ,",
    "we employ an intermediate - time cost function @xcite : the cost function @xmath53 limits leakage to higher states during the entire evolution , and at the same time prevents optimization to be misinformed by artificial non - linearity due to truncation .",
    "we note that the efficacy of this strategy is system dependent : it works well , for example , for harmonic oscillators or transmon qubits @xcite which have strong selection rules against direct transitions to more distant states , but may be less effective in systems such as the fluxonium circuit @xcite where low - lying states have direct matrix elements to many higher states .    .",
    "in the forward direction , starting from a set of control parameters @xmath54 , the computational graph effects time evolution of a quantum state or unitary , and the simultaneous computation of the cost function @xmath55 . the subsequent `` backward propagation ''",
    "extracts the gradient @xmath56 with respect to all control fields by reverse - mode automatic differentiation .",
    "this algorithm is directly supported by tensorflow , once such a computational network is specified . ]",
    "customarily , algorithms minimizing the cost function @xmath57 for a given evolution time interval @xmath58 $ ] aim to match the desired target unitary or target state at the very end of this time interval . to avoid detrimental effects from decoherence processes during the evolution ,",
    "it is often beneficial to additionally minimize the gate duration ( or state preparation ) time @xmath59 itself . instead of running the algorithms multiple times for a set of different @xmath60",
    ", we employ cost function contributions of the form @xmath61 for a target unitary , or @xmath62 for a target state , respectively .",
    "these expressions penalize deviations from the target gate or target state not only at the final time @xmath31 , but _ at every time step_. this contribution to the overall cost function therefore guides the evolution towards a desired unitary or state in as short a time as possible under the conditions set by the other constraints , and thus results in a time - optimal gate",
    ".    we will demonstrate the utility of these cost function contributions for a set of concrete examples in section [ sec : examples ] .",
    "the weighted sum of cost functions , @xmath57 , can be minimized through a variety of gradient - based algorithms .",
    "such algorithms are a very popular means of optimization thanks to their good performance and effectiveness in finding optimized solutions for a wide range of problems . at the most basic level , gradient - based algorithms",
    "minimize the cost function @xmath24 by the method of steepest descent , updating the controls @xmath63 in the opposite direction of the local cost - function gradient @xmath64 : @xmath65 the choice of the update step size @xmath66 for the control field parameters @xmath63 , plays an important role for the convergence properties of the algorithm .",
    "a number of schemes exist which adaptively determine an appropriate step size @xmath66 in each iteration of the minimization algorithm .",
    "our implementation is designed to allow employment of a variety of such methods including l - bfgs @xcite and adam @xcite .",
    "for the evaluation of the gradient @xmath67 we make use of automatic differentiation @xcite in reverse - accumulation mode . in brief",
    ", this algorithm utilizes the decomposition of the multivariable cost function @xmath68 into its computational graph of elementary operations ( addition , matrix multiplications , trace , etc . ) , each of which has a known derivative . in reverse - accumulation mode ,",
    "all partial derivatives of @xmath55 are evaluated in a recursion from the top level ( @xmath55 ) back towards the outermost branches ( variables @xmath63 )  rather similar to the procedure of obtaining a derivative with pencil and paper .",
    "for instance , for the simple function @xmath69=\\end{aligned}\\ ] ]",
    "\\(a ) at ( -1.5,0 ) [ start ] @xmath70 ; ( b ) at ( 4.5,0 ) [ start ] @xmath71 ; ( c ) at ( 3.7,1.4 ) [ state ] @xmath72 ; ( d ) at ( 2.4,2.1 ) [ state ] @xmath73 ; ( e ) at ( 0.2,2.1 ) [ state ] @xmath74 ; ( f ) at ( 2,3.4 ) [ state ] @xmath75 ; ( g ) at ( 1.5,4.7 ) [ start ] @xmath68 ; ( a ) edge [ bend right = 5 ] node[below = 0.15 cm ] ( d ) ; ( a ) edge [ bend right = -5]node[below = 0.15 cm ] ( e ) ; ( c ) edge [ bend right = 5]node[below = 0.15 cm ] ( d ) ; ( b ) edge [ bend right = 5 ] node[below = 0.15 cm ] ( c ) ; ( e ) edge [ bend right = -5 ] node[below = 0.15 cm ] ( f ) ; ( d ) edge [ bend right = 5 ] node[below = 0.15 cm ] ( f ) ; ( f ) edge [ bend right = 0 ] node[below = 0.15 cm ] ( g ) ;    ,    one obtains all partial derivatives by a recursion starting with the evaluation of @xmath76\\,\\frac{\\partial}{\\partial u_j}\\sin \\,+\\ ,   [ d_2 f_+]\\ , \\frac{\\partial}{\\partial u_j}f_{\\bigcdot } = \\cdots.\\ ] ] here , @xmath77 stands for the derivative of a multivariable function @xmath78 with respect to its @xmath79-th argument ; square brackets denote subsequent numerical evaluation of the enclosed term .",
    "( function arguments are suppressed for brevity . )",
    "automatic differentiation has become a central tool in machine learning @xcite , and equally applies to the problem of optimal control of quantum systems . in this approach ,",
    "the gradient of a set of elementary operations is defined and more complex functions are built as a graph of these operations .",
    "the value of the function is computed by traversing the graph from inputs to the output , while the gradient is computed by traversing the graph in reverse via the gradients .",
    "this methodology gives the same numerical accuracy and stability of analytic gradients without requiring one to derive and implement analytical gradients specific to each new trial cost function .",
    "all cost functions summarized in table [ table : cost_functions ] can be conveniently expressed in terms of common linear - algebra operations .",
    "figure [ figure : nn_graph ] shows the network graph of operations in our software implementation , realizing quantum optimal control with reverse - mode automatic differentiation . for simplicity , the graph only shows the calculation of the cost functions @xmath80 and @xmath53 .",
    "the cost function contributions @xmath81 , and @xmath82 are treated in a similar manner .",
    "the suppression of large control amplitudes or rapid variations , achieved by @xmath48 and @xmath50 , is simple to include , since the calculation of these cost function contributions is based on the control signals themselves and does not involve the time - evolved state or unitary .",
    "the host of steps for gradient evaluation is based on basic matrix operations like summation and multiplication .",
    "reverse - mode automatic differentiation @xcite provides an efficient way to carry out time evolution and cost function evaluation by one forward sweep through the computational graph , and calculation of the full gradient by one backward sweep .",
    "in contrast to forward accumulation , each derivative is evaluated only once , thus enhancing computational efficiency .",
    "the idea of backward propagation is directly related to the grape algorithm for quantum optimal control pioneered by khaneja and co - workers @xcite , see appendix [ app : gradient ] .",
    "while the original grape algorithm bases minimization exclusively on the fidelity of the final evolved unitary or state , advanced cost functions ( such as @xmath53 through @xmath82 ) require the summation of cost contributions from each intermediate step during time evolution of the system .",
    "such cost functions go beyond the usual grape algorithm , but can be included in the more general backward propagation scheme described above .",
    "[ appendix [ app : gradient ] shows analytical forms for gradients for cost functions that are based on time evolution ( @xmath83 ) . ]",
    "our quantum optimal control implementation utilizes the tensorflow library developed by google s machine intelligence research group @xcite .",
    "this library is open source , and is being extended and improved upon by an active development community .",
    "tensorflow supports gpu and large - scale parallel learning , critical for high - performance optimization .",
    "the simple interface to python allows non - software professionals to implement high - performance machine learning and optimization applications without excessive overhead .",
    "typical machine - learning applications require most of the same building blocks needed for quantum optimal control .",
    "predefined operations , along with corresponding gradients , include matrix addition and multiplication ; matrix traces ; and vector dot products .",
    "in addition , we have implemented an efficient kernel for approximate evaluation of the matrix exponential and its gradient . using these building blocks ,",
    "we have developed a fast and accurate implementation of quantum optimal control , well - suited to deal with a broad range of engineered quantum systems and realistic treatment of capabilities and limitations of control fields .    in common applications of quantum optimal control , time evolving the system under the schrdinger equation  more specifically , approximating the matrix exponential for the propagators @xmath84 at each time step @xmath8  requires the biggest chunk of computational time . within our matrix - exponentiation kernel ,",
    "we approximate @xmath85 by series expansion , taking into account that the order of the expansion plays a crucial role in maintaining accuracy and unitarity",
    ". the required order of the matrix - exponential expansion generally depends on the magnitude of the matrix eigenvalues relative to the size of the time step .",
    "general - purpose algorithms such as in python s scipy framework accept arbitrary matrices @xmath86 as input , so that the estimation of the spectral radius or matrix norm of @xmath86 , needed for choosing the appropriate order in the expansion , often costs more computational time than the final evaluation of the series approximation itself .",
    "direct series expansion with only a few terms is sufficient for @xmath87 with spectral radius smaller than @xmath88 . in the presence of large eigenvalues ,",
    "series convergence is slow , and it is more efficient to employ an appropriate form of the `` scaling and squaring '' strategy , based on the identity @xmath89^{2^n},\\ ] ] which reduces the spectral range by a factor of @xmath90 at the cost of recursively squaring the matrix @xmath91 times @xcite .",
    "overall , this strategy leads to an approximation of the short - time propagator of the form @xmath92^{2^n},\\ ] ] based on a taylor expansion truncated at order @xmath93 .",
    "more sophisticated series expansions have been discussed @xcite ; in our specific applications we have not found significant improvements in performance from such alternatives to taylor expansion .    as opposed to the challenges of general - purpose matrix exponentiation , matrices involved in a specific quantum control application @xmath94 ,",
    "will typically exhibit similar spectral radii .",
    "thus , rather than attempting to determine individual truncation levels @xmath95 , and performing scaling - and - squaring at level @xmath96 in each time step @xmath8 , we make a conservative choice for global @xmath93 and @xmath91 at the beginning and employ them throughout .",
    "this simple heuristic speeds up matrix exponentiation over the default scipy implementation significantly , primarily due to leaving out the step of spectral radius estimation .    by default",
    ", automatic differentiation would compute the gradient of the approximated matrix exponential via backpropagation through the series expansion .",
    "however , for sufficiently small spectral radius of @xmath86 , we may approximate @xcite @xmath97 neglecting higher - order corrections reflecting that @xmath98 and @xmath99 may not commute .",
    "( higher - order schemes taking into account such additional corrections are discussed in ref .  .",
    ") equation simplifies automatic differentiation : within this approximation , only the same matrix exponential is needed for the evaluation of the the gradient .",
    "we make use of this in a custom routine for matrix exponentiation and gradient - operator evaluation , further improving the speed and memory performance .",
    "the tensorflow library currently has one limitation relevant to our implementation of a quantum optimal control algorithm .",
    "operators and states in hilbert space have natural respresentations as matrices and vectors which are generically complex - valued .",
    "tensorflow , designed primarily for neural network problems , has currently only limited support for complex matrices . for now",
    ", we circumvent this obstacle by mapping complex - valued matrices to real matrices via the isomorphism @xmath100 , and state vectors @xmath101 . here",
    ", @xmath102 is the 2@xmath1032 unit matrix and @xmath104 one of the pauli matrices . real and imaginary part of the matrix @xmath105",
    "are denoted by @xmath106 and @xmath107 , respectively ; similarly , real and imaginary parts of state vectors are @xmath108 and @xmath109 . written out in explicit block matrix form , this isomorphism results in @xmath110 rendering all matrices and vectors real - valued .",
    "for the hamiltonian matrix , this currently implies a factor two in memory cost ( due to redundancy of real and imaginary part entries ) .",
    "there are promising indications that future tensorflow releases may improve complex - number support and eliminate the need for a mapping to real - valued matrices and vectors .",
    "obtaining a fair comparison between cpu - based and gpu - based computational performance is notoriously difficult @xcite .",
    "we attempt to provide a specific comparison under a unified computation framework .",
    "tensorflow allows for straightforward switching from running code on a cpu to a gpu . for each operation ( matrix multiplication , trace , etc . )",
    ", we use the default cpu / gpu kernel offered by tensorflow . note that properly configured , tensorflow automatically utilizes all threads available for a given cpu , and gpu utilization is found to be near 100% . not surprisingly , we observe that the intrinsic parallelism of gpu - based matrix operations allows much more efficient computation beyond a certain hilbert space size , see fig .  [",
    "fig : benchmark ] .    in this example",
    ", we specifically inspect how the computational speed scales with the hilbert space dimension when optimizing an @xmath91-spin hadamard transform gate and @xmath91-spin ghz state preparation for a coupled chain of spin-1/2 systems presented in section [ sec : spin_chain_example ] .",
    "( details of system parameters are described in the same section . )",
    "we benchmark the average runtime for a single iteration for various spin - chain sizes and , hence , hilbert space dimensions .",
    "we find that the gpu quickly outperforms the cpu in the unitary gate problem , even for a moderate system size of @xmath111 basis states .",
    "for optimization of state transfer , we observe that speedup from gpu usage , relative cpu performance , sets in for slightly larger system sizes of approximately @xmath112 basis states .",
    "the distinct thresholds for the gpu / cpu performance gain stem from the different computational complexities of gate vs.  state - transfer optimization .",
    "namely , optimizing unitary gates requires the propagation of a unitary operator ( a matrix ) , involving matrix - matrix multiplications , while optimizing state transfer only requires the propagation of a state ( a vector ) , involving only matrix - vector multiplications : @xmath113 computing the matrix - vector multiplication is generally much faster than computing the matrix exponential itself @xcite . for an @xmath91-dimensional matrix , the computation of the matrix exponential involves matrix - matrix multiplication , which scales as @xmath114 .",
    "the computation of state transfer only involves matrix - vector multiplication , which scales as @xmath115 [ or even @xmath116 for sufficiently sparse matrices ] .    for optimization of the hadamard transform as well as the ghz state preparation ,",
    "we observe a 19-fold gpu speedup for a 10-qubit system ( hilbert space dimension of 1,024 ) in the former case , and a 6-fold gpu speedup for an 11-qubit system ( hilbert space dimension of 2,048 ) in the latter case .",
    "since matrix operations are the most computationally intensive task in our software , this speedup is comparable to other gpu application studies that heavily use matrix operation @xcite .",
    "we emphasize that these numbers are indicative of overall performance trends , but detailed numbers will certainly differ according to the specific system architecture in place .",
    "the cpu model we used was an intel^^ core^^ i7 - 6700k cpu @ 4.00ghz , and the gpu model was an nvidia^^ tesla^^ k40c .",
    "in this study , all computations are based on dense matrices",
    ". since most physically relevant hamiltonians are sparse ( evolution generally affects sparsity , though ) , future incorporation of sparse matrices may further improve computation speed for both cpu and gpu @xcite .",
    "in this last section , we present a set of example applications of experimental relevance .",
    "the first application demonstrates the importance of cost functions suppressing intermediate occupation of higher - lying states during time evolution , as well as cost functions accounting for realistic pulse shaping capabilities . in a second application",
    ", we show how the cost function @xmath117 can yield high - fidelity state transfer within a reduced time interval .",
    "third , we discuss the application of schrdinger - cat state preparation  an example from the context of quantum optics and of significant interest in recent schemes aiming at quantum information processing based on such states @xcite .",
    "this application combines considerable system size with a large number of time steps , and utilizes most of the cost functions discussed in section [ sec : costfunctions ] . in the fourth application",
    ", we demonstrate the algorithm performance in finding optimal solutions for ghz state preparation and implementation of a hadamard transform gate in a chain of qubits with a variable number of qubits .",
    "we use either the adam @xcite or l - bfgs - b optimization algorithm @xcite for pulse optimization , and achieve a minimum fidelity of 99.9@xmath118 in all of our following examples .      in the first example , we study realization of a 2-qubit cnot gate in a system of two coupled , weakly anharmonic transmon qubits . for each transmon",
    "qubit @xmath119 @xcite , we take into account the lowest two states spanning the qubit computational space , as well as the next three higher levels .",
    "the system hamiltonian , including the control fields @xmath120 , then reads @xmath121\\\\\\nonumber      & \\quad       + j(b_{1}+b_{1}^{\\dagger})(b_{2}+b_{2}^{\\dagger})\\\\\\nonumber      & \\quad +   \\omega_{x_1 } ( t)(b_{1}+b_{1}^{\\dagger } )      +   \\omega_{x_2 } ( t)(b_{2}+b_{2}^{\\dagger } ) + \\omega_{z_2 } ( t)b_{2}^{\\dagger}b_{2}.\\end{aligned}\\ ] ] here , the ladder operators @xmath122 , and @xmath123 are truncated at the appropriate level .",
    "( the qubit frequencies @xmath124 are chosen as 3.5 and 3.9ghz , respectively ; both transmons have an anharmonicity of @xmath125 ; and the qubit - qubit coupling strength used in the simulation is @xmath126 . )",
    "consistent with recent circuit qed experiments utilizing classical drives as well as parametric modulation , we investigate control fields acting on @xmath127 , and @xmath128 .",
    "we next optimize control fields for the realization of a cnot gate , with transmon qubit @xmath129 acting as the control qubit .",
    "our control - field optimization reaches a prescribed fidelity of 99.9@xmath118 for a @xmath130 gate duration in all cases , as seen in fig .",
    "[ figure : transmon_transmon_cnot ] .",
    "results shown in fig .",
    "[ figure : transmon_transmon_cnot](a ) are obtained with the standard target - gate infidelity cost function ( @xmath80 ) only .",
    "it is evident that the solution encounters two issues : the occupation of the 3rd and 4th excited transmon level ( `` forbidden '' ) is significant , and control fields are polluted by high - frequency components .",
    "including a cost function contribution of type @xmath53 succeeds in strongly suppressing occupation of higher levels , see fig .",
    "[ figure : transmon_transmon_cnot](b ) .",
    "this both reduces exposure to increased relaxation rates and ensures that the evolution is minimally influenced by our numerical truncation of hilbert space . in the final improvement step , shown in fig .",
    "[ figure : transmon_transmon_cnot](c ) , our optimization additionally suppresses excessive control amplitudes and derivatives via cost contributions of type @xmath48 and @xmath50 .",
    "the inclusion of these terms in the overall cost lessens superfluous `` noise '' in the control signals , and also helps improve convergence of the algorithm  without reducing the achieved target - gate fidelity .     or @xmath82 ( see table i ) , the optimizer achieves target state preparation in a shorter time , without loss of fidelity . ]      in this second example , we illustrate the use of cost function contributions ( types @xmath117 , @xmath82 ) in minimizing the time needed to perform a specific gate or prepare a desired state . to this end",
    ", we consider a two - level spin qubit ( @xmath133 : @xmath134 ) .",
    "the system and control hamiltonians combined are taken to be @xmath135 we allow for a control field acting on the qubit @xmath136 degree of freedom , and constrain the maximum control - field strength @xmath137 to @xmath138 . when the evolution time needed to perform the state transfer is fixed ( rather than subject to optimization itself )",
    ", we observe that control fields generically spread across the prescribed gate duration time .",
    "the desired target state is realized only at the very end of the allowed gate duration .",
    "when we incorporate a @xmath117 or @xmath82-type cost contribution , the optimal control algorithm also aims to minimize the overall gate duration , so as to realize the target unitary or state in as short a time as possible , given other active constraints . in our example , this reduces the time for a state transfer from @xmath139 to less than @xmath140 , see fig .  [ figure : spin_pi_speed_up ] .",
    "we note that it is further possible to adaptively change the overall simulation time during optimization .",
    "for instance , if further optimization was desired in the case of fig .",
    "[ figure : spin_pi_speed_up](b ) , then the simulation time interval could be adaptively reduced to @xmath141  resulting in a significant cutback in overall computation time .          as an example of quantum state transfer , we employ our optimal control algorithm to the task of generating a photonic schrdinger - cat state .",
    "the system we consider to this end is a realistic , and recently studied @xcite circuit qed setup , consisting of a transmon qubit capacitively coupled to a three - dimensional microwave cavity .",
    "external control fields are restricted to the qubit .",
    "working in a truncated subspace for the transmon ( limiting ourselves to levels with energies well below the maximum of the cosine potential ) , the full hamiltonian describing the system is @xmath142 here , @xmath143 and @xmath144 are the usual lowering operators for photon number and transmon excitation number , respectively .",
    "the frequencies @xmath145 and @xmath125 denote the transmon 0 - 1 splitting and its anharmonicity .",
    "the frequency of the relevant cavity mode is taken to be @xmath146 .",
    "qubit and cavity are coupled , with a strength parameterized by @xmath147 .",
    "note that the rotating wave approximation is not applied in order to reflect the capabilities of modern arbitrary waveform generation .",
    "the state - transfer task at hand , now , is to drive the joint system from the zero - excitation state @xmath148 ( the ground state if counter - rotating terms in the coupling are neglected ) to the photonic cat state @xmath149 . here",
    ", the cat state in the resonator corresponds to a superposition of two diametrically displaced coherent states : @xmath150 .",
    "coherent states are defined in the usual way as normalized eigenstates of the photon annihilation operator @xmath143 , and correspond to displaced vacuum states @xmath151 .",
    "the cat state @xmath152 is approximately normalized for sufficiently large @xmath153 . as our concrete target state",
    ", we choose a cat state with amplitude @xmath154 ( normalization error of @xmath155 0.03% ) .",
    "the state transfer is to be implemented by control fields @xmath156 and @xmath157 acting on the transverse and longitudinal qubit degrees of freedom , @xmath158 and @xmath159 , respectively . matching experimental realizations of multi - mode cavity qed systems@xcite ,",
    "we do not allow for any direct control of the cavity degrees of freedom .",
    "this state - transfer problem provides an excellent test for an optimal control algorithm .",
    "it incorporates the simultaneous challenges of significant hilbert space size [ in our simulation , the overall dimension is @xmath160 , a large number of time steps ( 8,000 ) , a considerable evolution time ( @xmath161 ) , and the application of most of the cost functions we discussed in sect .",
    "[ sec : costfunctions ] and summarized in table [ table : cost_functions ] .",
    "specifically , in addition to minimizing the target state infidelity ( @xmath38 ) , we penalize occupation of transmon levels 3 to 6 and cavity levels 20 and 21 ( @xmath53 ) to avoid artifacts from truncation , and penalize control variations ( @xmath50 ) . ) was not included in this example . ]",
    "results from the optimization are presented in [ figure : cat_state ] , which shows the control - field sequence , as well as the induced state evolution . at the end of the @xmath161 time interval , the control fields generate the desired cat state with a fidelity of 99.9% .",
    "the maximum populations at the truncation levels of transmon and cavity are @xmath162 and @xmath163 , respectively .",
    "we independently confirm convergence with respect to truncation by simulating the obtained optimized pulse for enlarged hilbert space ( 8 transmon and 23 cavity levels ) , and find that the evolution continues to reach the target state with 99.9% fidelity .",
    "we present a final set of examples illustrating the algorithm performance for increasing system size . to that end",
    ", we consider a coupled chain of @xmath5 qubits , or spin-1/2 systems .",
    "we assume that all spins are on - resonance in the multiple - rotating frame .",
    "this system is described by the hamiltonian @xmath164,\\ ] ] where the coupling term is understood to be dropped for the final summand ( @xmath165 ) .",
    "the qubit - qubit coupling strength is fixed to @xmath166mhz .",
    "each qubit @xmath167 is controlled via fields @xmath168 and @xmath169 , with a maximum allowed drive strength of @xmath170mhz .    as a first optimization task",
    ", we search for control fields to implement the unitary realizing a hadamard transform , commonly used in various quantum algorithms .",
    "the gate time we allow for the hadamard transform is @xmath171 , simulated with @xmath172 time steps .",
    "figure [ figure : spin_chain_ghz_hadamard](a ) shows the number of iterations and wall - clock time required to converge to the desired 99.9% process fidelity . for the same spin - chain system ,",
    "we have also employed our code to optimize control fields for transferring the system ground state to a maximally entangled ghz state .",
    "the overall time and time steps we allow for the ghz state preparation is identical to that used for the hadamard transform gate . figure [ figure : spin_chain_ghz_hadamard](b ) shows the number of iterations necessary and the total wall - clock time spent for reaching convergence to a result with 99.9% state fidelity . for both examples",
    ", we employed computation on either cpu or gpu , depending which one is faster .",
    "( this performance benchmarking data was shown in section [ sec : performance_benchmarking ] ) .",
    "we note that , when using a modest desktop pc with graphics card , optimal control problems for small hilbert space size converge within seconds . for a 10-qubit hadamard gate ( hilbert space dimension of 1,024 ) or 11-qubit ghz state ( hilbert space dimension of 2048 ) , it takes @xmath1551 day to obtain a solution meeting the 99.9% fidelity threshold .",
    "( the total wall - clock time could likely have been reduced significantly by starting from a more targeted choice of the initial control fields . ) finding optimal control fields for a large system is challenging not only because of the exponentially growing hilbert space dimension , but also because of the increasing number of control fields .",
    "both factors inflate computation time and the difficulty in converging to the target gate .",
    "in conclusion , we have presented a quantum optimal control algorithm harnessing two key technologies that enable fast and low - overhead numerical exploration of control signal optimization .",
    "first , we have demonstrated that automatic differentiation can be leveraged to facilitate effortless inclusion of diverse optimization constraints , needed to obtain realistic control signals tailored for the specific experimental capabilities at hand .",
    "automatic differentiation dramatically lowers the overhead for adding new cost functions , as it renders analytical derivations of gradients unnecessary . for illustration",
    ", we have presented concrete examples of optimized unitary gates and state transfer , using cost functions relevant for applications in superconducting circuits and circuit qed .",
    "we emphasize that this is but one instance within a much larger class of quantum systems for which optimal control is instrumental , and the methods described here are not limited to the specific examples shown in this paper .    the second key technology we have incorporated is the implementation of gpu - based numerical computations , which offers a significant speedup relative to conventional cpu - based code . the use of the tensorflow library @xcite hides the low - level details of gpu acceleration , allowing implementation of new cost functions at a high level .",
    "the reduction in computational time will generally depend on a number of factors including system type , hilbert space size , and the specific hardware employed by the user .",
    "we observe that runtime speedup by an order of magnitude is not unusual when using a standard desktop pc , enabling the development of sophisticated quantum control without enormous investments into powerful computing equipment .",
    "the underlying libraries also have support for high - performance distributed computing systems for larger optimizations .",
    "our software implementation is open source and can be downloaded at : github.com/schusterlab/quantum-optimal-control .    the increased efficiency and ease of optimal quantum control due to the employment of gpus and automatic differentiation makes our work valuable to a broad range of research .",
    "future work will address sparse - matrix implementations , as well as the deployment of adaptive step size and runge - kutta methods for time evolution .",
    "the authors thank j.  kutasov for help in comparing optimization performance with existing frameworks as well as t.  berkelbach and d.  mazziotti for discussions .",
    "we gratefully acknowledge the support of nvidia^^ corporation through the donation of the tesla^^ k40 gpu used for this research , and support from the david and lucille packard foundation . this material is based upon work supported by the department of defense under contracts h98230 - 15-c0453 and w911nf-15 - 1 - 0421 .",
    "in the following , we outline the analytical calculation of gradients for cost functions such as those summarized in table [ table : cost_functions ] . we stress that our automatic - differentiation implementation evaluates these gradients autonomously , without the need of these analytical derivations or hard - coding any new gradients .",
    "the following derivations are thus merely intended as illustrations for a better mathematical understanding ( and appreciation ) of the gradients calculated without user input by means of automatic differentiation .    for a systematic treatment of the different types of cost functions",
    ", we note that most cost functions involve an absolute value squared of an inner product between target and final states or target and final unitaries ( hilbert - schmidt inner product ) . to obtain the gradients of expressions such as @xmath173 with respect to the control parameters ,",
    "we note that control parameters enter via the final states or unitaries through the evolution operators , @xmath174 . to streamline our exposition , we first summarize multiple matrix - calculus relations of relevance .",
    "consider two complex - valued matrices @xmath175 and @xmath176 , compatible in row / column format such that the matrix product @xmath177 is defined .",
    "then , one finds @xmath178 throughout this appendix , we use einstein convention for summation , and follow the jacobian formulation ( also known as numerator layout ) for derivatives with respect to matrices .",
    "we will further encounter expressions of the following form , involving a third matrix @xmath55 of the same dimensions as @xmath179 : @xmath180}{\\partial b } c\\bigg]= \\frac{\\partial [ \\operatorname{tr}(ab ) \\operatorname{tr}(ab)^*]}{\\partial b_{ji } } c_{ji}\\\\\\nonumber & \\qquad= \\frac{\\partial \\operatorname{tr}(ab)}{\\partial b_{ji } } \\operatorname{tr}(ab)^ * c_{ji } = a_{ij }   \\operatorname{tr}(ab)^ * c_{ji}\\\\ & \\qquad = \\operatorname{tr}(a c ) \\operatorname{tr}(ab)^*. \\label{eq : trace_calculus_3}\\end{aligned}\\ ] ] in the usual way , derivatives treat quantities @xmath181 and @xmath182 as independent variables , and eq .",
    "is used in the step from line 1 to line 2 .",
    "the evaluation of cost - function gradients requires the application of the chain rule to expressions of the type @xmath183 . here",
    ", @xmath184 maps a complex - valued @xmath185 matrix @xmath86 ( e.g. , the propagator @xmath30 with @xmath186 denoting the hilbert space dimension ) to a real number ( the cost ) .",
    "the matrix @xmath187 itself depends on the real - valued control parameters @xmath188 .",
    "the subscript in @xmath189 is understood as a multi - index @xmath190 encoding the control - field label @xmath191 and discretized - time index @xmath79 .",
    "the matrix - calculus result @xmath192 is straightforward to derive with the `` regular '' chain rule by re - interpreting the functions involved as @xmath193 and @xmath194 . in the following ,",
    "eqs .   and",
    "are used to obtain the analytical expressions for several examples of cost - function gradients .",
    "the cost function @xmath195|^2 $ ] , penalizes the infidelity of the realized unitary @xmath196 with respect to to the target propagator @xmath29 .",
    "it has the gradient @xmath197}{\\partial k_n}\\frac{\\partial k_n}{\\partial u_{k , j}}\\bigg ] + \\text{c.c . }",
    "\\\\\\nonumber & \\quad\\overset{\\eqref{eq : trace_calculus_3}}= -\\operatorname{tr}\\bigg ( k_t^\\dag \\frac{\\partial k_n}{\\partial u_{k , j}}\\bigg ) \\operatorname{tr } ( k_t^\\dag k_n)^ * + \\text{c.c . }",
    "\\\\\\nonumber & \\quad =   \\operatorname{tr}\\bigg(k_t^\\dagger \\big[\\prod_{j'>j } u_{j'}\\big ] i\\ , \\delta t\\ , \\mathcal{h}_k k_{j}\\bigg)\\operatorname{tr}(k_t^\\dagger k_{n})^ * + \\text{c.c.}\\\\\\nonumber & \\quad =   -2\\,\\delta t \\operatorname{im}\\bigg \\ {   \\operatorname{tr}\\bigg (   k_{t}^\\dagger \\big [   \\prod_{j'>j } u_{j ' } \\big]\\ , \\mathcal{h}_k   k_{j } \\bigg ) \\operatorname{tr}(k_t^\\dagger k_{n})^ *   \\bigg \\ } \\ ] ] where @xmath198 is understood to produce a time - ordered product .",
    "this expression shows that automatic reverse - mode differentiation requires the propagators @xmath13 from every time step . within tensorflow , the set of intermediate propagators @xmath199",
    "is stored in memory during the forward evolution .",
    "the resulting memory demand therefore scales as @xmath200 .",
    "_ alternative memory - efficient algorithm._ we note that storage of @xmath199 can be avoided by applying the strategy introduced in the original grape paper @xcite : since the evolution is unitary , one may time - reverse the evolution step by step , and re - calculate the intermediate propagator via @xmath201 . here , each short - time propagator @xmath84 is re - generated locally in time , using only the control fields at time @xmath8 .",
    "such a backwards - propagation algorithm leads to an increase in computation time by roughly a factor of 2 ( each @xmath84 is then calculated twice ) , but has a memory demand of only @xmath202  which does not scale with @xmath5 , the number of time steps .",
    "this memory - efficient algorithm , currently not realized in this implementation , is given by        for state preparation or unitaries specified only in a subspace , it is sufficient to optimize the evolution for only a few initial states , rather than for the complete basis .",
    "this is achieved by minimizing a cost function based on @xmath209 , where the realized final state @xmath210 depends on the control parameters @xmath63 . again applying equations followed by ( and using that the trace of a number results in that number ) , we obtain @xmath211   \\mathcal{h}_k   | \\psi_{j}\\rangle   \\langle \\psi_t | \\psi_{n}\\rangle^ *   \\bigg ] \\ ] ]    _ alternative memory - efficient algorithm._ in tensorflow - based automatic differentiation algorithm here , the intermediate states @xmath212 are stored , leading to a memory requirement of @xmath213 , rather than @xmath200 for the full propagators . by using the same backward propagation strategy as above , a more memory - efficient algorithm with memory requirement @xmath214 independent of the time - step number is possible :        occupation of a `` forbidden '' state",
    "is discouraged by the cost function @xmath219 .",
    "this cost function differs qualitatively from the gate and state infidelity cost functions : the latter are evaluated based on the result at the final time , while forbidden - state occupation involves intermediate states at every time step .",
    "accordingly , the corresponding gradient takes a different form .",
    "first , eq .",
    "is replaced by @xmath220 where introduction of the trace of a @xmath184-number is convenient for direct application of eq .  .",
    "we then obtain @xmath221}{\\partial \\psi_{j}}\\frac{\\partial \\psi_{j}}{\\partial u_{k , j } } \\bigg ] + \\text{c.c.}\\\\\\nonumber & \\quad\\overset{\\eqref{eq : trace_calculus_3}}= \\sum_{j\\ge j } \\operatorname{tr}\\big(\\psi_f^\\dag\\frac{\\partial \\psi_{j}}{\\partial u_{k , j}}\\big ) \\operatorname{tr}(\\psi_f^\\dag \\psi_{j})^ * + \\text{c.c . }",
    "\\\\\\nonumber & \\quad=2\\,\\delta t \\sum_{j\\ge j } \\operatorname{im}\\bigg",
    "[ \\big\\langle \\psi_f\\big|\\big[\\textstyle\\prod_{j'=j+1}^{j } u_{j ' } \\big ]",
    "\\mathcal{h}_k \\big| \\psi_{j } \\big\\rangle \\big\\langle \\psi_{j}\\big|\\psi_f\\big\\rangle \\bigg ] \\ ] ] the double sum of eq . makes it appear as though the computation of this gradient would take @xmath222 , however after simplification , the relationship between the limits of the sum and product allow it to be calculated in @xmath223 time .",
    "the corresponding backward propagation algorithm then takes the following form :      this cost function and gradient is also used as the time - optimal award function , using a negative cost to reward rather than penalize the target state at every time step ( rather than just at the end ) .",
    "the gradients of cost functions involving only control fields do not involve the time propagation , so we also omit their derivation .",
    "algorithms for each cost function along with their computation and memory costs have been presented .",
    "the computation time of the algorithms all scale linearly with the number @xmath5 of time steps .",
    "automatic gradient calculation which requires caching of each step causes memory to scale like @xmath5 , while reducing the run time by a constant factor of 2 .",
    "by contrast , algorithms which directly exploit the unitary structure of quantum evolution can have memory requirements do not scale with the number of time steps .",
    "hence , it may be worth implementing analytic gradients for very long computations which otherwise would not fit in memory .",
    "computing the fidelity and gradient for the whole unitary evolution as in algorithm 1 , requires @xmath202 , whereas state transfer requires @xmath214 memory .",
    "it should be noted that full unitary evolution fidelity can also be calculated as @xmath230 state transfer computations over a complete basis .",
    "this has the memory requirements of state transfer , and the same computation requirements as algorithm 1 , though is less efficient by a constant factor . in principle",
    ", each state transfer can be performed in parallel and assembled to compute the total cost and gradient .",
    "in addition , the hamiltonians of many physical problems can be represented sparsely allowing a significant speedup in computation as well . for practical problems , the number time steps required may scale with the size of the problem , as more complex quantum gates / algorithms require more time than simple ones .",
    "80ifxundefined [ 1 ] ifx#1 ifnum [ 1 ] # 1firstoftwo secondoftwo ifx [ 1 ] # 1firstoftwo secondoftwo `` `` # 1''''@noop [ 0]secondoftwosanitize@url [ 0 ]  + 12$12  & 12#1212_12%12@startlink[1]@endlink[0]@bib@innerbibempty link:\\doibase 10.1140/epjd%252fe2015 - 60464 - 1 [ * * ,   ( ) ] link:\\doibase 10.1016/j.jmr.2004.11.004 [ * * ,   ( ) ] link:\\doibase 10.1016/j.jmr.2011.07.023 [ * * ,   ( ) ] link:\\doibase 10.1103/physreva.66.053619 [ * * ,   ( ) ] link:\\doibase 10.1103/physreva.83.053426 [ * * ,   ( ) ] link:\\doibase 10.1103/physrevlett.101.073002 [ * * ,   ( ) ] link:\\doibase 10.1088/1367 - 2630/11/10/105032 [ * * ,   ( ) ] link:\\doibase 10.1063/1.3691827 [ * * ,   ( ) ] ,   link:\\doibase 10.1103/physreva.68.062308 [ * * ,   ( ) ] in  link:\\doibase 10.1007/978 - 1 - 4899 - 2326 - 4_24 [ _ _ ] ,  , vol .",
    ",  ( ,  )  pp .",
    "link:\\doibase 10.1063/1.475576 [ * * ,   ( ) ] link:\\doibase 10.1063/1.476575 [ * * , ( ) ] link:\\doibase 10.1063/1.1650297 [ * * ,   ( ) ] link:\\doibase 10.1063/1.1564043 [ * * ,   ( ) ] link:\\doibase    10.1103/physreva.75.033407 [ * * ,   ( ) ] link:\\doibase 10.1016/j.cam.2007.04.029 [ * * ,   ( ) ] link:\\doibase 10.1016/j.cpc.2007.09.007 [ *",
    "* ,   ( ) ] http://portal.acm.org/citation.cfm?id=541500[__ ] ,  ed .",
    "( , ,  ) in  link:\\doibase 10.1109/ijcnn.1989.118638 [ _ _ ]  ( ,  ) pp .   link:\\doibase 10.1016/j.patcog.2004.01.013 [ * * ,   ( ) ] in link:\\doibase 10.1145/1390156.1390170 [ _ _ ] ,  ( , ,  )  pp . in  link:\\doibase 10.1007/978 - 3 - 540 - 88693 - 8_44 [ _ _ ] ,  ,",
    "vol .  ,  ( ,  ,  )  chap .  , pp .   in link:\\doibase 10.1145/1553374.1553486",
    "[ _ _ ] ,  ( ,  ,  ) pp .   in link:\\doibase 10.1109/icdar.2005.251 [ _ _ ] ,  ( ,  ,  )  pp .   in  link:\\doibase 10.1109/icpads.2009.8 [ _ _ ]",
    "( ,  , )  pp .   in",
    "link:\\doibase 10.1145/1058129.1058148 [ _ _ ] ,  ( ,  ,  )  pp .",
    "link:\\doibase 10.1038/ncomms7979 [ * * ,   ( ) ] link:\\doibase    10.1038/nature18648 [ * * ,   ( ) ] link:\\doibase 10.1038/nature14270 [ * * , ( ) ] http://www.keysight.com/en/pd-2583267-pn-m8196a/92-gsa-s-arbitrary-waveform-generators?nid=-32928.1139580&#38;cc=us&#38;lc=eng[__ ] ( ) link:\\doibase 10.1016/j.cpc.2012.02.021 [ * * ,   ( ) ] ,   link:\\doibase 10.1016/j.cpc.2012.11.019 [ * * ,   ( ) ] link:\\doibase    10.1103/physreva.84.022305 [ * * ,   ( ) ] link:\\doibase 10.1016/j.jmr.2010.11.008 [ * * ,   ( ) ] link:\\doibase    10.1016/j.jmr.2008.11.020 [ * * , ( ) ] link:\\doibase 10.1016/j.jmr.2010.09.003 [ * * ,   ( ) ] link:\\doibase 10.1103/physreva.75.012302 [ * * ,   ( ) ] link:\\doibase 10.1103/physrevb.79.060507 [ * * ,   ( ) ] link:\\doibase 10.1103/physreva.79.012312 [ * * , ( ) ] link:\\doibase 10.1103/physrevlett.112.240504 [ * * ,   ( ) ] http://arxiv.org/abs/1507.04261 [ `` , ''  ] ( ) ,   link:\\doibase 10.1088/0953 - 2048/27/1/014001 [ * * ,   ( ) ] link:\\doibase 10.1103/physrevapplied.6.024022 [ * * ,   ( ) ] link:\\doibase 10.1103/physrevlett.102.090401 [ * * , ( ) ] link:\\doibase 10.1103/physrevlett.112.240503 [ * * ,   ( ) ] link:\\doibase 10.1103/physreva.88.052326 [ * * ,   ( ) ] link:\\doibase 10.1038/ncomms4371 [ * * ( ) ,  10.1038/ncomms4371 ] link:\\doibase 10.1103/physrevlett.105.020501 [ * * , ( ) ] link:\\doibase    10.1103/physreva.91.062306 [ * * ,   ( ) ] link:\\doibase 10.1103/physreva.91.062307 [ * * ,   ( ) ] http://arxiv.org/abs/1606.08825 [ `` , ''  ] ( ) ,   http://view.ncbi.nlm.nih.gov/pubmed/14987600 [ * * ,   ( ) ] link:\\doibase    10.1016/j.jmr.2004.06.017 [ * * , ( ) ] link:\\doibase    10.1016/j.jmr.2008.05.023 [ * * , ( ) ] link:\\doibase 10.1103/physreva.84.022307 [ * * ,   ( ) ] ,   link:\\doibase 10.1103/physreva.92.063415 [ * * ,   ( ) ] link:\\doibase 10.1103/physreva.71.053810 [ * * ,   ( ) ] link:\\doibase 10.1016/s0377 - 0427(00)00422 - 2 [ * * ,   ( ) ] link:\\doibase 10.1145/355586.364791 [ * * , ( ) ] http://arxiv.org/abs/1608.02430 [ `` , ''  ] ( ) ,   link:\\doibase 10.1016/j.cplett.2005.09.062 [ * * ,   ( ) ] link:\\doibase 10.1103/physreva.77.063412 [ * * ,   ( ) ] link:\\doibase 10.1103/physreva.76.042319 [ * * ,   ( ) ] , link:\\doibase 10.1126/science.1175552 [ * * ,   ( ) ] ,   link:\\doibase    10.1137/0916069 [ * * ,   ( ) ] http://arxiv.org/abs/1412.6980 [ `` , ''  ] ( ) ,   http://arxiv.org/abs/1502.05767 [ `` , ''  ] ( ) ,   http://arxiv.org/abs/1603.04467 [ `` , ''  ] ( ) ,   http://epubs.siam.org/doi/abs/10.1137/s00361445024180 [ * * ,   ( ) ] link:\\doibase 10.1016/0024 - 3795(94)00190 - 1 [ * * ,   ( ) ] link:\\doibase 10.1016/0021 - 9045(69)90030 - 6 [ * * ,   ( ) ] link:\\doibase    10.1145/1816038.1816021 [ * * ,   ( ) ] link:\\doibase 10.1145/285861.285868 [ * * ,   ( ) ] http://arxiv.org/abs/1605.02688 [ * * ( ) ] @noop _ _ ,   ( ,  ) in  link:\\doibase 10.1109/ipdps.2014.47 [ _ _ ]  ( ,  )  pp .",
    "link:\\doibase    10.1088/1367 - 2630/16/4/045014 [ * * ,   ( ) ] ,   link:\\doibase    10.1126/science.1243289 [ * * ,   ( ) ] link:\\doibase    10.1103/physrevlett.114.080501 [ * * ,   ( ) ]"
  ],
  "abstract_text": [
    "<S> we implement a quantum optimal control algorithm based on automatic differentiation and harness the acceleration afforded by graphics processing units ( gpus ) . </S>",
    "<S> automatic differentiation allows us to specify advanced optimization criteria and incorporate them in the optimization process with ease . </S>",
    "<S> we show that the use of gpus can speed up calculations by more than an order of magnitude . </S>",
    "<S> our strategy facilitates efficient numerical simulations on affordable desktop computers , and exploration of a host of optimization constraints and system parameters relevant to real - life experiments . </S>",
    "<S> we demonstrate optimization of quantum evolution based on fine - grained evaluation of performance at each intermediate time step , thus enabling more intricate control on the evolution path , suppression of departures from the truncated model subspace , as well as minimization of the physical time needed to perform high - fidelity state preparation and unitary gates . </S>"
  ]
}