{
  "article_text": [
    "many methods , e.g. , coupled cluster and mller - plesset perturbation theory , can accurately and efficiently treat the electronic correlation of single - reference ( weakly - correlated ) systems .",
    "in particular , coupled cluster with singles , doubles and perturbative triples ( ccsd(t ) ) is very accurate for such systems and is often referred to as the  gold standard \" of quantum chemistry . however , these methods fail catastrophically when applied to multireference ( strongly - correlated ) systems , such as molecules undergoing chemical reactions or systems containing transition metal atoms with partially filled _ d _ or _ f _ orbitals .",
    "one common approach for tackling such multireference problems is to abandon the hartree - fock wavefunction and instead use a multideterminantal reference wavefunction obtained by correlating a subset of orbitals around the fermi surface .",
    "examples include the complete active space ( cas ) method , in which all possible occupancies of orbitals within the active space are included , and the restricted / generalized active space ( ras / gas ) methods@xcite , in which further restrictions are placed on the occupancies of the active orbitals in order to reduce the size of the hilbert space .",
    "the cas method is limited to about 16 active electrons and orbitals .",
    "other possibilities include highly accurate but approximate methods such as the density matrix renormalization group ( dmrg )  @xcite , full configuration interaction quantum monte carlo ( fciqmc )  @xcite , and its semistochastic improvement ( s - fciqmc )  @xcite , which routinely treat up to about 40 - 50 active orbitals .",
    "a well - chosen active space often results in a reference wavefunction that contains qualitatively correct physics .",
    "however , quantitative accuracy requires one to take into account the dynamical correlation by allowing excitations into inactive - space orbitals .",
    "common methods for including dynamical correlation include multireference configuration interaction ( mrci ) and its size - consistent variants@xcite , various flavors of multireference perturbation theory@xcite , and multireference coupled cluster theories@xcite .",
    "the accuracy of these methods is often limited by the fact that only a relatively small number of active space orbitals can be used to obtain the reference wavefunction because the cost of enlarging the active space increases exponentially with the number of orbitals .",
    "although the number of determinants in a cas scales combinatorially with the number of active electrons and orbitals , many of these determinants are `` configurational deadwood , '' and do not contribute appreciably to the reference wavefunction .",
    "the so - called _ selected configuration interaction _ ( sci ) methods , which have been in use for more than four decades  @xcite , take advantage of this fact and generate a reference wavefunction by selecting only important determinants , rather than including all determinants in the cas .",
    "a subset of these methods improve upon the variational energy by employing a perturbative correction to the energy using multireference epstein - nesbet perturbation theory .",
    "we refer to these methods as _ selected configuration interaction plus perturbation theory _",
    "( sci+pt ) methods . the first such method was called _",
    "configuration interaction perturbing a multi - configurational zeroth - order wavefunction selected iteratively _",
    "( cipsi )  @xcite .",
    "the focus of this paper is on a newly - introduced sci+pt method called _ heat - bath configuration interaction _ ( hci ) .",
    "hci distinguishes itself from other sci+pt techniques by employing an algorithm that greatly improves the efficiency of both the variational and perturbative steps .",
    "although it is more efficient than other sci+pt methods , hci , in its original formulation , is limited by a memory bottleneck because it stores in memory all the determinants that contribute to the perturbative correction  @xcite .    in this paper , we introduce a stochastic implementation of multireference epstein - nesbet perturbation theory , and use it to overcome the memory bottleneck of hci .",
    "this method has several attractive properties .",
    "first , it does not have a sign problem that plagues quantum monte - carlo methods .",
    "second , instead of using the metropolis - hastings method , we use the alias method , so the samples are all uncorrelated .",
    "third , in addition to removing the memory bottleneck , stochastic - hci ( s - hci ) is faster than the deterministic variant for even the smallest system tried in the current work if a stochastic error of 0.1 mha is acceptable . fourth , within the s - hci algorithm one can trade memory for a modest increase in computer time .",
    "fifth , the perturbative calculation is embarrassingly parallel .    in section  [ hci_alg ]",
    "we review the improvements made in the original hci algorithm that make it much more efficient than other sci+pt algorithms . in section  [ stoch_pt ] , we present our stochastic perturbation theory which removes the memory bottleneck of the original hci algorithm . in section  [ implementation ]",
    "we provide various implementation details of both the variational and the perturbative parts of our algorithm .",
    "we then demonstrate the utility of the stochastic - hci ( s - hci ) method by applying it in section  [ results ] to various diatomic molecules including f@xmath0 with an active space of ( 14e , 108o ) , mn - salen cluster with an active space of ( 28e , 22o ) , and cr@xmath0 dimer with up to a quadruple - zeta basis set with an active space of ( 12e , 190o ) , obtaining energies that are accurate to better than 1 mha with very modest computer resources .",
    "finally , in section  [ conclusion ] , we conclude and discuss future research directions .",
    "we begin by describing the hci algorithm in its original formulation  @xcite , emphasizing the key innovations that make it much more efficient than other sci+pt methods . in the following discussion",
    "the indices @xmath1 will be used for determinants in the variational space @xmath2 and the indices @xmath3 will be used for determinants in @xmath4 , the space of determinants that are connected to @xmath2 but not in @xmath2 . similar to other sci+pt methods , hci has two stages : ( 1 ) a variational stage , in which a variational wavefunction is obtained as a linear combination of a set of determinants chosen by an iterative procedure , and ( 2 ) a perturbative stage , in which the second - order correction to the variational energy is computed using multireference epstein - nesbet perturbation theory  @xcite , but each stage is much faster than in other sci+pt methods .",
    "+ at the start of the algorithm , @xmath2 consists of some initial set of determinants , usually just the hartree - fock determinant .",
    "then , at each iteration , new determinants are added to @xmath2 , chosen using a parameter @xmath5 , as follows .",
    "the initial wavefunction is the ground state of the hamiltonian in @xmath2 , @xmath6 . at each iteration :    1 .",
    "add to the variational space @xmath2 , all determinants @xmath7 in the space of connections @xmath4 , such that @xmath8 for at least one determinant @xmath9 in the current @xmath2 .",
    "2 .   calculate the lowest eigenvalue @xmath10 with eigenvector @xmath6 of the hamiltonian in @xmath11 .",
    "the iterations are terminated when the number of new determinants is less than a threshold , e.g. , 1% of the current size of @xmath2 , or when a maximum number of iterations is reached . since the values of @xmath12 tend to be larger in the initial iterations when there are few determinants in @xmath2 , @xmath5 is set during the first few iterations to be larger than its final value .",
    "hci takes advantage of the fact that the double excitation matrix elements depend only on the four orbitals whose occupancy is changing .",
    "step 1 is performed efficiently by storing the double excitation matrix elements in order of decreasing magnitude , so that no time is wasted on determinants that do not meet the cutoff in eq .  [ eq : eps1 ] . for details",
    ", we refer the reader to the original hci paper  @xcite .",
    "thus , we see that hci identifies new determinants to add to @xmath2 in a manner that is more efficient than other sci methods in two ways :    * hci uses a selection criterion which is _ cheap to evaluate _ for each determinant , namely eq .",
    "[ eq : eps1 ] .",
    "in contrast , other sci methods use a criterion based on a perturbative expression which is more expensive ; for example , cipsi  @xcite uses the magnitude of the coefficient of the first - order correction to the wavefunction , namely @xmath13 .",
    "* hci evaluates its selection criterion ( eq .  [ eq : eps1 ] ) _ only for those determinants which will be added to @xmath2 ! _ by comparison , other sci methods iterate through _ all _ candidate determinants @xmath14 ( determinants for which there exists at least one nonzero matrix element @xmath15 with @xmath16 ) , evaluating their expensive selection criteria for each one .",
    "the simplification in hci is possible because it was demonstrated  @xcite that variation in the perturbative expression for the coefficients is dominated by variation in the largest - magnitude term in the numerator , since the matrix elements @xmath17 and coefficients @xmath18 span many orders of magnitude .",
    "the minor deviation from optimality in the choice of the most important determinants is by far outweighed by the fact that many more determinants can be included .",
    "+ the variational wavefunction is used to define the zeroth order hamiltonian , @xmath19 and the perturbation , @xmath20 , @xmath21 it can easily be verified that @xmath22 is the ground state of @xmath19 with eigenvalue @xmath10 .",
    "using the partitioning in eq .",
    "[ eq : part ] , the first order correction of the wavefunction @xmath23 and the second order energy correction @xmath24 can be written as @xmath25 and @xmath26 where @xmath27 .",
    "it is worth noting that the expression for the total energy , @xmath28 is identical to that for the mixed estimator of the energy used in quantum monte carlo calculations , provided that the projected wavefunction is replaced by the perturbed wavefunction .",
    "this expression in eq .",
    "[ eq : ptb ] is expensive to calculate , as it requires a summation over many small terms .",
    "instead , hci includes only those terms in the sum that contribute substantially , @xmath29 where @xmath30 denotes a sum in which all terms in the sum that are smaller in magnitude than @xmath31 are discarded , i.e. , @xmath32 includes only terms for which @xmath33 .    once again , since the double excitation matrix elements are stored in order of decreasing magnitude , no time is spent on terms that do not contribute to the sum .",
    "the parameter @xmath31 is kept much smaller than the parameter @xmath5 because discarding small amplitude determinants can lead to significant errors in the calculation of dynamical correlation . in the original hci paper  @xcite , for each @xmath5 , several values of @xmath31 were used , and the energy for @xmath34 was obtained by extrapolation . in this paper ,",
    "a single value of @xmath31 is used , that is sufficiently small to recover the @xmath34 limit to a precision that is better than 1 mha .",
    "it was shown in the previous publication  @xcite that the above algorithm is highly efficient and can be used to obtain sub - millihartree accuracy for challenging problems like all - electron ( 48e,42o ) cr@xmath0 with the small ahlrichs double - zeta basis  @xcite in a few minutes on a single computer core . however , since the contributions from all @xmath35 in eq .",
    "[ eq : ptc ] must be summed and then squared , the efficient deterministic approach to computing the perturbative correction requires storing the partial sums @xmath36 for all @xmath37 for which @xmath33 which creates a severe memory bottleneck  @xcite . in the following section",
    "we show how by using a stochastic version of the perturbation theory this memory bottleneck can be completely eliminated .",
    "we write the perturbative correction in a slightly different form than presented in eq ( [ eq : ptc ] ) to highlight the fact that it is a bilinear function of the coefficients of the zeroth - order state .",
    "@xmath38    we compute the expected value of this expression stochastically by performing @xmath39 iterations ; in each , we sample the zeroth order wavefunction by selecting @xmath40 determinants @xmath41 from @xmath11 each with probability p_i =",
    ". any given sample will contain @xmath42 distinct determinants @xmath9 with some number of repeats @xmath43 , such that @xmath44 the number of repetitions ( @xmath43 ) is distributed according to the well known multinomial distribution .",
    "the mean and second moment , for @xmath45 , of this distribution are @xmath46 where @xmath47 denotes the expectation value of a quantity evaluated for a sample of @xmath48 determinants , a notation we will use hereafter .    using these expressions , the unbiased estimate of the second order perturbation can be calculated from the sampled wavefunction as follows ,    @xmath49\\nonumber\\\\ = & \\sum_{a } \\frac{1}{e_0 - e_a}\\left[\\sum_{i\\neq j}^{\\v } h_{ai}h_{aj } c_ic_j + \\sum_{i}^{\\v } h_{ai}^2 c_i^2\\right]\\nonumber\\\\ = & \\left\\langle \\sum_{a } \\frac{1}{e_0 - e_a}\\left[\\sum_{i\\neq j}^{\\nddiff } \\frac{w_i w_j c_ic_j h_{ai}h_{aj}}{\\langle w_i w_j \\rangle } + \\sum_{i}^{\\nddiff } \\frac{w_i c_i^2 h_{ai}^2}{\\langle w_i \\rangle}\\right ] \\right\\rangle \\nonumber\\\\ = & \\left\\langle \\sum_{a } \\frac{1}{e_0 - e_a}\\left[\\sum_{i\\neq j}^{\\nddiff } \\frac{w_i w_j c_ic_j h_{ai}h_{aj}}{p_ip_j\\nd(\\nd-1 ) } + \\sum_{i}^{\\nddiff } \\frac{w_i c_i^2 h_{ai}^2}{p_i\\nd}\\right ] \\right\\rangle \\nonumber\\\\ = & \\frac{1}{\\nd(\\nd-1 ) } \\left\\langle \\sum_{a } \\frac{1}{e_0 - e_a } \\left[\\left(\\sum_{i}^{\\nddiff } \\frac { w_i   c_i h_{ai}}{p_i}\\right)^2   + \\sum_{i}^{\\nddiff } \\left(\\frac{w_i(\\nd-1)}{p_i } - \\frac { w_i^2}{p_i^2}\\right)c_i^2 h_{ai}^2\\right ] \\right\\rangle , \\label{eq : stofinal}\\end{aligned}\\ ] ]    where for brevity we have suppressed the superscript @xmath50 on the @xmath35 and @xmath51 sums , though of course we will always use a nonzero @xmath31 value for efficiency .    going from the @xmath52 to the @xmath53 line above , we replace the sum over the states in @xmath2 by a sum over the sample , so in order to have an unbiased expectation value we divide the two terms by @xmath54 and @xmath55 respectively . in going from the @xmath53 to the @xmath56 line we use eqs .",
    "[ expec_wi ] and [ expec_wij ] .    in practice ,",
    "the exact average in eq .",
    "( [ eq : stofinal ] ) will be replaced by an average over @xmath39 iterations . for any @xmath57",
    "we obtain an unbiased estimate of the second order correction to the energy and this estimate can be made progressively more precise by averaging over a large number of iterations @xmath39 .",
    "each batch contains an independently chosen set of @xmath42 determinants and thus there is no autocorrelation between consecutive batches .",
    "this is in sharp contrast to discrete - space quantum monte carlo methods , such as the fciqmc method  @xcite and its semistochastic improvement  @xcite , for which the autocorrelation time increases both with system size and the size of the basis to the point that it can become difficult to accurately estimate the statistical error .",
    "this drawback of the fciqmc method is ameliorated but not eliminated by using the more efficient sampling method of ref .  .",
    "we note that the expression in eq .",
    "( [ eq : stofinal ] ) is evaluated in much the same way as the deterministic evaluation of the perturbative correction using a single batch , the main difference being that the @xmath58 variational determinants have been replaced by the much smaller subset of @xmath42 distinct sampled determinants and that an additional summation is needed to ensure that the result is unbiased .",
    "it is important to note that for a single batch , the summation over @xmath37 in eq .",
    "( [ eq : stofinal ] ) is restricted to only those determinants in @xmath59 that have a non - zero hamiltonian matrix element with the @xmath42 determinants used to sample the zeroth order wavefunction .",
    "figure  [ fig : cpuscaling ] shows that the cpu time per sample increases nearly linearly with @xmath40 , the number of determinants in the sample , for the c@xmath0 and f@xmath0 molecules . as shown in section  [ implementation ] , the scaling contains two terms one that scales linearly with @xmath40 and another that scales as @xmath60 ( ref .  ) .",
    "figure  [ fig : cpuscaling2 ] shows the cpu time necessary to reach a standard deviation of less than 0.1 mha versus the number of determinants in the sampled wavefunction @xmath40 .",
    "there is a rapid initial decrease followed by a much shallower decrease beyond about @xmath61 .",
    "consequently , it is desirable to use as large a value of @xmath40 as memory allows , but the gain from using @xmath62 is minor .",
    "another consideration is that @xmath39 needs to be large enough to get a reasonable estimate of the statistical error , and since the computer time is approximately @xmath63 , it sometimes makes sense to use a smaller @xmath40 than available memory allows . in all the calculations presented in section  [ results ] , we have used @xmath61 .",
    "it is worth mentioning that the memory bottleneck can also be removed without recourse to the stochastic method .",
    "this can be achieved by dividing the @xmath58 determinants in @xmath2 into @xmath39 batches , each containing on average @xmath48 determinants ( @xmath64 since all determinants in @xmath2 need to be in a batch , and computing the contribution from all pairs of batches independently . for large systems the @xmath39 required to get",
    "a statistical error of 1 mha in the stochastic method is much smaller than the @xmath39 required in the deterministic calculation , making the stochastic approach the more efficient choice . in section  [ implementation ]",
    "we will see that the leading cost of performing the calculation for each pair of batches is @xmath65 and so the cost of performing the entire calculation containing @xmath39 batches is @xmath66 .",
    "thus the cost of the deterministic calculation scales linearly with @xmath39 and quickly becomes very expensive .",
    "here we briefly describe the implementation and the leading order cost of the various steps of the algorithm . in the variational stage",
    "there are three main operations , identifying the significant determinants to be included in the variational space , building the hamiltonian matrix and diagonalizing the matrix .",
    "the cost of identifying important determinants is @xmath67 , where @xmath58 is the number of determinants in the variational space , @xmath68 is the average number of @xmath15 elements for determinants @xmath9 in @xmath2 that satisfy eq .",
    "( [ eq : eps1 ] ) , and @xmath69 is the total number of new determinants that satisfy the criterion in eq .",
    "( [ eq : eps1 ] ) .",
    "the two terms in the cost function result from generating @xmath70 determinants and then doing a binary search of the list of the @xmath58 existing variational determinants and the @xmath71 newly generated determinants before including the determinant just generated in the newly generated determinant list .    in the current implementation",
    "we store all the nonzero elements of the hamiltonian in memory using a _ list of lists _ ( lil ) sparse storage format . in lil",
    "format for each row we store a list containing the column index and the value of the nonzero hamiltonian matrix elements .",
    "the determinant labels are bit - packed strings that represent the occupancies of the up - spin ( @xmath72 ) and the down - spin ( @xmath73 ) orbitals .",
    "to build the hamiltonian efficiently , we first generate a list of all unique @xmath73 strings and associated with each @xmath73 string we store a list of all determinants in @xmath2 that have that @xmath73 string .",
    "we also generate a list of all unique @xmath72 strings with @xmath74 electrons and associated with each @xmath72 string we store a list of all determinants in @xmath2 that give the @xmath72 string on removing one @xmath72 electron . here , @xmath75 is the number of @xmath72 electrons in our system .",
    "determinants that are related to each other by double or single @xmath72 excitations have the same @xmath73 string , and all the pairs of determinants that are related to each other with the remaining possible single or double excitations have the same @xmath72 string with @xmath76 electrons . hence to find the connected determinants , only the determinants in these two lists need to be considered rather than the entire set of @xmath58 determinants in @xmath2 .",
    "once the hamiltonian is generated the davidson algorithm is used to diagonalize it and the most expensive step there is the hamiltonian wavefunction multiplication which costs @xmath77 . despite the fact that the hamiltonian is sparse , building it is the most expensive part of the variational step , and storing it is currently the biggest memory bottleneck in the code . in the future",
    "we intend to implement the _ direct _ method for carrying out hamiltonian wavefunction multiplication which does not require storing the hamiltonian and can take less computer time as well@xcite .",
    "the stochastic perturbation step has two major components : sampling @xmath40 determinants from the list of @xmath58 variational determinants , and , identifying the determinants in @xmath4 that are connected to these @xmath40 determinants and computing their contribution to the perturbative correction , eq .",
    "( [ eq : stofinal ] ) .",
    "the @xmath40 determinants are sampled using the alias method  @xcite , which has an initial one - time memory cost of @xmath78 , and a subsequent cost of @xmath79 each time a sample is drawn .",
    "this method was used by some of us  @xcite for efficiently sampling determinants in the s - fciqmc method .",
    "the cpu time for identifying the connected determinants along with their contributions is @xmath80 , while the memory required is @xmath81 .",
    "since the minimum required value of @xmath40 is just two , the memory requirement for the stochastic perturbation theory is smaller than that of other parts of the calculation .",
    "we have parallelized the entire code using hybrid omp / mpi ( open multiprocessing / message passing interface ) programming to make full use of the symmetric multiprocessor ( smp ) architecture of most modern computers .",
    "a separate mpi process is initiated on each cpu and then each process forks into several threads ( one for each computational core ) on the cpu .",
    "the variational wavefunction is replicated on each cpu but a single copy is shared among the different threads on a cpu . as mentioned previously , the most memory intensive data structure is the lil used to store the sparse hamiltonian matrix .",
    "the list of nonzero matrix elements for each row of the hamiltonian is distributed in round - robin fashion between the different cpus . with this strategy",
    "the storage of the hamiltonian and the computation of the hamiltonian wavefunction multiplication is distributed approximately evenly between the different cpus and threads .",
    "the perturbative step is embarrassingly parallel and no special strategy is needed to parallelize this step .",
    "we perform frozen core calculations on a series of first row dimers including c@xmath0 , n@xmath0 , o@xmath0 , no and f@xmath0 with cc - pvdz , cc - pvtz and cc - pvqz basis sets .",
    "although the active spaces used for the first row diatomics have large hilbert spaces , they are not a very stringent test for our theory because these molecules in their equilibrium geometry are not strongly correlated and traditional methods like ccsd(t ) are cheap and reliable for such molecules .",
    "thus , we also perform frozen - core calculations on the cr@xmath0 dimer using cc - pvdz , cc - pvtz and cc - pvqz bases , which have active spaces containing ( 12e , 68o ) , ( 12e , 118o ) and ( 12e , 190o ) respectively .",
    "cr@xmath0 is well known for being very strongly correlated and most multi - reference methods can use no more than just the minimal active space and many of them fail to get even qualitatively correct dissociation curves .",
    "finally , we also perform calculations on the mn - salen model complex which is a prototypical strongly correlated inorganic molecule containing open shell _ d_-orbitals giving rise to nearly degenerate singlet and triplet ground states .",
    "for all the systems we obtain energies that are accurate to 1 mha for the chosen basis ; for the first - row dimers we compare to s - fciqmc energies  @xcite , for the cr@xmath0 dimer we perform internal convergence tests , and for mn - salen we compare to dmrg energies  @xcite .",
    "lccrrcccrrr & & & & & & & + molecule &  basis  &  sym  &  @xmath82  &  @xmath83  &  var  &  pt  & & var&pt&total + c@xmath0 & dz & @xmath84a@xmath85 & @xmath86&28566 & -75.7217 & -75.7286(2 ) & & 1 & 2&3 + c@xmath0 & tz & @xmath84a@xmath85 & @xmath87&142467 & -75.7738 & -75.7846(3 ) & & 10 & 6&16 + c@xmath0 & qz & @xmath84a@xmath85 & @xmath88&403071 & -75.7894 & -75.8018(4 ) & & 63 & 11&75 +   + n@xmath0 & dz & @xmath84a@xmath85 & @xmath86 & 37593 & -109.2692 & -109.2769(1 ) & & 1&3&4 + n@xmath0 & tz & @xmath84a@xmath85 & @xmath87 & 189080 & -109.3608 & -109.3748(6 ) & & 14&8&22 + n@xmath0 & qz & @xmath84a@xmath85 & @xmath88&499644 & -109.3884 & -109.4055(9 ) & & 77&18&95 +   + o@xmath0 & dz & @xmath84a@xmath85 & @xmath86 & 52907 & -149.9793 & -149.9878(2 ) & & 2&3&4 + o@xmath0 & tz & @xmath84a@xmath85 & @xmath87 & 290980 & -150.1130 & -150.1307(8 ) & & 24&7&30 + o@xmath0 & qz & @xmath84a@xmath85 & @xmath88 & 770069 & -150.1541 & -150.1748(9 ) & & 131&30&161 +   + no & dz & @xmath84b@xmath89 & @xmath86 & 48305 & -129.5881 & -129.5997(3 ) & & 2&3&5 + no & tz & @xmath84b@xmath89 & @xmath87&227004 & -129.6973 & -129.7181(9 ) & & 30&12&42 + no & qz & @xmath84b@xmath89 & @xmath88 & 606381 & -129.7311 & -129.7548(9 ) & & 207&60&267 +   + f@xmath0 & dz & @xmath84a@xmath85 & @xmath86 & 68994 & -199.0913 & -199.1001(7 ) & & 2&3&5 + f@xmath0 & tz & @xmath84a@xmath85 & @xmath87 & 395744 & -199.2782 & -199.2984(9 ) & & 37&8&46 + f@xmath0 & qz & @xmath84a@xmath85 & @xmath88&1053491 & -199.3349 & -199.3590(9 ) & & 216&41&257 +   +   + f@xmath0 & dz & @xmath84a@xmath85 & @xmath90&16824 & -199.0871 & -199.0994(4 ) & & 0&3&3 + f@xmath0 & tz & @xmath84a@xmath85 & @xmath86&141433 & -199.2787 & -199.2972(7 ) & & 11&16&27 + f@xmath0 & qz & @xmath84a@xmath85 & @xmath86&221160 & -199.3355 & -199.3590(9 ) & & 34&79&113 +    in the variational calculations we start with a value of @xmath5 during the first few iterations that is larger than its final value because the values of @xmath12 tend to be larger in the initial iterations when there are few determinants in @xmath2 . for example , for the cc - pvqz basis set , we successively reduce the value of @xmath5 from @xmath91 to @xmath86 to @xmath87 and @xmath88 ha , and perform 3 iterations at each value .",
    "the cost of performing the first iteration at a value of @xmath5 is larger than that for subsequent iterations because relatively few new determinants are introduced after the first iteration .",
    "table  [ tab : diatomics ] shows benchmark calculations on the first row dimers using a single node containing two intel^^ xeon^^ e5 - 2680 v2 processors of 2.80 ghz each and 128 gigabyte memory . among these calculations f@xmath0 had the largest active space containing 14 electrons in 108 orbitals ( 14e , 108o ) with a hilbert space containing over @xmath92 determinants . on a single node it required slightly more than 4 minutes to get the energy converged to better than 1 mha .",
    "it is not possible to perform the calculations for the larger systems and basis sets on a single node with the original algorithm because the cost of storing all the determinants in the space of connections @xmath4 that contribute to the perturbative corrections is prohibitive .",
    "interestingly , with our implementation the cost of obtaining sub - mha accuracy in energy using the stochastic method for even the smallest system considered here , c@xmath0 with dz basis , is less than that for the original deterministic algorithm .",
    "as expected , these calculations can be done even more efficiently , if the hartree - fock orbitals are replaced by natural orbitals from some approximate correlated theory .",
    "for example the last three rows of table  [ tab : diatomics ] show that the calculations on the f@xmath0 dimer , for all three basis sets , can be run with a larger @xmath5 resulting in more than a factor of 2 speed up when mp2 natural orbitals are used .",
    "ccrrcccrrrrc & & & & & & & +  basis  &  sym  &  @xmath82  &  @xmath83  &  var  &  pt  & & var&pt&total&pt(1)&#nodes + dz & @xmath84a@xmath85 & @xmath93&602984&-2099.4518 & -2099.4859(6 ) & & 66 & 6458 & 6524 & 2172&1 + dz & @xmath84a@xmath85 & @xmath94&2261194&-2099.4665 & -2099.4869(7 ) & & 355 & 5279 & 5634 & 2891&1 + dz & @xmath84a@xmath85 & @xmath95&3117630&-2099.4693 & -2099.4873(3 ) & & 685 & 28115 & 28799 & 2702&1 +   + tz & @xmath84a@xmath85 & @xmath95&6268840 & -2099.5051 & -2099.5280(6 ) & & 1245 & 7736 & 8981 & 2879&4 + tz & @xmath84a@xmath85 & @xmath96&12756099 & -2099.5113 & -2099.5292(6 ) & & 3197 & 8559 & 11756 & 3081&4 + tz & @xmath84a@xmath85 & @xmath97&17798876 & -2099.5166 & -2099.5295(5 ) & & 5717 & 14099 & 19816 & 3525&4 +   + qz & @xmath84a@xmath85 & @xmath95&9516339 & -2099.5246 & -2099.5578(4 ) & & 2164 & 54764 & 56928 & 8762&8 + qz & @xmath84a@xmath85 & @xmath96&19500559 & -2099.5315 & -2099.5562(7 ) & & 6267 & 16999 & 23265&8763&8 +    the cr@xmath0 dimer is well known to be a very challenging system for most electronic structure methods .",
    "we perform frozen core calculations by including all the available virtual orbitals in the active space with a bond length of 1.68   using the cc - pvdz - dk , cc - pvtz - dk and cc - pvqz - dk basis sets .",
    "the relativistic effects are included using the second order douglas - kroll - hess hamiltonian .",
    "all calculations are performed using natural orbitals obtained by first performing a short unconverged fciqmc calculation .",
    "the active spaces with the dz , tz and qz basis sets contained ( 12e , 68o ) , ( 12e , 118o ) and ( 12e , 190o ) respectively , with the largest hilbert space containing more than @xmath98 determinants . table  [ tab : cr2 ] shows that although the variational energies are far from convergence , the total energies including the perturbative corrections converge rapidly as @xmath5 is reduced .",
    "another point to note is that the variational wavefunction contains determinants with all excitation orders , all the way up to the maximum possible of 12 .",
    "hence a ci expansion that is truncated at some excitation level is not adequate .    since the stochastic error of the perturbative calculation decreases as @xmath99 , where @xmath39 is the number of batches used in the perturbative calculations , the time for the perturbative calculation can be greatly reduced if a larger statistical error is acceptable . for instance , in the cr@xmath0 calculation with qz basis set with @xmath100 ha ,",
    "if a stochastic error of 1 mha is acceptable , then the perturbative step would take 8763 seconds , which is comparable to the variational calculation that took 6267 seconds .",
    "it is noteworthy that the cost of the perturbative correction relative to the variational calculations decreases as the size of the variational space @xmath2 increases .",
    "in fact , the cpu time required to reach a fixed statistical error is virtually insensitive to the size of the variational space for a given basis set . the time for achieving an uncertainty of 1 mha",
    "is shown in the second last column of table  [ tab : cr2 ] .",
    "it changes very little with the size of @xmath2 .",
    "this indicates that much larger calculations could be performed if the memory and cpu cost of the variational step were reduced , potentially by using the direct ci method@xcite .",
    "finally , we demonstrate that s - hci can be used to calculate the active space energy of a prototypical strongly correlated molecule like mn - salen ( mnclo@xmath101n@xmath0c@xmath102h@xmath103 ) ( see figure  [ fig : mnsalen ] ) very quickly .",
    "mn - salen derivatives such as jacobson s catalyst are used to catalyze enantioselective epoxidation of olefins . despite their widespread use and importance ,",
    "the mechanism of the catalysis reaction is not known and has spawned a series of theoretical studies@xcite .",
    "recently , some of us performed dmrg - scf calculations  @xcite on the model cluster with the cc - pvdz basis set using an active space of ( 28e , 22o ) .",
    "the initial orbitals were obtained by using the homo-13 to lumo+7 canonical hartree fock orbitals , which were subsequently optimized using the dmrg - scf method . here",
    "we perform the s - hci calculations on the converged orbitals obtained at the end of the converged dmrg - scf calculations .",
    "the results in table  [ tab : mnsalen ] show that both the singlet and the triplet energies converge to better than 1 mha accuracy in only 65 seconds on a single node .",
    ".comparison of the dmrg and s - hci energies ( e+2251 ) ha of the singlet and triplet states of mn - salen .",
    "dmrg was performed with an @xmath104 and s - hci was performed with @xmath105 ha , @xmath106 ha and @xmath107 .",
    "the wall time needed to perform the s - hci calculation on a single compute node is shown in the final column(see text for additional details ) . [ cols=\"^,^,^,^,^,^ \" , ]",
    "we have introduced a stochastic implementation of multireference epstein - nesbet perturbation theory , for computing the expectation value of the perturbative correction to the variational energy of a multi - determinant wavefunction without storing all the contributing determinants .",
    "in addition to completely removing the memory bottleneck , the stochastic algorithm is faster than the fully deterministic algorithm even for relatively small systems , if a stochastic noise of 1 mha is acceptable .",
    "our method is capable of efficiently computing the correlation energies of very large active spaces , as we have demonstrated by computing the energies of the challenging , multireference systems mn - salen ( 28e , 22o ) and cr@xmath0 ( 12e , 190o ) .",
    "for all systems studied we obtained correlation energies accurate to within 1 mha . in the case of the first - row dimers and mn - salen we compared to fciqmc and dmrg energies in the literature .",
    "for cr@xmath0 there are no published values , but one of the positive features of our method is that one can reliably check the convergence within the method itself .    having removed the memory bottleneck in the perturbative step , the largest memory requirement comes from storing the hamiltonian in the variational space .",
    "the next step is to create an efficient method for obtaining the variational wavefunction without storing the hamiltonian .",
    "other research directions include the optimization of the orbitals within the cas space , and the calculation of excited states .",
    "the calculations made use of the facilities of the max planck society s rechenzentrum garching .",
    "ss acknowledges the startup package from the university of colorado .",
    "aah and cju were supported in part by nsf grant aci-1534965 .",
    "the @xmath58 determinants in the variational space are connected to @xmath108 determinants in the perturbative space with non - zero hamiltonian matrix elements , where @xmath109 is the number of electrons and @xmath110 is the number of virtual orbitals . for a relatively conservative number of @xmath111 , @xmath112 and @xmath113",
    ", the perturbative space will contain over @xmath114 determinants , requiring over 10 terabyte memory .",
    "the original hci algorithm reduces this storage requirement by orders of magnitude by only requiring the storage of determinants @xmath7 for which @xmath115 for at least one determinant @xmath116 .",
    "nevertheless , for many systems this is the most memory intensive part of the original algorithm .",
    "the logarithmic corrections come from having to do binary searches to check whether a determinant is already present in the variational space , or , in the space of connected determinants that have already been generated ."
  ],
  "abstract_text": [
    "<S> we extend the recently proposed heat - bath configuration interaction ( hci ) method [ holmes , tubman , umrigar , _ j. chem . theory comput . _ </S>",
    "<S> * 12 * , 3674 ( 2016 ) ] , by introducing a stochastic algorithm for performing multireference epstein - nesbet perturbation theory , in order to completely eliminate the severe memory bottleneck of the original method . </S>",
    "<S> the proposed stochastic algorithm has several attractive features . </S>",
    "<S> first , there is no sign problem that plagues several quantum monte carlo methods . </S>",
    "<S> second , instead of using metropolis - hastings sampling , we use the alias method to directly sample determinants from the reference wavefunction , thus avoiding correlations between consecutive samples . </S>",
    "<S> third , in addition to removing the memory bottleneck , stochastic - hci ( s - hci ) is faster than the deterministic variant for most systems if a stochastic error of 0.1 mha is acceptable . </S>",
    "<S> fourth , within the s - hci algorithm one can trade memory for a modest increase in computer time . </S>",
    "<S> fifth , the perturbative calculation is embarrassingly parallel . </S>",
    "<S> the s - hci algorithm extends the range of applicability of the original algorithm , allowing us to calculate the correlation energy of very large active spaces . </S>",
    "<S> we demonstrate this by performing calculations on several first row dimers including f@xmath0 with an active space of ( 14e , 108o ) , mn - salen cluster with an active space of ( 28e , 22o ) , and cr@xmath0 dimer with up to a quadruple - zeta basis set with an active space of ( 12e , 190o ) . for these systems we were able to obtain better than 1 mha accuracy with a wall time of merely 113 seconds , 65 seconds , and 3 hours on 1 , 1 , and 8 nodes , respectively . </S>"
  ]
}