{
  "article_text": [
    "smooth functionals of densities can be estimated by plug - in estimators , and densities of functions of two or more random variables can be estimated by local von mises statistics .",
    "such estimators often converge at the parametric rate @xmath6 .",
    "the response density of a nonparametric regression model can be written in both ways , but it also involves an additional infinite - dimensional parameter , the regression function . as explained below , this usually leads to a slower convergence rate of response density estimators , _ except _ when the regression function is strictly monotone in the strong sense that it has a nowhere vanishing derivative . in the latter case , we can again obtain the rate @xmath6 .    specifically , consider the nonparametric regression model @xmath7 with a one - dimensional random covariate @xmath1 that is independent of the unobservable error variable @xmath2 .",
    "we impose the following assumptions :    a.   the error variable @xmath2 has mean zero , a moment of order greater than @xmath8 , and a density @xmath9 , and there are bounded and integrable functions @xmath10 and @xmath11 such that @xmath12 and @xmath13 for @xmath14 .",
    "b.   the covariate @xmath1 is _ quasi - uniform _ on the interval @xmath15 $ ] in the sense that its density @xmath16 is bounded and bounded away from zero on the interval and vanishes outside .",
    "furthermore , @xmath16 is of bounded variation . c.   the unknown regression function @xmath17 is twice continuously differentiable on @xmath15 $ ] , and @xmath18 is strictly positive on @xmath15 $ ] .",
    "assume that @xmath19 are @xmath20 independent copies of @xmath21 .",
    "we are interested in estimating the density @xmath22 of the response @xmath3 .",
    "an obvious estimator is the kernel estimator @xmath23 where @xmath24 for some kernel @xmath25 and some bandwidth @xmath26 . under the above assumptions on @xmath9 and @xmath16 , the density @xmath22 has a lipschitz - continuous second derivative as demonstrated in section [ known ] .",
    "thus , if the kernel has compact support and is of order three , and the bandwidth @xmath26 is chosen proportional to @xmath27 , then the mean squared error of the kernel estimator is of order @xmath28 .",
    "this means that the estimator has the nonparametric rate @xmath29 of convergence .",
    "the above kernel estimator neglects the structure of the regression model .",
    "we shall see that by exploiting this structure one can construct estimators that have the faster ( parametric ) rate @xmath6 of convergence . for this",
    "we observe that the density @xmath22 is the convolution of the error density @xmath9 and the density @xmath30 of @xmath4 .",
    "the latter density is given by=-1 @xmath31=0 by our assumptions on @xmath17 and @xmath16 , the density @xmath30 is quasi - uniform on the interval @xmath32 $ ] , which is the image of @xmath15 $ ] under @xmath17 .",
    "furthermore , @xmath30 is of bounded variation .",
    "the convolution representation @xmath33 suggests a plug - in estimator or _ convolution estimator _",
    "@xmath34 based on estimators @xmath35 and @xmath36 of @xmath9 and @xmath30 , for example the kernel estimators @xmath37 with _ nonparametric residuals _ @xmath38 . setting @xmath39 , the convolution estimator @xmath40 has the form of a _ local von mises statistic _",
    "@xmath41    in section [ unknown ] , we show that the estimator @xmath42 is root-@xmath20 consistent in the sup - norm and obeys a functional central limit theorem in the space @xmath5 of all continuous function on @xmath43 that vanish at plus and minus infinity . as an auxiliary result , section [ known ] treats the case of a _ known _ regression function @xmath17 . when @xmath17 is _ unknown _ , we estimate it by a local quadratic smoother .",
    "the required properties of this smoother are proved in section [ smooth ] .",
    "the convergence rate of @xmath42 follows from a stochastic expansion which in turn is implied by equations ( [ e1])([e4 ] ) .",
    "these equations are proved in sections [ pe1][pe4 ] .",
    "plug - in estimators in nonparametric settings are often efficient ; see , for example , bickel and ritov @xcite , laurent @xcite , chaudhuri _ et al .",
    "_ @xcite and efromovich and samarov @xcite . in section [ eff ]",
    ", we first calculate the asymptotic variance bound and the efficient influence function for estimators of @xmath44 .",
    "surprisingly our estimator @xmath40 is not efficient unless the error distribution happens to be normal .",
    "we construct an additive correction term @xmath45 such that @xmath46 is efficient for @xmath44 .",
    "this estimator again obeys a uniform stochastic expansion and a functional central limit theorem in @xmath5 .",
    "the proof of this result is given in section [ pe5 ] .",
    "the estimator @xmath42 used here goes back to frees @xcite .",
    "he observed that densities of some ( known ) transformations @xmath47 of @xmath48 independent and identically distributed random variables @xmath49 can be estimated pointwise at the parametric rate by a local u - statistic .",
    "saavedra and cao @xcite consider the transformation @xmath50 with @xmath51 .",
    "schick and wefelmeyer @xcite and @xcite obtain this rate in the sup - norm and in @xmath52-norms for transformations of the form @xmath53 and @xmath54 .",
    "gin and mason @xcite obtain such functional results in @xmath55-norms for @xmath56 and general transformations @xmath47 .",
    "the results of nickl @xcite and @xcite are also applicable in this context .",
    "the same convergence rates have been obtained for convolution estimators or local von mises statistics of the stationary density of linear processes .",
    "saavedra and cao @xcite treat pointwise convergence for a first - order moving average process .",
    "schick and wefelmeyer @xcite and @xcite consider higher - order moving average processes and convergence in @xmath52 , and schick and wefelmeyer @xcite and @xcite obtain parametric rates in the sup - norm and in @xmath52 for estimators of the stationary density of invertible linear processes .",
    "analogous pointwise convergence results for response density estimators in nonlinear regression ( with responses missing at random ) and in nonparametric regression are in mller @xcite and stve and tjstheim @xcite , respectively .",
    "escanciano and jacho - chvez @xcite consider the nonparametric regression model and show uniform convergence , on compact sets , of their local u - statistic .",
    "their results allow for a multivariate covariate @xmath1 , but require the density of @xmath4 to be bounded and lipschitz .    in the above applications to regression models and time",
    "series , and also in the present paper , the ( auto-)regression function is assumed to have a nonvanishing derivative .",
    "this assumption is essential .",
    "suppose there is a point @xmath57 at which the regression function behaves like @xmath58 , for @xmath59 to the left or right of @xmath57 , with @xmath60 .",
    "then the density @xmath30 of @xmath4 has a strong peak at @xmath61 .",
    "this slows down the rate of the convolution density estimator or local von mises statistic for @xmath33 . for densities of transformations",
    "@xmath62 of independent and identically distributed random variables , see schick and wefelmeyer @xcite and @xcite and the review paper by mller _",
    "_ @xcite . in their simulations ,",
    "escanciano and jacho - chvez @xcite consider the regression function @xmath63 and a covariate following a beta distribution .",
    "this choice does not fit their assumptions because the density of @xmath4 is neither bounded nor lipschitz .",
    "indeed , for @xmath64 and @xmath65 , the regression function behaves as above with @xmath66 . in this case",
    ", the convolution density estimator does not have the rate @xmath67 , but at best the slower rate @xmath68 .",
    "we begin by proving an auxiliary result for the ( unrealistic ) case that the regression function @xmath17 is _ known_. then we can observe the error @xmath69 , and we can apply the results for known transformations cited in section [ sec1 ] .",
    "we obtain a root-@xmath20 consistent estimator of the response density @xmath22 by the local von mises statistic @xmath70 in the following , we specify conditions under which the convergence holds in @xmath5 .",
    "we shall assume that @xmath25 is the convolution @xmath71 for some continuous third - order kernel @xmath72 with compact support",
    ". then we can write @xmath73 where @xmath74 setting @xmath75 and @xmath76 , we have the decomposition @xmath77 note that @xmath78 . since @xmath30 is of bounded variation and is quasi - uniform on @xmath32 $ ] , we may and do assume that @xmath30 is of the form @xmath79 where @xmath80 is a finite measure with @xmath81)=0 $ ] , and @xmath82 is a measurable function such that @xmath83 .",
    "this allows us to write @xmath84 where @xmath85 is the distribution function corresponding to the error density @xmath9 .",
    "indeed , @xmath86 the properties of @xmath9 now yield that @xmath22 is three times differentiable with bounded derivatives @xmath87 as @xmath72 is of order three , so is @xmath25 .",
    "thus , it follows from a standard argument that @xmath88 for some constant @xmath89 .",
    "next , we note that @xmath90 with @xmath91 similarly , @xmath92 with @xmath93 as shown in schick and wefelmeyer @xcite , @xmath94 converges in @xmath5 to a centered gaussian process with covariance function @xmath95 and the following approximation holds , @xmath96 we can write @xmath97 where @xmath98 is the empirical distribution function based on the errors @xmath99 , @xmath100 , \\qquad t\\in{\\mathbb{r}}.\\ ] ] setting @xmath101 and writing @xmath102 for the @xmath52-norm , we obtain for each @xmath103 the inequalities @xmath104 & \\le & \\sup_{|y_1-y_2|\\le\\delta } \\int\\!\\!\\int \\bigl|\\delta(y_1-x - bu)- \\delta(y_2-x - bu ) \\bigr| \\bigl|k(u)\\bigr| \\ , \\mathrm{d}u \\nu(\\mathrm{d}x ) \\\\[-2pt ] & \\leq&\\|k\\|_1 \\nu({\\mathbb{r } } ) \\sup_{|y_1-y_2|\\le\\delta } \\bigl|\\delta(y_1)- \\delta(y_2)\\bigr|.\\end{aligned}\\ ] ] similarly , we obtain the inequalities @xmath105 & \\le & \\sup_{|y|>2 m } \\int\\!\\!\\int\\bigl|\\delta(y - x - bu)\\bigr| \\bigl|k(u)\\bigr|\\ , \\mathrm{d}u \\nu ( \\mathrm{d}x ) \\\\[-2pt ] & \\le&\\|k\\|_1 \\nu({\\mathbb{r } } ) \\sup_{|y| > m } \\bigl|\\delta(y)\\bigr|\\end{aligned}\\ ] ] for all @xmath106 such that @xmath107 , where the constant @xmath108 is such that the interval @xmath109 $ ] contains the support of @xmath25 . from these inequalities , the characterization of compactness as given in corollary 4 of schick and wefelmeyer @xcite , and the properties of the empirical process , we obtain tightness of the process @xmath110 in @xmath5 .",
    "we also have @xmath111 it is now easy to conclude that @xmath110 converges in @xmath5 to a centered gaussian process with covariance function @xmath112    finally , we have @xmath113 where @xmath114 denotes the @xmath115-norm .    the above yield the following result .",
    "[ thm.1 ] suppose _ ( f ) _ , _ ( g ) _ and _ ( r ) _ hold , the kernel @xmath25 is the convolution @xmath71 of some continuous third - order kernel @xmath72 with compact support , and the bandwidth @xmath26 satisfies @xmath116 and @xmath117 .",
    "then @xmath118 converges in distribution in the space @xmath5 to a centered gaussian process with covariance function @xmath119 . moreover , @xmath120",
    "our main result concerns the case of an _ unknown _ regression function @xmath17 .",
    "then we do not observe the random variables @xmath121 and @xmath122 . in the local von mises statistic @xmath123 of section [ known ] ,",
    "we therefore replace @xmath17 by a nonparametric estimator @xmath124 , substitute the residual @xmath125 for the error @xmath126 , and plug in surrogates @xmath127 for @xmath122 .",
    "the resulting estimator for @xmath33 is then @xmath128    our estimator @xmath124 will be a local quadratic smoother .",
    "more precisely , for a fixed @xmath57 in @xmath15 $ ] , we estimate @xmath61 by the first coordinate @xmath129 of the weighted least squares estimator @xmath130 where @xmath131 , the weight function @xmath132 is a three times continuously differentiable symmetric density with compact support @xmath133 $ ] , and the bandwidth @xmath134 is proportional to @xmath135 .",
    "this means that we undersmooth , since an optimal bandwidth for estimating a twice differentiable regression function is proportional to @xmath136 .",
    "we assume that @xmath25 is the convolution @xmath71 for some twice continuously differentiable third - order kernel @xmath72 with compact support",
    ". then we can write our estimator for @xmath22 as the convolution @xmath137 of the residual - based kernel estimator of @xmath9 , @xmath138 with the surrogate - based kernel estimator of @xmath30 , @xmath139 similarly as in section [ known ] , we have the decomposition @xmath140    let us introduce @xmath141 and @xmath142 we can write @xmath143 as the convolution @xmath144 with @xmath145-q(z ) \\bigr),\\qquad z\\in{\\mathbb{r}},\\ ] ] where @xmath146 denotes the distribution function of @xmath4 . write @xmath147 $ ] for the error variance . since @xmath148 $ ] equals @xmath149 and",
    "@xmath150 is integrable , we obtain from corollary 4 in schick and wefelmeyer @xcite and the remark after it that @xmath151 converges in distribution in @xmath5 to a centered gaussian process with covariance function @xmath152 , where @xmath153 note that @xmath10 and @xmath11 are bounded and integrable and therefore square - integrable",
    ".    we shall show in sections [ pe1][pe4 ] that @xmath154 the last two statements require also @xmath155 .",
    "these four statements and theorem [ thm.1 ] yield our main result .",
    "[ thm.2 ] suppose _ ( f ) _ , _ ( g ) _ and _ ( r ) _ hold , the kernel @xmath25 is the convolution @xmath71 of some twice continuously differentiable third - order kernel @xmath72 with compact support , and the bandwidth @xmath26 satisfies @xmath116 and @xmath156 .",
    "let @xmath124 be the local quadratic estimator for a weight function @xmath132 that is a three times continuously differentiable symmetric density with compact support @xmath133 $ ] , and for a bandwidth @xmath134 proportional to @xmath135 .",
    "then @xmath157 converges in distribution in the space @xmath5 to a centered gaussian process with covariance function @xmath158 .",
    "moreover , we have the uniform stochastic expansion @xmath159    we should point out that @xmath160 for @xmath161 , where @xmath162",
    "in this section , we treat the question of efficient estimation for @xmath22 . for the theory of efficient estimation of real - valued functionals on nonparametric statistical models , we refer to theorem 2 in section 3.3 of the monograph by bickel _",
    "it follows from ( [ hexp ] ) that the estimator @xmath40 has influence function @xmath163 we shall now show that this differs in general from the efficient influence function .",
    "the latter can be calculated as the projection of @xmath164 onto the tangent space of the nonparametric regression model considered here .",
    "the tangent space consists of all functions of the form @xmath165 where the function @xmath166 satisfies @xmath167 and @xmath168 , the function @xmath169 satisfies @xmath170 and @xmath171 , and the function @xmath172 satisfies @xmath173 ; see schick @xcite for details .",
    "the projection of the influence function onto the tangent space is @xmath174 + \\bigl[q(y- \\ve)-h(y ) - d(y ) \\ell(\\ve)\\bigr ] \\\\ & & { } + \\biggl[d(y)-\\frac{1}{j } \\bigl(f'\\bigl(y - r(x ) \\bigr)-h'(y ) \\bigr ) \\biggr ] \\ell(\\ve).\\end{aligned}\\ ] ] here @xmath175 denotes the score function for location , @xmath176 is the fisher information , which needs to be finite for efficiency considerations , and @xmath177 is the expectation @xmath178 $ ] . for later use ,",
    "we set @xmath179 to see that @xmath180 is indeed the projection of the influence function onto the tangent space , we note that @xmath180 belongs to the tangent space and that the difference @xmath181 is orthogonal to the tangent space . for this ,",
    "one uses the well - known identities @xmath182=0 $ ] and @xmath183=1 $ ] .",
    "we have @xmath184 if and only if @xmath185 , which in turn holds if and only if @xmath9 is a mean zero normal density .",
    "consequently , our estimator is efficient for normal errors , but not for other errors .    in order to see why our estimator for @xmath44 is not efficient in general , consider for simplicity the case of known @xmath9 and @xmath16 .",
    "the efficient influence function is then @xmath186 .",
    "thus , an estimator @xmath187 of @xmath44 is efficient if it satisfies the stochastic expansion @xmath188 a candidate would be obtained by replacing , in the relevant terms on the right - hand side , the unknown @xmath17 by an estimator @xmath189 , resulting in the estimator @xmath190 this shows that a correction term to the plug - in estimator @xmath191 is required for efficiency .    in the general situation , with @xmath9 , @xmath16 and @xmath17",
    "unknown , we must construct a stochastic term @xmath45 such that @xmath192 then the estimator @xmath46 has influence function @xmath180 , @xmath193 and hence is efficient .",
    "we shall construct @xmath45 such that ( [ hc ] ) , and hence ( [ hh ] ) , hold uniformly in @xmath59 .",
    "this implies a functional central limit theorem in @xmath5 also for the improved estimator @xmath194 .",
    "we mention that tightness of @xmath195 , with @xmath196 is verified by the same argument as used for @xmath197 .",
    "to construct the correction term , we use sample splitting .",
    "let @xmath198 denote the integer part of @xmath199 .",
    "let @xmath200 and @xmath201 denote the local quadratic smoothers constructed from the observations @xmath202 or @xmath203 , both with the same bandwidth @xmath134 as before .",
    "define residuals @xmath204 for @xmath205 and @xmath206 , and kernel density estimators @xmath207 and @xmath208 where @xmath209 for some bandwidth @xmath210 and a density @xmath211 fulfilling condition k of schick @xcite , such as the logistic kernel",
    ". then we can estimate @xmath212 by @xmath213 the fisher information @xmath214 by @xmath215 and @xmath216 by @xmath217 finally , we take @xmath218 with @xmath219 and @xmath220 we have the following result , which is proved in section [ pe5 ] .",
    "[ thm.3 ] suppose _ ( f ) _ , _ ( g ) _ and _ ( r ) _ hold , @xmath9 has finite fisher information @xmath214 , and the bandwidth @xmath210 satisfies @xmath221 and @xmath222 .",
    "then we have the stochastic expansion @xmath223 .",
    "theorems [ thm.2 ] and [ thm.3 ] imply that the improved estimator @xmath194 has the uniform stochastic expansion @xmath224 and is efficient . as mentioned above ,",
    "if the errors happen to be normally distributed , then @xmath185 . therefore ,",
    "@xmath225 so that @xmath226 collapses in the sense that @xmath227 .",
    "the weighted least squares estimator @xmath228 satisfies the normal equation @xmath229 with @xmath230 subtracting from both sides of the normal equation the term @xmath231 with @xmath232 we arrive at the equality @xmath233 where @xmath234 and @xmath235 since @xmath236 is uniformly continuous on @xmath15 $ ] , we see that @xmath237 it follows from the proof of lemma 1 in mller _",
    "_ @xcite that @xmath238 and @xmath239 with @xmath240 = \\int g(x+cu ) \\psi(u)\\psi^{\\top}(u ) w(u ) \\,\\mathrm{d}u.\\ ] ] since @xmath16 is quasi - uniform on @xmath15 $ ] , there is an @xmath241 with @xmath242 for which @xmath243 holds for all @xmath57 in @xmath15 $ ] and all @xmath244 . from this",
    "we obtain the expansion @xmath245 where @xmath246 denotes a generalized inverse of a matrix @xmath106 if its inverse does not exist . combining the above",
    ", we obtain that @xmath247 where @xmath248 is the first row of @xmath249 . for later use",
    ", we note that @xmath250 for all @xmath57 in @xmath15 $ ] and @xmath251 .",
    "we also have @xmath252 where @xmath253 it is easy to check that @xmath254 \\frac{1}{n}\\sum_{j=1}^n\\hat \\varrho^2(x_j ) & = & \\mathrm{o}_p \\biggl ( \\frac { 1}{nc } \\biggr ) , \\\\[-2pt ] \\sup_{0\\le x\\le1 } \\bigl|\\hat\\varrho(x)\\bigr|^2 & = & \\mathrm{o}_p \\biggl(\\frac{\\log n}{nc } \\biggr).\\end{aligned}\\ ] ] thus , we obtain @xmath255 \\label{r5 } \\int \\bigl(\\hr(x)-r(x ) \\bigr)^4 g(x)\\,\\mathrm{d}x & = & \\mathrm{o}_p \\biggl(\\frac { \\log n}{n^2c^2 } \\biggr).\\end{aligned}\\ ] ]    let @xmath256 be a square - integrable function .",
    "then the function @xmath172 defined by @xmath257 is bounded by @xmath258 and satisfies @xmath259 as @xmath260 . using this and the fact that @xmath132 has support @xmath133 $ ] , we derive @xmath261 & \\le & e \\biggl[\\int \\bigl ( \\chi(x \\pm cu)-\\chi(x ) \\bigr)^2 w(u)\\,\\mathrm{d}u \\biggr ] \\\\[-2pt ] & \\le & \\|g\\| \\int\\gamma(cu ) w(u)\\,\\mathrm{d}u\\\\[-2pt ] & \\to&0.\\end{aligned}\\ ] ] applying this with @xmath262",
    ", we can conclude @xmath263 \\to0,\\ ] ] where @xmath264 .",
    "from this and ( [ eigen ] ) , we derive that @xmath265 \\to0.\\ ] ] in particular , with @xmath266 , @xmath267 \\to0.\\ ] ] let us set @xmath268 & = & \\int \\bigl(g(x - cu)d(x - cu)-g(x)d(x ) \\bigr)\\psi(u)w(u)\\,\\mathrm{d}u \\\\[-2pt ] & & { } + \\bigl(g(x)d(x)-e^{\\top}\\psi^{-1 } \\bigr ) \\psi e + 1.\\end{aligned}\\ ] ] then we have @xmath269 & \\le & 6 e \\biggl[\\int \\bigl|g(x - cu)d(x - cu)-g(x)d(x ) \\bigr|^2 w(u)\\,\\mathrm{d}u \\biggr ] \\\\ & & { } + 2 e \\bigl [ \\bigl|g(x)d(x)-e^{\\top } \\psi^{-1 } \\bigr|^2 \\bigr ] |\\psi e|^2\\\\ & \\to&0,\\end{aligned}\\ ] ] since @xmath270 is square - integrable .",
    "this can be used to show that @xmath271 in view of ( [ r2 ] ) , this yields @xmath272",
    "since @xmath30 is of bounded variation , we can write @xmath273 , where @xmath274 with @xmath275 denoting the empirical distribution function based on the residuals @xmath276 , @xmath277,\\qquad t\\in{\\mathbb{r}}.\\ ] ] it was shown in mller _ et al .",
    "_ @xcite that @xmath278 from this and the representation ( [ h1 ] ) of @xmath279 , we immediately derive the expansion @xmath280 this lets us conclude that @xmath281",
    "since @xmath10 and @xmath11 are bounded , a taylor expansion and the bounds ( [ r2 ] ) and ( [ r3 ] ) yield the uniform expansion @xmath282 now set @xmath283 with @xmath284 then we have @xmath285 in view of @xmath286 , we have the identity @xmath287 with @xmath288-q(z ) \\bigr ) d(x_j ) v_c(x_i - x_j ) \\\\ & & \\hspace*{98pt } { } - \\int \\bigl(\\1\\bigl[r(x)\\le z\\bigr]-q(z ) \\bigr ) d(x)v_c(x_i - x)g(x)\\,\\mathrm { d}x \\biggr).\\end{aligned}\\ ] ] the terms in the sum have mean zero and are uncorrelated , with second moments bounded by @xmath289 e[|d(x_2)v_c(x_1-x_2)|^2]$ ] .",
    "thus , we have @xmath290 \\,\\mathrm{d}z \\le\\sigma^2 \\bigl(r(1)-r(0)\\bigr ) e\\bigl[\\bigl|d(x_2)v_c(x_1-x_2)\\bigr|^2 \\bigr ] = \\mathrm{o}(1/c),\\ ] ] from which we derive @xmath291 similarly , one has @xmath292 = \\mathrm{o}(1)$ ] and obtains @xmath293 next we have @xmath294 , where",
    "@xmath295 d(x)v_c(x_j - x)g(x ) \\,\\mathrm{d}x \\\\ & = & \\frac{1}{n}\\sum_{j=1}^n \\ve_j \\int\\1\\bigl[r(x_j - cu)\\le z\\bigr ] g(x_j - cu ) d(x_j - cu ) \\psi(u)w(u)\\,\\mathrm{d}u \\\\ & = & n_1(z)+n_2(z)+n_3(z)+ q(z)n\\end{aligned}\\ ] ] with @xmath296-\\1 \\bigl[r(x_j)\\le z\\bigr ] \\bigr ) \\\\ & & \\hspace*{33pt } { } \\times g(x_j - cu)d(x_j - cu ) \\psi(u)w(u)\\,\\mathrm{d}u , \\\\",
    "n_2(z ) & = & \\frac{1}{n}\\sum_{j=1}^n \\ve_j \\1\\bigl[r(x_j)\\le z\\bigr ] , \\\\",
    "n_3(z ) & = & \\frac{1}{n}\\sum_{j=1}^n \\ve_j \\bigl(t(x_j)-1\\bigr ) \\bigl(\\1\\bigl[r(x_j ) \\le z\\bigr]-q(z ) \\bigr ) , \\\\",
    "n & = & \\frac{1}{n}\\sum_{j=1}^n \\ve_j \\bigl(t(x_j)-1\\bigr).\\end{aligned}\\ ] ] it is easy to check that @xmath297 .",
    "recall the identity @xmath298 . using these identities",
    ", we see that @xmath299 we show now that the right - hand side is of order @xmath300 .",
    "first , we calculate @xmath301=\\sigma^2 e\\bigl[\\bigl(t(x)-1 \\bigr)^2\\bigr ] \\to0.\\ ] ] second , using the abbreviation @xmath302-\\1[r(x)\\le z]$ ] , we have @xmath303\\,\\mathrm{d}z & = & \\sigma^2 \\int e \\biggl [ \\biggl(\\int t(u , z)g(x - cu)d(x - cu)\\psi(u)w(u)\\ , \\mathrm{d}u \\biggr)^2 \\biggr]\\,\\mathrm{d}z \\\\ & \\le&\\sigma^2 \\int e \\biggl[\\int \\bigl(t(u , z)g(x - cu)d(x - cu)\\psi(u ) \\bigr)^2 w(u)\\,\\mathrm{d}u \\biggr]\\,\\mathrm{d}z \\\\ & \\le&\\sigma^2 \\int e \\biggl [ \\int t^2(u , z)\\,\\mathrm{d}z \\bigl(g(x - cu)d(x - cu)\\psi(u ) \\bigr)^2 \\biggr ] w(u)\\,\\mathrm{d}u \\\\ & \\le&\\sigma^2 \\int e \\bigl [ \\bigl|r(x - cu)-r(x ) \\bigr| \\bigl(g(x - cu)d(x - cu ) \\psi(u ) \\bigr)^2 \\bigr]w(u)\\,\\mathrm{d}u \\\\ & \\to&0.\\end{aligned}\\ ] ] third , we derive @xmath304\\,\\mathrm{d}z & = & \\sigma^2 \\int e \\bigl[\\bigl(t(x)-1\\bigr)^2 \\bigl(\\1 \\bigl[r(x)\\le z\\bigr]-q(z)\\bigr)^2 \\bigr]\\,\\mathrm{d}z \\\\ & = & \\sigma^2 e \\biggl[\\bigl(t(x)-1\\bigr)^2 \\int \\bigl(\\1 \\bigl[r(x)\\le z\\bigr]-q(z ) \\bigr)^2 \\,\\mathrm{d}z \\biggr ] \\\\ & \\le&\\sigma^2 \\bigl(r(1)-r(0 ) \\bigr ) e\\bigl[\\bigl(t(x)-1 \\bigr)^2\\bigr]\\\\ & \\to&0.\\end{aligned}\\ ] ] we can now conclude that @xmath305 .",
    "the above relations show that @xmath306 , where @xmath307 note that @xmath308 .",
    "thus , the desired ( [ e2 ] ) follows from the bound @xmath309 and the tightness of @xmath310 in @xmath5 .",
    "without loss of generality , we assume that @xmath244",
    ". then we have the inequality @xmath311 let us set @xmath312 , and , for a subset @xmath89 of @xmath313 , @xmath314 \\bigl(\\ve_j + r(x_j , x ) \\bigr ) d(x ) v_c(x_j - x).\\ ] ] note that @xmath315 . for @xmath316 with @xmath317",
    "we have @xmath318 where @xmath319 , |x - y|\\le c \\bigr\\}.\\ ] ] we abbreviate @xmath320 by @xmath321 and @xmath322 by @xmath323 .",
    "the above inequality and ( [ r1 ] ) yield the rates @xmath324 & = & \\mathrm{o}_p \\biggl(\\frac{1}{n^2c } \\biggr).\\end{aligned}\\ ] ] let us now set @xmath325 where @xmath326 for a continuous function @xmath210 .",
    "it follows from the properties of @xmath72 that @xmath327 for real numbers @xmath328 and @xmath329 .",
    "this inequality and statements ( [ a1 ] ) and ( [ a2 ] ) yield the rate @xmath330 the last step used the fact that @xmath331 is of order @xmath332 and tends to infinity .",
    "in addition , we have @xmath333= e\\bigl[t_1 ^ 2(z , \\ahat_1)\\bigr ] + ( n-1 ) e\\bigl[t_1(z,\\ahat_1)t_2(z , \\ahat_2)\\bigr].\\ ] ] conditioning on @xmath334 , we see that @xmath335 = e\\bigl [ t_2(z,\\ahat_{1,2})e \\bigl(t_1(z,\\ahat_1)|\\xi\\bigr)\\bigr ] = 0.\\ ] ] similarly one verifies that @xmath336 $ ] and @xmath337 $ ] are zero .",
    "an application of the cauchy ",
    "schwarz inequality shows that @xmath338 = e \\bigl [ \\bigl(t_1(z , \\ahat_1)-t_1(z,\\ahat_{1,2 } ) \\bigr ) \\bigl(t_2(z,\\ahat_2)-t_2(z , \\ahat_{1,2 } ) \\bigr ) \\bigr]\\ ] ] is bounded by @xmath339 $ ] which in turn is bounded by @xmath340.\\ ] ] with the help of ( [ a3 ] ) and ( [ kexp ] ) , we thus obtain the bound @xmath341\\,\\mathrm{d}z \\le\\frac{\\|k\\|_2 ^ 2}{nb } + \\frac{(n-1)}{nb^3 } \\bigl\\|k'\\bigr\\|_2 ^ 2 e \\bigl [ \\bigl(\\ahat_1(x_1)-\\ahat_{1,2}(x_1 ) \\bigr)^2 \\bigr ] = \\mathrm{o } \\biggl(\\frac{1}{nb } \\biggr).\\ ] ] it follows that we have the rate @xmath342 .",
    "now we set @xmath343 since @xmath344 equals @xmath345 , we have @xmath346 a taylor expansion yields the bound @xmath347 we have @xmath348 and @xmath349 . using these bounds , ( [ r5 ] ) and ( [ r6 ] )",
    ", we obtain the rate @xmath350 the desired result ( [ e3 ] ) follows from ( [ f1 ] ) and ( [ f2 ] ) .",
    "we assume again that @xmath244 and set @xmath351 an argument similar to the one leading to ( [ f1 ] ) yields @xmath352 note that @xmath353 and @xmath354 . a taylor expansion and ( [ r5 ] ) yield @xmath355 in view of ( [ r2 ] )",
    ", we find @xmath356 finally , we write @xmath357 & = & \\frac{1}{n}\\sum_{j=1}^n \\ve_j \\int \\bigl(k_b'\\bigl(z - r(x ) \\bigr)-k'_b\\bigl(z - r(x_j)\\bigr ) \\bigr ) d(x ) v_c(x_j - x ) g(x)\\ , \\mathrm{d}x \\\\[-2pt ] & & { } + \\frac{1}{n}\\sum_{j=1}^n \\ve_j k_b'\\bigl(z - r(x_j ) \\bigr ) t(x_j).\\end{aligned}\\ ] ] in view of ( [ dvb ] ) , we have the bound @xmath358 this inequality and an application of the cauchy ",
    "schwarz inequality yield the bound @xmath359\\,\\mathrm{d}z \\le 2 \\sigma^2 \\biggl ( \\frac{3 \\|g\\|}{\\eta } e[u ] + \\bigl\\|k_b'\\bigr\\|_2 ^ 2 e\\bigl[t^2(x)\\bigr ] \\biggr ) \\ ] ] with @xmath360 & \\le&\\bigl\\|k_b''\\bigr\\|_2 ^ 2 \\frac{3}{\\eta}\\int \\bigl(r(x)-r(x ) \\bigr)^2 \\frac{1}{c } w \\biggl(\\frac{x - x}{c } \\biggr ) g(x)\\,\\mathrm{d}x.\\end{aligned}\\ ] ] in the last step we used ( [ dvb ] ) and the analog of ( [ kexp ] ) with @xmath361 in place of @xmath362 . since @xmath17 is lipschitz on @xmath15 $ ]",
    ", we obtain @xmath363= \\mathrm{o}(b^{-5}c^2)= \\mathrm{o}(b^{-3})$ ] .",
    "the above relations show that @xmath364 the desired ( [ e4 ] ) follows from ( [ q1 ] ) and ( [ q2 ] ) .",
    "it suffices to show that @xmath365 for @xmath205 , with @xmath366 and @xmath367 .",
    "since the two cases are similar , we prove only the case @xmath368 .",
    "we begin by writing @xmath369 and @xmath370 where @xmath371 - q(z ) \\bigr)\\ ] ] and @xmath372- \\hq(z,\\hr_2 ) \\bigr)\\ ] ] with @xmath373.\\ ] ] in view of @xmath374= e[\\lambda^2(\\ve ) ] \\int q(z)(1-q(z))\\,\\mathrm{d}z < \\infty$ ] and the bound @xmath375 it suffices to show @xmath376 and @xmath377    let us first prove ( [ ff ] ) . with @xmath378 , we have @xmath379 for @xmath205 and @xmath206 . then we can write @xmath380 with @xmath381 let @xmath382 denote the conditional expectation given @xmath383 . using the square - integrability of @xmath11 and a standard argument , we find that @xmath384 & \\le & m^{-1}\\int\\bigl(\\la''(z ) \\bigr)^2\\ , \\mathrm{d}z \\\\ & & { } + \\int\\!\\!\\int\\!\\!\\int \\bigl(f''\\bigl(z- \\hd_2(x)-au\\bigr)-f''(z ) \\bigr)^2 \\kappa(u)\\,\\mathrm{d}u g(x)\\,\\mathrm{d}x \\,\\mathrm{d}z \\\\ & = & \\mathrm{o}\\bigl(m^{-1}a^{-5}\\bigr ) + \\mathrm{o}_p(1).\\end{aligned}\\ ] ] thus , @xmath385 .",
    "similarly , one verifies @xmath386 , and we obtain ( [ ff ] ) .    to prove ( [ nn ] ) , we set @xmath387- \\hq(z,\\hr_2 ) \\bigr)\\ ] ] and shall verify @xmath388 we can write @xmath389 with @xmath390 , @xmath391 , @xmath392 , @xmath393 , @xmath394 and @xmath395 where @xmath396 denotes the identity map on @xmath43 . now let @xmath397 denote the conditional expectation given @xmath398 .",
    "then we find @xmath399 with @xmath400 - \\hq(z,\\hr_2 ) \\bigr)^2 \\,\\mathrm{d}z , \\\\",
    "r_{2,j } & = & \\int \\bigl(\\1\\bigl[\\hr_2(x_j)\\le z \\bigr ] - \\hq(z,\\hr_2 ) -\\1\\bigl[r(x_j)\\le z\\bigr]+q(z ) \\bigr)^2 \\,\\mathrm{d}z.\\end{aligned}\\ ] ] by the properties of the quadratic smoother , we have @xmath401 several applications of the cauchy ",
    "schwarz inequality yield the bound @xmath402 - \\1\\bigl[r(x_j)\\le z \\bigr ] \\bigr)^2 \\,\\mathrm{d}z \\\\ & & { } + 3 \\int \\bigl(\\hq(z , r)-q(z ) \\bigr)^2 \\,\\mathrm{d}z.\\end{aligned}\\ ] ] now we use the identity @xmath403-\\1[v\\le z])^2 = \\1[u < z\\le v]$ ] , valid for @xmath404 , and ( [ qs ] ) , to conclude @xmath405 using the above identity and the uniform consistency of @xmath201 , we obtain @xmath406 -\\1\\bigl [ \\hr_2(x_i)\\le z\\bigr ] \\bigr)^2 \\,\\mathrm{d}z = \\mathrm{o}_p(1).\\ ] ] by lemma 10.1 in schick @xcite there is a constant @xmath407 so that @xmath408 \\\\[-8pt ] & & { } + \\mathrm{o}_p \\biggl(\\frac{1}{a^{6}m } \\biggr)+ \\mathrm{o}_p(1 ) .",
    "\\nonumber\\end{aligned}\\ ] ] from ( [ l0])([l2 ] ) and @xmath409 , we obtain @xmath410 . a similar argument yields @xmath411 .",
    "using ( [ l1 ] ) , ( [ l2 ] ) and the operator @xmath397 , we obtain @xmath412 it is now easy to see that @xmath413 is a consistent estimator of @xmath214 .",
    "this completes the proof of @xmath414 .",
    "we are left to verify @xmath415 . using the definition of @xmath416 ,",
    "we can write @xmath417- \\hq(z , \\hr_2 ) \\bigr ) \\biggl(\\hd_2(x_j ) + \\frac{1}{\\hat j } \\hat\\omega(x_j ) \\biggr),\\ ] ] where @xmath418 a taylor expansion yields @xmath419 since @xmath420 is bounded by @xmath421 , we obtain @xmath422 with @xmath423 .",
    "now set @xmath424- \\hq(z , \\hr_2 ) \\bigr)\\hd_2(x_j ) , \\\\",
    "\\upsilon(z ) & = & \\frac{1}{\\sqrt{n}}\\sum_{j=1}^m \\bigl(\\1\\bigl[r(x_j)\\le z\\bigr]- q(z ) \\bigr)\\hd_2(x_j).\\end{aligned}\\ ] ] using the minkowski inequality and the statements ( [ qs])([r1j ] ) , we derive @xmath425 using the inequality @xmath426-q(z)|\\le\\1[r(0)\\le z\\le r(1)]$ ] , valid for all @xmath427 and @xmath14 , we obtain @xmath428 \\bigr\\|_2 ^ 2\\bigr ] & \\le&\\frac{m}{n } \\int\\!\\!\\int \\bigl(\\1 \\bigl[r(x)\\le z\\bigr]-q(z ) \\bigr)^2 \\hd_2 ^ 2(x ) g(x)\\,\\mathrm{d}x \\,\\mathrm{d}z \\\\ & \\le & \\bigl(r(1)-r(0 ) \\bigr ) \\int\\hd_2 ^ 2(x)g(x)\\ , \\mathrm{d}x = \\mathrm{o}_p(1).\\end{aligned}\\ ] ] now introduce @xmath429 -q(z ) \\bigr ) \\rho(x)g(x)\\ , \\mathrm{d}x.\\ ] ] then we have @xmath430= n^{-1/2 } m i(z,\\hd_2)$ ] . in view of the above and @xmath431 , the desired property @xmath415 will follow if we show @xmath432 .",
    "the latter is equivalent to showing @xmath433 . in view of ( [ r2 ] ) , we have @xmath434 we can express @xmath435 as the average @xmath436 with @xmath437-q(z ) \\bigr ) \\frac{1}{c } w \\biggl(\\frac { x_j - x}{c } \\biggr ) d(x ) \\psi \\biggl ( \\frac{x_j - x}{c } \\biggr ) g(x)\\,\\mathrm{d}x \\\\ & = & \\int \\bigl(\\1\\bigl[r(x_j - cu)\\le z\\bigr]-q(z ) \\bigr ) w(u ) d(x_j - cu)\\psi(u ) g(x_j - cu)\\,\\mathrm{d}u.\\end{aligned}\\ ] ] since @xmath438 is bounded by a constant times @xmath439 $ ] , we conclude @xmath440 = \\int \\sigma^2 e\\bigl[\\tau^2(z , x)\\bigr ] \\,\\mathrm{d}z = \\mathrm{o}(1).\\ ] ] the above shows that @xmath433 , and the proof is finished .",
    "we thank the referee for suggesting to discuss the question of efficiency .",
    "this resulted in adding the present sections [ eff ] and [ pe5 ] .",
    "the research of anton schick was supported in part by nsf grant dms 09 - 06551 ."
  ],
  "abstract_text": [
    "<S> we consider a nonparametric regression model @xmath0 with a random covariate @xmath1 that is independent of the error @xmath2 . </S>",
    "<S> then the density of the response @xmath3 is a convolution of the densities of @xmath2 and @xmath4 . </S>",
    "<S> it can therefore be estimated by a convolution of kernel estimators for these two densities , or more generally by a local von mises statistic . </S>",
    "<S> if the regression function has a nowhere vanishing derivative , then the convolution estimator converges at a parametric rate . </S>",
    "<S> we show that the convergence holds uniformly , and that the corresponding process obeys a functional central limit theorem in the space @xmath5 of continuous functions vanishing at infinity , endowed with the sup - norm . </S>",
    "<S> the estimator is not efficient . </S>",
    "<S> we construct an additive correction that makes it efficient . </S>"
  ]
}