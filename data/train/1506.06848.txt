{
  "article_text": [
    "constrained optimisation problems ( cops ) , specially non - linear ones , are very important and widespread in real world applications @xcite .",
    "this has motivated introducing various algorithms to solve cops .",
    "the focus of these algorithms is to handle the involved constraints . in order to deal with the constraints ,",
    "various mechanisms have been adopted by evolutionary algorithms .",
    "these techniques include penalty function , decoder - based methods and special operators that separate the treatment of constraints and objective functions . for an overview of different types of methods we refer the reader to mezura - montes and coello coello @xcite .    with the increasing number of evolutionary algorithms , it is hard to predict which algorithm performs better for a newly given cop .",
    "various benchmark sets such as cec10 @xcite and bbob10 @xcite have been proposed to evaluate the algorithm performances on continuous optimization problems .",
    "the aim of these benchmarks is to find out which algorithm is good on which classes of problems . for",
    "constrained continuous optimization problems , there has been an increasing interest to understanding problem features from a theoretical perspective @xcite .",
    "the feature - based analysis of of hardness for certain classes of algorithms is a relatively new research area .",
    "such studies classify problems as hard or easy for a given algorithm based on the features of given instances .",
    "initial studies in the context of continuous optimization have recently been carried out in  @xcite .",
    "having enough knowledge on problem properties that make it hard or easy , we may choose the most suited algorithm to solve it . to do this ,",
    "two steps approach has been proposed by mersmann et al .",
    "first , one has to extract the important features from a group of investigated problems .",
    "second , in order to build a prediction model , it is necessary to analyse the performance of various algorithms on these features .",
    "feature - based analysis has also been used to gain new insights in algorithm performance for discrete optimization problems @xcite .    in this paper",
    ", we carry out a feature - based analysis for constrained continuous optimisation and generate a variety of problem instances from easy to hard ones by evolving constraints .",
    "this ensures that the knowledge obtained by analysing problem features covers a wide range of problem instances that are of particular interest .",
    "although what makes a problem hard to solve is not a standalone feature , it is assumed that constraints are certainly important in constrained problems .",
    "evolving constraints is a new technique to generate hard and easy instances .",
    "so far , the influence of one linear constraint has been studied @xcite . however , real world problems have more than one linear constraint ( such as linear , quadratic and their combination ) .",
    "hence , our study is to generate cop instances to investigate which features of the linear and quadratic constraints make the constrained problem hard to solve . to provide this knowledge , we need to use a common suitable evolutionary algorithm that handles the constraints . in this research ,",
    "the @xmath0-constrained differential evolution with an archive and gradient - based mutation ( @xmath0deag ) @xcite is used . the @xmath0deag ( winner of cec 10 special session for constrained problems )",
    "is applied to generate hard and easy instances to analyse the impact of set of constraints on it .",
    "our results provide evidence on the capability of constraints ( linear , quadratic or their set of combination ) features to classify problem instances to easy and hard ones .",
    "feature analysis by solving the generated instances with @xmath0deag enables us to obtain the knowledge of influence of constraints on problem hardness which could later could be used to design a successful prediction model for algorithm selection .",
    "the rest of the paper is organised as follows . in section 2",
    ", we introduce the constrained optimisation problems .",
    "then , we discuss @xmath0deag algorithm that we use to solve the generated problem instances .",
    "section 3 includes our approach to evolve and generate problem instances .",
    "furthermore , the constraint features are discussed . in section 4",
    ", we carry out the analysis of the linear and quadratic constraint features .",
    "finally , section 5 concludes with some remarks .",
    "constrained continuous optimisation problems are optimisation problems where a function @xmath1 on real - valued variables should be optimised with respect to a given set of constraints .",
    "constraints are usually given by a set of inequalities and/or equalities .",
    "without loss of generality , we present our approach for minimization problems .    formally , we consider single - objective functions @xmath2 , with @xmath3 .",
    "the constraints impose a feasible subset @xmath4 of the search space @xmath5 and the goal is to find an element @xmath6 that minimizes @xmath7 .",
    "we consider problems of the following form :    @xmath8    where @xmath9 is an @xmath10 dimensional vector and @xmath11 .",
    "also @xmath12 and @xmath13 are inequality and equality constraints respectively",
    ". both inequality and equality constraints could be linear or nonlinear . to handle equality constraints",
    ", they are usually transformed into inequality constraints as @xmath14 , where @xmath15 ( used in @xcite ) .",
    "also , the feasible region @xmath4 of the search space @xmath5 is defined by @xmath16    where both @xmath17 and @xmath18 denote lower and upper bounds for the @xmath19th variable and @xmath20 respectively .",
    "one of the most prominent evolutionary algorithms for cops is @xmath0-constrained differential evolution with an archive and gradient - based mutation ( @xmath0deag ) .",
    "the algorithm is the winner of latest cec competition for constrained constrained continuous problems @xcite .",
    "the @xmath0deag uses @xmath0-constrained method to transform algorithms for unconstrained problems to constrained ones @xcite .",
    "it adopts @xmath0-level comparison instead of ordinary ones to order the possible solutions . in other words ,",
    "the lexicographic order is performed in which constraint violation ( @xmath21 ) has more priority and proceeds the function value ( @xmath1 ) .",
    "this means feasibility is more important .",
    "let @xmath22,@xmath23 and @xmath24,@xmath25 are objective function values and constraint violation at @xmath26,@xmath27 respectively .",
    "hence , for all @xmath28 , the @xmath0-level comparison of two candidates @xmath29 and @xmath30 is defined as the follows :    @xmath31    @xmath32    in order to improve the usability , efficiency and stability of the algorithm , an archive has been applied . using it improves the diversity of individuals ( see algorithm [ alg : edeag ] ) .",
    "the offspring generation is adopted in such a way that if the child is not better than its parent , the parent generates another one ( see @xcite ) .",
    "this leads to more stability to the algorithm . for a detailed presentation of the algorithm ,",
    "we refer the reader to @xcite .",
    "* initializations : + - _ m _ randomly selected individuals from search space _",
    "is archived in _ a_. + - set @xmath0 level using control level function + - population : top _ n _",
    "individuals are selected from archive .",
    "the archive is ranked using @xmath0 level comparison * termination condition is set to maximum function evaluation number . *",
    "de operation : use de / rand/1/exp to generate new child .",
    "comparing is based on the @xmath0 level comparison * gradient based mutation : if child is infeasible , it is changed by the gradient - based mutation with probability p. go to step 3 and parent is considered as parent .",
    "* update and control the @xmath0-level * go to step 2",
    "it is assumed that the role of constraints in problem difficulty is certainly important for constrained optimisation problem .",
    "hence , it is necessary to analyse various effects that constraint can impose on a constrained optimisation problems .",
    "evolving constraints is a novel methodology to generate hard and easy instances based on the performance of the problem solver ( optimisation algorithm ) .      in order to analyse the effects of constraints ,",
    "the variety of them needs to be studied over a fixed objective function .",
    "first , constraint coefficients are randomly chosen to construct problem instances .",
    "second , the generated constrained problem is solved by a solver algorithm ( @xmath0deag ) .",
    "then , the required function evaluation number ( fen ) to solve this instance is considered as the fitness value for evolving algorithm .",
    "this process is repeated until hard and easy instances of constraint problem are generated ( see figure [ fig : flowchart ] ) .        to generate hard and easy instances ,",
    "we use the approach outlined in @xcite .",
    "it uses fast and robust differential evolution ( de ) proposed in @xcite ( see algorithm [ alg : de ] , [ alg : newsample ] ) to evolve through the problem instances ( by generating various constraint coefficients ) .",
    "it is necessary to note that the aim is to optimise ( maximise / minimise ) the fen that is required by a solver to solve the generated problem . also , to solve this generated problem instance and find the required fen we use @xmath0deg as a solver .",
    "the termination condition of this algorithm ( evolver ) is set to reaching fenmax number of function evaluations or finding a solution close enough to the feasible optimum solution as follows :    @xmath33    * inputs : problem and @xmath34 , @xmath35 , @xmath36 , outputs : @xmath37 * population @xmath38 initializepop + evaluatepopulation(population ) + @xmath37 @xmath38 getbestsolution(population ) * * repeat * * newpopulation @xmath38 @xmath39 * * for * i starts at 1 , i@xmath40 - 1 , increment i * @xmath41 @xmath38 newsample * * if * cost(@xmath41)@xmath42cost(@xmath43 ) * newpopulation @xmath38 @xmath41 * * else * * newpopulation @xmath38 @xmath44 * * endif * * * endfor * * population @xmath38 newpopulation * evaluatepopulation(population ) * @xmath37 @xmath38 getbestsolution(population ) * * until * ( stop condition )    * inputs : @xmath45 , population , np , f , cr , outputs : @xmath5 * * repeat * * @xmath46 @xmath38 randommember(population ) * * untill * @xmath46 @xmath47 @xmath46 * * repeat * * @xmath48 @xmath38 randommember(population ) * * untill * @xmath48 @xmath47",
    "@xmath45 @xmath49 @xmath48 @xmath47 @xmath46 * * repeat * * @xmath50 @xmath38 randommember(population ) * * untill * @xmath50 @xmath47 @xmath45",
    "@xmath49 @xmath50 @xmath47 @xmath46 @xmath50 @xmath47 @xmath48 * cutpoint @xmath38 randommember(population ) * @xmath51 @xmath38 0 * * for * i starts a 1 to np * * if * i @xmath52 cutpoint @xmath53 rand ( ) @xmath54 cr * @xmath55 @xmath38 @xmath56 + f*(@xmath57-@xmath58 ) * * else * * @xmath55 @xmath38 @xmath59 * * endif * * * endfor * * return @xmath5    this process generates harder and easier problem instances until it reaches the certain number of generation for the de algorithm ( evolver ) .",
    "once two distinct sets of easy and hard instances are ready , we start analysing various features of the constraints for these two categories",
    ". this could give us the knowledge to understand which features of constraints have more contribution to problem difficulty .",
    "we focus on analysing the effects of constraints ( linear , quadratic and their combination ) on the problem and algorithm difficulty .",
    "we will extract features of constraints and analyse their effect on constrained problem difficulty .",
    "the experimented constraints are linear and quadratic as the form of : + @xmath60    @xmath61    or combination of them .",
    "we also consider various numbers of these constraints in this study . here , @xmath62 are the variables from equation [ eq : f ] and @xmath63 are coefficients within the lower and upper bounds ( @xmath64 ) . in our research , we construct constrained problems where the optimum of the experimented unconstrained problem is feasible",
    ". we use quadratic function as the form of equation [ quadraticeq ] ( univariate ) since it is more popular in recent constrained problem benchmarks .",
    "also , the influence of each @xmath65s can be analysed independently ( exponent 2 ) .",
    "the optimum of these problems is @xmath66 and we ensure that this point is feasible by requiring @xmath67 , when evolving the constraints .      in this paper , we study a set of statistic based features that leads to generating hard and easy problem instances .",
    "these features are discussed as follows : +    * * constraint coefficients relationship : * it is likely that the statistics such as standard deviation , population standard deviation and variance of the constraints coefficients can represent the constraints influences to problem difficulty .",
    "these constraint coefficients are @xmath68 in equation [ lineareq ] and [ quadraticeq ] . * * shortest distance : * this feature is related to the shortest distance between the objective function optimum and constraint . in this paper ,",
    "the shortest distance to the known optimum from each constraint and their relations to each other is discussed . to find the shortest distance of optimum point @xmath69 to the linear constraint hyperplane ( @xmath70 ) we use equation [ eq : distance ] .",
    "also , for quadratic constraint hyperplane ( @xmath71 ) we need to find the minimum of equation [ eq : distance_quad ] .",
    "+ @xmath72 + @xmath73 + where @xmath74 in equation [ eq : distance_quad ] is the distance from a point to a quadratic hyperplane . minimizing the distance squared ( @xmath75 ) is equivalent to minimizing the distance @xmath74 . *",
    "* angle : * this feature describes the angle of the constraints hyperplanes to each other .",
    "it is assumed that the angle between the constraints can influence problem difficulty . to calculate the angle between two linear hyperplanes , we need to find their normal vectors and angle between them using the following equation : @xmath76 where @xmath77,@xmath78 are normal vectors for two hyperplanes . also , the angle between two quadratic constraints is the angle between two tangent hyperplanes of their intersection .",
    "then , the angle between these tangent hyperplanes can be calculated by equation [ eq : angle ] .",
    "* * number of constraints : * number of constraints plays an important role in problem difficulty . in this research ,",
    "number of constraints and their effects to make easy and hard problem instances is analysed . *",
    "* optimum - local feasibility ratio : * although the global feasibility ratio is important to find the initial feasible point , it should not affect the convergence rate during solving the problem .",
    "so , in this research , he feasibility ratio of generated cop is calculated by choosing @xmath79 random points within the vicinity of the optimum in search space and the ratio of feasible points to all chosen ones is reported . in our experiment",
    ", the vicinity of optimum is equivalent to 1/10 of boundaries from optimum for each dimension .",
    "we now analyse the features of constraints ( linear , quadratic and their combination ) for easy and hard instances .",
    "we generate these instances for ( @xmath0deg ) algorithm using well known objective functions . in our experiments , we generate two sets of hard and easy problem instances . due to stochastic nature of evolutionary algorithms , for each number of constraints we perform 30 independent runs for evolving easy and hard instances .",
    "we set the evolving algorithm ( de ) generation number to 5000 for obtaining the proper easy and hard instances .",
    "the other parameters of evolving algorithm are set to population size = @xmath80 , crossover rate = @xmath81 , scaling factor = @xmath82 and @xmath83 is @xmath84 .",
    "values for these parameters have been obtained by optimising the performance of the evolving algorithm in order to achieve the more easier and harder problem instances . for ( @xmath0deg ) algorithm ,",
    "its best parameters are chosen based on @xcite .",
    "the ( @xmath0deg ) algorithm parameters are considered as : generation number = @xmath85 , population size = @xmath80 , crossover rate = @xmath81 , scaling factor = @xmath82 . also , the parameters for e - constraint method are set to control generation ( @xmath86 ) = @xmath87 , initial @xmath88 level ( @xmath89 ) = @xmath82 , archive size = @xmath90 ( @xmath10 is dimension number ) , gradient - based mutation rate ( @xmath91 ) = 0.2 and number of repeating the mutation ( @xmath92 ) = 3 .      in order to focus only on constraints",
    ", we carry out our experiments on various well - known objective functions .",
    "these functions are considered as : sphere ( bowl shaped ) , ackley ( many local optima ) , rosenbrock ( valley shaped ) and schaffer ( many local minima ) ( see @xcite ) .",
    "the linear constraint is as the form of equation [ lineareq ] with dimension ( @xmath10 ) as 30 and all coefficients @xmath93s and @xmath94s are within the range of @xmath95 $ ] .",
    "also , number of constraints is considered as @xmath96 to @xmath97 . to discuss and study some features such as shortest distance to optimum , we assume that zero is optimum ( all @xmath94s should be negative ) .",
    "we used ( @xmath0deg ) algorithm as solver to generate more easy and hard instances .    to illustrate our investigation , we plot a 2 dimension sphere function with 2 to 5 linear constraints in figure [ fig : plotlinear ] .",
    "it is obvious that the first row ( easy ) instances have higher feasibility ratio than the second row ( hard ) .    in the following",
    "we will present our findings based on various features for linear constraints ( for each dimension ) .",
    "+    deg ( 2 dimension ) .",
    "the dark blue hyperplane is the feasible solution ]    deg ( 2 dimension ) .",
    "the dark blue hyperplane is the feasible solution ]    figure [ fig : box_plot_std ] shows some evidence about linear constraints coefficients relationship such as standard deviation .",
    "it is obvious that there is a systematic relationship between the standard deviation of linear constraint coefficients and problem difficulty .",
    "the box plot ( see figure [ fig : box_plot_std ] ) represents the results for easy and hard instances using sphere , ackley , rosenbrock and schaffer objective function for ( @xmath0deg ) algorithm ( solver ) .",
    "as it is observed , the standard deviation for coefficients in each constraint ( 1 to 5 ) for easy instances are lower than hard ones .",
    "both these coefficient values can be a significant role to make a constrained problem harder or easier to solve . also , interestingly ,",
    "all different objective functions follow the same pattern .",
    "+ figure [ fig : box_plot_dis ] represents variation of shortest distance to optimum feature for easy and hard instances using ( @xmath0deg ) algorithm .",
    "the lower value means the higher distance from optimum .",
    "this means , the linear hyperplanes in easy instances are further from optimum . based on results",
    ", there is a strong relationship between problem hardness and shortest distance of constraint hyperplanes to optimum . in other word , this feature is contributing to problem difficulty .",
    "as expected , all objective functions follow the same systematic relationship between their feature and problem difficulty .",
    "this means , this feature can be used as a proper source of knowledge for predicting problem difficulty .",
    "+ the angle between linear constraint hyperplanes feature shows relationship between the angle and problem difficulty ( see table [ table : anglelinear ] ) .",
    "as it is observed in this table , the angle between constraints in easier instances are less than higher ones .",
    "so , this feature is contributing in problem difficulty .",
    "[ table : anglelinear ]    table [ table : fenlin ] explains the variation of number of constraints feature group .",
    "it is shown that the problem difficulty ( required fen for easy and hard instances ) has a strong systematic relationship with number of constraints for the experimented algorithm .    to calculate the optimum - local feasibility ratio ,",
    "@xmath98 points are generated within the vicinity of optimum ( zero in our problems ) .",
    "later , the ratio of feasible points to all generated points are investigated for easy and hard instances .",
    "results point out that increasing number of linear constraints , decreases the feasibility ratio for experimented algorithms ( see table [ table : feasibilityratiolin ] ) .",
    "+ in summary the variation of feature values over the problem difficulty is more prominent in some of them than the other groups of features .",
    "features such as , coefficients standard deviation , shortest distance , angle between constraints , number of constraints and feasibility ratio exhibit a relationship to problem hardness .",
    "this relationship is stronger for some features .      in this section ,",
    "we carry out our experiments on quadratic constraints .",
    "we use various objective functions , dimension and coefficient range similar to linear analysis . in the following the group of features",
    "are studied for easy and hard instances using quadratic constraints .",
    "+ observing the figure [ fig : box_plot_std ] , we can identify the relationship of quadratic coefficients and their ability to make problem hard or easy .",
    "based on the experiments , quadratic coefficients has the ability to make problems hard or easier for algorithms . in other words , in each constraint , the quadratic coefficients ( within the quadratic constraint ) are more contributing to problem difficulty than linear coefficients ( see equation [ quadraticeq ] ) .",
    "figure [ fig : box_plot_std ] shows the standard deviation of quadratic coefficients for easy and hard cops . as shown , the standard deviation of quadratic coefficient in 1 to 5 constraints in easy instances are less than harder one . in contrast to quadratic coefficients ,",
    "our experiments show there is no systematic relationship between the linear coefficient in quadratic constraints and problem hardness .",
    "in other words , quadratic coefficients are more contributing than linear ones in the same quadratic constraint .",
    "+            box plots shown in figure [ fig : box_plot_dis ] represent the shortest distance of a quadratic constraint hyperplanes to optimum .",
    "as it is observed , harder instances have constraint hyperplanes closer to optimum than easier ones .",
    "the lower values in these box plots means closer to optimum . calculating the angles between constraints",
    "do not follow any systematic pattern and there is no relationship between angle feature and problem difficulty for quadratic constraints .",
    "we also study the number of quadratic constraints feature .",
    "as it is shown in table [ table : fenquad ] , number of quadratic constraints is contributing to problem difficulty .",
    "it is obvious that increasing number of quadratic constraints makes a problem harder to solve ( increases fen ) .",
    "as observed in table [ table : feasibilityratioquad ] , investigations on feasibility ratio show that increasing number of constraint decreases the problem optimum - local feasibility ratio for easy and hard instances respectively .",
    "as it is observed , some group of features are more contributing to problem difficulty than the others .",
    "it is shown that angle feature does not follow any systematic relationship with problem hardness for experimented algorithm for quadratic constraints .",
    "on the other hand standard deviation , feasibility ratio and number of constraints are more contributing for @xmath0deag .    .optimum - local feasibility ratio of search space near the optimum for 1,2,3,4 and 5 linear constraint [ cols=\"^,^,^\",options=\"header \" , ]     [ table : feasibilityratioquadlin ]      in this section , we consider the combination of linear and quadratic constraints .",
    "the generated cops have different numbers of linear and quadratic constraints ( 5 constraints ) .",
    "the obtained results show the higher effectiveness of quadratic constraints .",
    "in other words , these constraints are more contributing to problem difficulty than linear ones . by analysing the various number of constraints ( see table [ table : feasibilityratioquadlin ] ) we can conclude that required fen for sets of constraints with more quadratic ones is higher than sets with more linear constraints .",
    "this relationship holds the pattern for both easy and hard instances .",
    "+ in summary it is observed that the variation of linear and quadratic constraint coefficients over the problem difficulty is more contributing for some group of features . considering quadratic constraints only , it is obvious that some features such as angle do not provide useful knowledge for problem difficulty . in general , this experiments point out the relationship of the various constraint features of easy and hard instances with the problem difficulty while moving from easy to hard ones .",
    "this improves the understanding of the constraint structures and their ability to make a problem hard or easy for a specific group of evolutionary algorithms .",
    "in this paper , we performed a feature - based analysis on the impact of sets of constraints ( linear , quadratic and their combination ) on performance of well - known evolutionary algorithm ( @xmath0deag ) .",
    "various features of constraints for easy and hard instances have been analysed to understand which features contribute more to problem difficulty .",
    "the sets of constraints have been evolved using an evolutionary algorithm to generate hard and easy problem instances for @xmath0deag .",
    "furthermore , the relationship of the features with the problem difficulty have been examined while moving from easy to hard instances .",
    "later on , these results can be used to design an algorithm prediction model .",
    "frank neumann has been supported by arc grants dp130104395 and dp140103400 .",
    "o.  mersmann , b.  bischl , h.  trautmann , m.  preuss , c.  weihs , and g.  rudolph . exploratory landscape analysis . in _ proceedings of the 13th annual conference on genetic and evolutionary computation _ , pages 829836 .",
    "acm , 2011 .",
    "o.  mersmann , m.  preuss , and h.  trautmann . benchmarking evolutionary algorithms : towards exploratory landscape analysis . in r.",
    "schaefer , c.  cotta , j.  kolodziej , and g.  rudolph , editors , _ parallel problem solving from nature - ppsn xi , 11th international conference , krakw , poland , september 11 - 15 , 2010 , proceedings , part i _ , volume 6238 of _ lecture notes in computer science _ , pages 7382 .",
    "springer , 2010 .",
    "s.  nallaperuma , m.  wagner , f.  neumann , b.  bischl , o.  mersmann , and h.  trautmann . a feature - based comparison of local search and the christofides algorithm for the travelling salesperson problem . in _ proceedings of the twelfth workshop on foundations of genetic algorithms xii",
    "_ , pages 147160 .",
    "acm , 2013 .",
    "s.  poursoltan and f.  neumann . a feature - based analysis on the impact of linear constraints for @xmath99-constrained differential evolution . in _ evolutionary computation ( cec ) , 2014 ieee congress on _ , pages 30883095 .",
    "ieee , 2014 .",
    "t.  takahama and sakai . constrained optimization by @xmath99 constrained differential evolution with dynamic @xmath99-level control . in",
    "_ advances in differential evolution _ , pages 139154 .",
    "springer , 2008 .",
    "t.  takahama and s.  sakai .",
    "constrained optimization by the @xmath99 constrained differential evolution with an archive and gradient - based mutation . in _ evolutionary computation ( cec ) ,",
    "2010 ieee congress on _ , pages 19 .",
    "ieee , 2010 ."
  ],
  "abstract_text": [
    "<S> different types of evolutionary algorithms have been developed for constrained continuous optimization . </S>",
    "<S> we carry out a feature - based analysis of evolved constrained continuous optimization instances to understand the characteristics of constraints that make problems hard for evolutionary algorithm . in our study , we examine how various sets of constraints can influence the behaviour of @xmath0-constrained differential evolution . investigating the evolved instances , we obtain knowledge of what type of constraints and their features make a problem difficult for the examined algorithm . </S>"
  ]
}