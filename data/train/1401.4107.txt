{
  "article_text": [
    "nowadays the mrf prior is quite popular for solving various inverse problems in image processing in that it is a powerful tool for modeling the statistics of natural images .",
    "image models based on mrfs , especially higher - order mrfs , have been extensively studied and applied to image processing tasks such as image denoising  @xcite , deconvolution  @xcite , inpainting  @xcite , super - resolution  @xcite , etc .    due to its effectiveness",
    ", higher - order filter - based mrf models using the framework of the field of experts ( foe )  @xcite , have gained the most attention .",
    "they are defined by a set of linear filters and the potential function . based on the observation that responses of mean - zero linear filters typically exhibit heavy - tailed distributions  @xcite on natural images , three types of potential functions have been investigated , including the student - t distribution ( st ) , generalized laplace distribution ( glp ) and gaussian scale mixtures ( gsms ) function .    in recent years",
    "several training approaches have emerged to learn the parameters of the mrf models  @xcite .",
    "table  [ foelearningresults ] gives a summary of several typical methods and the corresponding average denoising psnr results based on 68 test images from berkeley database with @xmath0 gaussian noise .",
    "@xmath1 foe & st&lap . & contrastive divergence & map , cg & 27.77@xcite + @xmath2 foe & gsms & contrastive divergence & gibbs sampling & 27.95@xcite + @xmath1 foe & gsms & persistent contrastive divergence & gibbs sampling & 28.40@xcite + @xmath1 foe & st & loss - specific(truncated optimization ) & map , gd & 28.24@xcite + @xmath1 foe & st & loss - specific(truncated optimization ) & map , lbfgs  @xcite & 28.39@xcite + @xmath1 foe & st & loss - specific(implicit differentiation ) & map ,",
    "cg & 27.86@xcite +    [ foelearningresults ] existing training approaches typically fall into two main types : ( 1 ) probabilistic training using ( persistent ) contrastive divergence ( ( p)cd ) ; ( 2 ) loss - specific training .",
    "roth and black  @xcite first introduced the concept of foe and proposed an approach to learn the parameters of foe model which uses a sampling strategy and the idea of cd to estimate the expectation value over the model distribution .",
    "et al . _",
    "@xcite improved the performance of their previous foe model  @xcite by changing ( 1 ) the potential function to gsms and ( 2 ) the inference method from map estimate to bayesian minimum mean squared error estimate ( mmse ) .",
    "the same authors present their latest results in  @xcite , where they achieve significant improvements by employing an improved learning scheme called pcd instead of previous cd .",
    "samuel and tappen  @xcite present a novel loss - specific training approach to learn mrf parameters under the framework of bi - level optimization  @xcite .",
    "they use a plain gradient - descent technique to optimize the parameters , where the essence of this learning scheme - the gradients , are calculated by using implicit differentiation technique .",
    "domke  @xcite and barbu  @xcite propose two similar approaches for the training of mrf model parameters also under the framework of bi - level optimization .",
    "their methods are some variants of standard bi - level optimization method  @xcite . in the modified setting ,",
    "the mrf model is trained in terms of results after optimization is truncated to a fixed number of iterations , _",
    "i.e. _ , they do not solve the energy minimization problem exactly ; instead , they just run some specific optimization algorithm for a fixed number of steps .    in a recent work  @xcite ,",
    "the bi - level optimization technique is employed to train a non - parametric image restoration framework based on regression tree fields ( rtf ) , resulting a new state - of - the - art .",
    "this technique is also exploited for learning the so - called analysis sparsity priors  @xcite , which is somewhat related to the foe model .",
    "* arguments * : the loss - specific training criterion is formally expressed as the following bi - level optimization problem @xmath3    the goal of this model is to find the optimal parameters @xmath4 to minimize the loss function @xmath5 , which is called the upper - level problem in the bi - level framework .",
    "the mrf model is defined by the energy minimization problem @xmath6 , which is called the lower - level problem .",
    "the essential point for solving this bi - level optimization problem is to calculate the gradient of the loss function @xmath5 with respect to the parameters @xmath4 . as aforementioned",
    ", @xcite employs the implicit differentiation technique to calculate the gradients explicitly ; in contrast , @xcite and @xcite make use of an approximation approach based on truncated optimization .",
    "all of them use the same st - distribution as potential function ; however , the latter two approaches surprisingly obtain much better performance than the former , as can be seen in table  [ foelearningresults ] .    in principle , samuel and tappen",
    "should achieve better ( at least similar ) results compared to the approximation approaches , because they use a `` full '' fitting training scheme , but actually they fail in practice .",
    "therefore , we argue that there must exist something imperfect in their training scheme , and we believe that we will very likely achieve noticeable improvements by refining this `` full '' fitting training scheme .    * contributions * : motivated by the above investigation , we think it is necessary and worthwhile to restudy the loss - specific training scheme and we expect that we can achieve significant improvements . in this paper , we do not make any modifications to the training model used in  @xcite - we use exactly the same model capacity , potential function and training images .",
    "the only difference is the training algorithm .",
    "we exploit a refined training algorithm that we solve the lower - level problem in the loss - specific training with very high accuracy and make use of a more efficient quasi - newton s method for model parameters optimization .",
    "we conduct a series of playback experiments and we show that the performance of loss - specific training is indeed underestimated in previous work  @xcite .",
    "we argue that the the critical reason is that they have not solved the lower - level problem to sufficient accuracy .",
    "we also demonstrate that solving the lower - level problem with higher accuracy is indeed beneficial .",
    "this argument about the loss - specific training scheme is the major contribution of our paper .",
    "we further show that our trained model can obtain slight improvement by increasing the model size .",
    "it turns out that for image denoising task , our optimized mrf ( opt - mrf ) model of size @xmath7 has achieved the best result among existing mrf - based systems and been on par with state - of - the - art methods . due to the simplicity of our model , it is easy to implement the inference algorithm on parallel computation units , _",
    "e.g. _ , gpu .",
    "numerical results show that our gpu - based implementation can perform image denoising in near real - time with clear state - of - the - art performance .",
    "in this section , we firstly present the loss - specific training model .",
    "then we consider the optimization problem from a more general point of view .",
    "our derivation shows that the implicit differentiation technique employed in previous work  @xcite is a special case of our general formulation .",
    "our training model makes use of the bi - level optimization framework , and is conducted based on the image denoising task . for image denoising , the st - distribution based mrf model is expressed as @xmath8    this is the lower - level problem in the bi - level framework .",
    "wherein @xmath9 is the number of filters , @xmath10 is the number of pixels in image @xmath11 , @xmath12 is an @xmath13 highly sparse matrix , which makes the convolution of the filter @xmath14 with a two - dimensional image @xmath11 equivalent to the product result of the matrix @xmath12 with the vectorization form of @xmath11 , _",
    "i.e. _ , @xmath15 . in our training model",
    ", we express the filter @xmath12 as a linear combination of a set of basis filters @xmath16 , _",
    "i.e. _ , @xmath17 . besides , @xmath18 is the parameters of st - distribution for filter @xmath12 , and @xmath19 defines the trade - off between the prior term and data fitting term . @xmath20 denotes the lorentzian potential function @xmath21 , which is derived from st - distribution .    the loss function @xmath22 ( upper - level problem )",
    "is defined to measure the difference between the optimal solution of energy function and the ground - truth . in this paper",
    ", we make use of the same loss function as in  @xcite , @xmath23 , where @xmath24 is the ground - truth image and @xmath25 is the minimizer of .    given the training samples @xmath26 ,",
    "where @xmath27 and @xmath28 are the @xmath29 clean image and the associated noisy version respectively , our aim is to learn an optimal mrf parameter @xmath30 ( we group the coefficients @xmath31 and weights @xmath32 into a single vector @xmath4 ) , to minimize the overall loss function .",
    "therefore , the learning model is formally formulated as the following bi - level optimization problem @xmath33 where @xmath34 .",
    "we eliminate @xmath19 for simplicity , since it can be incorporated into weights @xmath35 .      in this paper",
    ", we consider the bi - level optimization problem from a general point of view . in the following derivation",
    "we only consider the case of a single training sample for convenience , and we show how to extend the framework to multiple training samples in the end .    according to the optimality condition , the solution of the lower - level problem in   is given by @xmath25 , such that @xmath36 .",
    "therefore , we can rewrite problem   as following constrained optimization problem @xmath37 where @xmath38 .",
    "now we can introduce lagrange multipliers and study the lagrange function @xmath39 where @xmath40 and @xmath41 are the lagrange multipliers associated to the inequality constraint @xmath42 and the equality constraint in  , respectively . here",
    "@xmath43 denotes the standard inner product .",
    "taking into account the inequality constraint @xmath42 , the first order necessary condition for optimality is given by @xmath44 where @xmath45      ( { \\langle}k_i^t\\rho'(k_ix),p { \\rangle})_{n_f \\times 1}-\\mu\\\\[1.7ex ]      ( { \\langle}b_j^t\\rho'(k_ix ) + k_i^t{\\mathcal{d}}_i b_jx , p { \\rangle})_{n \\times 1}\\\\[1.7ex ]       { \\sum\\nolimits_{i=1}^{n_f}}\\alpha_i k_i^t\\rho'(k_ix ) + x- f\\\\[1.7ex ]      \\mu - \\max(0,\\mu - c\\alpha ) \\end{pmatrix}.\\ ] ] wherein @xmath46 , @xmath47 , in the third formulation @xmath48 .",
    "note that the last formulation is derived from the optimality condition for the inequality constraint @xmath42 , which is expressed as @xmath49 .",
    "it is easy to check that these three conditions are equivalent to @xmath50 with @xmath51 to be any positive scalar and max operates coordinate - wise .",
    "generally , we can continue to calculate the generalized jacobian of g , _ i.e. _ , the hessian matrix of lagrange function , with which we can then employ a newton s method to solve the necessary optimality system  .",
    "however , for this problem calculating the jacobian of g is computationally intensive ; thus in this paper we do not consider it and only make use of the first derivatives .",
    "since what we are interested in is the mrf parameters @xmath52 , we can reduce unnecessary variables in . by solving for @xmath53 and @xmath11 in  , and substituting them into the second and the third formulation , we arrive at the gradients of loss function with respect to parameters @xmath4 @xmath54 in",
    ", @xmath55 denotes the hessian matrix of @xmath56 , @xmath57    in  , we also eliminate the lagrange multiplier @xmath58 associated to the inequality constraint @xmath42 , as we utilize a quasi - newton s method for optimization , which can easily handle this type of box constraints .",
    "we can see that is equivalent to the results presented in previous work  @xcite using implicit differentiation",
    ".    considering the case of @xmath59 training samples , in fact it turns out that the derivatives of the overall loss function in with respect to the parameters @xmath4 are just the sum of over the training dataset .",
    "as given by , we have collected all the necessary information to compute the required gradients , so we can now employ gradient descent based algorithms for optimization , _",
    "e.g. _ , steepest - descent algorithm . in this paper",
    ", we turn to a more efficient non - linear optimization method  the lbfgs quasi - newton s method  @xcite . in our experiments",
    ", we will make use of the lbfgs implementation distributed by l. stewart . in our work , the third equation in is completed the l - bfgs algorithm , since this problem is smooth , to which l - bfgs is perfectly applicable .",
    "the training algorithm is terminated when the relative change of the loss is less than a tolerance , _",
    "e.g. _ , @xmath60 or a maximum number of iterations _",
    "e.g. _ , @xmath61 is reached or l - bfgs can not find a feasible step to decrease the loss .",
    "in order to demonstrate that the loss - specific training scheme was undervalued in previous work  @xcite , we conducted a playback experiment using ( 1 ) the same 40 images for training and 68 images for testing ; ( 2 ) the same model capacity24 filters of size @xmath1 ; ( 3 ) the same basis ``inverse '' whitened pca @xcite , as in samuel and tappen s experiments .",
    "we randomly sampled four @xmath62 patches from each training image , resulting in a total of 160 training samples .",
    "we then generated the noisy versions by adding gaussian noise with standard deviation @xmath0 .",
    "the major difference between our training experiment and previous one is the training algorithm . in our refined training scheme ,",
    "we employed ( 1 ) our proposed algorithm to solve the lower - level problem with very high accuracy , and ( 2 ) lbfgs to optimize the model parameters , but in contrast , samuel and tappen used non - linear conjugate gradient and plain gradient descent algorithm , respectively . in our refined training algorithm , we used the normalized norm of the gradient , _",
    "i.e. _ , @xmath63 ( @xmath59 is the pixel number of the training patch ) as the stopping criterion for solving the lower - level problem . in our training experiment , we set @xmath64 ( gray - value in range [ 0 255 ] ) , which implies a very accurate solution .",
    "based on this training configuration , we learned 24 filters of size @xmath1 , then we applied them to image denoising task to estimate the inference performance using the same 68 test images .",
    "finally , we got an average psnr value of 28.51db for noise level @xmath0 , which is significantly superior to previous result of 27.86db in  @xcite .",
    "we argue that the major reason lies in our refined training algorithm that we solve the lower - level problem with very high accuracy .    to make this argument more clear",
    ", we need to eliminate the possibility of training dataset , because we did not exploit exactly the same training dataset as previous work ( unfortunately we do not have their dataset in hand ) .",
    "since the training patches were randomly selected , we could run the training experiment multiple times by using different training dataset .",
    "finally , we found that the deviation of test psnr values based on 68 test images is within 0.02db , which is negligible .",
    "therefore , it is clear that training dataset is not the reason for this improvement , and the only remaining reason is our refined training scheme .    * the influence of @xmath65 : * to investigate the influence of the solution accuracy of the lower - level problem",
    "@xmath65 more detailedly , we conducted a series of training and testing experiments by setting @xmath65 to different magnitudes .",
    "based on a fixed training dataset ( 160 patches of size @xmath62 ) and 68 test images , we got the performance curves with respect to the solution accuracy @xmath65 , as shown in figure  [ accuracycurve ] ( left ) . from figure  [ accuracycurve ] ( left ) , we can clearly see that it is indeed the high solution accuracy that helps us to achieve the above siginificant improvement .",
    "this finding is the main contribution of our paper .",
    "we also make a guess how accurate samuel and tappen solve the lower - level problem according to their result and our performance curve , which is marked by a red triangle in figure  [ accuracycurve ] ( left ) . the argument that higher solution accuracy of the lower - level problem is helpful is explicable , the reason is described below .",
    "& the filter size}. it is clear that solving the lower - level problem with higher accuracy is beneficial and larger filter size can normally bring some improvement.,title=\"fig:\",scaledwidth=30.0% ]   & the filter size}. it is clear that solving the lower - level problem with higher accuracy is beneficial and larger filter size can normally bring some improvement.,title=\"fig:\",scaledwidth=30.0% ]    [ accuracycurve ]    as we know , the key aspect of our approach is to calculate the gradients of the loss function with respect to the parameters @xmath4 . according to  , there is a precondition to obtain accurate gradients : both the lower - level problem and the inverse matrix of hessian matrix @xmath66 must be solved with high accuracy , _",
    "i.e. _ , we need to calculate a @xmath25 such that @xmath36 and compute @xmath67 explicitly . since the hessian matrix @xmath66 is highly sparse , we can solve the linear system @xmath68 efficiently with very high accuracy ( we use the `` backslash '' operator in matlab ) .",
    "however , for the lower - level problem , in practice we can only solve it to finite accuracy by using certain algorithms , _",
    "i.e. _ , @xmath63 .",
    "if the lower - level problem is not solved to sufficient accuracy , the gradients @xmath69 are certainly inaccurate which will probably affect the training performance .",
    "this has been demonstrated in our experiments .",
    "therefore , for the bi - level training framework , it is necessary to solve the lower - level problem as accurately as possible , _",
    "e.g. _ , in our training we solved it to a very high accuracy with @xmath64 .",
    "* the influence of basis : * in our playback experiments , we used the `` inverse '' whitened pca basis to keep consistent with previous work .",
    "however , we argue that the dct basis is a better choice , because meaningful filters should be mean - zero according to the findings in  @xcite , which is guaranteed by dct basis without the constant basis vector . therefore , we will exploit the dct filters excluding the filter with uniform entries from now on . using this modified dct basis ,",
    "we retrained our model and we got a test psnr result of 28.54db .    * the influence of training dataset : * to verify whether larger training dataset is beneficial , we retrained our model by using ( 1 ) 200 samples of size @xmath70 and ( 2 ) 200 samples of size @xmath71 , which is about two times and four times larger than our previous dataset , respectively .",
    "finally , we got a test psnr result of 28.56db for both cases .",
    "as shown before , the influence of training dataset is marginal .    * the influence of model capacity : * in above experiments",
    ", we concentrated on the model of size @xmath1 to keep consistent with previous work .",
    "we can also train models of different filter sizes , _ e.g. _ , @xmath2 , @xmath7 or @xmath72 , to investigate the influence of model capacity . based on the training dataset of 200 patches of size @xmath70 ,",
    "we retrained our model with different filter size ; the training results and testing performance are summarized in figure  [ accuracycurve ] ( right ) .",
    "we can see that normally increasing the filter size can bring some improvement . however , the improvement of filter size @xmath72 is marginal compared to filter size @xmath7 , yet the former is much more time consuming .",
    "the training time for the model with 48 filters of size @xmath7 was approximately 24 hours on a server ( intel x5675 , 3.07ghz , 24 cores ) , but in contrast , the model of size @xmath72 took about 20 days .",
    "more importantly , the inference time of the model of size @xmath72 is certainly longer than the model of size @xmath7 , in that it involves more filters of larger size .",
    "therefore , the model of size @xmath7 offers the best trade - off between speed and quality , and we use it for the following applications .",
    "the learned 48 filters together with their associated weights and norms are presented in figure  [ fig : dct7 ] .    ) .",
    "the first number in the bracket is the weight @xmath32 and the second one is the norm of the filter.,scaledwidth=98.0% ]    [ fig : dct7 ]",
    "an important question for a learned prior model is how well it generalizes . to evaluate this ,",
    "we directly applied the above 48 filters of size @xmath7 trained based on image denoising task to various image restoration problems such as image deconvolution , inpainting and super - resolution , as well as denoising . due to space limitation , here we only present denoising results and the comparison to state - of - the - arts .",
    "the other results will be shown in the final version  @xcite .",
    "we applied our opt - mrf model to image denoising problem and compared its performance with leading image denoising methods , including three state - of - the - art methods : ( 1 ) bm3d  @xcite ; ( 2 ) lssc  @xcite ; ( 3 ) gmm - epll  @xcite along with two leading generic methods : ( 4 ) a mrf - based approach , foe  @xcite ; and ( 5 ) a synthesis sparse representation based method , ksvd  @xcite trained on natural image patches .",
    "all implementations were downloaded from the corresponding authors homepages .",
    "we conducted denoising experiments over 68 test images with various noise levels @xmath73 . to make a fair comparison , we used exactly the same noisy version of each test image for different methods and different test images",
    "were added with distinct noise realizations .",
    "all results were computed per image and then averaged over the test dataset .",
    "we used l - bfgs to solve the map - based mrf model  .",
    "when is applied to various noise level @xmath74 , we need to tune the parameter @xmath19 ( empirical choice @xmath75 ) .",
    "ours + 15 & 30.87 & 30.99 & 31.08 & * 31.27 * & 31.19 & 31.18 + 25 & 28.28 & 28.40 & 28.56 & * 28.70 * & * 28.68 * & * 28.66 * + 50 & 25.17 & 25.35 & 25.62 & * 25.72 * & * 25.67 * & * 25.70 * +    [ comparison ]    table  [ comparison ] shows the summary of results .",
    "it is clear that our opt - mrf model outperforms two leading generic methods and has been on par with three state - of - the - art methods for any noise level . comparing the result of our opt - mrf model with results presented in table  [ foelearningresults ]",
    ", our model has obviously achieved the best performance among all the mrf - based systems . to the best of our knowledge ,",
    "this is the first time that a mrf model based on generic priors of natural images has achieved such clear state - of - the - art performance .",
    "we provide image denoising examples in the final version  @xcite .    in additional ,",
    "our opt - mrf model is well - suited to gpu parallel computation in that it only contains the operation of convolution .",
    "our gpu implementation based on nvidia geforce gtx 680 accelerates the inference procedure significantly ; for a denoising task with @xmath0 , typically it takes 0.42s for image size @xmath76 , 0.30s for @xmath77 and 0.15s for @xmath78 . in table",
    "[ runningtime ] , we show the average run time of the considered denoising methods on @xmath77 images . considering the speed and quality of our model , it is a perfect choice of the base methods in the image restoration framework recently proposed in  @xcite , which leverages advantages of existing methods .",
    "r|c|c|c|c|c|c & ksvd & foe & bm3d & lssc & epll & ours + t(s ) & 30 & 1600 & 4.3 & 700 & 99 & 12 ( * 0.3 * ) + psnr & 28.28 & 28.40 & 28.56 & 28.70 & 28.68 & 28.66 +    [ runningtime ]",
    "in this paper , we revisited the loss - specific training approach proposed by samuel and tappen in  @xcite by using a refined training algorithm .",
    "we have shown that the performance of the loss - specific training was indeed undervalued in previous work .",
    "we argued that the major reason lies in the solution accuracy of the lower - level problem in the bi - level framework , and we have demonstrated that solving the lower - level problem with higher accuracy is beneficial .",
    "we have shown that we can further improve the performance of the learned model a little bit by using larger filters . for image denoising task , our learned opt - mrf model of size",
    "@xmath7 presented the best performance among existing mrf - based systems , and has already been on par with state - of - the - art denoising methods .",
    "the performance of our opt - mrf model proves two issues : ( 1 ) the loss - specific training scheme under the framework of bi - level optimization , which is convergence guaranteed , is highly effective for parameters learning ; ( 2 ) map estimate should be still considered as one of the leading approaches in low - level vision ."
  ],
  "abstract_text": [
    "<S> it is now well known that markov random fields ( mrfs ) are particularly effective for modeling image priors in low - level vision . </S>",
    "<S> recent years have seen the emergence of two main approaches for learning the parameters in mrfs : ( 1 ) probabilistic learning using sampling - based algorithms and ( 2 ) loss - specific training based on map estimate . after investigating existing training approaches </S>",
    "<S> , it turns out that the performance of the loss - specific training has been significantly underestimated in existing work . in this paper , we revisit this approach and use techniques from bi - level optimization to solve it . </S>",
    "<S> we show that we can get a substantial gain in the final performance by solving the lower - level problem in the bi - level framework with high accuracy using our newly proposed algorithm . as a result , </S>",
    "<S> our trained model is on par with highly specialized image denoising algorithms and clearly outperforms probabilistically trained mrf models . </S>",
    "<S> our findings suggest that for the loss - specific training scheme , solving the lower - level problem with higher accuracy is beneficial . </S>",
    "<S> our trained model comes along with the additional advantage , that inference is extremely efficient . </S>",
    "<S> our gpu - based implementation takes less than 1s to produce state - of - the - art performance . </S>"
  ]
}