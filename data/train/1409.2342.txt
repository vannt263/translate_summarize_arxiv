{
  "article_text": [
    "this paper is concerned with the numerical solution of stochastic differential equations ( sdes ) by the multilevel monte carlo ( mlmc ) method .",
    "mlmc @xcite is an important variance - reduction method that is well established by now and has been successfully applied to a wide class of problems in stochastic simulation and in uncertainty quantification ; for example , @xcite . the variance reduction in mlmc",
    "is achieved by computing approximations of the solution on different `` levels '' consisting , in the sde case , of numerical integrators with different time - step sizes .",
    "these computations are then combined in an efficient way to define a multilevel estimator for the moments that has a smaller variance than the standard monte carlo estimator and can therefore be computed faster .",
    "let @xmath0 denote a probability space and let @xmath1 and @xmath2 denote the expectation and variance with respect to @xmath3 .",
    "consider first the initial - value problem @xmath4 for @xmath5 and @xmath6 and initial data @xmath7 . here",
    "@xmath8 is a vector of @xmath9 standard brownian motions on @xmath10 .",
    "suppose that there exists a well - defined solution @xmath11 when [ eq : sde ] is interpreted as an ito integral equation . for simplicity",
    ", we only consider approximating moments of the solution at a prescribed end time as the quantities of interest , but other , more complicated functionals could also be studied .",
    "that is , we are interested in computing @xmath12 for some @xmath13 and time @xmath14 .",
    "consider the approximation by a sequence of random variables @xmath15 for @xmath16 with @xmath17 and a time step @xmath18 .",
    "for example , @xmath19 may result from the euler ",
    "maruyama method @xmath20 with @xmath21 . in this case , @xmath19 is a weak first - order approximation to @xmath22 so that , for any @xmath23 in a suitable class of test functions @xmath24 , @xmath25 if @xmath26 and @xmath27 are sufficiently smooth , @xmath28 contains all infinitely differentiable functions whose derivatives are polynomially bounded ; for example , ( * ? ? ?",
    "* theorem  14.5.1 ) .    in some cases",
    "@xcite , it is possible to find a second sde , called the modified sde with solution @xmath29 , such that @xmath19 is a second - order weak approximation to @xmath29 ; that is , @xmath30 then , the solution of the modified equation @xmath29 is an order of @xmath31 closer to the numerical solution than @xmath32 .",
    "the modified equation takes the form @xmath33 where @xmath34 and @xmath35 for some @xmath36 and @xmath37 .",
    "this reduces to [ eq : sde ] with @xmath38 , and @xmath39 and @xmath40 describe the correction in the drift and diffusion needed to achieve [ eq : aa ] .",
    "our results concern sdes and numerical integrators where the second - order modified equation is available . except in special cases ( e.g. , if @xmath27 is independent of @xmath41 )",
    ", this does not include the euler ",
    "maruyama method @xcite .",
    "it does include the milstein method , which has a second - order modified equation @xcite . using weak - approximation theory and modified equations ,",
    "we develop an alternative method of analysis for mlmc in this paper . by doing this",
    ", we no longer depend directly on the strong - approximation properties of the integrator ( as in other papers , e.g. @xcite ) and this gives greater freedom in the application of mlmc .",
    "we focus on a class of integrators for an important model in molecular dynamics and atmospheric dispersion , the _ langevin equation _ : @xmath42 for parameters @xmath43 , a potential @xmath44 , and a @xmath45-dimensional vector @xmath8 of brownian motions .",
    "we specify initial conditions @xmath46 .",
    "this system is used in molecular dynamics to simulate a system of particles in a heat bath and has equilibrium distribution with pdf @xmath47 , known as the _ gibbs canonical distribution _",
    ", where @xmath48 is a normalisation constant , @xmath49 , and @xmath50 . as usual",
    ", @xmath51 denotes the boltzmann constant and @xmath52 temperature .",
    "the langevin equation is also used to model the dispersion of atmospheric pollutants in homogeneous turbulence @xcite . in that case",
    ", @xmath53 is the inverse velocity autocorrelation time and @xmath54 is the strength of turbulent velocity fluctuations , and @xmath45 is equal to the number of space dimensions .",
    "this is much smaller than in molecular dynamics applications , where @xmath45 is proportional to the number of particles . with a slight generalisation",
    ", it can also be used to model the dispersion in inhomogeneous turbulence .",
    "numerical integrators for the langevin equation are well developed for example in @xcite .",
    "recently , there has been a strong push to understand the invariant measure associated to the integrators @xcite .",
    "second - order modified equations are available for the most important integrators for the langevin equation .",
    "in particular , we study splitting methods based on exact sampling of an ornstein  uhlenbeck process and symplectic integrators ( symplectic euler and strmer  verlet ) for the hamiltonian part .",
    "we show how to couple the different levels and apply mlmc with these methods .",
    "we find the use of the exact ornstein  uhlenbeck process is particularly effective when @xmath53 is large .",
    "we also combine these new integrators with extrapolation @xcite .",
    "it is a natural addition to mlmc methods , already mentioned in the original work @xcite and studied in more detail in @xcite .",
    "it reduces the bias in the numerical approximation of the solution due to time stepping and relies on having a sharp estimate for the bias error . if such an estimate is available , it is possible to eliminate the leading - order error term in the bias error by extrapolating from a sequence of approximations with differing time - step sizes .",
    "these approximations are naturally available in mlmc .",
    "we provide a set of experiments for the langevin equation with a harmonic and a double - well potential , comparing integrators based on splitting methods and extrapolation within mlmc .",
    "our results confirm that the splitting methods are significantly more effective than the euler ",
    "maruyama method when combined with mlmc .",
    "all methods have the same asymptotic @xmath55-cost ; that is , the cost always grows inverse proportionally to the mean - square error , but the proportionality constant is reduced by an order of magnitude from the standard euler ",
    "maruyama method through our enhancements .",
    "finally , we show how discrete random variables , as an approximation to the gaussian increments of a brownian motion , can be used within mlmc .",
    "this would be difficult to analyse by the standard analysis , since all the approximation results for integrators based on discrete random variables are in distribution only ( e.g. ,  @xcite ) .",
    "in general , one must be careful in using discrete random variables in place of gaussian random variables .",
    "the discrete approximations do not share the property of gaussian random variables that the sum of two independent increments is itself an increment from the same distribution and hence the telescoping sum property , which is key to the standard mlmc idea , no longer holds .",
    "however , for a practical range of parameters in small - noise problems , the extra bias introduced is small and easy to estimate .",
    "accepting this extra bias can lead to a significant improvement in efficiency , since discrete random variables allow the exact evaluation of the expected value on the coarsest level .",
    "the cost of this direct evaluation grows exponentially with the number of time steps , but it requires no sampling and , for a small number of time steps , its cost is significantly smaller than that of a monte carlo estimator . to analyse this method , we prove a new complexity theorem that allows for extra bias to be introduced between levels in mlmc .",
    "the paper is organised as follows .",
    "[ sec : mlmc ] reviews mlmc , including the important complexity theorem .",
    "[ sec : anal ] uses modified equations to apply the complexity theorem , depending only on weak convergence of the integrators .",
    "[ sec : geom ] reviews splitting methods for the langevin equation and defines a number of integrators where modified equations are available .",
    "numerical experiments are presented in [ sec : num_exp ] to demonstrate the effectiveness of this methodology for the langevin equation and to give quantitative predictions of the possible gains .",
    "a final section considers approximation of the gaussian increments by discrete random variables and highlights the potential gains this can bring .",
    "the c++ source code that we developed for the numerical experiments is freely available for download under the lgpl 3 license .",
    "when solving an sde numerically , the total error consists of the bias due to the time - stepping method and the monte carlo sampling error .",
    "the sum of these two terms should be reduced below a given small tolerance @xmath55 .",
    "a standard monte carlo method achieves this by computing @xmath56 sample paths , with @xmath57 , and taking time step @xmath58 , where @xmath59 is the order of weak convergence ( e.g. , @xmath60 for the euler ",
    "maruyama method ) .",
    "hence , we can achieve accuracy @xmath55 with total cost @xmath61 .",
    "in contrast , mlmc uses a series of coarse levels with larger time steps to construct an estimator .",
    "if the strong order of convergence of the employed integrator is one , mlmc reduces the cost of the method to @xmath62 , which is the lower limit for a monte carlo method .",
    "while mlmc is more efficient than standard monte carlo in the limit @xmath63 , the actual value of the tolerance @xmath55 might be relatively large in practical applications .",
    "hence , not only the asymptotic rate of convergence , but also the cost of the method for a given @xmath55 is of interest .",
    "the exact value of the constant @xmath64 in the cost function @xmath65 and the size of higher - order corrections depends on the details of the method , such as the time - stepping scheme and the coarse - level solver .",
    "in particular , choosing a time - stepping scheme that becomes unstable on the coarser levels can severely limit the performance as only a small number of levels can be used ; see @xcite .",
    "suppose that we are interested in the expectation of @xmath66 , where @xmath67 is the solution to [ eq : sde ] at time @xmath52 and @xmath68 defines the quantity of interest .",
    "assume that the number of time steps used to discretise the sde is @xmath69 , where @xmath70 .",
    "our strategy is to approximate [ eq : sde ] using a numerical integrator with time step @xmath71 to define an approximate solution @xmath72 .",
    "then , we compute many independent samples of @xmath73 to define approximate samples @xmath74 of @xmath75 .",
    "the classical monte carlo method approximates @xmath76 by the sample average of @xmath74 .",
    "instead , the mlmc method constructs a sequence of approximations on levels indexed by @xmath77 with @xmath78 time steps of size @xmath79 .",
    "let @xmath80 denote independent samples of the approximation to @xmath81 on level @xmath82 and let @xmath83 denote the monte carlo estimator on level @xmath82 based on @xmath84 samples .",
    "an estimator for the finest level where @xmath85 can be written as the telescoping sum @xmath86 where @xmath87 and @xmath88 the estimator does not introduce any additional bias , as we recover the numerical discretisation error on the finest level ( where @xmath89 ) : @xmath90 where @xmath91 is the standard monte carlo estimator for @xmath85 time steps .",
    "the two key ideas of the mlmc method are now :    * the number of time steps @xmath92 is smaller on the coarser levels @xmath93 .",
    "hence , the calculation of a single sample @xmath80 is substantially cheaper . *",
    "the success of the method depends on coupling the samples @xmath80 and @xmath94 so that the variance of @xmath95 is small . by arranging for the variance of @xmath96 to be small",
    ", a smaller number @xmath84 of samples suffices to construct an accurate estimator @xmath97 .",
    "this allows the construction of a mlmc estimator with fixed total variance @xmath98/{n}_\\ell$ ] and lower computational cost .",
    "this is formalised in the following complexity theorem ( * ? ? ?",
    "* theorem 3.1 ) :    [ theo : complexitytheorem ] consider a real - valued random variable @xmath75 and estimators @xmath99 corresponding to a numerical approximation based on time step @xmath100 and @xmath84 samples .",
    "if there exist independent estimators @xmath97 based on @xmath84 monte carlo samples , and positive constants @xmath101 , @xmath102 , @xmath103 , @xmath104 such that    1 .",
    "@xmath105 , 2 .   @xmath106       { \\mean{{\\widehat{{\\mathcal{p}}}}_\\ell-{\\widehat{{\\mathcal{p}}}}_{\\ell-1 } } } , & \\ell>0,\\end{cases}$ ] 3 .",
    "@xmath107 , and 4 .",
    "@xmath108 , the computational complexity of @xmath97 , is bounded by @xmath109 ,    then there exists a positive constant @xmath110 such that for any @xmath111 , there are values @xmath112 and @xmath84 for which @xmath113 from [ eq22 ] has a mean - square error ( mse ) with bound @xmath114 and a computational complexity @xmath115 with bound @xmath116    the theorem can be extended to allow the variance to decay as @xmath117 @xcite .",
    "for all cases in this paper , @xmath118 and the cost is concentrated on the coarsest level ( as we see from [ alg ] line [ eqn : nestimator ] and ( iii ) above ) .",
    "the asymptotic dependence of the computational complexity on @xmath55 is independent of the weak order of convergence @xmath59 of the time - stepping method .",
    "however , the constant @xmath110 does depend on the particular time - stepping method .    to obtain the results in this paper , we used algorithm [ alg : mlmc ] and our choices for the numbers of samples @xmath84 on each of the levels are defined adaptively via @xmath119 using the sample variance following @xcite",
    ". given a tolerance @xmath120 , the algorithm gives an mlmc estimator @xmath121 with mean - square error @xmath55 in the range @xmath122 as defined in [ end ] .",
    "choose @xmath123 such that , on the finest level with @xmath124 time steps of size @xmath125 , the bias @xmath126 is smaller than @xmath127 .",
    "define @xmath128 .",
    "choose a minimum number of samples @xmath129 ( say @xmath130 or @xmath131 ) .",
    "set @xmath132 , @xmath133 , @xmath134 for all levels @xmath82 .",
    "calculate @xmath135 by applying the numerical integrator on levels @xmath82 and @xmath136 ( except for @xmath137 ) for sample @xmath138 . the two trajectories should be coupled ( see  [ ss : c ] ) , but @xmath135 should be independent of any other sample ( i.e. , of @xmath139 for @xmath140 or @xmath141 )",
    ". @xmath142 .",
    "[ eq : algestimators ] update estimators for the bias and variance : @xmath143.\\ ] ] @xmath144 .",
    "[ eqn : nestimator ] calculate the optimal @xmath119 according to formula ( 12 ) in @xcite : @xmath145 return estimator @xmath146",
    "our goal is to apply the complexity theorem to numerical integrators using only weak - approximation properties of the numerical methods .",
    "the complexity theorem makes assumptions on ( i ) the bias , ( ii ) the consistency of the estimators , and ( iii ) the variance of the corrections .",
    "( i ) can be understood from existing weak - convergence analysis .",
    "let @xmath147 be the set of infinitely differentiable functions @xmath148 such that all derivatives are polynomially bounded .",
    "for a time step @xmath149 , let @xmath19 be a @xmath150-valued random variable that approximates the solution @xmath32 to [ eq : sde ] at time @xmath151 .",
    "we say @xmath19 is a",
    "_ weak order-@xmath59 _",
    "approximation if , for all @xmath152 and @xmath14 , there exists @xmath153 such that for @xmath31 sufficiently small @xmath154    there are many integrators that provide weak order-@xmath59 approximations for @xmath60 or @xmath155 ( e.g. , @xcite or [ sec : geom ] ) . in the case that @xmath156 , @xmath157 , and @xmath158 for an @xmath159 with step @xmath160 that is weak @xmath59-order , the bias condition ( i ) holds .",
    "the consistency of the estimators ( ii ) is an easy consequence of the linearity of integration and [ eq : pp ] .    condition ( iii ) on the variance of corrections normally follows from the mean - square convergence of the integrator @xcite .",
    "mean - square convergence measures the approximation of individual sample paths of the solution @xmath32 and hence is a tool for understand the coupling of successive levels . in this paper",
    ", we use an alternative method based on weak - approximation theory and derive condition ( iii ) as a consequence of the existence of a second - order modified equation . to do this ,",
    "we introduce the following doubled - up system for @xmath161\\in\\real^{2d}$ ] : @xmath162 the same initial data is applied and the same @xmath8 drives both components and so @xmath163 a.s . for @xmath164 .",
    "we now have two copies of @xmath32 and we approximate each differently .",
    "formally , we approximate @xmath32 and @xmath165 by different numerical integrators with step @xmath149 and denote the resulting approximation to @xmath166 by @xmath167 $ ] at @xmath168 . in mlmc , there is usually one integrator applied with time steps @xmath18 for @xmath41 and @xmath169 for @xmath170 ( which is a little awkward for @xmath171 , as one increment of @xmath172 corresponds to two steps of the underlying integrator ) .",
    "the joint distribution of the @xmath173 and @xmath171 contains all the required information about the coupling of the approximations of each component and , as we now show , a weak - convergence analysis of the system gives condition ( ii ) .    for simplicity , we start by assuming that @xmath174 $ ] is a weak second - order approximation to @xmath175 $ ]",
    ". then , we can prove the following .",
    "[ maint1 ] fix @xmath14 and let @xmath176 for a @xmath177 .",
    "suppose that @xmath178 is a weak second - order approximation to @xmath179 .",
    "conditions ( i)(iii ) of [ theo : complexitytheorem ] hold with @xmath180 $ ] given by samples of @xmath181 $ ] with @xmath182 and @xmath183 .",
    "the condition on @xmath178 implies also that @xmath19 and @xmath171 are weak second - order approximations to @xmath32 .",
    "then , by the above discussion , conditions ( i ) with @xmath155 and ( ii ) hold .",
    "let @xmath184 .",
    "then @xmath185 since @xmath186 and hence @xmath187 are smooth and their derivatives are polynomially bounded .",
    "as @xmath178 is a weak second - order approximation to @xmath188 , @xmath189 by definition of @xmath190 , @xmath191 using the fact that @xmath192 a.s . , we have @xmath193 written in terms of @xmath194 and @xmath195 , this means @xmath196 in other words , the variance of each sample of the coarse  fine correction is order @xmath197 .",
    "this implies that the sample average @xmath97 of @xmath84 samples satisfies condition ( iii ) of [ theo : complexitytheorem ] .",
    "the above argument does not apply to weak first - order accurate methods , even though the complexity theorem only requires @xmath198 . in this case",
    ", we use the theory of modified equations to extend the analysis .",
    "a modified equation is a small perturbation of the original sde that the numerical method under consideration approximates more accurately .",
    "for the theory , we need a second - order modified equation for the doubled - up system and this contains second - order information about the coupling of the fine and coarse levels . in particular , we consider modified equations for the double - up system of the form : @xmath199 for @xmath200 and @xmath201 for @xmath202 .",
    "( this could be extended to allow @xmath203 to depend on both @xmath204 and @xmath205 . ) when the same integrator is used for each component , but with time steps @xmath31 and @xmath206 , it must hold that @xmath207 and @xmath208 .",
    "we show in [ maint ] that , subject to regularity conditions on the coefficients , the mlmc complexity theorem applies if a second - order modified equation exists and therefore mlmc works with @xmath209 complexity .",
    "the additional difficulty is that @xmath210 and we must estimate the variance of @xmath211 .",
    "we use a mean - square analysis and the following lemma , which gives a first - order @xmath212 bound on @xmath213 .",
    "the lemma requires a number of regularity assumptions on the coefficients of the modified equation , which hold , for example , if @xmath214 and @xmath215 are globally lipschitz continuous .    for @xmath216 $ ] , let @xmath188 satisfy the ito sde and @xmath217 $ ] satisfy the modified equation .",
    "suppose that    1 .",
    "@xmath218 and @xmath219 are globally lipschitz continuous with lipschitz constant @xmath220 .",
    "there exists @xmath221 such that , for all @xmath149 sufficiently small , @xmath222 ,    \\end{aligned}\\ ] ] where @xmath223 denotes the frobenius norm .",
    "then , if @xmath224 is globally lipschitz continuous , we have , for some constant @xmath225 independent of @xmath31 , @xmath226$.}\\ ] ]    this is an elementary calculation with the gronwall inequality and ito isometry .",
    "see [ useful_lem ] .",
    "we are now able to state and prove the main theorem of this article .",
    "in contrast to [ maint1 ] , @xmath186 is assumed to be lipschitz here .",
    "[ maint ] fix @xmath14 .",
    "let @xmath227 be globally lipschitz continuous .",
    "suppose that    1 .",
    "@xmath19 and @xmath171 are weak order-@xmath59 approximations to @xmath32 for some @xmath198 , 2 .",
    "@xmath178 are second - order weak approximations to @xmath228 , and 3 .",
    "the assumptions of [ lemma:1 ] hold",
    ".    then conditions ( i)(iii ) of [ theo : complexitytheorem ] hold with @xmath180 $ ] given by samples of @xmath181 $ ] with @xmath182 .    as before ,",
    "conditions ( i ) and ( ii ) are straightforward .",
    "it is the third condition , which normally follows from a strong - approximation theory , that requires the modified equation .",
    "let @xmath229 and note that @xmath230 .",
    "as @xmath178 is a second - order weak approximation to @xmath231 , we have @xmath232 by definition of @xmath190 , @xmath233 using the fact that @xmath192 a.s . , @xmath234 [ lemma:1 ] applies and the right - hand side in the last equation is @xmath235 .",
    "consequently , @xmath236 together , [ eq : a , eq : b ] imply that @xmath237 the remainder of the proof is the same as for [ maint1 ] .    by taking @xmath41 to be the exact solution ( i.e. , @xmath238 ) and @xmath239 as a projection onto the @xmath138th coordinate , [ eq : c ] implies that @xmath240 and hence first - order strong convergence",
    "can be proved by this method .",
    "this is consistent with the observation that the euler ",
    "maruyama method , which is not first - order strongly convergent in general , does not have a second - order modified equation .    in summary ,",
    "subject to smoothness conditions , if mlmc is applied with an integrator that has a second - order modified equation like [ eq : modeq ] then the variance of the coarse  fine correction is @xmath235 and the complexity of mlmc is @xmath209 .",
    "though the rate is fixed , the complexity of mlmc depends on the specific integrator used through the constant and , as we now show , this leads to large variations in efficiency .",
    "before showing how they can be used for mlmc , we introduce several integrators for the langevin equation .",
    "splitting methods are an important class of numerical integrators for differential equations . in the case of odes , they allow the vector field to be broken down into meaningful parts and integrated separately over a single time step , before combining into an integrator for the full vector field . see for example  @xcite .",
    "the langevin equation breaks down into the sum of a hamiltonian system and a linear sde for an ornstein  uhlenbeck ( ou ) process .",
    "then , for a splitting method , we define symplectic integrators for the hamiltonian system @xmath241 the ou process @xmath242 , which satisfies @xmath243 can be integrated exactly and we use this fact to define a so - called geometric integrator for [ eq : ou ] .",
    "it is clear that the sum of the right - hand sides of these two systems gives [ eq : langevin ] .",
    "there are a number of ways of combining integrators of [ eq : ham , eq : ou ] to define an integrator of the full system .",
    "the simplest , also known as the lie ",
    "trotter splitting , is to simulate [ eq : ham , eq : ou ] alternately on time intervals of length @xmath18 . in general",
    ", this technique can only be first - order accurate in the weak sense .",
    "alternatively , if the underlying integrators are second order , we can define a second - order splitting method by applying [ eq : ou ] on a half step , then [ eq : ham ] for a full step , and finally apply again [ eq : ou ] on a half step .",
    "this is called the symmetric strang splitting .",
    "see also @xcite .",
    "we now define specific integrators for [ eq : ham , eq : ou ] .",
    "[ eq : ham ] is a separable hamiltonian system , and the symplectic euler method and strmer  verlet methods provide simple , explicit methods for its numerical solution .",
    "the symplectic euler method is first - order accurate and the strmer  verlet method is second - order accurate .",
    "the solution of [ eq : ou ] is a multi - dimensional ou process and can be written as @xmath244 each component of @xmath245 is with mean zero and variance @xmath246 so that @xmath247 for @xmath248 .",
    "this suggests taking the following as the numerical integrator : for a time step @xmath249 , @xmath250 for @xmath251 .",
    "if @xmath252 , then @xmath253 has the same distribution as @xmath254 and this method is exact in the sense of distributions .",
    "methods of this type , where the variation of constants formula   is used for the discretisation , are often called geometric integrators @xcite .",
    "the full equations for the first order splitting ( symplectic euler ) and second - order splitting ( strmer  verlet ) are written as follows :    [ [ symplectic - eulerou ] ] symplectic euler / ou + + + + + + + + + + + + + + + + + + +    for @xmath255 with distribution @xmath256 , @xmath257    [ [ strmerverletou ] ] strmer  verlet / ou + + + + + + + + + + + + + + + + +    for @xmath258 with distribution @xmath256 @xmath259 subject to regularity conditions on the coefficients , [ eq : y ] is first - order and [ eq : yy ] second - order accurate in the weak sense by application of the baker  campbell ",
    "hausdorff formula .",
    "consider the langevin equation .",
    "following @xcite by using a computer algebra system to verify consistency of moments to fifth order , it is easy to find modified equations for the numerical integrators developed in [ sec : sm ] .",
    "for example , for the first - order splitting method with @xmath260 , the doubled - up modified equation is as follows : denote by @xmath261 $ ] the numerical approximation on the coarse level ( step @xmath31 ) and @xmath262 $ ] on the fine level ( step @xmath206 ) .",
    "the second - order modified equation is@xmath263 where @xmath264 is the same brownian motion for @xmath265 and @xmath266 .",
    "we conclude then that this method leads to @xmath235 variances in the coarse  fine correction , if the coefficients are sufficiently well behaved .",
    "identifying when the coefficients are well behaved is hard .",
    "for example , it is sufficient that the drift and diffusion in both the original and modified equations are globally lipschitz .",
    "these however are very strong conditions and do not hold for many realistic potentials .    for the second - order splitting method ( based on strmer  verlet method and exact ou integration )",
    ", we can apply [ maint1 ] to see that the variance of the coarse  fine corrections is @xmath235 .",
    "the regularity condition is on the original drift and diffusion and holds if @xmath267 is sufficiently smooth ( e.g. , infinitely differentiable and lipschitz ) .",
    "let @xmath268 $ ] denote the state - space variable .",
    "a key step in mlmc is computing approximations to @xmath269 at @xmath16 given @xmath22 based on integrators with time steps @xmath18 and @xmath270 that are coupled so the difference between the approximations has small variance . for the euler ",
    "maruyama method , this is achieved by choosing increments @xmath271 for the computation with time step @xmath169 , and choosing the sum @xmath272 for the corresponding interval of the computation with time step @xmath18 .",
    "it is hard to sample @xmath273 in [ eq : geom ] based on increments of the particular sample path of @xmath8 and , as a method for strong approximation , it is limited .",
    "it is easy however to sample @xmath273 as a gaussian random variable .",
    "we now show how to couple fine  coarse integrators for the mlmc method , without the direct link to the increment .",
    "first , note that @xmath274    @xmath275    we can simulate @xmath276 and @xmath277 , by generating @xmath278 and computing @xmath279 as @xmath280 and @xmath281 , we have @xmath282 .",
    "then , @xmath283 given @xmath284 at time @xmath285 , we find @xmath286 using two time steps of size @xmath31 by @xmath287 for @xmath251 .",
    "this is equivalent to a single time step of size @xmath288 and @xmath289 this method is used to generate the increments when using splitting methods within mlmc .",
    "we developed an object - oriented c++ code to compare the performance of different numerical methods for two model problems .",
    "the modular structure of the templated code makes it easy to change key components , such as the time - stepping method or random - number distribution , without negative impacts on the performance .",
    "the source code is available under the lgpl 3 license as a git repository on ` https://bitbucket.org/em459/mlmclangevin ` .",
    "key to the choice of parameters in algorithm [ alg : mlmc ] is the balance between bias error and statistical error .",
    "we assume that the bias error has the form in [ theo : complexitytheorem](i ) for a proportionality constant @xmath102 and that the finest time step @xmath290 .",
    "then , for a bias error of size @xmath291 , we require that @xmath292 given @xmath102 , @xmath293 and @xmath52 as well as a choice for @xmath294 , this can be solved to determine @xmath55 from @xmath112 or vice versa .",
    "the constant @xmath102 can be approximated by assuming that @xmath295 for some @xmath296 , so that @xmath297 and calculating @xmath298 after computing the left - hand side numerically .",
    "the following integrators are used in the numerical experiments below :    emg and emg+ : :    euler  maruyama as given by [ eq:1 ] with @xmath299 ( emg )    and @xmath300 ( emg+ ) .",
    "seg : :    first - order splitting method with symplectic euler / exact ou and    @xmath299 .",
    "see [ eq : y ] .",
    "svg : :    second - order splitting method with strmer ",
    "verlet / exact ou and    @xmath299 .",
    "see [ eq : yy ] .",
    "richardson extrapolation is a well - known technique for increasing the accuracy of a numerical approximation by computing two approximations with different discretisation parameters and taking a linear combination that eliminates the lowest - order term for the error .",
    "its extension to sdes was developed by @xcite and is particularly convenient for use with mlmc , as mlmc computes approximations on several levels and this has already been explored in @xcite .",
    "thus , we take @xmath301 and @xmath302 and suppose that , for some constants @xmath303 and @xmath304 , @xmath305 a simple linear combination of the two gives a higher - order approximation to @xmath306 ; in particular , for seg , we have @xmath60 and @xmath307 and @xmath308 an approximation to the left - hand side is given by @xmath309 .",
    "for svg , we have @xmath155 and @xmath310 , and @xmath311 an approximation to the left - hand side is given by @xmath312 . to observe the improved accuracy",
    ", the statistical error must also be reduced to match the bias error . an increase in accuracy from second- to fourth - order accuracy",
    "is achieved because the integrator is symmetric .    in the experiments , we apply extrapolation in the following scenarios :    emge and emge+ : :    emg / emg+ with extrapolation , increasing the weak order of convergence    from one to two .",
    "sege : :    seg with extrapolation , again increasing the weak order of convergence    from one to two .",
    "svge : :    svg with extrapolation , increasing the weak order of convergence from    two to four . due to the fourth - order convergence",
    ", it is sufficient to    take large time steps , and we choose @xmath313 and vary    @xmath294 rather than @xmath112 .      .",
    "the mean value is shown together with one ( dark gray ) and two standard deviations ( light gray).,title=\"fig : \" ] .",
    "the mean value is shown together with one ( dark gray ) and two standard deviations ( light gray).,title=\"fig : \" ] +    we first consider [ eq : langevin ] with @xmath260 and @xmath314 physically , with this potential , [ eq : langevin ] describes a randomly forced harmonic oscillator with resonance frequency @xmath315 and damping parameter @xmath53 ; the strength of the gaussian forcing is given by @xmath316 . for @xmath317 ( i.e. , in the absence of a potential )",
    ", the sde can be interpreted as a model for the dispersion of an atmospheric pollutant in a one - dimensional turbulent velocity field ( see @xcite ) . in this case",
    ", @xmath318 is the turbulent - velocity variance and @xmath319 the velocity relaxation - time . in [ fig : exactsolution ] , the marginal distributions for the position and velocity are visualised as a function of @xmath320 for the first set of parameters used in the numerical experiments ( @xmath321 and @xmath322 ) .",
    "we choose this simple example , for which we know the analytical solution , to verify the correctness of our code and to quantify numerical errors ; exact solutions of the langevin equation are also described in @xcite . as the system is linear ,",
    "the joint pdf of @xmath323 and @xmath266 is gaussian and is defined by their mean and covariance . denoting @xmath324 and the initial solution by @xmath325",
    ", we have @xmath326 with @xmath327 @xmath328 follows a gaussian distribution with mean @xmath329 and covariance matrix @xmath330 which can easily be evaluated using a computer algebra system .",
    "we compute @xmath331 for @xmath332 and the following set of parameters :    1 .",
    "@xmath333 , @xmath334 and @xmath335 .",
    "@xmath333 , @xmath336 and @xmath337 .",
    "the initial position and velocity were set to @xmath338 in both cases .",
    "errors are computed using the exact value computed from [ eq : bc , eq : bd ] .",
    "the exact values are @xmath339 and @xmath340 , respectively . the cpu time scaled by @xmath341 and the error ( bias error plus one standard deviation ) scaled by @xmath55",
    "are plotted in [ fig : results2_1_1,fig : results2_2_1 ] against @xmath55 .",
    "the scaling means we expect both graphs to be flat .",
    "we observe for both parameter sets that the integrators based on the exact ou process are the most efficient for small @xmath55 .",
    "even though svge uses a weak fourth - order accurate integrator , the complexity of mlmc can not be reduced beyond @xmath209 and it is the same as for the other integrators .",
    "the improvements come by improving constants , in this case by about a factor 4 in comparison to emg . for the second set of parameter values in [ fig : results2_2_1 ] ,",
    "the relaxation time is shorter and the noise is larger , and the improvement due to the splitting methods is even more pronounced ( factor 10 ) .    , the difference between the splitting methods and euler ",
    "maruyama is significantly larger.,title=\"fig : \" ] , the difference between the splitting methods and euler ",
    "maruyama is significantly larger.,title=\"fig : \" ]    , the difference between the splitting methods and euler ",
    "maruyama is significantly larger.,title=\"fig : \" ] , the difference between the splitting methods and euler ",
    "maruyama is significantly larger.,title=\"fig : \" ]    [ fig : results2_2_1 ]    in order to take large time - steps , it is necessary to ensure the stability of the integrator .",
    "it is well known from deterministic differential equations that most explicit integrators will have a stability constraint on the time - step size .",
    "this is the same for sdes and such stability constraints may severely restrict the number of levels that can be employed in the mlmc method and thus its efficiency @xcite .",
    "exact sampling of the ornstein  uhlenbeck process poses no stability constraints , allowing for smaller values of @xmath294 and thus for larger numbers of levels in mlmc in the case of splitting methods . for example , in the above simulations , increasing the number of time steps from @xmath299 to @xmath300 in euler - maruyama ( cf .",
    "emg and emg+ , as well as emge and emge+ ) lead to an improvement in efficiency .",
    "the same change has no effect in seg .",
    "however , the symplectic methods we are using for the hamiltonian part are explicit and have their own stability constraint @xcite , somewhat limiting this benefit of splitting methods .",
    "[ eg : dw ]    we now change the potential and consider the double - well potential @xmath342 where @xmath343 and @xmath315 are parameters .",
    "we compute @xmath12 for @xmath344 ( note @xmath345 takes distinct values at the bottom of the wells @xmath346 ) .",
    "for the numerical experiments in [ fig : results3_1_1 ] , we choose parameter values @xmath347 , @xmath348 , @xmath349 , and take initial data @xmath338 . the scaled cpu time and error for @xmath350",
    "are plotted against @xmath55 in [ fig : results3_1_1 ] , where errors are computed relative to a numerically computed value given by @xmath351 .",
    "it is noticeable again that the splitting methods and especially the symplectic euler - based methods are most efficient .",
    "in [ fig : results3_2_1 ] , we explore the behaviour of the algorithm as we increase the length of the time interval @xmath52 . for the plot ,",
    "we scale the cpu time by @xmath352 ; the computation time scales linearly with the number of time steps and , by scaling by @xmath52 , we see how the mlmc algorithm behaves with increasing @xmath52 .",
    "the errors are computed relative to the numerically computed values @xmath353 for @xmath354 ; @xmath355 for @xmath356 ; and @xmath357 for @xmath358 .",
    "the values for @xmath356 and @xmath358 are close , which indicates the system has moved close to the invariant measure by this time . in each case",
    ", seg is most efficient and we see the measure of cpu time @xmath359 decrease from about @xmath360 for @xmath350 to about @xmath361 for @xmath358 .",
    "the profiles are also less flat as @xmath52 is increased , indicating that the time steps may not be small enough to have entered the asymptotic regime .",
    "it is natural that the gains become less pronounced , when we come close to the invariant measure and the coupling between levels has decayed .     for @xmath362 and the double - well potential with @xmath333 , @xmath348 , @xmath349.,title=\"fig : \" ]   for @xmath362 and the double - well potential with @xmath333 , @xmath348 , @xmath349.,title=\"fig : \" ]     for @xmath362 and the double - well potential with @xmath333 , @xmath348 , @xmath349.,title=\"fig : \" ]   for @xmath362 and the double - well potential with @xmath333 , @xmath348 , @xmath349.,title=\"fig : \" ] +   for @xmath362 and the double - well potential with @xmath333 , @xmath348 , @xmath349.,title=\"fig : \" ]   for @xmath362 and the double - well potential with @xmath333 , @xmath348 , @xmath349.,title=\"fig : \" ]",
    "[ maint , maint1 ] provide a route to analysing the mlmc entirely by weak - approximation properties of the numerical method .",
    "this has a number of advantages : from the theoretical point of view , the analysis works for a wider set of test functions compared to the analysis of @xcite , which demands that quantities of interest @xmath186 are globally lipschitz continuous . from the algorithmic point of view , the order of weak convergence is determined by moment conditions up to a given degree depending on the order of convergence .",
    "there are a number of ways to satisfy these conditions .",
    "it is widely known @xcite that the gaussian random variables can be replaced by discrete random variables without disturbing the weak order of convergence .",
    "the obvious question then is whether we can use discrete random variables to our advantage also in the context of mlmc .",
    "mlmc depends crucially on the fact that the sum of two independent gaussian random variables is also gaussian .",
    "this allows increments to be generated on the fine levels and combined to give a random variable with the same distribution on the next coarser level , using [ ut ] .",
    "discrete variables do not have this property . while [ maint ] implies the coupling condition of [ theo : complexitytheorem](iii ) ,",
    "the sum of two three - point random variables is not a three - point random variable and the telescoping sum breaks down .",
    "in general , using discrete random variables with mlmc introduces extra error due to the telescoping sum no longer being exact .",
    "though @xcite provides an approach that preserves the telescoping sum by using a different discrete random variable on each level .",
    "here we do not follow this route .",
    "instead , we use the same discrete random variable on each level , accepting the additional bias error that this introduces , which crucially is of higher order . to control this additional bias and to ensure the total error is still below our chosen tolerance , we change the number of levels @xmath112 and the coarsest mesh size @xmath363 .",
    "discrete random variables are cheaper to generate than gaussian random variables and the coarsest level can be evaluated exactly , which we exploit to achieve a significant speed - up in the small noise case .",
    "the modified equations are unchanged if the gaussian random variables in the integrator are replaced by random variables with the same moments to order five ( including all cross moments to order five arising from the doubled - up system ) . for example",
    ", we can replace samples of @xmath364 random variables by samples of the random variable @xmath365 with distribution @xmath366 or @xmath367 we refer to @xmath365 as the three- and four - point approximations to the gaussian , respectively .",
    "this is a well - known trick for weak approximation of sdes , e.g. @xcite .",
    "the approximations have a number of advantages , as @xmath365 is quicker to sample than a gaussian and , due to the finite number of states , averages of functionals of @xmath365 can be computed exactly .      for all our integrators ,",
    "the evaluation of the coarse - level estimator @xmath368 with time step @xmath369 is the computationally most expensive part of the mlmc algorithm : even though the number of time steps and hence the number of samples per path is small , a large number of individual paths needs to be evaluated to reduce the variance of the coarse - level estimator .",
    "this cost can be reduced dramatically if a discrete distribution as discussed in [ seq : discretedistribution ] is used for the individual samples @xmath370 . in this case , a significantly cheaper estimator , which does not rely on monte carlo sampling , can be constructed .",
    "if the random numbers @xmath371 for each path are drawn from the three - point approximation in [ 16 ] , there is only a finite number @xmath372 of possible samples @xmath373 , each with associated probability @xmath374 .",
    "the expectation value of the quantity of interest can be calculated exactly on the coarsest level as @xmath375 for the three - point approximation , for example , we need to choose from the @xmath376 possible values of @xmath370 in each of the @xmath377 time steps , so @xmath378 is the number of different samples of @xmath379 . since the estimator contains no sampling error , its variance is zero . in algorithm",
    "[ alg : mlmc ] , we can replace @xmath380 and @xmath381 in lines [ eq : algestimators ] and [ eqn : nestimator ] .",
    "effectively , this implies that the sum in line [ eqn : nestimator ] only runs from @xmath382 to @xmath112 and it is not necessary to evaluate @xmath383 .",
    "naively , the computational complexity of evaluating [ eq : yexact ] is given by the product of the number of different samples and the number of time steps , @xmath384 .",
    "however , using a recursive algorithm , the computational complexity can be reduced to the number of nodes in the product - probability tree , which is only @xmath385 nevertheless , this still grows exponentially with the number @xmath377 of coarse time steps and so [ eq : yexact ] is only competitive for small values of @xmath377 and @xmath45 .",
    "however , exact evaluation can reduce the overall cost of the algorithm dramatically and this is exploited to significant advantage in [ subsec : num_discrete ] .",
    "we now state and prove a modified complexity theorem that allows for additional bias to be introduced between levels , as well as for a different computational cost on the coarsest level .",
    "let @xmath386 be the estimator corresponding to @xmath387 , but with increments given by [ ut ] . for gaussian increments",
    "these estimators are the same , but they are different when we use 3-point or 4-point approximations .",
    "recall that the fine , level @xmath82 , sample in each of the estimators @xmath97 uses increments sampled directly from the 3-point or 4-point distribution , while the coarse , level @xmath388 , sample is computed using two consecutive fine increments and formula [ ut ] .",
    "[ theo : modifiedcomplexity ] let us replace assumption ( ii ) of [ theo : complexitytheorem ] by    1 .",
    "@xmath389  and  @xmath390 ,    for some positive constants @xmath391 and @xmath392 .",
    "we suppose that all the other assumptions of [ theo : complexitytheorem ] hold , except that @xmath393 is not necessarily assumed to be bounded by @xmath394 any longer .",
    "then , there exists a positive constant @xmath395 such that for any @xmath111 , there are values @xmath396 , @xmath112 and @xmath84 for which @xmath113 from [ eq22 ] has a mse @xmath397 and a computational complexity @xmath115 with bound @xmath398    we only require slight modifications in the proof of ( * ? ? ?",
    "* theorem 3.1 ) to prove this result . in particular , it is sufficient to choose @xmath399 to bound the bias on the finest level .",
    "the factor @xmath400 appears , since we now have three error contributions , the bias on the finest level , the bias between levels and the sampling error , and since we require each of these contributions to the mse to be less than @xmath401 .    to guarantee that the bias between levels is less than @xmath401 , note that due to assumption ( ii ) we have @xmath402 and",
    "so a sufficient condition is @xmath403 with @xmath404 .    finally ,",
    "setting @xmath405 and exploiting standard results about geometric series , we get @xmath406 the computational cost can then be bounded by @xmath407 which leads to the desired bound with @xmath408 .",
    "( note that as in @xcite this ( optimal ) choice of @xmath84 is obtained by minimising the cost on levels 1 to @xmath112 subject to the constraint that the sum of the variances is less than @xmath401 . )",
    "if we use a @xmath409-point approximation and the expected value on the coarsest level is computed excactly , as described in [ sec : exactcoarselevel ] , then @xmath410 , for some @xmath411 .",
    "hence , the total cost grows exponentially with @xmath55 , as expected .",
    "however , for practically relevant values of @xmath55 , the exponential term may not be dominant and we may get significant computational savings , as we will see in the next section .",
    "note that @xmath412 for the three - point and @xmath413 for the four - point case , leading to a cost of @xmath414 and @xmath415 for the computation of the correction terms on levels 1 to @xmath112 , respectively .    since the sampling of discrete random variables is significantly cheaper , it may also be of interest to use standard monte carlo on the coarsest level , as in the earlier sections of this paper .",
    "if we slightly increase the constant in the formula for @xmath416 , @xmath417 , in the proof of [ theo : modifiedcomplexity ] and choose @xmath418 such that the total variance over all levels is below @xmath401 , then the dominant cost will be @xmath419 , and so @xmath420 , which will be @xmath421 and @xmath422 in the three- and four - point cases , respectively .",
    "however , in practice @xmath423 is significantly smaller than the constant @xmath110 in theorem [ theo : complexitytheorem ] , so that for moderate values of @xmath55 , the use of discrete random variables will pay off .",
    "-dependency of the additional bias term @xmath424 ( see ( ii ) in [ theo : modifiedcomplexity ] ) for @xmath425 , @xmath426 computed using the symplectic euler / exact ou splitting .",
    "results are shown both for three - point ( se3 ) and four - point ( se4 ) random variables . ]",
    "we carry out numerical experiments as in [ eg : ho ] with the damped harmonic oscillator , but change the parameters slightly to @xmath427 , @xmath426 ( i.e. , smaller noise ) .",
    "instead of sampling from a gaussian distribution , we use discrete random numbers , which introduce an additional bias as discussed above . to quantify this bias numerically , we plot the difference @xmath428 in [ fig : additional_bias ] for the symplectic euler / exact ou method both for three - point ( se3 ) and four point ( se4 ) distributions .",
    "the figure shows that , as predicted in @xcite , the additional bias is proportional to @xmath429 for se3 and to @xmath430 for se4 .",
    "we have also studied the dependence on the noise term ( not shown here ) and found that , as @xmath316 gets smaller , the additional bias is reduced very rapidly ( proportional to @xmath431 and @xmath432 , respectively ) .",
    "and @xmath426 computed using the symplectic euler / exact ou splitting method using three - point ( se3- , se3 , se3 + ) and four - point ( se4 ) random variables.,title=\"fig : \" ]   and @xmath426 computed using the symplectic euler / exact ou splitting method using three - point ( se3- , se3 , se3 + ) and four - point ( se4 ) random variables.,title=\"fig : \" ]    for the same setup , we measure the computational cost and the total error ( consisting of the statistical error , discretisation error and the additional bias introduced by sampling from discrete distributions ) .",
    "we calculate the same quantity of interest as in [ eg : ho ] .",
    "[ fig : discrete_distribution ] shows the results both for gaussian random variables ( seg ) and for the three- and four - point distributions ( se3 and se4 ) with @xmath433 .",
    "for the discrete distributions , the coarse - grid expectation value is calculated exactly .",
    "for the three - point distribution , we also varied the number of time steps on the coarsest level and use @xmath434 ( se3- ) , @xmath435 ( se3 ) and @xmath436 ( se3 + ) . in each case , we only show results up to the point where the additional bias error becomes too large . for fixed @xmath55 ,",
    "the se3 + method is more expensive than se3 and se3- , since the cost of the exact coarse - level evaluation grows exponentially with the number of time steps . on the other hand , using smaller time steps on the coarsest level allows the use of this method for smaller values @xmath55 where the additional bias becomes too large for se3- and se3 .",
    "the additional bias in the se4 method is so small that the method can be used up to values as small as @xmath437 .",
    "comparing the cost of this method to the gaussian case shows that using a discrete four - point distribution is more than 50-times faster in this case .",
    "we conclude that , if used with caution , approximating the gaussian increments in [ eq:1 ] by discrete approximations and calculating the coarse - level expectation value exactly can significantly improve the efficiency of the multilevel method .",
    "[ tab1 ] summarises our findings : mlmc gives a significant speed - up over the traditional monte carlo computation of averages and , even though the optimal complexity estimate @xmath209 for monte carlo - type methods holds for all the integrators under study , there is significant variation between the integrators .",
    "splitting methods are particularly appropriate for the langevin equation and using the exact ou solution yields a more stable integrator than euler  maruyama , even though both integrators are explicit . in the experiments ,",
    "the difference in computation time between euler  maruyama and the splitting methods is greater when the dissipation @xmath53 is higher , since euler ",
    "maruyama suffers from a more severe time - step restriction ( cf .",
    "figures [ fig : results2_1_1][fig : results3_2_1 ] ) .",
    ".comparison of monte carlo with euler  maruyama , mlmc with euler  maruyama and mlmc with the symplectic euler / ou integrator and extrapolation ( using gaussian increments ) . [ cols=\"^,^,^,^,^ \" , ]     this paper also introduced an alternative analysis method for mlmc based on modified equations .",
    "it provides a convenient approach to mlmc through weak - approximation theory ; strong - approximation theory is only needed to relate the original and modified equations and not the numerical methods .",
    "this accommodated the use of the splitting method and the exact ou solution easily .",
    "the weak - approximation analysis motivated the use of discrete random variables , such as three- and four - point approximations to the gaussian . in an example with small noise ( @xmath426 and @xmath438 ) , we saw between one and two orders of magnitude speed - up for a useful range of @xmath55 because we can evaluate the coarse level exactly .",
    "this method is easy to implement and it works well because the dominant cost lies on the coarsest level for these problems . while the speed improvements are impressive , this method should be used with care as it introduces an extra bias error .",
    "the extra bias can be estimated as shown in [ fig : additional_bias ] .",
    "the improvement would be less dramatic in higher dimensions as the number of samples required would increase dramatically and it may be impossible to compute the coarse level exactly . as an interesting side result , we proved a modified complexity theorem that allows for extra bias to be introduced between levels in mlmc .",
    "this is a standard gronwall argument with the ito isometry . the integral equation for the difference @xmath439 is @xmath440 where @xmath441 and @xmath442 .",
    "by conditions ( i ) and ( ii ) , @xmath443 and similarly for @xmath27 .",
    "assume @xmath444 .",
    "the ito isometry and jensen s inequality give @xmath445 gronwall s inequality gives @xmath235 bounds on @xmath446 .",
    "a similar argument can be applied to @xmath170 and applying the lipschitz condition on @xmath190 completes the proof .",
    "hairer , e. , lubich , c. , & wanner , g. 2010 . .",
    "springer series in computational mathematics , vol . 31 .",
    "springer , heidelberg .",
    "structure - preserving algorithms for ordinary differential equations , reprint of the second ( 2006 ) edition ."
  ],
  "abstract_text": [
    "<S> this paper applies several well - known tricks from the numerical treatment of deterministic differential equations to improve the efficiency of the multilevel monte carlo ( mlmc ) method for stochastic differential equations ( sdes ) and especially the langevin equation . </S>",
    "<S> we use modified equations analysis to circumvent the need for a strong - approximation theory for the integrator , and we apply this to introduce mlmc for langevin - type equations with integrators based on operator splitting . we combine this with extrapolation and investigate the use of discrete random variables in place of the gaussian increments , which is a well - known technique for the weak approximation of sdes . </S>",
    "<S> we show that , for small - noise problems , discrete random variables can lead to an increase in efficiency of almost two orders of magnitude for practical levels of accuracy .    </S>",
    "<S> keywords : :    numerical solution of stochastic differential equations , modified    equations , geometric integrators , weak approximation , extrapolation .    </S>",
    "<S> = 1 </S>"
  ]
}