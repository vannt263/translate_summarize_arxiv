{
  "article_text": [
    "phylogenetic invariants have been used for tree construction since the linear invariants of lake and cavender - felsenstein @xcite were found . although the linear invariants of the jukes - cantor model are powerful enough to asymptotically distinguish between trees on 4 taxa @xcite , these linear invariants do not perform well in simulations @xcite .    in the last 20 years",
    ", the entire set of phylogenetic invariants has been found for many models of evolution ( see @xcite and the references therein ) .",
    "since the invariants provide an essentially complete description of the model , using more invariants should give more power to distinguish between different models .",
    "however , different invariants give vastly different power at distinguishing models and it is not known how to find the most powerful invariants .    in this paper , we use techniques from machine learning to find metrics on the space of invariants which optimize their tree reconstruction power for the jukes - cantor and kimura 3-parameter phylogenetic models on trees with four leaves .",
    "specifically , we apply _ metric learning _",
    "algorithms inspired by @xcite to find the metric which best distinguishes the models . for training data we use simulations over a wide range of the parameter space .",
    "our main biological result is the construction of a metric which outperforms neighbor - joining on trees simulated from the felsenstein zone ( i.e. , trees with a short interior edge ) .",
    "more generally , we find that metric learning significantly improves upon other uses of invariants and is competitive with neighbor joining even for short sequences and homogeneous rates .",
    "casanellas and fernndez - snchez @xcite also used the kimura 3-parameter invariants to construct trees with four taxa .",
    "their results indicated that invariants can sometimes perform better than commonly used methods ( e.g. , neighbor joining and maximum likelihood ) for data that evolved with non - homogeneous rates and for extremely long sequences .",
    "they used the @xmath0 norm on the space of invariants , weighing each polynomial equally .",
    "this paper improves upon @xcite by showing how to improve upon the @xmath0 norm on the space of invariants .",
    "the @xmath0 norm behaves poorly since it weighs equally informative and non - informative invariants .",
    "simulating data and using metric learning improves the performance of invariants by putting much more weight on the powerful invariants .",
    "this allows us to build an algorithm which is very accurate for trees with short internal edges .",
    "we begin by briefly introducing the models and phylogenetic invariants we will use . section  [ sec : methods ] describes the metric learning algorithms ; section  [ sec : results ] gives the results of our simulation studies ; and we conclude with a short discussion .    by _ phylogenetic tree _",
    ", we mean a binary , unrooted tree with labelled leaves . there are three such trees with four leaves labelled @xmath1 , we call these trees @xmath2 , @xmath3 , and @xmath4 according to which leaf is on the same `` cherry '' as leaf 0 .",
    "we consider two phylogenetic models on these trees : the jukes - cantor ( jc69 ) model of evolution @xcite and the kimura 3-parameter ( k81 ) model @xcite , both with uniform root distribution .",
    "these models associate to each edge @xmath5 of the tree a transition matrix @xmath6 where where @xmath7 is the length of the edge @xmath5 and @xmath8 is a rate matrix : @xmath9 for jc69 and k81 respectively , where @xmath10 . for a given tree ,",
    "we write @xmath11 for @xmath12 and write @xmath13 for the joint probability distribution . _ phylogenetic invariants _ are polynomial equations which are satisfied between the joint parameters .",
    "for example , @xmath14 holds for both jc69 and k81 , but since this equation is true for all three trees , will ignore it and similar equations .",
    "[ ex:4pt ] consider the four - point condition on a tree metric @xcite .",
    "it says that if @xmath15 is a tree metric on @xmath16 , then @xmath17 given a probability distribution @xmath18 , the maximum likelihood jukes - cantor distance is @xmath19 where @xmath20 is the fraction of mismatches between the two sequences , e.g. , @xmath21 after substituting in ( [ eq:4pt ] ) and exponentiating , the equality becomes @xmath22 this observation is originally due to cavender and felsenstein @xcite .",
    "the difference of the two sides of ( [ eq:4ptp ] ) is a quadratic polynomial in the joint probabilities which we will call the four - point polynomial .",
    "since both models we consider are _ group - based _ , it is easiest to work in fourier coordinates which can roughly be thought of as the @xmath20 coordinates in example  [ ex:4pt ] ( cf .",
    "the website http://www.math.tamu.edu/~lgp/small-trees/ contains lists of invariants for different models on trees with a small number of taxa .",
    "our first task is building a set of invariants for the two models .",
    "the above website shows 33 polynomials ( plus two implied linear relations ) for jc69 and 8002 polynomials for k81 .",
    "however , these sets of invariants are not closed under the symmetries of @xmath23 .",
    "that is , each tree can be written in the plane in eight different ways ( for example , the tree @xmath2 can be written as ( 01 : 23 ) , ( 10 : 23 ) ,  , ( 32 : 10 ) ) , and each of these induces a different order on the probability coordinates @xmath24 .",
    "we need a set of invariants which does not change under this reordering if we do nt want the resulting algorithm to depend on the order of the input sequences .     and @xmath25 ranged from @xmath26 to @xmath27 in intervals of @xmath28 . ]    after performing this calculation , we are left with 49 polynomials for jc69 and 11612 for k81 .",
    "however , our metric learning algorithms run slowly as the number of invariants grows , so we had to find a subset of a more manageable size .",
    "we cut down the k81 invariants by testing each of the 11612 invariants individually on the entire parameter space and only keeping those which had good individual reconstruction rates .",
    "specifically , we picked several different values for @xmath29 and kept only those invariants which gave over a 62% reconstruction rate individually for sequences of length 100 .",
    "the result of this calculation is sets of invariants @xmath30 and @xmath31 of cardinality 49 and 52 .    given a probability distribution @xmath32 , and invariants @xmath33 , for tree @xmath34 ( for @xmath35 ) , let @xmath36 be the point in @xmath37 obtained by evaluating the invariants for @xmath34 at @xmath32 .",
    "if the probability distribution @xmath32 actually comes from the model @xmath23 , then we will have @xmath38 , @xmath39 , and @xmath40 generically ( that is , except for points @xmath32 which lie on the intersection of two or more models ) .",
    "this fact suggests that we can just pick the tree @xmath34 such that @xmath41 is closest to zero .",
    "however , the next example shows that it is quite important to pick good polynomials and weigh them properly .",
    "figure  [ fig : hist ] shows the distribution of four of the invariants from @xmath42 on data from simulations of 1000 i.i.d.draws from the jukes - cantor model on @xmath23 over a varying set of parameters .",
    "the histograms show the distributions for the simulated tree ( @xmath23 ) in yellow and the distributions for the other trees in gray and black .     and @xmath43 on simulated data",
    ". the yellow histogram corresponds to the correct tree , the black and gray are the other two trees . ]",
    "polynomial 10 ( upper left ) distinguishes nicely between the three trees with the correct tree tightly distributed around zero .",
    "it is correct 97% of the time on our space of trees ( figure  [ fig : param ] ) .",
    "polynomial 48 ( upper right ) also shows power to distinguish between all three trees , but the distributions are much more overlapping  it is only correct 50.8% of the time .",
    "polynomial 10 is the four - point invariant from example  [ ex:4pt ] , polynomial 48 is one of lake s linear invariants .",
    "the two other examples show a polynomial ( 23 ) which is biased towards selecting the wrong tree ( only 16% correct ) , and a polynomial ( 45 ) for which the correct tree is tightly clustered around zero , but the incorrect trees are indistinguishable and have wide variance ( 88.9% correct ) .",
    "the parameters used for the simulations are described in figure  [ fig : param ] . since 1000 samples",
    "should be quite enough to determine the structure of a tree on four taxa , it is revealing that many of the individual polynomials are quite poor ( the mean prediction rate for all 49 polynomials is only 42% ) .",
    "the invariants have quite different variances and means and it is not optimal to take each one with equal weight .",
    "this example shows that we need to scale and weigh the individual invariants .",
    "recall that for a positive ( semi)definite matrix @xmath44 the mahalanobis ( semi)norm @xmath45 is defined by @xmath46 notice that since @xmath44 is positive semidefinite , it can be written as @xmath47 where @xmath48 is orthogonal and @xmath49 is diagonal with non - negative entries .",
    "thus the square root @xmath50 is unique .",
    "now since @xmath51 , we can view learning such a metric as finding a transformation of the space of invariants that replaces each point @xmath52 with @xmath53 under the euclidean norm .",
    "accordingly , we will be searching for a positive semidefinite matrix @xmath44 on the space of invariants which is `` optimal '' .",
    "let @xmath54 be an empirical probability distribution generated from a phylogenetic model on tree @xmath23 with parameters @xmath55 .",
    "we wish to find @xmath44 such that the condition @xmath56 is typically true for most @xmath54 chosen from a suitable parameter space @xmath57 .",
    "now suppose that @xmath57 is a finite set of parameters from which we generate training data @xmath58 for @xmath59 .",
    "as we saw above , each of the eight possible ways of writing each tree induces a signed permutation of the coordinates of each @xmath60 .",
    "we write these permutations in matrix form as @xmath61 . given this training data , we wish to solve the following optimization problem .",
    "[ alg : meta ] * ( metric learning for invariants ) *    _ input : _ model invariants @xmath41 for @xmath34 and a finite set @xmath57 of model parameters .",
    "_ output : _ a semidefinite matrix @xmath44    _ procedure : _    1 .",
    "reduce the sets @xmath41 of invariants to a manageable size by testing individual invariants on data simulated from @xmath59 .",
    "2 .   augment the resulting sets so that they are closed under the eight permutations of the input which fix tree @xmath23 .",
    "3 .   compute the signed permutations @xmath62 which are induced on the invariants @xmath63 by the above permutations .",
    "solve the following semidefinite programming problem : @xmath64 where @xmath65 denotes that @xmath44 is a positive semidefinite matrix .",
    "5 .   alternatively ,",
    "if we restrict @xmath44 to be diagonal , this becomes a linear program and can be solved for much larger sets of invariants and parameters .    in the optimization step ,",
    "we use a regularization parameter @xmath66 to keep @xmath44 small and a margin parameter @xmath67 to increase the margin between the distributions .",
    "this is a convex optimization problem with a linear objective function and linear matrix equality and inequality constraints .",
    "hence it is a semidefinite programming ( sdp ) problem .",
    "the sdp problem above has a unique optimizer and can be solved in polynomial time .",
    "its complexity depends on the capacity of the set @xmath57 since each point in @xmath57 contributes a linear constraint .    for the range of parameters we consider in section  [ sec : results ] , we use @xmath68 .",
    "in our experiments to solve the sdp we use sedumi 1.1 @xcite or dsdp5 @xcite with yalmip @xcite as the parser .",
    "matlab code to implement the above algorithm can be found at http://math.stanford.edu/~yuany / metricphylo / matlab/.    we found that although sedumi often runs into numerical issues , it generally finds a good matrix @xmath44 with competitive performance to the neighbor - joining algorithm .",
    "dsdp is better in dealing with numerical stability at the cost of more computational time .",
    "we have found that setting @xmath69 and @xmath70 gives good results in our situation .",
    "for example , in the case of the jc69 model with a @xmath71 semidefinite matrix @xmath44 , yalmip - sedumi takes 55.7 minutes to parse the constraints and solve the sdp , while yalmip - dsdp takes 167.1 minutes to finish the same job . for details on experiments ,",
    "see the next section .",
    "our algorithm was inspired by some early results on metric learning algorithms such as @xcite and @xcite , which aim to find a ( pseudo)-metric such that the mutual distances between similar examples are minimized while the distances across dissimilar examples or classes are kept large .",
    "direct application of such an algorithm is not quite suitable in our setting .",
    "as shown in figure  [ fig : hist ] , the correct tree points are overlapped by the two incorrect trees . for points in the overlapping region , it is hard to tell whether to shrink or stretch their mutual distance .",
    "however , when the points appear in triples , it is possible that for each triple the one closest to zero is generated from the correct tree .",
    "our algorithm is based on such an intuition and proved successful in experiments .    after using algorithm  [ alg : meta ] to find a good metric @xmath44 ,",
    "the following simple algorithm allows us to construct trees on four taxa .",
    "[ alg:1 ] * ( tree construction with invariants ) *    _ input : _ a multiple alignment of 4 species and a semidefinite matrix @xmath44 from algorithm  [ alg : meta ] .",
    "_ output : _ a phylogenetic tree on the 4 species ( without branch lengths ) .",
    "_ procedure : _    1 .",
    "form empirical distributions @xmath32 by counting columns of the alignment .",
    "2 .   form the vectors @xmath72 for @xmath73 .",
    "3 .   return @xmath34 where the vector @xmath41 has smallest @xmath44-norm @xmath74 .",
    "we tested our metric learning algorithms for the invariants @xmath75 and @xmath76 as described above .",
    "we trained two metrics for jc69 on the tree in figure  [ fig : param ] .",
    "the first used simulations from branch lengths between @xmath26 and @xmath27 on a grid with increments of magnitude @xmath28 , for a total of @xmath77 different parameters .",
    "this region of parameter space was chosen for direct comparison with @xcite .",
    "the second metric used parameters @xmath78 and @xmath79 with increments of @xmath26 , giving @xmath80 trees with very short interior edges .",
    "similarly for k81 , we learned a metric using parameters @xmath81 and several sets of @xmath29 .",
    "after learning metrics , we performed simulation tests on the same space of parameters that was used to train .",
    "we compared neighbor - joining @xcite , phylogenetic invariants with the @xmath0 or @xmath82 norm and phylogenetic invariants with our learned norms .",
    "since the edge lengths are large for part of the parameter space , we often see simulated alignments with more than 75% mismatches between pairs of taxa .",
    "in such a case , ( [ eq : jcdist ] ) returns infinite distance estimates under the jukes - cantor model .",
    "so the results depend on how neighbor joining treats infinite distances . in phylip @xcite ,",
    "the program ` dnadist ` does nt return a distance matrix if some distances are infinite .",
    "however , in paup * , infinite distances are set to a fixed large number .",
    "since we are only concerned with the tree topology , we believe that the most fair comparison is between phylogenetic invariants and the method of paup*. however , it should be noted that this can make a major difference in results using neighbor joining , since often the correct tree can be returned even if some distances are infinite .",
    "see figure  [ fig : contour ] for an example of the difference and be warned that comparison between simulations studies done in different ways is difficult .",
    "table  [ tab : jc ] shows the results of 100 simulations at each of the 1444 parameter values for various sequence lengths using the jc69 model .",
    "it gives the percent correct over all 144,400 trials for five different methods : invariants with @xmath0 , @xmath82 , and @xmath44-norms and neighbor joining ( using jukes - cantor distances and allowing infinite distances ) .",
    "the contour plots in figure  [ fig : contour ] show how the reconstruction rates vary across parameter space for the five methods for a sequence length of 100 .",
    "notice that the @xmath44-norm shows particularly good behavior over the entire range of parameters , even in the `` felsenstein zone '' in the upper left corner . when trained on the felsenstein zone",
    ", the learned metric can perform even better .",
    "table  [ tab : jc ] shows the result of training a metric on this zone .",
    "notice that the @xmath44-norm is now quite a bit better than neighbor joining , even though the @xmath0 and @xmath82 norms are terrible . however , this learned norm is slightly worse on the whole parameter space than the metric trained on the whole space .",
    "cccccl@ccccc & & + length & @xmath0 & @xmath82 & @xmath44-norm & nj & & length & @xmath0 & @xmath82 & @xmath44-norm & nj + 25 & 62.7 & 59.8 & 74.5 & 75.5 & & 25 & 31.8 & 31.3 & 58.9 & 52.5 + 50 & 71.9 & 66.3 & 85.0 & 85.9 & & 50 & 35.9 & 33.0 & 69.9 & 63.2 + 75 & 76.7 & 69.6 & 90.0 & 90.4 & & 75 & 39.2 & 34.4 & 76.5 & 69.5 + 100 & 79.8 & 72.0 & 92.7 & 92.9 & & 100 & 42.2 & 35.8 & 81.2 & 73.9 + 200 & 86.4 & 77.6 & 97.0 & 96.6 & & 200 & 50.9 & 39.1 & 90.1 & 83.5 + 300 & 89.2 & 80.1 & 98.2 & 97.7 & & 300 & 55.4 & 40.4 & 93.6 & 87.8 + 400 & 91.1 & 82.1 & 98.7 & 98.2 & & 400 & 59.5 & 41.4 & 95.1 & 90.2 + 500 & 92.3 & 83.5 & 99.0 & 98.4 & & 500 & 62.4 & 42.5 & 95.6 & 91.4 +    table  [ tab : k81 ] shows results for the k81 model under two choices of @xmath83 .",
    "we only report the @xmath82 scores , since the @xmath0 scores are similar .",
    "of note is the column `` @xmath82 restrict '' which shows the @xmath82 norm on the top @xmath84 invariants as ranked by individual power on simulations as in the previous section .",
    "this column is better than the @xmath82 norm on all @xmath85 invariants , showing that many invariants are actually harmful .",
    "the @xmath44-norm again improves on even the restricted @xmath82 and beats neighbor - joining ( run with k81 distances ) on all examples .",
    "cccccl@ccccc & & + length & @xmath82 & @xmath82 restrict & a & nj & & length & @xmath82 & @xmath82 restrict & a & nj + 25 & 59.2 & 66.9 & 71.5 & 62.7 & & 25 & 63.7 & 67.4 & 70.9 & 65.0 + 50 & 68.0 & 77.5 & 82.1 & 72.7 & & 50 & 71.6 & 77.5 & 81.1 & 74.2 + 75 & 73.4 & 82.7 & 86.9 & 79.3 & & 75 & 75.4 & 82.7 & 86.4 & 80.5 + 100 & 76.8 & 85.8 & 89.7 & 82.6 & & 100 & 77.6 & 85.8 & 89.3 & 83.4 + 200 & 84.6 & 90.8 & 94.3 & 90.1 & & 200 & 83.3 & 91.3 & 94.1 & 89.7 + 300 & 88.2 & 92.6 & 93.1 & 95.6 & & 300 & 86.4 & 93.2 & 95.7 & 91.8 + 400 & 90.1 & 93.6 & 96.4 & 94.8 & & 400 & 88.5 & 94.3 & 96.5 & 93.0 + 500 & 91.4 & 94.2 & 96.9 & 95.7 & & 500 & 90.0 & 93.7 & 96.3 & 93.2 +    [ cols=\"^,^ \" , ]",
    "we have shown that machine learning algorithms can substantially improve the tree construction performance of phylogenetic invariants .",
    "as an example , for sequences of length 100 , the four - point invariant ( example  [ ex:4pt ] ) for the k81 model is correct 82% of the time on data simulated from k81 with parameters @xmath86 .",
    "this is quite a bit better than the @xmath82 norm on all 11612 invariants ( 76.8% , table  [ tab : k81 ] ) .",
    "the paper @xcite describes an algebraic method for picking a subset of invariants for the k81 model .",
    "they reduce to 48 invariants which give an improvement over all 11612 invariants ( up to 82.6% on the above example using the @xmath82 norm ) . however , of these 48 , only 4 of them are among the top 52 we selected for @xmath76 , and the remaining 44 invariants are mostly quite poor ( 42% average accuracy ) . after taking the closure of these 48 invariants , there are 156 total and the performance actually drops to 78.3% .",
    "it seems that the conditions for an invariant to be powerful are not particularly related to the algebraic criterion used in @xcite .",
    "all invariant based methods heavily depend on the set of invariants that we begin with .",
    "learning diagonal matrices @xmath44 had mixed performance , which further suggests that the generating set we are using for the invariants is non - optimal .",
    "we believe that it is an important mathematical problem to understand what properties are shared by the good invariants .",
    "we suggest that symmetry might be an important criterion to construct other polynomials like the four - point condition with good power .",
    "the learned metrics in this paper are somewhat dependant on the parameters chosen to train them .",
    "this can be a benefit , as it allows us to train tree construction algorithms for specific regions of parameter space ( e.g. , the felsenstein zone ) .",
    "however , we hope that improvements to the metric programming will allow us to train on larger parameter sets and thus obtain uniformly better algorithms .",
    "notice that these methods only recover the tree topology , not the edge lengths .",
    "we believe that if the edge lengths are needed , they should be estimated after building the tree , in which case standard statistical methods such as maximum likelihood can be used easily .",
    "while the invariants discussed in this paper may not be practical for large trees , we believe there is great use in understanding fully the problem of building trees on four taxa .",
    "for example , these methods can either be used as an input to quartet - based tree construction algorithms or as a verification step for larger phylogenetic trees .    for a method of building trees on more than four taxa using phylogenetic invariants ,",
    "see @xcite , which use numerical linear algebra to evaluate invariants given by rank conditions on certain matrices in order to construct phylogenetic trees .",
    "this amounts to evaluating many polynomials at once , allowing it to run in polynomial time .",
    "the matrices @xmath44 for jc69 and k81 used in the tests can be found at http://stanford.edu/~nke/data/metricphylo .",
    "a software package that can run these tests is available at the same website .",
    "it includes a program for simulating evolution using any markov model on a tree and several programs using phylogenetic invariants .",
    "n.  eriksson was supported by nsf grant dms-0603448 and wishes to thank msri and the i m a for their hospitality .",
    "y.  yao was supported by darpa grant 1092228 .",
    "we wish to thank e.  allman , m.  drton , d.  ge , f.  memoli , l.  pachter , j.rhodes , and y.  ye for helpful comments and especially g.  carlsson for his encouragement and computational resources .",
    "steven  j. benson and yinyu ye .",
    ": software for semidefinite programming .",
    "technical report anl / mcs - p1289 - 0905 , mathematics and computer science division , argonne national laboratory , argonne , il , september 2005 . submitted to acm transactions on mathematical software .",
    "nicholas eriksson .",
    "tree construction using singular value decompsition . in l.  pachter and b.  sturmfels , editors , _ algebraic statistics for computational biology _",
    ", chapter  19 , pages 347358 .",
    "cambridge university press , cambridge , uk , 2005 ."
  ],
  "abstract_text": [
    "<S> we introduce new methods for phylogenetic tree quartet construction by using machine learning to optimize the power of phylogenetic invariants . </S>",
    "<S> phylogenetic invariants are polynomials in the joint probabilities which vanish under a model of evolution on a phylogenetic tree . </S>",
    "<S> we give algorithms for selecting a good set of invariants and for learning a metric on this set of invariants which optimally distinguishes the different models . </S>",
    "<S> our learning algorithms involve linear and semidefinite programming on data simulated over a wide range of parameters . </S>",
    "<S> we provide extensive tests of the learned metrics on simulated data from phylogenetic trees with four leaves under the jukes - cantor and kimura 3-parameter models of dna evolution . </S>",
    "<S> our method greatly improves on other uses of invariants and is competitive with or better than neighbor - joining . </S>",
    "<S> in particular , we obtain metrics trained on trees with short internal branches which perform much better than neighbor joining on this region of parameter space .    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ phylogenetic invariants , algebraic statistics , semidefinite programming , felsenstein zone . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ </S>"
  ]
}