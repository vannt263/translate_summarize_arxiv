{
  "article_text": [
    "assume we want to sample from a sequence of probability distributions @xmath0 defined on a common measurable space @xmath1 where @xmath2 or @xmath3 . as a special case",
    ", one can set @xmath4 for all @xmath5 .",
    "alternatively the distribution can vary across @xmath6 . similarly to simulated annealing , one could be interested in the sequence of distributions @xmath7 for an increasing schedule @xmath8 so as to maximize @xmath9 or @xmath10 could be the posterior distribution of a parameter given the data collected till time @xmath11 . in this paper , we are interested in sampling this sequence of distributions _ sequentially _ ; that is first sampling from @xmath12 then @xmath13 and so on",
    ". we will further on refer to @xmath11 as the time index .",
    "the tools favoured by statisticians to achieve this are markov chain monte carlo ( mcmc ) methods ; see for example ( robert and casella , 1999 ) . to sample from @xmath10 , mcmc  methods",
    "consists of building an ergodic markov kernel @xmath14 with invariant distribution @xmath10 using metropolis - hastings ( mh ) steps , gibbs steps etc .",
    "mcmc have been successfully used in many applications in statistics and physics .",
    "when the distribution to sample is multimodal , mcmc  samplers can be easily stucked in one mode . a standard approach to improve mixing consists of using interacting parallel mcmc / tempering mechanisms where one runs a mcmc  chain on an extended space @xmath15 with a specified joint  invariant distribution admitting @xmath10 as a marginal ( geyer and thompson , 1995 ) .",
    "however , mcmc are not well adapted to sequential simulation . at index @xmath11",
    ", one needs to wait for the markov chain with kernel @xmath14 to reach its stationary distribution @xmath10 .",
    "we propose here a different approach to sample from @xmath16 .",
    "our approach is based on sequential monte carlo ( smc ) methods ( doucet _ et al .",
    "_ , 2001 ; liu , 2001 )",
    ". henceforth the resulting algorithms will be called smc  samplers .",
    "smc methods have been recently studied and used extensively in the context of sequential bayesian inference and physics ( doucet _ et al . _ , 2001 ; iba , 2001 ; liu , 2001 ) . at a given time @xmath11 ,",
    "the basic idea is to obtain a large collection of @xmath17 ( @xmath18 ) random samples @xmath19 named particles whose marginal distribution is asymptotically ( @xmath20 ) equal to @xmath10 .",
    "these particles are carried forward over time using a combination of sequential importance sampling ( sis ) and resampling ideas .",
    "this approach is very different from parallel mcmc  algorithms where one builds a markov kernel with a specified joint invariant distribution on @xmath15 .",
    "standard smc algorithms available in the literature do not apply to our problem .",
    "indeed , these algorithms deal with the case where each target distribution of interest @xmath10 is defined on @xmath21 with @xmath22 in ( chopin , 2002 ) , a smc  algorithm is proposed to deal with the case @xmath23 .",
    "however , this approach restricts severely the way particles can explore the space .",
    "the idea in this paper is different and consists of building an artificial sequence of distributions @xmath24 defined on @xmath25 with @xmath26 admitting a marginal @xmath10 .",
    "we are then back to the standard smc  framework .",
    "more precisely , @xmath26 is defined on @xmath27 by @xmath28 and @xmath29 where @xmath30 is a sequence of auxiliary markov transition kernels .",
    "our approach has some connections with annealed importance sampling ( ais ) ( neal , 2001 ) and the algorithm recently proposed in ( capp _ et al . _ , 2002 ) which are detailed in section 2 .",
    "however , the framework we present here is more general and allows to derive a whole class of principled integration and genetic - type optimization algorithms based on interacting particle systems . similarly to mcmc , the efficiency of the algorithms is dependent on the target distributions , the proposal and auxiliary kernels .",
    "nevertheless , generally speaking , one can expect that smc  samplers will outperform mcmc when the distributions to sample are multimodal with well - separated modes .",
    "moreover smc  samplers can be used for sequential bayesian inference problems with static parameters like those addressed by chopin ( 2002 ) .",
    "this paper focuses on the algorithmic aspects of smc  samplers .",
    "however , it is worth noting that our algorithms can be interpreted as an interacting particle approximating model of a nonlinear feynman - kac flow in distribution space . under additional assumptions ,",
    "we provide a nonlinear markov interpretation of the measure - valued dynamic system associated to the flow @xmath31 .",
    "we show that this interpretation is a natural nonlinear version of the mh algorithm .",
    "many convergence results are available for feynman - kac flows and their interacting particle approximations ( del moral and miclo , 2000 ; del moral and miclo , 2001 ) and , consequently , for smc  samplers .",
    "however , the feynman - kac flow associated to smc  samplers is such that many known estimates on the asymptotic behaviour of these interacting processes can be greatly improved .",
    "several of these results can be found in ( del moral and doucet , 2003 ) .",
    "the rest of the paper is organized as follows . in section 2",
    ", we review a generic smc algorithm to sample from the sequence of distributions ( [ eq : sequencedistributions ] ) .",
    "various settings for this algorithm are presented , some extensions and the connections with previous work are outlined .",
    "section 3 describes the distribution flow associated to our interacting particle approximating model and also presents an original nonlinear markovian interpretation of this flow .",
    "section 4 applies this class of algorithms to a nonlinear regression problem .",
    "finally , we discuss briefly a few open methodological and theoretical problems in section 5 .",
    "we describe here a generic smc  algorithm to sample from the sequence of distributions @xmath0 defined in ( [ eq : sequencedistributions ] ) based on a sampling importance resampling strategy ; see ( doucet _ et al .",
    "_ , 2001 ) for a booklength survey of the smc  literature .",
    "alternative smc algorithms such as the auxiliary particle method of pitt and shephard ( 1999 ) could also be used .",
    "further on we will use the notation @xmath32 to denote @xmath33 . at time",
    "@xmath34 , assume a set of particles @xmath35 ( @xmath36 ) distributed approximately according to @xmath37 is available , i.e. the empirical measure @xmath38 is an approximation of @xmath39 at time @xmath11 , we extend the path of each particle according to a markov kernel @xmath40 .",
    "the resulting path is thus approximately distributed according to @xmath41 .",
    "importance sampling can then be used to correct for the discrepancy between the sampling distribution and @xmath42 , with the importance weight satisfying @xmath43 and being assumed well - defined .  finally , the particles are resampled according to their importance weights ; particles with low weights are discarded whereas particles with high weights are multiplied .",
    "the resampled particles are given an equal weight .",
    "to sum up , the algorithm proceeds as follows .    * sequential monte carlo sampler *    * initialization ; * @xmath44 .    * @xmath45@xmath46 * @xmath45@xmath47@xmath48    * @xmath49@xmath50 @xmath17@xmath51 .",
    "* iteration * @xmath52 @xmath53 .",
    "* @xmath45@xmath54 @xmath55 * @xmath45@xmath56@xmath57    * @xmath58@xmath59 @xmath17@xmath60 .    in this algorithm , @xmath61 is the initial importance distribution .",
    "the resampling step can be done using a standard procedure such as multinomial resampling ( gordon _ et al .",
    "_ , 1993 ) , stratified resampling ( kitagawa , 1996 ) or minimum entropy resampling ( crisan , 2001 ) .",
    "all these resampling schemes are unbiased ; that is the number of times @xmath62 the particle @xmath63 is copied satisfies @xmath64 . mcmc  steps with invariant distribution @xmath26",
    "can also be included after the resampling step ( gilks and berzuini , 1999 ) .",
    "the complexity of this algorithm is in @xmath65 and it can be parallelized easily . in practice ,",
    "the memory requirements are in @xmath66 too and do not increase over time as one does not need to keep in memory at time @xmath11 the whole paths @xmath67 but only @xmath68 .",
    "the algorithm can be interpreted as an adaptive importance sampling strategy .",
    "initially , @xmath61 is used and the particles with the highest importance weights are multiplied whereas the ones with small weights are discarded . at time @xmath11 , new `` candidate '' particles are sampled according to a proposal distribution kernel @xmath69 .",
    "if @xmath69 is a random walk , then the new particles can be interpreted as a local exploration of the distribution .",
    "the crucial point is that these candidates are weighted by ( [ eq : importanceweight ] ) so as to ensure that after the resampling step their distribution is approximately @xmath26 .",
    "the introduction of the auxiliary kernel @xmath70 allows the use of importance sampling without having to compute the marginal distribution @xmath71 of the particles @xmath72 .",
    "indeed , this marginal importance distribution does not typically admit an analytical expression except when @xmath69 is a mcmc  kernel of invariant distribution @xmath73 .",
    "a similar idea is the basis of the conditional monte carlo method described by hammersley ( 1956 ) .      at time @xmath11 , we have the following empirical approximations of @xmath10 before the resampling step @xmath74 and after the resampling step it is equal to @xmath75 for any measure @xmath76 and function @xmath77 , we will denote @xmath78 .",
    "an estimate of @xmath79 is given by @xmath80 or alternatively @xmath81 which has higher variance .",
    "if @xmath4 , then the following estimate can be also used @xmath82 though the particles are statistically dependent , one can show under assumptions given in ( del moral and miclo , 2000 ) that this estimate is consistent as @xmath83 .    the algorithm described above",
    "can also be used to compute the ratio of normalizing constants .",
    "indeed , typically the sequence of distributions @xmath84 is only known up to a normalizing constant , i.e. say @xmath85 . in this case , the unnormalized importance weights one computes are equal to @xmath86 at time @xmath11 .",
    "it is possible to obtain an estimate of the ratio of the normalizing constants @xmath87 using @xmath88 thus an estimate of @xmath89 is given by @xmath90 if the resampling scheme used is unbiased , then ( [ eq : rationormestimate ] ) is also unbiased ( del moral and miclo , 2000 ) .",
    "the algorithm presented in the previous subsection is very general .",
    "there are many potential choices for @xmath91 leading to various integration and optimization algorithms .",
    "_ homogeneous sequences . _",
    "a simple choice consists of setting @xmath92 @xmath93 and @xmath94 . in this case , the importance weight ( [ eq : importanceweight ] ) is the following generalized mh ratio @xmath95 the standard mh ratio corresponds to @xmath96 . in this simple case ,",
    "the particles evolve independently according to a proposal distribution @xmath97 , their generalized mh  ratio is computed and normalized .",
    "the particles are then multiplied or discarded with respect to the value of their normalized mh ratio .    _ sequence of distributions _",
    "it might be of interest to consider non homogeneous sequence of distributions either to move `` smoothly '' from @xmath98 to a target distribution @xmath9 through a sequence of intermediate distributions or for the sake of optimization . in the case of integration as suggested by neal ( 2001 ) , one can select @xmath99 with @xmath100,@xmath101 and @xmath102 . for the case of optimization , one can select @xmath103 where @xmath3 , @xmath104 is an increasing sequence such that @xmath105 . in this case",
    ", the resulting algorithm is a genetic algorithm where the sampling step is the `` mutation '' step and the resampling step is the selection step ( goldberg , 1989 ) .",
    "however , there is a significant difference with standard genetic algorithms as we know the asymptotic ( @xmath83 ) distribution  of the particles .",
    "this makes the analysis of the resulting algorithm easier than in cases where this distribution is unknown such as in ( del moral and miclo , 2003 ) .",
    "convergence properties of the algorithm are currently under study .",
    "finally , another application of this algorithm consists of estimating the sequence of posterior distributions @xmath106 where @xmath107 is an observation available at time @xmath11 . as briefly discussed in the introduction , smc  algorithms",
    "have been recently proposed in this framework by chopin ( 2002 ) but the  sis framework used is somehow restricted : it only allows @xmath69 to be a mcmc  kernel of invariant distribution @xmath73 .    _",
    "sequence of proposal kernels _",
    "@xmath69 _ and _ _ auxiliary kernels _ @xmath70 . any couple of kernels can be used as long as the ratio ( [ eq : ratiometropolis ] ) is well defined .",
    "however , one can only expect good properties of the algorithm if this ratio admits a reasonable variance and also if @xmath70 is mixing .",
    "indeed , loosely speaking , the faster @xmath70 mixes , the faster the smc algorithm forgets monte carlo errors ( del moral and doucet , 2003 ) .    in smc  algorithms ( doucet _",
    "et al . _ , 2001 ) , it is known that the importance sampling distribution minimizing the conditional variance of the weights at time @xmath11 , i.e. @xmath108 fixed , is given by @xmath109 in this case , the importance weight @xmath110 is given by @xmath111 and is independent of @xmath112 .",
    "this allows the resampling step to be performed before the sampling step .    in standard applications of smc algorithms",
    ", the kernel @xmath70 is usually given by the problem at hand whereas in our setup this kernel is arbitrary and can be optimized for a given proposal distribution @xmath69 .",
    "one can alternatively select the kernel @xmath70 so as to be able to compute ( [ eq : optimalweight ] ) ; e.g. a mcmc  kernel of invariant distribution @xmath10 , and then sample the particles according to ( [ eq : optimalsampling ] ) .    for a fixed @xmath69 , an alternative natural choice consists of choosing @xmath113 in this case , the associated importance weight @xmath114 is given by @xmath115",
    "if @xmath69 is a mcmc kernel of invariant distribution @xmath73 , then the weight ( [ eq : optimalweight2 ] ) can be computed easily .",
    "if not , numerical integration using the current set of particles can be used to approximate it but the resulting algorithms would be of complexity @xmath116 .",
    "_ connections to previous work .",
    "_ ais is a method proposed recently by neal ( 2001 ) . reversing the time index in ( neal , 2001 ) to be consistent with our notation",
    ", ais corresponds to the case where one considers a finite sequence of distributions , @xmath69 is a mcmc  kernel of invariant distribution @xmath10 and @xmath117 for a given @xmath69 , one can check that this choice of @xmath70 ensures that ( [ eq : optimalsampling ] )  is satisfied . in this case , one obtains by combining ( [ eq : ratiometropolis ] )  and ( [ eq : nealchoice ] ) @xmath118 the resampling step is not included in the ais  algorithm . in our framework , we point out that this is a crucial step to include to make the method efficient as established theoretically in ( del  moral and miclo , 2000 ) and practically in our simulations .",
    "otherwise the method is just a special instance of sis and collapses  if @xmath11 is too large . in ( godsill and clapp , 2001 ) , the authors used the ais  algorithm in combination with resampling in the context of optimal filtering .",
    "a more recent work ( capp _ et al . _ , 2002 ) contemporary of ( del moral and doucet , 2003 ) and developed independently is another special case of our framework . in ( capp _",
    "et al . _ , 2002 ) , the authors consider the homogeneous case .",
    "their algorithm corresponds to the case where @xmath97 is an mcmc kernel of invariant distribution @xmath9 ( namely a gibbs sampler ) and @xmath119 , it follows that @xmath120 this particular case has limited applications as @xmath121 would not be defined in most applications ; e.g. @xmath122 and @xmath97 is an mh kernel .",
    "_ extensions .",
    "_ the algorithm described  in this section must be interpreted as the basic element of more complex algorithms .",
    "it is what the mh  algorithm is to mcmc . for complex mcmc  problems ,",
    "one typically uses a combination of mh steps where the @xmath123 components of @xmath124 say @xmath125 are updated by subblocks ( robert and casella , 1999 ) .",
    "similarly , to sample from high dimensional distributions , a practical smc  sampler can update the components of @xmath124 via subblocks .",
    "there are also numerous potential extensions :    * it is straightforward to develop a version of the algorithm so as to sample distributions defined on an union of subspaces of different dimensions .",
    "however , contrary to reversible jump mcmc algorithms ( green , 1995 ) , no reversibility condition is needed .",
    "* as suggested in ( crisan and doucet , 2000 ) , one can use a proposal kernel whose parameters are a function of the whole set of current particles .",
    "this allows the algorithm to automatically scale the proposal distribution based on the previous importance weights . * in the general case ,",
    "the sequence of probability distributions @xmath0 of interest is such that @xmath10 is defined on @xmath21 and not on @xmath1 .",
    "we can generalize the algorithm described in this section to this case .",
    "we introduce an auxiliary kernel @xmath70 from @xmath21 to @xmath126 and a proposal kernel @xmath69 from @xmath126 to @xmath21 . at time @xmath34 ,",
    "@xmath17 particles @xmath127 approximately distributed according to @xmath73 are available . at time",
    "@xmath17 new particles @xmath128 are sampled according to @xmath129 and the following importance weights are computed @xmath130 then the particles are resampled .",
    "in this section , we show that the algorithm presented in section [ sec : smcalgo ] corresponds to an interacting particle approximation model of a nonlinear feynman - kac flow in distribution space . we provide an alternative nonlinear markovian representation of this flow and its interacting particle approximation . here",
    "the feynman - kac flow corresponds to the special case where the so - called potential function is given by the generalized metropolis ratio ( [ eq : ratiometropolis ] ) .",
    "the abstract description and the analysis of general feynman - kac flows and their particle approximations have been investigated in several recent research articles .",
    "many asymptotic ( @xmath131 and/or @xmath83 ) results are available in this field including empirical process convergence , central limit theorems , large deviation principles as well as increasing propagation of chaos estimates and uniform convergence estimates with respect to the time parameter ; all of which can be used for smc  samplers .",
    "the interested reader is referred to the survey article ( del moral and miclo , 2000 ) and the more recent studies ( del moral and miclo , 2001 ; del moral and miclo , 2003 ) . as mentioned in the introduction ,",
    "the particular choice of the potential function ( [ eq : ratiometropolis ] ) simplifies the analysis and many known estimates on the asymptotic behaviour of these interacting processes can be greatly improved .",
    "several of these results can be found in ( del moral and doucet , 2003 ) .",
    "define the following distributions on @xmath132 @xmath133 using ( [ eq : sequencedistributions ] ) and ( [ eq : importanceweight ] ) , it is clear that the sequence of distributions @xmath134 ( with the convention @xmath135 ) admits the following so - called feynman - kac representation @xmath136 with @xmath137 where @xmath138 denotes the expectation with respect to @xmath139 this representation is at the core of the results given in ( del moral and miclo , 2000 ) .",
    "we give now two `` operator - like '' interpretations of the sequence @xmath140 . for a measure @xmath76 and a markov kernel @xmath141",
    ", we use the standard notation @xmath142 let @xmath143 be the set of probability measures on @xmath144 .",
    "the mapping @xmath145 is defined as @xmath146 where @xmath147 and @xmath148 is a markov kernel on @xmath144 defined as @xmath149 assuming that @xmath150 can be upper bounded over @xmath144 , one can easily check that an alternative representation is given by @xmath151 where @xmath152 @xmath153 being chosen such that @xmath154 over @xmath144 .",
    "the kernel @xmath155 is a so - called nonlinear markov kernel ; i.e. the transition kernel is dependent not only on the current state but also on its distribution .",
    "a  generic nonlinear markov chain @xmath156 satisfies @xmath157 it is typically impossible to simulate a realization from such a markov chain as the distribution of the state is not available .",
    "however , a particle approximation of it can be used . consider a markov chain",
    "@xmath158 taking values in @xmath144 with transition kernel @xmath155 .",
    "this kernel can be interpreted as follows .",
    "given @xmath159 , one first sample a candidate @xmath160 where @xmath161 .",
    "with probability @xmath162 , one sets @xmath163 , otherwise @xmath164 . by construction , one has @xmath165 .",
    "this algorithm can be interpreted as a nonlinear version of the mh algorithm .",
    "the main difference being that , when a candidate is rejected , the chain does not stay where it is a new state is proposed according to @xmath166 .",
    "the first particle interpretation of the flow follows ( [ eq : standardrep0])-([eq : standardrep ] ) .",
    "it corresponds to the standard algorithm which has been described in section [ sec : smcalgo ] .",
    "the second alternative algorithm corresponds to a particle interpretation of the flow corresponding to ( [ eq : nonlinearmarkovchain ] ) .",
    "it proceeds as follows .",
    "* iteration * @xmath52 @xmath53 .",
    "* @xmath45@xmath167 @xmath168 * @xmath45@xmath56@xmath57    * @xmath169 * @xmath45@xmath170@xmath171 @xmath172 * @xmath58@xmath59 @xmath173 .",
    "we consider the following harmonic regression model ( andrieu and doucet , 1999 ) @xmath174 where @xmath175 , @xmath176 @xmath177 , @xmath178 and @xmath179 is a @xmath180 matrix where for @xmath181 . @xmath182   _ { i+1,2j-1}=\\cos\\left (   \\omega _ { j}i\\right )   , \\text { } \\left [   d\\left (   \\omega\\right )   \\right ]   _ { i+1,2j}=\\sin\\left (   \\omega_{j}i\\right )   .\\ ] ] we assume that @xmath183 and we use the following prior @xmath184 with @xmath185 where @xmath186 ( @xmath187 ) , @xmath188 ; @xmath189 is uniform on @xmath190 . the posterior density satisfies on @xmath191 @xmath192 with @xmath193{l}$m^{-1}=\\left (   1+\\delta^{-2}\\right )   d^{\\mathtt{t}}\\left (   \\omega\\right ) d\\left (   \\omega\\right )   , $ \\\\ $ m = md^{\\mathtt{t}}\\left (   \\omega\\right )   y,$\\\\ $ p = i_{p}-d\\left (   \\omega\\right )   md^{\\mathtt{t}}\\left (   \\omega\\right )   .$\\end{tabular}\\ ] ]",
    "we simulate a realization of @xmath194 observations with @xmath195 , @xmath196 , @xmath197 the posterior density is multimodal with well - separated modes .      to sample from @xmath198 we use an homogeneous smc  sampler with @xmath199 particles where",
    "the @xmath200 components are updated one - at - a - time using a simple gaussian random walk proposal @xmath97 of standard deviation @xmath201 .",
    "we select @xmath202 to be equal to @xmath97 and use the stratified resampling procedure .",
    "we compare our algorithm with a mcmc  algorithm .",
    "the mcmc  algorithm updates the component one - at - a - time using a mh  step with the proposal kernel @xmath97 . in both case",
    ", the initial distribution is the uniform distribution on @xmath203    we consider the case where @xmath204 .",
    "obviously one could come up with a better proposal kernel .",
    "we want to emphasize here that the smc  approach is more robust to a poor scaling of the proposal .",
    "a similar remark was made in ( capp _ et al .",
    "_ , 2002 ) .",
    "in figure 1 , we present the marginal posterior distributions of @xmath205 and @xmath206 obtained using the smc  sampler with @xmath207 iterations .",
    "we then run @xmath208 iterations of the mcmc  algorithm so as the computational complexity to be roughly the same for the two algorithms .",
    "the mcmc  algorithm is more sensitive to the initialization . on @xmath209 realizations of the smc  and the mcmc  algorithm",
    ", the smc always explores the main mode whereas the mcmc  algorithm converges towards it only @xmath210 times .",
    "we also use an inhomogeneous version of the smc  sampler so as to optimize @xmath211 . in this case the target density at time @xmath11 is @xmath212 with @xmath213 and we use @xmath209 iterations .",
    "we compare this algorithm to a simulated annealing version of the mh algorithm with @xmath214 iterations with @xmath215 . in table 1 , we display the mean and standard deviations of the log - posterior density of the posterior mode estimate ; the posterior mode estimate being chosen as the sample generated during the simulation maximizing the posterior density .",
    "contrary to the simulated annealing algorithm , the smc algorithm converges consistently towards the same mode .",
    "[ c]lllalgorithm & smc & mcmc + mean of the log - posterior values & -326.12 & -328.87 + standard deviation of the & 0.12 & 1.48 + log - posterior values & & +    table 1 : performance of smc  and mcmc  algorithm obtained over 50 simulations",
    "in this article , we have presented a class of methods to sample from distributions known up to a normalizing constant .",
    "these methods are based on smc algorithms .",
    "this framework is very general and flexible .",
    "several points not discussed here are detailed in ( del moral and doucet , 2003 ) .",
    "* in the homogeneous case , assume that we do not initialize the algorithm in the stationary regime , i.e. we do not correct for the discrepancy between @xmath61 and @xmath9 .",
    "this has to be paralleled with mcmc algorithms which are not initialized in the stationary regime . under regularity assumptions",
    ", it can be shown that the distribution flow still converges towards the target distribution @xmath9 .",
    "moreover , it converges at a rate only dependent on the mixing properties of @xmath202 .",
    "this is in contrast with the mh  algorithm whose rate of convergence is dependent on @xmath9 and @xmath97 . *",
    "the algorithm we have presented can be used to simulate a markov chain with a fixed terminal point .",
    "indeed , one obtains at time @xmath11 samples from ( [ eq : sequencedistributions ] ) . by setting @xmath216 and reversing the time index ,",
    "one obtains an approximate realization of a markov process of initial distribution @xmath10 at time @xmath217 , transition @xmath218 and terminal point @xmath124 at time @xmath219 .",
    "this has applications in genetics and physics .",
    "there are also several important open methodological and theoretical problems to study .    *",
    "similarly to mcmc  methods , one needs to carefully design the various components of the algorithm to get good performance .",
    "in particular , it would be of interest to come up with an automated choice for @xmath70 given @xmath69 . for the homogeneous case",
    ", one could look at minimizing the variance of ( [ eq : ergodicaverage ] ) .",
    "it involves a tradeoff between the mixing properties of @xmath202 and the variance of the importance weights ( [ eq : importanceweight ] ) .",
    "this point is currently under study .",
    "* it would be interesting to weaken the assumptions of the results in ( del moral and miclo , 2000 ; del moral and doucet , 2003 ) which mostly only hold for compact spaces .",
    "the authors are extremely grateful to manuel davy for his comments and some of the numerical simulations .  we are also grateful to christophe  andrieu and elena punskaya for their comments .              del moral , p. and doucet , a. ( 2003 ) on a class of genealogical and interacting metropolis models . in _",
    "sminaire de probabilits xxxvii _ , ed .",
    "j. azma and m. emery and m. ledoux and m. yor , _ lecture notes in mathematics _ , berlin : springer - verlag , 34pp . , to appear .",
    "del moral , p. and miclo , l. ( 2000 ) branching and interacting particle systems approximations of feynman - kac formulae with applications to non - linear filtering__. _ _ in _ sminaire de probabilits xxxiv _ , ed.azma , j. , emery , m. , ledoux , m. and yor , m. , _",
    "lecture notes in mathematics _ , berlin : springer - verlag , * 1729 * , 1 - 145 .",
    "godsill , s.j . and clapp , t. ( 2001 ) improvement strategies for monte carlo particle filters . in _",
    "sequential monte carlo methods in practice , _ ed .",
    "doucet , a. , de freitas , j.f.g  and gordon , n.j . , * *  * * 139 - 158 ."
  ],
  "abstract_text": [
    "<S> _ keywords : _ genetic algorithm , importance sampling , resampling , markov chain monte carlo , sequential monte carlo , simulated annealing . </S>"
  ]
}