{
  "article_text": [
    "the importance of data cleaning and quality technologies for business practices is well recognized .",
    "data cleaning has been an active research topic in several communities including statistics , machine learning and data management .",
    "the quality of data suffers from typing mistakes , lack of standards for recording database fields , integrity constraints that are not enforced , inconsistent data mappings , etc . for years",
    ", data quality technology has grown independently from core data management .",
    "data quality tools became part of extract transform load ( etl ) technologies , commonly applied during the initial loading phase of data into a warehouse .",
    "although this might be a viable approach for data analytics , where data processed are static , it is far from acceptable for operational databases .",
    "dynamic databases however , face proliferating quality problems , that degrade common business practices .",
    "recently , there has been a major focus on tighter integration of data quality technology with database technology .",
    "in particular there has been research work on the efficient realization of popular data cleaning algorithms inside database engines as well as studies for the efficient realization of data quality primitives in a declarative way .",
    "the approaches are complementary , the former assuring great performance and the latter ease of deployment and integration with existing applications without modification of the underlying database engine",
    ". we are concerned with declarative implementations of data quality primitives in this thesis .",
    "in particular we study declarative realizations of several _ similarity predicates _ for the popular approximate ( flexible ) selection operation for data de - duplication @xcite .",
    "a similarity predicate @xmath0 is a predicate that numerically quantifies the similarity or closeness of two ( string ) tuples . given a relation @xmath1 , the approximate selection operation using similarity predicate @xmath0 , will report all tuples @xmath2 such that @xmath3 , where @xmath4 a specified numerical similarity threshold and @xmath5 a query tuple .",
    "approximate selections are special cases of the approximate join ( record linkage , similarity join ) operation @xcite .",
    "several efficient declarative implementations of this operation for specific similarity predicates have been proposed @xcite both for approximate selections and joins .    in this thesis , we conduct a thorough study of declarative realizations of similarity predicates for approximate selections . we introduce and adapt novel predicates , realize them declaratively and compare them with existing ones for accuracy and performance . in particular we make the following contributions :    * inspired by the success of _ tf - idf cosine similarity _ from information retrieval @xcite as a similarity predicate for approximate selections , we introduce declarative realizations of other successful predicates from information retrieval and in particular the popular bm25 measure .",
    "* we introduce declarative realizations of probabilistic similarity predicates inspired by language models from information retrieval @xcite and hidden markov models @xcite , suitably adapted for the case of approximate selections .",
    "* we present declarative realizations of previously proposed similarity predicates for the approximate selection problem and we propose a categorization of all measures both previously proposed and new according to their characteristics . *",
    "we present a thorough experimental study comparing all similarity predicates for accuracy and performance , under various types of quality problems in the underlying data .",
    "data quality has been an active research topic for many years .",
    "a collection of statistical techniques have been introduced initially for the record linkage problem @xcite .",
    "the bulk of early work on data quality was geared towards correcting problems in census files @xcite .",
    "a number of similarity predicates were developed taking into account the specific application domain ( i.e. , census files ) for assessing closeness between person names ( e.g. , jaro , jaro - winkler @xcite , etc ) .",
    "the work of cohen @xcite introduced the use of primitives from information retrieval ( namely cosine similarity , utilizing tf - idf@xcite ) to identify flexible matches among database tuples . a performance / accuracy study conducted by cohen et al . , @xcite demonstrated that such techniques outperform common predicates introduced for specific domains ( e.g. , jaro , jaro - winkler , etc ) .",
    "other techniques geared towards database tuples include the merge / purge technique @xcite .",
    "several predicates to quantify approximate match between strings have been utilized for dealing with quality problems , including edit distance and its variants @xcite .",
    "hybrid predicates combining notions of edit distance and cosine similarity have also been introduced @xcite .",
    "recently , @xcite presented ssjoin , a primitive operator for efficient set similarity joins . utilizing ideas from @xcite",
    ", such an operator can be used for approximate matching based on a number of similarity functions , including hamming distance , edit - distance and jaccard similarity",
    ". however , the choice of the similarity predicate in this approach is limited @xcite .",
    "the bulk of the techniques and predicates however have been introduced without a declarative framework in mind .",
    "thus , integrating them with applications utilizing databases in order to enable approximate selections is not very easy .",
    "gravano et al .",
    "@xcite , introduced a declarative methodology for realizing approximate joins and selections for edit distance .",
    "subsequently a declarative framework for realizing tf - idf cosine similarity was introduced @xcite .",
    "there has been a great deal of research in the information retrieval literature on weighting schemes beyond cosine similarity with tf - idf weighting .",
    "recent ir research has shown bm25 to be the most effective among the known weighting schemes @xcite .",
    "this weighting scheme models the distribution of within - document term frequency , document length and query term frequency very accurately .",
    "moreover , in the information retrieval literature , language modeling has been a very active research topic as an alternate scheme to weight documents for their relevance to user queries .",
    "starting with ponte and croft @xcite language models for information retrieval have been widely studied .",
    "hidden markov models ( hmm ) have been very successful in machine learning and they have been utilized for a variety of learning tasks such as named entity recognition and voice recognition@xcite . they have also been utilized for information retrieval as well @xcite . an experimental study on trec data demonstrated that an extremely simple realization of hmm outperforms standard tf - idf for information retrieval @xcite .",
    "several researchers @xcite have tried to formally reason about the relative goodness of information retrieval weighting schemes .",
    "let @xmath6 be a query string and @xmath7 a string record from a base relation @xmath8 .",
    "we denote by @xmath9 , @xmath10 the set of _ tokens _ in @xmath6 and @xmath7 respectively .",
    "we refer to substrings of a string as tokens in a generic sense .",
    "such tokens can be words or q - grams ( sequence of @xmath11 consecutive characters of a string ) for example . for @xmath6=`db lab ' , @xmath9=@xmath12`db ' , ` lab'@xmath13 for word - based tokenization and @xmath9=@xmath12`db ' , `",
    "b l ' , ` la ' , ` lab'@xmath13 for tokenization using 3-grams .",
    "we refer to tokens throughout the thesis when referring to words or q - grams .",
    "we make the choice specific ( word or q - gram ) for techniques we present , when is absolutely required . in certain cases ,",
    "we may associate a _ weight _ with each token .",
    "several weighting mechanisms exist .",
    "we present our techniques referring to weights of tokens , making the choice of the weighting scheme concrete when required . in chapter",
    "[ experimental ] we realize our techniques for specific choice of tokens and specific weighting mechanisms .",
    "our goal is to calculate a _ similarity score _ between @xmath6 and @xmath7 using a similarity predicate .",
    "we group similarity predicates into five classes based on their characteristics , namely :    * * overlap predicates : * these are predicates that assess similarity based on the overlap of tokens in @xmath14 . *",
    "* aggregate weighted predicates : * predicates that assess similarity by manipulating weights ( scores ) assigned to elements of @xmath14 * * language modeling predicates : * predicates that are based on probabilistic models imposed on elements of @xmath14 * * edit based predicates : * predicates based on a set of edit operations applied between @xmath6 and @xmath7 . * * combination predicates : * predicates combining features from the classes above .",
    "the classes were defined by studying the properties of previously proposed similarity predicates as well as ones newly proposed herein .",
    "the first four classes encompass predicates introduced previously in various contexts for data cleaning tasks , with the exception of bm25 which to the best of our knowledge is the first time that is deployed for data cleaning purposes .",
    "the language modeling class of predicates draws from work on information retrieval and is introduced herein for data cleaning tasks . within each class",
    "we discuss declarative realizations of predicates .",
    "suppose @xmath9 is the set of tokens in the query string @xmath6 and @xmath10 is the set of tokens in the string tuple @xmath7 .",
    "the _ intersectsize _",
    "predicate @xcite is simply the number of common tokens between @xmath6 and @xmath7 , i.e. : @xmath15 jaccard similarity @xcite is the fraction of tokens in @xmath6 and @xmath16 that are present in both , namely : @xmath17 if we assign a weight @xmath18 to each token @xmath19 , we can define weighted versions of the above predicates . _",
    "@xcite is the total weight of common tokens in @xmath9 and @xmath10 , i.e. , @xmath20 .",
    "similarly , _ weightedjaccard _ is the sum of the weights of tokens in @xmath21 divided by the sum of the weights of tokens in @xmath22 .",
    "the predicates in this class encompass predicates widely adopted from information retrieval ( ir ) . a basic task in ir",
    "is , given a query , identifying _",
    "relevant _ _ documents _ to that query . in our context",
    ", we would like to identify the _",
    "tuples _ in a relation that are _ similar _ to a query string .    given a query string @xmath6 and a string tuple @xmath7 ,",
    "the similarity score of @xmath6 and @xmath7 in this class of predicates is of the form @xmath23 , where @xmath24 is the query - based weight of the token @xmath19 in string @xmath6 and @xmath25 is the tuple - based weight of the token @xmath19 in string @xmath7 .",
    "the tf - idf cosine similarity@xcite between a query string @xmath6 and a string tuple @xmath7 is defined as follows :    @xmath26    where @xmath27 are the normalized tf - idf weights @xcite .",
    "the normalized tf - idf between a token @xmath19 and a string @xmath16 , @xmath28 is given by : @xmath29 the @xmath30 term makes the weight of a token inversely proportional to its frequency in the database ; the @xmath31 term makes it proportional to its frequency in @xmath16 .",
    "intuitively , this assigns low scores to frequent tokens and high scores to rare tokens .",
    "more discussion is available elsewhere @xcite .",
    "the @xmath32 similarity score between a query string @xmath6 and a tuple @xmath7 , is given as : @xmath33 where @xmath34 @xmath35 is a modified form of robertson - sparck jones weight : @xmath36 and @xmath37 is the number of tuples in the base relation @xmath1 , @xmath38 is the number of tuples in @xmath1 containing the token @xmath19 , @xmath39 is the frequency of occurrence of the token @xmath19 within tuple @xmath7 , @xmath40 is the number of tokens of tuple @xmath7 , @xmath41 is the average number of tokens per tuple , i.e. @xmath42 and @xmath43 , @xmath44 , and @xmath45 are independent parameters . for trec-4 experiments @xcite , @xmath46 $ ] , @xmath47 and @xmath48 $ ] .",
    "a language model , is a form of a probabilistic model . to realize things concretely",
    ", we base our discussion on a specific model introduced by ponte and croft @xcite . given a collection of documents , a language model is inferred for each ; then the probability of generating a given query according to each of these models is estimated and documents are ranked according to these probabilities . considering an approximate selection query , each tuple in the database",
    "is considered as a document ; a model is inferred for each tuple and the probability of generating the query given the model is the similarity between the query and the tuple .",
    "the similarity score between query @xmath6 and tuple @xmath7 is defined as : @xmath49 where @xmath50 is the probability of token @xmath19 occurring in tuple @xmath7 and is given as follows : @xmath51 @xmath52 is the maximum likelihood estimate of the probability of the token @xmath19 under the token distribution for tuple @xmath7 and is equal to @xmath53 where @xmath54 is raw term frequency and @xmath55 is the total number of tokens in tuple @xmath7 .",
    "@xmath56 is the mean probability of token @xmath19 in documents containing it , i.e. , @xmath57 where @xmath58 is the document frequency of token @xmath19 .",
    "this term is used since we only have a tuple sized sample from the distribution of @xmath59 , thus the maximum likelihood estimate is not reliable enough ; we need an estimate from a larger amount of data .",
    "the term @xmath60 is used to model the risk for a term @xmath19 in a document @xmath7 using a geometric distribution : @xmath61 @xmath62 is the expected term count for token @xmath19 in tuple @xmath7 if the token occurred at the average rate , i.e. , @xmath63 .",
    "the intuition behind this formula is that as the @xmath31 gets further away from the normalized mean , the mean probability becomes riskier to use as an estimate .",
    "finally , @xmath64 is the raw count of token @xmath19 in the collection , i.e. @xmath65 and @xmath66 is the raw collection size or the total number of tokens in the collection , i.e. @xmath67 .",
    "@xmath68 is used as the probability of observing a non - occurring token .",
    "the query generation process can be modeled by a discrete hidden markov process .",
    "figure [ hmm - fig ] shows a simple yet powerful two - state hmm for this process .",
    "the first state , labeled  string \" represents the choice of a token directly from the string .",
    "the second state , labeled  general english \" represents the choice of a token that is unrelated to the string , but occurs commonly in queries .",
    "suppose @xmath6 is the query string and @xmath7 is a string tuple from the base relation @xmath1 ; the similarity score between @xmath6 and @xmath7 , @xmath69 , is equal to the probability of generating @xmath6 given that @xmath7 is similar , that is : @xmath70 where : @xmath71 @xmath72 and @xmath73 and @xmath74 are transition probabilities of the hmm .",
    "the values for these parameters can be optimized to maximize accuracy given training data .",
    "an important and widely used class of string matching predicates is the class of edit - based predicates . in this class ,",
    "the similarity between @xmath6 and @xmath7 is the transformation cost of string @xmath6 to @xmath7 , @xmath75 .",
    "more specifically @xmath75 is defined as the minimum cost sequence of edit operations that converts @xmath6 to @xmath7 .",
    "edit operations include _ copy , insert , substitute _ and _ delete _ characters in @xmath6 and @xmath7 @xcite .",
    "algorithms exist to compute @xmath75 in polynomial time @xcite but complexity is sensitive to the nature of operations and their operands ( individual characters , blocks of consecutive characters , etc ) .",
    "the edit similarity is therefore defined as : @xmath76 edit operations have an associated cost . in the levenstein edit - distance @xcite",
    "which we will refer to as edit - distance , the cost of copy operation is zero and all other operations have unit cost .",
    "other cost models are also possible @xcite .",
    "we present a general similarity predicate and refer to it as generalized edit similarity ( ges ) ( following @xcite ) .",
    "consider two strings @xmath6 and @xmath7 that are tokenized into _ word _ tokens and a weight function @xmath18 that assigns a weight to each word token @xmath19 .",
    "the transformation cost of string @xmath6 to @xmath7 , @xmath75 is the minimum cost of transforming @xmath6 to @xmath7 by a sequence of the following transformation operations :    * _ token replacement _ : replacing word token @xmath77 in @xmath6 by word token @xmath78 in @xmath7 with cost @xmath79 \\cdot w(t_1)$ ] , where @xmath80 is the edit similarity score between @xmath77 and @xmath78 .",
    "* _ token insertion _ : inserting a word token @xmath19 into @xmath6 with cost @xmath81 where @xmath82 , is a constant token insertion factor , with values between 0 and 1 .",
    "* _ token deletion _ : deleting a word token @xmath19 from @xmath6 with cost @xmath18 .",
    "suppose @xmath83 is the sum of weights of all word tokens in the string @xmath6 .",
    "we define the _ generalized edit similarity _ predicate between a query string @xmath6 and a tuple @xmath7 as follows : @xmath84    a related predicate is the softtfidf predicate @xcite . in softtfidf ,",
    "normalized tf - idf weights of word tokens are used along with cosine similarity and any other similarity function @xmath85 to find the similarity between word tokens .",
    "therefore the similarity score , @xmath86 , is equal to : @xmath87 where @xmath88 are the normalized tf - idf weights and @xmath89 is the set of words @xmath90 such that there exists some @xmath91 such that @xmath92 .",
    "we now describe declarative realizations of predicates in each class .",
    "we present declarative statements using standard sql expressions . for all predicates",
    ", there is a preprocessing phase responsible for tokenizing strings in the base relation , @xmath1 , and calculating as well as storing related weight values which are subsequently utilized at query time .",
    "tokenization of relation @xmath1 ( ` base_table ` ) creates the table ` base_tokens ` ` ( tid , token ) ` , where @xmath93 is a unique token identifier for each tuple of ` base_table ` and _ token _ an associated token ( from the set of tokens corresponding to the tuple with identifier @xmath93 in ` base_table ` ) . the query string is also tokenized on the fly ( at query time ) creating the table ` query_tokens(token ) ` .",
    "in the rest of this chapter , we present sql expressions required for preprocessing and query time approximate selections for the different predicates . in some cases , we re - write formulas to make them amenable to more efficient declarative realization . the main sql codes are given along with their description here .",
    "appendix a contains detailed sql expressions .",
    "the intersectsize predicate requires token generation to be completed in a preprocessing step .",
    "sql statements to conduct such a tokenization , which is common to all predicates we discuss , is available in appendix a. the sql statement for approximate selections with the intersectsize predicate is shown on figure [ intersect ] .",
    "the jaccard coefficient predicate can be efficiently computed by storing the number of tokens for each tuple of the ` base_table ` during the preprocessing step .",
    "for this reason we create a table ` base_ddl(tid , token , len ) ` where ` len ` is the number of tokens in tuple with tuple - id ` tid ` .",
    "the sql statement for conducting approximate selections with the jaccard predicate is presented in figure [ jaccard ] .",
    "select & r1.tid , count ( * ) + from & base_tokens r1 , query_tokens r2 + where & r1.token = r2.token + group by & r1.tid +         + select & s1.tid , count(*)/(s1.len+s2.len - count ( * ) ) + from & base_ddl s1 , query_tokens r2 , + & ( select count ( * ) as len + & from query_tokens ) s2 + where & s1.token = r2.token + group by & s1.tid , s1.len , s2.len +    the weighted overlap predicates require calculation and storage of the related weights for tokens of the base relation during preprocessing . for the weightedmatch predicate , we store during the preprocessing step the weight of each token redundantly with each",
    "` tid , token ` pair in a table ` base_tokens_weights(tid , token , weight ) ` in order to avoid an extra join with a table ` base_weight(token , weight ) ` at query time . in order to calculate the similarity score at query time , we use sql statements similar to that used for the intersectsize predicate ( shown in figure [ intersect ] ) but replace table ` base_tokens ` by ` base_tokens_weights ` and ` count ( * ) ` , by ` sum(r1.weight ) ` .    for the weightedjaccard",
    "predicate , we create during preprocessing a table ` base_ddl(tid , token , weight , len ) ` where ` weight ` is the weight of ` token ` and ` len ` is the sum of weights of tokens in the tuple with tuple - id ` tid ` .",
    "the sql statement for approximate selections using this predicate is the same as the one shown in figure [ jaccard ] but ` count ( * ) ` is replaced by ` sum(weight ) ` .",
    "the sql implementation of the tf - idf cosine similarity predicate has been presented in @xcite . during preprocessing",
    ", we store tf - idf weights for the base relation in relation ` base_weights(tid , token , weight ) ` . a weight table ` query_weights(token , weight )",
    "` for the query string is created on the fly at query time .",
    "the sql statements in figure [ awsql ] will calculate the similarity score for each tuple of the base table .",
    "select & r1w.tid , sum(r1w.weight*r2w.weight ) + from & base_weights r1w , query_weights r2w + where & r1w.token = r2w.token + group by & r1w.tid +      realization of bm25 in sql involves generation of the table ` base_weights(tid , token , weight ) ` storing the weights for tokens in each tuple of the base relation .",
    "these weights ( @xmath25 ) consist of two parts that could be considered as modified versions of tf and idf . for a complete set of sql statements implementing the required preprocessing ,",
    "refer to appendix a. the query weights table ` query_weights(token , weight ) ` can be created on the fly using the following subquery : +   ( select tf.token , tf.tf*(@xmath44+1)/(@xmath44+tf.tf ) as weight + from ( select t.token , count ( * ) as tf + from query_tokens t + group by t.token ) tf ) +    the sql statement shown in figure [ awsql ] will calculate bm25 similarity scores .        in order to calculate language modeling scores efficiently , we rewrite the formulas and finally drop some terms that would not affect the overall accuracy of the metric . calculating the values in equations ( [ eqlm3 ] ) and ( [ eqlm4 ] ) is easy . we build the following relations during preprocessing : ` base_tf(tid , token , tf ) ` where ` tf`@xmath94 .",
    "+ ` base_dl(tid , dl ) ` where ` dl`@xmath95 .",
    "+ ` base_pml(tid , token , pml ) ` where ` pml`@xmath96 . + ` base_pavg(token , pavg ) ` where ` pavg`@xmath97 . + ` base_freq(tid , token , freq ) ` where ` freq`@xmath98 . + ` base_risk(tid , token , risk ) ` where ` risk`@xmath99 .",
    "we omit most of the sql statements in this chapter for readability .",
    "full sql statements are available in appendix a. in order to improve the performance of the associated sql queries , we rewrite the final score formula of equation ( [ eqlm1 ] ) , as follows :    @xmath100     \\times \\frac{\\displaystyle \\prod_{\\forall t}(1 - \\hat{p}(t|m_d ) ) } { \\displaystyle \\prod_{t \\in \\cal{q}}(1 - \\hat{p}(t|m_d ) ) }   \\end{aligned}\\ ] ]    we slightly change ( [ eqlm5 ] ) to the following : @xmath101   \\times \\frac{\\displaystyle \\prod_{\\forall t \\in \\cal{d}}(1 - \\hat{p}(t|m_d ) ) } { \\displaystyle \\prod_{t \\in \\cal{q } \\cap \\cal{d}}(1 - \\hat{p}(t|m_d ) ) }   \\end{aligned}\\ ] ]    this change results in a large performance gain , since the computation is restricted to the tokens of the query and the tokens of a tuple ( as opposed to the entire set of tokens present in the base relation ) .",
    "experiments demonstrate that accuracy is not considerably affected .    in equation ( [ eqlm2 ] )",
    ", we only materialize the first part ( i.e. , values of tokens that are present in the tuple @xmath7 ) in the relation ` base_pm ` during preprocessing ( storing the second part would result in unnecessary waste of space ) .",
    "we therefore have to divide all formulas that use @xmath50 into two parts : one for tokens present in the tuple under consideration and one for all other tokens .",
    "so we rewrite the first term in equation ( [ eqlm6 ] ) as follows : @xmath102 the term @xmath103 in the above formula is constant for any specific query string , so it can be dropped , since the goal is to find most similar tuples by ranking them based on the similarity scores .",
    "therefore , equation ( [ eqlm6 ] ) can be written as follows : @xmath104    this transformation allows us to efficiently compute similar tuples by just storing @xmath105 and @xmath68 for each pair of @xmath19 and @xmath7 .",
    "thus , we create table ` base_pm(tid , token , pm , cfcs ) ` where ` pm ` @xmath106 and ` cfcs ` @xmath107 as the final result of the preprocessing step . we also calculate and store the term @xmath108 during preprocessing in relation ` base_sumcompbase(tid , sumcompm ) ` .",
    "the query - time sql statement to calculate similarity scores is shown in figure [ lm ] .",
    "the subquery in the statement computes the three terms in equation [ eqlm8 ] that include intersection of query and tuple tokens and therefore needs a join between the two token tables .",
    "the fourth term in the equation is read from the table stored during the preprocessing as described above .",
    "[ cols= \" < \" , ]     to further investigate the effect of pruning based on idf on performance and accuracy of predicates , we examine the idf distribution in our data sets .",
    "figure [ idf - chart ] shows the distribution of idf weights for cu1 data set .",
    "the distribution is similar in all other data sets .",
    "as it can be seen , there is a huge number of tokens with low idf . for this data",
    "set , a pruning rate of @xmath109 will drop nearly 150,000 out of 250,000 tokens which results in a huge performance gain and a very little drop in accuracy of predicates ( except unweighted naive predicates that benefit from pruning as described above ) .",
    "we presented an exhaustive evaluation of approximate selection predicates by grouping them into five classes based on their characteristics : overlap predicates , aggregate weighted predicates , edit - based predicates , combination predicates and language modeling predicates .",
    "we experimentally show how predicates in each of these classes perform in terms of accuracy , preprocessing and execution time . within our framework ,",
    "the overlap predicates are relatively efficient but have low accuracy .",
    "edit based predicates perform worse in terms of accuracy but are relatively fast due to the filtering step they employ .",
    "the aggregate weighted predicates , specifically bm25 , perform very well both in terms of accuracy and efficiency .",
    "both the predicates from the language modeling cluster perform well in terms of accuracy . moreover",
    ", hmm is as fast as simple overlap predicates .",
    "the combination predicates are considerably slow due to their two levels of tokenization . among the combination predicates , ges based predicates are robust in handling edit errors but fail considerably in capturing token swap errors .",
    "softtfidf with jaro - winkler performs nearly equal to bm25 and hmm and is among the best in terms of accuracy , although it is the slowest predicate .",
    "this establishes the effectiveness of bm25 and hmm predicates for approximate matching in large databases .",
    "we proposed new similarity predicates for approximate selections based on probabilistic information retrieval and presented their declarative instantiation .",
    "we presented an in - depth comparison of accuracy and performance of these new predicates along with existing predicates , grouping them into classes based on their primary characteristics .",
    "our experiments show that the new predicates are both effective as well as efficient for data cleaning applications",
    ".    10    r.  ananthakrishna , s.  chaudhuri , and v.  ganti . eliminating fuzzy duplicates in data warehouses . in _ proceedings of the 28th international conference on very large databases ( vldb ) _ , 2002 .",
    "arvind arasu , venkatesh ganti , and raghav kaushik .",
    "efficient exact set - similarity joins . in _ proceedings of the 32nd international conference on very large data bases ( vldb ) _ , pages 918929 .",
    "vldb endowment , 2006 .",
    "andrei  z. broder , moses charikar , alan  m. frieze , and michael mitzenmacher .",
    "min - wise independent permutations . , 60(3):630659 , 2000 .",
    "surajit chaudhuri , kris ganjam , venkatesh ganti , and rajeev motwani .",
    "robust and efficient fuzzy match for online data cleaning . in _",
    "sigmod03 : proceedings of the 2003 acm sigmod international conference on management of data _ , pages 313324 , june 2003 .",
    "surajit chaudhuri , venkatesh ganti , and raghav kaushik .",
    "a primitive operator for similarity joins in data cleaning . in _",
    "icde 06 : proceedings of the 22nd international conference on data engineering ( icde ) _",
    ", page  5 , washington , dc , usa , 2006 .",
    "ieee computer society .",
    "william  w. cohen",
    ". integration of heterogeneous databases without common domains using queries based on textual similarity . in _",
    "sigmod 98 : proceedings of the 1998 acm sigmod international conference on management of data _ , pages 201212 , new york , ny , usa , 1998 .",
    "acm press .",
    "william  w. cohen , pradeep ravikumar , and stephen  e. fienberg . a comparison of string distance metrics for name - matching tasks . in _ proceedings of ijcai-03 workshop on information integration on the web ( iiweb-03 ) _ , pages 7378 , 2003 .",
    "j.  b. copas and f.  j. hilton .",
    "record linkage : statistical models for matching computer records . , pages 287320 , 1990 .",
    "ivan  p. fellegi and alan  b. sunter . a theory for record linkage .",
    ", 64(328):11831210 , 1969 .",
    "helena galhardas , daniela florescu , dennis shasha , eric simon , and cristian - augustin saita .",
    "declarative data cleaning : language , model , and algorithms . in _ proceedings of the international conference on very large databases ( vldb ) _ ,",
    "pages 371380 , 2001 .",
    "luis gravano , panagiotis  g. ipeirotis , h.  v. jagadish , nick koudas , s.  muthukrishnan , and divesh srivastava .",
    "approximate string joins in a database ( almost ) for free . in _ proceedings of the 27th international conference on very large data bases ( vldb ) _ , pages 491500 , san francisco , ca , usa , 2001 .",
    "morgan kaufmann publishers inc .",
    "luis gravano , panagiotis  g. ipeirotis , nick koudas , and divesh srivastava .",
    "text joins for data cleansing and integration in an rdbms . in _ proceedings of the 19th international conference on data engineering ( icde ) _ , pages 729731 , march 2003 .",
    "dan gusfield . .",
    "cambridge university press , new york , ny , usa , 1997 .",
    "mauricio  a. hernndez and salvatore  j. stolfo .",
    "real - world data is dirty : data cleansing and the merge / purge problem .",
    "2(1):937 , 1998 .    m.  a. jaro .",
    "advances in record linkage methodology as applied to matching the 1985 census of tampa .",
    ", pages 414420 , 1984 .",
    "nick koudas , amit marathe , and divesh srivastava .",
    "flexible string matching against large databases in practice . in _ proceedings of the international conference on very large databases ( vldb ) _",
    ", pages 10781086 , august 2004 .",
    "nick koudas , amit marathe , and divesh srivastava .",
    "spider : flexible matching in databases . in _",
    "sigmod 05 : proceedings of the 2005 acm sigmod international conference on management of data _ ,",
    "pages 876878 , new york , ny , usa , 2005 . acm press .",
    "nick koudas , amit marathe , and divesh srivastava . using spider : an experience report . in _",
    "sigmod 06 : proceedings of the 2006 acm sigmod international conference on management of data _",
    ", page 719 , 2006 .",
    "nick koudas , sunita sarawagi , and divesh srivastava .",
    "record linkage : similarity measures and algorithms . in _",
    "sigmod 06 : proceedings of the 2006 acm sigmod international conference on management of data _ ,",
    "pages 802803 , 2006 .",
    "nick koudas and divesh srivastava .",
    "approximate joins : concepts and techniques . in _ proceedings of the international conference on very large databases ( vldb ) _",
    ", page 1363 , 2005 .",
    "david r.  h. miller , tim leek , and richard  m. schwartz . a hidden markov model information retrieval system . in",
    "_ proceedings of the 22nd annual international acm sigir conference on research and development in information retrieval _ , pages 214221 , august 1999 .",
    "jay  m. ponte and w.  bruce croft . a language modeling approach to information retrieval . in _ proceedings of the 21st annual international acm sigir conference on research and development in information retrieval _ ,",
    "pages 275281 , august 1998 .",
    "l.r . rabiner",
    ". a tutorial on hidden markov models and selected applications inspeech recognition . in _ proceedings of the ieee _",
    ", volume  77 , pages 257286 , 1989 .",
    "stephen robertson .",
    "understanding inverse document frequency : on theoretical arguments . , 60(5):503520 , 2004 .",
    "stephen  e. robertson , steve walker , micheline hancock - beaulieu , mike gatford , and a.  payne .",
    "okapi at trec-4 . in _ trec _ , 1995 .",
    "gerard salton and chris buckley .",
    "term - weighting approaches in automatic text retrieval .",
    ", 24(5):513523 , 1988 .",
    "gerard salton and michael  j. mcgill . .",
    "mcgraw - hill , inc .",
    ", new york , ny , usa , 1986 .",
    "sunita sarawagi and alok kirpal .",
    "efficient set joins on similarity predicates . in",
    "sigmod04 : proceedings of the acm sigmod international conference on management of data _ , pages 743754 , june 2004 .    w.  e. winkler .",
    "the state of record linkage and current research problems .",
    "technical report rr99/04 , us bureau of the census , 1999 .",
    "we assume the base relation ` base_table ` has an integer tuple i d attribute ` tid ` and a string valued attribute ` string ` .",
    "the following sql statements tokenize the base relation , creating ` base_tokens(tid , token ) ` . assuming that the query relation ` query_table ` has a single string valued attribute ` string ` , the same sql statements can be used for tokenization of the query string by removing ` tid ` from the statements .       the size of the qgrams",
    ". + insert into integers(i ) values ( 1 ) , ( 2 ) ,  , ( max_str_size + @xmath110 ) +   + insert into base_tokens(tid , token ) + select tid , substring(concat(substring($  $,1,@xmath11 - 1 ) , + upper ( replace(concat(string ) ,   ,",
    "substring($  $,1,@xmath11 - 1 ) ) ) , + substring($  $,1,@xmath11 - 1 ) ) , integers.i , @xmath11 ) + from integers inner join base_table on + integers.i < = length ( replace(concat(string ) ,   ,",
    "substring($  $,1,@xmath11 - 1 ) ) ) + ( @xmath11 - 1 ) +       select & tid , substring(concat(string ) , 1 , locate(  , concat(string ) ) - 1 ) + from & base_table + where & locate(  , concat(string ) ) > 0 +   + select & tid , substring(concat(string ) , n1.i+1 , n2.i - n1.i-1 ) + from & base_table , integers n1 , integers n2 + where & n1.i = locate(  , concat(string ) , n1.i ) and n2.i = locate(  , concat(string ) , n1.i + 1 ) +   + select & tid , substring(concat(string ) , length(concat(string ) ) - locate(  , reverse(concat(string)))+2 ) + from & base_table + where & locate(  , concat(string ) ) > 0 +   + select & tid , concat(string ) + from & base_table + where & locate(  , concat(string ) ) = 0 +       select & tid , token , + & substring(concat(substring($  $,1,@xmath11 - 1 ) , upper(token ) , substring($  $,1,@xmath11 - 1 ) ) , integers.i , @xmath11 ) + from & integers inner join base_tokens on integers.i < = length(token ) + ( @xmath11 - 1 ) + group by & tid , token , qgram +",
    "+ select & t.tid , count ( * ) + from & base_tokens t + group by & t.tid + & +   + select & t.tid , t.token , d.ddl + from & base_tokens t , base_ddl d + where & t.tid = d.tid +   +   + select & s1.tid , count(*)/(s1.ddl + s2.ddl - count ( * ) ) + from & base_tokensddl s1 , query_tokens r2 , ( select count ( * ) as ddl from query_tokens t ) s2 + where & s1.token = r2.token + group by & s1.tid +        + select & count ( * ) + from & base_table + & +   + select & t.tid , t.token , count ( * ) + from & base_tokens t + group by & t.tid , t.token + & +   + select & t.token , log(s.size - count(t.tid ) + 0.5 ) - log(count(t.tid ) + 0.5 ) + from & base_tf t , base_size s + group by & t.token + & +   + select & t.tid , t.token , i.midf + from & base_bmidf i , base_tf t + where & i.token = t.token +   +   + select & w1.tid , t2.tid , sum(w1.weight ) + from & base_weights w1 , query_tokens t2 + where & w1.token = t2.token + group by & t2.tid , w1.tid +        + select & count ( * ) + from & base_table + & +   + select & t.tid , t.token , count ( * ) + from & base_tokens t + group by & t.tid , t.token + & +   + select & t.token , log(s.size - count(t.tid ) + 0.5 ) - log(count(t.tid ) + 0.5 ) + from & base_tf t , base_size s + group by & t.token + & +   + select & t.tid , t.token , i.midf + from & base_bmidf i , base_tokens t + where & i.token = t.token + & +   + select & w.tid , sum(weight ) + from & base_weights w + group by & w.tid + & +   + select & w.tid , w.token , d.ddl , w.weight + from & base_weights w , base_ddl d + where & w.tid = d.tid +   +   + select & s1.tid , sum(s1.weight)/(s1.ddl + s2.ddl - sum(s1.weight ) ) + from & base_tokensddl s1 , query_tokens r2 , + & ( select sum(t.weight ) as ddl + & from ( select t.token , i.idf as weight + & from base_idf i , query_tokens t + & where i.token = t.token ) t ) s2 + where & s1.token = r2.token + group & by s1.tid +          + select & count ( * ) + from & base_table + & +   + select & t.token , log(s.size ) - log(count(distinct t.tid ) ) + from & base_tokens t , base_size s + group by & t.token + & +   + select & t.tid , t.token , count ( * ) + from & base_tokens t + group by & t.tid , t.token + & +   + select & t.tid , sqrt(sum(i.idf*i.idf*t.tf*t.tf ) ) + from & base_idf i , base_tf t + where & i.token = t.token + group by & t.tid + & +   + select & t.tid , t.token , i.idf*t.tf/l.len + from & base_idf i , base_tf t , base_length l + where & i.token = t.token and t.tid = l.tid +      + select & r1w.tid , sum(r1w.weight*r2w.weight ) + from & base_weights r1w , + & ( select t.token , qidf.idf*qtf.tf/qlen.length as weight + & from ( select r.token , r.idf + & from query_tokens s , base_idf r + & where s.token = r.token + & group by s.token ) qidf , + & ( select t.token , count ( * ) as tf + & from query_tokens t + & group by t.tid , t.token ) qtf , + & ( select sqrt(sum(qidf.idf*qidf.idf*qtf.tf*qtf.tf ) ) as length + & from ( select r.token , r.idf + & from query_tokens s , base_idf r + & where s.token = r.token + & group by s.token ) qidf , + & ( select t.token , count ( * ) as tf + & from query_tokens t + & group by t.token ) qtf + & where i.token = t.token ) qlen + & where qidf.token = qtf.token ) r2w + where & r1w.token = r2w.token + group by & r1w.tid +        + select & count ( * ) + from & base_table + & +   + select & t.tid , t.token , count ( * ) + from & base_tokens t + group by & t.tid , t.token + & +   + select & t.token , log(s.size - count(t.tid ) + 0.5 ) - log(count(t.tid ) + 0.5 ) + from & base_tf t , base_size s + group by & t.token + & +   + select & t.tid , sum(t.tf ) + from & base_tf t + group by & t.tid + & +   + select & avg(len ) + from & base_bmbaselength + & +   + select & t.tid , t.token , ( t.tf*(@xmath43+1 ) ) / ( ( ( ( 1 - @xmath45)+(@xmath45*l.dl / a.avgdl))*@xmath43 ) + t.tf ) + from & base_bmbaselength l , base_bmbaseavglength a , base_tf t + where & l.tid = t.tid + & +   + select & t.tid , t.token , t.mtf*i.midf + from & base_bmbasemodtf t , base_bmidf i + where & t.token = i.token +   +   + select & b.tid , sum(b.weight * s.mtf ) + from & base_bmbaseweights b , + & ( select token , ( count(*))*(@xmath44 + 1 ) / ( @xmath44+count ( * ) ) as mtf + & from query_tokens t + & group by t.token ) s + where & b.token = s.token + group by & b.tid +          + select & t.tid , t.token , count ( * ) + from & base_tokens t + group by & t.tid , t.token + & +   + select & t.tid , count ( * ) + from & base_tokens t + group by & t.ti + & +   + select & t.tid , t.token , t.tf/d.dl + from & base_tf t , base_dl d + where & t.tid=d.tid + & +   + select & p.token , avg(p.pml ) + from &",
    "base_pml p + group by & p.token + & +   + select & t.tid , t.token , p.pavg*d.dl + from & base_tf t , base_pavg p , base_dl d + where & t.token = p.token and t.tid=d.tid + & +   + select & t.tid , t.token , ( 1.0/(1.0+q.freq ) ) * ( power(q.freq/(1.0+q.freq ) , t.tf ) ) + from & base_tf t , base_freq q + where & t.tid=q.tid and t.token = q.token + & +   + select & count ( * ) + from & base_tokens + & +   + select & t.token , count ( * ) / s.size + from & base_tokens t , base_tsize s + group by & t.token + & +     select & t.tid , t.token , 1e0 * power(m.pml , 1.0-r.risk ) * power(a.pavg , r.risk ) , c.cfcs + from & base_tf t , base_risk r , base_pml m , base_pavg a , base_cfcs c , base_tsize s + where & t.tid=r.tid and t.token = r.token and t.tid=m.tid and + & t.token = m.token and t.token = a.token and t.token = c.token + & +   + select & p.tid , sum(log(1.0-p.pm ) ) + from & base_pm p + group by & p.tid +   +   + select & b1.tid , b1.score + b2.sumcompm + from & ( select p1.tid , sum(log(p1.pm ) ) - sum(log(1.0-p1.pm ) ) - sum(log(p1.cfcs ) ) as score + & from base_pm p1 , query_tokens t2 + & where p1.token = t2.token + & group by p1.tid ) b1 , + & base_sumcompmbase b2 + where & b1.tid=b2.tid +        + select & count ( * ) + from & base_table + & +   + select & t.tid , t.token , count ( * ) + from & base_tokens t + group by & t.tid , t.token + & +   + select & t.tid , count ( * ) + from & base_tokens t + group by & t.tid + & +   + select & t.tid , t.token , f.tf/d.dl + from & base_tokens t , base_tf f , base_dl d + where & f.tid=t.tid and t.token = f.token and t.tid=d.tid + & +   + select & sum(t.dl ) + from & base_dl t + & +   + select & t.token , sum(t.tf)/d.sdl + from & base_tf t , base_sumdl d + group by & t.token + & +   + select & m.tid , m.token , log ( ( 1 + ( @xmath111*m.pml ) / ( @xmath112*p.ptge ) ) ) + from & base_ptge p , base_pml m + where & p.token = m.token + group by & tid , token +   +   + select & w1.tid , exp(sum(w1.weight ) ) + from & base_weights w1 , query_tokens t2 + where & w1.token = t2.token +   +          + select & count ( * ) + from & base_table + & +   + select & t.token , log(s.size ) - log(count(distinct t.tid ) ) + from & base_tokens t , base_size s + group by & t.token + & +   + select & avg(i.idf ) + from & base_idf i + & +   + select & t.tid , t.token , count ( * ) + from & base_qgrams t + group by & t.tid , t.toke + & +   + select & t.tid , t.token , t.qgram , s.size + from & base_qgrams t , base_tokensize s + where & t.tid = s.tid and t.token = s.token +     select & maxsim.tid , r.string + & ( 1.0 - 1.0/@xmath11 ) + ( 1/sum(qidf.idf ) ) * sum(qidf.idf * ( ( ( 2.0/@xmath11 ) * maxsim.maxsim ) ) ) as score + from & ( select jac_sim.tid , jac_sim.token2 , max(sim ) as maxsim + & from ( select bsize.tid as tid , bsize.token as token1 , q.token as token2 , + & count(*)/(bsize.size + qsize.size - count ( * ) ) as sim + & from base_qgramstokensize bsize , query_qgrams q , + & ( select t.token , count ( * ) as size + & from query_qgrams t + & group by t.token ) qsize + & where bsize.qgram = q.qgram and q.token = qsize.token + & group by bsize.tid , bsize.token , q.token ) jac_sim + & group by jac_sim.tid , jac_sim.token2 ) maxsim , + & ( select r.token , r.idf + & from query_tokens s , base_idf r + & where s.token = r.token + & group by s.token + & union + & select s.token , a.idfavg as idf + & from query_tokens s , base_idfavg a + & where s.token not in ( select i.token from base_idf i ) + & group by s.token ) qidf , + & base_table r + where & tm.token2 = i.token and r.tid = maxsim.tid + group by & tm.tid + having & score > = @xmath4 +        + select & count ( * ) + from & base_table + & +   + select & t.token , log(s.size ) - log(count(distinct t.tid ) ) + from & base_tokens t , base_size s + group by & t.token + & +   + select & avg(i.idf ) + from & base_idf i + & +   + select & n.i-1 , round(rand()*maxint ) + from & integers n + limit & hash_size + & +   + select & f.fid , q.qgram , mod(conv(hex ( q.qgram ) , 16 , 10 ) * maxint , f.func ) + from & base_hashfunc f , ( select distinct qgram from base_qgrams ) q + & +   + select & q.tid , q.token , h.fid , min(h.value ) + from & base_qgrams q , base_hashvalue h + where & q.qgram = h.qgram + group by & q.tid , q.token , h.fid +     select & maxsim.tid , r.string , + & ( 1.0 - 1.0/@xmath11 ) + ( 1/sum(i.idf ) ) * sum(i.idf * ( ( ( 2.0/@xmath11 ) * maxsim.maxsim ) ) ) as score + from & ( select mh_sim.tid , mh_sim.token2 , max(sim ) as maxsim + & from ( select bmhsig.tid as tid , bmhsig.token as token1 , + & qmhsig.token as token2 , count(*)/@xmath115 as sim + & from base_minhashsignature bmhsig , + & ( select q.token , h.fid , min(h.value ) as value + & from query_qgrams q , base_hashvalue h + & where q.qgram = h.qgram + & group by q.token , h.fid ) qmhsig + & where bmhsig.fid = qmhsig.fid and bmhsig.value = qmhsig.value + & group by bmhsig.tid , bmhsig.token , qmhsig.token ) mh_sim + & group by mh_sim.tid , mh_sim.token2 ) maxsim , + & ( select r.token , r.idf + & from query_tokens q , base_idf r + & where q.token = r.token + & group by q.token + & union + & select q.token , a.idfavg as idf + & from query_tokens q , base_idfavg a + & where q.token not in ( select i.token from base_idf i ) + & group by q.token ) qidf , + & base_table r + where & maxsim.token2 = i.token and r.tid = maxsim.tid + group by & maxsim.tid + having & score > = @xmath4 +        + select & count ( * ) + from & base_table + & +   + select & t.token , log(s.size ) - log(count(distinct t.tid ) ) + from & base_tokens t , base_size s + group by & t.token + & +   + select & t.tid , t.token , count ( * ) + from & base_tokens t + group by & t.tid , t.token + & +   + select & t.tid , sqrt(sum(i.idf*i.idf*t.tf*t.tf ) ) + from & base_idf i , base_tf t + where & i.token = t.token + group by & t.tid + & +   + select & t.tid , t.token , i.idf*t.tf/l.len + from & base_idf i , base_tf t , base_length l + where & i.token = t.token and",
    "t.tid = l.tid +         +   + select & maxtoken.tid , sum(wb.weight * wq.weight * maxtoken.maxsim ) + from & base_weights wb , + & ( select jaro_sim.tid , jaro_sim.token1 , jaro_sim.token2 , maxsim.maxsim + & from ( select jaro_sim.tid , jaro_sim.token2 , max(sim ) as maxsim + & from ( select r1.tid as tid , r1.token as token1 , + & r2.token as token2 , jarowinkler(r1.token ,",
    "r2.token ) as sim + & from base_tokens r1 , query_tokens r2 + & where jarowinkler(r1.token , r2.token ) > = @xmath4 ) jaro_sim + & group by jaro_sim.tid , jaro_sim.token2 ) maxsim , + & ( select r1.tid as tid , r1.token as token1 , + & r2.token as token2 , jarowinkler(r1.token , r2.token ) as sim + & from base_tokens r1 , query_tokens r2 + & where jarowinkler(r1.token , r2.token ) > = @xmath4 ) jaro_sim + & where jaro_sim.tid = maxsim.tid and jaro_sim.token2 = maxsim.token2 + & and maxsim.maxsim = jaro_sim.sim ) maxtoken , + & ( select qtf.token , qidf.idf*qtf.tf/qlen.length as weight + & from ( select r.token , r.idf + & from query_tokens s , base_idf r + & where s.token = r.token + & group by s.token ) qidf , + & ( select t.token , count ( * ) as tf + & from query_tokens t + & group by t.token ) qtf , + & ( select qtf.tid , sqrt(sum(qidf.idf*qidf.idf*qtf.tf*qtf.tf ) ) as length + & from ( select r.token , r.idf + & from query_tokens s , base_idf r + & where s.token = r.token + & group by s.token ) qidf , + & ( select t.token , count ( * ) as tf + & from query_tokens t + & group by t.token ) qtf + & where qidf.token = qtf.token ) qlen + & where qidf.token = t.token ) wq , + where & maxtoken.token2 = wq.token and maxtoken.tid = wb.tid and maxtoken.token1 = wb.token + group by & maxtoken.tid +"
  ],
  "abstract_text": [
    "<S> declarative data quality has been an active research topic . </S>",
    "<S> the fundamental principle behind a declarative approach to data quality is the use of declarative statements to realize data quality primitives on top of any relational data source . </S>",
    "<S> a primary advantage of such an approach is the ease of use and integration with existing applications .    over the last couple of years several similarity predicates </S>",
    "<S> have been proposed for common quality primitives ( approximate selections , joins , etc . ) and have been fully expressed using declarative sql statements . in this thesis , </S>",
    "<S> new similarity predicates are proposed along with their declarative realization , based on notions of probabilistic information retrieval . </S>",
    "<S> then , full declarative specifications of previously proposed similarity predicates in the literature are presented , grouped into classes according to their primary characteristics . finally , a thorough performance and accuracy study comparing a large number of similarity predicates for data cleaning operations is performed . </S>"
  ]
}