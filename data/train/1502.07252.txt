{
  "article_text": [
    "this work is incorporated within the field of uncertainty quantification in computer experiments . a crucial issue in engineering ( aerospace , car , nuclear , etc . )",
    "concerns the ability of computer codes ( also called simulators or computer models ) to mimic a physical phenomenon of interest as well as possible . in this regard",
    ", the field of so - called verification and validation ( v@xmath0v ) aims at assessing the accuracy of computer predictions for many applications .",
    "for instance , the study of v@xmath0v has become a huge preoccupation in the nuclear industry where numerical simulation is more and more used to assess the safety of installations for which physical experiments are impractical or economically infeasible .",
    "an essential prerequisite of v@xmath0v consists in quantifying all sources of uncertainty involved in a code output @xcite .",
    "our paper is focused on the reduction of one of them , called parameter uncertainty , caused by the lack of knowledge about the value of parameters which are specific to the computer code @xcite .",
    "they can be either non - measurable physical quantities or just tuning factors .",
    "calibration comes down to a statistical inference of these parameters after assuming a statistical model which makes explicit the relationship between the code outputs and the field measurements @xcite .",
    "another popular framework which deals with parameter uncertainty is called history matching ( hm ) @xcite",
    ". hm aims at detecting regions of parameter space which appear to be incompatible with the field measurements . based on an implausibility measure ,",
    "this method is well - suited for large systems for which the size of inputs makes immediate calibration intractable . in the same way",
    ", sensitivity analysis ( sa ) aims at detecting which parameters have a negligible effect on the code output @xcite .",
    "hence , both hm and sa can shrink the input space before carrying out calibration .",
    "our work focuses on calibration in a bayesian fashion @xcite , rather than frequentist methods @xcite .",
    "this strategy of inference provides an appropriate framework to quantify the parameter uncertainty from prior to posterior distribution as new data become available . in the literature",
    ", bayesian calibration is performed within a framework where the code predictions suffer from a systematic discrepancy for any value of parameters , which reflects the view that the mathematical equations underlying the code should be not considered as a perfect modeling of the real world @xcite . even if this framework is more realistic",
    ", some confounding can appear when the parameters and the shape of the discrepancy are jointly estimated @xcite . because this issue is not related with the scope of this paper , our presentation is centered on a statistical model which does not include the code discrepancy .",
    "however , it will be possible to generalize our framework provided the shape of the discrepancy is provided by prior expertise .",
    "we aim at addressing bayesian calibration when the code runs are time - consuming , which is a critical issue frequently arising in the field of computer experiments .",
    "indeed , as simulations needs several hours , even several days to run , at that time the mcmc algorithms become burdensome . for instance , if each simulation lasts one hour , then @xmath1 simulations launched by an mcmc exploration of the parameter space will require more than a year , making the calibration process impractical .",
    "a well - known solution to this issue is to replace the code in the likelihood expression by a gaussian process emulator ( gpe ) which is constructed from a training set of simulations run over a set of input locations , the so - called design of experiments .",
    "this paper propose two new algorithms for building sequential designs aiming at reducing the calibration error induced by the uncertainty of the gpe .",
    "although it was already mentioned by @xcite as an important research axis , few papers deal with that issue . to the best of our knowledge , @xcite has already proposed some empirical criteria for sequentially selecting the code runs .",
    "@xcite have also proposed some adaptive strategies whereby the expected improvement ( ei ) criterion is computed over a likelihood ratio .",
    "the ei criterion is used to optimize black box functions when a limited number of simulations is allocated @xcite .",
    "recently , it has been applied to solving a problem of optimization under uncertainty when the code inputs are @xmath2 where @xmath3 denotes a vector of control variables and @xmath4 denotes a vector of random variables @xcite .",
    "in the same spirit , we propose new algorithms which consist in applying the ei criterion to the sum of squares of the residuals between the code outputs and the field measurements when the code inputs are @xmath5 where @xmath6 is a vector of parameters . in this way",
    ", we aim at reducing the uncertainty due to the gpe in regions of high posterior density .",
    "this paper is divided into five sections . in section @xmath7 , the statistical framework is introduced , then the main features of the gpe are recalled . in section @xmath8 ,",
    "two new algorithms for bayesian calibration based on the expected improvement criterion are presented .",
    "their performances are illustrated on two academic examples in section @xmath9 .",
    "the conclusions of this paper are provided in section @xmath10 .",
    "[ [ notations - and - modeling ] ] notations and modeling + + + + + + + + + + + + + + + + + + + + + +    let @xmath11 be a physical quantity of interest where @xmath12 is a vector of control variables .",
    "this kind of variable is measurable in the field experiments and characterizes the system .",
    "it can include both physical variables ( temperature , pressure , velocity , etc . ) and design variables ( height , area , etc . ) .",
    "we suppose that a number of field experiments , say @xmath13 , has been collected . in this paper",
    ", the sites of field experiments will refer to as the matrix @xmath14^{t } \\in m_{n , d}(\\rr)$ ] and the corresponding measurements will refer to as the vector @xmath15 . due mainly to the measurements errors , @xmath16 is not exactly equal to @xmath17 . hence , for @xmath18 ,    @xmath19    where @xmath20 is modeled as a white noise . the variance @xmath21 is assumed to be known because in many cases , either the precision of the measuring device is known or it can be estimated from replicates .",
    "let @xmath22 be a deterministic computer code which predicts @xmath23 where @xmath24 is a vector of parameters including either factors attached to the field ( chemical rate , friction coefficient , etc . ) or mathematical tuning factors such as a discretization step having no counterpart in physics , or perhaps both @xcite .",
    "the computer code is seen as a black box function , which supposes nothing is known about the connection between the inputs @xmath5 and the output @xmath22 .",
    "a simulation refers to a code output @xmath22 .",
    "a numerical design of experiments means a set of input locations from which the code is run @xcite . following @xcite ,",
    "the computer code should be considered as an imperfect representation of the phenomenon @xmath25 .",
    "hence , @xmath26 where @xmath27 is the code discrepancy and @xmath28 is the optimal value of parameters . combining ( [ model_biais1 ] ) and ( [ eq2 ] ) , the statistical model which links the simulations with the field measurements is written as @xmath29 the computer code is said to be calibrated when an estimator @xmath30 of @xmath28 has been calculated as being the  best - fitting \" parameter according to the statistical model ( [ model_biais ] ) .",
    "due to potential non - identifiability between @xmath28 and @xmath27 in this model , the estimation of @xmath28 requires to set some prior hypothesis on the shape of @xmath27 .",
    "this issue has been widely studied over the past decade . @xcite and",
    "many others have modeled @xmath27 by a gaussian process .",
    "@xcite have proposed to use a more common regression model instead .    in our statistical setting",
    ", we suppose that @xmath27 is negligible in the sense where it can not be distinguished from the noise @xmath31 . if @xmath27 is significant , then the method developed in this paper could be still applied if it has been elicited from prior expertise .",
    "[ [ the - unbiased - model ] ] the unbiased model + + + + + + + + + + + + + + + + + +    let us suppose that @xmath32 .",
    "in other words , for at least one value in @xmath33 denoted by @xmath28 , the computer code is supposed to be a perfect representation of the physical phenomenon @xmath25 , which means that @xmath34 combining ( [ eq1 ] ) and ( [ eq2 ] ) leads to the equation @xmath35 we have chosen to conduct a bayesian inference of @xmath28 because it has been shown better suited than the standard mle , where flat likelihood may need regularization , especially if the dimension of @xmath28 is high @xcite .",
    "let @xmath36 be the vector of code outputs running over the input field data @xmath37 .",
    "the posterior distribution of @xmath28 given by the bayes formula is a normalized version of the following product : @xmath38}\\pi({\\boldsymbol{\\theta}}),\\end{aligned}\\ ] ] where @xmath39 is the sum of squares of the residuals between the simulations @xmath40 and the field measurements . throughout this paper , @xmath41 is referred to as the target posterior distribution .",
    "no closed - form expression exists for @xmath41 because @xmath42 is usually highly non linear with respect to @xmath28 . in such cases , @xmath41 needs to be sampled by running an mcmc algorithm which converges to @xmath41 over a very large number of samples , often several thousand @xcite . in our framework , the mcmc algorithms are thus impractical since each sample requires a time - consuming simulation . a way to circumvent this issue consists in setting a gaussian process emulator ( gpe ) , denoted by @xmath43 , as a prior distribution on @xmath44 where @xmath45 corresponds to a pair of inputs @xmath5 . in the following ,",
    "we need to employ the notation @xmath46 which refers to any value of the code parameter whereas @xmath47 refers to the optimal value to be estimated .    [",
    "[ gaussian - process - emulator ] ] gaussian process emulator + + + + + + + + + + + + + + + + + + + + + + + + +    the gaussian process was first developed within the field of computer experiments by @xcite .",
    "it is the most familiar surrogate model used to mimic a costly computer code . from a bayesian perspective ,",
    "the gaussian process , denoted in this paper by @xmath43 , should be considered as a prior structure on the code @xcite : @xmath48 where    * @xmath49 where @xmath50 is a vector of regression functions and @xmath51 is a vector of location parameters , * @xmath52 the variance of the process , * @xmath53 is the correlation function where @xmath54 is a vector of hyper - parameters including a range parameter and possibly a smoothness parameter .",
    "the choice of the correlation function @xmath55 should depend on the prior information about the shape of the code output over @xmath56 .",
    "in addition , for both practical and theoretical reasons , a stationary function is almost always specified @xcite .",
    "let @xmath57 be a numerical design of experiments : @xmath58 after running the code over @xmath59 , @xmath60 simulations can be collected : @xmath61 let @xmath62 and @xmath63 be two vectors in @xmath56 .",
    "then ,    * @xmath64 is the matrix of correlations between the simulations @xmath65 , * @xmath66 is the vector of correlations between @xmath67 and each of @xmath65 .    by conditioning the gaussian process ( [ gp_prior ] ) on the training set of simulations @xmath65 ,",
    "the resulting process is still a gaussian process : @xmath68 with the standard expressions for the conditional mean and covariance :    @xmath69\\\\ = & m_{{\\boldsymbol{\\beta}}}({\\mathbf{v}}_{pred})+\\sigma_{{\\boldsymbol{\\psi}}}({\\mathbf{v}}_{pred},{\\mathbf{d}}_n)^{\\textrm{t}}\\sigma_{{\\boldsymbol{\\psi}}}({\\mathbf{d}}_n)^{-1}\\big[y({\\mathbf{d}}_n ) -m_{{\\boldsymbol{\\beta}}}({\\mathbf{v}}_{pred})\\big]\\,,\\end{aligned}\\ ] ]    and @xmath70    the gpe is given by the conditional process ( [ eq6 ] ) which yields a stochastic prediction of the code for any input @xmath62 of the input space @xmath56 .",
    "in the case where @xmath62 belongs to @xmath59 , the gpe interpolates the simulations @xmath65 , that is for @xmath71 @xmath72 and @xmath73 which is expected for such an emulator of a deterministic computer code .",
    "lastly , the capability of a gpe to predict well the code should be checked thanks to some validation criteria @xcite . for more details about the gpe ,",
    "refer to @xcite .",
    "other more theoretical references dedicated to asymptotic properties are @xcite and @xcite .",
    "[ [ calibration - using - a - gpe ] ] calibration using a gpe + + + + + + + + + + + + + + + + + + + + + + +    in equation ( [ mc ] ) , the simulations @xmath74 are replaced with a gpe constructed from a design of experiments @xmath59 .",
    "let ,    * @xmath75 be the mean vector of the gaussian process evaluated in each location of @xmath59 , * @xmath76 and @xmath77 be the mean vector and the correlation matrix of the gaussian process , each evaluated in @xmath78 * @xmath79 be the correlation matrix between @xmath59 and @xmath80 .",
    "then , we now consider the available data @xmath81 .",
    "the joint likelihood of @xmath28 and @xmath82 , @xmath52 , @xmath83 is given by @xmath84\\ , , \\label{totallike}\\end{gathered}\\ ] ] where @xmath85    in the previous paragraph about the gpe , the parameters @xmath86 , @xmath52 and @xmath54 have been assumed to be known .",
    "if they are not , their estimation should be conducted jointly with @xmath28 based on the full likelihood ( [ totallike ] ) @xcite .",
    "however , inspired from the pioneering work of @xcite , a two - step procedure can be conducted instead . this technique , known as modularization in @xcite",
    ", is still used in a recent work dealing with calibration @xcite .",
    "it first consists in estimating the parameters @xmath86 , @xmath52 and @xmath54 only thanks to the simulations @xmath65 by maximizing the marginal density of @xmath65 , denoted by @xmath87 :    @xmath88\\bigg\\}\\,,\\end{gathered}\\ ] ]    then , the @xmath87 s maximum likelihood estimates ( mles ) @xmath89 of @xmath90 are plugged into the likelihood of @xmath16 conditional to the simulations @xmath65 , denoted below by @xmath91 : @xmath92\\bigg\\}\\end{gathered}\\ ] ] where @xmath93 and @xmath94 this modular approach goes hand - in - hand with intuition whereby the estimation of the calibration parameter @xmath28 should be separated from the estimation of @xmath95 , @xmath86 and @xmath54 because they are of different natures .",
    "@xcite also argue it is not a great loss to estimate @xmath95 , @xmath86 and @xmath54 only thanks to @xmath65 because the number of field measurements @xmath13 is usually much smaller than @xmath60 .",
    "@xcite has put in light a poorer mixing in the mcmc routine based on the full likelihood ( [ totallike ] ) than based on ( [ eq8 ] ) . from now on , the conditional likelihood ( [ eq8 ] ) is refered to as the approximated likelihood .",
    "let @xmath96 denote the approximated posterior distribution induced by ( [ eq8 ] ) .",
    "then , @xmath97 the approximated posterior ( [ bayescalib ] ) and the target posterior ( [ eq4 ] ) are different in that @xmath40 is replaced by the mean vector of the gpe @xmath98 and the conditional covariance matrix @xmath99 is added up to @xmath100 .",
    "contrary to the target posterior ( [ eq4 ] ) , the approximated posterior is cheap to evaluate , enabling to perform an mcmc algorithm in order to estimate @xmath28 .",
    "the first stage of the modular approach neglects the uncertainty of the parameters @xmath95 , @xmath86 and @xmath54 by fixing them to their mle .",
    "a possible manner to take in account their uncertainty would consist in adopting a bayesian inference of @xmath95 , @xmath86 and @xmath54 in the same way than @xmath28 .",
    "for instance , if a jeffreys prior distribution is specified on @xmath101 , then the distribution of the gpe follows a student distribution @xcite . yet",
    "unfortunately , the conditional likelihood ( [ eq8 ] ) has no closed - form expression anymore , causing an additional issue which is not the core of this paper .",
    "[ [ in - presence - of - a - code - discrepancy - bmathbfx ] ] in presence of a code discrepancy @xmath27 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the calibration setting ( [ eq4 ] ) has to be modified to prevent overfitting of @xmath28 , as showed in @xcite . in @xcite and @xcite ,",
    "@xmath27 is modeled by a zero mean gaussian process @xmath102 where @xmath45 corresponds here to an input @xmath3 . in this case , the logarithm of the approximated likelihood ( [ eq8 ] ) is now proportional to a weighted sum of squares of the residuals : @xmath103 where @xmath104 .",
    "sometimes , physical context helps us to fixed both @xmath105 and @xmath106 to plausible values , as done in @xcite . in such cases , @xmath107 becomes known and the algorithms that we present in section [ sec : adaptivedesign ] will be still practicable based on ( [ mcp ] ) instead of ( [ mc ] ) .    [ [ main - goal - of - the - paper ] ] main goal of the paper + + + + + + + + + + + + + + + + + + + + + +    our work focuses on reducing the distance between the approximated posterior distribution ( [ bayescalib ] ) and the target posterior distribution ( [ eq4 ] ) .",
    "the kullback - leibler ( kl ) divergence shows interesting theoretical properties to measure how far is a probability distribution from a reference one @xcite .",
    "it is written as @xmath108    [ [ the - design - of - experiments - mathbfd_n ] ] the design of experiments @xmath59 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    by using results of approximation theory , we can prove the proposition below .",
    "[ propkl ] under the following assumptions :    * @xmath109 has a bounded support @xmath33 , * the code output @xmath22 is uniformly bounded on @xmath110 , * the correlation function ( kernel ) is a classical radial basis function @xcite , * @xmath42 lies in the associated reproducing kernel hilbert space , * the covering distances associated with the sequence of designs @xmath111 tends to @xmath112 with @xmath113 ( see appendix ) ,    then , we have : @xmath114    ( [ robust ] ) results from the uniform convergence of @xmath115 to @xmath112 over @xmath33 when @xmath60 tends to @xmath116 ( see appendix ) .",
    "then , we can exchange limit and integral in ( [ kldiv ] ) , which completes the proof .",
    "this proposition reflects the fact that the calibration based on the approximated posterior ( [ bayescalib ] ) is consistent , in other words the larger @xmath59 the closer the approximated posterior ( [ bayescalib ] ) to the target posterior ( [ eq4 ] ) .",
    "however , when @xmath60 is small according to the dimension of the input space @xmath56 , ( [ bayescalib ] ) can be significantly different from ( [ eq4 ] ) leading to a large kl divergence ( [ kldiv ] ) .",
    "although an approximate posterior distribution constructed from an accurate gpe is likely to yield a small value of the kl - divergence ( [ kldiv ] ) , our own experience has shown such behavior is not systematic .    in practical use , @xmath59 is built as a space - filling design on the input space @xmath56 @xcite , and thus not taking into account that @xmath3 and @xmath6 have different roles to play in calibration .",
    "in fact , our interest is not to predict well the computer code over @xmath56 but rather to minimize the kl - divergence ( [ kldiv ] ) , which can be developed as    @xmath117    where @xmath118 and @xmath119 according to equation ( [ kl_expression_dev ] ) , the kl - divergence could be minimized by reducing the uncertainty of the gpe over input locations in the subspace @xmath120 and above all where the target posterior distribution @xmath121 is high . in section [ sec : adaptivedesign ] , we propose new algorithms for building proper numerical designs @xmath59 designed for this purpose .",
    "to identify the global minimum of a costly black box code , denoted by @xmath122 ( to avoid confusion with the calibration setting ) , the most efficient strategies are based on the expected improvement ( ei ) criterion @xcite .",
    "they consist in identifying sequentially the input locations where the code @xmath122 should be run to be close to the global minimum , which is relevant when only a small number of simulations is allocated . assuming @xmath123 simulations @xmath124 have already been run",
    ", the ei criterion assesses the expected improvement of a new run ( numbered @xmath125 ) in terms of getting close to the unknown global minimum of @xmath122 .",
    "let @xmath126 be the input where the ei value is at its highest , meaning that , @xmath127}\\,,\\end{aligned}\\ ] ] where    * @xmath128 is the current gpe which is built from @xmath129 , * @xmath130 is the current value for the minimum .",
    "if a deterministic emulator were used instead of @xmath131 , for instance the mean @xmath132 of @xmath131 , the ei criterion would be just the difference @xmath133 if @xmath134 and @xmath112 if @xmath135 .",
    "given @xmath131 is stochastic , equation ( [ eq9 ] ) is written as the expectation of this truncated difference with respect to the distribution of @xmath136 .",
    "the algorithm that consists of running the code at the input @xmath126 then updating the emulator and starting again is called efficient global optimization ( ego ) @xcite .",
    "the convergence of the ego algorithm to the global minimum of @xmath122 has been proven with respect to some assumptions about both the smoothness of the code and the correlation function of the gpe @xcite . in current use",
    ", the algorithm is stopped when the number of allocated simulations is over or when the improvement of @xmath137 becomes negligible . according to this last criterion ,",
    "ego requires less simulations than other optimization methods with comparable levels of performance @xcite .    [",
    "[ ei - designed - for - calibration ] ] ei designed for calibration + + + + + + + + + + + + + + + + + + + + + + + + + + +    our contribution now consists in resorting to the ei criterion for the sum of squares of the residuals function @xmath138 : @xmath139\\,\\,\\,\\in\\,\\,\\,[0,m_k],\\ ] ] where    * @xmath140 and @xmath141 denotes the sum of squares computed from actual runs of the computer code @xmath42 , * @xmath142 denotes the sum of squares of the residuals where @xmath42 is replaced with the gpe conditional to @xmath143 , denoted by @xmath144    where the subscript @xmath123 now refers to the current iteration of the algorithm .",
    "@xmath142 is thus a random process and its distribution inherits from the current gpe . at step @xmath123 ,",
    "@xmath145 new simulations need to be run to update @xmath137 .",
    "hence , the design @xmath129 contains all the simulations @xmath146 for all @xmath147 and @xmath148 ( do not confound with the notation in section @xmath7 where @xmath60 has referred to the number of simulations ) .",
    "let @xmath149 be the maximum of ( [ ei_calib ] ) : @xmath150    [ [ justification - for - minimizing - ssboldsymboltheta ] ] justification for minimizing @xmath138 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    as explained at the end of the previous section , running the code over input locations in @xmath151 where @xmath121 is high reduces the distance between @xmath152 and @xmath121 .",
    "in addition , when no expert knowledge is available on the value of @xmath28 ( except perhaps a lower and an upper bound ) , a noninformative prior should be specified for @xmath28 @xcite . in such cases , the higher the target posterior distribution @xmath121 , the lower @xmath138 . the kl - divergence ( [ kldiv ] ) can be then reduced faster by running the code over the input locations @xmath153 where @xmath154 is small .",
    "such locations @xmath155 can be identified by maximizing the ei criterion .    in some works",
    ", calibration consists in minimizing a gap between the code outputs and the available field measurements , such as @xmath138 @xcite or a weighted sum of squares like ( [ mcp ] ) @xcite . for our part ,",
    "the ei criterion applied to @xmath138 is just a circuitous way to efficiently build @xmath59 for bayesian calibration .",
    "[ [ ego - algorithms ] ] ego algorithms + + + + + + + + + + + + + +    algorithm @xmath156 corresponds to an exact ego algorithm based on equation ( [ ei_calib ] ) .",
    "it aims at identifying the input locations @xmath153 which will be added up sequentially to an initial design @xmath157 for @xmath60 iterations .",
    "algorithm @xmath7 is a one at time algorithm and should be understood as an approximation of algorithm @xmath156 .    ' '' ''    algorithm @xmath156    ' '' ''    * initialization *    * build a maximin latin hypercube design ( lhd ) @xmath158 of size @xmath159 .",
    "* run the code over @xmath157 . *",
    "compute @xmath160 as the maximum of @xmath161 . *",
    "* update the gaussian process distribution after running the code over @xmath163 . *",
    "set @xmath164 .    * from @xmath165 ,",
    "repeat the following steps as long as @xmath166 .",
    "*    * step @xmath167 * find an estimate @xmath168 of @xmath169 .",
    "* step @xmath170 * @xmath171 .",
    "* step @xmath172 * run the code over all new locations @xmath173 .    * step @xmath174 * update the gaussian process distribution based on @xmath175 .",
    "* step @xmath176 * let @xmath177 .    ' '' ''    because the distribution of the gpe is updated at step @xmath9 , the hyper - parameters @xmath86 , @xmath52 and @xmath54 are re - estimated , as done in the seminal ego work @xcite .",
    "algorithm @xmath156 could be efficiently performed by running the new simulations at step @xmath8 simultaneously on several computer nodes .",
    "we present below the steps of a one - at - a - time algorithm well - suited when the computer system has a single processor .    ' '' ''    algorithm @xmath7    ' '' ''    initialization is similar to algorithm @xmath156 except that @xmath178 .",
    "* for @xmath179 , repeat the same steps as in algorithm @xmath156 except that step @xmath8 is replaced with step @xmath180 and step @xmath10 is replaced with step @xmath181 .",
    "*    * step @xmath182 * run the code in @xmath183 where @xmath184 ( see equations ( [ firstcrit ] ) and ( [ second_crit ] ) below ) .    * step @xmath185 * @xmath186,\\cdots,\\ee[ss^{k}({\\boldsymbol{\\hat{\\theta}}}_{k})],\\ee[ss^{k}({\\boldsymbol{\\hat{\\theta}}}_{k+1})]\\}}$ ]    ' '' ''    algorithm @xmath7 should be understood as an approximation of algorithm @xmath156 where only a single simulation @xmath187 is run at each iteration . in the following , two criteria are proposed to pick up @xmath188 among @xmath189 . as the current minimum @xmath137 in algorithm @xmath156 can not be computed anymore , we have replaced it with the minimum of the expectations @xmath190 $ ] for @xmath191 with respect to @xmath192 .",
    "the first criterion is done to run the code at the input location @xmath193 where the variance of @xmath192 is at the highest .",
    "hence ,    @xmath194\\,.\\ ] ]    as the variance of the gaussian process decreases on the space @xmath151 , the approximated posterior distribution ( [ bayescalib ] ) comes close to the target posterior distribution ( [ eq4 ] ) , which justifies ( [ firstcrit ] ) .",
    "yet , a better way might perhaps consist in aiming for a reduction of the gpe uncertainty at an input location @xmath195 where the code @xmath196 is highly variable over @xmath28 , meaning that @xmath188 is important for calibration .",
    "we thus introduce a second criterion which does a trade - off between the calibration goal and ( [ firstcrit ] ) . a normalized version of it is written as    @xmath197}{\\max\\limits_{i=1,\\cdots , n}\\vv[y_{{\\boldsymbol{\\theta}}}({\\mathbf{x}}_{f}^{i})]},\\ ] ]    where @xmath198 $ ] is taken with respect to @xmath109 .",
    "since the code is unknown , an approximation of ( [ second_crit_exact ] ) can be based on the mean of @xmath199 :    @xmath200}{\\max\\limits_{i=1,\\cdots , n}\\vv[\\mu_{\\beta}^{k}({\\mathbf{x}}_f^i,{\\boldsymbol{\\theta}})]}\\ , .",
    "\\label{second_crit}\\ ] ]    a typical problem inherent to sequential designs is when two input locations come very close , making the covariance matrix numerically singular and thus difficult to invert . this issue can arise when both @xmath201 is too close to a previous iteration @xmath202 and @xmath188 is almost the same at iteration @xmath123 , then at @xmath203 . the usual way to circumvent this issue consists in adding a small diagonal matrix to the covariance matrix @xmath204 of the gpe , called nugget effect .",
    "the expectation in ( [ ei_calib ] ) is taken with respect to the multivariate gaussian distribution induced by the distribution of the gpe . another way",
    "would have been to emulate the function @xmath138 as a gaussian process , forcing @xmath13 simulations to be run at each iteration .",
    "doing so , one at a time algorithms would have been impossible , making this strategy irrelevant for large values of @xmath13 .",
    "[ [ computation - of - the - criterion ] ] computation of the criterion + + + + + + + + + + + + + + + + + + + + + + + + + + + +    by expanding equation ( [ ei_calib ] ) , we have :    @xmath205-\\frac{\\ee[ss^{k}({\\boldsymbol{\\theta}}){\\mathbf{1}}_{ss^{k}({\\boldsymbol{\\theta}})\\leq m_k}]}{m_k}\\bigg]>0\\,,\\ ] ]    implying @xmath206\\leq m_k \\pp[ss^{k}({\\boldsymbol{\\theta}})<m_k].\\ ] ] except in the trivial case @xmath207 , no closed form can be calculated for ( [ ei_calib2 ] ) .",
    "the expression of @xmath208 is proportional to the sum of @xmath209 $ ] which is the probability of sampling inside the hypersphere @xmath210 from a multivariate gaussian distribution and a second term which is the expectation of the right truncated @xmath211 with respect to @xmath137 .",
    "the first term can be calculated either as an infinite series in central chi - square distribution @xcite or thanks to an advanced sampling rejection method @xcite .",
    "this second method should be preferably performed because the second term has to be estimated using mcmc .",
    "the minimization of ( [ ei_calib2 ] ) can be performed in a greedy fashion where @xmath168 is taken as the value which maximizes @xmath208 over a grid @xmath212 .",
    "however , the computation of @xmath208 could be avoided for some candidates of @xmath213 , as explained hereafter .    ' '' ''    computation of @xmath168    ' '' ''",
    "compute @xmath214^{n}]$ ] which is an upper bound of @xmath215 $ ] for each @xmath28 of @xmath213 , 2 .",
    "let @xmath216^{n}]$ ] be a reference value .",
    "3 .   compute @xmath217 , 4 .   build the sub - grid @xmath218^{n}]\\}\\subset g$ ] ,",
    "5 .   compute @xmath208 for the values of the sub - grid @xmath219 , 6 .",
    "let @xmath220 .    for @xmath221 , we have @xmath222^{n}]$ ] implying @xmath223 .",
    "hence , there is no need to compute @xmath208 .",
    "unfortunately , this algorithm is only relevant when @xmath13 is small because in higher dimensions , the hypercube @xmath224^{n}$ ] has a much larger volume than the hypersphere .",
    "such a greedy optimization works very well on our toy examples , especially in small dimensions of @xmath33 because @xmath213 can be constructed fine enough ( see section [ simu ] ) . in higher dimensions ,",
    "the ei criterion could be maximized alternatively over several different grids as iterations of the ego algorithm ( see section @xmath9 ) . however ,",
    "if @xmath213 is coarser , we can expect our algorithms stay efficient in terms of reducing the kl - divergence because they do not aim at converging precisely to the global minimum of @xmath138 , but rather identifying the area of the input space where @xmath138 is small .",
    "on @xmath225 $ ] for several values of @xmath226 $ ] .",
    "red dots are the field measurements @xmath227 generated by equation ( [ data ] ) .",
    "right : the target posterior distribution._,title=\"fig : \" ]   on @xmath225 $ ] for several values of @xmath226 $ ] .",
    "red dots are the field measurements @xmath227 generated by equation ( [ data ] ) .",
    "right : the target posterior distribution._,title=\"fig : \" ] [ ftest ]    [ [ a-2d - example ] ] a 2d example + + + + + + + + + + + +    let us assume that the computer code is given by the following function : @xmath228 where @xmath229 $ ] and @xmath230 $ ] . for @xmath18 ,",
    "the field data @xmath16 are generated by @xmath231 where @xmath232 and @xmath233 .",
    "bayesian calibration of the function ( [ analyticf ] ) is done by sampling the target posterior distribution ( [ eq4 ] ) ( see figure [ ftest ] ) where the prior distribution @xmath234 is chosen as uniform on @xmath235 $ ] : @xmath236}\\,.\\ ] ] now , bayesian calibration of ( [ analyticf ] ) is done by sampling the approximated posterior distribution ( [ bayescalib ] ) .",
    "two cases are considered : with @xmath237 field measurements , then with @xmath238 field measurements .    ) from two different maximin lhd ( using the r library mcmcpack)._,title=\"fig : \" ] ) from two different maximin lhd ( using the r library mcmcpack)._,title=\"fig : \" ] [ maxinr ]    [ [ case - mathbf1-mathbfx_f0.10.30.8 ] ] case @xmath167 : @xmath239 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the approximated posterior distribution ( [ bayescalib ] ) is sampled from a gpe with a constant mean @xmath240 and a matern @xmath241 correlation function .",
    "all the parameters @xmath242 have been estimated under the modular approach based on a maximin lhd of size @xmath243 constructed with the r library dicedesign .",
    "this calibration procedure is repeated twice by sampling ( [ bayescalib ] ) from two different maximin lhd .",
    "according to the first one , as illustrated in figure  [ maxinr ] ( left ) , the regions of high posterior density corresponds to those of the target posterior distribution but the two modes are reversed in terms of height . according to the second one , as illustrated in figure  [ maxinr ] ( right ) ,",
    "the posterior distribution is very erroneous .",
    "such calibrations where the approximated posterior distribution is not close to the target one are unwanted , which justifies constructing some oriented designs for calibration instead of default space filling designs such as maximin lhd . in the following ,",
    "three adaptive strategies building on the ego algorithms from section [ sec : adaptivedesign ] are presented :    1 .   a first version based on algorithm @xmath156 .",
    "at each iteration , the code is run at all three inputs @xmath244 where @xmath245 is the current parameter maximizing the ei criterion .",
    "an example of a design obtained with this algorithm is displayed in figure  [ designs ] ( upper right ) .",
    "2 .   a second version based on algorithm @xmath7 .",
    "the code is run at a single input @xmath246 where @xmath247 comes from the input @xmath248 having the highest variance ( [ firstcrit ] ) among the three inputs @xmath244 .",
    "an example of a design obtained with this algorithm is displayed in figure  [ designs ] ( bottom left ) .",
    "3 .   a third version based on algorithm @xmath7 .",
    "the code is run at a single input @xmath246 where @xmath247 comes from the input @xmath248 which maximizes the criterion ( [ second_crit ] ) among the three inputs @xmath244 .",
    "an example of a design obtained with this algorithm is displayed in figure  [ designs ] ( bottom right )    .",
    "upper left : a maximin lhd ( @xmath243 ) .",
    "upper right : a sequential design built from version @xmath156 ( @xmath249 ) .",
    "bottom left : a sequential design built from version @xmath7 ( @xmath249 ) .",
    "bottom right : a sequential design built from version @xmath8 ( @xmath249 ) .",
    "the black dots are the initial design .",
    "the red stars are the new experiments selected from the ei criterion._,title=\"fig : \" ] .",
    "upper left : a maximin lhd ( @xmath243 ) .",
    "upper right : a sequential design built from version @xmath156 ( @xmath249 ) .",
    "bottom left : a sequential design built from version @xmath7 ( @xmath249 ) .",
    "bottom right : a sequential design built from version @xmath8 ( @xmath249 ) .",
    "the black dots are the initial design .",
    "the red stars are the new experiments selected from the ei criterion._,title=\"fig : \" ]    .",
    "upper left : a maximin lhd ( @xmath243 ) .",
    "upper right : a sequential design built from version @xmath156 ( @xmath249 ) .",
    "bottom left : a sequential design built from version @xmath7 ( @xmath249 ) .",
    "bottom right : a sequential design built from version @xmath8 ( @xmath249 ) .",
    "the black dots are the initial design .",
    "the red stars are the new experiments selected from the ei criterion._,title=\"fig : \" ] .",
    "upper left : a maximin lhd ( @xmath243 ) .",
    "upper right : a sequential design built from version @xmath156 ( @xmath249 ) .",
    "bottom left : a sequential design built from version @xmath7 ( @xmath249 ) .",
    "bottom right : a sequential design built from version @xmath8 ( @xmath249 ) .",
    "the black dots are the initial design .",
    "the red stars are the new experiments selected from the ei criterion._,title=\"fig : \" ] [ designs ]    let us now assess how good is the calibration by computing @xmath250 according to the kind of design @xmath59 which is used .",
    "the proportion of the time that the @xmath251 credibility interval of ( [ bayescalib ] ) covers the true value is computed as well .",
    "the robustness of the results is checked by repeating sampling of the approximated posterior distribution ( [ bayescalib ] ) many times . here",
    ", calibration is performed from @xmath252 data sets @xmath253 and for each of them , @xmath252 calibrations derived from @xmath252 different designs have been conducted . in figure",
    "[ distributions ] , the boxplots of the kl divergence and the boxplots of the coverage are each plotted against the kind of design ( including the maximin lhd and the sequential designs coming from the ego algorithms ) .",
    "each value of the boxplot is computed as the mean of the criterion over the @xmath252 calibrations derived from a particular data set @xmath253 . a sharp decrease in the kl divergence",
    "is noticed when the calibration is done with a sequential design .",
    "we can see that both versions @xmath7 and @xmath8 have the lowest values for this criterion .",
    "the reason is that algorithm @xmath7 can move more quickly inside the parameter space @xmath33 , which is expected when the target posterior distribution is multimodal .",
    "results about coverage look the same although some lower values can be seen . in such cases ,",
    "the coverage is poor because the true value ( @xmath232 ) is not covered by the @xmath251 interval of the target posterior distribution .",
    "the same feature is thus expected with the approximated posterior distribution .    .",
    "left : boxplots of the kl divergence computed between the target posterior distribution and the approximated posterior distribution ( using the r library fnn ) .",
    "right : ability of the @xmath251 credibility interval to cover the true value ( @xmath232)._,title=\"fig : \" ] . left : boxplots of the kl divergence computed between the target posterior distribution and the approximated posterior distribution ( using the r library fnn ) .",
    "right : ability of the @xmath251 credibility interval to cover the true value ( @xmath232)._,title=\"fig : \" ] [ distributions ]    ) _ , title=\"fig : \" ] [ ftest9 ]    [ [ case-2-mathbfx_f0.10.20.30.40.50.60.70.80.9 ] ] case @xmath7 : @xmath254 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the field data @xmath16 are still generated by @xmath255 where @xmath232 and @xmath256 .",
    "as @xmath13 is larger , the target posterior distribution @xmath257 now has a single narrow mode around the true value .",
    "similar to the first case , the calibration results are improved by using adaptive designs ( see figure [ distributions9 ] ) .",
    "as the number of field data is larger , the one - at - a - time versions @xmath7 and @xmath8 outperform version @xmath156 because they do not need all the evaluations corresponding to a @xmath258 around @xmath259 to discard this area , and evaluations remain to refine the sampling around @xmath232 ( see figure [ designs9 ] ) .    .",
    "upper left : a maximin lhd ( @xmath243 ) .",
    "upper right : a sequential design built from version @xmath156 ( @xmath249 ) .",
    "bottom left : a sequential design built from version @xmath7 ( @xmath249 ) .",
    "bottom right : a sequential design built from version @xmath8 ( @xmath249 ) .",
    "the black dots are the initial design .",
    "the red stars are the new experiments selected from the ei criterion._,title=\"fig : \" ] .",
    "upper left : a maximin lhd ( @xmath243 ) .",
    "upper right : a sequential design built from version @xmath156 ( @xmath249 ) .",
    "bottom left : a sequential design built from version @xmath7 ( @xmath249 ) .",
    "bottom right : a sequential design built from version @xmath8 ( @xmath249 ) .",
    "the black dots are the initial design .",
    "the red stars are the new experiments selected from the ei criterion._,title=\"fig : \" ]    .",
    "upper left : a maximin lhd ( @xmath243 ) .",
    "upper right : a sequential design built from version @xmath156 ( @xmath249 ) .",
    "bottom left : a sequential design built from version @xmath7 ( @xmath249 ) .",
    "bottom right : a sequential design built from version @xmath8 ( @xmath249 ) .",
    "the black dots are the initial design .",
    "the red stars are the new experiments selected from the ei criterion._,title=\"fig : \" ] .",
    "upper left : a maximin lhd ( @xmath243 ) .",
    "upper right : a sequential design built from version @xmath156 ( @xmath249 ) .",
    "bottom left : a sequential design built from version @xmath7 ( @xmath249 ) .",
    "bottom right : a sequential design built from version @xmath8 ( @xmath249 ) .",
    "the black dots are the initial design .",
    "the red stars are the new experiments selected from the ei criterion._,title=\"fig : \" ] [ designs9 ]    .",
    "left : boxplots of the kl divergence computed between the target posterior distribution and the approximated posterior distribution ( using the r library fnn ) .",
    "right : ability of the @xmath251 credibility interval to cover the true value ( @xmath232)._,title=\"fig : \" ] . left : boxplots of the kl divergence computed between the target posterior distribution and the approximated posterior distribution ( using the r library fnn ) .",
    "right : ability of the @xmath251 credibility interval to cover the true value ( @xmath232)._,title=\"fig : \" ] [ distributions9 ]    [ [ a-6d - example ] ] a @xmath260d example + + + + + + + + + + + + + + + + + + + + + + + + + +    inspired from @xcite , let us now assume that the computer code is given by the following function : @xmath261 this highly non - linear function is used within the field of global sensitivity analysis to assess the efficiency of new methods @xcite .",
    "for @xmath262 , the field data @xmath16 are still generated by @xmath263 where @xmath264 and @xmath265 . here",
    ", both @xmath3 and @xmath28 are three - dimensional vectors .",
    "in the same spirit as before , we aim at reducing the calibration error between the unknown target posterior ( [ eq4 ] ) and the approximated posterior ( [ bayescalib ] ) .",
    "the prior distribution @xmath109 is chosen as uniform on @xmath225^{3}$ ] : @xmath266^{3}}.\\ ] ] let the allocated number of simulations be equal to @xmath267 .",
    "similar to the @xmath7d example , maximin lhd are compared with the sequential designs .",
    "given the size of field data ( @xmath268 ) , only version @xmath7 and version @xmath8 are performed , both starting from an initial design of size @xmath269 simulations . as explained in section [ sec : adaptivedesign ] , a discretization @xmath213 of the parameter space is required for maximizing the ei criterion .",
    "given that the dimension of @xmath28 is larger than in the previous example , the choice of such a grid is more sensitive .",
    "indeed , if @xmath213 is too coarse , some promising area of the parameter space could be not explored whereas if @xmath213 is too fine , the computation time will be drastically increased .",
    "the solution that we suggest to address this problem consists in maximizing the ei criterion alternatively on several designs , as iterations , so that : @xmath270 and @xmath271 in this example , for simplicity we have chosen @xmath272 grids and , @xmath273\\times[0,0.2,0.4,0.6,0.8,1]\\times[0,0.2,0.4,0.6,0.8,1]\\ ] ] and @xmath274\\times[0.1,0.3,0.5,0.7,0.9]\\times[0.1,0.3,0.5,0.7,0.9]\\,.\\ ] ] thus , for odd iterations the ei criterion is maximized over @xmath275 whereas for even iterations the ei criterion is maximized over @xmath276 .",
    "the calibration results are illustrated in figure [ kl_6d ] .",
    "they look the same as those of the @xmath7d example , which again support the advantage of using a sequential design .",
    "let us see that neither @xmath275 nor @xmath276 covers the unknown true value .",
    "one can think that a more exhaustive decomposition in ( [ fine ] ) would make @xmath213 closer to the true value @xmath264 , perhaps making the results even better .",
    "boxplots for coverage rate have larger variance than in the previous examples because each of the @xmath251 credibility interval of the three marginal posterior distribution needs to cover the true value @xmath277 .",
    "_ , title=\"fig : \" ]   calibrations .",
    "_ , title=\"fig : \" ] [ kl_6d ]",
    "this paper deals with new adaptive numerical designs for calibrating time - consuming computer codes in a bayesian setting . for such codes ,",
    "bayesian calibration is based on a gaussian process emulator ( gpe ) which approximates the code output and thus making the mcmc algorithms practicable . after choosing a design of experiments ,",
    "the gpe is estimated by using a modular approach which allows us to separate the estimation of gpe parameters from the estimation of the code parameters .",
    "our contribution consists in taking advantage of the stochastic property of the gpe to build sequentially the design of experiments in such a way that the gap between the posterior distribution based on the gpe ( the so - called approximated posterior distribution ) and the posterior distribution based on the code ( the so - called target posterior distribution ) is the smallest possible in terms of the kl - divergence .",
    "we have shown it is of a great importance to reduce the uncertainty of the gpe where the density of the target posterior distribution is large . in an objective bayesian context where no prior expertise is available about the unknown parameters , this goal is equivalent to reduce the uncertainty of the gpe where the sum of squares of the residuals between the code outputs and the field measurements is small .",
    "we have thus proposed sequential strategies for building the design of experiments based on the ei criterion designed for the sum of squares of the residuals .",
    "our simulations on toy functions have shown such designs outperform space - filling designs in terms of low value of the kl- divergence .",
    "however , simulations have been performed in an unbiased framework where there is no discrepancy between the physical system and the computer code .",
    "if prior information is available about the shape of the code discrepancy , our algorithms could be applied to a weighted sum of squares function in a similar way .",
    "it may appear surprising that the prior distribution is not taken into account in the ei maximization .",
    "that is because the prior distribution is identical both for the approximated posterior distribution and the target posterior distribution .",
    "furthermore , if we aim at conducting a sensitivity analysis to the choice of the prior distribution , then we would like to be able to perform several calibrations from different prior distributions without having to rebuild a gpe each time .",
    "the main difference between the algorithms presented in this paper and the current use of the ego algorithm is that the objective function , that is the sum of squares of the residuals , is not modeled by a gaussian process .",
    "such modeling makes possible to conduct one - at - a - time sequential strategies where a single simulation @xmath278 is run at each iteration .",
    "two criteria have been suggested to pick up @xmath279 among the input field measurements but more sophisticated criteria might be more relevant .",
    "in addition , we might be also think about an ego algorithm which would be designed according to the number of available computers nodes .",
    "another concern is how to maximize the ei criterion over the parameter space .",
    "because it has no closed form expression , the optimization is performed in a greedy fashion , but free - derivative optimization algorithms could be also used @xcite .    finally , our ego algorithms start from a maximin lhd on @xmath56 whereas the posterior distribution only depends on the simulations running in the space @xmath151",
    ". a better strategy could be to build a space filling design on this input space .",
    "the authors of the paper want to thank joan sobota for correcting english mistakes .",
    "this work was supported by the research contract cifre @xmath280 between lectricit de france and agroparistech .",
    "seccntformat#1appendix  :",
    "we refer to the log - conditional likelihood as @xmath281 and the target log - likelihood as @xmath282 .",
    "it is sufficient to prove that @xmath283 is uniformly bounded in @xmath28 and the bound tends to zero with @xmath284",
    ". we can decompose ( [ log_diff ] ) as @xmath285 corresponds to the log - conditional likelihood where the function @xmath42 is replaced with @xmath286 and the covariance matrix of the gpe is neglected .",
    "the second term is bounded as : @xmath287 let us suppose that the minimax distance , say @xmath288 , of the designs sequence @xmath111 tends to @xmath112 , namely @xmath289 then , the uniform bound is deduced from the point - wise bound given for standard radial basis correlation function @xmath290 @xcite .",
    "we can obtain @xmath291 where @xmath292 is the norm of @xmath42 in the rkhs associated to @xmath290 and @xmath293 tends to @xmath112 when @xmath294 tends to @xmath112 .",
    "we know that the distance is maximum when @xmath295 .",
    "hence , the first term in ( [ twotermskl ] ) is written as , @xmath296 where the inequality of arithmetic and geometric means is used for bounding the determinant by a function of the trace .",
    "using again results in @xcite , we obtain @xmath297 .",
    "therefore , @xmath298 since @xmath288 tends to @xmath112 with @xmath60 , putting the two bounds together proves the uniform convergence of ( [ log_diff ] ) to @xmath112 ."
  ],
  "abstract_text": [
    "<S> making good predictions of a physical system using a computer code requires the inputs to be carefully specified . </S>",
    "<S> some of these inputs called control variables have to reproduce physical conditions whereas other inputs , called parameters , are specific to the computer code and most often uncertain . </S>",
    "<S> the goal of statistical calibration consists in estimating these parameters with the help of a statistical model which links the code outputs with the field measurements . in a bayesian </S>",
    "<S> setting , the posterior distribution of these parameters is normally sampled using mcmc methods . however , they are impractical when the code runs are high time - consuming . a way to circumvent this issue </S>",
    "<S> consists of replacing the computer code with a gaussian process emulator , then sampling a cheap - to - evaluate posterior distribution based on it . </S>",
    "<S> doing so , calibration is subject to an error which strongly depends on the numerical design of experiments used to fit the emulator . </S>",
    "<S> we aim at reducing this error by building a proper sequential design by means of the expected improvement criterion . </S>",
    "<S> numerical illustrations in several dimensions assess the efficiency of such sequential strategies . </S>",
    "<S> +    * keywords : * bayesian calibration , gaussian process emulator , expected improvement criterion . </S>"
  ]
}