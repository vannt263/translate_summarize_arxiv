{
  "article_text": [
    "variable selection is an important issue in regression .",
    "typically , measurements are obtained for a large number of potential predictors in order to avoid missing an important link between a predictive factor and the outcome .",
    "this practice has only increased in recent years , as the low cost and easy implementation of automated methods for data collection and storage has led to an abundance of problems for which the number of variables is large in comparison to the sample size .    to reduce variability and obtain a more interpretable model , we often seek a smaller subset of important variables . however , searching through subsets of potential predictors for an adequate smaller model can be unstable [ @xcite ] and is computationally unfeasible even in modest dimensions .    to avoid these drawbacks ,",
    "a number of penalized regression methods have been proposed in recent years that perform subset selection in a continuous fashion . penalized regression procedures",
    "accomplish this by shrinking coefficients toward zero in addition to setting some coefficients exactly equal to zero ( thereby selecting the remaining variables ) .",
    "the most popular penalized regression method is the lasso [ @xcite ] .",
    "although the lasso has many attractive properties , the shrinkage introduced by the lasso results in significant bias toward 0 for large regression coefficients .",
    "other authors have proposed alternative penalties , designed to diminish this bias .",
    "two such proposals are the smoothly clipped absolute deviation ( scad ) penalty [ @xcite ] and the mimimax concave penalty [ mcp ; @xcite ] . in proposing scad and mcp",
    ", their authors established that scad and mcp regression models have the so - called _ oracle property _ , meaning that , in the asymptotic sense , they perform as well as if the analyst had known in advance which coefficients were zero and which were nonzero .",
    "however , the penalty functions for scad and mcp are nonconvex , which introduces numerical challenges in fitting these models . for",
    "the lasso , which does possess a convex penalty , least angle regression [ lars ; @xcite ] is a remarkably efficient method for computing an entire path of lasso solutions in the same order of time as a least squares fit .",
    "for nonconvex penalties , @xcite have proposed making a local linear approximation ( lla ) to the penalty , thereby yielding an objective function that can be optimized using the lars algorithm .",
    "more recently , coordinate descent algorithms for fitting lasso - penalized models have been shown to be competitive with the lars algorithm , particularly in high dimensions [ @xcite ; @xcite ; @xcite ] . in this paper",
    "we investigate the application of coordinate descent algorithms to scad and mcp regression models , for which the penalty is nonconvex .",
    "we provide implementations of these algorithms through the publicly available ` r ` package , ` ncvreg ` ( available at http://cran.r-project.org ) .",
    "methods for high - dimensional regression and variable selection have applications in many scientific fields , particularly those in high - throughput biomedical studies . in this article",
    "we apply the methods to two such studies  a genetic association study and a gene expression study  each possessing a different motivation for sparse regression models . in genetic association studies , very few genetic markers are expected to be associated with the phenotype of interest . thus , the underlying data - generating process is likely to be highly sparse , and sparse regression methods likely to perform well .",
    "gene expression studies may also have sparse underlying representations ; however , even when they are relatively `` dense , '' there may be separate motivations for sparse regression models .",
    "for example , data from microarray experiments may be used to discover biomarkers and design diagnostic assays . to be practical in clinical settings",
    ", such assays must use only a small number of probes [ @xcite ] .    in section [ section : linear ]",
    "we describe algorithms for fitting linear regression models penalized by mcp and scad , and discuss their convergence . in section",
    "[ sec : logistic ] we discuss the modification of those algorithms for fitting logistic regression models .",
    "issues of stability , local convexity and diagnostic measures for investigating these concerns are discussed , along with the selection of tuning parameters , in section [ section : convexity ] .",
    "the numerical efficiency of the proposed algorithm is investigated in section [ section : sim ] and compared with lla algorithms .",
    "the statistical properties of lasso , scad and mcp are also investigated and compared using simulated data ( section [ section : sim ] ) and applied to biomedical data ( section  [ section : app ] ) .",
    "suppose we have @xmath0 observations , each of which contains measurements of an outcome @xmath1 and @xmath2 features @xmath3 .",
    "we assume without loss of generality that the features have been standardized such that @xmath4 , @xmath5 and @xmath6 .",
    "this ensures that the penalty is applied equally to all covariates in an equivariant manner , and eliminates the need for an intercept .",
    "this is standard practice in regularized estimation ; estimates are then transformed back to their original scale after the penalized models have been fit , at which point an intercept is introduced .",
    "we will consider models in which the expected value of the outcome depends on the covariates through the linear function @xmath7 .",
    "the problem of interest involves estimating the vector of regression coefficients @xmath8 .",
    "penalized regression methods accomplish this by minimizing an objective function @xmath9 that is composed of a loss function plus a penalty function . in this section",
    "we take the loss function to be squared error loss : @xmath10 where @xmath11 is a function of the coefficients indexed by a parameter @xmath12 that controls the tradeoff between the loss function and penalty , and that also may be shaped by one or more tuning parameters @xmath13 .",
    "this approach produces a spectrum of solutions depending on the value of @xmath12 ; such methods are often referred to as regularization methods , with @xmath12 the regularization parameter .    to find the value of @xmath8 that optimizes ( [ eq : q - lin ] ) , the lla algorithm makes a linear approximation to the penalty , then uses the lars algorithm to compute the solution .",
    "this process is repeated iteratively until convergence for each value of @xmath12 over a grid .",
    "details of the algorithm and its implementation may be found in @xcite .",
    "the lla algorithm is inherently inefficient to some extent , in that it uses the path - tracing lars algorithm to produce updates to the regression coefficients .",
    "for example , over a grid of 100 values for @xmath12 that averages 10 iterations until convergence at each point , the lla algorithm must calculate 1000 lasso paths to produce a single approximation to the mcp or scad path .",
    "an alternative to lla is to use a coordinate descent approach .",
    "coordinate descent algorithms optimize a target function with respect to a single parameter at a time , iteratively cycling through all parameters until convergence is reached .",
    "the idea is simple but efficient  each pass over the parameters requires only @xmath14 operations .",
    "if the number of iterations is smaller than @xmath2 , the solution is reached with even less computation burden than the @xmath15 operations required to solve a linear regression problem by qr decomposition .",
    "furthermore , since the computational burden increases only linearly with @xmath2 , coordinate descent algorithms can be applied to very high - dimensional problems .",
    "coordinate descent algorithms are ideal for problems that have a simple closed form solution in a single dimension but lack one in higher dimensions .",
    "the basic structure of a coordinate descent algorithm is , simply : for @xmath16 in @xmath17 , to partially optimize @xmath9 with respect to @xmath18 with the remaining elements of @xmath8 fixed at their most recently updated values . like the lla algorithm ,",
    "coordinate descent algorithms iterate until convergence is reached , and this process is repeated over a grid of values for @xmath12 to produce a path of solutions .",
    "the efficiency of coordinate descent algorithms comes from two sources : ( 1 )  updates can be computed very rapidly , and ( 2 ) if we are computing a continuous path of solutions ( see section  [ section : path ] ) , our initial values will never be far from the solution and few iterations will be required .",
    "rapid updates are possible because the minimization of @xmath9 with respect to @xmath18 can be obtained from the univariate regression of the current residuals @xmath19 on @xmath20 , at a cost of @xmath21 operations .",
    "the specific form of these updates depends on the penalty and whether linear or logistic regression is being performed , and will be discussed further in their respective sections .    in this section",
    "we describe coordinate descent algorithms for least squares regression penalized by scad and mcp , as well as investigate the convergence of these algorithms .",
    "zhang ( @xcite ) proposed the mcp , defined on @xmath22 by @xmath23\\\\[-8pt ] p_{\\lambda,\\gamma}'(\\theta)&=&\\cases { \\displaystyle\\lambda - \\frac{\\theta}{\\gamma},&\\quad if $ \\theta \\leq \\gamma\\lambda$,\\cr 0,&\\quad if $ \\theta > \\gamma\\lambda$}\\nonumber\\end{aligned}\\ ] ] for @xmath24 and @xmath25 .",
    "the rationale behind the penalty can be understood by considering its derivative : mcp begins by applying the same rate of penalization as the lasso , but continuously relaxes that penalization until , when @xmath26 , the rate of penalization drops to 0 .",
    "the penalty is illustrated in figure  [ fig : shapes ] .    .",
    "]    the rationale behind the mcp can also be understood by considering its univariate solution .",
    "consider the simple linear regression of @xmath27 upon @xmath28 , with unpenalized least squares solution @xmath29 ( recall that @xmath28 has been standardized so that @xmath30 ) .",
    "for this simple linear regression problem , the mcp estimator has the following closed form : @xmath31 where @xmath32 is the soft - thresholding operator [ @xcite ] defined for @xmath24 by @xmath33    noting that @xmath34 is the univariate solution to the lasso , we can observe by comparison that mcp scales the lasso solution back toward the unpenalized solution by an amount that depends on @xmath13 . as @xmath35 ,",
    "the mcp and lasso solutions are the same .",
    "as @xmath36 , the mcp solution becomes the hard thresholding estimate @xmath37 .",
    "thus , in the univariate sense , the mcp produces the `` firm shrinkage '' estimator of @xcite .    in the special case of an orthonormal design matrix ,",
    "subset selection is equivalent to hard - thresholding , the lasso is equivalent to soft - thresholding , and mcp is equivalent to firm - thresholding .",
    "thus , the lasso may be thought of as performing a kind of multivariate soft - thresholding , subset selection as multivariate hard - thresholding , and the mcp as multivariate firm - thresholding .",
    "the univariate solution of the mcp is employed by the coordinate descent algorithm to obtain the coordinate - wise minimizer of the objective function . in this",
    "setting , however , the role of the unpenalized solution is now played by the unpenalized regression of @xmath20 s partial residuals on @xmath20 , and denoted @xmath38 . introducing the notation @xmath39 to refer to the portion that remains after the @xmath16th column or element is removed , the partial residuals of @xmath20 are @xmath40 , where @xmath41 is the most recently updated value of @xmath8 .",
    "thus , at step @xmath16 of iteration @xmath42 , the following three calculations are made :    * calculate @xmath43 * update @xmath44 , * update @xmath45 ,    where the last step ensures that @xmath46 always holds the current values of the residuals .",
    "note that @xmath38 can be obtained by regressing @xmath20 on either the partial residuals or the current residuals ; using the current residuals is more computationally efficient , however , as it does not require recalculating partial residuals for each update .",
    "the scad penalty @xcite defined on @xmath22 is given by @xmath47\\\\[-8pt ] p_{\\lambda}'(\\theta ) & = & \\cases { \\lambda , & \\quad if $ \\theta \\leq \\lambda$,\\cr \\displaystyle\\frac{\\gamma\\lambda-\\theta}{\\gamma-1 } , & \\quad if $ \\lambda < \\theta \\leq\\gamma\\lambda$,\\cr 0 , & \\quad if $ \\theta > \\gamma\\lambda$}\\nonumber\\end{aligned}\\ ] ] for @xmath24 and @xmath48 .",
    "the rationale behind the penalty is similar to that of mcp .",
    "both penalties begin by applying the same rate of penalization as the lasso , and reduce that rate to 0 as @xmath49 gets further away from zero ; the difference is in the way that they make the transition .",
    "the scad penalty is also illustrated in figure  [ fig : shapes ] .",
    "the univariate solution for a scad - penalized regression coefficient is as follows , where again @xmath50 is the unpenalized linear regression solution : @xmath51 this solution is similar to , although not the same as , the mcp solution / firm shrinkage estimator .",
    "both penalties rescale the soft - thresholding solution toward the unpenalized solution .",
    "however , although scad has soft - thresholding as the limiting case when @xmath52 , it does not have hard thresholding as the limiting case when @xmath53 .",
    "this univariate solution plays the same role in the coordinate descent algorithm for scad as @xmath54 played for mcp .",
    "we now consider the convergence of coordinate descent algorithms for scad and mcp .",
    "we begin with the following lemma .",
    "[ 1lema ] let @xmath55 denote the objective function @xmath9 , defined in ( [ eq : q - lin ] ) , as a function of the single variable @xmath18 , with the remaining elements of @xmath8 fixed .",
    "for the scad penalty with @xmath56 and for the mcp with @xmath57 , @xmath55 is a convex function of @xmath18 for all @xmath16 .    from this lemma",
    ", we can establish the following convergence properties of coordinate descent algorithms for scad and mcp .",
    "[ prop : cd ] let @xmath58 denote the sequence of coefficients produced at each iteration of the coordinate descent algorithms for scad and mcp . for all @xmath59 @xmath60",
    "furthermore , the sequence is guaranteed to converge to a point that is both a local minimum and a global coordinate - wise minimum of @xmath61 .    because the penalty functions of scad and mcp are not convex , neither the coordinate descent algorithms proposed here nor the lla algorithm are guaranteed to converge to a global minimum in general .",
    "however , it is possible for the objective function @xmath9 to be convex with respect to @xmath8 even though it contains a nonconvex penalty component . in particular , letting @xmath62 denote the minimum eigenvalue of @xmath63 , the mcp objective function is convex if @xmath64 , while the scad objective function is convex if @xmath65 [ @xcite ] .",
    "if this is the case , then the coordinate descent algorithms converge to the global minimum .",
    "section  [ section : convexity ] discusses this issue further with respect to high - dimensional settings .",
    "usually , we are interested in obtaining @xmath66 not just for a single value of @xmath12 , but for a range of values extending from a maximum value @xmath67 for which all penalized coefficients are 0 down to @xmath68 or to a minimum value @xmath69 at which the model becomes excessively large or ceases to be identifiable . when the objective function is strictly convex , the estimated coefficients vary continuously with @xmath70 $ ] and produce a path of solutions regularized by @xmath12 .",
    "examples of such paths may be seen in figures  [ fig : dgn ] and [ fig : app - gamma ] .    because the coefficient paths are continuous ( under strictly convex objective functions ) , a reasonable approach to choosing initial values is to start at one extreme of the path and use the estimate @xmath66 from the previous value of @xmath12 as the initial value for the next value of @xmath12 . for mcp and scad , we can easily determine @xmath67 , the smallest value for which all penalized coefficients are 0 . from ( [ eq : mcp - lin - sol ] ) and ( [ eq : scad - lin - sol ] ) , it is clear that @xmath71 , where @xmath72 [ for logistic regression ( section  [ sec : logistic ] ) @xmath67 is again equal to @xmath73 , albeit with @xmath38 as defined in ( [ eq : logistic - z ] ) and the quadratic approximation taken with respect to the intercept - only model ] .",
    "thus , by starting at @xmath67 with @xmath74 and proceeding toward @xmath69 , we can ensure that the initial values will never be far from the solution .",
    "for all the numerical results in this paper , we follow the approach of @xcite and compute solutions along a grid of 100 @xmath12 values that are equally spaced on the log scale .",
    "for logistic regression , it is not possible to eliminate the need for an intercept by centering the response variable . for logistic regression , then , @xmath27 will denote the original vector of 01 responses .",
    "correspondingly , although @xmath75 is still standardized , it now contains an unpenalized column of 1 s for the intercept , with corresponding coefficient @xmath76 .",
    "the expected value of @xmath27 once again depends on the linear function @xmath77 , although the model is now @xmath78 estimation of the coefficients is now accomplished via minimization of the objective function @xmath79    minimization can be approached by first obtaining a quadratic approximation to the loss function based on a taylor series expansion about the value of the regression coefficients at the beginning of the current iteration , @xmath80 .",
    "doing so results in the familiar form of the iteratively reweighted least squares algorithm commonly used to fit generalized linear models [ @xcite ] : @xmath81 where @xmath82 , the working response , is defined by @xmath83 and @xmath84 is a diagonal matrix of weights , with elements @xmath85 and @xmath86 is evaluated at @xmath80 .",
    "we focus on logistic regression here , but the same approach could be applied to fit penalized versions of any generalized linear model , provided that the quantities @xmath82 and @xmath84 [ as well as the residuals @xmath87 , which depend on @xmath82 implicitly ] are replaced with expressions specific to the particular response distribution and link function .    with this representation , the local linear approximation ( lla ) and coordinate descent ( cd ) algorithms can be extended to logistic regression in a rather straightforward manner . at iteration @xmath42 ,",
    "the following two steps are taken :    approximate the loss function based on @xmath80 .",
    "execute one iteration of the lla / cd algorithm , obtaining @xmath88 .",
    "these two steps are then iterated until convergence for each value of @xmath12 .",
    "note that for the coordinate descent algorithm , step ( 2 ) loops over all the covariates .",
    "note also that step 2 must now involve the updating of the intercept term , which may be accomplished without modification of the underlying framework by setting @xmath68 for the intercept term .",
    "the local linear approximation is extended in a straightforward manner to reweighted least squares by distributing @xmath84 , obtaining the transformed covariates and response variable @xmath89 and @xmath90 , respectively .",
    "the implications for coordinate descent algorithms are discussed in the next section .",
    "briefly , we note that , as is the case for the traditional iteratively reweighted least squares algorithm applied to generalized linear models , neither algorithm ( lla / cd ) is guaranteed to converge for logistic regression .",
    "however , provided that adequate safeguards are in place to protect against model saturation , we have not observed failure to converge to be a problem for either algorithm .",
    "the presence of observation weights changes the form of the coordinate - wise updates .",
    "let @xmath91 , and redefine @xmath92 and @xmath93 now , the coordinate - descent update for mcp is @xmath94 for @xmath95 and for scad , @xmath96 for @xmath97 .",
    "updating of @xmath46 proceeds as in the linear regression case .    as is evident from comparing ( [ eq : mcp - lin - sol])/([eq : scad - lin - sol ] ) with ( [ mcp - log - sol])/([scad - log - sol ] ) , portions of both numerator and denominator are being reweighted in logistic regression . in comparison , for linear regression",
    ", @xmath98 is always equal to 1 and this term drops out of the solution .",
    "this reweighting , however , introduces some difficulties with respect to the choice and interpretation of the @xmath13 parameter . in linear regression ,",
    "the scaling factor by which solutions are adjusted toward their unpenalized solution is a constant [ @xmath99 for mcp , @xmath100 for scad ] for all values of @xmath12 and for each covariate .",
    "furthermore , for standardized covariates , this constant has a universal interpretation for all linear regression problems , meaning that theoretical arguments and numerical simulations investigating @xmath13 do not need to be rescaled and reinterpreted in the context of applied problems .    in logistic regression , however , this scaling factor is constantly changing , and is different for each covariate .",
    "this makes choosing an appropriate value for @xmath13 difficult in applied settings and robs the parameter of a consistent interpretation .    to illustrate the consequences of this issue , consider an attempt to perform logistic regression updates using @xmath101 , the value suggested for linear regression in @xcite . because @xmath102 can not exceed 0.25 , @xmath13",
    "can not exceed @xmath103 and the solution is discontinuous and unstable .",
    "note that this problem does not arise from the use of any particular algorithm ",
    "it is a direct consequence of the poorly behaved objective function with this value of @xmath13 .      to resolve these difficulties",
    ", we propose an adaptive rescaling of the penalty parameter @xmath13 to match the continually changing scale of the covariates .",
    "this can be accomplished by simply replacing @xmath104 with @xmath105 .",
    "the algorithmic consequences for the lla algorithm are straightforward . for coordinate descent ,",
    "the updating steps become simple extensions of the linear regression solutions : @xmath106    note that , for mcp , @xmath107 where @xmath108 .",
    "thus , the adaptively rescaled solution is still minimizing the objective function ( [ eq : log - obj - quad ] ) , albeit with an alternate set of shape parameters @xmath109 that are unknown until convergence is attained .    note",
    "that rescaling by @xmath98 does not affect the magnitude of the penalty ( @xmath12 ) , only the range over which the penalty is applied ( @xmath13 ) .",
    "is it logical to apply different scales to different variables ?",
    "keep in mind that , since @xmath110 for all @xmath16 , the rescaling factor @xmath98 will tend to be quite similar for all covariates .",
    "however , consider the case where a covariate is predominantly associated with observations for which @xmath111 is close to 0 or 1 .",
    "for such a covariate , adaptive rescaling will extend the range over which the penalization is applied .",
    "this seems to be a reasonable course of action , as large changes in this coefficient produce only small changes in the model s fit , and provide less compelling evidence of a covariate s importance .",
    "scad does not have the property that its adaptively rescaled solution is equal to a solution of the regular scad objective function with different shape parameters .",
    "this is due to the fact that the scale of the penalty is tied to the scale of the coefficient by the @xmath112 clause .",
    "one could make this clause more flexible by reparameterizing scad so that @xmath113 in the region @xmath114 , where @xmath115 would be an additional tuning parameter . in this generalized case ,",
    "adaptively rescaled scad would minimize a version of the original objective function in which the @xmath13 parameters are rescaled by @xmath98 , as in the mcp case .    as we will see in section [ section : sim ] , this adaptive rescaling increases interpretability and makes it easier to select @xmath13 .",
    "as mentioned in section  [ section : convergence ] , the mcp / scad - penalized linear regression objective function is convex provided that @xmath116 ( mcp ) or @xmath117 ( scad ) .",
    "this result can be extended to logistic regression as well via the following proposition .",
    "[ prop : logistic - convex ] let @xmath118 denote the minimum eigenvalue of @xmath119 , where @xmath84 is evaluated at @xmath8 . then the objective function defined in ( [ eq : log - obj ] ) is a convex function of @xmath8 on the region where @xmath120 for mcp , and where @xmath121 for scad .",
    "a convex objective function is desirable for ( at least ) two reasons .",
    "first , for any algorithm ( such as coordinate descent ) which converges to a critical point of the objective function , convexity ensures that the algorithm converges to the unique global minimum .",
    "second , convexity ensures that @xmath66 is continuous with respect to @xmath12 , which in turn ensures good initial values for the scheme described in section  [ section : path ] , thereby reducing the number of iterations required by the algorithm .",
    "there are obvious practical benefits to using an algorithm that converges rapidly to the unique global solution",
    ". however , convexity may also be desirable from a statistical perspective . in the absence of convexity",
    ", @xmath66 is not necessarily continuous with respect to the data  that is , a small change in the data may produce a large change in the estimate .",
    "such estimators tend to have high variance [ @xcite ; @xcite ] in addition to being unattractive from a logical perspective .",
    "furthermore , discontinuity with respect to @xmath12 increases the difficulty of choosing a good value for the regularization parameter .      however , it is not always necessary to attain global convexity . in high - dimensional settings where @xmath122 , global convexity is neither possible nor relevant . in such settings ,",
    "sparse solutions for which the number of nonzero coefficients is much lower than @xmath2 are desired .",
    "provided that the objective function is convex in the local region that contains these sparse solutions , we will still have stable estimates and smooth coefficient paths in the parameter space of interest .    of considerable practical interest ,",
    "then , is a diagnostic that would indicate , for nonconvex penalties such as scad and mcp , which regions of the coefficient paths are locally convex and which are not . here",
    ", we introduce a definition for local convexity and a diagnostic measure which can be easily computed from the data and which can indicate which regions of the coefficient paths retain the benefits of convexity and which do not .",
    "recall the conditions for global convexity : @xmath13 must be greater than @xmath123 for mcp ( @xmath124 for scad ) , where @xmath62 denoted the minimum eigenvalue of @xmath63 .",
    "we propose modifying this cutoff so that only the covariates with nonzero coefficients ( the covariates which are `` active '' in the model ) are included in the calculation of @xmath62 .",
    "note that neither @xmath13 nor @xmath75 change with @xmath12 .",
    "what does vary with @xmath12 is set of active covariates ; generally speaking , this will increase as @xmath12 decreases ( with correlated / collinear data , however , exceptions are possible ) .",
    "thus , local convexity of the objective function will not be an issue for large @xmath12 , but may cease to hold as @xmath12 is lowered past some critical value @xmath125 .",
    "specifically , let @xmath126 denote the minimizer of ( [ eq : q - lin ] ) for a certain value of @xmath12 , @xmath127 denote the active set of covariates , @xmath128 denote the set of covariates that are either currently active given a value @xmath12 or that will become active imminently upon the lowering of @xmath12 by an infinitesimal amount , and let @xmath129 denote the design matrix formed from only those covariates for which @xmath130 , with @xmath131 denoting the minimum eigenvalue of @xmath132 .",
    "now , letting @xmath133 and @xmath134 we define the @xmath12 interval over which the objective function is `` locally convex '' to be @xmath135 . correspondingly , the objective function is locally nonconvex ( and globally nonconvex ) in the region @xmath136 $ ] .",
    "because @xmath131 changes only when the composition of @xmath137 changes , it is clear that @xmath125 must be a value of @xmath12 for which @xmath138 .    for logistic regression ,",
    "let @xmath131 represent the minimum eigenvalue of @xmath139 , where @xmath84 is evaluated at @xmath126 and @xmath140 is a diagonal matrix that depends on the penalty function .",
    "for the fixed scale estimation of section [ s31 ] , @xmath140 has elements @xmath141 for mcp and @xmath142 for scad , while for the adaptively rescaled estimation of section [ s32 ] , @xmath140 has elements @xmath143 for mcp and @xmath144 for scad . for @xmath131",
    "defined in this manner , @xmath125 equals the smallest value of @xmath12 such that @xmath145 .    .",
    "the shaded region is the region in which the objective function is not locally convex .",
    "note that the solutions are continuous and stable in the locally convex regions , but discontinuous and erratic otherwise . ]",
    "the practical benefit of these diagnostics can be seen in figure  [ fig : dgn ] , which depicts examples of coefficient paths from simulated data in which @xmath146 and @xmath147 . as is readily apparent , solutions are smooth and well behaved in the unshaded , locally convex region , but discontinuous and noisy in the shaded region which lies to the right of @xmath125 .",
    "figure  [ fig : dgn ] contains only mcp paths ; the corresponding figures for scad paths look very similar .",
    "figure  [ fig : dgn ] displays linear regression paths ; paths for logistic regression can be seen in figure  [ fig : app - gamma ] .",
    "the noisy solutions in the shaded region of figure  [ fig : dgn ] may arise from numerical convergence to suboptimal solutions , inherent statistical variability arising from minimization of a nonconvex function , or a combination of both ; either way , however , the figure makes a compelling argument that practitioners of regression methods involving nonconvex penalties should know which region their solution resides in .",
    "estimation using mcp and scad models depends on the choice of the tuning parameters @xmath13 and @xmath12 .",
    "this is usually accomplished with cross - validation or using an information criterion such as aic or bic .",
    "each approach has its shortcomings .",
    "information criteria derived using asymptotic arguments for unpenalized regression models are on questionable ground when applied to penalized regression problems where @xmath122 . furthermore , defining the number of parameters in models with penalization and shrinkage present is complicated and affects lasso , mcp and scad differently .",
    "finally , we have observed that aic and bic have a tendency , in some settings , to select local mimima in the nonconvex region of the objective function .",
    "cross - validation does not suffer from these issues ; on the other hand , it is computationally intensive , particularly when performed over a two - dimensional grid of values for @xmath13 and @xmath12 , some of which may not possess convex objective functions and , as a result , converge slowly .",
    "this places a barrier in the way of examining the choice of @xmath13 , and may lead practitioners to use default values that are not appropriate in the context of a particular problem .",
    "it is desirable to select a value of @xmath13 that produces parsimonious models while avoiding the aforementioned pitfalls of nonconvexity .",
    "thus , we suggest the following hybrid approach , using bic , cross - validation and convexity diagnostics in combination , which we have observed to work well in practice . for a path of solutions with a given value of @xmath13 , use aic / bic to select @xmath12 and use the aforementioned convexity diagnostics to determine the locally convex regions of the solution path .",
    "if the chosen solution lies in the region below @xmath125 , increase @xmath13 to make the penalty more convex . on the other hand ,",
    "if the chosen solution lies well above @xmath125 , one can lower @xmath13 without fear of nonconvexity .",
    "once this process has been iterated a few times to find a value of @xmath13 that seems to produce an appropriate balance between parsimony and convexity , use cross - validation to choose @xmath12 for this value of @xmath13 .",
    "this approach is illustrated in section  [ section : app ] .",
    "in this section we assess the computational efficiency of the coordinate descent and lla algorithms for fitting mcp and scad regression models . we examine the time required to fit the entire coefficient path for linear and logistic regression models .    in these simulations ,",
    "the response for linear regression was generated from the standard normal distribution , while for logistic regression , the response was generated as a bernoulli random variable with @xmath148 for all @xmath149 .    to investigate whether or not the coordinate descent algorithm experiences difficulty in the presence of correlated predictors , covariate values were generated in one of two ways : independently from the standard normal distribution ( i.e. , no correlation between the covariates ) , and from a multivariate normal distribution with a pairwise correlation of @xmath150 between all covariates .    to ensure that the comparison between the algorithms was fair and not unduly influenced by failures to converge brought on by nonconvexity or model saturation , @xmath0 was chosen to equal 1000 and @xmath13 was set equal to @xmath123 , thereby ensuring a convex objective function for linear regression .",
    "this does not necessarily ensure that the logistic regression objective function is convex , although with adaptive rescaling , it works reasonably well . in all of the cases",
    "investigated , the lla and coordinate descent algorithms converged to the same path ( within the accuracy of the convergence criteria ) .",
    "the median times required to fit the entire coefficient path are presented as a function of @xmath2 in figure  [ fig : eff ] .",
    "interestingly , the slope of both lines in figure  [ fig : eff ] is close to 1 in each setting ( on the log scale ) , implying that both coordinate descent and lla exhibit a linear increase in computational burden with respect to @xmath2 over the range of @xmath2 investigated here .",
    "however , coordinate descent algorithm is drastically more efficient  up to 1000 times faster .",
    "the coordinate descent algorithm is somewhat ( 25 times ) slower in the presence of highly correlated covariates , although it is still at least 100 times faster than lla in all settings investigated .",
    "we now turn our attention to the statistical properties of mcp and scad in comparison with the lasso .",
    "we will examine two instructive sets of simulations , comparing these nonconvex penalty methods to the lasso .",
    "the first set is a simple series designed to illustrate the primary differences between the methods .",
    "the second set is more complex , and designed to mimic the applications to real data in section  [ section : app ] .    in the simple settings ,",
    "covariate values were generated independently from the standard normal distribution .",
    "the sample sizes were @xmath151 for linear regression and @xmath152 for logistic regression . in the complex settings ,",
    "the design matrices from the actual data sets were used .    for each data",
    "set , the mcp and scad coefficient paths were fit using the coordinate descent algorithm described in this paper , and lasso paths were fit using the ` glmnet ` package [ @xcite ] . tenfold cross - validation",
    "was used to choose @xmath12 .    _ setting _ 1 .",
    "we begin with a collection of simple models in which there are four nonzero coefficients , two of which equal to @xmath153 , the other two equal to @xmath154 . given these values of the coefficient vector ,",
    "responses were generated from the normal distribution with mean @xmath155 and variance 1 for linear regression , and for logistic regression , responses were generated according to ( [ eq : log - model ] ) .",
    "for the simulations in this setting , we used the single value @xmath156 in order to illustrate that reasonable results can be obtained without investigation of the tuning parameter if adaptive rescaling is used for penalized logistic regression .",
    "presumably , the performance of both mcp and scad would be improved if @xmath13 was chosen in a more careful manner , tailored to each setting ; nevertheless , the simulations succeed in demonstrating the primary statistical differences between the methods .",
    "we investigate the estimation efficiency of mcp , scad and lasso as @xmath157 and @xmath2 are varied .",
    "these results are shown in figure  [ fig : cmp1 ] .",
    "figure  [ fig : cmp1 ] illustrates the primary difference between mcp / scad and lasso .",
    "mcp and scad are more aggressive modeling strategies , in the sense that they allow coefficients to take on large values much more easily than lasso . as a result",
    ", they are capable of greatly outperforming the lasso when large coefficients are present in the underlying model . however , the shrinkage applied by the lasso is beneficial when coefficients are small , as seen in the regions of figure  [ fig : cmp1 ] in which @xmath157 is near zero . in such settings , mcp and scad",
    "are more likely to overfit the noisy data .",
    "this is true for both linear and logistic regression .",
    "this tendency is worse for logistic regression than it is for linear regression , worse for mcp than it is for scad , and worse when @xmath13 is closer to 1 ( results supporting this final comparison not shown ) .    _",
    "setting _ 2 . in this",
    "setting we examine the performance of mcp , scad and lasso for the kinds of design matrices often seen in biomedical applications , where covariates contain complicated patterns of correlation and high levels of multicollinearity .",
    "section  [ section : app ] describes in greater detail the two data sets , which we will refer to here as `` genetic association '' and `` microarray , '' but it is worth noting here that for the genetic association case , @xmath158 and @xmath159 , while for the microarray case , @xmath160 and @xmath161 .    in both simulations",
    "the design matrix was held constant while the coefficient and response vectors changed from replication to replication . in the genetic association case ,",
    "5 coefficient values were randomly generated from the exponential distribution with rate parameter 3 , given a random ( @xmath162 ) sign , and randomly assigned to one of the 532 snps ( the rest of the coefficients were zero ) . in the microarray case ,",
    "100 coefficient values were randomly generated from the normal distribution with mean 0 and standard deviation 3 , and randomly assigned among the 7129 features .",
    "once the coefficient vectors were generated , responses were generated according to ( [ eq : log - model ] ) .",
    "this was repeated 500 times for each case , and the results are displayed in table  [ tab : appsim ] .",
    ".simulation results : setting % correct refers to the percent of variables selected by the model that had nonzero coefficients in the generating model [ cols=\"<,^,^,^ \" , ]      next , we examine the gene expression study of leukemia patients presented in @xcite . in the study the expression levels of 7129 genes were recorded for 27 patients with acute lymphoblastic leukemia ( all ) and 11 patients with acute myeloid leukemia ( aml ) .",
    "expression levels for an additional 34 patients were measured and reserved as a test set .",
    "logistic regression models penalized by lasso , scad and mcp were fit to the training data .    biologically , this problem is more dense than the earlier application .",
    "potentially , a large number of genes are affected by the two types of leukemia .",
    "in addition , the sample size is much smaller for this problem .",
    "these two factors suggest that a higher value of @xmath13 is appropriate , an intuition borne out in figure  [ fig : app - gamma ] .",
    "the figure suggests that @xmath163 may be needed in order to obtain an adequately convex objective function for this problem ( we used @xmath164 for both mcp and scad ) .    with @xmath13 so large , mcp and scad are quite similar to lasso ; indeed , all three methods classify the test set observations in the same way , correctly classifying 31@xmath16534 . analyzing the same data , @xcite find that lasso - penalized logistic regression is comparable with or more accurate than several other competing methods often applied to high - dimensional classification problems .",
    "the same would therefore apply to mcp and scad as well ; however , mcp achieved its success using only 11 predictors , compared to 13 for lasso and scad .",
    "this is an important consideration for screening and diagnostic applications such as this one , where the goal is often to develop an accurate test using as few features as possible in order to control cost .",
    "note that , even in a problem such as this one with a sample size of 38 and a dozen features selected , there may still be an advantage to the sparsity of mcp and the parsimonious models it produces . to take advantage of mcp , however , it is essential to choose @xmath13 wisely  using the value @xmath166 ( much too sparse for this problem ) tripled the test error to 9@xmath16534 .",
    "we are not aware of any method that can achieve prediction accuracy comparable to mcp while using only 11 features or fewer .",
    "the results from the simulation studies and data examples considered in this paper provide compelling evidence that nonconvex penalties like mcp and scad are worthwhile alternatives to the lasso in many applications .",
    "in particular , the numerical results suggest that mcp is often the preferred approach among the three methods .",
    "many researchers and practitioners have been reluctant to embrace these methods due to their lack of convexity , and for good reason : nonconvex objective functions are difficult to optimize and often produce unstable solutions .",
    "however , we provide here a fast , efficient and stable algorithm for fitting mcp and scad models , as well as introduce diagnostic measures to indicate which regions of a coefficient path are locally convex and which are not .",
    "furthermore , we introduce an adaptive rescaling for logistic regression which makes selection of the tuning parameter @xmath13 much easier and more intuitive .",
    "all of these innovations are publicly available as an open - source ` r ` package ( http://cran.r-project.org ) called ` ncvreg ` .",
    "we hope that these efforts remove some of the barriers to the further study and use of these methods in both research and practice .",
    "although the objective functions under consideration in this paper are not differentiable , they possess directional derivatives and directional second derivatives at all points @xmath8 and in all directions @xmath167 for @xmath168 .",
    "we use @xmath169 and @xmath170 to represent the derivative and second derivative of @xmath9 in the direction @xmath167 .",
    "proof of lemma [ 1lema ] for all @xmath171 , @xmath172 thus , @xmath55 is a strictly convex function on @xmath173 if @xmath25 for mcp and if @xmath48 for scad .",
    "proof of proposition [ prop : cd ] @xcite establishes sufficient conditions for the convergence of cyclic coordinate descent algorithms to coordinate - wise minima .",
    "the strict convexity in each coordinate direction established in lemma 1 satisfies the conditions of theorems 4.1 and 5.1 of that article . because @xmath9 is continuous ,",
    "either theorem can be directly applied to demonstrate that the coordinate descent algorithms proposed in this paper converge to coordinate - wise minima . furthermore , because all directional derivatives exist , every coordinate - wise minimum is also a local minimum .",
    "proof of proposition [ prop : logistic - convex ] for all @xmath174 , @xmath175 thus , @xmath176 is a convex function on the region for which @xmath120 for mcp , and where @xmath177 for scad .",
    "the authors would like to thank professor cun - hui zhang for providing us an early version of his paper on mcp and sharing his insights on the related topics , and rob mullins for the genetic association data analyzed in section 6 .",
    "we also thank the editor , associate editor and two referees for comments that improved both the clarity of the writing and the quality of the research .",
    "golub , t. r. , slonim , d. k. , tamayo , p. , huard , c. , gaasenbeek , m. , mesirov , j. p. , coller , h. , loh , m.  l. , downing , j.  r. , caligiuri , m.  a. et  al .",
    "molecular classification of cancer : class discovery and class prediction by gene expression monitoring .",
    "_ science _ * 286 * 531536 .",
    "yu , j. , yu , j. , almal , a.  a. , dhanasekaran , s.  m. , ghosh , d. , worzel , w.  p. and chinnaiyan , a.  m. ( 2007 ) .",
    "feature selection and molecular classification of cancer using genetic programming .",
    "_ neoplasia _ * 9 * 292303 ."
  ],
  "abstract_text": [
    "<S> a number of variable selection methods have been proposed involving nonconvex penalty functions . </S>",
    "<S> these methods , which include the smoothly clipped absolute deviation ( scad ) penalty and the minimax concave penalty ( mcp ) , have been demonstrated to have attractive theoretical properties , but model fitting is not a straightforward task , and the resulting solutions may be unstable . here , we demonstrate the potential of coordinate descent algorithms for fitting these models , establishing theoretical convergence properties and demonstrating that they are significantly faster than competing approaches . </S>",
    "<S> in addition , we demonstrate the utility of convexity diagnostics to determine regions of the parameter space in which the objective function is locally convex , even though the penalty is not . </S>",
    "<S> our simulation study and data examples indicate that nonconvex penalties like mcp and scad are worthwhile alternatives to the lasso in many applications . </S>",
    "<S> in particular , our numerical results suggest that mcp is the preferred approach among the three methods .    . </S>"
  ]
}