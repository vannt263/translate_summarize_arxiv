{
  "article_text": [
    "the problem of source localization has attracted the attention of many researchers during the last decades .",
    "various applications rely on the recovery of the spatial position of an emitting source , such as : automated camera steering , teleconferencing and beamformer steering for robust speech recognition . for this reason , considerable amount of efforts",
    "have been devoted to investigate this field and a wide range of methods have been proposed over the years .",
    "common to all localization approaches is the utilization of multiple microphone recordings to infer the spatial information .",
    "the fundamental challenge is to attain robust localization in poor conditions , i.e. , in the presence of high reverberation and background noises .",
    "conventional localization approaches can be roughly divided into two main categories : single- and dual - step approaches . in the first class of algorithms ,",
    "the source location is determined directly from the microphone signals .",
    "the most dominant member of this class is the algorithm .",
    "the algorithm is derived by applying the criterion to a chosen statistical model of the received signals .",
    "this optimization often involves maximization of the output power of a beamformer , steered to all potential source locations  @xcite .",
    "another type of single - stage approaches is high resolution spectral estimation methods , such as the well - known algorithm  @xcite , and the techniques  @xcite .    in the dual - step approaches category ,",
    "the first stage involves estimation from spatially separated microphone pairs .",
    "the classical method for estimation is the algorithm introduced in the landmark paper by knapp and carter  @xcite .",
    "the method relies on the assumption of a reverberant - free model such that the , which relates the source and each of the microphones , is a pure delay .",
    "however , this assumption does not hold in the presence of room reverberation , rendering a performance deterioration  @xcite . consequently",
    ", improvements of the method for the reverberant case were proposed  @xcite .    in the second algorithmic stage",
    ", the noisy estimates are combined to carry out the actual localization .",
    "each estimate is associated with an infinite set of source positions , lying on a half of an hyperboloid .",
    "the locus of the speaker can be recovered by intersecting the hyperboloid surfaces corresponding to the measurements of different pairs of microphones . however , the computation of a 3-dimensional hyperboloids intersection is a cumbersome task and tends to be sensitive to estimation errors . in far - field regime",
    "the hyperboloid can be approximated by a cone , and linear intersection estimate can be applied  @xcite .",
    "another simplifying approach is to recast the hyperbolic equations into a spherical form , and apply the nonlinear least squares approach  @xcite .",
    "all the prementioned methods utilize the spatial information conveyed by the received signals , but do not rely on any prior information about the enclosure in which the measurements are obtained . in some scenarios , e.g. in meeting rooms or cars ,",
    "the source position is confined to a predefined area .",
    "it is reasonable to assume that representative samples from the region of interest can be measured in advance .",
    "examining the structures and patterns characterizing the representative samples can be utilized for formulating a data - driven model which relates the measured signals to their corresponding source positions .",
    "the additional information may help to better cope with the challenges posed by reverberation and noise .",
    "so far , only few attempts were made to involve training information for performing source localization .",
    "deleforge and horaud in  @xcite , discussed a 2-d sound localization scheme , in the binaural hearing context .",
    "their central assumption is that the binaural observations lie on an intrinsic manifold which is locally linear .",
    "accordingly , they proposed a probabilistic piecewise affine regression model , that learns the localization - to - interaural mapping and its inverse . in  @xcite",
    ", the authors have generalized the algorithm to deal with multiple sources using variational framework .    in  @xcite the task of estimation",
    "was formulated as a classification problem and a learning - based approach was presented .",
    "they proposed to extract features from the vectors and use a multilayer perceptron neural network to learn the nonlinear mapping from such features to the .",
    "talmon et al .",
    "@xcite introduced a supervised method based on _ manifold learning _ , using _",
    "diffusion kernels_. the main idea is specifying the fundamental controlling parameters of the using a manifold learning scheme .",
    "assuming that the position of the source is the only varying degree - of - freedom of the system at hand , this process is capable of recovering the unknown source locations .",
    "the key point of the algorithm is to use an appropriate diffusion kernel with a specifically - tailored distance measure , that is capable of finding the underlying independent parameters , dominating the system .",
    "talmon et al .",
    "@xcite have applied this method to a single microphone system with a input .    in  @xcite we adopted the paradigm of  @xcite and adapted it to a more realistic setting where the source is a speech signal rather than a signal .",
    "the power spectral density of the speech signal is non - flat ( as well as non - stationary ) .",
    "hence , the spectral variations may blur the variations attributed to the different possible locations of the source . in order to mitigate this problem",
    ", we committed two major changes in the algorithm presented in  @xcite : 1 ) a second microphone was added and 2 ) the feature vector , that was originally based on the correlation function has been replaced by a -based vector . it should be emphasized that in  @xcite the feature vector was associated with the , whereas in  @xcite the feature vector relied on the which is the fourier transform of the relative impulse response .    though localization algorithms based on the diffusion framework were shown to perform well ,",
    "their fundamental drawback is that they do not provide any guarantee for optimality . in general the diffusion - based methods are implemented by a dual - stage approach .",
    "first , a low dimensional embedding of the representative samples is recovered in an unsupervised manner .",
    "second , the new representation is used to estimate the unknown locations based on the labelled samples .",
    "the septation into two stages where one is entirely unsupervised and the other is entirely supervised is not necessarily optimal .",
    "moreover , the unlabelled data are not exploited for the estimation itself .",
    "the significance of combining both labelled and unlabelled data , in the source localization context , should be farther emphasized .",
    "classification and regression algorithms which rely on training data , are very popular in various applications , such as : text categorization , handwriting recognition , images classification and speech recognition . nowadays , there exist a rich database for each of these tasks , with considerable amount of examples with true labellings .",
    "thus , these problems are more usefully solved using fully supervised approaches . on the contrary , in the localization problem the training should fit to the specific acoustic environment in which the measurements are obtained , thus , we can not create a general database that corresponds to all possible acoustic scenarios .",
    "instead , the training set should be generated individually for each acoustic environment . to obtain labelled data",
    ", one needs to generate recordings in a controlled manner and calibrate each of them precisely .",
    "generating a large amount of labelled data is a cumbersome and impractical process .",
    "however , unlabelled data is freely available since it can be collected whenever someone is speaking .",
    "this greatly motivates the use of semi - supervised approaches , which mostly rely on unlabelled data , for the source localization problem .",
    "another motivation is related to the special characteristics of the acoustic environment . as will be further elaborated in the paper ,",
    "the unlabelled data can be utilized for forming a data - driven model of the acoustic environment that is very useful for performing robust source localization .",
    "to address the limitations of the previous diffusion - based approaches , and to better utilize the unlabelled data , we propose the algorithm .",
    "the method recovers the inverse mapping between the acoustic samples and their corresponding locations .",
    "the gist of the algorithm is based on the concepts of manifold regularization on a , introduced by belkin et al .",
    "the idea is to extended the standard supervised estimation framework by adding an extra regularization term which imposes a smoothness constraint on possible solutions with respect to a data - driven model .",
    "the model is learned empirically by forming a data adjacency graph over both labelled and unlabelled training samples . in this approach ,",
    "the estimated location relies not only on the labelled samples , but also on the unlabelled ones .",
    "moreover , in order to efficiently utilize unlabelled samples received during runtime , we propose an adaptive implementation .",
    "the algorithm iteratively updates the system , based on the new information which becomes available while accumulating new unlabelled data .",
    "we compare the proposed algorithm , with the method , which is a diffusion - based algorithm .",
    "the discussion is supported by an experimental study based on simulated data .",
    "the paper is organized as follows . in section  [ sec : problem ] , we formulate the problem in a general noisy and reverberant environment .",
    "we motivate the choice of the for forming a feature vector and describe how it can be estimated based on the microphone measurements . in section  [ sec",
    ": algo ] , we discuss the existence of an acoustic manifold and formulate an optimization problem which relies on a data - driven model computed based on both labelled and unlabelled data .",
    "this formulation leads to the algorithm which is sequentially adapted by the unlabelled data accumulated during runtime .",
    "we briefly describe our previous localization method based on the diffusion framework @xcite in section  [ sec : diffusion ] .",
    "accordingly , we describe the derivation of the algorithm which conducts a neighbours search using the diffusion distance as an affinity measurement between . in section",
    "[ sec : results ] , we demonstrate the algorithms performance by an extensive simulation study .",
    "a comparison between the and the algorithms is carried out in section  [ sec : discuss ] .",
    "section  [ sec : conclusions ] concludes the paper .",
    "we consider a standard enclosure , e.g. , a conference room or a car interior , with moderate reverberation time .",
    "a single source located at @xmath0^t$ ] generates an unknown speech signal @xmath1 , which is received by a pair of microphones .",
    "the received signals , denoted by @xmath2 and @xmath3 , are contaminated by an additive stationary noise , and are given by : @xmath4 where @xmath5 is the time index , @xmath6 are the corresponding relating the source at position @xmath7 and each of the microphones and @xmath8 are uncorrelated signals .",
    "linear convolution is denoted by @xmath9 .",
    "each of the is composed of the direct path between the source and the microphone , as well as reflections from the surfaces characterizing the enclosure .",
    "consequently , even in moderate reverberation conditions , the is typically modelled as a long fir filter .",
    "the purpose is to localize the speaker based on the current received microphone signals @xmath2 and @xmath3 .",
    "we assume that we are also given a set of prerecorded representative samples from the region of interest .",
    "the training set is composed of @xmath10 samples of measured signals @xmath11 from various positions within the specified region . only @xmath12 samples among the set are labelled , i.e. , their originating position @xmath13 is known . the rest @xmath14 samples are unlabelled , namely , their corresponding source locations are unknown . to summarize ,",
    "the training set is composed of @xmath12 labelled examples @xmath15 and @xmath16 unlabelled examples @xmath17 .",
    "we are interested in a realistic scenario , where the amount of labelled data is significantly smaller than the amount of unlabelled data which can be collected online .",
    "our goal is to build an on - line system which is initially given a small amount of labelled data , and is gradually adapted as new unlabelled samples are acquired .",
    "the first step is to define an appropriate feature vector that faithfully represents the characteristics of the acoustic path and is invariant to the other factors , i.e. , the stationary noise and the varying speech signals .",
    "an equivalent representation of is given by  @xcite : @xmath18 where @xmath19 is the relative impulse response between the microphones with respect to the source , satisfying @xmath20 .",
    "in  , the relative impulse response represents the system relating the measured signal @xmath2 as an input and the measured signal @xmath3 as an output .    for convenience ,",
    "we represent   in the frequency domain .",
    "the fourier transform of the relative impulse response , termed the , is obtained by : @xmath21 where @xmath22 is the , @xmath23 is the between @xmath3 and @xmath2 , @xmath24 is the of @xmath2 , @xmath25 is the of the noise in the first microphone @xmath26 , and @xmath27 is the of the source @xmath1 . @xmath28 and @xmath29 are the of the respective , and @xmath30 denotes a discrete frequency index .",
    "the choice of the value of @xmath31 should balance the tradeoff between the correspondence with the relative impulse response length ( large value ) and latency considerations ( small value ) .",
    "since @xmath28 and @xmath29 are unavailable , we estimate the by : @xmath32 note that this estimator is biased since we neglect the of the noise @xmath25 . alternatively ,",
    "unbiased estimators can be used , such as the estimator based on the non - stationarity of the speech signal  @xcite .",
    "however , we are not concerned with robust estimation of the since we will show that the proposed method is insensitive to this type of errors . accordingly , we define the feature vector @xmath33^t$ ] as the concatenation of estimated values in the @xmath31 frequency bins . in practice ,",
    "we discard high frequencies in which the ratio in is meaningless due to weak speech components . for the sake of clarity ,",
    "we omit the dependency on the position , and denote the feature vector by @xmath34 .",
    "our goal is to recover the target function which transforms each to its corresponding location , based on the training set comprised of both labelled and unlabelled samples .",
    "finding such an inverse mapping is non - trivial due to the complex nonlinear relation between the high dimensional and the originating locations . to mitigate this problem",
    "we adopt the concepts of manifold regularization , introduced by belkin et al .",
    "@xcite , and present it in the light of the acoustic environment and , in particular , for the source localization problem at hand .",
    "it is important to note that , originally , the concepts of manifold regularization were implemented for classification , whereas , here , it is applied to the problem of source localization which is a regression problem .",
    "two guiding principles are in the core of the proposed method , that will be termed .",
    "first , instead of using complex variational calculus for estimating the target function , we assume that the function resides in a . due to the special characteristics of the functions belonging to the",
    ", the problem can be formulated simply as a system of linear equations .",
    "second , we incorporate geometrical considerations , i.e. , we use the information implied by the intrinsic patterns observed in the set of to build a data - driven model . then , the solution is constrained to behave smoothly with respect to this data - driven model , representing the intrinsic structure of the .      as mentioned in section  [ sec : problem ] , the have a high dimensional representation in @xmath35 that corresponds to the vast amount of reflections from the different surfaces characterizing the enclosure .",
    "we assume that the samples , drawn from a specific region of interest in the enclosure , are not spread uniformly in the entire space of @xmath35 . instead",
    ", they are confined to a compact manifold @xmath36 of dimension @xmath37 , which is much smaller compared to the dimension of the ambient space , i.e. @xmath38 .",
    "this assumption is justified by the fact that the are influenced by only a small set of parameters related to the physical characteristics of the environment , such as : the enclosure dimensions and shape , the surfaces materials and the positions of the microphones and the source .",
    "moreover , we focus on a static configuration , in which the properties of the enclosure and the position of the microphones remain fixed . in such an acoustic environment ,",
    "the only varying degree of freedom is the source location .",
    "accordingly , we assume that the can be intrinsically embedded in a low dimensional manifold which is governed by the position of the source .",
    "the existence of such an acoustic manifold was discussed in detail in  @xcite , and was demonstrated with respect to the of the source .",
    "the main results will be briefly described in the experimental part , in section  [ sec : analysis ] .",
    "roughly , we consider a manifold of reduced dimensions which may have a complex nonlinear structure .",
    "however , in small neighbourhoods the manifold is locally linear , meaning that in the vicinity of each point it is flat and coincides with the tangent plane to the manifold at that point .",
    "hence , the euclidean distance can faithfully measure affinities between points that resides close to each other on the manifold . for larger scales ,",
    "the euclidean distance is meaningless , and we should rather use the geodesic distance on the manifold",
    ". however , the geodesic distance can be evaluated only when the structure of the manifold is known . in order to respect the manifold structure",
    "we will only examine local connections between points and disregard larger distances .",
    "our goal is to find the inverse - mapping function that receives an sample and returns the corresponding source location . in general , estimating a function that minimizes a cost function , is a cumbersome task that requires complex mathematical tools , such as variational calculus .",
    "one simplifying approach is to assume that the target function belongs to a certain class of functions with a specific structure .",
    "for example , it can be assumed that the target function belongs to a certain space of functions , spanned by an orthogonal basis .",
    "hence , the target function can be represented by a linear combination of the basis functions , where the weights are determined according to the projections of the function on each of the basis functions . in our case",
    "we assume that the target function belongs to a associated with a unique kernel function that evaluates each function in the space by an inner product . rather than computing the basis functions spanning the space",
    ", we use an analogues representation with linear combinations of the kernel function . according to this representation",
    ", the problem can be converted to a simple linear estimation of a finite set of parameters .",
    "we will first represent the kernel function and its properties , and then define the and discuss its representation by the kernel function that will be used for deriving the optimization problem in section  [ sec : opti ] . in appendix",
    "[ sec : appa ] , we show that the eigenfunctions associated with the kernel form an orthogonal basis for the , and discuss an analogue representation in terms of these basis functions .    as implied by its name , an is associated with a kernel function @xmath39 that measures a pairwise affinity between .",
    "the kernel function must satisfy the following two conditions :    1 .",
    "symmetry : @xmath40 .",
    "2 .   positive semi - definite : the @xmath41 matrix @xmath42 with @xmath43 is positive semi - definite , for any arbitrary finite set of points @xmath44 .",
    "another essential requirement from the kernel is that it defines a notion of locality , determined with accordance to a scaling factor @xmath45 : for @xmath46 , @xmath47 , and for @xmath48 , @xmath49 .",
    "a common choice is to use a gaussian kernel with variance @xmath45 : @xmath50 clearly , the gaussian kernel is a symmetric positive semi - definite function , and satisfies the locality property .",
    "the locality property is of major importance in our case , since the kernel receives , sampled from the manifold @xmath36 . as discussed above , the manifold is in general nonlinear and is assumed to be locally linear over small patches . due to its property of locality , the kernel function constitutes an affinity measure that respects the manifold structure .",
    "an , denoted as @xmath51 , is a hilbert space of functions , mapping each @xmath52 to @xmath53 , which is associated with a kernel @xmath30 .",
    "we skip the formal definition of an ( for details see  @xcite ) . instead , we state the two main properties of an :    * for all @xmath52 , @xmath54 * * the reproducing property : * for all @xmath55 and @xmath52 , @xmath56    where for each @xmath52 we define the real valued function @xmath57 .",
    "the first property simply states that the consists of all functions defined by the kernel @xmath30 at some point on the manifold .",
    "the second property implies that the kernel @xmath30 has a special property that it evaluates all the functions in the space by an inner product .",
    "for example , in @xmath58 the delta function has the reproducing property since it evaluates all the functions in @xmath58 : @xmath59 .",
    "however , this does not define an , since the delta function does not belong to @xmath58 .",
    "we have seen that an is associated with a unique reproducing kernel function . in the opposite direction , known as the moore - aronszajn theorem , every symmetric",
    ", positive definite kernel @xmath30 defines a unique rkhs @xmath51 that is given by the _ completion _ ( an expansion that includes the limits of all cauchy sequences ) of the space of functions spanned by the set @xmath60 : @xmath61 with respect to the following inner product : @xmath62 it can be easily verified that the two mentioned properties of an are satisfied by this definition .",
    "obviously , the reproducing kernel belongs to the space , and the reproducing property holds , since : @xmath63    additional view of an , based on mercer s theorem  @xcite , is discussed in appendix  [ sec : appa ] .",
    "according to this view point , any function @xmath64 can be represented by an orthogonal basis of functions @xmath65 related to the kernel @xmath30 : @xmath66 to circumvent the computation of the basis functions , we use the representation of , in terms of the kernel function .      in this section",
    "we present the optimization over the target function assuming that it belongs to an @xmath67 with a reproducing kernel @xmath30 .",
    "formally , we search for a function @xmath68 which is the inverse mapping between an and its corresponding position , i.e. @xmath69 . in this paper",
    "we focus on estimating one position coordinate , thus , we omit the coordinate subscript . however , the analysis , the results and the algorithm described here can be naturally extended to estimating several coordinates .",
    "the search will be formulated by the following optimization problem : @xmath70 where @xmath71 is the norm that corresponds to the inner product defined in  , @xmath72 is the _ intrinsic _ norm defined with respect to the manifold @xmath36 , and @xmath73 are scalar parameters .",
    "the optimization problem consists of three components .",
    "the first term is an empirical cost function defined over the labelled samples @xmath74 .",
    "the function @xmath75 evaluates the extent of correspondence between the evaluations of the target function @xmath76 and the true labels @xmath77 . in our case , we set the cost function to be the squared loss function @xmath78 .",
    "note that while the @xmath58 norm is not suitable for comparing between  @xcite , it is a reasonable choice for evaluating localization quality .",
    "the two last terms in   are regularization conditions .",
    "roughly , their role is to prevent the solution from overfitting to the labelled examples .",
    "the second term is the tikhonov regularization which penalizes the norm of the function to impose smoothness condition in @xmath51 .",
    "the additional regularization term , defined by the last term in  , was introduced by belkin et al .",
    "this is an intrinsic regularization that represents a smoothness penalty of the function with respect to the manifold @xmath36 .",
    "one natural choice for the intrinsic norm is to measure the gradient of the function along the manifold , i.e. , to measure the variability of the function with respect to small movements on the manifold .",
    "since the manifold structure is unknown , this term should be approximated on the basis of both labelled and unlabelled samples .",
    "the training set @xmath79 , which includes different realizations of possible acoustic paths , can be viewed as a discrete sampling of the manifold @xmath36 .",
    "the manifold can be empirically represented by a graph in which the training samples are the graph nodes , and the weights of the edges are defined according to an @xmath80 adjacency matrix @xmath81 between the samples : @xmath82 where @xmath83 is a set consisting of the @xmath37 nearest - neighbours of @xmath84 among @xmath85 .",
    "the adjacency matrix @xmath81 is used to form the graph laplacain @xmath86 , by @xmath87 , where @xmath88 is a diagonal matrix with @xmath89 .",
    "it can be shown , under certain conditions , that the graph laplacian @xmath86 converges to a differential operator on the manifold @xmath36 , as was discussed in detail in  @xcite .",
    "hence , the gradient of the function along the manifold can be approximated using the graph laplacian .",
    "accordingly , an intrinsic measure of data - dependent smoothness is given by : @xmath90 , where @xmath91 $ ] .",
    "thus , the optimization problem   can be recast as : @xmath92 further insight can be obtained by the expansion of the intrinsic regularization : @xmath93 intuitively , in , large @xmath94 , corresponding to strong similarity between @xmath95 and @xmath84 , implies a tendency of @xmath76 and @xmath96 to be close to each other .",
    "for this reason , a truncated kernel was chosen in , since it is reasonable to penalize the function only when the corresponding resides in the same local neighbourhood .",
    "note that   is a semi - supervised formulation , since it involves both labelled and unlabelled samples .",
    "while the first term is merely based on the labelled samples , the last two terms are based on both labelled and unlabelled data .",
    "the two regularization parameters @xmath97 and @xmath98 balance between maximizing the correspondence to the labelled data , and maintaining low - complexity of possible solutions . in some respects ,",
    "both regularization terms try to relate the target function to the manifold @xmath36 by the two different kernels defined in   and  .",
    "involving two kernels associated with different scales represents two different measurements of smoothness with respect to the manifold . since the real structure of the manifold is unknown , the combination of both kernels is essential for obtaining a more accurate modelling of the manifold .    the representer theorem  @xcite states that the minimizer @xmath99 of is a linear combination of the kernel functions only in the set of labelled and unlabelled points @xmath100 ,",
    "i.e. , it is given by : @xmath101 where @xmath102 are the interpolation weights . in appendix",
    "[ sec : appb ] we provide the proof of the theorem  @xcite , which is derived by a simple orthogonality argument , and relies on the specific structure of the functions in @xmath51 implied by , together with the reproducing property that uniquely characterizes the .",
    "the representer theorem dramatically simplifies the regularized optimization problem of   so it can be formulated as a linear optimization over a finite set of parameters @xmath102 .      in the previous section we formulated an optimization problem with manifold regularization for recovering the target function @xmath103 in .",
    "based on the representer theorem stated in , the optimization boils down to estimating the interpolation weights @xmath102 . substituting in yields a second - order polynomial objective function of @xmath104^t$ ] : @xmath105 where @xmath42 is the @xmath106 gram matrix of @xmath30 defined by @xmath107 ; @xmath108 is the @xmath80 identity matrix ; @xmath109 is a @xmath106 diagonal matrix : @xmath110 with @xmath12 ones and @xmath16 zeros on its diagonal ( functions as an indicator for the labelled samples in the set ) ; and @xmath111^t$ ] is a label vector comprising the @xmath12 known positions of the labelled samples with @xmath112 , for all @xmath113 . differentiating with respect to @xmath114 and comparing to zero , yields : @xmath115 by rearranging , we obtain the following linear system : @xmath116\\mathbf{a}=\\mathbf{q}.      \\label{eq : lineq}\\ ] ] accordingly , the interpolation weights @xmath114 are given by : @xmath117^{-1}\\mathbf{q}.     \\label{eq : weights}\\ ] ]    thus far , the computations were carried out offline based only on the training set , composed of both labelled and unlabelled samples .",
    "the input to the algorithm is a new pair of measurements @xmath118 , generated by an unknown source from an unknown location on the manifold .",
    "the corresponding feature vector @xmath34 is estimated according to .",
    "the kernel between the new sample @xmath34 and each of the training samples @xmath85 , is evaluated .",
    "the position of the new measurement is estimated according to by a weighted sum of these kernel evaluations multiplied by the weights given by : @xmath119      in this section we summarize the algorithm and formulate it in a dual - stage structure .",
    "we will take advantage of the fact that the optimization is derived in a semi - supervised manner , and propose an adaptive version .",
    "the algorithm is composed of two main parts : system adaptation and localization . in the adaptation stage ,",
    "the interpolation weights @xmath120 are computed according to   based on the labelled and unlabelled samples , which were collected up to this point in time . in the localization stage",
    ", we receive a new pair of measurements @xmath118 of an unknown source from an unknown location , and estimate the corresponding position based on the weights computed in the previous stage .",
    "the system is initialized with a small amount of labelled data , and after several iterations of the localization stage , the new unlabelled samples received during runtime , are utilized for system adaptation .",
    "note that the adaptation process can potentially adjust to changes in the environmental conditions .",
    "however , this attribute was not examined in the current paper that focuses on static configurations . examining dynamic scenarios with changing environmental conditions",
    "is left for future work .",
    "the proposed algorithm is summarized in algorithm  [ alg : algorithm1 ] and is illustrated in a flow diagram in fig .",
    "[ fig : flow ] .",
    "the flow diagram emphasizes the duality between the two parts of the algorithm and the interaction between them . in the downward direction , the model of the system derived in the adaptation part",
    "is utilized for localization . in the upward direction ,",
    "the new unlabelled samples acquired in the localization stage , are propagated and utilized for system adaptation .",
    "moreover , note that the two rightmost ( blue ) blocks are semi - supervised whereas the rest of the blocks are unsupervised .",
    "it should be emphasized that we do not present an update mechanism , but instead the weights are computed from scratch in each adaptation iteration .",
    "the development of a recursive version of the algorithm is left for future work .",
    "the number of localization iterations between two successive adaptations is chosen empirically to obtain satisfactory performance .",
    "note that if we choose a small value , increasing computational complexity , we will not gain much performance improvement . adding only a small amount of unlabelled information",
    "do not change the weights significantly .    :",
    "+    1 .   for each point",
    "estimate the corresponding @xmath95 according to .",
    "2 .   construct the reproducing kernel matrix @xmath42 and the adjacency matrix @xmath81 , according to and respectively , based on @xmath121 .",
    "3 .   compute the expansion weights @xmath120 according to .",
    "estimate the corresponding @xmath34 according to .",
    "compute the affinity between @xmath34 and each of @xmath121 , using the reproducing kernel .",
    "3 .   estimate the new point location using the estimated interpolation weights : @xmath122 .",
    "after a several number of newly acquired samples , return to system adaptation and add the new unlabelled samples .",
    "[ alg : algorithm1 ]",
    "in this section we briefly review a method for semi - supervised localization that was presented in  @xcite .",
    "this method , that will be termed , is a dual stage approach based on the concepts of diffusion maps  @xcite . in the first stage",
    "we recover the mapping between the original space @xmath35 and the embedded space @xmath123 which is governed by the controlling parameter , i.e. the position of the source .",
    "the second step is performing the localization by searching the neighbours of the new point among the training set in the new recovered space .",
    "note that both the and algorithms rely on the information implied by the manifold @xmath36 .",
    "nevertheless , there are several fundamental aspects that distinguish between the two , as will be elaborated in section  [ sec : discuss ] .      in the previous section we introduced a discrete representation of the manifold by a graph in which the training samples are the graph nodes , and the weights of the edges",
    "are defined according to the adjacency matrix @xmath81 of .",
    "the adjacency graph is normalized to obtain the transition matrix @xmath124 , which defines a markov process on the graph .",
    "accordingly , @xmath125 represents the probability of transition in a single markov step from node @xmath126 to node @xmath127 .",
    "a nonlinear mapping of the samples into a new embedded space is obtained by spectral decomposition of the transition matrix @xmath128 .",
    "the embedding is based on a parametrization of the manifold @xmath36 , which forms an intrinsic representation of the data .",
    "we apply singular value decomposition to the transition matrix @xmath128 , and pick the @xmath37 principal right - singular vectors @xmath129 that corresponds to the @xmath37 largest singular values @xmath130 .",
    "the @xmath37 principal right - singular vectors forms the diffusion mapping of the samples into an euclidean space @xmath123 , defined by : @xmath131^t .",
    "\\label{eq : map}\\ ] ] where @xmath132 denotes the @xmath133th entry of the vector @xmath134 .",
    "usually , @xmath135 is ignored since it is equal to a column vector of ones .    in the localization stage ,",
    "the embedding should be extended , given a new sample @xmath34 , corresponding to a new pair of measurements @xmath136 produced by unknown source from unknown location .",
    "further spectral decomposition is unnecessary according to nystrm extension .",
    "the new spectral coordinates are obtained by : @xmath137 where @xmath138 is an affinity vector between the training set and the new test point : @xmath139      in section  [ sec : manifold ] we described the structure of the acoustic manifold @xmath36 of the .",
    "we stated that in order to properly measure affinities between , we should use the geodesic distance , which is the shortest path on the manifold .",
    "an approximation of the geodesic distance is given by diffusion distance , defined as : @xmath140 where @xmath141 is the most dominant left - singular vector of @xmath128 .",
    "the diffusion distance incorporates information of the entire set to determine the connectivity between pairs of samples on the graph .",
    "pairs of points who are closely related to the same subset of points in the graph , are considered close to each other and visa versa .",
    "it can be shown that the diffusion distance is equal to the euclidean distance in the diffusion maps space when using all @xmath10 eigenvectors .",
    "this equivalence emphasizes the virtue of the diffusion mapping as it indicates that the mapping preserves the affinity between points with respect to the manifold .",
    "the diffusion distance can be well approximated by only the first @xmath37 principal eigenvectors  @xcite , i.e. , @xmath142    equipped with the ability to measure distances along the manifold using the diffusion distance , we are able to properly quantify the affinities between samples .",
    "samples which resides next to each other on the manifold , are assumed to be physically adjacent , i.e. , they are likely to represent sources from close positions .",
    "thus , the position of a new sample can be estimated by searching for its neighbours on the manifold .",
    "accordingly , the estimate will be formulated as a weighted sum of the positions of the labelled samples , where the weights are proportional to the corresponding diffusion distance between the new sample and each of the labelled samples : @xmath143 where the weights @xmath144 are given by : @xmath145 the procedure is summarized in algorithm  [ alg : dist ] .    note that both labelled and unlabelled samples participate in the first stage , for the construction of the graph laplacian . however , in the localization stage only the labelled samples are utilized because we rely on the labellings . though both and algorithms have evident similarities , we show in the experimental part that the later is inferior due to its different utilization of unlabelled data .",
    ": +    1 .   for each point",
    "estimate the corresponding @xmath95 according to",
    ". 2 .   construct the graph @xmath81 based on @xmath121 , and form the transition matrix @xmath128 .",
    "3 .   employ singular value decomposition of @xmath128 and obtain the singular - values @xmath146 and the right - singular vectors @xmath147 .",
    "4 .   construct the map @xmath148 according to to obtain an embedding that represents the intrinsic structure of manifold @xmath36 .",
    ": +    1 .   estimate the corresponding @xmath34 according to .",
    "apply nystrm extension according to to obtain the spectral coordinates of @xmath34 .",
    "3 .   compute the approximated diffusion distance between @xmath149 and each of the labelled samples @xmath150 , according to .",
    "4 .   estimate the new point location by as a linear combination of the positions of the labelled samples according to distances in the diffusion mapped space .",
    "we describe the simulated setup used for conducting the experimental study .",
    "we simulated a @xmath151  m room , using an efficient implementation  @xcite , of the image method  @xcite .",
    "in the room there are two microphones located at @xmath152  m and @xmath153  m , respectively .",
    "the source is known to be positioned at @xmath154  m distance with respect to the first microphone , on the same latitude .",
    "the goal is to recover the azimuth angle of the source .",
    "the initial analysis and examination of algorithms is carried out assuming that the azimuth angle of the source is ranging between @xmath155 .",
    "then , the algorithm performance is further demonstrated on a wider range of azimuth angles between @xmath156 .",
    "[ fig : setup ] illustrates the simulation setup .        for each location",
    ", we simulate a unique @xmath157  s speech signal , sampled at @xmath158  khz .",
    "the clean speech is convolved with the corresponding and is contaminated by a wgn .",
    "this forms the measured signals in the two microphones . for each source location , the and the are estimated with welch s method with @xmath159  s windows and @xmath160 overlap and are utilized for estimating the in   for @xmath161 frequency bins .      in this section",
    "we review the main results presented in  @xcite .",
    "we investigate the acoustic manifold of the and examine the proper distance between them that maintains physical adjacency .",
    "the analysis is carried out using a set of @xmath162 samples , corresponding to @xmath163 positions distributed uniformly in the specified range .",
    "two alternative distance measures for quantifying the affinity between different , are addressed .",
    "we start with the euclidean distance defined by : @xmath164 the euclidean distance is compared with the diffusion distance presented in section  [ sec : nn ] .    fig .",
    "[ fig : metrics&mapping](a ) depicts the euclidean distance and the diffusion distance between each of the and a reference corresponding to @xmath165 , as a function of the angle .",
    "we used moderate reverberation time of @xmath166  ms and @xmath167  db . we observe that the monotonic behaviour of the euclidean distance with respect to the angle is confined to approximately @xmath168 range .",
    "consequently , we conclude that the euclidean distance is meaningful only for small arcs . thus ,",
    "in general the euclidean distance is not a good distance measure between .",
    "however it can be properly utilized when inserted into a gaussian kernel in either the manifold regularization framework or the diffusion framework . according to its scaling parameter ,",
    "the gaussian kernel preserves small distances and suppresses large distances which are meaningless .",
    "the kernel scale should be adjusted to the distance at which monotonicity is maintained by the euclidean distance , in order to preserve locality .    for the diffusion distance ,",
    "only the first element in the mapping ( @xmath169 ) was considered .",
    "this choice will be justified in the sequel .",
    "we can see that for almost the entire range , the diffusion distance remains monotonic with respect to the angle , indicating that it is an appropriate metric in terms of the source .",
    "further insight into the mapping itself , is gained by plotting the single - element mapping @xmath170 , as depicted in fig .",
    "[ fig : metrics&mapping](b ) .",
    "we observe that the mapping corresponds well with the angle up to a monotonic distortion .",
    "thus , the diffusion mapping successfully reveals the latent variable , namely , the position of the source .",
    "the almost perfect matching between the first element of the mapping and the corresponding angle , justifies the use of @xmath169 for estimating the diffusion distance .",
    "to summarize , the presented results strengthen the claim on the existence of a nonlinear acoustic manifold . in small neighbourhoods around each point ,",
    "the manifold is approximately flat , meaning that it resembles an euclidean ( linear ) space . for larger scales the affinity between",
    "should be determined according to the geodesic distance on the manifold .",
    "the diffusion framework successfully reveals the latent variable controlling the acoustic manifold , and the diffusion distance properly reflects the distances on the manifold .",
    "these results motivate the involvement of manifold aspects in the localization process , as introduced by either the or the algorithms .      in this section",
    "we examine the ability of both and to recover the of the source . the training set consists @xmath162 representative samples distributed uniformly between @xmath155 . among the training set ,",
    "only @xmath171 samples were labelled , creating a grid with approximately @xmath172 distance between adjacent labelled samples , as depicted in fig .",
    "[ fig : setup ] .",
    "the performance is examined on a set of @xmath173 additional samples produced by unknown sources from unknown locations , confined to the defined range .",
    "the performance is measured according to the , defined by : @xmath174 where @xmath175 stands for the azimuth angle of the source . to prevent the results from being dependent on a specific reflection pattern of a certain room section",
    ", we repeated the simulation with rotations of the constellation described above .",
    "the rotation angle was generated uniformly between @xmath176 .",
    "the positions of the second microphone , the training points and the test points were rotated by this angle , with respect to the first microphone .",
    "the was averaged over @xmath177 rotations of the constellation .",
    "the results of the and the algorithms are compared with that obtained by the classical algorithm  @xcite for both noisy and reverberant conditions . in the first scenario",
    "we examine the algorithms performance for different reverberation times with fixed of @xmath167  db . in the second scenario the reverberation time",
    "is set to @xmath166  ms , and different noise levels are examined .",
    "the training set is generated with fixed level of @xmath178  db .",
    "the of the three algorithms in both scenarios , are shown in fig .",
    "[ fig : compare_rmse](a ) and ( b ) , respectively .",
    "it can be seen in fig .",
    "[ fig : compare_rmse](a ) that the performs well for low reverberation . however , its performance deteriorates gradually as reverberation increases , and becomes inferior compared with the performance of both the and the algorithms . in high reverberation , the is incapable of distinguishing between the direct arrival and the reflections . a misidentification of the direct path , results in a large estimation error . the proposed algorithms are more robust to reverberation , since the variations in the entire are taken in account .",
    "similar behaviour is observed in fig .",
    "[ fig : compare_rmse](b ) in which different noise levels are examined . here",
    "too , the method behaves well only in high conditions , and its performance significantly degrades as noise level increases . when the measurements are contaminated by a significant amount of noise , the correlation between the two measurements is also very noisy , and the can not correctly identify the peak corresponding to the direct path . on the contrary ,",
    "the semi - supervised algorithms are much more robust with respect to the background noise , and most of the time obtain lower error .",
    "these type of algorithms can compensate for the information loss caused by the poor conditions , by capitalizing on the prior information inferred from the training samples .",
    "we also observe that the approach exhibits better results compared with method .",
    "the reason for the visible gap between the of the two algorithms is related to the different ways they utilize unlabelled data , and will be further elaborated in section  [ sec : discuss ] .",
    "finally , we examine the iterative process of the algorithm through the following sequential simulation .",
    "we used reverberation time of @xmath179  ms and @xmath167  db .",
    "this time we examined a wider range of angles between @xmath156 .",
    "the initial adaptation was based on only @xmath180 labelled samples , creating a grid of @xmath172 distance between adjacent labelled samples , as depicted in fig .",
    "[ fig : setup ] .",
    "we conducted @xmath181 cycles of the sequential algorithm , each comprised of both stages of system adaptation and localization . in the localization stage , we estimated the angles of @xmath182 new samples from unknown locations .",
    "the total of the all set was computed . in the following iteration ,",
    "these @xmath182 new samples were treated as additional unlabelled data , utilized for system adaptation .",
    "the results are summarized in fig .",
    "[ fig : seq ] .    , where @xmath182 unlabelled points are added in each iteration .",
    "ms and snr=@xmath167  db , scaledwidth=50.0% ]    in this figure we observe that the decreases as a function of the number of iterations , indicating that the unlabelled data has an important role in reducing the estimation error .",
    "however , after a considerable amount of unlabelled data is accumulated , the process stabilizes on a certain error , and additional samples are redundant .",
    "in the previous section we demonstrated the robustness of the and the algorithms to noisy and reverberant conditions . we have also seen that the performance of the method is inferior with respect to that of the algorithm . in this section",
    "we discuss the interfacing points of both algorithms , on the one hand , and highlight the major differences between them , on the other hand .",
    "to investigate the role of the unlabelled data in the method , we inspect the expansion weights @xmath120 derived by the algorithm , as depicted in fig .",
    "[ fig : a_weights ] .",
    "the blue line corresponds to the weights of @xmath184 unlabelled examples , while the red x - marks corresponds to the weights of @xmath185 labelled examples .",
    "we observe a monotonic , almost linear , behaviour of the coefficients with respect to the angle .",
    "the obtained behaviour of the coefficients , resembles the monotonic relation between the single - element diffusion mapping @xmath170 and the corresponding angle , depicted in fig .",
    "[ fig : metrics&mapping](b ) .",
    "the correspondence between the two algorithms , suggests that they share similar aspects which lead to a parametrization of the manifold and recovery of the of the source .",
    "however , we have seen that the is a better localizer compared with the .",
    "the difference between the two , is attributed to their different utilization of the unlabelled data . in the algorithm ,",
    "the unlabelled data are used only in the learning phase , and the estimation merely comprises the positions of the labelled samples . in contrast , in the unlabelled data do not only take part in the recovery of the manifold , but also participate in the estimation itself , involving both labelled and unlabelled data .",
    "another advantage of over is that it is sequentially updated , hence , it is more suitable for on - line implementations .     with respect to the corresponding angle .",
    "the blue line corresponds to the weights of @xmath184 unlabelled examples , while the red x - marks corresponds to the weights of @xmath185 labelled examples.,title=\"fig:\",scaledwidth=50.0% ]",
    "a novel approach for semi - supervised localization , based on state - of - the - art manifold learning techniques , was presented .",
    "a set of representative samples in a defined room section is utilized for learning the acoustic manifold of the and building a data - driven model .",
    "equipped with this knowledge , we find the function relating the samples and the corresponding positions by solving a regularized optimization problem in an .",
    "simulation results confirm the algorithm robustness in noisy and reverberant environments .",
    "integrating between traditional signal processing techniques and novel machine learning tools may be the key for better addressing adverse conditions , such as high noise levels and reverberations , that are the main causes for performance degradation of classical localization approaches .",
    "the current results indicate that the manifold perspective exhibits an interesting insight into the general structure of the acoustic responses and offers better solutions for common signal processing problems .",
    "we define the integral operator on functions , associated with the kernel @xmath30 , by the following integral transform : @xmath186=\\int k(\\mathbf{t},\\mathbf{s})f(\\mathbf{s})d\\mathbf{s } = g(\\mathbf{t}).\\ ] ] the eigenfunctions @xmath65 and eigenvalues @xmath187 of the integral operator satisfy : @xmath188=\\int k(\\mathbf{t},\\mathbf{s})\\psi_i(\\mathbf{s})d\\mathbf{s } = \\lambda_i\\psi_i(\\mathbf{t}).\\ ] ] according to mercer s theorem , the kernel @xmath30 can be expanded by : @xmath189 where the convergence is absolute and uniform .",
    "the eigenfunctions @xmath190 form an orthogonal set and the can be defined as the space of functions spanned by this set : @xmath191 where the norm is defined by the inner product : @xmath192 the reproducing property holds in this representation , since : @xmath193        any function @xmath195 can be uniquely decomposed into @xmath154 components , which one is lying in the linear subspace spanned by the kernel functions in the training examples @xmath196 and the other is lying in the orthogonal complement @xmath197 : @xmath198 where @xmath199 for all @xmath200 .",
    "the above orthogonal decomposition and the reproducing property together , show that the evaluation of @xmath103 on any training point @xmath201 is independent of the orthogonal component @xmath197 : @xmath202 consequently , the value of the empirical terms involving the loss function and the intrinsic norm in the optimization problem ( the first and the third terms , respectively ) , are independent of @xmath197 .",
    "for the second term ( the norm of @xmath103 in @xmath51 ) , since @xmath197 is orthogonal to @xmath203 and only increases the norm of @xmath103 in @xmath51 , we have @xmath204 therefore setting @xmath205 does not affect the first and the third terms of , while it strictly decreases the second term .",
    "it follows that any minimizer @xmath206 of must have @xmath205 , and therefore admits a representation : @xmath207 ."
  ],
  "abstract_text": [
    "<S> conventional speaker localization algorithms , based merely on the received microphone signals , are often sensitive to adverse conditions , such as : high reverberation or low . in some scenarios , e.g. in meeting rooms or cars , it can be assumed that the source position is confined to a predefined area , and the acoustic parameters of the environment are approximately fixed . </S>",
    "<S> such scenarios give rise to the assumption that the acoustic samples from the region of interest have a distinct geometrical structure . in this paper , we show that the high dimensional acoustic samples indeed lie on a low dimensional manifold and can be embedded into a low dimensional space . </S>",
    "<S> motivated by this result , we propose a semi - supervised source localization algorithm which recovers the inverse mapping between the acoustic samples and their corresponding locations . </S>",
    "<S> the idea is to use an optimization framework based on manifold regularization , that involves smoothness constraints of possible solutions with respect to the manifold . </S>",
    "<S> the proposed algorithm , termed , is implemented in an adaptive manner . </S>",
    "<S> the initialization is conducted with only few labelled samples attached with their respective source locations , and then the system is gradually adapted as new unlabelled samples ( with unknown source locations ) are received . </S>",
    "<S> experimental results show superior localization performance when compared with a recently presented algorithm based on a manifold learning approach and with the algorithm as a baseline .    </S>",
    "<S> sound source localization , , manifold regularization , , diffusion distance . </S>"
  ]
}