{
  "article_text": [
    "recommendation ( or recommender ) systems are capable of predicting user responses to a large set of options  @xcite . they are generally implemented in web site applications related with music , video , shops , among others , and collect information about preferences of different users in order to predict the next preferences .",
    "more recently , social network sites such as facebook , also started to use recommender algorithms  @xcite .",
    "many recommendation systems are implemented using matrix factorization algorithms  @xcite . given a user - item interaction matrix @xmath0 , the objective of these algorithms is to find two matrices @xmath1 and @xmath2 such that @xmath3 approximate @xmath0 .",
    "matrix @xmath1 represents user profiles and @xmath2 represents items . with @xmath1 and @xmath2",
    "we can easily predict the preference of user @xmath4 regarding an item @xmath5 .",
    "the fact that these recommendation systems are based on matrices operations make them suitable to parallelization .",
    "in fact , due to the huge sizes of @xmath1 and @xmath2 and the nature of these algorithms , many authors have pursued the parallelization path .",
    "for example , popular algorithms like the alternating least squares ( als ) , or the stochastic gradient descent ( sgd ) have parallel versions for either shared - memory or distributed memory architectures  @xcite .",
    "recently , yu _ et al . _",
    "@xcite demonstrated that coordinate descent based methods ( ccd ) have a more efficient update rule compared to als .",
    "they also show more stable convergence than sgd .",
    "they implemented a new recommendation algorithm using ccd as the basic factorization method and showed that ccd++ is faster than both sgd and als .    with the increasing popularity of general purpose graphics processing units ( gpgpu ) , and their suitability to data parallel programming , algorithms that are based on data matrices operations",
    "have been successfully deployed to these platforms , taking advantage of their hundreds of cores . regarding recommendation systems ,",
    "we are only aware of the work of zhanchun _ et al .",
    "_  @xcite that implemented a neighborhood - based algorithm for gpus . in this paper",
    ", we describe gpu implementations of two recommendation algorithms based on matrix factorization .",
    "this is , to our knowledge , the first proposal of this kind in the field .",
    "we implement ccd++ and als gpu versions using the cuda programming model in windows and linux .",
    "we tested our versions on typical benchmarks found in the literature .",
    "we compare our results with an existing multi - core version of ccd++ and our own multi - core implementation of als .",
    "our best results with gpu - ccd++ and gpu - als versions show speedups of 14.8 and 6.2 , respectively over their sequential versions ( single core ) . the fastest cuda version ( ccd++ on windows ) is faster than the fastest 32-core version .",
    "all results on the gpu and multi - core have the same recommendation quality ( same root mean squared error ) as the sequential implementations .",
    "gpu - ccd++ can be a better parallelization choice over a multi - core implementation , given that it is much cheaper to buy a machine with a gpgpu with hundreds of cores than to buy a multi - core machine with a few cores .",
    "next , we present basic concepts about gpu programming and architecture , explain the basics of factorization algorithms and their potential to parallelization , describe our own parallel implementation of als and ccd++ , show results of experiments performed with typical benchmark data and , finally , we draw some conclusions and perspectives of future work .",
    "the cuda programming model  @xcite , developed by nvidia , is a platform for parallel computing on graphics processing units ( gpu ) .",
    "one single host machine can have one or multiple gpus , having a very high potential for parallel processing .",
    "gpus were mainly designed and used for graphics processing tasks , but currently , with tools like cuda or opencl  @xcite , other kinds of applications can take advantage of the many cores that a gpu can provide .",
    "this motivated the design of what today is called a gpgpu ( general purpose graphics processing unit ) , a gpu for multiple purposes  @xcite .",
    "gpus fall in to the single - instruction - multiple - data ( simd ) architecture category , where many processing elements simultaneously run the _ same program _",
    "but on distinct data items .",
    "this program , referred to as the _ kernel _ , can be quite complex including control statements such as _ if _ and _",
    "while_.    scheduling work for the gpu is as follows . a thread in the host platform ( e.g. , a multi - core ) first copies the data to be processed from host memory to gpu memory , and then invokes gpu threads to run the _ kernel _ to process the data .",
    "each gpu thread has a unique i d which is used by each thread to identify what part of the data set it will process .",
    "when all gpu threads finish their work , the gpu signals the host thread which will copy the results back from gpu memory to host memory and schedule new work  @xcite .",
    "gpu memory is organized hierarchically and each ( gpu ) thread has its own _ per - thread local _ memory .",
    "threads are grouped into _",
    "blocks _ , each _ block _ having a memory _ shared _ by all threads in the _ block_. finally , thread _ blocks _ are grouped into a single _ grid _ to execute a _ kernel _  different _ grids _ can be used to run different _",
    "kernels_. all _ grids _ share the _ global memory_.",
    "all data transfers between the host ( cpu ) and the gpu are made through reading and writing global memory , which is the slowest . a common technique to reduce the number of reads from global memory is _ coalesced memory access _ , which takes place when consecutive threads read consecutive memory locations allowing the hardware to coalesce the reads into a single one .",
    "programming a gpgpu is not a trivial task when algorithms do not present regular computational patterns when accessing data . but",
    "a gpu brings a great advantage over multiprocessors , since it has hundreds of processing units that can perform data parallelism , present in many applications , specially the ones that are based on recommendation algorithms , and because it is much cheaper than a ( cpu ) with a few cores .",
    "collaborative filtering recommendation algorithms can be implemented using different techniques , such as neighborhood - based and association rules",
    ". one powerful family of collaborative filtering algorithms use another technique known as matrix factorization  @xcite resourcing to the uv - decomposition or svd ( singular value decomposition ) matrix factorization methods .",
    "svd is also commonly used for image and video compression  @xcite .",
    "the uv - decomposition approach is applied to learning a recommendation model as follows .",
    "matrix @xmath0 is the @xmath6 ratings matrix and contains a non - zero @xmath7 value for each ( user @xmath4)(item @xmath5 ) interaction . using a matrix factorization ( mf ) algorithm",
    ", we obtain matrices @xmath8 and @xmath9 whose product approximates @xmath0 ( figure  [ fig : uvdecomp ] ) .",
    "the matrix @xmath1 profiles the users using @xmath10 latent features , known as factors . the matrix @xmath2 profiles the items using the same features . by the nature of the recommendation problem",
    ", @xmath0 is a sparse matrix that contains mostly zeros ( user - item pairs without any interaction ) .",
    "in fact , this matrix is never explicitly represented , but we can estimate any of its unknown values @xmath7 by computing the dot product of row @xmath4 of @xmath1 and row @xmath5 of @xmath2 . with these estimated values",
    "we can produce recommendations .",
    ".,scaledwidth=30.0% ]    the matrices @xmath1 and @xmath2 are obtained by minimizing the objective function in eq .",
    "( [ eq : mainobjectivefunction ] ) . in this function",
    ", @xmath11 is the classification matrix , @xmath12 is the number of users and @xmath13 is the number of items .    @xmath14    assuming that the classification matrix is sparse ( i.e.",
    ", a minority of ratings is known ) , @xmath15 is the set of indexes related to the observed classifications ( ratings ) , @xmath4 is the user counter and @xmath5 is the item counter .",
    "the sparse data is represented by the triplet @xmath16 .",
    "the @xmath17 parameter is a regularization factor , which determines how precise will be the factorization given by the objective function . in other words",
    ", it allows to control the error level and overfitting . the frobenius norm indicated by @xmath18 ,",
    "is used to calculate the distance between the matrix @xmath0 and its approximate matrix @xmath19 . in this context , @xmath20 ,",
    "is the frobenius norm , which consists of calculating @xmath21 .",
    "the lower the integer produced by the summation , the nearer is @xmath0 to @xmath22  @xcite .",
    "the @xmath23 vector corresponds to line @xmath4 of matrix @xmath1 and the @xmath24 vector corresponds to the line @xmath5 of matrix @xmath2 .",
    "summarizing , the objective function is used to obtain an approximation of the incomplete matrix @xmath0 , where @xmath1 and @xmath2 are matrices @xmath25 .",
    "it is not trivial to directly calculate the minimum of the objective function in eq .",
    "( [ eq : mainobjectivefunction ] ) .",
    "therefore , to solve the problem , several methods are used .",
    "next , we explain some of them , most relevant to this work .",
    "this method divides the minimization function in two quadratic functions .",
    "that way , it minimizes @xmath1 keeping @xmath2 constant and it minimizes @xmath2 keeping @xmath1 constant . when @xmath2 is constant to minimize @xmath1 , in order to obtain an optimal value to @xmath26 , the function in eq .",
    "( [ eq : als_objectivefunction ] ) is derived .",
    "@xmath27    next , it is necessary to minimize function in eq .",
    "( [ eq : als_objectivefunction ] ) .",
    "the expression :    @xmath28    gives a minimal value for @xmath29 , given that @xmath17 is always positive .",
    "the algorithm alternates between the minimizations of @xmath1 and @xmath2 until its convergence , or until it reaches a determined number @xmath30 of iterations , given by the user  @xcite .    in our implementation",
    ", the inverse matrix is obtained using the cholesky decomposition , since it is one of the most efficient methods for matrix inversion  @xcite .",
    "the complete sequential als is shown in algorithm  [ alg : als_alg ] .",
    "the algorithm is very similar to als , but instead of minimizing function in eq .",
    "( [ eq : mainobjectivefunction ] ) for all elements of @xmath2 or @xmath1 , it minimizes the function for each element of @xmath2 or @xmath1 at each iteration step  @xcite . assuming @xmath31 represents the line @xmath4 of @xmath1",
    ", then @xmath32 represents the element of line @xmath4 and column @xmath33 . in order to operate element by element , the objective function in eq .",
    "( [ eq : mainobjectivefunction ] ) needs to be modified such that only @xmath32 can be assigned a @xmath34 value .",
    "this reduces the problem to a single variable problem , as shown in function in eq .",
    "( [ eq : mainccdfunc ] ) .",
    "@xmath35 given that this algorithm performs a non - negative matrix factorization and function in eq .",
    "( [ eq : mainccdfunc ] ) is invariably quadratic , it has one single minimum . therefore , it is sufficient to minimize function in eq .",
    "( [ eq : mainccdfunc ] ) in relation to @xmath34 , obtaining eq .",
    "( [ eq : minto_z_ccd ] ) .",
    "@xmath36    finding @xmath37 requires @xmath38 iterations .",
    "if @xmath10 is large , this step can be optimized after the first iteration , thus requiring only @xmath39 iterations . in order to do that",
    ", it suffices to keep a residual matrix @xmath40 such that @xmath41 . therefore , after the first iteration , and after obtaining @xmath42 , the minimization of @xmath43 becomes : @xmath44 having calculated @xmath37 , the update of @xmath32 and @xmath42 proceeds as follows : @xmath45 @xmath46 after updating each variable @xmath47 using  ( [ eq : updateomega ] ) , we need to update the variables @xmath48 in a similar manner , obtaining : @xmath49 @xmath50 @xmath51 having obtained the updating rules shown in eqs .",
    "( [ eq : updaterbyz ] ) ,  ( [ eq : updateomega ] ) ,  ( [ eq : updaterbys ] ) and  ( [ eq : updateh ] ) , we can now apply any sequence of updates to @xmath1 and @xmath2 .",
    "next , we describe two ways of performing the updates : item / user - wise and feature - wise .      in this type of updating , @xmath1 and",
    "@xmath2 are updated as in algorithm  [ alg : ccdalg ] .",
    "in the first iteration @xmath1 is initialized with zeros , therefore the residual matrix @xmath40 is exactly equal to @xmath0 .",
    "assuming that @xmath52 corresponds to the columns of @xmath1 and @xmath53 , the columns of @xmath2 , the factorization @xmath54 can be represented as a summation of @xmath10 outer products . @xmath55",
    "some modifications need to be made to the original ccd functions . assuming that @xmath56 and @xmath57 are the vectors to be injected over @xmath52 and @xmath53 , then @xmath56 and @xmath57 can be calculated using the following minimization : @xmath58    @xmath59 is the residual entry of @xmath60 . but using this type of update , there is one more possibility which is to have pre - calculated values using a second residual matrix @xmath61 : @xmath62 this way , the objective function equivalent to ( [ eq : mainobjectivefunction ] ) is rewritten as : @xmath63 to obtain @xmath56 it suffices to minimize the function ( [ eq : mainccdfunc_feauturefinal ] ) regarding @xmath64 : @xmath65 to obtain @xmath57 it suffices to minimize ( [ eq : mainccdfunc_feauturefinal ] ) regarding @xmath66 : @xmath67 finally , after obtaining @xmath56 e @xmath57 we update",
    "@xmath68 and @xmath42 :    @xmath69    @xmath70    algorithm  [ alg : ccdppalg ] formalizes the feature - wise update of ccd , called ccd++ .",
    "parallelizing als consists of distributing the matrices @xmath1 and @xmath2 among threads .",
    "synchronization is needed as soon as the matrices are updated in parallel  @xcite .",
    "algorithm  [ alg : alsp_alg ] shows the modifications related to the sequential als algorithm .",
    "we also parallelized als for cuda .",
    "data are copied to the gpu and the host is responsible for the synchronization .",
    "when the computation finishes in the gpu , @xmath1 and @xmath2 are copied from the device to the host .",
    "algorithm  [ alg : alsp_cuda_alg ] shows how als was parallelized using cuda .",
    "allocate gpu memory for matrices @xmath0 , @xmath1 and @xmath2 copy matrices @xmath0 , @xmath1 and @xmath2 from the host to the gpu",
    "in the ccd++ algorithm , each solution is obtained by alternately updating @xmath1 and @xmath2 . when @xmath71 is constant , each variable @xmath64 is updated independently ( eq . [ eq : minui_ccdfeauture ] ) .",
    "therefore , the update of @xmath72 can be made by several processing cores .    given a computer with @xmath73 cores , we define the partition of the row indexes of @xmath74 as @xmath75 .",
    "vector @xmath72 is decomposed in @xmath73 vectors @xmath76 , where @xmath77 is the sub - vector of @xmath72 corresponding to @xmath78 .",
    "when the matrix @xmath1 is uniformly split in parts @xmath79 , there is a load balancing problem due to the variation of the size of the row vectors contained in @xmath1 . in this case , the exact amount of work for each @xmath80 core to update @xmath77 is given by @xmath81  @xcite .",
    "therefore , different cores have different workloads .",
    "this is one of the limitations of this algorithm .",
    "it can be overcome using dynamic scheduling , which is offered by most parallel processing libraries ( e.g. openmp  @xcite ) .    for each subproblem",
    ", each core @xmath80 builds @xmath82 with , @xmath83 where @xmath84 .",
    "then , for each core @xmath80 we have , @xmath85 the update of @xmath2 is analogous to the one of @xmath1 in ( [ eq : minui_ccdfeauture_p ] ) . for @xmath73 cores",
    "the row indexes of @xmath86 are partitioned into @xmath87 .",
    "so , for each core @xmath80 we have , @xmath88 since all cores share the same memory , no communication is needed to access @xmath72 and @xmath71 .",
    "after obtaining @xmath89 , the update of @xmath40 and @xmath90 is also implemented in parallel by the @xmath80 cores as follows .",
    "@xmath91    @xmath92    algorithm  [ alg : ccdppccd_parallel ] summarizes the parallel ccd operations .",
    "our cuda implementation of the ccd++ algorithm uses explicit memory management .",
    "it is inspired by the parallel version of ccd++ found in libpmf ( library for large - scale parallel matrix factorization ) .",
    "this is an open source library for linux  @xcite .",
    "libpmf is implemented in c++ for multi - core environments with shared memory .",
    "the parallel version uses the openmp library  @xcite .",
    "it employs double precision values .",
    "our version uses floats because gpus are faster when floats are used .",
    "allocate memory on gpu for matrices @xmath0 and @xmath40 and for vectors @xmath72 and @xmath71 copy matrices @xmath0 and @xmath40 from host to gpu    algorithm  [ alg : ccdppalg_cuda ] shows our implementation of the ccd++ for the gpus .",
    "we use the same stream in all copies from host to device , device to host and for _ kernels_. therefore , each of the operations is always blocking with respect to the main thread in the host .",
    "we performed our experiments using two operating systems : windows 8.1 pro x64 and linux fedora 20 .",
    "the cuda versions for these two systems can vary greatly in performance .",
    "the hardware used is described as follows : * gpu : * gainward geforce gtx 580 phantom , @xmath93 @xmath94 , with total dedicated memory 3 gb gddr5 and 512 cuda cores ; * processors : * 2 @xmath95 intel^^ xeon^^ x5550 , @xmath96 , with 24 gb of ram ( 6 @xmath95 4 gb hynix hmt151r7bfr4c - h9 ) ; * motherboard : * tyan s7020wagm2nr .",
    "all experiments use the netflix dataset ( 100,480,507 ratings that 480,189 users gave to 17,770 movies ) .",
    "our qualitative evaluation metric is the root mean squared error ( rmse ) produced on the probe data generated by the model .",
    "our quantitative measure is the speedup ( how fast it is the parallel implementation related to the sequential , calculated as the sequential execution time divided by the parallel execution time ) .",
    "ideally , we needed a secondary gpu with dedicated memory , but this was not possible . in our gpu , the memory is shared with the display memory .",
    "we used 16 blocks of 512 threads in our experiments .",
    "the parameters used by both ccd++ and als are @xmath97 , @xmath98 and @xmath99 .",
    "these were selected according to an empirical selection .",
    "lower values of @xmath10 give better speedups for the gpu implementation , while a variation of the @xmath10 values does not impact the multi - core implementation .",
    "higher values of @xmath10 also implies that more data will be copied to the gpu memory , which is not advisable .",
    "we performed our experiments with two versions of the ccd++ , one using float ( single decimal precision ) and the other using doubles ( double decimal precision ) , in order to evaluate how the gpu would behave with both kinds of numeric types .",
    "all experiments for ccd++ resulted on rmse equals to @xmath100 and for als resulted in rmse equals to @xmath101 .",
    "table  [ table : omp_libpmf_speedup_netflix ] shows the performance of the original ccd++ ( using the library libpmf ) on the multi - core machine with linux and windows , running the netflix benchmark , using the original double decimal precision ( c double ) .",
    "the speedups achieved in windows are higher than in linux , but this was expected , since the base execution of windows ( 717.3 s for 1 thread ) is higher than the linux ( 521.5 s ) .",
    "the maximum speedup achieved is 4.4 with 32 threads .",
    "[ table : omp_libpmf_speedup_netflix ]    table  [ table : omp_cuda_libpmf_speedup_netflix ] shows the same experiments , but now with our version of ccd++ , that uses a single decimal precision .",
    "the results are exactly the same in terms of rmse , but the performance is highly benefited by the numeric data type in this case . by using floats , instead of doubles , we reach speedups of 9.5 ( at 32 threads ) , which is more than twice the speedup achieved with the version that used a double numeric representation .",
    "note that the original libpmf uses doubles instead of floats .",
    "we could achieve even better speedups than they reported , by just using single precision data .",
    "the use of float or double did not affect much the linux implementations , but it considerably affected the windows implementations .",
    "again , with this version , the windows implementation achieves higher speedups than linux .",
    "this was expected , since the base execution time for 1 thread is much higher for windows .",
    "[ table : omp_cuda_libpmf_speedup_netflix ]    but our best results are for the cuda implementation .",
    "we obtained a speedup of 14.8 just using the gpu running our implementation of the ccd++ in windows .",
    "we managed to surpass the performance of a machine that costs more than twice as much as a gpu card , showing that these architectures have a great potential for the implementation of recommender systems based on matrix factorization .",
    "[ table : omp_cuda_ccdpp_speedup_netflix ]    we also implemented the als algorithm in cuda and results are presented in table  [ table : omp_cuda_ccdpp_speedup_netflix ] for comparison . in this table",
    "we show execution times and speedups for the multi - core version and for the gpu .",
    "once more the multi - core version presents better speedups with a higher number of threads , for the windows environment .",
    "we showed the advantage of using gpus to implement recommender systems based on matrix factorization algorithms . using a benchmark popular in the literature , netflix",
    ", we obtained maximum speedup of 14.8 , better than the best speedup reported in the literature .",
    "the advantages of using a cuda implementation over a multi - core server are : lower energy consumption , lower price and the ability of leaving the main host or other cores to be used by other tasks .",
    "currently , almost every computer comes with pci slots that can be used to install a gpu or various gpus .",
    "thus , it is relatively simple to expand the computational capacity of an existing hardware .",
    "we plan to perform more tests with our algorithms on more recent gpus and on larger datasets .",
    "one potential problem of gpus is their memory limitation .",
    "therefore , one path to follow is to implement efficient memory management mechanisms capable of dealing with bigger data .",
    "another track we would like to follow is to implement a load balancing mechanism to these algorithms .",
    "national funds through the fct - fundao para a cincia e a tecnologia ( proj .",
    "fcomp-01 - 0124-feder-037281 ) ."
  ],
  "abstract_text": [
    "<S> we describe gpu implementations of the matrix recommender algorithms ccd++ and als . </S>",
    "<S> we compare the processing time and predictive ability of the gpu implementations with existing multi - core versions of the same algorithms . </S>",
    "<S> results on the gpu are better than the results of the multi - core versions ( maximum speedup of 14.8 ) .    </S>",
    "<S> = 1000 = = 1 = 1 </S>"
  ]
}