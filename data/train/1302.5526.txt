{
  "article_text": [
    "in the main text , we derived a formula  given there as eq .",
    "( 1)for the probability @xmath52 that a lexicon of words is learned by time @xmath7 if they are learned independently by cross - situational learning .",
    "this read @xmath59 \\big\\rangle\\ ] ] where @xmath60 only if word @xmath1 has been presented by time @xmath7 , and @xmath61 is @xmath45 if word @xmath1 has never been presented , or , if in every presentation up to time @xmath7 , the confounding meaning @xmath62 has always appeared alongside .",
    "the angle brackets denote an average over all possible exposure sequences . under all other conditions ,",
    "these indicator variables are zero .",
    "this equation was first of all presented in an alternative form which follows from the fact that @xmath12 implies that @xmath8 for all @xmath63 .",
    "hence , for all allowed combinations of @xmath64 and @xmath61 , we have the identity @xmath65 a_{i , j}(t ) = [ 1-e_i(t)]$ ] which can be rearranged to obtain @xmath66 = [ 1-a_{i , j}(t)]$ ] . assuming that there is at least one confounding meaning for each word , the @xmath64 variables in the above equation are then redundant , and the more concise form @xmath67 \\big\\rangle\\ ] ] then applies .    in the main text , we discussed models where contexts of confounding meanings were independently sampled from distributions that may be word - dependent , but remain fixed over time .",
    "in particular , this implies that the contexts appearing against different words are independent , and we have factorization of the average into word - dependent factors : @xmath68 \\big\\rangle \\;.\\ ] ] since the confounder distributions are fixed , we find after @xmath69 presentations of word @xmath1 that @xmath70 where @xmath71 is the joint probability that all @xmath32 meanings @xmath72 appear in a single episode .",
    "since word @xmath1 is presented as a poisson process with frequency @xmath2 , we find that @xmath73 t}\\end{aligned}\\ ] ] therefore , on multiplying out the average in ( [ factor ] ) , we find a sum of exponential decays .",
    "we are interested in the slowest decay mode , which corresponds to the highest possible value of @xmath71 among all possible sets of confounding meanings .",
    "as noted in the main text , any combination of meanings @xmath74 can not appear more frequently than the least frequent meaning among that subset .",
    "if , for each word , the individual meaning frequencies @xmath75 are distinct for different @xmath9 , there will be a unique maximum appearance frequency , and the slowest decay is given by @xmath76 .",
    "multiplying the factors for each word @xmath1 together yields eq .",
    "( 2 ) of the main text .",
    "we note that in the special case where the most frequent meaning is @xmath77-fold degenerate , we acquire a prefactor @xmath77 in front of the dominant exponential decay .    for the case where @xmath23 is the same for all words @xmath1 , eq .",
    "( 3 ) in the main text is obtained by taking the logarithm of @xmath52 , replacing the sum with an integral , and expanding the logarithm to first order . for a zipf distribution of word frequencies , @xmath3 , @xmath78",
    ", this procedure yields @xcite @xmath79 where we have used the asymptotics of the exponential integral @xcite to obtain the second approximate equality .",
    "the solution of the equation @xmath80 yields the learning time given by eq .",
    "this involves the lambert w function which is defined by solutions of the equation @xmath81 @xcite .",
    "in the interacting rz model described in the main text , it is assumed that all meanings are labeled and so eq .  ( 4 ) for the learning probability simplifies to @xmath82 \\right\\rangle \\;,\\ ] ] where here @xmath44 for an ordered subset @xmath35 of the @xmath0 meanings .",
    "numerical investigations of the rz sampling procedure reveal that , when the context size is large , the most frequent confounders are all very likely to appear ( @xmath83 for the lowest @xmath9 ) , while the relative frequencies of _ non - appearance _ diverge with @xmath17 , i.e. , that @xmath84 as @xmath17 is increased . since it is these non - appearance probabilities , @xmath85 , that enter into the decay rates of correlation functions ( see above ) , it follows that the slowest - decaying confounder loops are those that are ( a ) short ; and ( b ) comprise only the most frequent meanings .",
    "the slowest decay of all loops therefore comes from @xmath51 .",
    "we have found that a good match between theory and numerical data is obtained by assuming this is the _ only _ loop that contributes to the late - time behavior of @xmath52 .    to obtain the theoretical prediction",
    ", we first make this single - loop approximation : @xmath86 \\right\\rangle \\nonumber\\\\ & = \\prod_{i=1}^{w } \\langle e_i(t ) \\rangle \\left [ 1 - \\frac { \\langle e_1(t)a_{1,2}(t ) \\rangle \\langle e_2(t)a_{2,1}(t ) \\rangle } { \\langle e_1(t ) \\rangle \\langle e_2(t ) \\rangle } \\right]\\;,\\end{aligned}\\ ] ] where we have used the fact that the contexts presented alongside different words are uncorrelated .",
    "@xmath87 is the probability that an event governed by a poisson process with frequency @xmath2 has occurred at least once by time @xmath7 .",
    "hence , @xmath88 \\;.\\ ] ] this is of the same form as eq .",
    "( 3 ) of the main text , but with @xmath50 , and so from ( [ lgzipf ] ) we have that @xmath89    turning now to the second term in ( [ eaa ] ) , we use again the identity @xmath65 a_{i , j}(t ) = [ 1-e_i(t)]$ ] from the previous section , to find that @xmath90 .",
    "hence , @xmath91 t } - { \\rm e}^{-\\phi_i t}}{1 - { \\rm e}^{-\\phi_i t } } \\;.\\ ] ] if @xmath92 is close to unity , as is the case for the high - frequency meanings in the rz model , we have at late times that @xmath93 t } \\;.\\ ] ]    combining this result with ( [ fm ] ) in ( [ eaa ] ) , and noting that @xmath3 , we arrive at @xmath94 \\right)\\ ] ] which gives an asymptotic expression for the learning probability as a function of time . to convert this into a learning time ,",
    "we need to solve the equation @xmath95 .",
    "unfortunately , we have not been able to do this exactly .",
    "it is however straightforward now to identify the slowest decay mode of @xmath52 by expanding out : @xmath96 thus , as stated in the main text , we find that the mode associated with exposure of the entire lexicon decays at a rate @xmath53 , and that the mode associated with elimination of the confounder loop decays at rate @xmath97 .    now , as @xmath98",
    ", the learning time @xmath7 must diverge towards infinity .",
    "hence , as @xmath21 is reduced , the subleading term in ( [ lexp ] ) can be made arbitrarily small , and the solution of @xmath99 yields the learning time @xmath18 to better and better accuracy in the limit @xmath98 .",
    "formally , as @xmath98 , the function @xmath100 exhibits a nonanalyticity at @xmath101 .",
    "it is in this sense that we regard this model to exhibit a dynamical phase transition .    for more general models , in which more than one candidate loop enters at large times",
    ", we have found that including only loops of length 2 in the product in ( [ lme ] ) yields very good agreement with simulation data .",
    "more precisely , numerically - determined roots of @xmath102 with @xmath52 given by the approximate expression @xmath103 \\prod_{\\langle i , j \\rangle } \\left [ 1 - \\lambda_{i , j } \\lambda_{j , i } \\right]\\ ] ] correspond well with simulation data , and furthermore provides evidence for our claim that the dynamical phase transition reported is not a peculiarity of the specific model discussed in the main text .",
    "44ifxundefined [ 1 ] ifx#1 ifnum [ 1 ] # 1firstoftwo secondoftwo ifx [ 1 ] # 1firstoftwo secondoftwo `` `` # 1''''@noop [ 0]secondoftwosanitize@url [ 0 ]",
    " + 12$12  & 12#1212_12%12@startlink[1]@endlink[0]@bib@innerbibempty @noop _",
    "( ,  ,  ) @noop _ _  ( ,  )  pp .",
    "@noop * * ( ) @noop * * ,   ( ) @noop * * ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop _ _  ( ,  , ) @noop * * ,   ( ) @noop * * ,   ( ) @noop _ _  ( ,  ,  ) @noop * * ,   ( ) in  @noop _ _  ( ,  ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) link:\\doibase 10.1073/pnas.1105040108 [ * * ,   ( ) ] @noop * * ,   ( ) @noop * * ,   ( ) in  @noop _ _ ,  , vol .",
    "( ,  , )  pp .",
    "@noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop _ _  ( ,  ,  ) @noop * * ,   ( ) @noop * * ,   ( ) in  @noop _ _ ( )  pp .   @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop @noop * * ,   ( ) @noop _ _ ,  ed ."
  ],
  "abstract_text": [
    "<S> we study the time taken by a language learner to correctly identify the meaning of all words in a lexicon under conditions where many plausible meanings can be inferred whenever a word is uttered . </S>",
    "<S> we show that the most basic form of _ cross - situational learning_whereby information from multiple episodes is combined to eliminate incorrect meanings  can perform badly when words are learned independently and meanings are drawn from a nonuniform distribution . </S>",
    "<S> if learners further assume that no two words share a common meaning , we find a phase transition between a _ maximally efficient _ learning regime , where the learning time is reduced to the shortest it can possibly be , and a _ partially - efficient _ regime where incorrect candidate meanings for words persist at late times . </S>",
    "<S> we obtain exact results for the word - learning process through an equivalence to a statistical mechanical problem of enumerating loops in the space of word - meaning mappings .    on average , </S>",
    "<S> children learn ten words a day , thereby amassing a lexicon of 60,000 words by adulthood @xcite . </S>",
    "<S> this speed of learning is remarkable given that every time a speaker says a word , a hearer can not be certain of its intended meaning @xcite . </S>",
    "<S> our aim is to identify which of the many proposed mechanisms for eliminating uncertainty can actually deliver such rapid word learning . in this work </S>",
    "<S> , we pursue this aim in the long tradition of applying quantitative methods from statistical mechanics to problems in learning @xcite and communication @xcite .    </S>",
    "<S> empirical research suggests that two basic types of learning mechanism are involved in word learning . </S>",
    "<S> first , a learner can apply various _ heuristics_e.g . </S>",
    "<S> , attention to gaze direction @xcite or prior experience of language structure at the moment a word is produced to hypothesize a set of plausible meanings . however , these heuristics may leave some residual uncertainty as to a word s intended meaning in a single instance of use </S>",
    "<S> . if the heuristics are weak , the set of candidate meanings could be very large . </S>",
    "<S> this residual uncertainty can be eliminated by comparing separate instances of a word s use : if only one meaning is plausible across all such instances , it is a very strong candidate for the word s intended meaning . </S>",
    "<S> this second mechanism is referred to as _ cross - situational learning _ @xcite . </S>",
    "<S> formally , it can be couched as a process whereby associations between words and meanings are strengthened when they co - occur @xcite , as in neural network models for learning @xcite . </S>",
    "<S> it can also be viewed as an error - correction process @xcite where a target set of associations is reconstructed from noisy data .    </S>",
    "<S> there is little consensus as to which word - learning mechanisms are most important in a real - world setting @xcite . in part </S>",
    "<S> this is because word - learning experiments ( e.g.  @xcite ) are necessarily confined to small lexicons . </S>",
    "<S> a major question is whether strategies observed in experiments allow realistically large lexicons to be learned rapidly : this can be fruitfully addressed through stochastic dynamical models of word learning @xcite . in these models , </S>",
    "<S> a key control parameter is the _ context size _ : </S>",
    "<S> the number of plausible , but unintended , meanings that typically accompany a single word s true meaning . </S>",
    "<S> even when contexts are large , the rapid rate of learning seen in children is reproduced in models where words are learned independently by cross - situational learning @xcite . </S>",
    "<S> this suggests that powerful heuristics , capable of filtering out large numbers of spurious meanings , are not required . </S>",
    "<S> however , a recent simulation study @xcite shows that this conclusion relies on the assumption that these unintended meanings are uniformly distributed . in the more realistic scenario where different meanings are inferred with different probabilities , </S>",
    "<S> word learning rates can decrease dramatically as context sizes increase . </S>",
    "<S> powerful heuristics may be necessary after all .    </S>",
    "<S> one heuristic , of great interest to empiricists ( e.g.  @xcite ) and modelers ( e.g.  @xcite ) , is a _ mutual exclusivity constraint _ @xcite . here </S>",
    "<S> , a learner assumes that no two words may have the same meaning . </S>",
    "<S> this generates nontrivial interactions between words which makes analysis of the corresponding models difficult . for example , if one begins with a master equation , as in @xcite , the expressions become unwieldy to write down , let alone solve . here , we adopt a fundamentally different approach which entails identifying the criteria that must be satisfied for a lexicon to be learned . </S>",
    "<S> this allows existing results for the simple case of independently - learned words and uniform meaning distributions @xcite to be generalized to arbitrary meaning distributions _ and _ exactly solves the interacting problem to boot . </S>",
    "<S> our main result is that mutual exclusivity induces a dynamical phase transition at a critical context size , below which the lexicon is learned at the _ fastest _ possible rate ( i.e. , the time needed to encounter each word once ) . as far as we are aware , the ability of a single heuristic to deliver such fast learning has not been anticipated in earlier work .     </S>",
    "<S> acquisition of a three - word lexicon . </S>",
    "<S> solid shapes are meanings that have appeared in every episode alongside a word ; open shapes are therefore excluded as candidate meanings . </S>",
    "<S> ( a ) in the noninteracting case , only the meaning of the word ` square' is learned . </S>",
    "<S> ( b ) in the interacting case , mutual exclusivity further removes meanings ( shown hatched ) of learned words , both prospectively and retrospectively ( shown by arrows ) . </S>",
    "<S> all three words are learned in this example . ]    </S>",
    "<S> we begin by defining our model for lexicon learning . </S>",
    "<S> the lexicon comprises @xmath0 words , and each word @xmath1 is uttered as a poisson process with rate @xmath2 . in all cases </S>",
    "<S> , we take words to be produced according to the zipf distribution , @xmath3 , that applies for the @xmath4 most frequent words in english @xcite . here , @xmath5 so that one word appears on average per unit time . </S>",
    "<S> each time a word @xmath1 is presented , the intended _ </S>",
    "<S> target _ meaning is assumed always to be inferred by the learner by applying some heuristics . at the same time , a set of non - target _ confounding _ meanings , called the _ context _ , is also inferred .    in the purest version of cross - situational learning @xcite , </S>",
    "<S> a learner assumes that all meanings that have appeared every time a word has been uttered are plausible candidate meanings for that word . </S>",
    "<S> the word becomes _ learned _ when the target is the only meaning to have appeared in each episode . in the _ noninteracting _ case , each word is learned independently  see fig .  </S>",
    "<S> [ xsl]a . in the _ </S>",
    "<S> interacting _ case , mutual exclusivity acts to further exclude the meanings of learned words as candidates for other words . </S>",
    "<S> we take this exclusion to occur at the instant a word is learned , which means a single learning event may trigger an avalanche of other learning events by repeated application of mutual exclusivity . </S>",
    "<S> an example of this nontrivial effect that is hard to handle within standard approaches @xcite is shown in fig .  </S>",
    "<S> [ xsl]b . here </S>",
    "<S> , learning `` square '' causes `` circle '' to be learned at the same time .    </S>",
    "<S> we consider the noninteracting case first both to introduce our more powerful analytical approach and to pinpoint the origin of the catastrophic increase in learning times noted in @xcite . </S>",
    "<S> two conditions must be satisfied for the lexicon to be learned by a given time : ( c1 ) all words must have been exposed at least once ; and ( c2 ) no confounding meaning may have appeared in every episode that any given word was uttered . to express these conditions mathematically , we introduce two stochastic indicator variables . </S>",
    "<S> we take @xmath6 if word @xmath1 has been uttered before time @xmath7 , and zero otherwise ; and @xmath8 if confounding meaning @xmath9 has appeared in every context alongside word @xmath1 up to time @xmath7 ( or if word @xmath1 has never been presented ) , and zero otherwise . </S>",
    "<S> conditions ( c1 ) and ( c2 ) then imply that the probability that the lexicon has been learned by time @xmath7 is @xmath10 \\big\\rangle =   \\big\\langle \\prod_{i\\ne j } [ 1 - a_{i , j}(t ) ] \\big\\rangle\\ ] ] where the angle brackets denote an average over all sequences of episodes that may occur up to time @xmath7 . </S>",
    "<S> the second equality holds because @xmath11 if @xmath12 .    </S>",
    "<S> this expression is valid for any distribution over contexts . for brevity </S>",
    "<S> , we consider a single , highly illustrative construction that we call _ resampled zipf _ ( rz ) . </S>",
    "<S> it is based on the idea that meaning frequencies should follow a similar distribution to word forms @xcite . </S>",
    "<S> it works by associating an ordered set , @xmath13 , of @xmath14 confounding meanings with each word @xmath1 . </S>",
    "<S> the @xmath15 meaning in each set has an _ a priori _ statistical weight @xmath16 . </S>",
    "<S> whenever word @xmath1 appears , meanings are repeatedly sampled from @xmath13 with their _ a priori _ weights , and added to the context if they are not already present until a context of @xmath17 distinct meanings has been constructed . </S>",
    "<S> when words are learned independently , the learning time depends only on @xmath14 , @xmath0 and @xmath17 , and not on which meanings are present in any given set @xmath13 @xcite .    </S>",
    "<S> we seek the time , @xmath18 , at which the lexicon is learned with some high probability @xmath19 . in the rz model </S>",
    "<S> , each context is an independent sample from a fixed distribution . </S>",
    "<S> hence , the correlation functions @xmath20 in ( [ lni ] ) all decay exponentially in time . to find @xmath18 to good accuracy in the small-@xmath21 limit , only the slowest decay mode for each word @xmath1 is needed . </S>",
    "<S> higher - order correlation functions depend on many meanings co - occurring , and so decay more rapidly than lower - order correlation functions . </S>",
    "<S> as shown in appendix  [ app : a ] , we find that at late times ( [ lni ] ) is well approximated by @xmath22\\ ] ] where @xmath23 is the fraction of episodes in which word @xmath1 s most frequent confounder appears alongside the target . </S>",
    "<S> this expression generalizes results for independently - learned words @xcite from uniform to _ arbitrary _ nonuniform confounder distributions .    </S>",
    "<S> the rz model has the further simplification that @xmath23 has a common value , @xmath24 , for all words @xmath1 . </S>",
    "<S> then , it is known from previous calculations @xcite for zipf - distributed word frequencies that the learning time is @xmath25 where @xmath26 is the principal branch of the lambert w function @xcite . for large argument </S>",
    "<S> , this function behaves as a logarithm .    in fig .  </S>",
    "<S> [ rz1 ] , we compare the analytical result ( [ tast1 ] ) with learning times obtained from direct monte carlo simulations , conducted as detailed in @xcite . </S>",
    "<S> the only complication is that we unfortunately have no analytic expression for @xmath24 arising from the rz procedure . </S>",
    "<S> we therefore obtain the frequency of the most common confounder for given @xmath17 and @xmath14 from independent monte carlo samples . the agreement between ( [ tast1 ] ) and simulation </S>",
    "<S> is very good .     </S>",
    "<S> time to learn a lexicon of @xmath0 words independently to a residual probability @xmath27 with @xmath17 of @xmath14 confounders present in each episode . </S>",
    "<S> points : data from monte carlo simulations ( over @xmath28 sampled lexicons in each case ) . </S>",
    "<S> lines : the analytical result , eq .  </S>",
    "<S> ( [ tast1 ] ) . ]    fig .  </S>",
    "<S> [ rz1 ] also shows that the learning time increases super - exponentially with the context size . </S>",
    "<S> we have found that the probability the @xmath15 most confounder appears in a context of size @xmath17 fits the form @xmath29 where @xmath30 is the _ a priori _ probability and @xmath31 is a fitting parameter that depends on @xmath14 and @xmath32 . </S>",
    "<S> as noted by vogt @xcite , the repeated sampling without replacement implies that @xmath33 . </S>",
    "<S> our analysis further reveals that the learning time is _ entirely _ determined by the frequency of the most common confounder , @xmath24 through ( [ tast1 ] ) . </S>",
    "<S> we note that this is true even when other confounders have comparable appearance frequencies ( @xmath34 ) .    </S>",
    "<S> we now turn to the case where the mutual exclusivity constraint serves to exclude the meanings of learned words as possible meanings for other words . in this case , it is important to distinguish between _ labeled _ and _ unlabeled _ meanings : an unlabeled meaning is not the target meaning of any word in the lexicon , and hence can not be excluded using the mutual exclusivity constraint . </S>",
    "<S> to generalize eq .  </S>",
    "<S> ( [ lni ] ) to this problem , we must identify the conditions for the lexicon to be learned . </S>",
    "<S> condition ( c1 ) still applies : each word must be uttered at least once for a learner to be able to learn it . </S>",
    "<S> condition ( c2 ) now applies only to unlabeled confounding meanings : these can only be excluded if they fail to appear in a context , as before . </S>",
    "<S> when these two conditions are satisfied , there is a third  necessary and sufficient  condition for the lexicon to be learned that takes into account all the interactions and avalanches generated by the mutual exclusivity constraint . </S>",
    "<S> this is condition ( c3 ) : no _ candidate loops _ exist at time @xmath7 . </S>",
    "<S> a candidate loop , @xmath35 , is a subset of distinct , labeled meanings whereby each meaning @xmath36 has appeared alongside the word associated with meaning @xmath37 ( or @xmath38 if @xmath39 ) every time it has been uttered . </S>",
    "<S> inspection of fig .  </S>",
    "<S> [ xsl]b shows that the one candidate loop ( , @xmath40 ) that exists after the third episode is destroyed in the fourth . </S>",
    "<S> then , in the fifth episode , the final word appears , and since no unlabeled meaning is a candidate for any word , the entire three - word lexicon is learned .    to see why condition ( c3 ) is necessary and sufficient in general </S>",
    "<S> when ( c1 ) and ( c2 ) hold , we first show that a candidate loop must exist if the lexicon has not been learned . </S>",
    "<S> suppose word @xmath41 has not been learned . </S>",
    "<S> then , at least one meaning , @xmath42 , must confound word @xmath41 . </S>",
    "<S> word @xmath42 must also not have been learned , otherwise meaning @xmath42 would not confound word @xmath41 . </S>",
    "<S> hence , word @xmath42 must be confounded by a meaning , @xmath43 , and so on . </S>",
    "<S> as there is a finite set of words , this sequence of meanings must eventually form a loop .    </S>",
    "<S> we now show the lexicon can not have been learned if a candidate loop exists by first assuming that it _ has _ been learned under these conditions . </S>",
    "<S> then , if word @xmath41 was learned at time @xmath7 , word @xmath42 must have been learned before time @xmath7 for mutual exclusivity to act ( even if words @xmath41 and @xmath42 are learned as part of the same avalanche ) . iterating this argument around the loop , one finds that word @xmath41 can only have become learned at time @xmath7 if it had already been learned at some earlier time . </S>",
    "<S> this contradiction therefore implies that the absence of candidate loops and a learned lexicon are equivalent .    </S>",
    "<S> we again use indicator variables to translate conditions ( c1)(c3 ) into an exact expression for the learning probability . introducing @xmath44 that equals @xmath45 </S>",
    "<S> if the loop @xmath46 persists at time @xmath7 , we have @xmath47 \\prod_{\\ell } \\left [ 1 - c_{\\ell}(t ) \\right ] \\right\\rangle \\;,\\ ] ] again valid for any distribution of confounding meanings . here , </S>",
    "<S> meanings @xmath45 to @xmath0 correspond to words @xmath45 to @xmath14 , and so meanings with an index @xmath48 are unlabeled . </S>",
    "<S> the product over @xmath46 is over all possible candidate loops . </S>",
    "<S> this expression has the remarkable property that it is expressed concisely in terms of the word and confounder appearance frequencies alone : the avalanche dynamics triggered by mutual exclusivity do not enter explicitly . </S>",
    "<S> this property , reminiscent of the avalanche dynamics of abelian sandpile models @xcite , reduces analysis of the learning probability to the statistical mechanical problem of enumerating candidate loops .    in the interacting problem , </S>",
    "<S> the structure of each candidate set @xmath13 is important , as this determines which words interact . </S>",
    "<S> we consider a model which has no unlabeled meanings and where each set @xmath13 is a sample of @xmath14 non - target meanings obtained via the rz prescription . </S>",
    "<S> then , in each episode , @xmath17 meanings are drawn from the relevant candidate set using rz again , but with an _ a priori _ weight @xmath16 where @xmath32 is the rank of a meaning within the set @xmath13 when ordered by the frequency of the corresponding words . </S>",
    "<S> thus meanings of high - frequency words are high - frequency confounders . learning times from monte carlo simulations </S>",
    "<S> are shown in fig .  </S>",
    "<S> [ rzl ] .     as fig .  </S>",
    "<S> [ rz1 ] but with the mutual exclusivity constraint . </S>",
    "<S> points : data from monte carlo simulations ( 100,000 lexicons for @xmath49 , at least 2,500 lexicons for larger @xmath17 ) . </S>",
    "<S> dotted lines : time for the entire lexicon to have been exposed with residual probability @xmath27 . dashed lines : time for the slowest decaying candidate loop to remain with probability @xmath21 . </S>",
    "<S> solid line : time to learn lexicon independently , eq .  </S>",
    "<S> ( [ tast1 ] ) , for comparison . ]    </S>",
    "<S> we observe two distinct learning - time regimes . at small @xmath17 </S>",
    "<S> , the learning time is constant , and close to the time it takes for all words in the lexicon to appear at least once . </S>",
    "<S> ( this time is given by eq .  </S>",
    "<S> ( [ tast1 ] ) with @xmath50 ) . in this regime , learning is as fast as it can possibly be : mutual exclusivity is _ maximally efficient _ and reverses the undesirable increase in learning times that arises from nonuniform confounder distributions . above a critical context size , the learning time rises , but remains much smaller than when words are learned independently : mutual exclusivity is _ partially efficient _ in this regime .    our exact result ( [ lme ] ) </S>",
    "<S> can be used to explain these observations , details of which appear in appendix  [ app : b ] . for the rz model as described above , it turns out that only one confounder loop @xmath51 is relevant at late times . </S>",
    "<S> consequently , the learning probability @xmath52 is asymptotically given as the product of two factors . </S>",
    "<S> the first gives the probability that all words have been encountered by time @xmath7 , and approaches unity exponentially with rate @xmath53 . the second is the probability that the loop @xmath51 has not decayed away : this approaches unity with rate @xmath54 . </S>",
    "<S> the appearance frequency of the most frequent confounder , @xmath24 , increases with context size . </S>",
    "<S> when @xmath55 , the slowest relaxational mode of the learning probability is associated with each word being uttered at least once , whereas for larger values , the slowest mode comes from eliminating the confounder loop . in this latter partially - efficient regime , the lexicon learning time is predicted as @xmath56 for small @xmath21 , in very good agreement with simulation data ( see fig .  </S>",
    "<S> [ rzl ] ) . </S>",
    "<S> we describe the sudden change in the dominant relaxational behavior  a phenomenon seen also in driven diffusive systems @xcite  as a _ dynamical phase transition_. it is broadly reminiscent of transitions exhibited by combinatorial optimization problems , whereby the number of unsatisfied constraints increases from zero above a critical difficulty threshold @xcite . in the present case </S>",
    "<S> the learning problem remains solvable in both regimes , but there is a transition from a regime where it is solved in constant time to one where the time grows super - exponentially in the difficulty of the problem ( here , the context size ) .    to summarize , we have found that mutual exclusivity is an extremely powerful word - learning heuristic . </S>",
    "<S> it can yield lexicon learning times in the presence of uncertainty that coincide with the time taken for each word to be heard at least once . </S>",
    "<S> empirical data ( summarized in @xcite ) suggests that this is easily fast enough for realistic lexicons of @xmath57 words to be learned . to enter the partially - efficient regime , </S>",
    "<S> each word s most frequent confounder would need to be present in at least @xmath58 of all episodes : even then , learning is over @xmath0 times faster than when mutual exclusivity is not applied . </S>",
    "<S> the dynamical transition between a maximally- and partially - efficient regime also appears to be present in a variety of word - learning models we have investigated , e.g. , those in which confounder frequencies are uncorrelated with their corresponding word frequencies , or using less memory - intensive learning strategies @xcite . </S>",
    "<S> we also expect the transition to be evident in models where the target meaning does not always appear , at least in the regime where learning is possible @xcite . </S>",
    "<S> we believe the analytical methods introduced in this work should allow more detailed quantities to be calculated , e.g. , the distribution of learning times for a given word , which would shed light on such phenomena as the childhood vocabulary explosion at around 18 months @xcite . </S>",
    "<S> similar thinking may also allow analysis of other nonequilibrium dynamical systems whose master equations are hard to solve directly . finally , our results suggest new empirical questions , such as whether high - frequency confounders correlate with high - frequency words , and the extent to which learners are able to apply the mutual - exclusivity constraint retroactively . </S>",
    "<S> we therefore contend that statistical physicists can contribute much to the understanding of how children learn the meaning of words .    </S>",
    "<S> _ acknowledgments _ </S>",
    "<S>  we thank mike cates and cait macphee for comments on the manuscript . </S>"
  ]
}