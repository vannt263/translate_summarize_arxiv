{
  "article_text": [
    "the problem of identifying relevant covariates in a regression model , sometimes known as variable selection , arises frequently in many fields . as computational and data - collection technologies have developed , the number of covariates typically measured in these kinds of problems has steadily increased , and it is now not unusual to come across data sets involving many thousands or millions of covariates . here",
    "we consider one particular setting where data sets of this size are common : genome - wide association studies ( gwas ) .",
    "current typical gwas [ e.g. , @xcite ] measure hundreds of thousands , or millions , of genetic variants ( typically single nucleotide polymorphisms , or snps ) , in hundreds , thousands , or tens of thousands of individuals , with the primary goal being to identify which regions of the genome harbor snps that affect some phenotype or outcome of interest .",
    "while many gwas are case - control studies , here we focus primarily on the computationally - simpler setting where a continuous phenotype has been measured on population - based samples , before briefly considering the challenges of extending these methods to binary outcomes .",
    "most existing gwas analyses are `` single - snp '' analyses , which simply test each snp , one at a time , for association with the phenotype . strong associations between a snp and the phenotype are interpreted as indicating that snp , or a nearby correlated snp , likely affects phenotype .",
    "the primary rationale for gwas is the idea that , by examining these snps in more detail  for example , examining which genes they are in or near  we may glean important insights into the biology of the phenotype under study .    in this paper",
    "we examine the potential to apply bayesian variable selection regression ( bvsr ) to gwas ( or other similar large - scale problems ) .",
    "variable selection regression provides a very natural approach to analyzing gwas : the phenotype is treated as the regression response , snps become regression covariates , and the goal of identifying genomic regions likely to harbor snps affecting phenotype is accomplished by examining the genomic locations of snps deemed likely to have nonzero regression coefficients .",
    "however , bvsr requires the use of computationally - intensive markov chain monte carlo ( mcmc ) algorithms , and , prior to performing this work , it was unclear to us whether such algorithms could produce reliable results in a practical time - frame for problems as large as a typical gwas .",
    "one important contribution of this paper is to show that , even using relatively simple mcmc algorithms , bvsr can indeed produce useful inferences in problems of this size .",
    "another important contribution is to discuss _ how _ bvsr should be used for gwas analysis , with particular focus on choice of appropriate prior distribution .",
    "further , and perhaps most importantly , we give reasons _",
    "why _ one might want to use bvsr to analyze gwas  rather than less computationally - demanding approaches such as single - snp analyses , or penalized regression such as lasso [ @xcite]by emphasizing qualitative advantages of bvsr in this context .",
    "in particular , we emphasize that , unlike penalized regression approaches , bvsr naturally produces easily - interpretable measures of confidence  specifically , posterior probabilities  that individual covariates have nonzero regression coefficients .",
    "this is a particularly important advantage in gwas because the primary goal of the analysis is to identify such covariates , and to use these identifications to learn about underlying biology ( in contrast to other settings where prediction may be the primary goal ) .",
    "although our work is motivated by gwas , many of the ideas and results should be of more general interest .",
    "in brief , the key elements are as follows :    * we demonstrate that bvsr can be practical for large problems involving hundreds of thousands of covariates and thousands of observations . *",
    "we introduce some new ideas for prior specification in bvsr .",
    "in particular , we emphasize the benefits of focusing on what the priors imply about the total proportion of variance in response explained by relevant covariates ( henceforth abbreviated as pve ) .",
    "we note that standard approaches to prior specification in bvsr , which put the same priors on the regression coefficients irrespective of how many covariates are included in the model , imply that models with many relevant covariates are likely to have much larger pve than models with few relevant covariates . we propose a simple alternative prior that does not make this potentially undesirable assumption , and has the intuitively appealing property that it applies stronger shrinkage in more complex models ( i.e. , models with more relevant covariates ) . *",
    "we emphasize the potential for bvsr to estimate the total amount of signal in a data set , specifically the pve , even when there is insufficient information to reliably identify all relevant covariates . as a result",
    ", bvsr has the potential to shed light on the so - called `` missing heritability '' observed in many gwas [ @xcite ; @xcite ] .",
    "* we compare and contrast bvsr with a penalized - regression approach , the lasso [ @xcite ] . despite the considerable literature on both bvsr and penalized regression , there",
    "exist few comparisons ( either qualitative or quantitative ) of these two approaches .",
    "we chose the lasso as a representative of penalized regression approaches both because of its popularity and because previous papers have applied it to the specific context of gwas [ e.g. , @xcite ; @xcite ] . in our limited simulation study",
    "bvsr outperforms lasso in terms of predictive performance .",
    "in addition , we emphasize the qualitative advantage of bvsr over lasso , and other penalized regression methods , that it produces posterior probabilities for each covariate having a nonzero regression coefficients .",
    "this qualitative advantage seems more fundamental , since predictive performance of different methods may vary depending on the underlying assumptions .",
    "the remainder of the paper is organized as follows . in section [ sec : modelpriors ] we describe bvsr and our choice of priors . in section [ sec : ci ] we discuss computation and inference , including markov chain monte carlo algorithms used , and a rao  blackwellization approach to estimating the marginal posterior inclusion probability for each covariate . section [ sec : goals ] reviews our main goals in applying bvsr to gwas . in section  [ sec : res ] we examine , through simulations , the effectiveness of bvsr for various tasks , including estimating the pve , prediction , and identifying relevant covariates . for some of these tasks we compare bvsr with lasso and single - snp analyses .",
    "we also illustrate bvsr on a gwas for c - reactive protein . in section",
    "[ sec : binary ] we briefly consider the challenges of extending our methods to deal with binary phenotypes . finally , in section  [ sec : discussion ] we discuss some limitations and pitfalls of bvsr as we have applied it in this context , and potential future directions.=1",
    "this section introduces notation and specifies the details of our bvsr model and priors used .",
    "our formulation up  to  section  [ subsec : novel ] is in the same vein as much previous work on bvsr , but with particular emphasis on putting priors on hyperparameters that are often considered fixed and known .",
    "key relevant references include @xcite , @xcite , @xcite , @xcite and @xcite ; see also @xcite and @xcite for more background and references .",
    "we consider the standard normal linear regression @xmath0 relating a response variable @xmath1 to covariates @xmath2 . here",
    "@xmath1 is an @xmath3-vector of observations on @xmath3 individuals , @xmath4 is an @xmath3-vector with components all equal to the same scalar @xmath5 , @xmath2 is an @xmath3 by @xmath6 matrix of covariates , @xmath7 is a @xmath6-vector of regression coefficients , @xmath8 denotes the inverse variance of the residual errors , @xmath9 denotes the @xmath3-dimensional multivariate normal distribution and @xmath10 the @xmath3 by @xmath3 identity matrix . the variables @xmath1 and @xmath2 are observed , whereas @xmath11 , and @xmath8 are parameters to be inferred . in more detail , @xmath12 , where @xmath13 is the measured response on individual @xmath14 , and @xmath15 , where @xmath16 is a column vector containing the observed values of the @xmath17th covariate . for example , in the context of a gwas , @xmath13 is the measured phenotype of interest in individual @xmath14 , and @xmath18 is the genotype of individual @xmath14 at snp @xmath17 , typically coded as 0 , 1 or 2 copies of a particular reference allele .",
    "[ by coding the genotypes as 0 , 1 , or 2 , we are assuming an additive genetic model .",
    "it would be straightforward to include dominant and recessive effects by adding another covariate for each snp , as in @xcite , e.g. , although this would increase computational cost . ]    in many contexts , including gwas , the number of covariates is very large  and , in particular , @xmath19but only a small subset of the covariates are expected to be associated with the response ( i.e. , have nonzero @xmath20 ) .",
    "indeed , the main goal of gwas is to identify these relevant covariates . to this end , we define a vector of binary indicators @xmath21 that indicate which elements of @xmath22 are nonzero . thus , @xmath23 where @xmath24 denotes the design matrix @xmath2 restricted to those columns @xmath17 for which @xmath25 , and @xmath26 denotes a corresponding vector of regression coefficients . in general , for observational studies one would be reluctant to conclude any causal interpretation for @xmath27 , but in the context of gwas , it is usually reasonable to interpret @xmath25 as indicating that snp @xmath17 , or an unmeasured snp correlated with snp @xmath17 , has a causal ( functional ) affect on @xmath1 .",
    "this is because in gwas reverse causation is generally implausible ( phenotypes can not causally affect genotype , since genotype comes first temporally ) , and there are few potential unmeasured confounders that could affect both genotype and @xmath1 [ @xcite ] .",
    "a  well - documented exception to this is population structure ; here we assume that this has been corrected for prior to analysis , for example , by letting @xmath1 be the residuals from regressing the observed phenotype values against measures of population structure , obtained , for example , by model - based clustering [ @xcite ] or principal components analysis [ @xcite ] .    taking a bayesian approach to inference , we put priors on the parameters : @xmath28 where @xmath29 , @xmath30 denotes the vector of @xmath7 coefficients for which @xmath31 , and @xmath32 denotes a point mass on 0 .",
    "here @xmath33 , and @xmath34 are hyperparameters .",
    "the hyperparameters @xmath35 and @xmath36 have important roles , with @xmath35 reflecting the sparsity of the model , and @xmath36 reflecting the typical size of the nonzero regression coefficients . rather than setting these hyperparameters to prespecified values , we place priors on them , hence allowing their values to be informed by the data ; the priors used are detailed below .",
    "( later we will argue that this ability to infer @xmath35 and @xmath36 from the data is an important advantage of analyzing all snps simultaneously , rather than one at a time . )",
    "the remaining hyperparameters are less critical , and , in practice , we consider the posterior distributions for which @xmath37 and @xmath38 , which has the attractive property that the resulting relative marginal likelihoods for  @xmath27 are invariant to shifting or scaling of @xmath1 .",
    "thus , for example , inference of which genetic variants are associated with height would be unaffected by whether height is measured in meters or inches .",
    "[ taking these limits is effectively equivalent to using the improper prior @xmath39 , but we prefer to formulate proper priors and take limits in their posteriors , to verify sensible limiting behavior . ]",
    "the parameter @xmath35 controls the sparsity of the model , and where the appropriate level of sparsity is uncertain a priori , as is typically the case , it seems important to specify a prior for @xmath35 rather than fixing it to an arbitrary value . in gwas , and probably in many other settings with extreme sparsity , uncertainty in @xmath35",
    "may span orders of magnitude : for example , there could be just a few relevant covariates or hundreds . in this case a uniform prior on  @xmath35 seems inappropriate , since this would inevitably place most of the prior mass on larger numbers of covariates ( e.g. , uniform on @xmath40 to @xmath41 puts about @xmath42 probability on @xmath4310@xmath44 ) .",
    "instead , we put a uniform prior on @xmath45 : @xmath46 where @xmath47 and @xmath48 , so the lower and upper limits on  @xmath35 correspond , respectively , to an expectation of @xmath49 and @xmath50 covariates in the model . in applications here we used @xmath51 , with this arbitrary limit being imposed partly due to computational considerations ( larger @xmath50 can increase computing time considerably ) .",
    "the assumption of a uniform distribution is , of course , somewhat artificial but has the merit of being easily interpretable .",
    "an alternative , which may be preferable in some settings , would be a normal prior on @xmath52 .",
    "the above formulation , with the exception of our slightly nonstandard prior on @xmath35 , follows previous work .",
    "however , since many formulations of bvsr differ slightly from one another , we now comment on some of the choices we made :    we chose , in ( [ eqn : betaprior ] ) , to put independent priors on the elements of @xmath26 .",
    "an alternative common choice is zellner s @xmath53-prior [ @xcite ; @xcite ] , which assumes correlations among the regression coefficients mimicking the correlations among covariates , @xmath54 for gwas we prefer the independent priors because we view the @xmath7 s as reflecting causal effects of @xmath2 on @xmath1 , and there seems to be no good reason to believe that the correlation structure of causal effects will follow that of the snps .",
    "some authors center each of the vectors @xmath1 and @xmath55 to have mean @xmath56 , and set @xmath57 .",
    "this approach yields the same posterior on @xmath27 as our limiting prior on @xmath5 ( derivation omitted ) , and simplifies calculations , and so we use it henceforth .",
    "it is common in variable selection problems to scale the covariates @xmath58 to each have unit variance , to avoid problems due to different variables being measured on different scales . in gwas",
    "these covariates are measured on the same scale , being counts of the reference allele , and so we do not scale the covariates in this way in our examples . however , one could so scale them , which would correspond to a prior assumption that snps with less variable genotypes ( i.e. , those with a lower minor allele frequency ) have larger effect sizes ; see @xcite for relevant discussion .",
    "the priors assume that the @xmath20 are exchangeable , and , in particular , that all covariates are , a priori , equally plausible candidates to affect outcome @xmath1 . in the context of a gwas ,",
    "this assumption means we are ignoring information that might make some snps better candidates for affecting outcome than others .",
    "our priors also ignore the fact that functional snps may tend to cluster near one another in the genome .",
    "these choices were made purely for simplicity ; one attractive feature of bvsr is that one could modify the priors to incorporate these types of information , but we leave this to future work .    some formulations of bvsr use a similar `` sparse '' prior , where the marginal prior on @xmath20 is a mixture of a point mass at 0 and a normal distribution , whereas others [ e.g. , @xcite ] instead use a  mixture of two normal distributions , one with a substantially larger variance than the other .",
    "the sparse formulation seems computationally advantageous in large problems because sparsity facilitates certain operations ( e.g. , integrating out @xmath7 given @xmath27 ) .",
    "while the above formulation is essentially standard and widely used , there is considerable variability in how different authors treat the hyperparameter @xmath36",
    ". some fix it to an essentially arbitrary value , while others put a prior on this parameter .",
    "several different priors have been suggested , and the lack of consensus among different authors may reflect the fact that most of them seem not to have been given a compelling motivation or interpretation . here",
    "we suggest a way of thinking about this prior that we believe aids interpretation , and hence appropriate prior specification .",
    "specifically , we suggest focusing on what the prior implies about the proportion of variance in @xmath1 explained by @xmath59 ( the pve ) . for example , almost all priors we have seen previously in this context assume independence of @xmath35 and @xmath36 , which implies independence of @xmath27 and @xmath36 .",
    "while this assumption may seem natural initially , it implies that more complex models are expected to have substantially higher pve . in our application",
    "this assumption does not capture our prior beliefs .",
    "for example , it seems quite plausible a priori that there could be either a large number of relevant covariates with small pve , or a small number of covariates with large pve .",
    "here we suggest specifying a prior on @xmath60 given @xmath27 by considering the induced prior on the pve , and , in particular , by making this induced prior relatively flat in the range of @xmath61 . to formalize this ,",
    "let @xmath62 denote the empirical variance of @xmath63 relative to the residual variance @xmath64 : @xmath65 ^ 2 } \\tau,\\ ] ] where this expression for the variance assumes that the covariates have been centered , and so @xmath63 has mean 0 .",
    "then the total proportion of variance in @xmath1 explained by @xmath2 if the true values of the regression coefficients are @xmath7 is given by @xmath66 our aim is to choose a prior on @xmath7 given @xmath8 so that the induced prior on @xmath67 is approximately uniform . to do this",
    ", we exploit the fact that the expected value of @xmath62 ( with expectation being taken over @xmath68 ) depends in a simple way on @xmath36 : @xmath69 = \\sigma_a^2 \\sum_{j\\dvtx \\gamma_j=1 } { s_j } , \\ ] ] where @xmath70 is the variance of covariate @xmath17 .",
    "define @xmath71 intuitively , @xmath72 gives a rough guide to the expectation of pve for a given value of @xmath27 and @xmath36 .",
    "( it is not precisely the expectation since it is the ratio of expectations , rather than the expectation of the ratio . ) to accomplish our goal of putting approximately uniform prior on @xmath73 , we specify a uniform prior on @xmath72 , independent of @xmath27 , which induces a prior on @xmath36 given @xmath27 via the relationship @xmath74 in all our mcmc computations , we parameterize our model in terms of @xmath75 , rather than @xmath76 .",
    "note that the induced prior on @xmath77 is diffuse : if @xmath78 , and @xmath79 , then @xmath80 has a probability density function @xmath81 , which is heavy tailed .",
    "our prior on @xmath36 has interesting connections with the prior suggested by @xcite . while @xcite use a @xmath53 prior ,",
    "if we consider the case where the covariates are orthogonal with variances @xmath82 , then their parameter @xmath53 is effectively equivalent to our @xmath83 .",
    "they suggest putting a  @xmath84 prior on @xmath85 , with @xmath86 or @xmath87 ; the case @xmath88 is uniform on @xmath85 , or in our notation uniform on @xmath89 .",
    "in contrast , our prior is uniform on @xmath90 . thus , our @xmath36 is effectively @xmath91 times the value of @xmath92 from @xcite , and so our @xmath36 is larger than theirs ( implying less shrinkage ) , provided that the number of relevant covariates  @xmath93 is less than @xmath3 .",
    "qualitatively , perhaps the main difference between the priors is that our prior applies less shrinkage ( larger @xmath36 ) in simpler models , which seems intuitively appealing .    of course",
    ", choice of appropriate prior distributions may vary according to context , and we do not argue that the prior used here is universally superior to other choices . however , we do believe the priors outlined above are suitable for general use in most gwas applications .",
    "in addition , we emphasize that these priors incorporate two principles that we believe should be helpful more generally : first , it seems preferable to place prior distributions on the hyperparameters @xmath35 and @xmath36 , rather than fixing them to specific values , as this provides the potential to learn about them from the data ; second , when comparing priors for @xmath36 , it is helpful to consider what the priors imply about @xmath94 .",
    "we use markov chain monte carlo to obtain samples from the posterior distribution of @xmath95 on the product space @xmath96 , which is given by @xmath97 here we are exploiting the fact that the parameters @xmath7 and @xmath8 can be integrated out analytically to compute the marginal likelihood @xmath98 . for each sampled value of @xmath99 from this posterior",
    ", we also obtain samples from the posterior distributions of @xmath7 and @xmath8 by sampling from their conditional distributions given @xmath100 .",
    "our markov chain monte carlo algorithm for sampling @xmath101 is detailed in appendix  [ appma ] . in brief , it is a metropolis ",
    "hastings algorithm [ @xcite ; @xcite ] , using a simple local proposal to jointly update @xmath102 .",
    "in particular , it explores the space of covariates included in the model , @xmath27 , by proposing to add , remove , or switch single covariates in and out of the model . to improve computational performance ,",
    "we use three strategies .",
    "first , in addition to the local proposal moves , we sometimes make a longer - range proposal by compounding randomly many local moves .",
    "this technique , named `` small - world proposal , '' improves the theoretical convergence rate of the mcmc scheme [ @xcite ] .",
    "second , and perhaps more importantly , when proposing new values for @xmath27 , and specifically when proposing to add a variable to the model , we focus more attention on those covariates with the strongest _ marginal _ associations .",
    "this idea is related to the sure independence screen [ @xcite ] ( sis ) , which uses marginal associations as an initial screening step .",
    "however , it is a `` softer '' use of these marginal associations than the sis , because every variable continues to have positive probability of being proposed . simulations ( not shown ) show that taking account of the marginal associations in this way dramatically increases the acceptance rate compared to a proposal that ignores the marginal associations . finally , when estimating quantities of interest , we make use where possible of rao  blackwellization techniques [ @xcite ] , detailed below , to reduce monte carlo variance.=1    we note that our computational scheme is relatively simple , and one can create data sets where it will perform poorly , for example , multiple correlated covariates that are far apart along a chromosome , where an efficient algorithm would require careful joint updates of the @xmath103 values for those correlated covariates .",
    "( in our current implementation , swap proposals only apply to snps that are close to one another in the genome , which is motivated by the fact that correlations decay quickly with respect to distance between snps . )",
    "however , our main focus in this paper is not on producing a computational scheme that will deal with difficult situations that might arise , but rather on prior specification , and to provide an initial assessment of the potential for bvsr to be applied to large - scale problems .",
    "indeed , we hope that our results stimulate more research on the challenging computational problems that can occur in applying bvsr to gwas and similar settings.=1      in the context of gwas , a key inferential question is which covariates have a high probability of being included in the model .",
    "that is , we wish to compute the _ posterior inclusion probability _ ( pip ) of the @xmath17th covariate , @xmath104 . although one could obtain a simple monte carlo estimate of this probability by simply counting the proportion of mcmc samples for which @xmath25 , this estimator may have high sampling variance . to improve precision",
    ", we instead use the rao  blackwellized estimate , @xmath105 where @xmath106 denote the @xmath14th mcmc sample from the posterior distribution of these parameters given @xmath1 , and @xmath107 and @xmath108 denote the vectors @xmath27 and @xmath7 excluding the @xmath17th coordinate .",
    "the probabilities that are being averaged here essentially involve simple univariate regressions of residuals against covariate @xmath17 , and so are fast to compute for all @xmath17 even when  @xmath6 is very large .",
    "details are given in appendix  [ appmb ] .      to perform inference on the total proportion of variance in @xmath1 explained by measured covariates , we use samples from the posterior distribution of @xmath109 , which is defined at equation ( [ eqn : h ] ) .",
    "these posterior samples are obtained by simply computing @xmath110 for each sampled value of @xmath111 from our mcmc scheme .      given",
    "observed covariates @xmath112 for a future individual , we can predict a value of @xmath113 for that individual by @xmath114 to estimate @xmath115 , we use the rao  blackwellized estimates @xmath116 expressions for the two terms in this sum are given in appendix  [ appmb ] .",
    "suppose that we estimate @xmath7 to be @xmath117 .",
    "one way to assess the overall quality of this estimate is to ask how well it would predict future observations , on average .",
    "motivated by this , we define the mean squared prediction error ( @xmath118 ) : @xmath119 where @xmath7 is the true value of the parameter , and @xmath120 is the variance of @xmath121 , defined at ( [ eqn : v ] ) .",
    "the @xmath118 has the disadvantage that its scale depends on the units of measurement of @xmath1 .",
    "hence , we define a _ relative prediction gain _ , @xmath122 , which contrasts the @xmath118 from an estimated @xmath7 with the prediction loss from simply predicting the mean of @xmath1 for each future observation ( @xmath123 ) and to the prediction error attained by the true value of @xmath7 ( @xmath124 ) : @xmath125 the @xmath122 does not depend on @xmath8 or on the scale of measurement of @xmath1 , and indicates what proportion of the extractable signal we are successfully obtaining from the data .",
    "for example , if the total proportion of variance in @xmath1 explained by @xmath24 is 0.2 , then an @xmath122 of 0.75 indicates that we are effectively able to extract three - quarters of this signal , leaving approximately 0.05 of the variance in @xmath1 `` unexplained . ''",
    "note that @xmath126 if the prediction performs as well as the mean , and @xmath127 if the prediction performs as well as the true value of @xmath7 .",
    "if @xmath128 , then the prediction is worse than simply using the mean , effectively indicating a problem with `` overfitting . ''",
    "at this point it seems helpful to review what we are attempting to achieve , and why it might be achievable despite the apparent severity of the computational burden . in brief , our primary goal is to extract more information from signals that exist in these data , particularly marginal associations , than do standard single - snp analyses that test each snp , one at a time , for association with phenotype .",
    "( the vast majority of gwas published so far restrict themselves to such single - snp analyses . )",
    "one of the main difficulties in single - snp analysis is to decide how confident one should be that individual snps are truly associated with the phenotype .",
    "this difficulty stems from the fact that confidence should depend on the unknown values of @xmath35 and @xmath36 . in a single - snp analysis",
    "one must make assumptions , either implicitly or explicitly , about these parameters .",
    "an important aim of our approach is to instead estimate these parameters from the data , and hence provide more data - driven estimates of confidence for each snp being associated with phenotype . to get intuition into why the data are informative about @xmath35 and @xmath36 , consider the following examples .",
    "first , suppose that in a gwas involving 300,000 snps , there are 10 snps ( in different genomic regions ) that show very strong marginal associations .",
    "then , effectively , we immediately learn that @xmath35 is likely at least of the order of @xmath129 ( and , of course , it may be considerably higher ) .",
    "further , the estimated size of the effects at these 10 snps also immediately gives some idea of plausible values for @xmath36 ( or , more precisely , for @xmath130 ) .",
    "conversely , suppose that in a different gwas none of the 300,000 snps show even modest marginal associations .",
    "this immediately suggests that either @xmath35 or @xmath130 ( or both ) must be `` small '' ( because if both were large , then there would be many strong effects , and we would have seen some of them ) .",
    "more generally , we note that the strength of the effect size at the most strongly associated snps immediately puts an upper bound on what kinds of effect size are possible , and hence an upper bound on plausible values for @xmath130 . in essence",
    ", bvsr provides a model - based approach to quantifying these qualitative ideas , taking account of relevant factors ( e.g. , sample size ) that affect the amount of information in the data .",
    "another limitation of single - snp analyses , at least as conventionally applied , is that once some snps are confidently identified to be associated with outcome , they are not controlled for , as they should be , in analysis of subsequent snps .",
    "controlling for snps that truly affect phenotype should help in identifying further such snps , and so a second key aim of our approach is to accomplish this . to see why our approach should attain this goal ,",
    "note that our rao  blackwellization procedure for estimating marginal posterior inclusion probabilities is effectively a conventional single - snp analysis that controls for the snps currently in the model .",
    "thus , for example , if we start our mcmc algorithm from a point that includes the strongest marginal associations , it effectively immediately accomplishes this second goal .",
    "we note two things we are _ not _ attempting to do .",
    "first , we are not attempting to identify a single best model ( i.e. , combination of snps ) , or to estimate posterior probabilities for specific models .",
    "in this context  and , we would argue , many other contexts where bvsr may be appropriate  these goals are of no interest , because the combination of small effect sizes and @xmath19 mean that the posterior probability on any particular model is going to be very small , and the chance of identifying the `` correct '' model is effectively zero .",
    "neither are we attempting to identify combinations of snps that interact in a nonadditive way to affect the phenotype ",
    "snps that have little marginal signal , but whose effect is only revealed when they are considered together in combination with others .",
    "while such combinations of snps may exist , and identifying them would be of considerable interest , this seems considerably more challenging , both statistically and computationally , than our more modest goals here .",
    "finally , we note a particular feature of gwas studies that may make it easier to obtain useful results from bvsr than in other contexts .",
    "specifically , correlations among snps tend to be highly `` local '' : each snp is typically correlated with only a relatively small number of other snps that are near to it ( linearly along the dna sequence ) , and any two randomly chosen snps are typically uncorrelated with one another . put another way , the matrix @xmath131 tends to have a highly banded structure , with large values clustering near the diagonal . to understand why this is helpful ,",
    "note that one of the main potential pitfalls in applying mcmc to bvsr is that the mcmc scheme may get stuck in a `` local mode '' where a particular covariate ( @xmath132 ,  say ) is included in the model , whereas in fact a different correlated covariate ( @xmath133 say ) should have been included . to help avoid getting stuck like this",
    ", the mcmc scheme could include specific steps that propose to interchange correlated covariates ( e.g. , remove @xmath132 from the model and add @xmath134 to the model ) , and the local correlation structure among snps in a gwas means that this is easily implemented by simply proposing to interchange nearby snps .",
    "furthermore , and perhaps more importantly , the local correlation structure means that getting stuck in such local modes may not matter very much , because if @xmath132 and @xmath134 are correlated , then they are also almost certainly close to one another in the genome , and hence implicate a similar set of genes , and correctly identifying a set of implicated genes is the ultimate goal of most gwas analyses .",
    "we now present a  variety of simulation results to illustrate features of our method , and assess its performance .",
    "because our priors and methodology were primarily motivated by gwas , these simulations are designed to mimic certain features of a typical gwas .",
    "these include particularly that @xmath19 ( in our simulations @xmath135 and @xmath136 ) , extreme sparsity ( in most of our simulations @xmath137 covariates affect response ) , and small effect sizes ( most relevant covariates individually explain @xmath1381% of the variance of @xmath1 ) .",
    "we performed simulations based on three different genotype data , including both simulated and real genotypes .",
    "the first is simulated @xmath139 independent snps ( henceforth 10k ) , the second is real genotypes at @xmath140 snps ( henceforth 317k ) , and the third is real genotypes at @xmath141 snps ( henceforth 550k ) .",
    "both 317k and 550k genotypes closely mimic real gwas , and comparison between them can illustrate the scalability of our method .",
    "the 10k data set is helpful for several reasons : smaller simulations run faster ; they allow us to assess methods in a simpler setting where computational problems are less of an issue ; and the independence of the covariates avoids problems with deciding what is meant by a  `` true association '' when covariates are correlated with one another.=1    for the 10k data , we simulated genotypes as follows . at each snp",
    "@xmath142 the minor allele frequency @xmath143 is drawn from a uniform distribution on @xmath144 $ ] , and then genotypes @xmath18 ( @xmath145 ) are drawn independently from a @xmath146 distribution .",
    "we use @xmath147 and @xmath148 .    both 317k and 550k data sets come from an association study performed by the pharmacogenomics and risk of cardiovascular disease ( parc ) consortium [ @xcite ; @xcite ] .",
    "the 317k genotypes come from the illumina 317k beadchip snp arrays for 980 individuals and the 550k genotypes come from the illumina 610k snp chip plus a custom 13,680 snp illumina chip in 988 individuals ( 550k snps remain after qc ) .",
    "we replaced missing genotypes with their posterior mean given the observed genotypes , which we computed under a hidden markov model [ @xcite ] implemented in the software package bimbam [ @xcite ] .",
    "for both the simulated and real genotypes we simulated sets of phenotypes in the following way .",
    "first , we specified a value of @xmath94 , the total proportion of variance in @xmath1 explained by the relevant snps , that we wanted to achieve in the simulated data .",
    "then we randomly selected a set of @xmath149 `` causal '' snps , @xmath150 , and simulated effect sizes @xmath20 for each of these snps independently from an effect size distribution @xmath151 ( discussed below ) .",
    "next we computed the value of @xmath8 that gives the desired value for @xmath67 in equation ( [ eqn : h ] ) .",
    "finally , we simulated phenotypes for each individual using @xmath152.=1    unless otherwise stated , for the 10k snp data sets we run bvsr for 1 million iterations , and for the 317k and 550k snp data sets we use 2 million iterations .",
    "run times for each data set varied from a few minutes to about one day on a single mac pro with 3 ghz processor .",
    "( note that the running time per iteration depends primarily on the inferred values for @xmath93 , not the total number of snps . )      in results presented below we compare our method with two other methods : simple single - snp analysis that tests each snp one at a time for association with phenotype , and the penalized regression method lasso [ @xcite ] .    for the single snp analyses we ranked snps by their single - snp bayes factors , computed using equation ( [ eqn : bf ] ) , with @xmath27 in the numerator being the vector with @xmath17th component 1 and all other components 0 , and averaging over @xmath153 , and 0.1 as in @xcite .",
    "( using standard single - snp @xmath6 values instead of bayes factors gives very similar performance in terms of ranking snps . )    the lasso procedure [ @xcite ] estimates @xmath7 by minimizing the penalized residual sum of squares : @xmath154 for sufficiently large penalties , @xmath155 , lasso produces sparse estimates @xmath117 .",
    "its main practical advantage over bvsr appears to be computational : for example , one can efficiently find the global optimal solution path for @xmath7 as @xmath155 varies . to apply the lasso procedure , we used the ` lars ` package ( v. 0.9 - 7 ) in r [ @xcite ] .",
    "the total proportion of variance in @xmath1 explained by the relevant covariates @xmath24 , or pve , is commonly used to summarize the results of a linear regression . in gwas",
    "the @xmath94 is , conceptually , closely related to the `` heritability '' of the trait , which is widely used , for better or worse , as a summary of how `` genetic '' the phenotype is .",
    "the key difference between the @xmath94 and heritability is that the @xmath94 reflects the optimal predictive accuracy that could be achieved for a linear combination of the _ measured _ genetic variants , whereas heritability reflects the accuracy that could be achieved by _ all _ genetic variants . in recent gwas , it has been generally observed , across a range of different diseases and clinical traits , that the proportion of phenotypic variance explained by `` significant '' genetic variants is much lower than previous estimates of heritability from family - based studies [ @xcite ] .",
    "there are several possible explanations for this `` missing heritability '' : for example , it may be that previous estimates of heritability are inflated for some reason .",
    "however , two explanations have received particular attention : some of the missing heritability could reflect genetic variants that were measured but simply did not reach stringent levels of `` significance '' in standard analyses , while other parts of the missing heritability could reflect genetic variants that were unmeasured ( and not strongly correlated with measured variants ) . because the measured genetic variants in current gwas studies are predominantly `` common '' genetic variants ( those with a population frequency exceeding a  few percent ) , the relative contribution of these two factors is connected to the contentious topic of the relative contributions of common vs rare variants to phenotypic variation and disease risk [ @xcite ] . comparing the @xmath94 with heritability",
    "should provide some insights into the relative contributions of these two factors .",
    "for example , at the simplest level , if the @xmath94 is almost as big as the heritability , then this suggests that most phenotypic variation is due to variation at snps that are highly correlated with measured genetic variants , and perhaps that rare genetic variants , which are usually not strongly correlated with measured common variants , contribute little to phenotypic variation .",
    "an important feature of bvsr that allows it to estimate the @xmath94 , together with measures of confidence , is its use of _ bayesian model averaging _",
    "( bma ) to average over uncertainty in which covariates are relevant .",
    "this is very different from single snp analyses and standard penalized regression approaches , which typically result in identification of a single set of potentially - relevant covariates , and so do not naturally provide estimates of the @xmath94 that take account of the fact that this set may be missing some relevant covariates and include some irrelevant covariates .",
    "since , as far as we are aware , the ability of bvsr to estimate @xmath94 has not been examined previously , we performed simulation studies to assess its potential .    for both real and simulated genotype data ( described above ) , we simulated 50 independent sets of phenotype data , each containing 30 randomly - chosen `` causal '' snps affecting phenotype , varying @xmath94 from 0.01 to 0.5 in steps of 0.01 .",
    "our bayesian model assumes , through the prior on @xmath7 , that the effect size distribution @xmath156 is normal . to check for robustness to deviations from this assumption",
    ", we simulated phenotype data using both @xmath157 ( as effectively assumed by our model ) and @xmath158 , where @xmath159 denotes the double exponential distribution .",
    "the results from these two different distributions were qualitatively similar , and so we show only the results for @xmath158 .    [",
    "cols=\"^,^ \" , ]     however , despite our generally upbeat assessment , there are a number of potential limitations of the methods we have described here , which present both pitfalls to be aware of in practice , as well as challenges and opportunities for future work .    one important aspect of analysis of any gwas is the potential for data quality to adversely impact results .",
    "for example , although modern genotyping technologies provided very high quality genotypes on average , some snps are much harder to genotype accurately than others , and genotyping error can occur at some snps at an appreciable rate . this can cause false positive associations if genotyping error is correlated with phenotype ( which it can be , particularly in case - control studies if the dna quality differs appreciably between cases and controls [ @xcite ] ) . while quality control is vital to any study , it is of potentially even greater import in multi - snp analyses than in single - snp analyses , because in multi - snp analyses the association results at one snp affect the results at other snps , and so low quality data at a few snps may impact estimated associations at other snps .",
    "thus , it seems particularly important to attempt to impose stringent data quality filters before embarking on a computationally - intensive multi - snp analysis .",
    "one limitation of the methods we present here is the assumption that effect sizes are normally distributed .",
    "although our simulations with exponential effect sizes suggest a certain amount of robustness to this assumption , it is important to note that there are some phenotypes where the normality assumption is clearly wrong .",
    "for example , in type 1 diabetes , one region of the genome , the mhc , contains genetic variants whose effect on phenotype may be substantially greater than any other region .",
    "when such regions of unusually large effect are known , it would be prudent to run methods like ours both including and excluding data at these loci , to check for robustness of conclusions .",
    "more generally , the robustness of our bvsr could be improved by replacing the assumption of normally - distributed effects with a heavy - tailed distribution such as a @xmath160 with small or moderate degree of freedom , or indeed with a prior on the degrees of freedom .",
    "another related issue is that we assume the residual distribution of the phenotypes to be normal . to improve robustness to this assumption",
    ", we typically normal quantile transform the observed phenotypes to have a normal distribution before analysis ( which while not strictly ensuring that the residuals are normal , does in our experience limit problems that might otherwise be caused by deviations from normality , such as occasional outlying values ) .",
    "again , the use of a @xmath160 distribution for the residuals might be preferable.=1    we view the work presented here as just the very start of what could be done with bvsr in gwas .",
    "one important extension would be to incorporate additional information into the prior distribution on which variables are included in the regression ( @xmath27 in our notation ) .",
    "here we have assumed that variables are included in the model , independently , with common probability @xmath35 .",
    "this independence assumption ignores likely local spatial dependence of @xmath27 .",
    "in particular , it would be unsurprising to see multiple functional variants occurring in a single gene , and , indeed , analyses of genetic data in the crp gene have suggested that it contains multiple snps affecting crp levels [ @xcite ; @xcite ] .",
    "the independence assumption in the prior we use here makes it overly skeptical about this possibility .",
    "another important possibility is that one could allow the prior probability of each snp being included in the regression to depend on annotations of the snp , such as where it lies relative to a gene , or whether it lies in a genomic region that is conserved across several species ( a sign that the region may be functional ) .",
    "of course it is not generally known a priori how much such annotations should affect the prior inclusion probabilities .",
    "however , with bvsr one could _ estimate _",
    "hyperparameters that affect the prior inclusion probabilities from the data [ @xcite ] .",
    "finally , despite our focus on gwas , many of the issues we have discussed here have broad relevance .",
    "in particular , while the computational challenges of bvsr remain considerably greater than penalized regression methods , we believe that the qualitative advantages of bvsr make it worth investing effort into designing more efficient inference algorithms for bvsr , to be able to better deal with the very large - scale applications that are becoming increasingly common .",
    "we use markov chain monte carlo to obtain samples from the posterior distribution of @xmath95 on the product space of @xmath96 , which is given by @xmath97 here we are exploiting the fact that the parameters @xmath7 and @xmath8 can be integrated out analytically to compute the marginal likelihood @xmath98 . indeed , in the limit for the hyperparameters @xmath161 and @xmath162 that we use here , we have @xmath163 where @xmath164 and @xmath165 denotes the @xmath6-vector of all 0s . [ for derivation , see @xcite , protocol s1 equation ( 13 ) . ]",
    "note that here @xmath166 is given by equation ( [ eqn : sigma ] ) .    for each sampled values of @xmath99 from this posterior ,",
    "we obtain samples from the posterior distributions of @xmath7 and @xmath8 by sampling from their conditional distributions given @xmath100 : @xmath167    our markov chain monte carlo algorithm for sampling @xmath101 is based on a metropolis ",
    "hastings algorithm [ @xcite ; @xcite ] , using a simple local proposal to jointly update @xmath102 . in outline ,",
    "the local proposal proceeds as follows .",
    "first a new proposed value of @xmath27 , @xmath168 , is obtained by small modification of the current value ( see below for more details ) ; then a  new value of @xmath35 is proposed from a @xmath169 distribution ; finally a proposed new value for @xmath72 is obtained by adding a @xmath170 random variable to the current value ( reflecting proposed values that lie outside @xmath171 about the boundary ) .",
    "the proposal distribution for @xmath35 is proportional to its full conditional distribution given @xmath172 inside the finite range of the prior on @xmath35 [ given by ( [ prior : s ] ) ] ; on the infrequent occasions that the proposed value for @xmath35 lies outside this range , it is of course rejected .",
    "in addition to the local proposal described above , we sometimes ( with probability 0.3 each iteration ) make a longer - range proposal by compounding randomly - many local moves [ the number being uniform on @xmath173 .",
    "this technique , named `` small - world proposal , '' improves the theoretical convergence rate of the mcmc scheme [ @xcite ] .",
    "we now give details on our update proposal for @xmath27 . when adding a covariate into the model we use a _ rank based proposal _ that focuses more attention on covariates that are more likely to be included in the model .",
    "to do this , we first rank the covariates based on their association with phenotype @xmath1 ( specifically we rank them by the bayes factor for the model including only that covariate vs the null model containing no covariates , evaluated at @xmath174 ) .",
    "let @xmath175 be a distribution on @xmath176 which has decreasing probability .",
    "here we choose @xmath175 to be a mixture @xmath177 , where @xmath178 is a uniform distribution on @xmath179 and @xmath180 is a geometric distribution truncated to @xmath179 , with its parameter chosen to give a mean of 2,000 .",
    "now let @xmath181 denote the set of covariates that are currently in the model , @xmath182 .",
    "let @xmath183 denote the complimentary set .",
    "we define three different types of moves , namely , add a covariate , remove a covariate , and exchange a pair of covariates in and out of the current model .",
    "each move starts by setting @xmath184 .",
    "then we randomly choose among the following :    * add covariate : generate @xmath185 , and find the covariate @xmath186 that has rank @xmath187 ( among covariates in @xmath183 ) .",
    "set @xmath188 .",
    "* remove covariate uniformly : uniformly pick @xmath189 , and set @xmath190 . *",
    "add a covariate and remove another : pick @xmath14 uniformly from @xmath181 and @xmath17 uniformly from @xmath183 , and set @xmath191 .    in our current implementation",
    ", at each update we randomly select among these moves with probabilities @xmath192 , @xmath193 and @xmath194 .",
    "in this appendix we derive the calculations need to compute the terms in equation ( [ post : inclusion ] ) .",
    "let @xmath195 denote the parameters @xmath196 .",
    "note that @xmath197 where @xmath198 the second term here arises because in our parameterization @xmath199 is not independent of @xmath200 ( because its prior variance , @xmath36 , is a function of @xmath201 ) .",
    "this term is easily computed from the fact that @xmath202 are i.i.d .",
    "@xmath203 .    to compute the numerator of the first term note that @xmath204 with the priors on @xmath205 [ from ( [ prior : mutau ] ) ] being @xmath206 \\\\[-8pt ] \\beta_j | \\tau&\\sim & n(0 , \\sigma_a^2/\\tau ) . % \\nonumber\\end{aligned}\\ ] ] integrating out @xmath205 gives @xmath207 where @xmath208 , @xmath209 denotes the vector obtained by taking @xmath27 and setting the @xmath17th coordinate to 0 , @xmath210 , @xmath211 , and @xmath212 is an @xmath213 design matrix whose first column is all 1s .",
    "[ see equation ( 8) from protocol s1 in @xcite . ] the posterior distribution on @xmath20 is given by @xmath214    similarly , to compute the denominator of the first term , we use @xmath215 with priors on @xmath216 . integrate out @xmath5 to get @xmath217 where @xmath218 and @xmath219 .    from this",
    "we obtain @xmath220 in the limit @xmath221 we have @xmath222 and @xmath223 and the above expression becomes @xmath224    note that this calculation effectively involves a univariate regression of the residuals @xmath225 against covariate @xmath17 .",
    "furthermore , all covariates @xmath226 use the same residuals : only for @xmath227 do the residuals need to be recomputed .",
    "* bma : bayesian model averaging * bvsr : bayesian variable selection regression * gwas : genome wide association studies * lasso : least absolute shrinkage and selection operator , a popular variable selection method * mcmc : markov chain monte carlo * pip : posterior inclusion probability * pve : proportion of variance explained * rpg : relative prediction gain * snp : single nucleotide polymorphism * sis : sure independence screen , a two - stage variable selection procedure .",
    "we thank two anonymous referees , and the editor and associate editor for helpful comments on the initial submission .",
    "we thank p. carbonetto for useful discussions ."
  ],
  "abstract_text": [
    "<S> we consider applying bayesian variable selection regression , or bvsr , to genome - wide association studies and similar large - scale regression problems . </S>",
    "<S> currently , typical genome - wide association studies measure hundreds of thousands , or millions , of genetic variants ( snps ) , in thousands or tens of thousands of individuals , and attempt to identify regions harboring snps that affect some phenotype or outcome of interest . </S>",
    "<S> this goal can naturally be cast as a variable selection regression problem , with the snps as the covariates in the regression . </S>",
    "<S> characteristic features of genome - wide association studies include the following : ( i ) a focus primarily on identifying relevant variables , rather than on prediction ; and ( ii ) many relevant covariates may have tiny effects , making it effectively impossible to confidently identify the complete `` correct '' subset of variables . </S>",
    "<S> taken together , these factors put a premium on having interpretable measures of confidence for individual covariates being included in the model , which we argue is a strength of bvsr compared with alternatives such as penalized regression methods . here </S>",
    "<S> we focus primarily on analysis of quantitative phenotypes , and on appropriate prior specification for bvsr in this setting , emphasizing the idea of considering what the priors imply about the total proportion of variance in outcome explained by relevant covariates . </S>",
    "<S> we also emphasize the potential for bvsr to estimate this proportion of variance explained , and hence shed light on the issue of `` missing heritability '' in genome - wide association studies . </S>",
    "<S> more generally , we demonstrate that , despite the apparent computational challenges , bvsr can provide useful inferences in these large - scale problems , and in our simulations produces better power and predictive performance compared with standard single - snp analyses and the penalized regression method lasso . </S>",
    "<S> methods described here are implemented in a software package , pi - mass , available from the guan lab website http://bcm.edu/cnrc/mcmcmc/pimass .    . </S>"
  ]
}