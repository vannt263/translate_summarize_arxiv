{
  "article_text": [
    "we observe independant pairs of variables @xmath0 , for @xmath1 , under a random design regression model : @xmath2 where @xmath3 is an unknown regression function that we aim at estimating , and @xmath4 are independant normal errors with @xmath5 , @xmath6 .",
    "the design points @xmath7 are assumed to be supported in the interval @xmath8 $ ] and have a density @xmath9 which will be supposed to be known .",
    "furthermore we assume that the design density @xmath9 is bounded from below , i.e. @xmath10 , where @xmath11 is a constant .",
    "many approaches have been proposed to tackle the problem of regression in random design , we mention among others the work of hall and turlach @xcite , kovac and silverman @xcite , antoniadis et _",
    "@xcite , cai and brown @xcite and the model selection point of view adopted by baraud @xcite . + the present paper provides a bayesian approach to this problem based on _ warped _ wavelet basis .",
    "warped wavelets basis @xmath12 in regression with random design were recently introduced by kerkyacharian and picard in @xcite .",
    "the authors proposed an approach which would depart as little as possible from standard wavelet thresholding procedures which enjoy optimality and adaptivity properties .",
    "these procedures have been largely investigated in the case of equispaced samples ( see a series of pioneered articles by donoho _ et al . _",
    "@xcite , @xcite , @xcite ) .",
    "kerkyacharian and picard actually pointed out that expanding the unknown regression function @xmath3 in the warped basis instead of the standard wavelets basis could be very interesting .",
    "of course , this basis has no longer the orthonormality property nonetheless it behaves under some conditions as standard wavelets .",
    "kerkyacharian and picard investigated the properties of this new basis and showed that not only is it well adapted to the statistical problem at hand by avoiding unnecessary calculations but it also offers very good theoretical features while being easily implemented .",
    "more recently brutti @xcite highlighted their easy - to - implement computational properties .",
    "+ the novelty of our contribution lies in the combination of bayesian techniques and _ warped _ wavelets to treat regression in random design .",
    "we actually want to investigate whether this yields optimal theoretical results and promising pratical performances , which will prove to be the case .",
    "we do not deal with the case of an unknown design density @xmath9 which requires further machinery and will be the object of another paper . + bayesian techniques for shrinking wavelet coefficients have become very popular in the last few years .",
    "the majority of them were devoted to fixed design regression scheme .",
    "let us cite among others , papers of abramovich _ et al . _",
    "@xcite , @xcite , clyde et al .",
    "@xcite , @xcite , @xcite , @xcite , chipman _",
    "@xcite , rivoirard @xcite , pensky @xcite in the case of i.i.d errors not necessarily gaussian .",
    "+ most of those works are taking as distribution prior a mixture of gaussian distributions .",
    "in particular , abramovich _",
    "_ in @xcite and @xcite have explored optimality properties of gaussian prior mixed with a point mass at zero and which may be viewed as an extreme case of a gaussian mixture : @xmath13 where @xmath14 are the wavelet coefficients of the unknown regression function , @xmath15 and @xmath16 are the hyperparameters .",
    "this particular form was devised to capture the sparsity of the expansion of the signal in the wavelets basis .",
    "+ our approach will consist in a first time in using the same prior but in the context of warped wavelets . in theorem 1",
    "we show that the bayesian estimator built using warped wavelets with this prior and this form of hyperparameters achieves the optimal minimax rate within logarithmic term on the considered besov functional space .",
    "unfortunately , the bayesian estimator turns out not to be adaptive .",
    "indeed , the hyperparameters depend on the besov smoothness class index . in order to compensate this drawback , autin _ et al .",
    "_ in @xcite suggested to consider bayesian procedures based on gaussian prior with large variance . following this suggestion",
    ", we will consider priors still specified in terms of a normal density mixed with a point mass at zero but with large variance gaussian densities . in theorem 2",
    "we prove again that the bayesian estimator built with this latter form of prior , still combined with warped wavelets achieves nearly optimal minimax rate of convergence while being adaptive .",
    "eventually , our simulations results highlight the very good performances and behaviour of these bayesian procedures whatever the regularity of the test functions , the noise level and the design density which can be far from the uniform case may be . +",
    "this paper is organized as follows .",
    "in section 2 some necessary methodology is given : we start with a short review of wavelets and warped wavelets , explain the prior model and discuss the two hyperparameters form we consider .",
    "we give in section 3 some definitions of functional spaces we consider . in section 4",
    ", we investigate the performances of our bayesian estimators in terms of minimax rates in two cases : the first one when the gaussian prior has small variance , the second case focuses on gaussian prior with large variance .",
    "section 5 is devoted to simulation results and discussion .",
    "finally , all proofs of main results are given in the appendix .",
    "wavelet series are generated by dilations and translations of a function @xmath17 called the mother wavelet .",
    "let @xmath18 denote the orthogonal father wavelet function .",
    "the function @xmath18 and @xmath17 are compactly supported .",
    "assume @xmath17 has @xmath19 vanishing moments .",
    "let : @xmath20 @xmath21 for a given square - integrable function @xmath3 in @xmath22 $ ] , let us denote @xmath23 in this paper , we use decompositions of 1- periodic functions on wavelet basis of @xmath22 $ ] .",
    "we consider periodic orthonormal wavelet bases on @xmath8 $ ] which allow to have the following series representation of a function @xmath3 : @xmath24 where we have denoted @xmath25 the scaling function .",
    "+ we are now going to give the essential background of _ warped _",
    "wavelets which were introduced in details in @xcite .",
    "first of all let us define @xmath26 g is assumed to be a known function , continuous and strictly monotone from @xmath8 $ ] to @xmath8 $ ] . + let us expand the regression function @xmath3 in the following sense : @xmath27 or equivalently @xmath28 where @xmath29",
    "+ hence one immediately notices that expanding @xmath30 in the standard basis is equivalent to expand @xmath3 in the new _ warped _",
    "wavelets basis @xmath31 .",
    "this may give a natural explanation that in the follow - on , regularity conditions will be expressed not for @xmath3 but for @xmath30 .",
    "+ we set @xmath32 .",
    "@xmath33 is an unbiased estimate of @xmath14 since @xmath34      we set in the following @xmath35 + as in abramovich _",
    "( see @xcite , @xcite ) , we use the following prior on the wavelet coefficients @xmath14 of the unknown function @xmath3 with respect to the _ warped _ basis @xmath31 : @xmath36 considering the @xmath37 loss , from this form of prior we derive the following bayesian rule which is the posterior median : @xmath38 where @xmath39 where @xmath40 is the normal cumulative distributive function and @xmath41 we set : @xmath42 we introduce now the estimator of the unknown regression @xmath3 @xmath43 where @xmath44 is a parameter which will be precised later .",
    "+ note that in our case , the estimator resembles the usual ones in @xcite , @xcite and @xcite , except that the deterministic noise variance has been replaced by a stochastic noise level @xmath45 .",
    "its expression is given by ( [ bruit_gamma ] ) .",
    "this change will have a marked impact both on the proofs of theorems by using now large deviations inequalities and on simulations results .",
    "+ futhermore , such @xmath37 rule is of thresholding type . indeed , as underlined in @xcite and @xcite , @xmath46 is null whenever @xmath33 falls below a certain threshold @xmath47 .",
    "some properties of the threshold @xmath47 that will be used in the sequel are given in lemma 1 in appendix .",
    "in this paper , two cases of hyperparameters will be considered .",
    "the first one involves gaussian priors with small variances .",
    "we will state as suggested in abramovich _",
    "et al _ ( see @xcite , @xcite ) : @xmath48 where @xmath49 and @xmath50 are non - negative constants , @xmath51 .",
    "+ this choice of hyperparameters is exhaustively discussed in abramovich _",
    "the authors stressed that this form of hyperparameters was actually designed in order to capture the sparsity of wavelet expansion .",
    "they pointed out the connection between besov spaces parameters and this particular form of hyperparameters .",
    "they investigate various practical choices .",
    "+ for this case of hyperparameters ( [ hyperparametresmall ] ) , the estimator of @xmath3 will be denoted @xmath52 .",
    "the second form of hyperparameters considered in the paper involves gaussian priors with large variance as suggested in autin _",
    "et al_. @xcite . + as a matter of fact , we suppose that the hyperparameters do not depend on @xmath53 and we set : @xmath54 besides , @xmath55 . we suppose that there exist @xmath56 and @xmath57 such that for @xmath58 large enough @xmath59 this form of hyperparameters was emphasized in @xcite in order to mimic heavy tailed priors such as laplace or cauchy distributions .",
    "indeed , johnstone and silverman in @xcite , @xcite showed that their empirical bayes approach for regular regression setting with a prior mixing a heavy - tailed density and a point mass at zero proved fruitful both in theory and practice .",
    "pensky in @xcite also underlined the efficiency of this kind of hyperparameters .",
    "+ we underscore that contrary to the first form of hyperparameters ( [ hyperparametresmall ] ) , this latter forms ( [ hyperparametrebigtau ] ) and ( [ hyperparametrebigw ] ) lead to an adaptive bayesian estimator .",
    "+ for this case of hyperparameters ( [ hyperparametrebigtau ] ) and ( [ hyperparametrebigw ] ) , the estimator of @xmath3 will be denoted @xmath60 .",
    "in this paper , functional classes of interest are besov bodies and weak besov bodies .",
    "let us define them . using the decomposition ( [ decomposition_wave ] )",
    ", we characterize besov spaces by using the following norm @xmath61}^{1/q } & \\mbox{if } q<\\infty \\\\",
    "\\sup_{j\\geq -1 } 2^{j(s+1/2 - 1/p)}\\|(\\beta_{j , k})_{k}\\|_{\\ell_{p } } & \\mbox{if } q=\\infty .",
    "\\end{array } \\right.\\ ] ] if @xmath62 and @xmath63 @xmath64 the besov spaces have the following simple relationship @xmath65 and @xmath66 the index @xmath67 indicates the smoothness of the function .",
    "the besov spaces capture a variety of smoothness features in a function including spatially inhomogeneous behavior when @xmath68 .",
    "+ we recall and stress that in this paper as mentioned above , the regularity conditions will be expressed for the function @xmath30 due to the _ warped _ basis context .",
    "+ more precisely we shall focus on the space @xmath69 .",
    "we have in particular @xmath70 + we define the besov ball of some radius r as @xmath71",
    ". + let us define now the weak besov space @xmath72    let @xmath73 .",
    "we say that a function @xmath3 belongs to the weak besov body @xmath72 if and only if : @xmath74^{1/2 } < \\infty.\\ ] ]    and we have the following proposition    let @xmath73 and @xmath75",
    ". then @xmath76    for the proof of this proposition see for instance @xcite .",
    "+ to conclude this section , we have the following embedding @xmath77 which is not difficult to prove ( see for instance @xcite ) .",
    "[ theo_1 ] assume that we observe model ( [ modele_de_base ] ) .",
    "we consider the hyperparameters defined by ( [ hyperparametresmall ] ) . set @xmath78 such that @xmath79 .",
    "+ let @xmath80 and @xmath81 , then we have the following upper bound : @xmath82    the optimal choice of the hyperparameter @xmath49 in theorem [ theo_1 ] should minimize the upper bound derived in ( [ borne_sup_1 ] ) .",
    "consequently , let us choose now in ( [ borne_sup_1 ] ) @xmath83 , we immediately deduce the following corollary .",
    "if one chooses for the prior parameter @xmath83 , one gets @xmath84",
    "this corollary shows that with this specific choice of hyperparameter @xmath49 , one recovers the minimax rate of convergence up to a logarithmic factor that one achieves in a uniform design .",
    "[ theorem_2 ] we consider the model ( [ modele_de_base ] ) .",
    "we assume that the hyperparameters are defined by ( [ hyperparametrebigtau ] ) and ( [ hyperparametrebigw ] ) .",
    "set @xmath85 such that @xmath86 , then we have : @xmath87    it is worthwhile to make some comments about the results of theorem 2 . here",
    ", the estimator turns out to be adaptive and contrary to the similar results in proposition 2 in @xcite we no longer have the limitation on the regularity index @xmath88 .",
    "moreover , kerkyacharian and picard @xcite had to stop the highest level @xmath44 such that @xmath89 , here we stop at the usual level @xmath90 such that @xmath91 one gets in standard thresholding .",
    "a simulation study is conducted in order to compare the numerical performances of the two bayesian estimators based on warped wavelets and on gaussian prior with small or large variance , described respectively in section 2.2.1 and 2.2.2 and the hard thresholding procedure using the universal threshold @xmath92 based on warped basis introduced by kerkyacharian and picard @xcite for the nonparametric regression model in a random design setting . for more details on kerkyacharian and picard procedure ,",
    "the readers are referred to willer @xcite , see also @xcite .",
    "in fact , we have decided to concentrate on the procedure of kerkyacharian and picard because it is interesting to point out differences and compare performances obtained by bayesian procedures which apply local thresholds and a universal threshold procedure .",
    "+ the main difficulties lie in implementing the bayesian procedures with the stochastic variance ( [ bruit_gamma ] ) .",
    "note also the responses proposed by amato et al .",
    "@xcite and kovac and silverman @xcite .",
    "+ all the simulations done in the present paper have been conducted with matlab and the wavelet toolbox of matlab .",
    "+ we consider here four test functions of donoho and johnstone @xcite representing different level of spatial variability .",
    "the test functions are plotted in fig .",
    "1 . for each of the four objects under study",
    ", we compare the three estimators at two noise levels , one with signal - to - noise ratio @xmath93 and another with @xmath94 . as in willer @xcite we also consider different cases of design density which are plotted in fig .",
    "2 . the first two densities are uniform or slightly varying whereas the last two ones aim at depicting the case where a hole occurs in the density design .",
    "the sample size is equal to @xmath95 and the wavelet we used is the symmlet8 .",
    "+ in order to compare the behaviors of the estimators , the rmse criterion was retained .",
    "more precisely , if @xmath96 is the estimated function value at @xmath97 and @xmath58 is the sample size , then @xmath98 + the rmse displayed in tab",
    ". 1 are computed as the average over @xmath99 runs of expression ( [ rmse ] ) . in each run , we hold all factors constant , except the design points ( random design ) and the noise process that were regenerated",
    ". + e1 corresponds to the bayesian estimator based on gaussian prior with large variance , e2 to the bayesian estimator based on gaussian prior with small variance and e3 to the estimator built following the kerkyacharian and picard procedure in @xcite .",
    "+ in order to implement e1 , we made the following choices of hyperparameters described in section 2.2.2 : in ( [ hyperparametrebigw ] ) , @xmath100 proved to be a good compromise whatever the function of interest to be estimated while leading to good graphics reconstructions .",
    "we set @xmath101 and @xmath102 .",
    "to implement e2 , we set @xmath103 , @xmath104 , @xmath105 and @xmath106 , following the choices recommended in @xcite . + the following plots compare the visual quality of the reconstructions ( see fig .",
    "3 . to fig .",
    "the solid line is the estimator and the dotted line is the true function .",
    "+    .values of rmse over 100 runs [ cols=\"^,^,^,^,^,^,^,^ \" , ]     fig . 1 test functions       fig .",
    "2 design density       fig . 3 blocks target and sine density , rsnr=4       fig .",
    "4 blocks target and hole2 design density , rsnr=4    fig .",
    "5 blocks target and hole2 design density , rsnr=7    fig .",
    "6 bumps target and sine design density , rsnr=4    fig . 7 heavisine target and sine design density , rsnr=7       fig . 8",
    "doppler target and hole2 design density , snr=4       we shall now comment and discuss the results displayed in tab.1 as well as the various visual reconstructions .",
    "+ the performances are always better for the bayesian estimators except for the case of the heavisine test function .",
    "more precisely , the rmse for blocks whatever the noise level and design densities are smaller for estimator 1 , moreover the rmse are almost equal for estimator 1 and 2 in the case of bumps test function , whatever the design densities and for a noise level rsnr=4",
    ". this may be due to the irregularity of the bumps , blocks and doppler test functions which are much rougher than the heavisine which is more regular . indeed",
    ", estimator 1 and 2 tend to detect better the corner of blocks , the high peaks in bumps , and the high frequency parts of doppler as the graphics show it .",
    "we may explain this by the fact that estimators 1 and 2 have level - dependent thresholds whereas estimator 3 has a hard universal threshold .",
    "+ as for the reconstructions , one can see that they are slighly better in the case of sine density and small noise , whereas there are small deteriorations when a hole occurs in the design density but this change does not affect the visual quality in too big proportions .",
    "this fact highlights the interest of `` warping '' the wavelet basis .",
    "warping the basis allows the estimators to behave still correctly when the design densities are far from the uniform density such as in the case of hole2 .",
    "* acknowledgements *    the author wishes to thank her advisor dominique picard and vincent rivoirard for interesting discussions and suggestions .",
    "+    10    f.  abramovich , u.  amato , and c.  angelini .",
    "on optimality of bayesian wavelet estimators . ,",
    "31(2):217234 , 2004 .",
    "f.  abramovich , t.  sapatinas , and b.  w. silverman .",
    "wavelet thresholding via a bayesian approach .",
    ", 60(4):725749 , 1998 .",
    "umberto amato , anestis antoniadis , and marianna pensky .",
    "wavelet kernel penalized estimation for non - equispaced design regression .",
    ", 16(1):3755 , 2006 .",
    "a.  antoniadis , g.  grgoire , and p.  vial .",
    "random design wavelet curve smoothing . 35:225232 , 1997 .",
    "f.  autin , d.  picard , and v.  rivoirard .",
    "large variance gaussian priors in bayesian nonparametric estimation : a maxiset approach . , 15(4):349373 , 2006 .",
    "y.  baraud .",
    "model selection for regression on a random design .",
    ", 6:127146 , 2002 .",
    "p.  brutti .",
    "warped wavelets and vertical thresholding .",
    "arxiv:0801.3319v1 .",
    "cai and l.d .",
    "wavelet shrinkage for nonequispaced samples .",
    ", 26:17831799 , 1998 .",
    "h.  a. chipman , e.  d. kolaczyk , and r.  e. mcculloch .",
    "adaptive bayesian wavelet shrinkage .",
    ", 92:14131421 , 1997 .",
    "m.  clyde and e.  i. george .",
    "flexible empirical bayes estimation for wavelets . ,",
    "62(4):681698 , 2000 .",
    "m.  clyde , g.  parmigiani , and b.  vidakovic . multiple shrinkage and subset selection in wavelets .",
    ", 85(2):391401 , 1998 .",
    "m.  a. clyde and e.  i. george .",
    "empirical bayes estimation in wavelet nonparametric regression . in",
    "_ bayesian inference in wavelet - based models _ ,",
    "volume 141 of _ lecture notes in statist .",
    "_ , pages 309322 .",
    "springer , new york , 1999 .",
    "d.  l. donoho and i.  m. johnstone .",
    "ideal spatial adaptation by wavelet shrinkage .",
    ", 81(3):425455 , 1994 .",
    "d.  l. donoho and i.  m. johnstone .",
    "adapting to unknown smoothness via wavelet shrinkage .",
    ", 90(432):12001224 , 1995 .",
    "d.  l. donoho , i.  m. johnstone , g.  kerkyacharian , and d.  picard .",
    "wavelet shrinkage : asymptopia ?",
    ", 57(2):301369 , 1995 . with discussion and a reply by the authors",
    ".    i.  gannaz . .",
    "thesis , universit joseph fourier , 2007 .",
    "p.  hall and b.  a. turlach .",
    "interpolation methods for nonlinear wavelet regression with irregularly spaced design . , 25:19121925 , 1997 .",
    "i.  m. johnstone and b.  w. silverman .",
    "needles and straw in haystacks : empirical bayes estimates of possibly sparse sequences . , 32(4):15941649 , 2004 .",
    "i.  m. johnstone and b.  w. silverman .",
    "empirical bayes selection of wavelet thresholds .",
    ", 33(4):17001752 , 2005 .",
    "g.  kerkyacharian and d.  picard .",
    "regression in random design and warped wavelets .",
    ", 10(6):10531105 , 2004 .",
    "g.  kerkyacharian and d.  picard . thresholding algorithms , maxisets and well concentrated basis .",
    ", 9(2):283344 , 2004 .",
    "a.  kovac and b.  w. silverman .",
    "extending the scope of wavelet regression methods by coefficient - dependent thresholding .",
    ", 95:172183 , 2000 .",
    "p.  massart . , volume 1896 of _ lecture notes in mathematics_. lectures from the 33rd summer school on probability theory held in saint - flour , july 623 , 2003 .",
    "m.  pensky .",
    "frequentist optimality of bayesian wavelet shrinkage rules for gaussian and non - gaussian noise . , 34(2):769807 , 2006 .",
    "v.  rivoirard .",
    "bayesian modeling of sparse sequences and maxisets for bayes rules .",
    ", 14(3):346376 , 2005 .",
    "t.  willer . .",
    "thesis , universit paris vii , 2006 .",
    "laboratoire de probabilits et modles alatoires , umr 7599 , universit paris 6 , case 188 , 4 , pl .",
    "jussieu , f-75252 paris cedex 5 , france . + e - mail : thanh.pham_ngoc@upmc.fr",
    "in the sequel @xmath107 denotes some positive constant which may change from one line to another line .",
    "we also assume without loss of generality that @xmath108 in model ( [ modele_de_base ] ) .",
    "+ we have that @xmath109 hence we get @xmath110 , the expression of @xmath45 being given by ( [ bruit_gamma ] ) . +",
    "let us define the following event : @xmath111 to make proofs clearer we recall the bernstein inequality that we will use in the sequel .",
    "( see in @xcite proposition 2.8 and formula ( 2.16 ) )    [ propbernstein ] let @xmath112 be independant and square integrable random variables such that for some nonnegative constant @xmath113 , @xmath114 almost surely for all @xmath115 .",
    "let @xmath116)\\ ] ] and @xmath117 .",
    "then for any positive @xmath118 , we have @xmath119 \\leq \\exp\\left ( \\frac{-v}{b^{2}}h(\\frac{bx}{v } ) \\right)\\ ] ] where @xmath120 . + it is easy to prove that @xmath121 which immediately yields @xmath122 \\leq \\exp\\left ( \\frac{-x^{2}}{2(v+bx/3 ) } \\right).\\ ] ]      * proof of lemma [ lemma_0 . ] * + let us deal with the first case @xmath126 . to bound @xmath127 we will use the bernstein inequality and apply proposition [ propbernstein ] . in the present situation @xmath128 .",
    "+ first of all , in order to apply the bernstein inequality , we need the value of the sum @xmath129\\ ] ] we have @xmath130 hence @xmath131 + moreover @xmath132 so @xmath133 let us now deal with the second case @xmath134 .",
    "to bound @xmath127 we will follow the lines of the proof of the first case . here again @xmath135 according to ( [ esperancepsi4 ] ) , we have @xmath136 and @xmath137 and @xmath138 consequently @xmath139 + the following lemma shows that the properties of the bayesian estimators @xmath60 and @xmath52 can be controlled on the event @xmath140 . to lighten the notations for the proof of this lemma",
    ", we will denote @xmath141 for @xmath140 and @xmath142 the complementary of @xmath141 .",
    "* proof of lemma [ lemma 3 ] . *",
    "+ we have @xmath145 & \\leq & cj_{n}{\\mathbb{e}}\\left[\\sum_{j\\leq j}\\sum_{k}(\\tilde{\\beta}_{jk}-\\beta_{jk})^{2}i(\\omega_{n}^{c})\\right ] + \\mathbb{p}(\\omega_{n}^{c})\\sum_{j > j_{n}}\\left(\\sum_{k}\\beta_{jk}^{2}\\right)^{1/2}\\\\ & \\leq & v+b.\\end{aligned}\\ ] ]    let us first deal with the variance term v. the estimator @xmath46 can be written as @xmath146 with @xmath147 .",
    "we have @xmath148\\\\ & \\leq&2c j_n\\mathbb{e}\\left[\\sum_{j\\leq j_n}\\sum_kw_{jk}^2(\\hat\\beta_{jk}-\\beta_{jk})^2i({\\omega_{n}^c})\\right]+2c j_n\\sum_{j\\leq j_n}\\sum_k\\mathbb{e}\\left[(1-w_{jk})^2\\beta_{jk}^2i({\\omega_{n}^c})\\right]\\\\ & \\leq&2c j_n\\mathbb{e}\\left[\\sum_{j\\leq j_n}\\sum_k(\\hat\\beta_{jk}-\\beta_{jk})^2i({\\omega_{n}^c})\\right]+2c j_n\\sum_{j\\leq j_n}\\sum_k\\mathbb{e}\\left[\\beta_{jk}^2i({\\omega_{n}^c})\\right]\\end{aligned}\\ ] ] because @xmath149",
    ". then , using cauchy scharwz inequality we get @xmath150^{\\frac{1}{2}}\\mathbb{p}(\\omega_{n}^c)^{\\frac{1}{2}}+2c j_n\\sum_{j\\leq j_n}\\sum_k\\beta_{jk}^2\\mathbb{p}(\\omega_{n}^c).\\end{aligned}\\ ] ] using ( [ proba_gamma_j_n ] ) and ( [ puissance_4_diff_beta_chap ] ) we have @xmath151 we recall that @xmath152 , accordingly by choosing @xmath123 large enough we have @xmath153 as for the term @xmath154 @xmath155 which completes the proof for @xmath60 . + the proof for @xmath52 is similar , all inequalities hold a fortiori since , in the case of the estimator @xmath52 we have @xmath156 ( see ( [ proba_gamma_j_alpha ] ) ) .",
    "* proof of lemma [ lemma_1 ] .",
    "* we follow the lines of the proof of lemma 1 . in @xcite .",
    "+ on the one hand we have ( see proof of lemma 1 . in @xcite page 228 ) @xmath162 where @xmath163 is some suitable positive constant . besides , we have @xmath164 , therefore @xmath165 + hence we get @xmath166 where @xmath167 denotes a positive constant depending on @xmath168 and @xmath169 and which may be different at different places . since @xmath170 we finally get @xmath171 on the other hand , for the reverse inequality , we have ( see proof of lemma 1 . in @xcite page 228 and formula",
    "( 14 ) in @xcite page 221 ) @xmath172 but @xmath173 consequently one has @xmath174 which completes the proof .",
    "* proof of theorem 1 . *",
    "+ let us place on the event @xmath140 defined by ( [ evenementomega ] ) with @xmath158 .",
    "+ by the usual decomposition of the mise into a variance and a bias term we get @xmath175 \\nonumber\\\\ & \\leq & 2(v+b ) \\nonumber\\end{aligned}\\ ] ] with @xmath176 @xmath177    we first deal with the term @xmath178 .",
    "we have @xmath179 we want to show that @xmath180 for this purpose we have @xmath181 where @xmath182 .",
    "+ now using inequality @xmath183 p. @xmath184 in @xcite we have @xmath185 where @xmath186 denotes the interval @xmath187 $ ] and @xmath188 .",
    "but the design density @xmath9 is bounded below by @xmath11 .",
    "hence we get @xmath189 and consequently @xmath190    we decompose now @xmath178 into three terms @xmath191\\ ] ] where @xmath192 with @xmath193 a positive constant and @xmath194 @xmath195 as a consequence we have @xmath196 we are now going to upperbound each term @xmath197 , @xmath198 and @xmath199 .",
    "we start with @xmath197 @xmath200.\\ ] ] + as precised in the beginning of section 2.2 p 6 , @xmath201 for @xmath202 . as well , @xmath203 for @xmath204 and @xmath205 monotonically as @xmath206 .",
    "hence @xmath207 which implies @xmath208 + we have @xmath209 and @xmath210 for @xmath211 hence we get @xmath212 + so @xmath213 finally @xmath214 let us now consider the second term @xmath198 @xmath215 + we have that @xmath216 , consequently it follows @xmath217 we have @xmath218 using inequality @xmath219 in @xcite p. 1086",
    "we have @xmath220 hence @xmath221 we now bound the term @xmath222 .",
    "@xmath223 we have @xmath224 let us focus on @xmath225 , we have @xmath226 hence @xmath227 where @xmath228 and @xmath229 kerkyacharian and picard in @xcite in order to prove inequality @xmath230 in @xcite showed p. 1088",
    "that @xmath231 if @xmath232 . as for @xmath233 , conditionally on @xmath234 we have @xmath235 where @xmath45 has been defined in ( [ bruit_gamma ] ) .",
    "using exponential inequality for gaussian random variable we have @xmath236 using ( [ proba_gamma_j_alpha ] ) with @xmath237 , we have for @xmath80 @xmath238 it remains to fix @xmath193 large enough so that we get @xmath239 so we have for @xmath222 , with @xmath80 , @xmath240 finally we get for @xmath198 @xmath241 let us consider now the term @xmath199 @xmath242 since @xmath243 , we get @xmath244 but @xmath30 belongs to the besov ball @xmath245 which entails @xmath246 hence @xmath247 we have @xmath248 we are now in position to give an upper bound for the variance term @xmath178 namely @xmath249 it remains to bound the bias term @xmath154 . in @xcite p.1083 using inequality ( 44 ) the authors have proved that for any @xmath250 we get @xmath251 applying ( [ kerk_bernoulli_2 ] ) with in our case of lower bounded design density , @xmath252 and @xmath253 , it follows @xmath254 hence @xmath255 which completes the proof of theorem 1 .",
    "[ lemma 2 . ]",
    "let @xmath256 a sequence of random weights lying in @xmath8 $ ] .",
    "we assume that there exist positive constants @xmath163 , @xmath11 and @xmath257 such that for any @xmath258 , @xmath259 is a shrinkage rule verifying for any @xmath58 , @xmath260 @xmath261 @xmath262 and let @xmath263 then @xmath264    * proof of lemma [ lemma 2 . ] .",
    "* @xmath265 we first consider the term @xmath266",
    "@xmath267 @xmath268 @xmath269 but according to ( [ inegalite_diff_carre_bernoulli ] ) we have for @xmath270 @xmath271 hence using ( [ lemme2condition2 ] ) it follows @xmath272 as for @xmath273 @xmath274 . \\nonumber   \\end{aligned}\\ ] ] by ( [ weakbesov ] ) we get @xmath275 we are going to bound @xmath276 .",
    "we have @xmath277 where @xmath278 and @xmath279 kerkyacharian and picard in @xcite in order to prove inequality @xmath230 in @xcite showed p. 1088",
    "that @xmath280 if @xmath232 . as for @xmath281 ,",
    "conditionally on @xmath234 we have @xmath235 where @xmath45 has been defined in ( [ bruit_gamma ] ) .",
    "@xmath282 using ( [ proba_gamma_j_n ] ) to bound @xmath127 we get @xmath283 thus @xmath284 which entails by fixing @xmath11 and @xmath123 large enough @xmath285 let us look at the term @xmath286",
    "@xmath287 @xmath288\\nonumber \\\\&=&t_{7}+t_{8}\\nonumber\\end{aligned}\\ ] ] for the term @xmath289 , we use the cauchy scharwz inequality @xmath290 furthermore , using inequality @xmath219 p. 1086",
    "in @xcite we get for @xmath232 @xmath291 and by ( [ proba_beta_jk_chapeau ] ) @xmath292 from which follows by fixing again @xmath11 and @xmath123 large enough @xmath293 for the term @xmath294",
    "@xmath295 @xmath296 @xmath297 using ( [ proba_beta_jk_chapeau ] ) we get for @xmath11 and @xmath123 large enough @xmath298 + as for @xmath299 @xmath300 using ( [ lemme2condition3 ] ) we get @xmath301 using ( [ weakbesov ] ) it follows @xmath302 it remains to bound the bias term @xmath303 . to this purpose",
    "we use the fact that @xmath304 @xmath305 which completes the proof .    * proof of theorem 2 . *",
    "+ in order to prove the theorem 2 .",
    ", we have to prove that the bayesian estimators ( [ bayesien_beta ] ) based on gaussian priors with large variance ( [ hyperparametrebigtau ] ) and ( [ hyperparametrebigw ] ) satisfy the conditions of lemma 2 .",
    "+ we will not get into details of the proof because this latter is identical to the proof of theorem 3 . in @xcite , with the sole exception that here , we will place ourselves on the event @xmath140 with @xmath306 , @xmath123 some positive constant . indeed , as precised above in section 2.2 , a key observation is that instead of having a deterministic noise @xmath307 like in @xcite , here we have to deal with a stochastic noise @xmath45 which expression is given by ( [ bruit_gamma ] ) ."
  ],
  "abstract_text": [
    "<S> in this paper we deal with the regression problem in a random design setting . </S>",
    "<S> we investigate asymptotic optimality under minimax point of view of various bayesian rules based on warped wavelets and show that they nearly attain optimal minimax rates of convergence over the besov smoothness class considered . </S>",
    "<S> warped wavelets have been introduced recently , they offer very good computable and easy - to - implement properties while being well adapted to the statistical problem at hand . </S>",
    "<S> we particularly put emphasis on bayesian rules leaning on small and large variance gaussian priors and discuss their simulation performances comparing them with a hard thresholding procedure .    * </S>",
    "<S> key words and phrases : * nonparametric regression , random design , warped wavelets , bayesian methods    * msc 2000 subject classification * 62g05 62g08 62g20 62c10 </S>"
  ]
}