{
  "article_text": [
    "in an epidemiological context the response variable is quite often binary .",
    "binomial regression models ( and specially the logistic regression model ) are some of the main techniques on which analytical epidemiology relies to estimate the effect of an exposure on an outcome , adjusting for confounding .",
    "other link functions can be used : for example , when the objective is to model the ratio of probabilities instead of the ratio of odds , the logistic approximation can be inappropriate , see @xcite , and a log - binomial model in which the link function is the logarithm is preferable to a logistic model .",
    "binomial regression models make it possible to estimate the effect of several risk factors and exposures on an outcome .",
    "while being able to estimate these effects is paramount , the statistical validation of the underlying model is equally of major importance . due to this issue",
    ", epidemiological studies most often associate to point estimations their associated confidence intervals and p - values for a contrast where the null hypothesis @xmath0 is the null effect of some specific factors of interest . however , a delicate issue is that the frequentist perspective makes it impossible to quantify the probability of the effect being true , that is , the probability of the alternative hypothesis @xmath1 .",
    "the purpose of the current work is to obtain the posterior probability of the alternative hypothesis in a binomial regression model with a general link function , using an automatic prior - modelling procedure that does not require the specification of tuning parameters or hyperpriors .",
    "indeed , we formulate here the hypothesis testing setting ( @xmath0 versus @xmath1 ) as a model selection problem and from a bayesian perspective , since its expression is based on the respective probabilities of both hypotheses after data are observed .",
    "each hypothesis provides a competing model to explain the sample data . to set some notations ,",
    "let us consider that , under the null hypothesis the distribution of the sample @xmath2 is @xmath3 , and under the alternative one is @xmath4 .",
    "if both models have a priori the same probability and the prior distributions on the parameters are @xmath5 , @xmath6 , then the posterior probability of the alternative hypothesis is @xmath7 where @xmath8 and @xmath9 is the bayes factor in favour of the alternative hypothesis that is defined as @xmath10    to compute the probability ( [ prob_post_m2 ] ) specification of the prior distributions @xmath11 on the parameters of the models to be compared is previously needed . in the literature are widely used diffuse , vague or flat priors and objective ones like the jeffreys prior @xcite or the reference prior ( bernardo , @xcite ; berger and bernardo , @xcite ) , to estimate the parameters of the regression models . however , the use of these priors is not recommended for bayesian model selection due to the fact , among other reasons , that their formulation does not take into account the null hypothesis , making difficult for @xmath12 to be _ concentrated _ around that hypothesis , which is a widely accepted condition ( see , e.g. , @xcite , pages 157 , 160 ) .",
    "another common problem with these priors is that they are usually not proper , a property that leads to the indetermination of the bayes factor , although it is not the case for our models , see @xcite and @xcite .",
    "the literature on objective prior distributions for testing in binomial regression models is quite limited .",
    "the intrinsic prior distributions ( berger and pericchi , @xcite ; moreno _ et al .",
    "_ , @xcite ) are objective priors which have been proved to behave well in problems involving normal linear models , see @xcite ; @xcite and @xcite . however , the implementation of this technique in binomial regression models with a general link function has not been yet developed .",
    "recently @xcite have applied the intrinsic priors to the problem of variable selection in the probit regression model .",
    "they took advantage of intrinsic priors for normal regression models @xcite due to the connection between the probit model and the normal regression model with incomplete information .",
    "therefore their results only apply to probit models .",
    "our setting is more general in that it can be directly applied to other link functions like the logit , the complementary log - log , the cauchit and the probit one .",
    "an extension of the zellner s @xmath13-prior to generalised linear models like binomial regression models has been developed by @xcite ; however , this extension needs the specification of the hyperprior distribution on the parameter @xmath13 .",
    "our proposal here is to use integrals priors .",
    "this methodology automatically provides prior distributions that do not depend on hyperparameters , thus on values ( or prior distributions ) to be subjectively assigned or estimated from the data , and it has proved to perform satisfactorily in a number of situations , see @xcite , ( @xcite ) and @xcite .",
    "next we formulate the problem .",
    "suppose that @xmath14 are independent observations , where @xmath15 is a bernoulli distributed random variable , @xmath16 , @xmath17 is a vector of covariates and @xmath18 is the matrix with rows @xmath19 .",
    "the probability @xmath20 is related with the vector @xmath21 through a link function such that @xmath22 , @xmath23 , where @xmath24 is the vector of the regression coefficients and @xmath25 , that is the intercept is @xmath26 . for a given value @xmath27",
    "we want to test the hypothesis @xmath28 _ versus _",
    "@xmath29 this contrast is equivalent to the problem of selecting between the models @xmath30 and @xmath31 , with @xmath32 there are @xmath33 unknown parameters in model @xmath30 and @xmath34 in model @xmath31 .",
    "the probability of the alternative hypothesis after the sample @xmath2 is observed is therefore the posterior probability ( [ prob_post_m2 ] ) of model @xmath31 .",
    "the solution we propose here is to compute this posterior probability based on a bayes factor associated with integral priors .",
    "to compare the models @xmath35 , @xmath6 , and to build appropriate objective priors , we rely on the integral priors proposed in @xcite , ( @xcite ) and @xcite .",
    "those priors are defined as the solutions @xmath11 of the following system of two integral equations @xmath36 and @xmath37 where @xmath38 is an objective prior distribution used for the purpose of estimation in model @xmath39 , @xmath40 and @xmath41 and @xmath42 are minimal imaginary training samples .",
    "see @xcite for details and motivations . while , usually @xmath41 and @xmath42 are training samples of a same size , this is not a requirement of the approach : the constraint is to take @xmath43 of minimal size under the constraint that @xmath44 is a proper distribution .    the argument to derive these equations",
    "is that _ a priori _ the two models are equally valid and they are provided with ideal unknown priors that yield to the true marginals , being _ a priori _",
    "neutral for judging between both models .",
    "moreover , these equations balance each model with respect to the other one since the prior @xmath45 is derived from the marginal @xmath46 and therefore from @xmath47 , @xmath48 as an unknown expected posterior prior @xcite .",
    "solving this system of integral equations is usually impossible .",
    "however , there exists a numerical approach that provides simulations from those integral priors .",
    "the above system of integral equations is indeed associated with a markov chain with transition @xmath49 that consists of the following four steps @xmath50 the invariant @xmath51-finite measure associated with this markov chain is the integral prior @xmath12 .",
    "therefore , it can be simulated indirectly by simulating this markov chain provided the latter is recurrent .    in regression models ,",
    "a training sample is associated with a set of rows of the design matrix and therefore there exist different training samples . to overcome this issue , in linear models ,",
    "@xcite have suggested that imaginary training samples can be defined as observations that arise by first randomly drawing linearly independent rows from the design matrix and then generating the corresponding observations from the regression model .",
    "( a similar perspective is adopted in bootstrap . )    in the context of the integral priors methodology with regression models , this simulation of training samples can be easily adapted by first randomly drawing linearly independent rows of the design matrix and then generating the corresponding observations from the regression model in steps 1 and 3 of the above algorithm .",
    "this procedure is exactly how we proceed for binomial regression models .",
    "different training samples provide different amounts of _ information _ that can and do impact the resulting bayes factor . in the context of intrinsic priors ,",
    "see @xcite about this issue .",
    "however , when using our procedure for integral priors , if a simulated training sample has a high _ information _ amount in , say , step 1 , it is _ compensated _ in step 3 where a new training sample is drawn conditional on a new set of rows drawn independently of the previously rows used in step 1 .",
    "in addition , a pragmatic approach to the evaluation of integral priors is to check whether or not they produce sensible and robust answers .",
    "we stress that , for this model , the associated markov chain is necessarily recurrent since the training samples have a finite state space and the full conditional densities @xmath52 , @xmath53 are strictly positive everywhere and therefore the markov chain is irreducible and hence ergodic .",
    "to simulate markov chains associated with the integral priors two steps are required : first , we need to generate imaginary training samples ( steps 1 and 3 ) and second , we need to simulate from the corresponding posteriors ( steps 2 and 4 ) . at this point",
    "we should account for the fact that training samples are subsets of the data such that the corresponding posteriors are proper . in the binomial regression problem ,",
    "if the vector @xmath54 is a subset of the data and the submatrix @xmath55 with rows @xmath56 of @xmath18 associated to @xmath57 is of full rank , then the jeffreys prior , @xmath58 , and the corresponding posterior , @xmath59 , are proper distributions , as can be seen in @xcite . furthermore , they stated that this is the case for binary regression models , such as the logistic , the probit and the complementary log - log regression models .",
    "therefore it is possible to select the imaginary training samples @xmath41 and @xmath42 that are needed in steps 1 and 3 in such a way that the dimensions of these samples are @xmath33 and @xmath34 respectively . to generate these , we first have to select the corresponding full rank submatrices @xmath55 .",
    "in addition , we need to simulate from the posterior distribution @xmath59 . in binomial regression models with link function @xmath13 ,",
    "it is usually the case that the posterior distribution of the regression coefficients does not enjoy a simple and closed form , which complicates the simulation .",
    "we could consider an accept - reject algorithm based , for instance , on laplace approximations to the posterior distribution or use instead mcmc steps .",
    "however , we propose a more efficient shortcut , namely that , when @xmath57 has dimension @xmath34 , @xmath60 , @xmath61 , @xmath62 , and the submatrix @xmath55 above is of full rank , to simulate @xmath59 is equivalent to simulate @xmath63 by the change of variables @xmath64 .",
    "usually @xmath65 , although , when @xmath66 reproduces restrictions ( _ e.g. _ when @xmath67 ) , we can always repeat simulations until the restriction is satisfied .",
    "the implementation of this idea is straightforward since , whatever the link function @xmath13 is , jeffreys prior is @xmath68 and therefore the posterior distribution , @xmath69 is easily simulated .",
    "this shortcut is an important reason for choosing imaginary training samples of appropriate and different sizes : @xmath41 of size @xmath70 and @xmath42 of size @xmath34 .",
    "when working with intrinsic priors , @xcite , @xcite , @xcite , among others , have found it more efficient to increase the size of the imaginary training samples when the data come from a binomial distribution .",
    "one way to achieve this in the case of binomial regression models , while keeping the simplicity in simulating from the posterior distribution of the regression coefficients , is to introduce more than a single bernoulli variable @xmath71 for each selected row @xmath72 . concretely ,",
    "if the vector @xmath73 is of dimension @xmath74 ( @xmath75 being a positive integer ) , @xmath76 , @xmath77 , @xmath78 , and @xmath61 , @xmath62 , then @xmath79 is @xmath80 where @xmath81 is the mean of the components of @xmath71 . as @xcite point out",
    ", the grade of concentration about the null hypothesis is controlled by the value of @xmath75 .",
    "these authors apply this augmentation scheme to independence in contingency tables , using intrinsic priors such that the size of the imaginary training samples does not exceed the size of the data . taking advantage of this perspective",
    ", we propose that the number of bernoulli variables be a discrete uniform random variable between @xmath82 and the number of times that each row is repeated in the matrix @xmath18 . if @xmath83 is the number of times that the row @xmath84 appears in the matrix @xmath18 and @xmath85 is a discrete uniform random variable in @xmath86 , @xmath62 , then we can take @xmath73 , @xmath87 , @xmath77 , @xmath88 , and @xmath61 , @xmath62 . in this case",
    "the posterior distribution @xmath89 @xmath90 is @xmath91    the value @xmath92 can be directly generated from the binomial distribution , avoiding the simulation of @xmath93 at the end of steps 1 and 3 , although no much gain in execution time is derived from this choice .    in the case of continuous covariates",
    "we need to only consider @xmath94 since an increase in the size of the imaginary training samples as described above makes no sense .",
    "when this happens , an alternative could be to discretise the continuous covariates using quantiles and to compute the value @xmath83 for all the rows @xmath84 using the discretised version instead of the continuous covariates , even though we work later with the original matrix @xmath18 .",
    "in this section , we describe in detail the algorithm used to simulate the markov chain with transition @xmath49 that is associated with our model selection problem .",
    "recall that , in order to simulate @xmath41 and @xmath42 , we need to select full - ranked submatrices of @xmath18 .",
    "the implementation is as follows : rows of @xmath18 are randomly ordered and they are consecutively chosen until we have a full rank matrix .",
    "the algorithm is divided in the following four steps :    * * step 1*. simulation of @xmath41 .",
    "+ - : :    randomly select @xmath70 rows of the matrix    @xmath18 : @xmath95 , with    the condition that if @xmath96 is the submatrix of    @xmath18 with these rows , and @xmath97 is the    submatrix of @xmath96 with columns @xmath98 ,    then @xmath99 .",
    "- : :    simulate @xmath100 ,    @xmath101 , where @xmath102 is the    number of times that the vector with the columns    @xmath98 of @xmath72 appears in the    design matrix of model @xmath30 .",
    "- : :    independently simulate @xmath103 , @xmath88 ,    @xmath101 , and take    @xmath104 where    @xmath87 . * * step 2*. simulation of @xmath105 . + - : :    simulate @xmath106 ,    @xmath101 , and compute    @xmath107 - : :    take @xmath108 .",
    "* * step 3*. simulation of @xmath42 .",
    "+ - : :    randomly select @xmath34 rows of the matrix @xmath18 :    @xmath109 , with the condition that    if @xmath110 is the submatrix of @xmath18 with these    rows , then @xmath111 .",
    "- : :    simulate @xmath112 ,    @xmath62 , where @xmath113 is the    number of times that @xmath72 appears in the design    matrix of model @xmath31 .",
    "- : :    independently simulate @xmath114 , @xmath88 ,    @xmath62 , and take    @xmath115 where    @xmath87 . * * step 4*. simulation of @xmath116 . + - : :    simulate @xmath106 ,    @xmath62 , and compute    @xmath117 - : :    take @xmath118 .      to compute the bayes factor @xmath119 that is associated to the integral priors @xmath120 , and therefore to obtain the posterior probability of model",
    "@xmath31 we can exploit the simulations from both integral priors derived from the markov chain(s ) . beginning with a value @xmath121 , each time the transition @xmath49 is simulated we obtain a value for @xmath122 and derive another one for @xmath105 .",
    "therefore with this procedure we obtain two markov chains @xmath123 and @xmath124 , whose stationary probability distributions are respectively @xmath125 and @xmath12 .",
    "the ergodic theorem thus implies @xmath126 and this result provides an inexpensive approximation to the bayes factor @xmath9 .",
    "however , the major difficulty with this approach is that when the likelihood is much more concentrated than its corresponding integral prior , @xmath127 , most of the simulations @xmath128 enjoy very small likelihood values , which means that the approximation procedure is then inefficient , i.e.  results in a high variance .",
    "this problem can be bypassed by importance sampling .",
    "however , importance sampling requires the ability to numerically evaluate the integral priors , even though we are only able to simulate from these distributions .",
    "the resolution of the difficulty is to resort to nonparametric density estimations based on the markov chains @xmath123 and @xmath124 . in the examples that we present here",
    "we have used the kernel density estimation from the package np of r , see @xcite . concretely , if @xmath129 is the kernel density estimation of @xmath5 , and @xmath130 is the importance density , then @xmath131 then , simulating from @xmath130 and evaluating @xmath132 , @xmath129 and @xmath130 , we can approximate the bayes factor .    alternatively , and still relying on kernel density estimation , the method of @xcite can be used to approximate the bayes factor .",
    "a rough comparison is provided by laplace type approximations as in @xcite .",
    "closer to the original rao - blackwellisation argument of @xcite , the training sample provides the following monte carlo approximation @xmath133 where @xmath134 are simulations from @xmath135 , which is more accurate than a nonparametric estimation of the integral priors .",
    "table [ greenland ] reproduces a dataset on the relation of receptor level and stage with the 5-year survival indicator , in a cohort of women with breast cancer @xcite .",
    ".data relating receptor level and stage to 5-year breast cancer mortality @xcite [ cols=\"^,^,^,^,^\",options=\"header \" , ]     [ tabla_ejemplo2 ]",
    "integral prior distributions have successfully been constructed towards an objective bayesian model selection analysis in binomial regression models and the methodology has been applied in two examples with the logistic regression . this analysis has been done within the model selection framework and it remains completely automatic since no other choice than the reference priors for the competing models under consideration is requested . although unrelated with the purpose of this paper , this methodology can be applied to variable selection problems , using an encompassing structure defined from above or from below as done applying the intrinsic priors methodology in @xcite .",
    "furthermore , for the sake of comparison we have applied the intrinsic prior methodology in @xcite to our examples . for the breast cancer example",
    "we have calculated 30 times the posterior probability of the full model using the package varselectip that implements the intrinsic priors for the probit model , see @xcite .",
    "the 30 computed values ranged from 0.607 to 0.809 with a mean of 0.703 and standard deviation 0.055 , thus exhibiting a similar answer but with more variability than the integral methodology , see table ( [ greenland_res1 ] ) .",
    "for the second example ( low birth - weight ) the posterior probability of the full model using the package varselectip 30 times ranged from 0.820 to 0.922 with a mean of 0.870 and standard deviation 0.024 , showing again that the integral methodology is more stable that the one implemented with intrinsic priors ( package varselectip ) ; at last , the conclusion using integral priors is more conservative , which is a rather positive argument in medical studies when one is trying to associate an exposure with an illness .    this feature could be the consequence of the property that despite the fact that both the integral and the intrinsic priors are centred around the null hypotheses , the corresponding null hypotheses are defined in different ways since , when we use the intrinsic priors methodology developed in @xcite , the intrinsic priors for all models under consideration are centred around a null model where all the @xmath136 are zero except for the intercept , that is the reference model for the intrinsic methodology .",
    "nevertheless , we should keep in mind that computations with integral priors were made for the logistic model while those for intrinsic priors were made for the probit model .",
    "this work straightforward applies to other link functions and can be extended to compare several link functions ( non - nested models ) .",
    "all the computations have been programmed in r and are freely available at the web https://webs.um.es/dsm/miwiki/doku.php?id=investigacion .",
    "this research was supported by the sneca foundation programme for the generation of excellence scientific knowledge under project 15220/pi/10 .",
    "cpr was partly supported by agence nationale de la recherche ( anr ) , on the project anr-11-bs01 - 0010 calibration ."
  ],
  "abstract_text": [
    "<S> in this work we apply the methodology of integral priors to handle bayesian model selection in binomial regression models with a general link function . </S>",
    "<S> these models are very often used to investigate associations and risks in epidemiological studies where one goal is to exhibit whether or not an exposure is a risk factor for developing a certain disease ; the purpose of the current paper is to test the effect of specific exposure factors . </S>",
    "<S> we formulate the problem as a bayesian model selection case and solve it using objective bayes factors . to construct the reference prior distributions on the regression coefficients of the binomial regression models , we rely on the methodology of integral priors that is nearly automatic as it only requires the specification of estimation reference priors and it does not depend on tuning parameters or on hyperparameters within these priors .    _ * keywords * _ : binomial regression model ; integral prior ; jeffreys prior ; markov chain ; objective bayes factor . </S>"
  ]
}