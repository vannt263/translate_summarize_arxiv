{
  "article_text": [
    "in portfolio allocation problems , investors aim to optimize the return of the invested capital based on some cost function by allocating a fraction of the capital in a number of different assets . in the long established _",
    "mean - variance _ theory ( see markowitz ( 1952 ) ) for asset allocation , the fraction of the capital invested in each asset is known as the portfolio weight , and all weights together form a linear combination ( portfolio ) that is optimal when the expected return of the portfolio is maximized for a fixed level of variance of the portfolio .",
    "the approach argues that maximization of expected returns does not guarantee that the portfolio will have the smallest variance .",
    "hence , a trade - off between the expected return and the variance of the portfolio provides a more effective diversification of investors funds .",
    "investors are considered risk averse and would prefer the portfolio with the smallest risk when expected returns are equal . moreover , a portfolio with smaller variance is a desirable attribute , as investors could leverage by increasing the capital allocation , so that the portfolio would achieve higher return on capital . although the mean - variance analysis theory , initially , generated little interest , it is now a mainstream theory whose principles are constantly visited and re - invented .",
    "we also wish to clarify that the meaning of the terms _ assets _ and _ instruments _ are used in this text interchangeably and they are deemed as available investment vehicles .",
    "however , in mean - variance optimization it is well known that the portfolio weights can be highly unstable .",
    "this is due to the difficulty of estimating expected returns ; see merton ( 1980 ) . as a result",
    ", there has been a substantial amount of recent interest in improving estimation procedures , including : baltutis ( 2009 ) ; demiguel & nogales ( 2009 ) ; demiguel et al .",
    "( 2009a , b ) ; fabozzi et al .",
    "( 2007 ) ; fabozzi et al .",
    "( 2009 ) ; jagannathan & ma ( 2003 ) ; ledoit & wolf ( 2003 , 2004 ) .",
    "this work ranges from imposing constraints on the optimization function , to robust portfolio estimation procedures .",
    "whilst these publications are vital in the understanding of portfolio allocation , they are mainly concerned with _ batch procedures _",
    "that requires historical observations as opposed to _",
    "online _ techniques which are equipped with recursive estimation mechanisms .",
    "batch procedures are not necessarily designed to be computationally efficient and address the streaming nature of financial data , nor to handle the high dimensionality of the available assets for allocation .",
    "we approach the asset allocation problem from the algorithmic trading perspective , that is when investment decisions regarding allocations are taken automatically through investment allocation algorithms , as soon as data arrive . _",
    "algorithmic trading _",
    ", otherwise known as automated or systematic trading , refers to the use of algorithms to conduct trading without any human intervention . as an example , in 2006 , one - fifth of global equity trading was administered through algorithmic techniques ( keehner ( 2007 ) ) .",
    "such transactions are executed within a few milliseconds and any latency can make a difference between a profitable or loss making trade . in this",
    "context batch algorithms are unsuitable and we must consider online procedures .",
    "one such consideration is implementation of algorithms relating to portfolio optimization .    in this study",
    "we use ideas from mean - variance theory to automate the process regarding portfolio optimisation .",
    "in particular , we make use of the algebraic link of the classic mean - variance theory to ordinary least squares ( ols ) to allocate capital among various assets .",
    "we construct these algorithms bearing in mind certain considerations with regard to efficiency of trading and characteristics of financial data .",
    "these algorithms may account for one or more of the following attributes :    * _ adaptive _ : they have the ability to adapt to non - stationary market environments . by dynamically incorporating new information into the portfolio weights ,",
    "one is likely to improve the financial performance of the resulting algorithms . *",
    "_ robust _ : they are able to counter the adverse effect of outliers in estimation . *",
    "_ regularized _ : they have mechanisms to reduce the high level of noise exhibited in financial data , either though direct regularization or dimensionality reduction techniques . * _ efficient _ : they are sequential , one - pass methods to suit to the nature of the problem , that is to process information fast in order to exploit investment opportunities as they occur .",
    "the above considerations , together with ideas of asset allocation using regression , enables us to devise suitable techniques for algorithmic trading .",
    "we will use these techniques on real datasets and compare them against established and well documented asset allocation methods .",
    "online or multi - period portfolio optimization has been investigated in the literature , a non - exhaustive list includes : agarwal et al .",
    "( 2006 ) ; chapados ( 2007 ) ; frauendorfer & seide ( 2000 ) ; helmbold et al .",
    "( 1998 ) ; kuhn et al .",
    "( 2009 ) ; li & ng ( 2000 ) ; smith ( 1967 ) .",
    "montana et al .",
    "( 2008 , 2009 ) investigated online algorithms for statistical arbitrage trading strategies .",
    "some of the ( computationally fast ) portfolio optimization techniques originate in computer science / machine learning and they are algorithmically distinct from the standard mean - variance type procedures that are often found in the empirical finance literature ( as exemplified in the list above ) . as such ,",
    "one of the main objectives of this article is to bridge efficient algorithmic techniques found in various disciplines with long established portfolio selection literature in finance .",
    "the online algorithms are developed here for three reasons .",
    "* for their importance from an applied perspective .",
    "* to cross fertilize financial ideas , with ideas from signal processing , statistics , computer science and lead to more efficient techniques . * to illustrate the potential improvements in financial performance .",
    "there are a substantial number of ideas in the listed financial literature which can improve the current allocation techniques .",
    "however , they appear to be seldom used in empirical finance and our objective is to provide a simple exposure to these ideas . for example , the constraints typically used in mean - variance problems ( e.g.  demiguel et al .",
    "( 2009 ) ) correspond to standard tikhonov regularization and are well - understood in signal processing as helping to guard against instability induced by ill - conditioned matrices .",
    "ill - conditioned matrices are often encountered in mean - variance theory because of the multi - collinearity of asset log - returns , which may lead to rank deficient problems ( see hansen ( 1996 ) ) . moreover , as mentioned earlier , adaptive algorithms have the ability to adapt their estimates to the underlying data and they are naturally more suitable for non - stationary environments , such as those in finance . in the sequel , we construct two algorithms which are related to batch mean - variance and minimum variance methodology .",
    "these two methods use simple ideas from signal processing and statistics to construct fast and robust approaches to portfolio selection .",
    "this paper is structured as follows . in section",
    "[ sec : markowitz ] the mean - variance theory and our online framework is introduced . in section [ sec : r - ewrls ] the computation for our two methods is developed . in section",
    "[ sec : application ] our methods are applied to 4 real datasets and , finally , in section [ sec : conclusion ] we conclude the paper discussing possible avenues of future work .      in this paper , the following notations are adopted .",
    "all vectors are column vectors , and we denote the transpose by the prime symbol i.e. @xmath0 .",
    "the column vector of @xmath1 ones is written @xmath2 and the @xmath3 identity matrix is written @xmath4 . given a collection of @xmath5vectors @xmath6 , @xmath7 say , the @xmath8 matrix composed of the concatenation of these vectors is written @xmath9 . denote the hlder @xmath10norm by @xmath11 .",
    "the trace of a matrix @xmath12 is written @xmath13 .",
    "in the following section we introduce the problem and describe our framework .",
    "the log - returns of @xmath1 financial instruments are observed at times @xmath14 : @xmath15 , @xmath16 for @xmath17 .",
    "an investor seeks to construct a portfolio by optimally ( in some sense ) allocating funds to a collection of @xmath1 instruments .",
    "most portfolio selection problems are stated in a static or batch manner . for completeness",
    "we describe the mean - variance theory ( markowitz ( 1952 ) ) .",
    "denote the mean and covariance matrices of the log - returns as @xmath18 and @xmath19 respectively .",
    "then the objective is to solve the problem @xmath20 where @xmath21 is the @xmath1-vector of portfolio weights .",
    "this optimization problem is straight - forwardly solved via lagrange multipliers . in practical situations ,",
    "the estimated mean and covariance is substituted into the optimization problem , leading to a data - dependent solution .",
    "intrinsically , many of the portfolio optimization problems that are considered in the literature may be written as @xmath22\\bigg\\ } \\quad \\textrm{or } \\quad \\min_{\\beta } \\bigg\\{f(x_{1:t};\\beta ) + \\eta[\\beta'\\mathbb{i}_d - 1]\\bigg\\}\\ ] ] for some function @xmath23 , lagrange multiplier @xmath24 and matrix of log - returns @xmath25 .",
    "for example , one of the problems in demiguel et al .",
    "( 2009b ) : @xmath26\\bigg\\}\\ ] ] corresponds to a minimum variance portfolio with @xmath27constraints , where that `` hat '' notation refers to an estimated quantity .",
    "we note that this approach involves constructing a covariance matrix and subsequently computing its inverse to arrive to a solution .",
    "as mentioned earlier , @xmath1 can be very large and this often leads to computational delays .",
    "these computational delays can be detrimental in algorithmic trading , where tick data are streaming and decisions about allocation need to be taken instantly based on the latest information .",
    "another known portfolio allocation technique , which is used throughout this article , is the _ naive _ strategy which assigns equal constant portfolio weights to all instruments in the portfolio ( i.e. @xmath28 ) .",
    "this simple allocation technique is of practical importance , as it has been shown in an empirical study by demiguel et al .",
    "( 2009b ) to outperform many more complicated allocation techniques .",
    "the simple extension that is studied in this paper , is to consider : @xmath29\\bigg\\ } \\quad\\quad \\alpha_n = 1\\vee(n - w+1)\\quad n\\in\\{1,\\dots , t\\}\\label{eq : objective}\\ ] ] where @xmath30 is a lagrange multiplier and @xmath31 is a fixed window of data .",
    "that is , the parameters are now estimated over a sliding window @xmath31 , rather using all available data .",
    "note that when @xmath32 , then @xmath33 i.e. @xmath34 which is the vector @xmath35 .",
    "this is chosen to ensure that our algorithms are of approximately fixed computational complexity per time - step ( see section [ sec : initialization ] for discussion on window length selection ) .",
    "note , the larger the sliding window , the more data are used for estimation .",
    "conversely , the smaller the sliding window , the more weight is given to more recent data .",
    "includes some interesting special cases such as : @xmath36 which could be considered a sequential ridge - regression , for @xmath37 being the regularization parameter .",
    "this latter formulation is equivalent to a mean - variance problem ( see section [ sec : online_optimization ] ) with @xmath38constraints ; see britten - jones ( 1999 ) for details .",
    "note also , that the function in helmbold et al .",
    "( 1998 ) ( @xmath39 in their notation ) also falls into the framework above .",
    "the reason for giving is to provide a link between mean - variance theory and recursive estimation algorithms . as such , we are able to devise recursive asset allocation algorithms , through the use of recursive least squares , for dealing with streaming data and take advantage of the number of regularisation methods developed for regression to deal with the inherent instability of the portfolio solution to estimation error",
    ".      the first case we propose is @xmath40 with @xmath31 equal to the size of all available observations , @xmath41 differentiable and @xmath42 . $ ] the parameter @xmath43 is a scale parameter estimate that is used to standardize the residual error @xmath44 ; we use a robust scale parameter defined later in section [ sec : robrecscaleestim ] .",
    "the parameter @xmath45 is a forgetting factor ; this is a well - known tool in adaptive filtering e.g.  haykin ( 1996 ) .",
    "the choice of 1 in @xmath46 follows the work in britten - jones ( 1999 ) .",
    "a heuristic explanation is as follows : setting the response variable equal to a positive constant implies that our portfolio is minimised against an ideal portfolio that has positive returns for each timestep and is risk - less ( a vector a constant has zero variance ) .",
    "the objective function corresponds to a sequential form of m - estimation ( see e.g.  deng ( 2008 ) for related ideas ) .",
    "follows the recent trend in portfolio optimization to use robust statistical procedures to estimate parameters of interest ; see e.g.  demiguel & nogales ( 2009 ) . for reasons that will become apparent ,",
    "the approach associated to is termed robust - exponentially weighted recursive least squares ( r - ewrls ) .",
    "the second case is : @xmath47 \\label{eq : minvarl2}\\ ] ] where @xmath48 . the task of estimating @xmath45 and @xmath37 parameters is discussed later in section [ sec : adaptivedelta ] .",
    "this corresponds to an online minimum - variance - type algorithm with @xmath38constraints ( termed online minimum - variance ( o - var ) throughout ) .",
    "the matrix @xmath49 introduces a forgetting - factor into the optimization scheme .",
    "the use of the estimated second moment , instead of the covariance is for computational reasons ; we did not find a substantial discrepancy ( in terms of financial performance ) when compared to using the covariance matrix .",
    "note that a more standard recursive estimate could be obtained using the function @xmath50\\ ] ] but is not considered here , due to the relationship of to the standard minimum - variance approach .",
    "the batch version of is studied in demiguel et al .",
    "( 2009b ) .",
    "the @xmath38constraints correspond to an @xmath51 distance with the naive allocation strategy .",
    "the naive approach to allocation surpasses estimation of the sample mean and one would expect relatively stable portfolio weights .",
    "note that , for both procedures there are unknown parameters @xmath45 , @xmath37 and @xmath43 .",
    "the next section discusses how these parameters may be set , in addition to recursive formulation of the proposed optimizations .",
    "in this section , we introduce our recursive updating approaches .",
    "this section is core to the development of the adaptive allocation algorithms as it formulates efficient regression techniques appropriate to the nature of algorithmic trading .",
    "let us introduce some notations : @xmath52\\quad\\quad q(x )   =   \\frac{1}{x}\\frac{d\\rho}{d x}(x).\\ ] ] then , ignoring the lagrange multiplier ( the result can be renormalized ) , we are to minimize . differentiating , it follows that the optimal @xmath53 solves @xmath54\\widetilde{x}_i.\\ ] ] since this equation is often non - linear , we use the approximation @xmath55 , with @xmath56 given ( i.e.  by the previous step , or by initialization ) .",
    "now , let @xmath57 denote the l.h.s .  and @xmath58",
    ", then we are to solve @xmath59 as @xmath60 , and writing @xmath61 , it follows via the sherman - morrison ( e.g.  haykin ( 1996 ) ) formula @xmath62 with @xmath63 using @xmath64 we thus have the recursion @xmath65    we have presented a recursive least squares procedure whose algebraic equivalence with the kalman filter is well - known and understood ( see chapter @xmath66 of sayed ( 2003 ) ) .",
    "it should be remarked that related ideas have appeared in cipra & romera ( 1991 ) and our approach is similar to robust filters ( martin ( 1979 ) ; masreliez ( 1975 ) ; schick & mitter ( 1994 ) ) .",
    "the calculation of the scale parameter is now detailed .",
    "our approach uses robust statistics .",
    "first , we note that the median absolute deviation ( mad ) ( e.g.  huber ( 2004 ) ) estimate of scale is given by @xmath67 where @xmath68 is a chosen data window and @xmath69 is the median function .",
    "recent research has pointed to efficient techniques to compute the median with @xmath70 average complexity using recursive binning schemes ( see tibshirani ( 2008 ) ) .",
    "second , an exponentially recursive median absolute deviation ( ewmad ) estimator is considered @xmath71 and where @xmath72 is another forgetting factor and @xmath73 is an ewmed ( exponentially weighted recursive median ) , given by @xmath74 where @xmath75 is a correction factor to make mad consistent with the normal distribution ( e.g.  huber ( 2004 ) ) .",
    "the ewmed is similar to the well documented ewma ( e.g.  hamilton ( 1994 ) ) with the only difference that the ewmed estimator replaces the latest information @xmath35 by its median estimate over the sliding window . on the basis of much preliminary investigation on specific datasets ,",
    "we have arbitrarily set @xmath76 and @xmath77 for all of the applications . due to the robust nature of the above estimation , this method is termed robust - exponentially weighted recursive least squares .      as discussed earlier ,",
    "financial data are inherently noisy and exhibit high degree of dependence .",
    "the noise hampers the ability to accurately forecast and the dependence structure of assets accentuates the problem , as pointed out in the introduction ; this is via the instability of portfolio weights caused by potential rank deficiency . to alleviate for these problems",
    "we adopt a low rank matrix approximation of @xmath78 , @xmath79 in order to eliminate those components of data that contain most of the noise .",
    "this approach aims to optimally approximate , with respect to some norm , a matrix of lower rank while retaining the same same dimension .",
    "it is well known that the best low rank approximation can be found by singular value decomposition ( svd ) under the frobenius norm ( see e.g.  stewart ( 1993 ) ) .",
    "the approach is as follows .",
    "let @xmath80 be given and denote the singular value decomposition ( svd ) of the returns matrix @xmath81 .",
    "consider the truncated svd ( see hansen ( 1987 ) ) @xmath82 then set @xmath83 .",
    "we replace @xmath35 in the recursions in section [ sec : seqsestimation ] with the final row of @xmath84 .",
    "the value of @xmath85 is set during training .",
    "note that the svd of @xmath78 can be updated incrementally using the methods in ( bunch & nielsen ( 1978 ) ) .",
    "the minimum - variance scheme is somewhat less involved .",
    "suppose @xmath45 , @xmath86 and @xmath31 is given .",
    "it is straight - forward to show that , at time @xmath87 , the solution of the optimization problem , with @xmath88 as in is @xmath89 the main objective here is to calculate this quantity quickly .",
    "suppose we are given the eigen - decomposition of @xmath90 , i.e.  @xmath91 , then the inverse in is equal to @xmath92 that is , one need only calculate the inverse of a diagonal matrix .",
    "the recursive calculation of the eigen - decomposition can be achieved by the methods of yu ( 1991 ) in @xmath93 ; i.e.  this operation is @xmath94 instead of the standard @xmath95 ( @xmath96 ) for matrix inversion .",
    "more specifically , the method of yu ( 1991 ) is to re - calculate the new eigen - decomposition of @xmath97 , from @xmath98 to @xmath97 of the form @xmath99 with @xmath100 vectors of the appropriate dimension . in our case",
    "we have that @xmath101 so the same ideas may be applied .",
    "note that the incremental svd mentioned above could also be used .",
    "there are still 2 free parameters to be set ; @xmath86 and @xmath45 .",
    "first , consider @xmath86 . lacking an analytical solution",
    ", we investigate @xmath37 numerically based on an initial training data period . to investigate the effect of @xmath37 perturbations to portfolio returns , we choose a short initial training sequence of data to calculate @xmath102 for a given @xmath45 .",
    "then , we select a collection of @xmath103 equally spaced points between @xmath104 and @xmath105 .",
    "the algorithm is initialized at any of those points . at re - balancing times",
    "( the times when the allocation is altered ) we compute the portfolio returns over the training period for each of the @xmath103 points and select the one that generates the largest portfolio return .",
    "the range of the grid is based upon the recommendations in ledoit & wolf ( 2004 ) .",
    "we found our results to be extremely robust to the initial value of @xmath37 .",
    "second , consider @xmath45 . in this scenario , we only recalculate @xmath45 at re - balancing times , which incurs the cost of re - computing the eigen - decomposition of @xmath90 .",
    "we follow a similar procedure to that in adaptive filtering .",
    "an attractive criterion for portfolio selection , is to minimize @xmath106 see britten - jones ( 1999 ) . as a result ,",
    "at the @xmath107re - balancing time , the following stochastic approximation type update is used : @xmath108 \\bigg\\}.\\ ] ] see e.g.  chapter 14 , haykin ( 1996 ) for similar self - tuning approaches for recursive filtering .",
    "note that if @xmath109 , then we set @xmath110 .      the two methods described here have some complementary aspects .",
    "firstly , from the perspective of dealing with noisy data , the methods use separate , but well known procedures .",
    "r - ewrls uses the truncated svd , whilst the o - var uses a form of tikhonov regularization via @xmath38constraint .",
    "secondly , the r - ewrls method accounts for outliers by down - weighting them through a by - product weighting quantity ( @xmath111 , see section [ sec : seqsestimation ] ) of the robust cost function . on the other hand , o - var does not have an embedded mechanism to account for outliers as they occur .",
    "thirdly , the o - var is adaptive to non - stationary environments and accounts for variability in the underlying environment through the self - tuning forgetting factor @xmath45 .",
    "however , the rank @xmath37 needs to be set during training . in r - ewrls case , @xmath45 needs to be calibrated in advance and such calibration needs to take place every time a shift occurred in the underlying environment . also , rank @xmath85 of the low rank approximation ( section [ sec : low_rank_approx ] ) needs to be set in advance .",
    "it is likely that one procedure is likely to be preferred given the scenario .",
    "for example , when the data are subject to a change in the economic cycle , one would expect the o - var to perform significantly better , however , o - var it does not take into consideration expected returns . in that respect , o - var may be more suitable for assets that are expected to grow in the future .",
    "for instance , it may be suitable for fund of funds whose underlying investments have positive expectation and desire to allocate robustly . alternatively , it could be suitable for an algorithmic trading system that allocates between allocation strategies in an adaptive and efficient way .",
    "finally , the r - ewrls is linked to mean - variance theory and should be suitable for any asset class and as a standalone allocation strategy .",
    "note that o - var is similar to a more efficient version of the function in demiguel et al .",
    "( 2009b ) .",
    "the techniques described in section [ sec : r - ewrls ] are applied to 4 datasets .",
    "financial performance is compared to standard methods .",
    "note that a zero - rate risk free interest rate is assumed throughout .",
    "we perform our analysis on 4 datasets ; spot foreign exchange ( fx ) , constituents of dj euro stoxx , portfolios of nyse , nasdaq , amex and constituents of ftse-100 ( see figure [ fig : fxspotprices ] ) .",
    "our first dataset consists of @xmath112 spot currencies quoted against the american dollar . for ease of interpretation",
    ", we use the convention  usd/  \" , where usd is always the base rate and is read ",
    "units of foreign currency per 1 usd \" .",
    "the dataset covers a period of approximately @xmath113 years of daily data , from @xmath114 until @xmath115 .",
    "the spot data have been obtained from the  fxhistory \" functionality of oanda ( ` www.oanda.com ` ) .",
    "the second consists of 43 constituents of dj euro stoxx 50 , of approximately @xmath116 years of closing prices , from @xmath117 until @xmath118 .",
    "the data have been obtained from yahoo ( ` http://uk.finance.yahoo.com/ ` ) and have been adjusted for discontinuities related to financial events , such as stock splits and bonus issues .",
    "the third dataset are the daily returns on 25 portfolios formed on size and book - to - market from nyse , nasdaq and amex .",
    "the data are from 01/07/63 - 31/12/08 .",
    "the data were obtained from ` http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ ` ` data_library.html ` .",
    "our final dataset are 6 constituents ( ba , barclays , lloyds tsb , m & s , rbs , tescos ) of the ftse-100 share index .",
    "the daily data are the adjusted closing prices taken from 17/07/04 - 17/07/09 and also obtained from yahoo .",
    "these particular data will be of interest , to observe the performance of relatively simple allocation schemes , during 2 financial crises : the selloff in 2006 caused by algorithmic trading and the sub - prime mortgage crisis in 2008 .",
    "it should be noted that some of our data are clearly subject to survivorship bias ; one should take this into account when looking at the performance measures .      in our comparison , in addition to the methods developed in section [ sec : r - ewrls ] , we consider 3 standard batch strategies :    * naive .",
    "this encompasses allocating funds in equal amount to each asset .",
    "as noted in demiguel et al .",
    "( 2009a ) , this strategy provides an important benchmark despite its simplicity .",
    "* mean - variance ( m - var ) .",
    "this is the standard mean - variance methodology . to remove any numerical difficulties with inversion ,",
    "as noted in introduction , the covariance matrix is replaced by @xmath119 , where @xmath120 is some non - negative constant .",
    "the regularisation parameter is chosen as @xmath121 , similar to ledoit & wolf ( 2004 ) . *",
    "minimum - variance ( var ) .",
    "standard minimum - variance methodology with the covariance replaced as for m - var .    for r - ewrls ,",
    "if one could interpret the procedure as a regression , this would imply a hyperbolic secant error distribution ( see benesty & gansler ( 2001 ) ) .",
    "we experimented with more standard choices of @xmath123 ( e.g.  huber s loss function , see huber ( 2004 ) ) but did not find that this significantly affected our conclusions . note that we implemented the method of helmbold et al .",
    "( 1998 ) , but did not find a significant difference with the naive strategy .",
    "in order to compare and investigate our strategies , we consider various criteria . the basic idea is to initialize all of the strategies in some way ; the first 2 years ( 504 data points ) of each dataset are used for training ( i.e.  omitted afterwards ) .",
    "in particular , and helping to avoid look - ahead bias , the m - var and var strategies use the first 2 years of data to estimate the portfolio weights and these are used until the first re - balancing instant .",
    "the re - balancing instant is then determined going forward by the re - balancing window @xmath31 i.e. re - balancing every 250 data points .",
    "then the data in the time up - to the last re - balancing period is used to re - estimate the weights .",
    "the weights are initialized as 1 and are employed from day 2 on - wards .",
    "note that the actual weights used to compute portfolio returns , they are only based upon those calculated at re - balancing times .",
    "that is to say , we update the weights for the online methods , but only employ new weights at re - balancing times .",
    "as such , trading is infrequent and the transaction cost associated with these allocation strategies is negligible .",
    "therefore , we refrain from using transaction costs , as this would have introduced another layer of assumptions since transaction costs often differentiate substantially from firm to firm given their `` bargaining '' power to negotiate down trading commissions .",
    "the criteria employed are standard in financial applications .",
    "the returns for each day are calculated and we consider : annualized returns and volatility , sharpe ratio , % average daily gain and loss , % of winning trades ( wt ) , maximum draw - down ( mdd ) and turnover ( to ) . of these ,",
    "perhaps the last 2 need a little explanation .",
    "the maximum draw - down is equal to @xmath124 where @xmath125 is the percentage return at period @xmath126 . in words",
    "it constitutes the maximum movement from peak to trough of the cumulative returns , in percentage terms .",
    "the turnover is a measurement of the frequency of trading .",
    "it is the average of the absolute difference of the portfolio weights between re - balancing times .",
    "we now discuss the selection of parameters for the r - ewrls approach .",
    "we explore the sharpe ratio for the spot fx and dj euro stoxx 50 datasets over a grid of equally - spaced values of the parameter @xmath85 and the forgetting factor @xmath45 .",
    "the results of the exploratory analysis are depicted by means of contour plots ( figure [ fig : sharpematrewrls ] ) .    for the r - ewrls allocation strategy using the equities data , we note in figure [ fig : sharpematrewrls ] that the sharpe ratio is positive throughout the parameter space .",
    "there is an evident pattern that lower values of @xmath85 exhibit higher sharpe ratio and the difference becomes more pronounced for higher values of @xmath45 . for the fx dataset , we note ( fig .",
    "( [ fig : sharpematrewrls ] ) ) that there are evident structures of higher sharpe ratio regions in the parameter space suggesting dependence to the @xmath45 and @xmath85 parameters . in particular ,",
    "the best performance is achieved for values of @xmath45 approximately between @xmath127 and @xmath128 , when @xmath85 is greater than @xmath129 . on the basis of such plots , we select the values of @xmath85 and @xmath45 .",
    "the choice of @xmath31 is important for both of our methods and to an extent , conflicting with @xmath45 .",
    "that is to say , instead of making @xmath45 large , @xmath31 can be made smaller and vice - versa .",
    "however , the choice of @xmath31 is also a computational issue ; we may only want to attribute a set memory to the storage associated to the data .",
    "this is the line we follow and set @xmath130 ( approximately 1 year of trading ) which is not too large for computational purposes and does not interfere substantially with the data memory profile implied by @xmath45 , for the purposes of portfolio selection .",
    "this is to say that the exponential decay profile would only be truncated for @xmath31 greater than @xmath131 .",
    "then the role of @xmath45 is far clearer with respect to the forgetting of the data .",
    "the algorithms were run with re - balancing performed every 50 , 150 and 250 days . on the basis of training , the r - ewrls used @xmath132 for the first 2 datasets , @xmath133 for the third and @xmath134 for the fourth ; respectively @xmath135 .",
    "for o - var , @xmath136 ( see section [ sec : adaptivedelta ] ) and the initial @xmath137 . note that in each instance , the forgetting factor converged close to 1 ( implying very little forgetting ) , when there were sufficient re - balancing periods .",
    "we conducted a computational speed comparison between the batch mean - variance optimization approach against our methods .",
    "we coded the methodologies in matlab ( version 7.4 ) . in a data matrix of @xmath138 dimension",
    ", we found that an iteration needs approximately @xmath139 milliseconds compared to @xmath140 seconds for the batch mean - variance computation . in a separate experiment",
    ", we increased the number of rows from @xmath141 to @xmath142 .",
    "the batch approach computation time increased to @xmath143 seconds .",
    "the results can be found in tables [ tab : res50]-[tab : res250 ]",
    ". some of the annualized volatilities of the strategies exhibited on the tables could be rather high and unrealistic for an investor , but the results are clearly valid as we compare the sharpe ratio which adjusts for volatility of the underlying strategy .",
    "however , one needs to be cautious when comparing maximum draw - down of allocation strategies , as this depends on the volatility of the underlying strategy .",
    "let us consider each dataset in turn .",
    "our first observation is that only the r - ewrls method is consistently producing positive returns .",
    "indeed , this is true with respect to different re - balancing periods .",
    "this partly suggests  inferring from figure [ fig : sharpematrewrls ] as well that for particularly noisy data , the truncated svd has a beneficial outcome in the portfolio weights computation ; this is in contract to m - var whose performance is sensitive to change in @xmath31 and that result is in line with those reported in the literature .",
    "we also note that the portfolio weights of r - ewrls are more `` active '' , as indicated by turnover .",
    "this could also imply the r - ewrls adapts better in the underlying environment , given that delivers consistently better performance than m - var . for the o - var , due to its similarity to the naive strategy",
    ", it is unable to provide positive returns ; the latter it exhibits particularly bad performance here .",
    "this is because the naive allocation strategy implies only long positions and is expected to benefit from a long - term growth typically exhibited in equities , but not necessarily for fx spot prices .      moving to the second dataset a more familiar pattern ( i.e.  as is often reported in the literature ) is displayed .",
    "the naive and var strategies perform relatively well , with quite favourable sharpe ratios , given the simplicity of the strategies .",
    "the o - var method performs marginally better than the var strategy , but with a noticeable increase in turnover .",
    "r - ewrls also delivers satisfactory performance and outperforms m - var .",
    "the portfolio data provide some very interesting results . in this case",
    "the o - var provides the most impressive results from a financial perspective , but performance tends to decrease as the re - balancing time increases .",
    "the success of the o - var method is linked to a wide variety of factors .",
    "firstly , due to its similarity to naive , this method is likely to fair very well ; see figure [ fig : fxspotprices ] and the remarks in section [ sec : spotfxrates ] . secondly , o - var method should fair well because all parameters are adaptive to the data . however , we note that r - ewrls is only trained on the first 2 years of data .",
    "since the data are 45 years long , 2 years is clearly insufficient in which to train the algorithm .",
    "although this is a little unfair ( e.g.  the parameters can be retrained every 5 years , as would be the case in practice ) , it highlights a small deficiency of the r - ewrls method .",
    "thirdly , against the var method , the the smoothness of the portfolio weights is regulated by the tikhonov regularization .",
    "this may have beneficial outcome in the performance through better estimation ( see introduction for rank deficiency discussion ) .",
    "the final data provide an interesting set of results . due to a variety of economic , cultural ( business - wise ) and investor related factors ,",
    "many quantitative equity hedge funds have performed poorly during the current financial crisis . as a result , it is of interest from an applied perspective to observe the results of our models in such a difficult trading period . rather unsurprisingly ,",
    "many of the strategies perform badly .",
    "however , in 2 instances , both of our online methods provide positive returns .",
    "this is encouraging , as to an extent it suggests that the ability to process data as it arrives and adapt our strategies accordingly is more useful in practice than standard batch methods .      on the basis of our investigations",
    ", we make the following observations :    1 .   the r - ewrls method can be successful ( positive returns ) for noisy data .",
    "however , when the initial training period is insufficient / unreliable , very unstable results are obtained .",
    "in addition , high turnovers were observed for this method .",
    "the online and adaptive nature of the o - var method , coupled with its link to the naive strategy leads consistently to strong performance in comparison to the methods tested here .    in terms of the first point",
    ", the r - ewrls approach is related to m - var procedures , which can work well when there is detectable drift signal in the data . when combined with the robust scale computation and noise reduction",
    "a potentially superior method is derived . however , there are a number of free parameters , which are to be set . as a result ,",
    "significant training is required and hence the success of the method is reliant on this latter procedure .    the second point is clearly reflected in the tables [ tab : res50]-[tab : res250 ] .",
    "the drawbacks of the r - ewrls method are alleviated , but with the potential deficiency of being related to the naive strategy , that is making a naive assumption for the direction of the market by having long only positions .",
    "this can lead to poor performance , e.g.  for the fx spot data .",
    ".algorithm performance across datasets .",
    "the portfolios are re - balanced every 50 days .",
    "see section [ sec : compcriteria ] for details . [",
    "cols=\"<,<,>,>,>,>,>,>,>,>\",options=\"header \" , ]",
    "we have derived two efficient methods to compute portfolio weights online without the need of matrix inversion .",
    "we compared the two methods with existing techniques in portfolio optimization using 4 datasets .",
    "we showed that our strategies predominantly outperform the benchmarks , when performance is measured by sharpe ratio ( note that this includes the method of helmbold et al .",
    "( 1998 ) ) .",
    "future research can focus in extending our approach to include transaction costs ( bid - ask spread and commission ) as a function of the portfolio weight , as well as to consider adaptive re - balancing strategies ( e.g.  baltutis , 2009 ) . for example , the o - var method does not explicitly incorporate previous weights in its estimate and , as such , can lead to high turnovers .",
    "in addition , future work could be focused upon making r - ewrls fully adaptive .",
    "this requires the online selection of the number of singular values and lies on the interface of statistics , finance , signal processing and computer science . finally , one of the drawbacks of the o - var method was its relation to naive allocation strategy .",
    "this could be removed , for example using @xmath27type constraints leading to an online lasso ( tibshirani ( 1996 ) ) method ( see e.g.  anagnostopoulos et al .  ( 2008 ) ) . in this context , as the portfolio weights are required to sum to one ( i.e.  standard path - wise co - ordinate optimization ( friedman et al .  ( 2007 ) ) does not apply , we are left with an online quadratic programming problem . to our knowledge , with the exception of zhang & li ( 2009 )",
    ", there is little methodology for this problem ; we are currently working towards a solution .",
    "our work also opens up interesting theoretical questions ; e.g.  to investigate the sensitivity of the portfolio weights ( as in demiguel & nogales ( 2009 ) ) of online algorithms ."
  ],
  "abstract_text": [
    "<S> we present an online approach to portfolio selection . </S>",
    "<S> the motivation is within the context of algorithmic trading , which demands fast and recursive updates of portfolio allocations , as new data arrives . in particular , we look at two online algorithms : robust - exponentially weighted least squares ( r - ewrls ) and a regularized online minimum variance algorithm ( o - var ) . </S>",
    "<S> our methods use simple ideas from signal processing and statistics , which are sometimes overlooked in the empirical financial literature . </S>",
    "<S> the two approaches are evaluated against benchmark allocation techniques using 4 real datasets . </S>",
    "<S> our methods outperform the benchmark allocation techniques in these datasets , in terms of both computational demand and financial performance . + * keywords * : portfolio selection , mean - variance portfolios , adaptive filtering , robust , online , investment management .    * robust and adaptive algorithms for online portfolio selection *    theodoros tsagaris , ajay jasra & niall adams </S>"
  ]
}