{
  "article_text": [
    "thermodynamics of information processing has seen a resurgence of interest recently . from a theoretical point of view , the advances in nonequilibrium statistical mechanics over the last two decades have opened up a new avenue of research to generally and quantitatively investigate the relationship between nonequilibrium thermodynamics and information theory  @xcite , shedding new light on the longstanding problem concerning maxwell s demon  @xcite . from an experimental point of view",
    ", developments in experimental techniques have led to the realization of maxwell s demon with small thermodynamic systems  @xcite .",
    "furthermore , the nonequilibrium equalities such as the fluctuation theorem ( ft )  @xcite have been generalized to the case under information processing .",
    "for example , we have derived a generalized ft in the presence of an information exchange  @xcite . however",
    ", a fundamental question remains elusive : what is the relationship between the exchanged information inside the universe and the total entropy production in the universe ?",
    "here , the `` universe '' means the relevant entire system including heat baths .    in the present paper",
    ", we address this question by focusing on the role of the mutual information in the total entropy production in the whole system . by deriving a decomposition formula of the total entropy production into the thermodynamic and informational parts",
    ", we investigate ft and the second law of thermodynamics ( sl ) in the presence of information processing .",
    "in particular , we examine sl under multiple information exchange .",
    "we also point out that there exists a certain duality between measurement and feedback , which relates the entropic cost for measurement to that for feedback .",
    "moreover , we study the detailed structure of a memory that stores information , and obtain a general formula that determines the fundamental energy cost needed for measurement and feedback control .",
    "all of our results are based on the detailed fluctuation theorem ( dft )  @xcite , and are therefore not restricted to the near - equilibrium regime .",
    "our theory provides the basis for understanding the entropic and energetic properties of information - driven nanomachines  @xcite .",
    "this paper is organized as follows . in sec .  2",
    ", we consider the case of a single information exchange , and derive a general formula of the decomposition of the entropy production . in sec .  3",
    ", we consider the case of multiple information exchanges , and apply the obtained general result to the composite process of measurement and feedback control ; this process includes a typical setup of maxwell s demon . in sec .  4",
    ", we analyze the entropic and informational roles played by the memory , which enables us to derive the minimal energy cost needed for measurement . in sec .  5",
    ", we conclude this paper . in appendix",
    "a , we discuss the entropy production in the heat bath , and clarify the physical meaning of the total entropy production along the line with the standard nonequilibrium statistical mechanics .",
    "in this section , we consider the case of a single information exchange . in sec .",
    "2.1 , we briefly review as much of information theory as is needed for later discussions . in sec .",
    "2.2 , we derive a general formula of the decomposition of the entropy production under information processing . in sec .  2.3 and 2.4",
    ", we apply the general formula to situations under feedback and measurement , respectively . in sec .",
    "2.5 , we discuss a duality between measurement and feedback .",
    "we first review the shannon entropy ( or information ) and the mutual information  @xcite , which play key roles in following discussions .",
    "let @xmath0 be a probability variable with probability distribution @xmath1 $ ] .",
    "the stochastic shannon entropy is defined by @xmath2 : = - \\ln p[x],\\ ] ] which characterizes how rare the occurrence of an outcome @xmath0 is ; the rarer it is , the greater @xmath3 $ ] becomes .",
    "the average of @xmath3 $ ] over the probability distribution @xmath1 $ ] gives the shannon entropy @xmath4 \\ln p[x ] .",
    "\\label{shannon}\\ ] ] if @xmath0 is a continuous variable , the sum in eq .",
    "( [ shannon ] ) is replaced by the integral .",
    "let @xmath0 and @xmath5 be two probability variables with joint probability distribution @xmath6 $ ] .",
    "the marginal distributions are given by @xmath1 : = \\sum_y p[x , y]$ ] and @xmath7 : = \\sum_x p[x , y]$ ] .",
    "the stochastic mutual information is defined by @xmath8 : = \\ln \\frac{p[x , y]}{p[x]p[y]}.\\ ] ] the ensemble average of @xmath9 $ ] gives the mutual information : @xmath10 \\ln \\frac{p[x , y]}{p[x]p[y]}.\\ ] ] the mutual information characterizes the correlation between the two probability variables .",
    "we also note the relation @xmath11 where @xmath12 \\ln p[y ] , \\",
    "\\langle s_{xy } \\rangle : = - \\sum_{xy } p[x , y ] \\ln p[x , y].\\ ] ]    the shannon entropy of @xmath0 and the mutual information between @xmath0 and @xmath5 satisfy the following inequalities : @xmath13 where the left equality is achieved if and only if the two variables are not correlated , or equivalently statistically independent ( i.e. , @xmath6 = p[x ] p[y]$ ] ) ; the right equality is achieved if and only if , for any @xmath5 , there exists a unique @xmath0 such that @xmath6 \\neq 0 $ ] .",
    "a parallel argument holds true if we replace @xmath14 by @xmath15 in eq .",
    "( [ mutual_shannon2 ] ) .",
    "we consider stochastic dynamics of two systems @xmath16 and @xmath17 in the presence of information exchange between them .",
    "we assume that @xmath16 is attached to heat baths with inverse temperatures @xmath18 ( @xmath19 ) .",
    "we denote the baths collectively as @xmath20",
    ". system @xmath16 then evolves under the influence of system @xmath17 , where we assume that the phase - space point of @xmath17 at a particular time , denoted as @xmath5 , only affects the dynamics of @xmath16 ( see also fig .  1 ) .",
    "we note that the present situation is the same as the one in our previous paper  @xcite , but we here adopt a different approach to deriving ft and sl .     under the influence of @xmath17 .",
    "system @xmath16 evolves from @xmath0 to @xmath21 along trajectory @xmath22 , where the phase - space point of @xmath17 at a particular time , denoted as @xmath5 , only affects the dynamics of @xmath16 .",
    "there may be initial and final correlations between @xmath16 and @xmath17 which are characterized by mutual information contents @xmath23 and @xmath24.,width=188 ]    let @xmath0 and @xmath21 be the initial and final phase - space points of @xmath16 , and @xmath5 be the phase - space point of @xmath17 .",
    "let @xmath25 $ ] and @xmath26 $ ] be the initial and final joint probability distributions of the composite system @xmath27 . here",
    "the subscript `` @xmath28 '' indicates the `` forward process . ''",
    "we define @xmath29 : = \\int dy p_f^i[x , y]$ ] , @xmath30 : = \\int dy p_f^f[x ' , y]$ ] , and @xmath31 : = \\int dx p_f^i[x , y ] = \\int dx ' p_f^f[x ' , y]$ ] .",
    "we note that the marginal distribution of @xmath5 does not change in time .",
    "we assume that there may , in general , be the initial and final correlations between @xmath16 and @xmath17 , i.e. , @xmath32 \\neq p_f^i [ x ] p_f [ y ] $ ] and @xmath33 \\neq p_f^f [ x ] p_f [ y]$ ] .",
    "we consider the difference between the shannon entropy of @xmath34 and that of @xmath35 , which is given by @xmath36 ) - ( - \\ln p_f^i [ x , y]).\\ ] ] it can be rewritten as @xmath37 where @xmath38 ) - ( - \\ln p_f^i [ x]),\\ ] ] @xmath39 ) - ( - \\ln p_f [ y ] ) = 0,\\ ] ] @xmath40}{p_f^f [ x']p_f[y ] } - \\ln \\frac{p_f^i [ x , y]}{p_f^i [ x ] p_f[y]}.\\ ] ] since @xmath41 , we obtain @xmath42 in the following , we denote the initial and final shannon entropies of @xmath16 as @xmath43 , \\",
    "s_x^f : = - \\ln p_f^f [ x'].\\ ] ]    let @xmath44 be the heat absorbed by system @xmath16 from the @xmath45th bath . following to the standard nonequilibrium thermodynamics  @xcite , the entropy production in the total system ( @xmath16 , @xmath17 , and @xmath20 ) during the present dynamics is given by @xmath46 where @xmath47 is the entropy production in @xmath20 ( see appendix a for details ) .",
    "we then obtain the decomposition of the total entropy production as follows : @xmath48 where @xmath49 is the entropy increase in @xmath50 .",
    "we examine the above result in terms of dft .",
    "let @xmath22 be the trajectory of @xmath16 in the forward process .",
    "the joint probability distribution of @xmath22 and @xmath5 is given by @xmath51 = p_f [ x_f | x , y]p_f^i [ x , y],\\ ] ] where @xmath52 $ ] is the conditional probability of @xmath22 under the initial condition @xmath34 , where the dependence on @xmath5 reflects the effect of information exchange .",
    "we write the ensemble average of an arbitrary quantity @xmath53 $ ] as @xmath54 a[x_f , y].\\ ] ]    to formulate dft , we need to introduce the concept of backward processes , where the time dependence of external parameters such as the magnetic field is time - reversed .",
    "the backward probability distribution is given by @xmath55 = p_f [ x_b | \\tilde x , \\tilde y ] p_b^i [ \\tilde x , \\tilde y ] , \\ ] ] where @xmath56 $ ] is the conditional probability of @xmath57 under the initial condition @xmath58 .",
    "let @xmath59 and @xmath60 be the time - reversal of the phase - space points @xmath0 and @xmath5 , respectively .",
    "for example , if @xmath61 with position @xmath62 and momentum @xmath63 , then @xmath64 . for @xmath65",
    ", we define its time - reversal as @xmath66 . in a broad class of nonequilibrium dynamics ,",
    "the entropy production in @xmath20 satisfies  @xcite @xmath67 } { p_b [ x_f^\\dagger | x'^\\ast , y^\\ast ] } , \\label{dft0}\\ ] ] where the left - hand side ( lhs ) is the entropy production in @xmath20 in the forward process , and the right - hand side ( rhs ) is the ratio of the probability distributions of the forward and backward trajectories .",
    "we then assume that the initial distribution of the backward processes is given by the time - reversal of the final distribution of the forward process : @xmath68 : = p_f^f [ x'^\\ast , y^\\ast ] , \\ ] ] which leads to dft for the total system : @xmath69 } { p_b [ x_f^\\dagger , y^\\ast]}.\\ ] ] we then have @xmath70 } { p_b [ x_f^\\dagger | x'^\\ast , y^\\ast ] } + \\ln \\frac{p_f^i [ x , y]}{p_f^f [ x ' , y ] } \\\\ & = \\ln \\frac{p_f [ x_f | x , y ] } { p_b [ x_f^\\dagger | x'^\\ast , y^\\ast ] } + \\ln \\frac{p_f^i [ x | y]}{p_f^f [ x ' |   y ] } \\\\ & = \\ln \\frac{p_f [ x_f | x , y ] } { p_b [ x_f^\\dagger | x'^\\ast , y^\\ast ] } + \\ln \\frac{p_f^i[x]}{p_f^f[x ' ] } +   \\ln \\frac{p_f^i [ x | y]}{p_f^i[x ] }   + \\ln \\frac{p_f^f[x']}{p_f^f [ x ' |   y ] } \\\\ & = \\ln \\frac{p_f [ x_f | x , y ] p_f^i[x ] } { p_b [ x_f^\\dagger | x'^\\ast , y^\\ast ] p_f^f[x ' ] } +   \\ln \\frac{p_f^i [ x | y]}{p_f^i[x ] }   - \\ln \\frac{p_f^f [ x ' |   y]}{p_f^f[x']}. } \\ ] ] by noting that @xmath71p_f^i[x ] } { p_b [ x_f^\\dagger | x'^\\ast , y^\\ast ] p_f^f[x']},\\ ] ] we reproduce eq .  ( [ main1 ] ) .    in the present setup , the kawai - parrondo - van den broeck ( kpb ) equality  @xcite",
    "is given by @xmath72 \\ln \\frac{p_f [ x_f , y ] } { p_b [ x_f^\\dagger , y^\\ast ] } , \\label{kpb1}\\ ] ] where the rhs is the relative entropy between the forward and backward trajectories . from the positivity of the relative entropy  @xcite , we obtain sl for the total process : @xmath73 which is equivalent to @xmath74 inequality  ( [ second2 ] ) implies that the lower bound of the entropy increase in @xmath50 is given by the change in the mutual information between @xmath16 and @xmath17 .",
    "let @xmath75 be the set of @xmath34 such that @xmath25 \\neq 0 $ ] .",
    "we then have @xmath76 \\frac{p_b [ x_f^\\dagger , y]}{p_f [ x_f , y ] } = \\int_{\\mathcal s } dx_f^\\dagger dy^\\ast p_b [ x_f^\\dagger , y ^\\ast],\\ ] ] where we used @xmath77 and @xmath78 .",
    "if @xmath75 is the whole phase space , we obtain the integral fluctuation theorem ( ift ) or the jarzynski equality : @xmath79 which is equivalent to @xmath80    the crucial assumption here is that the dynamics of @xmath16 is affected only by the phase - space point @xmath5 at a particular time .",
    "therefore , @xmath17 does not necessarily stay at @xmath5 as @xmath16 evolves , as long as the evolution of @xmath17 does not affect the dynamics of @xmath16 .",
    "therefore , the probability distribution of @xmath22 is characterized by @xmath52 $ ] that is not affected by the time evolution of @xmath17 .",
    "although we have obtained the same results as ( [ kpb1 ] ) , ( [ second2 ] ) , and ift  ( [ ift1 ] ) in a previous paper  @xcite , we stress that in this paper we have adopted a new approach to deriving them on the basis of the decomposition formula  ( [ main1 ] ) .",
    "the present approach gives a new insight compared with the previous one , in that it enables us to understand the generalized ft and sl as a result of the decomposition of the total entropy production .",
    "we note that a decomposition formula similar to eq .",
    "( [ main1 ] ) has been discussed in ref .",
    "@xcite for special cases .    in the absence of information exchange ,",
    "@xmath52 $ ] is independent of @xmath5 so that @xmath81 $ ] . in this case",
    ", @xmath82 satisfies the conventional dft and therefore its expectation value is nonnegative : @xmath83 \\ln \\frac{p_f [ x_f | x ] p_f^i[x ] } { p_b [ x_f^\\dagger | x'^\\ast ] p_f^f[x ' ] } \\geq 0 .",
    "\\label{no_information}\\ ] ] we also have @xmath84 \\ln \\frac{p_f^f [ x ' |   y]p_f^i[x]}{p_f^f[x']p_f^i [ x | y ] } \\\\ & =   \\int dx_f dy   p_f[x_f , y ] \\ln \\frac{p_f^f [ x ' |   y ] p_f [ x_f | x ] p_f^i[x]}{p_f^f[x']p_f [ x_f | x ] p_f^i [ x | y ] } \\\\ & = \\int dx_f dy   p_f[x_f , y ] \\ln \\frac{p_f [ x_f | x ' ] } { p_f [ x_f | x ' , y ] } \\\\ & =   - \\int dx_f dy p_f^f[x ' ]   p_f[x_f , y | x ' ] \\ln \\frac{p_f [ x_f , y | x ' ] } { p_f [ x_f | x ' ] p_f[y | x ' ] } \\\\ & \\leq 0 , } \\ ] ] which is a special case of the data processing inequality  @xcite . therefore , in the absence of information processing , we obtain @xmath85 in other words , inequality  ( [ no_information ] ) is stronger than inequality  ( [ second2 ] ) in this case ; @xmath86 can not be negative due to inequality  ( [ no_information ] ) , even when inequality  ( [ second2 ] ) gives a negative lower bound .",
    "therefore , in the absence of information exchange , it is consistent to regard @xmath50 as the whole `` universe '' even when there are initial and final correlations with @xmath17 ; we can ignore what s happening outside @xmath50 if there is no interaction between inside and outside of the universe .",
    "we apply the foregoing general framework to feedback control , where @xmath16 is the system to be controlled and @xmath17 is the memory that initially has the information about the initial condition of the system and controls it depending on that information ( see also fig .  2 ( a ) ) .",
    "the mutual information that is initially shared between the system and the memory is given by @xmath87 , and the final remaining correlation is given by @xmath88 .",
    "the decomposition ( [ main1 ] ) of the total entropy production is then given by @xmath89 which , together with inequality  ( [ second1 ] ) , leads to @xmath90 inequality  ( [ feedback1 ] ) implies that the entropy in @xmath50 can be decreased by the amount up to @xmath91 that characterizes the upper bound of the utilized information during feedback control .",
    "is the system to be controlled and @xmath17 is the memory .",
    "( b ) dynamics of measurement , where @xmath16 is the memory and @xmath17 is the measured system .",
    "these schematics illustrate the dual relationship between measurement and feedback control ; they have a one - to - one correspondence under time - reversal and exchange of the roles of the system and the memory.,width=453 ]    we next consider the energetics of feedback control .",
    "let @xmath92 $ ] and @xmath93 $ ] be the initial and final hamiltonians of system @xmath16 . here",
    ", we assume that the initial hamiltonian is independent of @xmath5 , and that the final one can depend on @xmath5 through feedback control .",
    "the intermediate hamiltonians during the feedback process can also depend on @xmath5 . for simplicity",
    ", we neglect the interaction hamiltonian between @xmath16 and @xmath17 in the initial and final states .",
    "the energy change in this process is given by @xmath94 - e^i_{x } [ x].\\ ] ] the first law of thermodynamics is given by @xmath95 where @xmath96 is the work performed on @xmath16 through the time dependence of external parameters .",
    "we now assume that there is a single heat bath at inverse temperature @xmath97 .",
    "inequality  ( [ feedback1 ] ) then reduces to @xmath98 where @xmath99 is the change in the effective ( nonequilibrium ) free energy defined by @xmath100 we next define the initial and final equilibrium free energies as follows : @xmath101 } , \\",
    "f^f_{x , y } : = -\\beta^{-1 } \\ln \\int dx ' e^{-\\beta e^f_{x , y } [ x ' ] } .\\ ] ] we further assume that the initial distribution of @xmath16 is the thermal equilibrium : @xmath102 = e^{\\beta ( f^i_{x } - e_{x}^i [ x])}.\\ ] ] we then obtain @xmath103 on the other hand , the final distribution can be different from the canonical distribution in general .",
    "let @xmath104 : = - \\ln p_f^f [ x ' | y ] $ ] be the conditional shannon entropy of the final distribution .",
    "we then have an inequality : @xmath105 where the equality is achieved if and only if @xmath106 $ ] is the conditional canonical distribution for a given @xmath5 : @xmath107 = e^{\\beta ( f^f_{x , y } - e_{x , y}^f [ x'])}.\\ ] ] we note that @xmath108 = - \\ln p_f^f [ x'|y ] + \\ln ( p_f^f [ x'|y ] / p_f^f[x ' ] ) $ ] , and therefore @xmath109 we finally obtain @xmath110 where @xmath111 f^f_{x , y } - f_x^i\\ ] ] is the average change in the conditional free energy .",
    "inequality  ( [ feedback2 ] ) sets the fundamental lower bound of the energy cost for feedback control , which is smaller by the amount of @xmath112 than the usual thermodynamic bound .",
    "we note that the same bound as  ( [ feedback2 ] ) has been obtained in refs .",
    "@xcite for a different setup .",
    "we next apply our general framework to measurement processes , where @xmath16 is the memory and @xmath17 is the measured system ( see also fig .  2 ( b ) ) .",
    "in other words , @xmath16 performs a measurement on @xmath17 in this setup .",
    "we first assume that the initial correlation is zero ( i.e. , @xmath113 ) before the measurement , and the final correlation is characterized by the information ( @xmath114 ) obtained by the measurement .",
    "the total entropy production is given by @xmath115 which , together with inequality  ( [ second1 ] ) , leads to @xmath116 inequality  ( [ meas1 ] ) implies that the entropy in @xmath50 inevitably increases due to the obtained information by the measurement .",
    "if the memory has prior knowledge about the system before the measurement , there is the corresponding initial correlation @xmath117 .",
    "we then obtain @xmath118 which , together with inequality  ( [ second1 ] ) , leads to @xmath119 inequality  ( [ meas2 ] ) implies that the entropy increase in @xmath50 is bounded from below by the obtained information @xmath120 .    to discuss the energetics of the memory",
    ", we need to examine the more detailed structure of the memory , which will be discussed in sec .  4 .",
    "we now discuss a fundamental relationship between measurement and feedback control .",
    "let us consider the time - reversal transformation of the dynamics and exchange the roles of the system and the memory at the same time ( see also fig .  2 ) .",
    "we then find that the measurement becomes feedback and vice versa , where @xmath121 in measurement corresponds to @xmath121 in feedback , and @xmath122 in measurement corresponds to @xmath123 in feedback .",
    "this implies a kind of dual structure between the measurement and feedback , as summarized in table 1 .",
    ".duality between measurement and feedback . [ cols=\"<,<,<\",options=\"header \" , ]     we consider a special case of @xmath124 . in this case , the lower bound of @xmath86 is given by @xmath125 for measurement and by @xmath126 for feedback , where the opposite signs are due to the fact that the final correlation in measurement corresponds to the initial correlation in feedback because of the time - reversal transformation .",
    "this explains the reason why the entropy in @xmath50 is increased by measurement but decreased by feedback control .",
    "we generally consider the case of multiple information exchanges in sec .",
    "3.1 , and then focus on the case of maxwell s demon in sec .  3.2 .",
    "we consider multiple information exchanges between two systems @xmath16 and @xmath17 , which are attached to different heat baths with each other . for simplicity",
    ", we use notation @xmath20 to indicate all baths .",
    "if the correlation time in the baths is sufficiently small compared with the time scale of the systems , we may apply this assumption to the situation in which the systems are attached to the same baths .",
    "we consider a composite process consisting of the following two processes ( see also fig .  3 ( a ) )",
    ".     evolves under the influence of the initial phase - space point of @xmath16 , denoted by @xmath0 . in the second process ( ii )",
    ", @xmath16 evolves under the influence of the final phase - space point of @xmath17 , denoted by @xmath127 .",
    "( b ) typical situation of maxwell s demon .",
    "@xmath16 is the system to be controlled and @xmath17 is the memory of the demon , where the first process describes the measurement with outcome @xmath127 and the second process describes the feedback control.,width=453 ]    in the first process ( i ) , @xmath17 evolves under the influence of the initial phase - space point of @xmath16 , denoted as @xmath0 .",
    "let @xmath128 $ ] be the initial distribution of the first process .",
    "system @xmath17 evolves along trajectory @xmath129 with probability @xmath130 $ ] under the initial condition of @xmath34 .",
    "the final distribution of @xmath17 is given by @xmath131 $ ] , where @xmath127 is the final phase - space point of @xmath17 .",
    "let @xmath132 and @xmath133 respectively be the entropy productions in @xmath134 and @xmath135 in this process .",
    "the change in the mutual information is given by @xmath136}{p_f^1 [ x ] p_f^1[y ' ] }   - \\ln \\frac{p_f^0[x , y]}{p_f^0 [ x ] p_f^0[y]}.\\ ] ]    in the second process ( ii ) , @xmath16 evolves under the influence of the final phase - space point of @xmath17 , denoted as @xmath127 ( see fig .",
    "3 ( a ) ) .",
    "let @xmath131 $ ] be the initial distribution of the second process .",
    "system @xmath16 evolves along trajectory @xmath22 with probability @xmath137",
    "$ ] under the condition of @xmath138 . the final distribution of @xmath16 is given by @xmath139 $ ] , where @xmath21 is the final phase - space point of @xmath16 .",
    "let @xmath140 and @xmath141 be the entropy productions in @xmath134 and @xmath50 in this process .",
    "the change in the mutual information is given by @xmath142}{p_f^2 [ x ' ] p_f^2[y ' ] }   - \\ln \\frac{p_f^1[x , y']}{p_f^1 [ x ] p_f^1[y']}.\\ ] ]    the total entropy production in the composite process , denoted by @xmath143 , is given by the sum of the entropy productions of the two processes : @xmath144 the change in the mutual information in the total process is given by @xmath145}{p_f^2 [ x ' ] p_f^2[y ' ] } - \\ln \\frac{p_f^0[x , y]}{p_f^0 [ x ] p_f^0[y]},\\ ] ] which can also be expressed as the sum of the changes in the two processes : @xmath146    in terms of dft , the entropy productions are given by @xmath147p_f^0[x , y]}{p_b [ y_f^\\dagger | x^\\ast , y'^\\ast]p_f^1 [ x , y ' ] } , \\ \\delta s_{yb}^{\\rm ( i ) } = \\ln \\frac{p_f [ y_f |   x , y ] p_f^0 [ y]}{p_b [ y_f^\\dagger | x^\\ast , y'^\\ast ] p_f^1 [ y']},\\ ] ] @xmath148p_f^1[x , y']}{p_b [ x_f^\\dagger | x'^\\ast , y'^\\ast]p_f^2 [ x ' , y ' ] } , \\ \\delta",
    "s_{xb}^{\\rm ( ii ) } = \\ln \\frac{p_f [ x_f |   x^\\ast , y'^\\ast ] p_f^1 [ x]}{p_b [ x_f^\\dagger | x'^\\ast , y'^\\ast   ] p_f^2 [ x ' ] } , \\ ] ] and @xmath149p_f [ y_f |   x , y ] p_f^0[x , y]}{p_b [ y_f^\\dagger | x^\\ast , y'^\\ast]p_b [ x_f^\\dagger | x'^\\ast , y'^\\ast]p_f^2 [ x ' , y']}.\\ ] ] here , we have assumed that the initial distributions of the two backward processes are given by @xmath150 $ ] and @xmath151 $ ] .",
    "we note that the initial distribution of the backward process of ( i ) is not necessarily equal to the final distribution of the backward process of ( ii ) .",
    "in other words , the first backward process is not necessarily followed by the second backward process ; one can not start the backward process of ( i ) immediately after the backward process of ( ii ) , but one should change the probability distribution to start the backward process of ( ii ) .",
    "on the other hand , the initial distribution of the forward process ( i ) is equal to the final distribution of the forward process ( ii ) .",
    "therefore , the forward process ( i ) is actually followed by the forward process ( ii ) , and one can start the forward process ( ii ) immediately after the forward process ( i ) .    since the total entropy production is nonnegative , we obtain @xmath152 and therefore @xmath153 inequality ( [ main_c ] ) implies that the sum of the entropy increases is bounded by the total change in the mutual information .",
    "we note that the foregoing argument can straightforwardly be generalized to the case of information exchanges which take place more than once .",
    "we next consider the composite process of measurement and feedback , which is a typical situation of maxwell s demon ( see also fig .  3 ( b ) ) .",
    "in this case , @xmath16 is the system to be controlled and @xmath17 is the memory of the demon .",
    "we assume that there is no initial correlation : @xmath154 .",
    "after the measurement , the memory obtains the mutual information @xmath155 and then uses it for feedback control . the remaining correlation after feedback control",
    "is given by @xmath156 . by applying eq .",
    "( [ composite1 ] ) to this case , the total entropy production of the composite process is given by @xmath157 therefore , we obtain @xmath158 since @xmath159 is non - negative , we obtain @xmath160 this inequality implies that the entropy decrease in @xmath50 by feedback control is compensated for by the entropy increase in @xmath135 by measurement .",
    "we note that , the total entropy productions @xmath161 and @xmath162 are both nonnegative during measurement and feedback , which confirms that the role of the demon does not contradict sl .",
    "the crucial observation here is that the mutual information @xmath163 which is stored during the measurement is used as a resource of the entropy decrease during the feedback process .",
    "we next discuss the detailed structure of the memory , and its roles in measurement and feedback control .",
    "we consider a situation in which the phase space of the memory , which we refer to as @xmath164 , is divided into several subspaces ( see also fig .  4 ) .",
    "each subspace is written as @xmath165 labeled by @xmath166 ( @xmath167 ) , where @xmath168 may be regarded as the set of measurement outcomes .",
    "we assume that @xmath165 s do not overlap with each other , and @xmath169 . for any @xmath170",
    ", there is a single @xmath166 such that @xmath171 , which we write as @xmath172 .    .",
    "( a ) symmetric memory with @xmath173 . ( b ) asymmetric memory @xmath174.,width=377 ]    we consider probability distribution @xmath7 $ ] over @xmath164 .",
    "let @xmath175 $ ] be the probability of @xmath171 , and @xmath176 $ ] be the conditional probability of @xmath5 under the condition of @xmath171 .",
    "we note that @xmath177 = 0 $ ] if @xmath178 , because @xmath165 s do not overlap with each other .",
    "the joint probability distribution is given by @xmath179 = p[y|m]p[m ] \\delta ( m , m_y),\\ ] ] where @xmath180 is the kronecker delta .",
    "the unconditional probability distribution is then given by @xmath181 = \\sum_m p[y , m ]   =   p[y|m_y]p[m_y].\\ ] ] we define the stochastic shannon entropies as @xmath182 & : = & -\\ln p[y ] , \\\\",
    "s_{y , m } [ y ] & : = & -\\ln p[y|m ] , \\\\",
    "h_m[m ] & : = & - \\ln p[m],\\end{aligned}\\ ] ] which satisfy @xmath183 =   h_m[m_y]+ s_{y , m_y } [ y].\\ ] ] therefore , we obtain @xmath184 where @xmath185 \\ln p [ y],\\ ] ] @xmath186 \\langle s_{y , m } \\rangle = - \\sum_{m } \\int_{\\mathcal y_{m } } dy p[y|m ] p[m ] \\ln p[y|m],\\ ] ] @xmath187 \\ln p[m].\\ ] ] equality  ( [ prob_formula ] ) implies that the total shannon entropy is decomposed into the shannon entropy over @xmath166 and the average shannon entropy of the phase - space points in @xmath165 , where the former characterizes the randomness of the measurement outcomes , while the latter characterizes the average of the fluctuations within individual subspaces .",
    "we now consider measurement processes with the memory structure in the presence of heat baths @xmath20 .",
    "let us choose a subspace @xmath188 which may be one of @xmath165 s , but not necessarily be so .",
    "in fact , @xmath188 may be equal to the whole phase space @xmath164 .",
    "we assume that the initial phase - space point @xmath5 is in @xmath188 with unit probability ; in this case , we say that the memory is in the standard state .",
    "let @xmath189 $ ] be the initial distribution of @xmath5 ; by assumption , @xmath190=0 $ ] if @xmath5 does not belong to @xmath188 .",
    "we also assume that there is no initial correlation between @xmath16 and @xmath17 .",
    "the memory then evolves along trajectory @xmath129 under the influence of @xmath16 with phase - space point @xmath0 , and stores outcome @xmath166 with probability @xmath191 $ ] .",
    "this measurement establishes the correlation between @xmath0 and @xmath166 . after the measurement ,",
    "the final phase - space point is @xmath127 .",
    "we note that the probability that @xmath127 is in subspace @xmath165 is given by @xmath191 $ ] .",
    "let @xmath192 $ ] be the final probability distribution of @xmath127 under the condition of @xmath166 .    the total entropy production during the measurement",
    "is then given by @xmath193 where @xmath194 } { p_b [ y_f^\\dagger , x^\\ast]},\\ ] ] @xmath195,\\ ] ] @xmath196 ) - ( - \\ln p_f^{i } [ y ] ) , \\ ] ] @xmath197}{p_f[x]p_f[y']}.\\ ] ] in the following , we write @xmath198 $ ] and @xmath199 $ ] .",
    "we next assume that there is a single heat bath at inverse temperature @xmath97 .",
    "let @xmath200 $ ] be the initial hamiltonian defined on subspace @xmath188 .",
    "we assume that the initial distribution is given by the canonical distribution in @xmath188 : @xmath201 = e^{\\beta ( f_{y,0}^i - e_{y , 0}^i [ y])},\\ ] ] where the conditional free energy is given by @xmath202}.\\ ] ] in this case , @xmath203 let @xmath204 $ ] be the final hamiltonian defined only on @xmath165 .",
    "we define the conditional free energy as @xmath205}.\\ ] ] we refer to the memory as symmetric if @xmath206 takes on the same value for all @xmath166 ( see also fig .  4 ) .",
    "we then have @xmath207 where the equality is achieved if and only if @xmath208 = e^{\\beta ( f_{y , m}^{f } - e_{y , m}^{f } [ y'])},\\ ] ] which vanishes outside of @xmath165 .",
    "we then have @xmath209 where @xmath210 - e_{y,0}^{i } [ y],\\ ] ] @xmath211 f_{y , m}^f - f_{y,0}^i.\\ ] ] therefore , we have @xmath212 where @xmath213 is the work performed on the memory during the measurement . since @xmath214 , we finally obtain @xmath215 which determines the minimal energy cost for measurement .",
    "the lower bound is characterized by the average free - energy difference , the shannon information of measurement outcomes , and the mutual information between @xmath16 and @xmath17 . on the rhs of inequality  ( [ meas_e1 ] )",
    ", @xmath216 arises from the increase in the shannon entropy of the memory by the measurement , and @xmath217 arises from the increase of the mutual information between the system and the memory by the measurement .",
    "the reason why the signs of @xmath216 and @xmath217 are different from each other is that the shannon information and the mutual information contribute to the total entropy with opposite signs as shown in eq .",
    "( [ mutual_shannon1 ] ) .",
    "we note that the actually utilizable information obtained by the memory is characterized by the mutual information between @xmath16 and outcome @xmath218 : @xmath219}{p_f[x]p_f[m]},\\ ] ] where @xmath220 $ ] is the joint probability distribution of @xmath0 and @xmath166 after the measurement .",
    "we then have @xmath221p_f[m]}{p_f^f[x , m]p_f^f[y ] } =   \\ln \\frac{p_f^f[x|y]}{p_f^f[x|m ] } \\\\ & = \\ln \\frac{p_f^f[x|y , m]}{p_f^f[x|m ] } = : \\tilde i_{xy } , } \\label{conditional}\\ ] ] where @xmath222 $ ] and @xmath223 $ ] are the conditional probabilities of @xmath0 under the condition of @xmath5 and @xmath166 , respectively .",
    "the ensemble average @xmath224 is the conditional mutual information between @xmath16 and @xmath17 under the condition of @xmath166 , which is by construction nonnegative [ see eq .",
    "( [ conditional ] ) ] : @xmath225 therefore , we obtain an inequality which is weaker than ( [ meas_e1 ] ) : @xmath226 inequality  ( [ measurement_cost2 ] ) is physically more transparent than inequality  ( [ meas_e1 ] ) , because the lower bound in ( [ measurement_cost2 ] ) is characterized by the physically utilizable information @xmath227 rather than the total correlation @xmath228 .",
    "we note that the same bound as ( [ measurement_cost2 ] ) has been derived in ref .",
    "@xcite for a different setup .",
    "( @xmath229 ) , the box is compressed from the right ( left ) quasi - statically and isothermally with the particle confined in the left ( right ) box corresponding to @xmath230 ( @xmath231 ) . in the final state ,",
    "@xmath0 and @xmath166 are perfectly correlated .",
    "( b ) measurement with error rate @xmath232 .",
    "the standard state is the same as in ( a ) .",
    "if the measured state is @xmath233 ( @xmath229 ) , a barrier is inserted and the box is divided into two compartments with volume ratio @xmath234 ( @xmath235 ) .",
    "the barrier is moved to the center of the box .",
    "the particle is finally in the left ( right ) box corresponding to @xmath230 ( @xmath231 ) , where @xmath0 and @xmath166 are not perfectly correlated if @xmath236 . if @xmath237 , this model is equivalent to the error - free model of ( a ) .",
    ", width=377 ]    as an illustration , we consider a simple model of measurement .",
    "figure 5 ( a ) shows a model of error - free measurement . the memory is a single particle in a box with a single heat bath at inverse temperature @xmath238 , where @xmath188 is the whole phase space .",
    "we assume that the measured state is @xmath233 or @xmath239 with equal probability @xmath240 .",
    "after the quasi - static and isothermal measurement described in fig .  5 ( a ) , the particle is in the left box or the right box corresponding to @xmath230 or @xmath239 , where @xmath241 and @xmath242 correspond to the left and right box , respectively .",
    "we note that @xmath243 in this model . in this case ,",
    "@xmath244 , @xmath245 , @xmath246 , and @xmath247 .",
    "therefore , the equality in inequality ( [ measurement_cost2 ] ) is achieved in this model .",
    "figure 5 ( b ) shows a model of measurement with error rate @xmath232 ( @xmath248 ) , where @xmath188 , @xmath241 , and @xmath242 are the same as in the previous example .",
    "we assume that the measured state is @xmath233 or @xmath239 with the equal probability of @xmath240 .",
    "in this case , @xmath244 , @xmath246 , and @xmath249,\\ ] ] @xmath250 therefore , the equality in ( [ measurement_cost2 ] ) is again achieved in this model .",
    "we now briefly discuss the information erasure from the memory . during the erasure , memory @xmath17",
    "is detached from the measured system @xmath16 , and @xmath17 returns to the standard state ; after the erasure , the phase - space point of @xmath17 is in @xmath188 with unit probability .",
    "the shannon entropy in @xmath218 after the erasure is @xmath251 by definition ; it changes by @xmath252 during the erasure , whose sign is opposite to that in the measurement .",
    "since @xmath17 is detached from @xmath16 during the erasure , dft and sl can apply to @xmath135 ( see also arguments in the last paragraph of sec .",
    "therefore , the entropy change in @xmath135 during the erasure satisfies @xmath253 where the equality can be achieved in the quasi - static erasure .",
    "we assume that there is a single heat bath at inverse temperature @xmath97 , and that the probability distribution of @xmath17 in each @xmath165 before the erasure is the canonical distribution under the condition of @xmath166 . by applying a similar argument used in deriving  ( [ meas_e1 ] ) to @xmath254",
    ", we obtain the lower bound of the work performed on the memory during the erasure : @xmath255 which is the generalized landauer principle  @xcite .",
    "we note that the free - energy change @xmath256 during the erasure satisfies @xmath257 . in the special case of @xmath258 , inequality  ( [ eras ] )",
    "reduces to the conventional landauer principle  @xcite , which is satisfied in the case of a symmetric memory as shown in fig .",
    "4 ( a ) .    by summing up inequalities ( [ meas_e1 ] ) and ( [ eras ] ) , the total work for measurement and erasure is given by @xmath259 where the lower bound is only determined by the mutual information ; @xmath260 and @xmath216 on the rhs of inequality  ( [ meas_e1 ] ) are canceled by the corresponding terms in inequality  ( [ eras ] ) .",
    "in fact , the measurement and erasure are time - reversal with each other if we only focus on @xmath135 and ignore the interaction with @xmath16 .",
    "however , they are not completely time - reversal if we take into consideration their interaction ; @xmath17 interacts with @xmath16 and establishes the correlation only in the measurement process . therefore , the mutual information obtained by the measurement process plays an essential role in determining the work for the entire process of measurement and erasure .",
    "we note that the assumption of the conditional canonical distribution before the erasure is not necessary to derive only inequality  ( [ trade_off ] ) ; we only need to assume that the probability distribution before the erasure is the same as that after the measurement .",
    "in fact , by summing up the entropy changes in measurement and erasure , we obtain @xmath261 . by applying a similar argument used in deriving  ( [ meas_e1 ] ) to the entire entropy change @xmath262 in measurement and erasure",
    ", we again obtain inequality  ( [ trade_off ] ) .",
    "we next consider feedback control on @xmath16 by @xmath17 after the measurement .",
    "more precisely , we assume that the dynamics of @xmath16 is determined only by the outcome @xmath166 .",
    "therefore , we can consider a composite system @xmath263 instead of @xmath27 .",
    "we assume that system @xmath16 is attached to heat baths that are different from those in contact with the memory .",
    "we denote the baths attached to @xmath16 again by @xmath20 .",
    "the probability distribution of the forward trajectory of @xmath16 and @xmath166 is given by @xmath264 = p_f [ x_f | x , m]p_f^f [ x , m],\\ ] ] where @xmath265 $ ] is the pre - feedback ( post - measurement ) distribution of @xmath266 , and @xmath267 $ ] is the conditional probability of @xmath22 under the initial condition @xmath266 of the feedback process .",
    "the argument is then completely parallel to that in sec .",
    "2.3 if we replace @xmath17 with @xmath218 .",
    "the total entropy production in @xmath268 is given by @xmath269 where @xmath270 describes the remaining correlation after the feedback control .",
    "sl is then expressed as @xmath271    if there is a single heat bath at inverse temperature @xmath97 and the initial state of system @xmath16 is in the canonical distribution , we obtain @xmath272 on the other hand , by considering @xmath134 , we can also obtain @xmath273 we note that inequality  ( [ feedback_energy2 ] ) is stronger than inequality  ( [ feedback_energy3 ] ) in the present setup .",
    "we have established the general relationship between the total entropy production of the whole system and the mutual information that is exchanged between two stochastic systems .    in sec .  2 , we have derived the general decomposition formula  ( [ main1 ] ) for a single information exchange .",
    "correspondingly , we have obtained the kpb equality  ( [ kpb1 ] ) , sl  ( [ second2 ] ) , and ift  ( [ ift1 ] ) , such that they explicitly include the mutual information .",
    "we have applied the general formula to the cases of feedback control ( [ feedback0 ] ) and measurement  ( [ meas2 ] ) . in sec .  3 , we have discussed the case of multiple information exchanges , and obtained a general decomposition formula  ( [ composite1 ] ) and the corresponding sl ( [ main_c ] ) . in sec .  4 , we have considered the structure of the memory ; its phase space is divided into several subspaces corresponding to the measurement outcomes .",
    "this formulation has clarified the role of the shannon information of measurement outcomes as well as the mutual information , as shown for the cases of measurement  ( [ measurement_cost2 ] ) and feedback control ( [ feedback_energy2 ] ) .",
    "our theory has clarified the role of mutual information in nonequilibrium thermodynamics with information processing , which is not restricted to the conventional case of maxwell s demon . as a consequence , we have revealed the fundamental relationship between the entropy production in the whole universe ( system and bath ) and the exchanged information inside the universe .",
    "our results would serve as the theoretical foundation of nonequilibrium thermodynamics of complex systems in the presence of information processing .",
    "we consider the entropy change in @xmath20 in the setup in sec .",
    "2 . following the standard approach in nonequilibrium statistical mechanics",
    ", we assume that the total system including the baths obeys the liouville dynamics that conserves the phase - space volume  @xcite .",
    "we also assume that there is no initial correlation between the system and the baths , and that the initial distribution of each bath is given by the canonical distribution  @xcite .",
    "let @xmath274 be the initial phase - space point of the @xmath45th bath with @xmath275 , @xmath276 $ ] be the hamiltonian of the @xmath45th bath , and @xmath277 be its free energy .",
    "the initial distribution is given by @xmath278 = \\prod_k e^{\\beta_k ( f_{b , k } - e_{b , k}[z_k ] ) } = : p_{\\rm can}[z].\\ ] ] let @xmath279 be the final phase - space point of the @xmath45th bath with @xmath280 , and @xmath281 $ ] be the final probability distribution that is in general different from the canonical distribution .",
    "the heat absorbed by the system from the @xmath45th bath is given by @xmath282 - e_{b , k}[z_k'].\\ ] ] on the basis of the above definitions along with eqs .",
    "( [ entropy_total ] ) and ( [ entropy_bath ] ) , dft  ( [ dft0 ] ) has been shown to hold  @xcite even in the presence of the final correlation between the system and the baths .",
    "we note that @xmath283 is in general different from the change in the shannon entropy of the total system , while @xmath283 is related to the relative entropy as follows  @xcite : @xmath284 \\ln \\frac{p_f^f[x',y , z']}{p_f^f[x',y]p_{\\rm can}[z']},\\ ] ] where @xmath285 $ ] and @xmath286 $ ] are respectively the final probability distribution of @xmath287 and @xmath288 in the forward process",
    ". therefore , there are two origins of the positive entropy production in the whole universe : the final correlation between the system and the baths  @xcite , and the lag between the canonical distribution and the final probability distribution of the baths  @xcite .",
    "we note that the role of the initial correlation between the system and the baths has been discussed in refs .",
    "@xcite .",
    "we are grateful to hal tasaki for valuable discussions .",
    "this work was supported by jsps kakenhi grant nos .",
    "25800217 and 22340114 , and by platform for dynamic approaches to living system from mext , japan .",
    "100 nielsen m a , caves c m , schumacher b and barnum h 1998 _ proc .",
    "london a _ * 454 * 277 touchette h and lloyd s 2000 _ phys .",
    "lett . _ * 84 * 1156 touchette h and lloyd s 2004 _ physica a _ * 331 * 140 cao f j , dinis l , parrondo j m r 2004 _ phys .",
    "lett . _ * 93 * 040603 kim k h and qian h 2007 _ phys .",
    "e _ * 75 * 022102 sagawa t and ueda m 2008 _ phys .",
    "lett . _ * 100 * 080403 cao f j and feito m 2009 _ phys .",
    "e _ * 79 * 041118 cao f j , feito m and touchette h 2009 _ physica a _ * 388 * 113 jacobs k 2009 _ phys .",
    "a _ * 80 * 012322    sagawa t and ueda m 2010 _ phys .",
    "lett . _ * 104 * 090602 fujitani y and suzuki h 2010 _ j. phys .",
    "jpn . _ * 79 * 104003 horowitz j m and vaikuntanathan s 2010 _ phys .",
    "e _ * 82 * 061120 morikuni y and tasaki h 2011 _ j. stat .",
    "* 143 * 1 sagawa t 2011 _ j. phys .",
    "ser . _ * 297 * 012015    jennings d and rudolph t 2010 _ phys .",
    "e _ * 81 * 061130 kim s w , sagawa t , liberato s d and ueda m , _ phys .",
    "_ 2011 * 106 * 070401 ito s and sano m 2011 _ phys .",
    "e _ * 84 * 021123 horowitz j m and parrondo j m r 2011 _ europhys lett .",
    "_ * 95 * 10005 abreu d and seifert u 2011 _ europhys lett . _ * 94 * 10001 vaikuntanathan s and jarzynski c 2011 _ phys .",
    "e _ * 83 * 061120 averin d v , mttnen m and pekola j p 2011 _ phys .",
    "b _ * 84 * 245448 horowitz j m and parrondo j m r 2011 _ new j. phys . _ * 13 * 123019 bauer m , abreu d and seifert u 2012 _ j. phys . a : math .",
    "* 45 * 162001 sagawa t and ueda m 2012 _ phys .",
    "e _ * 85 * 021104 munakata t and rosinberg",
    "m l 2012 _ j. stat .",
    "_ p05010 esposito m and schaller g 2012 _ europhys .",
    "lett . _ * 99 * 30003        piechocinska b 2000 _ phys .",
    "rev . a _ * 61 * 062314 allahverdyan a e and nieuwenhuizen th m 2001 _ phys .",
    "e _ * 64 * 0561171 horhammer c and buttner h 2008 _ j. stat .",
    "phys . _ * 133 * 1161 barkeshli m m 2005 arxiv : cond - mat/0504323 norton j d 2005 _ stud .",
    "phys . _ * 36 * 375 maroney o j e 2009 _ phys .",
    "e _ * 79 * 031105 turgut s 2009 _ phys .",
    "e _ * 79 * 041102 sagawa t and ueda m 2009 _ phys .",
    "lett . _ * 102 * 250602 , 2011 _ ibid . _ * 106 * 189901(e ) dillenschneider r and lutz e 2009 _ phys",
    "lett . _ * 102 * 210601 esposito m and van den broeck c 2011 _ europhys .",
    "lett . _ * 95 * 40004 rio l. del , aberg j , renner r , dahlsten o and vedral v 2011 _ nature _ * 474 * 61 lambson b , carlton d and bokor j 2011 _ phys . rev . lett . _ * 107 * 010604        mandal d and jarzynski c 2012 _ proc .",
    "_ * 109 * 11641 barato a c and seifert u 2013 _ europhys .",
    "_ * 101 * 60001 strasberg p , schaller g , brandes t and esposito m 2013 _ phys .",
    "lett . _ * 110 * 040601 horowitz j m , sagawa t and parrondo j m r 2013 _ phys .",
    "lett . _ * 111 * 010602 mandal d , quan h t and jarzynski c 2013 _ phys .",
    "* 111 * 030602 barato a c and seifert u 2013 arxiv:1308.4598    leff h s and rex a f ( eds ) 2003 _ maxwell s demon 2 : entropy , classical and quantum information , computing _ ( princeton university press , new jersey ) maxwell j c 1871 _ theory of heat _ ( appleton , london ) szilard l 1929 _ z. phys . _ * 53 * 840 brillouin l 1951 _ j. appl .",
    "* 22 * 334 landauer r 1961 _ ibm j. res .",
    "dev . _ * 5 * 183 bennett c h 1982 _ int .",
    "* 21 * 905    lopez b j , kuwada n j , craig e m , long b r and linke h 2008 _ phys .",
    "lett . _ * 101 * 220601 toyabe s , sagawa t , ueda m , muneyuki e and sano m 2010 _ nature physics _ * 6 * 988 brut a , arakelyan a , petrosyan a , ciliberto s , dillenschneider r and lutz e 2012 _ nature _ * 483 * 187    evans d j , cohen e g d and morris g p 1993 _ phys . rev",
    ". lett . _ * 71 * 2401 jarzynski c 1997 _ phys .",
    "lett . _ * 78 * 2690 crooks g e 1998 _ j. stat .",
    "_ * 90 * 1481 crooks g e 1999 _ phys .",
    "e _ * 60 * 2721 jarzynski c 2000 _ j. stat",
    ". phys . _ * 98 * 77 kurchan j 2000 arxiv : cond - mat/0007360 tasaki h 2000 arxiv : cond - mat/0009244 jarzynski c 2004 _ j. stat .",
    "mech : theor .",
    "_ p09005 seifert u 2005 _ phys .",
    "lett . _ * 95 * 040602 kawai r , parrondo j m r and van den broeck c 2007 _ phys . rev .",
    "lett . _ * 98 * 080602 vaikuntanathan s and jarzynski c 2009 _ europhys lett . _ * 87 * 60005 campisi m , talkner p and p. hnggi 2009 _ phys .",
    "_ * 102 * 210401 sagawa t 2012 arxiv:1202.0983"
  ],
  "abstract_text": [
    "<S> we relate the information exchange between two stochastic systems to the nonequilibrium entropy production in the whole system . by deriving a general formula that decomposes the total entropy production into the thermodynamic and informational parts , we obtain nonequilibrium equalities such as the fluctuation theorem in the presence of information processing . </S>",
    "<S> our results apply not only to situations under measurement and feedback control , but also to those under multiple information exchanges between two systems , giving the fundamental energy cost for information processing and elucidating the thermodynamic and informational roles of a memory in information processing . </S>",
    "<S> we clarify a dual relationship between measurement and feedback . </S>"
  ]
}