{
  "article_text": [
    "we consider the problem of encoding and decoding binary codes of constant hamming weight @xmath0 and block length @xmath1 .",
    "such codes are useful in a variety of applications : a few examples are fault - tolerant circuit design and computing  @xcite , pattern generation for circuit testing  @xcite , identification coding  @xcite and optical overlay networks  @xcite .",
    "the problem of interest is that of designing the encoder and decoder , i.e. , the problem of mapping all binary ( information ) vectors of a given length onto a subset of length-@xmath1 vectors of constant hamming weight @xmath0 in a one - to - one manner . in this work ,",
    "we propose a novel geometric method in which information and code vectors are represented by vectors in @xmath0-dimensional euclidean space , covering polytopes for the two sets are identified , and a one - to - one mapping is established by dissecting the covering polytopes in a specific manner .",
    "this approach results in an invertible integer - to - integer mapping , thereby ensuring unique decodability .",
    "the proposed algorithm has a natural recursive structure , and an inductive proof is given for unique decodability .",
    "the issue of efficient encoding and decoding is also addressed .",
    "we show that the proposed algorithm has complexity @xmath2 , where @xmath0 is the weight of the codeword , independent of the codeword length .",
    "dissections are of considerable interest in geometry , partly as a source of puzzles , but more importantly because they are intrinsic to the notion of volume . of the @xmath3 problems posed by david hilbert at the international congress of mathematicians in 1900 , the third problem dealt with dissections .",
    "hilbert asked for a proof that there are two tetrahedra of the same volume with the property that it is impossible to dissect one into a finite number of pieces that can be rearranged to give the other , i.e. , that the two tetrahedra are not equidecomposable .",
    "the problem was immediately solved by dehn  @xcite . in 1965 , after @xmath4 years of effort , sydler  @xcite completed dehn s work .",
    "the dehn - sydler theorem states that a necessary and sufficient condition for two polyhedra to be equidecomposable is that they have the same volume and the same dehn invariant .",
    "this invariant is a certain function of the edge - lengths and dihedral angles of the polyhedron .",
    "an analogous theorem holds in four dimensions ( jessen  @xcite ) , but in higher dimensions it is known only that equality of the dehn invariants is a necessary condition . in two dimensions",
    "any two polygons of equal area are equidecomposable , a result due to bolyai and gerwein ( see boltianskii  @xcite ) . among other books dealing with the classical dissection problem in two and three dimensions we mention in particular frederickson  @xcite , lindgren  @xcite and sah  @xcite .",
    "the remainder of the paper is organized as follows .",
    "we provide background and review relevant previous work in section  [ sec : review ] .",
    "section [ sec : geo ] describes our geometric approach and gives some low - dimensional examples .",
    "encoding and decoding algorithms are then given in section  [ sec : alg ] , and the correctness of the algorithms is established .",
    "section [ sec : con ] summarizes the paper .",
    "let us denote the hamming weight of a length-@xmath1 binary sequence @xmath5 by @xmath6 , where @xmath7 is the cardinality of a set .",
    "an @xmath8 constant weight binary code @xmath9 is a set of length-@xmath1 sequences such that any sequence @xmath10 has weight @xmath11 .    if @xmath12 is an @xmath8 constant weight code , then its rate @xmath13 . for fixed @xmath14 and @xmath15",
    ", we have @xmath16 where @xmath17 is the entropy function .",
    "thus @xmath18 is maximized when @xmath19 , i.e. , the asymptotic rate is highest when the code is balanced .",
    "the ( asymptotic ) efficiency of a code relative to an infinite - length code with the same weight to length ratio @xmath20 , given by @xmath21 , can be written as @xmath22 where @xmath23 and @xmath24 .",
    "the first term , @xmath25 , is the efficiency of a particular code relative to the best possible code with the same length and weight ; the second term , @xmath26 , is the efficiency of the best finite - length code relative to the best infinite - length code .    from stirling s formula",
    "we have @xmath27 a plot of @xmath26 as a function of @xmath1 is given in fig .",
    "[ fig : efficiency ] for @xmath19 .",
    "the slow convergence visible here is the reason one needs codes with large block lengths .     as a function of block length",
    "when @xmath19,width=283 ]    [ fig1 ]    comprehensive tables and construction techniques for binary constant weight codes can be found in  @xcite and the references therein . however , the problem of finding efficient encoding and decoding algorithms has received considerably less attention .",
    "we briefly discuss two previous methods that are relevant to our work .",
    "the first , a general purpose technique based on the idea of lexicographic ordering and enumeration of codewords in a codebook ( schalkwijk  @xcite , cover  @xcite ) is an example of ranking / unranking algorithms that are well studied in the combinatorial literature ( nijenhuis and wilf  @xcite ) .",
    "we refer to this as the _ enumerative _ approach .",
    "the second ( knuth  @xcite ) is a special - purpose , highly efficient technique that works for balanced codes , i.e. , when @xmath28 , and is referred to as the _ complementation _ method .",
    "the enumerative approach orders the codewords lexicographically ( with respect to the partial order defined by @xmath29 ) , as in a dictionary .",
    "the encoder computes the codeword from its dictionary index , and the decoder computes the dictionary index from the codeword .",
    "the method is effective because there is a simple formula involving binomial coefficients for computing the lexicographic index of a codeword .",
    "the resulting code is fully efficient in the sense that @xmath30 .",
    "however , this method requires the computation of the exact values of binomial coefficients @xmath31 , and requires registers of length @xmath32 , which limits its usefulness .",
    "an alternative is to use arithmetic coding ( rissanen and langdon  @xcite , rissanen  @xcite ; see also cover and thomas  @xcite ) .",
    "arithmetic coding is an efficient variable length source coding technique for finite alphabet sources .",
    "given a source alphabet and a simple probability model for sequences @xmath33 , let @xmath34 and @xmath35 denote the probability distribution and cumulative distribution function , respectively .",
    "an arithmetic encoder represents @xmath33 by a number in the interval @xmath36 $ ] .",
    "the implementation of such a coder can also run into problems with very long registers , but elegant finite - length implementations are known and are widely used ( witten , neal and cleary  @xcite ) . for constant weight codes , the idea is to reverse the roles of encoder and decoder , i.e. , to use an arithmetic decoder as an encoder and an arithmetic encoder as a constant weight decoder ( ramabadran  @xcite ) .",
    "ramabadran gives an efficient algorithm based on an adaptive probability model , in the sense that the probability that the incoming bit is a @xmath37 depends on the number of @xmath37 s that have already occurred .",
    "this approach successfully overcomes the finite - register - length constraints associated with computing the binomial coefficients and the resulting efficiency is often very high , in many cases the loss of information being at most one bit . the encoding complexity of the method is @xmath32 .",
    "knuth s complementation method  @xcite relies on the key observation that if the bits of a length-@xmath1 binary sequence are complemented sequentially , starting from the beginning , there must be a point at which the weight is equal to @xmath38 .",
    "given the transformed sequence , it is possible to recover the original sequence by specifying how many bits were complemented ( or the weight of the original sequence ) .",
    "this information is provided by a ( relatively short ) constant weight check string , and the resulting code consists of the transformed sequence followed by the constant weight check bits . in a series of papers , bose and",
    "colleagues extended knuth s method in various ways , and determined the limits of this approach ( see @xcite and references therein ) .",
    "the method is simple and efficient , and even though the overall complexity is @xmath32 , for @xmath39 we found it to be eight times as fast as the method based on arithmetic codes .",
    "however , the method only works for balanced codes , which restricts its applicability .",
    "the two techniques that we have described above both have complexity that depends on the length @xmath1 of the codewords .",
    "in contrast , the complexity of our algorithm depends only on the weight @xmath0 , which makes it more suitable for codes with relatively low weight .    as a final piece of background information",
    ", we define what we mean by a dissection .",
    "we assume the reader is familiar with the terminology of polytopes ( see for example coxeter  @xcite , grnbaum  @xcite , ziegler  @xcite ) . two polytopes @xmath40 and @xmath41 in @xmath42",
    "are said to be _ congruent _ if @xmath41 can be obtained from @xmath40 by a translation , a rotation and possibly a reflection in a hyperplane .",
    "two polytopes @xmath40 and @xmath41 in @xmath42 are said to be _",
    "equidecomposable _ if they can be decomposed into finite sets of polytopes @xmath43 and @xmath44 , respectively , for some positive integer @xmath45 , such that @xmath46 and @xmath47 are congruent for all @xmath48 ( see frederickson  @xcite ) .",
    "that is , @xmath40 is the disjoint union of the polytopes @xmath46 , and similarly for @xmath41 .",
    "if this is the case then we say that @xmath40 can be _ dissected _ to give @xmath41 ( and that @xmath41 can be dissected to give @xmath40 ) .",
    "note that we allow reflections in the dissection : there are at least four reasons for doing so .",
    "( i ) it makes no difference to the _ existence _ of the dissection , since if two polytopes are equidecomposable using reflections they are also equidecomposable without using reflections .",
    "this is a classical theorem in two and three dimensions ( * ? ? ?",
    "* chap .  20 ) and the proof is easily generalized to higher dimensions .",
    "( ii ) when studying congruences , it is simpler not to have to worry about whether the determinant of the orthogonal matrix has determinant @xmath49 or @xmath50 .",
    "( iii ) allowing reflections often reduces the number of pieces .",
    "( iv ) since our dissections are mostly in dimensions greater than three , the question of `` physical realizability '' is usually irrelevant .",
    "note also that we do not require that the @xmath46 can be obtained from @xmath40 by a succession of cuts along infinite hyperplanes .",
    "all we require is that @xmath40 be a disjoint union of the @xmath46 .",
    "one final technical point : when defining dissections using coordinates , as in eqns .",
    "( [ eq2da ] ) , ( [ eq2db ] ) below , we use a mixture of @xmath51 and @xmath52 signs in order to have unambiguously defined maps .",
    "this is essential for our application . on the other hand ,",
    "it means that the `` pieces '' in the dissection may be missing certain boundaries .",
    "it should therefore be understood that if we were focusing on the dissections themselves , we would replace each piece by its topological closure .    for further information about dissections",
    "see the books mentioned in section [ sec : intro ] .",
    "in this section , we first consider the problem of encoding and decoding a binary constant weight code of weight @xmath53 and arbitrary length @xmath1 , i.e. , where there are only two bits set to @xmath37 in any codeword . our approach is based on the fact that vectors of weight two can be represented as points in two - dimensional euclidean space , and can be scaled , or normalized , to lie in a right triangle .",
    "this approach is then extended , first to weight @xmath54 , and then to arbitrary weights @xmath0 .    for any weight @xmath0 and block length @xmath1 ,",
    "let @xmath55 denote the set of all weight @xmath0 vectors , with @xmath56 .",
    "our codebook @xmath9 will be a subset of @xmath55 , and will be equal to @xmath55 for a fully efficient code , i.e. , when @xmath30 .",
    "we will represent a codeword by the @xmath0-tuple @xmath57 , @xmath58 , where @xmath59 is the position of the @xmath60th @xmath37 in the codeword , counting from the left .",
    "if we normalize these indices @xmath59 by dividing them by @xmath1 , the codebook @xmath9 becomes a discrete subset of the polytope @xmath61 , the convex hull of the points @xmath62 .",
    "@xmath63 is a right triangle , @xmath64 is a right tetrahedron and in general we will call @xmath61 a _ unit orthoscheme_-dimensional simplex having an edge path consisting of @xmath0 totally orthogonal vectors ( coxeter  @xcite ) . in a _ unit orthoscheme _ these edges all have length @xmath37 . ] .",
    "the set of inputs to the encoder will be denoted by @xmath65 : we assume that this consists of @xmath0-tuples @xmath66 which range over a @xmath0-dimensional hyper - rectangle or `` brick '' .",
    "after normalization by dividing the @xmath67 by @xmath1 , we may assume that the input vector is a point in the hyper - rectangle or `` brick '' @xmath68 we will use @xmath69 and @xmath70 to denote the normalized versions of the input vector and codeword , respectively , defined by @xmath71 and @xmath72 for @xmath73 .    the basic idea underlying our approach is to find a dissection of @xmath74 that gives @xmath61 .",
    "the encoding and decoding algorithms are obtained by tracking how the points @xmath75 and @xmath76 move during the dissection .",
    "the volume of @xmath74 is @xmath77 .",
    "this is also the volume of @xmath61 , as the following argument shows .",
    "classify the points @xmath78 in the unit cube @xmath79^w$ ] into @xmath80 regions according to their order when sorted ; the regions are congruent , so all have volume @xmath81 , and the region where the @xmath82 are in nondecreasing order is @xmath61 .     to give triangle @xmath63 .",
    "piece 1 may be rotated about center into its new position , or reflected in main diagonal and translated downwards . ,",
    "width=340 ]    [ fig2 ]    we now return to the case @xmath53 .",
    "there are many ways to dissect the rectangle @xmath83 into the right triangle @xmath63 .",
    "we will consider two such dissections , both two - piece dissections based on fig .",
    "[ fig:2d ] .    in the first dissection ,",
    "the triangular piece marked 1 in fig .",
    "[ fig:2d ] is rotated clockwise about the center of the square until it reaches the position shown on the right in fig .",
    "[ fig:2d ] . in the second dissection ,",
    "the piece marked 1 is first reflected in the main diagonal of the square and then translated downwards until it reaches the position shown on the right in fig .",
    "[ fig:2d ] . in both dissections",
    "the piece marked 2 is fixed .",
    "the two dissections can be specified in terms of coordinates and @xmath52 signs , see the remark at the end of section [ sec : review ] . ] as follows .",
    "for the first dissection , we set @xmath84 and for the second , we set @xmath85    the first dissection involves only a rotation , but seems harder to generalize to higher dimensions .",
    "the second one is the one we will generalize ; it uses a reflection , but as mentioned at the end of section [ sec : review ] , this is permitted by the definition of a dissection .",
    "we next illustrate how these dissections can be converted into encoding algorithms for constant weight ( weight @xmath86 ) binary codes .",
    "again there may be several solutions , and the best algorithm may depend on arithmetic properties of @xmath1 ( such as its parity ) .",
    "we work now with the unnormalized sets @xmath87 and @xmath88 . in each case",
    "the output is a weight-@xmath86 binary vector with @xmath37 s in positions @xmath89 and @xmath90 .      1 .",
    "the input is an information vector @xmath91 with @xmath92 and @xmath93 .",
    "2 .   if @xmath94 , we set @xmath95 , @xmath96 , otherwise we set @xmath97 and @xmath98 .    for @xmath1 even , this algorithm generates all possible @xmath99 codewords . for @xmath1 odd",
    "it generates only @xmath100 codewords , leading to a slight inefficiency , and the following algorithm is to be preferred .      1 .",
    "the input is an information vector @xmath91 with @xmath101 , @xmath102 .",
    "2 .   if @xmath94 , we set @xmath95 , @xmath96 , otherwise we set @xmath103 , @xmath104 .    for @xmath1 odd , this algorithm generates all @xmath99 codewords , but for @xmath1 even it generates only @xmath99 codewords , again leading to a slight inefficiency .      1 .",
    "the input is an information vector @xmath91 with @xmath92 and @xmath93 .",
    "2 .   if @xmath94 , we set @xmath95 , @xmath96 , otherwise we set @xmath105 , @xmath106 .    for @xmath1 even , this algorithm generates all @xmath99 codewords , but for @xmath1 odd it generates only @xmath100 codewords , leading to a slight inefficiency .",
    "there is a similar algorithm , not given here , which is better when @xmath1 is odd .",
    "note that only one test is required in any of the encoding algorithms .",
    "the mappings are invertible , with obvious decoding algorithms corresponding to the inverse mappings from @xmath88 to @xmath87    we now extend this method to weight @xmath54 . fortunately , the dehn invariants for both the brick @xmath107 and our unit orthoscheme @xmath64 , which is the tetrahedron with vertices @xmath108 and @xmath109 , are zero ( since in both cases all dihedral angles are rational multiples of @xmath110 ) , and so by the dehn - sydler theorem the polyhedra @xmath107 and @xmath64 _ are _ equidecomposable . as already mentioned in section [ sec : intro ] , the dehn - sydler theorem applies only in three dimensions .",
    "but it will follow from the algorithm given in the next section that @xmath74 and @xmath61 are equidecomposable in all dimensions",
    ".    we will continue to describe the encoding step ( the map from @xmath74 to @xmath61 ) first .",
    "we will give an inductive dissection ( see fig .",
    "[ fig:3d ] ) , transforming @xmath107 to @xmath64 in two steps , effectively reducing the dimension by one at each step . in the first step ,",
    "the brick @xmath107 is dissected into a triangular prism ( the product of a right triangle , @xmath63 , and an interval ) , and in the second step this triangular prism is dissected into the tetrahedron @xmath64 .",
    "note that the first step has essentially been solved by the dissection given in eqn .",
    "( [ eq2db ] ) .",
    "[ fig3 ]    for the second step we use a four - piece dissection of the triangular prism to the tetrahedron @xmath64 .",
    "this dissection , shown with the tetrahedron and prism superimposed in fig .",
    "[ fig:3dpiece ] , appears to be new .",
    "there is a well - known dissection of the same pair of polyhedra that was first published by hill in 1896  @xcite .",
    "this also uses four pieces , and is discussed in several references : see boltianskii  @xcite , cromwell  @xcite , frederickson  ( * ? ? ? * fig .",
    "20.4 ) , sydler  @xcite , wells  @xcite . however , hill s dissection seems harder to generalize to higher dimensions .",
    "hill s dissection does have the advantage over ours that it can be accomplished purely by translations and rotations , whereas in our dissection two of the pieces ( pieces labeled 2 and 3 in fig .",
    "[ fig:3dpiece ] ) are also reflected .",
    "however , as mentioned at the end of section [ sec : review ] , this is permitted by the definition of a dissection , and is not a drawback for our application . apart from this , our dissection is simpler than hill s , in the sense that his dissection requires a cut along a skew plane ( @xmath111 ) , whereas all our cuts are parallel to coordinate axes .",
    "[ fig4 ]    to obtain the four pieces shown in fig .",
    "[ fig:3dpiece ] , we first make two horizontal cuts along the planes @xmath112 and @xmath113 , dividing the tetrahedron into three slices .",
    "we then cut the middle slice into two by a vertical cut along the plane @xmath114 .",
    "there appears to be a tradition in geometry books that discuss dissections of not giving coordinates for the pieces . to an engineer",
    "this seems unsatisfactory , and so in table [ t1 ] we list the vertices of the four pieces in our dissection .",
    "piece 1 has four vertices , while the other three pieces each have six vertices .",
    "( in the hill dissection the numbers of vertices of the four pieces are @xmath115 , @xmath116 , @xmath117 and @xmath117 respectively . ) given these coordinates , it is not difficult to verify that the four pieces can be reassembled to form the triangular prism , as indicated in fig .",
    "[ fig:3dpiece ] .",
    "as already remarked , pieces 2 and 3 are also reflected ( or `` turned over '' in a fourth dimension ) .",
    "the correctness of the dissection also follows from the alternative description of this dissection given below .",
    "@xmath118 , [ 0,0,1/3 ] , [ 0,1/3,1/3 ] , [ 1/3 , 1/3,1/3 ] .",
    "\\\\ 2 & [ 0,0,1/3 ] , [ 0,1/3,1/3 ] , [ 1/3,1/3,1/3 ] , \\\\    & [ 0,0,2/3 ] , [ 0,1/3,2/3 ] , [ 1/3,1/3,2/3 ] .",
    "\\\\ 3 & [ 0,1/3,1/3 ] , [ 1/3,1/3,1/3 ] , [ 0 , 1/3,2/3 ] , \\\\    & [ 0,2/3,2/3 ] , [ 2/3,2/3,2/3 ] , [ 1/3,1/3,2/3 ] . \\\\ 4 & [ 0,0,2/3 ] , [ 0,2/3,2/3 ] , [ 2/3 , 2/3,2/3 ] , \\\\    &",
    "[ 0,0,1 ] , [ 0,1,1 ] , [ 1 , 1,1 ] .",
    "\\\\ \\hline \\end{array}\\ ] ]    the dissection shown in fig .",
    "[ fig:3dpiece ] can be described algebraically as follows .",
    "we describe it in the more logical direction , going from the triangular prism to the tetrahedron since this is what we will generalize to higher dimensions in the next section .",
    "the input is a vector @xmath119 with @xmath120 , @xmath121 ; the output is a vector @xmath122 with @xmath123 , given by @xmath124    @xmath125    the four cases in eqn .",
    "( [ eqf3 ] ) , after being transformed , correspond to the pieces labeled 4 , 3 , 2 , 1 respectively in fig .",
    "[ fig:3dpiece ] .",
    "we see from eqn .",
    "( [ eqf3 ] ) that in the second and third cases the linear transformation has determinant @xmath50 , indicating that these two pieces must be reflected .",
    "since it is hard to visualize dissections in dimensions greater than three , we give a schematic representation of the above dissection that avoids drawing polyhedra .",
    "[ fig : shift ] shows a representation of the transformation from the triangular prism to the tetrahedron @xmath64 , equivalent to that given in eqn .",
    "( [ eqf3 ] ) .",
    "the steps shown in fig .  [ fig : shift ] may be referred to as `` cut and paste '' operations , because , as fig .",
    "[ fig : shift ] shows , the vector in the triangular prism is literally cut up into pieces which are rearranged and relabeled .",
    "note that , to complete the transformation , we precede this operation by the dissection given in eqn .",
    "( [ eq2db ] ) , finally establishing the bijection between @xmath107 and @xmath64 .",
    "[ fig5 ]    we now describe the mapping shown in fig .  [",
    "fig : shift ] in more detail .",
    "the triangular prism is represented by the set of partially ordered triples @xmath126 with @xmath127 and @xmath128 , and we wish to transform this into the tetrahedron consisting of the points @xmath122 with @xmath129 .",
    "we divide the interval @xmath130 into @xmath54 equal segments of length @xmath131 , and consider where the points @xmath132 and @xmath133 fall in this interval , given that @xmath119 is in the triangular prism .",
    "there are three possibilities for where @xmath133 lies in relation to @xmath134 , and we further divide the case @xmath135 into two subcases depending on whether @xmath136 or @xmath137 .",
    "these are the four cases shown in fig .",
    "[ fig : shift ] , and correspond one - to - one with the four dissection pieces in fig .",
    "[ fig:3dpiece ] .",
    "[ fig : shift ] shows how the triples @xmath138 ( reindexed according to their relative positions ) are mapped to the triples @xmath139 .",
    "the last column of fig .",
    "[ fig : shift ] shows the ranges of the @xmath140 in the four cases ; the fact that these ranges are disjoint guarantees that the mapping from @xmath138 to @xmath139 is invertible .",
    "the ranges of the @xmath140 will be discussed in more detail in the following section after the general algorithms are presented .",
    "this operation can now be described without explicitly mentioning the underlying dissection .",
    "each interval of length @xmath141 , together with the given @xmath82 values within it , is treated as a single complete unit . in the  cut and paste \"",
    "operations , these units are rearranged and relabeled in such a way that the operation is invertible .",
    "in the previous section we provided an encoding and decoding algorithm for weights @xmath53 and @xmath54 , based on our geometric interpretation of @xmath88 and @xmath142 as points in @xmath42 . in this section , the algorithm is generalized to larger values of the weight @xmath0 .",
    "we start with the geometry , and give a dissection of the `` brick '' @xmath74 into the orthoscheme @xmath61 .",
    "we work with the normalized coordinates @xmath143 ( for a point in @xmath74 ) and @xmath144 ( for a point in @xmath61 ) , where @xmath145 . later in this section ,",
    "we discuss the modifications needed to take into account the fact that the @xmath59 must be integers .      restating the problem ,",
    "we wish to find a bijection @xmath146 between the sets @xmath74 and @xmath61 .",
    "the inductive approach developed for @xmath54 ( where the @xmath53 case was a subproblem ) will be generalized .",
    "of course the bijection @xmath147 between @xmath148 and @xmath149 is trivial .",
    "we assume that a bijection @xmath150 is known between @xmath151 and @xmath152 , and show how to construct a bijection @xmath146 between @xmath74 and @xmath61 .",
    "the last step in the induction uses a map @xmath153 from the prism @xmath154 to @xmath155 ( @xmath156 is the map described in eqn .",
    "( [ eq2db ] ) and @xmath157 is described in eqn .",
    "( [ eqf3 ] ) ) .",
    "the mapping @xmath146 from @xmath74 to @xmath61 is then given recursively by @xmath158 , where @xmath159 for @xmath160 we set @xmath161 by iterating eqn .",
    "( [ eqf ] ) , we see that @xmath146 is obtained by successively applying the maps @xmath162  , @xmath153 .",
    "the following algorithm defines @xmath153 for @xmath163 .",
    "we begin with an algebraic definition of the mapping and its inverse , and then discuss it further in the following section .",
    "the input to the mapping @xmath153 is a vector @xmath164 , with @xmath165 and @xmath166 ; the output is a vector @xmath167 .    _ forward mapping @xmath153 _ ( @xmath163 ) :    \\1 ) let @xmath168    \\2 ) let @xmath169    \\3 ) set @xmath170 equal to : @xmath171 eqn .",
    "( [ eqn - f - shift ] ) identifies the  cut and paste \" operations required to obtain @xmath172 for different ranges of the variable @xmath173 . if the initial index in one of the four cases in eqn .",
    "( [ eqn - f - shift ] ) is smaller than the final index , that case is to be skipped . a case",
    "is also skipped if the subscript for an @xmath82 is not in the range @xmath174 . note in step 1 that @xmath175 if @xmath176 is the largest of the @xmath82 s .",
    "this implies that @xmath177 , and then step 3 is the identity map .",
    "the inverse mapping @xmath178 from @xmath61 to @xmath74 has a similar recursive definition .",
    "the @xmath0th step in the induction is the map @xmath179 defined below .",
    "for @xmath160 we set @xmath180 the map @xmath178 is obtained by successively applying the maps @xmath181  , @xmath182",
    ".    _ inverse mapping @xmath183 _ ( @xmath163 ) :    \\1 ) let @xmath184    \\2 ) if @xmath185 , let @xmath186 , otherwise let @xmath187 in either case , let @xmath188 .",
    "\\3 ) set @xmath189 equal to : @xmath190    note that the transformations in eqn .",
    "( [ eqn - f - shift ] ) and eqn .",
    "( [ eqn - g - shift ] ) are formal inverses of each other , and that these transformations are volume - preserving .",
    "the underlying linear transformations are orthogonal transformations with determinant @xmath49 or @xmath50 .    before proceeding further ,",
    "let us verify that in the case @xmath54 , the mapping @xmath191 agrees with that given in eqn .",
    "( [ eqf3 ] ) .    * if @xmath192 , then @xmath193 , @xmath177 and the map is the identity , as mentioned above . * if @xmath194 there are two subcases : * * if @xmath195 then @xmath196 , @xmath177 . * * if @xmath197 then @xmath196 , @xmath198 . *",
    "if @xmath199 , then @xmath200 , @xmath177 .    the transformations in eqn .",
    "( [ eqn - f - shift ] ) now exactly match those in eqn .",
    "( [ eqf3 ] ) .          in fig .",
    "[ fig : intuitive ] , we give a graphical interpretation of the algorithm , which can be regarded as a generalization of the `` cut and paste '' description given above .",
    "this figure shows the transformation defined by the @xmath0th step @xmath153 in the algorithm . at this step ,",
    "we begin with a list of @xmath201 numbers @xmath202 in increasing order , and a further number @xmath176 which may be anywhere in the interval @xmath203 . this list of @xmath0 numbers is plotted in the plane as the set of @xmath0 points @xmath204 for @xmath205 ( indicated by the solid black circles in fig .",
    "[ fig : intuitive ] ) . in the first step in the forward algorithm ,",
    "the augmented list @xmath206 is sorted into increasing order . in the sorted list",
    ", @xmath176 now occupies position @xmath207 , so the point @xmath208 moves to the left , to the new position @xmath209 , and the points @xmath210 for @xmath211 move to the right .",
    "this is indicated by the arrows in the figure .",
    "the new positions of these points are marked by hollow circles .",
    "the point @xmath209 now lies between the grid points @xmath212 and @xmath213 ( it may coincide with the latter point ) , since @xmath214 .",
    "we draw the line @xmath215 ( shown as the dashed - and - dotted line in fig .",
    "[ fig : intuitive ] ) .",
    "this has unit slope and passes through the points @xmath213 and @xmath216 .",
    "the algorithm then computes @xmath217 to be the smallest index @xmath60 for which @xmath218 is on or above this line . once @xmath219 and @xmath220 have been determined , the forward mapping proceeds as follows .",
    "the points @xmath210 for @xmath221 are shifted to the right of the figure and are moved upwards by the amount @xmath222 , their new positions being indicated by crosses in the figure .",
    "finally , the origin is moved to the grid point @xmath223 and the points are reindexed .",
    "the @xmath224 points which originally had indices @xmath225 become points @xmath226 after reindexing . in the new coordinates ,",
    "the final positions of the points lie inside the square region @xmath227 .",
    "the reader can check that this process is exactly equivalent to the algebraic description of @xmath153 given above .    to recover @xmath219 and @xmath220",
    ", we first determine the value of @xmath224 .",
    "this can indeed be done since @xmath228 is precisely the index of the largest @xmath229 that lies on or above the line @xmath230 in the new coordinate system .",
    "note that the position of this line is independent of @xmath219 and @xmath220 and @xmath231 .",
    "this works because the points @xmath232 in the original coordinate system , before the origin is shifted , are moved right by @xmath0 units and upwards by @xmath0 units , so points below the dashed - and - dotted line remain below the line .",
    "furthermore , observe that in the new coordinate system the number of points @xmath233 below the line @xmath234 is equal to @xmath235 .",
    "thus the correct @xmath219 and @xmath220 values may be recovered , and the inverse mapping can be successfully performed .",
    "the following remarks record two properties of the algorithm that will be used later .",
    "_ remark 1 : _ step 2 of the forward algorithm implies that @xmath236 and @xmath237 .",
    "it follows that there is no @xmath60 in the range @xmath238 for which @xmath239    _ remark 2 : _ the forward algorithm produces a vector @xmath240 whose components satisfy @xmath241 @xmath242 and @xmath243 eqns .",
    "( [ eqn : moveleft ] ) and ( [ eqn : moveright ] ) follow from the minimizations in steps  1 and 2 of the forward algorithm , respectively . the right - hand side of eqn .",
    "( [ eqbelow ] ) expresses the fact , already mentioned , that the first @xmath220 points remain below the dotted - and - dashed line after they are shifted .",
    "we now give the formal proof that the algorithm is correct .",
    "this is simply a matter of collecting together facts that we have already observed .",
    "[ th1 ] for any @xmath244 , the forward mapping @xmath153 is a one - to - one mapping from @xmath245 to @xmath61 with inverse @xmath183 .",
    "first , it follows from remark 2 that , for @xmath246 , @xmath247 satisfies @xmath248 , and so is an element of @xmath61 .",
    "suppose there were two different choices for @xmath33 , say @xmath249 and @xmath250 , such that @xmath251 we know that @xmath252 determines @xmath253 and @xmath219 .",
    "so @xmath249 and @xmath250 have the same associated values of @xmath219 and @xmath220 .",
    "but for a given pair @xmath254 , eqn .",
    "( [ eqn - f - shift ] ) is invertible .",
    "hence @xmath255 , and @xmath153 is one - to - one .",
    "note that the transformations in eqn .",
    "( [ eqn - f - shift ] ) and eqn .",
    "( [ eqn - g - shift ] ) are inverses of each other . hence @xmath153 is also an onto map , and @xmath183 is its inverse .",
    "the map @xmath153 , which dissects the prism @xmath154 to give the orthoscheme @xmath155 , has one piece for each pair @xmath256 . if @xmath257 then @xmath177 , while if @xmath258 , @xmath220 takes all values from @xmath259 to @xmath260 .",
    "( it is easy to write down an explicit point in the interior of the piece corresponding to a specified pair of values of @xmath219 and @xmath220 .",
    "assume @xmath261 and set @xmath262 .",
    "take the point with coordinates @xmath263 given by @xmath264 ; @xmath265 for @xmath266 ; @xmath267 for @xmath268 ; @xmath269 for @xmath270 . )",
    "the total number of pieces in the dissection is therefore @xmath271 which is @xmath272 for @xmath273 .",
    "this is a well - known sequence , entry a124 in @xcite , which by coincidence also arises in a different dissection problem : it is the maximal number of pieces into which a circular disk can be cut with @xmath201 straight cuts .",
    "for example , with three cuts , a pizza can be cut into a maximum of seven pieces , and this is also the number of pieces in the dissection defined by @xmath274 .      to apply the above algorithm to the problem of encoding and decoding constant weight codes , we must work with positive integers rather than real numbers , which entails a certain loss in rate , although the algorithms remain largely unchanged .",
    "let @xmath275 , and let @xmath1 and @xmath0 be given with @xmath276 . in a manner analogous to the real - valued case , we find a bijection between a finite hyper - rectangle or brick @xmath277 and a subset of the finite orthoscheme @xmath278 , where @xmath279 is the set of vectors @xmath280 satisfying @xmath281 for @xmath205 , and @xmath282 is the set of vectors @xmath280 satisfying @xmath283 note that usually @xmath284 , which entails a loss in rate .",
    "the forward mapping @xmath153 is now replaced by the map @xmath285 , which sends @xmath286 with @xmath287 and @xmath288 to an element of @xmath289 .",
    "let us write @xmath290 , where @xmath291 and @xmath292 .",
    "we partition the range @xmath293 into @xmath0 parts , where the first @xmath294 parts each have @xmath295 elements , the next @xmath296 parts each have @xmath297 elements , and the last part has @xmath295 elements ( giving a total of @xmath1 elements ) .",
    "this is similar to the real - valued case , where each interval had length @xmath141 .",
    "\\1 ) let @xmath298    \\2 ) let @xmath299 where @xmath300 .",
    "\\3 ) set @xmath301 equal to : @xmath302    the inverse mapping @xmath183 is similarly replaced by the map @xmath303 @xmath304 , defined as follows . again , assume @xmath290 .",
    "\\1 ) let @xmath305 where @xmath306 .",
    "\\2 ) if @xmath185 , let @xmath186 , otherwise let @xmath307 in either case , let @xmath188 .",
    "\\3 ) set @xmath308 equal to : @xmath309    we omit the proofs , since they are similar to those for the real - valued case .",
    "the overall complexity of the transform algorithm is @xmath2 , because at each induction step the complexity is linear in the weight at that step .",
    "recall that the complexities of the arithmetic coding method and knuth s complementation method are both @xmath32 .",
    "thus when the weight @xmath0 is larger than @xmath310 , the geometric approach is less competitive .",
    "when the weight is low , the proposed geometric technique is more efficient , because knuth s complementation method is not applicable , while the dissection operations of the proposed algorithm makes it faster than the arithmetic coding method .",
    "furthermore , due to the structure of the algorithm , it is possible to parallelize part of the computation within each induction step to further reduce the computation time .",
    "so far little has been said about mapping a binary sequence to an integer sequence @xmath311 such that @xmath312 $ ] , where @xmath313 and @xmath314 are the lower and upper bound of the valid range as specified by the algorithm .",
    "a straightforward method is to treat the binary sequence as an integer number and then use  quotient and remainder \" method to find such a mapping .",
    "however , this requires a division operation , and when the binary sequence is long , the computation is not very efficient .",
    "a simplification is to partition the binary sequence into short sequences , and map each short binary sequence to a pair of integers , as in the case of a weight two constant weight codes . through proper pairing of the ranges ,",
    "the loss in the rate can be minimized .",
    "the overall rate loss has two components , the first from the rounding involved in using natural numbers , the second from the loss in the above simplified translation step .",
    "however , when the weight is on the order of @xmath310 , and @xmath1 is in the range of @xmath315 , the rate loss is usually @xmath316 bits per block .",
    "for example , when @xmath317 , @xmath318 , then the rate loss is 2 bits / block compared to the best possible code which would encode @xmath319 information bits .",
    "we propose a novel algorithm for encoding and decoding constant weight binary codes , based on dissecting the polytope defined by the set of all binary words of length @xmath1 and weight @xmath0 , and reassembling the pieces to form a hyper - rectangle corresponding to the input data .",
    "the algorithm has a natural recursive structure , which enables us to give an inductive proof of its correctness .",
    "the proposed algorithm has complexity @xmath2 , independent of the length of the codewords @xmath1 .",
    "it is especially suitable for constant weight codes of low weight ."
  ],
  "abstract_text": [
    "<S> we present a novel technique for encoding and decoding constant weight binary codes that uses a geometric interpretation of the codebook . </S>",
    "<S> our technique is based on embedding the codebook in a euclidean space of dimension equal to the weight of the code . </S>",
    "<S> the encoder and decoder mappings are then interpreted as a bijection between a certain hyper - rectangle and a polytope in this euclidean space . </S>",
    "<S> an inductive dissection algorithm is developed for constructing such a bijection . </S>",
    "<S> we prove that the algorithm is correct and then analyze its complexity . </S>",
    "<S> the complexity depends on the weight of the code , rather than on the block length as in other algorithms . </S>",
    "<S> this approach is advantageous when the weight is smaller than the square root of the block length .    </S>",
    "<S> constant weight codes , encoding algorithms , dissections , polyhedral dissections , bijections , mappings , dehn invariant . </S>"
  ]
}