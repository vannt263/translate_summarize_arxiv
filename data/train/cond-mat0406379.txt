{
  "article_text": [
    "kinetic monte carlo ( kmc ) is an extremely efficient method @xcite to carry out dynamical simulations of stochastic and/or thermally activated processes when the relevant activated atomic - scale processes are known .",
    "kmc simulations have been successfully used to model a variety of dynamical processes ranging from catalysis to thin - film growth .",
    "the basic principle of kinetic monte carlo is that in order to efficiently simulate a dynamical system with a variety of different rates or processes , at each step in the simulation one picks the next process to occur with a probability proportional to the rate for that process .",
    "the time of the next event is determined by the total overall rate for all processes to occur , and after each event the rates for all processes are updated as necessary .",
    "in contrast to metropolis monte carlo , @xcite in which each monte carlo step corresponds to a configuration - independent time interval and each event is selected randomly but only accepted with a configuration - dependent probability , in kinetic monte carlo both the selected event and the time interval between events are configuration - dependent while the acceptance probability is fixed ( all attempts are accepted ) . in the context of traditional equilibrium monte carlo simulations this is sometimes referred to as the n - fold way .",
    "@xcite although kmc requires additional book - keeping to keep track of the rates ( probabilities ) for all possible events , the kmc algorithm is typically significantly more efficient than the metropolis algorithm since no selected moves are rejected . in particular , for problems such as thin - film growth",
    "in which the possible rates or probabilities for events can vary by several orders of magnitude , the kinetic monte carlo algorithm can be orders of magnitude more efficient than metropolis monte carlo .",
    "the standard kmc algorithm is a serial algorithm since only one event can occur at each step .",
    "however , for some problems one needs to simulate larger length and time - scales than can be simulated using a serial algorithm . for these problems it would be desirable to develop efficient parallel kinetic monte carlo algorithms so that many processors can be used simultaneously in order to carry out realistic computations over extended time- and length - scales .    recently there has been a great deal of work on asynchronous parallel algorithms for metropolis monte carlo using domain decomposition .",
    "in particular , because the attempt time in metropolis monte carlo is independent of system configuration , an asynchronous  conservative \" algorithm may be used . @xcite in such an algorithm all processors whose next attempt time is less than their neighbor s next attempt times are allowed to proceed .",
    "unfortunately such a  conservative \" algorithm does not work for kinetic monte carlo since in kmc the event - time depends on the system configuration .",
    "in particular , since fast events may  propagate \" across processors , the time for an event already executed by a processor may change due to earlier events in nearby processors , thus leading to an incorrect evolution . as a result , the development of efficient parallel algorithms for kinetic monte carlo simulations remains a challenging problem .",
    "a more efficient version of the conservative asynchronous algorithm for metropolis monte carlo has been developed by lubachevsky@xcite in the context of ising simulations and has been implemented by korniss et al.@xcite in this approach , ",
    "n - fold way \" @xcite simulations are carried out in the interior of each processor , while metropolis simulations are carried out at the boundary . at each step , either an interior move or a boundary move is selected with the appropriate probability . while all  n - fold way \" interior moves are immediately accepted , all metropolis attempts must wait until the neighboring processor s next attempt time is later before being either accepted or rejected . since such an algorithm is equivalent to the conservative metropolis monte carlo algorithm described above , it is generally scalable,@xcite and has been found to be relatively efficient in the context of kinetic ising model simulations in the metastable regime .",
    "@xcite    such an approach can be generalized@xcite in order to carry out parallel kmc simulations by mapping all kmc moves to a metropolis move with an acceptance probability given by the rate for that event divided by the fastest possible rate in the kmc simulation .",
    "however , because of the possibility of significant rejection of boundary events , the parallel efficiency may be very low for problems with a wide range of rates for different processes .",
    "for example , we have recently@xcite used such a mapping to carry out parallel kmc simulations of a simple 2d solid - on - solid  fractal \" model of submonolayer growth with a moderate monomer ratio @xmath0 of monomer hopping rate @xmath1 to ( per site ) deposition rate @xmath2 . however",
    ", due to the rejection of boundary events , an extremely low parallel efficiency was obtained.@xcite furthermore , in order to use such an approach , in general one needs to know in advance all the possible events and their rates and then to map them to metropolis dynamics so that all events may be selected with the appropriate probabilities .",
    "while such a mapping may be carried out for the simplest models , for more complicated models it is likely to be prohibitive .    a more efficient algorithm , which is also rigorous , is the synchronous relaxation ( sr ) algorithm .",
    "@xcite this algorithm was originally used by eick et al @xcite to simulate large circuit - switched communication networks and more recently by lubachevsky and weiss@xcite in the context of ising model simulations . in this approach , all processors remain synchronized at the beginning and end of a time interval @xmath3 , while an iterative relaxation method is used to correct errors due to boundary events .",
    "this algorithm has the advantage of generality ( for example , it is not necessary to know the types and/or rates of all possible events in advance ) and flexibility since the cycle length @xmath3 can be dynamically tuned@xcite to optimize the parallel efficiency .",
    "recently , we have studied the parallel efficiency and applicability of the sr algorithm in parallel kmc simulations of epitaxial growth@xcite and have found that in some cases a reasonable parallel efficiency can be obtained .",
    "however , due to fluctuations ( which increase logarithmically@xcite with the number of processors @xmath4 ) as well as the requirement of global communications at the end of each cycle ( the global communications time also increases logarithmically with @xmath4 ) the computational speedup as a function of @xmath4 is sublinear for fixed processor size .",
    "in addition , implementing such an algorithm is relatively complex .",
    "therefore , there is a need for a somewhat simpler and more efficient algorithm .    in order to address these problems ,",
    "we have developed a simpler synchronous sublattice ( sl ) algorithm for parallel kinetic monte carlo which we describe in detail here . while the sl algorithm is not rigorous",
    ", we find that using certain reasonable assumptions on the cycle length and processor size , the results obtained are identical to those obtained in serial simulations .",
    "furthermore , because the sl algorithm requires only local communications , the parallel efficiency is essentially independent of the number of processors in the large @xmath4 limit , thus leading to linear scaling . as a result ,",
    "the parallel efficiency is in general significantly greater than for the synchronous relaxation algorithm .",
    "the organization of this paper is as follows . in section",
    "ii we describe the algorithm . in section",
    "iii we present results obtained using this algorithm for several different models of thin - film growth , including a comparison with serial results .",
    "we also study the effects of fluctuations on the parallel efficiency and present results for the measured and theoretical parallel efficiency as a function of processor size and number of processors .",
    "the effects of finite processor size on the accuracy of the results obtained using our algorithm are also discussed and compared with finite - size effects due to finite system - size .",
    "finally , in section iv we summarize our results and discuss the general applicability of the sl algorithm to parallel kinetic monte carlo simulations .",
    "as in previous work on the  conservative \" asynchronous algorithm,@xcite in the synchronous sublattice ( sl ) algorithm , different parts of the system are assigned via spatial decomposition to different processors . however , in order to avoid conflicts between processors due to the synchronous nature of the algorithm , each processor s domain is further divided into different regions or sublattices ( see fig .",
    "[ figsub ] ) .",
    "a complete synchronous cycle corresponding to a time interval @xmath3 is then as follows . at the beginning of a cycle",
    "each processor s local time is initialized to zero .",
    "one of the sublattices is then randomly selected so that all processors operate on the same sublattice during that cycle .",
    "each processor then simultaneously and independently carries out kmc events in the selected sublattice until the time of the next event exceeds the time interval @xmath3 ( see fig .",
    "[ figcycle ] ) . as in the usual serial kmc",
    ", each event is carried out with time increment @xmath5 where @xmath6 is a uniform random number between @xmath7 and @xmath8 and @xmath9 is the total kmc event rate .",
    "each processor then communicates any necessary changes ( boundary events ) with its neighboring processors , updates its event rates and moves on to the next cycle using a new randomly chosen sublattice .",
    "[ figsub ] shows two possible methods of spatial and sublattice decomposition which are appropriate for simulations of thin - film growth@xmath10 a square sublattice decomposition ( fig . [ figsub](a ) ) and a strip sublattice decomposition ( fig . [",
    "figsub](b ) ) . in the square sublattice decomposition ,",
    "the system is divided into squares , each of which is assigned to a different processor , and each processor s domain is further divided into 4 square sublattices . at the beginning of each cycle one of the 4 sublattices ( a , b , c , or d ) is randomly chosen . in the strip - sublattice geometry ,",
    "the system is divided into strips , each of which is assigned to a different processor , and each processor s domain is further divided into 2 strips or sublattices . at the beginning of each cycle one of the 2 sublattices ( a or b ) is randomly chosen .    in order to avoid conflicts ,",
    "the sublattice size must be larger than the range of interaction ( typically only a few lattice units in simulations of thin - film growth ) .",
    "in addition , in order for each processor to calculate its event rates , the configuration in neighboring processors must be known as far as the range of interaction . as a result ,",
    "in addition to containing the configuration information for its own domain , each processor s array also contains a  ghost - region \" which includes the relevant information about the neighboring processor s configuration beyond the processor s boundary .    at the end of each cycle , each processor exchanges information with its neighboring processors in order to properly update its corresponding boundary and ghost regions .",
    "for example , if sublattice a is selected in the case of square - sublattice decomposition , then at the end of a cycle , possible boundary events must be communicated to the three processors north , west and northwest of each processor . by using sequential north and west communications",
    ", one can eliminate the northwest communication , and so only two communications are needed at the end of each cycle .",
    "similarly , if sublattice b is selected in the case of strip - sublattice decomposition , then at the end of a cycle , possible boundary events must be communicated to the processor to the east .    since moves are only allowed in the selected sublattice during a cycle , several cycles are needed for the entire system time to progress by @xmath3 .",
    "thus , in the square ( strip ) geometry , it takes on average @xmath11 cycles ( @xmath12 cycles ) to increase the overall system time by @xmath3 . during each cycle , the event rates in the non - selected sublattices of a given processor are automatically updated as each event proceeds , just as in the usual serial kmc .",
    "sublattice selection can be carried out either by having one processor select the sublattice for that cycle and then distribute it to all processors , or more efficiently by seeding all processors with the same random number generator so that they all independently select the same sublattice for each cycle .",
    "we note that due to the reduced communication in the strip - sublattice decomposition compared to the square - sublattice decomposition , the strip - sublattice decomposition is more efficient .",
    "in addition , since the sublattice in the strip - geometry is twice as large as for the square - geometry for the same processor size @xmath13 , there will be twice as many events per cycle in the strip geometry thus further reducing the overhead due to communication time .",
    "thus , we expect that the overhead due to communication latency in the strip - geometry will be approximately one - half of that for the square - geometry .",
    "diagram showing ( a ) square sublattice decomposition ( 9 processors ) and ( b ) strip sublattice decomposition ( 3 processors ) .",
    "solid lines correspond to processor domains while dashed lines indicate sublattice decomposition .",
    "dotted lines in ( a ) and ( b ) indicate  ghost - region \" surrounding central processor .",
    ", width=245 ]     diagram showing time - evolution in sl algorithm . dashed lines correspond to selected events , while dashed line with an @xmath14 corresponds to an event which is rejected since it exceeds the cycle time.,height=151 ]    we now consider the validity and efficiency of the synchronous sublattice ( sl ) algorithm",
    ". if the time interval @xmath3 is not too large , then the sl algorithm corresponds to allowing different sublattices to get slightly  out of synch \" during each cycle . over many cycles one expects such fluctuations to cancel out and",
    "so the parallel evolution should be identical to the corresponding serial kmc simulation .",
    "of course , in order to maximize the efficiency of the algorithm ( i.e. the average number of events per processor per cycle ) and minimize the communication time overhead , one would like to have the largest possible value of @xmath3 which does not ",
    "corrupt \" the time - evolution . as we shall demonstrate below , by picking the cycle length @xmath3 less than or equal to the average time for the fastest possible activated event ( e.g. monomer hopping in the simplest possible model of thin - film growth ) we do indeed obtain ( except for very small processor sizes for which finite - size effects may occur ) results which are identical to those obtained in serial kmc except for very small sublattice sizes",
    "thus , by using the general rule that the time interval @xmath3 must be smaller than or equal to the inverse of the fastest possible event rate in the kmc table , we expect that the synchronous algorithm will provide accurate results for sufficiently large sublattices .",
    "we note that the synchronous sublattice algorithm can also be used in a  self - learning \" kmc@xcite in which the kmc rate - tables are updated as the simulation goes along . in this case , if a new  fastest - event - rate \" is discovered in the middle of a cycle , then one merely restarts the cycle from the beginning using a smaller cycle time @xmath3 .",
    "in order to test the performance and accuracy of our synchronous sublattice algorithm we have used it to simulate three specific models of thin - film growth . in particular , we have studied three solid - on - solid ( sos ) growth models on a square lattice : a  fractal \" growth model , an edge - and - corner diffusion ( ec ) model , and a reversible model with one - bond detachment (  reversible model \" ) . in each of these three models the lattice configuration is represented by a two - dimensional array of heights and periodic boundary conditions are assumed . in the  fractal \" model,@xcite atoms ( monomers ) are deposited onto a square lattice with ( per site ) deposition rate @xmath2 , diffuse ( hop ) to nearest - neighbor sites with hopping rate @xmath1 and attach irreversibly to other monomers or clusters via a nearest - neighbor bond ( critical island size of @xmath8 ) .",
    "the key parameter is the ratio @xmath15 which is typically much larger than one in epitaxial growth . in this",
    "model fractal islands are formed in the submonolayer regime due to the absence of island relaxation .",
    "the ec model is the same as the fractal model except that island relaxation is allowed , i.e. atoms which have formed a single nearest - neighbor bond with an island may diffuse along the edge of the island with diffusion rate @xmath16 and around island - corners with rate @xmath17 ( see fig .",
    "[ figrelax ] ) .",
    "finally , the reversible model is also similar to the fractal model except that atoms with one - bond ( edge - atoms ) may hop along the step - edge or away from the step with rate @xmath18 , thus allowing both edge - diffusion and single - bond detachment . for atoms hopping up or down a step , an extra ehrlich - schwoebel barrier to interlayer diffusion @xcite",
    "may also be included . in this model",
    ", the critical island size @xmath19@xcite can vary from @xmath20 for small values of @xmath21 , to @xmath22 for sufficiently large values of @xmath15 and @xmath21.@xcite     schematic diagram of island - relaxation mechanisms for ( a ) edge - and - corner and ( b ) reversible models.,height=124 ]    for the fractal and reversible models , the range of interaction corresponds to one nearest - neighbor ( lattice ) spacing , while for the ec model it corresponds to the next - nearest - neighbor distance .",
    "thus , for these models the width of the  ghost - region \" corresponds to one lattice - spacing .",
    "we note that at each step of the simulation , either a particle is deposited within the appropriate sublattice , or a particle diffuses to a nearest - neighbor or next - nearest - neighbor lattice site . in order to avoid ",
    "double - counting \" , only particles within a processor s domain may diffuse , e.g. if a particle diffuses from the boundary region of a processor into its ghost - region during a cycle , then that particle is no longer free to move during that cycle . in more general models , for which concerted moves involving several atoms may occur ,",
    "@xcite the ghost region needs to be at least as large as the range of interaction and/or the largest possible concerted move .",
    "in such a case , the processor and sublattice to which a concerted event belongs can be determined by considering the location of the center - of - mass of the atoms involved in the concerted move .    in order to maximize both the serial and parallel efficiency in our kmc simulations",
    ", we have used lists to keep track of all possible events of each type and rate . for each sublattice ,",
    "a set of lists is maintained which contains all possible moves of each type .",
    "a binary tree is used to select which type of move will be carried out , while the particular move is then randomly chosen from the list of the selected type . after each move , the lists are updated .      in order to test our algorithm",
    "we have carried out both  serial emulations \" as well as parallel simulations . however , since our main goal is to test the performance and scaling behavior on parallel machines we have primarily focused on direct parallel simulations using the itanium and amd clusters at the ohio supercomputer center ( osc ) as well as on the alpha cluster at the pittsburgh supercomputer center ( psc ) .",
    "all of these clusters have fast communications@xmath10the itanium and amd clusters have myrinet and the alphaserver cluster has quadrics . in our simulations ,",
    "the interprocessor communications were carried out using mpi ( message - passing interface ) .      as a test of our algorithm we first present some detailed comparisons with serial results for different numbers of processors and system sizes for both the square and the strip geometries .",
    "[ figaccuracy1 ] shows a comparison of parallel and serial results for the fractal model with @xmath0 and a square system of size @xmath23 .",
    "the parallel simulations were carried out using a strip sublattice decomposition with processor sizes @xmath24 and @xmath25 with @xmath26 corresponding to @xmath27 and @xmath11 respectively , where @xmath4 is the number of processors .",
    "in particular , fig .",
    "[ figaccuracy1](a ) shows the substrate monomer density @xmath28 and island density @xmath29 ( averaged over 500 runs ) as a function of coverage in the first half - layer of growth , while fig .",
    "[ figaccuracy1](b ) shows the r.m.s .",
    "surface height fluctuations or surface width ( averaged over 100 runs ) as a function of coverage in the first few layers of growth .",
    "the inset of fig .",
    "[ figaccuracy1](b ) also shows the monomer density as a function of coverage in the first 5 layers of growth .",
    "as can be seen , there is no difference within statistical error between the serial and the parallel results .",
    "comparison between serial and parallel results using synchronous sublattice algorithm with strip decomposition ( @xmath30 ) for fractal model with @xmath0.,width=226 ]    a similar comparison is shown in fig .",
    "[ figaccuracy2 ] for the edge - diffusion ( ec ) model ( @xmath31 ) using a strip sublattice decomposition . as can be seen there is again no difference between the parallel and serial results .",
    "comparison between serial and parallel results using synchronous sublattice algorithm with strip decomposition for ec model with @xmath0 , @xmath23 , and @xmath32 , @xmath33.,width=226 ]      we now consider the performance of the synchronous sublattice algorithm , starting with the dependence of the parallel efficiency on the monomer diffusion rate @xmath15 for the fractal model for a fixed number of processors ( @xmath34 ) . here",
    "we define the parallel efficiency as equal to the ratio of the execution time for an ordinary serial simulation of one processor s domain ( using the same sublattice decomposition as in the parallel simulation ) to the parallel execution time of @xmath4 domains using @xmath4 processors .",
    "thus , the overall  performance factor \" of the parallel simulation ( e.g. boost in events / sec over a serial simulation ) is given by the parallel efficiency multiplied by the number of processors @xmath4 .",
    "there are two primary factors which determine the parallel efficiency .",
    "the first is the overhead due to communications at the end of every cycle , when all processors exchange boundary information with their neighbors .",
    "since in our simulations the number of boundary events is relatively small ( i.e. the processor size is not too large ) the primary cause of communications overhead is the latency time for local communications which is independent of processor domain size .",
    "the second important factor controlling the efficiency is the existence of fluctuations in the number of events in different processors . in particular , in any given cycle one processor may have many events , while its nearest - neighbor may have fewer events . as a result , while the processor with many events is calculating its events , its neighboring processor with few events must idle ( wait ) until it has received the boundary information from the first processor before moving to the next cycle .    to illustrate this effect more quantitatively ,",
    "we consider the effects of fluctuations on the parallel efficiency in the case of the one - dimensional strip sublattice decomposition shown in fig . [ figsub ]",
    "( b ) . in this case",
    "there are two sublattices ( a and b ) and during each cycle one of the sublattices is randomly selected .",
    "for example , if the b sublattice is selected , then at the end of a cycle all processors will do a ( non - blocking ) send of any boundary events in the b sublattice to the processor on their right , followed by a ( blocking ) receive of the corresponding boundary information from the processor on their left . thus , for example , if processor 1 has more events than processor 2 , and so takes longer to execute these events before initiating its send to processor 2 , then processor 2 must wait before moving to the next cycle , thus leading to inefficiency .",
    "however , processor 2 s execution is not affected by processor 3 during the same cycle , since its send to processor 3 is non - blocking .    denoting the communication overhead per cycle as @xmath35 and taking into account the fluctuations of events between nearest - neighbor processors , we obtain the following expression for the average time per cycle : @xmath36 where @xmath37 is the time for a single processor serial simulation of a single processor s domain , @xmath38 is the average number of events per processor per cycle , @xmath39 is the relevant fluctuation in a given cycle @xmath40 ( averaged over all processors ) , and the brackets denote an average over all cycles .",
    "the ratio @xmath41 in the last term of eq .",
    "1 corresponds to the average calculation time to process an event .",
    "therefore , the parallel efficiency @xmath42 may be written as @xmath43^{-1 } \\label{pesublattice}\\ ] ] in the limit of negligible communication time @xmath44 , this implies that the maximum possible parallel efficiency is given by , @xmath45^{-1 }   \\label{pesublatticemax}\\ ] ]    we also note that @xmath46 and since the fluctuations are _ on average _ uncorrelated , one expects @xmath47 .",
    "this implies that the maximum possible parallel efficiency may be written as , @xmath48^{-1 } \\label{pesublatticemax2}\\ ] ] where the constant @xmath49 is model - dependent .",
    "this result shows clearly that the maximum theoretical efficiency approaches @xmath8 in the limit of large @xmath38 corresponding to large @xmath50 .",
    "there are two distinct ways in which the average fluctuation @xmath51 might be calculated .",
    "if we assume that at the beginning of each cycle all processors are perfectly synchronized , then for the strip geometry one may write , @xmath52 where @xmath53 is the number of events in processor @xmath19 in cycle @xmath40 , @xmath54 if @xmath55 is negative ( positive ) and @xmath56 if the a ( b ) sublattice is selected in cycle @xmath40 . since we are interested in the average over many cycles , this is equivalent to the simpler form , @xmath57 where the factor of @xmath58 is due to the fact that only half the time will the relative fluctuation in the relevant neighboring processor be positive , and thus lead to a delay",
    ".    however , due to fluctuations one must also take into account the existence of desynchronization at the beginning of a cycle .",
    "in order to take this into account , we may calculate the sum or  starting time \" @xmath59 corresponding to the sum of the total number of events in processor @xmath19 and the sum of all delay - events due to neighboring processors in a given processor @xmath19 at the start of cycle @xmath40 . at the start of the first cycle ( @xmath60 )",
    "one has @xmath61 for all processors @xmath19 and @xmath62 is the number of events in processor @xmath19 in that cycle . at the start of each subsequent cycle , the sum @xmath59 may be calculated in each processor in terms of the previous values of @xmath63 and @xmath64 as follows , @xmath65 where @xmath66 and where @xmath56 if the a ( b ) sublattice is selected in cycle @xmath40 .",
    "then the average delay @xmath67 due to fluctuations in a given cycle @xmath40 may be written , @xmath68    fig .",
    "[ figflucdf ] shows the measured fluctuations @xmath69 and @xmath70 for the simple fractal model as a function of @xmath15 for fixed processor size @xmath71 , @xmath72 and @xmath34 . as can be seen , for @xmath34 ,",
    "the full fluctuation @xmath73 is approximately 30% larger than that obtained assuming that all processors are synchronized at the beginning of each cycle . for the simple fractal model , one expects @xmath74 which implies @xmath75 . as can be seen in fig .",
    "[ figflucdf ] , there is very good agreement with this form for the @xmath15-dependence .",
    "fluctuations as function of @xmath15 for fractal model with @xmath34 and strip geometry with @xmath71 , @xmath72.,width=321 ]    fig .",
    "[ figfracpedf ] shows the corresponding results ( symbols ) for the parallel efficiency as a function of the ratio @xmath15 .",
    "results are shown for parallel kmc simulations with @xmath34 of a square system with system size @xmath76 with both square sublattice decomposition ( @xmath77 ) and strip sublattice decomposition ( @xmath78 ) . due to the decreased communication overhead in the strip - geometry ( 1 send / receive versus 2",
    "send / receives ) the parallel efficiency of the strip geometry simulations is significantly larger than for the square geometry .",
    "as can be seen , for @xmath79 , the parallel efficiency for the strip geometry is greater than 50% .",
    "however , with increasing @xmath15 the parallel efficiency decreases significantly since the decrease in the number of events per cycle @xmath38 ( see fig .",
    "[ figfracpedf ] ( a ) ) leads to an increase in the communications overhead @xmath80 as well as in the relative fluctuations @xmath81 .",
    "also shown in fig .",
    "[ figfracpedf ] ( dashed lines ) is the parallel efficiency calculated using eq .  2 based on the measured values of @xmath82 , @xmath41 , and the measured interprocessor communication time @xmath83 per",
    "send / receive .",
    "as can be seen , there is good agreement between the measured and calculated results .",
    "the maximum theoretical parallel efficiencies calculated using eq .",
    "3 assuming negligible communication overhead are also shown ( solid lines ) .",
    "as can be seen , the maximum theoretical efficiencies are significantly higher than the measured efficiencies for large @xmath15 , although they also decrease with increasing @xmath15 due to the increase in fluctuations . for the simple fractal model with strip - geometry ,",
    "our results for the maximum possible parallel efficiency may be well described by the expression , @xmath84^{-1 } \\label{pesublattice3}\\ ] ] this result may be used to estimate the maximum possible efficiency for the fractal model for different processor sizes and values of @xmath15 .",
    "for example , if @xmath85 and @xmath0 , then this implies a maximum possible parallel efficiency given by @xmath86 .",
    "this result shows that even in the absence of delays due to interprocessor communication , due to the existence of fluctuations , the parallel efficiency will decrease with increasing @xmath15 .",
    "( a ) events per cycle and ( b ) parallel efficiency for fractal model with @xmath34 as function of d / f .",
    "dashed lines correspond to eq .",
    "[ pesublattice ] while solid lines correspond to maximum theoretical efficiency given by eq .",
    "[ pesublatticemax ] . ,",
    "width=321 ]    fig .",
    "[ figedgepedf ] shows similar results for the parallel efficiency as a function of @xmath15 for the edge - diffusion model with @xmath87 , @xmath34 and @xmath88 .",
    "as can be seen , although the parallel efficiency for the edge - diffusion model still decreases with increasing @xmath15 , it is significantly larger than for the fractal model . in particular , due to the increased number of events per cycle and the resulting reduced communication overhead , the parallel efficiency remains above 50% for large @xmath15 . as an example , for the case of strip - geometry and @xmath89 , the parallel efficiency is more than 3 times that for the simple fractal model , while the number of events is approximately 10 times larger . as for the fractal model , the calculated parallel efficiency ( dashed lines )",
    "is in good agreement with the measured values . due to the increase in the number of events per cycle , and the resulting decrease in the relative fluctuations , the maximum theoretical efficiencies ( solid lines ) are also significantly higher than for the fractal case .",
    "( a ) events per cycle and ( b ) parallel efficiency for edge - diffusion model with @xmath34 as function of @xmath15 .",
    "dashed lines correspond to eq .",
    "[ pesublattice ] while solid lines correspond to maximum theoretical efficiency given by eq .",
    "[ pesublatticemax ] . , width=321 ]       total computational speed ( events / sec ) as function of number of processors for fractal model with @xmath0.,width=321 ]    fig .",
    "[ figcompspeed ] shows the performance ( events / sec ) for the simple fractal model with @xmath0 as a function of the number of processors @xmath4 with fixed processor size , using both square decomposition with @xmath90 and strip decomposition with @xmath71 and @xmath72 . as can be seen in both cases there",
    "is a roughly linear speedup of the performance with increasing number of processors @xmath4 . for comparison ,",
    "the equivalent single - processor ( serial ) computation speed for this model is approximately @xmath91 events / sec .",
    "however , due to the decreased communication cost , the speed - up using the strip geometry is significantly higher than for the square decomposition .",
    "we now consider the dependence of the parallel efficiency on the number of processors @xmath4 in more detail for the case of strip geometry .",
    "[ figflucnp ] shows the measured fluctuations @xmath69 and @xmath70 as a function of @xmath4 for the fractal model for @xmath0 . with increasing @xmath4 ,",
    "the relative event - fluctuation @xmath70 remains constant .",
    "in contrast , the full fluctuation @xmath92 increases slowly with increasing @xmath4 but appears to saturate at a finite value for large @xmath4 .",
    "this is supported by the fit shown in fig .",
    "[ figflucnp ] ( solid line ) which agrees quite well with the simulation results and which has the form , @xmath93 . due to the saturation of fluctuations",
    ", we expect that the parallel efficiency will also saturate for large @xmath4 .",
    "fluctuations for fractal model with @xmath0 as function of @xmath4 .",
    "solid line corresponds to fit of form @xmath94,width=321 ]    fig .",
    "[ figpenp ] shows our results for the measured parallel efficiency ( open and closed symbols ) as a function of @xmath4 for fixed processor size for the fractal and edge - diffusion models .",
    "as expected , the parallel efficiency is essentially constant for large @xmath4 , although there is a slight decrease due to increased communication overhead for @xmath95 . also shown ( dashed lines ) are the parallel efficiencies calculated from eq .",
    "[ pesublattice ] using the measured fluctuations and communication times , as well as the maximum possible theoretical parallel efficiencies ( solid lines ) calculated using eq .",
    "[ pesublatticemax ] .",
    "as can be seen , there is relatively good agreement between the calculated and measured parallel efficiencies .",
    "we note that the results for large @xmath4 ( open symbols ) were obtained using the alpha cluster at the pittsburgh supercomputer center ( psc ) for which the communication latency is somewhat larger than for the osc cluster . as a result",
    "the parallel efficiencies are somewhat lower than would be obtained with the osc cluster . for comparison ,",
    "osc results for the fractal model with @xmath96 and with @xmath78 are also shown up to @xmath97 ( filled symbols ) . as can be seen , due to the decreased communication time",
    ", the osc results for the parallel efficiency are significantly larger than the corresponding psc results . except for the psc fractal results with @xmath96 , the parallel efficiencies are all larger than 50% .",
    "parallel efficiency ( symbols ) as function of number of processors @xmath4 for fractal and edge - diffusion models with @xmath0 and strip - sublattice decomposition ( @xmath71 with @xmath26 and @xmath98 ) .",
    "dashed lines correspond to eq .",
    "[ pesublattice ] while solid lines correspond to maximum theoretical efficiency calculated using eq .",
    "[ pesublatticemax ] .",
    ", width=321 ]      we now consider the effects of finite processor size on the accuracy of the results obtained using the synchronous sublattice algorithm . for simplicity",
    "we focus on the case of strip - sublattice decomposition .",
    "as we have already shown ( see figs .",
    "[ figaccuracy1 ] and [ figaccuracy2 ] ) , for sublattice sizes which are not too small , there is perfect agreement between the synchronous sublattice results and the corresponding serial results .",
    "however , for very small processor sizes there exists a small  finite - size \" effect which leads to results which are slightly different from those obtained using the usual serial kmc algorithm .",
    "in particular , as shown in fig .",
    "[ figfse1 ] , there is essentially perfect agreement between the synchronous sublattice results for the fractal model with system size @xmath23 , @xmath0 , and @xmath99 and the corresponding serial results .",
    "however , for the smallest processor size ( @xmath100 ) there is approximately a 2% difference between the synchronous sublattice results for the peak island density @xmath29 and the corresponding serial results ( although there are no differences in the monomer density @xmath28 ) . in order to compare these effects with those of finite system size , in fig .",
    "[ figfse1 ] ( c ) and ( d ) we also show the corresponding serial results for systems of size @xmath101 and @xmath26 .",
    "as can be seen , the finite - size effects which occur for small system size are much larger than those due to finite processor size .",
    "this indicates that the effects of finite processor size are both qualitatively different as well as much weaker than those due to finite - system size .",
    "finite - size effects in parallel and serial simulations of fractal model with @xmath0 .",
    "parallel simulations ( averaged over 200 runs ) are for system size @xmath102 with processor sizes @xmath103 , and @xmath104 and @xmath26 .",
    "serial simulations ( averaged over 500 runs ) are for system size @xmath105 . ,",
    "width=321 ]     diffusion length @xmath106 in parallel and serial simulations of fractal model ( 200 runs ) and edge - diffusion model ( 100 runs ) with @xmath0 and @xmath107 ( open symbols ) .",
    "all simulations are for system size @xmath23 with processor sizes @xmath108 , and @xmath109 and @xmath26 .",
    "horizontal dashed lines correspond to error bars for serial simulations . , width=321 ]    we now consider these finite - size effects in somewhat more detail .",
    "while a variety of length scales ( such as the typical mound or feature size in multilayer growth ) may occur in the models studied here , there is one _ dynamical _ length scale corresponding to the  diffusion length \" @xmath110 ( e.g. the typical distance a monomer may diffuse before being captured ) which plays a particularly important role . in particular , the diffusion length may be written in terms of the peak submonolayer island density , i.e. @xmath111 . since in the synchronous sublattice algorithm , particles which diffuse outside the active sublattice are no longer active during that cycle , we conjecture that for sublattice sizes @xmath112 which are smaller than the diffusion length @xmath110 , finite - size effects may occur . for the fractal model studied here",
    ", one has @xmath113 which implies @xmath114 , e.g. the diffusion length increases slowly with increasing @xmath15 . as shown in fig .",
    "[ figfse2 ] , by measuring the peak island density for @xmath0 , we obtain @xmath115 which implies a critical processor size @xmath116 given by @xmath117 .",
    "this result is in good agreement with the observation of the onset of significant finite - size effects for @xmath118 .    also shown in fig .",
    "[ figfse2 ] are similar results for the edge - diffusion model with @xmath0 and @xmath119 . in this case",
    "the diffusion length is slightly higher than for the fractal model .",
    "however , again there are no finite - size effects for @xmath120 .",
    "similar results have also been obtained for the reversible one - bond detachment model , as well as a reversible bond - counting model ( not shown ) . in all cases , we find that there are no differences between the serial results and the parallel kmc results as long as @xmath121 .",
    "in contrast , for @xmath122 , noticeable but weak finite - size effects are observed .",
    "while these results are for a cycle length @xmath107 given by the inverse of the fastest hopping rate , for a smaller cycle - length we expect that the critical processor size @xmath116 corresponding to finite - size effects will be significantly reduced .",
    "as shown in fig .",
    "[ figfse2 ] ( filled symbols ) for the fractal model with cycle length @xmath123 , the critical processor size @xmath116 is significantly smaller than the diffusion length @xmath110 .",
    "however , for such a reduced cycle length , the parallel efficiency is also significantly reduced .    as a further test of both the parallel efficiency and finite - size effects in the sl algorithm ,",
    "we have carried out multilayer simulations of the reversible model at @xmath124 k , with system size @xmath76 , @xmath0 , @xmath125 ev , and an ehrlich - schwoebel barrier to interlayer diffusion given by @xmath126 ev . in our simulations particles",
    "freshly deposited near step - edges are assumed to  cascade \" randomly to the nearest - neighbor sites below (  knockout \" ) .",
    "fig [ figmultilayer ] ( a ) shows serial results ( solid line ) for the r.m.s .",
    "surface height fluctuations ( surface width ) and monomer density up to 500 ml along with the corresponding parallel results obtained using the sl algorithm with processor sizes @xmath72 and @xmath127 , and @xmath109 corresponding to @xmath128 , and @xmath11 respectively .",
    "as can be seen , there is no difference between the serial and parallel results even though the typical mound size of approximately @xmath129 lattice units ( see fig . [ figmultilayer](b ) ) is significantly larger than the smallest sublattice size @xmath130 .",
    "this indicates that the relevant length - scale determining the existence of finite - size effects in the sl algorithm is indeed the diffusion length @xmath110 and not the characteristic feature size .",
    "since in these simulations the total system size @xmath131 was fixed , the parallel efficiency may be written , @xmath132 where @xmath37 is the calculation time for a serial simulation of the entire system . since the processor size decreases with increasing @xmath4 , both the relative magnitude of event fluctuations and the overhead due to communication latency will also increase . as a result , the parallel efficiency decreases with increasing @xmath4 rather than saturating as in the case of fixed processor - size .",
    "the parallel efficiencies obtained in these simulations were 92% ( @xmath34 ) , 85% ( @xmath133 ) , and 70% ( @xmath134 ) respectively .",
    "( a ) comparison between serial and parallel results for reversible model with @xmath135 ev and @xmath136 ( @xmath137 ) .",
    "( b ) gray - scale plot of @xmath138 portion of system at @xmath139 ml . , title=\"fig:\",width=321 ] 0.25 truein   ( a ) comparison between serial and parallel results for reversible model with @xmath135 ev and @xmath136 ( @xmath137 ) .",
    "( b ) gray - scale plot of @xmath138 portion of system at @xmath139 ml . , title=\"fig:\",width=245 ]",
    "we have developed and tested a synchronous sublattice ( sl ) algorithm for parallel kinetic monte carlo simulations . in our algorithm , the maximum cycle length @xmath3 is given by the inverse of the fastest diffusion rate .",
    "for sublattice sizes which are smaller than the diffusion length @xmath110 , weak finite - size effects are observed which lead to deviations from the results obtained using a serial algorithm .",
    "however , for sublattice sizes larger than the diffusion length @xmath110 , the results obtained are identical to those obtained in serial simulations .",
    "since in many systems of interest the diffusion length is typically relatively small ( e.g. of the order of a few to a few tens of lattice spacings ) while significantly larger system sizes are needed to avoid finite system - size effects , the sl algorithm should provide a useful , efficient , and accurate method to carry out parallel kmc simulations of these systems .",
    "we have also measured the parallel efficiency of the sl algorithm as a function of the number of processors @xmath4 for fixed processor size . because the sl algorithm is synchronous , the parallel efficiency is affected by fluctuations in the number of events in different processors over a given cycle . however , because only local communications are required , these fluctuations saturate as the number of processors increases . as a result , linear scaling behavior for the total speedup as a function of the number of processors is observed , e.g. the parallel efficiency is independent of the number of processors in the large @xmath4 limit .    for the simple models we have studied here ,",
    "the calculation time for a single event such as diffusion or deposition is significantly smaller than the latency time for nearest - neighbor communication . as a result ,",
    "the parallel efficiency increases with processor size , since the communications overhead per event is reduced by the increased number of events in a cycle .",
    "however , even for relatively modest processor sizes , we have obtained reasonable values for the asymptotic parallel efficiency @xmath42 ranging from 50% for the fractal model with @xmath0 and @xmath96 , to 70% for the fractal model with @xmath71 , @xmath72 . for the slightly more complicated edge - diffusion ( ec ) model ,",
    "for which the number of events per cycle is larger , significantly larger efficiencies are obtained for the same processor size , e.g. 60% for @xmath96 and @xmath0 .",
    "similarly , for the reversible model , we have obtained a parallel efficiency @xmath140 for the same effective processor size ( @xmath141 ) with @xmath134 .",
    "we have also studied the effects of fluctuations on the parallel efficiency in our simulations . in particular",
    ", we found that the relevant relative fluctuations @xmath70 scale as one over the square root of the processor size ( see eq.[pesublattice3 ] and fig.[figflucdf ] ) . by taking into account the effects of fluctuations and communication delays ,",
    "calculated parallel efficiencies were obtained which are in excellent agreement with those obtained in our simulations .",
    "in addition , by measuring the relevant fluctuations , we have been able to predict the maximum possible theoretical efficiencies in the absence of communication delays .",
    "for example , for the fractal and edge - diffusion models with @xmath0 and @xmath96 , maximum theoretical parallel efficiencies of 80% and 90% respectively were obtained .    since increasing the processor size decreases the effects of fluctuations as well as communications overhead , the parallel efficiency may be further increased by increasing the processor size .",
    "alternatively , in simulations on machines with faster communications ( such as shared memory machines ) or in simulations of more complicated kmc models for which the calculation time is significantly larger ( such as self - learning kmc@xcite or accelerated dynamics@xcite ) efficiencies approaching these values may be possible even without increasing the processor size .",
    "it is worth noting that in our simulations we have used two slightly different definitions for the parallel efficiency . in the first definition ( eq . [ pesublattice ] ) , the parallel execution time was compared with the serial execution time of a system whose size is the same as a single processor .",
    "in contrast , in the second definition ( eq . [ pesublattice2 ] )",
    "the parallel execution time was directly compared with @xmath142 times the serial execution time of a system whose total system size is the same as in the parallel simulation .",
    "if the serial kmc calculation time per event is independent of system size , then there should be no difference between the two definitions .",
    "however , in general this may not be the case .",
    "for example , in kmc simulations in which the rates for all events are stored separately , the calculation time per event will increase as @xmath143 ( where @xmath144 is the system size ) using the maksym algorithm@xcite and as log(@xmath145 ) using a binary tree algorithm.@xcite in this case , the parallel efficiency calculated using eq .",
    "[ pesublattice2 ] may be significantly higher than that obtained using eq .",
    "[ pesublattice ] , and may even be larger than @xmath8 , since the division of a system into smaller subsystems may reduce the calculation time per event per processor .",
    "since in the models studied here we have used lists for each type of event , rather than the  partial - sum \" algorithms described above , we would expect the serial calculation time per event to be independent of system size , and thus the two definitions of parallel efficiency should be equivalent . to test",
    "if this is the case , we have calculated the serial simulation time per event for the fractal model for @xmath146 and @xmath0 for a variety of system sizes ranging from @xmath147 to @xmath148 .",
    "somewhat surprisingly , we found that the serial calculation time per event increases slowly with increasing processor size .",
    "in particular , an increase of approximately 50% in the calculation time per event was obtained when going from a system of size @xmath147 to @xmath148 .",
    "we believe that this is most likely due to memory or  cache \" effects in our simulations .",
    "this increase in the serial calculation time per event with increasing system size indicates that the calculated parallel efficiencies shown in fig .",
    "[ figpenp ] would actually be somewhat larger if the more direct definition of parallel efficiency ( eq . [ pesublattice2 ] ) were used . however , since for large @xmath4 this requires serial simulations of very large systems ( e.g. @xmath149 for @xmath150 ) , the first definition ( eq . [ pesublattice ] ) was used .",
    "finally , we return to the general question of the applicability and validity of the sl algorithm .",
    "in general , we expect that for a wide class of non - equilibrium processes there exists a clearly defined diffusion length @xmath110 , which may or may not vary slowly with time . for these processes",
    "we expect that finite - size effects will not occur as long as the sublattice size is larger than @xmath110 .",
    "furthermore , as long as this length scale is not too large compared to the desired system size , parallel simulations using the sl algorithm will be advantageous . as our results indicate , parallel kmc simulations using the synchronous sublattice algorithm are in general likely to be significantly faster than either the conservative asynchronous algorithm or the synchronous relaxation algorithm . as a result , we expect that the synchronous sublattice algorithm may be particularly useful as a means to carry out a variety of parallel non - equilibrium simulations .",
    "this research was supported by the nsf through grant no .",
    "we would also like to acknowledge grants of computer time from the ohio supercomputer center ( grant no .",
    "pjs0245 ) and the pittsburgh supercomputer center ( grant no .",
    "dmr030007jp ) .",
    "g. korniss , m.a .",
    "novotny , z. toroczkai , and p.a .",
    "rikvold , in _ computer simulated studies in condensed matter physics xiii _ ,",
    "landau , s.p .",
    "lewis and h .- b .",
    "schuttler eds .",
    "springer proceedings in physics , vol .",
    "86 ( springer - verlag , berlin heidelberg , 2001 ) ."
  ],
  "abstract_text": [
    "<S> the standard kinetic monte carlo algorithm is an extremely efficient method to carry out serial simulations of dynamical processes such as thin - film growth . </S>",
    "<S> however , in some cases it is necessary to study systems over extended time and length scales , and therefore a parallel algorithm is desired . here </S>",
    "<S> we describe an efficient , semi - rigorous synchronous sublattice algorithm for parallel kinetic monte carlo simulations . </S>",
    "<S> the accuracy and parallel efficiency are studied as a function of diffusion rate , processor size , and number of processors for a variety of simple models of epitaxial growth . </S>",
    "<S> the effects of fluctuations on the parallel efficiency are also studied . since only local communications are required , </S>",
    "<S> linear scaling behavior is observed , e.g. the parallel efficiency is independent of the number of processors for fixed processor size . </S>"
  ]
}