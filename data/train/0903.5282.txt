{
  "article_text": [
    "in recent years , cognitive radio has attracted extensive studies in the community of wireless communications .",
    "it allows users without license ( called secondary users ) to access licensed frequency bands when the licensed users ( called primary users ) are not present .",
    "therefore , the cognitive radio technique can substantially alleviate the problem of under - utilization of frequency spectrum @xcite@xcite .",
    "the following two problems are key to the cognitive radio systems :    * resource mining , i.e. how to detect the available resource ( the frequency bands that are not being used by primary users ) ; usually it is done by carrying out spectrum sensing .",
    "* resource allocation , i.e. how to allocate the detected available resource to different secondary users .",
    "substantial work has been done for the resource mining .",
    "many signal processing techniques have been applied to sense the frequency spectrum @xcite , e.g. cyclostationary feature @xcite , quickest change detection @xcite , collaborative spectrum sensing @xcite . meanwhile , plenty of researches have been conducted for the resource allocation in cognitive radio systems @xcite @xcite .",
    "typically , it is assumed that the secondary users exchange information about detected available spectrum resources and then negotiate the resource allocation according to their own requirements of traffic ( since the same resource can not be shared by different secondary users if orthogonal transmission is assumed ) .",
    "these studies typically apply theories in economics , e.g. game theory , bargaining theory or microeconomics .",
    "however , in many applications of cognitive radio , such a negotiation based resource allocation may incur significant overhead . in traditional wireless communication systems , the available resource is almost fixed ( even if we consider the fluctuation of channel quality , the change of available resource is still very slow and thus can be considered stationary ) . therefore , the negotiation need not be carried out frequently and the negotiation result can be applied for a long period of data communication , thus incurring tolerable overhead . however , in many cognitive radio systems , the resource may change very rapidly since the activity of primary users may be highly dynamic . therefore , the available resource needs to be updated very frequently and the data communication period should be fairly short since minimum violation to primary users should be guaranteed .",
    "in such a situation , the negotiation of resource allocation may be highly inefficient since a substantial portion of time needs to be used for the negotiation .",
    "to alleviate such an inefficiency , high speed transceivers need to be used to minimize the time consumed on negotiation .",
    "particularly , the turn - around time , i.e. the time needed to switch from receiving ( transmitting ) to transmitting ( receiving ) should be very small , which is a substantial challenge to hardware design .",
    "motivated by the previous discussion and observation , in this paper , we study the problem of spectrum access without negotiation in multi - user and multi - channel cognitive radio systems . in such a scheme ,",
    "each secondary user senses channels and then choose an idle frequency channel to transmit data , as if no other secondary user exists .",
    "if two secondary users choose the same channel for data transmission , they will collide with each other and the data packets can not be decoded by the receiver(s ) .",
    "such a procedure is illustrated in fig .",
    "[ fig : compete ] , where three secondary users access an access point via four channels .",
    "since there is no mutual communication among these secondary users , conflict is unavoidable .",
    "however , the secondary users can try to learn how to avoid each other , as well as channel qualities ( we assume that the secondary users have no _ a priori _ information about the channel qualities ) , according to its experience .",
    "in such a context , the cognition procedure includes not only the frequency spectrum but also the behavior of other secondary users .    to accomplish the task of learning channel selection",
    ", multi - agent reinforcement learning ( marl ) @xcite is a powerful tool .",
    "one challenge of marl in our context is that the secondary users do not know the payoffs ( thus the strategy ) of each other in each stage ; thus the environment of each secondary user , including its opponents , is dynamic and may not assure convergence of learning .",
    "in such a situation , fictitious play @xcite@xcite , which estimates other users strategy and plays the best response , can assure convergence to a nash equilibrium point within certain assumptions . as an alternative way",
    ", we adopt the principle of q - learning , i.e. evaluating the values of different actions in an incremental way .",
    "for simplicity , we consider only the case of two secondary users and two channels . by applying the theory of stochastic approximation @xcite",
    ", we will prove the main result of this paper , i.e. the learning converges to a stationary point regardless of the initial strategies ( propositions [ prop : eqi_ode ] and [ prop : conv_ode ] ) .",
    "note that our study is one extreme of the resource allocation problem since no negotiation is considered while the other extreme is full negotiation to achieve optimal performance .",
    "it is interesting to study the intermediate case , i.e. limited negotiation for resource allocation .",
    "however , it is beyond the scope of this paper .",
    "the remainder of this paper is organized as follows . in section [ sec :",
    "model ] , the system model is introduced .",
    "the proposed q - learning for channel selection is explained in section [ sec : q ] .",
    "intuitive explanation and rigorous proof for convergence are explained in sections [ sec : intuition ] and [ sec : sto_approx ] , respectively .",
    "the numerical results are provided in section [ sec : numerical ] while the conclusions are drawn in section [ sec : conclusion ] .",
    "for simplicity , we consider only two secondary users , denoted by @xmath0 and @xmath1 , and two channels , denoted by 1 and 2 .",
    "the reward to secondary user @xmath2 , @xmath3 , of channel @xmath4 , @xmath5 , is @xmath6 if secondary user transmits data over channel @xmath4 and channel @xmath4 is not interrupted by primary user or the other secondary user ; otherwise the reward is 0 since the secondary user can not convey any information over this channel . for simplicity ,",
    "we denote by @xmath7 the other user ( channel ) different from user ( channel ) @xmath4 .",
    "the following assumptions are placed throughout this paper .    * the rewards @xmath8 are unknown to both secondary users .",
    "they are fixed throughout the game . *",
    "both secondary users can sense both channels simultaneously , but can choose only one channel for data transmission .",
    "it is more interesting and challenging to study the case that the secondary users can sense only one channel , thus forming a partially observable game .",
    "however , it is beyond the scope of this paper .",
    "* we consider only the case that both channels are available since the actions that the secondary users can take are obvious ( transmit over the only available channel or not transmit if no channel is available ) .",
    "thus , we ignore the task of sensing the frequency spectrum , which has been well studied by many researchers , and focus on only the cognition of the other secondary user s behavior .",
    "* there is no communication between the two secondary users .",
    "in this section , we introduce the corresponding game and the application of q - learning to the channel selection problem .      the channel selection problem is a @xmath10 game , in which the payoff matrices are given in fig .",
    "[ fig : game ] .",
    "note that the actions , denoted by @xmath11 for user @xmath2 at time @xmath12 , in the game are the selections of channels .",
    "obviously , the diagonal elements in the payoff matrices are all zero since conflict incurs zero reward .",
    "it is easy to verify that there are two nash equilibrium points in the game , i.e. the strategies such that unilaterally changing strategy incurs its own performance degradation .",
    "both equilibrium points are pure , i.e. @xmath13 and @xmath14 ( orthogonal transmission ) .",
    "since we assume that both channels are available , then there is only one state in the system .",
    "therefore , the @xmath9-function is simply the expected reward of each action ( note that , in traditional learning in stochastic environment , the @xmath9-function is defined over the pair of state and action ) , i.e. @xmath15,\\end{aligned}\\ ] ] where @xmath16 is the action , @xmath17 is the reward dependent on the action and the expectation is over the randomness of the other user s action . since the action is the selection of channel , we denote by @xmath18 the value of selecting channel @xmath4 by secondary user @xmath2 .",
    "in contrast to fictitious play @xcite , which is deterministic , the action in @xmath9-learning is stochastic to assure that all actions will be tested .",
    "we consider boltzmann distribution for random exploration , i.e. @xmath19 where @xmath20 is called temperature , which controls the frequency of exploration .",
    "obviously , when secondary user @xmath2 selects channel @xmath4 , the expected reward is given by @xmath21=\\frac{r_{ij}e^{q_{i^-j^-}/\\gamma}}{e^{q_{i^-j}/\\gamma}+e^{q_{i^-j^-}/\\gamma}},\\end{aligned}\\ ] ] since secondary user @xmath22 chooses channel @xmath4 with probability @xmath23 ( collision happens and secondary user @xmath2 receives no reward ) and channel @xmath7 with probability @xmath24 ( the transmissions are orthogonal and secondary user @xmath2 receives reward @xmath6 ) .      in the procedure of @xmath9-learning , the @xmath9-functions are updated after each spectrum access via the following rule : @xmath25 where @xmath26 is a step factor ( when channel @xmath4 is not selected by user @xmath2 , @xmath27 ) and @xmath28 is the reward of secondary user @xmath2 and @xmath29 is characteristic function for the event that channel @xmath4 is selected at the @xmath12-th spectrum access .",
    "our study is focused on the dynamics of ( [ eq : q_learning ] ) . to assure convergence , we assume that @xmath30",
    "as will be shown in propositions [ prop : eqi_ode ] and [ prop : conv_ode ] , the updating rule of @xmath9 functions in ( [ eq : q_learning ] ) will converge to a stationary equilibrium point close to nash equilibrium if the step factor satisfies certain conditions . before the rigorous proof",
    ", we provide an intuitive explanation for the convergence using the geometric argument proposed in @xcite .",
    "the intuitive explanation is provided in fig .",
    "[ fig : graph ] ( we call it _ metrick - polak plot _ since it was originally proposed by a. metrick and b. polak in @xcite ) , where the axises are @xmath31 and @xmath32 , respectively . as labeled in the figure ,",
    "the plane is divided into four regions by two lines @xmath33 and @xmath34 , in which the dynamics of @xmath9-learning are different .",
    "we discuss these four regions separately :    * region i : in this region , @xmath35 ; therefore , secondary user @xmath0 prefers visiting channel 1 ; meanwhile , secondary user @xmath1 prefers accessing channel 2 since @xmath36 ; then , with large probability , the strategies will converge to the nash equilibrium point in which secondary users @xmath0 and @xmath1 access channels 1 and 2 , respectively .",
    "* region ii : in this region , both secondary users prefer accessing channel 1 , thus causing many collisions . therefore",
    ", both @xmath37 and @xmath38 will be reduced until entering either region i or region iii .",
    "* region iii : similar to region i. * region iv : similar to region ii .",
    "then , we observe that the points in regions ii and iv are unstable and will move into region i or iii with large probability . in regions",
    "i and iii , the strategy will move close to the nash equilibrium points with large probability .",
    "therefore , regardless where the initial point is , the updating rule in ( [ eq : q_learning ] ) will generate a stationary equilibrium point with large probability .",
    "in this section , we prove the convergence of the @xmath9-learning . first , we find the equivalence between the updating rule ( [ eq : q_learning ] ) and robbins - monro iteration @xcite for solving an equation with unknown expression .",
    "then , we apply the conclusion in stochastic approximation @xcite to relate the dynamics of the updating rule to an ordinary differential equation ( ode ) and prove the stability of the ode .      at a stationary point ,",
    "the expected values of @xmath9-functions satisfy the following four equations : @xmath39    define @xmath40 .",
    "then ( [ eq : equil_equa0 ] ) can be rewritten as @xmath41 where @xmath42 and the matrix @xmath43 ( as a function of @xmath44 ) is given by @xmath45    then , the updating rule in ( [ eq : q_learning ] ) is equivalent to solving the equation ( [ eq : equil_equa ] ) ( the expression of the equation is unknown since the rewards , as well as the strategy of the other user , are unknown ) using robbins - monro algorithm @xcite , i.e. @xmath46 where @xmath47 is a random observation on function @xmath48 contaminated by noise , i.e. @xmath49 where @xmath50 , @xmath51 is noise and ( recall that @xmath52 means the reward of secondary user @xmath2 at time @xmath12 ) @xmath53      the procedure of using robbins - monro algorithm ( i.e. the updating of @xmath9-function ) is the stochastic approximation of the solution of the equation .",
    "it is well known that the convergence of such a procedure can be characterized by an ode . since the noise @xmath54 in ( [ eq : stoch_approx ] )",
    "is a martingale difference , we can verify the conditions in theorem 12.3.5 in @xcite ( the verification is omitted due to limited length of this paper ) and obtain the following proposition :    [ prop : eqi_ode ] with probability 1 , the sequence @xmath55 converges to some limit set of the ode @xmath56    what remains to do is to analyze the convergence property of the ode ( [ eq : ode ] ) .",
    "we obtain the following proposition :    [ prop : conv_ode ] the solution of ode ( [ eq : ode ] ) converges to the stationary point determined by ( [ eq : equil_equa ] ) .",
    "we apply lyapunov s method to analyze the convergence of the ode ( [ eq : ode ] ) .",
    "we define the lyapunov function as @xmath57    then , we examine the derivative of the lyapunov function with respect to time @xmath12 , i.e. @xmath58 where @xmath59 .",
    "we have @xmath60 where @xmath61 and we applied the ode ( [ eq : ode ] ) .    then , we focus on the computation of @xmath62 . when @xmath63 and @xmath64 , we have @xmath65 where we applied the ode ( [ eq : ode ] ) again .",
    "using similar arguments , we have @xmath66 and @xmath67 and @xmath68    combining the above results , we have @xmath69 where    @xmath70    and    @xmath71    and    @xmath72    and    @xmath73    it is easy to verify that @xmath74    now , we assume that @xmath75 , then @xmath76 . therefore , we have @xmath77 therefore , when @xmath75 , the derivative of the lyapunov function is strictly negative , which implies that the ode ( [ eq : ode ] ) converges to a stationary point .",
    "the final step of the proof is to remove the condition @xmath78 .",
    "this is straightforward since we notice that the convergence is independent of the scale of the reward @xmath6 .",
    "therefore , we can always scale the reward such that @xmath78 .",
    "this concludes the proof .",
    "in this section , we use numerical simulations to demonstrate the theoretical results obtained in previous sections . for all simulations ,",
    "we use @xmath79 , where @xmath80 is the initial learning factor .",
    "figures [ fig : dynamics ] and [ fig : dynamics2 ] show the dynamics of @xmath81 versus @xmath82 of several typical trajectories .",
    "note that @xmath83 in fig .",
    "[ fig : dynamics ] and @xmath84 in fig .",
    "[ fig : dynamics2 ] .",
    "we observe that the trajectories move from unstable regions ( ii and iv in fig .",
    "[ fig : graph ] ) to stable regions ( i and iii in fig .",
    "[ fig : graph ] ) .",
    "we also observe that the trajectories for smaller temperature @xmath20 is smoother since less explorations are carried out .",
    "[ fig : prob ] shows the evolution of the probability of choosing channel 1 when @xmath83 .",
    "we observe that both secondary users prefer channel 1 at the beginning and soon secondary user @xmath0 intends to choose channel 2 , thus avoiding the collision .    -learning . ]    -learning . ]",
    "figures [ fig : cdf1 ] and [ fig : cdf2 ] show the delays of learning ( equivalently , the learning speed ) for different learning factor @xmath80 and different temperature @xmath20 , respectively .",
    "the original @xmath9 values are randomly selected .",
    "when the probabilities of choosing channel 1 are larger than 0.95 for one secondary user and smaller than 0.05 for the other secondary user , we claim that the learning procedure is completed .",
    "we observe that larger learning factor @xmath80 results in smaller delay while smaller @xmath20 yields faster learning procedure .    .",
    "]    . ]      in practical systems , we may not be able to use vanishing @xmath26 since the environment could change ( e.g. new secondary users emerge or the channel qualities change ) .",
    "therefore , we need to set a lower bound for @xmath26 .",
    "similarly , we also need to set a lower bound for the probability of exploring all actions ( notice that the exploration probability in ( [ eq : exploration ] ) can be arbitrarily small ) .",
    "[ fig : fluc ] shows that the learning procedure may yield substantial fluctuation if the lower bounds are improperly chosen ( the lower bounds for @xmath26 and exploration probability are set as 0.4 and 0.2 in fig .",
    "[ fig : fluc ] ) .",
    "we have discussed the @xmath85 case of learning procedure for channel selection without negotiation in cognitive radio systems . during the learning ,",
    "each secondary user considers the channel and the other secondary user as its environment , updates its @xmath9 values and takes the best action .",
    "an intuitive explanation for the convergence of learning is provided using metrick - polak plot . by applying the theory of stochastic approximation and ode ,",
    "we have shown the convergence of learning under certain conditions .",
    "numerical results show that the secondary users can learn to avoid collision quickly . however ,",
    "if parameters are improperly chosen , the learning procedure may yield substantial fluctuation .",
    "a. gahsemi and e. s. sousa , `` collaborative spectrum sensing for opportunistic access in fading environment , '' in _ proc . of ieee international symposium of new frontiers in dynamic spectrum access networks ( dyspan ) _ ,",
    "2005 .",
    "k. kim , i. a. akbar , k. k. bae , et al , `` cyclostationary approaches to signal detection and classificition in cognitive radio , '' in _ proc . of ieee international symposium of new frontiers in dynamic spectrum access networks ( dyspan )",
    "_ , april 2007 .",
    "d. niyato , e. hossain and z. han , `` dynamics of multiple - seller and multiple - buyer spectrum trading in cognitive radio networks : a game theoretic modeling approach , '' to appear in _ ieee trans . mobile computing_."
  ],
  "abstract_text": [
    "<S> resource allocation is an important issue in cognitive radio systems . </S>",
    "<S> it can be done by carrying out negotiation among secondary users . </S>",
    "<S> however , significant overhead may be incurred by the negotiation since the negotiation needs to be done frequently due to the rapid change of primary users activity . in this paper , a channel selection scheme without negotiation is considered for multi - user and multi - channel cognitive radio systems . to avoid collision incurred by non - coordination , </S>",
    "<S> each user secondary learns how to select channels according to its experience . </S>",
    "<S> multi - agent reinforcement leaning ( marl ) is applied in the framework of q - learning by considering the opponent secondary users as a part of the environment . </S>",
    "<S> the dynamics of the q - learning are illustrated using metrick - polak plot . </S>",
    "<S> a rigorous proof of the convergence of q - learning is provided via the similarity between the q - learning and robinson - monro algorithm , as well as the analysis of convergence of the corresponding ordinary differential equation ( via lyapunov function ) . </S>",
    "<S> examples are illustrated and the performance of learning is evaluated by numerical simulations . </S>"
  ]
}