{
  "article_text": [
    "the intention of this article is to make it clear to theorists how to use available experimental data to carry out standard bayesian inference , by which they can estimate and set limits on parameters of interest of their own theoretical models or , depending on the mood , of their friends models .",
    "[ [ disclaimer ] ] disclaimer : + + + + + + + + + + +    the method described in this article is not , currently , an official recommendation of any experimental collaboration .",
    "the author is a high energy experimentalist who writes on his own behalf . the strengths and limitations of the method",
    "will be explained , so , the readers should use their own judgement , as always .",
    "one should never think it s possible to claim a discovery without consulting with the experimental collaboration which produced the data .",
    "if one suspects something significant is seen in some data , it is essential to investigate possible detector effects , or other experimental factors that could explain it , before attributing it to new physics .",
    "interpretations should be discussed with the experimentalists who produced the data you use ! _ the goal of this article is to strengthen the collaboration between theorists and experimentalists , not to let theorists run off with potentially wrong conclusions . _",
    "bayesian inference dates back to the 18th century , and is based on solid theoretical ground : standard probability theory , which underpins bayes theorem .",
    "so , although in the last years it has emerged as the cutting edge of statistics , it is actually very old .",
    "luckily , bayesian inference is very easy to carry out , which makes it possible to propose here a practical procedure that theorists can actually use without complicated software or large computing power .",
    "the fundamental advantage of bayesian inference , compared to frequentist methods , is that it makes statements directly about the * parameter of interest ( poi)*. namely , in the end one finds the * probability density function ( pdf ) * of his poi .",
    "this is not true for any of the frequentist constructions , where confidence intervals are obtained .",
    "there , no statement is made about the probability of the poi to be within any interval .",
    "the coverage of a frequentist confidence interval is a statistical property of the confidence interval itself , _ not _ the probability for the poi to be within that interval , as many wrongly think .",
    "the frequentist approach is to assume various values for the poi , and compute how likely the data would be according to each value , and then set the limit at the value which , if assumed , starts making the observed data very unlikely .",
    "but the fact @xmath1 is small does nt entail that also @xmath2 is small , and what one really asks is the latter and @xmath3 . ] .",
    "the bayesian approach is to assume the data , and find how likely each hypothesis is , thus compute @xmath2 directly .",
    "a necessary ingredient of inference is the prior , which is the pdf assumed for the poi before seeing the data .",
    "this prior pdf could be the posterior of a previous experiment , but the latter too would depend on some prior used to interpret the data of the previous experiment . in the end",
    "it is impossible to avoid the dependence on some prior .",
    "some people , the so - called `` subjective bayesians '' , embrace the prior as a means to express the mathematical fact that the conclusion ( i.e.  the posterior pdf ) does nt depend only on the evidence ( i.e.  the data ) , but also on the initial assumptions under which this evidence is interpreted ( i.e.  the prior pdf ) .",
    "not surprisingly , different people will arrive to different conclusions , or , the same person will arrive to different conclusions , if he starts from different assumptions .",
    "these assumptions do nt have to reflect anyone s subjective distribution of probability , since nobody prohibits asking what the result would be if the prior was different , regardless of personal preferences .    to draw an analogy ,",
    "the posterior is like a function ( @xmath4 ) of the prior ( @xmath5 ) . just like the function @xmath6 could be evaluated at any @xmath5 , a posterior can be computed for any prior . in the case of a function mapping @xmath7",
    ", it is easy to plot @xmath6 versus @xmath5 to visualize the function .",
    "unfortunately , this can not be done on a piece of paper when @xmath5 is a prior pdf and @xmath6 is a posterior pdf .",
    "still , it should be possible to plug in a prior and easily evaluate the corresponding posterior .",
    "this convenience is offered by the method presented here .",
    "it is not surprising that the posterior depends on the prior , and it does nt mean that the results are not well - defined . for each prior , the posterior is unique , and determined by the data . ultimately , with more data , all priors asymptotically result in the same posterior , except for very extreme priors , like the kronecker @xmath8 function which represents an unshakeable prior conviction .",
    "the prior allows to interpret the same data from various starting points , including even the interpretation of someone with an unshakeable prior conviction . in this sense ,",
    "any prior is legitimate ; even a kronecker @xmath8 , although most would nt find interesting the inferences of someone who was committed to note letting data change his mind .",
    "that s why every bayesian result must be accompanied by a statement of the assumed prior , to know how to judge it .",
    "other people , the so - called `` objective bayesians '' , view the prior as something undesirable .",
    "since it is impossible to eliminate , they try at least to prescribe its definition .",
    "there are prescriptions which offer the posterior specific properties that some consider important , such as independence of the result under re - parametrization of the poi .",
    "other prescriptions try to achieve the opposite effect of a kronecker @xmath8 , namely , maximal susceptibility to the data .",
    "to use the previous analogy , these efforts are like prescribing a value of @xmath5 with some special property ; for example the @xmath5 which maximizes @xmath9 .",
    "some would argue that , if we ca nt plot @xmath6 for all values of @xmath5 , let s at least compute @xmath6 at that special @xmath5 that has some ( subjectively ? ) interesting property .",
    "a couple of criteria for this were mentioned already .    for a `` subjective bayesian '' , since all priors are fine , so are these special priors , which are known as `` non - informative '' priors .",
    "it should be mentioned , though , that if one follows the prescription to compute a non - informative prior ( which can be quite cumbersome ) , he may not be satisfied with the result , because it is highly unlikely to reflect any intuitive guess anyone would have made for the poi .",
    "such priors lose their meaning as distributions of prior belief , and become _ devices _ used to _ tune _",
    "the properties of the posterior .",
    "for example , non - informative priors often depend on the expected background . to see how paradoxical this is , consider that , if an experimental device registered more background noise for any instrumental reason , we would have to change accordingly our prior pdf of the higgs mass , or some other fundamental poi that the device would be supposed to measure .",
    "one would think that our prior pdf for the higgs mass should have nothing to do with how much background is registered by some instrument .",
    "but again , if that is the prior an `` objective bayesian '' wants to try , a `` subjective bayesian '' has no reason to object .",
    "the method presented here allows the readers to plug in any prior they wish , including even non - informative priors .",
    "this article shows how to set an _ exact _",
    "limit to your own signal , ignoring systematic uncertainties .",
    "the basic principles of including systematic uncertainties , with some examples , will be given in section  [ sec : systematics ] .",
    "it is not possible , however , to provide a complete general prescription for this , because not all systematic uncertainties are the same .",
    "the reader will have to generalize a little the examples provided here .",
    "theorists are equipped to evaluate theoretical systematic uncertainties , but experimental uncertainties are the expertise of experimenters .",
    "collaboration is necessary for a complete result .",
    "most theorists would be satisfied with limits which ignore systematic uncertainty , since they are typically only a few per - cent different from the limits with full treatment of systematic uncertainty . given that it is practically impossible for the experimentalist to compute limits to all possible theories of the present and the future , it is important for theorists to be able to easily set limits , even with the approximation of ignoring some systematic uncertainties .",
    "an approximation is better than nothing .",
    "furthermore , if an experiment uses a benchmark model to demonstrate the impact of systematic uncertainties , that can be used as a guideline to estimate the impact of the same uncertainties on another model , though for some uncertainties the impact may depend on the signal .",
    "if someone has a reliable model of systematic uncertainties , section  [ sec : systematics ] , will allow him , in principle , to convolute these uncertainties .",
    "this article is not trying to address the issue of detector simulation .",
    "it is assumed that the theorist can approximate the distribution of his signal after reconstruction .",
    "many theorists do this with tools like pgs @xcite .",
    "experiments also provide their acceptance to objects ( jets , leptons ) as a function of quantities accessible to theorists , such as transverse momentum ( @xmath10 ) and pseudo - rapidity ( @xmath11 ) . in some cases",
    "the detector resolution is also parametrized , so , a theorist can approximately smear the energy of jets and leptons .",
    "it is often claimed that unfolding @xcite the experimental data allows theorists to test their theories without needing detector simulation .",
    "this is an idealization of the actual situation @xcite .",
    "there are many ways to do unfolding ; it is not as unique and well - defined as the data .",
    "regularization , which plays central role in unfolding , depends on some arbitrary choices .",
    "the root of all problems with unfolding is that it is impossible to recover information that is lost during detector smearing .",
    "the unbiased estimator of the true spectrum has enormous variance , which makes it useless , so during regularization one introduces some bias , on purpose , to reduce the variance . in practice , unfolding may introduce more difficulties than it solves , so , it s advisable to avoid it unless nothing else is possible .",
    "unfolded spectra are estimators that follow complicated probability distributions ; it is no longer correct to treat each bin as an independent observation , or to assume that its contents follow a poisson or gaussian distribution .",
    "so , simple tests like @xmath12/(degrees of freedom ) are no longer correct .",
    "there are bin - to - bin correlations which are usually not published .",
    "even if a correlation matrix is provided , it assumes that the multidimensional pdf of the estimator is gaussian , but in reality its shape is irregular , especially when low statistics appear in some bins .",
    "the bias that is introduced by regularization is typically larger in parts of the spectrum where statistics are lower , which is precisely where exotic effects might be . in reality",
    "it is impossible to estimate the actual bias of unfolding , unless we knew the actual spectrum of the data before smearing , which is obviously unknown , and if we look for new physics it can not be assumed to be given by standard model ( sm ) simulation prior to smearing , or else we would nt be looking for new physics .",
    "so , searches for new physics are an unfavorable environment for unfolding .",
    "if an experimentalist has a matrix of migrations , which is the main ingredient of all unfolding methods , it is better to publish the matrix to allow theorists to fold the detector smearing into their theoretical signal , instead of using the matrix to unfold the data .",
    "this works without problems because , while it is impossible to recover lost information , it is totally possible to reduce existing information .",
    "the benefits of smearing , or folding , compared to unfolding , are the following : ( a ) unlike data , the theoretical prediction before smearing does not have statistical fluctuations , so there is no need for regularization . a simple multiplication of the folding matrix with the spectrum prior to smearing returns the expected spectrum after detector smearing . ( b )",
    "since the data are not unfolded , they follow a well - known pdf , e.g.  poisson , binomial , or gaussian .",
    "each bin can be used as an independent observation , so , there is no need to consider complicated ( and inevitably approximate ) correlations among data in different bins .",
    "it is simple to compare the data to the folded theoretical spectrum using simple methods such as a @xmath12 or a likelihood test .",
    "while folding solves some of the problems of unfolding , it faces a difficulty : different theoretical models would require different folding matrices to be folded correctly . to see why ,",
    "imagine new particles of different spin , whose decay products would be distributed differently in @xmath11 , thus measured by different parts of the detector , thus suffering different amounts of smearing .",
    "that is why it is difficult to provide folding matrices that would work equally well with all theories .",
    "probably the best strategy is to model detector effects in the level of measurable objects ( jets and leptons ) . by smearing each object separately",
    ", we can smear any signal that decays into such objects .",
    "this article allows a theorist to easily assume different signal distributions , therefore , if there is some doubt about the exact signal shape after detector smearing , it is easy to try different possibilities .",
    "the data , however , have to always be the observed data , _ not _ the output of any unfolding .",
    "if an experimental analysis chooses to use unfolding , always ask also for the original data , because _ there _ one can see reality ; unfolding offers mere interpretations .      to model the signal that makes it to the final plot , it is necessary to apply the event selection of the analysis whose data are used",
    ".    analyses always publish the event selection they apply .",
    "typically , the selection applies to single objects , or combinations of objects .",
    "for example , `` cuts '' are made in transverse momentum ( @xmath10 ) , pseudorapidity ( @xmath13 ) or rapidity ( @xmath14 ) , differences in azimuthal angles @xmath15 , differences in rapidity or pseudorapidity , scalar or vectorial sums of transverse momenta , and missing transverse energy ( met ) .",
    "a theorist has easy access to the 4-vectors of quarks , gluons , leptons , and to exotic particles escaping detection ( e.g.  stable or long - lived neutralinos ) . with generators like pythia @xcite , it is also possible to have access to hadronic showers resulting from emitted quarks and gluons , and using jet clustering algorithms such as those implemented in fastjet @xcite it is possible to define hadron - level jets .    for leptons",
    "it is simple to apply kinematic cuts . for jets",
    "it is a little less simple .",
    "if a theorist has hadron - level jets , their energy corresponds ( on average ) to the energy of calibrated reconstructed jets on which experimentalists typically apply @xmath10 cuts .",
    "so , it is acceptable to apply the same @xmath10 cut to hadron - level jets . the situation is a little more tricky when one has access only to partons , before showering and hadronization . in that case",
    ", one needs to consider that the hadron - level @xmath10 differs from the parton @xmath10 , due to out - of - cone losses .",
    "to apply a lower @xmath10 at hadron level , a slightly higher threshold is necessary at parton level . this becomes less of an issue when a large jet size parameter is used , or if the partons ( and jets ) are at higher @xmath10 , thus more boosted , thus having less out - of - cone losses . for anti-@xmath16 jets with @xmath17=0.4 , the out - of - cone energy fraction is roughly 10% at hadron - level @xmath18  gev , it reduces to about 4% at 100 gev , and is less than 2% at @xmath19  gev . for anti-@xmath16 jets reconstructed with r=0.6 ,",
    "the out - of - cone fraction is less than 2% even at @xmath18  gev .",
    "usually jets produced by new physics carry large momentum , well above such @xmath10 thresholds , so such differences would nt be important .",
    "when a minimum met is required , this translates into a minimum @xmath10 carried by the vectorial sum of neutrinos , gravitons , neutralinos etc .  in the signal .",
    "similar to charged leptons , one has the momenta of these objects , so it is simple to add vectorially their transverse momenta and apply the same threshold . in reality",
    ", there is some met even if at parton level all objects balance perfectly .",
    "that ( fake ) met comes mostly from the finite energy resolution of the calorimeter , from detector cracks , beam remnants , etc .",
    "detector simulation reproduces this met .",
    "it can be approximated by smearing , according to detector resolution , the transverse momenta of any jets or other objects in each event .",
    "however , fake met is typically much less than the real met produced by exotic particles that escape detection , and it is also well below the met thresholds required in searches for such particles .",
    "so , it should be safe to ignore fake met when genuine met is part of a new physics signature .",
    "a source of data is the hepdata project , hosted at the university of durham @xcite .",
    "from there , anyone can retrieve the observed and expected spectrum in bins of specified delimiters , for an increasingly number of analyses from various experiments .",
    "other disciplines , such as observational astrophysics , have a culture of open access to data .",
    "since the author is a high energy experimentalist , the focus of this article is in high energy physics , but it must be obvious how the methods shown here can be used in other disciplines .",
    "as many theorists evidently know , there is software which enhance one s ability to read numbers off of published figures .",
    "an example is datathief @xcite",
    ". do nt let the name of this software fill you with guilt ; there is nothing unethical in reading values off of a published and peer - reviewed experimental plot . just beware that , due to limited resolution , it may be hard to `` see '' exactly the content and the delimiters of each bin .",
    "it ca nt hurt to ask an experimental collaboration to publish the exact values , to avoid approximations .",
    "since most high energy theorists are familiar with mathematica is proprietary software , developed by the wolfram research company . ]",
    ", we will use it here for demonstration . very elementary use of mathematica",
    "is made , so , anyone should be able to read the code shown here and understand what it does .",
    "the computation is so simple that , with some perseverance , it can be carried out even by hand .",
    "no generation of monte carlo events is required for bayesian inference .",
    "mathematica provides an intuitive , interpreted language , that makes easy also the visualization of results .    of course ,",
    "once the computation method is understood , it can be ported to any programming environment .      for the purposes of this article",
    ", it does nt matter where the data come from .",
    "they can be considered fictitious .",
    "let s start with the very common case of a spectrum where events are counted in defined intervals ( bins ) of some observable .",
    "as a representative example , consider the distribution of events in some measured mass , as was done in @xcite and numerous other analyses .",
    "let @xmath20 denote the number of events observed in bin @xmath21 , and @xmath22 the number of events expected in bin @xmath21 if there is no new physics .",
    "the index @xmath21 runs from 1 to @xmath23 , which is the total number of bins .",
    "let @xmath24 be the expected number of produced signal events , _ which is our poi_. one would simply divide @xmath25 by the integrated luminosity to transform it to the cross - section of the new physics process .",
    "let @xmath26 be the fraction of the produced signal that ends up in bin @xmath21 after detector reconstruction and event selection .",
    "by definition , the total signal acceptance ( times reconstruction efficiency ) is @xmath27 : @xmath28 if @xmath29 , then all signal makes it into the @xmath23 bins of the spectrum after event selection .",
    "if @xmath30 , then some of the signal does nt make it to the final spectrum , either due to detector inefficiency , or because it fails some of the analysis cuts . in either case",
    ", the array of @xmath31 values completely determines the expected signal distribution after detector smearing and event selection .",
    "one uses @xmath31 to specify his model .",
    "to do so one needs to calculate the theoretically predicted signal distribution in @xmath23 bins , and model the effect of detector smearing ( section  [ sec : detectorsimulation ] ) and event selection ( section  [ sec : selection ] ) .",
    "if @xmath25 signal events are produced , then the expected events in bin @xmath21 are @xmath32 .",
    "assuming that the data are indeed the data , and not the product of some unfolding ( see section [ sec : detectorsimulation ] ) , the likelihood of the data under the assumption of @xmath25 produced events , which are distributed according to @xmath31 , is : @xmath33    applying bayes theorem , the posterior pdf  of our poi ( @xmath25 ) is @xmath34 where @xmath35 is the prior pdf  ( see section [ sec : prior ] ) , and @xmath36 is a constant which normalizes the posterior to 1 : @xmath37    let s use some numbers .",
    "the data are represented by the array @xmath38 , and the background by the array @xmath39 , and the signal distribution by @xmath4 , each with @xmath40 elements :    .... d = { 20839 , 14404 , 10285 , 7094 , 4841 , 3440 , 2338 , 1555 , 1059 , 706 , 515 , 367 , 214 , 155 , 112 , 73 , 45 , 31 , 23 , 14 , 2 , 9 , 2 , 1 , 1 , 0 , 2 , 1 , 0 , 0 } ;   b = { 21000 .",
    ", 14000 . , 10000 , 7100 . , 4800 . , 3400 . , 2300 . , 1600 . , 1100 . , 740 . , 500 , 350 . , 230 . , 160 . , 100 , 70 . , 46 . , 30 . , 20 . , 13 . , 8.2 , 5.2 , 3.2 , 2.0 , 1.2 , 0.71 , 0.42 , 0.24 , 0.13 , 0.074 } ; f = { 0 , 0.0000105 , 0.000335 , 0.000485 , 0.00015 , 0.0008 , 0.00115 , 0.00425 , 0.0022 , 0.0034 , 0.00495 , 0.0055 , 0.0095 , 0.018 , 0.0185 , 0.028 , 0.085 , 0.21 , 0.085 , 0.0125 , 0.0044 , 0 , 0.0000105 , 0 , 0 , 0.000335 , 0 , 0 , 0 , 0 } ; ....    the above inputs are plotted in fig .  [",
    "fig : data1 ] .    the following lines compute the posterior of eq .",
    "[ eq : posteriorpoisson ] :    .... nbins = length[b ] ( * d , b & f should all have the same length * ) a = total[f ]       ( * the acceptance * ) l[s _ ] : = exp[sum[d[[i]]*log[b[[i]]+s*f[[i]]],{i,1,nbins}]-s*a ] prior[s _ ] : = unitstep[s ] normconst = nintegrate[l[s]*prior[s],{s ,- infinity , infinity } ] posterior[s _ ] : = l[s]*prior[s]/normconst               ....",
    "it is now easy to visualize the posterior and define credibility intervals from it . for example :    .... plot[posterior[s],{s , -50 , 100},axeslabel->{\"s \" , \" p(s|data ) \" } ] nintegrate[posterior[s ] , { s , -infinity , infinity } ] findroot[nintegrate[posterior[s ] , { s , 0 , x } ] - 0.95 , { x , 10 } ] ....    line 1 produces a plot similar to those in fig .",
    "[ fig : posterior1 ] , and line 2 confirms that @xmath52 .",
    "line 3 computes numerically the 95% quantile of @xmath53 , namely the 95% credibility level upper limit on @xmath25 , which is this exaple is 55.7 events .. it can be different , obviously . ]",
    "the measured quantity is not always a poisson - distributed variable . for example , consider the dijet angular distribution analysis by atlas @xcite .",
    "the observable , @xmath54 , is the fraction of events which are central in each dijet mass bin .",
    "the exact definition of `` central '' is of no importance here . in general",
    ", there is some criterion , and each event represents a bernoulli trial , leading to success if the criterion is satisfied .",
    "the probability of having @xmath55 successes in @xmath56 trials , when each success has probability @xmath57 , is given by the binomial distribution : @xmath58    in each bin @xmath21 of the total @xmath23 dijet mass bins , the observed number of events in bin @xmath21 be @xmath59 , of which @xmath60 are central .",
    "let the probability of non - signal ( i.e.  background ) events be @xmath61 .",
    "a theorist knows the probability of signal events to be central when they belong in bin @xmath21 , which is denoted @xmath62 .",
    "he also knows , like in section  [ sec : poisson ] , the _ total _ ( not only central ) number of signal events in bin @xmath21 , which is @xmath63 .    as before",
    ", @xmath25 is the poi .",
    "the likelihood of the observed data , assuming some value for @xmath25 , is    @xmath64    the computation in this case is a little more complicated than the simple poisson case , because the inputs are more .",
    "the data are not one array , but two : @xmath59 and @xmath60 .",
    "the signal is also described by two arrays : @xmath31 and @xmath62 .",
    "we can use , like in eq .",
    "[ eq : logl ] , the logarithm of @xmath65 , to simplify and speed up the computation :    @xmath66    let s create an example , where for simplicity @xmath62 is constant in all bins @xmath21 , and equal to @xmath67 .",
    "let s use for @xmath31 the same values that we used in section  [ sec : poisson ] . for the background we will assume that @xmath68 for all bins @xmath21 .",
    "we will use as @xmath60 the same array that we called @xmath43 in the example of section  [ sec : poisson ] , and we will add a new array @xmath59 . to make the example look like a scenario without new physics , we will define @xmath59 by sampling a poisson distribution with mean @xmath69 .",
    "let s put the above in code :    .... t = { 208837 , 144387 , 102698 , 70993 , 48508 , 34414 , 23578 , 15452 , 10461 , 7127 , 5078 , 3767 , 2160 , 1591 , 1098 , 739 , 437 , 284 , 244 , 148 , 13 , 78 , 21 , 13 , 7 , 10 , 18 , 8 , 5 , 5 } ; t = { 20839 , 14404 , 10285 , 7094 , 4841 , 3440 , 2338 , 1555 , 1059 , 706 , 515 , 367 , 214 , 155 , 112 , 73 , 45 , 31 , 23 , 14 , 2 , 9 , 2 , 1 , 1 , 0 , 2 , 1 , 0 , 0 } ; nbins = length[t ] ; epsilonsig = table[0.5 , { i , 1 , nbins } ] ; epsilonbkg = table[0.1 , { i , 1 , nbins } ] ; f = { 0 , 2.1 * 10 ^ -05 , 0.00067 , 0.00097 , 0.0003 , 0.0016 , 0.0023 , 0.0085 , 0.0044 , 0.0068 , 0.0099 , 0.011 , 0.019 , 0.036 , 0.037 , 0.056 , 0.17 , 0.42 , 0.17 , 0.025 , 0.0088 , 0 , 2.1 * 10 ^ -05 , 0 , 0 , 0.00067 , 0 , 0 , 0 , 0}/2 ; ....    the above choice of numbers is depicted in fig .",
    "[ fig : databinom ] .",
    "( green bars ) .",
    "details about the chosen example are given in section  [ sec : binomial ] .",
    "the vertical axis shows the fraction of events which satisfy a criterion , e.g.  being central .",
    "the error bars are , for simplicity , just the pearson interval that spans @xmath70 .",
    "[ fig : databinom ] ]    now that the inputs are defined , let s write the computational part :",
    "above , line 1 obviously defines @xmath71 as a function of @xmath25 and @xmath21 , which is then used in line 2 that reproduces eq .",
    "[ eq : loglbinom ] , except for the constant term which is omitted on purpose , to be included in the overall normalization constant that is computed in line 4 .",
    "line 3 defines the prior @xmath35 of our choice ( not normalized ) , which here is uniform in @xmath72 , and line 4 computes the normalization constant @xmath73 .    at this point ,",
    "in the example we use , a computational difficulty appeared .",
    "this is an opportunity to explain how to overcome it , and how to investigate such cases .",
    "when we executed line 4 , a complain was returned that the numerical integration did nt converge , and a half - done result was returned , which was @xmath74 , which looked like an approximation of @xmath75 . to understand what was going on , we tried to plot directly @xmath76 , using the command plot[l[s],s,0,100 ] , however that failed too , so , no wonder the integral was failing .",
    "then instead of plotting @xmath76 we tried something more humble ; to just compute it for @xmath77 , and for @xmath78 .",
    "the result was two very small numbers , with a similar order of magnitude : @xmath79 and @xmath80 .",
    "this is a sign that the likelihood function is computable ( it had no reason to not be anyway ) , but its extremely small numerical value makes its plotting and integration problematic .",
    "solution : remember that we can multiply the likelihood with any constant , since in the end the posterior it will be normalized to 1 anyway .",
    "it would makes computation easier to divide the likelihood it by a constant of the same order of magnitude , to transform @xmath79 to 3.9 , i.e.  a much easier number to treat .",
    "one way to do this is to divide @xmath76 by @xmath81 .",
    "this is indeed done in the following code :    .... normconstdividedbyl0 = nintegrate[l[s]/l[0]*prior[s ] , { s , -infinity , infinity } ] normconst = normconstdividedbyl0 * l[0 ] posterior[s _ ] : = l[s]*prior[s]/normconst ....    the trick we played was to add the division by l[0 ] in the integrand of line 1 , defining this auxiliary variable normconstdividedbyl0 , and then we defined normconst in line 2 based on normconstdividedbyl0 .",
    "line 3 is nothing but the definition of the normalized posterior @xmath53 , according to bayes theorem , as was done in section  [ sec : poisson ] .",
    "it may seem strange that dividing and multiplying by the same number makes any difference , but in computation such things can matter .",
    "it is simple to plot the posterior pdf , to compute limits , and to verify that the integral of the posterior is indeed 1 , using the same commands given in section [ sec : poisson ] .",
    "e.g. , fig .",
    "[ fig : posteriorsbinom ] shows the posteriors corresponding to a variety of priors .",
    "in some theoretical models , the signal is not just added to a fixed standard model background , but interferes with it . as a result , the likelihood of the data , assuming some value for the poi , may not be as simple to express analytically as in eq .",
    "[ eq : likelihood ] .",
    "it is still possible to set limits to such models , and to compute the posterior pdf  of their parameter(s ) of interest , as long as there is a way to map each value of the poi into a shape for the expected distribution .",
    "this needs to be done in a continuous way , if the poi is continuous .    to use the nomenclature of section  [ sec : poisson ]",
    ", one needs a function @xmath82 to express the expected content of bin @xmath21 , if the poi is @xmath25 .",
    "then , the likelihood function of would in general be    @xmath83    if it is not possible to have an analytical function @xmath82 , one can compute the expected spectrum ( @xmath84 ) for several discrete values of @xmath25 , and interpolate to intermediate values of @xmath25 by using a _ morphing _ technique , as described in @xcite .",
    "an example of morphing would lie beyond the scope of the current document .",
    "previously we talked about data in bins of an observable quantity .",
    "nothing , however , would change if the index @xmath21 enumerated bins of different observables , or even different experiments .",
    "all one would do is expand the arrays to contain all independent observations .    combining two , or more , sets of data proceeds by writing down the joint likelihood of all observations , as a function of the poi . in this aspect ,",
    "section  [ sec : poisson ] _ was already _ a combination of datasets , if we view the 30 bins as 30 independent observation , which , in that case originated from the same experiment .",
    "we will construct also an example where we combine observations from different experiments , which is usually what people refer to by `` combination '' .",
    "let s keep , as input from the first experiment , the numbers used in section  [ sec : poisson ] .",
    "we add the suffix 1 in the variable names , to remind us that they come from the first experiment .    .... d1 = { 20839 , 14404 , 10285 , 7094 , 4841 , 3440 , 2338 , 1555 , 1059 , 706 , 515 , 367 , 214 , 155 , 112 , 73 , 45 , 31 , 23 , 14 , 2 , 9 , 2 , 1 , 1 , 0 , 2 , 1 , 0 , 0 } ;   b1 = { 21000 . , 14000 . , 10000 , 7100 .",
    ", 4800 . , 3400 . ,",
    "2300 . , 1600 .",
    "500 , 350 .",
    ", 230 . , 160 . , 100 , 70 . , 46 .",
    ", 30 . , 20 . , 13 . , 8.2 , 5.2 , 3.2 , 2.0 , 1.2 , 0.71 , 0.42 , 0.24 , 0.13 , 0.074 } ; f1 = { 0 , 0.0000105 , 0.000335 , 0.000485 , 0.00015 , 0.0008 , 0.00115 , 0.00425 , 0.0022 , 0.0034 , 0.00495 , 0.0055 , 0.0095 , 0.018 , 0.0185 , 0.028 , 0.085 , 0.21 , 0.085 , 0.0125 , 0.0044 , 0 , 0.0000105 , 0 , 0 , 0.000335 , 0 , 0 , 0 , 0 } ; a1 = total[f1 ]   ( * which returns 0.494476 * ) nbins1 = length[b1 ]   ( * returns 30 * ) ....    let s consider a second experiment , where a different observable is used , and",
    "we have it distributed in 20 bins ( instead of 30 bins that we had in the first experiment ) . that observable is affected by the same new physics , but the detector is different , the background level is different , the shapes of background and signal are different from the first experiment .",
    "for example ,    .... d2 = { 496 , 1007 , 1495 , 1937 , 2392 , 2785 , 3022 , 3279 , 3733 , 3848 , 4046 , 4177 , 4413 , 4178 , 3960 , 3834 , 3711 , 3598 , 3247 , 2934 } ; b2 = { 498 , 990 , 1466 , 1921 , 2346 , 2738 , 3088 , 3393 , 3648 , 3850 , 3997 , 4087 , 4119 , 4094 , 4015 , 3883 , 3702 , 3477 , 3214 , 2919 } ; f2 = { 0.0016 , 0.0023 , 0.0085 , 0.0044 , 0.0068 , 0.0099 , 0.011 , 0.019 , 0.036 , 0.037 , 0.056 , 0.17 , 0.42 , 0.17 , 0 , 0 , 0 , 0 , 0 , 0 } ; a2 = total[f2 ]    ( * which returns 0.9525 * ) nbins2 = length[b2 ]   ( * returns 20 * ) ....    to make the example more interesting , the data of the second experiment ( d2 ) correspond to the background ( b2 ) plus 600 signal events produced in the second experiment , which are distributed according to f2 .",
    "the elements of f2 have sum @xmath85 , so , we have assumed for the second experiment most of the signal gets reconstructed .",
    "figure  [ fig : b2andd2 ] shows the above inputs from the second experiment .",
    ".[fig : b2andd2],scaledwidth=50.0% ]    now that we have the data , background , and signal distribution in both experiments , we need to compute their joint likelihood , as a function of a poi , which may be some quantity proportional to the ( unknown ) cross - section of new physics .",
    "for example , the poi could be the number of produced events in the first experiment , or in the second experiment , or in both experiments together , or it could be an expression of the coupling constant itself .",
    "let s make a choice that will spare us one proportionality constant in our expressions , and define as poi the number of signal event produced in the first experiment , which we denoted already with @xmath25 in section  [ sec : poisson ] . here , to remember the definition of our poi",
    ", we will denote it with s1 .",
    "if we infer the true value of s1 , it will be easy to divide it by the integrated luminosity of the first experiment , to convert it to the cross - section of the new physics process in the conditions of the first experiment .",
    "when that is known , the coupling strength of the new physics can be extracted , which is a universal characteristic of the new physics and does nt depend on the experiment .",
    "the number of signal events produced in the second experiment ( s2 ) is proportional to the signal events produced in the first experiment ( s1 ) .",
    "the proportionality constant depends on the respective integrated luminosities , and on the cross - section of the new physics process in the two experiments .",
    "let s assume that the second experiment recorded 2 times larger integrated luminosity than the first experiment , and the signal cross - section in the second experiment is 3 times larger than in the first experiment .",
    "that means that @xmath86 we will need this proportionality constant ( @xmath87 ) when we write the joint likelihood of the two experiments as a function of s1 .",
    "obviously , a theorist can calculate @xmath88 , if he can compute the cross - section of his model in the conditions of the two experiments , and if he knows how the two integrated luminosities compare .",
    "time to write the joint likelihood analytically , assuming that the two experiments are statistically independent : @xmath89    now , let s implement this in mathematica .",
    "we will first merge the arrays d1 and d2 into the _ joint _ data d , and the arrays b1 and b2 into the _ joint _ background b :    .... d = join[d1 , d2 ]   ( * this concatenates d1 and d2 * ) b = join[b1 , b2 ]   ( * this concatenates b1 and b2 * ) ....    then we will define a new array f using f1 and f2 . note that , in eq .",
    "[ eq : jointl ] , all elements of f2 are multiplied by @xmath88 .",
    "we can simplify our expressions by defining letting f2 _ absorb _ @xmath88 .",
    "this is done by writing f as :    .... r = 2 * 3     ( * this is what we assume in this example * ) f = join[f1,r*f2 ]   ( * first f1 elements , then r*f2 elements * ) ....    figure  [ fig : jointdata ] shows the contents of b and d , and how the new physics would appear in this joint dataset ( according to f ) if we assumed @xmath90 events , i.e.  @xmath91 events .    using the same computational trick as in eq .",
    "[ eq : logl ] , we write the joint likelihood ( up to a constant that will be absorbed by the normalization constant ) of eq .",
    "[ eq : jointl ] as :",
    "the above expression uses explicitly the arrays of the two experiments and the constant @xmath88 , but since we have also defined the joint arrays d , b and f which absorbs @xmath88 in its second part , we can write the _ totally equivalent _ expression :",
    "the rest is just like before .",
    "we define a prior pdf  as a function of s1 , we find the normalization constant , and we get a posterior pdf :    .... prior[s1 _ ] : = unitstep[s1 ]    ( * constant for s1>0 * ) normconst = nintegrate[l[s1]*prior[s1 ] , { s1 , -infinity , infinity } ] posterior[s1 _ ] : = l[s1]*prior[s1]/normconst ....    figure  [ fig : combinationposterior ] shows the results we get from the current numerical example , with the three following indicative prior assumptions for s1 :    .... prior[s1 _ ] : = unitstep[s1 ] prior[s1 _ ] : = unitstep[s1]*(exp[-0.02 s1 ] ) prior[s1 _ ] : = unitstep[s1]*(0.1 + exp[-(s1 - 80)^2/100 ] ) ....    figure  [ fig : combinationposterior ] includes the posteriors inferred using only the first or only the second experiment .",
    "these are computed as follows ( the suffix 1 and 2 distinction the first from the second experiment ) :",
    "the poi in the above examples was always a single variable ( @xmath25 ) , but it can be multidimensional ( @xmath92 ) . a characteristic class of models with multiple pois are susy models .",
    "here is an example where the data of section .",
    "[ sec : poisson ] are interpreted to find a posterior in a 2-dimensional parameter space .",
    "let s consider a quite generic model , where the signal is gaussian - distributed , its mean depends on an unknown poi s1 , and its amplitude by another unknown poi s2 .",
    "its width could be given by a third parameter s3 , but for visualization purposes it is better to keep the space of unknown parameters 2-dimensional , so , we will assume that the width is constant .",
    "here is such a model :    .... f[s1 _ ] : = table[exp[-(s1 - i)^2/10 ] , { i , 1 , nbins } ] ....    where nbins is the length of the b array of section [ sec : poisson ] , namely @xmath93 .",
    "figure  [ fig : signals ] shows some examples of f[s1 ] for various values of s1 .",
    "defined in section  [ sec : combo ] , for s1 equal to 1 , 8 , 15 , 22 , and 29.[fig : signals],scaledwidth=50.0% ]    in this example we will reuse the background array b of section  [ sec : poisson ] .    .... b = { 21000 . , 14000 . , 10000 , 7100 . , 4800 . , 3400 . ,",
    "2300 . , 1600 . , 1100 .",
    ", 740 . , 500 , 350 . , 230 . , 160 . ,",
    "100 , 70 . , 46 . ,",
    ", 20 . , 13 . , 8.2 , 5.2 , 3.2 , 2.0 , 1.2 , 0.71 , 0.42 , 0.24 , 0.13 , 0.074 } ; ....    to make the case more interesting , we will use data which are generated after injecting some signal on top of this background .",
    "specifically , the injected signal will be distributed according to @xmath94 , where @xmath21 is the bin index .",
    "this injected signal is on purpose narrower than f[10 ] , to show what happens when the actual signal shape is not exactly like the hypothesis one uses to interpret the data .",
    "so , here are the data of this example :    .... d = { 20985 , 13927 , 9899 , 7139 , 4821 , 3398 , 2348 , 1617 , 1079 , 798 , 555 , 365 , 224 , 163 , 88 , 75 , 52 , 31 , 21 , 11 , 8 , 2 , 5 , 3 , 0 , 1 , 0 , 0 , 0 , 0 } ....    it is simple to write the likelihood of the data , as a function of the two pois ( s1 , s2 ) :",
    "for computational reasons , to avoid enormous numbers , we multiply l[s1,s2 ] by @xmath95 , which is proportional to the likelihood of the data when no signal is assumed .",
    "notice that s1 is passed as an argument to f[s1 ] , to determine the signal shape , and then the shape is scaled by s2 .",
    "the prior of course needs to be defined in the same 2-dimensional space .",
    "for example , it could represent the presumption that s2 ( the produced signal amount ) has to be non - negative , while all values of s1 are considered equally likely :    .... prior[s1_,s2 _ ] : = unitstep[s2 ] ....    it is interesting to demonstrate , in the 2-dimensional case , what would happen if we introduced some non - trivial presumption in the prior .",
    "let s presume that s1 is more likely to be around 15 ( which is at the middle of the spectrum ) , as expressed by the following prior :    .... prior[s1 _ , s2 _ ] : = unitstep[s2]*exp[-(s1 - 15)^2/5 ] ....    figure  [ fig:2dposteriors1 ] shows the shape of the posterior ( ignoring the normalization constant ) for both priors .",
    "the posterior , up to a normalization constant , is :    .... posterior[s1 _ , s2 _ ] : = l[s1,s2]*prior[s1,s2 ] ....    the posterior with uniform prior , which has the same shape as the likelihood function , does not have its maximum exactly at ( s1,s2)=(10,50 ) , and the reason is dual :    * the data ( d ) are _ consistent _ with injected signal of ( s1,s2)=(10,50 ) , but it is ultimately the result of poisson random fluctuations in each bin , so , it is expectable that the best - fitting ( s1,s2 ) will be close to that point , but not exactly there . *",
    "the signal shape f[s1 ] that is used to compute the likelihood is wider than the actual signal that has been injected , on purpose , to demonstrate this scenario , which is quite plausible , because nature may produce some signal , which we ignore , so we may try to interpret the data to infer the parameters of a different signal .    for comparison , fig .",
    "[ fig:2dposteriors2 ] shows the same result , with exactly the same b and d , when f[s1 ] has been modified to have the same width as the injected signal :    .... f[s1 _ ] : = table[exp[-(s1-i)^2 / 5 ] , { i , 1 , nbins } ] ....    the difference is that , when the prior is uniform ( red contours in fig .",
    "[ fig:2dposteriors2 ] ) , the posterior is more narrow in s1 .",
    "this makes sense ; it s more clear where the signal is , when we have gotten the signal width right . as a result ,",
    "the effect of the non - uniform prior is quite different in fig .",
    "[ fig:2dposteriors2 ] than in [ fig:2dposteriors1 ] : the prior `` pulls '' the posterior towards s1=15 , but the likelihood is larger around @xmath96 , so , the resulting posterior has two local maxima , of which the one near s1=15 prevails with greater probability density .",
    "it is also worth noting that , in both fig .",
    "[ fig:2dposteriors1 ] and [ fig:2dposteriors2 ] , the non - uniform prior in s1 does not only pull s1 towards 15 , but it also changes the most likely value of s2 .",
    "this happens because s1 and s2 are correlated , as one can see from the asymmetric shape of the contours .",
    "this can only be appreciated in a multidimensional space , where there is room for correlations : the prior may be factorized to one part that depends only on s1 and another that depends only on s2 , but its effect on the posterior is not factorized in a similar way ; a change in the prior with respect to s1 will modify the posterior in all dimensions .",
    "systematic uncertainties are uncertainties about assumptions which affect the measurement . if these assumptions were slightly different , within their own ( systematic uncertainty ) , that would have an effect on the measurement . to quantify this effect , we need first to use parameters to quantify the assumptions .",
    "these parameters are called `` nuisance parameters '' .",
    "the procedure to take these uncertainties into account starts by treating the nuisance parameters as if they were pois , alongside with the actual pois .",
    "this leads to a multi - dimensional space of parameters , where a prior needs to be defined , and a posterior is computed based on the data .",
    "the posterior pdf  can be integrated along the dimension of the nuisance parameter(s ) , leaving only the actual pois as free variables in the posterior .",
    "let s write this analytically , denoting the nuisance parameter(s ) with @xmath97 , and the actual poi with @xmath25 . from bayes theorem , @xmath98",
    "where @xmath99 then , since we are not interested in the actual value of @xmath97 , but only of @xmath25 , the posterior we actually care about is @xmath100 if the prior @xmath101 is factorable as : @xmath102 then @xmath103 this integral can be read as `` the expected likelihood function , over all possible values of the nuisance parameter @xmath97 '' , which can be denoted : @xmath104    notice the similarity between eq .",
    "[ eq : posteriornuis2 ] and eq .",
    "[ eq : posteriorpoisson ] .",
    "the only difference is that the likelihood at @xmath25 is replaced by the average likelihood .",
    "if one wishes to try a different prior for @xmath25 he can do it by just changing @xmath105 , without having to recalculate the average likelihood .",
    "this can be a great advantage in practical applications , where calculating the average likelihood ( namely , performing the `` convolution '' of the nuisance parameters ) is time - consuming .",
    "equation  [ eq : posteriornuis2 ] is based on the condition that the prior is factorable as in eq .",
    "[ eq : priorfactor ] .",
    "this condition is easy to satisfy , and actually most intuitive prior choices would satisfy it .",
    "usually the nuisance parameters express some uncertainty about the experimental conditions , like the actual detector response etc .",
    "there is no reason to correlate , in the prior , the true cross - section of a process with the nuisances of the detector may indicate that @xmath25 and @xmath97 are correlated , but do nt confuse the prior with the posterior ; eq .",
    "[ eq : priorfactor ] concerns just the prior . ]",
    ". however , this may not be the case for theoretical nuisance parameters , which may be intimately related to @xmath25 even a - priori .",
    "after all , eq .  [ eq : priorfactor ] refers to the prior , so , someone may wish to assume some @xmath101 that is nt factorable , just because that s what he finds interesting .",
    "we can not prevent that , so , the numerical examples below do not rely on the assumption of eq .",
    "[ eq : priorfactor ] , but make use of eq .",
    "[ eq : posteriornuis1 ] , which is generally true .",
    "for example , let s take the background and data of the example in section  [ sec : poisson ] .",
    "the goal now is to infer the amount of produced signal ( @xmath25 ) , where the signal is known to be gaussian - distributed around bin 15 , with standard deviation equal to 3 bins .",
    "however , there is some doubt about the actual position of the signal ; maybe the mean is not exactly 15 .",
    "this could reflect , for example , an uncertainty about the _",
    "actual _ detector energy response , if the bins are defined in an observable which depends on energy .",
    "let s parametrize this uncertainty using a nuisance parameter @xmath97 , such that the signal peaks at @xmath106 .",
    "the following lines implement this parametrization .",
    "the array f is a function of n , and is normalized to sum a = 0.49 , simply to keep the same acceptance as in section  [ sec : poisson ] .    .... a = 0.49    ( * some arbitrary acceptance * ) f[n _ ] : = a * table[exp[-(15*n - i)^2/(2 * 3 ^ 2 ) ] , { i , 1 , nbins } ] / sum[exp[-(15*n - i)^2/(2 * 3 ^ 2 ) ] , { i , 1 , nbins } ] ] ....    figure  [ fig : convsig1 ] demonstrates this f[n ] .",
    "then we compute , up to a constant term which will be absorbed in the final normalization , the likelihood function l[s , n ] , which corresponds to @xmath107 of eq .",
    "[ eq : posteriornuis1 ] . to avoid problematically large numbers , we divide by the constant l0 , which is assuming no signal :",
    "note that f[n ] in this example is constructed to have @xmath108 , for any choice of @xmath97 . in a more general case , where the acceptance depends on @xmath97 , one would replace a by total[f[n ] ] , to compute the acceptance at the same time with l[s , n ] .",
    "this would make computation slightly slower , which is why it is avoided here .",
    "we need to define a prior pdf ( up to a constant ) , which will be assumed to be uniform in s , allowing only positive values of s , and gaussian in n , with maximum probability density at n = 1 and standard deviation equal to 0.1 :    .... prior[s _ , n _ ] : = unitstep[s ] * exp[-(n - 1)^2/(2 * 0.1 ^ 2 ) ] ....    the posterior @xmath109 , before integration along @xmath97 , is given ( up to a constant ) by the product l[s , n ] * prior[s , n ] , which is shown in fig .",
    "[ fig : convposteriormeansn ] .",
    "the next step is to `` integrate out '' @xmath97 , in which we are not really interested . here ,",
    "this is done using a simple approximation of the integral , where we break the interval @xmath110 $ ] in 100 steps of size 0.01 , and we approximate @xmath111 this approximation is justified by @xmath101 being almost zero for @xmath112 or @xmath113 , and eq .  [ eq : trapezoid ] being a simple numerical integration method , admittedly not the most advanced , but simple enough to implement in the following few lines :    .... integ[s _ ] : = sum[l[s , n]*prior[s , n ] , { n,0.5,1.5,0.01 } ] normconst = nintegrate[integ[s ] , { s , -infinity , infinity } ] posterior[s _ ] : = integral[s ] / normconst ....    the function integ[s ] corresponds to the result of eq .",
    "[ eq : trapezoid ] . the constant",
    "0.01 has been omitted , or rather absorbed by the normconst normalization constant computed in line 2 .",
    "finally , the ( normalized ) posterior pdf  of @xmath25 is given in line 3 , which can now be plotted , or used to compute its 95% or any other quantile , as shown in section  [ sec : poisson ] .",
    "figure  [ fig : posteriormeans ] shows the resulting posterior , after this convolution of the nuisance parameter @xmath25 , and compares it to the posterior one would get if there were no uncertainty in @xmath97 , namely , if @xmath101 were @xmath114 where @xmath115 is the step function represented in mathematica by unitstep[s ] , and @xmath116 is just the kronecker @xmath8 , pinning @xmath97 to 1 .",
    "the latter posterior , which is unaffected by systematic uncertainty , is given simply by :    .... normconst = nintegrate [ l[s,1]*prior[s,1 ] , { s ,- infinity , infinity } ] posterior[s _ ] : = l[s,1]*prior[s,1 ] / normconst ....",
    "this comparison shows that the convolution of @xmath97 makes the posterior wider , and the upper limit worse ( looser ) .",
    "specifically , the upper limit , and 95% credibility level , without systematic uncertainty is 131.315 , and with this uncertainty it becomes 153 .",
    "however , it is not always true that inclusion of systematic uncertainty loosens the upper limit .",
    "we will see in section  [ sec : convrms ] such an example .",
    "in this example we follow the same steps as in section  [ sec : convmean ] , except that we formulate f[n ] at the beginning in a different way . here",
    "we wish @xmath97 to parametrize some uncertainty in the width of signal which is known to be gaussian with mean equal to 15 and width somewhere near 3 .",
    "here is the only different command :    .... f[n _ ] : = a*table[exp[-(15 - i)^2/(2*(3*n)^2 ) ] , { i , 1 , nbins}]/      sum[exp[-(15 - i)^2/(2*(3*n)^2 ) ] , { i , 1 , nbins } ] ] ; ....    figure  [ fig : convsig2 ] shows some examples of this f[n ] .",
    "the posterior is assumed the same as in the previous example .",
    "the resulting @xmath109 is shown in fig .",
    "[ fig : convposteriorrmssn ] , and the final @xmath53 in fig .",
    "[ fig : convposteriorrmss ] .",
    "interestingly , the effect of this uncertainty is much smaller than the uncertainty of section  [ sec : convmean ] .",
    "not only it is much smaller , but it goes in the opposite direction : it makes the upper limit slightly stricter than if we had no uncertainty at all . specifically , the 95% upper limit moves from 131.315 to 130.825 .",
    "this is admittedly a minuscule improvement , but it is possible to find an example where the improvement is noticeable .",
    "for example , if instead of the prior of section  [ sec : convmean ] we use a `` box '' prior in @xmath97 :    .... prior[s _ , n _ ] : = unitstep[s ] * unitstep[n-0.5]*unitstep[1.5-n ] ....    then the effect of this width systematic uncertainty is more visible ( fig .",
    "[ fig : convflatrms ] ) , and it changes the 95% upper limit to 126.7 , which is a more clear improvement .    , except that this time the systematic uncertainty of section  [ sec : convrms ] is convoluted using a prior which does not constrain @xmath97 to be gaussian - distributed around 1@xmath490.1 , but gives @xmath97 equal probability to be anywhere between 0.5 and 1.5 .",
    "[ fig : convflatrms],scaledwidth=45.0% ]    many people are under the impression that systematic uncertainties have to always make limits worse , because `` less information has to make things worse '' , or something along these lines .",
    "this is a verbal over - simplification of the actual mathematical procedure .",
    "systematic uncertainty is not only `` less information '' ; it is also `` more possibilities '' .",
    "upper limits get worse ( looser ) when the data show an excess , and better ( stricter ) when there is a deficit . if we have an excess , and no uncertainty whatsoever , we are in a situation that disfavors the limit , and there is no chance the situation is any different .",
    "but if some systematic uncertainty is introduced , it might allow some scenarios where the situation is more favorable .",
    "if we average out all scenarios , which is what the convolution of eq .",
    "[ eq : posteriornuis1 ] does , then the limit might improve . empirically , this does nt happen very often , but it has been observed several times , in numerous analyses , including the numerical example above .",
    "this section will please readers who like frequentist limits .",
    "the `` holy grail '' in frequentist limits is _",
    "coverage_. frequentist constructions provide ( or should provide at least ) intervals of specific coverage , a typical choice in high energy physics being 95% .",
    "intervals of coverage 95% are called `` 95% confidence intervals '' ( cis ) .",
    "[ [ what - is - coverage ] ] what is coverage ?",
    "+ + + + + + + + + + + + + + + + +    to understand that , one needs first to realize that , even if the laws of nature do nt change , a different observation of nature would result in different data ; there are random fluctuations . both bayesian inference and frequentist constructions use data as input ,",
    "so , their outputs ( pdfs and cis ) are also subject to statistical fluctuation .",
    "if the poi has some value ( @xmath117 ) , and we collect many ( in principle infinite ) independent datasets , and we compute an interval using each one of these datasets , we will find @xmath117 within our interval with frequency @xmath118 .",
    "this number ( @xmath118 ) is the coverage of the interval .",
    "it is a statistical property of the interval , and the procedure used to determine it .",
    "obviously , the coverage may depend on @xmath117 , and on the procedure used to find the interval .",
    "coverage is not only a property of frequentist intervals ; any interval , however it is defined , has some coverage .    to compute the coverage of a bayesian credibility interval ,",
    "we can write a loop which repeatedly creates pseudo - data that are consistent with some assumed value of the poi , repeats the bayesian limit - setting procedure , and in the end count how many times the assumed value of the poi was within the interval .",
    "the following lines compute the coverage of an upper limit with 95% credibility",
    "line 1 : : :    define the background in each bin .",
    "same as in section  [ sec : poisson ] .",
    "line 2 : : :    define the signal distribution .",
    "same as in section  [ sec : poisson ] .",
    "line 3 : : :    number of bins .",
    "nbins is 30 in this example .",
    "line 4 : : :    the acceptance to the signal , given by eq .",
    "[ eq : acceptance ] . in this    example @xmath119 .",
    "line 5 : : :    define the likelihood function .",
    "line 6 : : :    define a flat prior for @xmath72 .",
    "it can obviously change ,    and the coverage will have some dependence on the prior , since the    prior is part of the procedure that defines the bayesian credibility    interval .",
    "line 7 : : :    define the formula for the posterior .",
    "line 8 : : :    the assumed amount of produced signal .",
    "variable v corresponds to the    @xmath117 used above , in the definition of coverage .",
    "this is the    amount of signal that will be added to the background to generate each    set of pseudo - data , in line 13 .",
    "line 9 : : :    define how many iterations to make in the loop which starts in line 12    and ends in line 18 .",
    "a large number of iterations will lead to a more    precise estimation of the actual coverage .",
    "line 10 : : :    define the credibility level of the upper limit whose coverage will be    estimated .",
    "0.95 means 95% credibility level .",
    "line 11 : : :    initialize an array of npseudo answers .",
    "the elements initially are all    0 , and some of them will turn into 1 inside the loop , in line 17 .",
    "each    element represents the output of a set of pseudo - data .",
    "if the element    is 0 , it means that the interval failed to contain the true poi value    ( @xmath117 ) .",
    "if it is 1 , it means that the interval succeeded to    contain @xmath117 , namely the upper limit is a number greater    than @xmath117 .",
    "line 12 : : :    starting the loop .",
    "line 13 : : :    define the data , which consist of poisson fluctuations of the content    of each bin , with mean equal to the background of the bin , plus the    signal events that would end up in the bin if @xmath117 signal    events were produced . clearly , d are data consistent with the    hypothesis that @xmath117 signal events are produced .",
    "line 14 : : :    calculate the constant l0 which is introduced in line 5 to make l[s ]    easier the handle numerically .",
    "line 15 : : :    compute the normalization constant which normalizes the posterior    defined in line 7 .",
    "line 16 : : :    compute @xmath120 , and store    it in variable integ . line 17 : : :    if integ is less than cred , then register the value 1 in the answers    array , in the position that corresponds to the current pseudo - data    set .",
    "the logic is that , if integ is less than cred , then the upper    limit with credibility cred ca nt be but some number greater than    @xmath117 . that s obvious , since the upper limit is defined as    the @xmath5 which satisfies    @xmath121 .",
    "this    trick allows us to know if the interval covers @xmath117 ,    without really computing the interval , which would be a more    cpu - expensive computation .",
    "line 18 : : :    the loop closes , after npseudo iterations .",
    "line 19 : : :    out of the npseudo trials , some have succeeded , in the sense that the    interval covered the actual poi value ( @xmath117 ) .",
    "we can count    these successes by summing the elements of the answers array . dividing    by npseudo",
    ", we get an estimator of the success rate , which is , by    definition , the coverage .    running",
    "the above code , with the numbers given , returned coverage 0.960 .",
    "smaller values of @xmath117 result in larger coverage , and when @xmath117 increases the coverage asymptotically becomes equal to credibility , namely 0.95 .",
    "this is true for any prior one may assume , and there are some special non - informative priors which make the convergence faster .",
    "it has been shown how to compute posterior pdfs and limits to any arbitrary signal in the most common case of poisson - distributed data ( section  [ sec : poisson ] ) and in the case of binomially distributed data ( section  [ sec : binomial ] ) .",
    "the treatment is described for signals that are not simply additive to the background , but interfere with it ( section  [ sec : nonadd ] ) .",
    "it was then shown how to combine datasets in the most general case where the datasets are coming from dissimilar experiments and dissimilar observables ( section  [ sec : combo ] ) .",
    "then the case of simultaneous estimation of multiple pois was shown in section  [ sec : multidim ] .",
    "all the above computations assumed no systematic uncertainties , until section  [ sec : systematics ] , where the principle was laid out to perform convolution of systematic uncertainties , and two complete examples where shown .",
    "finally , section  [ sec : coverage ] shows the way to compute the coverage of a bayesian upper limit , which can be interesting to someone who , being used to frequentist limits , may appreciate coverage .",
    "emphasis has been given to the practical implementation of all computations , and remarks have been made to gain some insight in the results .",
    "g.c .  thanks the theorists michele redi and mads frandsen for their encouragement to proceed with this work , expecting it to be welcomed with interest by many theorists and experimentalists .",
    "g.  aad _ et al . _ [ atlas collaboration ] , `` search for new physics in the dijet mass distribution using 1 fb@xmath122 of @xmath123 collision data at @xmath124  tev collected by the atlas detector , '' submitted to plb , arxiv:1108.6311 [ hep - ex ] .",
    "glen cowan , a survey of unfolding methods for particle physics , http://www.ippp.dur.ac.uk/old/workshops/02/statistics/proceedings/cowan.pdf              s.  chatrchyan _ et al . _ [ cms collaboration ] , `` search for resonances in the dijet mass spectrum from 7 tev @xmath123 collisions at cms , '' phys .",
    "b * 704 * , 123 ( 2011 ) [ arxiv:1107.4771 [ hep - ex ] ] .",
    "g.  aad _ et al . _ [ atlas collaboration ] , `` search for new physics in dijet mass and angular distributions in @xmath123 collisions at @xmath124 tev measured with the atlas detector , '' new j.  phys .",
    "* 13 * , 053044 ( 2011 ) [ arxiv:1103.3864 [ hep - ex ] ] .",
    "alex read in nim a 425 ( 1999 ) 357 - 369 linear interpolation of histograms"
  ],
  "abstract_text": [
    "<S> this article is geared towards theorists interested in estimating parameters of their theoretical models , and computing their own limits using available experimental data and elementary mathematica@xmath0 code . </S>",
    "<S> the examples given can be useful also to experimentalists who wish to learn how to use bayesian methods . </S>",
    "<S> a thorough introduction precedes the practical part , to make clear the advantages and shortcomings of the method , and to prevent its abuse . </S>",
    "<S> the goal of this article is to help bridge the gap between theory and experiment . </S>"
  ]
}