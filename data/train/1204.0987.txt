{
  "article_text": [
    "`` physical unclonable functions '' ( pufs ) are electronic hardware devices that are hard to reproduce and can be uniquely identified @xcite . they promise to enable qualitatively novel security mechanisms ( see e.g. @xcite ) and have consequently become a `` hot topic '' in hardware security@xcite .",
    "the present work asks the question `` what characteristics exactly define the qualitative novelty of the puf concept ? '' .",
    "we hope that a precise answer will aid the security evaluation of existing pufs and help to develop new ideas for puf security mechanisms .",
    "we searched for    1 .   a formal definition of the properties that are required from a hardware device to be called `` puf '' , and a 2 .   a formal definition of the criteria that have to be fulfilled to consider a puf `` unclonable '' .",
    "the formal puf definition should not suffer from weaknesses of previous definitions ( see section [ previ ] ) , encompass at least the large majority of the existing puf constructions , and be as flexible as possible , i.e. does not restrict further progress in puf development ( e.g. by demanding constructional details , like the amount of stored information ) .",
    "this aim is achieved in section [ form ] . after formulating a simple definition of puf - security ( based on armknecht et al.@xcite ) in section [ pufsec ] we delineate pufs from some closely related security concepts ( section [ related ] ) and",
    "outline the elements of a puf - security evaluation ( section [ security ] ) . in a second part of the paper we systematically analyse and classify puf security mechanisms and calculate their quantitative security levels against attacks that attempt mathematical cloning ( section [ prac ] ) .",
    "the aims of this section are to give a quantitative answer to maes & verbauwhedes@xcite question whether mathematically - unclonable pufs are possible in principle , and to apply and thereby illustrate the puf - definitions of the first part of the paper . in section [ discu ]",
    "we characterise the qualitative novelty of pufs as a new primitive of physical cryptography and discuss the future use and development of pufs .",
    "there have already been several proposal for the first definition of required puf properties .",
    "gassend et al.@xcite who invented the term `` puf '' ( earlier work by pappu was on the slightly different concept of a physical one - way function@xcite ) demand that the function must be `` easy to evaluate '' , i.e. it must efficiently yield a response value `` r '' for a challenge argument `` c '' . and `` hard to predict ( characterize ) '' .",
    "the latter property means that an attacker who has obtained a polynomial number of c  r pairs ( crps ) but has no longer physical access to the puf can only extract a negligible amount of information about the r for a random c. rhrmair et al.@xcite criticised this definition because the information content of finite physical objects is always polynomially bound , and therefore no puf fulfilling this definition can exist .",
    "they propose an alternative formal definition in which the puf must only be hard to predict for an attacker `` who may execute any physical operation allowed by the current stage of technology '' .",
    "maes & verbauwhede@xcite chose to _ exclude _ unpredictability from their `` least common property subset '' of pufs , because they put into question whether it is possible in principle to construct a mathematically unclonable puf .",
    "they demand that a puf is `` easy to evaluate '' ( property `` evaluatable '' ) and that it is `` reproducible '' , meaning that a c always leads to the same r within a small error .",
    "moreover they demand `` physical unclonability '' i.e. that it must be `` hard '' for an attacker to construct a device that reproduces the behaviour of the puf .",
    "however , pufs that are mathematically clonable are also physically clonable because the mathematical algorithm for pf can then be implemented on a device that is then a functional physical clone of the puf .",
    "summarizing , a first generation of definitions roughly defined pufs to be devices that are efficiently evaluatable and are mathematically and physically unclonable .",
    "they remain unsatisfactory for two reasons :    1 .",
    "most of the devices currently called pufs do not fulfill these definitions ( according to rhrmair et al.@xcite there are only some `` candidates '' ) , i.e. the definition does evidently not really capture the puf concept .",
    "they combine the definition of a puf with the definition of its security , i.e. points 1 . and 2 .",
    "a puf is defined by its unclonability i.e. its security against attacks .",
    "this is problematic because an open - ended security analysis of a puf clearly must have an `` insecure puf '' as one a priori possible outcome .",
    "based on the above definitions an `` insecure puf '' is a paradox , pufs would be secure by definition .",
    "these two problems were elegantly solved in a seminal paper by armknecht et al.@xcite who propose to formalize a puf as  physical function ( `` pf '' ) - which is a physical device that maps bit - string - challenges `` c '' to bit - string - responses `` r '' .",
    "the unclonability is recognized by armknecht et al . as only one crucial security property , that they further formally define in great detail .",
    "we will supply a simplified version of their general security definition in section [ pufsec ] below .",
    "following armknecht et al . , the puf definition 1 .",
    "consists in an answer to the question : what are the required characteristics of pf ( ) in order to be a puf ?",
    "armknecht et al .",
    "do not demand any specific mathematical properties but only that a pf is a `` probabilistic procedure '' that maps a set of challenges to a set of responses and that internally pf is a combination of a physical component and an evaluation procedure that creates a response .",
    "armknecht et al .",
    "explain that the responses rely heavily on the properties of the physical component but also on uncontrollable random noise ( hence `` probabilistic '' ) .",
    "this definition of pf ( ) still faces the following problem :    * consider a standard authentication chip with a stored secret in a physically protected memory that calculates a response from the challenge and the secret .",
    "such a chip must contain a `` physical component '' ( the memory ) and an evaluation procedure ( its read - out ) that fulfills armknecht et al.s definition because some ( very small ) uncontrollable random noise is unavoidable even in standard computer memories .",
    "there is also no reason why a well designed standard authentication chip can not posess armknecht et al.s security properties",
    ".    therefore , even though armknecht et al.s definitions constitute great progress of lasting value , they still do not capture the distinctive properties of the puf concept . in practice armknecht et al .",
    "define all devices that run any challenge - response protocol as pufs .",
    "in the following we assume armknecht et al.s model of a puf as physical function pf ( ) ( see section [ previ ] ) .",
    "we break up the physical function pf ( ) into three steps ( see fig.([explot ] ) ) .",
    "c , s@xmath0,s and r are bit strings .    1 .   in the first `` physical read - out '' step pf@xmath1 = s@xmath0 , internal information s@xmath0 ( the `` raw secret '' ) is physically read - out from the puf in response to a challenge c foreseen by the system architecture .",
    "2 .   in an optional second step pf@xmath2(s@xmath0 ) = s error correction and/or privacy amplification are performed , such that errors in the read - out are corrected and parts of s@xmath0 which may be known by the attacker ( e.g. by guessing parts of the challenge ) are removed by privacy amplification algorithms .",
    "3 .   in an optional third step pf@xmath3(s ) = r , some additional algorithm is performed with s as input to calculate the final response r. typically pf@xmath3 is some cryptographic protocol that proves the possession of s without disclosing it .    in many existing puf architectures",
    "the challenge c is an address of information inside the puf which is output as the response r. e.g. in arbiter pufs@xcite c defines the choice of a set of delay switches whose cumulative delay path defines s ( and from this r ) .",
    "our idea is that the possibility for this mode of addressing , rather than its `` unclonability '' , defines a puf .",
    "the challenge c can then be understood as a key required for physical access to the response r. r remains secret without access to c. +        a security architecture based on this idea requires certain properties of pf ( ) which define the puf concept :    * * formal definition 1 of a puf * _ a hardware device is called `` puf '' if : + a. a physical function pf@xmath2(pf@xmath1 ( ) ) which is deterministic for a specific set of challenges @xmath4 , can be evaluated with each challenge at least once and + b. the value s = pf@xmath2(pf@xmath1(c ) ) changes with its argument , for all outside challenges c @xmath5 @xmath4 , i.e. pf@xmath2(pf@xmath1(c ) ) = s is not a constant function . _",
    "one difference to some previous puf definitions is that pf ( ) is not required to be _ easily _ evaluatable",
    ". an efficient evaluation of s is certainly a desirable design goal , but there is no reason why a device with inefficient read out can not be a puf by definition . +",
    "another difference to most previous definitions is that it allows a puf to be clonable . as an example",
    "consider the following physical function that fulfills the above definition 1 :    * pf@xmath1(any c with more 1s than 0s ) = 1001101101 * pf@xmath1(any c with more 0s than 1s or equal number of 1s and 0s ) = 0001101000    clearly a puf with this pf@xmath1 can be reproduced by a trivial algorithm , i.e. it is trivially mathematically clonable .",
    "this is a desirable property because `` clonable pufs '' do exist in the real world and should not present a puf definition with a paradox .",
    "`` unclonability '' is then a property that is aimed for , rather than achieved by definition .",
    "analogously `` cryptography '' aims for secrecy ( crypto ) rather than achieving it by definition . even though it is a child s game to break it ,",
    "the cesar cipher is a valid cryptographical algorithm according to this definition .",
    "consequently cryptographic algorithms are commonly defined to be `` key - dependent injective '' ( rather than `` unbreakable '' ) mappings@xcite .",
    "+ where does this leave puf security ?",
    "it is not possible in principle to extract the secret s from a puf without knowing of the challenge .",
    "this is true even for the above insecure puf .",
    "however in the example above it is easy to reproduce pf@xmath1 , and therefore , as soon as the challenge becomes known , s becomes known .",
    "therefore the crucial necessary objective for the security of a puf is the unclonability of pf@xmath2(pf@xmath1 ( ) ) . in the next section",
    "we make this insight more precise . a complete and quantitative set of security requirements ( i.e. with requirements on their length @xmath6 , the number of independent challenges n etc .",
    ") can only be made in the context of a concrete puf architecture .",
    "one example is discussed further in section [ mrt ] .",
    "even though the response of a puf can in principle be used for various purposes , we will conclude in section [ discu ] that one central puf capability is the distribution of remote authentication secrets .",
    "if s is used for authentication purposes , an attacker must be able to fully predict it , i.e. a partial prediction of s = pf@xmath2(pf@xmath1(c ) ) for a given argument c will not be considered a successful attack in the following .",
    "therefore , the natural `` basic objective '' of puf security is that the attacker can not predict a complete , correct bit string s for a given bit string c.      security can only be defined relative to an attack model , that lays down the assumptions about the security environment .",
    "we assume in the following two models from the literature that seem realistic in practice .",
    "the first one models an attempt to break armknecht et al.s@xcite selective unclonability . ] .",
    "it does not put any restriction on the attack strategy , therefore adaptive choices of challenges are possible .",
    "the second one is an attempt to do the same with a certain reasonable amount of insider knowledge .",
    "both models assume that the attacker has only access to one single puf , i.e. attacks exploiting correlations between different pfs are excluded by assumption ( see armknecht et al.@xcite for the general case ) .",
    "+ _ attack model 1 : `` outsider attack '' : the attacker has physical access only to the attacked puf only for a finite amount of time @xmath7t@xmath8 . after this access period , she tries to predict a secret s from the puf to a challenge c , randomly chosen from the set of all challenges .",
    "she has no knowledge of the set of challenges and secrets that will be used during the active lifetime of the puf or any further previous knowledge of the puf . _",
    "+ _ attack model 2 : `` insider attack '' : the attacker has physical access only to the attacked puf only for a finite amount of time @xmath7t@xmath8 .",
    "after this access period she tries to predict a secret s from the puf to a challenge c , randomly chosen from the set of all challenges .",
    "she has no knowledge of the set of challenges and secrets that will be used during the active lifetime of the puf but she has all other information that the manufacturer of the puf has about the attacked individual puf . _ + the attack models assume that the attacker tries to predict s rather than r , because pf@xmath3 might be protected with non - puf security mechanisms , e.g. with a secure tamper - resistance scheme in combination with a secure crypto algorithm .",
    "such a security mechanism shall remain out of our consideration because we aim to define the security of the puf itself .",
    "+ security against a model-2 attacker corresponds to unclonability against an attacker who has most of the inside knowledge about the puf production , but who can not directly manipulate the production process .",
    "this unclonability is weaker than `` existential unclonability '' ( see footnote 1 ) but perhaps more relevant in practice .",
    "the puf - security definition now follows from the requirement that the attack shall be unsuccessful :    * * formal definition 2 of the puf - security objective * + _ a puf is secure against an attack of a model-1 ( `` selectively unclonable '' ) attacker if a model-1 attacker can compute or physically copy the function pf@xmath2(pf@xmath1(c ) ) = s for not more than a negligible fraction l of challenges from the set of all possible challenges . here",
    "`` compute '' means via a computation independent of the puf and corresponds to `` mathematical cloning '' .",
    "`` physically copy '' means to create a device that functionally reproduces pf@xmath2(pf@xmath1(c ) ) and corresponds to `` physical cloning '' .",
    "_    replacing the model-1 by a model-2 attacker defines a puf that is `` insider selectively unclonable '' .",
    "l is the security level of a secure puf , i.e. the probability for an attacker to successfully predict the secret s for a challenge c without being in posession of the puf after the access period .",
    "+ a precise quantification of `` negligible '' , i.e. the decision which upper limit of l is required , can not be made on the level of this general definition because it depends on the detailed security environment .",
    "l is analogous to the required probability p of a successful brute force attack in classical cryptography that depends on the key length .",
    "we propose as a reasonable upper limit on l that it is `` negligible on a terrestrial scale '' which has been estimated by emile borel as @xmath9 10@xmath10@xcite .",
    "in this section we delineate the concept of a puf as defined in section [ form ] and [ securepuf ] from five closely related concepts .",
    "let us first differentiate between a puf and a conventional physical function that serves the same function as a puf ( called `` conventional unclonable function '' cuf in the following ) .",
    "a cuf contains secret information that is protected by tamper resistance , by anti side - channel- and fault - induction - attack measures and by a cryptographic algorithm that protects the secret from disclosure via the response .",
    "a cuf does not fulfill the puf definition 1 .",
    ", because the secret does not depend on the challenge . in other words : the first physical secret readout step pf@xmath1(c ) is a constant function in a cuf .",
    "+ puf and cuf differ qualitatively in the way they protect the secret . in a puf",
    "the lack of knowledge of the challenges protects the secret s in a similar sense that the lack of knowledge of a cryptographical key protects the clear text in a cipher text .",
    "there is no analogous `` key '' in a cuf .",
    "its security mechanisms merely rely on physical barriers and arrangements that prevent access to secret information .",
    "devices that extract physical information with `` non - standard '' methods are currently called puf even if there is no ( or effectively a single fixed internal ) challenge ( e.g. in sram pufs@xcite ) . in this case pf@xmath1 ( ) is formally constant , so that such devices are no pufs in the sense of our definition 1 .",
    "we endorse rhrmair et al.s suggestion@xcite to call information extracted in this way in general `` physically obfuscated keys '' ( poks ) .",
    "this limit of n=1 is the only one where devices that are currently called pufs , would no longer be classified as puf under our proposed definition .",
    "we find this appropriate because while poks can enable valuable tamper - resistance mechanisms ( see below ) , they are not the _ qualitatively _ novel primitive of physical cryptography that pufs promise to be ( see section [ discu ] for further discussion of the nature of this primitive ) .",
    "+ the protection by obfuscation is valuable : it consists in the extra - time an attacker needs to learn the non - standard readout mechanism or position in a standard memory where an obfuscated key has to be stored at least temporarily .",
    "poks are delineated from cuf only by the `` non - standard '' qualifier because stored information is _ always _ physical@xcite .",
    "the secrets of pufs will usually be stored in a non - standard way , i.e. they will also be poks .",
    "but this is no necessary requirement for a puf .",
    "there is no fundamental reason why pufs can not have `` standard '' computer memories ( see e.g. shics@xcite , a puf using a standard crossbar memory ) . +",
    "physically obfuscated functions ( pofs ) may also appear in puf architectures .",
    "they are defined as computation with non - standard physical processes , e.g. via scattering of light or folding of proteins .      in both deterministic and physical random number generators",
    "the initial read - out step pf@xmath1 ( the read out of the seed ) does not depend on a challenge c. in secure deterministic rngs pf@xmath1(c ) must be a constant function . in physical rngs",
    "pf@xmath1 is not constant but intrinsically random , i.e. not deterministic .",
    "therefore , rngs do not meet the puf definition 1 .      in controlled pufs@xcite tamper - resistance measures prevent the attacker from obtaining c ",
    "s@xmath0 pairs from the puf .",
    "only the c  r pairs - from which s@xmath0 can not be derived if pf@xmath3 is a suitable , secure cryptographical algorithm - can be accessed by an attacker .",
    "it seems likely that pufs e.g. used in smart cards will eventually all be controlled pufs , because such an additional well understood security layer stands to reason .",
    "however the security of pufs themselves should be analysed under the assumption of no such a control because if one trusts the control mechanism , mathematically clonable pufs suffice anyway .",
    "rhrmair et al.@xcite defined a puf to be `` strong '' if it `` has so many c ",
    "r pairs ... that an attack ... based on exhaustively measuring the c  r pairs has a negligible probability of success '' . in our nomenclature",
    "a strong puf is roughly a mrt - puf that fulfills our second security requirement ( see section [ mrt ] below , for further explanation of mrt ) .",
    "it is thus appropriate to call them  strong  , but there can be secure pufs which are not `` strong '' in rhrmair et al.s sense , e.g. eur - pufs(see section [ quant ] below for further explanation of eur ) .",
    "a main purpose of the present proposed formal puf definitions 1 . and 2 . of the concept",
    "`` secure puf '' is to establish a consistent basis for security evaluations and certifications of pufs .",
    "what is the structure of an evaluation on this basis ?",
    "+ if the proposed puf fulfills definition 1 , the basic informal questions of a security evaluation based on definition 2 are :    1 .",
    "which form has pf@xmath1(c ) and by which physical mechanism is s@xmath0 extracted ?",
    "what is the form of pf@xmath2(s@xmath0)=s and how is the function evaluated physically ?",
    "what is the total information content in the set of all secrets s ? 4 .",
    "for what fraction l of the allowed challenges can pf@xmath2(pf@xmath1(c ) ) be either mathematically computed or physically copied ?",
    "5 .   which comprehensible physical security mechanisms prevent an attacker to compute or copy pf@xmath2(pf@xmath1(c ) ) for more than a fraction l of all challenges ?",
    "answers to questions 1 . - 4 . allow to evaluate quantitative and comprehensible security levels against `` mathematical - cloning brute force attacks '' ( see section [ prac ] ) .",
    "question 5 will have a more qualitative answer , similar to answers to the question whether a mathematical cryptographic algorithm is secure against non - brute force attacks .",
    "the holy grail of puf construction is to construct pufs that are unclonable i.e. fulfill the security definition 2 ( section [ securepuf ] ) .",
    "if an attacker succeeds to access the puf s internal secrets , she will usually be able to compute pf@xmath2 . because physical reproduction of a puf without knowledge of its internal secrets will probably be hard in practice , puf security mechanisms must above all prevent the attacker from _ computing _ pf@xmath2 . in other words :",
    "mathematical unclonability is the hardest nut .",
    "therefore we will classify the known puf security mechanism and calculate their security level against brute - force mathematical cloning attacks . + up to now all proposed and constructed pufs are based on a mechanism that we propose to call `` minimum readout time''(mrt ) and that is further discussed in subsection [ mrt ] .",
    "all these existing pufs turn out to fulfill our puf - definition 1 , i.e. they `` remain '' pufs , even in case they have turned out to be clonable ( see below ) . because currently the mrt mechanism dominates the field",
    ", one might be tempted to equate the very concept of pufs with it .",
    "however , the flexibility of our definition allows a completely different puf security mechanism that we call `` erasure upon read - out''(eur ) ( see section [ quant ] ) for devices .",
    "one concrete eur puf , the quantum puf will be introduced below .",
    "+ these examples show that our proposed definitions have achieved their aims : nearly all existing ( mrt ) pufs can be included in its scope , but its flexibility allows to include completely novel puf constructions ( the eur pufs ) .",
    "this well known puf security mechanism is to store a large enough number n of c  s pairs on the puf so that the total time @xmath11 to read them all out is much longer than the time @xmath7t@xmath8 during which an attacker possesses the puf .",
    "@xmath12 is the read - out time for one c  s pair .",
    "the maximal fraction of pairs the attacker can read - out is then @xmath7t@xmath8/@xmath13 = l@xmath14 .",
    "l@xmath14 is the security level against mathematical - cloning brute force attacks",
    ". + pappu s optical puf@xcite , the arbiter puf@xcite and nearly all other current pufs are mrt - pufs . ] .",
    "these constructions are valid pufs according to our definition because their values of pf@xmath2 changes with the challenge .",
    "+ however , many of the existing pufs are insecure according to our definition because rhrmair et al.@xcite succeeded to employ machine - learning methods that allow to infer pf@xmath2(pf@xmath1 ( ) ) from a small fraction of all c  r for which only short @xmath7t@xmath8 is necessary@xcite . because all c ",
    "s pairs can be thus predicted , the security level against machine - learning attacks l@xmath15 = 1 which is `` not negligible '' in general , i.e. the puf must be considered mathematically clonable according to puf - security definition 2 .",
    "+ the exact form of pf ( ) depends on the detailed architecture of the mrt puf . in general mrt pufs can be hardened against mathematical cloning if their pf@xmath2(pf@xmath1 ) fulfills the following demands : + * security requirements for the mrt - puf *    * _ n must satisfy : @xmath16 ) _ * _ suppose pf@xmath2(pf@xmath1(c@xmath17 ) ) = s@xmath18 with n = 1 ... n where both c@xmath17 and s@xmath18 contain @xmath6 bits .",
    "then the combined information content ( entropy ) i of all c@xmath17 and s@xmath18 must satisfy : i @xmath19 2 n @xmath6 _ * _ the set of challenges to be used in operation must not be contained in any form in the puf .",
    "_ * _ the lengths of the challenge @xmath6 and response @xmath20 must both fulfill : @xmath6,@xmath20 @xmath19 log@xmath2(n ) . _",
    "the first condition expresses that to prevent brute force mathematical - cloning attacks the number of stored c ",
    "r pairs n must be extremely large if l = 10@xmath10 , ( see section [ securepuf ] on the choice of l ) . with representative values of @xmath21",
    "= 1 day and @xmath12 = 1 second the required n would be on the order of 10@xmath22 which is exponentially larger than e.g. storable in common data storage devices of much larger size than a typical puf . this is the sense in which a secure mrt - puf requires the storage of an `` exponentially large '' secret .",
    "the second condition expresses that in order to reliably ward successful machine - learning attacks pf@xmath2 must be just an ordered list of c  s pairs with random values that can not be represented in any more compact form .",
    "the third requirement prevents an attack in which only the set of challenges to be used in the field operation of a puf ( which is much smaller than @xmath4 in secure mrt pufs ) are extracted in an attack .",
    "the fourth constraint is necessary to avoid a decrease in the the effective l.      consider a puf with only a single c  s pair foreseen by the system architecture . because there is at least one other non - foreseen c , there are then at least two possible c. a novel puf security mechanism requires the following : + * security requirements for `` erasure upon readout '' ( eur ) puf *    * _",
    "the correct s is returned if the challenge c is correct ( i.e. the one foreseen by the puf s architecture ) and s is erased and returns a random value if it is not .",
    "_ * _ the length of the challenge @xmath6 and response @xmath20 must both fulfill @xmath6,@xmath20 @xmath19 log@xmath2(1/l ) . _ * _ the set of challenges to be used in operation must not be contained in any form in the puf .",
    "_    eur pufs can fulfill the puf - definition 1 if they are non - constant pfs that are deterministic for the foreseen set of challenges .",
    "for eur pufs - completely opposite to the mrt case ( see section [ mrt ] ) - the total number of challenges `` n '' can remain as small as 2 but still be secure because by way of the second and third security requirement the probability to guess the correct challenge is only l and challenging with the wrong challenge will erase s by the first requirement .",
    "n can be chosen to as many different challenges as are actually needed during the practical deployment of the pufs .",
    "+ the only concrete `` erasure upon readout '' architecture proposed up to now , is wiesner s `` quantum money '' and `` quantum unforgeable subway token''@xcite that can be described as an electronic hardware device running a challenge - response protocol ( such a kind of `` money '' or `` token '' has to be ) and that fulfill our definition 1 of a puf . in such a `` quantum - puf '' the secret information consists of @xmath6 quantum - mechanical two - state systems ( `` qubits '' ) that are prepared either in one of the two quantum mechanical so called `` fock '' states @xmath23 or @xmath24 ( base @xmath25 0 ) or in either one of the states @xmath26 ( @xmath23 + @xmath24 ) or @xmath26 ( @xmath23 - @xmath24 ) ( base @xmath25 1 ) . @xmath23 and @xmath26 ( @xmath23 + @xmath24 ) encode a `` 0 '' secret bit and @xmath24 and @xmath26 ( @xmath23 - @xmath24 ) encode a `` 1 '' secret bit .",
    "the challenge bits indicate the correct chosen measurement bases . the raw secret s@xmath0 is encoded with the choice of the state within a chosen basis according to the rule stated above . + in order to decode or copy s@xmath0 , it is necessary to know in which of the two bases @xmath25 0 or @xmath25 1 the @xmath6 qubits for one challenge were prepared .",
    "if a qubit is read out in a wrong base , the laws of quantum mechanics determine that the read - out result is a perfect random number and additional read out attempts will again yield this random number , rather than the original , correct number . the physical function pf of the quantum - puf is thus given as : + * quantum - eur pf@xmath1 ( ) * :    * first read - out : + pf@xmath1(correct base bit ) = correct bit of s@xmath0 + pf@xmath1(incorrect base bit ) = random bit . *",
    "any further read - out in the same base : + pf@xmath1 ( ) = same bit as in first read - out    evidently in the first read - out pf@xmath1 is not constant and deterministic for the foreseen c i.e. a quantum - puf fulfills definition 1 .",
    "reading out a c  s pair more than once is possible , but after the first read - out , the information is no longer secure because the qubits are no longer in a quantum - mechanical superposition of states .",
    "+ in the most simple case without any read - out errors or inefficiencies ( so that no further processing is done pf@xmath2(s@xmath0)= s@xmath0 ) and implementation mistakes ( an assumption that will be difficult to fulfill @xcite ) the only potentially successful attack is to guess the challenge . on average , for half of the bits the guess will be correct and the correct corresponding bits of s@xmath0 will be output . for the other half the probability to get the correct output bit is 1/2 .",
    "the total probability to get a correct output bit of s@xmath0 is therefore 0.75 and l@xmath8 = @xmath27 , which is the absolute ( i.e. not only mathematical - cloning brute force ) security level of a quantum - puf against this attack .",
    "e.g. with a secret s@xmath0 consisting of 128 qubits , l @xmath9 10@xmath10 thus fulfilling the criterion for a secure puf with borel s estimate for an upper bound on l ( see section [ securepuf ] ) .",
    "wiesner s quantum money , interpreted as a `` quantum puf '' , thus proves that an absolutely unclonable puf is not in contradiction to the laws of physics . +",
    "the use of quantum - pufs for authentication is beyond the reach of current technology because qubits are unavoidably read out on very short timescales ( presently qubits can not be isolated for longer than milliseconds@xcite ) by interactions with their environment .",
    "as explained above , quantum - pufs are no longer secure after read - out .",
    "quantum cryptography@xcite can be described as sending a quantum - puf in the form of a chain of photons in order to distribute its secret s for use as cryptographic key . in the laboratory",
    "such a `` light - field '' puf remains in the initially prepared coherent state for no longer than about a millisecond .",
    "the protection of secrets in hardware devices that need to access these secrets in their normal operation - a necessary condition for any authentication procedure - can not be implemented with methods of mathematical cryptography alone .",
    "some physical protection mechanism is needed .",
    "the conventional tamper resistance mechanisms ( employed in cufs see section [ cuf ] ) rely on protecting the memory with physical barriers .",
    "cufs withstand known , vigorous direct attacks typically for not longer than a few months@xcite .",
    "we showed that pufs are a _ qualitatively _ novel alternative .",
    "the secret is protected by the absence of information from the device of where of where the challenge is stored . in cufs and",
    "poks this information must exist on the device because otherwise the response can not be evaluated , even if it is protected by direct , physical barriers .",
    "thereby pufs protect the secret by a novel genuine primitive of physical cryptography .",
    "the possibility of realizing pufs based on the principles of quantum mechanics demonstrates that in principle the laws of physics allow to construct absolutely secure pufs .",
    "this situation motivates more security - related physics research on unclonable quantum - puf and mrt - puf , to invent entirely new puf construction principles .",
    "the real puf promise are pufs that withstand any known , practical attack , period , i.e. provide a level of authenticity protection similar to the one provided by mathematical cryptography for confidentiality .",
    "+ in the future pufs will probably authenticate hardware devices . if alice knows the c ",
    "s@xmath0 pairs of a puf she gave to bob ( e.g. from the designer of the puf ) she can publicly broadcast a challenge and be sure that the correct response s can only be created on bob s original puf . therefore effectively pufs allow the remote distribution of authenticated secret entropy ( the s for bob ) via sending the challenges ( the c chosen and sent by alice ) over standard channels .",
    "these entropy could `` update '' the secrets in conventional unclonable functions . in this way existing architectures based on cufs could be augmented by pufs without the need for a completely new puf security architecture . +",
    "* acknowledgements . *",
    "we thank r. breithaupt , u. gebhardt , m. ullmann , c. wieschebrink and anonymous referees at the trusted 2011 and pilates 2012 workshops for helpful discussion and criticism on earlier versions of this manuscript .",
    "f. armknecht et al . , _ memory leakage - resilient encryption based on physically unclonable functions . _ ,",
    "asiacrypt 09 proceedings of the 15th international conference on the theory and application of cryptology and information security : advances in cryptology .",
    "r.pappu , _ physical one - way functions _ , phd thesis , mit , 2001 ; r.pappu , b.recht , j.taylor , n.gershenfeld , science , 297,2026 ( 2002 ) .",
    "u.rhrmair , j.sltner , f.sehnke , _ on the foundations of physical unclonable functions _ ,",
    "cryptology eprint archive , report 2009/277 .",
    "u. rhrmair , f. sehnke , j. slter , g. dror , s. devadas , and j. schmidhuber , _ modeling attacks on physical unclonable functions _ , in acm conference on computer and communications security ( ccs ) , 2010 , pp . 237  249 .",
    "u. rhrmair , c. jaeger , m. algasinger , _ an attack on puf - based session key exchange , and a hardware - based countermeasure : erasable pufs _ , 15th international conference on financial cryptography and data security , st .",
    "lucia , february 28  march 4 , 2011 .      c. tarnovsky ,",
    "_ deconstructing a `` secure '' processor _ , black hat conference , washington , https://www.blackhat.com/presentations/bh-dc-10/tarnovsky_chris/blackhat-dc-2010-tarnovsky-dasp-slides.pdf , ( 2010 ) ."
  ],
  "abstract_text": [
    "<S> the characteristic novelty of what is generally meant by a `` physical unclonable function '' ( puf ) is precisely defined , in order to supply a firm basis for security evaluations and the proposal of new security mechanisms . </S>",
    "<S> a puf is defined as a hardware device which implements a physical function with an output value that changes with its argument . </S>",
    "<S> a puf can be clonable , but a secure puf must be unclonable . + </S>",
    "<S> this proposed meaning of a puf is cleanly delineated from the closely related concepts of `` conventional unclonable function '' , `` physically obfuscated key'',``random - number generator '' , `` controlled puf '' and `` strong puf '' . </S>",
    "<S> the structure of a systematic security evaluation of a puf enabled by the proposed formal definition is outlined . </S>",
    "<S> practically all current and novel physical ( but not conventional ) unclonable physical functions are pufs by our definition . </S>",
    "<S> thereby the proposed definition captures the existing intuition about what is a puf and remains flexible enough to encompass further research . </S>",
    "<S> + in a second part we quantitatively characterize two classes of puf security mechanisms , the standard one , based on a minimum secret read - out time , and a novel one , based on challenge - dependent erasure of stored information . </S>",
    "<S> the new mechanism is shown to allow in principle the construction of a `` quantum - puf '' , that is absolutely secure while not requiring the storage of an exponentially large secret . </S>",
    "<S> the construction of a puf that is mathematically and physically unclonable in principle does not contradict the laws of physics . </S>"
  ]
}