{
  "article_text": [
    "statistical modelling plays a key role in extracting the structures of a system that may be hidden behind observed data and using them for prediction or control .",
    "a statistical model approximates the true generative process of the data , which is generally expressed by a probability distribution .",
    "although it is necessary to adopt an appropriate statistical model , this will depend on the purpose of the modelling , and the definition of appropriateness is not unique .",
    "akaike proposed an information criterion for model selection , where the appropriate model is defined using kullback  leibler divergence @xcite .",
    "this criterion validates the relative effectiveness of the model under consideration , and mathematically expresses the contribution of the model to the prediction performance .    since the systemization of the least absolute shrinkage and selection operator ( lasso ) @xcite , which simultaneously achieves variable selection and estimation , sparse estimation has been attracting considerable attention in fields such as signal processing @xcite and machine learning @xcite . in general , sparse estimation is formulated as the problem of minimizing the estimating function penalized by sparse regularization .",
    "the estimated variables have zero components , a property known as sparsity . to find the sparse representation of the system from among various candidates , a seemingly hidden rule that controls the system",
    "is sought .",
    "similar to lasso , @xmath0 regularization is widely used because of its convexity , which yields mathematical and algorithmic tractability @xcite .",
    "in addition , non - convex regularization , such as using the @xmath1-norm @xcite , has been studied to obtain a sparser representation than that given by @xmath0-norm regularization @xcite .",
    "furthermore , the smoothly clipped absolute deviation ( scad ) and adaptive lasso penalty have been investigated @xcite to acquire the oracle property , which the lasso estimator does not possess .",
    "the emergence of the estimation paradigm associated with sparsity requires the development of appropriate model selection criteria . in sparse estimation",
    ", the determination of the regularization parameter can be regarded as the selection of a model from a family of models that have different sparsities controlled by the regularization parameter .",
    "in addition to the cross - validation ( cv ) method @xcite , which is a simple numerical approach for sparse estimation @xcite , analytical model selection methods with lower computational costs have been developed .",
    "one such method involves estimating the _ generalized degrees of freedom _ ( gdf ) @xcite .",
    "the gdf is a key quantity for mallows @xmath2 , a model selection criterion based on the prediction error @xcite . in particular ,",
    "the derivation of gdf has been studied in linear regression with a known variance @xcite .",
    "the analytical form of the gdf for lasso @xcite and elastic net regularization @xcite are well known , but general expressions for other regularizations have not yet been derived .    in this paper , we propose an analytical method based on statistical physics for the derivation of gdf in sparse estimation .",
    "certain aspects of statistical physics developed for random systems have already been applied to sparse estimation problems @xcite .",
    "the analysis of typical properties provides physical interpretations of the problems based on phase transition pictures , and this contributes to the development of algorithms @xcite .",
    "the statistical physical method can be applied to the estimation of gdf for sparse regularization .",
    "we show that gdf is expressed as the effective fraction of non - zero components for any sparse regularization .",
    "this expression is a mathematical realization of the meaning of gdf in terms of `` model complexity '' @xcite .",
    "the remainder of this paper is organized as follows .",
    "section [ sec : overview ] summarizes the model selection criterion discussed in this paper and highlights some previous related studies on sparse estimation .",
    "section [ sec : problem ] explains our problem setting for the estimation of gdf .",
    "sections [ sec : replica ] and [ sec : replica_one_body ] describe our analytical method based on the replica method for sparse estimation .",
    "section [ sec : gdf ] represents the behaviour of gdf for @xmath0 , elastic net , @xmath3 , and scad regularization .",
    "section [ sec : bp ] proposes the numerical calculation of gdf using the belief propagation algorithm , and discusses the generality of the results . in section [ sec : l0_exhaustive ] , the approximation performance of our method for the calculation of gdf is examined in the case of @xmath3 regularization . finally , section [ sec : summary ] concludes the paper .",
    "in this section , we explain the criteria for model selection discussed in this paper . in addition , we summarize previous studies and identify our contributions .",
    "we focus on the parametric model , where the true generative model of @xmath4 , denoted by @xmath5 , is approximated by @xmath6 with a parameter @xmath7 , where @xmath8 is a parameter space .",
    "the parameter is estimated under the given model to effectively describe the true distribution using training data @xmath9 @xmath10 .",
    "let us prepare a set of candidate models @xmath11 for the approximation of the true distribution , where @xmath12 is the estimated parameter under the @xmath13-th model .",
    "model selection is then the problem of adopting a model based on a certain criterion .",
    "the information criterion evaluates the quality of the statistical model based on kullback ",
    "leibler ( kl ) divergence .",
    "kl divergence describes the closeness between the true distribution @xmath5 and the assumed distribution @xmath14 as @xmath15- e_{z\\sim q(z)}[\\log{p(z|\\hat{\\bm{\\theta}}_{\\rm ml}(\\bm{w } ) ) } ] , \\label{eq : def_kl}\\end{aligned}\\ ] ] where @xmath16 is the maximum likelihood estimator from the training sample @xmath17 .",
    "the dependency on the model appears only in the second term of , called the predicting log - likelihood , i.e. @xmath18 $ ] .",
    "therefore , the maximization of the predicting log - likelihood is the basis for the information criterion .",
    "unfortunately , it is generally impossible to evaluate the predicting log - likelihood , because we can not determine the true distribution .",
    "we define the estimator of the predicting log - likelihood using the empirical distribution @xmath19 which corresponds to the maximum log - likelihood .",
    "the expected value of the difference between the predicting log - likelihood and the maximum log - likelihood , termed the bias , is given by @xmath20\\right],\\end{aligned}\\ ] ] where @xmath21 .",
    "the information criterion is defined as an unbiased estimator of the negative predicting log - likelihood : @xmath22 where @xmath23 is an unbiased estimator of the bias , and the coefficient @xmath24 is a conventional value .",
    "the optimal model is defined as that which minimizes @xmath25 among the models in @xmath26 .",
    "intuitively , the first and second terms represent the training error and the complexity of the model , respectively . as the complexity of the model increases ,",
    "the model can express various distributions .",
    "however , overfitting is likely to occur , which hampers the prediction of unknown data .",
    "the information criterion selects the model that achieves the best trade - off between the training error and the level of model complexity .",
    "the values of @xmath27 can be calculated asymptotically .",
    "in particular , when the statistical model contains the true model , namely a parameter @xmath28 exists such that @xmath29 , the information criterion is known as akaike s information criterion ( aic ) , where the bias term @xmath27 is reduced to the dimension of the parameter @xmath30 @xcite .",
    "the criterion explained thus far is for models constructed by maximum likelihood estimation . to determine the parameter with other learning strategies , we focus on maximum likelihood estimation under regularization , where the gdf facilitates the extension of the information criterion @xcite .",
    "a general expression of gdf is naturally derived from another model selection criterion , namely , mallows @xmath2 @xcite .",
    "the prediction of unknown data is another criterion for the evaluation of a model .",
    "we define the squared prediction error per component as @xmath31,\\end{aligned}\\ ] ] where @xmath32 is the estimate of @xmath17 and @xmath33 is independent of @xmath34 , but each component of @xmath35 is generated according to the same distribution as @xmath17 . when the training sample is generated as @xmath36 , where @xmath37 is the @xmath38-dimensional identity matrix , mallows @xmath2 , calculated as @xmath39 is an unbiased estimator of the prediction error . here , @xmath40 is the training error and @xmath41 is an unbiased estimator of gdf defined by @xmath42 which quantifies the complexity of the model @xcite , where @xmath43)(\\hat{\\bm{w}}(\\bm{w})-e_{\\bm{w}}[\\hat{\\bm{w}}(\\bm{w})])]$ ] . in the framework of @xmath2 ,",
    "the optimal model is defined as that which minimizes @xmath44 among the models in @xmath26 .",
    "another expression of gdf is given by @xcite @xmath45 , \\label{eq : gdf_ye}\\end{aligned}\\ ] ] which corresponds to the expectation of stein s unbiased risk estimate ( sure ) for the prediction error @xcite .",
    "gdf was originally introduced as an extension of the degrees of freedom in the linear estimation rule for a general modelling procedure in the form @xcite . when the assumed model obeys a gaussian distribution @xmath46 with a known variance , and taking @xmath47 , aic ( normalized by the number of training samples )",
    "is given by @xcite @xmath48 equations and indicate that model selection based on aic and that based on @xmath2 give the same result ; they are proportional to each other @xmath49 .      the regression problems with sparse regularization is formulad as @xmath50 where @xmath51 measures the difference between training data @xmath17 and its fit using regression coefficients @xmath52 under the predictor matrix @xmath53 , and @xmath54 is the regularization term with the regularization parameter @xmath55 that enhances zero components in @xmath52 .",
    "the regularization parameter determines the number of predictors used in the expression of the data distribution , and the model distribution under the determined number of predictors can be regarded as a model : @xmath56 , where @xmath57 is the support of the regularization parameter .",
    "therefore , tuning the regularization parameter @xmath55 corresponds to model selection .",
    "however , in general , the derivation of aic based on the asymptotic expansion is not straightforwardly applicable to sparse regularization . in such cases",
    ", @xmath2 is useful for deriving the model selection criterion when the squared error is considered . in lasso",
    ", it is mathematically proven that , when the number of training samples is greater than the number of predictors , the ratio of the number of non - zero regression coefficients to the number of training samples is an unbiased estimator of the degrees of freedom in a finite sample @xcite .",
    "however , the derivation of gdf is analytically difficult for general sparse regularizations . to overcome this difficulty , gdf computation techniques",
    "have been developed using the parametric bootstrap method @xcite and sure @xcite .    in the present paper ,",
    "we propose an estimation technique for gdf using the replica method under a replica symmetric ( rs ) assumption for linear regression with gaussian i.i.d .",
    "the replica symmetric analysis for the estimation problems under sparse regularization are shown in @xcite . in these papers ,",
    "the replica method is employed to study phase transition or the property of estimators .",
    "we extend this analytical method for the calculation of gdf that is not taken into account in the current formalism of the replica analysis .",
    "the technique we propose is applicable to general sparse regularization . using our method , the correspondence between gdf and the effective fraction of non - zero components in the large - system - size limit is shown to be independent of the form of regularization .",
    "our approach differs from previous methods in which gdf has been derived for specific types of regularization .",
    "we apply our method to @xmath0 , elastic net , @xmath3 , and scad regularization to obtain the gdf .",
    "the results shown here for @xmath0 and elastic net regularization are weaker than those in previous studies , where the unbiased estimator of gdf , @xmath58 , is derived for one instance of the predictor . however ,",
    "our method is consistent with previous results , which supports the validity of our approach .",
    "furthermore , our method can be applied to non - convex sparse regularizations such as @xmath3 and scad , and extends the discussion of gdf to general sparse regularization . for the @xmath3 case ,",
    "the solution under the rs assumption is always unstable against perturbations that break the replica symmetry , but we show that gdf under the rs assumption approximates the true value of gdf . in the case of scad regularization , our method can identify the most appropriate model based on the prediction error within the range of the rs assumption when the mean of the data is sufficiently small .",
    "the generality of the result in terms of the correspondence between gdf and the effective fraction of non - zero components is discussed using a belief propagation algorithm for other predictor matrices .",
    "we apply a linear regression model with sparse regularization @xmath59 , where @xmath55 is a regularization parameter , to a set of training data @xmath60 : @xmath61 where the column vectors of @xmath62 and components of @xmath63 correspond to predictors and regression coefficients , respectively . here , the coefficient of the squared error , @xmath64 , is introduced for mathematical convenience .",
    "the variable @xmath52 to be estimated here corresponds to the parameter @xmath30 in the previous section , and the number of non - zero components in @xmath52 corresponds to the number of parameters used in the model .",
    "we introduce the posterior distribution of @xmath52 : @xmath65 where @xmath66 is the normalization constant .",
    "the distribution as @xmath67 is the uniform distribution over the minimizers of .",
    "estimate of the solution of under a fixed set of @xmath68 , denoted by @xmath69 , is given by @xmath70 where @xmath71 denotes the expectation according to at @xmath72 .",
    "using this estimate @xmath69 of @xmath52 , the training sample @xmath73 is estimated as @xmath74 to understand the typical performance of , we calculate the expectation of the training error with respect to @xmath73 and @xmath53 , @xmath75.\\end{aligned}\\ ] ] at a sufficiently large system size @xmath76 , we set the scaling relationship as @xmath77 and @xmath78 , where @xmath79 is the number of non - zero components of @xmath80 .",
    "the training error relates to the free energy density @xmath81 as @xcite @xmath82 where @xmath83 $ ] and @xmath84 . \\ ] ] the expectation of the regularization term @xmath85 is derived separately from @xmath81 , as shown in the following section .",
    "hence , the training error is derived as @xmath86    for the calculation of gdf , we introduce external fields @xmath87 and @xmath88 , and define the extended posterior distribution as @xmath89 where @xmath90 is the normalization constant .",
    "we define the extended free energy density as @xmath91 where @xmath92 $ ] and @xmath93 .",
    "we derive the following quantities from the extended free energy density : @xmath94=\\frac{1}{\\alpha}\\frac{\\partial }   { \\partial \\kappa}f_{\\kappa,\\nu}\\big|_{\\kappa,\\nu=0}\\label{eq : gdf_tmp1}\\\\ \\hat{m}_y&\\equiv\\frac{1}{m}\\sum_\\mu e_{\\bm{y},\\bm{a}}[\\sum_i a_{\\mu i}\\hat{x}_i(\\bm{y},\\bm{a})]=\\frac{1}{\\alpha}\\frac{\\partial}{\\partial \\nu}f_{\\kappa,\\nu}\\big|_{\\kappa,\\nu=0}. \\label{eq : gdf_tmp2}\\end{aligned}\\ ] ] using these , the gdf for a gaussian training sample is derived as @xmath95 where @xmath96 and @xmath97 are the mean and variance of the training sample , respectively .",
    "further , @xmath2 given by is the unbiased estimator of the prediction error .",
    "hence , the expectation of the prediction error with respect to @xmath73 and @xmath53 , @xmath98,\\end{aligned}\\ ] ] is given by @xmath99",
    "for the derivation of gdf , we resort to the replica method @xcite .",
    "the rs calculations for @xmath3 and @xmath0 minimization are shown in the typical performance analysis of compressed sensing @xcite and dictionary learning @xcite .",
    "we summarize the analytical method and explain how it can be extended to the evaluation of gdf .",
    "hereafter , we consider gaussian i.i.d . predictors @xmath100 .",
    "we calculate the generating function @xmath101 using the following identity : @xmath102= \\lim_{n\\to 0}\\frac{e_{\\bm{y},\\bm{a}}[z_{\\beta}^n(\\bm{y},\\bm{a})]-1}{n}. \\label{eq : replica}\\end{aligned}\\ ] ] assuming that @xmath103 is a positive integer , we can express the expectation of @xmath104 by introducing @xmath103 replicated systems : @xmath105&=\\int d\\bm{a}d\\bm{y}p_{a}(\\bm{a})p_y(\\bm{y})\\int   d\\bm{x}^{(1)}\\cdots d\\bm{x}^{(n)}\\\\ & \\times\\exp\\big[\\sum_{a=1}^n\\big\\{-\\frac{\\beta}{2}||\\bm{y}-\\bm{ax}^{(a ) }   ||_2 ^ 2-\\beta r(\\bm{x}^{(a)};\\eta)\\big\\}\\big ] , \\label{eq : e_replica}\\end{aligned}\\ ] ] where @xmath106 and @xmath107 .",
    "we characterize the microscopic states of @xmath108 with the macroscopic quantities @xmath109 introducing the identity for all combinations of @xmath110 @xmath111 the integration with respect to @xmath53 leads to the following expression : @xmath105&=\\int d{\\cal q}{\\cal   s}({\\cal q})\\int \\{d\\bm{u}^{(a)}\\}p_u(\\{\\bm{u}^{(a)}\\}|{\\cal q})\\int d\\bm{y}p_y(\\bm{y})\\\\ & \\times\\exp\\big\\{-\\frac{\\beta}{2}\\sum_a||\\bm{y}-\\bm{u}^{(a)}||^2_2\\big\\},\\end{aligned}\\ ] ] where each component of @xmath112 , denoted by @xmath113 , is statistically equivalent to @xmath114 , and @xmath115 is a matrix representation of @xmath116 . setting @xmath117 ,",
    "its probability distribution is given by @xcite @xmath118 and the function @xmath119 is given by @xmath120 where @xmath121 is the conjugate variable for the integral representation of the delta function in , and @xmath122 is the matrix representation of @xmath123 .    to obtain an analytic expression with respect to @xmath124 and",
    "take the limit as @xmath125 , we restrict the candidates for the dominant saddle point to those of rs form as @xmath126 for @xmath67 , rs order parameters scale to keep @xmath127 , @xmath128 , and @xmath129 of the order of unity . under the rs assumption ,",
    "the free energy density is given by @xmath130 where @xmath131 denotes extremization with respect to the variables @xmath132 .",
    "the function @xmath133 , where the subscript @xmath134 denotes the dependency on the regularization , is given by @xmath135 where @xmath136 is the random field that effectively represents the randomness of the problem introduced by @xmath73 and @xmath53 , and @xmath137 .",
    "the solution of @xmath138 concerned with the effective single - body problem , denoted by @xmath139 , is statistically equivalent to the solution of the original problem .",
    "therefore , the expectation of the regularization term is derived as @xmath140    the variables @xmath141 are determined by saddle point equations to satisfy the extremum conditions of the free energy density : @xmath142 note that the functional form of the parameters @xmath143 and @xmath144 does not depend on the regularization , but the values of @xmath145 and @xmath146 are regularization - dependent . at the extremum ,",
    "the parameters @xmath146 and @xmath145 are related to the physical quantities by @xmath147\\\\ \\chi&=\\lim_{\\beta\\to\\infty}\\frac{\\beta}{m}\\sum_{i=1}^ne_{\\bm{y},\\bm{a}}[\\langle||\\bm{x}||^2_2\\rangle_\\beta-||\\langle\\bm{x}\\rangle_\\beta||^2_2 ] , \\label{eq : chi_physical}\\end{aligned}\\ ] ] and can be expressed using @xmath148 as @xmath149    the extended free energy density with the external fields @xmath87 and @xmath88 is given by @xmath150 to evaluate @xmath151 for non - zero @xmath87 and @xmath88 , one has to solve the saddle point equation at non - zero @xmath87 and @xmath88 to determine the saddle point value of @xmath145 .",
    "however , since one would only need to evaluate derivatives of @xmath151 at @xmath152 to obtain gdf , the saddle point value of @xmath145 that is to be used in such evaluations should remain the same as that obtained in the calculation of @xmath81 . from ",
    ", gdf is obtained as @xmath153 where @xmath145 and @xmath144 satisfy the saddle point equations and , respectively .",
    "this expression is also independent of the form of the regularization",
    ". the effective single - body problem can be interpreted as a scalar estimation problem in which @xmath138 is estimated on the basis of the prior ( regularization ) @xmath154 and the random observation @xmath155 , which is assumed to be generated as @xmath156 , where @xmath157 is the gaussian observation noise .",
    "if one uses the observation itself in the single - body problem as an estimate of @xmath138 , then it is an unbiased estimator of @xmath138 and its variance is @xmath158 .",
    "however , the actual variance of the estimates can change according to the regularization .",
    "the variable @xmath145 is the rescaled variance of the system expressed as .",
    "therefore , gdf corresponds to the effective fraction of the non - zero components of @xmath80 ( parameters ) , which is estimated by dividing the variance of the total system by that of one component when the observation is used as the estimate .",
    "the effective fraction of the non - zero components is measured under the assumption that the regularization does not change the variance of one component from @xmath158 .",
    "if this assumption is correct and the fluctuation of non - zero components is the unique source of the system s fluctuation , gdf is considered to be equal to the ratio of the number of non - zero components to the number of training samples .",
    "the rs solution discussed thus far loses local stability under perturbations that break the symmetry between replicas in a certain parameter region .",
    "known as the de almeida  thouless ( at ) instability @xcite , this phenomenon appears when @xmath159 in general , when at instability appears , we have to construct the full - step replica symmetry breaking ( rsb ) solution for an exact evaluation .",
    "however , the rs solution remains meaningful as an approximation @xcite .",
    "as shown in the previous section , some regularization - dependency appears in the effective single - body problem .",
    "we now apply the analytical method to @xmath0 , elastic net , @xmath3 , and scad regularization .",
    "the ratio of the number of non - zero components to the number of training samples is denoted by @xmath160 , and we focus on the physical region @xmath161 , where the number of unknown variables is smaller than the number of known variables .     for ( a ) @xmath0 ( @xmath162 ) ,",
    "( b ) elastic net ( @xmath163 ) , ( c ) @xmath3 ( @xmath162 ) , and ( d ) scad regularization ( @xmath164 ) .",
    "the dashed diagonal lines of gradient 1 are the maximizers under no regularization , and the threshold @xmath165 denotes @xmath166.,title=\"fig:\",width=288 ]   for ( a ) @xmath0 ( @xmath162 ) , ( b ) elastic net ( @xmath163 ) , ( c ) @xmath3 ( @xmath162 ) , and ( d ) scad regularization ( @xmath164 ) .",
    "the dashed diagonal lines of gradient 1 are the maximizers under no regularization , and the threshold @xmath165 denotes @xmath166.,title=\"fig:\",width=288 ]   for ( a ) @xmath0 ( @xmath162 ) , ( b ) elastic net ( @xmath163 ) , ( c ) @xmath3 ( @xmath162 ) , and ( d ) scad regularization ( @xmath164 ) .",
    "the dashed diagonal lines of gradient 1 are the maximizers under no regularization , and the threshold @xmath165 denotes @xmath166.,title=\"fig:\",width=288 ]   for ( a ) @xmath0 ( @xmath162 ) , ( b ) elastic net ( @xmath163 ) , ( c ) @xmath3 ( @xmath162 ) , and ( d ) scad regularization ( @xmath164 ) .",
    "the dashed diagonal lines of gradient 1 are the maximizers under no regularization , and the threshold @xmath165 denotes @xmath166.,title=\"fig:\",width=288 ]      in the @xmath0 regularization @xmath167 , the maximizer of the single - body problem is given by @xmath168 where @xmath169 denotes the sign of @xmath4 and is @xmath170 when @xmath171 .",
    "figure [ fig : single - body ] ( a ) shows the behaviour of @xmath172 at @xmath173 and @xmath162 . setting @xmath174 , the fraction of non - zero components is given by the probability that the solution of the rs single - body problem is non - zero : @xmath175 , where @xmath176 and @xmath177 the regularization - dependent saddle point equations are given by @xmath178 where @xmath179 from and , the solutions of the saddle point equations and can be derived as @xmath180 substituting the saddle point equations , the free energy density and the expectation of the regularization term are given by @xmath181=\\alpha(\\chi\\hat{\\chi}-q\\hat{q}),\\end{aligned}\\ ] ] respectively .",
    "hence , the training error is given by @xmath182 at instability appears when @xmath183 which is outside the region of interest for physical parameters .",
    "equation leads to the following expression for the gdf : @xmath184 this expression is consistent with the result in @xcite , which verifies the validity of this rs analysis for the derivation of gdf .",
    "elastic net regularization , given by @xmath185 was developed to encourage the grouping effect , which is not exhibited by @xmath0 regularization , and to stabilize the @xmath0 regularization path @xcite . here",
    ", the coefficient @xmath64 is introduced for mathematical convenience , and @xmath186 and @xmath187 correspond to @xmath0 regularization and @xmath188 regularization , respectively .",
    "the solution of the effective single - body problem for elastic net regularization is given by @xmath189 the behaviour of this solution is shown in fig .",
    "[ fig : single - body ] ( b ) for @xmath190 , @xmath191 , and @xmath192 where @xmath193 .",
    "the fraction of non - zero components is given by @xmath194 , and the regularization - dependent saddle point equations are given by @xmath195 at the saddle point , the free energy density and the expectation of the regularization term can be simplified as @xmath196\\\\ & = \\alpha\\{\\hat{\\chi}\\chi-(\\hat{q}+\\eta_2)q\\}+\\frac{\\alpha\\eta_2 q}{2},\\end{aligned}\\ ] ] respectively .",
    "hence , the training error is given by @xmath197 at instability arises when @xmath198 the right - hand side is always greater than @xmath199 because @xmath200 and @xmath201 .",
    "therefore , the rs solution is always stable under symmetry breaking perturbations in the physical parameter region @xmath202 .    from",
    ", the gdf for elastic net regularization is given by @xmath203 which reduces to the gdf for @xmath0 regularization at @xmath186 .",
    "an unbiased estimator of the gdf for one instance of @xmath53 is derived in @xcite as @xmath204 where @xmath205 is the set of indices of non - zero components , and the columns @xmath206 constitute the submatrix @xmath207 . the number of the components of @xmath205 is denoted by @xmath208 .",
    "our expression for df corresponds to the typical value ( or the expectation ) of @xmath209 for a gaussian random matrix @xmath53 .",
    "the physical implications suggested by the cavity method @xcite , which is complementary to the replica method , supports the correspondence relationship between @xmath144 in the replica method and the gram matrix of @xmath53 .",
    "this correspondence indicates that our rs analysis is valid for the derivation of gdf under elastic net regularization .",
    "as shown in , the gdf for elastic net regularization deviates from @xmath160 . the @xmath188 regularization term in elastic net regularization changes the variance of the non - zero components from @xmath158 to @xmath210 .",
    "hence , the effective fraction of the non - zero components measured by @xmath211 does not coincide with @xmath212 . by defining the rescaled estimates of the single - body problem as @xmath213 ,",
    "the corresponding variance is reduced to @xmath214 from , and this gives @xmath215 .",
    "this rescaling corresponds to that shown in @xcite , which was introduced to cancel out the shrinkage caused by @xmath188 regularization and improve the prediction performance .",
    "taking the limit as @xmath216 , the gdf for @xmath188 regularization can be obtained where the estimate is not sparse .",
    "the solution of the effective single - body problem is given by @xmath217 and the function @xmath218 is given by @xmath219 this expression leads to the following gdf : @xmath220 which corresponds to the limit as @xmath221 of the elastic net regularization .",
    "an unbiased estimator of the gdf for one instance of @xmath53 is proposed as @xcite @xmath222 corresponds to the expectation of @xmath209 for a gaussian random matrix @xmath53 .",
    "the @xmath3 regularization is expressed by @xmath223 , which corresponds to the number of non - zero components in @xmath52 .",
    "the solution to the single - body problem for @xmath3 regularization is given by @xmath224 where @xmath225 , and by setting the fraction of non - zero components to @xmath226 , we can derive @xmath227 figure [ fig : single - body ] ( c ) shows the @xmath4-dependence of the maximizer @xmath228 at @xmath173 and @xmath162 .",
    "the regularization - dependent saddle point equations  are given by @xmath229 and have two solutions : finite @xmath145 and @xmath146 and infinite @xmath145 and @xmath146 .",
    "we denote the finite and infinite solutions as @xmath230 and @xmath231 , respectively . using and",
    ", the finite solution can be simplified as @xmath232 where @xmath233 by definition , @xmath234 and @xmath235 should be positive , and so  are only valid when @xmath236 . according to a local stability analysis of around @xmath237 , solution",
    "@xmath238 is a locally stable solution of the rs saddle point equation when @xmath239 , where as it is unstable when @xmath236 .",
    "therefore , the stable solution of the rs saddle point equation changes from @xmath240 to @xmath238 at @xmath241 .",
    "note that the stability discussed here refers to the rs solution , and does not relate to at instability .",
    "the free energy density is simplified by substituting the saddle point equations as @xmath242 the second term of corresponds to the expectation of the regularization term , and so the training error can be derived as @xmath197 the gdf is given by @xmath243 the term @xmath244 , given by , in the gdf originates from the discontinuity of the single - body problem at the threshold @xmath245 , as shown in ( c ) .",
    "in addition to the fluctuation generated by the non - zero components , this discontinuity induces fluctuations in the system and increases the gdf from @xmath212 .    under @xmath3 regularization , at instability always appears , but the estimated gdf under the rs assumption can be regarded as an approximation of the true value of the gdf , as shown in sec .",
    "[ sec : l0_exhaustive ] .",
    "our calculations based on the one - step rsb assumption indicate that the form of the gdf , as the fraction of non - zero components plus the discontinuity term , is unchanged , although the values of these two terms does change ( unreported ) .",
    "scad regularization is a non - convex sparse regularization in which the estimator has the desirable properties of being unbiased , sparse , and continuous @xcite .",
    "mathematically , the scad estimator is asymptotically equivalent to the oracle estimator @xcite .",
    "scad regularization is given by @xmath246 where @xmath247 and @xmath248 are parameters that control the form of the regularization .",
    "the maximizer of the single - body problem for scad regularization is given by @xmath249 figure [ fig : single - body ] ( d ) shows an example of the behaviour of the maximizer @xmath250 at @xmath251 , and @xmath162 , where three thresholds are given by @xmath252 , @xmath253 , and @xmath254 .",
    "the threshold @xmath255 gives the fraction of non - zero components as @xmath256 . between the thresholds",
    "@xmath255 and @xmath257 , and beyond the third threshold @xmath258 , the estimate @xmath250 behaves like the @xmath0 and @xmath3 estimates , respectively . between @xmath257 and @xmath258 ,",
    "the estimate transits linearly between the @xmath0 and @xmath3 estimates .",
    "the function @xmath218 for scad regularization is derived as @xmath259 where @xmath260\\\\ \\nonumber \\pi_2&=\\frac{\\hat{\\chi}}{\\hat{q}-\\frac{\\eta}{a-1}}\\big[\\frac{2}{\\sqrt{\\pi}}\\big\\{\\big(\\theta_{s2}-\\frac{2\\theta_{s3}\\eta}{\\hat{q}(a-1)}\\big)e^{-\\theta_{s2}^2 } -\\big(1-\\frac{2\\eta}{\\hat{q}(a-1)}\\big)\\theta_{s3}e^{-\\theta_{s3}^2}\\big\\}\\\\ & + \\big\\{1 + 2\\big(\\frac{\\eta\\theta_{s3}}{\\hat{q}(a-1)}\\big)^2\\big\\}\\pi_4\\big]\\\\ \\pi_3&=\\frac{\\hat{\\chi}}{\\hat{q}}\\big[\\frac{2\\theta_{s3}}{\\sqrt{\\pi}}e^{-\\theta_{s3}^2}+{\\rm erfc}(\\theta_{s3})\\big]\\\\ \\pi_4&={\\rm erfc}(\\theta_{s2})-{\\rm erfc}(\\theta_{s3}).\\end{aligned}\\ ] ] the regularization - dependent saddle point equations are given by @xmath261 , \\ ] ] and the expectation of the regularization term is given by @xmath262 substituting these equations into the free energy density , we get @xmath197 the at instability condition is given by @xmath263>1,\\end{aligned}\\ ] ] which reduces to that for @xmath0 regularization as @xmath264",
    ".    there are three solutions of @xmath265 : @xmath266 , @xmath267 , and @xmath268 .",
    "for sufficiently large @xmath248 , the finite solution @xmath240 is a locally stable solution of the rs saddle point equation when @xmath269 beyond the range of , the stable rs solution is replaced by @xmath238 . for sufficiently small @xmath55 ,",
    "the stable rs solution can switch from @xmath238 to @xmath270 depending on the scad parameter , but this is not important in estimating the gdf , because both solutions give the same gdf value .",
    "the gdf for scad regularization can be summarized as @xmath271 as @xmath264 , we get @xmath272 and solution @xmath240 is always a stable rs solution , satisfying ; hence , the gdf reduces to that for @xmath0 regularization .",
    "the second term of the gdf for solution @xmath240 arises from the weight between the thresholds @xmath257 and @xmath258 .",
    "the manner of assigning the non - zero components to this transient region between the @xmath0 and @xmath3 estimates increases the fluctuation in the system , and the gdf does not coincide with @xmath212 .",
    "we note the pathology of solution @xmath270 under the rs assumption . as shown in the solution to the single - body problem ( fig .",
    "[ fig : single - body ] ( d ) ) , the magnitude relation @xmath273 should hold .",
    "however , the @xmath274 solution leads to @xmath275 with finite @xmath276 .",
    "solution @xmath270 appears in the region where at instability appears , and so this non - physical phenomenon is considered to be caused by an inappropriate rs assumption .",
    "hence , we must construct the rsb solution to correctly describe the gdf corresponding to solution @xmath270 .",
    "-dependence of gdf for @xmath0 , elastic net , @xmath3 , and scad regularization at @xmath277 , @xmath278 , and @xmath279 . the parameters for elastic net and scad regularization are @xmath280 , @xmath281 , and @xmath282 , and the vertical dashed line indicates the appearance of the at instability for scad regularization , @xmath283 .",
    "the @xmath0 result corresponds to the line @xmath215.,width=336 ]    figure [ fig : gdf ] illustrates the @xmath212-dependence of the gdf for @xmath0 , the elastic net with @xmath280 , @xmath3 , and scad regularization with @xmath281 and @xmath282 at @xmath277 , @xmath278 , and @xmath279 . at each point of @xmath212 ,",
    "the regularization parameters @xmath55 for @xmath0 , @xmath3 and scad regularization and @xmath284 for elastic net regularization are controlled such that @xmath160 . under @xmath0 regularization ,",
    "the gdf is always equal to @xmath212 , as shown in . in elastic net regularization ,",
    "the gdf is less than @xmath212 as the @xmath188 parameter @xmath284 increases . for @xmath3 regularization ,",
    "the rs solution @xmath240 is unstable at @xmath285 in this parameter region , and is replaced by solution @xmath238 , which gives @xmath286 . in scad regularization ,",
    "the solution @xmath240 loses local stability within the rs assumption at @xmath287 , and at instability appears before the rs solution @xmath240 becomes unstable at @xmath288 ( denoted by the dashed vertical line in . )",
    "-dependence of the prediction error @xmath289 for @xmath0 , elastic net , and scad regularization at @xmath277 , @xmath290 , and @xmath279 .",
    "the parameters for elastic net and scad regularization are @xmath280 , @xmath281 , and @xmath282 .",
    "( b ) region where the magnitude relationship between each regularization changes .",
    "( c ) prediction error for @xmath3 regularization .",
    ", width=384 ]    figure [ fig : pre_error ] shows the prediction error for the same parameter region as . at @xmath279 ,",
    "the prediction error is equivalent to the expectation of aic . in the entire range of @xmath212 shown in ( a ) , the rs solutions for @xmath0 , elastic net , and scad regularization are stable under symmetry breaking perturbations .",
    "thus , we can identify the value of @xmath212 that minimizes the prediction error for each regularization . in this case , the models with @xmath291 ( denoted by ) , @xmath292 ( @xmath293 ) , and @xmath294 ( @xmath295 ) are selected for @xmath0 , elastic net , and scad regularization , respectively . in the current problem setting , sparse estimation with scad regularization minimizes the prediction error within the rs region when the mean of the data is sufficiently small . to identify the appropriate model using rs analysis ,",
    "it is useful to standardize the data . as shown in ( b ) , the magnitude of the prediction errors at @xmath296 , @xmath297 , and @xmath298 runs in descending order as elastic net@xmath299scad@xmath300 , scad@xmath299elastic net@xmath300 , and scad@xmath301elastic net , respectively .",
    "the estimates @xmath80 have different supports depending on the regularization , even when the regularization parameters are controlled to give a certain value of @xmath212 . a comparison of the prediction errors within the framework of rs analysis guides the choice of regularization for each value of @xmath212 .    the prediction error for @xmath3 regularization under the rs assumption is shown in ( c ) alongside those for other regularization types .",
    "the rs prediction error is minimized at @xmath302 .",
    "this indicates that the appropriate model under rs analysis has a non - zero component of @xmath303 .",
    "our analysis assumes that the number of non - zero components is @xmath304 ; hence , the derived model selection criterion can not identify the appropriate model in the current problem setting for @xmath3 regularization .",
    "the correspondence between replica analysis and the belief propagation ( bp ) algorithm suggests that the typical properties of bp fixed points at the large - system - size limit can be described by the rs saddle point @xcite .",
    "thus , we may expect that the numerically obtained gdf will be consistent with the rs analysis at finite system sizes using the bp algorithm . for the ordinary least squares with a regularization that can be written as , a tentative estimate of the @xmath305-th component at step @xmath306 , denoted by @xmath307 ,",
    "is given by the solution to the single - body problem with the substitutions @xmath308 and @xmath309 @xcite , where @xmath310 and setting @xmath311 , @xmath312 the variable @xmath313 represents the variance of @xmath314 , and its determination rule depends on the regularization .",
    "for @xmath0 and elastic net regularization , the variable is given by @xmath315 and @xmath316 respectively .",
    "for these regularizations , the gdf at the bp fixed point converges to that given by rs analysis as the system size increases .",
    "however , for these regularizations , the gdf can be calculated using least angle regression ( lars ) @xcite , which has a lower computational cost than the bp algorithm .",
    "hence , there is no need to introduce the bp algorithm . in the case of @xmath3 regularization",
    ", the variable @xmath317 is given by @xmath318 unfortunately , at instability appears across the whole parameter region for @xmath3 regularization , and the bp algorithm does not converge .",
    "for scad regularization , no numerical method for the precise evaluation of gdf has been proposed . as shown in the previous section",
    ", scad regularization gives a parameter region where the rs solution is stable .",
    "therefore , the bp algorithm is useful as a method of numerically calculating the gdf for scad regularization .",
    "the variable @xmath319 for scad regularization is given by @xmath320 after updating the estimates @xmath321 , we can numerically evaluate the value of gdf using with the data estimates @xmath322 . to ensure convergence , appropriate damping",
    "is required at each update .    , @xmath278 , and @xmath279 for ( a ) @xmath323 , @xmath282 for @xmath324 ( @xmath277 ) , ( b ) @xmath281 , @xmath325 for @xmath324 ( @xmath277 ) , ( c ) @xmath326 , @xmath327 for @xmath328 ( @xmath329 ) , and ( d ) @xmath281 , @xmath330 for @xmath328 ( @xmath329 ) .",
    "the bp results , averaged over 100 realizations of @xmath68 , are denoted by circles .",
    "the theoretical estimation of gdf by rs analysis is denoted by solid and dashed lines for the rs and rsb regions , respectively . to provide a visual guide",
    ", the dashed line has a gradient of @xmath199.,title=\"fig:\",width=288 ] , @xmath278 , and @xmath279 for ( a ) @xmath323 , @xmath282 for @xmath324 ( @xmath277 ) , ( b ) @xmath281 , @xmath325 for @xmath324 ( @xmath277 ) , ( c ) @xmath326 , @xmath327 for @xmath328 ( @xmath329 ) , and ( d ) @xmath281 , @xmath330 for @xmath328 ( @xmath329 ) .",
    "the bp results , averaged over 100 realizations of @xmath68 , are denoted by circles .",
    "the theoretical estimation of gdf by rs analysis is denoted by solid and dashed lines for the rs and rsb regions , respectively . to provide a visual guide",
    ", the dashed line has a gradient of @xmath199.,title=\"fig:\",width=288 ] , @xmath278 , and @xmath279 for ( a ) @xmath323 , @xmath282 for @xmath324 ( @xmath277 ) , ( b ) @xmath281 , @xmath325 for @xmath324 ( @xmath277 ) , ( c ) @xmath326 , @xmath327 for @xmath328 ( @xmath329 ) , and ( d ) @xmath281 , @xmath330 for @xmath328 ( @xmath329 ) .",
    "the bp results , averaged over 100 realizations of @xmath68 , are denoted by circles .",
    "the theoretical estimation of gdf by rs analysis is denoted by solid and dashed lines for the rs and rsb regions , respectively . to provide a visual guide",
    ", the dashed line has a gradient of @xmath199.,title=\"fig:\",width=288 ] , @xmath278 , and @xmath279 for ( a ) @xmath323 , @xmath282 for @xmath324 ( @xmath277 ) , ( b ) @xmath281 , @xmath325 for @xmath324 ( @xmath277 ) , ( c ) @xmath326 , @xmath327 for @xmath328 ( @xmath329 ) , and ( d ) @xmath281 , @xmath330 for @xmath328 ( @xmath329 ) .",
    "the bp results , averaged over 100 realizations of @xmath68 , are denoted by circles .",
    "the theoretical estimation of gdf by rs analysis is denoted by solid and dashed lines for the rs and rsb regions , respectively . to provide a visual guide",
    ", the dashed line has a gradient of @xmath199.,title=\"fig:\",width=288 ]    as for the replica analysis , we apply the bp algorithm for the case of gaussian random data @xmath73 and predictors @xmath53 .",
    "figure [ fig : bp ] shows the numerically calculated gdf given by the bp algorithm at @xmath331 , @xmath278 , and @xmath279 .",
    "the bp algorithm is updated until @xmath332 for each component , and the result is averaged over 100 realizations of @xmath68 .",
    "the solid and dashed lines represent the analytical results given by the replica method for the rs and rsb regions , respectively . in the rs regime ,",
    "the numerically calculated gdf from the bp algorithm coincides with that evaluated by the replica method .",
    "the rs analysis discussed so far has been applied to gaussian i.i.d .",
    "random predictors .",
    "its extension to other predictors is not straightforward . to check the generality of the gdf being given by the effective fraction of non - zero components @xmath333 at the rs saddle point for other predictor matrices , we resort to the bp algorithm .",
    "the typical properties of @xmath313 and @xmath334 at the bp fixed point denoted by @xmath335 and @xmath336 are described in the replica method by @xmath145 and @xmath144 of the rs saddle point at the large - system - size limit .",
    "therefore , it is reasonable to define the effective fraction of non - zero components at the bp fixed point as @xmath337 where the overline represents the average over @xmath73 and @xmath53 .",
    "if @xmath338 and the gdf from coincide at the bp fixed point , it is considered that the correspondence between gdf and the effective fraction of non - zero components holds at the rs saddle point . for @xmath0 , elastic net , and scad regularization , we examine the behaviour of gdf and @xmath338 under two predictors @xcite in a parameter region where the bp algorithm converges .",
    "example 1 : : :    gaussian predictors with pairwise correlation .",
    "the correlation between    predictors @xmath339 and @xmath340 is set to    be @xmath341 , and the predictors are normalized such    that @xmath342 .",
    "example 2 : : :    the predictors are generated as @xmath343 where the components of the    @xmath38-dimensional vectors    @xmath344 , @xmath345 ,    and @xmath346 are i.i.d .",
    "gaussian random    variables with mean zero and variance @xmath199 , and    @xmath347 is a parameter that takes an integer value smaller    than @xmath348 .",
    "the predictors are normalized such    that @xmath342 .",
    "-dependence of gdf and @xmath338 at bp fixed point of @xmath349 , @xmath350 @xmath351 for ( a ) @xmath0 , ( b ) elastic net of @xmath280 , and ( c ) scad regularization of @xmath352 and @xmath282 under the predictor matrix of example 2 with @xmath353 .",
    "we used 1000 samples of predictor matrices to calculate the gdf and @xmath338 at the bp fixed point .",
    "the @xmath212-region where the bp algorithm converges within @xmath354 steps is shown .",
    "the dashed lines in ( a ) and ( b ) denote the results reported in @xcite and @xcite.,title=\"fig:\",width=192 ] -dependence of gdf and @xmath338 at bp fixed point of @xmath349 , @xmath350 @xmath351 for ( a ) @xmath0 , ( b ) elastic net of @xmath280 , and ( c ) scad regularization of @xmath352 and @xmath282 under the predictor matrix of example 2 with @xmath353 .",
    "we used 1000 samples of predictor matrices to calculate the gdf and @xmath338 at the bp fixed point .",
    "the @xmath212-region where the bp algorithm converges within @xmath354 steps is shown .",
    "the dashed lines in ( a ) and ( b ) denote the results reported in @xcite and @xcite.,title=\"fig:\",width=192 ] -dependence of gdf and @xmath338 at bp fixed point of @xmath349 , @xmath350 @xmath351 for ( a ) @xmath0 , ( b ) elastic net of @xmath280 , and ( c ) scad regularization of @xmath352 and @xmath282 under the predictor matrix of example 2 with @xmath353 .",
    "we used 1000 samples of predictor matrices to calculate the gdf and @xmath338 at the bp fixed point .",
    "the @xmath212-region where the bp algorithm converges within @xmath354 steps is shown .",
    "the dashed lines in ( a ) and ( b ) denote the results reported in @xcite and @xcite.,title=\"fig:\",width=192 ]    -dependence of gdf and @xmath338 at bp fixed point of @xmath349 , @xmath350 @xmath351 for ( a ) @xmath0 , ( b ) elastic net of @xmath280 , and ( c ) scad regularization of @xmath352 and @xmath282 under the predictor matrix of example 2 with @xmath353 .",
    "we used 1000 samples of predictor matrices to calculate the gdf and @xmath338 at the bp fixed point .",
    "the @xmath212-region where the bp algorithm converges within @xmath354 steps is shown .",
    "the dashed lines in ( a ) and ( b ) denote the results reported in @xcite and @xcite.,title=\"fig:\",width=192 ] -dependence of gdf and @xmath338 at bp fixed point of @xmath349 , @xmath350 @xmath351 for ( a ) @xmath0 , ( b ) elastic net of @xmath280 , and ( c ) scad regularization of @xmath352 and @xmath282 under the predictor matrix of example 2 with @xmath353 .",
    "we used 1000 samples of predictor matrices to calculate the gdf and @xmath338 at the bp fixed point .",
    "the @xmath212-region where the bp algorithm converges within @xmath354 steps is shown .",
    "the dashed lines in ( a ) and ( b ) denote the results reported in @xcite and @xcite.,title=\"fig:\",width=192 ] -dependence of gdf and @xmath338 at bp fixed point of @xmath349 , @xmath350 @xmath351 for ( a ) @xmath0 , ( b ) elastic net of @xmath280 , and ( c ) scad regularization of @xmath352 and @xmath282 under the predictor matrix of example 2 with @xmath353 .",
    "we used 1000 samples of predictor matrices to calculate the gdf and @xmath338 at the bp fixed point .",
    "the @xmath212-region where the bp algorithm converges within @xmath354 steps is shown .",
    "the dashed lines in ( a ) and ( b ) denote the results reported in @xcite and @xcite.,title=\"fig:\",width=192 ]    figures [ fig : example1 ] and [ fig : example2 ] show the @xmath212-dependence of gdf and @xmath338 at the bp fixed point for @xmath0 , elastic net , and scad regularization at @xmath349 and @xmath277 @xmath355 .",
    "the values of each point have been averaged over 1000 samples of @xmath68 . under @xmath0 and elastic net regularization ,",
    "the gdf value calculated as the expectation of the unbiased estimator derived in @xcite is shown as a dashed line . in both examples ,",
    "the correspondence between gdf and @xmath338 holds for each regularization , although a small discrepancy appears due to finite - size effects at large @xmath212 .",
    "furthermore , the values of @xmath338 and gdf at the bp fixed point are consistent with those of previous studies for @xmath0 and elastic net regularization . the parameters @xmath356 and @xmath347 in these examples do not influence the results , although they do affect the convergence of the bp algorithm .",
    "these results imply that the correspondence between gdf and the effective fraction of non - zero components holds outside of gaussian i.i.d .",
    "for both examples , the convergence of the bp algorithm is worse than with the gaussian i.i.d . predictors , particularly at large @xmath212 .",
    "thus , the algorithm must be improved to enable a discussion of the large-@xmath212 region and application to other predictor matrices .    in the case of @xmath3 regularization",
    ", the bp algorithm does not converge .",
    "thus , we can not confirm the generality of the result using the properties of bp fixed points .",
    "the replica analysis for non - gaussian i.i.d .",
    "predictor matrices is a necessary step towards verifying the generality of the result for @xmath3 regularization . although the range of applicable predictor matrices for replica analysis is narrower than that for the bp algorithm , the analysis of rotationally invariant predictor matrices offers a promising means towards demonstrating the generality @xcite .",
    "for @xmath3 regularization , the rs solution is unstable in the whole parameter region , but it is known that this solution generally approximates the true solution . one can numerically obtain the exact solution of for @xmath3 regularization by an exhaustive search , and calculate the exact value of gdf at small system sizes . comparing the gdf under rs analysis with its exact value , we can evaluate the approximation performance of the rs solution .",
    "compares the gdf approximated by the rs solution with its exact value for @xmath357 , and @xmath358 as calculated by 1000 samples of @xmath68 . as @xmath359 increases , the exact gdf approaches the rs solution , although intense finite - size effects are observed in the small-@xmath212 region . for a comparison at larger system sizes , we must develop a computationally feasible algorithm for obtaining precise solutions of for @xmath3 regularization , but this is beyond the scope of the present paper .",
    "we have derived the gdf using a method based on statistical physics . within the range of the rs assumption ,",
    "the gdf is represented as @xmath360 , where @xmath145 and @xmath158 correspond to the rescaled variance around estimates and the variance of estimates when the regularization term is omitted , respectively .",
    "this expression does not depend on the type of regularization , and indicates that gdf can be regarded as the effective fraction of non - zero components .",
    "we applied our method for the derivation of gdf to @xmath0 , elastic net , @xmath3 , and scad regularization .",
    "our rs analysis was stable for @xmath0 and elastic net regularization in the entire physical parameter region , and the gdfs for these regularizations were consistent with previous results .",
    "this correspondence supports the validity of our rs analysis .",
    "the model selection criterion of prediction error was derived by combining the gdf with the training error .",
    "theoretical predictions in the rs phase were then algorithmically achieved using the belief propagation method .    it has been implied that the equivalence between gdf and the ratio of the number of non - zero components to the number of samples , @xmath212 , only holds for @xmath0 regularization @xcite . our representation of gdf as the effective fraction of non - zero components clarifies the origin of the additional component of the gdf from the fraction of non - zero components .    * in @xmath0 regularization ,",
    "the gdf is given by @xmath212 because there is no factor that induces fluctuations other than the non - zero components .",
    "* elastic net regularization changes the variance of the components , and so the gdf does not coincide with @xmath212 . however , as with @xmath0 regularization , the non - zero components are the unique source of fluctuations , and so the correspondence between gdf and @xmath212 can be recovered by appropriately rescaling the estimates .",
    "* in @xmath3 regularization , the discontinuity of the estimates leads to additional fluctuations besides those caused by the non - zero components .",
    "hence , the gdf is greater than @xmath212 . * in scad regularization ,",
    "the assignment of non - zero components to the transient region between @xmath0-type estimates and @xmath3-type estimates induces additional components in the gdf .    for regularizations with at instabilities in certain parameter regions ( e.g. @xmath3 , scad , and other non - convex regularizations ) ,",
    "it is generally necessary to construct the full - step rsb solution . in the case of scad regularization , model selection based on the prediction error under rs analysis",
    "can be achieved in the current problem setting .",
    "even when the rs solution is unstable , the prediction error gives a meaningful approximation of the true value .",
    "further development of our method for the general function of prediction error @xcite and real data will be useful for practical applications .",
    "the bp algorithm discussed here can numerically calculate the gdf and model selection criterion for practical settings at reasonable computational cost .",
    "the author would like to thank yukito iba , yoshiyuki kabashima , and yoshiyuki ninomiya for insightful discussions and comments .",
    "this work was supported by jsps kakenhi no.25120013 , 26880028 and 16k16131 .",
    "99            zhu j , rosset s , hastie t and tibshirani r 2004 _ adv .",
    "neural . inf . process _",
    "* 16 * 49 foucart s and lai m - j 2009 _ appl .",
    "* 26 * 395 wang m , xu w and tang a 2011 _ ieee trans .",
    "inform . theory _ * 57 * 7255 xu z , chang x , xu f and zhang h 2012 _ ieee trans .",
    "neural networks and learning systems _ * 23 * 1013 fan j and li r 2001 _ j. amer .",
    "assoc . _ * 96 * 1348 fan j and peng h 2004 _ annal .",
    "stat . _ * 32 * 928 huang j , ma s and zhang c - h 2008 _ statistica sinica _ * 18 * 1603 stone m 1974 _ biometrika _ * 61 * 509 zhang y , li r and tsai c - l 2010 _ j. amer .",
    "assoc . _ * 105 * 312        mallows c 1973 _ technometrics _ * 15 * 661 efron b 2004 _ j. amer . statist .",
    "assoc . _ * 99 * 619 zou h , hastie t and tibshirani r 2007 _ annal .",
    "stat . _ * 35 * 2173 zou h 2005 _ ph.d .",
    "dissertation _ dept .",
    "statistics , stanford univ ."
  ],
  "abstract_text": [
    "<S> we develop a method to evaluate the generalized degrees of freedom ( gdf ) for linear regression with sparse regularization . </S>",
    "<S> the gdf is a key factor in model selection , and thus its evaluation is useful in many modelling applications . </S>",
    "<S> an analytical expression for the gdf is derived using the replica method in the large - system - size limit with random gaussian predictors . </S>",
    "<S> the resulting formula has a universal form that is independent of the type of regularization , providing us with a simple interpretation . within the framework of replica symmetric ( rs ) analysis </S>",
    "<S> , gdf has a physical meaning as the effective fraction of non - zero components . </S>",
    "<S> the validity of our method in the rs phase is supported by the consistency of our results with previous mathematical results . </S>",
    "<S> the analytical results in the rs phase are calculated numerically using the belief propagation algorithm .    </S>",
    "<S> _ keywords _ : cavity and replica method , statistical inference , learning theory </S>"
  ]
}