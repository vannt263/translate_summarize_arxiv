{
  "article_text": [
    "neural networks ( cnns ) have given the computer vision community a significant shock  @xcite , and have been improving state - of - the - art results in many computer vision applications . since alexnets",
    "@xcite ground - breaking victory at the imagenet large scale visual recognition challenge 2012 ( ilsvrc 2012 )  @xcite , deeper and deeper cnns  @xcite have been proposed and achieved better performance on imagenet or other benchmark datasets .",
    "the results of these models revealed     is the residual mapping and @xmath0 is the identity mapping . the original mapping represents as @xmath1 .",
    "the right image with dashed line is our new residual networks of residual networks architecture with three levels .",
    "ror is constructed by adding identity shortcuts level by level based on original residual networks . ]",
    "the importance of network depth , as deeper networks lead to superior results .    with a dramatic increase in depth , residual networks ( resnets )",
    "@xcite achieved the state - of - the - art performance at ilsvrc 2015 classification , localization , detection , and coco detection , segmentation tasks .",
    "however , very deep models will suffer vanishing gradients and over - fitting problems , thus the performance of thousand - layer resnets is worse than hundred - layer resnets .",
    "then the identity mapping resnets ( pre - resnets )  @xcite simplified the residual networks training by bn - relu - conv order .",
    "pre - resnets can alleviate vanishing gradients problem , so that the performance of thousand - layer pre - resnets can be further improved .",
    "the latest wide residual networks ( wrn )  @xcite treated vanishing gradients problem by decreasing depth and increasing width of residual networks .",
    "nevertheless , the exponentially increasing number of parameters brought by broader networks worsens the over - fitting problem . as a sesult , dropout and drop - path methods",
    "are usually used to alleviate over - fitting , and the leading method on resnets is stochastic depth residual networks ( sd )  @xcite which can improve the test accuracy and reduce training time in the meantime .",
    "no matter what kind of residual networks they are , they all based on one basic hypothesis : by using shortcuts connections , residual networks perform residual mapping fitted by stacked nonlinear layers , which is easier to be optimized than the original mapping  @xcite .",
    "furthermore , we hypothesize that the residual mapping of residual mapping be easier to be optimized than original residual mapping . so based on this hypothesis , we can construct a better residual - network architecture to enhance performance .    in this paper",
    ", we present a novel and simple architecture called residual networks of residual networks ( ror ) .",
    "firstly , comparing with the original one - level - shortcut - connection resnets , we add extra shortcut connections on original resnets level by level . a multilevel network is then constructed , as seen in fig .",
    "[ fig : basicnetworks ] . for this network ,",
    "we analyze the effects of different shortcut level numbers , shortcut types and maximum epoch numbers . secondly , in order to alleviate over - fitting , we train ror with drop - path method , and obtain apparent performance boost .",
    "thirdly , we build ror on various residual networks ( pre - resnets and wrn ) , and we find out that ror is suitable to not only original resnets but also other residual networks . through massive experiments on cifar-10 , cifar-100  @xcite and svhn  @xcite ,",
    "our ror can dig as much optimization ability of the residual networks as possible , only by adding a few shortcuts .",
    "although this approach seems quite simple , it is surprisingly effective in practice , and achieves the new state - of - the - art results on above datasets .",
    "our main contribution is threefold :    we introduce ror which improve the optimization ability of resnets .",
    "ror can easily be constructed by adding a few identity shortcuts on resnets , and achieve better performance than resnets with the same number of layers .",
    "we elucidate that our ror is also suitable to other residual networks and boost their performance .",
    "our ror is an important and effective complement of residual networks family .    through experiments , we analyse the effects of different depth , width , shortcut level numbers , shortcut types and maximum epoch numbers to ror , and give reasonable strategies to construct ror .    the remainder of the paper is organized as follows .",
    "section ii briefly reviews related works for deep convolutional neural networks and residual networks family . the proposed ror method is illustrated in section iii .",
    "the optimization of ror is described in section iv .",
    "experimental results and analysis are presented in section v , leading to conclusions in section vi .",
    "in the past several years , deeper and deeper cnns have been constructed , because the more convolutional layers are in cnns , the better optimization ability cnns can obtain . from 5-conv+3-fc alexnet ( ilsvrc2012 winner )",
    "@xcite to the 16-conv+3-fc vgg networks  @xcite and 21-conv+1-fc googlenet ( ilsvrc2015 winner )  @xcite , both the accuracy and the depth of cnns were increasing .",
    "but very deep cnns face a crucial problem , vanishing gradients  @xcite .",
    "earlier works adopted initialization methods and layer - wise training to reduce this problem  @xcite .",
    "moreover , relu activation function  @xcite and its variants elu  @xcite , prelu  @xcite , pelu  @xcite also can prevent vanishing gradients .",
    "fortunately , this problem can be largely addressed by batch normalization ( bn )  @xcite and carefully normalized weights initialization  @xcite according to recent research .",
    "bn  @xcite standardized the mean and variance of hidden layers for each mini - batch , while msr  @xcite initialized the weights by more reasonable variance . in another aspect",
    ", a degradation problem has been exposed  @xcite that is , not all systems are similarly easy to optimize . in order to resolve this problem ,",
    "several methods were proposed .",
    "highway networks  @xcite consisted of a mechanism allowing 2d - cnns to interact with a simple memory mechanism . even with hundreds of layers , highway networks can be trained directly through simple gradient descent .",
    "resnets  @xcite simplified highway networks using a simple skip connection mechanism to propagate information to deeper layers of networks .",
    "resnets are more simple and effective than highway networks .",
    "recently , fractalnet  @xcite generated an extremely deep network whose structural layout was precisely a truncated fractal by repeating application of a single expansion rule , and this method showed that residual learning was not required for ultra - deep networks .",
    "however , in order to get the competitive performance as resnets , fractalnet need much more parameters than resnets . now more and more residual network variants and architectures  @xcite have been proposed , and they form a residual networks family together .",
    "the basic idea of resnets  @xcite is that residual mapping is easy to optimize , so resnets skip blocks of convolutional layers by using shortcut connections to form shortcut blocks ( residual blocks ) .",
    "these stacked residual blocks greatly improve training efficiency and largely resolve the degradation problem by employing bn  @xcite and msr  @xcite . the resnets architecture and residual block",
    "are shown in fig .",
    "[ fig : basicnetworks ] , where each residual block can be expressed in a general form : @xmath2 where @xmath3 and @xmath4 are input and output of the @xmath5-th block , @xmath6 is a residual mapping function , @xmath7 is an identity mapping function and @xmath8 is a relu function .",
    "however , the vanishing gradients problem still exists , as the test result of 1202-layer resnets is worse than 110-layer resnets on cifar-10  @xcite .    in the pre - resnets , he et al .",
    "@xcite created a `` direct '' path for propagating information through the entire network by both @xmath9 and @xmath8 being identity mappings .",
    "the residual block of pre - resnets performs the follow computations : @xmath10 the new residual block with bn - relu - conv order can reduce training difficulties , so that pre - resnets can get better performance than original resnets .",
    "more importantly , pre - resnets can reduce vanishing gradients problem even for 1001-layer pre - resnets .",
    "inspired by pre - resnets , shen et al .",
    "@xcite proposed weighted residuals for very deep networks ( wresnet ) , which removed the relu from highway and used weighted residual functions to create a `` direct '' path .",
    "this method can also train 1000 + layers residual networks , and achieve good accuracy . in order to further reduce vanishing gradients , shah et al .",
    "@xcite and trottier et al .",
    "@xcite proposed the use of elu and pelu respectively instead of relu in residual networks .    besides vanishing gradients problem , over - fitting is another challenging issue for cnns .",
    "huang and sun et al .",
    "@xcite proposed a drop - path method , stochastic depth residual networks ( sd ) which randomly dropped a subset of layers and bypassed them with the identity mapping for every mini - batch .",
    "sd can alleviate over - fitting and reduce vanishing problem , so it is a good complement of residual networks . by combining dropout and sd , singh et al .",
    "@xcite proposed a new stochastic training method , swapout , which can be viewed as an ensemble of resnets , dropout resnets and sd resnets .    recently",
    ", more variants of residual networks are proposed , and they all promote learning capability by expending width of model .",
    "resnet in resnet ( rir )  @xcite was a generalized residual architecture which combines resnets and standard cnns in parallel residual and non - residual streams .",
    "wrn  @xcite decreased depth and increased width of residual networks by adding more feature planes , and achieves the latest state - of - the - art results on cifar-10 and svhn .",
    "the newest convolutional residual memory networks ( crmn )  @xcite were inspired by wrn and highway networks .",
    "crmn augment convolutional residual networks with a long short term memory mechanism based on wrn , and achieve the latest state - of - the - art performance on cifar-100 .",
    "these wider residual networks indicate that wide and shallow residual networks result in good performance and can easy training .",
    "ror is based on a hypothesis : to dig the optimization ability of residual networks , we can optimize the residual mapping of residual mapping . so we add shortcuts level by level to construct ror based on residual networks .    fig .  [",
    "fig : rornetworks ] shows the original residual network with @xmath11 residual blocks .",
    "these @xmath11 original residual blocks are denoted as the final level of shortcuts .",
    "firstly , we add a shortcut above all residual blocks , and this shortcut can be called root shortcut or first level shortcut .",
    "generally we use 16 , 32 and 64 filters sequentially in the convolutional layers  @xcite , and each kind of filter has @xmath11/3 residual blocks which form three residual block groups .",
    "secondly , we add a shortcut above each residual block group , and these three shortcuts are called second level shortcuts",
    ". then we can continue adding shortcuts as the inner level shortcuts by separating each residual block group equally . finally , the shortcuts in original residual blocks are regarded as the final level shortcuts .",
    "let @xmath12 denote a shortcut level number , @xmath12=1 , 2 , 3 .... when @xmath12=1 ror is an original residual networks without other shortcut level .",
    "when @xmath12=2 , the ror have root level and final level . in this paper , @xmath12 is 3 , so the ror have root level , middle level and final level , shown in fig .",
    "[ fig : rornetworks ] . compared to the top - right residual block ,",
    "the bottom - right one is without relu in the end , because there are extra additions following it .    1 convolutions .",
    "ror adopts conv - bn - relu order in residual blocks . ]",
    "the addition is followed by the relu .",
    "projection shortcut is done by 1@xmath131 convolutions .",
    "bn - relu - conv order in residual blocks is adopted . if @xmath14=1 , this is a pre - ror-3 architecture , otherwise this is a ror-3-wrn architecture .",
    "there are several `` direct '' paths for propagating information created by identity mappings . ]",
    "when @xmath12=3 , three residual blocks which locate the end of each residual block group can be expressed by the following formulations , and the other original residual blocks remain the same .",
    "@xmath15 @xmath16 @xmath17 where @xmath3 and @xmath3 + 1 are input and output of the @xmath5-th block , and @xmath6 is a residual mapping function , @xmath7 and @xmath18 are both identity mapping functions . @xmath19",
    "expresses the identity mapping of the first level and second level shortcuts , and @xmath9 denotes the identity mapping of the final level shortcuts .",
    "@xmath19 function is type b projection shortcut  @xcite .    in this paper",
    ", we will construct ror based on resnets , pre - resnets and wrn respectively .",
    "when we use original resnets as basic residual networks , @xmath8 is relu function , and ror adopts conv - bn - relu order in residual blocks .",
    "the architecture of ror-3 in detail is shown in fig .",
    "[ fig : rornetworks ] .",
    "ror constructed on pre - resnets and wrn are named after pre - ror and ror - wrn .",
    "their order of residual blocks is bn - relu - conv , and all @xmath19 , @xmath9 and @xmath8 functions are identity mapping .",
    "the formulations of the three different residual blocks are changed by the following formulas .",
    "the architecture of pre - ror and ror - wrn in detail is shown in fig .  [",
    "fig : pre - rornetworks ] .",
    "@xmath20 @xmath21 @xmath22",
    "in order to optimize ror , we must determine some important parameters and principles , such as shortcut level number , identity mapping type , maximum epoch number and whether using drop - path .",
    "it is important to choose suitable level number of ror for satisfying performance .",
    "the greater shortcut level number we choose , the more branches and parameters are added",
    ". the over - fitting problem will be exacerbated , and the performance may be decreasing .",
    "however , the enhancement using ror will be less obvious once the number is too small .",
    "so we must find the suitable number to keep the balance of these two .",
    "we do some experiments on cifar-10 with different depth and shortcut level number , and the results are described in fig .",
    "[ fig : shortlevel ] .",
    "we find out the performance of ror ( @xmath12=3 ) to be the best of all , and the performance of ror ( @xmath12=4 or 5 ) are worse than the original resnets ( @xmath12=1 ) .",
    "so in this paper we choose @xmath12=3 , which is shown in fig .",
    "[ fig : rornetworks ] and denoted as ror-3 .    . when @xmath12=1 , it is the original resnets .",
    "when @xmath12=3 , ror reaches the best performance . ]",
    "he et al .",
    "@xcite has investigated three types of projection shortcuts : ( a ) zero - padding shortcuts are used for increasing dimensions , and all shortcuts are parameter - free ; ( b ) projection shortcuts ( done by 1@xmath131 convolutions ) are used for increasing dimensions , and other shortcuts are identity ; and ( c ) all shortcuts are projections .",
    "type b is slightly better than type a , and type c is marginally better than b , but type c has too many extra parameters .",
    "so in this paper we use type a or b as the types of final level shortcuts .",
    "cifar-10 has only 10 classes , the problem of over - fitting is not critical , and extra parameters will not obviously escalate over - fitting , so we use type b in the original residual blocks on cifar-10 . fig .",
    "[ fig : mappingtype10 ] shows that we can achieve better performance using type b than type a on cifar-10 .",
    "however , for cifar-100 , which has 100 classes with less training examples , over - fitting is critical , so we use type a in the final level .",
    "[ fig : mappingtype100 ] shows that we can achieve better performance using type a than type b on cifar-100 .",
    "the original resnets are used in all these experiments .",
    "the shortcuts in level 1 and level 2 of ror are all projection shortcuts .",
    "we use type b in these levels , because the input and output planes of these shortcuts are very different ( especially level 1 ) , and the zero - padding ( type a ) will bring more deviation .",
    "table i shows that the final level using type a ( on cifar-100 ) or type b ( on cifar-10 ) while other levels using type b can achieve better performance than pure type a , no matter @xmath12=2 or @xmath12=3 . in table",
    "[ tab : shortcuttype ] , type a and type b indicate the shortcut type in all but final level .    .test error ( % ) on cifar-10/100 with different shortcut type and level [ cols=\"<,<,<,<,<,<,<\",options=\"header \" , ]     although some variants of residual networks ( wrn and crmn ) or other cnns ( fractalnet ) can achieve competitive results , the number of parameters in these models is too large ( shown in table  [ tab : dfmethods ] ) . through experiments and analysis , we argue that our ror method can outperform other methods with similar order of magnitude parameters .",
    "our ror models with about only 3 m parameters ( pre - ror-3 - 164 , ror-3-wrn40 - 2 and pre - ror-3 - 218 ) can outperform fractalnet ( 30 m parameters ) and crmn-28 ( more than 40 m parameters ) on cifar-10 . our ror-3-wrn40 - 4 model with 8.9 m parameters can outperform all of the exiting methods .",
    "our best ror-3-wrn58 - 4 model with 13.3 m parameters achieves the new state - of - the - art performance .",
    "we have reasons to argue that better performance can be achieved by additional depth and width .",
    "this paper proposes a new residual networks of residual networks architecture ( ror ) , which obtains new state - of - the - art performance on cifar-10 , cifar-100 and svhn for image classification . through empirical studies ,",
    "this work not only significantly advances the image classification performance , but also provides an effective complement of residual networks family in the future .",
    "in other words , whatever residual network it is , it can be improved by ror .",
    "that is to say , ror has a good prospect of application on various image recognition tasks .",
    "the authors would like to thank the editor and the anonymous reviewers for their careful reading and valuable remarks .",
    "p.  sermanet , d.  eigen , x.  zhang , m.  mathieu , r. fergus , and y.  lecun , `` overfeat : integrated recognition , localization and detection using convolutional networks , '' _ arxiv preprint arxiv:1312.6229 _ , 2013 .            c.  szegedy , w.  liu , y.  jia , p.  sermanet , s.  reed , d.  anguelov , d.  erhan , v.  vanhoucke , and a.  rabinovich , `` going deeper with convolutions , '' in _ proc .",
    "ieee conf .",
    "pattern recog .",
    "_ , 2015 , pp . 19 .",
    "y.  netzer , t.  wang , a.  coates , a.  bissacco , b.  wu , and a.  y. ng , `` reading digits in natural images with unsupervised feature learning , '' in _ proc . nips workshop deep learning and unsupervised feature learning .",
    "_ , 2011 , pp . 19 .",
    "d.  erhan , y.  bengio , a.  courville , p.  a. manzagol , p.  vincent , and s.  bengio , `` why does unsupervised pre - training help deep learning ?",
    ", '' _ the journal of machine learning research _ , vol .  11 , pp .",
    "625660 , mar .",
    "2010 .",
    "n.  srivastava , g.  hinton , a.  krizhevsky , i.  sutskever , and r.  salakhutdinov , `` dropout : a simple way to prevent neural networks from overfitting , '' _ the journal of machine learning research _ , vol .  15 , pp . 19291958 , jun ."
  ],
  "abstract_text": [
    "<S> residual networks family with hundreds or even thousands of layers dominate major image recognition tasks , but building a network by simply stacking residual blocks inevitably limits its optimization ability . </S>",
    "<S> this paper proposes a novel residual - network architecture , residual networks of residual networks ( ror ) , to dig the optimization ability of residual networks . </S>",
    "<S> ror substitutes optimizing residual mapping of residual mapping for optimizing original residual mapping , in particular , adding level - wise shortcut connections upon original residual networks , to promote the learning capability of residual networks . </S>",
    "<S> more importantly , ror can be applied to various kinds of residual networks ( pre - resnets and wrn ) and significantly boost their performance . </S>",
    "<S> our experiments demonstrate the effectiveness and versatility of ror , where it achieves the best performance in all residual - network - like structures . </S>",
    "<S> our ror-3-wrn58 - 4 models achieve new state - of - the - art results on cifar-10 , cifar-100 and svhn , with test errors 3.77% , 19.73% and 1.59% respectively . </S>",
    "<S> these results outperform 1001-layer pre - resnets by 18.4% on cifar-10 and 13.1% on cifar-100 .    </S>",
    "<S> shell : bare demo of ieeetran.cls for ieee journals    image classification , residual networks , residual networks of residual networks , shortcut , stochastic depth . </S>"
  ]
}