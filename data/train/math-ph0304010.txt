{
  "article_text": [
    "entropy is a measure of the _ size _ of a data distribution contained within a bounded region ( distribution support ) of some space . in a thermodynamic context",
    "this distribution size is interpreted as the number of quantum states accessible to a dynamical system given macroscopic constraints . more generally ,",
    "if a measure space is partitioned , the measure distribution size is estimated by the effective number of partition elements given the distribution weighting .",
    "definition of the space partition is a central element of entropy calculation .",
    "the partition is sometimes defined as a small - scale limiting partition of the space ( _ e.g. , _ thermodynamic limit , limit procedures in classical analysis @xcite ) , sometimes based on properties of the data distribution itself and/or on the analysis goals ( as in wavelet analysis @xcite ) .",
    "we define entropy as an explicit function of the partition definition , a scaled binning of the measure space .",
    "we calculate the entropy and related quantities as functions of the partition scale(s ) , similar to the multi - scale partition approaches used in fractal dimension calculations @xcite and image deconvolution @xcite . unlike some analysis methods we invoke no scale limits .",
    "quantities are defined on bounded scale intervals explictly excluding asymptotic limits ; the analysis system is in this sense _ scale local_.    the end result of this approach is an entropy which represents _ arbitrary _ data correlations as a distribution on scale , particularly useful for problems where the detailed scaling behavior of correlations over substantial scale intervals is of interest ( _ e.g. , _ condensation , coalescence , critical phenomena , strange attractors ) , where instrumental effects may distort scale distributions , and where the correlation structure is not simply expressible as a power law or other elementary function .",
    "@xcite    in this paper we describe precision binning methods , define the basic scale - local entropy measure and generalize other aspects of information theory to define the scale - dependent entropy difference or information between an object distribution and a model reference as a differential correlation measure . based on scale derivatives of entropy and information",
    "we define scale - local dimension and dimension transport as generalizations of conventional counterparts based on limit concepts .",
    "we apply these correlation measures to several simulations and real data analysis problems .",
    "the entropy definition employed here is based on @xmath1 , the rank-@xmath2 correlation integral at scale @xmath3 .",
    "given a data distribution in a @xmath0-dimensional primary measure space spanned by variables @xmath4 , we consider a set of corresponding correlation spaces containing all possible @xmath2-point clusters of data points ( @xmath2-tuples ) .",
    "there is one such @xmath2-point distribution ( in a @xmath2-fold cartesian product space ) for each unique @xmath2 value .",
    "the @xmath2-point correlation integral is the projection of the @xmath2-point distribution onto its difference subspace spanned by @xmath5 , integrating over the sum variable(s ) .",
    "@xcite the integration limit of the correlation integral on the difference variables is in the simplest case ( isotropic binning ) the single scale of the analysis .",
    "the reciprocal of the correlation integral estimates the _ effective _ bin number in the @xmath6-dimensional difference subspace .",
    "it s counterpart in the primary measure space is the @xmath7 root , the effective bin number in the primary space .",
    "defining entropy as the logarithm of the effective primary - space bin number is consistent with entropy as a logarithmic size measure and is a generalization of the thermodynamic definition , the logarithm of the number of accessible states .",
    "the correlation integral can be approximated by binning the primary measure space , and expressed in terms of normalized bin contents @xmath8 , in which case @xmath9 .",
    "this results in the rank-@xmath2 rnyi entropy @xcite @xmath10 \\simeq \\frac{1}{1-q}\\log \\left[\\sum_{i=1}^{m(e)}p_{i}(e)^q\\right].\\ ] ] the entropy @xmath11 , the number of occupied bins @xmath12 , and the bin probability @xmath13 are explicit functions of the binning scale @xmath3 .",
    "more generally , a non - isotropic binning ( one utilizing bins without unit aspect ratio ) would imply a multidimensional scale space .",
    "given scale - local entropy we define scale - local information as a basis for differential comparisons between object and reference distributions , data and model .",
    "there are a number of possible information definitions from information theory and topology , with significant differences in performance .",
    "we define information here as the difference between entropies for object and reference distributions .",
    "this implies that the effective bin number for an object distribution is compared _ in ratio _ to that of a reference .",
    "nonzero information implies _ multiplicative _ reduction of effective bin number by increased correlation structure in the object distribution relative to the reference ( cumulant analysis is an alternative differential approach emphasizing _ linear or additive _ reduction of distribution size ) .",
    "scale - local information is then defined as @xmath14 information so defined provides a differential comparison between a data or _",
    "object _ distribution and a model or _",
    "reference_. for example , the reference distribution for an arbitrary point set would be a distribution with the same number of points which maximally ` fills ' the bounded support  a uniform random distribution . since the uniform reference is useful in many applications we derive explicit analytic forms for its entropy and dimension .",
    "although these scale - local analysis methods are completely general as to the nature of the measure distribution , for the purpose of illustration we emphasize point sets in this paper .",
    "the measure distribution is then a set of points in a @xmath0-dimensional embedding space ( _ e.g. , _ the distribution of particles from a heavy - ion collision in momentum space ) .",
    "the analysis begins by applying a partition to the embedding space . for algebraic simplicity",
    "we consider a grid of @xmath0-dimensional cubes , an isotropic binning . at each scale",
    "there is a continuum of possibilities for the relative position of the binning system on the embedding space .",
    "differing partition placement effects the analysis in general , and there is no _ a priori _ reason to prefer any single placement .",
    "thus , we average ( dither ) over all partition placements to calculate the entropy at each scale .",
    "we define a dithering phase @xmath15 for each of the @xmath0 embedding - space dimensions .",
    "the relationship between the partition system and the measure distribution is controlled by varying @xmath15 . to implement dithering",
    "we calculate the correlation integral @xmath16 of an event @xmath17 times at each scale , incrementing @xmath15 each time .",
    "finally we average over these results to obtain the entropy .",
    "the dithered entropy @xmath18 is thus @xmath19 .",
    "\\end{aligned}\\ ] ] where @xmath20 is the bin probability of the @xmath21th bin for binning phase @xmath15 and scale @xmath3 .",
    "@xmath13 is normalized so that @xmath22 at each scale @xmath3 and dithering phase @xmath15 , the sum taken over all occupied bins in each partition .",
    "a simple application of scale - local entropy and dithered binning illustrates the analysis process .",
    "we obtain the scaled entropy of a 2d uniform distribution of @xmath23 randomly generated points on a unit - square support for several values of index @xmath2 .",
    "a monte carlo simulation with 50k points is shown in figure  [ 1drgud ] along with analysis results for @xmath24 .    .",
    "a box plot of the distribution itself is shown in the left panel , the right panel shows the measured entropy for @xmath25 ( dashed line ) , @xmath26 ( dotted line ) , and @xmath27 ( dot - dashed line).,width=384 ]    to interpret these results we consider small- , intermediate- , and large - scale regimes . in the small - scale limit , well below the point - separation scale ( @xmath28 ) , each distribution point occupies a single bin ; there are n occupied bins ( @xmath29 ) , each with bin probability @xmath30 .",
    "thus , at small scale the entropy approaches @xmath31=\\log n.\\ ] ] at intermediate scales we idealize the uniform point set to a continuum ( @xmath32 ) .",
    "the number of occupied bins is then simply @xmath33 and the bin probability is @xmath34 giving @xmath35 . at scales substantially greater than the boundary scale ( @xmath36 ) the entire distribution is contained in a single bin : @xmath37 and @xmath38 .",
    "the rank-@xmath2 entropy thus vanishes at scales much larger than the distribution boundary size .",
    "the detailed @xmath2-dependence near the particle - separation and boundary scales is amenable to an analytic treatment .",
    "information is a relative quantity , a matter of definition .",
    "it is impossible to make an absolute determination of the information content of an arbitrary distribution . using a maximum - entropy reference we can measure information relative to a distribution which is minimally correlated ( given certain constraints ) .",
    "this motivates us to derive an algebraic form for the scale - local entropy of a _ bounded uniform distribution _ ( bud ) .",
    "this distribution represents a maximum - entropy hypothesis within a boundary , a maximum filling of the distribution support .",
    "this is a correlation reference from which any object distribution may deviate with reduced entropy .",
    "the derivation is presented in two parts : scales below and above the boundary scale .      for partitions below the boundary scale",
    "there is at least one bin in the interior of the embedding space . to derive an analytic form for the entropy of a bud we consider a two - dimensional distribution ( and",
    "later generalize to @xmath0 dimensions ) .",
    "the bud is defined on a square support with side length @xmath40 . because the distribution is uniform the probability of finding a point in any given bin",
    "is simply determined by the bin area .",
    "figure  [ dithervariables2d ] shows a bud binned with a general rectangular binning ( thin dark lines ) .",
    "we calculate the bin contents for each bin ( as a function of scale ) and integrate over all possible dithering configurations to determine the analytic form of the entropy . for bins that are entirely contained within the embedding space ( interior bins )",
    "the bin probability is trivial : @xmath41 is the area of the bin divided by the total area of the support , independent of bin dithering . for edge and corner bins",
    "the problem is more complicated .",
    "we consider each bin type  interior ( white ) , edge ( light grey ) and corner ( dark grey )  separately .",
    "corner bins contain both an x- and y - axis support edge .",
    "for all dithering configurations there are four corner bins .",
    "the effective area of these corner bins is @xmath42 scaled down by the fraction of the bin along the @xmath21th axis ( @xmath43 $ ] ) that overlaps the data . by definition ,",
    "the phase along an axis is @xmath44 , where @xmath45 is the bin overlap fraction for the first bin on axis @xmath21 ( @xmath46 $ ] ) .",
    "we define @xmath47 and express the amount of overlap between the last bin on axis @xmath21 and the edge of the support along that axis as @xmath48 . with these definitions , calculating the contribution to the correlation integral of the corner bins is a matter of integrating over all @xmath15 values using the relevant bin probabilities .",
    "labeling the corner bins from right to left starting with the upper left bin we write down the @xmath15-dependent corner bin probabilities as @xmath49^q\\left ( \\frac{e_{\\text{x}}e_{\\text{y}}}{l^2}\\right)^q \\\\ \\nonumber p_{\\text{c}}^q&=&[\\phi_{\\text{x}}+\\delta_{\\text{x}}-\\text{int } ( \\phi_{\\text{x}}+\\delta_{\\text{x}})]^q(1-\\phi_{\\text{y}})^q\\left ( \\frac{e_{\\text{x}}e_{\\text{y}}}{l^2}\\right)^q \\\\ \\nonumber p_{\\text{d}}^q&=&[\\phi_{\\text{x}}+\\delta_{\\text{x}}-\\text{int } ( \\phi_{\\text{x}}+\\delta_{\\text{x}})]^q[\\phi_{\\text{y}}+\\delta_{\\text{y}}-\\text{int } ( \\phi_{\\text{y}}+\\delta_{\\text{y}})]^q\\left ( \\frac{e_{\\text{x}}e_{\\text{y}}}{l^2}\\right)^q.\\end{aligned}\\ ] ] we calculate the dither - averaged correlation integral @xmath50 by integrating over the different dithering configurations for each bin and summing @xmath51 . following this approach",
    "we integrate the @xmath52 expressions over the two dithering variables and sum results to calculate the correlation integral .",
    "the @xmath15 integral over the first corner bin yields @xmath53 the second term is @xmath54^q\\ , \\rmd\\phi_{\\text{x}}\\ , \\rmd\\phi_{\\text{y } }   \\\\ \\nonumber & = & \\left(\\frac{e_{\\text{x}}e_{\\text{y}}}{l^2}\\right)^q\\left(\\frac{1}{1+q}\\right)\\left[\\int_{\\delta_{\\text{y}}}^1 v^q\\ , \\rmd v+\\int_0^{\\delta_{\\text{y } } } v^q\\ , \\rmd v\\right ] \\\\",
    "\\nonumber & = & \\left(\\frac{e_{\\text{x}}e_{\\text{y}}}{l^2}\\right)^q\\left(\\frac{1}{1+q}\\right)^2.\\end{aligned}\\ ] ] the third and fourth terms are similar to the first and second ; each of the four corner bins contributes an @xmath55 term to the total dither - averaged correlation integral .",
    "the contributions from edge bins are simpler to calculate .",
    "the overlap fraction is unity in the direction parallel to the support edge ; along that axis we merely count the number of edge bins .",
    "the integral along the second axis is similar to the corner bins .",
    "again there are four terms , but symmetry simplifies the problem @xmath56 ( 1-\\phi_{\\text{y}})^q\\ , \\rmd\\phi_{\\text{x}}\\ , \\rmd\\phi_{\\text{y } } \\\\ \\nonumber & = & \\left(\\frac{e_{\\text{x}}e_{\\text{y}}}{l^2}\\right)^q \\int_0 ^ 1\\int_0 ^ 1 \\left[\\text{int}\\left(\\frac{l}{e_{\\text{x}}}+1+\\phi_{\\text{x}}\\right ) -2\\right ] [ \\phi_{\\text{y}}+\\delta_{\\text{y}}-\\text{int}(\\phi_{\\text{y}}+\\delta_{\\text{y}})]^q \\ , \\rmd\\phi_{\\text{x}}\\ , \\rmd\\phi_{\\text{y } } \\\\ \\nonumber & = & \\left(\\frac{e_{\\text{x}}e_{\\text{y}}}{l^2}\\right)^q\\left(\\frac{l}{e_{\\text{x}}}-1\\right)\\left(\\frac{1}{1+q}\\right).\\end{aligned}\\ ] ] which is the expression for the x - axis border bins .",
    "contributions from the y - axis bins are obtained by switching indices .",
    "the contribution from edge bins is thus @xmath57.\\ ] ] there remains the integral over interior bins , a simple matter of bin counting @xmath58 the full correlation integral can be assembled from the corner , edge , and interior bin integrals @xmath59 \\\\",
    "\\nonumber \\fl \\;\\;\\;\\;\\;\\;\\;\\;\\;\\ , = \\left(\\frac{e_{\\text{x}}e_{\\text{y}}}{l^2}\\right)^{q-1 } \\left[1+\\left(\\frac{1-q}{1+q}\\right)\\left(\\frac{e_{\\text{x}}}{l}\\right)\\right]\\left[1+\\left(\\frac{1-q}{1+q}\\right)\\left(\\frac{e_{\\text{y}}}{l}\\right)\\right].\\end{aligned}\\ ] ] inserting this result into the definition of the rank-@xmath2 entropy we find that @xmath60 \\\\",
    "\\nonumber \\fl \\;\\;\\;\\;\\;\\;\\;\\;\\;\\",
    ", = \\frac{q-1}{1-q } \\log \\left(\\frac{e_{\\text{x}}e_{\\text{y}}}{l^2}\\right)+\\frac{1}{1-q } \\log \\left[1+\\left(\\frac{1-q}{1+q}\\right)\\left(\\frac{e_{\\text{x}}}{l}\\right)\\right]+\\frac{1}{1-q } \\log \\left[1+\\left(\\frac{1-q}{1+q}\\right)\\left(\\frac{e_{\\text{y}}}{l}\\right)\\right ] \\\\",
    "\\nonumber \\fl \\;\\;\\;\\;\\;\\;\\;\\;\\;\\ , = \\left\\{\\log\\left(\\frac{l}{e_{\\text{x}}}\\right)+\\frac{1}{1-q}\\log\\left[1+\\left(\\frac{1-q}{1+q}\\right)\\frac{e_{\\text{x}}}{l}\\right]\\right\\}+\\left\\{\\log\\left(\\frac{l}{e_{\\text{y}}}\\right)+\\frac{1}{1-q}\\log\\left[1+\\left(\\frac{1-q}{1+q}\\right)\\frac{e_{\\text{y}}}{l}\\right]\\right\\}.\\end{aligned}\\ ] ] because of the additivity of entropy under product this entropy expression contains two terms , one for each axis , which suggests the @xmath0-dimensional generalization : adding an equivalent term for each additional dimension .",
    "for the derivation of eq .",
    "( [ genent ] ) we considered contributions from three types of bins : corner , border , and interior . in the 2d derivation",
    "this approach is only valid when both @xmath62 and @xmath63 .",
    "if either of the scales is larger than the support size there are no _ interior _ bins and eq .",
    "( [ genent ] ) is not valid .",
    "thus , to obtain the entropy expression for a bud at large scale consider a single axis ( we now exploit the additivity of entropy with respect to dimension for uncorrelated systems ) with @xmath61 .",
    "when there is a bin edge in the support there are exactly two corner bins ; we express the bin probability of the second bin in terms of the first @xmath64^q \\\\ \\nonumber p_2^q&=&[1-p_1]^q.\\end{aligned}\\ ] ] when the support fits completely inside a single bin @xmath65 ; the distance from the support edge to the nearest bin edge is larger than the size of the support ( @xmath66 ) .",
    "we now evaluate the relevant integrals @xmath67^q\\ , \\rmd\\alpha=1-\\frac{l}{e},\\ ] ] @xmath68 since the definition of corner bins is arbitrary , we could relabel and get the same result ; symmetry requires that @xmath69 .",
    "these results can now be assembled to calculate the 1d scaled entropy for scales larger than the support size @xmath70 \\\\ \\nonumber & = & \\frac{1}{1-q } \\log \\left[1-\\frac{l}{e}+\\frac{l}{e}\\left(\\frac{2}{1+q}\\right)\\right ] \\\\",
    "\\nonumber & = & \\frac{1}{1-q } \\log \\left[1+\\left(\\frac{1-q}{1+q}\\right)\\frac{l}{e}\\right].\\end{aligned}\\ ] ] combining results , we obtain the rank-@xmath2 scaled entropy for each degree of freedom of a bud over all scales @xmath71,&for $ e \\le l$\\cr \\frac{1}{1-q } \\log\\left[1+\\left(\\frac{1-q}{1+q}\\right)\\frac{l}{e}\\right],&for $ e \\ge l$.\\cr}\\ ] ] we have generated the exact expression for the scale - local entropy in the general case of a @xmath0-dimensional bounded uniform distribution .",
    "dashed , @xmath26 dotted , @xmath27 dot - dashed lines ) with analytic brud entropy for @xmath25 ( solid line ) as a reference .",
    "data and reference entropy distributions differ near the mean interparticle spacing scale ( @xmath78 - 2.35 ) as discussed in the text.,width=384 ]    this entropy is precise for a discrete , random point set and @xmath25 . for @xmath79 the reference entropy",
    "differs from the entropy for real point sets in a small scale interval near @xmath80 , the typical two - point separation .",
    "this information ( entropy difference ) derives from the fact that the form of rnyi entropy employed here is based on ordinary moments ( averaged powers of @xmath81 ) , whereas the point - set data are poisson distributed .",
    "a correlation integral based on factorial moments would give zero information for uniform ( uncorrelated ) point sets @xcite .",
    "we could conclude that a factorial - moment approximation to the correlation integral should be used for point sets at least , if not other applications .",
    "however , extending our previous observation that boundedness itself is a form of correlation in a self - consistent system we can also view the point set as a result of increasing correlation ( coalescence ) of a continuous distribution at a characteristic scale .",
    "the point set does have correlations additional to the boundedness of the continuous bud .",
    "the apparent discrepancies in the entropies of brud and real points sets ( the nonzero @xmath82 in fig .",
    "[ 50krud ] ) reveal a genuine correlation feature in a discrete point set compared to a continuum .",
    "the rnyi entropies based on ordinary moments are preferred as a _",
    "more general _ formulation applicable to arbitrary measure distributions .",
    "the information corresponding to the difference between factorial moments and ordinary moments for the uniform random point set is meaningful and can be expressed analytically ( the subject of a future paper ) .",
    "having obtained entropy and information as scale - local distributions we can similarly express the dimension of a distribution as a function of scale .",
    "we start with a conventional definition of dimension based on asymptotic limits @xcite @xmath83 = \\lim_{e\\rightarrow 0 } \\biggl [ -\\frac{s_q(e)}{\\log ( e ) } \\biggr].\\ ] ] in this approach dimension is a single number defined in the limit of a zero - scale partition .",
    "the slope of the @xmath18 distribution is evaluated in the limit of zero scale .",
    "this definition favors certain specific types of correlation ( power laws ) , and assumes that the limit exists , which may not be true even in principle . for more general cases",
    "the results can be misleading .",
    "we relax the asymptotic limit restriction as we did with scale - local entropy to obtain a more general _ scale - local _ dimension @xmath84",
    "= -\\frac{\\partial [ s_q(e)]}{\\partial [ \\log ( e)]}.\\end{aligned}\\ ] ] applying this definition to the generalized entropy of a brud in eq .",
    "( [ point - ent ] ) yields @xmath85     dashed , @xmath26 dotted , @xmath27 dot - dashed lines ) .",
    "the solid lines shows the analytical @xmath25 results derived for a generalized brud.,width=384 ]    dimension expressed as a scale - local distribution is a novel aspect of this entropy treatment . to understand how scale can affect",
    "the inferred dimension of an object consider the apparent dimension of a planet in the solar system from different viewpoints .",
    "for an observer on one planet other planets appear to the eye as isolated points ( with zero dimension ) . with a powerful telescope",
    "the resolution size ( scale ) of the observation decreases substantially .",
    "planets appear as 2d disks with 1d border .",
    "a radar probe orbiting a planet can determine that the planet at smaller scale has a rich 3d surface and internal structure that could not be supported by a 1d point or 2d plane .",
    "continuing to the atomic scale planets are made of atoms and molecules that appear point - like . at this scale a planet",
    "s dimensionality returns to zero .",
    "this general principle is illustrated in figure  [ dq ] .",
    "as an exercise in precision correlation analysis using scale - local entropy and dimension we model cluster formation via condensation by generating a hierarchical point distribution with correlation features distributed over a range of scales .",
    "this model is relevant to phase transitions and complex systems analysis . to create a two - dimensional , two - tier cluster hierarchy on a square region of side @xmath40",
    "we generate a uniform random distribution of @xmath86 cluster sites , providing correlations at the characteristic length scale @xmath87  the mean site separation . at each cluster site",
    "we throw a randomly generated uniform distribution of @xmath88 points with width @xmath89 , giving the distribution a second characteristic length scale @xmath90 .    , @xmath91 & @xmath92 .",
    "the scaled dimension of the data ( dashed line ) is compared to the analytic brud reference with the corresponding number of points ( @xmath93 ) at scale @xmath94 ( black solid line ) as well as the analytic reference for @xmath95 at scales @xmath96 and @xmath97 ( gray solid lines).,width=384 ]    if the two tiers of the hierarchy are sufficiently separated on scale , as in fig .",
    "[ 2d2steph ] , the sub - structure of the clusters is not evident to the analysis at large scale ( @xmath98 ) .",
    "the distribution appears to be a brud of @xmath86 random points . at smaller scale ( @xmath99 )",
    "the apparent structure is dominated instead by the internal cluster structure , a brud of @xmath88 points .",
    "this two - tiered hierarchical distribution is a first approximation to a self - similar distribution : it appears as a brud at scales @xmath40 and @xmath89 simultaneously ( see fig .",
    "[ 2d2steph ] ) . extending",
    "the hierarchy by recursive self - similar cluster generation would converge to the limiting case of a fractal point distribution over an arbitrarily large scale interval ( _ e.g. , _ cantor set ) .",
    "the ` fractal ' dimension would depend on the relations among the hierarchy scale separation , the cluster size and the point count .",
    "the lower right panel of figure  [ 2d2steph ] shows the _ dimension transport _ for the two - tier hierarchy .",
    "dimension transport is defined as the scale derivative of information , @xmath100/\\partial [ \\log ( e)]$ ] , and is a measure of the scale - dependent dimension difference between reference and object distributions .",
    "dimension transport measures increasing correlation as the transport of dimensionality from larger to smaller scale . in the example of the two - tier hierarchy correlation ( relative to the brud of the same multiplicity )",
    "anticorrelation of points at larger scale is achieved when points condense toward the cluster sites .",
    "the anti - correlation at larger scale results in a reduction of larger - scale dimensionality ( the system appears more point - like ) and an increase in the local point density and dimensionality at smaller scale .    ) in a system with a fixed multiplicity ( @xmath101 ) .",
    "data are compared to a reference brud with equal multiplicity.,width=384 ]    the extended two - tier condensation example in figure  [ 6panelh ] shows how small - scale correlations increase by condensing points of a brud onto cluster sites . at the onset of cluster formation ( @xmath1023000 cluster sites , @xmath1023 points per cluster )",
    "the transport of dimension to smaller scale is barely visible ( but still non - statistical ) .",
    "when the size of the clusters becomes significant ( @xmath1021000 cluster sites , @xmath10210 points per cluster ) the analysis indicates what the eye perceives directly , that the distribution of points is in some way correlated .",
    "when the cluster size is 10% of the number of clusters ( @xmath102300 cluster sites , @xmath10230 points per cluster ) the dimension transport shows quite dramatically and quantitatively the transport of dimension from larger to smaller scale .",
    "we have developed a novel analysis system which is well - suited to the task of precision correlation analysis for general measure distributions and especially for systems which exhibit clustering or other self - similar behavior . by extending the rnyi - entropy concept to a locally - defined function of scale",
    "we are able to establish a more complete picture of data correlations and make precision comparisons among data , simulations and model distributions in the context of information theory .",
    "comparison of monte carlo results and analytic distributions have led to a detailed understanding of scale - local entropy measures .",
    "analysis of simulated clustering data suggests the power of this method in the quantification of scale - dependent correlation structure .",
    "the authors would like to thank all of the people who have contributed to and supported this work .",
    "in particular , we would like to thank dhammika weerasundara who was instrumental in the development of scale - local entropy methods , as well as stephen bailey , justin prosser , and curtis reynolds who helped implement several applications of this analysis .",
    "5    baker g l and gollub j p 1990 _ chaotic dynamics _",
    "( cambridge : cambridge university ) grassberger p 1983 _ phys .",
    "lett . _ a * 97 * 224 - 230 kittel c and kroemer h 1980 _ thermal physics _",
    "( new york : freeman ) pando j and fang l 1998 _ phys .",
    "* 3553 - 3601 pantin e and starck j - l 1996 _ astron . astrophys .",
    "ser . _ * 118 * 575 - 585 reid j g 2002 event - by - event analysis methods and applications to relativistic heavy - ion collision data _ doctoral dissertation _",
    "nucl - ex/0302001 rnyi a 1960 _ mta iii .",
    "* 10 * 251 - 282 trainor t a 1998 scale - local topological measures , _ preprint _ , university of washington , 1998"
  ],
  "abstract_text": [
    "<S> a novel method for correlation analysis using scale - dependent rnyi entropies is described . </S>",
    "<S> the method involves calculating the entropy of a data distribution as an explicit function of the scale of a @xmath0-dimensional partition of @xmath0-cubes , which is dithered to remove bias . </S>",
    "<S> analytic expressions for dithered scale - local entropy and dimension for a uniform random point set are derived and compared to monte carlo results . </S>",
    "<S> simulated nontrivial point - set correlations representing condensation and clustering are similarly analyzed . </S>",
    "<S> .2 in keywords : scale , entropy , dimension , information , fractal , complex system , phase transition </S>"
  ]
}