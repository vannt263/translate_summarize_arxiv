{
  "article_text": [
    "estimating the mean function based on discretely sampled noisy observations is one of the most basic problems in functional data analysis .",
    "much progress has been made on developing estimation methodologies .",
    "the two monographs by ramsay and silverman ( @xcite , @xcite ) provide comprehensive discussions on the methods and applications .",
    "see also @xcite .",
    "let @xmath0 be a random function defined on the unit interval @xmath1 $ ] and @xmath2 be a sample of @xmath3 independent copies of @xmath4 .",
    "the goal is to estimate the mean function @xmath5 based on noisy observations from discrete locations on these curves : @xmath6 where @xmath7 are sampling points , and @xmath8 are independent random noise variables with @xmath9 and finite second moment @xmath10 .",
    "the sample path of @xmath4 is assumed to be smooth in that it belongs to the usual sobolev ",
    "hilbert spaces of order @xmath11 almost surely , such that @xmath12 ^ 2\\,dt\\biggr)<+\\infty.\\ ] ] such problems naturally arise in a variety of applications and are typical in functional data analysis [ see , e.g. , @xcite , @xcite ] .",
    "various methods have been proposed .",
    "however , little is known about their theoretical properties .    in the present paper ,",
    "we study optimal estimation of the mean function in two different settings .",
    "one is when the observations are sampled at the same locations across curves , that is , @xmath13 for all @xmath14 .",
    "we shall refer to this setting as _",
    "common design _ because the sampling locations are common to all curves .",
    "another setting is when the @xmath7 are independently sampled from @xmath15 , which we shall refer to as _",
    "independent design_. we establish the optimal rates of convergence for estimating the mean function in both settings .",
    "our analysis reveals interesting and different phase transition phenomena in the two cases .",
    "another interesting contrast between the two settings is that smoothing is necessary under the independent design , while , somewhat surprisingly , it is not essential under the common design .",
    "we remark that under the independent design , the number of sampling points oftentimes varies from curve to curve and may even be random itself .",
    "however , for ease of presentation and better illustration of similarities and differences between the two types of designs , we shall assume an equal number of sampling points on each curve in the discussions given in this section .",
    "earlier studies of nonparametric estimation of the mean function @xmath16 from a collection of discretely sampled curves can be traced back to at least @xcite and @xcite in the case of common design . in this setting , ignoring the temporal nature of @xmath17 , the problem of estimating @xmath16 can be translated into estimating the mean vector @xmath18 , a typical problem in multivariate analysis .",
    "such notions are often quickly discarded because they essentially lead to estimating @xmath19 by its sample mean @xmath20 based on the standard gauss  markov theory [ see , e.g. , @xcite ] .",
    "note that @xmath21 and that the smoothness of @xmath4 implies that @xmath16 is also smooth .",
    "it is therefore plausible to assume that smoothing is essential for optimal estimation of @xmath16 .",
    "for example , a natural approach for estimating @xmath16 is to regress @xmath22 on @xmath7 nonparametrically via kernel or spline smoothing .",
    "various methods have been introduced along this vein [ see , e.g. , @xcite ] .",
    "however , not much is known about their theoretical properties .",
    "it is noteworthy that this setting differs from the usual nonparametric smoothing in that the observations from the same curve are highly correlated .",
    "nonparametric smoothing with certain correlated errors has been previously studied by @xcite , @xcite and @xcite , among others .",
    "interested readers are referred to @xcite for a recent survey of existing results .",
    "but neither of these earlier developments can be applied to account for the dependency induced by the functional nature in our setting . to comprehend the effectiveness of smoothing in the current context",
    ", we establish minimax bounds on the convergence rate of the integrated squared error for estimating @xmath16 .    under the common design",
    ", it is shown that the minimax rate is of the order @xmath23 where the two terms can be attributed to discretization and stochastic error , respectively .",
    "this rate is fundamentally different from the usual nonparametric rate of @xmath24 when observations are obtained at @xmath25 distinct locations in order to recover an @xmath11 times differentiable function [ see , e.g. , @xcite ] . the rate obtained here is jointly determined by the sampling frequency @xmath26 and the number of curves @xmath3 rather than the total number of observations @xmath27 .",
    "a distinct feature of the rate is the phase transition which occurs when @xmath26 is of the order @xmath28 . when the functions are sparsely sampled , that is , @xmath29 , the optimal rate is of the order @xmath30 , solely determined by the sampling frequency . on the other hand ,",
    "when the sampling frequency is high , that is , @xmath31 , the optimal rate remains @xmath32 regardless of @xmath26 .",
    "moreover , our development uncovers a surprising fact that _ interpolation _ of @xmath33 , that is , estimating @xmath19 by @xmath34 , is rate optimal .",
    "in other words , contrary to the conventional wisdom , smoothing does not result in improved convergence rates .",
    "in addition to the common design , another popular sampling scheme is the independent design where the @xmath7 are independently sampled from @xmath15 .",
    "a natural approach is to smooth observations from each curve separately and then average over all smoothed estimates .",
    "however , the success of this two - step procedure hinges upon the availability of a reasonable estimate for each individual curve .",
    "in contrast to the case of common design , we show that under the independent design , the minimax rate for estimating @xmath16 is @xmath35 , which can be attained by smoothing @xmath36 altogether .",
    "this implies that in the extreme case of @xmath37 , the optimal rate of estimating @xmath16 is @xmath38 , which also suggests the sub - optimality of the aforementioned two - step procedure because it is impossible to smooth a curve with only a single observation .",
    "similar to the common design , there is a phase transition phenomenon in the optimal rate of convergence with a boundary at @xmath39 .",
    "when the sampling frequency @xmath26 is small , that is , @xmath29 , the optimal rate is of the order @xmath24 which depends jointly on the values of both @xmath26 and @xmath3 . in the case of high sampling frequency with @xmath31 , the optimal rate is always @xmath32 and does not depend on  @xmath26 .",
    "it is interesting to compare the minimax rates of convergence in the two settings .",
    "the phase transition boundary for both designs occurs at the same value , @xmath39 .",
    "when @xmath26 is above the boundary , that is , @xmath40 , there is no difference between the common and independent designs , and both have the optimal rate of  @xmath41 .",
    "when @xmath26 is below the boundary , that is , @xmath42 , the independent design is always superior to the common design in that it offers a faster rate of convergence .",
    "our results connect with several observations made earlier in the literature on longitudinal and functional data analysis .",
    "many longitudinal studies follow the independent design , and the number of sampling points on each curve is typically small . in such settings , it is widely recognized that one needs to pool the data to obtain good estimates , and the two - step procedure of averaging the smooth curves may be suboptimal .",
    "our analysis here provides a rigorous justification for such empirical observations by pinpointing to what extent the two - step procedure is suboptimal .",
    "the phase transition observed here also relates to the earlier work by @xcite on estimating eigenfunctions of the covariance kernel when the number of sampling points is either fixed or of larger than @xmath43 for some @xmath44 .",
    "it was shown that the eigenfunctions can be estimated at the rate of @xmath45 in the former case and @xmath32 in the latter .",
    "we show here that estimating the mean function has similar behavior .",
    "furthermore , we characterize the exact nature of such transition behavior as the sampling frequency changes .    the rest of the paper is organized as follows . in section [ commondesignsec ] the optimal rate of convergence under the common design is established .",
    "we first derive a minimax lower bound and then show that the lower bound is in fact rate sharp .",
    "this is accomplished by constructing a rate - optimal smoothing splines estimator .",
    "the minimax upper bound is obtained separately for the common fixed design and common random design .",
    "section [ inddesignsec ] considers the independent design and establishes the optimal rate of convergence in this case .",
    "the rate - optimal estimators are easily implementable .",
    "numerical studies are carried out in section [ numericsec ] to demonstrate the theoretical results .",
    "section [ discussionsec ] discusses connections and differences of our results with other related work .",
    "all proofs are relegated to section [ proofsec ] .",
    "in this section we consider the common design where each curve is observed at the same set of locations @xmath46 .",
    "we first derive a minimax lower bound and then show that this lower bound is sharp by constructing a smoothing splines estimator that attains the same rate of convergence as the lower bound .",
    "let @xmath47 be the collection of probability measures for a random function @xmath4 such that its sample path is @xmath11 times differentiable almost surely and @xmath48 ^ 2\\,dt\\le m_0\\ ] ] for some constant @xmath49 .",
    "our first main result establishes the minimax lower bound for estimating the mean function over @xmath47 under the common design .",
    "[ thestcommonlo ] suppose the sampling locations are common in model ( [ eqobsmod ] ) .",
    "then there exists a constant @xmath50 depending only on @xmath51 and the variance @xmath52 of measurement error @xmath53 such that for any estimate @xmath54 based on observations @xmath55 , @xmath56    the lower bound established in theorem [ thestcommonlo ] holds true for both common fixed design where @xmath57 s are deterministic , and common random design where @xmath57 s are also random .",
    "the term @xmath30 in the lower bound is due to the deterministic approximation error , and the term @xmath41 is attributed to the stochastic error .",
    "it is clear that neither can be further improved . to see this ,",
    "first consider the situation where there is no stochastic variation and the mean function @xmath16 is observed exactly at the points @xmath57 , @xmath58 .",
    "it is well known [ see , e.g. , @xcite ] that due to discretization , it is not possible to recover @xmath16 at a rate faster than @xmath30 for all @xmath16 such that @xmath59 ^ 2\\le m_0 $ ] . on the other hand ,",
    "the second term @xmath41 is inevitable since the mean function @xmath16 can not be estimated at a faster rate even if the whole random functions @xmath60 are observed completely .",
    "we shall show later in this section that the rate given in the lower bound is optimal in that it is attainable by a smoothing splines estimator .",
    "it is interesting to notice the phase transition phenomenon in the minimax bound .",
    "when the sampling frequency @xmath26 is large , it has no effect on the rate of convergence , and @xmath16 can be estimated at the rate of @xmath32 , the best possible rate when the whole functions were observed .",
    "more surprisingly , such saturation occurs when @xmath26 is rather small , that is , of the order @xmath28 . on the other hand , when the functions are sparsely sampled ,",
    "that is , @xmath29 , the rate is determined only by the sampling frequency @xmath26 . moreover , the rate @xmath30 is in fact also the optimal interpolation rate . in other words ,",
    "when the functions are sparsely sampled , the mean function @xmath16 can be estimated as well as if it is observed directly without noise .",
    "the rate is to be contrasted with the usual nonparametric regression with @xmath25 observations at arbitrary locations .",
    "in such a setting , it is well known [ see , e.g. , @xcite ] that the optimal rate for estimating @xmath16 is @xmath61 , and typically stochastic error and approximation error are of the same order to balance the bias - variance trade - off .",
    "we now consider the upper bound for the minimax risk and construct specific rate optimal estimators under the common design .",
    "these upper bounds show that the rate of convergence given in the lower bound established in theorem [ thestcommonlo ] is sharp .",
    "more specifically , it is shown that a smoothing splines estimator attains the optimal rate of convergence over the parameter space @xmath47 .",
    "we shall consider a smoothing splines type of estimate suggested by @xcite . observe that @xmath62 ^ 2",
    "$ ] is a squared semi - norm and therefore convex . by jensen s inequality ,",
    "@xmath63 ^ 2\\,dt\\le{\\mathbb e}\\int_{\\mathcal t}\\bigl[x^{(r)}(t)\\bigr]^2\\,dt<\\infty,\\ ] ] which implies that @xmath16 belongs to the @xmath11th order sobolev",
    " hilbert space , @xmath64)&=&\\bigl\\{g\\dvtx[0,1]\\to{\\mathbf r}| g , g^{(1)},\\ldots , g^{(r-1 ) } \\\\ & & \\hspace*{2pt}\\mbox { are absolutely continuous and } g^{(r)}\\in{\\mathcal l}_2([0,1])\\bigr\\}.\\end{aligned}\\ ] ] taking this into account , the following smoothing splines estimate can be employed to estimate @xmath16 : @xmath65 ^ 2\\,dt\\biggr\\},\\ ] ] where @xmath66 is a tuning parameter that balances the fidelity to the data and the smoothness of the estimate .",
    "similarly to the smoothing splines for the usual nonparametric regression , @xmath67 can be conveniently computed , although the minimization is taken over an infinitely - dimensional functional space .",
    "first observe that @xmath67 can be equivalently rewritten as @xmath68 ^ 2\\,dt\\biggr\\}.\\ ] ] appealing to the so - called representer theorem [ see , e.g. , @xcite ] , the solution of the minimization problem can be expressed as @xmath69 for some coefficients @xmath70 , where @xmath71 where @xmath72 is the @xmath26th bernoulli polynomial .",
    "plugging ( [ eqrep ] ) back into ( [ eqdefss ] ) , the coefficients and subsequently @xmath67 can be solved in a straightforward way .",
    "this observation makes the smoothing splines procedure easily implementable .",
    "the readers are referred to @xcite for further details .    despite the similarity between @xmath67 and the smoothing splines estimate in the usual nonparametric regression , they have very different asymptotic properties .",
    "it is shown in the following that @xmath67 achieves the lower bound established in theorem  [ thestcommonlo ] .",
    "the analyses for the common fixed design and the common random design are similar , and we shall focus on the fixed design where the common sampling locations @xmath73 are deterministic . in this case , we assume without loss of generality that @xmath74 .",
    "the following theorem shows that the lower bound established in theorem [ thestcommonlo ] is attained by the smoothing splines estimate  @xmath67 .",
    "[ thsi ] consider the common fixed design and assume that @xmath75 for some constant @xmath76 where we follow the convention that @xmath77 and .",
    "then @xmath78 for any @xmath79 .    together with theorem [ thestcommonlo ] , theorem [ thsi ] shows that @xmath67 is minimax rate optimal if the tuning parameter @xmath80 is set to be of the order @xmath81 .",
    "we note the necessity of the condition given by ( [ eqlocdis ] ) .",
    "it is clearly satisfied when the design is equidistant , that is , @xmath82 .",
    "the condition ensures that the random functions are observed on a sufficiently regular grid .",
    "it is of conceptual importance to compare the rate of @xmath67 with those generally achieved in the usual nonparametric regression setting .",
    "defined by ( [ eqdefss0 ] ) , @xmath67 essentially regresses @xmath22 on @xmath57 .",
    "similarly to the usual nonparametric regression , the validity of the estimate is driven by @xmath83 .",
    "the difference , however , is that @xmath84 are highly correlated because they are observed from the same random function @xmath85 .",
    "when all the @xmath22 s are independently sampled at @xmath57 s , it can be derived that the optimal rate for estimating @xmath16 is @xmath86 . as we show here",
    ", the dependency induced by the functional nature of our problem leads to the different rate @xmath23 .",
    "a distinct feature of the behavior of @xmath67 is in the choice of the tuning parameter  @xmath80 . tuning parameter selection plays a paramount role in the usual nonparametric regression , as it balances the the tradeoff between bias and variance .",
    "optimal choice of @xmath80 is of the order @xmath87 in the usual nonparametric regression . in contrast , in our setting , more flexibility is allowed in the choice of the tuning parameter in that @xmath67 is rate optimal so long as @xmath80 is sufficiently small .",
    "in particular , taking @xmath88 , @xmath67 reduces to the splines interpolation , that is , the solution to @xmath89 ^ 2 \\qquad\\mbox{subject to } g(t_j)=\\bar{y}_{\\cdot j},\\qquad j=1,\\ldots , m.\\ ] ] this amounts to , in particular , estimating @xmath19 by @xmath34 . in other words , there is no benefit from smoothing in terms of the convergence rate .",
    "however , as we will see in section [ numericsec ] , smoothing can lead to improved finite sample performance .",
    "more general statements can also be made without the condition on the spacing of sampling points .",
    "more specifically , denote by @xmath90 the discretization resolution . using the same argument",
    ", one can show that the optimal convergence rate in the minimax sense is @xmath91 and @xmath67 is rate optimal so long as @xmath92 .",
    "although we have focused here on the case when the sampling points are deterministic , a similar statement can also be made for the setting where the sampling points are random .",
    "in particular , assuming that @xmath57 are independent and identically distributed with a density function @xmath93 such that @xmath94 and @xmath95 , it can be shown that the smoothing splines estimator @xmath96 satisfies @xmath97 for any @xmath79 .",
    "in other words , @xmath98 remains rate optimal .",
    "in many applications , the random functions @xmath99 are not observed at common locations . instead , each curve is discretely observed at a different set of points [ see , e.g. , @xcite , @xcite , @xcite , @xcite ] . in these settings , it is more appropriate to model the sampling points @xmath7 as independently sampled from a common distribution . in this section",
    "we shall consider optimal estimation of the mean function under the independent design .",
    "interestingly , the behavior of the estimation problem is drastically different between the common design and the independent design . to keep our treatment general , we allow the number of sampling points to vary .",
    "let @xmath26 be the harmonic mean of @xmath100 , that is , @xmath101 denote by @xmath102 the collection of sampling frequencies @xmath103 whose harmonic mean is @xmath26 .",
    "in parallel to theorem [ thestcommonlo ] , we have the following minimax lower bound for estimating @xmath16 under the independent design .    [ thestmeanlo ]",
    "suppose @xmath7 are independent and identically distributed with a density function @xmath93 such that @xmath94",
    ". then there exists a constant @xmath50 depending only on @xmath51 and @xmath52 such that for any estimate @xmath54 based on observations @xmath104 , @xmath105    the minimax lower bound given in theorem [ thestmeanlo ] can also be achieved using the smoothing splines type of estimate . to account for the different sampling frequency for different curves , we consider the following estimate of @xmath16 : @xmath106 ^ 2\\,dt\\biggr\\}.\\ ] ]    [ thestmean ] under the conditions of theorem [ thestmeanlo ] , if @xmath107 , then the smoothing splines estimator @xmath96 satisfies @xmath108\\\\[-8pt ] & & \\qquad=0.\\nonumber\\end{aligned}\\ ] ] in other words , @xmath67 is rate optimal",
    ".    theorems [ thestmeanlo ] and [ thestmean ] demonstrate both similarities and significant differences between the two types of designs in terms of the convergence rate . for either the common design or the independent design , the sampling frequency only plays a role in determining the convergence rate when the functions are sparsely sampled , that is , @xmath29 .",
    "but how the sampling frequency affects the convergence rate when each curve is sparsely sampled differs between the two designs . for the independent design , the total number of observations @xmath27 , whereas for the common design @xmath26 alone , determines the minimax rate .",
    "it is also noteworthy that when @xmath29 , the optimal rate under the independent design , @xmath87 , is the same as if all the observations are independently observed . in other words ,",
    "the dependency among @xmath84 does not affect the convergence rate in this case .",
    "we emphasize that theorems [ thestmeanlo ] and [ thestmean ] apply to both deterministic and random sampling frequencies .",
    "in particular for random sampling frequencies , together with the law of large numbers , the same minimax bound holds when we replace the harmonic mean by @xmath109 when assuming that @xmath110 s are independent .",
    "a popular strategy to handle discretely sampled functional data in practice is a two - stage procedure . in the first step ,",
    "nonparametric regression is run for data from each curve to obtain estimate @xmath111 of @xmath99 , @xmath112 .",
    "for example , they can be obtained by smoothing splines @xmath113 ^ 2\\,dt\\biggr\\}.\\ ] ] any subsequent inference can be carried out using the @xmath114 as if they were the original true random functions . in particular , the mean function @xmath16 can be estimated by the simple average @xmath115 although formulated differently , it is worth pointing out that this procedure is equivalent to the smoothing splines estimate @xmath116 under the common design .",
    "[ prequiv ] under the common design , that is , @xmath117 for @xmath118 and @xmath119 .",
    "the estimate @xmath120 from the two - stage procedure is equivalent to the smoothing splines estimate @xmath116 : @xmath121 .    in light of theorems [ thestcommonlo ] and [ thsi ]",
    ", the two - step procedure is also rate optimal under the common design .",
    "but it is of great practical importance to note that in order to achieve the optimality , it is critical that in the first step we _ undersmooth _ each curve by using a sufficiently small tuning parameter .    under independent design",
    ", however , the equivalence no long holds .",
    "the success of the two - step estimate @xmath120 depends upon getting a good estimate of each curve , which is not possible when @xmath26 is very small . in the extreme case of @xmath37 ,",
    "the procedure is no longer applicable , but theorem [ thestmean ] indicates that smoothing splines estimate @xmath67 can still achieve the optimal convergence rate of @xmath38 .",
    "the readers are also referred to @xcite for discussions on the pros and cons of similar two - step procedures in the context of estimating the functional principal components .",
    "the smoothing splines estimators are easy to implement .",
    "to demonstrate the practical implications of our theoretical results , we carried out a set of simulation studies .",
    "the true mean function @xmath16 is fixed as @xmath122 where @xmath123 and @xmath124 for @xmath125 .",
    "the random function @xmath4 was generated as @xmath126 where @xmath127 are independently sampled from the uniform distribution on@xmath128 $ ] , and @xmath129 are deterministic .",
    "it is not hard to see that @xmath130 are the eigenvalues of the covariance function of @xmath4 and therefore determine the smoothness of a sample curve .",
    "in particular , we take @xmath131 .",
    "it is clear that the sample path of @xmath4 belongs to the second order sobolev space ( @xmath132 ) .",
    "( solid grey line ) are given in the right panel together with the spline interpolation estimate ( solid black line ) and smoothing splines estimate ( red dashed line ) with the tuning parameter chosen to yield the smallest integrated squared error .",
    "the left panel gives the integrated squared error of the smoothing splines estimate as a function of the tuning parameter .",
    "it is noted that the smoothing splines estimate essentially reduces to the spline interpolation for @xmath80 smaller than @xmath133 . ]",
    "we begin with a set of simulations designed to demonstrate the effect of interpolation and smoothing under common design .",
    "a data set of fifty curves were first simulated according to the aforementioned scheme .",
    "for each curve , ten noisy observations were taken at equidistant locations on each curve following model ( [ eqobsmod ] ) with @xmath134 .",
    "the observations , together with @xmath16 ( grey line ) , are given in the right panel of figure [ figssi ] .",
    "smoothing splines estimate @xmath67 is also computed with a variety of values for @xmath80 .",
    "the integrated squared error , @xmath135 , as a function of the tuning parameter @xmath80 is given in the left panel .",
    "for @xmath80 smaller than @xmath133 , the smoothing splines estimate essentially reduces to the spline interpolation . to contrast the effect of interpolation and smoothing",
    ", the right panel also includes the interpolation estimate ( solid black line ) and @xmath116 ( red dashed line ) with the tuning parameter chosen to minimize the integrated squared error .",
    "we observe from the figure that smoothing does lead to slightly improved finite sample performance although it does not affect the convergence rate as shown in section [ commondesignsec ] .",
    "the next numerical experiment intends to demonstrate the effect of sample size  @xmath3 , sampling frequency @xmath26 as well as design . to this end , we simulated @xmath3 curves , and from each curve , @xmath26 discrete observations were taken following model ( [ eqobsmod ] ) with @xmath136 .",
    "the sampling locations are either fixed at @xmath137 , @xmath138 , for common design or randomly sampled from the uniform distribution on @xmath139 $ ] .",
    "the smoothing splines estimate @xmath67 for each simulated data set , and the tuning parameter is set to yield the smallest integrated squared error and therefore reflect the best performance of the estimating procedure for each data set .",
    "we repeat the experiment with varying combinations of @xmath140 or @xmath141 , @xmath142 or @xmath143 . for the common design",
    ", we restrict to @xmath144 or @xmath143 to give more meaningful comparison .",
    "the true function @xmath16 as well as its estimates obtained in each of the settings are given in figure [ figtypical ] .",
    ", @xmath3 and type of design on estimating @xmath16 : smoothing splines estimates obtained under various combinations are plotted together with @xmath16 . ]",
    "figure [ figtypical ] agrees pretty well with our theoretical results .",
    "for instance , increasing either @xmath26 or @xmath3 leads to improved estimates , whereas such improvement is more visible for small values of @xmath26 . moreover , for the same value of @xmath26 and @xmath3 , independent designs tend to yield better estimates .",
    "to further contrast the two types of designs and the effect of sampling frequency on estimating @xmath16 , we now fix the number of curves at @xmath145 . for the common design , we consider @xmath146 or @xmath147 . for the independent design ,",
    "we let @xmath148 or @xmath147 . for each combination of @xmath149 ,",
    "two hundred data sets were simulated following the same mechanism as before .",
    "figure [ figdesignfig ] gives the estimation error averaged over the one hundred data sets for each combination of @xmath149 .",
    "it clearly shows    : the black solid line and circles correspond to common design whereas the red dashed lines and circles correspond to independent design .",
    "the error bars correspond to the average @xmath150 one standard errors based on two hundred repetitions .",
    "note that both axes are in log scale to yield better comparison .",
    "]    that independent design is preferable over common design when @xmath26 is small ; and the two types of designs are similar when @xmath26 is large .",
    "both phenomena are in agreement with our theoretical results developed in the earlier sections .",
    "we have established the optimal rates of convergence for estimating the mean function under both the common design and independent design .",
    "the results reveal several significant differences in the behavior of the minimax estimation problem between the two designs .",
    "these revelations have important theoretical and practical implications .",
    "in particular , for sparsely sampled functions , the independent design leads to a faster rate of convergence when compared to the common design and thus should be preferred in practice .    the optimal rates of convergence for estimating the mean function based on discretely sampled random functions behave in a fundamentally different way from the minimax rate of convergence in the conventional nonparametric regression problems .",
    "the optimal rates in the mean function estimation are jointly determined by the sampling frequency @xmath26 and the number of curves @xmath3 rather than the total number of observations @xmath27 .",
    "the observation that one can estimate the mean function as well as if the the whole curves are available when @xmath31 bears some similarity to some recent findings on estimating the covariance kernel and its eigenfunction under independent design . assuming that @xmath4 is twice differentiable ( i.e. , @xmath132 ) , @xcite showed that when @xmath151 for some @xmath44 , the covariance kernel and its eigenfunctions can be estimated at the rate of @xmath32 when using a two - step procedure .",
    "more recently , the cutoff point is further improved to @xmath152 with general @xmath11 by @xcite using an alternative method .",
    "intuitively one may expect estimating the covariance kernel to be more difficult than estimating the mean function , which suggests that these results may not be improved much further for estimating the covariance kernel or its eigenfunctions .",
    "we have also shown that the particular smoothing splines type of estimate discussed earlier by @xcite attains the optimal convergence rates under both designs with appropriate tuning .",
    "the smoothing splines estimator is well suited for nonparametric estimation over sobolev spaces .",
    "we note , however , other nonparametric techniques such as kernel or local polynomial estimators can also be used .",
    "we expect that kernel smoothing or other methods with proper choice of the tuning parameters can also achieve the optimal rate of convergence .",
    "further study in this direction is beyond the scope of the current paper , and we leave it for future research .    finally , we emphasize that although we have focused on the univariate sobolev space for simplicity , the phenomena observed and techniques developed apply to more general functional spaces .",
    "consider , for example , the multivariate setting where @xmath1^d$ ] .",
    "following the same arguments , it can be shown that the minimax rate for estimating an @xmath11-times differentiable function is @xmath153 under the common design and @xmath154 under the independent design .",
    "the phase transition phenomena thus remain in the multidimensional setting under both designs with a transition boundary of @xmath155 .",
    "proof of theorem [ thestcommonlo ] let @xmath156 be the collection all measurable functions of @xmath157 . first note that it is straightforward to show that @xmath158 by considering @xmath4 as an unknown constant function where the problem essentially becomes estimating the mean from @xmath3 i.i.d .",
    "observations , and @xmath32 is known as the optimal rate .",
    "it now suffices to show that @xmath159 let @xmath160 be @xmath161 functions from @xmath162 with distinct support , that is , @xmath163 where @xmath164 , @xmath165 , and @xmath166 is an @xmath11 times differentiable function with support @xmath167 $ ] .",
    "see @xcite for explicit construction of such functions .    for each @xmath168 , define @xmath169 it is clear that @xmath170 the claim then follows from an application of assouad s lemma [ @xcite ] .",
    "proof of theorem [ thsi ] it is well known [ see , e.g. , @xcite ] that @xmath116 can be characterized as the solution to the following : @xmath171 ^ 2\\,dt \\qquad\\mbox{subject to } g(t_j)=\\hat{g}_\\lambda(t_j),\\qquad j=1,\\ldots , m.\\ ] ] write @xmath172 and let @xmath173 be the linear interpolation of @xmath174 , that is , @xmath175 then @xmath176 where @xmath177 be the operator associated with the @xmath11th order spline interpolation , that is , @xmath178 is the solution to @xmath171 ^ 2\\,dt\\qquad \\mbox{subject to } g(t_j)=f(t_j),\\qquad j=1,\\ldots , m.\\ ] ]    recall that @xmath177 is a linear operator in that @xmath179 [ see , e.g. , @xcite ] .",
    "therefore , @xmath180 . by the triangular inequality ,",
    "@xmath181 the first term on the right - hand side represents the approximation error of spline interpolation for @xmath16 , and it is well known that it can be bounded by [ see , e.g. , @xcite ] @xmath182 ^ 2\\,dt\\\\ & & \\qquad\\le c_0m_0m^{-2r}.\\nonumber\\end{aligned}\\ ] ] hereafter , we shall use @xmath183 as a generic constant which may take different values at different appearance .",
    "it now remains to bound @xmath184 .",
    "we appeal to the relationship between spline interpolation and the best local polynomial approximation .",
    "let @xmath185\\ ] ] with the convention that @xmath186 for @xmath187 and @xmath188 for @xmath189 .",
    "denote by @xmath190 the best approximation error that can be achieved on @xmath191 by a polynomial of order less that @xmath11 , that is , @xmath192",
    "^ 2\\,dt.\\ ] ] it can be shown [ see , e.g. , theorem 4.5 on page 147 of @xcite ] that @xmath193 then @xmath194 together with the fact that @xmath195 we have @xmath196 ^ 2\\\\ & \\le&c_0m^{-1}\\sum_{j=1}^m\\bigl([\\bar{y}_{\\cdot j}-\\hat { g}_\\lambda(t_j)]^2+[\\bar{y}_{\\cdot j}-g_0(t_j ) ] ^2\\bigr).\\end{aligned}\\ ] ]    observe that @xmath197 ^ 2\\biggr)=c_0\\sigma_0 ^ 2n^{-1}.\\ ] ] it suffices to show that @xmath198 ^ 2=o_p(m^{-2r}+n^{-1}).\\ ] ] to this end , note that by the definition of @xmath67 , @xmath199 ^ 2\\,dt\\\\ & \\le & { 1\\over m}\\sum_{j=1}^m\\bigl(\\bar{y}_{\\cdot j}-g_0(t_{j})\\bigr)^2",
    "+ \\lambda\\int_{\\mathcal t}\\bigl[g_0^{(r)}(t ) \\bigr]^2\\,dt\\\\ & \\le & o_p(m^{-2r}+n^{-1}),\\end{aligned}\\ ] ] because @xmath79 .",
    "the proof is now complete .",
    "proof of theorem [ thestmeanlo ] note that any lower bound for a specific case yields immediately a lower bound for the general case .",
    "it therefore suffices to consider the case when @xmath4 is a gaussian process and @xmath200 .",
    "denote by @xmath201 where @xmath202 is a constant to be specified later .",
    "let @xmath203 be a binary sequence , and write @xmath204 where @xmath205 .",
    "it is not hard to see that @xmath206 ^ 2\\,dt&=&m_0\\pi^{-2r}\\sum_{k\\ge n+1}^{2n}(\\pi",
    "k)^{2r}(n^{-1/2}k^{-r}b_{k - n } ) ^2\\\\ & = & m_0n^{-1}\\sum_{k= n+1}^{2n}b_{k - n}\\le m_0.\\end{aligned}\\ ] ] furthermore , @xmath207 for some constant @xmath183 . by the varshamov ",
    "gilbert bound [ see , e.g. , @xcite ] , there exists a collection of binary sequences @xmath208 such that @xmath209 , and @xmath210 then @xmath211    assume that @xmath4 is a gaussian process with mean @xmath212 , @xmath213 follows a uniform distribution on @xmath15 and the measurement error @xmath214 .",
    "conditional on @xmath215 , @xmath216 follows a multivariate normal distribution with mean @xmath217 and covariance matrix @xmath218 .",
    "therefore , the kullback  leibler distance from probability measure @xmath219 to @xmath220 can be bounded by @xmath221\\\\ & \\le&n\\sigma_0^{-2}{\\mathbb e}_{t } \\bigl\\|\\mu_{g_{b^{(j)}}}-\\mu_{g_{b^{(k)}}}\\bigr\\| ^2\\\\ & = & nm\\sigma_0^{-2}\\bigl\\|g_{b^{(j)}}-g_{b^{(k)}}\\bigr\\|_{{\\mathcal l}_2}^2\\\\ & \\le&c_1nm\\sigma_0^{-2}n^{-2r}.\\end{aligned}\\ ] ] an application of fano s lemma now yields @xmath222 with an appropriate choice of @xmath223 , for any estimate @xmath54 .",
    "this in turn implies that @xmath224 the proof can then be completed by considering @xmath4 as an unknown constant function .",
    "proof of theorem [ thestmean ] for brevity , in what follows , we treat the sampling frequencies @xmath225 as deterministic .",
    "all the arguments , however , also apply to the situation when they are random by treating all the expectations and probabilities as conditional on @xmath100 .",
    "similarly , we shall also assume that @xmath57 s follow uniform distribution .",
    "the argument can be easily applied to handle more general distributions .",
    "it is well known that @xmath162 , endowed with the norm @xmath226 forms a reproducing kernel hilbert space [ @xcite ] .",
    "let @xmath227 be the collection of all polynomials of order less than @xmath11 and @xmath228 be its orthogonal complement in @xmath162 .",
    "let @xmath229 be a set of orthonormal basis functions of @xmath227 , and @xmath230 an orthonormal basis of @xmath228 such that any @xmath231 admits the representation @xmath232 furthermore , @xmath233 where @xmath234 and @xmath235 .",
    "recall that @xmath236 ^ 2\\biggr\\},\\ ] ] where @xmath237 for brevity , we shall abbreviate the subscript of @xmath238 hereafter when no confusion occurs .",
    "write @xmath239 ^ 2\\biggr)\\\\ & = & { \\mathbb e}\\bigl([y_{11}-g_0(t_{11})]^2\\bigr)+\\int_{{\\mathcal t}}[g(s)-g_0(s)]^2\\,ds.\\end{aligned}\\ ] ] let @xmath240 ^ 2\\biggr\\}.\\ ] ] denote @xmath241 ^ 2;\\qquad \\ell_{\\infty,\\lambda}(g)=\\ell_\\infty(g)+\\lambda\\int\\bigl[g^{(r)}\\bigr]^2.\\ ] ]    let @xmath242 where @xmath243 and @xmath244 stands for the frchet derivative .",
    "it is clear that @xmath245 we proceed by bounding the three terms on the right - hand side separately . in particular",
    ", it can be shown that @xmath246 ^ 2\\ ] ] and @xmath247 furthermore , if @xmath248 then @xmath249 therefore , @xmath250 taking @xmath251 yields @xmath252    we now set to establish bounds ( [ eqerr1])([eqerr3 ] ) . for brevity , we shall assume in what follows that all expectations are taken conditionally on @xmath100 unless otherwise indicated .",
    "define @xmath253 where @xmath254 .",
    "we begin with @xmath255 .",
    "write @xmath256 then @xmath257 ^ 2 \\bigr)+\\sum_{k\\ge1}(b_{k}-{a}_{k})^2.\\ ] ] it is not hard to see @xmath258 hence , @xmath259 ^ 2.\\end{aligned}\\ ] ]    next , we consider @xmath260 .",
    "notice that @xmath261 . therefore @xmath262",
    "^ 2&=&{\\mathbb e}[d\\ell _ { mn}(\\bar{g})f - d\\ell_{\\infty}(\\bar{g})f]^2\\\\ & = & { 4\\over n^2}\\sum_{i=1}^n{1\\over m_i^2}{\\operatorname{{\\mathbb var}}}\\biggl[\\sum _ { j=1}^{m_i}\\bigl([y_{ij}-\\bar{g}(t_{ij } ) ] f(t_{ij})\\bigr)\\biggr].\\ ] ] note that @xmath263f(t_{ij})\\bigr)\\biggr]\\\\ & & \\qquad={\\operatorname{{\\mathbb var}}}\\biggl[{\\mathbb e}\\biggl(\\sum_{j=1}^{m_i}[y_{ij}-\\bar { g}(t_{ij})]f(t_{ij})| t\\biggr)\\biggr]+{\\mathbb e}\\biggl[{\\operatorname{{\\mathbb var}}}\\biggl(\\sum_{j=1}^{m_i}y_{ij}f(t_{ij})|t\\biggr)\\biggr]\\\\ & & \\qquad={\\operatorname{{\\mathbb var}}}\\biggl[\\sum_{j=1}^{m_i}\\bigl([g_0(t_{ij})-\\bar { g}(t_{ij})]f(t_{ij})\\bigr)\\biggr]+{\\mathbb e}\\biggl[{\\operatorname{{\\mathbb var}}}\\biggl(\\sum_{j=1}^{m_i}y_{ij}f(t_{ij})|m_i , t\\biggr)\\biggr].\\end{aligned}\\ ] ]    the first term on the rightmost - hand side can be bounded by @xmath264f(t_{ij})\\bigr)\\biggr]\\\\ & & \\qquad= m_i{\\operatorname{{\\mathbb var}}}\\bigl([g_0(t_{i1})-\\bar{g}(t_{i1 } ) ] f(t_{i1})\\bigr)\\\\ & & \\qquad\\le m_i{\\mathbb e}\\bigl([g_0(t_{i1})-\\bar{g}(t_{i1 } ) ] f(t_{i1})\\bigr)^2\\\\ & & \\qquad= m_i\\int_{\\mathcal t}\\bigl([g_0(t)-\\bar{g}(t)]f(t ) \\bigr)^2\\,dt\\\\ & & \\qquad\\le m_i\\int_{\\mathcal t}[g_0(t)-\\bar{g}(t)]^2\\,dt\\int_{\\mathcal t}f^2(t)\\,dt,\\end{aligned}\\ ] ] where the last inequality follows from the cauchy  schwarz inequality .",
    "together with ( [ eqerr1 ] ) , we get @xmath265f(t_{ij})\\bigr)\\biggr]\\le c_0m_i\\|f\\|_{{\\mathcal l}_2}^2\\lambda.\\ ] ]    we now set out to compute the the second term .",
    "first observe that @xmath266 where @xmath267 is kronecker s delta .",
    "therefore , @xmath268 \\\\ & & \\qquad = m_i(m_i-1)\\int_{{\\mathcal t}\\times{\\mathcal t } } f(s)c_0(s , t)f(t)\\,ds\\,dt\\\\ & & \\qquad\\quad{}+m_i\\sigma _ 0 ^ 2 \\|f\\|_{{\\mathcal l}_2}^2 + m_i\\int_{{\\mathcal t } } f^2(s)c(s , s)\\,ds.\\end{aligned}\\ ] ]    summing up , we have @xmath269 ^ 2 \\le { c_0\\over",
    "n^2}\\biggl(\\sum_{i=1}^n { 1\\over m_i}\\biggr)+{4c_k\\over n},\\ ] ] where @xmath270 therefore , @xmath271\\\\ & \\le & { c_0\\over n^2}\\biggl(\\sum_{i=1}^n { 1\\over m_i}\\biggr)\\sum _ { k\\ge1}(1+\\rho_{k}^{-1})^\\alpha(1+\\lambda\\rho_{k}^{-1})^{-2}\\\\ & & { } + { 1\\over n}\\sum_{k\\ge1}(1+\\rho_{k}^{-1})^\\alpha(1+\\lambda \\rho_{k}^{-1})^{-2}c_k.\\end{aligned}\\ ] ]    observe that @xmath272 and @xmath273 thus , @xmath274.\\ ] ]    it remains to bound @xmath275 .",
    "it can be easily verified that @xmath276.\\ ] ] then @xmath277 ^ 2.\\end{aligned}\\ ] ] clearly , @xmath278 .",
    "write @xmath279 then , by the cauchy ",
    "schwarz inequality , @xmath280 ^ 2\\\\ & & \\qquad=\\biggl[\\sum_{k_1\\ge1 } h_{k_1}\\biggl({1\\over n}\\sum _",
    "{ i=1}^n{1\\over m_i}\\sum_{j=1}^{m_i}\\phi_{k_1}(t_{ij})- \\int_{{\\mathcal t}}\\phi_{k_1}(s)\\,ds\\biggr)\\biggr]^2\\\\ & & \\qquad\\le\\biggl[\\sum_{k_1\\ge1 } ( 1+\\rho_{k_1}^{-1})^{\\gamma } h^2_{k_1}\\biggr]\\\\ & & \\qquad\\quad{}\\times\\biggl[\\sum_{k_1\\ge1 } ( 1+\\rho_{k_1}^{-1})^{-\\gamma } \\biggl({1\\over n}\\sum_{i=1}^n{1\\over m_i}\\sum_{j=1}^{m_i}\\phi _ { k_1}(t_{ij})- \\int_{{\\mathcal t}}\\phi_{k_1}(s)\\,ds\\biggr)^2\\biggr]\\\\ & & \\qquad\\le\\|\\hat{g}-\\bar{g}\\|_\\gamma^2(1+\\rho_k^{-1})^\\gamma\\\\ & & \\qquad\\quad{}\\times \\biggl[\\sum_{k_1\\ge1 } ( 1+\\rho_{k_1}^{-1})^{-\\gamma}\\biggl({1\\over n}\\sum _ { i=1}^n{1\\over m_i}\\sum_{j=1}^{m_i}\\phi_{k_1}(t_{ij})- \\int_{{\\mathcal t}}\\phi_{k_1}(s)\\,ds\\biggr)^2\\biggr]\\end{aligned}\\ ] ] for any @xmath281 , where in the last inequality , we used the fact that @xmath282 following a similar calculation as before , it can be shown that @xmath283\\\\ & & \\qquad\\le{1\\over n^2}\\biggl(\\sum_{i=1}^n { 1\\over m_i}\\biggr)\\sum_{k_1\\ge 1 } ( 1+\\rho_{k_1}^{-1})^{-\\gamma},\\end{aligned}\\ ] ] which is finite whenever @xmath284 . recall that @xmath26 is the harmonic mean of @xmath100 . therefore , @xmath285    if @xmath286 , then taking @xmath287 yields @xmath288 assuming that @xmath289 together with the triangular inequality @xmath290 therefore , @xmath291 together with ( [ eqgaal ] ) , @xmath292    we conclude by noting that in the case when @xmath225 are random , @xmath26 can also be replaced with the expectation of the harmonic mean thanks to the law of large numbers .",
    "proof of proposition [ prequiv ] let @xmath293 be the smoothing spline operator , that is , @xmath294 is the solution to @xmath295 ^ 2\\,dt\\biggr\\}.\\ ] ] it is clear that @xmath296 and @xmath297 because @xmath293 is a linear operator [ see , e.g. , @xcite ] , we have @xmath298",
    "we thank an associate editor and two referees for their constructive comments which have helped to improve the presentation of the paper ."
  ],
  "abstract_text": [
    "<S> the problem of estimating the mean of random functions based on discretely sampled data arises naturally in functional data analysis . in this paper , we study optimal estimation of the mean function under both common and independent designs . </S>",
    "<S> minimax rates of convergence are established and easily implementable rate - optimal estimators are introduced . </S>",
    "<S> the analysis reveals interesting and different phase transition phenomena in the two cases . under the common design </S>",
    "<S> , the sampling frequency solely determines the optimal rate of convergence when it is relatively small and the sampling frequency has no effect on the optimal rate when it is large . on the other hand , under the independent design </S>",
    "<S> , the optimal rate of convergence is determined jointly by the sampling frequency and the number of curves when the sampling frequency is relatively small . </S>",
    "<S> when it is large , the sampling frequency has no effect on the optimal rate . </S>",
    "<S> another interesting contrast between the two settings is that smoothing is necessary under the independent design , while , somewhat surprisingly , it is not essential under the common design .    . </S>"
  ]
}