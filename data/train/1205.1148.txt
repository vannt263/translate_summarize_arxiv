{
  "article_text": [
    "in recent years machine learning researchers and practitioners have been focusing on convex optimization methods due to their computational advantages and well understood mathematical properties .",
    "the many successes of convexity - based algorithms are witnesses to that .",
    "while it is easily recognized that allowing for non - convex objectives opens up a plethora of possibilities for better solutions to machine learning problems , much of the contemporary research has deliberately avoided them .",
    "the reason is the widely known fact that non - convexity often results in np - hard problems .",
    "however this choice comes at a cost as the shortcomings of convex objectives are also well understood .",
    "recent work @xcite showed that convex loss functions can not be made robust in the presence of label noise because they cause unbounded growth of penalties for large negative margins .",
    "@xcite further characterized this effect by analyzing various convex losses and found that none of them is tolerant to non - uniform label noise . in practice label noise",
    "turns out to be a serious problem due to the fact that it affects real - world data sets to a significant degree . since label noise manifests itself throughout the optimization as large negative margins , the finally constructed decision hyperplane that represents the global minimum of any convex loss tends to be pulled by the mislabeled training examples away from the minimizer of classification error .",
    "therefore , even though solving convex losses to optimality is feasible , when label noise causes the lowest objective value to not correspond to the lowest attainable training error , the entire exercise misses the mark .",
    "consequently , any approach exhibiting this problem does not stand to benefit from improved optimization techniques .",
    "for example , fig .",
    "[ fig : non - monotonic ] shows the broken correspondence between training error and objective value when a convex loss is used in a training problem of practical significance``ocr in photos '' .",
    "the human task of tagging characters in photos of potentially poor quality is not easy , so the presence of mislabeled examples in the training set is not surprising .",
    "even worse , routinely used semi - automatic preparation of training data is also contributing to mistakes . the problem may gradually disappear for cleaner data sets , which often happen to be the cases when convex losses produce excellent classifiers .",
    "unfortunately the nature of large - scale supervised learning does not permit elaborate quality assurance for data sets that are handed out to training algorithms ; accordingly label noise will continue to pollute real - world data sets .",
    "moreover future intelligent systems will rely increasingly on weakly labeled or unlabeled data increasing the need for noise tolerance .",
    "@xcite and @xcite took these lessons and independently studied two different non - convex but seemingly well behaved types of loss functions .",
    "@xcite also explored non - convexity in the context of svm with ramp loss , but their focus was on achieving sparser sets of support vectors and speed of training rather than improved accuracy and robustness of the constructed classifier .",
    "[ name = plot1,tick label style = font=,xtick = 3.61,3.62,ytick=5.95,6.03,6.1 ] coordinates ( 3.605463,6.095900 ) ( 3.605463,6.095900 ) ( 3.605463,6.095900 ) ( 3.605463,6.095900 ) ( 3.605463,6.095900 ) ( 3.605463,6.095900 ) ( 3.605463,6.095900 ) ( 3.605463,6.095900 ) ( 3.605463,6.095900 ) ( 3.621708,6.071400 ) ( 3.622302,6.046900 ) ( 3.622481,6.022300 ) ( 3.622482,6.022300 ) ( 3.622482,6.022300 ) ( 3.622587,6.034600 ) ( 3.624878,5.948700 ) ( 3.624878,5.948700 ) ( 3.624878,5.936500 ) ( 3.624878,5.924200 ) ;    [ name = plot2,tick label style = font=,at=(@xmath1),anchor = west , xtick = 3.61,3.62 ] coordinates ( 3.607717,6.095900 ) ( 3.607717,6.095900 ) ( 3.607717,6.095900 ) ( 3.607717,6.095900 ) ( 3.607717,6.095900 ) ( 3.607717,6.095900 ) ( 3.607717,6.095900 ) ( 3.607717,6.095900 ) ( 3.617321,6.010100 ) ( 3.617685,6.046900 ) ( 3.618311,5.997800 ) ( 3.618514,5.973300 ) ( 3.622547,5.924200 ) ( 3.622547,5.924200 ) ( 3.622547,5.924200 ) ( 3.622549,5.899700 ) ( 3.622549,5.899700 ) ( 3.622550,5.887400 ) ( 3.622550,5.899700 ) ;    [ name = plot3,tick label style = font=,at=(@xmath2),anchor = west ] coordinates ( 3.512335,4.023100 ) ( 3.512335,4.023100 ) ( 3.512335,4.023100 ) ( 3.512335,4.023100 ) ( 3.512335,4.023100 ) ( 3.512335,4.023100 ) ( 3.512336,4.047600 ) ( 3.512336,4.047600 ) ( 3.512336,4.047600 ) ( 3.512336,4.047600 ) ( 3.531346,3.532400 ) ( 3.532142,3.507900 ) ( 3.532143,3.495600 ) ( 3.532143,3.495600 ) ( 3.532693,3.507900 ) ( 3.532693,3.507900 ) ( 3.534466,3.532400 ) ( 3.535221,3.520200 ) ( 3.535221,3.520200 ) ;    [ name = plot4,tick label style = font=,at=(@xmath3),anchor = north , xtick = 3.615,3.625,xlabel=1/risk , ylabel style = xshift=22pt , ylabel = training error ( @xmath4 ) ] coordinates ( 3.614818,5.985500 ) ( 3.614818,5.985500 ) ( 3.614818,5.985500 ) ( 3.614818,5.985500 ) ( 3.614818,5.985500 ) ( 3.614818,5.985500 ) ( 3.614818,5.985500 ) ( 3.614818,5.985500 ) ( 3.614996,5.973300 ) ( 3.623098,5.961000 ) ( 3.623182,5.973300 ) ( 3.623685,5.973300 ) ( 3.623685,5.973300 ) ( 3.623871,5.924200 ) ( 3.623989,5.924200 ) ( 3.627479,5.789300 ) ( 3.627572,5.826100 ) ( 3.627572,5.826100 ) ( 3.627577,5.826100 ) ;    [ name = plot5,tick label style = font=,at=(@xmath5),anchor = north , ytick=4.7,5,5.4,xlabel=1/risk ] coordinates ( 2.076761,5.384500 ) ( 2.076761,5.384500 ) ( 2.076761,5.384500 ) ( 2.076761,5.384500 ) ( 2.076761,5.384500 ) ( 2.076761,5.384500 ) ( 2.076761,5.384500 ) ( 2.076761,5.384500 ) ( 2.076761,5.384500 ) ( 2.111767,4.746700 ) ( 2.111965,4.759000 ) ( 2.114714,4.820300 ) ( 2.118439,4.943000 ) ( 2.118439,4.943000 ) ( 2.120929,5.077900 ) ( 2.123235,5.188300 ) ( 2.123235,5.188300 ) ( 2.124805,5.188300 ) ( 2.125519,5.200500 ) ;    [ name = plot6,tick label style = font=,at=(@xmath6),anchor = north , xtick = 3.04,3.08,3.12,xlabel=1/risk , ytick=7.6,8,8.4 ] coordinates ( 3.035601,7.567800 ) ( 3.035601,7.567800 ) ( 3.035601,7.567800 ) ( 3.035601,7.567800 ) ( 3.035601,7.567800 ) ( 3.036419,7.592300 ) ( 3.036419,7.592300 ) ( 3.036419,7.592300 ) ( 3.036419,7.592300 ) ( 3.102356,7.911200 ) ( 3.114548,8.303700 ) ( 3.114548,8.303700 ) ( 3.114548,8.303700 ) ( 3.114548,8.303700 ) ( 3.114548,8.303700 ) ( 3.114548,8.303700 ) ( 3.115177,8.352800 ) ( 3.115177,8.352800 ) ( 3.115177,8.352800 ) ;    in the present work we continue the study of non - convexity .",
    "we report on training with a non - convex objective using discrete optimization in a formulation adapted to take advantage of emerging hardware that performs adiabatic quantum optimization ( aqo ) .",
    "aqo , first proposed in @xcite , is a quantum computing model with good prospects for scalable and practically useful hardware implementation .",
    "studies of its purported computational superiority over classical computing have repeatedly given encouraging results , e.g. @xcite .",
    "significant investments are underway by the canadian company d - wave to develop a hardware implementation .",
    "a series of rigorous studies of the quantum mechanical properties of the d - wave processors , culminating in a recent nature publication @xcite , have increased the excitement in the quantum computing community for this approach .",
    "this was further fueled by news of a successful collaboration with google @xcite and of lockheed martin purchasing an adiabatic quantum computer .",
    "for machine learning purposes , d - wave s implementation of aqo can be regarded as a black - box discrete optimization engine that accepts any problems formulated as quadratic unconstrained binary optimization ( qubo ) , also equivalent to the ising model and weighted max-2-sat .",
    "it should be noted that this training formulation is a good format for aqo independently of d - wave s efforts since it can be physically realized as the simplest possible multi - qubit configuration  an ising system @xcite .",
    "we do not claim principled superiority of q - loss over other non - convex losses .",
    "however @xmath0-loss is distinguished by the fact that it can be formulated for aqo on quantum hardware that only supports quadratic ( 2-local ) interactions among its qubits using a number of ancillary variables that just grows linearly with the number of training examples . to the best of our current knowledge",
    ", no other non - convex loss has this property .",
    "while all other non - convex losses are tackled by heuristic optimization with very limited success , @xmath0-loss may be solvable to optimality by aqo .",
    "the paper is organized as follows : section  [ training ] defines the training problem ; section  [ q_loss_section ] introduces @xmath0-loss , derives its qubo formulation , and discusses the intuition behind it ; sections  [ bounding_q ]  and  [ discrete_weights ] deal with choosing hyper - parameter values and discretization of variables ; section  [ experiments ] presents our experiments ; and section  [ conclusion ] concludes with an overview and discussion .",
    "technical details can be found in the supplementary material .",
    "we study binary classifiers @xmath7 , where @xmath8 is an input pattern to be classified , @xmath9 is the label associated with @xmath10 , @xmath11 is a vector of weights to be optimized , and @xmath12 is the bias .",
    "training , also known as regularized risk minimization , consists of choosing @xmath13 and @xmath14 by simultaneously minimizing two terms : _ empirical risk _ @xmath15 and _ regularization _ @xmath16 .",
    "@xmath17 , via a _ loss function _",
    "@xmath18 , estimates the error that any candidate classifier causes over a set of @xmath19 training examples @xmath20 .",
    "the argument of @xmath18 is known as the _ margin _ of example @xmath21 with respect to the decision hyperplane defined by @xmath13 and @xmath14 : @xmath22 @xmath23 controls the complexity of the classifier and is necessary for good generalization because classifiers with high complexity display overfitting  they can classify the training set with low error but may not do well on previously unseen data . training amounts to solving @xmath24    the most natural choice for @xmath18 is _ 0 - 1 loss _ , which simply indicates a misclassification for a negative margin : @xmath25 due to the non - convexity of @xmath26 , the resulting optimization problem is np - hard @xcite . to avoid dealing with np - hard optimization problems , in practice @xmath26",
    "is replaced by some convex upper bound ( e.g. square , logistic , exponential , hinge ) , and @xmath23 is usually chosen as @xmath27- or @xmath28-norm penalization of @xmath13 .",
    "this allows arriving at convex optimization problems that can be rigorously analyzed and efficiently solved by classical means .",
    "however , such relaxations are known to compromise the original goal of training because convex losses can be severely misled by label noise in the training data .",
    "because the quantum hardware natively represents a general family of quadratic functions , the simplest loss function that would work is _ square loss _ , which is a convex upper bound to @xmath26 : @xmath29    however , there are two drawbacks of square loss when applied to binary classification . first , in binary classification it does not make sense to penalize large positive margins .",
    "second , as mentioned earlier , square loss has the same flaw as all convex losses  penalties for large negative margins grow unboundedly , which can cause non - robustness with respect to label noise .    coordinates ( -4,16 ) ( -3.75,16 ) ( -3.5,16 ) ( -3.25,16 ) ( -3,16 ) ( -2.75,14.0625 ) ( -2.5,12.25 ) ( -2.25,10.5625 ) ( -2,9 ) ( -1.75,7.5625 ) ( -1.5,6.25 ) ( -1.25,5.0625 ) ( -1,4 ) ( -0.75,3.0625 ) ( -0.5,2.25 ) ( -0.25,1.5625 ) ( 0,1 )",
    "( 0.25,0.5625 ) ( 0.5,0.25 ) ( 0.75,0.0625 ) ( 1,0 ) ( 1.25,0 ) ( 1.5,0 ) ( 1.75,0 ) ( 2,0 ) ( 2.25,0 ) ( 2.5,0 ) ( 2.75,0 ) ( 3,0 ) ( 3.25,0 ) ( 3.5,0 ) ( 3.75,0 ) ( 4,0 ) ; coordinates ( -4,9 ) ( -3.75,9 ) ( -3.5,9 ) ( -3.25,9 ) ( -3,9 ) ( -2.75,9 ) ( -2.5,9 ) ( -2.25,9 ) ( -2,9 ) ( -1.75,7.5625 ) ( -1.5,6.25 ) ( -1.25,5.0625 ) ( -1,4 ) ( -0.75,3.0625 ) ( -0.5,2.25 ) ( -0.25,1.5625 ) ( 0,1 ) ( 0.25,0.5625 ) ( 0.5,0.25 ) ( 0.75,0.0625 ) ( 1,0 ) ( 1.25,0 ) ( 1.5,0 ) ( 1.75,0 ) ( 2,0 ) ( 2.25,0 ) ( 2.5,0 ) ( 2.75,0 ) ( 3,0 ) ( 3.25,0 ) ( 3.5,0 ) ( 3.75,0 ) ( 4,0 ) ; coordinates ( -4,4 ) ( -3.75,4 ) ( -3.5,4 ) ( -3.25,4 ) ( -3,4 ) ( -2.75,4 ) ( -2.5,4 ) ( -2.25,4 ) ( -2,4 ) ( -1.75,4 ) ( -1.5,4 ) ( -1.25,4 ) ( -1,4 ) ( -0.75,3.0625 ) ( -0.5,2.25 ) ( -0.25,1.5625 ) ( 0,1 ) ( 0.25,0.5625 ) ( 0.5,0.25 ) ( 0.75,0.0625 ) ( 1,0 ) ( 1.25,0 ) ( 1.5,0 ) ( 1.75,0 ) ( 2,0 ) ( 2.25,0 ) ( 2.5,0 ) ( 2.75,0 ) ( 3,0 ) ( 3.25,0 ) ( 3.5,0 ) ( 3.75,0 ) ( 4,0 ) ;    coordinates ( -8.000000,36.000000 ) ( -7.500000,36.000000 ) ( -7.000000,36.000000 ) ( -6.500000,36.000000 ) ( -6.000000,36.000000 ) ( -5.500000,36.000000 ) ( -5.000000,36.000000 ) ( -4.500000,30.250000 ) ( -4.000000,25.000000 ) ( -3.500000,20.250000 ) ( -3.000000,16.000000 ) ( -2.500000,12.250000 ) ( -2.000000,9.000000 ) ( -1.500000,6.250000 ) ( -1.000000,4.000000 ) ( -0.500000,2.250000 ) ( 0.000000,1.000000 ) ( 0.500000,0.250000 ) ( 1.000000,0.000000 ) ( 1.500000,0.000000 ) ( 2.000000,0.000000 ) ( 2.500000,0.000000 ) ( 3.000000,0.000000 ) ( 3.500000,0.000000 ) ( 4.000000,0.000000 ) ( 4.500000,0.000000 ) ( 5.000000,0.000000 ) ( 5.500000,0.000000 ) ( 6.000000,0.000000 ) ( 6.500000,0.000000 ) ( 7.000000,0.000000 ) ( 7.500000,0.000000 ) ( 8.000000,0.000000 ) ; coordinates ( -8.000000,40.000000 ) ( -7.500000,38.250000 ) ( -7.000000,37.000000 ) ( -6.500000,36.250000 ) ( -6.000000,36.000000 ) ( -5.500000,36.250000 ) ( -5.000000,37.000000 ) ( -4.500000,38.250000 ) ( -4.000000,40.000000 ) ( -3.500000,42.250000 ) ( -3.000000,45.000000 ) ( -2.500000,48.250000 ) ( -2.000000,52.000000 ) ( -1.500000,56.250000 ) ( -1.000000,61.000000 ) ( -0.500000,66.250000 ) ( 0.000000,72.000000 ) ( 0.500000,78.250000 ) ( 1.000000,85.000000 ) ( 1.500000,92.250000 ) ( 2.000000,100.000000 ) ( 2.500000,108.250000 ) ( 3.000000,117.000000 ) ( 3.500000,126.250000 ) ( 4.000000,136.000000 ) ( 4.500000,146.250000 ) ( 5.000000,157.000000 ) ( 5.500000,168.250000 ) ( 6.000000,180.000000 ) ( 6.500000,192.250000 ) ( 7.000000,205.000000 ) ( 7.500000,218.250000 ) ( 8.000000,232.000000 ) ; coordinates ( -8.000000,81.000000 ) ( -7.500000,72.250000 ) ( -7.000000,64.000000 ) ( -6.500000,56.250000 ) ( -6.000000,49.000000 ) ( -5.500000,42.250000 ) ( -5.000000,36.000000 ) ( -4.500000,30.250000 ) ( -4.000000,25.000000 ) ( -3.500000,20.250000 ) ( -3.000000,16.000000 ) ( -2.500000,12.250000 ) ( -2.000000,9.000000 ) ( -1.500000,6.250000 ) ( -1.000000,4.000000 ) ( -0.500000,2.250000 ) ( 0.000000,1.000000 ) ( 0.500000,0.250000 ) ( 1.000000,0.000000 ) ( 1.500000,0.250000 ) ( 2.000000,1.000000 ) ( 2.500000,2.250000 ) ( 3.000000,4.000000 ) ( 3.500000,6.250000 ) ( 4.000000,9.000000 ) ( 4.500000,12.250000 ) ( 5.000000,16.000000 ) ( 5.500000,20.250000 ) ( 6.000000,25.000000 ) ( 6.500000,30.250000 ) ( 7.000000,36.000000 ) ( 7.500000,42.250000 ) ( 8.000000,49.000000 ) ; coordinates ( -8.000000,144.000000 ) ( -7.500000,132.250000 ) ( -7.000000,121.000000 ) ( -6.500000,110.250000 ) ( -6.000000,100.000000 ) ( -5.500000,90.250000 ) ( -5.000000,81.000000 ) ( -4.500000,72.250000 ) ( -4.000000,64.000000 ) ( -3.500000,56.250000 ) ( -3.000000,49.000000 ) ( -2.500000,42.250000 ) ( -2.000000,36.000000 ) ( -1.500000,30.250000 ) ( -1.000000,25.000000 ) ( -0.500000,20.250000 ) ( 0.000000,16.000000 ) ( 0.500000,12.250000 ) ( 1.000000,9.000000 ) ( 1.500000,6.250000 ) ( 2.000000,4.000000 ) ( 2.500000,2.250000 ) ( 3.000000,1.000000 ) ( 3.500000,0.250000 ) ( 4.000000,0.000000 ) ( 4.500000,0.250000 ) ( 5.000000,1.000000 ) ( 5.500000,2.250000 ) ( 6.000000,4.000000 ) ( 6.500000,6.250000 ) ( 7.000000,9.000000 ) ( 7.500000,12.250000 ) ( 8.000000,16.000000 ) ;    coordinates ( -8.000000,-28.000000 ) ( -7.500000,-20.250000 ) ( -7.000000,-13.000000 ) ( -6.500000,-6.250000 ) ( -6.000000,0.000000 ) ( -5.500000,5.750000 ) ( -5.000000,11.000000 ) ( -4.500000,10.000000 ) ( -4.000000,9.000000 ) ( -3.500000,8.000000 ) ( -3.000000,7.000000 ) ( -2.500000,6.000000 ) ( -2.000000,5.000000 ) ( -1.500000,4.000000 ) ( -1.000000,3.000000 ) ( -0.500000,2.000000 ) ( 0.000000,1.000000 ) ( 0.500000,0.000000 ) ( 1.000000,-1.000000 ) ( 1.500000,-2.250000 ) ( 2.000000,-4.000000 ) ( 2.500000,-6.250000 ) ( 3.000000,-9.000000 ) ( 3.500000,-12.250000 ) ( 4.000000,-16.000000 ) ( 4.500000,-20.250000 ) ( 5.000000,-25.000000 ) ( 5.500000,-30.250000 ) ( 6.000000,-36.000000 ) ( 6.500000,-42.250000 ) ( 7.000000,-49.000000 ) ( 7.500000,-56.250000 ) ( 8.000000,-64.000000 ) ; coordinates ( -8.000000,-24.000000 ) ( -7.500000,-18.000000 ) ( -7.000000,-12.000000 ) ( -6.500000,-6.000000 ) ( -6.000000,0.000000 ) ( -5.500000,6.000000 ) ( -5.000000,12.000000 ) ( -4.500000,18.000000 ) ( -4.000000,24.000000 ) ( -3.500000,30.000000 ) ( -3.000000,36.000000 ) ( -2.500000,42.000000 ) ( -2.000000,48.000000 ) ( -1.500000,54.000000 ) ( -1.000000,60.000000 ) ( -0.500000,66.000000 ) ( 0.000000,72.000000 ) ( 0.500000,78.000000 ) ( 1.000000,84.000000 ) ( 1.500000,90.000000 ) ( 2.000000,96.000000 ) ( 2.500000,102.000000 ) ( 3.000000,108.000000 ) ( 3.500000,114.000000 ) ( 4.000000,120.000000 ) ( 4.500000,126.000000 ) ( 5.000000,132.000000 ) ( 5.500000,138.000000 ) ( 6.000000,144.000000 ) ( 6.500000,150.000000 ) ( 7.000000,156.000000 ) ( 7.500000,162.000000 ) ( 8.000000,168.000000 ) ; coordinates ( -8.000000,17.000000 ) ( -7.500000,16.000000 ) ( -7.000000,15.000000 ) ( -6.500000,14.000000 ) ( -6.000000,13.000000 ) ( -5.500000,12.000000 ) ( -5.000000,11.000000 ) ( -4.500000,10.000000 ) ( -4.000000,9.000000 ) ( -3.500000,8.000000 ) ( -3.000000,7.000000 ) ( -2.500000,6.000000 ) ( -2.000000,5.000000 ) ( -1.500000,4.000000 ) ( -1.000000,3.000000 ) ( -0.500000,2.000000 ) ( 0.000000,1.000000 ) ( 0.500000,0.000000 ) ( 1.000000,-1.000000 ) ( 1.500000,-2.000000 ) ( 2.000000,-3.000000 ) ( 2.500000,-4.000000 ) ( 3.000000,-5.000000 ) ( 3.500000,-6.000000 ) ( 4.000000,-7.000000 ) ( 4.500000,-8.000000 ) ( 5.000000,-9.000000 ) ( 5.500000,-10.000000 ) ( 6.000000,-11.000000 ) ( 6.500000,-12.000000 ) ( 7.000000,-13.000000 ) ( 7.500000,-14.000000 ) ( 8.000000,-15.000000 ) ; coordinates ( -8.000000,80.000000 ) ( -7.500000,76.000000 ) ( -7.000000,72.000000 ) ( -6.500000,68.000000 ) ( -6.000000,64.000000 ) ( -5.500000,60.000000 ) ( -5.000000,56.000000 ) ( -4.500000,52.000000 ) ( -4.000000,48.000000 ) ( -3.500000,44.000000 ) ( -3.000000,40.000000 ) ( -2.500000,36.000000 ) ( -2.000000,32.000000 ) ( -1.500000,28.000000 ) ( -1.000000,24.000000 ) ( -0.500000,20.000000 ) ( 0.000000,16.000000 ) ( 0.500000,12.000000 ) ( 1.000000,8.000000 ) ( 1.500000,4.000000 ) ( 2.000000,0.000000 ) ( 2.500000,-4.000000 ) ( 3.000000,-8.000000 ) ( 3.500000,-12.000000 ) ( 4.000000,-16.000000 ) ( 4.500000,-20.000000 ) ( 5.000000,-24.000000 ) ( 5.500000,-28.000000 ) ( 6.000000,-32.000000 ) ( 6.500000,-36.000000 ) ( 7.000000,-40.000000 ) ( 7.500000,-44.000000 ) ( 8.000000,-48.000000 ) ;    with these considerations in mind , we modify square loss in order to obtain a training formulation for binary classification that is both compatible with quantum hardware and robust to label noise .",
    "the resulting loss , which we name _ q - loss _",
    "[ fig:@xmath0-loss ] , top ) , is essentially a doubly truncated version of with parameterization over @xmath30 $ ] defined as follows :      unfortunately , does not lead to a qubo . however , it turns out that we can transform it into a problem which can be solved as a qubo .",
    "the basic idea is to find a variational approximation via a family of quadratic functions that upper - bound @xmath0-loss and are governed by a variational parameter @xmath32 as shown in fig .",
    "[ fig:@xmath0-loss ] , middle .",
    "since @xmath0-loss is non - convex , the standard derivation via convex duality @xcite dictates that we first find a new coordinate system in which @xmath0-loss is concave or convex .",
    "then we calculate the conjugate function for linear bounds in the transformed space and transform back to the original space where the linear bounds become the quadratic bounds shown in fig .",
    "[ fig:@xmath0-loss ] , middle .",
    "because of the presence of two constant segments in @xmath0-loss , any coordinate system in which the two axes are independent transformations of the original @xmath34 and @xmath35 axes clearly can not result in concavity or convexity .",
    "thereby we are led to the transformation @xmath36 , which gives @xmath37 .",
    "it can be seen ( fig .",
    "[ fig:@xmath0-loss ] , bottom ) that in this transformed space @xmath0-loss is concave and the quadratic upper bounds become tangent lines .",
    "the conjugate function in the transformed space is @xmath38 .    to minimize",
    ", we seek stationary points by differentiating @xmath39 with respect to @xmath40 : @xmath41 as yielded by piecewise differentiation of @xmath42 .",
    "setting to @xmath43 gives the stationary points @xmath44 plugging them back into the conjugate function yields @xmath45 in accordance with convex duality ,",
    "@xmath46 transforming back into the original space and setting @xmath47 , the variational upper bound for @xmath0-loss is @xmath48      traditionally when facing non - convex optimization problems , a viable approach is to introduce latent variables that allow reformulating over a simpler family of functions .",
    "this is precisely what theorem  [ q_loss_theorem ] achieves .",
    "for any fixed @xmath40 , the latent variable @xmath32 gives a convex optimization problem whose minimum is @xmath49 : @xmath50    the regularized risk minimization with empirical risk over @xmath51 in the form is amenable to a block coordinate descent method for jointly optimizing the model parameters @xmath52 and the latent variables @xmath53 for @xmath54 : similarly to em , alternate between convex optimization runs over the latent variables ( _ @xmath55 step _ ) and the model parameters ( _ @xmath56 step _ ) .",
    "even though such methods do well on some problems with certain benign structure  e.g .",
    "gaussian mixtures @xcite)they are also known to fail on other problems that lack such structure .",
    "we believe @xmath0-loss belongs to the latter group and have verified that a block coordinate descent method is likely to be sensitive to initialization and is quickly terminating in bad local minima .",
    "the intuitive reason is that due to the quadratically growing penalty for mismatching a margin with its latent variable , the @xmath55 step tends to lock in the model parameters found during the previous @xmath56 step , thus possibly preventing the next @xmath56 step from moving to a different model .",
    "the impact of this effect becomes ever more severe for large data with @xmath57 .",
    "on the other hand , by transforming into we have made training with @xmath0-loss representable in qubo form albeit at the expense of additional variables .",
    "section  [ explicit_qubo ] of the supplementary material explicitly shows the qubo problem that can be derived from .",
    "since the goal of aqo is to perform global optimization simultaneously over all variables , we believe aqo is a much better candidate for training with @xmath0-loss .",
    "besides making the qubo formulation possible , the introduction of latent variables also gives rise to an intuitive interpretation of the mechanism by which @xmath0-loss achieves robustness when compared to the non - robustness of square loss .",
    "while in the fixed target @xmath58 has to be matched as closely as possible by @xmath40 , in @xmath55 plays the role of a flexible target that can change sign for a large negative margin , thereby flagging that training example as mislabeled . for any @xmath40",
    ", the minimizer @xmath59 in belongs to one of three cases : +"
  ],
  "abstract_text": [
    "<S> we propose a non - convex training objective for robust binary classification of data sets in which label noise is present . </S>",
    "<S> the design is guided by the intention of solving the resulting problem by adiabatic quantum optimization . </S>",
    "<S> two requirements are imposed by the engineering constraints of existing quantum hardware : training problems are formulated as quadratic unconstrained binary optimization ; and model parameters are represented as binary expansions of low bit - depth . in the present work we validate this approach by using a heuristic classical solver as a stand - in for quantum hardware . </S>",
    "<S> testing on several popular data sets and comparing with a number of existing losses we find substantial advantages in robustness as measured by test error under increasing label noise . </S>",
    "<S> robustness is enabled by the non - convexity of our hardware - compatible loss function , which we name _ @xmath0-loss_. </S>"
  ]
}