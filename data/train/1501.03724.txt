{
  "article_text": [
    "shape matching is an important area of computational geometry , that has applications in computer vision , pattern recognition , and other fields that are concerned with matching objects by shape similarity .",
    "generally , in shape matching we are given two geometric objects @xmath8 and @xmath9 and we want to measure to what extent they are similar .",
    "usually we may allow certain transformations , like translations , rotations and/or scalings , of one object relative to the other , in order to improve the quality of the match .",
    "in many applications , the input data consists of finite sets of points sampled from the actual objects . to measure similarity between the sampled point sets , various distance functions",
    "have been used .",
    "one popular function is the hausdorff distance that equals to the maximum distance from a point in one set to its nearest point in the other set .",
    "however , when the objects which we compare are curves , sequences , or contours of larger objects , and the sampled points are ordered along the compared contours , the discrete frchet distance may be a more appropriate similarity measure .",
    "this is because the discrete frchet distance takes into account the ordering of the points along the contours which the hausdorff distance ignores .",
    "comparing curves and sequences is a major task that arises in computer vision , image processing and bioinformatics ( e.g. , in matching backbone sequences of proteins ) .",
    "the _ discrete frchet distance _ between a sequence of points @xmath3 and another sequence of points @xmath2 is defined as the minimum , over all possible independent ( forward ) traversals of the sequences , of the maximum distance between the current point of @xmath3 and the current point of @xmath2 during the traversals .",
    "see below and in section  [ sec : preliminaries ] for a more formal definition . in this work ,",
    "we focus on the problem of computing the minimum discrete frchet distance _ under translation_. that is , given two sequences @xmath3 and @xmath2 of @xmath10 and @xmath11 points , respectively , in the plane , we wish to translate @xmath2 by a vector @xmath12 such that the discrete frchet distance between @xmath3 and @xmath13 is minimized .    * background . *",
    "the frchet distance has been extensively studied during the past 20 years . the main variant , the continuous frchet distance , where no transformation is allowed , measures similarity between ( polygonal ) curves .",
    "it is the smallest @xmath14 for which there exist forward simultaneous traversals of the two curves , from start to end , so that at all times the distance between the corresponding points on the curves is at most @xmath14 .",
    "the discrete frchet distance considers sequences @xmath3 and @xmath2 of points instead of curves .",
    "it is defined analogously , where ( a ) the simultaneous traversals of the sequences are represented as a sequence of pairs @xmath15 , where @xmath16 , @xmath17 , for @xmath18 , ( b ) the first ( resp . , last ) pair consists of the starting ( resp",
    ". , terminal ) points of the two sequences , and ( c ) each @xmath19 is obtained from @xmath20 by moving one ( or both ) point(s ) to the next position in the corresponding sequence .",
    "most studies of the problem consider the situation where no translation ( or any other transformation ) is allowed . in this `` stationary '' case , the discrete frchet distance in the plane can be computed , using dynamic programming , in @xmath21 time ( eiter and mannila  @xcite ) .",
    "agarwal et al .",
    "@xcite slightly improve this bound , and show that the ( stationary ) discrete frchet distance can be computed in @xmath22 time on a word ram , and a very recent result of bringmann  @xcite indicates that a substantially subquadratic solution ( one that runs in time @xmath23 , for some @xmath24 ) is unlikely to exist .",
    "alt and godau  @xcite showed that the ( stationary ) continuous frchet distance of two planar polygonal curves with @xmath10 and @xmath11 edges , respectively , can be computed , using dynamic programming , in @xmath25 time .",
    "this has been slightly improved recently by buchin et al .",
    "@xcite , who showed that the continuous frchet distance can be computed in @xmath26 time on a pointer machine , and in @xmath27 time on a word ram ( here @xmath28 denotes the number of edges in each curve ) . in short , the best known algorithms for the stationary case , for both discrete and continuous variants , hover around the quadratic time bound .",
    "not surprisingly , the problems become much harder , and their solutions much less efficient , when translations ( or other transformations ) are allowed . for the problem of computing the minimum continuous frchet distance under translation , alt et al .  @xcite",
    "give an algorithm with @xmath29 running time , where @xmath10 and @xmath11 are the number of edges in the curves .",
    "they also give a @xmath30-approximation algorithm for the problem , that runs in @xmath31 time .",
    "that is , they compute a translation of one of the curves relative to the other , such that the frchet distance between the resulting curves is at most @xmath30 times the minimum frchet distance under any translation . in three dimensions ,",
    "wenk  @xcite showed that , given two polygonal chains with @xmath10 and @xmath11 edges respectively , the minimum continuous frchet distance between them , under any reasonable family of transformations , can be computed in @xmath32 time , where @xmath33 is the number of degrees of freedom for moving one chain relative to the other .",
    "so with translations alone @xmath34 , the minimum continuous frchet distance in @xmath35 can be computed in @xmath36 time , and when both translations and rotations are allowed @xmath37 , the corresponding minimum continuous frchet distance can be computed in @xmath38 time .",
    "the situation with the discrete frchet distance under translation is somewhat better , albeit still inefficient .",
    "jiang et al .",
    "@xcite show that , given two sequences of points in the plane , the minimum discrete frchet distance between them under translation can be computed in @xmath7 time . for the case where both rotations and translations are allowed , they give an algorithm that runs in @xmath39 time .",
    "they also design a heuristic method for aligning two sequences of points under translation and rotation in three dimensions .",
    "mosig et al .",
    "@xcite present an approximation algorithm that computes the discrete frchet distance under translation , rotation and scaling in the plane , up to a factor close to @xmath40 , and runs in @xmath41 time .",
    "* our results .",
    "* our algorithm improves the bound of jiang et al .",
    "@xcite by a nearly linear factor , with running time @xmath5 , assuming @xmath6 .",
    "it uses a @xmath42-matrix @xmath43 of size @xmath44 , whose rows ( resp . , columns )",
    "correspond to the points of @xmath3 ( resp .",
    ", of @xmath2 ) .",
    "assuming a stationary situation , or , rather , a fixed translation of @xmath2 , an entry in the matrix is equal to 1 if and only if the distance between the two corresponding points is at most @xmath14 , where @xmath14 is some fixed distance threshold .",
    "we use @xmath45 to denote an entry in the matrix that corresponds to the points @xmath46 and @xmath47 , and we use @xmath48 to denote its value .",
    "the discrete frchet distance is at most @xmath14 if and only if there is a row- and column - monotone path of ones in @xmath49 that starts at @xmath50 and ends at @xmath51 ( see section  [ sec : preliminaries ] for a more precise definition ) .",
    "we can partition the plane of translations into a subdivision @xmath52 with @xmath41 regions , so that , for all translations in the same region , the matrix @xmath49 is fixed ( for the fixed @xmath14 ) .",
    "we then traverse the regions of @xmath52 , moving at each step from one region to a neighboring one .",
    "assuming general position , in each step of our traversal exactly one entry of @xmath49 changes from @xmath53 to @xmath54 or vice versa .",
    "we present a dynamic data structure @xmath55 that supports an update of an entry of @xmath49 , in @xmath56 time , assuming @xmath6 , and @xmath11 by flipping @xmath49 . ]",
    "and then re - determines whether there is a monotone path of ones from @xmath50 to @xmath51 , in @xmath57 additional time .",
    "if we find such a monotone path in @xmath49 , we have found a translation @xmath58 ( actually a whole region of translations , the region can degenerate to a single vertex of @xmath52 ; see sections  [ sec : arrangement ] and  [ sec : optimization ] for details . ] ) such that the discrete frchet distance between @xmath3 and @xmath13 is at most @xmath14 .",
    "otherwise , when we traverse the entire @xmath52 and fail after each update , we conclude that no such translation exists . using this procedure ,",
    "combined with the parametric searching technique  @xcite , we obtain an algorithm for computing the minimum discrete frchet distance under translation .",
    "we reduce the dynamic maintenance of @xmath49 to dynamic maintenance of reachability in a planar graph , as edges are inserted and deleted to / from the graph .",
    "specifically , we can think of ( the 1-entries of ) @xmath49 as a representation of a planar directed graph with @xmath59 nodes .",
    "each 1-entry of @xmath49 corresponds to a node in the graph , and each possible forward move in a joint traversal is represented by an edge ( see section  [ sec : preliminaries ] for details ) .",
    "then , determining whether there is a row- and column - monotone path of ones from @xmath50 to @xmath51 corresponds to a reachability query in the graph ( from @xmath50 to @xmath51 ) .",
    "a data structure for dynamic maintenance of reachability in directed planar graphs was given by subramanian  @xcite .",
    "this data structure supports updates and reachability queries in @xmath60 time , where @xmath61 is the number of nodes in the graph .",
    "diks and sankowski  @xcite improved this data structure , and gave a structure that supports updates and reachability queries in @xmath62 time .",
    "we give a simpler and more efficient structure for maintaining reachability in @xmath49 that exploits its special structure .",
    "our structure can update reachability information in @xmath49 in @xmath56 time , assuming @xmath6 , and answers reachability query ( from @xmath50 to @xmath51 ) in @xmath57 time .",
    "in contrast , the data structure of @xcite applied in our context performs an update and a query in @xmath63 time . using our structure",
    ", we obtain an algorithm for computing the minimum discrete frchet distance under translation that runs in @xmath64 time ( again , assuming @xmath6 ) .    to summarize the contributions of this paper are twofold : ( a ) the reduction of the problem of computing the minimum discrete frchet distance to a dynamic planar directed graph reachability problem .",
    "( b ) an efficient data structure for this reachability problem . for @xmath65",
    "our structure is faster than the general reachability structure of @xcite by a polylogarithmic factor , and when @xmath66 the improvement is considerably more significant ( roughly by a factor @xmath67 ) .",
    "moreover , our data structure is simpler than that of diks and sankowski .",
    "we now define the ( stationary ) discrete frchet distance formally .",
    "let @xmath68 and @xmath69 be the two planar sequences defined in the introduction .    for some fixed distance @xmath24",
    "we define a @xmath42-matrix @xmath70 formally as follows .",
    "the rows ( resp . , columns ) of @xmath71 correspond to the points of @xmath3 ( resp .",
    ", of @xmath2 ) in their given order .",
    "an entry @xmath45 of @xmath71 is 1 if the distance between @xmath46 and @xmath47 is at most @xmath14 , and is 0 otherwise .",
    "we denote @xmath71 by @xmath49 when @xmath3 and @xmath2 and @xmath14 are clear from the context .",
    "the directed graph @xmath72 associated with @xmath3 , @xmath2 and @xmath14 has a vertex for each pair @xmath73 and an edge for each pair of adjacent ones in @xmath71 .",
    "specifically , we have an edge from @xmath74 to @xmath75 if and only if both @xmath45 and @xmath76 are 1 in @xmath49 , an edge from @xmath74 to @xmath77 if and only if both @xmath45 and @xmath78 are 1 in @xmath49 , and an edge from @xmath74 to @xmath79 if and only if both @xmath45 and @xmath80 are 1 in @xmath49 .",
    "we denote @xmath81 by @xmath82 when @xmath3 and @xmath2 and @xmath14 are clear from the context .",
    "the _ ( stationary ) discrete frchet distance _ between @xmath3 and @xmath2 , denoted by @xmath83 , is the smallest @xmath24 for which @xmath84 is reachable from @xmath85 in @xmath86 .",
    "informally , think of @xmath3 and @xmath2 as two sequences of stepping stones and of two frogs , the @xmath3-frog and the @xmath2-frog , where the @xmath3-frog has to visit all the @xmath3-stones in order and the @xmath2-frog has to visit all the @xmath2-stones in order .",
    "the frogs are connected by a rope of length @xmath14 , and are initially placed at @xmath87 and @xmath88 , respectively . at each move , either one of the frogs jumps from its current stone to the next one and the other stays at its current stone , or both of them jump simultaneously from their current stones to the next ones .",
    "furthermore , such a jump is allowed only if the distances between the two frogs before and after the jump are both at most @xmath14 .",
    "then @xmath83 is the smallest @xmath24 for which there exists a sequence of jumps that gets the frogs to @xmath89 and @xmath90 , respectively .",
    "the problem of computing the minimum discrete frchet distance under translation , as reviewed in the introduction , is to find a translation @xmath58 such that @xmath91 is minimized .",
    "we say that an entry @xmath45 of @xmath49 is _ reachable _ from an entry @xmath92 , with @xmath93 , if @xmath74 is reachable from @xmath94 in @xmath95 .",
    "a path from @xmath94 to @xmath74 in @xmath95 corresponds to a ( weakly ) row - monotone and column - monotone sequence of ones in @xmath49 connecting the one in entry @xmath92 to the one in entry @xmath45 .",
    "this is sequence consists of three kinds of moves : 1 ) _ upward moves _ between entries of the form @xmath96 to @xmath97 in which the @xmath3-frog moves from @xmath98 to @xmath99 , 2 ) _ right moves _ between entries of the form @xmath96 to @xmath100 in which the @xmath3-frog moves from @xmath101 to @xmath102 , and 3 ) _ diagonal moves _ between entries of the form @xmath96 to @xmath103 in which the @xmath3-frog moves from @xmath101 to @xmath102 both frogs move simultaneously  the @xmath3-frog from @xmath98 to @xmath99 , and the @xmath2-frog from @xmath101 to @xmath102",
    ". see figure  [ fig : staircase ] .",
    "we call such a monotone sequence of ones in @xmath49 a _ path in m _ from @xmath92 to @xmath45 . to determine whether @xmath104",
    ", we need to determine whether there is such a path in @xmath49 that starts at @xmath50 and ends at @xmath51 .",
    "we say that an entry @xmath45 of @xmath49 is _ reachable _ if there is a path from @xmath50 to @xmath45 .",
    "we denote the concatenation of two paths @xmath105 by @xmath106 , assuming that the last entry of @xmath107 is the first entry of @xmath108 ; this entry appears only once in the concatenation .",
    "[ cols=\"^,^ \" , ]     we store the reachability data of @xmath109 ( of some arbitrary face @xmath33 from which we start the traversal of the arrangement @xmath52 ) in a so - called _ decomposition tree _ @xmath110 , by halving @xmath3 and @xmath2 alternately . that is , the root @xmath111 of @xmath110 corresponds to the entire matrix @xmath109 and we store at @xmath111 the reachability information @xmath112 , as described in the previous section .",
    "( the actual construction of the reachability data , at all nodes of @xmath110 , is done bottom - up , as described below . ) in the next level of @xmath110 we partition @xmath3 into two subsequences @xmath113 , of at most @xmath114 points each , such that the last point of @xmath115 is the first point of @xmath116 , and obtain a corresponding `` horizontal '' partition of @xmath109 into two blocks @xmath117 , @xmath118 , each of size at most @xmath119 , with a common `` horizontal '' boundary .",
    "we create two children @xmath120 of @xmath111 and store at each @xmath121 the reachability information @xmath122 , for @xmath123 . in the next level of @xmath110 , we partition @xmath2 into two subsequences @xmath124 , of at most @xmath114 points each , such that the last point of @xmath125 is the first point of @xmath126 , and",
    "obtain a corresponding `` vertical '' partition of each block @xmath127 , into two blocks @xmath128 , each of size at most @xmath129 , with a common vertical boundary .",
    "we construct four respective grandchildren , and store the corresponding reachability structures @xmath130 at these nodes .",
    "we continue recursively to partition each block by halving it horizontally or vertically , alternately , in the same manner , until we reach blocks of size @xmath131 . for each node @xmath111 of @xmath110 , let @xmath132 and @xmath133 denote the subsequences of @xmath3 and @xmath2 that form the block @xmath134 that is associated with @xmath111 . to simplify the notation",
    ", we denote @xmath135 as @xmath136 , for each node @xmath111 .    the reachability data @xmath136 at the nodes @xmath111 of @xmath110",
    "is computed by a bottom - up traversal of @xmath110 , starting from the leaves .",
    "the construction of @xmath135 at a leaf @xmath111 is trivial , and takes constant time .",
    "the following lemma provides an efficient procedure for constructing the reachability data at inner nodes of @xmath110 .",
    "[ lem : union ] let @xmath137 be an inner node of @xmath110 with left and right children @xmath111 and @xmath138 , where the blocks stored at @xmath139 have a common horizontal boundary . given the reachability data @xmath140 , the data @xmath141 can be computed in @xmath142 time .",
    "an analogous statement holds when the common boundary of the children blocks is vertical .    note that in the setup of the lemma , we have @xmath143 and @xmath144 . by construction",
    ", @xmath145 lies below @xmath146 .",
    "denote @xmath145 by @xmath147 , @xmath146 by @xmath148 , and @xmath149 by @xmath150 .",
    "for each entry @xmath151 of @xmath152 , denote by @xmath153 ( resp .",
    ", @xmath154 ) the first ( resp . , last ) entry of @xmath155 that is reachable from @xmath151 .",
    "we also use @xmath156 to denote an entry of @xmath155 that is reachable from @xmath151 in @xmath150 .",
    "analogous notations are used for the children blocks @xmath157 .",
    "see figure  [ fig : united_blocks ] .",
    "we first copy the reachability information from the boundaries of @xmath147 and @xmath148 to the boundary of @xmath150 ( except for the `` interior '' portion @xmath158 of the common boundary @xmath159 of @xmath147 and @xmath148 , which is not a boundary of @xmath150 ) .",
    "the data for the @xmath53-entries on the left boundary of @xmath148 ( which are of type [ enum : b^- ] in the definition of @xmath160 ) is still valid , since the reachability paths of @xmath150 that start at these entries are fully contained in @xmath148 .",
    "similarly , the data for the @xmath53-entries on the right boundary of @xmath147 ( which are of type [ enum : b^+ ] ) is still valid , since the reachability paths of @xmath150 that end at these entries are fully contained in @xmath147 .",
    "we thus need to determine the reachability information from the @xmath53-entries of the input boundary @xmath161 of @xmath147 to the entries of the output boundary @xmath162 of @xmath148 , and merge it with the already available data , to get the complete structure @xmath160 at @xmath137 .",
    "first note that an entry @xmath163 of @xmath162 that is reachable from @xmath164 may now become unreachable from @xmath152 .",
    "this happens if all the reachability paths in @xmath150 to @xmath163 go through entries on @xmath158 that are not reachable from @xmath161 .",
    "see figure  [ fig : united_blocks](a ) .",
    "we thus need to turn the flag @xmath165 of such entries to false . to do this",
    ", we go over the entries of @xmath162 in order , and maintain a queue @xmath166 that satisfies the invariant that , when we are at an entry @xmath163 of @xmath162 , @xmath166 contains all the entries @xmath151 of @xmath164 that are reachable from @xmath152 , such that @xmath163 is reachable from @xmath151 .",
    "that is , @xmath166 contains all the entries @xmath167 that are reachable from @xmath161 such that @xmath163 is reachable from @xmath151 , and all the entries @xmath168 ( that is , the left side of @xmath164 ) such that @xmath163 is reachable from @xmath151 .",
    "we start with an empty queue .",
    "for each @xmath53-entry @xmath163 of @xmath162 we first go over the list @xmath169 ( of @xmath170 ) , and for each element @xmath151 in @xmath169 that is in @xmath158 , we check if it is reachable from @xmath161 ( using the flag @xmath171 from @xmath136 ) . if it is , we put it in @xmath166 .",
    "we also add to @xmath166 each element in @xmath169 that is in @xmath172 .",
    "if @xmath166 is empty , there is no reachability path from @xmath152 to @xmath163 and we set @xmath165 to be false .",
    "we then go over the list @xmath173 ( of @xmath170 ) and remove from @xmath166 each element in @xmath173 that is in @xmath166 .",
    "this traversal takes @xmath142 time , since each element of @xmath164 appears at most once in the lists @xmath174 and at most once in the lists @xmath175 .",
    "the correctness follows from the invariant that when we go over an entry @xmath176 , all the entries of @xmath164 that @xmath163 is reachable from , and that are reachable from @xmath152 , are in @xmath166 .",
    "the invariant is maintained correctly because each time that an interval @xmath177 $ ] of an entry @xmath178 begins ( and @xmath151 is reachable from @xmath152 ) , @xmath151 is inserted into @xmath166 , and when the interval ends , @xmath151 is removed from @xmath166 , so @xmath151 is in @xmath166 for all entries @xmath163 that are reachable from @xmath151 . in conclusion , if @xmath166 is empty , @xmath163 is not reachable from @xmath152 and the flag @xmath165 can be turned false .",
    "otherwise , @xmath151 is reachable from @xmath152 .",
    "we now update the intervals @xmath177 $ ] of the entries @xmath179 and , in correspondence , the lists @xmath180 of @xmath162 ( where @xmath181 is any entry of @xmath162 that is reachable from @xmath151 ) . consider a @xmath53-entry @xmath151 of @xmath161 and consider an entry @xmath182 in @xmath158 ; that is , @xmath182 is a 1-entry in @xmath183 $ ] that is reachable from @xmath151 . by transitivity ,",
    "the entries @xmath184 of @xmath162 that are reachable from @xmath182 are also reachable from @xmath151 .",
    "we update @xmath177 $ ] according to this rule , as follows ( see figure  [ fig : united_blocks](b ) ) .",
    "we set @xmath185 , for each entry @xmath179 such that @xmath186 ; correspondingly , we also add @xmath151 to @xmath187 .",
    "similarly , for each entry @xmath179 such that @xmath188 , we set @xmath189 and we add @xmath151 to @xmath190 .",
    "( recall that if @xmath191 ( or @xmath192 ) is in @xmath193 , this reachability information was already copied to @xmath141 and that the reachability information for @xmath172 was also copied to @xmath141 . ) clearly , for each entry @xmath194 , no entry of @xmath195 $ ] is reachable from @xmath151 .",
    "this traversal takes @xmath142 time .",
    "finally , when we copied information from @xmath162 to @xmath155 , we also copied the lists @xmath174 and @xmath175 that may include entries of @xmath158 .",
    "since @xmath158 is not a part of the boundary of @xmath150 , we need to remove this information from the lists @xmath174 and @xmath175 of @xmath155 .",
    "we thus go over the entries of @xmath158 . for each entry @xmath196 of @xmath158 , we remove @xmath196 from @xmath197 and from @xmath198",
    ". clearly , this traversal takes @xmath199 time .",
    "we now show how to use lemma  [ lem : union ] to construct @xmath110 in @xmath200 time and to update it , when a single entry changes , in @xmath201 time .",
    "we also show how to determine , using @xmath110 , whether @xmath202 is reachable from @xmath50 in constant time after the update .",
    "[ lem : constructgamma ] ( a ) given a square matrix @xmath49 , the decomposition tree @xmath110 ( including the reachability data at its nodes ) can be constructed from scratch in @xmath200 time .",
    "( b ) if a single entry of @xmath49 is updated , then @xmath110 can be updated in @xmath201 time .",
    "( c ) given @xmath110 , we can determine whether @xmath202 is reachable from @xmath50 in constant time .",
    "\\(a ) we construct @xmath110 in a bottom - up manner , as prescribed in lemma  [ lem : union ] . for the blocks at the leaves ,",
    "the reachability data is computed in brute force , in @xmath57 time per block , and at each inner node @xmath137 , the data is computed from the data at its children in time @xmath142 , using lemma  [ lem : union ] ; we refer to @xmath203 as the _ size _ of the block @xmath150 at @xmath137 .",
    "the sizes of the blocks at levels @xmath204 and @xmath205 is @xmath206 , and the number of these blocks is @xmath207 .",
    "the height of @xmath110 is @xmath208 .",
    "the cost of the overall construction of @xmath110 is proportional to the sum of the sizes of its blocks ( this also holds at the leaf level ) , which is thus @xmath209    \\(b ) the main observation here is that to update @xmath110 when an entry @xmath196 of @xmath49 changes , it suffices to update the reachability data along the single path of @xmath110 of those nodes @xmath137 for which @xmath210 .",
    "( actually , because of the overlap between block boundaries , there are two such paths that meet at the unique node @xmath137 for which @xmath196 belongs to the `` interior '' of the common boundary of the blocks of its children . )",
    "the reachability data of the nodes along this path is constructed again from scratch in a bottom - up manner , using lemma  [ lem : union ] .",
    "the cost of the updates of these blocks is proportional to the sum of their sizes , which is @xmath211    \\(c ) to determine whether @xmath202 is reachable from @xmath50 , we simply check in the reachability data structure @xmath212 of the root of @xmath110 whether @xmath202 is a @xmath53-entry that belongs to @xmath213 $ ] and the flag @xmath214 is true .",
    "we next describe a modified variant of the structure for the case where @xmath10 and @xmath11 are unequal . in what follows we assume , as above and without loss of generality , that @xmath215 .",
    "we first partition @xmath49 into @xmath216 square blocks @xmath217 , of size @xmath218 each such that consecutive blocks overlap in a single column .",
    "( the last block may be of smaller width , but we handle it in the same manner as the other blocks ; it is easy to show that the bounds of lemma  [ lem : constructgamma ] still hold . )",
    "we build the decomposition tree and the associated reachability data for each of these blocks , as in section  [ sec : data ] ; denote the structure for block @xmath219 by @xmath220 , for @xmath221 .",
    "we now combine the structures @xmath222 into a single global structure @xmath223 . for this",
    ", we construct a balanced binary tree @xmath224 , with @xmath225 leaves @xmath226 , where @xmath121 , for @xmath221 , corresponds to @xmath219 and stores @xmath220 .",
    "each node @xmath111 of @xmath224 represents a block @xmath147 that is the concatenation of the blocks stored at the leaves of the subtree rooted at @xmath111 .",
    "since each leaf block spans all the rows of @xmath49 , the common boundary of any pair of consecutive blocks consists only of a full single column of @xmath49 .",
    "the same holds at any node @xmath137 of @xmath224 , with left child @xmath111 and right - child @xmath138 .",
    "that is , the common boundary @xmath227 between @xmath147 and @xmath148 is vertical , and consists of a full single column of @xmath49 .",
    "we claim that we can merge the reachability structures @xmath136 of @xmath147 and @xmath170 of @xmath148 into the structure @xmath141 of @xmath150 in @xmath201 time , instead of @xmath228 time ( as was the cost in the preceding subsection ) , which can be much larger .",
    "the main observation that facilitates this improvement is that there is no need to maintain the reachability data @xmath136 at the horizontal portions of the boundary of any of the blocks @xmath147 .",
    "this follows from the obvious property that any path @xmath229 from the initial entry @xmath50 to any entry @xmath45 in any leaf block reaches @xmath45 by crossing all the vertical boundaries @xmath230 that delimit all the preceding leaf blocks , and the portion @xmath231 of @xmath229 within each of the preceding blocks @xmath232 connects an entry on the left vertical boundary of @xmath232 to an entry on its right vertical boundary .",
    "note that @xmath231 can `` crawl '' along the lower or upper boundary of @xmath232 , but to exit @xmath232 it has to cross the vertical boundary , possibly through its entries in row @xmath53 or row @xmath11 .",
    "figure  [ fig : reachable ] is an illustration of an inner block @xmath150 of @xmath223 that is composed of a left block @xmath147 and a right block @xmath148 .    , corresponding to a node @xmath137 of @xmath223 , is composed of the blocks @xmath233 and @xmath234 of the children @xmath139 of @xmath137 , with @xmath111 being the left child and @xmath138 being the right child .",
    "we have @xmath235 and @xmath236 . ]",
    "we therefore use the same reachability data structure @xmath136 at @xmath111 as defined in the previous subsection , except that we limit the input and output domains of its maps to the vertical boundaries only .",
    "recall our notation from section  [ sec : preliminaries ] , where the left ( resp .",
    ", right ) vertical boundary of a block @xmath9 is denoted as @xmath237 ( resp . , @xmath238 ) .",
    "specifically , denoting the modified structure as @xmath239 , it stores the following items .    1 .   for each @xmath53-entry @xmath151 of @xmath240",
    "we store 1 .",
    "the first entry @xmath241 of @xmath242 that is reachable from @xmath151 , and 2 .",
    "the last entry @xmath243 of @xmath242 that is reachable from @xmath151 .",
    "2 .   for each @xmath53-entry @xmath163 of @xmath242",
    "we store 1 .   a flag @xmath244 indicating whether @xmath163 is reachable from some entry of @xmath240 .",
    "2 .   a list @xmath245 of the @xmath53-entries @xmath246 such that @xmath247 , and 3 .   a list @xmath248 of the @xmath53-entries @xmath246 such that @xmath249 .    in other words , @xmath250 is a constrained variant of @xmath251 , obtained by replacing @xmath161 and @xmath252 by @xmath240 and @xmath242 , respectively .",
    "the structure @xmath253 of the root @xmath111 of a child @xmath220 of @xmath223 is obtained from @xmath136 by first setting , for each entry @xmath151 of @xmath240 for which @xmath254 is in @xmath242 and @xmath255 is not in @xmath242 , @xmath255 to be the last reachable entry @xmath225 of @xmath242 , then updating @xmath256 accordingly , and finally ignoring the horizontal parts of the boundaries of @xmath147 and deleting the data regarding them from the lists @xmath174 and @xmath175 of entries of @xmath242 .",
    "we next claim that the modified structures @xmath257 are sufficient for obtaining reachability data for the blocks of @xmath223 , in the precise sense stated below , and that the structure @xmath257 at an inner node @xmath137 of @xmath223 can be obtained from the structures at the children of @xmath137 in @xmath201 time .",
    "concretely , we have the following variants of lemmas  [ lem : linear ] and  [ lem : union ] .    [",
    "lem : linear2 ] given the data structure @xmath258 for a block @xmath9 of size @xmath259 .",
    ", and given the entries of @xmath237 that are reachable from @xmath50 , we can determine , in @xmath260 time , the entries of @xmath238 that are reachable from @xmath50 .",
    "[ cor : gamma+2 ] let @xmath137 be an inner node of @xmath223 with left and right children @xmath111 and @xmath138 . given the reachability data @xmath261 , the data @xmath262 can be computed in @xmath201 time .",
    "the proof is essentially identical to those of lemma  [ lem : linear ] and lemma  [ lem : union ] , except that we restrict the domains and the images of each of the maps ( i.e. , @xmath263 , and @xmath175 ) to the vertical portions of the boundaries .",
    "this is justified using the observation made earlier that all the reachability paths traverse only vertical boundaries of the relevant blocks  those that are stored at @xmath223 , from its leaves up , which span the entire range @xmath3 of rows of @xmath49 .",
    "since we only traverse vertical boundaries , the cost of constructing @xmath257 from @xmath253 and @xmath264 is @xmath201 .",
    "@xmath265    the following lemma extends lemma [ lem : constructgamma ] .",
    "[ lem : constructgamma2 ] ( a ) given the matrix @xmath49 , @xmath223 can be constructed in @xmath21 time .",
    "( b ) if a single entry of @xmath49 is updated , then @xmath223 can be updated in @xmath56 time , assuming @xmath6 .",
    "( c ) given @xmath223 , we can determine whether @xmath51 is reachable from @xmath50 in constant time .",
    "\\(a ) we construct the structure @xmath220 for each block @xmath219 , and extract @xmath266 from it .",
    "we then construct @xmath267 for each inner node @xmath268 by merging the corresponding data structures of the children of @xmath137 in @xmath201 time .",
    "we obtain @xmath223 at the root of @xmath224 .",
    "since @xmath224 is of size @xmath269 and we spend @xmath201 time at each block , it takes @xmath270 time to construct @xmath223 from the leaf structures @xmath220 for @xmath221 .",
    "the cost of constructing each @xmath220 , @xmath271 , is @xmath200 , by lemma  [ lem : constructgamma ] , for a total of @xmath272 .",
    "it follows that that the overall construction of @xmath223 takes @xmath273 time .",
    "\\(b ) to update @xmath223 when an entry @xmath196 of @xmath49 changes we first need to update the reachability structures along a path in the structure of the block @xmath219 containing @xmath196 ( if @xmath196 is in the common column of two blocks we update both structures . ) .",
    "this takes @xmath201 time by lemma [ lem : constructgamma ] .",
    "once we have the updated @xmath220 we update the reachability structures along the path @xmath229 of @xmath224 of those nodes @xmath137 for which @xmath210 .",
    "( there are two such paths if @xmath196 is in the common column of two consecutive blocks . )",
    "since the depth of @xmath224 is @xmath274 and we spend @xmath201 time to reconstruct the structure at each node of @xmath224 , we update @xmath224 in @xmath56 time .",
    "\\(c ) as in lemma  [ lem : constructgamma ] , to determine whether @xmath51 is reachable from @xmath50 , we simply check in the reachability data structure @xmath275 of the root of @xmath223 whether @xmath276 $ ] and the flag @xmath277 is true .",
    "we now put together the pieces of the decision procedure .",
    "we construct the arrangement @xmath52 of the disks @xmath278 as in section  [ sec : arrangement ] in @xmath279 time .",
    "we pick an arbitrary ( @xmath54- , @xmath53- , or @xmath40- dimensional ) face @xmath280 of @xmath52 .",
    "@xmath280 corresponds to a unique matrix @xmath281 and we construct the data structure @xmath223 of section  [ sec : improved ] based on @xmath281 .",
    "we then perform a traversal of the entire arrangement @xmath52 . in each step of the traversal we move from a face @xmath33 of @xmath52 to a neighbor face @xmath282 ( both faces are of any dimension @xmath54 , @xmath53 , or @xmath40 ) . in this step ,",
    "we either enter a single disk of @xmath52 or exit a single disk of @xmath52 .",
    "this corresponds to a change in a single entry of @xmath109 .",
    "we update @xmath223 accordingly , in time @xmath56 , and thereby determine whether @xmath283 .",
    "we continue in this manner till we process the entire arrangement .",
    "if we encounter a face @xmath33 along the traversal at which @xmath284 we report that the minimum distance under translation is @xmath285 , and otherwise we report that the minimum distance is @xmath286 .",
    "we thus obtain the following intermediate result .",
    "[ th : decision ] let @xmath3 , @xmath2 be two sequences of points in @xmath287 of sizes @xmath10 and @xmath11 , respectively and let @xmath288 be a parameter .",
    "then the decision problem , where we want to determine whether there exists a translation @xmath12 such that @xmath289 , can be solved in @xmath290 time , assuming that @xmath215 .",
    "we now show how to use the decision procedure of section  [ sec : dynamic ] to compute the minimum discrete frchet distance under translation .",
    "assume without loss of generality that @xmath6 .",
    "as we increase @xmath14 , the disks @xmath278 expand , and their arrangement @xmath52 varies accordingly . nevertheless , except for a discrete set of critical values of @xmath14 , the combinatorial structure of @xmath52 does not change . that is , the pairs of intersecting disk boundaries remain the same , all their intersection points remain distinct and vary continuously , and no pair of disks are tangent to each other .",
    "consequently , the representation of @xmath52 that we use , namely , a collection of circular sequences of vertices , each containing the vertices of @xmath52 along some circle @xmath291 , for @xmath292 , @xmath293 , sorted along the circle , remain unchanged .",
    "the critical values of @xmath14 , at which this representation of @xmath52 changes qualitatively , are      there are @xmath296 critical values ( most of which are of type [ enum : critical1 ] ) , so we can not afford to enumerate them and run an explicit binary search to locate the optimal value of @xmath14 among them .",
    "instead , we use the parametric searching technique of  @xcite . in general , using parametric searching can be fairly complicated , since it is based on a simulation of a parallel version of the algorithm",
    ". however , we only have to simulate , by a parallel algorithm , the part of the decision procedure that depends on ( the unknown value of ) the optimum @xmath297 . in our case , this portion is the construction of @xmath52 .    instead of actually constructing @xmath52 , we first observe that it suffices to restrict our attention to vertices of @xmath52 , in the sense that each face @xmath33 of @xmath52 has a vertex @xmath298 , such that all the @xmath53-entries of @xmath109 are also @xmath53-entries of @xmath299 ( the latter matrix can contain additional @xmath53-entries ) , so it suffices to test for reachability in the matrices @xmath299 associated with vertices @xmath298 of @xmath52 .",
    "( technically , we add to the set of vertices one additional point , say the rightmost point , on each disk boundary , to cater to faces that have no real vertices . )    hence , our parallel implementation of the algorithm will only simulate the construction of the sorted lists of vertices along each of the circles @xmath291 . recall that during the parametric searching simulation , we collect comparisons that the decision procedure performs and that depend on @xmath14 , and resolve them .",
    "this is done by finding the critical values of @xmath14 at which the outcome of some comparison changes , during a single ( simulated ) parallel step of the algorithm and then by running a binary search through these critical values of @xmath14 , guided by the decision procedure of theorem  [ th : decision ] . in this manner , we maintain a shrinking half - open interval @xmath300 $ ] of values of @xmath14 that contains @xmath301 .",
    "note that we have called the decision procedure at @xmath302 and it has determined that @xmath303 .",
    "then , as is easily seen , @xmath301 must be at least as large as the first critical value of @xmath14 within @xmath304 ( and it can not be arbitrarily close to @xmath302 ) .",
    "assume that we have simulated the construction of @xmath52 , and obtained a half - open interval range @xmath300 $ ] of @xmath14 that contains @xmath301 .",
    "that is , we know that @xmath305 , and we know the sorted sequences of vertices of @xmath306 along each circle @xmath307 .",
    "none of the comparisons that the decision procedure has performed has a critical value inside @xmath304 , other than those comparisons that have produced ( @xmath302 and ) @xmath308 .",
    "hence the output representation of @xmath52 is fixed in the interior of @xmath304 .",
    "the rest of the algorithm , which constructs the structure @xmath223 , traverses the vertex sequences along the circles @xmath307 , and dynamically updates the reachability data , is purely combinatorial , and does not introduce new critical values ( i.e. , does not involve comparisons that depend on @xmath301 ) , so there is no need to run it at all .",
    "since the decision procedure fails at @xmath302 and succeeds at @xmath308 , it follows that @xmath309 .      1 .",
    "[ enum : intersection ] finds the intersection points of each circle @xmath310 with the circles @xmath311 , other than itself , and 2 .",
    "[ enum : sorting ] sorts , for each circle @xmath307 , the intersection points that were found on its boundary in step [ enum : intersection ] , along this boundary .",
    "we first obtain all the @xmath41 critical values of type [ enum : critical2 ] , sort them , and run an explicit binary search among them guided by the decision procedure .",
    "( this part requires no parametric simulation . )",
    "as a result @xmath304 is shrunk to an interval @xmath313 $ ] , where @xmath314 are two consecutive critical values of type [ enum : critical2 ] .",
    "this takes @xmath315 time .",
    "we can now accomplish step [ enum : intersection ] , because the property that a pair of circles @xmath316 intersect either holds for all @xmath317 or does not hold for any such @xmath14 .",
    "we then execute step [ enum : sorting ] .",
    "the task at hand is to sort , for each circle @xmath318 , the resulting fixed set of intersection points along @xmath318 . for each pair @xmath319 of such circles , the order of the intersection points can change only at the radius @xmath320 of the circumcircle of @xmath321 .",
    "we then simulate a parallel sorting procedure , to sort these intersection points along @xmath318 , and run it in parallel over all these circles .",
    "we omit the ( by now ) routine details of this simulation ( see , e.g. ,  @xcite for similar application of parametric searching ) .",
    "they imply that we can simulate this sorting , for each circle @xmath307 , using @xmath21 processors and @xmath322 parallel steps ( for a total of @xmath41 processors ) .",
    "thus , for each parallel step , we need to resolve @xmath41 comparisons , each of which compares @xmath301 to a critical circumradius of type [ enum : critical1 ] .",
    "we run a binary search among these critical values using the decision procedure .",
    "this takes @xmath5 time for each parallel step , for an overall @xmath323 time for @xmath324 steps . to ( slightly )",
    "improve this running time we use the improvement of cole  @xcite which finds , for each parallel step , the ( weighted ) median of the ( suitably weighted ) unresolved critical values involved in this step , and calls the decision procedure only at this value , instead of using a complete binary search .",
    "this allows us to resolve comparisons that contribute at least some fixed fraction of the total weight , while the other unresolved critical values are carried over to the next step with their weights increased . proceeding in this manner",
    ", we make only one call to the decision procedure at each parallel step , and add only @xmath325 parallel steps to the whole procedure .",
    "we thus obtain an overall algorithm with @xmath5 running time .",
    "[ th : optimization ] let @xmath3 , @xmath2 be two sequences of points in @xmath287 of respective sizes @xmath10 and @xmath11 , where @xmath6 .",
    "then the minimum discrete frchet distance under translation between @xmath3 and @xmath2 can be computed in @xmath5 time .",
    "our algorithm is composed of two main parts .",
    "the first part is the construction of the subdivision @xmath52 whose complexity is @xmath326 .",
    "the challenge here is either to argue that , in favorable situations , the actual complexity of @xmath52 is @xmath327 , or be able to process only a portion of @xmath52 that has @xmath327 complexity . here is a simple illustration of such an approach .",
    "consider the case where @xmath3 and @xmath2 are sampled along a pair of _ @xmath328-packed curves _",
    ", where a curve @xmath329 is @xmath328-packed if , for every disk @xmath330 , the length of @xmath331 is at most @xmath328 times the radius of @xmath330 .",
    "assume also that the sampling is more or less uniform , so that the distance between any pair of consecutive points of @xmath3 or of @xmath2 is roughly some fixed value @xmath332 .",
    "we may assume , without loss of generality that @xmath333 .",
    "consider the decision procedure with a given parameter @xmath14 , and observe that if @xmath58 is any translation for which @xmath334 then @xmath335 .",
    "therefore , for each @xmath292 , the only points @xmath293 that can align with @xmath46 during a simultaneous traversal of @xmath3 and @xmath13 , for any such `` good '' translation @xmath58 , are those at distance @xmath336 from @xmath46 . the assumptions on @xmath3 and @xmath2 imply that the number of such points is at most roughly @xmath337 .",
    "that is , instead of constructing the entire arrangement @xmath52 , it suffices to construct a coarser arrangement , involving only roughly @xmath338 disks .",
    "then , traversing the coarser arrangement is done as before , where each update step ( and the following reachability query ) cost @xmath339 time , assuming that @xmath6 .",
    "this improves the running time of the decision procedure to @xmath340 , assuming that @xmath6 and @xmath341 .",
    "given this decision procedure , we can solve the optimization problem using parametric searching .",
    "however , to ensure that the decision procedure does not become too expensive , we want to run it only with values @xmath342 .",
    "this will become significant only when @xmath343 ; otherwise the running time will be close to the running time of the algorithm of section  [ sec : optimization ] .",
    "therefore , in the following we describe how to solve the optimization problem assuming that @xmath344 ( if the following procedure fails , we run the algorithm of section  [ sec : optimization ] ) .",
    "we also assume , for now , that @xmath345 ( we explain below how the case where @xmath346 is dealt with ) .",
    "we consider the interval @xmath347 that is assumed to contain @xmath301 , and run an `` exponential search '' through it , calling the decision procedure with the values @xmath348 , for @xmath349 , in order , until the first time we reach a value @xmath350 ( and @xmath351 ) .",
    "note that the cost of running the decision procedure at @xmath352 and at @xmath301 differ by at most a factor of @xmath353 , so the cost of running the decision procedure at @xmath352 is asymptotically the same as at @xmath301 .",
    "moreover , since the running time bounds on the executions of the decision procedure at @xmath354 form a geometric sequence , the overall cost of the exponential search is also asymptotically the same as the cost of running the decision procedure at @xmath301 .",
    "we then run the parametric searching technique as above , with the constraint that @xmath301 is at most @xmath352 ( i.e. , we set @xmath352 as the minimal @xmath308 obtained so far ) .",
    "hence , from now on , each call to the decision procedure made by the parametric searching , will cost no more than the cost of calling the decision procedure with @xmath352 ( which is asymptotically the same as calling the procedure with @xmath301 ) .",
    "we thus obtain an overall algorithm with @xmath355 running time .",
    "note that , in the case where @xmath346 , after running the decision procedure with @xmath356 , we realize that @xmath357 , and run the parametric searching technique with the constraint that @xmath301 is at most @xmath332 . in this case , the running time of the algorithm is @xmath358 .",
    "the second part of the algorithm presented in this work is the dynamic data structure for maintaining reachability in @xmath49 .",
    "it is an open question of independent interest whether this data structure can be improved .",
    "a related problem is whether the techniques used in our structure can be extended to the general case of reachability in planar directed graphs , so as to simplify and improve the efficiency of the earlier competing method of diks and sankowski  @xcite .",
    "k.  bringmann , why walking the dog takes time : frchet distance has no strongly subquadratic algorithms unless seth fails , _ proc .",
    "55th annu .",
    "ieee sympos . found .",
    "( 2014 ) , and in arxiv:1404.1448 ( 2014 ) ."
  ],
  "abstract_text": [
    "<S> the discrete frchet distance is a useful similarity measure for comparing two sequences of points @xmath0 and @xmath1 . in many applications , </S>",
    "<S> the quality of the matching can be improved if we let @xmath2 undergo some transformation relative to @xmath3 . in this paper </S>",
    "<S> we consider the problem of finding a translation of @xmath2 that brings the discrete frchet distance between @xmath3 and @xmath2 to a minimum . </S>",
    "<S> we devise an algorithm that computes the minimum discrete frchet distance under translation in @xmath4 , and runs in @xmath5 time , assuming @xmath6 . </S>",
    "<S> this improves a previous algorithm of jiang et al .  </S>",
    "<S> @xcite , which runs in @xmath7 time . </S>"
  ]
}