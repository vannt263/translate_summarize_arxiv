{
  "article_text": [
    "markov jump processes ( mjp ) provides us with a formal description of the underlying stochastic behaviour of many physical systems and as such they have a wide applicability in many scientific fields . in chemistry and biology , for example",
    ", they are applied for modelling reactions between chemical species @xcite . in ecology and epidemiology ,",
    "they are used for modelling the population of interacting species in the environment @xcite while in telecommunications they describe the population of information packets over a network @xcite . in order to introduce some terminology and notation we will give a more concrete example from chemical kinetics .",
    "however , the modelling methodology is similar in other applications although different assumptions are needed , depending on the system being modelled , for calculating reaction rates . consider a model for the population of molecules of two interacting chemical species , @xmath0 and @xmath1 , in a solution of volume @xmath2 , where @xmath0 and @xmath1 denote the number of molecules of chemicals @xmath3 and",
    "@xmath4 respectively . the interactions between the species are modelled using",
    "_ reactions _ which are specified using the following notation : @xmath5 . on the left hand side appear the _",
    "reactants _ and on the right hand side the _ products _ of the reaction while over the arrow appears the _ rate constant _ @xmath6 which is the probability that a randomly chosen pair of @xmath3 and @xmath4 will react according to @xmath7 .",
    "this reaction , for example , specifies that a pair of molecules @xmath3 , @xmath4 react with probability @xmath6 to produce a new molecule of @xmath3 . for calculating the probability of a reaction taking place",
    "given the current state of the system , _",
    "i.e. _ the number of molecules of chemicals @xmath3 and @xmath4 , several system dependent assumptions must be made . for chemical reactions",
    "it is assumed that in a well stirred solution the probability of a reaction is proportional to the populations of its products @xcite . for @xmath7 we can write it as @xmath8 .",
    "following the same reasoning additional reactions and species can be added in order to construct large and complex reaction networks .",
    "together , the state of the system @xmath9 , the set of reactions and the reaction rates specify a markov jump process where the occurrences of reactions are modelled as a poisson process .",
    "for this particular example the probability of the reaction has a simple form and is linear with respect to the populations .",
    "however in many real applications this is often not the case while the _ rate constants _ , @xmath6 , are unknown . given a fully specified mjp , _",
    "i.e. _ a mjp with known parameters , rate constants and initial conditions , it is possible to perform exact simulation and obtain samples from the underlying stochastic process using the stochastic simulation algorithm ( ssa ) of @xcite . in many problems",
    "there are system parameters which are not specified or are unknown while it is relatively easy to collect partial observations of the physical process at discrete time points .",
    "the interest is therefore to obtain statistical estimates of the unknown parameters using the available data .    as a consequence of the markov property , mjps",
    "satisfy the chapman - kolmogorov equation from which we can directly obtain the forward master equation describing the evolution of the system s state probability over any time interval .",
    "however , even for small and simple systems the master equation is intractable and it is not straightforward as to how partially and discretely observed data from the physical process should be incorporated in order to perform inference over unknown system parameters .",
    "recently , @xcite have shown that it is possible to construct a markov chain whose stationary probability distribution is the posterior of the unknown parameters without resorting to any approximations of the original mjp .",
    "their method however is computationally expensive while the strong correlation between posterior samples means that a large number of mcmc iterations are required in order to obtain monte carlo estimates with sufficient accuracy .",
    "an alternative is to consider suitable approximations of the likelihood function .",
    "the system size expansion of ( * ? ? ? * chap .",
    "10 ) provides a systematic method for obtaining approximations of a physical process approaching its thermodynamic limit .",
    "the most simple approximation yields the macroscopic rate equation ( mre ) which describes the thermodynamic limit of the system with a set of ordinary differential equations neglecting any random fluctuations .",
    "although the mre has been extensively studied in the literature , see for example , @xcite , it is not applicable for problems where information about the noise and the random fluctuations is necessary or the system is far from its thermodynamic limit .",
    "the diffusion approximation @xcite describes the physical process by a set of non - linear stochastic differential equations with state dependent brownian motion .",
    "similar to the master equation however , the likelihood is intractable . in",
    "@xcite a transformation is applied such that the brownian increments are independent of the system state and thus the system can be easily simulated .",
    "however this limits the applicability of the methodology into systems where such a transformation is possible . a more general methodology in presented in @xcite where an approximation of the likelihood is used instead .",
    "finally , a less studied approach for the purpose of inference is the linear noise approximation ( lna ) which conveniently decouples non - linearity in the diffusion approximation into a non - linear set of ordinary differential equations in the mre and a set of linear stochastic differential equations for the random fluctuations around a deterministic state ( * ? ? ?",
    "* chap . 10),@xcite .",
    "recently , @xcite have shown the simple analytic form of the approximate likelihood obtained by the lna simplifies mcmc inference and can be applied to problems with relatively small number of molecules .",
    "a commonly employed algorithm for mcmc is the metropolis - hastings algorithm @xcite , which relies on random perturbations around the current state using a local proposal mechanism .",
    "it should be noted here that the state of the markov chain is different from the state of the stochastic process . in the mcmc context state",
    "refers the current values of the unknown system parameters whereas the state of the system refers to the value of the stochastic process at a given time .",
    "we will use the term state interchangeably for the rest of this paper and its meaning will be clear from the context . due to the local nature of the proposal mechanism used by the metropolis - hastings algorithm , samples from the posterior exhibit strong random walk behaviour and auto - correlation . tuning the proposal mechanisms to achieve good mixing and",
    "fast convergence is far from straightforward even though some theoretical guidance is provided @xcite .",
    "mcmc methods , such as the metropolis adjusted langevin algorithm ( mala ) @xcite and the hamiltonian monte carlo ( hmc ) @xcite , have also been studied in the literature and have been shown to be more efficient than random walk metropolis - hastings in terms of effective sample size ( ess ) and convergence rates on several problems .",
    "however , hmc and mala also require extensive tuning of the proposal mechanisms , see for example @xcite and @xcite . for mjps",
    "the problem is compounded further since system parameters , such as probability rate constants of chemical reactions , are often highly correlated and whose values may differ by orders of magnitudes .",
    "the resulting posterior distributions have long narrow `` valleys '' preventing any local proposal mechanism from proposing large moves about the parameter space .",
    "more recently @xcite proposed exploitation of the underlying riemann manifold of probability density functions when defining mcmc methods thus exploiting the intrinsic geometry of statistical models , thereby providing a principled framework and systematic approach to the proposal design process .",
    "these algorithms rely on the gradient and fisher information matrix of the likelihood function to automatically tune the proposal mechanism such that large moves on the parameter space are possible and therefore improve convergence and mixing of the chains . in @xcite this approach has been successfully applied for the mre approximation of chemical reaction networks . for the lna the fisher information and the gradient of the likelihood function",
    "can be easily obtained @xcite . in this paper",
    "we study the application of the riemann manifold mcmc methods for the lna approximation and compare the mixing efficiency and computational cost with to the commonly used metropolis - hastings algorithm .",
    "moreover we study how the the markov chains and the resulting monte carlo estimates behave for systems which are far from their thermodynamic limit .",
    "the aim is to improve the efficiency of mcmc inference for mjps in order to allow for larger and more complex models frequently encountered in biology and chemistry to be studied in more detail .    in the next section",
    "we give a brief overview of markov jump processes . the diffusion and linear noise approximations",
    "are presented in section [ sec : approximations ] .",
    "we then discuss mcmc and the riemann manifold algorithms in section  [ sec : mcmc ] .",
    "numerical simulations are presented in section  [ sec : experiments ] while section  [ sec : conclusions ] concludes the paper .",
    "a @xmath10-dimensional stochastic process is a family of @xmath10 random variables @xmath11^{t}$ ] indexed by a continuous time variable @xmath12 with initial conditions @xmath13 .",
    "a markov jump process ( mjp ) is a stochastic process satisfying the markov property such that @xmath14 = p[{\\boldsymbol{x}}(t_{0})]\\prod_{i=1}^{n}p[{\\boldsymbol{x}}(t_{i})|{\\boldsymbol{x}}(t_{i-1})],\\ ] ] where the dependence on any parameters or other quantities has been suppressed .",
    "that is , the conditional probability of the system state at time @xmath15 only depends on state of the system at the previous time @xmath16 .",
    "a mjp is characterised by a finite number , @xmath17 , of state transitions with rates @xmath18 and state change vectors @xmath19 with @xmath20 $ ] .",
    "@xmath21 is the probability , given the state of the system at time @xmath12 , @xmath22 , of a jump to a new state @xmath23 in the infinitesimal time interval @xmath24 . for the problems we consider in this paper the transition rates not only depend on the current state and time but also on unknown rate parameters @xmath25 . from the markov property",
    "we can directly obtain the conditional probability of the system being in state @xmath26 at time @xmath12 given initial conditions which is characterised by the master equation @xmath27 .",
    "\\label{eq : masterequation}\\ ] ]    equation  ( [ eq : masterequation ] ) in general form is intractable especially when the transition rate functions @xmath28 are nonlinear with respect to the system state .",
    "numerical simulation is also prohibitively expensive as the computational cost grows exponentially with @xmath10 @xcite .",
    "however , given initial conditions @xmath13 and values for the unknown rate parameters @xmath25 we can simulate realisations of the mjp by first noting that the time @xmath29 to the next state transition is exponentially distributed with rate @xmath30 and the new state @xmath31 will be @xmath32 with probability @xmath33 this results in an iterative algorithm from which we can forward simulate a complete trajectory for the stochastic process @xmath34 , known as the stochastic simulation algorithm ( ssa ) @xcite in the chemical kinetics literature .    from the specification of the mjp",
    "we can also write the likelihood function with respect to the parameters @xmath25 for a completely observed process @xmath34 at the time interval @xmath35 $ ] as @xmath36 where n is the number of transitions occurred in the time interval @xmath35 $ ] , @xmath37 $ ] is the type of the @xmath38 transition and @xmath39 are the time and state at the @xmath38 transition respectively .",
    "notice that the likelihood function corresponds to the generative process described by the ssa . by specifying a suitable prior and applying bayes theorem ,",
    "we can obtain the posterior distribution @xmath40 which we can use for inference over the unknown parameters @xmath25 @xcite .",
    "in many problems of interest however we can not observe the times and types of all transitions in a given time interval .",
    "rather , we can only observe the state of the system @xmath41 at discrete time points @xmath42 $ ] .",
    "the solution proposed in @xcite is to treat the trajectories , as well as the number , times and types of transitions , between observed time points as latent variables .",
    "this leads to a data augmentation framework @xcite where a markov chain is constructed to sample from the joint posterior of the parameters and the latent variables . at each mcmc iteration",
    "the complete trajectory of the mjp process has to be simulated conditional on the observed data and the parameters which for some systems can be computationally demanding .",
    "furthermore , due to the high dimensional nature of the simulated trajectory and the strong dependence on the system parameters and observed data the mcmc algorithm has very poor convergence and mixing properties requiring many samples from the posterior in order to obtain sufficiently accurate monte carlo estimates .",
    "finally , a further complication that arises is that the number of transitions between two observed time points is also unknown and has to be sampled using a reversible - jumps type algorithm @xcite . for more details",
    "see @xcite .",
    "the resulting algorithm therefore is computationally demanding thus limiting its applicability on small and relatively simple mjps .",
    "a more efficient version of the algorithm is also suggested in @xcite where instead of simulating the trajectories between observations using the exact mjp an approximate proposal distribution is employed to sample trajectories which are accepted or rejected using the metropolis - hastings ratio .",
    "an alternative to working directly with the master equation and the original mjp is to consider approximations which provide for efficient simulation and possibly an easy to evaluate likelihood function for discretely observed data .",
    "although the resulting posterior will also be approximate in nature , it can be sufficient for inferential purposes given that the system under consideration is near its thermodynamic limit .",
    "here we describe the diffusion approximation and from that how we can arrive at the lna .",
    "our presentation is rather informal and follows @xcite and @xcite . for",
    "a more formal derivation the reader should refer to @xcite and @xcite .",
    "the requirement for these approximations to be consistent is the existence of a proportionality constant @xmath2 which governs the size of the fluctuations such that for large @xmath2 the jumps will be relatively small and as both @xmath2 and @xmath26 tend to infinity approaching the system s thermodynamic limit then , @xmath43 where @xmath44 and @xmath45 are independent of @xmath2 . for many physical processes where the fluctuations are due to the discrete nature of matter",
    "there is a natural @xmath2 parameter with such properties .",
    "examples of such parameters can be the system size in chemical kinetics , the capacity of a condenser in electric circuits or the mass of a particle @xcite .      in order to obtain a langevin equation which closely matches the dynamics of the mjp it is assumed that there is an infinitesimal time interval @xmath46 which satisfies the following conditions @xmath47 \\label{eq : taucondition1}\\\\ f_{j}({\\boldsymbol{x}}_{t},{\\boldsymbol{\\theta}},t)dt \\gg 1 & \\quad & \\forall j\\in[1,m ] .",
    "\\label{eq : taucondition2}\\end{aligned}\\ ] ] the first condition constrains @xmath46 to be small enough such that the transition rate functions remain approximately constant .",
    "this implies that the number of transitions of type @xmath48 is distributed as a poisson random variable with mean @xmath49 and is independent from other transitions of type @xmath50 .",
    "the second condition constrains @xmath46 to be large enough such that the number of transitions for each state is significantly larger than 1 , which further implies that the poisson distribution can be accurately approximated by a gaussian distribution .",
    "it can be shown @xcite that we can choose @xmath46 and @xmath2 such that both conditions can be satisfied and this generally occurs when the system approaches its thermodynamic limit .",
    "given such a timescale , the state of the system at time @xmath51 can be computed by @xmath52{\\boldsymbol{s}}_{j } \\label{eq : tauleaping}\\ ] ] where @xmath53 $ ] denotes a gaussian random variate with mean @xmath54 and variance @xmath55 . from equation  ( [ eq : tauleaping ] )",
    "we can directly obtain a langevin equation of the form @xmath56}d{\\boldsymbol{b}}_{t } \\label{eq : langevin}\\ ] ] where we used @xmath57 to denote the matrix whose columns are the state change vectors @xmath58 , @xmath59 to denote the vector whose elements are the transition rates @xmath28 , @xmath60 a function that returns a diagonal matrix with elements taken from the vector @xmath61 and @xmath62 an @xmath17 dimensional wiener process .",
    "notice that the dimension of @xmath63 differs from that of @xmath62 .",
    "due to the nonlinear state dependent drift and diffusion coefficients in equation  ( [ eq : langevin ] ) the transition density of the stochastic process is also intractable .",
    "therefore a data augmentation approach similar to the one in @xcite has to be followed .",
    "however , there is no longer the need to sample the number , times and types of state transitions as the mjp is approximated with a continuous process .",
    "moreover , the latent variables corresponding to unobserved states can now be efficiently simulated by an euler - maruyama scheme which is computationally more efficient than the ssa .",
    "this approach has been followed by @xcite and @xcite for inference over the unknown parameters @xmath25 while in @xcite a similar methodology has been applied on a real data from an auto - regulatory gene expression network .",
    "substituting equation  ( [ eq : varchange ] ) in the langevin equation  ( [ eq : langevin ] ) and dividing by @xmath2 we get @xmath64}d{\\boldsymbol{b}}_{t } \\label{eq : sdeconsentrationsform}\\ ] ] from which we can see that the fluctuations are of the order of @xmath65 and in the thermodynamic limit ( [ eq : sdeconsentrationsform ] ) reduces to the macroscopic rate equation ( mre ) @xmath66 to obtain the linear noise approximation ( lna ) we make the assumption that for sufficiently large @xmath2 a solution to ( [ eq : sdeconsentrationsform ] ) will differ from the mre by a stochastic term of order @xmath65 .",
    "that is @xmath67 where @xmath68 are deterministic or sure variables satisfying the mre and @xmath69 are stochastic variables . rewriting the transition rate functions using ( [ eq : ansatz ] ) and taylor",
    "expand around @xmath70 we get @xmath71 we can now substitute ( [ eq : ansatz ] ) and ( [ eq : rateftaylor ] ) back into ( [ eq : sdeconsentrationsform ] ) and collect terms of @xmath72 to get the expression for the differential of @xmath70 which is nothing other than the mre @xmath73 finally , collecting remaining terms and neglecting terms of @xmath74 and higher we get the differential of @xmath75 as @xmath76}d{\\boldsymbol{b}}_{t } \\label{eq : linearsde}\\ ] ] where we used @xmath77 to denote the jacobian of the transition rates @xmath78 .",
    "equation  ( [ eq : linearsde ] ) characterises the fluctuations around the deterministic state @xmath70 and its validity depends on the size of @xmath2 .",
    "as @xmath2 increases the magnitude of the individual jumps @xmath58 becomes negligible relative to the distance in @xmath70 over which the non - linearity of @xmath45 becomes noticeable .",
    "a measure of the sufficiency of lna is the coefficient of variation , i.e. the ratio of the standard deviation to the mean . for a more thorough discussion on the validity of lna",
    "the reader is referred to @xcite and the supplementary material of @xcite .",
    "lna provides a convenient expression for the approximate likelihood since the mre  ( [ eq : mre ] ) can be easily solved numerically and its computational cost is polynomial in @xmath10 .",
    "moreover , equation  ( [ eq : linearsde ] ) is a system of linear stochastic differential equations which has an explicit solution of the form @xmath79}d{\\boldsymbol{b}}_{s } \\right ) \\label{eq : xisolution}\\ ] ] where the integral is in the it sense and @xmath80 is the solution of @xmath81 since the it integral of a deterministic function is a gaussian random variable @xcite , equation  ( [ eq : xisolution ] ) implies that @xmath69 has a multivariate normal distribution . to simplify",
    "further the analysis assume that the initial condition for @xmath82 has a multivariate normal distribution such that @xmath83 .",
    "for the rest of the paper we will assume that @xmath84 and @xmath85 are known . in cases where the initial conditions are unknown they can be treated as additional parameters .",
    "equations  ( [ eq : ansatz ] , [ eq : mre ] , [ eq : linearsde ] , [ eq : xisolution ] ) and the specification of initial conditions further imply that @xmath86 where @xmath68 are solutions of the mre and @xmath87 are solutions of @xmath88{\\boldsymbol{s}}^{t}.\\ ] ] finally , multiplying ( [ eq : lnasolution ] ) by @xmath2 we get @xmath89    assume that we have observations from the stochastic process @xmath34 at discrete time points @xmath90 . moreover ,",
    "assume that each observation @xmath63 is obtained by a independent realisation of @xmath34 .",
    "for example to obtain an observation at @xmath91 the ssa is used to simulate a trajectory from @xmath92 to @xmath93 and the state of the system at @xmath93 is kept . for @xmath94 the ssa",
    "is again used to simulate a new trajectory from @xmath92 to @xmath95 keeping only the state of the system at @xmath95 and the process continues until all necessary observations are gathered .",
    "this kind of data are very frequently encountered in biology where in order to obtain a single measurement the sample has to be `` sacrificed '' .",
    "this is common in data obtained using polymerase chain reaction reporter assays @xcite for example .",
    "see also @xcite for an example of an inference problem with such data .",
    "due to the independence between different observations and the markov property the likelihood is simply @xmath96    in this paper we only consider observations of this kind .",
    "however the methodology is readily applicable when observations from a single realisation of @xmath34 are available . in this case",
    "the likelihood also has a simple form @xmath97\\ ] ] where @xmath98 is an @xmath99 vector with all the observations , @xmath100 , is also a @xmath99 vector with solutions of the mre and @xmath101 is a @xmath102 block matrix @xmath103 \\}$ ] such that @xmath104 this stems from the fact that due to the markov property and equation  ( [ eq : lnasolution ] ) each @xmath105 can be written as a sum of multivariate normal random variables and therefore @xmath106 is also a multivariate normal random variable . for more details",
    "refer to the supplementary material of @xcite and @xcite .",
    "the only additional complication which arises for time - series data is that the off - diagonal components of the lna variance in equation ( [ eq : varlna ] ) need to be estimated by numerically solving the system of odes in equation ( [ eq : phimatlna ] ) .",
    "notice that despite the fact that the variance matrix is full we can still exploit the markov property and write the likelihood as a product of the conditional likelihoods and therefore avoid the cost of inverting the @xmath102 variance matrix .",
    "in this section we give a brief overview of the mcmc algorithms that we consider in this work . some familiarity with the concepts of mcmc",
    "is required by the reader since an introduction to the subject is out of the scope of this paper .      for a random vector @xmath107 with density @xmath108",
    "the metropolis - hastings algorithm employs a proposal mechanism @xmath109 and proposed moves are accepted with probability @xmath110 . in the context of bayesian inference",
    "the target density @xmath108 corresponds to the posterior distribution of the model parameters .",
    "tuning the metropolis - hastings algorithm involves selecting the right proposal mechanism .",
    "a common choice is to use a random walk gaussian proposal of the form @xmath111 , where @xmath112 denotes the multivariate normal density with mean @xmath113 and covariance matrix @xmath114 .    selecting the covariance matrix",
    "however , is far from trivial in most cases since knowledge about the target density is required .",
    "therefore a more simplified proposal mechanism is often considered where the covariance matrix is replaced with a diagonal matrix such as @xmath115 where the value of the scale parameter @xmath116 has to be tuned in order to achieve fast convergence and good mixing .",
    "small values of @xmath116 imply small transitions and result in high acceptance rates while the mixing of the markov chain is poor .",
    "large values on the other hand , allow for large transitions but they result in most of the samples being rejected .",
    "tuning the scale parameter becomes even more difficult in problems where the standard deviations of the marginal posteriors differ substantially , since different scales are required for each dimension , and this is exacerbated when correlations between different variables exist .",
    "adaptive schemes for the metropolis - hastings algorithm have also been proposed @xcite though they should be applied with care @xcite .",
    "parameters such as reaction rate constants often differ orders of magnitude , thus a scaled diagonal covariance matrix will be a bad choice for such problems . in the numerical simulations in the next section we used a metropolis within gibbs scheme where each parameter is updated conditional on all others using a univariate normal density with a parameter - specific scale parameter .",
    "this allows us to tune the scale for each proposal independently and achieve better mixing .",
    "denoting the log of the target density as @xmath117 , the manifold mala ( mmala ) method , @xcite , defines a langevin diffusion with stationary distribution @xmath108 on the riemann manifold of density functions with metric tensor @xmath118 . by employing a first order euler integrator to solve the diffusion a proposal mechanism with density @xmath119",
    "is obtained , where @xmath116 is the integration step size , a parameter which needs to be tuned , and the @xmath120th component of the mean function @xmath121 is @xmath122 where @xmath123 are the christoffel symbols of the metric in local coordinates @xcite .",
    "similarly to mala @xcite , due to the discretisation error introduced by the first order approximation , convergence to the stationary distribution is not guaranteed anymore and thus the metropolis - hastings ratio is employed to correct this bias .",
    "the mmala algorithm can be simply stated as in algorithm [ alg : mala ] and more details can be found in @xcite .",
    "inititialise @xmath124 @xmath125 @xmath126 @xmath127}$ ] @xmath128",
    "@xmath129    we can interpret the proposal mechanism of mmala as a local gaussian approximation to the target density similar to the adaptive metropolis - hastings of @xcite .",
    "in contrast to @xcite , the effective covariance matrix in mmala is the inverse of the metric tensor evaluated at the current position and no samples from the chain are required in order to estimate it , therefore avoiding the difficulties of adaptive mcmc discussed in @xcite .",
    "furthermore a simplified version of the mmala algorithm ( smmala ) can also be derived by assuming a manifold with constant curvature , thus cancelling the last term in equation ( [ eq : meanmmala ] ) which depends on the christoffel symbols .",
    "finally , the mmala algorithm can be seen as a generalisation of the original mala @xcite since , if the metric tensor @xmath118 is equal to the identity matrix corresponding to an euclidean manifold , then the original algorithm is recovered .",
    "the riemann manifold hamiltonian monte carlo ( rmhmc ) method defines a hamiltonian on the riemann manifold of probability density functions by introducing the auxiliary variables @xmath130 , which are interpreted as the momentum at a particular position @xmath131 and by considering the negative log of the target density as a potential function .",
    "more formally , the hamiltonian defined on the riemann manifold is : @xmath132 where the terms @xmath133 and @xmath134 are the potential energy and kinetic energy terms , respectively . simulating the hamiltonian",
    "requires a time - reversible and volume preserving numerical integrator . for this purpose",
    "the generalised leapfrog algorithm can be employed and provides a deterministic proposal mechanism for simulating from the conditional distribution , i.e. @xmath135 . more details about the generalised leapfrog integrator can be found in @xcite . to simulate a path across the manifold ,",
    "the leapfrog integrator is iterated @xmath136 times which along with the integration step size @xmath116 are parameters requiring tuning . again , due to the integration errors on simulating the hamiltonian , in order to ensure convergence to the stationary distribution the metropolis - hastings ratio is applied .",
    "moreover , following the suggestion in @xcite the number of leapfrog iterations @xmath136 is randomised in order to improve mixing .",
    "the rmhmc algorithm is given in algorithm [ alg : rmhmc ] .",
    "inititialise @xmath124 @xmath137 @xmath138 @xmath139}$ ] @xmath140 + solve @xmath141 solve @xmath142 $ ] @xmath143 @xmath144 + @xmath145 @xmath127}$ ] @xmath146 @xmath129    similar to the mmala algorithm , when the metric tensor @xmath118 is equal to the identity matrix corresponding to an euclidean manifold , then rmhmc is equivalent to the hmc algorithm of @xcite .",
    "for the manifold mcmc algorithms discussed in this section we will need the gradient of the log likelihood as well as a metric tensor for the lna . for density functions the natural metric tensor is the expected fisher information , @xmath147 , @xcite and for a multivariate normal with mean @xmath148 and",
    "covariance matrix @xmath101 its general form is @xmath149 for the likelihood in equation  ( [ eq : lnalikelihood ] ) the fisher information is then a sum of @xmath150 matrices @xmath151 , one evaluated at each time point .",
    "similarly the general form of the partial derivatives for the log of a multivariate normal is @xmath152}{\\partial \\theta_{i } } = \\frac{1}{2}\\mbox{tr}\\left [ ( { \\boldsymbol{c}}{\\boldsymbol{c}}^{t } - { \\boldsymbol{\\sigma}}^{-1}({\\boldsymbol{\\theta } } ) ) \\frac{\\partial { \\boldsymbol{\\sigma}}({\\boldsymbol{\\theta}})}{\\partial \\theta_{i}}\\right ] + { \\boldsymbol{c}}^{t}\\frac{\\partial{\\boldsymbol{\\mu}}({\\boldsymbol{\\theta}})}{\\partial\\theta_{i}}\\ ] ] where @xmath153 $ ] .",
    "moreover , during the leap - frog integration for the rmhmc and for the mean function of mmala the partial derivatives of the fisher information are needed .",
    "their general form is @xmath154\\\\   & + & \\frac{1}{2}\\mbox{tr}\\left[{\\boldsymbol{\\sigma}}^{-1}({\\boldsymbol{\\theta}})\\left(\\frac{\\partial { \\boldsymbol{\\sigma}}({\\boldsymbol{\\theta}})}{\\partial \\theta_{i}\\partial\\theta_{k}}{\\boldsymbol{a}}_{j } + \\frac{\\partial { \\boldsymbol{\\sigma}}({\\boldsymbol{\\theta}})}{\\partial \\theta_{j}\\partial\\theta_{k}}{\\boldsymbol{a}}_{i } \\right ) \\right]\\end{aligned}\\ ] ] where @xmath155 and @xmath156 .",
    "the above quantities require first and second order sensitivities for the @xmath70 and @xmath157 which we obtain by augmenting the ode systems with the additional sensitivity equations . for an ode system of @xmath158 equations with form @xmath159 and @xmath160 parameters @xmath25 , the first and second order forward sensitivity equations are given by ( [ eg : firstsens ] ) and ( [ eg : secsens ] ) respectively .",
    "@xmath161 @xmath162\\frac{\\partial^{2 } { \\boldsymbol{y}}}{\\partial{\\boldsymbol{\\theta}}\\partial{\\boldsymbol{\\theta}}^{t } } + \\left[{\\boldsymbol{i}}_{n_{y}}\\otimes\\frac{\\partial { \\boldsymbol{y}}^{t}}{\\partial { \\boldsymbol{\\theta } } } \\right]\\left[{\\boldsymbol{f}}_{{\\boldsymbol{y}},{\\boldsymbol{y}}}\\frac{\\partial { \\boldsymbol{y } } } { \\partial { \\boldsymbol{\\theta } } } + { \\boldsymbol{f}}_{{\\boldsymbol{y}},{\\boldsymbol{\\theta}}}\\right ] \\nonumber \\\\ & + & \\left[{\\boldsymbol{f}}_{{\\boldsymbol{\\theta}},{\\boldsymbol{y}}}\\frac{\\partial { \\boldsymbol{y } } } { \\partial { \\boldsymbol{\\theta } } } + { \\boldsymbol{f}}_{{\\boldsymbol{\\theta}},{\\boldsymbol{\\theta}}}\\right ] , \\nonumber   \\\\ & & \\frac{\\partial^{2 } { \\boldsymbol{y}}(t_{0})}{\\partial{\\boldsymbol{\\theta}}^{2 } } = \\frac{\\partial^{2 } { \\boldsymbol{y}}_{0}}{\\partial{\\boldsymbol{\\theta}}^{2 } }   \\label{eg : secsens}\\end{aligned}\\ ] ]    we use @xmath163 to denote the @xmath164 matrix where its @xmath165 column is the partial derivatives of @xmath166 with respect to @xmath167 .",
    "@xmath168 denotes the derivative of @xmath163 with respect to @xmath169 and is an @xmath170 matrix where its @xmath165 column is the partial derivatives of @xmath171 with respect to @xmath172 .",
    "@xmath173 denotes the @xmath174 identity matrix , @xmath175 the kronecker product and @xmath176 an operator that creates a column vector by stacking the columns of matrix @xmath177 .      in many problems",
    "the parameters @xmath25 can be constrained in certain parts of @xmath178 where @xmath160 is the number of parameters . in models of chemical kinetics for example",
    ", rate parameters must be positive and can differ by orders of magnitude . for the mcmc algorithms described in the previous section we will need a re - parameterisation in order to allow the algorithms to operate on an unbounded and unconstrained parameter space .    for the numerical simulations in section  [ sec : experiments ] we use a @xmath179 re - parameterisation by introducing the variables @xmath180 , @xmath181 $ ] . to ensure that we sample from the correct posterior the joint density is scaled by the determinant of the jacobian such that @xmath182 where @xmath183 is a @xmath184 diagonal matrix with elements @xmath185 .",
    "the gradient and fisher information along with its partial derivatives follow from the chain rule as @xmath186      in bayesian statistics priors provide the means for incorporating existing knowledge for the parameters in question .",
    "the choice of a suitable prior distribution can be informed from knowledge about the process being modelled , the experimental design and empirical observations .",
    "for example we might want to restrict rate parameters in chemical kinetics from becoming very high since we assume from the experimental design that reactions are slow enough to be able to be observed . in some cases",
    "the model itself can also guide the choice of the prior .",
    "for example when a model is only defined for a certain range of values of the parameters , a prior restricting the parameters in that range should be used .    in the numerical simulations of the next section we use independent normal priors for the parameters @xmath187 . due to the re - parameterisation introduced earlier",
    ", this corresponds to a log - normal prior with base 10 for the parameters @xmath25 .",
    "this choice allows parameters to differ several orders of magnitude while it ensures they are strictly positive . moreover , as noted in @xcite the negative hessian of the prior is added to the fisher information in order to form the metric tensor used during mcmc sampling .",
    "this has the added benefit of regularising the fisher information when it is near - singular @xcite although we have not observed such problems in the simulations presented here .",
    "in this section we consider two examples from chemical kinetics @xcite and study the effect of the system size parameter on inference using mcmc .",
    "the first system consists of three species where an unstable monomer , @xmath188 , can dimerise to an unstable dimer , @xmath189 , which is then converted to a stable form , @xmath190 .",
    "the reaction set for this system is @xmath191 and the state of the system at time @xmath12 will be denoted by @xmath192^{t}$ ] .",
    "the propensity functions , or state transition probabilities are @xmath193^{t}$ ] and the corresponding state change matrix is @xmath194    @llll + @xmath2 & m.h . &",
    "smmala & rmhmc + 1 & 121 ( 3.6 ) & 150 ( 3.9 ) & 245 ( 0.06 ) + 2 & 226 ( 6.7 ) & 2163 ( 57.2 ) & 4775 ( 1.3 ) + 5 & 132 ( 3.9 ) & 3539 ( 93.6 ) & 4618 ( 1.2 ) + 10 & 180 ( 5.3 ) & 3397 ( 89.8 ) & 5954 ( 1.6 ) + 100 & 214 ( 6.4 ) & 3725 ( 98.5 ) & 6066 ( 1.7 ) +    for our experiments we will assume that initial conditions are known and set them to @xmath195 , @xmath196 , @xmath197 .",
    "moreover we will set the reaction rate parameters to @xmath198 , @xmath199 , @xmath200 and @xmath201 .",
    "notice that we make explicit the relation between the system size and parameter @xmath202 and we will infer rate @xmath203 up to a proportionality constant . for all the experiments we simulate data using the ssa of @xcite for the time interval @xmath204 $ ] and we discretise such that @xmath205 .",
    "each observation @xmath206 is obtained independently by simulating a trajectory from @xmath92 to @xmath15 and keeping only the last state discarding the rest of the trajectory .",
    "moreover for each time point @xmath15 we also simulate 10 independent observations .",
    "since each observation is obtained by a different trajectory of the mjp we assume that initial conditions do not have a point mass rather for each trajectory we sample its initial condition from a poisson with means @xmath207 .",
    "we use the synthetic data to perform inference for the rate parameters @xmath208 by drawing samples from the posterior @xmath209\\ ] ] where @xmath210 indexes independent observations for the same time point . for all simulations in this paper",
    "we assume that the means for the initial conditions are known .",
    "following similar arguments as for the derivation of the lna in section [ sec : approximations ] , namely that as the system approaches its thermodynamic limit transition densities become gaussian , the initial conditions for the ode systems for the mean and variance of the transition densities are @xmath211 and @xmath212 , where @xmath213 is the identity matrix . in a more realistic scenario",
    "the initial conditions must be included as additional parameters in @xmath25 .",
    "for all parameters we used an independent log - normal prior with base 10 , zero mean and one standard deviation and chains are initialised by drawing a random sample from the prior .",
    "for the metropolis - hastings sampler we set the initial proposal scale parameters to @xmath214 and automatically adapt them every 100 samples during the burn - in phase in order to achieve an acceptance rate of @xmath215 @xcite .",
    "the same adaptation strategy was followed for the simplified mmala and rmhmc algorithms where the initial step size was also set to @xmath214 and was tuned in order to achieve acceptance rates in the order of @xmath216 @xcite .",
    "finally , the number of leap - frog steps for rmhmc was fixed to 5 .",
    "we have found that a burn in period of 10,000 to 20,000 samples was adequate for all algorithms to converge to the stationary distribution .",
    "@lllll + @xmath2 & @xmath6 & @xmath217 & @xmath218 & @xmath219 + true & 1 & 2@xmath220 & 0.5 & 0.04 + 1 & 0.88 ( 0.031 ) & 1.72 ( 0.253 ) & 0.39 ( 0.039 ) & 0.003 ( 0.002 ) + 2 & 1.3 ( 0.041 ) & 0.69 ( 0.066 ) & 0.35 ( 0.016 ) & 0.014 ( 0.002 ) + 5 & 0.93 ( 0.019 ) & 0.39 ( 0.028 ) & 0.48 ( 0.025 ) & 0.034 ( 0.002 ) + 10 & 1.0 ( 0.015 ) & 0.18 ( 0.008 ) & 0.47 ( 0.015 ) & 0.037 ( 0.001 ) + 100 & 0.99 ( 0.004 ) & 0.01 ( 0.0002 ) & 0.52 ( 0.004 ) & 0.039 ( 0.0003 ) +     ( left panel ) and @xmath219 right panel for different values of @xmath2 .",
    "results are obtained by 10,000 posterior samples using rmhmc .",
    "[ fig : dimer : marginal : omega ] ]    table  [ table : dimer : essomega ] compares the minimum effective sample size ( ess ) and the time normalised ess obtained by all algorithms for different values of the system size parameter @xmath2 .",
    "the smmala and rmhmc samplers utilise the gradients and the fisher information of the approximate likelihood obtained by the lna in order to make efficient proposals .",
    "as the system size increases and thus the lna better approximates the true likelihood then mixing of the manifold mcmc algorithms improves . for this particular example we can see",
    "that good mixing can be achieved even for very small systems with only @xmath221 molecules , ( @xmath222 ) .",
    "sampler is not affected by the system size but its mixing is very poor in all cases . from the time normalised ess we can also see that despite the improved mixing of rmhmc the computational cost is significant . on the contrary smmala",
    "provides a good tradeoff between mixing efficiency and computational cost . finally , table  [ table : dimer : meansdrmhmc ] reports the marginal posterior means and standard deviations for different values of @xmath2 obtained by rmhmc .",
    "the marginal posteriors for parameters @xmath218 and @xmath219 with @xmath223 are also shown in figure  [ fig : dimer : marginal : omega ] .",
    "results from the mh and smmala samplers are similar and are omitted . for small system sizes we can observe that there is an increased bias of the monte carlo estimate while the posterior standard deviation is higher reflecting the high degree of uncertainty around the mean .",
    "the bias however significantly reduces as the system size increases and for @xmath224 reasonable estimates can be obtained .    ]",
    "the second example from the chemical kinetics literature that we consider is the schlgl reaction set .",
    "@xmath225 the corresponding state transition rates and state change matrix are given in equations  ( [ eq : schlogl : s ] ) and ( [ eq : schlogl : ratef ] ) respectively .",
    "the state of the system consists only of the number of molecules of a single species @xmath226 .",
    "@xmath227    @xmath228    the system is known to have two stable states which appear at different times depending on the size of the system . @xcite",
    "have shown that the lna fails to provide a reasonable approximation of this system even for large concentration numbers .",
    "their numerical experiments demonstrate that the lna approximation can only approximate one of the two modes depending on the initial conditions . here",
    "our aim is to show that using the lna to obtain an approximate posterior over the unknown reaction rate constants can be very misleading for bi - stable systems . using the resulting posterior means for the reaction rates",
    "gives us an lna that fails to approximate any of the two stable modes .    to demonstrate that we follow the same experimental procedure as in the previous example",
    "that is , we simulate data using the ssa for the time interval @xmath229 $ ] , @xmath230 with fixed rate parameters and then use this data for posterior inference of the rate parameters using mcmc .",
    "values for the true rate parameters and initial conditions where set as in @xcite .",
    "namely , @xmath231 , @xmath232 , @xmath233 , @xmath234 and @xmath235 , where @xmath2 was fixed to 1 .",
    "after 10,000 burn - in samples all samplers converged to a posterior distributions with mean @xmath236 \\approx ( 0.130 , 3.3e^{-4 } , 3.5e^{+3 } , 26.22)^{t}\\ ] ] and variance @xmath237 \\approx ( 1.2e^{-4 } , 8.2e^{-10 } , 8.6e^{+4 } , 4.53)^{t}\\ ] ] the lna obtained by using the posterior means for the rate constants is shown in figure  [ fig : schlogl : lna ] along with the data obtained by the ssa and the lna using the true values for the rate constants .",
    "we can see that the lna approximation obtained by the posterior means fails to approximate any of the two modes .",
    "rather it approximates the empirical mean and variance of the data .",
    "finally , to illustrate the applicability of the methodology to systems biology we also consider a simplified model for the biochemical reactions involved in the expression of a single gene to protein .",
    "the model presented in this section is the same with the model used in the study of @xcite and we adopt the same notation in order to make comparisons easier .",
    "gene expression is modelled in terms of three biochemical species ; dna , mrna and protein ; and four chemical reactions or state transitions ; transcription , mrna degradation , translation and protein degradation .",
    "the model can be written in chemical reaction notation as @xmath238    the system state at time @xmath12 is @xmath239^{t}$ ] where @xmath240 and @xmath241 are the number of mrna and protein molecules respectively .",
    "the corresponding state dependent transition rates are @xmath242^{t}$ ] where @xmath243 and @xmath244 are unknown reaction rate constants .",
    "@xmath245 is the time dependent transcription rate of the gene which for the purposes of this section is modelled as @xmath246 where all the @xmath247s are also unknown parameters controlling gene transcription .",
    "this corresponds to a transcription rate that due to some stimulus ( experimental or environmental ) increases for @xmath248 and then it drops towards the base line @xmath249 for @xmath250 .",
    "finally , the state change matrix for this set of reactions is given in equation  ( [ eq : siglegene : s ] ) .",
    "@xmath251    ]    as in the study of @xcite we also consider a non - linear extension of this model where the transcription rate of the gene @xmath245 is a function of the protein concentration that the gene is transcribed to .",
    "this is modelled using a hill function @xmath252 where for the experiments of this section we will set @xmath253 and @xmath254 making the protein an inhibitor of mrna transcription .",
    "a schematic representation of this model is shown in figure  [ fig : signlegene : schematic ] .",
    "for the rest of this section we will refer to this model as the auto - regulatory single gene expression model .",
    "standard deviation predicted by lna . left column shows the mrna molecules and right column the protein .",
    "( online version in colour.)[fig : signlegene : traj ] ]    using the transition probabilities @xmath255 and matrix @xmath57 we simulate synthetic data using the stochastic simulation algorithm ( ssa ) @xcite and sample at discrete time points",
    ". values for the unknown rate constants and the parameters controlling gene transcription are shown in table  [ table : signlegene : results ] .",
    "the time interval is taken to be @xmath256 $ ] while the interval between two observations @xmath257 .",
    "each time point is sampled from an independent trajectory by starting the ssa from @xmath92 and simulate up to @xmath15 keeping only the state @xmath206 and discarding the rest of the trajectory .",
    "this resembles the experimental conditions often encountered in biology where in order to make an observation the sample has to be `` sacrificed '' .",
    "finally for each time point we also generate 10 independent observations from different trajectories .",
    "initial conditions @xmath258 are simulated from a poisson distribution with means @xmath259 and @xmath260 for the mrna and protein molecules respectively .",
    "the system size parameter @xmath2 is considered to be unknown and for this experiment is set to 1 such that concentrations are equal to the number of molecules .",
    "figures  ( [ fig : signlegene : traj].a ) and  ( [ fig : signlegene : traj].b ) show data simulated from this process from the singe gene expression model as well as the lna prediction . simulated data for the auto - regulatory model are presented in figures  ( [ fig : signlegene : traj].c ) and  ( [ fig : signlegene : traj].d ) .",
    "we use the simulated data to infer the unknown parameters @xmath261 by sampling using mcmc from the lna approximate posterior @xmath262\\ ] ] where @xmath210 indexes independent samples for the same time point and @xmath263 .",
    "table  [ table : signlegene : results ] summarises the results from the mcmc chains for the two models of gene expression .",
    "firstly , we can see that despite the relatively small number of molecules in both systems the lna approximation provides very accurate estimates for the true parameters .",
    "moreover we can see that the mixing of the metropolis - hastings sampler is very poor for both models while rmhmc and simplified manifold mala algorithms perform very well .",
    "this can be explained by the strong correlations between parameters in the posterior distribution preventing the m.h .",
    "sampler to make sufficiently large proposals .",
    "for example , the parameters @xmath264 control mrna translation and protein degradation respectively .",
    "the concentration of protein molecules is directly affected by the two rates and they are expected to be heavily correlated . in figures  [ fig",
    ": singlegene : marginal : post].a  [ fig : singlegene : marginal : post].b we show the marginal joint posterior for parameters @xmath264 and @xmath265 for the single gene expression model which exhibit very strong positive correlation . finally figure  [ fig : singlegene : trace ] compares the trace plots obtained from mh , smmala and rmhmc for parameters @xmath244 and @xmath266 of the auto - regulatory gene expression model .",
    "lrrrrrrr + parameters & @xmath267 & @xmath244 & @xmath266 & @xmath268 & @xmath269 & @xmath270 & @xmath249 + true values & 0.44 & 0.52 & 10.0 & 15.0 & 0.40 & 7.0 & 3.0 +   + ( a.r . ) & ( 0.28 ) & ( 0.33 ) & ( 0.30 ) & ( 0.34 ) & ( 0.29 ) & ( 0.29 ) & ( 0.34 ) + ( @xmath116 ) & ( 0.013 ) & ( 0.007 ) & ( 0.008 ) & ( 0.022 ) & ( 0.056 ) & ( 0.007 ) & ( 0.016 ) + mean & 0.45 & 0.54 & 10.54 & 14.86 & 0.39 & 7.03 & 3.14 + s.d .",
    "& 0.017 & 0.017 & 0.336 & 0.509 & 0.029 & 0.056 & 0.149 + ess & 42 & 34 & 34 & 149 & 117 & 58 & 44 + ess / time & 1.42 & 1.15 & 1.15 & 5.05 & 3.96 & 1.96 & 1.49 +   + mean & 0.45 & 0.54 & 10.57 & 14.88 & 0.39 & 7.04 & 3.17 + s.d . &",
    "0.018 & 0.016 & 0.306 & 0.537 & 0.030 & 0.053 & 0.152 + ess & 2891 & 2911 & 2958 & 2787 & 3310 & 3183 & 2878 + ess / time & 83.79 & 84.37 & 85.73 & 80.78 & 95.94 & 92.26 & 83.42 +   + mean & 0.46 & 0.54 & 10.57 & 14.95 & 0.39 & 7.04 & 3.18 + s.d .",
    "& 0.018 & 0.015 & 0.300 & 0.555 & 0.030 & 0.052 & 0.153 + ess & 7731 & 8238 & 8304 & 7160 & 7380 & 7791 & 7950 + ess / time & 0.52 & 0.55 & 0.56 & 0.48 & 0.49 & 0.52 & 0.53 +   +   + ( a.r . ) & ( 0.26 ) & ( 0.36 ) & ( 0.31 ) & ( 0.33 ) & ( 0.24 ) & ( 0.30 ) & ( 0.35 ) + ( @xmath116 ) & ( 0.028 ) & ( 0.012 ) & ( 0.016 ) & ( 0.071 ) & ( 0.231 ) & ( 0.019 ) & ( 0.029 ) + mean & 0.4360 & 0.52 & 10.40 & 14.61 & 0.40 & 6.82 & 3.13 + s.d . &",
    "0.016 & 0.018 & 0.424 & 1.089 & 0.076 & 0.090 & 0.142 + ess & 201 & 71 & 73 & 465 & 339 & 420 & 239 + ess / time & 6.12 & 2.16 & 2.22 & 14.17 & 10.33 & 12.80 & 7.28 +   + mean & 0.43 & 0.52 & 10.44 & 14.24 & 0.38 & 6.82 & 3.12 + s.d .",
    "& 0.016 & 0.018 & 0.422 & 1.125 & 0.075 & 0.091 & 0.142 + ess & 2990 & 3270 & 3454 & 3124 & 3164 & 3316 & 3195 + ess / time & 76.86 & 84.06 & 88.79 & 80.30 & 81.33 & 85.24 & 82.13 +   + mean & 0.43 & 0.52 & 10.43 & 14.52 & 0.40 & 6.82 & 3.13 + s.d .",
    "& 0.016 & 0.017 & 0.412 & 1.158 & 0.078 & 0.089 & 0.144 + ess & 6532 & 6593 & 6614 & 5112 & 5384 & 6595 & 6642 + ess / time & 0.41 & 0.41 & 0.41 & 0.32 & 0.34 & 0.41 & 0.42 +     left panel , figure ( a ) , and @xmath271 right panel , figure ( b ) for the single gene expression model .",
    "dashed lines are the true values used to generate the synthetic data .",
    "dots are samples from the posterior .",
    "iso - contours and shaded region are obtained by kernel density estimation using posterior samples .",
    "( online version in colour . )",
    "[ fig : singlegene : marginal : post ] ]     and @xmath266 .",
    "red solid line denotes the true values .",
    "( online version in colour . )",
    "[ fig : singlegene : trace ] ]",
    "bayesian inference for markov jump processes is a challenging problem which has many important practical applications .",
    "previous research @xcite has shown that although exact inference is possible , the computational cost and the autocorrelation of the markov chains is such that limits its applicability to small problems .",
    "the main problem stems from the requirement to simulate the mjp for the trajectory of the system between discrete observations .",
    "@xcite has shown that by considering a diffusion approximation the simulation can be performed in a much more efficient manner . in this paper",
    "we considered the linear noise approximation which only requires to simulate a system of ordinary differential equations while the stochastic fluctuations have an exact analytic solution .",
    "the linear noise approximation is valid only when the system is sufficiently close to its thermodynamic limit , a condition that is also required for the diffusion approximation .",
    "previous research on the linear noise approximation @xcite has focussed on the metropolis - hastings sampler .",
    "we have demonstrated here that when the posterior distribution exhibits strong correlation between parameters then the metropolis - hastings sampler has strong auto - correlations .",
    "such correlations are very common for chemical reaction and gene regulatory systems .",
    "the riemann manifold mcmc algorithms we considered in this work exploit the geometric structure of the target posterior in order to design efficient proposal mechanisms .",
    "in particular the simplified manifold mala algorithm is a conceptually simple algorithm which provides a good trade - off between computational cost and sample auto - correlation .    although the problems considered in this work are relatively small , but certainly non - trivial , we believe that the proposed methodology is applicable for larger and more complex systems .",
    "the systems we studied in this paper all have a linear dependence on the unknown parameters and we have not observed any local modes in our simulations .",
    "the analysis of such systems is the subject of on - going work .",
    "moreover , in real applications it is not possible to observe the populations of all species and there is an additional measurement error term .",
    "extension of the lna to handle such cases is straight forward , see @xcite for example , however the effect of partial observations and measurement error on the mcmc inference is something that needs to be studied in more detail .",
    "komorowski , m. , costa , m. j. , rand , d. a. , and stumpf , m. p. h. 2011 sensitivity , robustness , and identifiability in stochastic chemical kinetics models .",
    "_ proceedings of the national academy of sciences _ * 108 * ( 21 ) , 86458650 , ( doi 10.1073/pnas.1015814108 . ) .",
    "boys , r. j. , wilkinson , d. j. , and kirkwood , t. b. 2008 bayesian inference for a discretely observed stochastic kinetic model . _ statistics and computing _ * 18 * , 125135 , ( doi 10.1007/s11222 - 007 - 9043-x . ) .",
    "xu , t. , vyshemirsky , v. , gormand , a. , von kriegsheim , a. , girolami , m. , baillie , g. s. , ketley , d. , dunlop , a. j. , milligan , g. , houslay , m. d. , and kolch , w. 2010 inferring signaling pathway topologies from multiple perturbation measurements of specific biochemical species .",
    "_ science signaling _ * 3 * ( 113 ) , ra20 , ( doi 10.1126/scisignal.2000517 . ) .",
    "calderhead , b. and girolami , m. 2011 statistical analysis of nonlinear dynamical systems using differential geometric sampling methods .",
    "_ interface focus _ * 1 * ( 6 ) , 821 - 835 , ( doi 10.1098/rsfs.2011.0051 . ) .",
    "van kampen , n. g. 1982 the diffusion approximation for markov processes . in _",
    "thermodynamics & kinetics of biological processes _ ( ed .",
    "lamprecht , i. and zotin , a. i. ) new york , usa : walter de gruyter & co ..      \\12 .",
    "roberts , g. o. and stramer , o. 2001 on inference for partially observed nonlinear diffusion models using the metropolis - hastings algorithm .",
    "_ biometrika _ * 88 * ( 3 ) , 603621 , ( doi 10.1093/biomet/88.3.603 . ) .",
    "golightly , a. and wilkinson , d. j. 2011 bayesian parameter inference for stochastic biochemical network models using particle markov chain monte carlo",
    ". _ interface focus _ * 1 * ( 6 ) , 807 - 820 , ( doi 10.1098/rsfs.2011.0047 . )",
    "wallace , e. , gillespie , d. , sanft , k. , and petzold , l. 2012 the linear noise approximation is valid over limited times for any chemical system that is sufficiently large . _ to appear in iet systems biology _ * ? ?",
    "* ( ? ? ) , ? ? ,    \\15 .",
    "komorowski , m. , finkenstadt , b. , harper , c. , and rand , d. 2009 bayesian inference of biochemical kinetic parameters using the linear noise approximation .",
    "_ bmc bioinformatics _ * 10 * ( 1 ) , 343 , ( doi 10.1186/1471 - 2105 - 10 - 343 . ) .",
    "roberts , g. , o. , gelman , a. , and gilks , w. , r. 1997 weak convergence and optimal scaling of random walk metropolis algorithms . _ the annals of applied probability _ * 7 * ( 1 ) , 110120 , ( doi 10.1214/aoap/1034625254 . ) .",
    "roberts , g. , o. and stramer , o. 2003 langevin diffusions and metropolis - hastings algorithms .",
    "_ methodology and computing in applied probability _ * 4 * ( 4 ) , 337 - 358 , ( doi doi:10.1023/a:1023562417138 . ) .",
    "roberts , g. o. and rosenthal , j. s. 1998 optimal scaling of discrete approximations to langevin diffusions .",
    "_ journal of the royal statistical society .",
    "series b ( statistical methodology ) _ * 60 * ( 1 ) , 255268 , ( doi 10.1111/1467 - 9868.00123 . ) .",
    "girolami , m. and calderhead , b. 2011 riemann manifold langevin and hamiltonian monte carlo methods .",
    "_ journal of the royal statistical society : series b ( statistical methodology ) _ * 73 * ( 2 ) , 123214 , ( doi 10.1111/j.1467 - 9868.2010.00765.x . ) .",
    "ferm , l. , ltstedt , p. , and hellander , a. 2008 a hierarchy of approximations of the master equation scaled by a size parameter .",
    "_ journal of scientific computing _ * 34 * , 127151 , ( doi 10.1007/s10915 - 007 - 9179-z . ) .",
    "tanner , m. a. and wong , w. h. 1987 the calculation of posterior distributions by data augmentation .",
    "_ journal of the american statistical association _ * 82 * ( 398 ) , pp . 528 - 540 , ( doi 10.2307/2289457 . ) .",
    "heron , e. , a. , finkenstdt , b. and rand , d. , a. bayesian inference for dynamic transcriptional regulation ; the hes1 system as a case study .",
    "_ bioinformatics _ * 23 * ( 19 ) , 25962603 , ( doi 10.1093/bioinformatics / btm367 . ) ."
  ],
  "abstract_text": [
    "<S> bayesian analysis for markov jump processes is a non - trivial and challenging problem . </S>",
    "<S> although exact inference is theoretically possible , it is computationally demanding thus its applicability is limited to a small class of problems . in this paper </S>",
    "<S> we describe the application of riemann manifold mcmc methods using an approximation to the likelihood of the markov jump process which is valid when the system modelled is near its thermodynamic limit . </S>",
    "<S> the proposed approach is both statistically and computationally efficient while the convergence rate and mixing of the chains allows for fast mcmc inference . </S>",
    "<S> the methodology is evaluated using numerical simulations on two problems from chemical kinetics and one from systems biology . </S>"
  ]
}