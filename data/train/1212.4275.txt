{
  "article_text": [
    "many practical optimization problems must be addressed within the framework of large - scale structured convex optimization and need to be solved in a parallel and distributed manner .",
    "such problems may appear in many fields of science and engineering : e.g. graph theory , networks , transportation , distributed model predictive control , distributed estimation and multistage stochastic optimization , see e.g. @xcite and the references quoted therein .",
    "solving large - scale optimization problems is still a challenge in many applications @xcite due to the limitations of computational devices and computer systems .",
    "recently , thanks to the development of parallel and distributed computer systems , many large - scale problems have been solved by using the framework of decomposition .",
    "however , methods and algorithms for solving this type of problems , which can be performed in a parallel or distributed manner , are still limited @xcite .    in this paper",
    "we develop a new optimization algorithm to solve the following structured convex optimization problem with a separable objective function and coupling linear constraints : @xmath1 where , for every @xmath2 , @xmath3 is convex ( not necessarily strictly convex ) and possibly _ nonsmooth _ functions , @xmath4 is nonempty , closed and convex sets , @xmath5 and @xmath6 , and @xmath7 . here ,",
    "@xmath8 is referred to as a local convex constraint and the final constraint is called _ coupling linear constraint_.    in the literature , several approaches based on decomposition techniques have been proposed to solve problem . in order to observe the differences between those methods and our approach in this paper , we briefly classify some of these that we found most related . the first class of algorithms is based on lagrangian relaxation and subgradient methods of multipliers @xcite .",
    "it has been observed that subgradient methods are usually slow and numerically sensitive to the choice of step sizes in practice @xcite .",
    "moreover , the convergence rate of these methods is in general @xmath9 , where @xmath10 is the iteration counter .",
    "the second approach relies on augmented lagrangian functions , see e.g. @xcite .",
    "many variants were proposed and tried to process the inseparability of the crossproduct terms in the augmented lagrangian function in different ways . besides this approach , the authors in @xcite considered the dual decomposition based on fenchel s duality theory . another research direction is based on alternating direction methods which were studied , for example , in @xcite .",
    "alternatively , proximal point - type methods were extended to the decomposition framework , see , e.g. @xcite .",
    "other researchers employed interior point methods in the framework of decomposition such as @xcite . furthermore , the mean value cross decomposition in @xcite , the partial inverse method in @xcite and the accelerated gradient method of multipliers in @xcite were also proposed to solve problem .",
    "we note that decomposition and splitting methods are very well developed in convex optimization , especially in generalized equations and variational inequalities , see e.g. @xcite .",
    "recently , we have proposed a new decomposition method to solve problem in @xcite based on two primal steps and one dual step .",
    "it is proved that the convergence rate of the algorithm is @xmath11 which is much better than the subgradient - type methods of multipliers @xcite but its computational complexity per iteration is higher that of these classical methods . moreover",
    ", the algorithm uses an automatic strategy to update the parameters which improves the numerical efficiency in practice .    in this paper",
    ", we propose a new inexact decomposition algorithm for solving which employs smoothing techniques @xcite and excessive gap condition @xcite .",
    "0.1 cm * contribution .",
    "* the contribution of the paper is as follows :    1 .   we propose a new decomposition algorithm based on inexact dual gradients .",
    "this algorithm requires only one primal step and two dual steps at each iteration and allows one to solve the subproblem of each component inexactly and in parallel .",
    "moreover , all the algorithmic parameters are updated automatically without using any tuning strategy .",
    "2 .   we prove the convergence of the proposed algorithm and show that the convergence rate is @xmath12 , where @xmath10 is the iteration counter .",
    "due to the automatic update of the algorithmic parameters and the low computational complexity per iteration , the proposed algorithm performs better than some related existing decomposition algorithms from the literature in terms of computational time . 3 .",
    "an extension to a switching strategy is also presented .",
    "this algorithm updates simultaneously two smoothness parameters at each iteration and makes use of the inexactness of the gradients of the smoothed dual function .",
    "let us emphasize the following points of the contribution .",
    "the first algorithm proposed in this paper consists of two dual steps and one primal step per iteration .",
    "this requires solving the primal subproblems in parallel only _ once _ but needs one more dual step . because the dual step corresponds only to a simple matrix - vector multiplication , the computational cost of the proposed algorithm",
    "is significantly reduced compared to some existing decomposition methods in the literature . moreover , since solving the primal subproblems exactly is only _ conceptual _ ( except existing a closed form solution ) , we propose an inexact algorithm which allows one to solve these problems up to a given accuracy .",
    "the accuracies of solving the primal subproblems are adaptively chosen such that the convergence of the whole algorithm is preserved .",
    "the parameters in the algorithm are updated automatically based on an analysis of the iteration scheme .",
    "this is different from augmented lagrangian approaches @xcite where we need to find an appropriate way to tune the penalty parameter in each practical situation .    in the switching variant ,",
    "apart from the inexactness , this algorithm allows one to update simultaneously both smoothness parameters at each iteration .",
    "the advantage of this algorithm compared to the first one is that it takes into account the convergence behavior of the primal and dual steps which accelerates the convergence of the algorithm in some practical situations . since both algorithms are primal - dual methods , we not only obtain an approximate solution of the dual problem but also an approximate solution of the original problem without any auxiliary computation",
    ".    0.1 cm * paper outline . *",
    "the rest of this paper is organized as follows . in the next section ,",
    "we briefly describe the lagrangian dual decomposition method for separable convex optimization .",
    "section [ sec : smoothing ] mainly presents the smoothing technique via prox - functions as well as the inexact excessive gap condition .",
    "section [ sec : decomp_alg ] builds a new inexact algorithm called _ inexact decomposition algorithm with two dual steps _ ( algorithm [ alg : a1 ] ) .",
    "the convergence rate of this algorithm is established .",
    "section [ sec : inexact_switching_alg ] presents an inexact switching variant of algorithm [ alg : a1 ] proposed in section [ sec : decomp_alg ] .",
    "numerical examples are presented in section [ sec : num_results ] to examine the performance of the proposed algorithms and a numerical comparison is made . in order to make the paper more compact , we move some the technical proofs to appendix [ sec : proofs ]",
    ".    0.1 cm * notation .",
    "* throughout the paper , we shall consider the euclidean space @xmath13 endowed with an inner product @xmath14 for @xmath15 and the norm @xmath16 .",
    "for a given matrix @xmath17 , the spectral norm @xmath18 is used in the paper .",
    "the notation @xmath19 represents a column vector in @xmath13 , where @xmath20 is a subvector in @xmath21 and @xmath22 .",
    "we denote by @xmath23 and @xmath24 the sets of nonnegative and positive real numbers , respectively .",
    "we also use @xmath25 for the subdifferential of a convex function @xmath26 . for a given convex set @xmath27 in @xmath13",
    ", we denote @xmath28 the relative interior of @xmath27 , see , e.g. @xcite .",
    "in this section , we briefly describe the lagrangian dual decomposition technique in convex optimization , see , e.g. @xcite .",
    "let @xmath29 be a vector and @xmath30 $ ] be a matrix formed from @xmath31 components @xmath20 and @xmath32 , respectively .",
    "let @xmath33 and @xmath34 . the lagrange function associated with the coupling constraint @xmath35 is defined by @xmath36,\\ ] ] where @xmath37 is the lagrange multiplier associated with @xmath35 .",
    "the dual problem of is written as @xmath38 where @xmath39 is the dual function defined by @xmath40 note that the dual function @xmath41 can be computed in _",
    "parallel _ for each component @xmath20 as @xmath42 we denote @xmath43 a solution of the minimization problem in .",
    "consequently , @xmath44 is a solution of .",
    "it is well - known that @xmath41 is concave and the dual problem is convex but nondifferentiable in general .    throughout the paper , we assume that the following assumptions hold @xcite .",
    "[ as : a1 ] the solution set @xmath45 of is nonempty and either @xmath27 is polyhedral or the slater constraint qualification condition for problem holds , i.e. @xmath46 for each @xmath47 , @xmath48 is proper , lower semicontinuous and convex in @xmath21 .    if @xmath27 is convex and bounded then @xmath45 is also convex and bounded .",
    "note that the objective function @xmath49 is not necessarily smooth .",
    "for example , @xmath50 , which is nonsmooth and separable , can be handled in our framework . under assumption ,",
    "the solution set @xmath51 of the dual problem is nonempty and bounded .",
    "moreover , _ strong duality condition _ holds , i.e. for all @xmath52 we have @xmath53 .",
    "if strong duality holds then we can refer to @xmath54 or @xmath55 as the _ primal - dual optimal value_.",
    "since the dual function @xmath41 is in general nonsmooth , one can apply smoothing techniques to approximate @xmath41 up to a desired accuracy . in this section",
    ", we propose to use a smoothing technique via proximity functions proposed in @xcite .",
    "0.1 cm * 3.1 .",
    "proximity functions .",
    "* let @xmath56 be a nonempty , closed and convex set in @xmath13 .",
    "we consider a nonnegative , continuous and strongly convex function @xmath57 with a convexity parameter @xmath58 . as usual , we call @xmath59 a _ proximity function _ ( prox - function ) associated with the convex set @xmath56",
    ". let @xmath60 since @xmath59 is strongly convex , there exists a unique point @xmath61 such that @xmath62 . the point @xmath63 is called the _ proximity center _ of @xmath56 w.r.t .",
    "@xmath59 . moreover ,",
    "if @xmath56 is bounded then @xmath64 .",
    "without loss of generality , we can assume that @xmath65 . otherwise , we can shift this function as @xmath66 , where @xmath67 .",
    "[ re : quad_prox ] we note that the simplest prox - function is the quadratic form @xmath68 , where @xmath69 , @xmath58 and @xmath61 are given .",
    "if the set @xmath56 has a specific structure then one can choose an appropriate prox - function that captures better the structure of @xmath56 than the quadratic prox - function .",
    "for example , if @xmath56 is a standard simplex , one can choose the entropy prox - function as mentioned in @xcite .",
    "if @xmath56 has no specific structure , then we can use the quadratic prox - function given above .",
    "consequently , the convex problem generated using quadratic prox - functions reduces in some cases to a simple optimization problem , so that its solution can be computed numerically very efficient .    0.1 cm * 3.2",
    ". smoothed approximations . * in order to build smoothed approximations of the objective function @xmath49 and the dual function @xmath41 in the framework of the primal - dual smoothing technique proposed in @xcite , we make the following assumption .    [ as : a2 ] each feasible set @xmath70 admits a prox - function @xmath71 with a convexity parameter @xmath72 and the proximity center @xmath73 .",
    "further , we assume @xmath74    if @xmath70 is bounded for @xmath75 , then assumption [ as : a2 ] is satisfied .",
    "if @xmath70 is unbounded , then we can assume that our sample points generated by the proposed algorithms are bounded .",
    "in this case , we can restrict the feasible set of problem on @xmath76 , where @xmath56 is a given compact set which contains the sample points and the desired solutions of .",
    "we denote by @xmath77 since @xmath48 is not necessarily strictly convex , the function @xmath78 defined by may not be differentiable .",
    "we consider the following function @xmath79 for @xmath22 and @xmath80 is a given _",
    "smoothness parameter_. we denote @xmath81 the unique solution of , i.e. @xmath82 and @xmath83 .",
    "we call each minimization problem in a _ primal subproblem_. note that we can use different smoothness parameters @xmath84 in for each @xmath85 .",
    "first , we recall the following properties of @xmath86 , see @xcite .    [",
    "le : dual_smoothed_estimates ] for any @xmath80 , the function @xmath86 defined by is concave and differentiable .",
    "the gradient of @xmath87 is given by @xmath88 which is lipschitz continuous with a lipschitz constant @xmath89 moreover , we have the following estimates : @xmath90 and @xmath91    next , we consider the variation of the function @xmath92 w.r.t . the parameter @xmath93 .",
    "[ le : g_beta_1 ] let us fix @xmath94 .",
    "the function @xmath95 defined by is well - defined , nondecreasing , concave and differentiable in @xmath24 .",
    "moreover , the following inequality holds : @xmath96 where @xmath97 is defined by .",
    "since @xmath98 and @xmath99 , it is sufficient to prove the inequality for @xmath100 , with @xmath22 . let us fix @xmath94 and @xmath85 .",
    "we define @xmath101 a function of two joint variables @xmath20 and @xmath93 . since @xmath102 is strongly convex w.r.t . @xmath20 and linear w.r.t .",
    "@xmath93 , @xmath103 is well - defined and concave w.r.t .",
    "moreover , it is differentiable w.r.t . @xmath93 and @xmath104 , where @xmath105 is defined in .",
    "hence , @xmath100 is nonincreasing . by using the concavity of @xmath100 we have @xmath106 by summing up the last inequality from @xmath107 to @xmath31 and then using we obtain .",
    "@xmath108    finally , we consider a smooth approximation to @xmath49 .",
    "let @xmath109 be a prox - function defined in @xmath110 with a convexity parameter @xmath111 .",
    "it is obvious that the proximity center of @xmath112 is @xmath113 .",
    "we define the following function on @xmath27 : @xmath114 where @xmath115 is the second smoothness parameter .",
    "we denote by @xmath116 the solution of . from",
    ", we see that @xmath117 and @xmath116 can be computed explicitly as @xmath118 it clear that @xmath119 for all @xmath120 .",
    "now , we define the function @xmath121 as @xmath122 then , @xmath121 is exactly a quadratic penalty function of .",
    "the following lemma shows that @xmath123 is an approximation of @xmath49 .",
    "[ le : primal_smoothed_estimates ] the function @xmath124 defined by satisfies the following estimate : @xmath125 where @xmath126 .",
    "moreover , the function @xmath26 defined by satisfies @xmath127    by the definition of @xmath124 , we have @xmath128 .",
    "thus follows from this equality by applying some elementary inequalities .",
    "the bounds follow directly from the definition of @xmath26 . @xmath108    0.1 cm * 3.3 .",
    "inexact solutions of the primal subproblem . * regarding the primal subproblem ,",
    "if the objective function @xmath48 has a specific form , e.g. univariate functions , then we can solve this problem analytically ( exactly ) to obtain a _ closed form _ solution .",
    "a simple example of such function is @xmath129 .",
    "however , in most practical problems , solving the primal subproblem exactly is only conceptual . in practice",
    ", we only solve this problem up to a given accuracy .",
    "in other words , for each @xmath130 , the solution @xmath105 in is approximated by @xmath131 in the sense of the following definition .",
    "[ de : inexactness ] we say that the point @xmath132 approximates @xmath105 defined by up to a given accuracy @xmath133 if :    * it is feasible to @xmath70 , i.e. @xmath134 ; * @xmath132 satisfies the condition : @xmath135    where @xmath136 .    in practice , for a given accuracy @xmath137",
    ", we can check whether the conditions of definition [ de : inexactness ] are satisfied by applying classical convex optimization algorithms , e.g. ( sub)gradient or interior - point algorithms @xcite .",
    "since @xmath138 is strongly convex with a convexity parameter @xmath139 , we have @xmath140 where @xmath141 is defined as in definition [ de : inexactness ] .",
    "consequently , we have : @xmath142 for @xmath22 .",
    "let @xmath143 and @xmath144 the quantity @xmath145 can be referred to as an approximation of the gradient @xmath146 defined in lemma [ le : dual_smoothed_estimates ] .",
    "if we denote by @xmath147 the vector of accuracy levels then we can easily estimate @xmath148    0.1 cm * 3.4 .",
    "inexact excessive gap condition . * since problem is convex , under assumption strong duality holds .",
    "the aim is to generate a primal - dual sequence @xmath149 such that for a sufficiently large @xmath10 the point @xmath150 is approximately feasible to , i.e. @xmath151 , and the primal - dual gap satisfies @xmath152 for given tolerances @xmath153 and @xmath154 .",
    "the algorithm designed below will employ the approximate functions - to solve the primal - dual problems - .",
    "first , we modify the excessive gap condition introduced by nesterov in @xcite to the inexact case in the following definition .",
    "[ de : exessive_gap_cond ] a point @xmath155 satisfies the _ inexact excessive gap _ ( @xmath156-_excessive gap _ ) condition w.r.t . @xmath157 and @xmath115 and a given accuracy @xmath158 if @xmath159    if @xmath160 then reduces to the exact excessive gap condition considered in @xcite .",
    "the following lemma provides an upper bound estimate for the primal - dual gap and the feasibility gap of problem .      from the estimates",
    "and we have @xmath164 then , by using , the last inequality implies the right - hand side of .",
    "next , for a given @xmath162 we have @xmath165 .",
    "thus we obtain the left - hand side of .",
    "finally , the estimate follows from after a few simple calculations .",
    "let us define @xmath166 the diameter of @xmath51 .",
    "since @xmath51 is bounded , we have @xmath167 . the estimates and",
    "can be simplified as @xmath168",
    "in this section we first show that , for a given @xmath169 , there exists a point @xmath170 such that the condition is satisfied .",
    "then , we propose a decomposition scheme to update successively a sequence @xmath171 that maintains the condition while it drives the sequences of smoothness parameters @xmath172 and @xmath173 to zero .",
    "let us introduce the following quantities @xmath174 } & : = \\left[\\sum_{i=1}^m\\sigma_i\\varepsilon_i^2\\right]^{1/2},\\\\ d_{\\sigma } & : = \\left[2\\sum_{i=1}^m\\frac{d_{x_i}}{\\sigma_i}\\right]^{1/2},\\\\ c_d                   & : =   { \\|a\\|}^2d_{\\sigma } + { \\|a^t(ax^c - b)\\|},\\\\ l_{\\mathrm{a } }      & : =   m\\max{\\left\\{\\frac{{\\|a_i\\|}^2}{\\sigma_i}~|~1\\leq i\\leq m\\right\\}}. \\end{cases}\\ ] ] from we see that the constant @xmath175 depends on the data of the problem ( i.e. @xmath176 , @xmath177 , @xmath178 , @xmath179 and @xmath63 ) . moreover , @xmath180 } = { \\|\\mathbf{\\varepsilon}\\|}$ ] .",
    "if we choose the accuracy @xmath181 for all @xmath22 , then @xmath180 } = \\sqrt{m}\\hat{\\varepsilon}$ ] and @xmath182 } = [ \\sum_{i=1}^m\\sigma_i]^{1/2}\\hat{\\varepsilon}$ ] .    0.1 cm * 4.1 .",
    "finding a starting point .",
    "* for a given a positive value @xmath80 , let @xmath183 be a point in @xmath184 computed as @xmath185 where @xmath186 is the origin and @xmath187 is defined by and @xmath188 is given by .",
    "the following lemma shows that @xmath189 satisfies the @xmath190-excessive gap condition .",
    "the proof of this lemma is given later in appendix [ sec : proofs ] .",
    "[ le : init_point ] the point @xmath170 generated by satisfies the @xmath190-excessive gap condition w.r.t . @xmath93 and",
    "@xmath191 provided that @xmath192 where @xmath193 } + \\frac{1}{2}\\varepsilon^2_{[\\sigma]}\\right ) \\geq 0 $ ] .",
    "note that if we use @xmath194 instead of @xmath187 into , i.e. the exact solution @xmath195 is used , then @xmath183 satisfies the @xmath196-excessive gap condition .    0.1 cm * 4.2 . the inexact main iteration with one primal step and two dual steps .",
    "* let us assume that @xmath197 is a given point in @xmath184 that satisfies the @xmath156-excessive gap condition w.r.t .",
    "@xmath93 , @xmath191 and @xmath156 .",
    "the aim is to compute a new point @xmath198 such that the condition holds for the new values @xmath199 , @xmath200 and @xmath201 with @xmath202 , @xmath203 and @xmath204 .    first , for a given @xmath37 and @xmath80 , we define the following mapping @xmath205 where @xmath206 is defined by and @xmath188 is the lipschitz constant . since this maximization problem is unconstrained and convex , we can show that the quantity @xmath207 can be computed explicitly as @xmath208.\\ ] ] next , the main scheme to update @xmath198 is presented as @xmath209 here , the smoothness parameters @xmath93 and @xmath191 and the step size @xmath210 will be appropriately updated to obtain @xmath199 , @xmath200 and @xmath211 , respectively .",
    "note that line 1 and line 3 in are simply matrix - vector multiplications , which can be computed distributively based on the structure of the coupling constraints and can be expressed as @xmath212 only line 2 in requires one to solve @xmath31 convex primal subproblems up to a given accuracy .",
    "however , this can be done in _",
    "parallel_.    let us define @xmath213 then , by assumption , we can see that @xmath214 .",
    "we consider an update rule for @xmath93 and @xmath191 as @xmath215 in order to show that @xmath198 satisfies the @xmath201-excessive gap condition , where @xmath201 will be defined later , we define the following function @xmath216}^2 + \\left[\\frac{\\beta_1}{l_{\\mathrm{a}}}c_d + ( 1-\\tau)\\tau\\left ( \\frac{c_d}{\\beta_2 } + { \\|a\\|}{\\|\\bar{y}\\|}\\right)\\right]\\varepsilon_{[1]},\\end{aligned}\\ ] ] where @xmath182 } , c_d$ ] and @xmath217 are defined in .    the next theorem provides a condition such that @xmath198 generated by satisfies the @xmath201-excessive gap condition . for clarity of the exposition we move the proof of this theorem to appendix [ sec : proofs ] .",
    "[ th : alg_scheme ] suppose that assumptions and are satisfied",
    ". let @xmath155 be a point satisfying the @xmath156-excessive gap condition w.r.t .",
    "two values @xmath93 and @xmath191 .",
    "then if the parameter @xmath218 is chosen such that @xmath219 and @xmath220 then the new point @xmath198 generated by is in @xmath184 and maintains the @xmath201-excessive gap condition w.r.t two new values @xmath199 and @xmath221 defined by , where @xmath222 with @xmath223 defined by .    0.1 cm * 4.3 . the step size update rule . *",
    "next , we show how to update the step size @xmath210 .",
    "indeed , from we have @xmath224 . by combining this inequality and we have @xmath225 . in order to ensure @xmath226 we require @xmath227 .",
    "since @xmath228 and @xmath229 $ ] , we have @xmath230^{1/2 } - ( 1-\\tilde{\\alpha}\\tau)\\tau\\right\\ } < \\tau.\\ ] ] hence , if we choose @xmath231^{1/2 } - ( 1-\\tilde{\\alpha}\\tau)\\tau\\right]$ ] then we obtain the tightest rule for updating @xmath218 . based on the above analysis",
    ", we eventually define a sequence @xmath232 as follows : @xmath233^{1/2 } - ( 1-\\tilde{\\alpha}_k\\tau_k)\\tau_k\\right\\ } , ~ \\forall k\\geq 0,\\ ] ] where @xmath234 is given and @xmath235 $ ] .",
    "the following lemma provides the convergence rate of the sequence @xmath236 , whose proof can be found in appendix [ sec : proofs ] .",
    "[ re : choice_of_tau ] the estimates of lemma [ le : choice_of_tau ] show that the sequence @xmath242 converges to zero with the convergence rate @xmath243 .",
    "consequently , by , we see that the sequence @xmath244 also converges to zero with the convergence rate @xmath245 . from and",
    ", we can derive an initial value @xmath246 .    in order to choose the accuracy for solving the primal subproblem",
    ", we need to analyze the formula .",
    "let us consider a sequence @xmath247 computed by @xmath248 where @xmath249 is given in .",
    "the sequence @xmath250 defined by @xmath251 where @xmath190 is chosen _ a priori _ , is nonincreasing if @xmath252 for all @xmath253 .",
    "[ le : accuracy_level_choice ] if the accuracy @xmath254 at the iteration @xmath10 of algorithm [ alg : a1 ] below is chosen such that @xmath255 for @xmath22 , where @xmath256,\\end{aligned}\\ ] ] then the sequence @xmath250 generated by is nonincreasing .    since @xmath257 for all @xmath22",
    ", we have @xmath180}^k \\leq \\sqrt{m}\\bar{\\varepsilon}^k$ ] and @xmath258}^k)^2 \\leq \\left(\\sum_{i=1}^m\\sigma_i\\right)(\\bar{\\varepsilon}^k)^2 \\leq \\left(\\sum_{i=1}^m\\sigma_i\\right)\\bar{\\varepsilon}^k$ ] . by substituting these inequalities into of @xmath249 and then using and the notation @xmath259",
    ", we have @xmath260 on the other hand , from we have @xmath261 for all @xmath262 .",
    "thus , @xmath250 is nonincreasing if @xmath263 for all @xmath253 .",
    "if we choose @xmath264 such that @xmath265 , i.e. @xmath266 , then @xmath252 . @xmath108    from lemma [ le : accuracy_level_choice ] it follows that if we choose @xmath267 sufficiently small , then the sequence @xmath268 generated by @xmath269 maintains the @xmath270-excessive gap condition with @xmath271 for all @xmath10 . now , by using lemmas [ le : excessive_gap ] and [ le : init_point ] , if we choose @xmath272 in lemma [ le : accuracy_level_choice ] such that @xmath273 , where @xmath274 and @xmath275 is a given accuracy level , then the condition holds with @xmath276 .",
    "0.1 cm * 4.4 .",
    "the algorithm and its convergence .",
    "* finally , we present the algorithm in detail and estimate its worst - case complexity . for simplicity of discussion",
    ", we fix the accuracy at one level @xmath264 for all the primal subproblems .",
    "however , we can alternatively choose different accuracy for each subproblem by slightly modifying the theory presented in this paper .",
    "_ inexact decomposition algorithm with two dual steps_[alg : a1 ] * initialization : * perform the following steps :    * _ step 1 _ : provide an accuracy level @xmath275 for solving and a value @xmath277 .",
    "set @xmath278 , @xmath279 and @xmath280 .",
    "* _ step 2 _ : compute @xmath281 by . set @xmath282 and @xmath283 .",
    "* _ step 3 _ : compute @xmath284 and @xmath285 from as @xmath286 and @xmath287 up to the accuracy @xmath272 .",
    "* iteration : * for @xmath288 , perform the following steps :    * _ step 1 _ : if a given stopping criterion is satisfied then terminate . *",
    "_ step 2 _ : compute @xmath289 by . set @xmath290 and",
    "update @xmath291 . *",
    "_ step 3 _ : solve the primal subproblems in _ in parallel _ up to the accuracy @xmath264 . * _ step 4 _ : compute @xmath292 by . * _ step 5 _ : compute @xmath293 , where @xmath294 . *",
    "_ step 6 _ : update @xmath295 and @xmath296 . *",
    "_ step 7 _ : update @xmath297 as @xmath298^{1/2 } - ( 1-\\tilde{\\alpha}_k\\tau_k)\\tau_k\\right\\}$ ] .",
    "the stopping criterion of algorithm [ alg : a1 ] at step 1 will be discussed in section [ sec : num_results ] . the maximum number of iterations @xmath299 provides a safeguard to prevent the algorithm from running to infinity .",
    "the following theorem provides the worst - case complexity estimate for algorithm [ alg : a1 ] under assumptions and .",
    "[ th : convergence ] suppose that assumptions and are satisfied .",
    "let @xmath300 be a sequence generated by algorithm [ alg : a1 ] after @xmath301 iterations .",
    "if the accuracy level @xmath302 in algorithm [ alg : a1 ] is chosen such that @xmath303 for some positive constant @xmath304 , then the following primal - dual gap holds @xmath305^{\\alpha^{*}}},\\ ] ] and the feasibility gap satisfies @xmath306 where @xmath307 and @xmath308 is defined by .",
    "consequently , the sequence @xmath149 generated by algorithm [ alg : a1 ] converges to a solution @xmath309 of the primal and dual problems - as @xmath310 and @xmath311 .    from lemma",
    "[ le : excessive_gap ] , we have @xmath312 and @xmath313 .",
    "moreover , @xmath314 due to the choice of @xmath190 and the update rule of @xmath315 at step 2 of algorithm [ alg : a1 ] . by combining these inequalities and and then using the definition of @xmath316 and @xmath317 we obtain and .",
    "the last conclusion is a direct consequence of and .",
    "@xmath108    the conclusions of theorem [ th : alg_scheme ] show that the initial accuracy of solving the primal subproblems needs to be chosen as @xmath11 .",
    "then , we have @xmath318 and @xmath319 .",
    "thus , if we choose the ratio @xmath320 such that @xmath321 then we obtain an asymptotic convergence rate @xmath11 for algorithm [ alg : a1 ] .",
    "we note that the accuracy of solving has to be updated at each iteration @xmath10 in algorithm [ alg : a1 ] .",
    "the new value is computed by @xmath322 at step 2 , which is the same @xmath323 order .",
    "now , we consider a particular case , where we can get an @xmath324 worst - case complexity ( @xmath325 is a desired accuracy ) .",
    "[ co : worst_case_complexity ] suppose that the smoothness parameter @xmath326 in algorithm [ alg : a1 ] is fixed at @xmath327 for all @xmath253 .",
    "suppose further that the accuracy level @xmath302 in algorithm [ alg : a1 ] is chosen as @xmath328 and that the sequence @xmath242 is updated by @xmath329 starting from @xmath278 .",
    "then , after @xmath330 iterations , one has @xmath331 where @xmath332 and @xmath333 .    if we assume that @xmath326 is fixed in algorithm [ alg : a2 ] then , by the new update rule of @xmath242 we have @xmath334 due to and with @xmath335 .",
    "since @xmath336 , if we choose @xmath337 then @xmath338 .",
    "furthermore , by lemma [ le : excessive_gap ] we have @xmath339 and @xmath340 . by combining these estimates",
    ", we obtain the conclusion .",
    "@xmath108    ( _ distributed implementation_)[re : dist_opt ] in algorithm [ alg : a1 ] , only the parameter @xmath341 requires centralized information . instead of using @xmath341 , we can use its lower bound @xmath320 to compute @xmath297 and @xmath326 . in this case",
    ", we can modify algorithm [ alg : a1 ] to obtain a distributed implementation .",
    "the modification is at steps 5 , 6 and 7 , where we can parallelize these steps by using the same formulas for the all subsystems to compute the parameters @xmath326 , @xmath342 and @xmath297 .",
    "we note that the points @xmath343 and @xmath344 in the scheme can be computed in parallel , while @xmath345 and @xmath346 can be computed distributively based on the structure of the coupling constraints of problem .",
    "since the ratio @xmath347 defined in may be small , algorithm [ alg : a1 ] only provides a suboptimal approximation @xmath348 to the optimal solution @xmath309 such that @xmath349 in the worst - case .",
    "for example , if we choose the prox - function @xmath350 , where @xmath351 , then worst - case complexity of algorithm [ alg : a1 ] is lower than subgradient methods , see of theorem [ th : convergence ] .",
    "algorithm [ alg : a1 ] leads to a poor performance .    in this section ,",
    "we propose to combine the scheme @xmath352 defined by in this paper and an inexact decomposition scheme with two primal steps and one dual step to ensure that the parameter @xmath93 always decreases to zero .",
    "apart from the inexactness , this variant allows one to update simultaneously both smoothness parameters at each iteration .    0.1 cm * 5.1 .",
    "the inexact main iteration with two primal steps .",
    "* let us consider the approximate function @xmath353 defined by .",
    "we recall that @xmath49 is only assumed to be convex and possibly nonsmooth , while @xmath354 is convex and lipschitz continuously differentiable .",
    "we define @xmath355 and the mapping @xmath356 where @xmath357 is the lipschitz constant of @xmath358 defined in lemma [ le : primal_smoothed_estimates ] . since @xmath359 is strongly convex , @xmath360 is well - defined .",
    "[ re : bregman_dist ] note that we can replace the quadratic term @xmath361 in by any bregman distance as done in @xcite",
    ". however , the convergence analysis based on this type of prox - functions is more complicated than the one given in this paper .",
    "suppose that we can only solve the minimization problem up to a given accuracy @xmath362 to obtain an approximate solution @xmath363 in the sense of definition .",
    "more precisely , @xmath364 and @xmath365 we denote @xmath366 and @xmath367 . in particular ,",
    "if @xmath48 is differentiable and its gradient is lipschitz continuous with a lipschitz constant @xmath368 for some @xmath369 then one can replace the approximate mapping @xmath370 by the following one : @xmath371^t\\!\\!\\!\\!(x_i \\!-\\ !",
    "\\hat{x}_i ) \\!+\\ ! \\frac{\\hat{l}_i(\\beta_2)}{2}{\\|x_i - \\hat{x}_i\\|}^2\\!\\!\\right\\},\\end{aligned}\\ ] ] where @xmath372 , in the sense of definition [ de : inexactness ] .",
    "note that the minimization problem defined in @xmath373 is a quadratic program with convex constraints .",
    "now , we can present the decomposition scheme with two primal steps in the case of inexactness as follows .",
    "suppose that @xmath155 satisfies w.r.t .",
    "@xmath93 , @xmath191 and @xmath156 .",
    "we update @xmath374 as @xmath375 where the step size @xmath210 will be appropriately updated and    1 .   the parameters @xmath93 and @xmath191 are updated by @xmath376 and @xmath377 ; 2 .",
    "@xmath378 is computed by ; 3 .",
    "@xmath379 is an approximation of @xmath380 defined in and .",
    "the following theorem states that the new point @xmath198 updated by @xmath381 maintains the @xmath201-excessive gap condition .",
    "the proof of this theorem is postponed to appendix [ sec : proofs ] .",
    "[ th : alg_scheme2 ] suppose that assumptions and are satisfied .",
    "let @xmath197 be a point in @xmath184 and satisfy the @xmath156-excessive gap condition w.r.t .",
    "two values @xmath93 and @xmath191",
    ". then if the parameter @xmath218 is chosen such that @xmath219 and @xmath382 then the new points @xmath198 updated by maintains the @xmath201-excessive gap condition w.r.t .",
    "two new values @xmath199 and @xmath221 , where @xmath383 } + \\frac{1}{2}\\sum_{i=1}^ml_i^{\\psi}(\\beta_2^{+})\\varepsilon_i^2 $ ] , and @xmath182}$ ] and @xmath384 are defined in .    finally , we note that the step size @xmath218 is updated by @xmath385 for @xmath253 starting from @xmath386 in the scheme , see @xcite for more details .",
    "0.1 cm * 5.2 .",
    "the algorithm and its convergence . *",
    "first , we provide an update rule for @xmath156 in definition [ de : exessive_gap_cond ] . with @xmath182}$ ] and @xmath384 defined in , let us consider the function @xmath387 } + \\frac{1}{2}\\sum_{i=1}^ml_i^{\\psi}(\\beta_2^{+})\\varepsilon_i^2,\\ ] ] and a sequence @xmath388 generated by @xmath389 , where @xmath190 is given and @xmath390 is chosen appropriately .",
    "the aim is to choose @xmath267 such that @xmath391 and @xmath388 is nonincreasing . by letting @xmath392 then , if we choose @xmath393 such that @xmath394 then we have @xmath395 .    by combining both schemes and , we obtain a new variant of algorithm [ alg : a1 ] with a switching strategy as described as follows .",
    "_ inexact decomposition algorithm with switching primal - dual steps_[alg : a2 ] * initialization : * perform as in algorithm [ alg : a1 ] with @xmath386 .",
    "* iteration : * for @xmath396 perform the following steps :    * _ step 1 _ : if a given stopping criterion is satisfied then terminate .",
    "* _ step 2 _ : if @xmath10 is _ even _ then perform the scheme with two primal steps : * * 2.1 . compute @xmath397 by . set @xmath398 and update @xmath399 . * * 2.2 .",
    "update @xmath296 . * * 2.3 .",
    "compute @xmath400 up to the accuracy @xmath264 . * * 2.4 .",
    "update @xmath401 . * * 2.5 .",
    "update the step - size parameter @xmath297 as @xmath402 . * _ step 3 _ : otherwise , ( i.e. @xmath10 is _ odd _ ) perform the scheme with two dual steps : * * 3.1 .",
    "compute @xmath289 by .",
    "set @xmath290 and update @xmath403 . *",
    "compute @xmath292 up to the accuracy @xmath264 . *",
    "compute @xmath404 , where @xmath405 . * * 3.4 .",
    "update @xmath406 and @xmath296 . * * 3.5 .",
    "update @xmath297 as @xmath407^{1/2 } - ( 1-\\tilde{\\alpha}_k\\tau_k)\\tau_k\\right\\}$ ] .",
    "note that the first line and third line of the scheme @xmath381 can be parallelized .",
    "they require one to solve @xmath31 convex subproblems of the form and , respectively _ in parallel_. if the function @xmath48 is differentiable and its gradient is lipschitz continuous for some @xmath85 , then we can use the approximate gradient mapping @xmath373 instead of @xmath370 and the corresponding minimization subproblem in the third line reduces to a quadratic program with convex constraints .",
    "the stopping criterion at step 1 will be given in section [ sec : num_results ] .",
    "similar to the proof of lemma [ le : choice_of_tau ] we can show that the sequence @xmath232 generated by step 2.5 or step 3.5 of algorithm [ alg : a2 ] satisfies estimates .",
    "consequently , the estimate for @xmath342 in is still valid , while the parameter @xmath326 satisfies @xmath408 .    finally , we summarize the convergence results of algorithm [ alg : a2 ] in the following theorem .",
    "[ th : convergence2 ] suppose that assumptions and are satisfied .",
    "let @xmath300 be a sequence generated by algorithm [ alg : a2 ] after @xmath301 iterations .",
    "if the accuracy level @xmath302 in algorithm [ alg : a2 ] is chosen such that @xmath409 for some positive constant @xmath304 , then the following primal - dual gap holds @xmath410 and the feasibility gap satisfies @xmath411 where @xmath412 and @xmath308 is defined as in .",
    "consequently , the sequence @xmath149 generated by algorithm [ alg : a2 ] converges to a solution @xmath413 of the primal and dual problems - as @xmath310 and @xmath311 .",
    "the proof of this theorem is similar to theorem [ th : convergence ] and thus we omit the details here .",
    "we can see from the right hand side of in theorem [ th : convergence2 ] that this term is better than the one in theorem [ th : convergence ] .",
    "consequently , the worst case complexity of algorithm [ alg : a2 ] is better than the one of algorithm [ alg : a1 ] .",
    "however , as a compensation , at each even iteration , the scheme @xmath381 is performed .",
    "it requires an additional cost to compute @xmath414 at the third line of @xmath381 . as an exception ,",
    "if the primal subproblem can be solved in a _ closed form _ then the cost - per - iteration of algorithm [ alg : a2 ] is almost the same as in algorithm [ alg : a1 ] .",
    "[ re : primal_update_variant ] note that we can only use the inexact decomposition scheme with two primal steps @xmath381 in to build an inexact variant of ( * ? ? ?",
    "* algorithm 1 ) .",
    "moreover , since the role of the schemes @xmath381 and @xmath352 is symmetric , we can switch them in algorithm [ alg : a2 ] .",
    "in this section we compare algorithms [ alg : a1 ] and [ alg : a2 ] derived in this paper with the two algorithms developed in ( * ? ? ?",
    "* algorithms 1 and 2 ) which we named ` 2pdecompalg ` and ` pddecompalg ` , the proximal center - based decomposition algorithm in @xcite , an exact variant of the proximal based decomposition algorithm in @xcite and three parallel variants of the alternating direction method of multipliers ( with three different strategies to update the penalty parameter ) .",
    "we note that these variants are the modifications of the algorithm in @xcite , and they can be applied to solve problem with more than two objective components ( i.e. @xmath415 ) .",
    "we named these algorithms by ` pcbdm ` , ` epbdm ` , ` admm - v1 ` , ` admm - v2 ` and ` admm - v3 ` , respectively . for more simulations and comparisons",
    "we refer to the extended technical report @xcite .",
    "the algorithms have been implemented in c++ running on a @xmath416 cores intel xeon @xmath417ghz workstation with @xmath418 gb of ram . in order to solve the general convex programming subproblems",
    ", we either used a commercial software called ` cplex ` or an open - source software package ` ipopt ` @xcite .",
    "all the algorithms have been parallelized by using ` openmp ` .    in the four numerical examples below , since the feasible set @xmath70 has no specific structure , we chose the quadratic prox - function @xmath419 in the four first algorithms , i.e. algorithms [ alg : a1 ] and [ alg : a2 ] , ` 2pdecompalg ` and ` pddecompalg ` , where @xmath420 and @xmath421 are given , for @xmath22 , as mentioned in remark [ re : quad_prox ] . with this choice",
    "we can solve the primal subproblem in the first example by using ` cplex ` .",
    "we terminated these algorithms if @xmath422 and either the approximate primal - dual gap satisfied @xmath423 or the value of the objective function did not significantly change in @xmath424 successive iterations , i.e. : @xmath425 here @xmath426 is the approximate value of @xmath427 evaluated at @xmath428 .    in `",
    "admm - v1 ` and ` admm - v2 ` we used the update formula in ( * ? ? ?",
    "* formula ( 21 ) ) to update the penalty parameter @xmath429 starting from @xmath430 and @xmath431 , respectively . in `",
    "admm - v3 ` this penalty parameter was fixed at @xmath432 for all iterations . in ` pcbdm ` , we chose the same prox - function as in our algorithms and the parameter @xmath93 in the subproblems was fixed at @xmath433 .",
    "we terminated all the remaining algorithms if the both conditions and were satisfied .",
    "the maximum number of iterations @xmath434 was set to @xmath435 in all algorithms .",
    "we warm - started the ` cplex ` and ` ipopt ` solvers at the iteration @xmath10 at the point given by the previous iteration @xmath436 for @xmath437 .",
    "the accuracy levels @xmath267 in ` cplex ` and ` ipopt ` and @xmath315 were updated as in algorithms [ alg : a1 ] and [ alg : a2 ] starting from @xmath438 and then set to @xmath439 . in other algorithms , this accuracy level was fixed at @xmath440 .",
    "we concluded that `` the algorithm is failed '' if either the maximum number of iterations ` maxiter ` was reached or the primal subproblems could not be solved by ` ipopt ` or ` cplex ` due to numerical issues .",
    "we benchmarked all algorithms with performance profiles @xcite .",
    "recall that a performance profile is built based on a set @xmath441 of @xmath442 algorithms ( solvers ) and a collection @xmath443 of @xmath444 problems .",
    "suppose that we build a profile based on computational time .",
    "we denote by @xmath445 .",
    "we compare the performance of algorithm @xmath446 on problem @xmath447 with the best performance of any algorithm on this problem ; that is we compute the performance ratio @xmath448 .",
    "now , let @xmath449 for @xmath450 .",
    "the function @xmath451 $ ] is the probability for solver @xmath446 that a performance ratio is within a factor @xmath452 of the best possible ratio .",
    "we use the term `` performance profile '' for the distribution function @xmath453 of a performance metric . in the following numerical examples",
    ", we plotted the performance profiles in @xmath454-scale , i.e. @xmath455 .    0.1 cm * 6.1 . basic pursuit problem . *",
    "the basic pursuit problem is one of the fundamental problems in signal processing and compressive sensing .",
    "mathematically , this problem can be formulated as follows : @xmath456 where @xmath17 and @xmath457 are given .",
    "since @xmath458 , the primal subproblem formed from in the algorithms can be expressed as @xmath459 this problem can be solved in a closed form without any subiteration .",
    "we implemented algorithms [ alg : a1 ] and [ alg : a2 ] to solve this problem in order to compare the effect of the parameter @xmath320 on the performance of the algorithm .",
    "the data of this problem is generated as follows .",
    "matrix @xmath176 is generated randomly such that it is orthogonal .",
    "vector @xmath460 , where @xmath461 is a @xmath10-sparse random vector ( @xmath462 ) .",
    "we tested algorithms [ alg : a1 ] and [ alg : a2 ] with @xmath424 problems and the results reported by these algorithms are presented in table [ tb : basic_lasso ] with @xmath463 and @xmath464 .",
    "1054 ) +   + ` algorithm 1 ` & 8254/1.3858 & 5090/0.8769 & 10144/2.2876 & 29773/7.8386 & 35615/10.9315 + ` algorithm 2 ` & 4836/0.9025 & 4744/0.8808 & 8060/1.9809 & 13220/3.7619 & 12348/3.7967 +   + ` algorithm 1 ` & 7115/1.1851 & 6644/1.1632 & 8749/1.9632 & 14927/4.0424 & 16128/4.9169 + ` algorithm 2 ` & 15016/2.6689 & 14284/2.6121 & 17140/4.0286 & 34048/9.6910 & 36668/11.1801 +    as we can see from this table that algorithm [ alg : a2 ] performs better than algorithm [ alg : a1 ] in terms of number of iterations as well as computational time for the case @xmath463 . in the case",
    "@xmath464 , algorithm [ alg : a1 ] performs better than algorithm [ alg : a2 ] .",
    "this example claims the theoretical results .",
    "0.1 cm * 6.2 .",
    "nonsmooth separable convex optimization .",
    "* let us consider the following simple nonsmooth convex optimization problem : @xmath465 where @xmath466 are given @xmath467 .",
    "let us assume that @xmath468 $ ] is a given interval in @xmath469 .",
    "then , this problem can be formulated in the form of [ eq : primal_cvx_prob ] with @xmath470 . since the lagrange function @xmath471 $ ] is nonsmooth , where @xmath472 is a lagrange multiplier , we choose @xmath473 such that the primal subproblem can be written as @xmath474}\\!\\!\\!\\!\\big\\ { i{|x_i \\!-\\ ! x_i^a| } \\!+\\ !",
    "y\\big(x_i \\!-\\ !",
    "\\frac{b}{n}\\big ) \\!+\\ ! \\frac{\\beta_1}{2}{|x_i \\!-\\",
    "! x_i^c|}^2 \\!+\\ ! 0.75d_{x_i}\\big\\},\\ ] ] where @xmath80 .",
    "now , we assume that we can choose the interval @xmath475 $ ] sufficiently large such that the constraint @xmath476 $ ] is inactive .",
    "then , the solution of problem can be computed explicitly as @xmath477 , where the _ soft - thresholding - type operator _ @xmath478 is defined as follows : @xmath479 .",
    "\\end{cases}\\ ] ] in this example , we tested five algorithms : algorithm [ alg : a1 ] , algorithm [ alg : a2 ] , ( * ? ? ?",
    "* algorithm 1 ) , ( * ? ? ?",
    "* algorithm 2 ) and ` pcbdm ` for @xmath480 problems with the size varying from @xmath481 to @xmath482 .",
    "note that if we reformulate as a linear programming problem ( lp ) by introducing slack variables , then the resulting lp problem has @xmath483 variables and @xmath484 inequality constraints .",
    "the data of these tests were created as follows .",
    "the value @xmath485 was set to @xmath486 , @xmath487 , where @xmath488 .",
    "the maximum number of iterations ` maxiter ` was increased to @xmath489 instead of @xmath490 .",
    "the performance of the five algorithms is reported in table [ tb : exam1_results ] .",
    ".performance comparison of five algorithms for solving [ cols=\"^,^ , > , > , > , > , > , > , > , > , > , > \" , ]     here , ` iters ` is the number of iterations and ` time ` is the cpu time in seconds .",
    "as we can see from table [ tb : exam1_results ] , algorithm [ alg : a1 ] is the best in terms of number of iterations and computational time .",
    "algorithm [ alg : a2 ] works better than ` pddecompalg ` .",
    "the first four algorithms have consistently outperformed ` pcbdm ` in terms of number of iterations as well as computational time in this example .",
    "0.1 cm * 6.3 .",
    "separable convex quadratic programming .",
    "* let us consider a separable convex quadratic program of the form : @xmath491 here @xmath492 is a symmetric positive semidefinite matrix , @xmath493 , @xmath494 for @xmath495 and @xmath457 . in this example",
    ", we compared the above algorithms by building their performance profiles in terms of number of iterations and the total computational time .    0.1 cm _ problem generation . _",
    "the input data of the test was generated as follows .",
    "matrix @xmath496 , where @xmath497 is an @xmath498 random matrix in @xmath499 $ ] with @xmath500 .",
    "matrix @xmath32 was generated randomly in @xmath501 $ ] .",
    "vector @xmath502 , where @xmath503 is a given feasible point in @xmath504 and vector @xmath505 .",
    "the density of both matrices @xmath32 and @xmath497 is @xmath506 .",
    "note that the problems generated as above are always feasible .",
    "moreover , they are not strongly convex .",
    "the tested collection consisted of @xmath507 problems with different sizes and the sizes were generated randomly as follows :    * _ class 1 : _ @xmath508 problems with @xmath509 , @xmath510 , @xmath511 and @xmath512 .",
    "* _ class 2 : _ @xmath508 problems with @xmath513 , @xmath514 , @xmath515 and @xmath516 . * _ class 3 : _ @xmath480 problems with @xmath517 , @xmath518 , @xmath519 and @xmath520 .    0.1 cm _ scenarios .",
    "_ we considered two different scenarios : _ scenario i : _ in this scenario , we aimed at comparing algorithms [ alg : a1 ] and [ alg : a2 ] , ` 2pdecompalg ` , ` pddecompalg ` , ` admm - v1 ` and ` epbdm ` , where we generated the values of @xmath521 relatively small .",
    "more precisely , we chose @xmath522 = [ -0.1 , 0.1]$ ] , @xmath501 = [ -1 , 1]$ ] and @xmath523 . _",
    "scenario ii : _ the second scenario aimed at testing the affect of the matrix @xmath176 and the update rule of the penalty parameter to the performance of ` admm ` .",
    "we chose @xmath522 = [ -1 , 1]$ ] , @xmath501 = [ -5 , 5]$ ] and @xmath524 .",
    "0.1 cm _ results .",
    "_ in the first scenario , the size of the problems satisfied @xmath525 , @xmath526 and @xmath527 .",
    "the performance profiles of the six algorithms are plotted in figure [ fig : perf_qp_exam02 ] with respect to the number of iterations and computational time .",
    "scale for scenario i by using ` ipopt ` : left - number of iterations , right - computational time.,width=464,height=143 ]    -0.2 cm    from these performance profiles , we can observe that the algorithm [ alg : a1 ] , algorithm [ alg : a2 ] , + ` 2pdecompalg ` and ` pddecompalg ` converged for all problems . `",
    "admm - v1 ` was successful in solving @xmath528 ( @xmath529 ) problems while ` epbdm ` could only solve @xmath530 @xmath531 problems .",
    "it shows that algorithm [ alg : a1 ] is the best one in terms of number of iterations . it could solve up to @xmath532 ( @xmath533 ) problems with the best performance . `",
    "admm - v1 ` solved @xmath534 ( @xmath535 ) problems with the best performance , while this ratio was only @xmath536 ( @xmath537 ) and @xmath538 ( @xmath539 ) in ` pddecompalg ` and algorithm [ alg : a2 ] , respectively .",
    "if we compare the computational time then algorithm [ alg : a1 ] is the best one .",
    "it could solve up to @xmath540 ( @xmath541 ) problems with the best performance . `",
    "admm - v1 ` solved @xmath542 ( @xmath543 ) problems with the best performance .",
    "since the performance of algorithms [ alg : a1 ] and [ alg : a2 ] , ` 2pdecompalg ` , ` pddecompalg ` and ` admm ` are relatively comparable , we tested algorithms [ alg : a1 ] and [ alg : a2 ] , ` 2pdecompalg ` , ` pddecompalg ` , ` admm - v1 ` , ` admm - v2 ` and ` admm - v3 ` on a collection of @xmath507 problems in the second scenario .",
    "the performance profiles of these algorithms are shown in figure [ fig : perf_qp_exam01 ] .",
    "-0.2 cm     scale for scenario ii by using ` cplex ` with simplex method : left - number of iterations , right - computational time.,width=464,height=143 ]    -0.5 cm    from these performance profiles we can observe the following :    * the six first algorithms were successful in solving all problems , while ` admm - v3 ` could only solve @xmath544 ( @xmath545 ) problems .",
    "* algorithm [ alg : a1 ] and ` admm - v1 ` is the best one in terms of number of iterations .",
    "it both solved @xmath546 ( @xmath547 ) problems with the best performance .",
    "this ratio is @xmath548 @xmath549 in ` admm - v2 ` .",
    "* algorithm [ alg : a1 ] is the best one in terms of computational time",
    ". it could solve @xmath550 @xmath551 the problems with the best performance , while this quantity is @xmath552 in ` admm - v2 ` .",
    "0.1 cm * 6.4 .",
    "nonlinear smooth separable convex programming .",
    "* we consider the following nonlinear , smooth and separable convex programming problem : @xmath553 here , @xmath554 is a positive semidefinite and @xmath555 is given vector , @xmath22 .    0.1 cm _ problem generation . _ in this example , we generated a collection of @xmath507 test problems as follows .",
    "matrix @xmath554 is diagonal and was generated randomly in @xmath522 $ ] .",
    "matrix @xmath32 was generated randomly in @xmath501 $ ] with the density @xmath556 .",
    "vectors @xmath557 and @xmath558 were generated randomly in @xmath559 $ ] and @xmath560 $ ] , respectively , such that @xmath561 and @xmath562 .",
    "vector @xmath563 for a given @xmath564 in @xmath565 $ ] .",
    "the size of the problems was generated randomly based on the following rules :    * _ class 1 : _ @xmath480 problems with @xmath566 , @xmath567 , @xmath515 and @xmath568 .",
    "* _ class 2 : _ @xmath480 problems with @xmath569 , @xmath570 , @xmath571 and @xmath512 . * _ class 3 : _ @xmath480 problems with @xmath572 , @xmath573 , @xmath574 and @xmath516 . * _ class 4 : _ @xmath480 problems with @xmath575 , @xmath518 , @xmath574 and @xmath520 .",
    "* _ class 5 : _ @xmath480 problems with @xmath576 , @xmath518 , @xmath574 and @xmath577 .",
    "0.1 cm _ scenarios .",
    "_ we also considered two different scenarios as in the previous example : _ scenario i : _ similar to the previous example , with this scenario , we aimed at comparing algorithms [ alg : a1 ] and [ alg : a2 ] , ` 2pdecompalg ` , ` pddecompalg ` , ` admm - v1 ` , ` pcbdm ` and ` epbdm ` . in this scenario",
    ", we chose : @xmath522 \\equiv [ -0.01 , 0.01]$ ] , @xmath559 \\equiv [ 0 , 100]$ ] , @xmath501 \\equiv [ -1 , 1]$ ] and @xmath578 . _",
    "scenario ii : _ in this scenario , we only tested two first variants of ` admm ` and compared them with the four first algorithms . here , we chose @xmath522 \\equiv [ 0.0 , 0.0]$ ] ( i.e. without quadratic term ) , @xmath559 \\equiv [ 0 , 100]$ ] , @xmath501 \\equiv [ -1 , 1]$ ] and @xmath579 .",
    "0.1 cm _ results . _ for _ scenario i _ , we see that the size of the problems is in @xmath580 , @xmath581 and @xmath582 .",
    "the performance profiles of the algorithms are plotted in figure [ fig : perf_scp2 ] .",
    "-0.2 cm     scale by using ` ipopt ` : left - number of iterations , right - computational time.,width=464,height=143 ]    -0.3 cm    the results on this collection shows that algorithm [ alg : a1 ] is the best one in terms of number of iterations",
    ". it could solve up to @xmath583 ( @xmath584 ) problems with the best performance , while ` admm - v1 ` solved @xmath534 ( @xmath585 ) problems with the best performance .",
    "algorithm [ alg : a1 ] is also the best one in terms of computational time .",
    "it could solve @xmath586 ( @xmath587 ) problems with the best performance . `",
    "pcbdm ` was very slow compared to the rest in this scenario",
    ".    for _ scenario ii _ , the size of the problems was varying in @xmath588 , @xmath589 and @xmath590 .",
    "the performance profiles of the tested algorithms are plotted in figure [ fig : perf_scp1 ] .",
    "-0.0 cm     scale for ` scenario i ` by using ` ipopt ` : left - number of iterations , right - computational time.,width=464,height=143 ]    -0.3 cm    we can see from these performance profiles that algorithm [ alg : a1 ] is the best one in terms of number of iterations .",
    "it could solve up to @xmath591 ( @xmath592 ) problems with the best performance , while this number were @xmath593 ( @xmath594 ) and @xmath595 ( @xmath596 ) problems in ` 2pdecompalg ` and ` admm - v1 ` , respectively .",
    "algorithm [ alg : a1 ] was also the best one in terms of computational time .",
    "it solved all problems with the best performance . `",
    "admm - v2 ` was slow compared to the rest in this scenario .    from the above two numerical tests",
    ", we can observe that algorithm [ alg : a1 ] performs well compared to the rest in terms of computational time due to its low cost per iteration .",
    "admm encounters some difficulty regarding the choice of the penalty parameter as well as the effect of matrix @xmath176 .",
    "theoretically , pcbdm has the same worst - case complexity bound as algorithms [ alg : a1 ] and .",
    "however , its performance is quite poor .",
    "this happens due to the choice of the lipschitz constant @xmath217 of the gradient of the dual function and the evaluation of the quantity @xmath597 .",
    "we have proposed a new decomposition algorithm based on the dual decomposition and excessive gap techniques . the new algorithm requires to perform only one primal step which can be parallelized efficiently , and two dual steps .",
    "consequently , the computational complexity of this algorithm is very similar to other dual based decomposition algorithms from the literature , but with a better theoretical rate of convergence .",
    "moreover , the algorithm automatically updates both smoothness parameters at each iteration .",
    "we notice that the dual steps are only matrix - vector multiplications , which can be done efficiently with a low computational cost in practice .",
    "furthermore , we allow one to solve the primal convex subproblem of each component up to a given accuracy , which is always the case in any practical implementation . an inexact switching variant of algorithm [ alg : a1 ] has also been presented .",
    "apart from the inexactness , this variant allows one to simultaneously update both smoothness parameters instead of switching them .",
    "moreover , it improves the disadvantage of algorithm [ alg : a1 ] when the constant @xmath320 in theorem [ th : alg_scheme ] is relatively small , though it did not outperform algorithm [ alg : a1 ] in the numerical tests .",
    "the worst - case complexity of both new algorithms is at most @xmath324 for a given tolerance @xmath598 .",
    "preliminary numerical tests show that both algorithms outperforms other related existing algorithms from the literature",
    ".    0.2 cm    * acknowledgments .",
    "* this research was supported by research council kul : coe ef/05/006 optimization in engineering(optec ) , goa ambiorics , iof - scores4chem , several phd / postdoc & fellow grants ; the flemish government via fwo : phd / postdoc grants , projects g.0452.04 , g.0499.04 , g.0211.05 , g.0226.06 , g.0321.06 , g.0302.07 , g.0320.08 ( convex mpc ) , g.0558.08 ( robust mhe ) , g.0557.08 , g.0588.09 , research communities ( iccos , anmmm , mldm ) and via iwt : phd grants , mcknow - e , eureka - flite+eu : ernsi ; fp7-hd - mpc ( collaborative project strep - grantnr .",
    "223854 ) , contract research : aminal , highwind , and helmholtz gemeinschaft : vicerp ; austria : accm , and the belgian federal science policy office : iuap p6/04 ( dysco , dynamical systems , control and optimization , 2007 - 2011 ) , european union fp7-embocon under grant agreement no 248940 ; cncs - uefiscdi ( project te231 , no .",
    "19/11.08.2010 ) ; ancs ( project pn ii , no .",
    "80eu/2010 ) ; sectoral operational programme human resources development 2007 - 2013 of the romanian ministry of labor , family and social protection through the financial agreement posdru/89/1.5/s/62557 .",
    "in this appendix we provide the full proof of theorem [ th : alg_scheme ] , theorem [ th : alg_scheme2 ] , lemma [ le : init_point ] and lemma [ le : choice_of_tau ] .",
    "0.2 cm _ a.1 .",
    "the proof of theorem [ th : alg_scheme ] .",
    "_ let us denote by @xmath599 , @xmath600 and @xmath601 . from the definition of @xmath26 , the second line of and",
    ", we have @xmath602^ty \\!-\\ !",
    "\\frac{\\beta_2^{+}}{2}{\\|y\\|}^2\\right\\ } \\nonumber\\\\ & \\overset{\\tiny{\\phi-\\mathrm{convex}+\\eqref{eq : beta_update}}}{\\leq } \\max_{y\\in\\mathbb{r}^m}\\big\\ { \\ ! ( 1 \\!-\\ ! \\tau)\\big[\\phi(\\bar{x } ) \\!+\\ ! ( a\\bar{x } \\!-\\",
    "! b)^ty \\!-\\",
    "! \\frac{\\beta_2}{2}{\\|y\\|}^2\\big]_{[1 ] }   +    \\tau\\left[\\phi ( \\tilde{x}^1 ) \\!+\\ !",
    "( a\\tilde{x}^1 \\!-\\ ! b)^ty \\right]_{[2 ] } \\big\\}.\\end{aligned}\\ ] ] now , we estimate two terms in the last line of .",
    "first we note that @xmath603 for any vectors @xmath604 and @xmath37 and @xmath605 .",
    "moreover , since @xmath606 is the solution of the strongly concave maximization with a concavity parameter @xmath191 , we can estimate @xmath607_{[1 ] } & & : = \\phi(\\bar{x } ) + ( a\\bar{x } - b)^ty - \\frac{\\beta_2}{2}{\\|y\\|}^2 = \\phi(\\bar{x } ) + \\frac{1}{2\\beta_2}{\\|a\\bar{x } - b\\|}^2   - \\frac{\\beta_2}{2}{\\|y - \\bar{y}^2\\|}^2 \\nonumber\\\\ & & = \\phi(\\bar{x } ) + \\psi(\\bar{x};\\beta_2 ) - \\frac{\\beta_2}{2}{\\|y - \\bar{y}^2\\|}^2 \\overset{\\tiny{\\eqref{eq : smoothed_phi}}}{= } f(\\bar{x } ; \\beta_2 ) - \\frac{\\beta_2}{2}{\\|y - \\bar{y}^2\\|}^2 \\\\ & & \\overset{\\tiny{\\eqref{eq : excessive_gap_inexact}}}{\\leq } g(\\bar{y } ; \\beta_1 ) - \\frac{\\beta_2}{2}{\\|y - \\bar{y}^2\\|}^2 + \\delta \\overset{\\tiny{g(\\cdot;\\beta_1)-\\mathrm{concave}}}{\\leq } g(\\hat{y } ; \\beta_1 ) + \\nabla_yg(\\hat{y } ; \\beta_1)^t(\\bar{y } - \\hat{y } ) - \\frac{\\beta_2}{2}{\\|y - \\bar{y}^2\\|}^2 + \\delta \\nonumber\\\\ & & \\overset{\\eqref{eq : app_deriv_g}}{\\leq } { \\!\\ ! } g(\\hat{y } ; \\beta_1 ) \\!+\\ ! \\widetilde{\\nabla}_yg(\\hat{y } ; \\beta_1)^t(\\bar{y } \\!-\\ ! \\hat{y})\\!-\\ ! \\frac{\\beta_2}{2}{\\|y \\!-\\ !",
    "\\bar{y}^2\\|}^2 \\!+\\ ! ( \\bar{y } \\!-\\ !",
    "\\hat{y})^ta(x^1 \\!-\\ !",
    "\\tilde{x}^1 ) \\!+\\ ! \\delta . \\nonumber\\end{aligned}\\ ] ] alternatively , by using , the second term @xmath608_{[2]}$ ] can be estimated as @xmath609_{[2 ] } & & : = \\phi(\\tilde{x}^1 ) + ( a\\tilde{x}^1-b)^ty \\nonumber\\\\ & & = \\phi(\\tilde{x}^1 ) + ( a\\tilde{x}^1 - b)^t\\hat{y } + \\beta_1p_x(\\tilde{x}^1 ) + ( a\\tilde{x}^1 - b)^t(y - \\hat{y } ) - \\beta_1p_x(\\tilde{x}^1 ) \\nonumber\\\\ [ -1.6ex]\\\\[-1.6ex ] & & \\overset{\\tiny\\eqref{eq : inexactness_xi}}{\\leq}{\\!\\!\\ ! }",
    "\\phi(x^1 ) \\!+\\ ! ( ax^1 \\!-\\ ! b)^t\\hat{y } \\!+\\ !",
    "\\beta_1p_x(x^1 ) \\!+\\ ! ( a\\tilde{x}^1 \\!-\\ ! b)^t\\!(y \\!-\\ ! \\hat{y } ) \\!-\\ !",
    "\\beta_1p_x(\\tilde{x}^1 ) \\!+\\ ! \\frac{\\beta_1}{2}\\varepsilon_{[\\sigma]}^2 \\nonumber\\\\ & & \\overset{\\tiny{\\eqref{eq : smoothed_gi}+\\mathrm{lemma}~\\ref{le : dual_smoothed_estimates}}}{= }   g(\\hat{y } ; \\beta_1 ) \\!+\\ ! \\widetilde{\\nabla}_yg(\\hat{y } ; \\beta_1)^t(y \\!-\\ !",
    "\\hat{y } )   \\!-\\ !",
    "\\beta_1p_x(\\tilde{x}^1 ) \\!+\\ ! \\frac{\\beta_1}{2}\\varepsilon_{[\\sigma]}^2 . \\nonumber\\end{aligned}\\ ] ] next , we consider the point @xmath610 with @xmath219 . on the one hand , it is easy to see that if @xmath94 then @xmath611 .",
    "moreover , we have @xmath612 on the other hand , it follows from that @xmath613 by substituting and into and then using and , we conclude that @xmath614_{[1 ] } + \\tau [ \\cdot]_{[2 ] } \\right\\ }   \\nonumber\\\\ & & \\overset{\\tiny{\\eqref{eq : th31_proof2}+\\eqref{eq : th31_proof3}}}{\\leq } \\max_{y\\in\\mathbb{r}^m}\\big\\{(1-\\tau)g(\\hat{y};\\beta_1 ) + \\tau g(\\hat{y};\\beta_1 )   + \\widetilde{\\nabla}_yg(\\hat{y};\\beta_1)^t\\left[(1-\\tau)(\\bar{y}-\\hat{y } ) + \\tau(y-\\hat{y})\\right ] \\nonumber\\\\ & & - \\frac{(1-\\tau)\\beta_2}{2}{\\|y-\\bar{y}^2\\|}^2\\big\\ } - \\tau\\beta_1p_x(\\tilde{x}^1 ) + 0.5\\tau\\beta_1\\varepsilon_{[\\sigma]}^2 + ( 1-\\tau)\\delta + ( 1-\\tau)(\\bar{y}-\\hat{y})^ta(x^1-\\tilde{x}^1 ) \\nonumber\\\\ & & { \\!\\!\\!\\!\\!\\!\\!\\!\\!}\\overset{\\tiny{\\eqref{eq : th31_proof4}}}{=}{\\!\\!\\!\\ ! } \\left[\\max_{u\\in\\mathbb{r}^m}\\left\\ { g(\\hat{y};\\beta_1 ) + \\widetilde{\\nabla}_yg(\\hat{y};\\beta_1)^t(u - \\hat{y } ) - \\frac{(1-\\tau)\\beta_2}{2\\tau^2}{\\|u-\\hat{y}\\|}^2\\right\\ } \\right]_{[3]}\\nonumber\\\\ & & + \\left[0.5\\tau\\beta_1\\varepsilon_{[\\sigma]}^2 + ( 1-\\tau)\\delta + ( 1-\\tau)(\\bar{y}-\\hat{y})^ta(x^1-\\tilde{x}^1 ) - \\tau\\beta_1p_x(\\tilde{x}^1)\\right]_{[4]}.\\end{aligned}\\ ] ] let us consider the first term @xmath608_{[3]}$ ] of . we see that @xmath615_{[3 ] } & & = \\max_{u\\in\\mathbb{r}^m}\\left\\ { g(\\hat{y};\\beta_1 ) + \\widetilde{\\nabla}_yg(\\hat{y};\\beta_1)^t(u - \\hat{y } ) - \\frac{(1-\\tau)\\beta_2}{2\\tau^2}{\\|u-\\hat{y}\\|}^2\\right\\ } \\nonumber\\\\ & & \\overset{\\tiny{\\eqref{eq : th31_proof5}}}{\\leq}{\\!\\!\\!\\!\\ ! }   \\max_{u\\in\\mathbb{r}^m}\\left\\ { g(\\hat{y};\\beta_1 ) + \\widetilde{\\nabla}_yg(\\hat{y};\\beta_1)^t(u - \\hat{y } ) - \\frac{l^g(\\beta_1)}{2}{\\|u-\\hat{y}\\|}^2\\right\\}\\nonumber\\\\ & & { \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}\\overset{\\tiny{\\eqref{eq : gradient_mapping_ex}+\\eqref{eq : alg_scheme}(\\mathrm{line}~3)}}{=}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\ ! } g(\\hat{y } ; \\beta_1 ) + \\widetilde{\\nabla}_yg(\\hat{y};\\beta_1)^t(\\bar{y}^{+ } - \\hat{y } ) - \\frac{l^g(\\beta_1)}{2}{\\|\\bar{y}^{+ } - \\hat{y}\\|}^2\\nonumber\\\\ & & \\overset{\\tiny{\\eqref{eq : app_deriv_g_est}}}{= } { \\!\\!\\!\\ ! }",
    "g(\\hat{y};\\beta_1 ) \\!+\\ ! \\nabla_yg(\\hat{y};\\beta_1)^t\\!\\ ! ( \\bar{y}^{+ } \\!\\!-\\ !",
    "\\hat{y } ) \\!-\\ !",
    "\\frac{l^g(\\beta_1)}{2}{\\|\\bar{y}^{+ } \\!\\!-\\",
    "! \\hat{y}\\|}^2 \\!+\\ ! ( \\bar{y}^{+ } \\!-\\",
    "! \\hat{y})^t\\!\\!a(\\tilde{x}^1 \\!-\\ !",
    "x^1 ) \\\\ & & \\overset{\\tiny\\eqref{eq : g_lipschitz_bound}}{\\leq } g(\\bar{y}^{+ } ; \\beta_1 ) + ( \\bar{y}^{+ } - \\hat{y})^ta(x^1-\\tilde{x}^1)\\nonumber\\\\ & & \\overset{\\tiny\\eqref{eq : g_beta1_lipschitz_bound}}{\\leq } g(\\bar{y}^{+ } ; \\beta_1^{+ } ) + ( \\beta_1-\\beta_1^{+})p_x(x^{*}(\\bar{y}^{+};\\beta_1^{+ } ) ) + ( \\bar{y}^{+ } - \\hat{y})^ta(x^1-\\tilde{x}^1)\\nonumber\\\\ & & \\overset{\\tiny\\eqref{eq : beta_update}+\\eqref{eq : d_x_p_x}}{\\leq } g(\\bar{y}^{+ } ; \\beta_1^{+ } ) + \\left[\\tilde{\\alpha}\\tau\\beta_1d_x + ( \\bar{y}^{+ } - \\hat{y})^ta(x^1-\\tilde{x}^1)\\right]_{[5]}\\nonumber.\\end{aligned}\\ ] ] in order to estimate the term @xmath608_{[4 ] } + [ \\cdot]_{[5]}$ ] , we can observe that @xmath616 which leads to @xmath617 & \\leq l_{\\mathrm{a}}^{-1}\\beta_1{\\|a\\|}^2{\\|\\tilde{x}^1-x^c\\| } + l_{\\mathrm{a}}^{-1}\\beta_1{\\|a^t(ax^c - b)\\| } + \\frac{(1-\\tau)\\tau}{\\beta_2}{\\|a\\|}^2{\\|\\bar{x}-x^c\\| } \\nonumber\\\\ & + \\beta_2^{-1}(1-\\tau)\\tau{\\|a^t(ax^c - b)\\| } + ( 1-\\tau)\\tau{\\|a\\|}{\\|\\bar{y}\\| } .\\end{aligned}\\ ] ] note that similar to , we have @xmath618 and @xmath619 . by substituting these estimates into and using the definitions of @xmath608_{[4]}$ ] and @xmath608_{[5]}$ ] we have @xmath620_{[4 ] } + [ \\cdot]_{[5 ] } \\leq ( 1-\\tau)\\delta + \\frac{\\tau\\beta_1}{2}\\varepsilon_{[\\sigma]}^2 + \\tau\\beta_1(\\tilde{\\alpha } d_x - p_x(\\tilde{x}^1 ) ) + \\left[\\frac{\\beta_1}{l_{\\mathrm{a}}}c_d + ( 1-\\tau)\\tau\\left(\\frac{c_d}{\\beta_2 } + { \\|a\\|}{\\|\\bar{y}\\|}\\right)\\right]\\varepsilon_{[1]}.\\ ] ] by combining , and and noting that @xmath621",
    ", we obtain @xmath622 which is indeed inequality w.r.t .",
    "@xmath199 , @xmath200 and @xmath201 .",
    "@xmath108 0.2 cm _ a.2 .",
    "the proof of theorem [ th : alg_scheme2 ] .",
    "_ let us denote by @xmath623 , @xmath624 , @xmath625 , @xmath626 and @xmath627 . from the definition of @xmath86 , the second line of and @xmath628 we have @xmath629_{[1 ] }",
    "+ \\tau \\left [ \\phi(x ) + ( y^2_{+})^t(ax - b)\\right]_{[2]}\\big\\}.\\end{aligned}\\ ] ] first , we estimate the term @xmath608_{[1]}$ ] in .",
    "since each component of the function in @xmath608_{[1]}$ ] is strongly convex w.r.t .",
    "@xmath20 with a convexity parameter @xmath139 for @xmath22 , by using the optimality condition , one can show that @xmath630_{[1 ] }   & & \\overset{\\tiny{\\eqref{eq : smooth_dual_sol}}}{\\geq }   \\min_{x\\in x}\\left\\{\\phi(x ) + \\bar{y}^t(ax - b ) + \\beta_1p_x(x)\\right\\ } + \\frac{\\beta_1}{2}{\\|x - x^1\\|}_{\\sigma}^2 \\overset{\\tiny\\eqref{eq : smoothed_gi}}{= } g(\\bar{y } ; \\beta_1 ) + \\frac{\\beta_1}{2}{\\|x - x^1\\|}_{\\sigma}^2 \\nonumber\\\\ [ -1.8ex]\\\\[-1.8ex ] & & \\overset{\\tiny{\\eqref{eq : excessive_gap_inexact}}}{\\geq } f(\\bar{x } ; \\beta_2 ) + \\frac{\\beta_1}{2}{\\|x - x^1\\|}_{\\sigma}^2 - \\delta \\nonumber.\\end{aligned}\\ ] ] moreover , since @xmath631 , by substituting this relation into we obtain @xmath632_{[1 ] } & & \\geq \\phi(\\bar{x } ) + \\psi(\\bar{x } ; \\beta_2 ) + \\frac{\\beta_1}{2}{\\|x - x^1\\|}_{\\sigma}^2   - \\delta \\nonumber\\\\ & & =   \\phi(\\bar{x } ) + \\psi(\\bar{x } ; \\beta_2^{+ } ) - \\tau\\psi(\\bar{x};\\beta_2^{+ } ) + \\frac{\\beta_1}{2}{\\|x - x^1\\|}_{\\sigma}^2",
    "- \\delta \\\\ & & { \\!\\!\\!\\!\\!}\\overset{\\tiny\\mathrm{def.}~\\psi}{\\geq } \\phi(\\bar{x } ) \\!+\\ ! \\psi(x^2 ; \\beta_2^{+ } ) \\!+\\ ! \\nabla_x{\\psi}(x^2 ; \\beta_2^{+})^t(\\bar{x } \\!-\\ !",
    "x^2 ) \\!+\\ ! \\frac{\\beta_1}{2}{\\|x \\!-\\ ! x^1\\|}_{\\sigma}^2 \\!-\\ ! \\delta + \\frac{1}{2\\beta_2^{+}}{\\|a(\\bar{x}-x^2)\\|}^2 - \\tau\\psi(\\bar{x};\\beta_2^{+ } ) .",
    "\\nonumber\\end{aligned}\\ ] ] here , the last inequality follows from the fact that @xmath633 .",
    "next , we consider the term @xmath608_{[2]}$ ] of .",
    "we note that @xmath634 .",
    "hence , @xmath635_{[2 ] } & & = \\phi(x ) + ( y^2_{+})^ta(x - x^2 ) + ( ax^2 - b)^ty^2_{+ } \\nonumber\\\\ & & \\overset{\\tiny\\mathrm{lemma}~\\ref{le : primal_smoothed_estimates}}{= } \\phi(x ) + \\nabla_x{\\psi}(x^2 ; \\beta_2^{+})^t(x - x^2 ) + \\frac{1}{\\beta_2^{+}}{\\|ax^2-b\\|}^2\\\\ & & = \\phi(x ) + \\psi(x^2;\\beta_2^{+ } ) + \\nabla_x{\\psi}(x^2;\\beta_2^{+})^t(x - x^2 ) + \\psi(x^2 ; \\beta_2^{+}).\\nonumber\\end{aligned}\\ ] ] from the definitions of @xmath636 , @xmath384 and @xmath182}$ ] we have @xmath637 , @xmath638 and @xmath639}$ ] .",
    "moreover , @xmath640 . by using these estimates",
    ", we can derive @xmath641 ^ 2 \\nonumber\\\\ & & = { \\|x-\\tilde{x}^1\\|}_{\\sigma}^2 - 2{\\|x-\\tilde{x}^1\\|}_{\\sigma}{\\|x^1-\\tilde{x}^1\\|}_{\\sigma } + { \\|x^1-\\tilde{x}^1\\|}_{\\sigma}^2 \\nonumber\\\\ [ -1.5ex]\\\\[-1.5ex ] & & \\geq { \\|x-\\tilde{x}^1\\|}_{\\sigma}^2 - 2{\\|x^1-\\tilde{x}^1\\|}_{\\sigma}\\left[{\\|x - x^c\\|}_{\\sigma } + { \\|\\tilde{x}^1 - x^c\\|}_{\\sigma}\\right]\\nonumber\\\\ & & \\geq { \\|x-\\tilde{x}^1\\|}_{\\sigma}^2 - 4d_{\\sigma}\\varepsilon_{[\\sigma]}.\\nonumber\\end{aligned}\\ ] ] furthermore , the condition can be expressed as @xmath642 by substituting , and into and then using and note that @xmath643 , we obtain @xmath644_{[1 ] } + \\tau [ \\cdot]_{[2]}\\right\\}\\nonumber\\\\ & & { \\!\\!\\!\\!}\\overset{\\tiny{\\eqref{eq : thm41_proof2}+\\eqref{eq : thm41_proof3}}}{\\geq}{\\!\\!\\!\\ ! } \\min_{x\\in x}\\big\\ { ( 1 \\!-\\ ! \\tau)\\phi(\\bar{x } ) \\!+\\ !",
    "\\tau\\phi(x ) \\!+\\ ! \\nabla{\\psi}(x^2;\\beta_2^{+})^t\\left[(1 \\!-\\ !",
    "\\tau)(\\bar{x } \\!-\\ !",
    "x^2 ) \\!+\\ ! \\tau(x \\!-\\ !",
    "x^2)\\right ] + \\frac{(1-\\tau)\\beta_1}{2}{\\|x - x^1\\|}^2_{\\sigma}\\big\\ } \\nonumber\\\\ & &   - ( 1-\\tau)\\delta   + \\left[\\tau\\psi(x^2;\\beta_2^{+ } ) - ( 1-\\tau)\\tau\\psi(\\bar{x};\\beta_2^{+ } ) + \\frac{(1-\\tau)}{2\\beta_2^{+}}{\\|a(\\bar{x}-x^2)\\|}^2\\right]_{[3]}.\\end{aligned}\\ ] ] we further estimate as follows @xmath645_{[3 ] } - ( 1-\\tau)\\delta -   2(1-\\tau)\\beta_1d_{\\sigma}\\varepsilon_{[\\sigma ] } \\nonumber\\\\ & \\overset{\\tiny\\eqref{eq : thm41_proof3b}}{\\geq } { \\!\\!\\!\\!\\!\\!\\ ! } \\min_{u : = ( 1 \\!-\\ !",
    "\\tau)\\bar{x } \\!+\\ ! \\tau x \\in x}{\\!\\!\\!}\\big\\{\\phi(u ) \\!+\\ ! \\psi(x^2;\\beta_2^{+ } ) \\!+\\ ! \\nabla{\\psi}(x^2;\\beta_2^{+})(u \\!-\\ !",
    "x^2 ) \\!+\\ ! \\frac{(1 \\!-\\ ! \\tau)\\beta_1}{2\\tau^2}{\\|u \\!-\\ !",
    "x^2\\|}^2_{\\sigma}\\big\\ } \\nonumber\\\\ & - 2(1-\\tau)\\beta_1d_{\\sigma}\\varepsilon_{[\\sigma ] }   - ( 1-\\tau)\\delta + [ \\cdot]_{[3]}\\nonumber\\\\ & { \\!\\!\\!\\!\\!\\!\\!\\!\\!}\\overset{\\tiny\\eqref{eq : alg_scheme_cond_p_tmp}}{\\geq}\\!\\!\\ ! \\min_{u \\in x}\\!\\big\\{\\ !",
    "\\phi(u ) \\!+\\ ! \\psi(\\hat{x};\\beta_2^{+ } ) \\!+\\ ! \\nabla{\\psi}(x^2;\\beta_2^{+})(u \\!-\\ !",
    "x^2 ) \\!+\\ ! \\frac{l^{\\psi}(\\beta_2^{+})}{2}{\\|u \\!-\\ ! x^2\\|}^2_{\\sigma}\\big\\ } \\!-\\ ! 2(1 \\!-\\",
    "! \\tau)\\beta_1d_{\\sigma}\\varepsilon_{[\\sigma ] }   \\!-\\ !",
    "( 1 \\!-\\ ! \\tau)\\delta \\!+\\ !",
    "[ \\cdot]_{[3]}\\nonumber\\\\ & = q(\\bar{x}^{*+},y^2_{+};\\beta_2^{+ } ) - 2(1-\\tau)\\beta_1d_{\\sigma}\\varepsilon_{[\\sigma ] }   - ( 1-\\tau)\\delta + [ \\cdot]_{[3]}\\nonumber\\\\ & \\overset{\\tiny\\eqref{eq : pi_approx}}{\\geq } q(\\bar{x}^{+ } , y^2_{+};\\beta_2^{+ } ) - 2(1-\\tau)\\beta_1d_{\\sigma}\\varepsilon_{[\\sigma ] }   - ( 1-\\tau)\\delta + [ \\cdot]_{[3 ] } - 0.5\\varepsilon_{a}^2\\nonumber\\\\ & \\overset{\\tiny{\\eqref{eq : psi_lipschitz_bound}}}{\\geq } f(\\bar{x}^{+ } ; \\beta_2^{+ } ) - 2(1-\\tau)\\beta_1d_{\\sigma}\\varepsilon_{[\\sigma ] }   - ( 1-\\tau)\\delta + [ \\cdot]_{[3 ] } - 0.5\\varepsilon_{a}^2,\\end{aligned}\\ ] ] where @xmath646^{1/2}$ ] .",
    "to complete the proof , we estimate @xmath608_{[3]}$ ] as follows @xmath647_{[3 ] } & & = \\tau\\psi(x^2;\\beta_2^{+ } ) - \\tau(1-\\tau)\\psi(\\bar{x};\\beta_2^{+ } ) + \\frac{(1-\\tau)}{2\\beta_2^{+}}{\\|a(\\bar{x}-x^2\\|}^2 \\nonumber\\\\ & & = \\frac{1}{2\\beta_2^{+}}\\left[\\tau{\\|ax^2 - b\\|}^2 - \\tau(1-\\tau){\\|a\\bar{x}-b\\|}^2 + ( 1-\\tau){\\|a(\\bar{x}-x^2)\\|}^2\\right]\\\\ & & = \\frac{1}{2\\beta_2^{+}}{\\|(ax^2-b ) + ( 1-\\tau)(a\\bar{x}-b)\\|}^2   \\geq 0.\\nonumber\\end{aligned}\\ ] ] by substituting into and using the definition of @xmath201 in we obtain @xmath648 where @xmath383 } + 0.5\\sum_{i=1}^ml_i^{\\psi}(\\beta_2^{+})\\varepsilon_i^2 = ( 1-\\tau)\\delta + \\xi(\\tau , \\beta_1 , \\beta_2 , \\varepsilon)$ ] .",
    "this is indeed with the inexactness @xmath201 .",
    "@xmath108 0.2 cm _ a.3 .",
    "the proof of lemma [ le : init_point ] .",
    "_ for simplicity of notation , we denote by @xmath649 and @xmath650 , @xmath651 , where @xmath652 is defined in definition [ de : inexactness ] . by using the inexactness in inequality and @xmath653 , we have @xmath654}$ ] which is rewritten as @xmath655}^2.\\end{aligned}\\ ] ] since @xmath87 is concave , by using the estimate and @xmath656 we have @xmath657}^2\\\\ & = \\phi(\\tilde{x}^ { * } ) + ( a\\tilde{x}^ { * } - b)^t\\bar{y}^0 - \\frac{l^g(\\beta_1)}{2}{\\|\\bar{y}^0\\|}^2 + ( \\bar{y}^0)^ta(x^ { * } -\\tilde{x}^ { * } ) + \\beta_1p_x(\\tilde{x}^ { * } ) - \\frac{\\beta_1}{2}\\varepsilon_{[\\sigma]}^2 : = t_0 \\nonumber.\\end{aligned}\\ ] ] since @xmath658}$ ] , @xmath659 and @xmath285 is the solution of , we estimate the last term @xmath660 of as @xmath661}^2 \\nonumber\\\\ ~&&~ \\overset{\\tiny\\eqref{eq : init_point_cond}+\\eqref{eq : accuracy_constants}}{\\geq } \\phi(\\tilde{x}^ { * } ) + \\max_{y\\in\\mathbb{r}^m}\\left\\{(a\\tilde{x}^ { * } - b)^ty - \\frac{\\beta_2}{2}{\\|y\\|}^2\\right\\ }   -{\\|a^t\\bar{y}^0\\|}\\varepsilon_{[1 ] } - \\frac{\\beta_1}{2}\\varepsilon_{[\\sigma]}^2 \\\\ ~&&~ \\overset{\\tiny\\eqref{eq : psi_approx}}{\\geq } f(\\bar{x}^0 ; \\beta_2 ) - \\left[{\\|a^t\\bar{y}^0\\|}\\varepsilon_{[1 ] } + \\frac{\\beta_1}{2}\\varepsilon_{[\\sigma]}^2\\right].\\nonumber\\end{aligned}\\ ] ] now , we see that @xmath662 . thus , @xmath663 for all @xmath22 . by using the definition of @xmath384 in ,",
    "the last inequalities imply @xmath664 finally , we note that @xmath665 due to .",
    "this relation leads to @xmath666 \\overset{\\tiny\\eqref{eq : lm31_proof4b}}{\\leq } l_{\\mathrm{a}}^{-1}\\beta_1\\left[{\\|a\\|}^2d_{\\sigma } + { \\|a^t(ax^c - b)\\|}\\right ] \\\\ & & \\overset{\\tiny\\eqref{eq : accuracy_constants}}{= } l_{\\mathrm{a}}^{-1}\\beta_1c_d.\\nonumber\\end{aligned}\\ ] ] by substituting and into and then using the definition of @xmath190 we obtain the conclusion of the lemma .",
    "@xmath108 0.2 cm _ a.4 .",
    "the proof of lemma [ le : choice_of_tau ] .",
    "_ let us consider the function @xmath667 , where @xmath668 $ ] and @xmath669 .",
    "after few simple calculations , we can estimate @xmath670 for all @xmath671 .",
    "these estimates lead to @xmath672 from the update rule we can show that the sequence @xmath232 satisfies @xmath673 . if we define @xmath674 , then @xmath675 .",
    "therefore , one can estimate @xmath676 for @xmath677 .",
    "note that @xmath678 by assumption and by induction we can show that @xmath679 for @xmath253 and @xmath680 .",
    "however , since @xmath681 , this leads to : @xmath682 which is indeed . here , @xmath683 and @xmath684^{-1}$ ] .",
    "next , we prove the bound on @xmath326 . since @xmath688",
    ", we have @xmath689 . by using the following elementary inequalities @xmath690 for all @xmath691",
    "$ ] , we obtain @xmath692 , where @xmath693 and @xmath694 . from , on the one hand , we have @xmath695 which leads to @xmath696 for some constant @xmath697 . on the other hand",
    ", we have @xmath698 converging to some constant @xmath699 . combining all estimates together",
    "we eventually get @xmath700 for some positive constant @xmath241 .",
    "m.  kojima , n.  megiddo , s.  mizuno , and et  al .",
    "horizontal and vertical decomposition in interior point methods for linear programs . technical report . , information sciences , tokyo institute of technology , tokyo , 1993 .",
    "q.  tran - dinh , c.  savorgnan , and m.  diehl . combining lagrangian decomposition and excessive gap smoothing technique for solving large - scale separable convex optimization problems .",
    ", ( in press):137 , 2012 ."
  ],
  "abstract_text": [
    "<S> in this paper we propose a new inexact dual decomposition algorithm for solving separable convex optimization problems . </S>",
    "<S> this algorithm is a combination of three techniques : dual lagrangian decomposition , smoothing and excessive gap . </S>",
    "<S> the algorithm requires only one primal step and two dual steps at each iteration and allows one to solve the subproblem of each component inexactly and in parallel . </S>",
    "<S> moreover , the algorithmic parameters are updated automatically without any tuning strategy as in augmented lagrangian approaches . </S>",
    "<S> we analyze the convergence of the algorithm and estimate its @xmath0 worst - case complexity . </S>",
    "<S> numerical examples are implemented to verify the theoretical results .    </S>",
    "<S> example.eps gsave newpath 20 20 moveto 20 220 lineto 220 220 lineto 220 20 lineto closepath 2 setlinewidth gsave .4 setgray fill grestore stroke grestore </S>"
  ]
}