{
  "article_text": [
    "most cortical neurons are noisy , or at least appear so to experimenters . when a sensory neuron s spikes are recorded in response to a well - controlled stimulus , they will show a large variability from trial to trial .",
    "this noisiness has been acknowledged from early on , as a nuisance preventing experimenters from easy access to the encoding properties of sensory neurons .",
    "but what is the impact of trial - to - trial sensory noise on the organism itself ?",
    "this question gained renewed interest a few decades ago , with the generalization of experimental setups recording neural activity from awake , behaving animals @xcite . in these setups ,",
    "animals are presented with a set of stimuli @xmath3 and trained to respond differentially to different values of @xmath3 , thus providing an ( indirect ) report of their percept of @xmath3 . as neural activity and animal behavior",
    "are simultaneously monitored , it becomes possible to seek a causal link between the two .",
    "in such setups , one particular hypothesis  which we refer to as the `` sensory noise '' hypothesis  has proven instrumental in linking neural activity and percepts .",
    "it postulates that trial - to - trial noise at the level of sensory neurons is the main factor limiting the accuracy of the animal s perceptual judgements @xcite .",
    "indeed , signal detection theory provides the adequate tools to estimate such accuracies .",
    "any type of biological response to a stimulus @xmath3say @xmath4can be associated to a signal - to - noise ratio ( snr ) , which measures how typical variations in @xmath5 due to a change of stimulus @xmath3 ( the _ signal _ ) compare to intrinsic variations of @xmath5 from trial to trial ( the _ noise _ ) .",
    "when @xmath4 measures the response of a neuron to stimulus @xmath3 , the resulting snr is often called the _ neurometric _ sensitivity for that particular neuron .",
    "alternatively , @xmath4 may also be the response of the animal itself to stimulus @xmath3 .",
    "the resulting snr is called the animal s _ psychometric _ sensitivity , which quantifies the animal s ability to discriminate nearby stimulus values @xmath3 .",
    "reformulated in terms of snrs , the `` sensory noise '' hypothesis states that neurometric sensitivity , computed from the population of sensory neurons under survey , is equal to the psychometric sensitivity for the animal in the task .",
    "applying this idea , neurometric and psychometric sensitivities have often been computed and compared , in various sensory systems and behavioral tasks ( see , e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* for reference ) .",
    "however , it was progressively realized that most of these comparisons bear no simple interpretation , because the neurometric sensitivity is not a fixed quantity : it depends on how information is read out from the neurons .",
    "for example , if the various sensory neurons in the population behave independently one from another , then the overall snr from the population will essentially be the sum of individual snrs and thus , the experimenter s estimate of neurometric sensitivity will depend on how many neurons ",
    "say @xmath6they included in their analysis .",
    "this intuition still holds in realistic populations where neurons are not independent , with the additional complexity that the evolution of neural snr with @xmath6 is very influenced by the correlation structure of noise in the population @xcite .",
    "more subtly , another parameter has a direct influence on estimated neurometric snrs : the time scale @xmath0 used to integrate each neuron s spike train , to describe the neuron s activity over the trial @xcite . indeed , through the central limit theorem",
    ", the more neural spikes are integrated into the readout , the more accurate that readout will be .",
    "adding extra neurons through @xmath6 , or extra spikes for each neuron through @xmath0 , will thus have the same type of impact on the readout s overall snr .",
    "in fact , if all neurons from the population are identical , independent poisson encoders , one can easily show that the readout s overall snr scales with @xmath7 , emphasizing the duality between @xmath6 and @xmath0 .    as there is no unique way of reading out information from a population of sensory neurons ,",
    "a question naturally arises : what type of readout does the organism use ?",
    "for example , how many sensory neurons @xmath6 , and what typical integration time scale @xmath0 , provide a relevant description of the animal s percept formation ?",
    "the `` sensory noise '' hypothesis can precisely be used to answer this question : the ` true ' neuronal readout for the organism must be the one providing the best account of animal behavior .",
    "however , the previous @xmath6@xmath0 discussion clearly shows that comparing neurometric snr to psychometric snr is not sufficient to target the true readout : there will be several combinations of @xmath6 and @xmath0 leading to the same overall neurometric snr , while corresponding to very different extraction strategies by the animal .",
    "thus , an additional experimental measure is required to recover the typical scales of integration of the true readout .",
    "_ choice signals _ are a good candidate for this additional measure . in two - alternative tasks , where the animal must report a binary discrimination of stimulus value ( say , @xmath8 or @xmath9 ) ,",
    "choice signals are generally computed in the form of _ choice probabilities _ ( cp ) @xcite .",
    "cp is computed for each recorded neuron individually , and quantifies the trial - to - trial correlation between the activity of that neuron and the animal s ultimate ( binary ) choice on the trial , all other features being held constant . in particular , since cp is computed across trials with the same stimulus value ( generally uninformative , i.e. , @xmath10 ) , the observed correlations can not reflect the influence of stimulus on neural activity and animal choice . instead , a significant cp can only result from the process by which the neuron s activity influences  or is influenced by ",
    "the animal s forming perceptual decision .",
    "it is intuitively clear that cps reveal something about the way information is extracted from sensory neurons . for example , if the animal s percept is built from a single neuron , then that neuron will have a very large cp , because its activity on every trial directly predicts the animal s percept . instead ,",
    "if several independent neurons contribute to form the animal s percept , then they are all expected to have low cp value , as the activity of each neuron has only a marginal impact on the animal s decision . however , converting this intuition about choice signals into a quantitative interpretation was long hampered by the fact that , just like neurometric snr , cp values are largely influenced by the population s noise covariance structure . for example , a neuron may not be utilized by the animal to form its percept , and yet display significant cp because its activity is correlated with that of another neuron being utilized . as a result ,",
    "early studies relating cp values to the animal s perceptual readout only relied on numerical simulations @xcite , assuming very specific noise correlation structures that weakened the generalizations of their results . only very recently have @xcite provided an analytical expression for cp values in the presence of noise correlations ( see section  [ sec : cp ] ) , opening the door to general , quantitative interpretations of choice probabilities . in this article , we show how the combined information of animal sensitivity ( snr ) and choice signals allows to estimate the typical scales of percept formation by the animal , both across neurons ( number of neurons involved @xmath6 ) and in time ( integration window @xmath0 ) .",
    "our results apply in the standard feedforward model of percept formation , and can be derived for any noise covariance structure in the neural population .",
    "we first show how the joint covariance structure of neural activities and animal percept leads to a set of characteristic equations for the readout , which implicitly determine the animal s perceptual readout policy across neurons and time .",
    "then , we show how these characteristic equations can be used in a statistical form , across the ensemble of trials and neurons available to the experimenter , to determine the typical scales @xmath6 and @xmath0 of percept formation from the activity of sensory neurons .",
    "this approach is mandatory since experimental measurements can only provide statistical samples of the full neural population . using an artificial neural network to provide sensory encoding ,",
    "we show that our method can reliably recover the true scales of perceptual integration , without requiring full measurement of the neural population .",
    "thus , our method can readily be applied to real experimental data , and provide new insights into the nature of sensory percept formation .",
    "we place ourselves in a general framework , describing a typical perceptual decision - making experiment ( fig .  [",
    "fig : frame ] ) . on each trial , a different stimulus @xmath3 is presented to the animal ( fig .",
    "[ fig : frame]a , top ) , which then takes a decision according to its internal judgement @xmath11 of stimulus value .",
    "our framework assumes that this percept @xmath11 is directly available to the experimenter on each trial . in real experimental setups ,",
    "the animal s report is generally more indirect  typically a binary choice based on the unknown percept @xmath11 .",
    "we choose the former approach because it applies generically to most perceptual decision - making experiments , whereas the `` choice '' part is more dependent on each particular setup .",
    "we detail later how both approaches can be reconciled through simple models of the animal s behavior ( section  [ sec : cp ] ) .",
    "simultaneously , experimenters record neural activities from a large population of sensory neurons , which is assumed to convey the basic information about @xmath3 used by the animal to take its decision ( fig .  [",
    "fig : frame]a , bottom ) .",
    "typical examples could be area mt in the context of a moving dot discrimination task ( e.g. , * ? ? ?",
    "* ) , area mt or v2 in the context of a depth discrimination task ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ) , or area s1 in the context of a tactile discrimination task ( e.g. , * ? ? ?",
    "we describe the activity of this neural population on every trial as a point process @xmath12 , where each @xmath13 is the spike train for neuron @xmath14 , viewed as a series of dirac pulses . as an important remark",
    ", @xmath15 denotes the full population size , a very large and unknown number .",
    "it is _ not _ the number of neurons actually recorded by the experimenter , which is generally much smaller .    for simplicity , we assume a fine discrimination task , where the different stimulus values @xmath3 presented to the animal display only moderate variations around a central value , say @xmath10 .",
    "this substantially simplifies snr computations , because the ` signal ' part of any response @xmath4 is then summarized by its slope in @xmath10 : @xmath16 , where @xmath17 denotes the average response over trials .",
    "we assume that this linearization with @xmath3 can be performed both for the psychometric report @xmath11 , and for individual neuron activities .",
    "this is mostly a convenience though , and the framework could be generalized to more complex dependencies on stimulus @xmath3 .",
    "framework and main experimental measures .",
    "( a ) experimental setup .",
    "top : a set of stimulus values @xmath3 ( color - coded as blue , yellow , red ) are repeatedly presented to an animal , which reports its percept @xmath11 on each trial ( color - coded as green ) .",
    "bottom : in each session , several task - relevant sensory neurons are recorded simultaneously with behavior . ( b )",
    "perceptual sensitivity @xmath18 is defined as the square snr of the animal s reports @xmath19 .",
    "( c ) the noise covariance structure can be assessed in each each pair of simultaneously recorded neurons , as their joint peri - stimulus histogram ( jpsth ) .",
    "( d ) trial - wise response of a particular neuron .",
    "each thin line is the schematical representation of the spike train on each trial . segregating trials according to stimulus ( top )",
    ", we access the neuron s peri - stimulus histogram ( psth ) and its tuning curve  shown in panel ( e ) . segregating trials according to the animal s perceptual error @xmath20 ( bottom )",
    ", we access the neuron s percept covariance ( pcv ) curve  shown in panel ( f ) . ]    from the raw data of @xmath11 and @xmath21 on each trial , a number of measures are routinely used to describe neural activity and animal behavior .",
    "first , the psychometric sensitivity @xmath22 describes the animal s accuracy in distinguishing nearby frequency values .",
    "it can be computed from the distribution of @xmath23 across trials ( fig .",
    "[ fig : frame]b ) , according to the formula : @xmath24 where notation @xmath25 denotes an average across stimulus conditions .",
    "this is exactly the ( squared ) snr for random variable @xmath19 , assuming that the ` signal ' term @xmath26 is equal to 1 because the animal s average judgement of @xmath3 is unbiased ( the framework easily generalizes to a biased percept ) .    on the other hand , for each recorded neuron , it is common practice to compute its peri - stimulus time histogram ( psth ) in response to each different tested stimulus ( fig .",
    "[ fig : frame]d ) : @xmath27 where @xmath28 denotes averaging over trials . since all stimuli @xmath3 are assumed to be close one from another , the dependency of @xmath29 on @xmath3 is essentially linear , and can be summarized by the ( temporal ) tuning curve for the neuron ( fig .",
    "[ fig : frame]e ) : @xmath30    furthermore , as recent techniques allow the simultaneous recording of many neurons , experimenters also have access to samples from the trial - to - trial covariance structure in the population ( fig .  [",
    "fig : frame]c ) . for every pair of neurons @xmath31 and instants in time",
    "@xmath32 , this covariance structure is assessed through the neurons joint peri - stimulus time histogram ( jpsth , * ? ? ?",
    "* ) : @xmath33 we only consider the average covariance structure , over different stimuli @xmath3 .",
    "first , as above , nearby values of @xmath3 insure that the covariance structure will remain mostly unchanged .",
    "second , trial - to - trial covariances correspond to second - order effects on neural activity , which require several trials to be reliably estimated  another reason to lump data from different stimuli @xmath3 into a single estimate .    finally , we can measure a _ choice signal _ for each neuron , estimating the trial - to - trial covariance of neuron activity @xmath13 with the animal s choice ( fig .",
    "[ fig : frame]f ) . since in our framework",
    "the animal directly reports its percept @xmath11 , we readily describe the choice signal of each neuron by its _ percept covariance _ ( pcv )",
    "curve : @xmath34 again , this covariance information is lumped across the different ( nearby ) stimulus values @xmath3 , in order to improve experimental measurement .",
    "the pcv curve captures the core intuition behind the more traditional measure of choice probability ( cp ) , while retaining a linear form convenient for analytical treatment .",
    "percept covariance curves are not directly measurable in classic experimental setups where the animal only reports a binary choice  ;  however their analytical link to available measures such as cps can be easily derived given simple models of the animal s decision policy ( see section  [ sec : cp ] ) .",
    "unlike many characterizations of neural activity that rely only on spike counts , our framework requires an explicit temporal description of neural activity through psths ( eq .",
    "[ eq : psth ] ) , jpsths ( eq .  [ eq : jpsth ] ) and percept covariance curves ( eq .  [ eq : pcov ] ) . indeed",
    ", our method ultimately predict _ when _ , and _ how long _ , perceptual integration takes place in the organism .",
    "readers may feel uncomfortable that the resulting definitions are directly expressed over trains of dirac pulses . while these notations are fully justified in the framework of point processes @xcite",
    ", they describe idealized quantities that can not be estimated from a finite number of trials , leading to jaggy estimates formed from the collection of dirac peaks .",
    "so in practice , spike trains @xmath13 are computed in temporal bins of finite precision .",
    "all experimental measures described above , taken together , provide a full characterization of the joint covariance structure of variables @xmath35 across stimuli and trials ( fig .",
    "[ fig : model]c ) . the key argument to exploit these data , which is actually a reformulation of the ` sensory noise ' hypothesis ,",
    "is that the animal s percept @xmath11 is built on every trial from the activity of the sensory neurons , meaning that @xmath36 for some unknown readout @xmath37 . as a result , each proposed readout @xmath38 directly yields an estimate for the joint covariance structure of @xmath35through a set of relationships which constitute the readout s _",
    "characteristic equations_. conversely , since this joint covariance structure is experimentally measurable , it implicitly constrains the nature of the true readout @xmath37 which was applied by the animal . in this section",
    ", we introduce a generic form of linear readout , stemming from the standard feedforward model of perceptual integration , and derive its characteristic equations .",
    "we show that in theory , these equations totally characterize the readout applied by the animal .",
    "we define a generic linear readout from the activity of sensory neurons @xmath21 ( fig .",
    "[ fig : model]a ) , based on a given readout vector : @xmath39 , a given integration kernel with normalized shape @xmath40 and time constant @xmath0 : @xmath41 , and a given readout time @xmath42 : @xmath43 the readout is noted @xmath44 , as it must ultimately be an estimator of stimulus value @xmath3 .",
    "we explicitely note the dependence on @xmath42 to emphasize that @xmath45 is built from a sliding temporal average of the spike trains  ; so that each instant in time yields a potential readout .",
    "this is a classical form of readout from a neural population , which has often been used previously and described as the ` standard ' model of perceptual integration @xcite .",
    "the temporal parameters @xmath46 and @xmath42 describe how each neuron s temporal spike train @xmath13 is integrated into a single number describing the neuron s activity over the trial : @xmath47 . in turn , the percept is built linearly from the population activity as @xmath48 through a specific readout vector , or ` perceptual policy ' , @xmath49 .    however , traditional studies generally make ad hoc choices for the various constituants of this readout .",
    "most often , @xmath50 simply describes the total spike count for neuron @xmath14 , which in our model corresponds to choosing a square kernel @xmath40 , and parameters @xmath51 describing an integration over the full period @xmath52 of sensory stimulation .",
    "as mentionned in the introduction , there is no reason that this should be a relevant description of sensory integration by the organism : the integration window @xmath0 has a direct influence on predicted snrs for the readout , and experiments suggest that animals do not always use the full stimulation period to build their judgement @xcite .    instead , we make no assumption on the nature of @xmath0 and @xmath42 , and view them as free parameters of the model .",
    "then , the model parameters implicitly characterize the typical scales of perceptual integration by the animal .",
    "the number of significantly nonzero entries in @xmath49 , say @xmath6 , defines the number of neurons contributing to the percept .",
    "the readout window @xmath0 characterizes the behavioral scale of temporal integration from the sensory neurons , and time @xmath42 characterizes when during stimulation this integration takes place .",
    "the exact shape @xmath40 given to the integration kernel is of less importance  ; for conceptual and implementational simplicity we assume it to be a square window .",
    "however , we note that ( 1 ) other shapes may have a higher biological relevance , such as the decreasing exponential mimicking synaptic integration by downstream neurons , and ( 2 ) nothing prevents our method from making @xmath40 itself a free parameter , provided the data contain enough power to estimate it .",
    "finally , our model can also be extended to versions where extraction time @xmath42 is not fixed , but varies from trial to trial  ; this issue is discussed in section  [ sec : withg ] .",
    "thanks to its linear structure , the readout defined in eq .",
    "[ eq : readout ] allows for a simple characterization of the covariance structure that it induces between neural activity @xmath21 and the resulting percept @xmath44 ( fig .",
    "[ fig : model]b ) .",
    "we show in appendix [ sec : appchar ] that this covariance structure can be summarized by three characteristic equations : @xmath53 where vector @xmath54 and matrices @xmath55 and @xmath56 respectively describe the population s tuning and noise covariance structures , derived from the underlying neural statistics @xmath57 and @xmath58 introduced in eq .",
    "[ eq : tuning]-[eq : jpsth ] : @xmath59 we here note the explicit dependency of @xmath54 , @xmath55 and @xmath56 on the temporal parameters of the readout @xmath0 and @xmath42 .",
    "we will generally omit it in the sequel .",
    "thus , the right - hand sides of eq .",
    "[ eq : char - tuning]-[eq : char - pcov ] depend only on readout parameters @xmath0 , @xmath42 , @xmath49 and on the statistics of neural activity , independently of the animal s percept .    on the other hand , the left - hand sides of eq .",
    "[ eq : char - tuning]-[eq : char - pcov ] describe experimental quantities related to the readout s resulting percept @xmath44 .",
    "the first line describes the average tuning of @xmath44 to stimulus @xmath3 , that is @xmath60 , which is equal to 1 because we assume that @xmath44 is unbiased .",
    "the second line expresses the resulting sensitivity @xmath61 for the readout , defined as in eq .",
    "[ eq : zstar ] .",
    "it reveals the dual influence of the number of neurons ( through @xmath49 ) and integration window @xmath0 on the readout s overall sensitivity : indeed , under mild assumptions , the covariance matrix @xmath56 scales with @xmath62 ( see appendix [ sec : appw ] ) .",
    "finally , the third line expresses the resulting covariance between @xmath44 and the activity of each neuron @xmath13 , defined as in eq .",
    "[ eq : pcov ] .",
    "this is essentially the relationship already revealed by @xcite , that choice probabilities are related to readout weights through the noise covariance matrix  ; however , our formalism focuses on the simpler linear measure of pcv curves , and explicitly takes time into account .",
    "both the neural measures @xmath57 and @xmath63 on the right - hand side , and the percept - related measures @xmath61 and @xmath64 on the left - hand side , can be estimated from data .",
    "as a result , the characteristic equations define an implicit constraint on the readout parameters @xmath0 , @xmath42 and @xmath49 ( fig .",
    "[ fig : model]d ) . actually , if the readout model in eq .",
    "[ eq : readout ] is true , and precise measures are available for all neurons in the population , one sees easily that these constraints would uniquely determine the readout parameters . indeed , for fixed parameters @xmath0 and @xmath42 , eq .",
    "[ eq : char - tuning ] and [ eq : char - pcov ] impose linear constraints on vector @xmath49 .",
    "these constraints are generally overcomplete , since @xmath49 is @xmath15-dimensional , while each time @xmath65 in eq .",
    "[ eq : char - pcov ] provides @xmath15 additional linear constraints .",
    "thus , in general , a solution @xmath49 will only exist if one has targeted the true parameters @xmath0 and @xmath42 , and it will then be unique .      in the previous section we have shown",
    "that , in the standard linear model of percept formation , the trial - to - trial covariance structure between spike trains @xmath21 and the resulting percept @xmath44 leads to a set of characteristic equations which implicitly define the parameters of the perceptual readout , provided the covariance structure has been fully estimated .",
    "unfortunately , this direct approach makes a fundamental assumption which can not be reconciled with real , experimental recordings : it assumes we have recorded all neurons from the population under survey , whereas real recordings only ever record from a small subset of that population .",
    "thus we can not hope to reconstruct the real vector @xmath49 , simply because some ",
    "probably most  of the neurons contributing to @xmath49 were not recorded . moreover , even across those neurons which were recorded through a series of sessions in a given area , the noise covariance structure can never be fully assessed  ; it remains elusive between neurons which were not recorded simultaneously .    for this fundamental reason",
    ", the characteristic equations [ eq : char - tuning]-[eq : char - pcov ] should be used with a different perspective than the full recovery of readout parameters .",
    "instead , we propose to exploit the structure of the equations in a statistical approach , with the restricted goal of estimating the typical scales of readout most compatible with recorded data .",
    "a first necessary step in our approach is to statistically describe the nature of readout vector @xmath49 .",
    "we are mostly interested in the support of @xmath49 , meaning , the number and nature of neurons contributing to percept formation .",
    "thus , we assume that the percept is built only from the activities of an unknown ensemble @xmath66 of neurons in the population and that , for given @xmath66 and temporal parameters @xmath67 , the readout vector @xmath49 is chosen optimally to maximize the snr of the resulting percept . indeed ,",
    "through this hypothesis , we totally reformulate the problem of characterizing @xmath49 in that of characterizing @xmath66  ; which allows for much simpler statistical descriptions .",
    "the readout vector @xmath49 achieving the maximum sensitivity @xmath61 in eq .",
    "[ eq : char - z ] , under the constraints of eq .",
    "[ eq : char - tuning ] and having support on @xmath66 , is well known from the statistical literature .",
    "it is uniquely given by fisher s linear discriminant formula @xcite : @xmath68 where @xmath69 , @xmath70 and @xmath71 are the versions of vectors @xmath49 , @xmath54 ( eq .  [ eq : b ] ) and matrix @xmath56 ( eq .  [ eq : c ] ) restricted to neuron ensemble @xmath66 . by injecting the form ( eq .  [ eq : a - opt ] ) into eq .",
    "[ eq : char - z]-[eq : char - pcov ] we obtain a new version of the characteristic equations , under the assumption that percept is built optimally from some given ensemble @xmath66 , and temporal parameters @xmath67 : @xmath72 @xmath61 in eq .  [ eq : new - z ] is the ( optimal ) sensitivity associated to this particular choice of @xmath66 , @xmath0 and @xmath42 . in eq .",
    "[ eq : new - pcov ] , @xmath73 is the resulting , predicted pcv curve for every neuron @xmath14 in the population ( not only in ensemble @xmath66 ) .",
    "@xmath74 is a row vector whose entries are equal to @xmath75 ( eq .  [ eq : gamma ] ) for neurons @xmath76 .",
    "+ these equations open the door to a statistical description of percept formation in the neural population : we can now parse through a large set of candidate ensembles @xmath66 and temporal parameters @xmath67 , and ask when the predictions for sensitivity ( eq .  [ eq : new - z ] ) and pcv curves ( eq .  [ eq : new - pcov ] ) match their true psychophysical counterparts @xmath18 ( eq .  [ eq : zstar ] ) and @xmath77 ( eq .",
    "[ eq : pcov ] ) . for sensitivity ,",
    "the straightforward comparison is to require that @xmath78 .    on the other hand , for the pcv equation ( eq .  [ eq : new - pcov ] ) ,",
    "it is pointless to search an elementwise match for every neuron @xmath14 , between the predicted curve @xmath73 and its true measure @xmath77 . indeed , since only a small subset of the neurons have been recorded , no candidate readout ensemble @xmath66 will be equal to the true ensemble ( say @xmath79 ) that was used by the animal  ; and there is no guarantee that the covariance structure between @xmath14 and @xmath66 , which gives rise to prediction ( eq .  [ eq : new - pcov ] ) , should be similar to that between @xmath14 and @xmath79 .",
    "instead , a given set of readout parameters @xmath80 should be deemed plausible if they predict the correct _ distribution _ of pcv signals across the population , irrespective of exact neuron identities @xmath14 .",
    "full distributions are difficult to estimate from finite amounts of data , and we will find the following population _",
    "averages _ to convey sufficient information : @xmath81 where @xmath82 denotes averaging over the full population of neurons @xmath83 .",
    "we will deem a set of readout parameters plausible if they yield @xmath84 depends on parameters @xmath67 only through the neurons tunings @xmath85 . in practice ,",
    "as neural activities are rather stationary in time , @xmath86 changes very little for different values of parameters @xmath67 . ] . multiplying each pcv curve by the neuron s tuning @xmath87 ( eq .  [ eq : b ] ) yields more stable estimates for @xmath88 , as discussed in section  [ sec : full - valid ] and appendix [ sec : appsvd ] .",
    "there are many ways to compare the real values of sensitivity and pcv signals , to their predictions given by eq .",
    "[ eq : new - z]-[eq : new - pcov ] .",
    "we propose here an ad - hoc method , whose main characteristics are the following : ( 1 ) focus mostly on first - order statistics ( i.e. , means ) across the neural population , ( 2 ) use arbitrary tolerance values to compare real and predicted data , ( 3 ) fit the two indicators sequentially : first snr , then percept covariance . due to its simplicity , this method will prove robust to measurements errors arising from finite amounts of data ( section  [ sec : finite ] ) .    our method is also designed to cope with a fundamental limitation of real recordings : all neurons ( ensemble @xmath66 , neurons @xmath14 ) contributing to predictions eq .",
    "[ eq : new - z]-[eq : new - pcov ] must have been recorded simultaneously , to assess their noise covariance structure .",
    "this constraint sets a limit on ensemble sizes @xmath1 which can be easily investigated ( but see section  [ sec : extrapolation ] ) .",
    "moreover , it prevents from estimating the full average of choice signals ( eq .  [ eq : w - kap ] ) predicted by a given ensemble @xmath66it is only available for simultaneously recorded neurons @xmath14 . as a result ,",
    "predictions ( eq .  [ eq : new - pcov ] ) from different tested ensembles @xmath66 must somehow be aggregated to produce a reliable prediction of choice signals .",
    "we propose that each tested ensemble @xmath66 contribute to our estimates in proportion to its ability to account for the animal s sensitivity : @xmath89 normalized to insure @xmath90 across all tested ensembles ( @xmath0 and @xmath42 being fixed ) .",
    "parameter @xmath91 is the required tolerance for the fit , set by the experimenter .",
    "it is a regularization parameter creating a tradeoff between precision of fit ( small @xmath91 ) and reliability of measurements , since a larger @xmath91 leads to more samples @xmath66 with a substantial contribution @xmath92 .",
    "when testing our method ( section  [ sec : results ] ) we choose @xmath91 as 5% of @xmath18 .    for each tested couple @xmath67 , we then use @xmath93 as a weighting factor over all tested ensembles @xmath66 , which yields two quantities : @xmath94 where @xmath95 denotes an average across all neurons @xmath14 available to compute a prediction with eq .",
    "[ eq : new - pcov ] .",
    "these neurons must have been recorded simultaneously to ensemble @xmath66 and , in order to produce an unbiased estimate of choice signals in the full population , they should not belong to @xmath66 itself .    in eq .",
    "[ eq : kest ] , @xmath96 is the ensemble size @xmath1 which most likely explains the animal s sensitivity , given readout parameters @xmath67 . in eq .",
    "[ eq : west ] , @xmath97 is the mean prediction for pcv signals @xmath98 across neurons @xmath14 in the population , but stemming only from ensembles @xmath66 which are compatible with the animal s sensitivity .",
    "considering quantity @xmath99 introduced in eq .",
    "[ eq : w - kap ] , we see that @xmath100 the equality is only approximate , because only neurons @xmath14 recorded simultaneously to @xmath66 are available to estimate @xmath101 .",
    "however , as neurons @xmath14 are random and we average over many ensembles @xmath66 , @xmath101 rapidly converges to the quantity described in eq .",
    "[ eq : wapp ] .    both @xmath97 and @xmath102",
    "are temporal signals defined over some interval @xmath103 $ ] corresponding to one trial repetition . defining the l2 norm for such temporal signals as @xmath104",
    "we will deem parameters @xmath67 plausible if they lead to a small value of @xmath105 . to yield a quantitative estimate of fit ,",
    "we introduce a tolerance @xmath106 and define the following weighting function : @xmath107 normalized to insure @xmath108 across all tested temporal parameters @xmath67 .",
    "again , tolerance @xmath106 is set arbitrarily by the experimenter .",
    "when testing our method ( section  [ sec : results ] ) we choose @xmath106 as 5% of @xmath109 .",
    "+ overall , the statistical method introduced above reduces readout parameters to three numbers : the temporal extraction parameters @xmath0 and @xmath42 , and the typical number of neurons @xmath1 used by the readout .",
    "thus , we can now apply a ` brute - force ' approach : test all possible combinations @xmath110 , compute the population statistics from eq .",
    "[ eq : psnr]-[eq : ppcv ] , and target the parameters that provide the best fit . in the next section , we show the validity of this statistical approach , which allows us to recover the typical scales @xmath110 of perceptual integration in an artificial network simulation .",
    "we further detail how this statistical approach can be adapted to counteract measurement errors which typically arise in real experiments from the finite number of available trials .",
    "in this section , we show how the statistical analysis of sensitivity and choice signals described above allows to recover the scales of integration of the neural readout . naturally , to assess the validity of our method , it is necessary to know the true nature of this readout .",
    "this can only be achieved through an artificial simulation of sensory integration , where we have full control on neural activities and readout procedure .",
    "we thus implemented an artificial neural network , that encodes some input stimulus @xmath3 in the spiking activity of its neurons ( fig .",
    "[ fig : network]a )",
    ". precise parameters of this network are provided as supplementary material ( section s1 ) .",
    "briefly , on each trial , 100 input poisson neurons fire with rate @xmath3 , taking one of three possible values 25 , 30 and 35 hz . the encoding population _",
    "per se _ consists of 500 leaky integrate - and - fire ( lif ) neurons .",
    "100 of these neurons receive sparse excitatory projections from the input poisson neurons , which naturally endows them with a positive tuning to stimulus @xmath3 .",
    "100 other neurons receive sparse inhibitory projections from the poisson neurons , which naturally endows them with negative tuning .",
    "the remaining 300 neurons receive no direct projections from the input . instead",
    ", all neurons in the encoding population are coupled through a sparse connectivity with random delays up to 5 ms .",
    "synaptic weights are random and balanced , tuned to ensure overall firing rates around @xmath111 hz .",
    "we implemented and simulated the network using brian , a spiking neural network simulator in python @xcite .",
    "the statistics of activity for the resulting population are depicted in fig .",
    "[ fig : network]b , c , e .",
    "we then define the true perceptual readout from this network .",
    "we pick a random set of @xmath112 neurons in the population , whose activity is integrated over @xmath113 ms and read out at time @xmath114 ms , on each stimulus presentation ( each presentation lasting 500 ms ) .",
    "the resulting estimator @xmath11 of stimulus value is built optimally given these constraints , through fisher linear discriminant analysis ( eq .  [ eq : a - opt ] ) are not used in the subsequent analysis . ] .",
    "this leads to a ` psychometric ' sensitivity @xmath115 0.06 hz@xmath116 , meaning that the network can typically discriminate variations of @xmath117 4.2 hz in the input @xmath3 .",
    "( for comparison , over the same integration period @xmath118 , the 100 input poisson neurons can discriminate variations around 2.5 hz . )",
    "we then compute a pcv for every recorded neuron , measuring its trial - to - trial covariance with estimator @xmath11 ( fig .",
    "[ fig : network]d ) . applying the statistical method described above",
    ", our goal is now to recover the scales @xmath119 of perceptual integration , on the basis of the experimental measures depicted in ( fig .",
    "[ fig : network]c - e ) .      to show the theoretical validity of our analysis",
    ", we first apply it to a situation where the trial - to - trial covariance of neurons and percept @xmath11 ( eq .  [ eq : zstar]-[eq : pcov ] ) has been fully measured , with high precision ) . using bootstrap resamplings over stimulus repetitions",
    ", we checked that the resulting measures were virtually error - free . ] .",
    "in particular , we assume full knowledge of the noise covariance structure @xmath63 in our population ( eq .  [ eq : jpsth ] ) .",
    "actually , in these irrealistic conditions , the statistical analysis described above is not useful : instead , one could directly solve the characteristic equations ( eq .",
    "[ eq : char - tuning]-[eq : char - pcov ] ) to recover the readout parameters @xmath49 , @xmath0 and @xmath42 . however , this is a necessary first step to verify that our method is not flawed theoretically .",
    "do the statistical quantities introduced in eq .",
    "[ eq : psnr]-[eq : ppcv ] allow to recover the true scales of integration @xmath119 ?",
    "statistical recovery of readout parameters : noiseless measures .",
    "( a ) for each tested temporal parameters @xmath67 , predicted sensitivities @xmath120 are computed for several candidate readout ensembles @xmath66 of varying sizes .",
    "the goodness of fit to true sensitivity @xmath18 defines a weighting function @xmath92 across ensembles .",
    "( b ) the weighting function is used to compute a compound prediction @xmath101 for the average pcv signal in the population , which is compared to the true average @xmath86 .",
    "the three columns in panels a - b correspond to different candidates @xmath67 for temporal integration .",
    "( c ) best - fitting ensemble size @xmath121 depending on candidate parameters @xmath67 .",
    "the @xmath122 tradeoff on sensitivity is clearly visible .",
    "( d ) goodness of fit of pcv signals depending on candidate parameters @xmath67 shows a clear optimum around the true parameters of the readout .",
    "( e ) same as panel d , but transformed into a weighting function @xmath123 over candidate temporal parameters . ]",
    "assuming a square integration kernel @xmath40 , we test a set of candidate temporal integration windows @xmath0 from 10 to 100 msec , and a set of candidate readout times @xmath42 from 10 to 200 msec , all in steps of 10 msec .",
    "we then pick randomly candidate neural ensembles @xmath66 , of sizes ranging from 2 to 90 neurons , with 50 different random ensembles for each tested size @xmath1 . for each tested parameters @xmath67 ,",
    "we compute the distribution of predicted snrs @xmath120 given by eq .",
    "[ eq : new - z ] , across all candidate neural ensembles ( fig .",
    "[ fig : noiseless]a ) .",
    "each neural sample @xmath66 is then associated to a weight @xmath92 describing the goodness of fit to the true snr @xmath18 ( eq .  [ eq : psnr ] ) .",
    "following eq .",
    "[ eq : kest]-[eq : west ] , this yields an estimate for the best - fitting population size @xmath96 ( fig .",
    "[ fig : noiseless]a , dashed vertical line ) and mean pcv curve @xmath97 ( fig .",
    "[ fig : noiseless]b ) .",
    "since we assume full knowledge of experimental data , all 500 neurons @xmath14 are involved in estimating @xmath101 , independently of ensemble @xmath66 .    in fig .",
    "[ fig : noiseless]c , we show the estimated population size @xmath96 as a function of @xmath0 and @xmath42 .",
    "it shows the mark of the @xmath1@xmath0 tradeoff on sensitivity , mentioned in the introduction : smaller integration windows @xmath0 require larger ensemble sizes @xmath1 to account for the animal s sensitivity . in fig .",
    "[ fig : noiseless]d , we show two measures of the resulting fit between @xmath97 and its true value @xmath102 . in the first panel , we plot the plain l2 norm between the two temporal signals ( using @xmath124 msec and @xmath125 msec as integration bounds ) . in the second panel ,",
    "we reexpress this l2 norm as a weighting @xmath123 over the set of tested temporal parameters ( eq .  [ eq : ppcv ] ) . applying this final weighting over candidate values @xmath0 , @xmath42 , and @xmath96 yields numerical estimates for the scales of the readout : @xmath126 these estimates are very close to the true values @xmath118 , @xmath127 and @xmath128 , showing the theoretical validity of this approach .",
    "the estimated @xmath129 is somewhat smaller than its true value @xmath112 , however this is no bias in our method : it simply means that the 40 neurons chosen randomly as the source of percept were slightly less sensitive than the ` average ' 40 neurons in the population .",
    "we also remind that these estimates depend on the tolerance levels fixed by the experimenter to compute @xmath92 ( eq .  [ eq : psnr ] ) and @xmath123 ( eq .  [ eq : ppcv ] ) .",
    "numerically , we find the resulting mean estimates to be rather stable across a range of sensible tolerances . on the other hand , the resulting error bars  which are obtained as second - order moment of the quantities weighted by @xmath123only",
    "describe the typical variations of the parameters that lead to estimates within the fixed tolerances .",
    "in particular , driving the tolerances to zero always drives the error bars to zero , even though the predicted averages may become false as too little data enter their computation . +",
    "why does the method work ?",
    "essentially , it proceeds in two successive steps .",
    "first , @xmath67 being held fixed , it uses snr information to target plausible neural ensembles @xmath66 ( fig .  [ fig : noiseless]a , c ) .",
    "since the readout is assumed to be optimal , the mean snr can only increase with the size @xmath1 of the ensembles considered ( fig .",
    "[ fig : noiseless]a , plain blue curve ) .",
    "plausible ensembles @xmath66 are those lying near the crossing of this curve with the true ` psychometric ' snr ( fig .",
    "[ fig : noiseless]a , dashed red curve ) . for a straightforward application of our method",
    ", this crossing should occur within the typical ensemble sizes @xmath1 tested  which are , in practice , limited by the number of simultaneously recorded neurons . in section",
    "[ sec : extrapolation ] , we discuss possible extensions of the method to the case where the crossing does not occur .     mean percept covariance curves depend on readout ensemble @xmath66 .",
    "( a ) the mean value of pcv curves @xmath73 across neurons @xmath14 in the population depends strongly on the readout ensemble @xmath66 giving rise to the percept .",
    "( b ) the mean value of tuning - multiplied pcv curves @xmath98 depends much less on the exact ensemble @xmath66 , only on its size .",
    "this justifies our definition for the mean pcv curve @xmath130 ( eq .  [ eq : w - kap ] ) .",
    "( c ) relative variance of mean pcv curves , across readout ensembles @xmath66 of the same size .",
    "it is defined as the average of @xmath131 across all ensembles ( @xmath132,@xmath133 ) of similar size , divided by @xmath134 , power of the average curve across ensembles of size @xmath1 . for the tuning - multiplied version of @xmath130 ( blue ) ,",
    "this ratio quicky drops to zero .",
    "this is not the case for the plain mean @xmath135 ( green ) . ]",
    "second , @xmath67 being still fixed , an average pcv prediction @xmath97 is built , using the neural ensembles @xmath66 targeted above , and compared to the true mean pcv curve @xmath102 .",
    "it is not trivial that this comparison should work . to simplify the argumentation ,",
    "let us assume that parameters @xmath0 and @xmath42 are fixed at their true values @xmath118 and @xmath127 .",
    "on the one hand , since the true percept is built from some ( unknown ) neural ensemble @xmath79 , we have @xmath136 , using the notations of eq .",
    "[ eq : w - kap]-[eq : wstar ] . on the other hand ,",
    "the prediction @xmath101 is built as a compound mean of @xmath137 over several candidate ensembles @xmath66 ( see eq .",
    "[ eq : wapp ] ) . as our method requires a match between @xmath86 and @xmath101 , it implicitly supposes that all ensembles @xmath66 contributing to @xmath101 lead to very similar population averages @xmath137",
    ".    predicted curves @xmath137 are generally not available experimentally .",
    "however , we can compute them in our full - data simulation ( fig .  [",
    "fig : prod - pcov ] ) .",
    "we find that , amongst ensembles @xmath66 of similar size @xmath1 , the @xmath14-population means of @xmath138 rapidly converge to a single curve , independently of ensemble @xmath66 ( fig .",
    "[ fig : prod - pcov]b ) .",
    "furthermore , this result is not trivial : when the same analysis is performed on the plain pcv curves , not multiplied by tuning @xmath87 , the convergence does not occur anymore ( fig .",
    "[ fig : prod - pcov]a ) , or at least not as fast . in fig .",
    "[ fig : prod - pcov]c , we plot the ratio between the variance of curves accross different ensembles @xmath66 , and the power of the mean curve , across all ensembles @xmath66 of same size @xmath1 .",
    "this ratio quickly drops to zero for the tuning - multiplied pcv curves ( blue ) , but not for the plain pcv curves ( green ) .    to summarize , it is crucial for our method that each pcv curve @xmath73 be multiplied by the neuron s tuning @xmath87 before computing population averages . aside from the experimental observations of fig .  [",
    "fig : prod - pcov ] , several arguments justify this operation .",
    "first , it is well - known experimentally that choice signals and tuning for individual neurons are often positively correlated at the population level @xcite . intuitively , this is because positively - tuned neurons contribute positively to stimulus estimation , and conversely for negatively - tuned neurons .",
    "the strong population - wide correlation is indeed present in our simulated network ( fig .",
    "[ fig : network]b ) . as a result",
    ", the population average for @xmath98 is expected to be mostly positive ( fig .",
    "[ fig : prod - pcov]b ) , which diminishes possible variations from one ensemble @xmath66 to the other .",
    "second , theoretical arguments ( appendix [ sec : appsvd ] and supplementary material s2 ) show that @xmath98 is a form better suited to compute an @xmath14-population average .",
    "it can be shown to be positive under mild assumptions , and its laws of convergence can be related to the overall spectrum of covariance in the population .      having shown the theoretical efficiency of the statistical quantities introduced above in retrieving the correct scales of perceptual integration , we now test our method on its real purpose : recovering the scales from incomplete experimental data ( fig .",
    "[ fig : noisy ] ) .",
    "we thus limit our measures to 150 repetitions for each tested stimulus .",
    "furthermore , we split our population in 5 ensembles of 100 ` simultaneously recorded ' neurons , so that noise covariance information ( eq .",
    "[ eq : jpsth ] ) is only available between neurons belonging to the same ensemble .",
    "we use the same candidate values for parameters @xmath0 , @xmath42 and @xmath1 as before , picking 50 candidate ensembles @xmath66 for each tested size @xmath1 .",
    "neurons in @xmath66 always belong to the same ` simultaneous ensemble ' , which is picked randomly .",
    "finally , for each ensemble @xmath66 , we consider 10 additional neurons @xmath14 , from the same ` simultaneous ensemble ' but segregated from neurons in @xmath66 , to compute the pcv prediction @xmath101 ( eq .  [ eq : west ] ) .",
    "the method then proceeds as above , save a couple of modifications due to the incompleteness of the data .",
    "first , concerning snr computations ( eq .  [ eq : new - z ] ) , the estimated covariance matrix @xmath139 may turn out to be rank - deficient up to numerical precision ( although it should be full - rank theoretically , since the number of trials ( 450 ) is larger than the largest tested size @xmath1 ) .",
    "we thus replace its inverse @xmath140 by its moore - penrose pseudo - inverse , with the default numerical tolerance of our mathematical software ( matlab ) .",
    "even so , we observe a global overestimation of predicted sensitivities @xmath61 , compared to their values in the full - data case ( dashed blue lines in fig .  [",
    "fig : noisy]a , reproduced from fig .",
    "[ fig : noiseless]a ) .",
    "this overfitting is a well - known feature when estimating fisher sensitivity from insufficient data @xcite .",
    "second , concerning mean pcv predictions ( eq .  [ eq : west ] ) , our final estimates @xmath101 become noisy , reflecting the jagginess of the underlying neural measures due to insufficient trials ( fig .",
    "[ fig : noisy]b ) .",
    "this jagginess is problematic , as it artificially increases measured values for the divergence @xmath105 , which is our final criterion to retrieve plausible values of @xmath67 .",
    "however , this effect can be largely compensated by resorting to resampling over trials ( bootstrap ) .",
    "more precisely , for each tested parameters @xmath67 , we may describe our noisy measures in the form : @xmath141 where @xmath142 and @xmath143 describe our ( unknown ) measurement errors on @xmath144 and @xmath145 . from this",
    "follows the estimate : @xmath146 where @xmath28 denotes the ( theoretical ) expectancy over the set of trials giving rise to measures @xmath144 and @xmath145 .",
    "this estimate is based on the assumption that measurement errors @xmath142 and @xmath143 are independent , which is likely to be the case given that @xmath101 stems from predictions ( on the basis of neural tunings and noise covariances ) whereas @xmath86 stems from measurements of the true pcv curves . all terms involving an expectancy @xmath28 in eq .",
    "[ eq : wboot ] can be estimated by resampling with replacement over the set of recorded trials . by computing their difference",
    ", we thus get a corrected estimate for @xmath147 .",
    "this is the estimate plotted in fig .",
    "[ fig : noisy]d .",
    "a drawback of this method is that the resulting estimate may become ( slightly ) negative when the underlying match between @xmath144 and @xmath145 is `` too good '' .",
    "however , this does not prevent from estimating the resulting weighting function @xmath123 ( eq .  [ eq : ppcv ] ) , accepting that some terms in the exponential may become ( slightly ) positive .",
    "statistical recovery of readout parameters : noisy measures .",
    "same legends as fig .",
    "[ fig : noiseless ] , but with modifications specific to small sample data . in panel b ,",
    "the thin curves are different versions obtained through bootstrap resampling over trials , and the thick curve is the average across bootstrap samples . in panel d",
    ", the l2 norm is corrected for measurement errors , using the bootstrap samples and eq .",
    "[ eq : wboot ] . with this modification",
    ", our method can recover the true readout parameters on the basis of finite amounts of data . ]    the final results , in fig .",
    "[ fig : noisy ] , show that our method is still able to recover the most plausible scales of perceptual integration . using the same tolerances as previsouly ( 5% of the power of the true measures ) , we find the following final estimates : @xmath148 again very close to the true scales of the readout .",
    "notably , @xmath129 is smaller than its prediction in the full - data analysis : this is a consequence of the slight overfitting on estimated snrs , which leads to an underestimation of the population size @xmath1 required to match the psychometric snr .",
    "the issue remains minor in this setup  ; however we note that standard cross - validation and regularization techniques exist , that respectively assess and counteract the effects of overfitting @xcite .    in conclusion ,",
    "the statistical method introduced above allows to overcome the missing data inherent to realistic recordings , by integrating information from all recorded neurons into a few reliable statistical estimators .",
    "we have proposed a framework to interpret sensitivity and choice signals in a standard model of perceptual decision - making .",
    "the purpose of our study is to help understand how perceptual integration takes place from a full sensory neural population .",
    "this question requires , not only to compute neurometric sensitivities or choice signals for individual neurons , but also to integrate these measures in a single big picture of how information is read out from the population as a whole .",
    "the sensitivity to stimulus achievable by a neural population has received much attention , both experimental and theoretical .",
    "it was progressively realized that ( 1 ) the structure of noise correlations influences the amount of information that can be extracted from a neural population , and ( 2 ) the linear readout maximizing sensitivity is generally not a simple average of neural activities , but rather an adequate weighting optimizing the ratio between signal and noise extracted from the population , which corresponds to fisher s linear discriminant ( see * ? ? ?",
    "* ; * ? ? ?",
    "* and references therein ) . similarly ,",
    "the role of time window @xmath0 used to integrate the spike counts of each neuron has long been acknowledged to have a direct effect on the overall estimated sensitivity ( see , e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "choice signals have also received much attention since their first measurements , in the form of choice probabilities @xcite .",
    "the temporal evolution of choice signals is routinely computed to qualitatively establish the instants in time when a given population covaries with the animal s percept @xcite .",
    "recently , the specific temporal evolution of cp signals during a depth discrimination task has cast doubt on the traditional , feedforward interpretation of cp signals ( * ? ? ?",
    "* see section  [ sec : hypothesis ] ) . however , very little studies have _ quantitatively _ interpreted cp signals so far , because no analytical relationship was available to interpret their values .",
    "only recently have @xcite derived the analytical expression of cps in the standard model of perceptual integration ( see section  [ sec : cp ] ) .    to the best of our knowledge",
    ", only one study has explicitly proposed to jointly use sensitivity and choice signals , as two independent constraints characterizing the underlying neural code . in this seminal study",
    ", @xcite proposed a feed - forward model of perceptual integration in visual area mt responding to a moving dots stimulus , and studied how the population s sensitivity and individual neuron cps vary as a function of model parameters such as the number of neurons , strength of noise correlations , etc . in section",
    "[ sec : readout ] , we have formalized this intuition of @xcite , by showing that sensitivity and choice signals are two distinct , constitutive elements of the joint covariance structure between percept @xmath44 and neural activity @xmath21 ( fig .",
    "[ fig : model]c ) .",
    "the third constitutive element is the noise covariance structure of @xmath21 itself , a result also intuited by @xcite even though they assumed an oversimplified , homogeneous noise correlation matrix .    unlike most previous theoretical studies on the subject",
    ", we explicitly modeled all neural activities in time .",
    "indeed , this is the only way of targeting the instants of sensory stimulation which contribute to percept formation , and thus to decipher to @xmath1@xmath0 tradeoff on sensitivity . finally , the statistical approach developped in section  [ sec : statistical ] is , to our knowledge , the first attempt to build inhomogeneous , partial measures of neural activity into a quantitative interpretation of percept formation from the full neural population .",
    "our model , as presented above , assumes a direct perceptual report of stimulus value @xmath11 on every trial .",
    "real experiments generally involve a more indirect report : to allow easier task learning by the animal , the report is always binary . in the classic random dot motion discrimination task @xcite",
    ", a monkey is visually presented with a set of randomly moving dots whose overall motion is slightly biased towards the left ( @xmath9 in our notations ) or towards the right ( @xmath8 ) .",
    "the monkey must then press either of two buttons depending on its judgement of the overall movement direction . in another classic task @xcite",
    ", monkeys must discriminate the frequencies @xmath149 and @xmath150 of two successive vibrating stimuli on their fingertip .",
    "they must press one button if they consider that @xmath151 , and the other button otherwise .",
    "thus , classic choice signals such as cp only measure the covariation between the spike train of each neuron and the animal s binary choice @xmath152 on each trial . to infer anything about the animal s underlying _ percept _ @xmath11 , it is also necessary to assume a behavioral model describing how the monkey takes a binary decision , on every trial , on the basis of its sensory percept .",
    "most often , this behavioral model is implicitly assumed to be optimal .",
    "for example , in the random dot motion task , it is generally assumed that @xmath153 ( heavyside function ) , which is clearly the optimal policy if the animal has no further information about @xmath3 .",
    "in the two - frequency task , the optimal behavioral model would be @xmath154 .",
    "however , in the real experiment , the monkeys have to memorize @xmath149 for a few seconds before @xmath150 is presented , so potential effects of memory loss may also come into play .",
    "more generally , behaving animals can display biases , lapses of attention , various exploratory and reward - maximization policies that lead to deviations from the optimal behavioral model .",
    "to summarize , choosing a relevant behavioral model is a connex problem that can not be addressed here , and that will vary depending on the task and individual considered .",
    "however , for most tractable behavioral models , the predicted sensitivities and choice signals will ultimately rely on the quantities introduced in this article . to take the simplest example",
    ", we focus on the random dot motion task with optimal policy @xmath153as assumed in most models of the task  and make the classic assumption that the statistics of @xmath11 ( given @xmath3 ) are gaussian ( fig .  [ fig : disc]a ) .",
    "this model predicts the following psychometric curve ( probability of button presses as a function of stimulus value ) : @xmath155 where @xmath156 is the standard cumulative normal distribution , and @xmath18 is the square snr for @xmath11 , as defined in eq .",
    "[ eq : zstar ] .",
    "thus , @xmath18 used in our model can easily be retrieved from experimental measures of the psychometric curve .",
    "same results hold for choice signals .",
    "generally , choice signals are directly computed over some temporal average @xmath50 of the underlying spike trains .",
    "choice probability for every neuron @xmath14 measures the area under the roc curve between the two distributions of @xmath50 , respectively conditioned on @xmath157 and @xmath158 @xcite .",
    "recently @xcite have shown that , assuming ( 1 ) multivariate gaussian statistics between @xmath159 and @xmath11 , and ( 2 ) the optimal behavioral model @xmath153 , choice probability can be analytically expressed as : @xmath160 a formula virtually exact over the full range of plausible cp values .",
    "the rightmost fraction is nothing but the pearson correlation between variables @xmath50 and @xmath11 .",
    "the numerator involves the linear covariance between @xmath50 and @xmath11 which is , in our notations , the temporally averaged pcv curve @xmath161 .",
    "the authors further derived that , in the standard model of percept formation with readout vector @xmath49 , this term is given by @xmath162 , which is exactly the pcv characteristic equation ( eq .  [ eq : char - pcov ] ) in its temporally - averaged form .",
    "the cp formula involves a normalization by @xmath163 , the standard deviation of spike count @xmath50 .",
    "this prevents from a straightforward extension of the formula in time , because @xmath163 tends to infinity as the integration window used to compute @xmath50 tends to zero .",
    "a simpler measure of choice signals is the choice - conditioned difference in firing rate @xcite , which can be computed for every individual neuron @xmath14 as @xmath164 . under the same assumptions as above ( gaussian statistics for @xmath165(t ) and",
    "@xmath11 , optimal behavioral model ) , this difference can be analytically expressed , applicable to any bivariate normal variables @xmath166 with means 0 , unitary variances , and correlation coefficient @xmath167 .",
    "we note that the assumption of normality is violated at small time scales because @xmath13 is clearly not gaussian in that case .",
    "however , in practice , @xmath168 is always computed with a minimal amount of temporal smoothing which resolves this potential issue . ] as : @xmath169 this is very close to the cp formula , but without the additional normalization by @xmath163 .",
    "thus it directly allows for a simple generalization to temporal signals .",
    "since @xmath168 is easily computable from experimental data , it provides the easiest way of accessing the underyling pcv curves @xmath77 used in our article .",
    "( a ) classic behavioral model .",
    "if the task is to judge whether @xmath8 , the optimal behavioral policy consists of the simple threshold rule @xmath153 ( heavyside function ) .",
    "furthermore , the trial - to - trial distribution of percept @xmath11 given @xmath3 ( distributions with different colors ) is generally assumed to be gaussian . under these hypotheses , sensitivity and pcv signals used in this article",
    "are directly computed from real experimental data ( neurometric curve and choice signals ) .",
    "( b ) if readout time @xmath42 varies strongly from trial to trial ( with density @xmath170 ) , it leads to a flattening of pcv signals ( thick green curve ) compared to the case with deterministic @xmath42 ( dashed green curve ) .",
    "( c ) if a decision - related signal feedbacks into sensory areas , it leads to a divergence of pcv signals ( thick green curve ) after the readout time @xmath42 , compared to the case without feedback ( dashed green curve ) . ]        the readout model ( eq .",
    "[ eq : readout ] ) used to analyze sensitivity and choice signals is an instalment of the ` standard ' , feedforward model of percept formation .",
    "as such it makes a number of hypothesis which should be understood when applying our methods to real experimental data .",
    "first , it assumes that the percept @xmath11 is built linearly from the activities of the neurons . there is no guarantee that this is the case during real percept formation , but linearity is an unavoidable ingredient to make quantitative predictions at the population level . even if the real percept formation departs from linearity , fitting a linear model will most likely retain meaningful estimates for the coarse information ( temporal scales , number of neurons involved ) that we seek to estimate in this work .",
    "more precisely , the model in eq .",
    "[ eq : readout ] assumes that spikes are integrated using a kernel separable across neurons and time , that is @xmath171 .",
    "theory does not prevent from studying a more general integration , where each neuron @xmath14 contributes with a different time course @xmath172 .",
    "the readout s characteristic equations are derived equally well in that case .",
    "rather , assuming a separable form reflects ( 1 ) the intuition that the temporal components of integration are rather uniform across the population , and ( 2 ) the impossibility to fit a model with general kernel @xmath172 .",
    "instead , we summarize temporal integration from the population by two parameters @xmath0 and @xmath42 , opening the door to a reliable estimation from data . although the integration shape @xmath40 could also be fit from data in theory , it seems more fruitful to assume a simple shape from the start ( a classic square window kernel in our applications ) . given that our goal is to estimate the coarse scales of percept formation",
    ", our method will likely be robust to various simple choices for @xmath40 .",
    "as a simple example , we tested our method , assuming a square window kernel , on data produced by a readout using an exponential kernel , and still recovered the correct parameters @xmath0 , @xmath42 and @xmath1 .",
    "our model , as presented above , makes another important assumption : that perceptual readout occurs at the same time @xmath42 on every stimulus presentation .",
    "this assumption is likely to be valid in perceptual tasks that allow a fast reaction from the animal ( ` reaction time ' tasks ) , in which case @xmath42 will generally be as small as it can get ( see , e.g. , * ? ? ?",
    "however , when sensory stimulation lasts longer ( say , over 500 msec ) it opens the door to variations in @xmath42 from trial to trial , or even to several reactualizations of percept @xmath11 during the same trial .",
    "for example , imagine that the stimulus is a particular rgb color on a monitor , and you are asked to judge whether it contains more green ( g ) or blue ( b ) . from intuition , we can tell that our performance in such a task will not sensibly increase whether we watch the color for one second or one minute .",
    "in our model s formalism ( eq .  [ eq : readout ] ) , this reveals built - in limitations on the effective integration window @xmath0 that we can use in the task ( remember that the readout s performance is proportional to @xmath0 ) .",
    "but then , if our percept arises from a limited integration window @xmath0 and we indeed watch the color for a full minute , when is our percept built ?    in appendix",
    "[ sec : appchar ] , we derive a more general version of the characteristic equations ( eq .",
    "[ eq : char - tuning]-[eq : char - pcov ] ) assuming that @xmath42 in eq .",
    "[ eq : readout ] is itself a random variable , drawn on each trial following some probability distribution @xmath170 .",
    "because sensory neurons have rather stationary activities in time , this additional assumption does not strongly affect the readout s sensitivity .",
    "on the other hand , it affects strongly pcv curves .",
    "essentially , the resulting pcv curve resembles a convolution of the deterministic pcv curve by @xmath170 ( fig .",
    "[ fig : disc]b , section  [ sec : appg ] ) .",
    "if @xmath170 is substantially distributed in time , the pcv curves will become broader , and flatter . in practice , this means that if a behavioral task is built such that @xmath42 can display strong variations from trial to trial , the statistical method introduced above will produce biased estimates . in theory",
    ", this issue could be resolved by adding an additional parameter in the analysis to describe @xmath170 ( see section  [ sec : appg ] ) , but the validation remains to be done .",
    "finally , our ` standard ' model assumes that percept formation is exclusively feed - forward .",
    "the activities @xmath13 of the sensory neurons are integrated to give rise to percept @xmath44 and the animal s resulting choice @xmath152 , and this forming decision does not affect sensory neurons in return .",
    "recent evidence suggests that reality is more complex . by looking at the temporal evolution of cp signals in v2 neurons during a depth discrimination task , @xcite evidenced dynamics which are best explained by a top - down signal , biasing the activity of the neurons on each trial _ after the choice is formed_. in our notations , the population spikes @xmath13 would thus display a choice - dependent signal which kicks in on every trial after time @xmath42 , resulting in pcv signals that deviate from their prediction in the absence of feedback ( fig .",
    "[ fig : disc]c ) .",
    "what descriptive power does our model retain , if such top - down effects are strong ?",
    "the answer depends on the nature of the putative feedback .",
    "if the feedback depends linearly on percept @xmath44 ( and thus , on the spike trains ) , its effects are fully encompassed in our model .",
    "indeed , this feedback signal will then be totally captured by the neurons linear covariance structure @xmath173 , so that our predictions will naturally take it into account .",
    "this is also the case if the oddity noted by @xcite is due to global shifts of neural excitability from trial to trial . on the other hand ,",
    "if the feedback depends directly on the choice @xmath152which displays a nonlinear , ` all - or - none ' dependency on @xmath44then it will not be captured by our model , and lead to possible biases .",
    "even so , the effects of the feedback could be largely alleviated through a small trick : compare true and predicted pcv signals only up to ( candidate ) time @xmath42 ( see eq .",
    "[ eq : l2 ] ) .",
    "can we understand in more depth the statistical principles at work underneath our method of estimation ?",
    "what factors govern the evolution of sensitivity @xmath120 ( fig .",
    "[ fig : noiseless]a , eq .  [ eq : new - z ] ) and mean pcv signal @xmath130 ( fig .",
    "[ fig : noiseless]b , eq .  [ eq : w - kap ] ) , as a function of the number of neurons @xmath1 used for readout ?",
    "this question is not only of theoretical , but also of practical interest .",
    "indeed , it may happen in real applications that the number of simultaneously recorded neurons @xmath174 is too small to observe the crossing of predicted and true snr curves ( fig .",
    "[ fig : noiseless]a ) , following the @xmath1@xmath0 tradeoff . ] .",
    "in such a case , predictions will be biased because no recorded ensemble @xmath66 can readily account for animal sensitivity .",
    "what predictive power do ensembles up to size @xmath174 contain about larger ensembles ?",
    "for example , can we extrapolate the shape of the mean snr curve ( fig .",
    "[ fig : noiseless]a ) to @xmath175 ? in appendix",
    "[ sec : appsvd ] we address this question theoretically , by studying the value of snr and pcv signals as a function of ensemble size @xmath1 , and of the general structure of activity in the population .",
    "our study relies on the singular value decomposition ( svd ) of neural activity in the population .",
    "the svd reveals a set of @xmath176 independent _ modes _ of population activity , each mode being associated to a power @xmath177 and a sensitivity @xmath178 .",
    "essentially , the sensitivity embedded in a neural ensemble @xmath66 of size @xmath1 increases as the sum of sensitivities for the @xmath1 first modes in the population  which are the modes with the largest powers @xmath179 .",
    "conversely , the overall power of pcv signal @xmath99 decreases as the average value of @xmath177 in these @xmath1 first modes , weighted by their respective sensitivities . because there is no general relationship between the power @xmath179 of a mode and its sensitivity to stimulus @xmath180 , there is no trivial way of extrapolating snr and pcv predictions to ensemble sizes @xmath1 that were not monitored simultaneously .",
    "any such extrapolation can only be done through specific assumptions about the link between @xmath179 and @xmath180which essentially amounts to characterizing the relative embedding of signal and noise in the full population @xcite .",
    "for example , it is classically assumed that the noise covariance matrix is `` smooth '' with respect to the signal covariance matrix , so that the former can be predicted on the basis of the latter @xcite .",
    "thus , while extrapolation of the statistical method above to larger populations is not trivial , it can be performed under specific assumptions about the embedding of signal and noise in the population considered .",
    "we have shown how classic data recorded during perceptual decision - making experiments can be interpreted as samples from the joint covariance structure of neural activities and animal decision . assuming a standard linear model of percept formation from neural activities ,",
    "we derived a set of characteristic equations which relate neural and perceptual data , and thus define implicitly the parameters of perceptual integration by the animal on the basis of its sensory neurons .",
    "the neural data consist of neural psths ( first moment of neural activities ) and jpsths ( second moment of neural activities ) .",
    "the perceptual data consist of the animal s sensitivity , and of each neuron s covariance with the animal s choice  a quantity often assessed through choice probabilities , and for which we proposed a simpler linear equivalent coined _ percept covariance _ ( pcv ) .",
    "we then proposed a method to utilize these characteristic equations in a case of practical interest , when experimenters only have access to finite statistical samples of neural data across the full population .",
    "our goal was to successfully recover the instants in time and the typical number of neurons being used for percept formation  a difficult problem which can not be solved on the sole basis of sensitivity information , due to the `` @xmath6@xmath0 tradeoff '' .",
    "our method relies on statistical averages of predicted sensitivity and pcv signals arising from random , candidate neural ensembles used as the source of percept formation  ; and seeks to match these predictions with the true , recorded perceptual data .",
    "we tested this method on an artificial neural network producing a form of stimulus encoding , and showed that it successfully recovers the scales of perceptual integration , on the basis of sample recordings of realistic size .",
    "our method opens the way to novel experimental assessments of percept formation in sensory decision - making tasks .",
    "indeed , the two main quantities used in our statistical analysis ",
    "sensitivity @xmath61 ( eq .  [ eq : new - z ] ) and mean pcv curve @xmath99",
    "( eq .  [ eq : w - kap])rely on classic experimental measures .",
    "the main limitation of our approach is the size of candidate readout ensembles which can be considered , as they should necessarily have been recorded simultaneously .",
    "however , the number of simultaneously recorded neurons is constantly pushing upwards with modern experimental techniques , so we may expect that this limitation , if it exists , will soon be overcome . furthermore , through a theoretical analysis based on the singular value decomposition ( svd ) of neural activities , we showed the possibility of extrapolation to larger ensemble sizes than those simultaneously recorded , although such extrapolations can only be done under specific assumptions , and on a case - by - case basis .",
    "for all these reasons , our method can readily be tested on real data , and hopefully provide new insights into the nature of percept formation from populations of sensory neurons .",
    "we here derive the characteristic equations for the linear readout introduced in the main text , and further comment some of its properties .",
    "we consider a more general version of eq .",
    "[ eq : readout ] , where the extraction time @xmath42 is allowed to vary from trial to trial .",
    "we thus assume that @xmath42 is itself a random variable , drawn on each trial according to some density function @xmath170 , independently of neural activities @xmath21 .",
    "the full readout model then writes :    @xmath181    this model naturally encompasses the simpler version presented in the main text , with a deterministic time @xmath42 : it corresponds to taking @xmath170 as a dirac function located on that deterministic value .",
    "the characteristic equations for this model rely on a straightforward computation of the second order statistics of @xmath44 , starting from eq .",
    "[ eq : gen - readout ] . to deal with random time @xmath42",
    ", we note that for any random process @xmath182 independent of @xmath42 , @xmath183 .",
    "this expression is valid only if @xmath42 is independent from the random variables contributing to @xmath184 ( in our case , the spike trains ) .",
    "then , the expected value of @xmath44 given a stimulus @xmath3 writes : @xmath185 where @xmath186 is the temporal correlation between @xmath187 and @xmath46 , and @xmath188 is the psth for neuron @xmath14 in stimulus condition @xmath3 , defined as in the main text ( eq .  [ eq : psth ] ) .",
    "similarly , the expected value of @xmath189 given a stimulus @xmath3 writes : @xmath190 where we have defined @xmath191 , and @xmath192 .",
    "@xmath193 is very related to the covariance structure in the population .",
    "it corresponds to the `` plain '' jpsth for the neurons in stimulus condition @xmath3 , before correcting by the so - called `` product predictor '' @xcite .",
    "finally , the expected value for the product of @xmath44 and the activity of any neuron @xmath13 writes : @xmath194 using the same notations as above .",
    "+ the three expressions eq .  [ eq : gen - efhat]-[eq : gen - efhatsj ] roughly correspond to the three characteristic equations for the readout . to obtain them ,",
    "we consider the variational versions of the previous expressions .",
    "first , we obtain the characteristic equation for tuning by differentiating eq .",
    "[ eq : gen - efhat ] with respect to stimulus .",
    "second , equations [ eq : gen - efhat2 ] and [ eq : gen - efhatsj ] are expressed in ` product ' form @xmath195 , whereas the corresponding characteristic equations are expressed in ` covariance ' form @xmath196 . once this is done , and after some rearrangement of the terms , we obtain the characterisitic equations for tuning ( eq .  [ eq : gen - tuning ] ) , sensitivity ( eq .  [ eq : gen - z ] ) and percept covariance ( eq .  [ eq : gen - pcov ] ) : @xmath197 in eq .",
    "[ eq : gen - tuning ] , @xmath198 is the temporal tuning curve for neuron @xmath14 , defined as in eq .",
    "[ eq : tuning ] .",
    "if the readout is unbiased , the left - hand side is equal to 1 , as in the main text . in eq .",
    "[ eq : gen - z ] and [ eq : gen - pcov ] , @xmath173 is the covariance structure ( jpsth ) between neurons @xmath14 and @xmath199 , defined as in eq .",
    "[ eq : jpsth ] .    finally in eq .",
    "[ eq : gen - z ] , matrix @xmath200 is an additional source of variance that appears only when @xmath170 has an extended temporal support , i.e. , when @xmath42 is non - deterministic .",
    "it then writes : @xmath201 where @xmath202 is the temporal average already used above ( eq .  [ eq : gen - efhat ] ) .",
    "thus , @xmath200 measures a form of temporal covariance in the psths for the neurons .",
    "when @xmath42 is deterministic , as in the main text , we have @xmath203 , a dirac function .",
    "then , the temporal integration kernels used in eq .  [ eq : gen - tuning]-[eq : gen - pcov ] boil down to @xmath204 and @xmath205 .",
    "one checks easily that in these conditions , the additional temporal variance term @xmath200 vanishes , and we recover the characteristic equations from the main text .        in the form of eq .",
    "[ eq : gen - z ] , it is not clear how the value of @xmath0 influences the variance of @xmath44 , and thus the readout s sensitivity . to get a better intuition ,",
    "let us first neglect the temporal variance term @xmath200 .",
    "one checks easily that kernel @xmath206 , introduced above , verifies the following property : @xmath207 , the autocorrelation of kernel @xmath46 . as a result , we can rewrite [ eq : gen - z ] in the form : @xmath208 in the first line , the function of @xmath65 defined by the fraction is positive and has an integral of 1 , so it operates as a temporal averaging on @xmath209 . the resulting average over @xmath65 ,",
    "noted @xmath210 in the second line , is thus a form of _ cross - correlogram _ between neurons @xmath14 and @xmath199 , measuring the average covariance between the spikes from @xmath14 and @xmath199 separated by a time lag @xmath211 .    because @xmath46 is a low - pass kernel with scale @xmath0 , its autocorrelation function typically has support on @xmath212 $ ] , and verifies : @xmath213 . ] : @xmath214 . on the other hand , @xmath210 typically has support on some interval @xmath215 $ ] , where @xmath216 is the typical time scale of noise correlations in the population . as a result ,",
    "as soon as @xmath0 gets bigger than @xmath216 , the integral in ( [ eq : gen - ccgm ] ) starts behaving like @xmath62 , and the snr of @xmath44 scales as @xmath0 .",
    "a similar analysis can be performed on the additional term @xmath200 ( eq .  [ eq : gen - z ] ) .",
    "what are the main departures from the main text when function @xmath170 has an extended temporal support ? from eq .",
    "[ eq : gen - tuning]-[eq : gen - pcov ] , it is clear that the general form of the characteristic equations still holds : @xmath217 but with more general definitions of @xmath218 , @xmath219 and @xmath220 .",
    "first , an additional covariance matrix @xmath221 may contribute to @xmath56 , if neural activities are not stationary in time .",
    "indeed , if @xmath42 varies from trial to trial , any variation of firing rates in time creates an additional source of variability in @xmath44 .",
    "second , through eq .",
    "[ eq : gen - pcov ] , @xmath222 acts a weighting factor over the pcv curves that would be obtained for each @xmath42 : @xmath223 .",
    "this leads to the spreading of pcv curves sketched in fig .",
    "[ fig : disc]c .",
    "these two features lead to lose one specific property of the deterministic case . when the `` natural '' temporal averaging of pcv signals was considered , that is @xmath224 , the integrated pcv equation yielded @xmath225 , because @xmath226 . in the general case ,",
    "the `` natural '' temporal averaging is @xmath227 , and one checks easily that @xmath228 .",
    "thus , with general @xmath170 , the sensitivity ( eq .",
    "[ eq : gen - z ] ) and pcv ( eq .  [ eq : gen - pcov ] ) equations become more dissociated . + in these conditions",
    ", it is unclear whether the statistical approach introduced in the main text could be extended , to also recover a non - deterministic extraction function @xmath170 .",
    "the main concern is that the temporal evolution of pcv signals is only determined by the aggregate function @xmath229 ( eq .  [ eq : gen - pcov ] ) , which can not be used to disentangle @xmath170 and @xmath0 separately .",
    "however , general considerations suggest that the method could still work in that case . indeed , the respective effects of @xmath170 and @xmath0 on the covariance structures used in eq .",
    "[ eq : gen - z]-[eq : gen - pcov ] can roughly be thought of as a scaling : @xmath230 because the overall `` shape '' of covariance between neurons ( as opposed to its `` strength '' ) does not depend much on the precise temporal integration used to compute their activity . actually , under the specific assumption that @xmath231 ( stationary activities with uniform temporal correlations ) , relationship ( eq .  [ eq : app - scaling ] ) can be shown to be exact , with @xmath232 expressed in terms of fourier transforms .",
    "as a result , the mean pcv curve @xmath99 ( eq .  [ eq : new - pcov ] , [ eq : w - kap ] ) is predicted to scale as @xmath233 .",
    "so , while matching the temporal support of @xmath99 and @xmath86 constrains the value of @xmath234 , matching their overall power constrains @xmath233 , and we can hope to disentangle the values of @xmath170 and @xmath0 separately . in practice though , this would require the fitting of at least one additional temporal parameter  ; typically , the standard deviation of @xmath42 from trial to trial .",
    "we summarize here the main results of a theoretical analysis to understand the evolution of snr and pcv signals achieved by readout ensembles of growing size @xmath6 .",
    "detailed mathematical derivations are available in supplementary section s2 .",
    "for simplicity we focus only on time - integrated neural activities @xmath235 , assuming a fixed choice of @xmath67 .",
    "we consider random readout ensembles @xmath66 in the population , and two resulting indicators .",
    "first , we consider the sensitivity @xmath236 , linked to snr @xmath61 by relationship @xmath237 .",
    "this is the natural description of sensitivity in the framework below .",
    "it is obtained like @xmath61 in the main text ( eq .  [ eq : new - z ] ) but replacing the noise covariance matrix @xmath56 by the total covariance matrix @xmath238 .",
    "second , we consider the mean pcv in the population @xmath239 , obtained as the `` natural '' temporal integration of signal @xmath130 from the main text ( eq .  [ eq : w - kap ] ) : @xmath240 . since @xmath99 is mostly positive , @xmath241 roughly corresponds to the overall power in @xmath99 .",
    "[ [ svd - reformulation - of - neural - activity . ] ] svd reformulation of neural activity . + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the analysis relies on the singular value decomposition ( svd ) of population activity into @xmath176 orthogonal _ modes _ : @xmath242 where the lower index @xmath243 indicates neurons in the population , and the upper index indicates all possible stimuli @xmath3 and random realizations @xmath244 of network activity",
    ". each mode @xmath245 is defined by its power @xmath246 , its distribution vector ( over neurons ) @xmath247 , and its appearance variable @xmath248 which takes a different random value on every trial . by construction ,",
    "the various modes are orthogonal across neurons ( @xmath249 ) , and linearly independent across trials ( @xmath250 ) , so they typically correspond to distinct `` patterns of activity '' in the population .",
    "the power @xmath179 describes the overall impact of mode @xmath245 on population activity .",
    "we assume @xmath251 , so we progressively include modes with lower power  either because they involve only a small fraction of neurons , either because they appear only on rare trials .",
    "the number of modes @xmath252 is the intrinsic dimensionality of the neural population s activity . in real populations we expect @xmath253 , because neural activities are largely correlated .",
    "the svd is best viewed as a change of variables reexpressing neural activities @xmath254 in terms of mode appearance variables @xmath255 .",
    "just like individual neurons , each mode @xmath245 can be associated to a _",
    "sensitivity _ to stimulus @xmath180 , which describes the proportion of the mode s power @xmath179 due to variations of the signal ( @xmath3 ) , as opposed to variations of the noise ( @xmath244 ) .",
    "since modes are linearly independent , the full population s sensitivity corresponds to the sum of individual mode sensitivities : @xmath256 .",
    "[ [ sensitivity - and - pcv - from - finite - neural - ensembles . ] ] sensitivity and pcv from finite neural ensembles .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we now want to estimate the amount of stimulus sensitivity @xmath236 that can be extrated , not from the full population , but from neural subensembles of size @xmath66 .",
    "the svd provides a natural reinterpretation of this problem in terms of activity modes : each ensemble @xmath66 `` reveals '' only a fraction of the underlying modes .",
    "the pivotal object to perform this reinterpretation is our so - called _ data matrix _ : @xmath257 an @xmath258 matrix describing the activity of neural ensemble @xmath66 in the space of modes . in the original problem formulation ,",
    "the @xmath259 matrix @xmath260 describes the covariance of neural activity in ensemble @xmath66 , and we want to estimate the resulting sensitivity . in the dual formulation ,",
    "the @xmath261 matrix @xmath262 describes a covariance structure between modes , but estimated only from the sample neurons in @xmath66 .",
    "the problem now lives in a space of fixed dimensionality @xmath252 , and can be related to classical problems of estimating covariance structures from a finite number of samples  in our case , the neurons .    applying this dual approach , we find that @xmath236 and @xmath239 depend on readout ensemble @xmath66 only through an @xmath261 matrix @xmath263 , the ( rank @xmath1 ) orthogonal projector on the span of vectors @xmath264 in mode space : @xmath265 where @xmath266 is the average square tuning in the population .",
    "furthermore , the average projector @xmath263 across ensembles of size @xmath1 , that is @xmath267 , is approximately diagonal in mode space .",
    "noting @xmath268 for its diagonal , we thus obtain the approximations : @xmath269 where @xmath270 is the average `` proportion '' of mode @xmath245 revealed by @xmath1 random neurons .",
    "as modes with larger power @xmath179 tend to be revealed first , a rough but useful image is to consider that @xmath271only the @xmath1 first modes are revealed by ensembles of @xmath1 neurons .",
    "thus , sensitivity @xmath272 grows with @xmath1 as mode sensitivities @xmath180 are progressively revealed .",
    "saturation occurs when all nonzero @xmath180 are revealed , in which case @xmath273 .",
    "conversely , the mean pcv @xmath274 decreases with @xmath1 .",
    "indeed , the fraction in eq .",
    "[ eq : ekw - summ ] can be viewed as an average power @xmath275 , where each mode @xmath245 contributes with a weight @xmath276 . as",
    "@xmath277 progressively reveals modes with lower power @xmath179 , this average power is expected to decrease with @xmath1 .",
    "again , saturation occurs when all nonzero @xmath180 are revealed , and then @xmath278 , the predicted value for choice signals in case of optimal readout from the full population @xcite .",
    "[ [ extrapolation - to - large - k . ] ] extrapolation to large @xmath1 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    what do these results tell us about possible extrapolations to ensemble sizes @xmath1 larger than the maximum number of neurons simultaneously recorded by the experimenter ? essentially , that such extrapolations always require further assumptions about the structure of activity in the population .",
    "indeed , one can imagine scenarios in which the most sensitive modes ( those with highest @xmath279 ) are associated to relatively low powers @xmath177 and thus , appear only at large @xmath1",
    ". this could be the case , for example , if a very local circuit of neurons carries a lot of information about the stimulus , independently from the rest of the population .",
    "because it involves few neurons , the corresponding mode of activity will have a low power @xmath177 , and will require very large ensembles @xmath66 to be detected  simply because the corresponding neurons are not recorded in smaller ensembles .",
    "a similar discussion can be found in @xcite .",
    "another example is the encoding network theoretically proposed by @xcite , where each neuron spikes only if its information is not already encoded in the activity of the remaining neurons .",
    "this results in the appearance of a few , global modes of activity which are specifically designed to have a very large snr , meaning high @xmath180 and low @xmath179 . in this case",
    ", any estimation of sensitivity from a subpopulation @xmath66 will consistently be smaller than the full population s sensitivity .    to summarize",
    ", extrapolation can only be performed under additional assumptions about the overall link between @xmath180 and @xmath179or equivalently , about the relationship between ` signal ' and ` noise ' contributions to population activity ( see also discussions in * ? ? ?",
    "* ; * ? ? ?",
    "the extent to which such assumptions are justified will depend on each specific context .",
    "* supplementary information *",
    "we detail here the architecture of the artificial encoding network used to test our method ( summarized in section 3.1 from the main text ) .",
    "this ad - hoc network was designed to display some classic features of sensory cortical neurons involved in perceptual decision - making tasks ( e.g , v2 , mt , s1 , s2  ) . to reproduce",
    "the diversity of response naturally observed at the population level @xcite , neurons in our network have broadly distributed firing rates , and some diversity in their temporal response profiles .",
    "we also wished to reproduce the continuum of tuning to stimulus observed in real populations , where some neurons have positive tuning to stimulus ( rate increase when @xmath3 increases ) , and other neurons have negative tuning .",
    "finally , we wished to reproduce realistic strengths of noise correlations between neurons in the population ( figure 3b from the main text ) , and insure that the tunings of each pair of neurons ( their `` signal '' correlation ) be only slightly predictive of their noise correlation  another feature often observed in real sensory populations @xcite .",
    "the network consists of two distinct layers of spiking neurons , of which only the second layer ( encoding layer ) is `` visible '' to the experimenter .",
    "the first layer ( l1 ) consists of @xmath280 independent poisson neurons , whose firing intensity @xmath3 constitutes the stimulus encoded by the second layer . on each trial , @xmath3 takes one of three possible values @xmath281 , @xmath111 and @xmath282 hz .",
    "all neurons are equivalent , but segregated in two distinct populations according to their projections on the second layer .",
    "the poisson firing constitutes the only source of randomness in the network from trial to trial .",
    "the second layer ( l2 ) consists of 500 leaky integrate - and - fire ( lif ) neurons , some of which receive input from l1 , and who are all coupled through a sparse , balanced connectivity .",
    "the generic equation for these neurons writes @xmath283 the neuron emits a spike at each time @xmath284 when @xmath285 reaches threshold @xmath286 , after what the neuron s potential is reinitialized at resting value @xmath287 .",
    "all neurons share the same membrane time constant @xmath288 msec , threshold @xmath289 mv , and resting potential @xmath290 mv .",
    "upper index @xmath291 denotes one of three possible subtypes of neurons in l2 : positively - biased neurons ( @xmath292 , 100 neurons ) , negatively - biased neurons ( @xmath293 , 100 neurons ) and unbiased neurons ( @xmath294 , 300 neurons ) .",
    "positively - biased neurons receive sparse excitatory connections from 50 neurons in l1 ( @xmath295 ) , whereas negatively - biased neurons receive sparse inhibitory connections from the 50 other neurons in l1 ( @xmath296 ) .",
    "unbiased neurons receive no direct input from l1 ( @xmath297 ) .",
    "as these asymmetries create biases in the total synaptic inputs to each type of cell , the intrinsic currents @xmath298 , @xmath299 and @xmath300 also vary depending on neuron subtype , to insure homogeneous firing properties inside the three populations ( see table [ tab : l2params ] ) . finally , all l2 neurons are connected through a single matrix @xmath301 of recurrent connections  independently of their subtype .",
    "all connection matrices @xmath302 and @xmath301 are sparse with ( erds - renyi ) connection probability @xmath303 .",
    "non - zero connection strengths are picked uniformly in an interval @xmath304 $ ] , which depends on the connection considered : see table [ tab : l2params ] .",
    "note that l2 recurrent connections can be both excitatory and inhibitory , a departure from biology which allows for an easier implementation .",
    "finally , the recurrent connections in l2 are associated to synaptic delays : for each pair @xmath305 of connected l2 neurons , the random delay @xmath306 is drawn uniformly between @xmath307 and @xmath308 msec .",
    "this substantially increases the diversity of neural responses in the population , particularly at the level of jpsths ( figure 3e from the main text)this is interesting because our method is specifically designed to analyse generic , heterogeneous population activities .",
    "+ we implemented and simulated the network using brian , a spiking neural network simulator in python @xcite .",
    "our simulation consisted of many successive epochs of 500 msec with all possible successions of the three stimulus values @xmath3 ( as in figure 1a from the main text ) .",
    "since the input poisson neurons were always firing close to 30 hz , there was no strong transient at stimulus onset as is often observed in real sensory neurons . in our case ,",
    "the change of activity between two successive stimuli was always only differential , and rather weak ( see figure 3c from the main text ) .",
    "c c c c c c     + subtype & @xmath309 & @xmath310 & @xmath311 & @xmath312 & @xmath313 +   + pos .",
    "biased ( @xmath314 ) & 0 & 0 & 2 & -2 & 2 + neg .",
    "biased ( @xmath315 ) & 14 & -3 & 0 & -2 & 2 + unbiased ( @xmath316 ) & 5 & 0 & 0 & -2 & 2 +",
    "we detail here our mathematical analysis to understand the evolution of snr and pcv estimates in growing populations of size @xmath6 , as a function of the underlying structure of the full population .",
    "these results expand the condensed presentation proposed in appendix b of the main text .        for simplicity , we consider a timeless version of neural activities , although the whole analysis could be extended to include time as well . in our readout framework",
    ", this means that we fix some candidate temporal integration parameters @xmath67 , and consider the resulting neural activities @xmath317 , constructed from the temporal integration of each neuron @xmath14 s spikes is noted @xmath50 in the main text . ] .",
    "since our main results have been presented in the case of linear tuning to stimuli , we stick to this hypothesis .",
    "this implies that all signal / noise properties can be understood by considering only two stimuli ( as the difference in response between these two stimuli totally defines the linear tuning of each neuron ) .",
    "we thus note @xmath318 the two possible stimulus values which can be input to the network .",
    "finally , we may want to consider the possibility of imprecise neural measurements , due to recording from only a finite number of trials ( although it is not the main concern of this note ) .",
    "we thus denote @xmath319 the set of all possible different realizations of network activity . in theory",
    ", @xmath320 is an infinite set of possible events .",
    "however , we will formally assume it to be finite , with ( huge ) cardinality @xmath321so on a given trial , each possible network realization @xmath244 has a probability @xmath322 of coming out .",
    "we thus summarize all possible network realizations through the array @xmath323 , where @xmath324 denotes all neurons in the population is noted @xmath15 in the main text .",
    "] , @xmath325 denotes stimulus value , and @xmath326 denotes all possible realizations . the notation @xmath327 , somewhat abusive , applies the same indexing @xmath244 for possible realizations in both stimulus conditions @xmath10 and @xmath328which can only be done if both stimulus conditions allow the same number @xmath321 of possible network realizations .",
    "however , given the formal nature of ensemble @xmath320 , this notation abuse appears harmless .",
    "as we start doing statistics across neurons and trials , we will need to compute expectancies ( i.e. , means ) and covariance structures across various dimensions . in all cases ,",
    "we apply the generic notation @xmath329 to denote the empirical mean of quantity @xmath330 when @xmath331 is varied over ensemble @xmath332 ( @xmath333 being any other parameters that are held fixed ) .",
    "when ensemble @xmath332 is unambiguous , meaning that it includes all possible values for @xmath331 , we will omit it .",
    "finally , second order variances and covariance structures will generically be computed as @xmath334 .    as a first application of these notations",
    ", remember that the whole sensitivity analysis derived in the main text deals only with variations : the `` signal '' measures variations of activity with a change in stimulus @xmath3 , while the `` noise '' measures variations of activity across trials @xmath244 .",
    "thus , the overall mean level of activity for each neuron @xmath14 , that is @xmath335 , plays no role in the analysis : it always disappears from the computations of tuning and noise covariance structure . to clarify further notations",
    ", we can thus offset all neural signals and assume that @xmath336 , for every neuron @xmath14 in the population .",
    "the key argument of this note relies on interpreting @xmath323 as a very large @xmath337 matrix , and considering its singular value decomposition ( svd ) .",
    "the ( compact ) svd is a standard decomposition which can be applied to any rectangular matrix @xmath338 .",
    "it writes @xmath339 , where @xmath340 is an @xmath341 diagonal matrix with strictly positive entries @xmath179 ( the singular values ) , @xmath342 is an @xmath343 matrix of orthogonal columns ( meaning @xmath344 ) , and @xmath345 is an @xmath346 matrix of orthogonal columns ( meaning @xmath347 ) .    with our current definition of neural activity @xmath348 ,",
    "the svd decomposition writes @xmath349 where the orthogonality of @xmath342 writes : @xmath350 and the orthogonality of @xmath345 similarly writes @xmath351 . in the case of @xmath345 ,",
    "our above convention that @xmath336 for all neurons @xmath14 actually imposes that @xmath352 for all modes @xmath245 .",
    "we thus reinterpret the orthogonality of @xmath345 as a linear independence between the different random variables @xmath248 : @xmath353 note that we reinterpret the sum over trials ( @xmath354 ) as an expectancy ( thus rescaling @xmath179 by ensemble size @xmath355 ) .",
    "this allows to emphasize the statistical interpretation of the svd decomposition in this case .",
    "+ each triplet @xmath356 defines one particular _",
    "mode _ of activity in the population .",
    "we call @xmath179 the _ power _ of the mode , @xmath247 ( viewed as an @xmath357-dimensional vector ) its _ distribution vector _ , and @xmath248 ( viewed as a scalar random variable ) its _ appearance variable_. the appearance variable @xmath248which takes a different value @xmath358 on every repetition of the experiment describes the probability of appearance of each mode @xmath245 across stimuli and trials . through eq .",
    "[ eq : orthv ] , each mode @xmath245 verifies @xmath359 , meaning that all modes have the same overall `` expected appearance '' across trials .",
    "similarly , eq .  [ eq : orthu ] implies that @xmath360 , so @xmath247 describes the normalized distribution of the mode across the neural population .",
    "some modes @xmath245 may correspond to a rather homogeneous distribution of @xmath361 across the population , meaning that the mode is very _ distributed _ , whereas other modes may have power concentrated only over a small subensemble of neurons .",
    "these are the modes corresponding to local patterns of activity which only impact a small fraction of the total neural population .",
    "finally , the power @xmath179 describes the overall impact of mode @xmath245 on population activity .",
    "indeed , although distribution vectors @xmath247 and appearance variables @xmath248 display the same normalization across modes , this does not mean that all modes are equivalent . instead , only those modes with the largest values @xmath179 will truly impact the population , in the form of measurable changes of activity across neurons and trials .",
    "conversely , modes with small values @xmath179 will scarcely impact population activity , either because they involve only a small fraction of neurons , either because they are distributed but very weak .",
    "the overall number of modes @xmath252 is equal to the rank of matrix @xmath323 , so it is by construction smaller or equal to the population size @xmath362 ( which we assume to be smaller than the huge number @xmath321 of possible realizations across trials ) .",
    "@xmath252 defines the typical dimension of the manifold in which all neural activity occurs . in real neural populations , although @xmath362 is itself a very large number , there are reasons to believe that @xmath252 is sensibly smaller , due to correlated activity between neurons .",
    "we now reinterpret classical measures of neural activity in the framework defined above . at this point",
    ", we need to carefully specify the nature of the ensembles truly available for measures : a finite subset @xmath66 of neurons from the population , and a finite ensemble @xmath363 of trials ( each element of @xmath363 providing one realization for stimulus @xmath10 and one realization for @xmath328 ) .    for every neuron @xmath364 , recorded over trials @xmath365 , we compute the tuning to stimulus as @xmath366 that is , the difference between the experimental mean firing rates in stimulus conditions @xmath328 and @xmath10 . from this appendix",
    "corresponds to @xmath367 from the main text , where @xmath368 gives typical variations of input stimulus . ]",
    "similarly , we compute the noise covariance term between any two neurons @xmath14 and @xmath199 as : @xmath369 that is , the stimulus - averaged noise covariance between @xmath14 and @xmath199 . finally , we introduce the total covariance matrix @xmath370 summing up all sources of variance across the population : @xmath371 the last line provides the classic decomposition of the total covariance matrix into noise covariance matrix @xmath372 and signal covariance matrix @xmath373which has rank 1 under our assumption of linear tuning to stimulus .",
    "when ensemble @xmath363 is equal to the full space @xmath320 of possible realizations , the above formulas define the `` true '' measures of covariance , as would be obtained given a sufficient amount of trials . in the sequel , we refer to these true , error - free values , by removing the mention to @xmath363 .",
    "that is : @xmath87 , @xmath374 and @xmath375 .",
    "+ the svd decomposition ( eq .  [ eq : svd ] ) is best interpreted as a change of variables reexpressing neural activities @xmath376 in terms of mode appearance variables @xmath255 . as a result",
    ", we can define the respective equivalents of tuning , noise covariance and total covariance in the space of activity modes .",
    "indeed , although mode appearance variables @xmath248 are never directly observed , they still have some statistics across trials .",
    "we thus define : @xmath377 which define tuning and total covariance in mode space ( noise covariance being implicitly defined as @xmath378 ) .",
    "again , we will denote the true tuning and covariance by removing the mention to @xmath363 : true tuning @xmath379 and true total covariance @xmath380 .",
    "importantly , the normalization of variables @xmath248 in eq .",
    "[ eq : orthv ] implies that @xmath381 .",
    "mode powers @xmath179 and distribution vectors @xmath247 then allow to relate the statistics at the levels of neurons and modes . injecting the svd formula ( eq .  [ eq : svd ] ) into equations  [ eq : tun - i ] and [ eq : tot - ij ] yields",
    "( in matricial form ) : @xmath382 in particular , when true noiseless measures are considered so that @xmath381 , we see that @xmath342 and @xmath340 directly provide the standard ( nonzero ) eigenvalue decomposition of the total covariance matrix @xmath383 , as @xmath384      we now wish to understand which factors determine the evolution of curve @xmath385 , the average snr embedded in neural subensembles @xmath66 of cardinal @xmath6 .",
    "we can also study the evolution of percept covariance ( pcv ) signals , in the same framework .    in the main text",
    ", we compute snr and pcv for ensemble @xmath66 through fisher s linear discriminant ( eq . 13 - 16 ) .",
    "one sees easily that these definitions , involving tuning @xmath54 and noise covariance matric @xmath56 , are equivalently expressed in terms of tuning @xmath54 and _ total _ covariance matrix @xmath383 : @xmath386 we call @xmath387 the signal - to - total ratio ( str ) , which relates directly to snr @xmath61 by the formula @xmath388 .",
    "@xmath387 always takes values between @xmath307 ( @xmath389 ) and @xmath390 ( @xmath391 ) , it thus avoids singularities which may occur in the direct @xmath61 formulation .",
    "if matrix @xmath392 is rank - deficient , we consider its ( moore - penrose ) pseudoinverse without loss of generality ( see further down ) .",
    "the svd decomposition ( eq .  [ eq : svd ] ) reexpresses neural activity in the space of modes @xmath176 .",
    "when the full neural population is considered , the full matrix @xmath383 and vector @xmath54 are involved in eq .",
    "[ eq : y - kap ] . using the svd formulations ( eq .  [ eq : b - svd]-[eq : a - svd ] )",
    "we thus find : @xmath393 thus , each mode contributes to total sensitivity by the strength of its intrinsic sensitivity @xmath180 .",
    "this computation can also be derived assuming a finite number of experimental trials @xmath363 . in this case however , we must introduce the _ experimental sensitivity _ @xmath394 of each mode , defined as @xmath395 where @xmath396 is the unique ( moore penrose ) pseudo - inverse of the symmetric , non - negative square root matrix of @xmath397 .",
    "actually , any other choice of matrix square root could also be used , because by construction @xmath398 , in the sense of symmetric positive matrices .",
    "this insures that @xmath399 is orthogonal to @xmath400 , and thus the unicity of @xmath401 as defined in eq .",
    "[ eq : bzeta ] .",
    "the computation of @xmath402 then goes along the same lines as previously : @xmath403 generally , one expects @xmath404 , because the estimated @xmath397 is flatter than its true value of @xmath381 , with eigenvalues closer to 0 .",
    "this is a classic result when estimating snr ( or str ) from an insufficient number of trials , a typical example of overfitting .",
    "as mentionned in the main text , there is no miracle cure to this problem , which should be addressed through appropriate methods of regularization and cross - validation@xcite .",
    "we now turn to the sensitivity embedded in finite subensembles @xmath66 from the population .",
    "the definitions of @xmath392 and @xmath70 used in eq .",
    "[ eq : y - kap ] amount to a projection from the full neural space @xmath405 to subensemble @xmath66 : @xmath406 where @xmath407 is the @xmath408 orthogonal projector on recorded neurons @xmath66 . through the svd decomposition in eq .",
    "[ eq : b - svd]-[eq : a - svd ] , we reexpress these quantities as : @xmath409 where @xmath410 is our so - called _ data matrix _ , an @xmath411 matrix with elements @xmath412 . it represents the experimental data from neurons @xmath66 , expressed in mode space .    to compute the resulting sensitivity predicted by eq .",
    "[ eq : y - kap ] , we note that through eq .",
    "[ eq : ak - svd ] , matrix @xmath392 has the same eigenvalues as its dual gram matrix @xmath413 , an @xmath261 matrix with rank @xmath414generally equal to @xmath6 .",
    "we introduce the ( compact ) svd decomposition of this matrix : @xmath415 where @xmath416 is a @xmath417 diagonal matrix , and @xmath418 is an @xmath419 matrix of orthogonal columns ( for clarity we remove the unambiguous references to ensemble @xmath66 ) .",
    "it is shown easily that this decomposition also provides the svd for @xmath392 , in the form : @xmath420 where @xmath421 is a @xmath422 matrix of orthogonal columns , as required in the svd decomposition .",
    "thus , the ( pseudo-)inverse of @xmath392 writes : @xmath423 this allows to finally compute the experimental str , from eq .",
    "[ eq : bk - svd]-[eq : ak - svd ] : @xmath424 making use of the fact that @xmath425 .",
    "intriguingly matrix @xmath426 , which describes the eigenvalues of @xmath392 , disappears from the final equation .",
    "only matrix @xmath418 , corresponding to the _ eigenvectors _ of @xmath262 , remains in the equations .",
    "we note @xmath427 which is nothing but the @xmath261 orthogonal projector on @xmath428 .",
    "this leads to the final expression : @xmath429    neuron ensemble @xmath66 only appears through @xmath263 . in particular , as soon as @xmath6 is larger than the number of modes @xmath252 , necessarily @xmath430 , and @xmath431 : all modes are available experimentally , and sensitivity estimates saturate to their maximum value , independently of ensemble @xmath66 .",
    "+ the whole analysis can be performed similarly assuming a finite number of measurement trials @xmath363 .",
    "the only difference is a modification in data matrix @xmath432 , to take into account the biases in mode space induced by an insufficient number of trials : @xmath433 using the same square root of @xmath397 as in eq .",
    "[ eq : bzeta ] .",
    "similar computations lead to the final result : @xmath434 which depends on experimental mode sensitivities ( eq .  [ eq : bzeta ] ) and on @xmath435 , the orthogonal projector on @xmath436 , of dimension @xmath437 .      similarly to the approach above , we can express pcv signals in mode space .",
    "since we do not model time , we only have access to the temporal average @xmath438 , where @xmath73 is the full pcv curve from the main text . from eq .",
    "9 of the main text , it falls easily that @xmath225 . using the optimal @xmath49 for readout ensemble @xmath66 ( eq .  [ eq : a - kap ] , with @xmath439 since @xmath49 has support on @xmath66 )",
    ", we thus predict : @xmath440 which provides the value of @xmath441 for every neuron @xmath14 in the population ( not only in ensemble @xmath66 ) .",
    "making use of the same svd decompositions as above , and of relationship @xmath442 , we finally find : @xmath443 which expresses @xmath444 as a linear combination of mode distribution vectors @xmath247 .",
    "as @xmath66 tends to the full population , @xmath263 tends to @xmath445 and we get @xmath446 , the prediction for choice signals in case of optimal readout @xcite .    in turn , the population average for pcv is @xmath447 for the pcv curve @xmath130 defined in the main text ( eq .",
    "using eq .",
    "[ eq : pi - kap - final ] , and the general fact that @xmath448 , we obtain @xmath449 because @xmath450 ( eq .  [ eq : b - svd ] ) and @xmath451 .",
    "this reveals the interest of multiplying @xmath441 by the corresponding tuning @xmath87 ( see discussion in main text ) : it allows to get rid of the unknown distribution vectors @xmath342 , and instead produce a quantity @xmath88 which is directly related to the underlying modes powers @xmath340 and sensitivities @xmath379 .",
    "we are now better armed to understand how sensitivity and pcv predictions vary as a function of the readout ensemble @xmath66 .",
    "we are mostly interested in averages of these quantities over randomly chosen ensembles @xmath66 of size @xmath1  ; we thus use the generic notation @xmath453 . from eq .",
    "[ eq : y - kap - final ] we find : @xmath454 .",
    "to understand the properties of the @xmath455 matrix @xmath456 , we view the @xmath457 data matrix @xmath458 ( eq .  [ eq : d - kap ] ) as a collection of @xmath1 random vectors @xmath459 in mode space , viewing neuron identities @xmath14 as the random variable .",
    "thus , @xmath263 is the orthogonal projector on the linear span of the @xmath1 sample vectors @xmath264 . as a projector ,",
    "its trace is equal to its rank , so we have @xmath460 .",
    "furthermore , since @xmath461 samples span on average more space than @xmath1 samples , we are insured that @xmath462 , in the sense of positive definite matrices .",
    "finally , intuition and numerical simulations suggest that @xmath456 is almost diagonal . indeed , as the various modes are linearly independent ( eq .  [ eq : orthu ] ) , there is no linear interplay between the different dimensions of @xmath463 across samples @xmath14 : @xmath464 , or equivalently @xmath465 assuming a form of independence between @xmath418 and @xmath426 , it is reasonable to suppose that @xmath466 is close to diagonal as well . in the general case , small deviations from diagonality can probably occur . ] .    assuming that @xmath456 is diagonal , we note its diagonal terms @xmath467 and consider the resulting approximations of sensitivity ( eq .  [ eq : y - kap - final ] ) and mean pcv ( eq .",
    "[ eq : w - kap - final ] ) : @xmath468 the properties of @xmath456 imply that @xmath469 ( trace property ) , and @xmath470 ( growth property ) . as @xmath1 augments , @xmath471 progressively `` fills - in '' the space of modes , starting from the modes with larger power @xmath179 .",
    "indeed , the larger @xmath179 , the more often mode @xmath245 appears in samples @xmath472 . as a useful image",
    ", we may think of the ( very ) rough approximation @xmath473 : only the @xmath1 first modes are revealed by a sample of @xmath1 neurons .",
    "naturally this is only a gross approximation , as can be seen easily by considering a single sample @xmath14 ( @xmath474 ) . from intuition and simulation ,",
    "the true shape of @xmath471 ( at fixed @xmath1 ) is a `` smoothed '' version of @xmath475 , and the degree of smoothing depends on the power law governing the spectrum @xmath476 .    with this image in mind , eq .",
    "[ eq : eky ] shows that the growth of sensitivity with @xmath1 is linked to the progressive summation of mode sensitivities @xmath279 , starting from modes with highest power @xmath179 : @xmath477 with a saturation as soon as all nonzero mode sensitivities @xmath180 are revealed .",
    "conversely , for pcv signals , we can make the rough assumption that @xmath478 , in which case eq .",
    "[ eq : ekwy ] rewrites @xmath479 where each mode @xmath245 contributes with a weight @xmath480 , and @xmath481 provides the normalization factor .",
    "thus , @xmath482 reflects the average power of modes with the higher sensitivity , that are already revealed with @xmath1 neurons . as @xmath1 grows , @xmath471 progressively `` fills - in '' modes in the order of decreasing @xmath179 .",
    "thus we expect @xmath482 to decrease with @xmath1 . finally , as soon as @xmath483 , we have @xmath484 , and @xmath485 reckognizing the expressions for @xmath486 ( eq .  [ eq : b2 ] ) and @xmath487 ( eq .  [ eq : y - infty ] ) . since @xmath488 ,",
    "the predicted evolution of mean pcv signal with @xmath1 follows : @xmath489 @xmath241 is predicted to be positive , to decrease with increasing size @xmath1 , and to saturate at its minimum value once all significant mode sensitivities @xmath180 have been revealed  which is also the moment when sensitivity @xmath387 saturates at its maximum value ( eq .",
    "[ eq : eky ] ) , and corresponds to an optimal readout from the full population .",
    "the implications of these results in terms of extrapolation to large @xmath1 are discussed in the main text ."
  ],
  "abstract_text": [
    "<S> we study a standard linear readout model of perceptual integration from a population of sensory neurons . </S>",
    "<S> we show that the readout can be associated to a set of characteristic equations which summarize the joint trial - to - trial covariance structure of neural activities and animal percept . </S>",
    "<S> these characteristic equations implicitly determine the readout parameters that were used by the animal to create its percept . </S>",
    "<S> in particular , they implicitly constrain the temporal integration window @xmath0 and the typical number of neurons @xmath1 which give rise to the percept . comparing neural and behavioral sensitivity alone </S>",
    "<S> can not disentangle these two sources of perceptual integration , so the characteristic equations also involve a measure of choice signals , like those assessed by the classic experimental measure of choice probabilities . </S>",
    "<S> we then propose a statistical method of analysis which allows to recover the typical scales of integration @xmath0 and @xmath1 from finite numbers of recorded neurons and recording trials , and show the efficiency of this method on an artificial encoding network . </S>",
    "<S> we also study the statistical method theoretically , and relate its laws of convergence to the underlying structure of neural activity in the population , as described through its singular value decomposition . </S>",
    "<S> altogether , our method provides the first thorough interpretation of feedforward percept formation from a population of sensory neurons . </S>",
    "<S> it can readily be applied to experimental recordings in classic sensory decision - making tasks , and hopefully provide new insights into the nature of perceptual integration .    </S>",
    "<S> * 1 group for neural theory , inserm u960 , cole normale suprieure , paris , france + * 2 champalimaud neuroscience program , libson , portugal + @xmath2 e - mail : adrien.wohrer@ens.fr * * </S>"
  ]
}