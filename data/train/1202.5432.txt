{
  "article_text": [
    "the goal of this paper is to investigate the asymptotic behavior of the nadaraya - watson estimator of the regression function @xmath0 in the semiparametric regression model given , for all @xmath1 , by @xmath2 where @xmath3 is a sequence of independent and identically distributed random vectors of @xmath4 and the driven noise @xmath5 is a real martingale difference sequence independent of @xmath3 .",
    "we assume in all the sequel that the unknown @xmath6-dimensional parameter @xmath7 . on the one hand , we make use of the recursive version of the sliced inverse regression ( sir ) method , originally proposed by li @xcite and duan and li @xcite , in order to estimate @xmath8 . on the other hand , we estimate the unknown regression function @xmath0 via a recursive nadaraya - watson estimator which takes into account the previous estimation of the parameter @xmath8 .",
    "our purpose is precisely to investigate the asymptotic behavior of the recursive nadaraya - watson estimator of @xmath0 .",
    "+ one can find a wide range of literature on nonparametric estimation of a regression function .",
    "we refer the reader to @xcite , @xcite , @xcite , @xcite for some excellent books on density and regression function estimation . in the classical situation without any parameter @xmath8 ,",
    "the almost sure convergence of the nadaraya - watson estimator @xcite , @xcite was proved by noda @xcite and its asymptotic normality was established by schuster @xcite . moreover ,",
    "choi , hall and rousson @xcite propose three data - sharpening versions of the nadaraya - watson estimator in order to reduce the asymptotic variance in the central limit theorem .",
    "in our situation , we propose to make use of a recursive nadaraya - watson estimator @xcite of @xmath0 which takes into account the previous estimation of the parameter @xmath8 .",
    "it is given , for all @xmath9 , by @xmath10 with @xmath11 where the kernel @xmath12 is a chosen probability density function and the bandwidth @xmath13 is a sequence of positive real numbers decreasing to zero , such that @xmath14 tends to infinity . for the sake of simplicity",
    ", we propose to make use of @xmath15 with @xmath160,1[$ ] .",
    "the main difficulty arising here is that we have to deal with the recursive sir estimator @xmath17 of @xmath8 inside the kernel @xmath12 .   + the paper is organized as follows .",
    "section @xmath18 is devoted to the recursive sir estimator @xmath17 .",
    "our main results on the asymptotic behavior of @xmath19 are given in section @xmath20 .",
    "under standard regularity assumptions on the kernel @xmath12 , we establish the almost sure pointwise convergence of @xmath19 together with its asymptotic normality .",
    "section @xmath21 contains some numerical experiments on simulated data , illustrating the good performances of our semiparametric estimation procedure .",
    "all the technical proofs are postponed in appendices a and b.",
    "from the seminal work of li @xcite and duan and li @xcite devoted to the sir theory , we know that the eigenvector associated with the maximum eigenvalue of the matrix @xmath22 is collinear with @xmath8 where @xmath23 is positive definite , @xmath24 and @xmath25 is a slicing of the range of @xmath26 into @xmath27 non overlapping slices @xmath28 .",
    "one can observe that since the link function @xmath0 is unknown in the semiparametric regression model , the parameter @xmath8 is not entirely identifiable .",
    "only its direction can be identified without assuming additional constraints .",
    "li @xcite called effective dimension reduction ( edr ) , any direction collinear with @xmath8 .",
    "moreover , the sir theory mainly relies on the so - called linear condition ( lc ) which imposes that for all @xmath29 , @xmath30 $ ] is linear in @xmath31 .",
    "it means that one can find @xmath32 such that @xmath33= \\alpha + \\beta \\theta^{\\prime}x_n .",
    "\\leqno ( \\text{lc})\\ ] ] this condition is required to only hold for the true parameter @xmath8 .",
    "since @xmath8 is unknown , it is not possible in practice to verify it a priori .",
    "hence , we can assume that ( lc ) holds for all possible values of @xmath8 , which is equivalent to elliptical symmetry of the distribution of the identically distributed sequence @xmath3 .",
    "finally , hall and li @xcite mentioned that ( lc ) is not a severe restriction because ( lc ) holds to a good approximation in many problems as the dimension @xmath6 of the regression vector @xmath34 increases .",
    "chen and li @xcite or cook and ni @xcite also provide interesting discussions on the linear condition .    in order to obtain a recursive version of an edr direction estimated with sir approach , we need an analytic expression of the maximum eigenvector of @xmath22 .",
    "it is easily tractable when the range of @xmath26 is divided into two non overlapping slices @xmath35 and @xmath36 .",
    "hereafter we shall assume that @xmath37 .",
    "in this special case , it is not hard to see that @xmath38 where @xmath39 and @xmath40-{\\mathbb{e}}[x_n]$ ] with @xmath41 for @xmath42 .",
    "moreover , it is straightforward to show that the eigenvector associated to the maximum eigenvalue of @xmath22 can be written as @xmath43 this vector @xmath44 is therefore an edr direction . for the sake of simplicity ,",
    "we identify in all the sequel the edr direction @xmath44 with @xmath8 .",
    "our purpose is now to propose an estimator of the edr direction @xmath8 .",
    "first of all , let us recall the non recursive sir estimator @xmath45 of @xmath8 given by nguyen and saracco @xcite .",
    "the estimator @xmath45 can be easily obtained from the sample @xmath46 by substituting the theoritical moments by their sample couterparts .",
    "more precisely , @xmath45 is given by @xmath47 where @xmath48 and , for @xmath49 , @xmath50 where @xmath51 next , we focus our attention on the recursive sir estimator @xmath52 of @xmath8 proposed by bercu , nguyen and saracco @xcite , @xcite .",
    "we split the sample into two parts : the subsample of the first @xmath53 observations @xmath54 , and the new observation @xmath55 . on the one hand ,",
    "the inverse of the matrix @xmath56 given by may be recursively calculated via the riccati equation @xcite , @xmath57 where @xmath58 and @xmath59 . on the other hand ,",
    "we can also obtain the recursive form of @xmath60 . as a matter of fact",
    ", we have for @xmath49 , @xmath61 where @xmath62 denotes the slice containing the observation @xmath26 and @xmath63 .",
    "we deduce from and that the recursive sir estimator @xmath52 is given by @xmath64    the sir estimators @xmath45 and @xmath52 share the same asymptotic properties , previously established in @xcite , under the following classical hypothesis .",
    "@xmath65    [ lemlgnsir ] assume that @xmath66 and @xmath67 hold .",
    "then , @xmath52 converges a.s . to @xmath8 , @xmath68 where the limiting covariance matrix @xmath69 may be explicitely calculated .",
    "our purpose is to investigate the asymptotic properties of the recursive nadaraya - watson estimator @xmath19 of the link function @xmath0 given by .",
    "first of all , we assume that the kernel @xmath12 is a positive symmetric function , bounded with compact support , twice differentiable with bounded derivatives , satisfying @xmath70 moreover , it is necessary to add the following standard hypothesis . @xmath71    our first result deals with the almost sure convergence of the estimator @xmath72 .",
    "[ thmascvg ] assume that @xmath66 and @xmath67 to @xmath73 hold .",
    "in addition , suppose that the sequence @xmath74 has a finite moment of order @xmath75 .",
    "then , for any @xmath76 , we have @xmath77 more precisely , if the bandwidth @xmath13 is given by @xmath15 with @xmath78 , @xmath79 while , if @xmath80 , @xmath81    the proof is given appendix a.    in the particular case where @xmath3 is a sequence of independent random vectors of @xmath4 sharing the same @xmath82 distribution where the covariance matrix @xmath83 is positive definite , we can replace @xmath84 by @xmath85 into and .",
    "consequently , for any @xmath76 , we obtain that if @xmath78 , @xmath86 while , if @xmath80 , @xmath87    the asymptotic normality of the estimator @xmath72 is as follows .",
    "[ thmclt ] assume that @xmath66 and @xmath67 to @xmath73 hold .",
    "in addition , suppose that the sequence @xmath74 has a finite moment of order @xmath88 and that the sequence @xmath89 has a finite conditional moment of order @xmath90 .",
    "then , as soon as the bandwidth @xmath13 satisfies @xmath15 with @xmath91 , we have for any @xmath76 , the pointwise asymptotic normality @xmath92 where @xmath93 stands for the probability density function associated with @xmath94 .    the proof is given appendix b.",
    "the goal of this section is to illustrate via some numerical experiments the theoretical results of section @xmath20 .",
    "we will provide the numerical behavior of our recursive estimators combining the recursive nadaraya - watson estimator of the link function @xmath0 together with the recursive sir estimator of the parameter @xmath8 .",
    "first of all , we describe in section 4.1 the simulated model used in the numerical study and we present the estimation procedure , in particular the choice of the bandwidth parameter @xmath95 by a cross - validation criterion .",
    "then , we illustrate in sections 4.2 and 4.3 the almost sure convergence and the asymptotic normality of our recursive nadaraya - watson estimator of @xmath0 .",
    "we consider the semiparametric regression model given , for all @xmath1 , by @xmath96 where the link function @xmath0 is defined , for all @xmath97 , by @xmath98 the parameter @xmath8 belongs to @xmath4 with @xmath99 and is given by @xmath100 moreover , @xmath3 is a sequence of independent random vectors of @xmath4 sharing the same @xmath101 distribution , while @xmath5 is a sequence of independent random variables with standard @xmath102 distribution , independent of @xmath3 . in figure",
    "[ nuage ] , we present two scatterplots for a sample of size @xmath103 generated from model ( m ) . on the left side , one can observe the data in the `` true '' reduction subspace , that is the scatterplot of @xmath104 based on the `` true '' edr direction @xmath8 . on the right side",
    ", we plot the data obtained from the estimated edr direction @xmath52 calculated via our recursive sir procedure , that is the scatterplot of @xmath105 .",
    "one can clearly notice that the edr direction has been well estimated .",
    "[ nuage ]    [ cols=\"^,^ \" , ]     figure 4.3 .",
    "+ asymptotic normality of @xmath106 to @xmath107 for @xmath18 different values of @xmath108 .",
    "proof of theorem [ thmascvg ]    in order to prove the almost sure pointwise convergence of theorem [ thmascvg ] , we shall denote for all @xmath109 @xmath110 and @xmath111 where @xmath112 .",
    "we clearly obtain from the main decomposition @xmath113 we shall establish the asymptotic behavior of each sequence @xmath114 , @xmath115 and @xmath116 .",
    "let @xmath117 be the filtration given by @xmath118 .",
    "first of all , we can split @xmath119 into two terms , @xmath120 where @xmath121\\bigr ) \\hspace{0.5 cm } \\text{and } \\hspace{0.5 cm } r_n^{(n)}(x)=\\sum_{k=1}^{n } { \\mathbb{e } } [ w_{k}(x ) | { \\mathcal{f}}_{k-1}].\\ ] ] on the one hand , we have @xmath122=\\frac{1}{h_{n}}\\int_{{\\mathbb{r}}^p}k\\bigl(\\frac{x-{\\widehat}{\\theta}_{n-1}^{\\,\\prime}x_n}{h_{n}}\\bigr)g(x_n)\\,dx_n.\\ ] ] we can assume without loss of generality that , for @xmath123 large enough , at least one component of @xmath124 is different from zero a.s . as a matter of fact",
    ", we already saw from lemma [ lemlgnsir ] that @xmath124 converges a.s .",
    "to @xmath8 which is different from zero . for the sake of simplicity",
    ", suppose that the first component @xmath125 a.s .",
    "we can make the change of variables @xmath126 and @xmath127 .",
    "the jacobian of this linear transformation is given by @xmath128 consequently , we obtain that @xmath129= \\int _ { { \\mathbb{r}}}k(z ) h(\\widehat\\theta_{n-1},x - zh_n)dz\\ ] ] where @xmath130 one can observe that @xmath93 is exactly the probability density function associated with the identically distributed sequence @xmath94 . therefore ,",
    "as the probability density function @xmath131 is continuous , twice differentiable with bounded derivatives , we deduce from togheter with taylor s formula that @xmath132   & = & \\int_{{\\mathbb{r}}}k(z)\\bigl ( h({\\widehat}{\\theta}_{n-1},x ) -zh_nh^\\prime({\\widehat}{\\theta}_{n-1},x )   \\\\ & & \\qquad + \\frac{z^2h_n^2}{2}h^{\\prime \\prime}({\\widehat}{\\theta}_{n-1},x -zh_n\\xi ) \\bigr)dz , \\\\ & = & h({\\widehat}{\\theta}_{n-1},x)+\\frac{h_{n}^{2}}{2}\\int_{{\\mathbb{r}}}z^{2}k(z ) h^{\\prime \\prime}({\\widehat}{\\theta}_{n-1},x -zh_n\\xi)dz \\end{aligned}\\ ] ] where @xmath133 .",
    "consequently , for @xmath123 large enough , @xmath134 -h({\\widehat}{\\theta}_{n-1},x)\\bigr|\\leq m_{h}\\tau^{2}h_{n}^{2 } \\hspace{1cm}\\text{a.s.}\\ ] ] where @xmath135 hence , we find from that @xmath136 - h({\\widehat}{\\theta}_{k-1},x)\\bigr| = \\mathcal{o}\\bigl(\\sum _ { k=1}^{n}h^2_k\\bigr ) \\hspace{1cm}\\text{a.s.}\\ ] ] it follows from the continuity of @xmath137 together with the fact that @xmath124 converges to @xmath8 a.s .",
    "and @xmath138 goes to zero that @xmath139 = h(\\theta , x ) \\hspace{1cm}\\text{a.s.}\\ ] ] which of course immediately implies that for all @xmath97 @xmath140 on the other hand , @xmath141 is a square integrable martingale difference sequence with predictable quadratic variation given by @xmath142 , \\\\ & = & \\sum_{k=1}^n \\bigl ( { \\mathbb{e}}[w_{k}^2(x)|{\\mathcal{f}}_{k-1}]-   { \\mathbb{e}}^2[w_{k}(x)|{\\mathcal{f}}_{k-1}]\\bigr).\\end{aligned}\\ ] ] via the same change of variables as in , we obtain that @xmath143 & = & \\frac{1}{h_n } \\int_{{\\mathbb{r}}}k^2(z ) h(\\widehat\\theta_{n-1},x - zh_n)dz ,   \\\\ & = & \\frac{1}{h_n } \\int_{{\\mathbb{r}}}k^2(z)\\bigl ( h({\\widehat}{\\theta}_{n-1},x ) -zh_nh^\\prime({\\widehat}{\\theta}_{n-1},x )   \\\\ & & \\qquad   + \\frac{z^2h_n^2}{2}h^{\\prime \\prime}({\\widehat}{\\theta}_{n-1},x -zh_n\\xi ) \\bigr)dz\\end{aligned}\\ ] ] where @xmath133 . consequently ,",
    "for @xmath123 large enough , @xmath144 -\\frac{\\nu^2}{h_n } h({\\widehat}{\\theta}_{n-1},x)\\bigr|\\leq m_{h}\\mu^{2}h_{n } \\hspace{1cm}\\text{a.s.}\\ ] ] where @xmath145 hence , ensures that @xmath146 - \\frac{\\nu^2}{h_k } h({\\widehat}{\\theta}_{k-1},x)\\bigr| = \\mathcal{o}\\bigl(\\sum _ { k=1}^{n}h_k\\bigr ) \\hspace{1cm}\\text{a.s.}\\ ] ] however , it is not hard to see that @xmath147 therefore , it follows from together with the almost sure convergence of @xmath148 to @xmath93 and toeplitz s lemma that @xmath149 = \\frac{\\nu^2}{1+\\alpha}h(\\theta , x )   \\hspace{1cm}\\text{a.s.}\\ ] ] furthermore , we also have from that @xmath150 = h^2(\\theta , x ) \\hspace{1cm}\\text{a.s.}\\ ] ] consequently , we deduce from and that for all @xmath97 , @xmath151 we are now in position to make use of the strong law of large numbers for martingales given e.g. by theorem 1.3.15 of @xcite .",
    "as the probability density function @xmath131 is positive on its support , we have for all @xmath109 , @xmath152 , which implies that @xmath153 goes to infinity a.s .",
    "hence , for any @xmath154 , @xmath155 a.s . which leads to @xmath156 then , we obtain from , and that for all @xmath97 @xmath157 we shall now investigate the asymptotic behavior of the sequence @xmath114 .",
    "since @xmath3 and @xmath5 are independent , @xmath114 is a square integrable martingale difference sequence with predictable quadratic variation given by @xmath158   = \\sigma^2 \\sum_{k=1}^n { \\mathbb{e}}[w_{k}^2(x)|{\\mathcal{f}}_{k-1}].\\ ] ] then , it follows from convergence that @xmath159 consequently , we obtain from the strong law of large numbers for martingales that for any @xmath154 and that for all @xmath97 , @xmath160 it remains to study the asymptotic behavior of the sequence @xmath115 .",
    "we can split @xmath161 into two terms , @xmath162 where @xmath163 , @xmath164 the right - hand side of is easy to handle . as a matter of fact ,",
    "the kernel @xmath12 is compactly supported which means that one can find a positive constant @xmath165 such that @xmath12 vanishes outside the interval @xmath166 $ ] .",
    "thus , for all @xmath167 and all @xmath97 , @xmath168 in addition , the function @xmath0 is lipschitz , so it exists a positive constant @xmath169 such that for all @xmath167 latexmath:[\\[\\label{flip }    consequently , we obtain from that for all @xmath97 @xmath171 moreover , via the same lines as in the proof of , we find that @xmath172 = \\frac{1}{1- \\alpha}h(\\theta , x ) \\hspace{1cm}\\text{a.s.}\\ ] ] furthermore , denote @xmath173\\bigr).\\ ] ] one can observe that @xmath174 is a square integrable martingale difference sequence with bounded increments and predictable quadratic variation given by @xmath175 , \\\\ & = & \\sum_{k=1}^n h_k^2 \\bigl ( { \\mathbb{e}}[w_{k}^2(x)|{\\mathcal{f}}_{k-1}]-   { \\mathbb{e}}^2[w_{k}(x)|{\\mathcal{f}}_{k-1}]\\bigr).\\end{aligned}\\ ] ] hence , it follows from and together with the almost sure convergence of @xmath148 to @xmath93 and toeplitz s lemma that @xmath176 consequently , we obtain from the strong law of large numbers for martingales that @xmath177 then , we infer from the conjunction of , and that for all @xmath97 latexmath:[\\[\\label{cvgdelta }    the left - hand side of is much more difficult to handle .",
    "we can use once again the assumption that the function @xmath0 is lipschitz to deduce that it exists a positive constant @xmath169 such that for all @xmath167 @xmath179 .",
    "hence , it immediately follows from that for all @xmath97 @xmath180 denote @xmath181 where @xmath182 is a sequence of positive real numbers which will be explicitely given later . on the one hand",
    ", we immediately have from the triangle inequality that on the set @xmath183 , @xmath184 on the other hand , we also have on the set @xmath185 , @xmath186 which implies that @xmath187 .",
    "consequently , we obtain from that @xmath188 we already saw from that @xmath189 moreover , it is assumed that the sequence @xmath74 has a finite moment of order @xmath75 which ensures that @xmath190 consequently , we find from lemma that @xmath191 therefore , we clearly have @xmath192 furthermore , it is not hard to see that @xmath193 hence , via the same lines as in the proof of , we obtain that @xmath194 then , we deduce from the conjunction of , , , and that @xmath195 consequently , we infer from and that for all @xmath97 @xmath196 finally , we can conclude from together with , and that @xmath197 with the almost sure rates of convergence given by and , which completes the proof of theorem [ thmascvg ] . @xmath198",
    "proof of theorem [ thmclt ]    we already saw that @xmath114 is a square integrable martingale difference sequence with predictable quadratic variation satisfying @xmath199 in order to establish the asymptotic normality of theorem [ thmclt ] , it is necessary to prove that the sequence @xmath114 satisfies the lindeberg condition , that is for all @xmath200 , @xmath201 \\limp 0\\ ] ] where @xmath202 .",
    "we have assumed that the sequence @xmath203 has a finite conditional moment of order @xmath90 which means that @xmath204 < + \\infty \\hspace{1cm}\\text{a.s.}\\ ] ] consequently , for all @xmath200 , we have @xmath205 , \\nonumber \\\\ & \\leq & \\frac{1}{{\\varepsilon}^{b-2 } n^c}\\sum_{k=1}^{n}{\\mathbb{e } } [ w_{k}^b(x)|{\\mathcal{f}}_{k-1 } ] { \\mathbb{e}}[| { \\varepsilon}_{k } |^b|{\\mathcal{f}}_{k-1 } ] , \\nonumber \\\\ & \\leq & \\frac{1}{{\\varepsilon}^{b-2 } n^c } \\sup_{1 \\leq k \\leq n } { \\mathbb{e}}[| { \\varepsilon}_{k } |^b|{\\mathcal{f}}_{k-1}]\\sum_{k=1}^{n}{\\mathbb{e } } [ w_{k}^b(x)|{\\mathcal{f}}_{k-1 } ]   \\label{lindeberg2}\\end{aligned}\\ ] ] where @xmath206 .",
    "in addition , via the same lines as in the proof of , we obtain that @xmath207 = \\frac{\\xi^b}{1+\\alpha(b-1)}h(\\theta , x )   \\hspace{1cm}\\text{a.s.}\\ ] ] where @xmath208 therefore , we deduce from together with and that , for all @xmath200 , @xmath209 where @xmath210 .",
    "we recall that @xmath90 which means that @xmath211 .",
    "it ensures that the lindeberg condition is satisfied .",
    "hence , it follows from the central limit theorem for martingales given e.g. by corollary 2.1.10 of @xcite that for all @xmath97 , @xmath212 furthermore , as soon as @xmath213 and @xmath91 , we clearly obtain from that @xmath214 finally , we find from together with , , and slutsky s lemma that , for all @xmath97 , @xmath215 which acheives the proof of theorem [ thmclt ] . @xmath198"
  ],
  "abstract_text": [
    "<S> we investigate the asymptotic behavior of the nadaraya - watson estimator for the estimation of the regression function in a semiparametric regression model . on the one hand , we make use of the recursive version of the sliced inverse regression method for the estimation of the unknown parameter of the model . on the other hand , we implement a recursive nadaraya - watson procedure for the estimation of the regression function which takes into account the previous estimation of the parameter of the semiparametric regression model . </S>",
    "<S> we establish the almost sure convergence as well as the asymptotic normality for our nadaraya - watson estimator . </S>",
    "<S> we also illustrate our semiparametric estimation procedure on simulated data . </S>"
  ]
}