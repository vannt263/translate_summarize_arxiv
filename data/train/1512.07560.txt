{
  "article_text": [
    "surrogate modeling techniques are widely used and studied in engineering and research .",
    "their main purpose is to replace an expensive - to - evaluate function @xmath0 by a simple response surface @xmath1 also called surrogate model or meta - model . notice that @xmath0 can be a computation - intensive simulation code .",
    "these surrogate models are based on a given training set of @xmath2 observations @xmath3 where @xmath4 and @xmath5 .",
    "the accuracy of the surrogate model relies , _ inter alia _",
    ", on the relevance of the training set .",
    "the aim of surrogate modeling is generally to estimate some features of the function @xmath0 using @xmath1 .",
    "of course one is looking for the best trade - off between a good accuracy of the feature estimation and the number of calls of @xmath0 .",
    "consequently , the design of experiments ( doe ) , that is the sampling of @xmath6 , is a crucial step and an active research field .",
    "there are two ways to sample : either drawing the training set @xmath6 at once or building it sequentially . among the sequential techniques ,",
    "some are based on surrogate models .",
    "they rely on the feature of @xmath0 that one wishes to estimate .",
    "popular examples are the ego @xcite and the stepwise uncertainty reduction ( sur ) @xcite .",
    "these two methods use gaussian process regression also called kriging model .",
    "it is a widely used surrogate modeling technique .",
    "its popularity is mainly due to its statistical nature and properties .",
    "indeed , it is a bayesian inference technique for functions . in this stochastic frame",
    ", it provides an estimate of the prediction error distribution .",
    "this distribution is the main tool in gaussian surrogate sequential designs .",
    "for instance , it allows the introduction and the computation of different sampling criteria such as the expected improvement ( ei ) @xcite or the expected feasibility ( ef ) @xcite .",
    "away from the gaussian case , many surrogate models are also available and useful .",
    "notice that none of them including the gaussian process surrogate model are the best in all circumstances @xcite .",
    "classical surrogate models are for instance support vector machine @xcite , linear regression @xcite , moving least squares @xcite .",
    "more recently a mixture of surrogates has been considered in @xcite",
    ". nevertheless , these methods are generally not naturally embeddable in some stochastic frame .",
    "hence , they do not provide any prediction error distribution . to overcome this drawback ,",
    "several empirical design techniques have been discussed in the literature .",
    "these techniques are generally based on resampling methods such as bootstrap , jackknife , or cross - validation .",
    "for instance , gazut et al .",
    "@xcite and jin et al .",
    "@xcite consider a population of surrogate models constructed by resampling the available data using bootstrap or cross - validation .",
    "then , they compute the empirical variance of the predictions of these surrogate models . finally , they sample iteratively the point that maximizes the empirical variance in order to improve the accuracy of the prediction . to perform optimization , kleijnen et al .",
    "@xcite use a bootstrapped kriging variance instead of the kriging variance to compute the expected improvement .",
    "their algorithm consists in maximizing the expected improvement computed through bootstrapped kriging variance .",
    "however , most of these resampling method - based design techniques lead to clustered designs @xcite .    in this paper",
    ", we give a general way to build an empirical prediction distribution allowing sequential design strategies in a very broad frame .",
    "its support is the set of all the predictions obtained by the cross - validation surrogate models .",
    "the novelty of our approach is that it provides a prediction uncertainty distribution .",
    "this allows a large set of sampling criteria .",
    "furthermore , it leads naturally to non - clustered designs as explained in section [ sec : ref ] .",
    "the paper is organized as follows .",
    "we start by presenting in section [ sec : notations ] the background and notations . in section [ sec : up ] we introduce the universal prediction ( up ) empirical distribution . in sections [ sec : ref ] and",
    "[ sec : opt ] , we use and study features estimation and the corresponding sampling schemes built on the up empirical distribution .",
    "section [ sec : ref ] is devoted to the enhancement of the overall model accuracy .",
    "section [ sec : opt ] concerns optimization . in section",
    "[ sec : mix ] , we study a real life industrial case implementing the methodology developed in section [ sec : ref ] . section [ sec : empinversion ] deals with the inversion problem . in section [ sec : conclusion ] , we conclude and discuss the possible extensions of our work .",
    "all proofs are postponed to section [ sec : proofs ] .",
    "to begin with , let @xmath0 denote a real - valued function defined on @xmath7 , a nonempty compact subset of the euclidean space @xmath8 ( @xmath9 ) . in order to estimate @xmath0 , we have at hand a sample of size @xmath2 ( @xmath10 ) : @xmath11 with @xmath12 , @xmath13 and @xmath14 where @xmath15 for @xmath16 .",
    "we note @xmath17 .",
    "let @xmath18 denote the observations : @xmath19 .",
    "using @xmath18 , we build a surrogate model @xmath20 that mimics the behaviour of @xmath0 .",
    "for example , @xmath20 can be a second order polynomial regression model . for @xmath21 ,",
    "we set @xmath22 and so @xmath23 is the surrogate model obtained by using only the dataset @xmath24 .",
    "we will call @xmath20 the master surrogate model and @xmath25 its sub - models .",
    "further , let @xmath26 denote a given distance on @xmath8 ( typically the euclidean one ) .",
    "for @xmath27 and @xmath28 , we set : @xmath29 and if @xmath30 is finite ( @xmath31 ) , for @xmath32 let @xmath33 denote @xmath34 . finally , we set @xmath35 , the largest distance of an element of @xmath36 to its nearest neighbor .      training an algorithm and evaluating its statistical performances on the same data yields an optimistic result @xcite . it is well known that it is easy to over - fit the data by including too many degrees of freedom and so inflate the fit statistics .",
    "the idea behind cross - validation ( cv ) is to estimate the risk of an algorithm splitting the dataset once or several times .",
    "one part of the data ( the training set ) is used for training and the remaining one ( the validation set ) is used for estimating the risk of the algorithm .",
    "simple validation or hold - out @xcite is hence a cross - validation technique .",
    "it relies on one splitting of the data .",
    "then one set is used as training set and the second one is used as validation set .",
    "some other cv techniques consist in a repetitive generation of hold - out estimator with different data splitting @xcite .",
    "one can cite , for instance , the leave - one - out cross - validation ( loo - cv ) and the @xmath37-fold cross - validation ( kfcv ) .",
    "kfcv consists in dividing the data into @xmath38 subsets .",
    "each subset plays the role of validation set while the remaining @xmath39 subsets are used together as the training set .",
    "loo - cv method is a particular case of kfcv with @xmath40 .",
    "the sub - models @xmath41 introduced in paragraph [ sec : form ] are used to compute loo estimator of the master surrogate model @xmath42 .",
    "in fact , the loo errors are @xmath43 .",
    "notice that the sub - models are used to estimate a feature of the master surrogate model . in our study , we will be interested in the distribution of the local predictor for all @xmath44 ( @xmath45 is not necessarily a design point ) and we will also use the sub - models to estimate this feature . indeed , this distribution will be estimated by using loo - cv predictions leading to the definition of the universal prediction ( up ) distribution .",
    "as discussed in the previous section , cross - validation is used as a method for estimating the prediction error of a given model . in our case",
    ", we introduce a novel use of cross - validation in order to estimate the local uncertainty of a surrogate model prediction .",
    "in fact , we assume , in equation , that cv errors are an approximation of the errors of the master model .",
    "the idea is to consider cv prediction as realizations of @xmath1 .",
    "hence , for a given surrogate model @xmath1 and for any @xmath46 , @xmath47 @xmath48 define an empirical distribution of @xmath49 at @xmath45 . in the case of an interpolating surrogate model and a deterministic simulation code @xmath0 ,",
    "it is natural to enforce a zero variance at design points .",
    "consequently , when predicting on a design point @xmath50 we neglect the prediction @xmath23 .",
    "this can be achieved by introducing weights on the empirical distribution .",
    "these weights avoid the pessimistic sub - model predictions that might occur in a region while the global surrogate model fits the data well in that region .",
    "let @xmath51 be the weighted empirical distribution based on the @xmath2 different predictions of the loo - cv sub - models @xmath52 and weighted by @xmath53 defined in equation :    @xmath54    such binary weights lead to unsmooth design criteria .",
    "in order to avoid this drawback , we introduce smoothed weights .",
    "direct smoothing based on convolution product would lead to computations on voronoi cells .",
    "we prefer to use the simpler smoothed weights defined in equation .",
    "@xmath55    notice that @xmath53 increases with the distance between the @xmath56 design point @xmath50 and @xmath45 .",
    "in fact , the least weighted predictions is @xmath57 where @xmath58 is the index of the nearest design point to @xmath45 . in general",
    ", the prediction @xmath59 is locally less reliable in a neighborhood of @xmath50 .",
    "the proposed weights determine the local relative confidence level of a given sub - model predictions .",
    "the term `` relative '' means that the confidence level of one sub - model prediction is relative to the remaining sub - models predictions due to the normalization factor in equation .",
    "the smoothing parameter @xmath60 tunes the amount of uncertainty of @xmath61 in a neighborhood of @xmath50 .",
    "several options are possible to choose @xmath60 .",
    "we suggest setting @xmath62 . indeed , this is a well suited choice for practical cases .",
    "the universal prediction distribution ( up distribution ) is the weighted empirical distribution : @xmath63    this probability measure is nothing more than the empirical distribution of all the predictions provided by cross - validation sub - models weighted by local smoothed masses .    for @xmath44",
    "we call @xmath64 ( equation ) the local up variance and @xmath65 ( equation ) the up expected value",
    ". @xmath66    @xmath67      let us consider the viana function defined over @xmath68 $ ] @xmath69    let @xmath70 be the design of experiments such that @xmath71 and @xmath72 their image by @xmath73 .",
    "we used a gaussian process regression @xcite with constant trend function and matrn 5/2 covariance function @xmath74 .",
    "we display in figure [ fig : viana7dp ] the design points , the cross - validation sub - models predictions @xmath41 , @xmath75 and the master model prediction @xmath76 .",
    "notice that in the interval @xmath77 $ ] ( where we have 4 design points ) the discrepancy between the master model and the cv sub - models predictions is smaller than in the remaining space .",
    "moreover , we displayed horizontally the _ up distribution _ at @xmath78 and @xmath79 to illustrate the weighting effect .",
    "one can notice that :    * at @xmath80 the least weighted predictions are @xmath81 and @xmath82 .",
    "these predictions do not use the two closest design points to @xmath80 : ( @xmath83 , respectively @xmath84 ) .",
    "* at @xmath85 , @xmath86 is the least weighted prediction",
    ".    furthermore , we display in figure [ fig : vianavar ] the master model prediction and region delimited by @xmath87 and @xmath88 .",
    "one can notice that the standard deviation is null at design points .",
    "in addition , its local maxima in the interval @xmath89 $ ] ( where we have more design points density ) are smaller than its maxima in the remaining space region .",
    "in this section , we use the _ up distribution _ to define an adaptive refinement technique called the universal prediction - based surrogate modeling adaptive refinement technique up - smart .",
    "the main goal of sequential design is to minimize the number of calls of a computationally expensive function .",
    "gaussian surrogate models @xcite are widely used in adaptive design strategies . indeed ,",
    "gaussian modeling gives a bayesian framework for sequential design . in some cases",
    ", other surrogate models might be more accurate although they do not provide a theoretical framework for uncertainty assessment .",
    "we propose here a new universal strategy for adaptive sequential design of experiments .",
    "the technique is based on the up distribution .",
    "so , it can be applied to any type of surrogate model .    in the literature , many strategies have been proposed to design the experiments ( for an overview , the interested reader is referred to @xcite ) .",
    "some strategies , such as latin hypercube sampling ( lhs ) @xcite , maximum entropy design @xcite , and maximin distance designs @xcite are called one - shot sampling methods .",
    "these methods depend neither on the output values nor on the surrogate model .",
    "however , one would naturally expect to design more points in the regions with high nonlinear behavior .",
    "this intuition leads to adaptive strategies .",
    "a doe approach is said to be adaptive when information from the experiments ( inputs and responses ) as well as information from surrogate models are used to select the location of the next point .    by adopting this definition ,",
    "adaptive doe methods include for instance surrogate model - based optimization algorithms , probability of failure estimation techniques and sequential refinement techniques .",
    "sequential refinement techniques aim at creating a more accurate surrogate model .",
    "for example , lin et al .",
    "@xcite use multivariate adaptive regression splines ( mars ) and kriging models with sequential exploratory experimental design ( seed ) method .",
    "it consists in building a surrogate model to predict errors based on the errors on a test set .",
    "goel et al .",
    "@xcite use an ensemble of surrogate models to identify regions of high uncertainty by computing the empirical standard deviation of the predictions of the ensemble members .",
    "our method is based on the predictions of the cv sub - models . in the literature , several cross - validation - based techniques have been discussed .",
    "@xcite propose to add the design point that maximizes the accumulative error ( ae ) .",
    "the ae on @xmath44 is computed as the sum of the loo - cv errors on the design points weighted by influence factors .",
    "this method could lead to clustered samples . to avoid this effect ,",
    "the authors @xcite propose to add a threshold constraint in the maximization problem .",
    "busby et al .",
    "@xcite propose a method based on a grid and cv .",
    "it affects the cv prediction errors at a design point to its containing cell in the grid .",
    "then , an entropy approach is performed to add a new design point .",
    "more recently , xu et al .",
    "@xcite suggest the use of a method based on voronoi cells and cv .",
    "kleijnen et al.@xcite propose a method based on the jackknife s pseudo values predictions variance .",
    "jin et al .",
    "@xcite present a strategy that maximizes the product between the deviation of cv sub - models predictions with respect to the master model prediction and the distance to the design points .",
    "aute et al .",
    "@xcite introduce the space - filling cross - validation trade - off ( sfcvt ) approach .",
    "it consists in building a new surrogate model over loo - cv errors and then add a point that maximizes the new surrogate model prediction under some space - filling constraints",
    ". in general cross - validation - based approaches tend to allocate points close to each other resulting in clustering @xcite . this is not desirable for deterministic simulations .",
    "the idea behind up - smart is to sample points where the up distribution variance ( equation ) is maximal .",
    "most of the cv - based sampling criteria use cv errors . here",
    ", we use the local predictions of the cv sub - models .",
    "moreover , notice that the up variance is null on design points for interpolating surrogate models .",
    "hence , up - smart does not naturally promote clustering .",
    "however , @xmath90 can vanish even if @xmath45 is not a design points . to overcome this drawback , we add a distance penalization .",
    "this leads to the up - smart sampling criterion @xmath91 ( equation ) .",
    "@xmath92 where @xmath93 is called exploration parameter .",
    "one can set @xmath94 as a small percentage of the global variation of the output .",
    "up - smart is the adaptive refinement algorithm consisting in adding at step @xmath2 a point @xmath95 .      in this subsection ,",
    "we present the performance of the up - smart .",
    "we present first the used surrogate - models .      [ [ kriging ] ] kriging + + + + + + +    kriging  @xcite or gaussian process regression is an interpolation method .",
    "universal kriging fits the data using a deterministic trend and governed by prior covariances .",
    "let @xmath96 , be a covariance function on @xmath97 , and let @xmath98 be the basis functions of the trend .",
    "let us denote @xmath99 the vector @xmath100 and let @xmath101 be the matrix with entries @xmath102 . furthermore , let @xmath103 be the vector @xmath104 and @xmath105 the matrix with entries @xmath106 , for @xmath107 .",
    "then , the conditional mean of the gaussian process with covariance @xmath108 and its variance are given in equations ( , ) @xmath109    note that the conditional mean is the prediction of the gaussian process regression .",
    "further , we used two kriging instances with different sampling schemes in our test bench .",
    "both use constant trend function and a matrn 5/2 covariance function .",
    "the first design is obtained by maximizing the _ up distribution _ variance ( equation ) . and the second one is obtained by maximizing the kriging variance @xmath110 .    [",
    "[ genetic - aggregation ] ] genetic aggregation + + + + + + + + + + + + + + + + + + +    the genetic aggregation response surface is a method that aims at selecting the best response surface for a given design of experiments .",
    "it solves several surrogate models , performs aggregation and selects the best response surface according to the cross - validation errors .",
    "the use of such response surface , in this test bench , aims at checking the universality of the _ up distribution _ : the fact that it can be applied for all types of surrogate models .      in order to test the performances of the method we launched different refinement processes for the following set of test functions :    * branin : @xmath111 . * six - hump camel : @xmath112 .",
    "* hartmann6 : @xmath113 . @xmath36,@xmath114 and",
    "@xmath115 can be found in @xcite . *",
    "viana : ( equation )    for each function we generated by optimal latin hyper sampling design the number of initial design points @xmath116 , the number of refinement points @xmath117 .",
    "we also generated a set of @xmath118 test points and their response @xmath119 .",
    "the used values are available in table [ tab:1 ] .",
    ".used test functions [ cols=\"<,<,<,<,<\",options=\"header \" , ]     the results show that up - smart gives a better approximation . here",
    ", it is used with a genetic aggregation of several response surface .",
    "even if the good quality may be due to the response surface itself , it highlights the fact that up - smart made the use of such surrogate model - based refinement strategy possible .",
    "inversion approaches consist in the estimation of contour lines , excursion sets or probability of failure .",
    "these techniques are specially used in constrained optimization and reliability analysis .",
    "several iterative sampling strategies have been proposed to handle these problems .",
    "the empirical distribution @xmath120 can be used for inversion problems .",
    "in fact , we can compute most of the well - known criteria such as the bichon s criterion @xcite or the ranjan s criterion @xcite using the up distribution . in this section",
    ", we discuss some of these criteria : the targeted mean square error @xmath121 @xcite , bichon @xcite and the ranjan criteria @xcite .",
    "the reader can refer to chevalier et al .",
    "@xcite for an overview .",
    "let us consider the contour line estimation problem : let @xmath122 be a fixed threshold .",
    "we are interested in enhancing the surrogate model accuracy in @xmath123 and in its neighborhood .",
    "[ [ targeted - mse - tmse ] ] targeted mse ( tmse ) + + + + + + + + + + + + + + + + + + +    the targeted mean square error ( tmse ) @xcite aims at decreasing the mean square error where the kriging prediction is close to t.    it is the probability that the response lies inside the interval @xmath124 $ ] where the parameter @xmath125 tunes the size of the window around the threshold @xmath122 .",
    "high values make the criterion more exploratory while low values concentrate the evaluation around the contour line .",
    "we can compute an estimation of the value of this criterion using the _ up distribution _ ( equation ) .",
    "@xmath126 } \\big(\\hat{s}_{n ,- i}(\\mathbf{x})\\big)\\\\   & =   \\sum\\limits_{i=1}^n w_{i , n}(\\mathbf{x } )   1 _ { \\big[-\\varepsilon , \\varepsilon \\big ] } \\big(\\hat{s}_{n ,- i}(\\mathbf{x } ) - t\\big )     \\end{aligned}\\ ] ] notice that the last criterion takes into account neither the variability of the predictions at @xmath45 nor the magnitude of the distance between the predictions and @xmath122 .",
    "[ [ bichon - criterion ] ] bichon criterion + + + + + + + + + + + + + + + +    the expected feasibility defined in @xcite aims at indicating how well the true value of the response is expected to be close to the threshold @xmath122 .",
    "the bounds are defined by @xmath127 which is proportional to the kriging standard deviation @xmath128 .",
    "bichon proposes using @xmath129 @xcite .",
    "this criterion can be extended to the case of the up distribution .",
    "we define in equation @xmath130 the empirical bichon s criterion where @xmath127 is proportional to the empirical standard deviation @xmath131 ( equation ) .",
    "i}(\\mathbf{x } ) - t ) \\label{eq : bichon } \\end{aligned}\\ ] ]    [ [ ranjan - criterion ] ] ranjan criterion + + + + + + + + + + + + + + + +    ranjan et al .",
    "@xcite proposed a criterion that quantifies the improvement @xmath133 defined in equation @xmath134 } ( y(\\mathbf{x } ) - t)\\ ] ] where @xmath135 , and @xmath136 .",
    "@xmath127 defines the size of the neighborhood around the contour @xmath122 .",
    "it is possible to compute the up distribution - based ranjan s criterion ( equation ) .",
    "note that we set @xmath137 .",
    "@xmath138   & =   \\sum\\limits_{i=1}^n   w_{i , n}(\\mathbf{x } )   \\big(\\varepsilon_{\\mathbf{x}}^2 - ( \\hat{s}_{n ,- i}(\\mathbf{x})-t)^2\\big ) 1 _ { [ -\\varepsilon_{\\mathbf{x } } , \\varepsilon_{\\mathbf{x } } ] } ( \\hat{s}_{n ,- i}(\\mathbf{x } ) - t ) \\label{eq : ranjanemp } \\end{aligned}\\ ] ]      the use of the pointwise criteria ( equations , , ) might face problems when the region of interest is relatively small to the prediction jumps .",
    "in fact , as the cumulative distribution function of the up distribution is a step function , the probability of the prediction being inside an interval can vanish even if it is around the mean value .",
    "for instance @xmath139\\big)$ ] can be zero .",
    "this is one of the drawbacks of the empirical distribution .",
    "some regularization techniques are possible to overcome this problem . for instance , the technique that consists in defining the region of interest by a gaussian density @xmath140 @xcite .",
    "let @xmath141 be this gaussian probability distribution function .",
    "the new @xmath121 denoted @xmath142 criterion is then as in equation .",
    "@xmath143    the use of the gaussian density to define the targeted region seems more relevant when using the up local varaince .",
    "similarly , we can apply the same method to the ranjan s and bichon s criteria .",
    "to perform surrogate model - based sequential sampling , several relevant techniques require to quantify the prediction uncertainty associated to the model .",
    "gaussian process regression provides directly this uncertainty quantification .",
    "this is the reason why gaussian modeling is quite popular in sequential sampling . in this work",
    ", we defined a universal approach for uncertainty quantification that could be applied for any surrogate model .",
    "it is based on a weighted empirical probability measure supported by cross - validation sub - models predictions .",
    "hence , one could use this distribution to compute most of the classical sequential sampling criteria . as examples",
    ", we discussed sampling strategies for refinement , optimization and inversion .",
    "further , we showed that , under some assumptions , the optimum is adherent to the sequence of points generated by the optimization algorithm up - ego",
    ". moreover , the optimization and the refinement algorithms were successfully implemented and tested both on single and multiple surrogate models .",
    "we also discussed the adaptation of some inversion criteria .",
    "the main drawback of up distribution is that it is supported by a finite number of points . to avoid this",
    ", we propose to regularize this probability measure . in a future work",
    ", we will study and implement such regularization scheme and study the case of multi - objective constrained optimization .",
    "we present in this section the proofs of proposition [ prop : kappanull ] , lemma [ prop : wu ] and theorem [ propconv ] . here , we use the notations of section [ sec : upegoconv ] .",
    "let @xmath144 , @xmath145 and @xmath74 a model that interpolates the data i.e @xmath146 , @xmath147 .",
    "first , we have @xmath148 . since @xmath149 then @xmath150 .",
    "further , @xmath151 . notice that @xmath152 and @xmath153    then @xmath154 . finally , @xmath155 .",
    "[ lemma [ prop : wu ] ] let us note :    * @xmath156 .",
    "* @xmath157 .",
    "convex inequality gives @xmath158 , @xmath159 then @xmath160 .",
    "further , let @xmath161 be two different design points of @xmath162 , @xmath163 , @xmath164 otherwise the triangular inequality would be violated . consequently ,",
    "@xmath165 , @xmath166    @xmath165 , @xmath167 : @xmath168    considering @xmath169 ends the proof .",
    "@xmath7 is compact so @xmath170 has a convergent sub - sequence in @xmath171 ( bolzano - weierstrass theorem ) .",
    "let @xmath172 denote that sub - sequence and @xmath173 its limit .",
    "we can assume by considering a sub - sequence of @xmath174 and using the continuity of the surrogate model @xmath74 that :    * @xmath175 for all @xmath176 * @xmath177 such that @xmath178 , @xmath179 @xmath180 , where @xmath181 .    for all @xmath182 , we note @xmath183 , the step at which up - ego algorithm selects the point @xmath184 .",
    "so , @xmath185 .",
    "notice first that for all @xmath176 , @xmath186 where @xmath187 is the closed ball of center @xmath188 and radius @xmath189 .",
    "so : @xmath190 according to lemma [ prop : wu ] , @xmath191 so @xmath192 .",
    "consequently : @xmath193    further , @xmath194 , @xmath195 , @xmath196 since the surrogate model is an interpolating one .",
    "hence @xmath197 and so @xmath198 .",
    "triangular inequality gives : @xmath199 and finally : @xmath200 we have : @xmath201 considering , and : @xmath202 notice that : + @xmath203 and @xmath204 @xmath205 . since @xmath206 so @xmath207",
    "we gratefully acknowledge the french national association for research and technology ( anrt , cifre grant number 2015/1349 ) .",
    "in this section , we use boxplots to display the evolution of the best value of the optimization test bench . for each iteration",
    ", we display : left : ego in red .",
    ", middle up - ego using genetic aggregation in blue , right : up - ego using kriging in green ."
  ],
  "abstract_text": [
    "<S> the use of surrogate models instead of computationally expensive simulation codes is very convenient in engineering . roughly speaking , there are two kinds of surrogate models : the deterministic and the probabilistic ones . </S>",
    "<S> these last are generally based on gaussian assumptions . </S>",
    "<S> the main advantage of probabilistic approach is that it provides a measure of uncertainty associated with the surrogate model in the whole space . </S>",
    "<S> this uncertainty is an efficient tool to construct strategies for various problems such as prediction enhancement , optimization or inversion .    in this paper , we propose a universal method to define a measure of uncertainty suitable for any surrogate model either deterministic or probabilistic . </S>",
    "<S> it relies on cross - validation ( cv ) sub - models predictions . </S>",
    "<S> this empirical distribution may be computed in much more general frames than the gaussian one . so that it is called the universal prediction distribution ( _ up distribution _ ) . </S>",
    "<S> it allows the definition of many sampling criteria . </S>",
    "<S> we give and study adaptive sampling techniques for global refinement and an extension of the so - called efficient global optimization ( ego ) algorithm . </S>",
    "<S> we also discuss the use of the _ up distribution _ for inversion problems . </S>",
    "<S> the performances of these new algorithms are studied both on toys models and on an engineering design problem .    </S>",
    "<S> * keywords * surrogate models , design of experiments , bayesian optimization </S>"
  ]
}