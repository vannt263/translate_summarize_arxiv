{
  "article_text": [
    "supervised classification concerns the task of assigning an object ( or a number of objects ) to one of two or more groups , based on a sample of labelled training data .",
    "the problem was first studied in generality in the famous work of @xcite , where he introduced some of the ideas of linear discriminant analysis ( lda ) , and applied them to his iris data set . nowadays ,",
    "classification problems arise in a plethora of applications , including spam filtering , fraud detection , medical diagnoses , market research , natural language processing and many others .",
    "in fact , lda is still widely used today , and underpins many other modern classifiers ; see , for example , @xcite and @xcite .",
    "alternative techniques include support vector machines @xcite , tree classifiers @xcite , kernel methods @xcite and nearest neighbour classifiers @xcite .",
    "more substantial overviews and in - depth discussion of these techniques , and others , can be found in @xcite and @xcite .",
    "an increasing number of modern classification problems are _ high - dimensional _ , in the sense that the dimension @xmath0 of the feature vectors may be comparable to or even greater than the number of training data points , @xmath1 . in such settings , classical methods such as those mentioned in the previous paragraph tend to perform poorly @xcite , and may even be intractable ; for example , this is the case for lda , where the problems are caused by the fact that the sample covariance matrix is not invertible when @xmath2 .",
    "many methods proposed to overcome such problems assume that the optimal decision boundary between the classes is linear , e.g. @xcite and @xcite .",
    "another common approach assumes that only a small subset of features are relevant for classification .",
    "examples of works that impose such a sparsity condition include @xcite , where it is also assumed that the features are independent , as well as @xcite and @xcite , where soft thresholding is used to obtain a sparse boundary .",
    "more recently , @xcite and @xcite both solve an optimisation problem similar to fisher s linear discriminant , with the addition of an @xmath3 penalty term to encourage sparsity .",
    "in this paper we attempt to avoid the curse of dimensionality by projecting the feature vectors at random into a lower - dimensional space .",
    "the use of random projections in high - dimensional statistical problems is motivated by the celebrated johnson ",
    "lindenstrauss lemma ( e.g. * ? ? ?",
    "this lemma states that , given @xmath4 , @xmath5 and @xmath6 , there exists a linear map @xmath7 such that @xmath8 for all @xmath9 . in fact , the function @xmath10 that nearly preserves the pairwise distances can be found in randomised polynomial time using random projections distributed according to haar measure as described in section  [ sec  chooserp ] below .",
    "it is interesting to note that the lower bound on @xmath11 in the johnson ",
    "lindenstrauss lemma does not depend on @xmath0 .",
    "as a result , random projections have been used successfully as a computational time saver : when @xmath0 is large compared to @xmath12 , one may project the data at random into a lower - dimensional space and run the statistical procedure on the projected data , potentially making great computational savings , while achieving comparable or even improved statistical performance .",
    "as one example of the above strategy , @xcite obtained vapnik ",
    "chervonenkis type bounds on the generalisation error of a linear classifier trained on a single random projection of the data .",
    "see also @xcite , @xcite and @xcite for other instances .",
    "other works have sought to reap the benefits of aggregating over many random projections .",
    "for instance , @xcite considered estimating a @xmath13 population inverse covariance ( precision ) matrix using @xmath14 , where @xmath15 denotes the sample covariance matrix and @xmath16 are random projections from @xmath17 to @xmath18 .",
    "@xcite used this estimate when testing for a difference between two gaussian population means in high dimensions , while @xcite applied the same technique in fisher s linear discriminant for a high - dimensional classification problem .",
    "our proposed methodology for high - dimensional classification has some similarities with the techniques described above , in the sense that we consider many random projections of the data , but is also closely related to _ bagging _",
    "@xcite , since the ultimate assignment of each test point is made by aggregation and a vote .",
    "bagging has proved to be an effective tool for improving unstable classifiers ; indeed , a bagged version of the ( generally inconsistent ) @xmath19-nearest neighbour classifier is universally consistent as long as the resample size is carefully chosen ; see @xcite .",
    "more generally , bagging has been shown to be particularly effective in high - dimensional problems such as variable selection @xcite .",
    "another related approach to ours is @xcite , who consider ensembles of random rotations , as opposed to projections .     from model  2 in section",
    "[ sec  tsims ] with @xmath20 dimensions and prior probability @xmath21 .",
    "top row : three projections drawn from haar measure ; bottom row : the projections with smallest estimate of test error out of 100 haar projections with lda ( left ) , quadratic discriminant analysis ( middle ) and @xmath22-nearest neighbours ( right ) . ]    one of the basic but fundamental observations that underpins our proposal is the fact that aggregating the classifications of all random projections is not sensible , since most of these projections will typically destroy the class structure in the data ; see the top row of figure  [ fig : useless ] . for this reason",
    ", we advocate partitioning the projections into non - overlapping blocks , and within each block we retain only the projection yielding the smallest estimate of the test error .",
    "the attraction of this strategy is illustrated in the bottom row of figure  [ fig : useless ] , where we see a much clearer partition of the classes .",
    "another key feature of our proposal is the realisation that a simple majority vote of the classifications based on the retained projections can be highly suboptimal ; instead , we argue that the voting threshold should be chosen in a data - driven fashion in an attempt to minimise the test error of the infinite - simulation version of our random projection ensemble classifier .",
    "in fact , this estimate of the optimal threshold turns out to be remarkably effective in practice ; see section  [ sec  alpha ] for further details .",
    "we emphasise that our methodology can be used in conjunction with any base classifier , though we particularly have in mind classifiers designed for use in low - dimensional settings .",
    "the random projection ensemble classifier can therefore be regarded as a general technique for either extending the applicability of an existing classifier to high dimensions , or improving its performance .",
    "our theoretical results are divided into three parts . in the first",
    ", we consider a generic base classifier and a generic method for generating the random projections into @xmath18 and quantify the difference between the test error of the random projection ensemble classifier and its infinite - simulation counterpart as the number of projections increases .",
    "we then consider selecting random projections from non - overlapping blocks by initially drawing them according to haar measure , and then within each block retaining the projection that minimises an estimate of the test error . under a condition implied by the widely - used sufficient dimension reduction assumption @xcite , we can then control the difference between the test error of the random projection classifier and the bayes risk as a function of terms that depend on the performance of the base classifier based on projected data and our method for estimating the test error , as well as terms that become negligible as the number of projections increases .",
    "the final part of our theory gives risk bounds on the first two of these terms for specific choices of base classifier , namely fisher s linear discriminant and the @xmath22-nearest neighbour classifier .",
    "the key point here is that these bounds only depend on @xmath11 , the sample size @xmath1 and the number of projections , and not on the original data dimension @xmath0 .",
    "the remainder of the paper is organised as follows .",
    "our methodology and general theory are developed in sections  [ sec  rp ] and  [ sec  chooserp ] .",
    "specific choices of base classifier are discussed in section  [ sec  base ] , while section  [ sec  prac ] is devoted to a consideration of the practical issues of the choice of voting threshold , projected dimension and the number of projections used . in section  [ sec ",
    "empirical ] we present results from an extensive empirical analysis on both simulated and real data where we compare the performance of the random projection ensemble classifier with several popular techniques for high - dimensional classification .",
    "the outcomes are extremely encouraging , and suggest that the random projection ensemble classifier has excellent finite - sample performance in a variety of different high - dimensional classification settings .",
    "we conclude with a discussion of various extensions and open problems .",
    "all proofs are deferred to the appendix .    finally in this section , we introduce the following general notation used throughout the paper . for a sufficiently smooth real - valued function @xmath23 defined on a neighbourhood of @xmath24 , let @xmath25 and @xmath26 denote its first and second derivatives at @xmath27 , and",
    "let @xmath28 and @xmath29 denote the integer and fractional part of @xmath27 respectively .",
    "we start by describing our setting and defining the relevant notation .",
    "suppose that the pair @xmath30 takes values in @xmath31 , with joint distribution @xmath32 , characterised by @xmath33 , and @xmath34 , the conditional distribution of @xmath35 , for @xmath36 . for convenience ,",
    "we let @xmath37 . in the alternative characterisation of @xmath32 ,",
    "we let @xmath38 denote the marginal distribution of @xmath39 and write @xmath40 for the regression function . recall that a _ classifier _ on @xmath17 is a borel measurable function @xmath41 , with the interpretation that we assign a point @xmath42 to class @xmath43 .",
    "we let @xmath44 denote the set of all such classifiers .    the misclassification rate , or _ risk _ , of a classifier @xmath45 is @xmath46 , and is minimised by the _ bayes _",
    "classifier @xmath47 ( e.g. * ? ? ?",
    "its risk is @xmath48 $ ] .    of course",
    ", we can not use the bayes classifier in practice , since @xmath49 is unknown . nevertheless , we often have access to a sample of training data that we can use to construct an approximation to the bayes classifier . throughout this section and section  [ sec  chooserp ] ,",
    "it is convenient to consider the training sample @xmath50 to be fixed points in @xmath31 .",
    "our methodology will be applied to a base classifier @xmath51 , which we assume can be constructed from an arbitrary training sample @xmath52 of size @xmath1 in @xmath53 ; thus @xmath54 is a measurable function from @xmath55 to  @xmath56 .",
    "now assume that @xmath57 .",
    "we say a matrix @xmath58 is a _ projection _ if @xmath59 , the @xmath11-dimensional identity matrix .",
    "let @xmath60 be the set of all such matrices . given a projection @xmath61 ,",
    "define projected data @xmath62 and @xmath63 for @xmath64 , and let @xmath65 .",
    "the projected data base classifier corresponding to @xmath54 is @xmath66 , given by @xmath67 note that although @xmath68 is a classifier on @xmath17 , the value of @xmath69 only depends on @xmath70 through its @xmath11-dimensional projection @xmath71 .",
    "we now define a generic ensemble classifier based on random projections . for @xmath72 ,",
    "let @xmath73 denote independent and identically distributed projections in @xmath74 , independent of @xmath30 .",
    "the distribution on @xmath74 is left unspecified at this stage , and in fact our proposed method ultimately involves choosing this distribution depending on @xmath75 .",
    "now set @xmath76 for @xmath77 , the _ random projection ensemble _ classifier is defined to be @xmath78 we emphasise again here the additional flexibility afforded by not pre - specifying the voting threshold @xmath79 to be @xmath80 .",
    "our analysis of the random projection ensemble classifier will require some further definitions .",
    "let @xmath81 where the randomness here comes from the random projections .",
    "let @xmath82 and @xmath83 denote the distribution functions of @xmath84 and @xmath85 , respectively .",
    "we will make use of the following assumption :    * ( a.1 ) * : :    @xmath82 and @xmath83 are twice    differentiable at @xmath79 .",
    "the first derivatives of @xmath82 and @xmath83 , when they exist , are denoted as @xmath86 and @xmath87 respectively ; under  * ( a.1 ) * , these derivatives are well - defined in a neighbourhood of @xmath79 .",
    "our first main result below gives an asymptotic expansion for the test error @xmath88 of our generic random projection ensemble classifier as the number of projections increases .",
    "in particular , we show that this test error can be well approximated by the test error of the infinite - simulation random projection classifier @xmath89 this infinite - simulation classifier turns out to be easier to analyse in subsequent results .",
    "note that under * ( a.1 ) * , @xmath90    [ thm  bvar ] assume * ( a.1)*. then @xmath91 as @xmath92 , where @xmath93    the proof of theorem  [ thm  bvar ] in the appendix is lengthy , and involves a one - term edgeworth approximation to the distribution function of a standardised binomial random variable .",
    "one of the technical challenges is to show that the error in this approximation holds uniformly in the binomial proportion .",
    "define the test error of @xmath68 by through an integral rather than defining @xmath94 to make it clear that when @xmath95 is a random projection , it should be conditioned on when computing @xmath96 . ]",
    "@xmath97 our next result controls the test excess risk , i.e. the difference between the test error and the bayes risk , of the infinite - simulation random projection classifier in terms of the expected test excess risk of the classifier based on a single random projection .",
    "an attractive feature of this result is its generality : no assumptions are placed on the configuration of the training data @xmath75 , the distribution @xmath32 of the test point @xmath30 or on the distribution of the individual projections .",
    "[ thm : baggingbound ] we have @xmath98",
    "in this section , we study a special case of the generic random projection ensemble classifier introduced in section  [ sec  rp ] , where we propose a screening method for choosing the random projections .",
    "let @xmath99 be an estimator of @xmath96 , based on @xmath100 , that takes values in the set @xmath101 .",
    "examples of such estimators include resubstitution and leave - one - out estimates ; we discuss these choices in greater detail in section  [ sec  base ] . for @xmath102 ,",
    "let @xmath103 denote independent projections , independent of @xmath30 , distributed according to haar measure on @xmath74 .",
    "one way to simulate from haar measure on the set @xmath74 is to first generate a matrix @xmath104 , where each entry is drawn independently from a standard normal distribution , and then take @xmath105 to be the matrix of left singular vectors in the singular value decomposition of @xmath106 . for @xmath107 , let @xmath108 where @xmath109 denotes the smallest index where the minimum is attained in the case of a tie .",
    "we now set @xmath110 , and consider the random projection ensemble classifier from section  [ sec  rp ] constructed using the independent projections @xmath111 .",
    "let @xmath112 denote the optimal test error estimate over all projections .",
    "the minimum is attained here , since @xmath99 takes only finitely many values . for @xmath113 ,",
    "let @xmath114 where @xmath95 is distributed according to haar measure on @xmath74 .",
    "we assume the following :    * ( a.2 ) * : :    there exist @xmath115 and    @xmath116 such that    @xmath117    for    @xmath118 .    condition  * ( a.2 ) * asks for a certain growth rate of the distribution function of @xmath99 close to its minimum value @xmath119 ; observe that the strength of the condition decreases as @xmath120 increases . under this condition , the following result is a starting point for controlling the expected test excess risk of the classifier based on a single projection chosen according to the scheme described above .",
    "[ thm  ela * ] assume * ( a.2)*. then @xmath121 where @xmath122 .",
    "the form of the bound in proposition  [ thm  ela * ] motivates us to seek to control @xmath123 in terms of the test excess risk of a classifier based on the projected data , in the hope that we will be able to show this does not depend on @xmath0 . to this end , define the regression function on @xmath18 induced by the projection @xmath61 to be @xmath124 .",
    "the corresponding induced bayes classifier , which is the optimal classifier knowing only the distribution of @xmath125 , is given by @xmath126 its risk is @xmath127 in order to ensure that @xmath119 will be close to the bayes risk , we will invoke an additional assumption on the form of the bayes classifier :    * ( a.3 ) * : :    there exists a projection @xmath128 such that    @xmath129    where @xmath130    denotes the symmetric difference of two sets @xmath131 and    @xmath45 .",
    "condition  * ( a.3 ) * requires that the set of points @xmath132 assigned by the bayes classifier to class 1 can be expressed as a function of a @xmath11-dimensional projection of @xmath70 .",
    "note that if the bayes decision boundary is a hyperplane , then * ( a.3 ) * holds with @xmath133 .",
    "moreover , proposition  [ prop : cond ] below shows that , in fact , * ( a.3 ) * holds under the sufficient dimension reduction condition , which states that @xmath134 is independent of @xmath39 given @xmath135 ; see @xcite for many statistical settings where such an assumption is natural .",
    "[ prop : cond ] if @xmath134 is independent of @xmath39 given @xmath135 , then * ( a.3 ) * holds .    finally , then , we are in a position to control the test excess risk of our random projection ensemble classifier in terms of the test excess risk of a classifier based on @xmath11-dimensional data , as well as terms that reflect our ability to estimate the test error of classifiers based on projected data and terms that depend on @xmath136 and @xmath120 .",
    "[ thm : main ] assume * ( a.1 ) * , * ( a.2 ) * and * ( a.3)*. then @xmath137 as @xmath138 , where @xmath139 is defined in theorem  [ thm  bvar ] , @xmath140 is defined in proposition  [ thm  ela * ] and @xmath141 .    regarding the bound in theorem  [ thm : main ] as a sum of four terms , we see that the last two of these can be seen as the price we have to pay for the fact that we do not have access to an infinite sample of random projections .",
    "these terms can be made negligible by choosing @xmath136 and @xmath120 to be sufficiently large , but it should be noted that @xmath140 may increase with @xmath120 .",
    "this is a reflection of the fact that minimising an estimate of test error may lead to overfitting .",
    "the behaviour of this term , together with that of @xmath142 and @xmath143 , depends on the choice of base classifier , but in the next section below we describe settings where these terms can be bounded by expressions that do not depend on @xmath0 .",
    "in this section , we change our previous perspective and regard the training data @xmath144 as independent random pairs with distribution @xmath32 , so our earlier statements are interpreted conditionally on @xmath75 .",
    "we consider particular choices of base classifier , and study the first two terms in the bound in theorem  [ thm : main ] .",
    "linear discriminant analysis ( lda ) , introduced by @xcite , is arguably the simplest classification technique .",
    "recall that in the special case where @xmath145 , we have @xmath146 so * ( a.3 ) * holds with @xmath133 and @xmath147 , a @xmath148 matrix . in lda , @xmath149 , @xmath150 and @xmath151",
    "are estimated by their sample versions , using a pooled estimate of @xmath151 .",
    "although lda can not be applied directly when @xmath2 since the sample covariance matrix is singular , we can still use it as the base classifier for a random projection ensemble , provided that @xmath152 . indeed , noting that for any @xmath61 , we have @xmath153 , where @xmath154 and @xmath155 , we can define@xmath156 here , @xmath157 , @xmath158 , @xmath159 and @xmath160 .",
    "write @xmath161 for the standard normal distribution function . under the normal model specified above",
    ", the test error of the lda classifier can be written as @xmath162 where @xmath163 and @xmath164 .",
    "@xcite studied the excess risk of the lda classifier in an asymptotic regime in which @xmath11 is fixed as @xmath1 diverges .",
    "in fact , he considered a very slightly different data generating model , in which the training sample sizes from each population are assumed to be known in advance , so that without loss of generality , we may assume that @xmath165 and @xmath166 , while @xmath167 , as before .",
    "specialising his results for simplicity to the case where @xmath1 is even and @xmath168 , @xcite showed that using the lda classifier   with @xmath169 , @xmath170 and @xmath171 yields @xmath172 as @xmath173 , where @xmath174 .",
    "it remains to control the errors @xmath140 and @xmath143 in theorem [ thm : main ] .",
    "for the lda classifier , we consider the resubstitution estimator @xmath175 @xcite provided a vapnik  chervonenkis bound for @xmath99 under no assumptions on the underlying data generating mechanism : for every @xmath176 and @xmath177 , @xmath178 see also ( * ? ? ?",
    "* theorem  23.1 ) . we can then conclude that @xmath179 the more difficult term to deal with is @xmath180 . in this case , the bound   can not be applied directly , because @xmath181 are no longer independent conditional on @xmath182 .",
    "nevertheless , since @xmath183 are independent of @xmath75 , we still have that @xmath184 we can therefore conclude by almost the same argument as that leading to   that @xmath185 note that none of the bounds  , and   depend on the original data dimension @xmath0 .",
    "moreover , substituting the bound   into theorem  [ thm : main ] reveals a trade - off in the choice of @xmath120 when using lda as the base classifier .",
    "choosing @xmath120 to be large gives us a good chance of finding a projection with a small estimate of test error , but we may incur a small overfitting penalty as reflected by  .",
    "quadratic discriminant analysis ( qda ) is designed to handle situations where the class - conditional covariance matrices are unequal .",
    "recall that when @xmath186 , and @xmath187 , for @xmath188 , the bayes decision boundary is given by @xmath189 , where @xmath190 in qda , @xmath149 , @xmath150 and @xmath191 are estimated by their sample versions . if @xmath192 , where @xmath193 is the number of training sample observations from the @xmath194th class , then at least one of the sample covariance matrix estimates is singular , and qda can not be used directly .",
    "nevertheless , we can still choose @xmath195 and use qda as the base classifier in a random projection ensemble .",
    "specifically , we can set @xmath196 where @xmath197 , @xmath198 and @xmath199 were defined in section  [ sec  lda ] , and where @xmath200 for @xmath188 .",
    "unfortunately , analogous theory to that presented in section  [ sec  lda ] does not appear to exist for the qda classifier ( unlike for lda , the risk does not have a closed form ) .",
    "nevertheless , we found in our simulations presented in section  [ sec  empirical ] that the qda random projection ensemble classifier can perform very well in practice . in this case",
    ", we estimate the test errors using the leave - one - out method given by @xmath201 where @xmath202 denotes the classifier @xmath203 , trained without the @xmath204th pair , i.e. based on @xmath205 . for a method like qda that involves estimating more parameters than lda",
    ", we found that the leave - one - out estimator was less susceptible to overfitting than the resubstitution estimator .",
    "the @xmath22-nearest neighbour classifier ( @xmath22nn ) , first proposed by @xcite , is a nonparametric method that classifies the test point @xmath132 according to a majority vote over the classes of the @xmath22 nearest training data points to @xmath70 .",
    "the enormous popularity of the @xmath22nn classifier can be attributed partly due to its simplicity and intuitive appeal ; however , it also has the attractive property of being universally consistent : for every distribution @xmath32 , as long as @xmath206 and @xmath207 , the risk of the @xmath22nn classifier converges to the bayes risk ( * ? ? ?",
    "* theorem  6.4 ) .",
    "@xcite derived the rate of convergence of the excess risk of the @xmath22-nearest neighbour classifier . under regularity conditions ,",
    "the optimal choice of @xmath22 , in terms of minimising the excess risk , is @xmath208 , and the rate of convergence of the excess risk with this choice is @xmath209 .",
    "thus , in common with other nonparametric methods , there is a ` curse of dimensionality ' effect that means the classifier typically performs poorly in high dimensions .",
    "@xcite found the optimal way of assigning decreasing weights to increasingly distant neighbours , and quantified the asymptotic improvement in risk over the unweighted version , but the rate of convergence remains the same .",
    "we can use the @xmath22nn classifier as the base classifier for a random projection ensemble as follows : let @xmath210 , where @xmath211 and @xmath212 .",
    "given @xmath213 , let @xmath214 be a re - ordering of the training data such that @xmath215 , with ties split at random .",
    "now define @xmath216 where @xmath217 . the theory described in the previous paragraph",
    "can be applied to show that , under regularity conditions , @xmath218 .",
    "once again , a natural estimate of the test error in this case is the leave - one - out estimator defined in  , where we use the same value of @xmath22 on the leave - one - out datasets as for the base classifier , and where distance ties are split in the same way as for the base classifier . for this estimator",
    ", @xcite showed that for every @xmath176 , @xmath219 see also ( * ? ? ? * chapter  24 ) .",
    "it follows that @xmath220 @xcite also provided a tail bound analogous to   for the leave - one - out estimator .",
    "arguing very similarly to section  [ sec  lda ] , we can deduce that under no conditions on the data generating mechanism , @xmath221",
    "we now discuss the choice of the voting threshold @xmath79 in  ( [ eq  rp ] ) .",
    "the expression for the test error of the infinite - simulation random projection ensemble classifier given in   suggests the ` oracle ' choice @xmath222 } \\bigl[\\pi_1 g_{n,1}(\\alpha ' ) + \\pi_2\\{1 - g_{n,2}(\\alpha')\\}\\bigr].\\ ] ] note that , if assumption * ( a.1 ) * holds and @xmath223 then @xmath224 and in theorem  [ thm  bvar ] , we have @xmath225 of course , @xmath226 can not be used directly , because we do not know @xmath82 and @xmath83 ( and we may not know @xmath227 and @xmath228 either ) . nevertheless ,",
    "for the lda base classifier we can estimate @xmath229 using @xmath230 for @xmath188 .",
    "for the qda and @xmath22-nearest neighbour base classifiers , we use the leave - one - out - based estimate @xmath231 in place of @xmath232 .",
    "we also estimate @xmath149 by @xmath233 , and then set the cut - off in  ( [ eq  rp ] ) as @xmath234 } \\bigl[\\hat{\\pi}_1 \\hat{g}_{n,1}(\\alpha ' ) + \\hat{\\pi}_2\\{1 - \\hat{g}_{n,2}(\\alpha')\\}\\bigr].\\ ] ] since empirical distribution functions are piecewise constant , the objective function in   does not have a unique minimum , so we choose @xmath235 to be the average of the smallest and largest minimisers . an attractive feature of the method is that @xmath236 ( or @xmath237 in the case of qda and @xmath22nn ) are already calculated in order to choose the projections , so calculating @xmath238 and @xmath239 carries negligible extra computational cost",
    ".    figures  [ fig  gplots0.5 ] and  [ fig  gplots0.33 ] illustrate @xmath240 as an estimator of @xmath241 , for different base classifiers as well as different values of @xmath1 and @xmath227 . here , a very good approximation to the estimand was obtained using an independent data set of size 5000 . unsurprisingly",
    ", the performance of the estimator improves as @xmath1 increases , but the most notable feature of these plots is the fact that for all classifiers and even for small sample sizes , @xmath235 is an excellent estimator of @xmath226 , and may be a substantial improvement on the naive choice @xmath242 ( which may result in a classifier that assigns every point to a single class ) .",
    "in   ( black ) and @xmath240 ( red ) for the lda ( left ) , qda ( middle ) and @xmath22nn ( right ) base classifiers after projecting for one training data set of size @xmath243 50 ( top ) , 100 ( middle ) and 1000 ( bottom ) from model 1 . here , @xmath244 , @xmath245 and @xmath246 .",
    "]     in   ( black ) and @xmath240 ( red ) for the lda ( left ) , qda ( middle ) and @xmath22nn ( right ) base classifiers after projecting for one training data set of size @xmath243 50 ( top ) , 100 ( middle ) and 1000 ( bottom ) from model 1 .",
    "here , @xmath247 , @xmath245 and @xmath246 . ]      we want to choose @xmath11 as small as possible in order to obtain the best possible performance bounds as described in section  [ sec  base ] above .",
    "this also reduces the computational cost .",
    "the performance bounds rely on assumption * ( a.3 ) * , whose strength decreases as @xmath11 increases , so we want to choose @xmath11 large enough that this condition holds ( at least approximately ) .",
    "in section [ sec  empirical ] we see that the random ensemble projection method is quite robust to the choice of @xmath11",
    ". nevertheless , in some circumstances it may be desirable to have an automatic choice .",
    "as one way to do this , suppose that we wish to choose @xmath11 from a set @xmath248 .",
    "for each @xmath249 , generate independent and identically distributed projections @xmath250 according to haar measure on @xmath251 .",
    "for each @xmath249 and for @xmath252 , we can then set @xmath253 where @xmath254 . finally , we can select @xmath255    in figures  [ fig  empiricaldist1d ] and  [ fig  empiricaldist2d ] we present the empirical distribution functions of @xmath256 , where @xmath257 , for one training dataset from model 1 ( described in section [ sec  independent ] ) , and model 3 ( described in section [ sec  multisims ] ) . in each case",
    ", we set @xmath258 , @xmath259 , @xmath20 and @xmath260 .     for the lda ( left ) , qda ( middle ) and @xmath22nn ( right ) base classifiers after projecting for",
    "model 1 , @xmath258 , @xmath259 , @xmath245 , and @xmath246 ( black ) , 3 ( red ) , 4 ( blue ) and 5 ( green).,scaledwidth=100.0% ]     for the lda ( left ) , qda ( middle ) and @xmath22nn ( right ) base classifiers after projecting for model 2 , @xmath258 , @xmath259 , @xmath245 , and @xmath246 ( black ) , 3 ( red ) , 4 ( blue ) and 5 ( green).,scaledwidth=100.0% ]    figures  [ fig  empiricaldist1d ] and  [ fig  empiricaldist2d ] do not suggest great differences in performance for different choices of @xmath11 , especially for the qda and @xmath22nn base classifiers .",
    "for the lda classifier , it appears , particularly for model  2 , that projecting into a slightly larger dimensional space is preferable , and indeed this appears to be the case from the relevant entry of table  [ tab  tdist ] below .",
    "the ideas presented here may also be used to decide between two different base classifiers .",
    "for example , comparing the green lines across different panels of figure  [ fig  empiricaldist1d ] , we see that for model 1 and @xmath261 , we might expect the best results with the qda base classifier , and indeed this is confirmed by the simulation results in table  [ tab  indep ] below .      in order to minimise the third term in the bound in theorem  [ thm : main ]",
    ", we should choose @xmath136 to be as large as possible .",
    "the constraint , of course , is that the computational cost of the random projection classifier scales linearly with @xmath136 .",
    "the choice of @xmath120 is more subtle ; while the fourth term in the bound in theorem  [ thm : main ] decreases as @xmath120 increases , we saw in section  [ sec  base ] that upper bounds on @xmath262 may increase with @xmath120 . in principle , we could try to use the expressions given in theorem  [ thm : main ] and section  [ sec  base ] to choose @xmath120 to minimise the overall upper bound on @xmath263 . although @xmath264 and @xmath265 are unknown to the practitioner , these could be estimated based on the empirical distribution of @xmath266 , where @xmath267 are independent projections drawn according to haar measure . in practice",
    ", however , we found that an involved approach such as this was unnecessary , and that the ensemble method was robust to the choice of @xmath136 and @xmath120 . in all of our simulations , we set @xmath260 .",
    "in this section , we assess the empirical performance of the random projection ensemble classifier . throughout this section , rp-@xmath22nn@xmath268 , rp - lda@xmath268 and rp - qda@xmath268 denote the random projection classifier with lda , qda and @xmath22nn base classifiers , respectively ; the subscript @xmath11 refers to the dimension of the image space of the projections . for comparison , we also present results for several state - of - the - art methods for high - dimensional classification , namely penalized lda @xcite , nearest shrunken centroids @xcite , shrunken centroids regularized discriminant analysis @xcite , and independence rules ( ir ) @xcite , as well as for the base classifier applied in the original space . for the standard @xmath22nn classifier",
    ", we chose @xmath22 via leave - one - out cross validation .",
    "the tuning parameters for the other methods were chosen using the default settings in the corresponding ` r ` packages ` penlda ` @xcite , ` nsc ` @xcite and ` scrda ` @xcite , namely 6-fold , 10-fold and 10-fold cross validation , respectively .    in each of the simulated examples below",
    ", we used @xmath269 , @xmath20 , two different values of the prior probability @xmath227 and @xmath270 .",
    "each experiment was repeated 100 times and the risk was estimated on an independent test set of size 1000 .",
    "model 1 : here , @xmath271 is the distribution of @xmath0 independent components , each with a standard laplace distribution , while @xmath272 , with @xmath273 .",
    ".misclassification rates multiplied by 100 ( with standard errors as subscripts ) for model 1 , with smallest in bold .",
    "* n / a : not available due to singular covariance estimates . [ cols=\">,^,^,^,^,^,^ \" , ]     [ tab  ozone ]    in this example , all of the random projection ensemble classifiers performed very well , though scrda was also competitive , particularly for the largest sample size .",
    "we have introduced a general framework for high - dimensional classification via the combination of the results of applying a base classifier on carefully selected low - dimensional random projections of the data .",
    "one of its attractive features is its generality : the approach can be used in conjunction with any base classifier . moreover , although we explored in detail one method for combining the random projections ( partly because it facilitates rigorous statistical analysis ) , there are many other options available here .",
    "for instance , instead of only retaining the projection within each block yielding the smallest estimate of test error , one might give weights to the different projections , where the weights decrease as the estimate of test error increases .",
    "another interesting avenue to explore would be alternative methods for estimating the test error , such as sample splitting .",
    "the idea here would be to split the sample @xmath75 into @xmath274 and @xmath275 , say , where @xmath276 and @xmath277 .",
    "we then use @xmath278 to estimate the test error @xmath279 based on the training data @xmath274 .",
    "since @xmath274 and @xmath275 are independent , we can apply hoeffding s inequality to deduce that @xmath280 it then follows by very similar arguments to those given in section  [ sec  lda ] that @xmath281 the advantages of this approach are twofold : first , the bounds hold for any choice of base classifier ( and still without any assumptions on the data generating mechanism ) ; second , the bounds on the terms in   merely rely on hoeffding s inequality as opposed to vapnik  chervonenkis theory , so are typically sharper .",
    "the disadvantage is that the other terms in the bound in theorem  [ thm : main ] will tend to be larger due to the reduced effective sample size .",
    "the choice of @xmath282 and @xmath283 in such an approach therefore becomes an interesting question .",
    "many practical classification problems involve @xmath284 classes .",
    "the main issue in extending our methodology to such settings is the definition of @xmath285 analogous to  . to outline one approach ,",
    "let @xmath286 for @xmath287 . given @xmath288 with @xmath289",
    ", we can then define @xmath290 the choice of @xmath291 is analogous to the choice of @xmath79 in the case @xmath292 .",
    "it is therefore natural to seek to minimise the the test error of the corresponding infinite - simulation random projection ensemble classifier as before .    in other situations",
    ", it may be advantageous to consider alternative types of projection , perhaps because of additional structure in the problem .",
    "one particularly interesting issue concerns ultrahigh - dimensional settings , say @xmath0 in the thousands . here",
    ", it may be too time - consuming to generate enough random projections to explore adequately the space @xmath251 . as a mathematical quantification of this , the cardinality of an @xmath293-net in the euclidean norm of the surface of the euclidean ball in @xmath17 increases exponentially in @xmath0 ( e.g. * ? ? ? * ; * ? ? ?",
    "in such challenging problems , one might restrict the projections @xmath95 to be axis - aligned , so that each row of @xmath95 consists of a single non - zero component , equal to 1 , and @xmath294 zero components .",
    "there are then only @xmath295 choices for the projections , and if @xmath11 is small , it may be feasible even to carry out an exhaustive search .",
    "of course , this approach loses one of the attractive features of our original proposal , namely the fact that it is equivariant to orthogonal transformations . nevertheless , corresponding theory",
    "can be obtained provided that the projection @xmath296 in * ( a.3 ) * is axis - aligned .",
    "this is a much stronger requirement , but it seems that imposing greater structure is inevitable to obtain good classification in such settings .",
    "recall that the training data @xmath297 are fixed and the projections @xmath298 , are independent and identically distributed in @xmath74 , independent of the pair @xmath30 .",
    "the test error of the random projection ensemble classifier has the following representation : @xmath299 where @xmath300 is defined in  .",
    "let @xmath301 , for @xmath302 .",
    "then , conditional on @xmath303 $ ] , the random variables @xmath304 are independent , each having a bernoulli(@xmath305 ) distribution .",
    "recall that @xmath82 and @xmath83 are the distribution functions of @xmath306 and @xmath307 , respectively .",
    "we can therefore write @xmath308 where here and throughout the proof , @xmath309 denotes a @xmath310 random variable .",
    "similarly , @xmath311 it follows that @xmath312 where @xmath313 .",
    "writing @xmath314 , we now show that @xmath315 as @xmath92 .",
    "our proof involves a one - term edgeworth expansion to the binomial distribution function in  , where the error term is controlled uniformly in the parameter .",
    "the expansion relies on the following version of esseen s smoothing lemma .",
    "* chapter 2 , theorem  2b ) let @xmath316 , @xmath317 , @xmath318 , let @xmath319 be a non - decreasing function and let @xmath320 be a function of bounded variation .",
    "let @xmath321 and @xmath322 be the fourier ",
    "stieltjes transforms of @xmath323 and @xmath324 , respectively .",
    "suppose that    * @xmath325 and @xmath326 ; * @xmath327 ; * the set of discontinuities of @xmath323 and @xmath324 is contained in @xmath328 , where @xmath329 is a strictly increasing sequence with @xmath330 ; moreover @xmath323 is constant on the intervals @xmath331 for all @xmath332 ; * @xmath333 for all @xmath334 .",
    "then there exist constants @xmath335 such that @xmath336 provided that @xmath337 .",
    "[ thm  esseen ]    let @xmath338 , and let @xmath161 and @xmath339 denote the standard normal distribution and density functions , respectively . moreover , for @xmath24 , let @xmath340 and @xmath341 in proposition [ cor  edgeworth ] below we apply theorem [ thm  esseen ] to the following functions : @xmath342 and @xmath343    [ cor  edgeworth ] let @xmath344 and @xmath345 be as in ( [ eq  fb1 ] ) and ( [ eq  gb1 ] ) .",
    "there exists a constant @xmath346 such that , for all @xmath72 , @xmath347    proposition  [ cor  edgeworth ] , whose proof is given after the proof of theorem  [ thm : main ] , bounds uniformly in @xmath305 the error in the one - term edgeworth expansion @xmath345 of the distribution function @xmath344 . returning to the proof of theorem [ thm  bvar ]",
    ", we will argue that the dominant contribution to the integral in ( [ eq  rtp ] ) arises from the interval @xmath348,@xmath349 , where @xmath350 .",
    "for the remainder of the proof we assume @xmath136 is large enough that @xmath351 \\subseteq ( 0,1)$ ] .    for the region @xmath352 , by hoeffding s inequality , we have that @xmath353 for each @xmath354 , as @xmath92 .",
    "it follows that @xmath355 for each @xmath354 , as @xmath92 .    for the region @xmath356 , by proposition  [ cor  edgeworth ]",
    ", there exists @xmath357 such that , for all @xmath136 sufficiently large , @xmath358 where @xmath359 . hence , using the fact that for large @xmath136 , @xmath360 under * ( a.1 ) * , we have @xmath361 as @xmath138 . to aid exposition ,",
    "we will henceforth concentrate on the dominant terms in our expansions , denoting the remainder terms as @xmath362 .",
    "these remainders are then controlled at the end of the argument .",
    "for the first term in ( [ eq  main1 ] ) , we write @xmath363 now , for the first term in ( [ eq  main2 ] ) , @xmath364 for the second term in ( [ eq  main2 ] ) , write @xmath365 returning to the second term in ( [ eq  main1 ] ) , observe that @xmath366 the claim   will now follow from  ( [ eq : hoeffding ] ) , ( [ eq  main1 ] ) , ( [ eq  main2 ] ) , ( [ eq  main3 ] ) , ( [ eq  dom3 ] ) and ( [ eq  main4 ] ) , once we have shown that @xmath367 as @xmath138 .    _ to bound @xmath368 _ : for @xmath369 , let @xmath370 .",
    "observe that , by a taylor expansion about @xmath371 , there exists @xmath372 , such that , for all @xmath373 and all @xmath374 , @xmath375 } |\\ddot{h}_\\theta(\\zeta')|   \\leq ( \\zeta-\\alpha)^2\\frac{\\log^{3}{b_1}}{2\\sqrt{2\\pi } \\{\\alpha(1-\\alpha)\\}^{7/2}}.\\end{aligned}\\ ] ] using this bound with @xmath376 , we deduce that , for all @xmath136 sufficiently large , @xmath377 as @xmath92 .",
    "_ to bound @xmath378 _",
    ": since @xmath379 is differentiable at @xmath79 , given @xmath380 , there exists @xmath381 such that @xmath382 for all @xmath383 .",
    "it follows that , for all @xmath136 sufficiently large , @xmath384 we deduce that @xmath385 as @xmath92 .    _ to bound @xmath386 _ :",
    "for large @xmath136 , we have @xmath387 as @xmath92 .    _ to bound @xmath388 _ : by the bound in ( [ eq  cont ] ) , we have that , given @xmath177 , for all @xmath136 sufficiently large , @xmath389    _ to bound @xmath390 _ : for all @xmath136 sufficiently large , @xmath391 as @xmath92 .    _ to bound @xmath392 _ :",
    "we write @xmath393 where @xmath394 and @xmath395 since @xmath379 is continuous at @xmath79 , given @xmath380 , there exists @xmath396 such that , for all @xmath397 , @xmath398 it follows that , for @xmath397 , @xmath399 for all sufficiently large @xmath136 .",
    "we deduce that @xmath400 as @xmath92 .    to control @xmath401 , by the mean value theorem , we have that for all @xmath136 sufficiently large and all @xmath402 $ ] , @xmath403 thus , for large @xmath136 , @xmath404 we deduce that @xmath405 as @xmath92 .",
    "_ to bound @xmath406",
    "_ : write @xmath407 , where @xmath408 and @xmath409 by the bound in ( [ eq  cont ] ) , given @xmath410 , for all @xmath136 sufficiently large , latexmath:[\\ ] ] similarly , @xmath475 but , for @xmath476 $ ] , @xmath477 it follows that @xmath478 now @xmath479 where @xmath480 . by (",
    "* theorem 2 , section 40 ) , we have that @xmath481 moreover , @xmath482 finally , @xmath483 by ( [ eq  ij1 ] ) , ( [ eq : ij ] ) , ( [ eq  ij2 ] ) , ( [ eq  ij3 ] ) , ( [ eq  ij4 ] ) , ( [ eq  ij5 ] ) and ( [ eq  ij6 ] ) , it follows that @xmath484 by ( [ eq  interval ] ) , ( [ eq  s12 ] ) , ( [ eq : q * ] ) , ( [ eq : f*middle ] ) , ( [ eq : phi*middle ] ) , ( [ eq : p*middle ] ) , ( [ eq  s1s2 ] ) , ( [ eq : phipend ] ) and ( [ eq  s2b1 ] ) , we conclude that  ( [ eq  fourierbound ] ) holds .",
    "the result now follows from theorem  [ thm  esseen ] , by taking @xmath485 , @xmath486 and @xmath487 in that result .",
    "the research of the second author is supported by an engineering and physical sciences research council fellowship .",
    "the authors thank rajen shah and ming yuan for helpful comments .",
    "bickel , p.  j. and levina , e. ( 2004 ) . some theory for fisher s linear discriminant function , ` naive bayes ' , and some alternatives when there are more variables than observations .",
    "_ bernoulli _ , * 10 * , 9891010 .",
    "hastie , t. , tibshirani , r. , and friedman , j. ( 2009 ) . _ the elements of statistical learning : the elements of statistical learning : data mining , inference , and prediction._. springer series in statistics ( 2nd ed . ) .",
    "springer , new york .",
    "tibshirani , r. , hastie , t. , narisimhan , b. and chu , g. ( 2002 ) .",
    "diagnosis of multiple cancer types by shrunken centroids of gene expression .",
    "_ procedures of the natural academy of science , usa _ , * 99 * , 65676572 ."
  ],
  "abstract_text": [
    "<S> we introduce a very general method for high - dimensional classification , based on careful combination of the results of applying an arbitrary base classifier to random projections of the feature vectors into a lower - dimensional space . </S>",
    "<S> in one special case that we study in detail , the random projections are divided into non - overlapping blocks , and within each block we select the projection yielding the smallest estimate of the test error . </S>",
    "<S> our random projection ensemble classifier then aggregates the results of applying the base classifier on the selected projections , with a data - driven voting threshold to determine the final assignment . </S>",
    "<S> our theoretical results elucidate the effect on performance of increasing the number of projections . </S>",
    "<S> moreover , under a boundary condition implied by the sufficient dimension reduction assumption , we show that the test excess risk of the random projection ensemble classifier can be controlled by terms that do not depend on the original data dimension . </S>",
    "<S> the classifier is also compared empirically with several other popular high - dimensional classifiers via an extensive simulation study , which reveals its excellent finite - sample performance . </S>"
  ]
}