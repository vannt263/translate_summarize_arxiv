{
  "article_text": [
    "sequential recurrent neural networks ( rnns ) are remarkably effective models of natural language . in the last few years , language model results that substantially improve over long - established state - of - the - art baselines have been obtained using rnns @xcite as well as in various conditional language modeling tasks such as machine translation @xcite , image caption generation @xcite , and dialogue generation @xcite . despite these impressive results , sequential models are _ a priori _ inappropriate models of natural language , since relationships among words are largely organized in terms of latent nested structures rather than sequential surface order  @xcite .    in this paper",
    ", we introduce * recurrent neural network grammars * ( rnngs ;  [ sec : formal ] ) , a new generative probabilistic model of sentences that explicitly models nested , hierarchical relationships among words and phrases .",
    "rnngs operate via a recursive syntactic process reminiscent of probabilistic context - free grammar generation , but decisions are parameterized using rnns that condition on the entire syntactic derivation history , greatly relaxing context - free independence assumptions .",
    "the foundation of this work is a top - down variant of transition - based parsing (  [ sec : top - down ] ) .",
    "we give two variants of the algorithm , one for parsing ( given an observed sentence , transform it into a tree ) , and one for generation .",
    "while several transition - based neural models of syntactic generation exist @xcite , these have relied on structure building operations based on parsing actions in shift - reduce and left - corner parsers which operate in a largely bottom - up fashion .",
    "while this construction is appealing because inference is relatively straightforward , it limits the use of top - down grammar information , which is helpful for generation  @xcite .",
    "rnngs maintain the algorithmic convenience of transition - based parsing but incorporate top - down ( i.e. , root - to - terminal ) syntactic information  (  [ sec : model ] ) .",
    "the top - down transition set that rnngs are based on lends itself to discriminative modeling as well , where sequences of transitions are modeled conditional on the full input sentence along with the incrementally constructed syntactic structures .",
    "similar to previously published discriminative bottom - up transition - based parsers  ( * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* _ inter alia _ ) , greedy prediction with our model yields a linear - time deterministic parser ( provided an upper bound on the number of actions taken between processing subsequent terminal symbols is imposed ) ; however , our algorithm generates arbitrary tree structures directly , without the binarization required by shift - reduce parsers .",
    "the discriminative model also lets us use ancestor sampling to obtain samples of parse trees for sentences , and this is used to solve a second practical challenge with rnngs : approximating the marginal likelihood and map tree of a sentence under the generative model .",
    "we present a simple importance sampling algorithm which uses samples from the discriminative parser to solve inference problems in the generative model  (  [ sec : inference ] ) .",
    "experiments show that rnngs are effective for both language modeling and parsing  (  [ sec : exp ] ) .",
    "our generative model obtains ( i )  the best - known parsing results using a single supervised generative model and ( ii )  better perplexities in language modeling than state - of - the - art sequential lstm language models . surprisingly ",
    "although in line with previous parsing results showing the effectiveness of generative models  @xcite  parsing with the generative model obtains significantly better results than parsing with the discriminative model .",
    "formally , an rnng is a triple @xmath0 consisting of a finite set of nonterminal symbols ( @xmath1 ) , a finite set of terminal symbols ( @xmath2 ) such that @xmath3 , and a collection of neural network parameters @xmath4 . it does not explicitly define rules since these are implicitly characterized by @xmath4 .",
    "the algorithm that the grammar uses to generate trees and strings in the language is characterized in terms of a transition - based algorithm , which is outlined in the next section . in the section after that ,",
    "the semantics of the parameters that are used to turn this into a stochastic algorithm that generates pairs of trees and strings are discussed .",
    "rnngs are based on a top - down generation algorithm that relies on a stack data structure of partially completed syntactic constituents . to emphasize the similarity of our algorithm to more familiar bottom - up shift - reduce recognition algorithms",
    ", we first present the parsing ( rather than generation ) version of our algorithm  (  [ sec : parse ] ) and then present modifications to turn it into a generator  (  [ sec : gen ] ) .",
    "the parsing algorithm transforms a sequence of words @xmath5 into a parse tree @xmath6 using two data structures ( a stack and an input buffer ) . as with the bottom - up algorithm of @xcite ,",
    "our algorithm begins with the stack ( @xmath7 ) empty and the complete sequence of words in the input buffer ( @xmath8 ) .",
    "the buffer contains unprocessed terminal symbols , and the stack contains terminal symbols , `` open '' nonterminal symbols , and completed constituents . at each timestep , one of the following three classes of operations ( fig .",
    "[ fig : parser ] ) is selected by a classifier , based on the current contents on the stack and buffer :    @xmath9 introduces an `` open nonterminal '' x onto the top of the stack .",
    "open nonterminals are written as a nonterminal symbol preceded by an open parenthesis , e.g. , `` ( vp '' , and they represent a nonterminal whose child nodes have not yet been fully constructed .",
    "open nonterminals are `` closed '' to form complete constituents by subsequent reduce operations .",
    "reduce repeatedly pops completed subtrees or terminal symbols from the stack until an open nonterminal is encountered , and then this open nt is popped and used as the label of a new constituent that has the popped subtrees as its children .",
    "this new completed constituent is pushed onto the stack as a single composite item .",
    "a single reduce operation can thus create constituents with an unbounded number of children .",
    "the parsing algorithm terminates when there is a single completed constituent on the stack and the buffer is empty .",
    "[ fig : example ] shows an example parse using our transition set .",
    "note that in this paper we do not model preterminal symbols ( i.e. , part - of - speech tags ) and our examples therefore do not include them . and also reduces the number of action types , both of which reduce the runtime .",
    "furthermore , standard parsing evaluation scores do not depend on preterminal prediction accuracy . ]",
    "our transition set is closely related to the operations used in earley s algorithm which likewise introduces nonterminals symbols with its predict operation and later completes them after consuming terminal symbols one at a time using scan @xcite .",
    "it is likewise closely related to the `` linearized '' parse trees proposed by @xcite and to the top - down , left - to - right decompositions of trees used in previous generative parsing and language modeling work  @xcite .",
    "a further connection is to @xmath11 parsing which uses an unbounded lookahead ( compactly represented by a dfa ) to distinguish between parse alternatives in a top - down parser  @xcite ; however , our parser uses an rnn encoding of the lookahead rather than a dfa ."
  ],
  "abstract_text": [
    "<S> we introduce recurrent neural network grammars , probabilistic models of sentences with explicit phrase structure . </S>",
    "<S> we explain efficient inference procedures that allow application to both parsing and language modeling . </S>",
    "<S> experiments show that they provide better parsing in english than any single previously published supervised generative model and better language modeling than state - of - the - art sequential rnns in english and chinese .    this is modified version of a paper originally published at naacl 2016 that contains a corrigendum at the end , with improved results after fixing an implementation bug in the rnng composition function . </S>"
  ]
}