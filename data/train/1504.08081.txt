{
  "article_text": [
    "paper focuses on the following dynamical system @xmath1",
    "y = g(\\lambda x),\\\\[2pt ] \\end{cases}\\label{mg0.1}\\end{aligned}\\ ] ] where @xmath2 is the state vector , @xmath3 is a constant self - inhibition matrix , the cost function @xmath4 is an _ analytic _ function and @xmath5 is a constant input vector .",
    "@xmath6 is the output vector with the sigmoid function @xmath7 as nonlinear activation function .",
    "eq . was firstly proposed in @xcite and is a general model of neural - network system arising in recent years .",
    "for example , the well - known hopfield neural network @xcite , whose continuous - time version can be formulated as @xmath8 y_{i}=g_{i}(\\lambda_{i}x_{i}),\\\\[2pt ] \\end{cases}\\label{mg0.2}\\end{aligned}\\ ] ] for @xmath9 , where @xmath10 stands for the state of neuron @xmath11 and each activation function @xmath12 is sigmoid . with the symmetric weight condition ( @xmath13 for all @xmath14 ) ,",
    "( [ mg0.2 ] ) can be formulated as eq .",
    "( [ mg0.1 ] ) with @xmath15 .",
    "this model has a great variety of applications .",
    "it can be used to search for local minima of the quadratic objective function of @xmath4 over the discrete set @xmath16 @xcite-@xcite , for example , the traveling - sales problem @xcite .",
    "one step further , this model was extended for a multi - linear cost function @xmath17 @xcite .",
    "this model can be also regarded as a special form of ( [ mg0.1 ] ) with @xmath18 and was proved that this model can minimize @xmath19 over the discrete set @xmath16 @xcite .    in application for optimisation , analysis of convergence dynamics",
    "is fundamental , which has attracted many interests from different fields .",
    "see @xcite-@xcite and the references therein .",
    "the linearization technique and the classical lasalle approach for proving stability @xcite could be invalid when the system had non - isolated equilibrium points ( e.g. , a manifold of equilibria ) @xcite . a new concept `` absolute stability '' was proposed in @xcite to show that each trajectory of the neural network converges to certain equilibrium for any parameters and activation functions satisfying certain conditions by proving the finiteness of the trajectory length and the celebrated @xmath0ojasiewicz inequality @xcite-@xcite .",
    "this idea can also be seen in an earlier paper @xcite",
    ".    however , in the model ( [ mg0.1 ] ) , the synaptic feedback of each neuron is continuous bsed on the output states of its neighbours , which is costly in practice for a network of a large number of neurons . in recent years , with the development of sensing , communications , and computing equipment , event - triggered control @xcite-@xcite and self - triggered control @xcite-@xcite have been proposed and proved effective in reducing the frequency of synaptic information exchange significantly . in this paper",
    ", we investigate global convergence of analytic neural networks with event - triggered synaptic feedbacks . here",
    ", we present event - triggered rules to reduce the frequency of receiving synaptic feedbacks . at each neuron ,",
    "the synaptic feedback is a constant that is determined by the outputs of its neighbours at its latest triggering time and changes at the next triggering time of this neuron that is triggered by a criterion via its neighborhood information as well .",
    "we prove that the analytic neural networks are convergent ( see _ definition [ convergence ] _ ) under these event - triggered rules by the @xmath0ojasiewicz inequality .",
    "in addition , we further prove that the event - triggered rule is viable , owing to the exclusion of zeno behaviors .",
    "these event - triggered rules are distributed ( each neuron only needs the information of its neighbours and itself ) , asynchronous , ( all the neurons are not required to be triggered in a synchronous way ) , and independent of each other ( triggering of an neuron will not affect or be affected by triggering of other neurons ) . it should be highlighted that our results can be easily extended to a large class of neural networks .",
    "for example , the standard cellular networks @xcite-@xcite .    the paper is organized as follows : in section [ sec2 ] ,",
    "the preliminaries are given ; in section [ sec3 ] , the convergence and the zeno behaviours of analytic neural networks with the triggering rules : distributed event - triggered rule is proved in section [ sec3 ] ; in section [ sec5 ] , examples with numerical simulation are provided to show the effectiveness of the theoretical results and illustrate its application ; the paper is concluded in section [ sec6 ] .    * notions * : @xmath20 denotes @xmath21-dimensional real space .",
    "@xmath22 represents the euclidean norm for vectors or the induced 2-norm for matrices .",
    "@xmath23 stands for an @xmath21-dimensional ball with center @xmath24 and radius @xmath25 . for a function @xmath26",
    ", @xmath27 is its gradient .",
    "for a set @xmath28 and a point @xmath24 , @xmath29 indicates the distance from @xmath30 to @xmath31 .",
    "in this section , we firstly provide some definitions and results on algebraic graph theory , which will be used later .",
    "( see the textbooks @xcite , @xcite for details )    for a directed graph @xmath32 of @xmath21 neurons ( or nodes ) . where @xmath33 is the set of neurons , @xmath34 is the set of the links ( or edges ) , and @xmath35_{n\\times n}$ ] with nonnegative adjacency elements @xmath36 is the adjacency matrix , a link of @xmath37 is denoted by @xmath38 if there is a directed link from neuron @xmath39 to @xmath40 and the adjacency elements associated with the links of the graph are positive , ( i.e. , @xmath41 if and only if @xmath42 ) . we take @xmath43 for all @xmath9",
    ". moreover , the in - neighbours and out - neighbours set of neuron @xmath40 are defined as @xmath44 and @xmath45 .",
    "the neighbours of the neuron @xmath40 denoted by @xmath46 is the union of in - neighbours @xmath47 and out - neighbours @xmath48 , that is , @xmath49 .",
    "consider the discrete - time synaptic feedback , eq .",
    "can be reformulated as follows @xmath50_{i}+\\theta_{i}\\\\[5pt ] y_{i}(t)=g_{i}\\big(\\lambda_ix_i(t)\\big)\\\\[3pt ] \\end{cases}\\end{aligned}\\ ] ] for @xmath9 and @xmath51 , where @xmath52 , @xmath53 and @xmath54 .",
    "@xmath55 is an _",
    "analytic _ cost function function and @xmath56 is the output vector with a scaling parameter @xmath57 and the sigmoid functions @xmath12 as nonlinear activation functions . in this paper , we take @xmath58 and the gradient of the activation function @xmath7 at @xmath2 can be written as @xmath59 .",
    "the strict increasing triggering event time sequence @xmath60 ( to be defined ) are neuron - wise and @xmath61 , for all @xmath9 . at each @xmath62",
    ", each neuron @xmath11 changes the information from its neighbours with respect to an identical time point @xmath63 with @xmath64 . throughout the paper",
    ", we may simplify the notation @xmath63 as @xmath65 unless there is a potential ambiguity .",
    "thus , we have @xmath66_{i}+\\theta_{i}\\\\[5pt ] y_{i}(t)=g_{i}\\big(\\lambda_ix_i(t)\\big)\\\\[3pt ] \\end{cases}\\label{mg}\\end{aligned}\\ ] ] for @xmath9 and @xmath67 .",
    "let @xmath68^{\\top}$ ] be the vector at the right - hand side of , where @xmath69_{i}+\\theta_{i}.\\end{aligned}\\ ] ] note that when we consider the trajectories , the right - hand side of can be written as @xmath70^{\\top}$ ] .",
    "denote the set of equilibrium points for as @xmath71 we first recall the definition of convergence for model @xcite .",
    "[ convergence]@xcite given an analytic function @xmath72 , a sigmoid function @xmath12 and three constants @xmath73 , @xmath74 and @xmath75 specifically , system is said to be _ convergent _ if and only if , for any trajectory @xmath76 of , there exists @xmath77 such that @xmath78    since the @xmath79-limit set of any trajectory @xmath76 for the system ( i.e. , the set of points that are approached by @xmath76 as @xmath80 ) is isolated equilibrium points , the convergence of the system is global .",
    "our main focus lies in proving that the state @xmath76 of the system under some given rule can converge to these equilibrium points .",
    "the following lemma shows that all solutions for are bounded and there exists at least one equilibrium point .",
    "[ existence ] given a constant matrix @xmath3 , a constant vector @xmath81 and two specific functions @xmath72 and @xmath7 , for any triggering event time sequence @xmath82 , there exists a unique solution for the piece - wise cauchy problem with some initial data @xmath83 .",
    "moreover , the solutions with different initial data are bounded for @xmath84 .",
    "firstly , we prove the existence and uniqueness of the solution for the system ( [ mg ] ) . denotes @xmath85^{\\top}$ ] , where @xmath67 . given a time sequence @xmath82 ordered as @xmath86 ( same items in @xmath87",
    "treat as one ) , there exists a unique solution of in the interval @xmath88 by using @xmath89 as the initial data ( see , existence and uniqueness theorem in @xcite ) . for the next interval @xmath90",
    ", @xmath91 can be regarded as the new initial data , which can derive another unique solution in this interval . by induction",
    ", we can conclude that there exists a piecewise unique solution over the time interval @xmath84 , which is for the cauchy problem with the initial data @xmath92 .",
    "secondly , since @xmath93 , there exists a constant @xmath94 such that @xmath95 thus for any @xmath96 , there exists @xmath97 such that @xmath98 where @xmath9 . let @xmath99 if @xmath100 , @xmath76 will drop into the set @xmath101 in finite time , which implies that @xmath101 is the @xmath79-limit set of any trajectory @xmath76 and it is also positively invariant .",
    "thus all the solutions of with different initial data are eventually confined in @xmath101 , hence they are bounded for @xmath84 .",
    "consider now the set of equilibrium points @xmath102 .",
    "the following lemma is established in @xcite , which shows that there exists at least one equilibrium point in @xmath102 .",
    "[ nontrivial ] for the of equilibrium points @xmath102 , the following statements hold :    1 .",
    "@xmath102 is not empty .",
    "there exists a constant @xmath25 such that @xmath103    to depict the event that triggers the next feedback basing time point , we introduce the following candidate lyapunov ( or energy ) function : @xmath104 -\\theta^{\\top } y.\\label{ly}\\end{aligned}\\ ] ] where @xmath105 $ ] with @xmath106 . the function @xmath107 generalizes the lyapunov function introduced for in @xcite , and it can also be thought of as the energy function for the hopfield and the cellular neural networks model @xcite . in this paper",
    ", we will prove that the candidate lyapunov function is a strict lyapunov function @xcite , as stated in the following definition .",
    "a lyapunov function @xmath108 is said to be strict if @xmath109 , and the derivative of @xmath110 along trajectories @xmath76 , i.e. @xmath111 , satisfies @xmath112 and @xmath113 for @xmath114 .",
    "the next lemma provides an inequality , named @xmath0ojasiewicz inequality @xcite .",
    "it will be used to prove the finiteness of length for any trajectory @xmath76 of the system , which can finally derive the convergence of system .",
    "the definition of trajectory length is also listed in definition [ def2 ] .    [ loj ] consider an analytic and continuous function @xmath115 .",
    "let @xmath116 for any @xmath117 , there exist two constants @xmath118 and @xmath119 , such that @xmath120 for @xmath121 .",
    "[ def2 ] let @xmath76 on @xmath84 , be some trajectory of .",
    "for any @xmath122 , the length of the trajectory on @xmath123 is given by @xmath124    it was pointed out in @xcite that finite length implied the convergence of the trajectory , and was also used to discuss the global stability of the analytic neural networks in @xcite .",
    "in this section we synthesize distributed triggers that prescribe when neurons should broadcast state information and update their control signals .",
    "section [ primary ] presents the evolution of a quadratic function that measures network disagreement to identify a triggering function and discusses the problems that arise in its implementation .",
    "these observations are our starting point in section [ morsesardtheorem ] and section [ zenobehavior ] , where we should overcome these implementation issues .",
    "to design appropriate triggering time point @xmath87 of system for @xmath9 , we define the state measurement error vector @xmath125^{\\top}$ ] where @xmath126_{i}-\\big[\\nabla f\\big(y(t_{k}^{i})\\big)\\big]_{i}\\end{aligned}\\ ] ] for @xmath127 with @xmath9 and @xmath67 .",
    "to design the triggering function @xmath128 for the updating rule , we define a function vector @xmath129^{\\top}$ ] with @xmath130 where can be thought of as a normalized function of @xmath131 by exponential decay function @xmath132 .",
    "thus the coefficient @xmath133 with respect to time @xmath62 in eq . can be seen as a parameter from this normalization process . ]",
    "@xmath134    for @xmath9 and @xmath67 .",
    "what we can observe is a neuron s state @xmath135 at a particular time point or a time period ( a subset of @xmath136 ) from system . given a specific analytic function @xmath72",
    ", we can directly figure out the right - hand term @xmath137 without knowing the theoretical formula of the trajectory @xmath76 of the system on @xmath136 in advance .",
    "thus , @xmath133 can also be calculated straightly .",
    "the samplings for @xmath138 with continuous monitoring or discrete - time monitoring would determine the efficiency level and the adjustment cost for the system s convergence .",
    "the continuous monitoring can ensure a high level of efficiency with large costs , while discrete - time monitoring on @xmath135 can reduce the cost , but sacrifice the efficiency . in section [ monitoring ] , we will discuss the discrete - time monitoring and give a prediction algorithm for the triggering time point @xmath65 based on the obtained information of @xmath135 for all @xmath9 .",
    "[ primaryrule ] set @xmath139 as the time point by the updating rule that @xmath140 that is @xmath141 for @xmath9 and @xmath67 . then , system is convergent .",
    "the proof of this theorem comprises of the five propositions as follow .",
    "[ proposition1 ] under the assumptions in theorem [ primaryrule ] , @xmath107 in serves as a strict lyapunov function for the system .",
    "the partial derivative of the candidate lyapunov function @xmath107 along the trajectory @xmath76 can be written as_{i}+\\theta_{i}$ ] , for @xmath9 . ]",
    "@xmath142_{i}+\\theta_{i}\\bigg\\}\\nonumber\\\\ = & -\\lambda_{i}g'\\big(\\lambda_{i}x_{i}(t)\\big)\\bigg\\{-d_{i } x_{i}(t ) -\\big[\\nabla f\\big(y(t^{i}_{k})\\big)\\big]_{i}+\\theta_{i}\\nonumber\\\\   & -\\big[\\nabla f\\big(y(t)\\big)\\big]_{i } + \\big[\\nabla f\\big(y(t^{i}_{k})\\big)\\big]_{i}\\bigg\\}\\nonumber\\\\ = & -\\lambda_{i}g'\\big(\\lambda_{i}x_{i}(t)\\big)\\big[f_{i}\\big(x(t)\\big)-e_i(t)\\big ] , \\label{dlyx}\\end{aligned}\\ ] ] and the time derivative of @xmath143 @xmath144f_{i}\\big(x(t)\\big).\\end{aligned}\\ ] ] consider the inequality @xmath145 the time derivative @xmath146 can be bounded as @xmath147\\nonumber\\\\ \\leqslant   & -\\big(1-\\frac{a}{2}\\big)\\sum_{i=1}^{n}\\lambda_{i}g'\\big(\\lambda_{i}x_{i}(t)\\big)\\big|f_{i}\\big(x(t)\\big)\\big|^{2}\\nonumber\\\\   & + \\frac{1}{2a}\\sum_{i=1}^{n}\\lambda_{i}g'\\big(\\lambda_{i}x_{i}(t)\\big)\\big|e_{i}(t)\\big|^{2}\\nonumber\\\\ \\leqslant   & -\\alpha\\sum_{i=1}^{n}\\big|f_{i}\\big(x(t)\\big)\\big|^{2}+\\beta\\sum_{i=1}^{n}\\big|e_{i}(t)\\big|^{2}\\end{aligned}\\ ] ] by using the rule , it holds @xmath148 for all @xmath67 . for any @xmath114 ,",
    "there exits @xmath149 such that @xmath150 . thus @xmath151 .",
    "proposition [ proposition1 ] is proved .    with the lyapunov function @xmath107 for system ( [ mg ] ) and the event triggering condition",
    ", the consequent proof follows @xcite with necessary modifications .",
    "[ proposition2 ] there exist finite different energy levels @xmath152 , such that each set of equilibrium points @xmath153 is not empty .",
    "given an analytic function @xmath72 , a sigmoid function @xmath12 and three constants @xmath73 , @xmath74 and @xmath75 specifically , it follows that the candidate lyapunov function @xmath107 in is analytic on @xmath20 .",
    "suppose that there exist infinite different values @xmath154 such that @xmath155 is not empty .",
    "from lemma [ nontrivial ] , it is known that there exists @xmath156 such that outside @xmath157 there are no equilibrium points .",
    "hence @xmath158 for @xmath159 .",
    "consider points @xmath160 for @xmath159 . since @xmath161 , it holds @xmath162 and from eq .",
    ", @xmath163 .",
    "since @xmath164 is a compact set , hence , there exist a point @xmath165 and a subsequence @xmath166 such that @xmath167 for all @xmath168 and @xmath169 as @xmath170 .",
    "since @xmath171 is continuous , taking into account that @xmath172 for all @xmath168 , it results @xmath173 .    according to lemma [ loj ] , there exist @xmath174 and @xmath175 such that @xmath176 for @xmath177 .",
    "since @xmath169 as @xmath170 and @xmath178 have different energy levels @xmath179 , we can pick a point @xmath180 such that @xmath181 .",
    "then @xmath182 which is a contradiction .",
    "this completes the proof .    without loss of generality ,",
    "assume that the energy levels @xmath183 are ordered as @xmath184 .",
    "thus there exists @xmath185 such that @xmath186 , for any @xmath187 . for",
    "any given @xmath188 , define @xmath189 and @xmath190\\big\\}.\\end{aligned}\\ ] ]    [ proposition3 ] for @xmath191 , @xmath192 is a compact set and @xmath193 .    from lemma [ nontrivial ] ,",
    "@xmath194 is bounded , hence @xmath195 is a compact set and @xmath196\\big\\}$ ] is a closed set .",
    "thus , @xmath197\\big\\}$ ] is a compact set",
    ". then proterty @xmath198 is an immediate consequence of proposition [ proposition2 ] .",
    "[ proposition4 ] for any trajectory @xmath76 of the system and any given time point @xmath199 , let @xmath192 , for some @xmath200 , be a compact set as defined in .",
    "then there exist a constant @xmath201 and an exponent @xmath202 such that @xmath203 for @xmath204 .",
    "since the notion @xmath205 is simplified as @xmath65 where @xmath206 , the following equation @xmath207_{i}+\\theta_{i},\\end{aligned}\\ ] ] can be rewritten as @xmath208_{i}+\\theta_{i},\\end{aligned}\\ ] ] for @xmath9 . from eq . and the condition , we have @xmath209\\bigg|^{2}\\nonumber\\\\[2pt ] \\leqslant &    ~\\beta_{j}^{2}\\sum_{i=1}^{n }    \\bigg [     f_{i}^{2}\\big(x(\\tau)\\big )    + e_{i}^{2}\\big(x(\\tau)\\big )    + 2\\big|f_{i}\\big(x(\\tau)\\big)e_{i}\\big(x(\\tau)\\big)\\big|    \\bigg]\\nonumber\\\\[2pt ] \\leqslant &    ~\\beta_{j}^{2}\\sum_{i=1}^{n }    \\bigg[(1+c)f_{i}^{2}\\big(x(\\tau)\\big)+\\bigg(1+\\frac{1}{c}\\bigg)e_{i}^{2}\\big(x(\\tau)\\big)\\bigg]\\nonumber\\\\[2pt ] \\leqslant &    ~\\beta_{j}^{2}(1+c)\\sum_{i=1}^{n}\\big|f_{i}\\big(x(\\tau)\\big)\\big|^{2}+    ~\\beta_{j}^{2}\\bigg(1+\\frac{1}{c}\\bigg)\\gamma^{2}\\sum_{i=1}^{n}\\psi_{i}^{2}(t)\\nonumber\\\\[2pt ] = & ~\\beta_{j}^{2}(1+c)\\bigg [    \\sum_{i=1}^{n}\\big|f_{i}\\big(x(\\tau)\\big)\\big|^{2 }   + \\frac{\\gamma^{2}}{c}\\sum_{i=1}^{n}\\big(\\delta(t)\\big)^{2}{\\rm e}^{-2d_{i}(t - t_{k}^{i})}\\bigg]\\nonumber\\\\[2pt ] = & ~\\beta_{j}^{2}(1+c)\\bigg(1+\\frac{\\gamma^{2}}{c}\\bigg)\\sum_{i=1}^{n}\\big|f_{i}\\big(x(\\tau)\\big)\\big|^{2}\\nonumber\\\\ = & ~\\beta_{j}^{2}(1+c)\\bigg(1+\\frac{\\gamma^{2}}{c}\\bigg)\\big\\|f\\big(x(\\tau)\\big)\\big\\|^{2},\\end{aligned}\\ ] ] where @xmath210 . then it holds @xmath211 where @xmath212 from eq . , we have @xmath213    for the point @xmath204 , from eq",
    ". , @xmath214 .",
    "there exists @xmath215 , @xmath216 and an exponent @xmath217 such that @xmath218 for @xmath219 . indeed , if @xmath215 is small , we have @xmath220 for @xmath221 .",
    "therefore , it holds @xmath222 & \\geqslant\\big(\\alpha-\\beta\\gamma^{2}\\big)h_{j}c\\big(x(\\tau)\\big)\\big|l\\big(x(\\tau)\\big)-l_{j}\\big|^{v(x(\\tau))}\\\\[3pt ] & \\geqslant~c_{j}\\big|l\\big(x(\\tau)\\big)-l_{j}\\big|^{v_{j}},\\end{aligned}\\ ] ] where @xmath223 and @xmath224 for @xmath225 .",
    "now , we are at the stage to prove that the length of @xmath76 on @xmath136 is finite .",
    "the statement proposition is given as follow .",
    "[ proposition5 ] any trajectory @xmath76 of the systm has a finite length on @xmath136 , i.e. , @xmath226\\end{aligned}\\ ] ]    assume without loss of generality that @xmath92 is not an equilibrium point of eq . .",
    "due to the uniqueness of solutions , we have @xmath227 for @xmath228 , i.e. , @xmath229 for @xmath228 . from proposition",
    "[ proposition1 ] , it is seen that @xmath143 satisfies @xmath230 for @xmath231 , i.e. , @xmath143 strictly decreases for @xmath231 .",
    "thus , since @xmath76 is bounded on @xmath136 and @xmath143 is continuous , @xmath143 will tend to a finite value @xmath232 . from proposition",
    "[ proposition1 ] and the lasalle invariance principle @xcite , @xcite , it also follows that @xmath233 .",
    "thus , from the continuity of @xmath110 , it results @xmath234 for some @xmath200 and @xmath235 .    since @xmath235 and @xmath236",
    ", it follows that there exists @xmath237 such that @xmath238 for @xmath239 . by using proposition [ proposition4 ] , considering that @xmath240 for @xmath228 and @xmath241 for @xmath239 , we have that there exists @xmath201 and @xmath202 such that @xmath242 for @xmath239",
    ". then @xmath243 the change of variable @xmath244 derives @xmath245^{1-v_{j}}\\\\   & -\\big[l\\big(x(t)\\big)-l(+\\infty)\\big]^{1-v_{j}}\\bigg\\}\\\\ \\leqslant&\\,\\frac{1}{c_{j}(1-v_{j})}\\big[l\\big(x(\\widetilde{t})\\big)-l(+\\infty)\\big]^{1-v_{j}},\\end{aligned}\\ ] ] for @xmath239 .",
    "therefore , we have @xmath246^{1-v_{j}}}{c_{j}(1-v_{j})}\\\\ & < + \\infty.\\end{aligned}\\ ] ] this completes the proof of proposition [ proposition5 ] .    in what follows",
    "it remains to address the proof of theorem [ primaryrule ] , which is given in section [ primary ] .",
    "_ proof of theorem [ primaryrule ] : _ suppose that the condition holds . then from proposition [ proposition5 ] , for any trajectory @xmath76 of the system , we have @xmath247 from cauchy criterion on limit existence , for any @xmath188 , there exists @xmath248 such that when @xmath249 , it results @xmath250 .",
    "thus , @xmath251 it follows that there exists an equilibrium point @xmath252 of , such that @xmath253 . recalling the definition [ convergence ]",
    ", we can obtain that system is convergence .",
    "the event - triggered condition implies that the next time interval for neuron @xmath40 depends on states of the neurons @xmath39 that are synaptically linked to neuron @xmath40 we say that neuron @xmath39 is synaptically linked to neuron @xmath40 if @xmath254_{i}$ ] depends on @xmath255 , in other words , @xmath256    it seems naturally that when the event triggers , the neuron @xmath40 has to send its current state information @xmath135 to its out - neighbours immediately in order to avoid having @xmath257 .",
    "however , such a trigger would have the following problems :    1 .",
    "[ p1 ] the triggering function @xmath258 may hold even after neuron @xmath40 sends its new state to its neighbours .",
    "a bad situation is that @xmath259 happens at the same time when @xmath260 .",
    "this may cause the neuron to send its state continuously .",
    "this is called _ continuous triggering situation _ in the zeno behavior .",
    "2 .   [ p2 ] event if @xmath259 and @xmath260 never happen at the same time point .",
    "the zeno behavior may still exist .",
    "for example , one neuron @xmath40 broadcasting its new state to its out - neighbours may cause the triggering rules for two neurons @xmath261 and @xmath262 in @xmath48 are broken alternately . that is to say , the inter - event time for both @xmath261 and @xmath262 will decrease to zero .",
    "this is called _ alternate triggering situation _ in the zeno behavior .",
    "these observations motivate us to introduce the morse - sard theorem for avoiding the _ continuous triggering situation _ ( p[p1 ] ) in subsection [ morsesardtheorem ] . in subsection [ zenobehavior ]",
    ", we will also prove that for all the neuron @xmath263 , the _ alternate triggering situation _ is absent by using our distributed event - triggered rule in theorem [ primaryrule ] .      from the rule , we know that a triggering event happens at a threshold time @xmath65 satisfying @xmath264 for @xmath9 and @xmath67 .",
    "to avoid the situation that @xmath265 and @xmath260 happen at the same triggering time point @xmath65 for some @xmath266 , when the triggering function @xmath258 still holds after the neuron @xmath40 sends the new state to its neighbours , we define a function vector @xmath267\\end{aligned}\\ ] ] where @xmath268 is one of the triggering time points before the present time @xmath62 and @xmath269^{\\top}$ ] .",
    "the following morse - sard theorem will be used for excluding this continuous triggering .",
    "[ mstheorem]for each initial data @xmath92 , there exists a measure zero subset @xmath270 such that for any given neuron @xmath263 , the threshold time @xmath271 for @xmath272 corresponding to initial data @xmath273 are countable for all @xmath67 .",
    "that is to say , the triggering time point set @xmath274 is a countable set .    to show that the threshold time @xmath271 are countable for each @xmath273",
    ", we prove a statement that the jacobian matrix @xmath275^{\\top}$ ] has rank @xmath21 at next triggering time point @xmath276 , where @xmath277.\\end{aligned}\\ ] ] the two components of the above equation satisfy @xmath278_{i }    + \\gamma^{2}d_{i}\\delta_{k}^{2}\\,{\\rm e}^{-2d_{i}(t - t_{k}^{i})}\\\\ = & ~e_{i}(t)\\frac{{\\rm d}}{{\\rm d}t}\\big[\\nabla f\\big(y(t)\\big)\\big]_{i }    + \\gamma^{2}d_{i}\\psi_{i}^{2}(t)\\end{aligned}\\ ] ] and @xmath279_{i }    -\\gamma^{2}d_{i}\\delta_{k}^{2}\\,{\\rm e}^{-2d_{i}(t - t_{k}^{i})}\\\\ = & -e_{i}(t)\\frac{{\\rm d}}{{\\rm d}t_{\\tau}}\\big[\\nabla f\\big(y(t_{\\tau})\\big)\\big]_{i }    -\\gamma^{2}d_{i}\\psi_{i}^{2}(t)\\end{aligned}\\ ] ] when event triggers and @xmath280 resets to @xmath281 in the short time period after the next time point @xmath139 , that is , @xmath282 when @xmath283 , then it follows @xmath284 and @xmath285 define a initial data set for neuron @xmath40 by @xmath286 and it holds @xmath287 in the sense of lebesgue measure .",
    "take the initial data @xmath288 from @xmath289 , we have @xmath290 that is , @xmath291 which implies @xmath292\\bigg|_{t = t_{k+1}^{i}+\\varepsilon}\\\\ = & \\big[\\gamma^{2}d_{i}\\psi_{i}^{2}(t_{k+1}^{i}),-\\gamma^{2}d_{i}\\psi_{i}^{2}(t_{k+1}^{i})\\big]\\\\[2pt ] \\neq&0\\end{aligned}\\ ] ] thus , for each initial data @xmath273 with @xmath293 the jacobian matrix @xmath294 has rank @xmath21 at time @xmath276 .    now using the inverse function theorem at each @xmath273",
    ", we can obtain that for each threshold time @xmath271 defined in eq .",
    ", the next triggering time point @xmath139 is isolated , hence the set @xmath274 is a countable set .",
    "the morse - sard theorem is proved .",
    "recalling the triggering function @xmath128 , we can obtain the results that if the initial data @xmath273 , then @xmath295 that is to say , @xmath265 and @xmath260 may never happen at the same time at all the triggering time point @xmath65 where @xmath9 and @xmath67 .",
    "therefore , the _ continuous triggering situation _ in the zeno behavior ( p[p1 ] ) is avoided .    to refrain @xmath92 from the zero measured subset @xmath296 ,",
    "a small perturbation on initial data @xmath92 can be introduced , which can make it be away from the value that leads to @xmath297 .",
    "the small perturbation on initial data has no influence on the convergence of the system , for the equilibria of the system do not depend on the initial data sensitively .",
    "after we exclude the _ continuous triggering situation _ in the above section , what remains is the _ alternate triggering situation _ in the zeno behavior .",
    "to prove that this situation is absent when using the distributed event - triggered rule , we will find a common positive lower - bound for all the inter - event time @xmath298 , where @xmath9 and @xmath67 .",
    "[ zeno ] let @xmath296 be a zero measured set as defined in . under the distributed event - triggered rule in theorem [ primaryrule ] , for each @xmath273 , the next inter - event interval of every neuron is strictly positive and has a common positive lower - bound .",
    "furthermore , the _ alternate triggering situation _ in the zeno behavior is excluded .",
    "let us consider the following derivative of the state measurement error for neuron @xmath263 @xmath299_{ij}\\,\\dot{y_{j}}(t)\\bigg|\\\\[2pt ] & = \\bigg|\\sum_{j=1}^{n}\\big[\\nabla^2 f\\big(y(t)\\big)\\big]_{ij }    \\lambda_{j}g'_{j}\\big(\\lambda_{j}x_{j}(t)\\big)f_{j}\\big(x(t)\\big)\\bigg|\\\\ & \\leqslant    \\big\\|\\nabla^{2 } f\\big(y(t)\\big)\\big\\|    \\big\\|\\lambda\\,\\partial g\\big(\\lambda x(t)\\big)\\big\\|    \\sqrt{\\sum_{j=1}^{n}\\big|f_{j}\\big(x(t)\\big)\\big|^{2}}\\\\ & = \\big\\|\\nabla^{2 } f\\big(y(t)\\big)\\big\\|    \\big\\|\\lambda\\,\\partial g\\big(\\lambda x(t)\\big)\\big\\|\\delta(t )   \\sqrt{\\sum_{j=1}^{n}{\\rm e}^{-2d_{j}(t - t_{k}^{j})}}\\\\[2pt ] & \\leqslant\\mathcal m \\big\\|\\lambda\\big\\| \\sqrt{n}\\,\\delta(t),\\end{aligned}\\ ] ] where @xmath300 then it follows @xmath301 based on the distributed event - triggered rule , the event will not trigger until @xmath302 at time point @xmath303 .",
    "thus , for each @xmath273 , it holds @xmath304 namely , @xmath305 with @xmath306 , which possesses a positive solution .",
    "hence , for all the neuron @xmath263 , the next inter - event time has a common positive lower - bound which follows @xmath307    it can be seen that @xmath308 has no concern with all the neurons states @xmath138 .",
    "thus , there exists a common positive lower - bound , which is a constant , for the next inter - event interval of each neuron . that is to say , the next triggering time point @xmath139 satisfies @xmath309 for all @xmath9 and @xmath67 , hence the absence of the _ alternate triggering situation _ in the zeno behavior ( p[p2 ] ) is proved .    to sum up",
    ", we have excluded both the _ continuous triggering situation _ and _ alternate triggering situation _ in the zeno behavior , when the distributed event - triggered rule is taken into account .",
    "therefore , we can assert that there is no zeno behavior for all the neurons .",
    "the continuous monitoring strategy for theorem [ primaryrule ] may be costly since the state of the system should be observed simultaneously .",
    "an alternative method is to predict the triggering time point when inequality does not hold and update the triggering time accordingly .    for any neuron @xmath263 , according to the current event timing @xmath65 , its state can be formulated as    [ stateformula ]    x_i(t)=x_i(t_k^)+ \\{d_ix_i(t_k^)+_i-_i } +   + y_i(t)=g_i(_ix_i(t ) ) +    & &    for @xmath310 , where @xmath311 is the newest timing of all @xmath40 s in - neighbours , that is @xmath312 and @xmath139 is the next triggering time point at which neuron @xmath40 happens the triggering event .",
    "then , solving the following maximization problem @xmath313 we have the following prediction algorithm ( algorithm [ algorithm ] ) for the next triggering time point . with the information of each neuron at time @xmath314 and the proper parameters @xmath315 , search the observation time @xmath316 by at first . if no triggering events occur in all @xmath40 s in - neighbours during @xmath317 , the neuron @xmath40 triggers at time @xmath318 and record as the next triggering event time @xmath139 , that is @xmath319 .",
    "renew the neuron @xmath40 s state and send the renewed information to all its out - neighbours .",
    "the prediction of neuron @xmath40 is finished .",
    "if some in - neighbours of @xmath40 triggers at time @xmath320 , update @xmath311 in state formula and go back to find a new observation time @xmath318 by solving the maximization problem .",
    "@xmath321 @xmath47 is the set of neuron @xmath40 s in - neighbours    initialize @xmath185 @xmath322 @xmath323 for all @xmath9 @xmath324 search @xmath316 by the strategy @xmath325 @xmath40 triggers at time @xmath326 @xmath40 renew its state information @xmath327 @xmath40 sends the state information to its out - neighbours @xmath328 update @xmath311 in the state formula    in addition , when neuron @xmath40 updates its observation time @xmath316 , the triggering time predictions of @xmath40 s out - neighbours will be affected . therefore , besides the state formula and the maximization problem as given before , each neuron should take their triggering event time whenever any of its in - neighbours renews and broadcasts its state information .",
    "in other word , if one neuron updates its triggering event time , it is mandatory to inform all its out - neighbours .",
    "the discrete - time monitoring by using the state formula may lose the high - level efficiency of the convergence , because it abandons the continuous adjustment on @xmath133 as defined in eq . .",
    "but the advantage is that a discrete - time inspection on @xmath76 can be introduced to ensure the convergence in theorem [ primaryrule ] .",
    "this can reduce the monitoring cost .",
    "in this section , two numerical examples are given to demonstrate the effectiveness of the presented results and the application .",
    "* example 1 : * considering a 2-dimension analytic neural network with @xmath329 where @xmath330 we have @xmath331 and adopt the distributed event - triggered rule ( theorem [ primaryrule ] ) .",
    "the initial value of each neuron is randomly selected in the interval @xmath332 $ ] .",
    "figure [ fig:1 ] shows that the state @xmath76 converges to @xmath333^{\\top}$ ] with the initial value @xmath334^{\\top}$ ] by taking @xmath335",
    ".     of two neurons converges to @xmath333^{\\top}$ ] with @xmath334^{\\top}$].,scaledwidth=48.0% ]    take the different values of the parameter @xmath315 under the distributed event - triggered rule , the simulation results are shown in table [ table1 ] . in this table , @xmath308 is the theoretical lower - bound for the inter - event time of all the neurons calculated by .",
    "@xmath336 is the actual calculation value of the minimal length of inter - event time .",
    "@xmath337 is number of triggering times and @xmath338 stands for the first time when @xmath339 , as an index for the convergence rate .",
    "all results are drawn by averaging over 50 overlaps .",
    "it can be seen that the actual calculation minimal inter - event time @xmath340 is larger than the corresponding theoretical lower - bound @xmath308 .",
    "this implies that we have excluded the zeno behavior with the lower - bound @xmath308 of the inter - event time for all the neurons .",
    "moreover , the actual number of event @xmath337 decrease while @xmath338 increases with @xmath315 increasing , which is in agreement with the theoretical results .",
    "c|c|c|c|c    ' '' ''",
    "@xmath315 & @xmath308 & @xmath340 & @xmath337 & @xmath341 + 0.1 & 0.4676 & 0.6072 & 42.10 & 31.6612 + 0.2 & 0.4914 & 0.8920 & 26.90 & 32.5330 + 0.3 & 0.5378 & 1.0560 & 21.16 & 32.8354 + 0.4 & 0.5974 & 1.1643 & 17.92 & 33.0597 + 0.5 & 0.6514 & 1.2014 & 15.58 & 33.0533 + 0.6 & 0.7224 & 1.2020 & 14.48 & 33.2765 + 0.7 & 0.7826 & 1.2018 & 12.70 & 33.4677 + 0.8 & 0.8232 & 1.2028 & 13.22 & 33.4744 + 0.9 & 0.8446 & 1.2043 & 12.76 & 33.4854 +    [ table1 ]    according to the definition of lyapunov ( or energy ) function , if the input @xmath342 takes a sufficient small value and @xmath343 for @xmath344 , then @xmath345 .",
    "thus , as an application of our results , system with the distributed event - triggered rule can be utilised to seek the local minimum point of @xmath4 over @xmath346 . denote @xmath347 where @xmath76 is the trajectory of the system .",
    "thus @xmath348 is the local minimum point of @xmath349 as @xmath350 figure [ fig:2 ] shows that the terminal limit @xmath348 converge to a local minimum points @xmath351^{\\top}$ ] as @xmath343 for @xmath344",
    ".     converges to a local minimum point @xmath351^{\\top}$ ] .",
    "we select @xmath352^{\\top}$ ] , @xmath335 , and random initial data in the interval @xmath332 $ ] .",
    "@xmath353 are selected from 0.01 to 100.,scaledwidth=49.0% ]    * example 2 : * consider a 2-dimension neural network ( [ mg ] ) with @xmath354 where @xmath355 we have @xmath356 and take the distributed event - triggered rule ( theorem [ primaryrule ] ) .",
    "the initial value of each neuron is randomly selected in the interval @xmath357 $ ] .",
    "figure [ fig:3 ] shows that the state @xmath76 converges to @xmath358^{\\top}$ ] with taking @xmath335",
    ".     of two neurons converges to @xmath358^{\\top}$ ] with @xmath359^{\\top}$].,scaledwidth=48.0% ]    we also calculate the index @xmath308 , @xmath340 , @xmath337 and @xmath341 with different values of @xmath315 , as shown in table [ table2 ] by averaging over 50 overlaps .",
    "the notions are the same as those in table [ table1 ] .",
    "c|c|c|c|c    ' '' ''    @xmath315 & @xmath308 & @xmath340 & @xmath337 & @xmath341 + 0.1 & 0.1540 & 0.2892 & 20.38 & 21.7786 + 0.2 & 0.1639 & 0.3052 & 19.47 & 21.1697 + 0.3 & 0.1716 & 0.3308 & 18.52 & 20.8751 + 0.4 & 0.1854 & 0.3654 & 17.95 & 20.4182 + 0.5 & 0.1983 & 0.3939 & 18.09 & 20.2560 + 0.6 & 0.2014 & 0.4213 & 17.26 & 19.9064 + 0.7 & 0.2154 & 0.4582 & 16.67 & 19.5929 + 0.8 & 0.2279 & 0.5106 & 16.25 & 19.3727 + 0.9 & 0.2348 & 0.5352 & 16.04 & 18.6549 +    [ table2 ]    it can also be seen from the table [ table2 ] that we have excluded the zeno behavior with the theoretical lower - bound @xmath308 of the inter - event time smaller than the actual calculation value @xmath340 under the distributed event - triggered rule .",
    "in addition , the actual number of events @xmath337 decreases while @xmath338 increases with the increasing @xmath315 .",
    "similar to the first example , if @xmath342 is sufficiently small and let @xmath343 for @xmath344 , it follows @xmath345 . as an application , we use the distributed event - triggered rule to minimize @xmath360 over @xmath346 . denote @xmath347 where @xmath76 is the trajectory of ( [ mg ] ) . then @xmath348 is the local minimum point of @xmath349 when @xmath342 is sufficiently small and @xmath343 .",
    "figure [ fig:4 ] shows that the terminal limit @xmath348 converges to two local minimum points @xmath361^{\\top}$ ] and @xmath362^{\\top}$ ] as @xmath343 for @xmath344 .",
    "converges to two local minimum points @xmath361^{\\top}$ ] and @xmath362^{\\top}$ ] .",
    "we select @xmath352^{\\top}$ ] , @xmath335 , and random initial data in the interval @xmath357 $ ] .",
    "@xmath353 are picked from 0.01 to 100.,scaledwidth=49.0% ]",
    "in this paper , two triggering rules for discrete - time synaptic feedbacks in a class of analytic neural network have been proposed and proved to guarantee neural networks to be completely stable .",
    "in addition , the zeno behaviors can be excluded . by these distributed and asynchronous event - triggering rules ,",
    "the synaptic information exchanging frequency between neurons are significantly reduced .",
    "the main technique of proving complete stability is finite - length of trajectory and the @xmath0ojasiewicz inequality @xcite .",
    "two numerical examples have been provided to demonstrate the effectiveness of the theoretical results .",
    "it has also been shown by these examples the application in combinator optimisation , following the routine in @xcite .",
    "moreever , the proposed approaches can reduce the cost of synaptic interactions between neurons significantly .",
    "one step further , our future work will include the self - triggered formulation and event - triggered stability of other more general systems as well as their application in dynamic optimisation .",
    "m. forti , and a. tesi , `` new conditions for global stability of neural networks with application to linear and quadratic programming problems , '' _ ieee trans .",
    "circuits syst .",
    "i , reg . papers _ , vol .",
    "42 , no . 7 , pp .",
    "354 - 366 , jul . 1995 .",
    "m. a. cohen , and s. grossberg , `` absolute stability of global pattern formation and parallel memory storage by competitive neural networks , '' _ ieee trans .",
    "syst . , man , cybern .",
    "_ , vol . 13 , no . 15 , pp",
    "815 - 821 , sep .",
    "1983 .",
    "cao , and j. wang , `` global asymptotic stability of a general class of recurrent neural networks with time - varying delays , '' _ ieee trans .",
    "circuit syst .",
    "i , fundam .",
    "theory appl .",
    "34 - 44 , jan . 2003 .",
    "m. forti , and a. tesi , `` absolute stability of analytic neural networks : an approach based on finite trajectory length , '' _ ieee trans .",
    "i , reg . papers _ , vol .",
    "12 , pp . 2460 - 2469 , dec . 2004 .",
    "m. forti , p. nistri , and m. quincampoix , `` convergence of neural networks for programming problems via a nonsmooth @xmath0ojasiewicz inequality , '' _ ieee trans .",
    "neural netw .",
    "17 , no . 6 , pp .",
    "1471 - 1486 , nov .",
    "2006 .",
    "s. @xmath0ojasiewicz , `` une propriet@xmath364 topologique des sous - ensembles analy - tiques r@xmath364els , '' _ colloques internationaux du c.n.r.s .",
    "lesquations aux d@xmath364rive@xmath364s partielles _ , vol .",
    "87 - 89 , 1963 .",
    "a. molin , s. hirche , `` suboptimal event - based control of linear systems over lossy channels estimation and control of networked systems , '' _ proc .",
    "2nd ifac workshop on distributed estimation and control in networked systems _ ,",
    "pp . 5560 , 2010 .",
    "z. liu , z. chen , and z. yuan , `` event - triggered average - consensus of multi - agent systems with weighted and direct topology , '' _ journal of systems science and complexity _ , vol .",
    "845 - 855 , 2012 ."
  ],
  "abstract_text": [
    "<S> in this paper , we investigate convergence of a class of analytic neural networks with event - triggered rule . </S>",
    "<S> this model is general and include hopfield neural network as a special case . </S>",
    "<S> the event - trigger rule efficiently reduces the frequency of information transmission between synapses of the neurons . </S>",
    "<S> the synaptic feedback of each neuron keeps a constant value based on the outputs of its neighbours at its latest triggering time but changes until the next triggering time of this neuron that is determined by certain criterion via its neighborhood information . </S>",
    "<S> it is proved that the analytic neural network is completely stable under this event - triggered rule . </S>",
    "<S> the main technique of proof is the @xmath0ojasiewicz inequality to prove the finiteness of trajectory length . </S>",
    "<S> the realization of this event - triggered rule is verified by the exclusion of zeno behaviors . </S>",
    "<S> numerical examples are provided to illustrate the theoretical results and present the optimisation capability of the network dynamics .    analytic neural network , complete stability , distributed event - triggered rule , self - triggered rule </S>"
  ]
}