{
  "article_text": [
    "when it is known that the high - dimensional solutions of a computational problem lie on a lower - dimensional manifold we may wish to operate only on that manifold . by repeatedly solving the problem numerically , we can compute points that are approximately on the manifold and we will assume that we are able to compute a set of such points over the manifold so that they are  reasonably distributed \" ( i.e. , the average density of the points does not vary too greatly from region to region .",
    "we also assume that we can approximate any point on the manifold sufficiently accurately by interpolation from these pre - computed points and we would like to find a convenient parameterization of the manifold that can be used for the interpolation .",
    "for example , if the solutions of a differential equation in the high - dimensional space rapidly approach a low - dimensional manifold it may be desirable to integrate a reduced system on the manifold to avoid stiffness . in this case , it might be even more convenient to integrate in the parameter space .",
    "note that in this example it is important that the parameterization be non singular : otherwise the differential equations in the parameter space would have a singularity .",
    "we suppose that we are given a set of @xmath0 points @xmath1 , @xmath2 in @xmath3-dimensional euclidean space , s , with coordinates @xmath4 , @xmath5 that are on ( or close to , due to noise or computational error ) a low - dimensional manifold , m. our ultimate objective is to find , computationally , a parameterization of any manifold that can be simply parameterized .",
    "for now we rule out manifolds such as tori that can only be parameterized by using periodicity in some of the parameters .",
    "a desiderata is to be able to parameterize any manifold that can be formed by  twisting \" ( without any stretching ) a linear manifold in the high dimensional space ( such as the well - known ",
    "swiss roll \" example in ( b ) ) so that the parameterization is a good approximation to a cartesian coordinate system for the original linear manifold .",
    "( in that figure the x - axis is foreshortened and has a range of about 2.5 times that of the other two axes so as to show the structure .",
    "consequently it appears to be rolled from a fairly narrow rectangle but it is actually rolled from the square shown in ( a ) . the randomness of these points was constrained to get a ",
    "reasonable \" distribution - the 1 by 1 square was divided into 40 by 40 equal smaller squares , a point was placed in the middle of each and then subject to a random uniform perturbation in each coordinate in the range [ -0.4 , 0.4]/40 , thus ensuring that no points were closer than 0.2/40 . )    the learning and statistical communities have long been interested in manifold learning and representation so there is a large body of literature . in those cases",
    "the typical task is to start with the data set @xmath6 and produce a  representation \" in a lower - dimensional space such that the differences in the distances of pairs of points in the two spaces are suitably small .",
    "this was achieved by minimizing a _ stress _ function in sammon s early paper@xcite and numerous modifications have been proposed , such as @xcite . if the low - dimensional manifold is linear , the principal component analysis ( pca ) is the standard way to identify and represent it - the principal components provide an orthogonal basis for the smallest linear space containing the manifold .",
    "many methods for non - linear manifolds start with some modification of pca .",
    "for example , @xcite effectively uses pca locally and then stitches the tangent spaces together .",
    "local linear embedding ( lle ) @xcite looks for a local linear approximation which is related to pca .",
    "another class of methods is based on the structure of the graph generated by focussing attention primarily on nearby neighbors .",
    "diffusion maps @xcite are one type of this method .",
    "we have looked at the use of diffusion maps for the parameterization of slow manifolds @xcite .",
    "diffusion maps use a parameter , @xmath7 , that effectively specifies the distance of what is considered a nearby neighbor .",
    "when a small @xmath7 is used , the parameterization is nearly singular at the boundary .",
    "a much larger @xmath7 sometimes overcomes this difficulty , but can lead to other difficulties .    in this report",
    "we show that that the limit of a particular case of diffusion maps for large @xmath7 is pca if we use the left eigenvectors rather than the right eigenvectors normally used in diffusion maps . however , because , in diffusion maps , we work with distances in the graph structure rather than with the coordinates @xmath8 that are the input to pca , we can consider modifications to the distances of pairs of points that are not nearby neighbors to improve the parameterization .    in",
    "we discuss the problems of diffusion maps for the data set in ( b ) - for small @xmath7 the map has a nearly singular jacobians , but breaks down for large @xmath7 because points distant in the manifold are relatively close in the high - dimensional space .",
    "then , in , we consider the large @xmath7 limit for diffusion maps on linear manifolds and prove that the left eigenvectors provide the pca decomposition and show that this can be obtained by working directly with the distance matrix .",
    "in we consider ways to modify the distance matrix to improve the parameterization .",
    "define the ( symmetric ) distance matrix * h * to have the entries @xmath9 where @xmath10 is the euclidean distance between the points , that is _",
    "the diffusion map method @xcite now forms the matrix @xmath11 where @xmath12 is a suitable non - negative function such that @xmath13 and @xmath14 is monotonic non - increasing for @xmath15 with @xmath16 . we will use w(h ) = ( - ( ) ^2 ) .",
    "the diffusion map method continues by dividing each row of @xmath17 by its corresponding row sum to get a markov matrix @xmath18 whose largest eigenvalue is 1 with a corresponding eigenvector * e * = @xmath19^t$ ] .",
    "some of its next few eigenvectors serve to identify the low - dimensional manifold and the @xmath20-th components of these eigenvectors provide a parameterization for the points , @xmath1 , on the manifold .",
    "it is well known that for small @xmath7 the parameterization is poor near the boundaries - see , for example , @xcite - but that a large @xmath7 can give a good parameterization under some circumstances .    in",
    "we show the results of applying a diffusion map to the data in ( b ) . here , @xmath7 was chosen as @xmath21 where @xmath22 was the smallest value such that",
    "if all edges in the graph corresponding to the distance matrix @xmath23 longer than @xmath22 were removed , the graph would still be connected .",
    "( for this particular data set , @xmath22 is 0.0322 . ) ( a ) and ( b ) plot the components of the second and third eigenvectors , @xmath24 and @xmath25 , against the @xmath26 and @xmath27 coordinates of the points in ( b ) .",
    "( @xmath26 and @xmath27 are the coordinates of the points in ( a ) .",
    "after the square is  rolled \" @xmath27 is the arc length around the spiral from its inner stating point while @xmath26 is the coordinate along the length of the cylindrical spiral . )",
    "we see that @xmath24 and @xmath25 come very close to providing direct parameterizations of @xmath26 and @xmath27 , respectively , and this is shown more clearly in ( c ) and ( d ) ( which are projections of the previous two figures onto axial panes ) .",
    "however , we see that the parameterization is very poor near the boundary where @xmath28 and @xmath29 are nearly zero ( because the curve is close to a cosine half cycle ) . for example , if one were trying to integrate a system on its slow manifold using the parameterization as the dependent variables the sensitivity of the derivative to the parameter values would be very large near the boundary .    in some cases ,",
    "this can be overcome by using a large @xmath7 . in ( a ) and ( b )",
    "we show the equivalent of ( c ) and ( d ) for a diffusion map of the same data with @xmath30 - the distance along the cylinder .",
    "note that @xmath24 provides a very good parameterization of the @xmath26 direction - it is almost linear , but that @xmath25 is no longer a ( 1 - 1 ) map .",
    "this occurs because @xmath31 for points on adjacent turns of the spiral is about 0.985 so the points are seen as close .",
    "( the spiral started with radius 0.03 and increased radially by 0.12 on each full turn . )",
    "we would like to get the benefits of a large @xmath7 - the linearity in ( a ) without the drawbacks illustrated in ( b ) .",
    "for that reason , we study the large @xmath7 limit of diffusion maps in the next section .",
    "we are really interested in non - linear manifolds , but for expository purposes we start by assuming that the manifold is linear . set the ( arbitrary ) origin to be at the point mean .",
    "this implies that _",
    "i=1^n x_q , i = 0 for all @xmath32 .",
    "since the manifold is linear , it is now a linear subspace , d.    define @xmath33 to be the vector whose @xmath20-th component is @xmath34 and @xmath35^t$ ] so can be written as ^t*x*_q = 0 for all @xmath32 .",
    "( bold face lower - case roman letters will always refer to @xmath0-dimensional column vectors while non - bold face roman letters will refer to @xmath3-dimensional column vectors . )    suppose @xmath7 is large enough that @xmath36 can be ignored .",
    "in that case , _ ij 1 - ( ) ^2 the row sums of this matrix are _ j=1^n * w*_ij n - _ j=1^n * h*_ij^2",
    "ij ( 1 - [ * h*_ij^2 -_k=1^n * h*_ik^2 ] ) in other words , ( * e * - ) where @xmath37 is the matrix of all ones and @xmath38   \\nonumber \\\\   & = & \\sum_{q=1}^s[x_{q , j}^2 - 2x_{q , i}x_{q , j } - \\sum_{k=1}^nx_{q , k}^2/n ] . \\label{fdef}\\end{aligned}\\ ] ]    writing @xmath39 for @xmath40 and defining vector @xmath41 to have @xmath20-th entries @xmath42 , can be written as = _",
    "q=1^s[- 2*x*_q*x*_q^t ] .",
    "the points @xmath1 lie in the linear subspace d. chose a coordinate system for s such that the first @xmath43 coordinates span d. hence , for @xmath44 @xmath45 and @xmath46 so we can rewrite as = _",
    "q=1^d[- 2*x*_q*x*_q^t ] .",
    "note that if a left or right eigenvector , @xmath47 of @xmath18 given by satisfies @xmath48 then it is also an eigenvector of @xmath49and _ vice versa_.",
    "* observation *    if the points @xmath50 are regularly placed in the linear subspace - that is is they lie on the points of a regular rectangular grid or its extension to higher dimensions - and if the coordinate system is aligned with this grid , then @xmath51 are eigenvectors of @xmath49 and hence @xmath18 .",
    "this follows by construction . in this case",
    "the vector @xmath33 takes the form _",
    "q = e_1e_2e_q-1z_q e_q+1 e_d where @xmath52 is a column vector of ones of length equal to the number of grid points in the @xmath20-th dimension and @xmath53 is the set of coordinate points in the @xmath32-th dimension .",
    "note that @xmath54 for all @xmath32 so that the @xmath33 are mutually orthogonal and all orthogonal to @xmath55 . hence @xmath56{\\bf",
    "x}_q   \\nonumber \\\\   & = & \\sum_{p=1}^d\\frac{{\\bf e}{\\bf v}_p^t}{n}{\\bf x}_q -2({\\bf x}_q^t{\\bf x}_q){\\bf x}_q . \\label{fxq}\\end{aligned}\\ ] ]    if @xmath57 then @xmath58 directly from so it only remains to show that for this data , _",
    "q^t * x*_q = _",
    "i=1^n x_q , i^3 = 0 which is true by virtue of the regular spacing and @xmath59 .",
    "if the points are  reasonably \" distributed , then this property is approximately true . in ( a )",
    "we show the points plotted via their entries , @xmath60 and @xmath61 of the eigenvectors of @xmath18 evaluated with @xmath30 for the planar data in ( a ) . because the eigenvalue computation finds the dominant axis as second leading eigenvector ( after the eigenvector @xmath55 ) and that tends towards one of the diagonals when the data is random , the square has been rotated . to demonstrate that these are close to linearly dependent on the original coordinates , we computed the affine translation of this representation that was closest to the original data in ( a ) ( that is , the transformation that minimized the sum of the squares of the distances between associated points ) and then plotted the components of the eigenvectors @xmath24 and @xmath25 against the original coordinates , as shown in ( b ) .",
    "( @xmath25 is offset by 0.1 for clarity . ) as can be seen , the dependence is almost linear .",
    "the average distance error was 0.0023 .",
    "however , it is more important to note that the left eigenvectors of @xmath49 always have this property , regardless of the distribution of points , so we propose to use the left eigensystem of @xmath49 but to avoid having to repeat  left eigenvector \" we will instead work with @xmath62 whose eigenvectors * u * are the left eigenvectors of @xmath49 .",
    "we are dealing with vectors in two different spaces : the @xmath3-dimensional original euclidean space containing the points whose coordinates are @xmath63^t$ ] and the @xmath0-dimensional space , p , containing the vectors @xmath64^t$ ] .",
    "if we pick any basis for the euclidean space ( which has to be orthogonal so that is based on the distance ) it determines the entries in the @xmath0-dimensional vectors @xmath33 . for each orthogonal",
    "set of vectors in the euclidean space ( which we will call _ geometric _ vectors ) there is a corresponding set of vectors of the point coordinates ( which we will call _ point _",
    "vectors ) .",
    "in particular , if we pick an ( orthogonal ) basis set for s such that the first @xmath43 basis vectors span d , then the @xmath34 satisfy @xmath65 . hence , the remaining @xmath66 basis vectors do not affect the @xmath43 vectors @xmath67 . in this sense , to every orthogonal basis for d there is a corresponding set of @xmath43 linearly - independent point vectors in p.    to avoid verbal complexity , we will refer to  non - zero eigenvectors \" to mean  eigenvectors corresponding to non - zero eigenvalues \" and its obvious extensions .",
    "we find that @xmath68 has the interesting properties summarized in + * theorem *    _ if the components of @xmath69 are given by _",
    "q=1^d[- 2*x*_q*x*_q^t ]    1 .",
    "@xmath69 has exactly @xmath43 non - zero eigenvalues .",
    "the zero eigenvalue has algebraic multiplicity @xmath70 and geometric multiplicity at least @xmath71 although there are special cases where the geometric multiplicity is also @xmath70 .",
    "3 .   there exists a geometrically orthogonal basis for d such that the @xmath43 corresponding non - null point vectors @xmath33 are the non - zero eigenvectors of @xmath69 .",
    "4 .   these eigenvectors are orthogonal .",
    "the corresponding eigenvalues are @xmath72 so that all non - zero eigenvalues are negative .",
    "* proof *    we restrict ourselves to coordinate systems in s such that the first @xmath43 cordinates span m. from we see that if * b * is any vector orthogonal to * e * and @xmath73 then @xmath74",
    "so * b * is a zero eigenvector .",
    "hence any @xmath75 ( linearly independent ) vectors that span the space orthogonal to * e * and @xmath73 are independent zero eigenvectors and are orthogonal to * e*. note that @xmath76 is a zero left eigenvector of @xmath18 . since it is orthogonal to the @xmath75 zero right eigenvectors just enumerated , there must be one more zero eigenvalue .",
    "( usually its right eigenvector is a generalized eigenvector but there are special cases when this is not so , for example , when @xmath77 . )",
    "we complete the proof by showing that there are @xmath43 independent non - zero eigenvectors .",
    "let @xmath78 be the matrix whose columns are @xmath51 , and let @xmath79 be a linear combination of those vectors where @xmath80 is a @xmath43-dimensional vector . hence from and @xmath81",
    "we have = -2*x * * x*^t*x * * * define y = * x**x*^t .",
    "it is a @xmath43 by @xmath43 symmetric matrix so has real eigenvalues and mutually orthogonal eigenvectors . because the points are in the @xmath43-dimensional subspace d and no lower dimensional subspace",
    ", @xmath82 is positive definite so all of its eigenvalues are strictly positive .",
    "let @xmath80 be one of the eigenvectors with corresponding eigenvalue @xmath83 .",
    "hence @xmath84 so @xmath85 is an eigenvector of * g * with eigenvalue @xmath86 .",
    "this provides a complete set of @xmath43 non - zero eigenvectors .",
    "if @xmath87 is an orthonormal matrix whose columns are the ( scaled ) eigenvectors , @xmath88 , of @xmath82 then @xmath87 can be applied to the initially selected cartesian coordinate system for d to get a new cartesian coordinate system with property 3 .",
    "property 4 follows because the action of @xmath69 defined by on the subspace spanned by @xmath89 is exactly the same as the action of = -2_q=1^d*x*_q*x*_q^t @xmath90 is a symmetric matrix so its eigenvectors are orthogonal .",
    "property 5 follows from when we choose a basis for d such that the eigenvectors are @xmath89 ( we have demonstrated how to do above ) .",
    "then @xmath82 is diagonal with entries @xmath91 which are its @xmath43 eigenvalues @xmath92 .    as a footnote to this proof",
    "we observe that the number of non - zero eigenvalues follows quickly from a paper of nearly 75 years ago by young and householder@xcite .",
    "they show that the matrix has rank @xmath93 ( see eq .",
    "( 2 ) in their paper ) .",
    "if we subtract the average of the first @xmath0 rows from the last and then add the new last row to each of the first @xmath0 rows ( not changing the matrix rank ) we get the matrix where * z * is some @xmath0-dimensional vector whose value does not affect the rank .",
    "hence , @xmath94 has rank @xmath95 . since @xmath76 is a left eigenvector of @xmath69 and @xmath37 with eigenvalues 0 and 1 respectively and @xmath37 has rank 1 , the rank of @xmath69 is @xmath43 .",
    "we see from that the non - zero eigenvectors of @xmath69 are the eigenvectors of @xmath96 and these are precisely the principal components from pca so the method proposed in provides exactly the pca representation of the data set , which raises the question  what is the advantage of this method over pca ? \"",
    "pca starts with the coordinates of the data whereas this method starts with the distances of each point pair ( and incidentally provides the coordinates in an orthogonal coordinate system from the calculation ) .",
    "a potential application of this method is to a set of objects that are not given in a euclidean space but for which a similarity distance can be defined ( e.g. , a set of graphs ) . in this report , we focus only on sets that can be represented as points in a euclidean space .",
    "the problems with the diffusion map parameterization ( using large @xmath7 as discussed in ) arose when distances that were large in the manifold were small in the original space .",
    "because the algorithm only uses distances we can consider modifying the distances of more distant objects to be approximately compatible with those in the manifold .      1 .",
    "remove all edges from the graph representation of the distance matrix @xmath23 longer than some value .",
    "2 .   compute the new shortest path to all edges to get a modified distance matrix 3 .",
    "use this modified matrix to generate a parameterization using the technique of    in ( a ) we show the points plotted in the @xmath97-plane using the modified distance matrix approach for the swiss roll data in ( b ) .",
    "edges were removed from the graph that were longer than @xmath98 where @xmath22 was the minimum value that did not lead to a disconnected graph when all larger edges were cut ( @xmath22 is 0.0322 ) . as happened in ,",
    "the square is twisted .",
    "as before , we found the best affine transformation of these points to match the data in ( a ) .",
    "the new coordinate systems is called @xmath99 and @xmath100 . in ( b ) and ( c ) we show the plots of @xmath26 versus @xmath99 and @xmath27 versus @xmath100 . as can be seen , the relationship is almost linear .",
    "the distance modification scheme gives good results for the swiss roll data because it was obtained by twisting a linear manifold without stretching .",
    "however , if it is necessary to stretch the data to  unfold \" it into a linear manifold , the scheme will not work unless some of the nearest neighbor distances are also modified .",
    "this can be seen with the example in ( a ) which consists of a set of points somewhat uniformly placed on the majority of a surface of a sphere . in this example , the surface of the sphere is included from the  north pole \" to to southern latitude @xmath101 .",
    "a point was placed at the north pole and then points were place on each of 15 circles of constant latitude at constant separations of @xmath102 .",
    "the points were equi - spaced on each circle .",
    "40 points were placed on the equator and the number on each of the other circles was chosen to make the spacing on each circle as similar as possible except that a minimum of 6 points were placed on each circle .",
    "the location of one of the points was chosen from a uniform random distribution . in ( a )",
    "all points are plotted as dots except for the points on the southern - most latitude which are circles .",
    "clearly this 2d surface can be parameterized easily .",
    "the southern - most latitude circle is the boundary of the finite , non - linear manifold .",
    "the results of the standard diffusion map and the modified distance scheme are shown in ( b ) and ( c ) , respectively .",
    "the plots shown the circles of constant latitude in the eigenvalue coordinates . the southern - most circle",
    "is shown as a thicker line .    as can be seen from the figure",
    ", the modified distance scheme has the boundary of the manifold in the interior of parameterization it produces ( as does the diffusion map process , although not as badly ) .",
    "this arise because the scheme places a great emphasis preserving the length of nearby neighbors , and in this case there is a circle of nearby neighbors with a perimeter that is much smaller than the perimeter of other circles that are in the interior of the 2d manifold .",
    "clearly it is necessary to modify the lengths of some nearby neighbors to get a good parameterization .",
    "a clue to the need for this can be obtained from the size of additional eigenvalues of @xmath69 and local changes in the components of their eigenvectors . since",
    "the method is equivalent to a pca decomposition , other eigenvalues must be large when we find that points significantly separated in the original space are close in the parameter space .",
    "how to systematically modify nearby neighbors distances in this case is a subject of ongoing research ."
  ],
  "abstract_text": [
    "<S> in this report we consider the parameterization of low - dimensional manifolds that are specified ( approximately ) by a set of points very close to the manifold in the original high - dimensional space . </S>",
    "<S> our objective is to obtain a parameterization that is ( 1 - 1 ) and non singular ( in the sense that the jacobian of the map between the manifold and the parameter space is bounded and non singular ) .    </S>",
    "<S> * keywords * diffusion maps , pca , distance matrices </S>"
  ]
}