{
  "article_text": [
    "it is well - known that exact inference in _ tree - structured _ graphical models can be accomplished efficiently by message - passing operations following a simple protocol making use of the distributive law @xcite .",
    "it is also well - known that exact inference in _ arbitrary _ graphical models can be solved by the junction - tree algorithm ; its efficiency is determined by the size of the maximal cliques after triangulation , a quantity related to the treewidth of the graph .",
    "figure [ fig : examples_intro ] illustrates an attempt to apply the junction - tree algorithm to some graphical models containing cycles . if the graphs are not chordal ( ( a ) and ( b ) ) , they need to be triangulated , or made chordal ( red edges in ( c ) and ( d ) ) .",
    "their clique - graphs are then guaranteed to be _ junction - trees _ , and the distributive law can be applied with the same protocol used for trees ; see @xcite for a beautiful tutorial on exact inference in arbitrary graphs .",
    "although the models in this example contain only pairwise factors , triangulation has increased the size of their maximal cliques , making exact inference substantially more expensive .",
    "hence approximate solutions in the original graph ( such as loopy belief - propagation , or inference in a loopy factor - graph ) are often preferred over an exact solution via the junction - tree algorithm . even when the model s factors are the same size as its maximal cliques , neither exact nor approximate inference algorithms take advantage of the fact that many factors consist only of _ latent _ variables . in many models ,",
    "those factors that are conditioned upon the observation contain fewer latent variables than the purely latent cliques .",
    "examples are shown in figure [ fig : examps ] .",
    "this encompasses a wide variety of models , including grid - structured models for optical flow and stereo disparity as well as chain and tree - structured models for text or speech .",
    "simple analysis reveals that the probability of choosing a permutation that does not contain a value inside a square of size @xmath1 is @xmath2 this is precisely @xmath3 , where @xmath4 is the cumulative density function of @xmath5 .",
    "it is immediately clear that @xmath6 , which defines the best and worst - case performance of algorithm [ alg1 ] .",
    "the case where we are sampling from multiple permutations simultaneously ( i.e. , algorithm [ alg : ext ] ) is analogous .",
    "we consider @xmath9 permutations embedded in a @xmath10-dimensional hypercube , and we wish to find the width of the smallest shaded hypercube that includes exactly one element of the permutations ( i.e. , @xmath11 , \\ldots , p_{k-1}[i]$ ] ) .",
    "this is represented in figure [ fig : perms](c ) for @xmath12 .",
    "note carefully that @xmath10 is the number of _ lists _ in ( eq .",
    "[ eq : hatk ] ) ; if we have @xmath10 lists , we require @xmath9 permutations to define a correspondence between them .    unfortunately , the probability that there is no non - zero entry in a cube of size @xmath13 is not trivial to compute .",
    "it is possible to write down an expression that generalizes ( eq .  [ eq : factprob ] ) , such as @xmath14 ( in which we simply enumerate over all possible permutations and ` count ' which of them do not fall within a hypercube of size @xmath15 ) , and therefore state that @xmath16 however , it is very hard to draw any conclusions from ( eq .",
    "[ eq : pkm ] ) , and in fact it is intractable even to evaluate it for large values of @xmath17 and @xmath10 .",
    "hence we shall instead focus our attention on finding an upper - bound on ( eq .  [ eq : runtimeexact ] ) .",
    "finding more computationally convenient expressions for ( eq .  [ eq : pkm ] ) and ( eq .  [ eq : runtimeexact ] ) remains as future work .",
    "although ( eq .  [ eq : runtimek1 ] ) and ( eq .  [ eq : runtimeexact ] ) precisely define the running times of algorithm [ alg1 ] and algorithm [ alg : ext ] , it is not easy to ascertain the speed improvements they achieve , as the values to which the summations converge for large @xmath17 are not obvious . here , we shall try to obtain an upper - bound on their performance , which we assessed experimentally in section [ sec : experiments ] .",
    "in doing so we shall prove theorems [ the : alg1 ] and [ the : algext ] .",
    "( see algorithm [ alg1 ] ) consider the shaded region in figure [ fig : perms](d ) .",
    "this region has a width of @xmath19 , and its height @xmath1 is chosen such that it contains precisely one non - zero entry .",
    "let @xmath20 be a random variable representing the height of the grey region needed in order to include a non - zero entry .",
    "we note that @xmath21 our aim is to find the smallest @xmath19 such that @xmath22 .",
    "the probability that none of the first @xmath1 samples appear in the shaded region is @xmath23 next we observe that if the entries in our @xmath24 grid do not define a permutation , but we instead choose a _ random _ entry in each row , then the probability ( now for @xmath25 ) becomes @xmath26 ( for simplicity we allow @xmath1 to take arbitrarily large values ) .",
    "we certainly have that @xmath27 , meaning that @xmath28 is an upper - bound on @xmath29 , and therefore on @xmath30 .",
    "thus we compute the expected value @xmath31 this is just a geometric progression , which sums to @xmath32 .",
    "thus we need to find @xmath19 such that @xmath33 clearly @xmath34 will do .",
    "thus we conclude that @xmath35    ( see algorithm [ alg : ext ] ) we would like to apply the same reasoning in the case of multiple permutations in order to compute a bound on @xmath18 .",
    "that is , we would like to consider @xmath9 _ random _ samples of the digits from @xmath36 to @xmath17 , rather than @xmath9 permutations , as random samples are easier to work with in practice .    to do so , we begin with some simple corollaries regarding our previous results .",
    "we have shown that in a permutation of length @xmath17 , we expect to see a value less than or equal to @xmath37 after @xmath38 steps .",
    "there are now @xmath39 other values that are less than or equal to @xmath37 amongst the remaining @xmath40 values ; we note that @xmath41 hence we expect to see the _ next _ value less than or equal to @xmath37 in the next @xmath38 steps also .",
    "a consequence of this fact is that we not only expect to see the _ first _ value less than or equal to @xmath37 earlier in a permutation than in a random sample , but that when we sample @xmath1 elements , we expect _ more _ of them to be less than or equal to @xmath37 in a permutation than in a random sample .",
    "furthermore , when considering the _ maximum _ of @xmath9 permutations , we expect the first @xmath1 elements to contain more values less than or equal to @xmath37 than the maximum of @xmath9 random samples .",
    "( eq .  [ eq : pkm ] ) is concerned with precisely this problem .",
    "therefore , when working in a @xmath10-dimensional hypercube , we can consider @xmath9 random samples rather than @xmath9 permutations in order to obtain an upper - bound on ( eq .  [ eq : runtimeexact ] ) .",
    "thus we define @xmath25 as in ( eq .  [ eq : replace ] ) , and conclude that @xmath42 thus the expected value of @xmath25 is again a geometric progression , which this time sums to @xmath43 .",
    "thus we need to find @xmath44 such that @xmath45 clearly @xmath46 will do .",
    "as mentioned , each step takes @xmath47 , so the final running time is @xmath48 .    to summarize , for problems decomposable into @xmath49 groups , we will need to find the index that chooses the maximal product amongst @xmath10 lists ; we have shown an upper - bound on the expected number of steps this takes , namely @xmath50"
  ],
  "abstract_text": [
    "<S> _ maximum a posteriori _ inference in graphical models is often solved via message - passing algorithms , such as the junction - tree algorithm , or loopy belief - propagation . the exact solution to this problem </S>",
    "<S> is well known to be exponential in the size of the model s maximal cliques after it is triangulated , while approximate inference is typically exponential in the size of the model s factors . in this paper , we take advantage of the fact that many models have maximal cliques that are larger than their constituent factors , and also of the fact that many factors consist entirely of latent variables ( i.e. , they do not depend on an observation ) . </S>",
    "<S> this is a common case in a wide variety of applications , including grids , trees , and ring - structured models . in such cases , </S>",
    "<S> we are able to decrease the exponent of complexity for message - passing by @xmath0 for both exact _ and _ approximate inference . </S>"
  ]
}