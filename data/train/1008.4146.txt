{
  "article_text": [
    "with rare exceptions , binary star and exoplanet science hinges not on the specific value of any individual eccentricity ( or mass or period ) , but rather on the distribution , or the distribution as a function of stellar properties or other parameters .",
    "the goal of any statistical study should be to determine the _ distribution _ of the quantity of interest  we will concentrate on orbital eccentricity for specificity  that _ would _ have been observed if the investigator had extremely high signal - to - noise data and some ( magical ) method for precise determination of all nuisance parameters , such as instrument calibration and system inclination and so forth .",
    "that is , the investigator wants the observational - uncertainty - deconvolved distribution of the quantity of interest .",
    "one exciting development in the study of binaries and exoplanets is that many groups are building probabilistic modeling software ( @xcite ; @xcite ; @xcite ) . rather than fitting and returning a single set of parameters , these probabilistic packages ( approximately ) _ sample _ from the posterior probability distribution under weak - prior assumptions .",
    "these posterior samplings are much more useful than best - fit parameter values , because they permit subsequent investigators to perform probabilistic inference on the output without going back to the raw radial - velocity data while still properly propagating uncertainties . in this",
    "_ article _ , we give an example of probabilistic meta - analysis that becomes possible when the parameter outputs for individual stars are probabilistic .    for any plausibly exoplanet- or binary - hosting star @xmath0 , there are parameters @xmath1 where @xmath2 is the velocity amplitude , @xmath3 is the period , @xmath4 is some orbital phase or fiducial time , @xmath5 is the eccentricity , and @xmath6 is the longitude of perihelion . fitting to these parameters is non - linear and unbiased estimators are rare .",
    "for these reasons , maximum - likelihood ( or , for gaussian noise , minimum-@xmath7 ) parameters are _ not _ in general unbiased estimators of the true parameters  maximum - likelihood estimators only become unbiased in the limit of an infinite amount of data .",
    "almost all single - point estimates , including maximum-_a - posteriori _ or median - of - sampling parameters , are also biased in general ( @xcite ) .",
    "indeed , along these lines , it has been shown that the eccentricity @xmath5 of any star @xmath0 has this property : the maximum - likelihood estimate @xmath8 of the eccentricity is biased high ; any histogram of estimated eccentricities @xmath8 will have a mean ( and variance ) that is higher than that of the true distribution of true eccentricities @xmath5 ( @xcite ) .",
    "these results  and the incredible diversity of eccentricities observed in exoplanet systems  motivate a concentration on the eccentricity distribution in what follows . without the analysis methods we propose here ,",
    "it is possible that conclusions about the high eccentricities of exoplanet systems might be over - stated or distorted .",
    "there are three fundamental approaches to determination of the true eccentricity distribution  or true distribution of any quantity  given noisy measurements @xmath8 , where @xmath0 is an index over instances ( in this case , stars with binary or exoplanet companions ) .",
    "the first ( and worst ) is just to _ adopt _ the estimated values as good estimates of the true values and create a histogram ( or other density estimate ) of the observed values @xmath8 . because these estimators are biased , and because they are noisy , the distribution created in this way will have the wrong mean and variance ; it will be a strongly biased estimate of the true distribution of true eccentricities @xmath5 .",
    "furthermore , adding new @xmath8 estimates from new stars @xmath0 will not decrease these biases ; there is no @xmath9 improvement as new data are added .",
    "there are suggestions of less - biased estimators for eccentricity ( @xcite ) , but anything unbiased in eccentricity @xmath10 will still be biased for any nonlinear function of @xmath10 , and point estimates still have the property that the distribution of point estimates will in general be different in _ variance _ from the true distribution , if only because of observational noise .",
    "the second approach is to _ deconvolve _ the distribution of maximum - likelihood or best - fit @xmath8 values . in this approach",
    "the investigator recognizes that the distribution of estimates @xmath8 is the true distribution convolved with the uncertainty distribution , where that uncertainty can be described as the probability of estimating @xmath8 when the true value is @xmath5 , or @xmath11 .",
    "the investigator finds the distribution of true values @xmath5 that , when it is convolved with the uncertainties , produces the distribution of estimates @xmath8 .",
    "done correctly , this will be performed in a forward - modeling of the observed distribution , starting at the true distribution .",
    "this method is much more responsible , but when the investigator works at the distribution level ( that is , not at the individual - star level ) , the investigator must assume things about the distribution of uncertainties , equivalent to assuming that all the stars have the same relationship @xmath11 between the estimated and true values .",
    "it is also a disadvantage that when performed navely , deconvolutions can be very sensitive to histogram binning ( or , equivalently , choices about the density estimation ) and are unstable to `` ringing '' and other issues coming from shot noise in the observed distribution .",
    "the third approach  and the approach taken here  is forward modeling of the heteroscedastic observed data ( or eccentricity estimates , if these estimates are not single point estimates but rather posterior samplings ) .",
    "that is , the investigator makes a non - parametric ( or highly parameterized ) model of the frequency distribution function @xmath12 for the true @xmath5 values , and finds the best - fit values of the distribution parameters @xmath13the values of the parameters @xmath13 that , after convolution with the ( suitably transformed ) distributions in the nuisance parameters , explains best the full set of eccentricity samplings .",
    "this is also essentially deconvolution , but it has the enormous advantage over nave deconvolution that it accounts for the fact that different stars have different levels of ( and functional forms for ) uncertainty in the nuisance parameters .",
    "this forward - modeling approach is slowly being adopted in astrophysics ( an early proponent is @xcite ) ; we used a simple version of it to estimate the galaxy luminosity function ( @xcite ) and the velocity distribution in the galaxy disk ( @xcite ) , and we built a general tool for situations in which distributions are smooth and observational uncertainties are simple ( @xcite ) .",
    "here we perform this forward modeling for a situation in which the observational uncertainties are not simple ( uncertainties are asymmetric ; measurements are biased ) in an area ( exoplanet eccentricities ) of great current scientific interest .",
    "everything that follows is straightforwardly generalized to other parameters and other kinds of systems .",
    "for example , parallax - based stellar distances , photometric redshifts , and faint - source fluxes also suffer from systematic biases ( @xcite ; @xcite ; @xcite ) .",
    "scientific results based on these measurements rely on the true distributions , not the distributions of ( biased ) measurements , and in all of these cases , the objects of greatest interest have measurements at relatively low precision or low signal - to - noise .",
    "reliable scientific results can be obtained nonetheless , though only by modeling the data ; that is the fundamental motivation for this work .",
    "there are @xmath14 stars @xmath0 ( @xmath15 ) , each of which has some number @xmath16 of radial velocity measurements @xmath17 . for each star @xmath0 ,",
    "the set of measurements ( data ) @xmath18 is modeled as being affected by a single companion .",
    "we are not explicitly considering multiple companion stars or planets at this stage , although the generalization is straightforward .",
    "the model is @xmath19 where @xmath20 is an overall system velocity , the function @xmath21 is the radial velocity equation , and the @xmath22 are noise contributions drawn from a gaussian of zero mean and variance @xmath23 $ ] , where @xmath24 is the uncertainty variance for the @xmath25th observation of star @xmath0 and @xmath26 is a noise variance from intrinsic stellar variability and other unmodeled sources of noise .",
    "the radial velocity equation @xmath27 for star @xmath0 is parameterized by velocity amplitude @xmath2 , period @xmath3 , orbital phase @xmath4 , eccentricity @xmath5 , and longitude of perihelion @xmath6 .",
    "( five parameters ) .",
    "this model of one star has 7 parameters ( @xmath20 , @xmath28 , and five orbit parameters per star ) , which we can think of as living in a list @xmath29 , and the model of al @xmath14 stars has @xmath30 $ ] continuous parameters in a bigger list @xmath31 .",
    "the likelihood @xmath32 for the seven parameters @xmath29 for star @xmath0 is just the probability of the data @xmath33 for star @xmath0 given the parameters @xmath29 for star @xmath0 @xmath34 ^ 2 }                           { \\sigma_{nj}^2 + s_n^2 } \\quad , \\end{aligned}\\ ] ] where @xmath35 is some constant .",
    "this looks like @xmath7 but is modified for the jitter parameter @xmath26 .    for each system",
    "@xmath0 we imagine that we have been provided ( by the exoplanet observing or fitting team , say ) not the original data , but just a @xmath36-element _ sample _ from a posterior probability distribution function ( posterior pdf ) created from the likelihood and an uninformative prior pdf @xmath37 : @xmath38 where @xmath39 is a normalization constant ( for our purposes ) .",
    "the prior pdf @xmath37 will be decided not by us but by the exoplanet - fitter ; we expect ( need ) it to be uninformative , for example , flat in all parameters , or in their logarithms . for each star @xmath0",
    "this sampling takes the form of a chain of @xmath36 samples @xmath40 , each of which is a set of 7 parameters @xmath41 , such that the distribution of the samples is consistent with a random draw from the posterior pdf .",
    "the total likelihood @xmath42 for all the parameters of all the stars @xmath0 is just the product of the individual - star likelihoods @xmath43 this product formulation of the total likelihood makes the implicit assumption that the different star observations are independent ; that is , we are assuming that there are no likelihood covariances among the parameters of _ different _ systems @xmath0",
    ". that assumption will be at least weakly violated in any real survey of binaries or exoplanets , because different observations will share hardware issues and calibration information .",
    "we want , however , not the likelihood for all the star and orbital parameters but instead the likelihood @xmath44 for the parameters @xmath13 of the eccentricity distribution @xmath12 .",
    "this requires a slight re - thinking , because once we know the true eccentricity distribution , _ that _ is a better prior pdf to be using than the uninformative prior pdf @xmath37 .",
    "it is this process  opening up the prior pdf to modeling and putting what we want to infer in the place of the prior pdf from previous inferences  that makes the method _",
    "hierarchical_. to make this inference we must change variables and integrate out the individual - star parameters which are  for our purposes  nuisance parameters . for the likelihood function @xmath44 ,",
    "this change of variables and integration is @xmath45 where the integrals are over the @xmath14 7-dimensional parameter spaces and we have multiplied the uninfomative prior pdf @xmath37 by a ratio of the eccentricity distribution we want to infer to its uninformative counterpart .",
    "this is a _",
    "marginalized likelihood _",
    "( or `` marginal likelihood '' ) because we have inserted a prior pdf for the nuisance parameters and integrated them out , but left the result in the dimensions of likelihood ( probability of data given parameters ) .",
    "in multiplying the prior pdf by @xmath46 we have implicitly assumed that the true distribution of parameters is _ separable _ ; that is , that both the uninformative prior pdf and the informative prior pdf parameterized by @xmath13 can be written as a prior pdf on the eccentricity @xmath10 multiplied by a prior pdf on the other parameters .",
    "this is not true in general , and is a limitation of this formulation .",
    "the limitation is not fundamental ; @xmath12 can be replaced with a multivariate distribution function in the general case .    the @xmath30$]-dimensional integral ( or product of @xmath14 7-dimensional integrals ) looks intimidating , but that integration is exactly the capability that the sampling from each individual system @xmath0 provides for us : given a @xmath36-element sampling with elements @xmath41 , @xmath47 where @xmath48 represents the posterior pdf under the uninformative prior pdf upon which the sampling is based .",
    "the point is that all probability integrals can be approximated as sums over samples .",
    "so the sampling approximation to the marginalized likelihood for the @xmath13 is just @xmath49 where all that is inside the sum is the _ ratio _ between the uninformative prior pdf ( on which the sampling is based ) and the new prior pdf that we want to infer , and the @xmath50 are the @xmath36 samples of each @xmath5 .",
    "this sampling approximation to the likelihood @xmath44 for the parameters @xmath13 can be optimized to obtain a maximum - likelihood true eccentricity distribution , or it can be multiplied by a prior pdf , normalized , and sampled to obtain a posterior pdf sampling for the parameters @xmath13 .",
    "either way , it is the _ likelihood _ that non - trivially enters into our inference .    the expression for the likelihood @xmath44 in equation  ( [ eq : like ] ) is an importance - sampling approximation to the ratio of bayes factors ( integrals of the posterior probability distribution over the nuisance parameters ) .",
    "the comparison of marginalized likelihoods of two models ( between the default prior pdf and the distribution parameterized by @xmath13 or between two different values of the parameters @xmath13 ) is equivalent to a marginalized bayesian comparison between two models .",
    "it is an importance sampling because it uses a sampling but re - weights the samples by the ratio of probabilities between the two models .",
    "the usual caveats concerning importance sampling apply here as well : since the samples returned by the mcmc are generally not independent , the importance - sampling approximation does not improve as @xmath9 and if the default prior pdf and the distribution parameterized by @xmath13 are very different , the importance - sampling approximation will be noisy .",
    "therefore , we prefer the default prior pdf to be uninformative",
    ". we also need it to be uninformative to ensure that the posterior pdf generated with @xmath51 has support  and samples  wherever the posterior pdf generated with @xmath12 is significant .    from an inference perspective , it is more sensible to simultaneously infer @xmath13 and all the @xmath29 for all the systems , and perform the marginalization on the joint inference .",
    "here we use this importance - sampling approximation because by assumption we do not have the exoplanet data ; we have only the @xmath36-element samplings from the posterior pdfs ( the posterior pdfs created with the uninformative prior pdf ) .",
    "all that remains is to choose functional forms for ( parameterizations of ) the eccentricity distribution function @xmath12 , and a prior pdf on the parameters @xmath13 ( if we want to perform sampling or further marginalization , which we do ) .",
    "there are many possible choices here , and a true bayesian does nt choose but rather does them all and includes them all in the output .",
    "however , for specificity and clarity , we consider only two forms for the eccentricity distribution .",
    "the first is a step function with @xmath52 steps : @xmath53 where we have laboriously defined the step function as a mixture of top - hats and given the normalization constraint on the elements @xmath54 of the parameter vector @xmath13 . for the prior pdf on the @xmath13 we use @xmath55",
    "^ 2 ) \\quad , \\label{eq : prior}\\end{aligned}\\ ] ] where the delta function ensures the normalization of @xmath12 , and @xmath56 is a control parameter that controls our expectation that @xmath12 be smooth .",
    "of course this smoothness parameter @xmath56and the bin number @xmath52should be learned along with @xmath13 , but as this is beyond our scope , we simply set @xmath57 and @xmath58 .",
    "empirically , this keeps the distributions smooth when the data sets get small , but permits good freedom and does nt influence the results much for large data sets , as we show below .",
    "this is a simple smoothness prior ( @xcite ) ; more sophisticated versions can employ a gaussian process ( @xcite ) on the @xmath54the prior in equation  ( [ eq : prior ] ) is a special case of this  and the hyper - parameters ( only @xmath56 in this case ) controlling the smoothness of the distribution can be marginalized out .",
    "this has the advantage that the number of bins can be very large , but it has the disadvantage that sampling highly correlated bin heights is challenging ( @xcite ) .",
    "the second form we consider for the eccentricity distribution is that of the beta distribution @xmath59}\\,(1-e)^{[b-1 ] } \\nonumber\\\\ { { \\boldsymbol{\\alpha}}}&\\equiv & ( a , b ) \\quad , \\end{aligned}\\ ] ] where @xmath60 is the gamma function , and we are redefining the parameter list @xmath13 to contain the beta distribution shape parameters @xmath61 and @xmath62 .",
    "this distribution is defined on the interval @xmath63 and has remarkable freedom with only two parameters .",
    "we take the prior pdf on the parameters @xmath64 to be flat in the allowed region @xmath65 and @xmath66 .",
    "technically this prior pdf is improper , but the posterior pdf under any realistic data set is proper .    in either case  step function or beta distribution ",
    "when we have a set of @xmath67 samples @xmath68 of the distribution - function parameter vector that are effectively samples from the posterior probability distribution , we can use that distribution of distributions or else marginalize out the parameters .",
    "the marginalized distribution @xmath69 is given by @xmath70 where by `` @xmath71 '' we mean the distribution @xmath12 made using parameters @xmath68 of the @xmath25th posterior sample .",
    "we generated @xmath72 ersatz radial velocity data sets @xmath0 , each of which contained @xmath73 radial velocity measurements scattered uniformly over a time baseline of 1000 days .",
    "these measurements were created using the model for the radial velocity of the parent star of a theoretical exoplanet ( or companion star in a binary system ) described by equation  ( [ eq : model ] ) .",
    "we assumed reasonable distributions for the governing parameters of the model , together with some ( trivial ) simplifications :    each ersatz parent or primary star was assumed to have a mass of @xmath74 and vanishing overall system velocity ( @xmath75 ) in the absence of its exoplanet ( or stellar companion ) .",
    "intrinsic radial velocity and primary star mass were , though , left free in fitting .",
    "each ersatz exoplanet or companion star was assigned a mass @xmath76 drawn from a distribution @xmath77 ( flat in @xmath78 ) constrained to lie in @xmath79<m_n<[10\\,m_{{\\mathrm{jup}}}]$ ] .",
    "companions were given orbital periods drawn from a distribution @xmath80 constrained to lie in @xmath81<t<[2000\\,{{\\mathrm{d}}}]$ ] .",
    "phases @xmath4 and @xmath6 were drawn from uniform distributions @xmath82 , except for the inclination @xmath83 , which was drawn from a distribution flat in @xmath84 .",
    "the masses @xmath16 and @xmath76 and inclinations @xmath83 enter into the radial velocity model through the @xmath2 : @xmath85^{1/3}\\,m_n\\,\\sin i_n}{t_n^{1/3}\\,[m_n+m_n]^{2/3}\\,[1-e_n^2]^{1/2 } } \\quad .\\end{aligned}\\ ] ]    the eccentricity @xmath5 for each ersatz companion was drawn from one of two frequency distributions : the first is an intuitive distribution @xmath86 ^ 4}-\\frac{e}{2 ^ 4}\\right]\\end{aligned}\\ ] ] ( @xcite ) , where @xmath87 is a normalization constant ; hereafter we call this distribution `` st4 ''",
    ". the second is an unrealistic straw - man designed to stress - test the inference methodology .",
    "it is a gaussian with mean @xmath88 and variance @xmath89 ( but set to zero outside of the range @xmath63 ) .",
    "the error added to each ersatz measurement was drawn from a gaussian of known observational noise variance , but with every point assigned its own individual ( heteroscedastic ) noise variance , uniformly distributed ( in variance ) in the interval @xmath90 .",
    "for the ersatz observations , the jitter parameters @xmath26 were set to vanish , but , as with the system velocity and mass of the parent ( or primary ) star , the jitter parameters @xmath26 were left free in the fitting .",
    "we optimized , fit , and marginalized the radial velocity model to each of the ersatz data sets using metropolis - hastings markov chain monte carlo ( mcmc ) sampling . for the mcmc we adopted a standard uninformative prior pdf that is flat in @xmath91 , flat in @xmath92 , flat in @xmath4 , flat in @xmath6 , and flat in @xmath5 .",
    "we performed the sampling not in the naive parameter space @xmath93 but in a better - behaved sampling space of @xmath94          , \\kappa_n\\,\\sin[\\phi_n+{\\varpi}_n ]          , e_n\\,\\cos{\\varpi}_n          , e_n\\,\\sin{\\varpi}_n ) \\quad.\\end{aligned}\\ ] ] in this space , sampling is better behaved , but the `` natural prior '' pdf is not flat in @xmath91 or @xmath5 , so a compensating prior pdf ( or jacobian ) must be multiplied in .",
    "we confirmed that our sampler is using the correct uninformative prior pdf by running it on empty data sets ; the resulting no - data samplings are samplings of the prior pdf .    on each system",
    "@xmath0 we set parameter step - sizes ( for a gaussian proposal distribution in the sampling space ) such that acceptance ratios were near @xmath95 . on each system",
    "@xmath96 links of mcmc were run , checked for mixing ( convergence ) , and thinned ( subsampled uniformly ) to produce a set of @xmath97 nearly independent parameter samples @xmath41independence is however not required for the sampling approximation of equation  ( [ eq : like ] ) to work .",
    "radial velocity curves for two ersatz exoplanets are shown in  [ fig : exoplanet ] , together with their mcmc samplings in period and eccentricity .",
    "for each system @xmath0 we search the chain for the maximum-_a - posteriori _ parameters .",
    "we also used a modified ( simulated annealing ) mcmc sampling to find the maximum - likelihood parameters @xmath98 , one member of which is the `` best - fit '' eccentricity @xmath8 .",
    "at this point we have the full @xmath99 sampling @xmath50 .",
    "this makes it possible to compute the marginalized likelihood of equation  ( [ eq : like ] ) for any parameters @xmath13 .",
    "again , we perform metropolis - hastings mcmc , but now in the space of @xmath13 with the prior pdf of equation  ( [ eq : prior ] ) . the proposal distribution used in the mcmc was a small gaussian perturbation applied to every component of @xmath13 , with gaussian variance chosen to keep the acceptance ratio near 0.4 . for each experiment",
    "@xmath100 links of mcmc were run , and checked for mixing . from this posterior sampling ,",
    "we can obtain whatever results are desired , the maximum-_a - posteriori _ distribution , the marginalized ( mean - of - posterior ) distribution , a sampling of distributions consistent with the data , or any quantile of the eccentricity distribution .",
    "in  [ fig:300 ] and  [ fig:30 ] we show the results of four experiments , two with @xmath101 systems and two with @xmath102 , and two with the st4 input eccentricity distribution , and two with the gaussian input eccentricity distribution .",
    "we show the true ( input ) eccentricities , the maximum - likelihood eccentricities , and the inferred distribution . in each case",
    ", we find  as expected  that the marginalized ( mean - of - posterior ) inferred eccentricity distribution is a far better description of the original input eccentricity distribution than the naive distribution created by histogramming the maximum - likelihood eccentricity estimates @xmath8 .",
    "the inferred distribution captures well the smooth distribution that was used to generate the ersatz data .",
    "furthermore , where the actual finite sampling of true values departs ( just by poisson statistics ) from the smooth distribution , the inferred distribution even captures those deviations , especially when there are @xmath101 systems (  [ fig:300 ] ) .",
    "[ fig:300 ] and  [ fig:30 ] also show that the mean of the eccentricity distribution is over - estimated when the best - fit eccentricities @xmath8 are used to represent the eccentricity distribution , and that the over - estimate does not decrease as the number of objects increases .",
    "however , the marginalized inferred mean  the mean obtained by marginalizing over samples  is a very good estimate of the true mean of the true distribution .",
    "the same holds for all of the quantiles of the distribution .",
    "the sampling in @xmath13 provides full uncertainty information , including the uncertainty in every component of @xmath13 and also all of the component  component covariances . in  [ fig:300 ] and",
    "[ fig:30 ] we show only a superimposed sampling .",
    "this conveys information about the uncertainty distribution in each bin , but it does nt display the full power of the sampling for error analysis and propagation .",
    "we repeat these experiments but now using the beta distribution for @xmath12 .",
    "once again we perform metropolis - hastings mcmc , but now in the space of the beta - distribution shape parameters @xmath103 . again the proposal distribution used in the mcmc was a small gaussian perturbation applied to the two components of @xmath13 , with gaussian variance chosen to keep the acceptance ratio near 0.4 . for each experiment",
    "@xmath104 links of mcmc were run , and checked for mixing .",
    "we show the results of the beta - distribution fitting in  [ fig : beta ] .",
    "it also performs extremely well .",
    "we obtained @xmath97 samples per star , but this is almost certainly overkill . in  [ fig : k50 ] we show how the results change when this sampling is thinned down to just @xmath105 samples of the posterior pdf .",
    "the thinning was done uniformly , taking every 2000th sample from the parent set of @xmath106 .",
    "the results are only slightly worse with @xmath105 ; this speeds up the code by a factor of 2000 , of course .",
    "we have shown that proper inference of the eccentricity distribution outperforms the naive approach of histogramming best - fit eccentricity values .",
    "our inference proceeds by inserting the model eccentricity distribution as a prior on the eccentricities , after which a good eccentricity distribution is one that makes the data  the set of all exoplanet observations  probable . because this method models the distribution prior to observation",
    ", it is effectively a deconvolution working on heteroscedastic data ; it is a generalization to arbitrarily non - gaussian uncertainties of previous work in this area ( @xcite ) .",
    "there is nothing crucial about eccentricity ; the method presented here can be applied to any problem in which the true distribution @xmath107 is desired and there are only noisy measurements . as long as the measurements are presented as likelihood functions or samplings under an uninformative prior , the function @xmath107 can be inferred as described here .    though it is common for investigators to present as measured distribution functions the histogram of estimated values , sometimes even worse mistakes are made .",
    "for example , it is sometimes tempting for an investigator to see the output of the sampler as providing a better estimate of the distribution function .",
    "the thinking is `` well , the object has some probability of being in each of these eccentricity bins , so i will add a bit into each bin on its behalf '' .",
    "this thinking is wrong : it convolves the error - convolved distribution with the errors once again",
    ". we will not cite specific examples ( to protect the guilty ) , but this is occasionally done and always incorrect .",
    "one limitation of this study is that we made a simplifying assumption of a separable prior ; that is , we imagined that there was one eccentricity distribution for all of the stars in the sample . in reality",
    ", the eccentricity distribution will depend on parent star and exoplanet parameters , and there will be no clean separation of the eccentricity distribution .",
    "there are two reactions to this .",
    "the first is to live with the result , understanding that the distribution returned by the method is correct for the specific sample in hand , marginalizing over all other parameters .",
    "the second is to permit non - separable distribution functions . in this latter case ,",
    "hierarchical modeling becomes even more necessary .",
    "all of the correlations between orbital parameters or dependences on the parent star s properties can be modeled on the prior level and fit as we did here .",
    "this situation is not substantially more complicated ; it is just that the terms in the sum in the likelihood of equation  ( [ eq : like ] ) becomes a function of multiple parameters , and the parameterization of the function changes .",
    "another limitation of this work is that we only considered single - planet systems .",
    "this was in part because we do not have a simple prior for generating realistic multiple - planet systems , but also because there is nothing fundamental that changes if we switch to multiple - planet systems .",
    "we also permitted additional `` jitter '' in the observations but did not in fact add jitter or any other kind of additional noise or data outliers .",
    "again , addition of these things  and proper modeling of them  changes the individual - object likelihood functions and samplings , but does not change the procedure by which the eccentricity distribution is inferred .    finally , this study has some danger of `` garbage in , garbage out : '' we generated data in accord with our general expectations , and fit it in accord with those same expectations .",
    "this is a limitation of all studies on artificial data , of course .",
    "however , we note that the parameterized eccentricity distributions that we fit are mixtures of step functions and beta distributions ; neither of these are related directly to  or even very appropriate for representing  either of the distributions we used to generate the ersatz data .",
    "furthermore , the `` uninformative '' prior pdf we used in the exoplanet samplings were wrong , not just because it got the eccentricity prior pdf wrong ( the subject of this _ article _ ) but because they also got the velocity amplitude @xmath108 prior pdf wrong : the ersatz exoplanets were generated with isotropic inclinations and a power - law distribution of masses @xmath109 ; this does not generate a power - law frequency distribution in @xmath108 .",
    "that is , this method works well even when the priors in the other parameters are substantially wrong . and of course these prior pdfs can be inferred too , in a generalization .",
    "in  [ fig:300 ] and  [ fig:30 ] , there are many objects whose eccentricities have been over - estimated by the maximum - likelihood method , some of them drastically . this arises naturally because the maximum - likelihood eccentricity tends to be higher than the true eccentricity , but it also arises a bit un - naturally because we have included in these figures some ersatz systems for which the exoplanet is not detected at significant signal - to - noise .",
    "that is , we threw in every system , even though at some periods , masses , and inclinations , the signal - to - noise is near or below unity . in most real experiments ,",
    "these systems are removed ( no - one measures the eccentricity distribution for undetected planets ! ) .",
    "however , this method is not thrown off significantly by the low signal - to - noise systems .",
    "this is a good property of a proper probabilistic data analysis methodology : it is not moved around by the lowest signal - to - noise objects .",
    "many popular methods in astrophysics do _ not _ have this simple property ( necessary property , some would say ) .",
    "for example , in principal components analysis , the highest - noise systems often dominate the total data variance and therefore dominate the analysis .",
    "in general , measurements of variance ( as measurements of distribution functions often are ) can be thrown by noisy data .",
    "forward models tend not to be .    in the real world , an investigator might want to infer the eccentricity distribution , but use input from many different exoplanet observers , each of whom might have sampled their exoplanets with different uninformative priors @xmath51 .",
    "this is not a problem ; in the importance sampling of equation  ( [ eq : like ] ) , the sum for each object @xmath0 should make use of the uninformative prior @xmath51 used on object @xmath0 .",
    "also in the real world , an investigator might want to infer the eccentricity distribution using a mixture of measurements , some of which have been delivered as samplings , and some of which have been delivered just as maximum - likelihood estimates .",
    "unfortunately these latter  maximum - likelihood estimates or any other single - point estimates  are nearly useless for modeling .",
    "the method presented here is a baby step towards a hierarchical bayesian method .",
    "the method would be fully hierarchical if we went back to the objects and re - sampled them using the inferred prior , or , even better , inferred the prior and performed the individual - object samplings simultaneously .",
    "that is , this method becomes fully hierarchical when the inferred distribution function is used to improve the individual - object estimates .",
    "when the measurements are given as likelihood functions , this should be the preferred method .    along those lines",
    ", we could have performed a less simple but faster hierarchical sampling : rather than marginalizing over the individual eccentricities by summing over a ( potentially large ) number of eccentricity samples for each exoplanet , we can use the provided samplings of the individual eccentricities as the basis of an approach that samples both the parameters of the eccentricity distribution and the true eccentricities of the planets .",
    "this approach avoids the sum in equation  ( [ eq : like ] ) .",
    "it also returns updated posterior samples of the individual eccentricies using the better prior  the prior inferred from the population of planets .",
    "briefly , this approach entails writing down the joint posterior probability of the individual eccentricies and the parameters of the eccentricity distribution . using bayes s theorem , we can write this joint distribution as @xmath110 by sampling from this posterior distribution , we simultaneously obtain samples of the eccentricities of the individual planets and of the parameters of the eccentricity distribution .",
    "these eccentricity samples will then have used the better eccentricity prior parameterized by @xmath13 , rather than the uninformative original prior . for planets detected at low signal - to - noise ,",
    "this leads to more realistic estimates of the eccentricity ( see  [ fig : compare ] ) .",
    "we can re - write equation  ( [ eq : hierarchicaljoint ] ) as @xmath111   \\,\\frac{p(\\{e_n\\ } | { { \\boldsymbol{\\alpha}}})}{p_0(\\{e_n\\})}\\,p({{\\boldsymbol{\\alpha } } } ) \\quad.\\end{aligned}\\ ] ] the provided eccentricity chains give us samples of the distribution in square brackets .",
    "we can re - use these samples in a manner similar to that in equation  ( [ eq : like ] ) to sample from the joint distribution of @xmath112 and @xmath13 .",
    "starting from initial @xmath113 , first _ ( 1 ) _",
    "metropolis - sample @xmath114 from @xmath115 by sampling from @xmath116 which can be done by just picking a random sample from the given eccentricity chains for the individual planets  and using the metropolis - hastings acceptance probability on @xmath117 using this acceptance probability ensures that the samples are `` importance - resampled '' according to the new prior parameterized by @xmath13 .",
    "( 2 ) _  sample @xmath118 from @xmath119 using any mcmc sampler .",
    "the resulting sampling of @xmath13 can then be used in exactly the same way as in section  [ sec : experiments ] .    just to demonstrate the power of the hierarchical approach , in  [ fig : compare ] we show a comparison of the maximum - likelihood eccentricity estimates to the mean - of - sampling estimates made with the inferred prior .",
    "that is , we take the marginalized inferred distribution @xmath69 , re - weight the eccentricity samples with this inferred distribution , and produce as a point estimate for each system @xmath0 the mean of the re - weighted sampling .",
    "these mean - of - sampling estimates , made with a correctly informative prior , are  not surprisingly ",
    "better than the maximum - likelihood estimates .",
    "in general , hierarchical inference must become a standard tool in astrophysics going forward : our science is fundamentally statistical , and the objects of greatest interest are measured always at the limits of instrumental sensitivity .",
    "hierarchical methods are the right tools for these jobs .",
    "it is a pleasure to thank brendon brewer ( ucsb ) , fengji hou ( nyu ) , dustin lang ( princeton ) , iain murray ( edinburgh ) , daniel mortlock ( imperial ) , hans - walter rix ( mpia ) , sam roweis ( deceased ) , and christian schwab ( yale ) for valuable discussions , and an anonymous referee for useful feedback .",
    "dwh and jb acknowledge financial support from nasa ( grants nnx08aj48 g ) , the nsf ( grant ast-0908357 ) , and a research fellowship of the humboldt foundation .",
    "adm acknowledges financial support from nasa ( grants nnx08aj28 g and go9 - 0114 ) .",
    "this research made use of the _ sao / nasa astrophysics data system _ , the _ python _ programming language , and open - source software in the _ numpy _ and _ matplotlib _ projects .",
    "all code used in this project is available from dwh upon request .    70 balan ,  s.  t. & lahav ,  o. , 2009 , , 394 , 1936 blanton , m.  r. _ et  al _ , 2003 , , 592 , 819 bovy ,  j. , hogg ,  d.  w. , & roweis ,  s.  t.  2009a , , 700 , 1794 bovy ,  j. , hogg ,  d.  w. , & roweis ,  s.  t.  2009b , arxiv:0905.2979 connolly ,  a.  j. , csabai ,  i. , szalay ,  a.  s. , koo ,  d.  c. , kron ,  r.  g. , & munn ,  j.  a. , 1995 , , 110 , 2655 ford ,  e.  b. , 2005 , , 129 , 1706 gregory ,  p.  c. , 2005 , , 631 , 1198 hogg ,  d.  w. , & turner ,  e.  l. , 1998 , , 110 , 727 kitagawa ,  g. & gersch ,  w. , 1996 , smoothness priors analysis of time series , lecture notes in statistics 116 ( springer ) lehmann ,  e.  l. & casella ,  g. , 1998 , theory of point estimation ( springer ) loredo ,  t.  j. , 2004 , american institute of physics conference series , 735 , 195 lutz ,  t.  e. , & kelker ,  d.  h. , 1973 , , 85 , 573 murray ,  i. , adams ,  r.  p. , & mackay ,  d.  j.  c. , 2010a , in proceedings of the 13th international conference on artificial intelligence and statistics ( aistats ) , vol .  9 , ed .  y.  w.  teh & m.  titterington , 541 murray ,  i. , & adams ,  r.  p. , 2010b",
    ", arxiv:1006.0868 rasmussen ,  c.  e. & williams ,  c. , 2006 , gaussian processes for machine learning ( mit press ) shen ,  y. & turner ,  e.  l. , 2008 apj 685 , 553 zakamska ,  n.  l. , pan ,  m. , & ford ,  e.  b. , 2010 , , in press ( arxiv:1008.4152 )                -point sampling ) .",
    "the vertical line shows the mean of the marginalized inferred distribution ( the marginalized mean ) .",
    "the dotted curves are repeated in all panels to guide the eye.[fig:300],title=\"fig : \" ] -point sampling ) .",
    "the vertical line shows the mean of the marginalized inferred distribution ( the marginalized mean ) .",
    "the dotted curves are repeated in all panels to guide the eye.[fig:300],title=\"fig : \" ] + -point sampling ) .",
    "the vertical line shows the mean of the marginalized inferred distribution ( the marginalized mean ) .",
    "the dotted curves are repeated in all panels to guide the eye.[fig:300],title=\"fig : \" ] -point sampling ) .",
    "the vertical line shows the mean of the marginalized inferred distribution ( the marginalized mean ) .",
    "the dotted curves are repeated in all panels to guide the eye.[fig:300],title=\"fig : \" ] + -point sampling ) .",
    "the vertical line shows the mean of the marginalized inferred distribution ( the marginalized mean ) .",
    "the dotted curves are repeated in all panels to guide the eye.[fig:300],title=\"fig : \" ] -point sampling ) .",
    "the vertical line shows the mean of the marginalized inferred distribution ( the marginalized mean ) .",
    "the dotted curves are repeated in all panels to guide the eye.[fig:300],title=\"fig : \" ]     but for just the first 30 ersatz exoplanets.[fig:30],title=\"fig : \" ]   but for just the first 30 ersatz exoplanets.[fig:30],title=\"fig : \" ] +   but for just the first 30 ersatz exoplanets.[fig:30],title=\"fig : \" ]   but for just the first 30 ersatz exoplanets.[fig:30],title=\"fig : \" ] +   but for just the first 30 ersatz exoplanets.[fig:30],title=\"fig : \" ]   but for just the first 30 ersatz exoplanets.[fig:30],title=\"fig : \" ]     ( 300 stars , st4 distribution ) , but using the beta distribution rather than the step function for @xmath12 . the true st4 distribution is shown with a dotted line , a sampling from the posterior pdf is shown with solid grey lines , and the marginalized inferred distribution ( the mean of a @xmath104-point sampling ) is shown with a solid black line . on the right , the same but similar to the bottom - left panel of  [ fig:30 ] ( 30 stars , st4 distribution).[fig : beta],title=\"fig : \" ]   ( 300 stars , st4 distribution ) , but using the beta distribution rather than the step function for @xmath12 .",
    "the true st4 distribution is shown with a dotted line , a sampling from the posterior pdf is shown with solid grey lines , and the marginalized inferred distribution ( the mean of a @xmath104-point sampling ) is shown with a solid black line . on the right ,",
    "the same but similar to the bottom - left panel of  [ fig:30 ] ( 30 stars , st4 distribution).[fig : beta],title=\"fig : \" ]        .",
    "the horizontal line near 0.2 in the left figure and near 0.3 in the right figure comes from very low signal - to - noise systems ( signal - to - noise at or below unity ) for which the mean - of - sampling estimate ends up being very close to the mean of the prior pdf ; this is the only reasonable mean estimate when the data are not informative.[fig : compare],title=\"fig : \" ] .",
    "the horizontal line near 0.2 in the left figure and near 0.3 in the right figure comes from very low signal - to - noise systems ( signal - to - noise at or below unity ) for which the mean - of - sampling estimate ends up being very close to the mean of the prior pdf ; this is the only reasonable mean estimate when the data are not informative.[fig : compare],title=\"fig : \" ] + .",
    "the horizontal line near 0.2 in the left figure and near 0.3 in the right figure comes from very low signal - to - noise systems ( signal - to - noise at or below unity ) for which the mean - of - sampling estimate ends up being very close to the mean of the prior pdf ; this is the only reasonable mean estimate when the data are not informative.[fig : compare],title=\"fig : \" ] .",
    "the horizontal line near 0.2 in the left figure and near 0.3 in the right figure comes from very low signal - to - noise systems ( signal - to - noise at or below unity ) for which the mean - of - sampling estimate ends up being very close to the mean of the prior pdf ; this is the only reasonable mean estimate when the data are not informative.[fig : compare],title=\"fig : \" ]"
  ],
  "abstract_text": [
    "<S> standard maximum - likelihood estimators for binary - star and exoplanet eccentricities are biased high , in the sense that the estimated eccentricity tends to be larger than the true eccentricity . as with most non - trivial observables , </S>",
    "<S> a simple histogram of estimated eccentricities is _ not _ a good estimate of the true eccentricity distribution . </S>",
    "<S> here we develop and test a hierarchical probabilistic method for performing the relevant meta - analysis , that is , inferring the true eccentricity distribution , taking as input the likelihood functions for the individual - star eccentricities , or samplings of the posterior probability distributions for the eccentricities ( under a given , uninformative prior ) . </S>",
    "<S> the method is a simple implementation of a hierarchical bayesian model ; it can also be seen as a kind of heteroscedastic deconvolution . </S>",
    "<S> it can be applied to any quantity measured with finite precision  </S>",
    "<S> other orbital parameters , or indeed any astronomical measurements of any kind , including magnitudes , distances , or photometric redshifts  </S>",
    "<S> so long as the measurements have been communicated as a likelihood function or a posterior sampling . </S>"
  ]
}