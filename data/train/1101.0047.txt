{
  "article_text": [
    "consider the additive regression model @xmath0 where @xmath1 are the components of @xmath2 , @xmath3 , @xmath4 are unknown smooth functions , and @xmath5 is a sequence of i.i.d random variables with a mean of 0 and a finite variance @xmath6 .",
    "this model was first proposed by friedman and stuetzle ( 1981 ) , and has become a popular multivariate nonparametric regression model in practice .",
    "hastie and tibshirani ( 1990 ) gave a comprehensive review of this model and showed that it could be widely used in multivariate nonparametric modeling .",
    "the additive model provides an approximation , with an additive structure , for multivariate nonparametric regression .",
    "there are at least two benefits of such an additive approximation .",
    "first , as every single individual additive component can be estimated using a univariate smoother in an iterative manner , the so - called `` curse of dimensionality '' that besets multivariate nonparametric regression is largely avoided . stone ( 1985 , 1986 )",
    "theoretically confirmed this by showing that one can construct an estimator of @xmath7 that achieves the same optimal convergence rate for a general value of @xmath8 as for @xmath9 .",
    "second , the estimate of each individual component explains how the dependent variable changes with the corresponding independent variables ; essentially , the simpler structure improves the interpretability of the model .",
    "there are several methods available in the literature for fitting the additive model .",
    "these include the backfitting algorithm ( friedman and stuetzle 1981 ; buja , hastie and tibshirani 1989 ; opsomer and ruppert 1998 ) , the smooth backfitting algorithm ( mammen , linton and nielsen 1999 , mammen and park 2005 , nielsen and sperlich 2005 ; mammen and park 2006 ; yu , park and mammen 2008 ) , marginal integration estimation methods ( tjstheim and auestad 1994 ; linton and nielsen 1995 ; fan _ et al_. 1998 ) , the fourier series or wavelets approximation approach ( amato , antoniadis and de feis 2002 ; amato and antoniadis 2001 ; sardy and tseng 2004 ) , the penalized b - splines method ( eilers and marx 2002 ) , among others .    to make the additive model more efficient , the search for a parsimonious version is clearly of importance .",
    "although estimation has been intensively investigated , insignificant independent variables and function components increase the complexity of the model , which leads to a great computational burden and numerical unstability .",
    "hence , deriving a method for obtaining estimations in a parsimonious additive model that still achieve an optimal convergence rate , as is the case with only one nonparametric component , is an interesting issue .",
    "we use a real data example to demonstrate why selecting significant components and searching for a parsimonious additive model is of importance for statistical additive modeling .",
    "fan and peng ( 2004 ) used an additive model and penalized scad least - squares to analyze the employee dataset of the fifth national bank of springfield based on data from 1995 ( see example 11.3 in albright _ et al . _ 1999 ) .",
    "the bank , whose name has since changed , was charged in court with paying its female employees substantially smaller salaries than its male employees . for each of its 208 employees ,",
    "the dataset includes the following variables .",
    "* edulev : education level , a categorical variable with categories 1 ( finished high school ) , 2 ( finished some college courses ) , 3 ( obtained a bachelor s degree ) , 4 ( took some graduate courses ) , 5 ( obtained a graduate degree ) .",
    "* jobgrade : a categorical variable indicating the current job level , the possible levels being 1 - 6 ( 6 is the highest ) .",
    "* yrhired : the year that an employee was hired . *",
    "yrborn : the year that an employee was born . *",
    "gender : a categorical variable that takes the value `` female '' or `` male '' . *",
    "yrsprior : the number of years of work experience that employee had at another bank before working at the fifth national bank .",
    "* pcjob : a dummy variable that takes the value of 1 if the empolyee s current job is computer - related , and 0 otherwise . * salary : the current ( year 1995 ) annual salary in thousands of dollars .    based on the discussions of lam and fan ( 2008 ) and zhang ( 2008 )",
    ", both yrsexp and age should have a nonlinear relationship with `` salary '' , and an additive model should be an appropriate model to fit the data . the @xmath10 is @xmath11 . in the model ,",
    "the nonparametric components of age and yrsexp are included .",
    "this is informally confirmed by figure [ bk1 ] , which presents the estimated curves of `` yrsexp '' and `` age '' , respectively .",
    "however , the estimated function @xmath12 is not an increasing function .",
    "this is inconsistent with the general intuition that salary should increase with `` yrsexp '' .",
    "it is natural to explore the reasons behind this inconsistency .",
    "as we might suppose that `` age '' and `` yrsexp '' will be strongly correlated , we may naturally ask whether the phenomenon regarding `` yrsexp '' is caused by inappropriately including insignificant variables or components in the model . to demonstrate the necessity of component selection , we manually remove one component to see what happens .",
    "that is , we consider two additive models , each of which includes either `` age '' or `` yrsexp '' .",
    "we find that the model without the `` age '' component has a larger @xmath13 than the model with both `` age '' and``yrsexp '' , and the model without the `` yrsexp '' component has a smaller @xmath10 value of @xmath14 .",
    "this indicates that we should keep `` yrsexp '' in the model .",
    "more importantly , from figure [ bk2 ] , we can see that when the `` age '' component is selected out , the estimated function of `` yrsexp '' is an increasing function of `` yrsexp '' , which fits the intuition .",
    "this also suggests that when insignificant components are selected out , the remaining components have a better explanatory power . as such ,",
    "the means of automatically selecting the `` age '' component out from the model is of importance , because we need to select out a nonparametric component rather than a variable that is observable .",
    "we thus need a new method to handle this modeling issue .",
    "[ bk1 ]    [ bk2 ]      there have been some studies on variable selection in additive modeling .",
    "smith and kohn ( 1996 ) proposed a bayesian approach to select significant variables .",
    "chen and h@xmath15rdle ( 1995 ) used a simple threshold method to select significant independent variables for the additive model , in which the function components are estimated by the marginal integration method .",
    "shively , kohn and wood ( 1999 ) proposed a hierarchical bayesian approach to variable selection and function estimation that uses a data - driven prior , and estimated their functions by model averaging .",
    "lin and zhang ( 2006 ) penalized the norm of the two - order derivative of component functions to obtain sparse additive model in which the functions are estimated by using a smoothing spline technique .",
    "ravikumar _ et al_. ( 2009 ) proposed a new method to produce sparse additive model based on the idea of group variable selection and nonnegative garrote variable selection .",
    "all of these methods involve decoupled smoothing and sparsity , and penalize the norm of the estimated additive component functions to produce a sparse additive model .",
    "most are based on classical variable selection methods for linear models , and hence can not select significant independent variables and estimate the components simultaneously .",
    "the statistical properties of the estimates are also difficult to analyze .",
    "furthermore , these methods impose a large computational burden , especially when there are many nonparametric function components to be estimated .",
    "recently , some attempts have been made to resolve these problems in the additive models ( see , for example , meier , van de geer and bhlmann 2009 and huang , horowitz and wei 2009 ) .",
    "these methods are based on the b - spline and group variable selection techniques and are capable of estimating and selecting component functions simultaneously , even in high - dimension situations .",
    "however , the approach developed by meier , van de geer and bhlmann ( 2009 ) seems to be unstable in the selection process , because it uses every observation as a knot , which results in much fluctuation .",
    "the method proposed by huang , horowitz and wei ( 2009 ) does not provide optimal estimates for the component functions .",
    "this is well known problem with spline regression because the efficiency of the estimates depends on the number and the position of the knots .    in this paper",
    ", we propose a lasso - type spline method ( lsm ) for component selection and estimation .",
    "first , we use a penalized regression spline approximation to parametrize the nonparametric components in the additive model , and then consider the spline approximation as a group of variables for selection .",
    "it is worth mentioning that in our setting , the design matrix in each group is formed from the truncated power spline basis functions .",
    "hence , there is a diverging number of strongly correlated variables in each group , which makes the study more complicated and difficult .",
    "nevertheless , the estimate of every single function component achieves the same optimal convergence rate as that in univariate local adaptive nonparametric regression splines , and our final selected model is rather parsimonious . to make the lsm in stable in computation and able to adapt its estimates to the level of smoothness of the component function ,",
    "weighted penalized regression splines method and projected weighted penalized regression splines method are proposed .",
    "the two - stage estimation is obtained by using one - dimensional non - parametric techniques to refine the estimates in the first stage , which serve as initial approximations for the additive components .",
    "our proposed procedure depends on only one parameter , which controls both prediction error and misclassification error .",
    "hence , to a certain degree , it reduces the computational burden and attains computational stability .",
    "simulation results illustrate that the method is superior in a set - up with independent predictors , and is comparable when the predictors are correlated .",
    "the outline of the remainder of this paper is as follows . in section 2 ,",
    "we describe our new method , study its asymptotic properties , and propose an approximation algorithm . in section 3 , simulations and a real data application",
    "are presented to illustrate the performance of the proposed method . a brief conclusion and discussion",
    "are given in section 4 .",
    "the technical details of the proof are relegated to section [ addproof ] .",
    "as the components in the additive model are unobservable nonparametric functions , it is impossible to perform selection directly , and an approximation is needed . to this end , we first examine the univariate nonparametric regression model with only one independent variable as a basis for our method .",
    "@xmath16 where @xmath17 is in @xmath18 $ ] .",
    "mammen and van de geer ( 1997 ) proposed the use of the total variation @xmath19 of the function @xmath20 as a penalty and to minimize the following penalized sum of the squared residuals to obtain the estimation of @xmath20 , @xmath21 as with the smoothing spline , mammen and van de geer ( 1997 ) proved that the minimizer of this equation falls into the spline space such that the estimate of @xmath20 itself is also a spline function .",
    "they also showed that the estimate of @xmath20 has some good asymptotic properties , such as local adaption and an optimal convergence rate .",
    "to implement their idea , consider the following spline space @xmath22 with knots @xmath23 for @xmath24 , @xmath25 is defined as @xmath26 { ~\\rm where~ } s(x)~~ { \\rm is ~ a ~polynimial~ of~ the~order~ } p { \\rm~ on~ each~ subinterval~ } [ t_i , t_{i+1}]\\}.\\ ] ] when @xmath27 , @xmath25 is the set of step functions with jumps at the knots .",
    "it is known that the space @xmath25 is a @xmath28 dimensional linear function space , and that the truncated power function series @xmath29 forms its basis ( see de boor , 1978 ) .",
    "thus , if the number of knots @xmath30 is sufficiently large , then we can approximate @xmath31 by a spline function with the form @xmath32 note that @xmath33 by minimizing @xmath34 we can obtain an estimate @xmath35 for the function @xmath20 .",
    "we now return to the additive regression model @xmath36 for every function component , we assume that @xmath37 , is approximated by the spline function @xmath38 where @xmath39 is the series of knots for the @xmath30th function component .    for any @xmath30 ,",
    "let @xmath40 be the spline bases ( note that although the number of bases should be @xmath41 , for convenience , we still denote it as @xmath42 ) .",
    "the additive model can then be approximated by the following linear model .",
    "@xmath43 for any @xmath30 with @xmath44 , the basis series @xmath45 , @xmath46 can be regarded as a natural group of variables in the foregoing linear model , and the group variable selection can be used to estimate @xmath47 and to select the grouped variables .",
    "we combine the hierarchical lasso method ( zhou and zhu s 2007 ) , the group bridge approach ( huang _ et al_. 2009 ) , and the ideas of mammen and van de geer ( 1997 ) and propose the criterion @xmath48 to select the groups .",
    "that is , we simultaneously to select the significant components , and estimate the parameters @xmath47 .",
    "however , this linear approximation does not mean that the problem is exactly identical to the case in the classical linear model .",
    "first , to make a good approximation of the function @xmath49 , @xmath42 , the number of basis functions in the spline approximation , must be sufficiently large , and theoretically , increases with the sample size @xmath50 .",
    "thus , even when @xmath8 , the number of function components , is of a moderate size , the linear structure derived has a diverging number of predictors if we do want to regard the model as linear .",
    "second , the grouped variables @xmath51 , are all related to the variable @xmath52 , and are thus strongly correlated , especially when the power basis functions are used .",
    "third , distinct from zhou and zhu ( 2007 ) , in our setup , the estimation accuracy of the whole function , rather than the estimation accuracy of a particular coefficient , is of interest and importance .",
    "thus , as the objective here is to find a good approximation of each function component , the asymptotical results obtained by zhou and zhu ( 2007 ) , and huang _ et al_. ( 2009 ) can not be directly applied to the additive model .      to study the asymptotic behavior of the model",
    ", we first consider a more general situation .",
    "let @xmath53 be a class of functions on @xmath54 $ ] . for a linear subspace @xmath55 of @xmath53 , we consider a penalty @xmath56 that satisfies @xmath57 and @xmath58    consider the additive model ( 1.1 ) with @xmath59 for a tuning variable @xmath60 , @xmath61 are estimated by minimizing the penalized sum of squares over @xmath62 : @xmath63 write @xmath64 .",
    "for a subset @xmath65 of @xmath53 , we denote the @xmath66 entropy of @xmath65 by @xmath67 .",
    "this is the logarithm of the minimal number of balls of a radius @xmath66 covering @xmath65 , where @xmath68 is the @xmath69-norm with respect to the empirical probability measure of @xmath70 @xmath71 with the form @xmath72 to obtain the required result , we must first assume the following condition first .",
    "assume that condition 1 holds .",
    "let @xmath78 be a positive number sequence such that for the functions @xmath79 in @xmath55 we have @xmath80 let @xmath81 furthermore , assume that for some @xmath82 and @xmath83 , the following entropy bound condition is satisfied .",
    "@xmath84 we then have @xmath85    from ( 2.3)(2.5 ) , we can define the penalty functional as the @xmath86 norm of the coefficients for the spline approximation @xmath87 , that is , @xmath88    in fact , this gives @xmath89 , where @xmath90 denotes the total variation .",
    "we obtain the following result for the entropy of the total variation space .    [ lemm1 ]",
    "define @xmath91 . there then exists a constant @xmath92 such that @xmath93    to state our results for the asymptotic behavior of the penalized least - squares estimate ( 2.5 ) , we need some further conditions .",
    "+ condition 2 for any @xmath94@xmath95 where @xmath96 , @xmath97 is the number of knots , and @xmath92 is a predetermined constant .",
    "+    assume that @xmath98 , and that the total variation of its @xmath99-th derivative is bounded .",
    "let @xmath100 , where @xmath101 is a large constant .",
    "then , under conditions 1 and 2 , we have @xmath102 for @xmath103 .        the penalty function in ( [ addspline ] )",
    "can be regarded as nonconcave .",
    "hence , the quadratic approximation method and the iterative algorithm proposed by fan and li ( 2001 ) can be used to define estimates of the coefficients .",
    "first , consider the derivative of penalty function for @xmath47 .",
    "let @xmath104 .",
    "this gives @xmath105 to simplify the notation , we rewrite ( [ addspline ] ) in matrix form as @xmath106 where @xmath107 , @xmath108 and @xmath109 .    if @xmath110 with nonzero coefficients @xmath111 minimizes the equation ( [ addspline2 ] ) , then the following equation is satisfied .",
    "@xmath112 where @xmath113 , and @xmath114 and @xmath115 hence , as in fan and li ( 2001 ) , given an initial value @xmath116 , ( [ addspline3 ] ) requires an iterative algorithm to update the estimate to @xmath117 according to the following equation @xmath118 fan and li ( 2001 ) suggested that this iterative step is similar to the one - step mle if the initial value is sufficiently good .",
    "if a reasonable initial value of @xmath119 is selected , then our algorithm should converge within a few steps .",
    "the tuning parameter @xmath120 is very important for estimating @xmath119 .",
    "fan and li ( 2001 ) proposed using generalized cross - validation to select @xmath120 .",
    "let @xmath121 be the estimate of @xmath122 with the tuning parameter @xmath120 .",
    "the generalized cross - validation statistic is defined as @xmath123 and @xmath124 where @xmath125 $ ] , @xmath126 .    according to wang , li , and tsai ( 2007 ) , the log(gcv )",
    "is very similar to the traditional model selection criterion aic .",
    "although aic is an efficient selection criterion that selects the best finite - dimensional candidate model in terms of prediction accuracy , it is not a consistent selection criterion because it does not select a correct model with a probability approaching 1 as the sample size goes to infinity .",
    "however , for our proposed method the number of knots , or the dimension of @xmath119 is very large and increases with the sample size @xmath50 , and thus an adjustment for such a criterion is necessary .",
    "accounting for the effect of dimensionality to correctly select the significant variables , we suggest using the inflated factor for gcv .",
    "a modified generalized cross - validation(mgcv ) is defined as @xmath127 where @xmath128 is the inflated factor . when @xmath129 , the mgcv is no different from the gcv proposed by fan and li ( 2001 ) . based on our experience and the discussions of luo and wahba ( 1997 ) and friedman and silverman ( 1989 ) , we suggest selecting @xmath128 within the interval @xmath130 as an extra penalty .",
    "our method is based on the power spline regression .",
    "it is well known that the power spline regression is not stable in computation because of a strong correlation between power bases , and many base functions are related to only a few observations . to make our numerical results more stable , we weight the power spline base for every component function as @xmath131 where @xmath132 and @xmath133 is given in ( [ addspline3 ] ) .",
    "( [ addspline2 ] ) can then be rewritten as @xmath134    by some elementary calculations , it is easy to determine that when all of the components of @xmath135 are independent , the variance of the least - squares estimate of @xmath136 should be of the order @xmath137 . also , when the sample points @xmath138 are equally spaced , our method is equivalent to transferring the power base spline approximation to a b - spline approximation with an @xmath86-norm penalty of a linear combination of the coefficients in the b - spline approximation . furthermore , as the variance of the least - squares estimate of @xmath139 is of the order @xmath137 , as to the wavelet approximation ( donoho and johnstone 1994 ) , the universal threshold @xmath140 can be used to penalize each coefficient .",
    "in other words , the tuning parameter can be searched within a small interval with the length @xmath141 .",
    "these modifications result in a stable final penalized component function estimate .      to make our final estimated model parsimonious and easy to interpret , in addition to selecting significant component functions",
    ", we also suggest the following procedure for the component estimation .",
    "note that the power spline approximation ( see the definition of @xmath142 above ( 2.1 ) ) expands the component function as the sum of a polynomial and a linear combination of truncated power base functions .",
    "divide @xmath135 by @xmath143 , with @xmath144 being the block from the @xmath145-th column to the @xmath146-th column in matrix @xmath135 . here",
    ", @xmath147 is an @xmath148 column vector in which all the elements are equal to 1 .",
    "we then write @xmath149 , where @xmath150 are the coefficients of the polynomial part and @xmath151 are the coefficients of the truncated power base functions .",
    "we regard these as two groups and then penalize each group separately , which make it possible to adaptively estimate the component functions when they are actually polynomial functions without any great effect from the truncated power base functions .",
    "this provides a way of adaptively estimating the component functions if they are actually polynomial , and means that the estimation is adaptive to the level of smoothness of the component functions .",
    "this approach may result in a more parsimonious estimation than that obtained by the previous estimation algorithm .",
    "however , we note that the two groups are strongly correlated .",
    "to realize the approach and to make the algorithm more efficient , we consider the following empirical power base functions . @xmath152 where @xmath153 is the project matrix from @xmath151 to @xmath154 .",
    "this projection method is able to reduce the correlation between the two groups in the spline approximation .",
    "let @xmath155 ( [ addspline2 ] ) can also be written as @xmath156 where @xmath157 are the group penalty functions for the polynomial coefficients for all of the component additive functions and @xmath158 are the group penalty functions for the coefficients of the truncated power bases .",
    "when the dimension of the additive regression model is very high , selecting significant component functions becomes very difficult .",
    "the model selection and estimation may not be consistent , and the estimation procedure may also become unstable . to improve the estimation accuracy , we suggest a two - stage estimation approach . in the first stage ,",
    "we use our proposed methods to select and estimate significant component functions as initial approximations of all of the selected components .",
    "let @xmath159 and denote the corresponding estimates by @xmath160 . in the second stage ,",
    "we obtain refined estimates as follows . for the @xmath161 selected in the first stage , define @xmath162 and then estimate @xmath163 non - parametrically using the following model @xmath164 for this new model",
    ", we can again use the method applied in the first - stage estimation to obtain the final estimator of @xmath163 .",
    "we conduct simulations to examine the effectiveness of the proposed lasso - type spline method for component function selection and estimation in the additive regression model .",
    "the algorithm proposed in section [ oprs ] is called the original lasso - type spline method ( olsm ) , that in section [ wprs ] the weighted lasso - type spline method ( wlsm ) , and that in section [ pwprs ] the projected weighted lasso - type spline method ( pwlsm ) . we also compare the results with those obtained using the sparsity - smoothness penalty ( ssp ) approach recently proposed by meier , van de geer , and bhlmann ( 2009 ) by using the r packages provided by the authors . for selection performance ,",
    "we compute the true positive ratio ( tpr ) and false positive ratio ( fpr ) ; and for estimation accuracy , we compute the empirical prediction mean square error ( mse ) .",
    "letting @xmath165 be the estimator of @xmath166 , mse is defined as @xmath167 where @xmath168 are the data points .    in the simulations ,",
    "the sample size @xmath169 and a total of @xmath170 simulation runs are used . to reduce the computational burden ,",
    "the knots are designed as follows .",
    "let the number of knots be @xmath30 .",
    "for each predictor @xmath17 , the knots are selected to be the @xmath171$]-th order statistics @xmath172 ) } , j=1,\\ldots , k\\}$ ] of @xmath173 .",
    "quadratic splines are used , which gives a total number of base functions with @xmath8 function components of @xmath174 . to check the sensitivity of the methods to the knot number selection , we tried the values @xmath175 , and @xmath176 with a fixed @xmath120 , and found that the numerical results did not differ much .",
    "we thus posit that our proposed three procedures are insensitive to the initial knot number as long as it is sufficiently large .",
    "however , with a larger number knots , the computation time is grated and the performance is a little worse , as the computation may be less stable due to strongly correlated variables in the splines .",
    "we thus set @xmath30 at @xmath177 in the simulations .",
    "the penalty parameter @xmath120 is found to be critical .",
    "we choose @xmath120 by computing the mgcv criterion defined in ( [ mgcv ] ) for a grid of @xmath178 values and choosing the minimizer over the grid .",
    "the inflated factor @xmath128 is taken to be @xmath179 .",
    "the grid of @xmath120 for all three proposed procedures has 100 values and satisfies the condition that the values of @xmath180 are equally spaced between @xmath181 and @xmath182 . the sparsity - smoothness penalty approach ( ssp )",
    "require teh selection of two parameters @xmath183 and @xmath184 , where the former serves to control the sparsity and the latter the smoothness .",
    "both parameters are chosen by using @xmath170 grid points for @xmath183 and @xmath177 grid points for @xmath184 in the spirit of meier , van de geer , and bhlmann ( 2009 ) .",
    "the simulation experiments are similar to those in example 1 and example 3 of meier , van de geer , and bhlmann ( 2009 ) . as our focus is on simultaneous selection and estimation , @xmath8",
    "is chosen to be @xmath185 rather than an ultra - high dimension .",
    "[ example - ind](covariates are independent ) .",
    "the data are generated from @xmath186 where @xmath187 and @xmath188 .",
    "the predictors are sampled from the uniform distribution of @xmath189 .",
    "[ example - cor](covariates are correlated ) .",
    "the model is @xmath186 with @xmath190 and @xmath191 .",
    "the covariates @xmath192 are generated from @xmath193 where @xmath194 and @xmath195 are i.i.d uniform(0 , 1 ) .",
    "this provides a design with a correlation coefficient of @xmath196 between all of the covariates .",
    "the simulation results are reported in tables [ table - ind ] and [ table - cor ] .",
    "the median of the mse and the robust standard deviation of the mse ( the ratio of the interquartile and the standard normal interquartile @xmath197 ) are reported.``@xmath198 '' means the mse value of the estimates for @xmath49 , and `` @xmath199 '' means the mse for the full model .",
    "the row `` ssp '' shows the results of the ssp method developed by meier , van de geer , and bhlmann ( 2009 ) .",
    "the rows `` olsm '' , `` wlsm '' , and `` pwlsm '' respectively summarize the results that are based on `` oracle '' ( assuming that all of the functions are known except that to be estimated ) , `` one - stage '' , and `` two - stage '' estimates for the original lasso - type spline method , the weighted lasso - type spline method , and the projected weighted lasso - type method , respectively . the tpr and fpr results for each method are reported in table [ table - tp ] .",
    "the curve estimations for the component functions are respectively summarized in figures [ olsm1][pwlsm2 ] .",
    "the results tabulated in tables  [ table - ind ] and [ table - tp ] and plotted in figures  [ wlsm1 ] and [ pwlsm1 ] with independent covariates in example [ example - ind ] show the differences among the `` oracle '' cases and the olsm , wlsm , and pwlsm cases to be nearly negligible .",
    "the mse of the two - stage estimates is significantly smaller than that of the corresponding one - stage estimates and approximate that in the `` oracle '' case . of all of these methods ,",
    "the two - stage estimates of the pwlsm are superior to the others .",
    "the numbers of true positives and false positives for the pwlsm are the same as those of the ssp approach .",
    "however , the olsm has a smaller number of true positives and the wlsm has a larger variation in true positives than the ssp , although the true positives for the wlsm is the same as that for the ssp .",
    "figures [ olsm1 ] , [ wlsm1 ] , and [ pwlsm1 ] show that the olsm and wlsm may fail to select the first component function .",
    "this is because the first function is small in magnitude , and is easily selected out from the model due to the penalty . in contrast , the pwlsm always selects all of the non - zero component functions into the model , and the knots used in the spline basis are very sparse .",
    "furthermore , the pwlsm selects the linear function ( for example , the third panel on the top ) as linear , whereas the ssp fails to do so .",
    "the pwlsm thus outperforms the other methods in this setting .    for the correlated covariate case in example [ example - cor ]",
    ", the results presented in tables [ table - cor ] and [ table - tp ] and figures [ olsm2 ] , [ wlsm2 ] and [ pwlsm2 ] ) suggest that all of the `` oracle '' estimations perform similarly .",
    "our proposed three lsms all apparently improve on the method the ssp in terms of the mse .",
    "the numbers of true positives and false positives for the wlsm are the same as those for the ssp .",
    "however , the pwlsm does not perform better in every respect , as it keeps selecting all of the true components at the expense of including a component that is slightly more noisy than the other insignificant components .      in this section , we give more details of the analysis of the dataset described in section  [ sec - data ] . as described , the dataset has been analyzed by fan and peng ( 2004 ) with the linear model and the additive model , respectively .",
    "we now use the method proposed in this paper to analyze the dataset and to make a comparison with the results of fan and peng ( 2004 ) .",
    "similar to the approach of fan and peng ( 2004 ) , we also move out the outliers and use the 199 remaining observations for our analysis .",
    "consider the additive model @xmath200 we use lasso for the linear part and our method for the component function selection .",
    "the @xmath201 sample quantiles of the variables `` yrsexp '' and `` age '' are selected as knots , which gives 15 initial knots to estimate the component functions .",
    "fan and peng ( 2004 ) used only 5 knots to estimate each component function , whereas our method gives 20 more parameters to model data . despite this , the computational complexity is not increased because the quadratic approximation algorithm can be easily implemented , and most of the knots will be removed in an iterative fashion by our component selection procedure . in line with fan and peng ( 2004 ) and the foregoing discussion , we first weight the  design matrix \" such that the original least - squares estimate has the standard deviation of every estimate of the coefficients of the prediction variables and a truncated power basis function close to the order of @xmath202 .",
    "two tuning parameters are then used to select the variables in the linear part and the component functions .",
    "mgcv with an inflation parameter of @xmath179 is used to select the tuning parameters .",
    "the results are reported in the fourth column ( wlsm , see section [ addsimu ] ) of table [ table - data ] and in figure [ bk3 ] .",
    "table [ table - data ] shows that our method does not select the component function of `` age '' .",
    "this is consistent with the result of scad - pls for the linear model , which is reported in the second column of table [ table - data ] .",
    "the other estimates of the coefficients are similar to those obtained with the first three methods .",
    "the function of `` yrsexp '' is now estimated as an increasing function ( see figure [ bk3 ] ) . only two spline bases , from among the 17 are selected to estimate the component function of `` yrsexp '' .",
    "hence , the selected model is much simpler than the selected model derived with scad - pls under the additive structure . the @xmath10 value in the fourth column for the wlsm is larger than that in the second or third column for scad - pls",
    "this means that , compared with the scad - pls method for either the linear model or the additive model , our method provides a more reasonable estimation and selects a simpler model in this real data example .",
    "[ cols=\"^,^ \" , ]",
    "in this paper , we propose a lasso - type method for selecting nonparametric components in the additive regression model .",
    "we can use this method to simultaneously select and estimate components .",
    "simulations show that for a high - dimensional additive model , the proposed methods can shrink the function components that correspond to the nonsignificant predictors exactly to zero and produce a parsimonious model . for an ultra - high dimensional additive model , we follow the idea of fan _ et al _  ( 2009 ) and use then sis method to first reduce the ultra - dimension of the additive model to a high dimension , and then use our proposed method to select and estimate the significant components . intuitively , it is possible to extend this idea to generalized additive models with binary response data or poisson data , or with a given link function .",
    "research in this area is ongoing .",
    "proof of theorem 1 : as @xmath203 , a natural consistent estimate of @xmath204 is @xmath205 . without loss of generality , assume that @xmath206 . by the condition ( 2.6 )",
    ", there exist @xmath207 such that @xmath208 define @xmath209 that satisfies @xmath210 and @xmath211 by the definition of @xmath212 , we have @xmath213 and @xmath214 where the second inequality is derived from the cauchy - schwarz inequality .",
    "note that the condition ( 2.7 ) on the entropy bound implies @xmath215 define @xmath216 as @xmath55 is a linear space , it is easy to see that @xmath217 and @xmath218 . then , by the entropy bound , we have @xmath219 which implies that @xmath220 this inequality holds only when either one of the following inequalities is fulfilled .",
    "@xmath221 @xmath222 to prove this , we consider each case separately .",
    "\\(i ) if @xmath223 , and @xmath224 , then we have @xmath225 and then @xmath226 by @xmath224 and @xmath227 it is clear that @xmath228    \\(ii ) when @xmath229 , we consider the two cases separately .    case ( ii)-1 . @xmath230 the foregoing inequality implies that @xmath231 then , @xmath232 substituting this into the preceding equation , and noting that @xmath233 , we obtain @xmath234 invoking the condition for @xmath60 , we have @xmath235 again , using this in the foregoing equation , we have @xmath236    case ( ii)-2 . @xmath237 the above equation yields @xmath238 which implies that either @xmath239 or @xmath240 as @xmath241 , both inequalities give @xmath242    following this equation and proposition 1 in stone ( 1985 ) and the definition of @xmath243 we have @xmath244 this complete the proof",
    ". @xmath245    proof of proposition  [ lemm1 ] : it is easy to verify that the functions in @xmath246 are uniformly bounded . by applying the results for entropy bounds in birman and solmjak ( 1967 ) , we can easily obtain the solution . @xmath245    proof of theorem 2 : by conditions  1 and 2 , and the results for the spline approximation ( see de boor , 1978 ) , when the number of initial knots is sufficiently large , @xmath247 , it is obvious that the condition ( 2.6 ) of theorem 1 , @xmath248 and @xmath249 are satisfied when @xmath78 is a constant and @xmath250 . by proposition  [ lemm1 ] , the entropy condition ( 2.7 ) in theorem 1",
    "is also satisfied by the spline approximation function @xmath251 when @xmath250 .",
    "then , letting @xmath252 , it is easy to see that theorem 2 is a corollary of theorem 1 .",
    "heng peng s research is supported by cerg grants of hong kong research grant council ( hkbu 201809 and hkbu 201610 ) , frg grant from hong kong baptist university frg/08 - 09/ii-33 , and a grant from national nature science foundation of china ( nnsf 10871054 ) .",
    "sardy , s. and tseng , p. ( 2004 ) , amlet , ramlet , and gamlet : automatic nonlinear fitting of additive models , robust and generalised with wavelets .",
    "_ journal of computational and graphical statistics _ , * 13 * , 283 - 309 .",
    "shively , t. , kohn , r. and wood , s. ( 1999 ) .",
    "variable selection and function estimation in additive nonparametric regression using a data - based prior . _ journal of the american statistical assocation _ * 94 * , 777 - 794 ."
  ],
  "abstract_text": [
    "<S> similar to variable selection in the linear regression model , selecting significant components in the popular additive regression model is of great interest . </S>",
    "<S> however , such components are unknown smooth functions of independent variables , which are unobservable . as such , some approximation is needed . in this paper , we suggest a combination of penalized regression spline approximation and group variable selection , called the lasso - type spline method ( lsm ) , to handle this component selection problem with a diverging number of strongly correlated variables in each group . </S>",
    "<S> it is shown that the proposed method can select significant components and estimate nonparametric additive function components simultaneously with an optimal convergence rate simultaneously . to make the lsm stable in computation and able to adapt its estimators to the level of smoothness of the component functions , </S>",
    "<S> weighted power spline bases and projected weighted power spline bases are proposed . </S>",
    "<S> their performance is examined by simulation studies across two set - ups with independent predictors and correlated predictors , respectively , and appears superior to the performance of competing methods . </S>",
    "<S> the proposed method is extended to a partial linear regression model analysis with real data , and gives reliable results .    </S>",
    "<S> * keywords : * additive model , nonparametric component , group variable selection , penalized splines , lasso , generalized cross - validation . </S>"
  ]
}