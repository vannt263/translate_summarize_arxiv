{
  "article_text": [
    "in many neural systems anatomical and physiological motifs are present repeatedly in the service of a variety of different functions .",
    "a prime example is the canonical cortical microcircuit that is found in many different regions of the six - layered mammalian neocortex .",
    "these different regions serve various sensory , cognitive , and motor functions , but how can a common circuit be used for such a variety of different purposes ?",
    "this issue has spawned interest in finding a common abstract framework within which the relevant information processing functions can be specified .",
    "several solutions for such an abstract framework have been proposed previously , among them approaches that still use semantics to a certain extent ( predictive coding with its initial focus on sensory perception ) , teleological ones that prescribe a goal based on statistical physics of the organism and its environment ( free energy principle ) and information theoretic ones that focus on local operations on information ( coherent infomax ) .",
    "while these are all encouraging developments , they also beg the question of how to compare these approaches , and how many more possibilities of defining new approaches of this kind exist . ideally , an abstract framework that would comprise these approaches as specific cases would be desirable .",
    "this article suggests a possible starting point for the development of such a unifying framework .    by definition",
    "this framework can not be cast in processing - domain specific language , such as ` edge - filtering ' or ",
    "face perception , or  visual working memory , for example , but must avoid any use of semantics beyond describing the elementary operations that information processing is composed of . a framework that has these properties is information theory . in fact , information theory is often criticized exactly for its lack of semantics , i.e. for ignoring the _ meaning _ of the information that is processed in a system . as we will demonstrate here",
    ", this apparent shortcoming can be a strength when trying to provide a unified description of the goals of neural information processing .",
    "moreover , by identifying separate component processes of information processing , information theory provides a meta - semantics that serves to better understand what neural systems do at an abstract level ( for more details see @xcite ) .",
    "last , information theory is based on evaluating probabilities of events and thereby closely related to the concepts and hypotheses of probabilistic inference that are at the heart of predictive coding theory @xcite .",
    "thus information theory is naturally linked to the domain - general semantics of this and related theories .",
    "based on the domain - generality of information theory several variants of information theoretic goal functions for neural networks have been proposed .",
    "the optimization of these abstract goal functions on artificial neural networks leads to the emergence of properties also found in biological neural systems  this can be considered an amazing success of the information theoretic approach given that we still know very little about general cortical algorithms .",
    "this success raises hopes for finding unifying principles in the flood of phenomena discovered in experimental neuroscience .",
    "examples of successful , information - theoretically defined goal functions are linsker s infomax @xcite  producing receptive fields and orientation columns similar to those observed in primary visual cortex v1 @xcite , recurrent infomax  producing neural avalanches , and an organization to synfire - chain like behaviour @xcite , and coherent infomax @xcite .",
    "the goal function of coherent infomax is to find coherent information between two streams of inputs from different sources , one conceptualized as sensory input , the other as internal contextual information . as coherent",
    "infomax requires the precomputation of an integrated receptive field input as well as an integrated contextual input to be computable efficiently ( and thereby , in a biologically plausible way ) , the theory predicted the recent discovery of two distinct sites of neural integration in neocortical pyramidal cells @xcite . for details",
    "see the contribution of phillips to this special issue .",
    "we will revisit some of these goal functions below and demonstrate how they fit in the larger abstract framework aiming at a unified description that is presented here .",
    "apart from the desire for a unified description of the common goals of repeated anatomical motifs , there is a second argument in favor of using an abstract framework .",
    "this argument is based on the fact that a large part of neural communication relies on axonal transmission of action potentials and on their transformation into post - synaptic potentials by the receiving synapse .",
    "thus , for neurons , there is only one currency of information .",
    "this fact has been convincingly demonstrated by the successful rewiring of sensory organs to alternative cortical areas that gave rise to functioning , sense - specific perception ( see for example the cross - wiring , cross - modal training experiments in @xcite ) . in sum , neurons only see the semantics inherent in the train of incoming action potentials , not the semantics imposed by the experimenter . therefore , a neurocentric framework describing information processing must be necessarily abstract . from this perspective information theory is again a natural choice .",
    "classic shannon information theory , however , mostly deals with the transmission of information through a communication channel with one input and one output variable . in a neural setting this would amount to asking how much information present at the soma of one cell reaches the soma of another cell across the connecting axons , synapses and dendrites , or how much information is passed from one circuit to another .",
    "information processing , however , comprises more operations on information than just its transfer .",
    "a long tradition dating back all the way to turing has identified the elementary operations of information as information transfer , active storage , and modification .",
    "correspondingly , measures of information transfer have been extended to cover more complex cases than shannon s channels , incorporating directed and dynamic couplings @xcite and multivariate interactions @xcite , and also measures of active information storage have been introduced @xcite .",
    "information modification , seemingly comprising of subfunctions such as _ de novo _ creation and fusion of information , however , has been difficult to define @xcite .",
    "one reason for extending our view of information processing to more complicated cases is that even the most simple function from boolean logic that any other logic function can be composed of ( nand , see for example @xcite , chapter 1 ) uses two distinct input variables and one output .",
    "while such a logic function could be described as a channel between the two inputs and the outputs , this does not do justice to the way the two inputs interact with each other .",
    "what is needed instead is an extension of classic information theory to three way systems , describing how much information in the output of this boolean function , or any other three - way processor of information , comes uniquely from one input , uniquely from the other input , how much they share about the output , and how much output information can only be obtained from evaluating both inputs jointly .",
    "these questions can be answered using an extension of information theory called partial information decomposition ( pid ) @xcite .",
    "this article will introduce pid and show how to use it to specify a generic goal function for neural information processing .",
    "this generic goal function can then be adapted to represent previously defined neural information processing goals such as infomax , coherent infomax and predictive coding .",
    "this representation of previous neural goal functions in just one generic framework is highly useful to understand their differences and commonalities .",
    "apart from a reevaluation of existing neural goal functions , the generic neural goal function introduced here also serves to define novel goals not investigated before .",
    "the remainder of the text will first introduce partial information decomposition , and then demonstrate its use to decompose the total output information of a neural processor . from this decomposition",
    "we derive a generic neural goal function `` @xmath0 '' , and then express existing neural goal functions as specific parameterizations of g. we will then discuss how the use of @xmath0 simplifies the comparison of these previous goal functions and how it helps to develop new ones .",
    ", @xmath1 , and output @xmath2 .",
    "( b ) processor with local weighted summation of inputs as used in coherent infomax and in this study . to establish the link to the coherent infomax literature we identify the input @xmath3 with the receptive field input @xmath4 , which may be excitatory ( e ) or inhibitory ( i ) , and which is summed . in the same way ,",
    "@xmath1 is identified with the contextual input @xmath5 .",
    "( c ) overlay of the coherent infomax neural processor on a layer 5 pyramidal cells , highlighting potential parallels to existing physiological mechanisms .",
    "layer 5 cells created with the trees toolbox @xcite , courtesy of hermann cuntz.,scaledwidth=100.0% ]    in this section we will describe the framework of partial information decomposition ( pid ) to the extent that is necessary to understand the decomposition of the mutual information between the output @xmath2 of a neural processor and a set of two inputs @xmath3 , @xmath1 ( figure [ fig : neuralprocessor ] ) . the inputs themselves may be multivariate random variables but we will not attempt to decompose their contributions further .",
    "this is linked to the fact that in many neurons contextual and driving inputs are first summed separately before being brought to interact to produce the output .",
    "this summation strongly reduces the parameter space and thereby makes learning tractable ",
    "see @xcite .",
    "therefore , we limit ourselves to the pid of the mutual information between one `` left hand side '' or `` output '' variable @xmath2 and two `` right hand side '' or `` input '' variables @xmath3 , @xmath1 . that is , we decompose the mutual information @xmath6 , : `` : '' separates _ sets _ of variables between which mutual information or partial information terms are computed , `` ; '' separates multiple _ sets _ of variables on one side of a partial information term , whereas `` , '' separates variables within a set that are considered jointly ( see the appendix  [ app : notation ] for examples ) . ] the total amount of information held in the set @xmath7 about @xmath2 : . ]",
    "@xmath8 where the @xmath9 signifiy the support of the random variables and @xmath10 , @xmath11 are the entropy and the conditional entropy , respectively ( see @xcite for definitions of these information theoretic measures ) .",
    "the pid of this mutual information addresses the questions :    1 .",
    "what information does one of the variables , say @xmath3 , hold individually about @xmath2 that we can not obtain from any other variable ( @xmath1 in our case ) ?",
    "this information is the _",
    "unique information _ of @xmath3 about @xmath2 :  @xmath12 .",
    "2 .   what information does the joint input variable @xmath13 have about @xmath2 that we can not get from observing both variables @xmath3 , @xmath1 separately ?",
    "this information is called the _ synergy _ , or _",
    "complementary information _ , of @xmath14 with respect to @xmath2 :  @xmath15 .",
    "3 .   what information does one of the variables , again say @xmath3 , have about @xmath2 that we could also obtain by looking at the other variable ( @xmath1 ) alone ?",
    "this information is the _ shared _ information of @xmath3 and @xmath1 about @xmath2 :  @xmath16 .",
    "following @xcite , the above three types of partial information terms together by definition provide all the information that the set @xmath17 has about @xmath2 , and other sources agree on this @xcite , i.e. :    @xmath18    figure [ fig : pid ] is a graphical depiction of this notion by means of the partial information ( pi- ) diagrams introduced in @xcite . in addition , there is agreement that the information one input variable has about the output should decompose into a unique and a shared part as : @xmath19    for the treatment of neural goal functions we have to furthermore give pid representations of the relevant conditional mutual information terms .",
    "these can be obtained from equations [ eq : totalmi ] and [ eq : singlemi ] as : @xmath20    moreover , all parts of the pi - diagram are typically required to be positive to allow an interpretation as information terms .",
    "due to the pioneering work of williams and beer @xcite it is now well established that neither unique , nor shared , nor synergistic information can be obtained from the definitions of entropy , mutual information and conditional mutual information in classical information theory . essentially , this is because we have an underdetermined system , i.e. we have fewer independent equations relating the output and inputs in classical information theory ( three for two input variables ) than we have pid terms ( four for two input variables ) . for at least one of these pid terms a new ,",
    "axiomatic definition is necessary , from which the others then follow , as per equations [ eq : totalmi]-[eq : condmi ] . to date",
    ", the equivalent axiom systems introduced by bertschinger and colleagues @xcite , and by griffiths and koch @xcite have found the widest acceptance .",
    "they also yield results that are very close to an earlier proposal by harder and colleagues @xcite .",
    "all of these axiom systems lead to measures that are sufficiently close to a common sense view of unique , shared and synergistic information , and all satisfy equations [ eq : totalmi]-[eq : condmi ] .",
    "hence , their exact details do not matter at first reading for the purposes of this paper , and will therefore be presented in appendix  [ app : pid ] .",
    "the one exception to this statement is that we have to mention here already that shared information may arise in the frameworks of bertschinger at al .",
    "@xcite , griffiths et al.@xcite , and also harder et al .",
    "@xcite for two reasons .",
    "first , there can be shared information because the two inputs @xmath3 , @xmath1 have mutual information between them ( termed _ source redundancy _ in @xcite , and source shared information here )  this is quite intuitive for most .",
    "second , shared information can arise because of certain _ mechanisms _ creating the output @xmath2 ( _ mechanistic redundancy _ in @xcite , mechanistic shared information here ) .",
    "this second possibility of creating shared information is less intuitive but nevertheless arises in all of the frameworks mentioned above .",
    "for example , the binary and operation on two _ independent _ ( identically distributed ) binary random variables creates @xmath21 bits of shared information in @xcite , and @xmath22 bits of synergistic mutual information , while there is no unique information about the inputs in its output .",
    "we here present a definition of unique information given by bertschinger et al .",
    "@xcite , which is equivalent to that provided by griffith and koch @xcite .",
    "we assume ( that neural signals can be described by ) discrete random variables @xmath3 , @xmath1 , @xmath2 with ( finite ) alphabets @xmath139 , @xmath140 , @xmath141 , described by their joint probability distribution @xmath142 .",
    "as already mentioned above , a definition of either unique , or shared , or synergistic information that @xmath3 , @xmath1 and @xmath17 have about a variable @xmath2 is enough to have a well defined pid . among these possibilities , bertschinger and",
    "colleagues opt for a definition of unique information based on the everyday notion that having unique information about @xmath2 implies that we can exploit this unique information to our favor against others who do not have this information  at least given a suitable situation .",
    "thus if we are allowed to construct such a suitable situation to our liking , we may prove to have unique information for example by winning bets on the outcomes of @xmath2 , where the bets are constructed by us against an opponent who does not have that unique information .",
    "more formally , one can imagine two players @xmath136lice and @xmath137ob .",
    "alice has access to the variable @xmath3 from equation [ eq : totalmi ] , while she does neither have access to variable @xmath1 , nor direct access to variable @xmath2 .",
    "bob has access to the variable @xmath1 , but neither direct access to @xmath3 , nor to @xmath2 . to the extent that the mutual information terms @xmath114 , and @xmath143 allow , alice and bob however , do have _ some _ information about the variable @xmath2 , despite not having direct access to @xmath2 . if alice wants to prove that having access to @xmath3 gives her unique information _ about _",
    "@xmath2 , so we re looking at information _ about @xmath2_. we do not care how much information @xmath3 has about @xmath1 , and vice versa . ]",
    ", then she can suggest to bob to play a specific game , designed by her , where the payout depends only on the outcomes of @xmath2 .",
    "in such a game , her reward will depend only on the probability distribution @xmath144 , while bob s reward will depend only on @xmath145 .",
    "the winner is thus determined simply by the two distributions @xmath146 and @xmath147 , but not by the details of the full distribution @xmath130 .",
    "practically speaking , alice should therefore construct the game in such a way that her payout is high for outcomes @xmath128 about which she can be relatively certain , knowing @xmath126 .",
    "( see text ) . note that if the synergy in ( b6 ) can not be reduced to 0 , then we simply _ define _ the unique information measure as @xmath148 . ]    from this argument , it follows that alice could not only prove to have unique information in the case described by the full joint distribution @xmath149 , but also for all other cases described by distributions @xmath150 that have the same pairwise marginal distributions , i.e. @xmath151 . based on this observation it makes sense to request that @xmath12 and @xmath60 stay constant on a set @xmath152 of probability distributions that is defined by : @xmath153 where @xmath154 is the set of all joint probability distributions of @xmath3 , @xmath2 , @xmath1 .",
    "from this , it follows from equation [ eq : singlemi ] that also the shared information @xmath155 must be constant on @xmath152 ( consult figure [ fig : pid_full ] , and take into account that the mutual information terms @xmath114 and @xmath143 are also constant on @xmath152 ) .",
    "hence , the only thing that may vary when exchanging the distribution p , for which we want to determine the unique information terms , for another distribution @xmath156 is the synergistic information @xmath157 .",
    "it therefore makes sense to look for a specific distribution @xmath158 where the unique information terms coincide with something computable from classic information theory . from figure",
    "[ fig : pid ] we see that for the case of a distribution @xmath158 where synergistic information vanishes , the unique information terms would coincide with conditional mutual information terms , i.e. @xmath159 .",
    "it is known , however , that a @xmath160 with this property does not necessarily exist for all definitions of unique , shared and synergistic information that satisfy equations [ eq : totalmi]-[eq : condmi ] , and that also satisfy the above game - theoretic property ( being able to prove the possession of unique information ) .",
    "therefore , bertschinger and colleagues suggested to define a measure @xmath161 of unique information via the following minimization : @xmath162    from this , measures for shared and synergistic information can be immediately obtained via equations [ eq : singlemi ] , [ eq : totalmi ] as : @xmath163 note that @xmath164 refers to the co - information @xmath165 ( see @xcite for details ) . for this particular choice of measures it can be shown that there is always at least one distribution @xmath166 for which the synergy vanishes , as was desired above .",
    "as knowledge of the pairwise marginal distributions @xmath167 , @xmath168 only specifies the problem up to any @xmath169 , and as the synergy varies on @xmath152 , we need to know the joint distribution @xmath125 to know about the synergy .",
    "this is indeed an intuitively plausible property and supports the functionality of the definitions given by bertschinger and colleagues @xcite .    from figure",
    "[ fig : pid ] and the definition of @xmath161 , @xmath170 , and @xmath171 in equations [ eq : ti_unq]-[eq : ti_syn ] it seems obvious that the following bounds hold for these measures : @xmath172 and this can indeed be proven , given that @xmath173 , @xmath174 , and @xmath175 is taken to mean any other definition of pid that satisfies equations [ eq : totalmi]-[eq : condmi ] @xcite and the above game theoretic assumption of a constant @xmath173 on @xmath152 .",
    "the measures @xmath161 , @xmath170 , and @xmath171 require finding minima and maxima of conditional mutual information terms on @xmath152 .",
    "fortunately , these constrained optimization problems are convex for two inputs as shown in @xcite , meaning that there is only one local minimum ( maximum ) which is the desired global minimum ( maximum ) . incorporating the constraints imposed by @xmath152 into the optimization maybe non - trivial , however .",
    "a short example may demonstrate the above reasoning : let alice and bob bet on the outcomes @xmath2 of a ( perfect , etc . )",
    "roulette table at a casino in a faraway city , such that they do not have immediate access to these outcomes ; they will only get a list of these outcomes when the casino closes , but will have to place their bets before that .",
    "alice has a spy @xmath3 at the casino who informs here directly after an outcome was obtained there , but only tells the truth when the outcome was even ( this includes 0 ) .",
    "otherwise he tells her a random possible outcome from a uniform distribution across natural numbers from 0 and 36 ( just like the roulette ) .",
    "bob also has a spy @xmath1 at the casino , but in contrast to alice s spy he only tells bob the truth for uneven outcomes and for 0 , otherwise he lies in the same way as the one of alice , picking a random number .",
    "neither alice nor bob knows about the spy of the other . only .",
    "the statement is intended for readers with a game theoretic background and should clarify that this is a trivial game , where knowledge about the opponent does nt influence alice s or bob s strategy . ]",
    "while this situation looks quite symmetric at first glance , both can prove to each other to have unique information about the outcomes at the casino , @xmath128 . to see this , remember that alice may suggest a game constructed by herself when trying to prove the possession of unique information .",
    "thus , alice could suggest to double the stakes for bets on even numbers , will be even , and in these cases alice will be told the truth . in the other 50% of the cases",
    ", the outcome will be odd , and the spy will report a random number . of these roughly 50% will be even , roughly 50% will be odd .",
    "thus alice will receive on average roughly 75% even and 25% odd numbers . of the even numbers 2/3",
    "will be correct .",
    "of the odd numbers only 1/18 will be correct  by chance .",
    "for bob the situation is reversed .",
    "forcing higher stakes for even outcomes will , therefore , be an advantage for alice . ] . at the end of the day",
    ", both alice and bob will have won a roughly equal amount of bets , but the bets alice will typically have won payed out more , and alice wins . in the same way , bob could suggest to double the stakes for uneven outcomes if it were his turn to prove the possession of unique information .",
    "thus , both have the same amount of information about the outcomes at the casino , but a part of that information is about different outcomes .    in this example , there is also redundancy as both will have the same information about the outcome @xmath176 .",
    "it is left for the reader to verify that alice and bob will gain some information ( i.e. synergy ) by combining what their spies tell them , but that this is not enough to be certain about the the outcome of the roulette , i.e. @xmath177  .      while synergy , shared and unique information are already difficult to estimate for discrete variables , it is not immediately clear how to extend the definitions to continuous variables in general .",
    "barrett has made significant advances in this direction though by considering pid for jointly gaussian variables @xcite .",
    "approaches to gaussian variables are important analytically because the classical information theoretic terms there may be computed directly from the covariance matrix of @xmath2 , @xmath3 , @xmath1 , and are important empirically due to the wide use of gaussian models to simplify analysis ( e.g. in neuroscience ) .",
    "first , barrett was able to demonstrate the existence of cases of non - zero quantities for each of synergy and shared information for such variables .",
    "this was done without reference to any specific formulation of pid measures by examining the ` net synergy ' ( synergy minus shared information ) , i.e. @xmath178 , which provides a sufficient condition for synergy where it is positive and for shared information where it is negative .",
    "this was an important result , since the intuition of many authors was that the linear relationship between such gaussian variables could not support synergy .",
    "next , barrett demonstrated a unique form for the pid for jointly gaussian variables which satisfies the original axioms of williams and beer @xcite as well as having unique and shared information terms depending only on the marginal distributions @xmath179 and @xmath180 ( as argued by bertschinger et al .",
    "@xcite above , and consistent with @xcite ) . to be specific",
    ", this unique form holds only for a _ univariate _ output ( though multivariate inputs are allowed ) .",
    "this formulation maps the shared information to the _ minimum _ of the marginal mutual information terms @xmath181 and @xmath182  hence is labeled the _ minimum mutual information _ ( mmi ) pid  and the other pid terms follow from equations [ eq : totalmi]-[eq : condmi ] .",
    "interestingly , this formulation always attributes zero unique information to the input providing less information about the output .",
    "furthermore , synergy follows directly as the additional information provided by this `` weaker '' input after considering the `` stronger '' input .",
    "some additional insights into this behaviour have recently been provided by rauh and colleagues in @xcite",
    "we use pid in this section to decompose the information @xmath23 that is contained in the output of a general neural processor ( figure [ fig : neuralprocessor ] ) with two input ( sets ) @xmath3 and @xmath1 and an output @xmath2 : @xmath24    to arrive at a neural goal function we can add weight coefficients to each of the terms in the entropy decomposition above to specify how desirable each one of one of these should be for the neural processor , i.e. we can specify a neural goal function @xmath0 as a function of these coefficients . since all the terms in equation [ eq : bandwidth ] are non - overlapping , and the coefficients can be be chosen independently , this is the most generic way possible to specify such a goal function : @xmath25 which can also be rewritten with another set of of coefficients @xmath26 as : @xmath27 using @xmath28 ( @xmath29 ) , @xmath30 ( and equation  [ eq : bandwidth ] ) .",
    "note that training a neural processor will obviously change the value of the goal function in equation [ eq : goalfunction ] , but of course also change the relative composition of the entropy in equation [ eq : bandwidth ] .",
    "this decomposition of the entropy and its parametrization are closely modeled on the approach taken by kay and phillips in their formulation of another versatile information theoretic goal function ( `` @xmath31 '' , see below ) for the coherent infomax principle @xcite .    in general , we will choose the formulation used in equation [ eq : goalfunction ] because the conditional entropy does not overlap with the parts in the pi - diagram ( figure [ fig : pid ] ) , but note that the formulation used in equation [ eq : goalfunction_hy ] may be useful when goals with respect to total bandwidth , rather than unused bandwidth , are to be made explicit .",
    "this could for example happen when neuronal plasticity acts to increase to total bandwidth of a neural processor and @xmath32 ( or,@xmath33 and @xmath34 , respectively ) to @xmath0 .",
    "however , since the neural processor has only control over the output @xmath2 , changing the amount of this initial information in the inputs is beyond the scope of its goal function . ] .    in the next sections we introduce coherent infomax and analyze it by means of pid .",
    "we then show how to ( re-)formulate infomax , and predictive coding using specific choices of parameters for @xmath0 .",
    "last , we will introduce a neural goal function , called _ coding with synergy _ , that explicitly exploits synergy for information processing .",
    "the coherent infomax principle ( cip ) proposes an information theoretically defined neural goal function in the spirit of domain - independence laid out in the introduction , and a neural processor implementing this goal function @xcite .",
    "the neural processor operates on information it receives from two distinct types of inputs @xmath3 , @xmath1 and send the results to a single output @xmath2 ( see figure [ fig : neuralprocessor ] ) .",
    "the two distinct types of input in cip were described as driving and modulatory , formally defined by their distinct roles in local processing as detailed in the coherent infomax principles [ 1]-[4 ] , below . here",
    "we will denote the driving input by @xmath3 , and the contextual input by @xmath1 .",
    "in the mammalian brain the driving input @xmath3 includes , but is not limited to , both external information received from the sensors and information retrieved from memory .",
    "the contextual input @xmath1 arises from diverse sources as lateral long - range input from the same or different brain regions , descending inputs from hierarchically higher regions , and input via non - specific thalamic areas .",
    "phillips , clark and silverstein @xcite provide a recent in - depth review of this issue in relation to the evidence for such distinct inputs from several disciplines .    the coherent infomax principle ( cip )",
    "states the following four goals of information processing :    1 .",
    "[ 1]the output @xmath2 should transmit information that is shared between the two inputs , so as to enable the processor to preferentially transmit information from the driving inputs ( @xmath3 ) that is supported by context - carrying information from internal sources elsewhere in the system arriving at input @xmath1 .",
    "this is what the term coherent refers to",
    "[ 2 ] the output @xmath2 could transmit _ some _ information that is only in the driving input @xmath3 , but not in the context , so as to enable that local processors transmit some information that is not related to the information currently available to it from elsewhere in the system .",
    "[ 3 ] the output @xmath2 should minimize transmission of information that is only in the contextual input @xmath1 .",
    "this is necessary to ensure that the effects of the context do not become confounded with the effects of the drive and thereby reduce the reliability of coding .",
    "[ 4 ] the output @xmath2 should be optimally used in terms of bandwidth .    to state these goals more formally , kay and phillips",
    "first decomposed the total entropy of the output , @xmath23 as : @xmath35 where the three - term multi - information @xmath36 is defined as : @xmath37    kay and phillips then re - weighted the terms of this decomposition by coefficients @xmath38 to obtain a _",
    "information theoretic goal function @xmath31 as : @xmath39    here , the first term , @xmath36 , was meant to reflect the information in the output that is shared between the two inputs , the second term the information in the output that was only in the driving input , the third term the information in the output that was only in the contextual input , while the last term represents the unused bandwidth ( see figure [ fig : pidofcip ] for a graphical representation of these terms ) .",
    "below , these assignments will be investigated using pid .    in previous work",
    "@xcite , the goal of _ coherent infomax _ was implemented by setting @xmath40 leading to the objective function @xmath41 . while this objective function appears not to explicitly embody any asymmetry between the influences of the @xmath3 and @xmath1 inputs , it is important to realize that the modulatory role played by the contextual input @xmath1 is expressed through the special form of activation function introduced in phillips et al .",
    "( 1995 ) , and defined in appendix 7.4 .",
    "the possibility of expressing this asymmetry explicitly in the objective function was also discussed in @xcite by taking @xmath42 , @xmath43 leading to the goal function @xmath44 which is a weighted combination of the multi - information and the information between @xmath2 and the driving input @xmath3 conditional on the contextual input @xmath1 .",
    "this last term was meant to represent information that was both in the output @xmath2 and the driving input @xmath3 , but not in the contextual input @xmath1 .",
    "next , we will investigate how this goal function @xmath45 implements the goals [ 1]-[4 ] when these are restated using the language of pid .",
    "we first take the generic goal function @xmath31 from equation [ eq : f ] , that is independent of cip proper , and rewrite it as a sum of mutual information terms and decompose these using pid .",
    "we will sort the resulting decomposition by pid terms and compare this result to the general goal function @xmath0 .",
    "this will tell us about the space of goal functions covered by @xmath31 . knowing this space is highly useful as a working neural network implementation of @xmath31 with learning rules exists ( reviewed in @xcite ) . this implementation can also be used to implement goal functions formulated in the precise pid framework based on @xmath0 , whenever the specific @xmath0 that is of interest lies in the space that can be represented by @xmath31 s .",
    "we begin by decomposing @xmath31 mutual information terms : @xmath46    which , using the pid equations [ eq : totalmi]-[eq : condmi ] , and collecting pid terms , turns into : @xmath47    comparing this to the general pid goal function @xmath0 , we see that the coefficients @xmath48 $ ] and @xmath49 $ ] are linked by the matrix @xmath50 as : @xmath51 since @xmath50 is not invertible , there are parameter choices in terms of @xmath52 that have no counterpart in @xmath53 .",
    "these are described by the complement of the range of this matrix ( the null space of @xmath54 ) .",
    "this one - dimensional subspace is described by reads : @xmath55^t,~\\alpha \\in \\mathbb{r}\\}$ ] . ] : @xmath56^t,~\\alpha \\in \\mathbb{r}\\}~.\\ ] ] the existence of this subspace of coefficients not expressible in terms of @xmath38 s means that it is impossible to prescribe the goal of simultaneously maximizing synergistic and shared information , while minimizing the two unique contributions , and vice versa when using @xmath31 . ultimately , the existence of a subspace not representable by @xmath38 s is a consequence of the fact that pid terms can not be expressed using classic information theory ( while @xmath31 in contrast was defined from classical information theoretic terms only ) .      for the investigation of the _ specific _ goal function @xmath45 ,",
    "we first want to clarify how we understand the four goals listed in the previous section . to this end",
    "we identify them one to one with goals in terms of pid as :    1 .",
    "[ cip1byus ] @xmath57 [ 1 ] : the output should contain as much shared information @xmath58 as possible .",
    "2 .   @xmath57 [ 2 ] : the output could contain some unique information @xmath59 .",
    "3 .   @xmath57 [ 3 ] : the output should minimize unique information @xmath60 .",
    "4 .   @xmath57 [ 4 ] : the unused output bandwidth @xmath61 should be minimized .    with respect to item [ cip1byus ] on this list ,",
    "it is important to recall from section [ sec : pid ] that shared information can arise from mutual information between the sources ( source shared information ) or be created by a mechanism in the processor ( mechanistic shared information ) .",
    "kay and phillips had in mind the first of these two possibilities .",
    "to see whether @xmath45 indeed reflects these goals as stated via pid , we look at the specific choice of parameters , @xmath62 , @xmath63 , @xmath64 , that was used to implement the coherent infomax principle , and find using equations [ eq : totalmi]-[eq : condmi ] ( the reader may also verify this graphically using figure [ fig : pidofcip ] ) : @xmath65    we will now discuss the various contributions to @xmath45 in detail , starting with the shared information , which figures most prominently in the goals [ 1]-[4 ] .    [ [ shared - information ] ] shared information + + + + + + + + + + + + + + + + + +    we see that shared information is maximized .",
    "this shared information contains contributions from mutual information between the sources ( source shared information ) as well as shared information created by mechanisms in the processor ( mechanistic shared information , see the note on item [ cip1byus ] above ) .",
    "the first type of shared information is the one aimed for in [ 1 ] .",
    "thus , for inputs that are not independent the coherent infomax goal function indeed maximizes source shared information as desired .",
    "we will investigate the case of independent inputs below .",
    "[ [ unique - information ] ] unique information + + + + + + + + + + + + + + + + + +    in addition to the shared information , the unique information from the driving input is also maximized , albeit to a lesser degree .",
    "in contrast , synergy between the output and the combined inputs is minimized .",
    "therefore , goals 1 , 2 and 3 are expressed explicitly in this objective function but there is no explicit mention of minimizing the output bandwidth .",
    "[ [ synergistic - information ] ] synergistic information + + + + + + + + + + + + + + + + + + + + + + +    of all the pid terms , synergy is discouraged .",
    "this may at first seem surprising as the mapping of goals of coherent infomax to pid , did not appear to make any explicit statements about synergistic components  _ unless _ one views the transmission of undesirable synergistic components as being an extra component of the bandwidth ( along with @xmath61 ) that is not used in the optimal attainment of goals 1 - 3 .",
    "nevertheless the minimization of synergy serves the original goals of coherent infomax .",
    "this can be seen when we consider that these were formulated for two different types of inputs , driving and modulatory .",
    "for these two types of input , the goal of coherent infomax is to use the modulatory inputs to guide transmission of information about the driving inputs .",
    "synergistic components would transmit information about both driving and modulatory inputs , so transmitting them would be treating the modulatory inputs as driving inputs .",
    "this is clearly undesirable in the setting of coherent infomax .    at a more technical level ,",
    "we note the trade - off in that increasing the value of the parameter @xmath66 towards 1 at once serves to enhance promotion of the unique information from the driving input while simultaneously lessens the pressure to minimize the synergy .",
    "this is a remnant of the term @xmath67 in equation  [ eq : f_cip ] which had been included in order to capture information that was both in @xmath2 and @xmath3 but not in @xmath1 ( i.e. the unique information from the driving input ) , but inadvertently also served to capture the synergy .    in terms of the range of tasks that can be learned by a processor with @xmath45",
    ", the minimization of synergy between the two types of inputs means for example that learning tasks that require a lot of synergy between the inputs , like the ` xor`-function , can not be achieved easily .",
    "it is crucial , however , to realize that discouragement of synergy concerns only relations between drive @xmath3 and modulation @xmath1 .",
    "in contrast , synergistic relations between just the components of a multivariate @xmath68 can be learned by the coherent infomax learning rule .",
    "the ` xor ` between components of @xmath68 for example can be learned reliably if supervised , and still occasionally if not @xcite .",
    "[ [ independent - sources ] ] independent sources + + + + + + + + + + + + + + + + + + +    what remains to be investigated is what the goal functions aims for in the specific case of statistically independent inputs , i.e. when source shared information can not be obtained . in other words",
    ", we may ask whether the coherent infomax processor will maximize mechanistic shared information in this case ?    since the mutual information between the inputs , @xmath69 , is assumed to be zero , then using one of the forms of the multi - information ( eq . [ eq : multiinf ] ) we have @xmath70 and so the multi - information is non - positive .",
    "it follows from the other forms of the multi - information ( eq . [ eq : multiinf ] ) that @xmath71    this implies directly ( compare [ fig : pid]a ) that for independent inputs we must have : @xmath72  an important additional constraint that arises from independent inputs .",
    "thus , in this case the minimization of synergy and the maximization of shared information compete , giving more effective weight to the unique information from the driving input .",
    "nevertheless , limited shared information may exist in this scenario , and if so it will be of the mechanistic type .    in sum , we showed that ( i ) the generic goal function @xmath31 in the coherent infomax principle can not represent all goal functions that are possible in the pid framework using the goal function @xmath0  specifically , @xmath31 lacks one degree of freedom ; ( ii ) for the cip this leads to a weighted maximization of the shared information ( source shared information and mechanistic shared information ) and the unique information from the driving input ; ( iii ) it can be shown that within the space of all possible goal functions @xmath31 it is impossible to maximize synergy and shared information together , while minimizing the two unique information terms , and vice versa ; ( iv ) and for the cip synergy between the driving and modulatory inputs is explicitly discouraged .     and their weighting coefficients in the pid diagram .",
    "( a ) classical unconditional mutual information terms .",
    "( b ) unused bandwidth , weighted by @xmath73 .",
    "( c ) conditional mutual information @xmath74 , weighted by @xmath66 .",
    "( d ) conditional mutual information @xmath75 , weighted by @xmath76 . note the overlap of this contribution with the one from ( c ) .",
    "( e ) the three way information @xmath36 , weighted by @xmath77 . here",
    "the three way information is the checkered minus the striped area .",
    "( f ) this region appears in ( c),(d),(e ) and is weighted accordingly by three coefficients simultaneously ( @xmath77,@xmath66 , @xmath76 ) .",
    "the area in ( f ) is the synergistic mutual information that is also shown in cyan in fig .",
    "[ fig : pid ] . ]",
    "in the this section we will use pid to investigate infomax , another goal function proposed for neural systems , and we will formulate an information - theoretic goal function for a neural processor aimed at predictive coding .      to investigate infomax",
    ", we recall that the goal stated there is to maximize the information in the output about the relevant input @xmath3 , which typically is multivariate @xcite .",
    "this goal function is implicitly designed for situations with limited output bandwidth , i.e. @xmath78 .",
    "not considering a second type of input @xmath1 it is obvious that pid will not contribute to the understanding of infomax .",
    "this changes however if the variables in a multivariate input will be considered separately .",
    "then , it may make sense to ask whether the output information in a given system is actually being maximized predominantly due to unique or synergistic information .",
    "mathematically , the infomax goal can also be represented by using @xmath31 with two types of inputs @xmath3 , @xmath1 , where the information transmitted about @xmath3 is to be maximized .",
    "this can be achieved by choosing @xmath79 to obtain ( e.g. @xcite ) :    @xmath80    the insight to be gained using pid here is that infomax does not incorporate the use of auxiliary variables @xmath1 to extract even more information from @xmath3 via the synergy @xmath81 , nor does it prefer either shared or unique information over the other .      in predictive coding",
    "the goal is to predict inputs @xmath82 using information available from past inputs @xmath83 $ ] ) , meaning that also long term memory in a system may contribute to the predictions . ] .",
    "thus , the processor has to learn a model @xmath84 that yields predictions @xmath85 , such that @xmath86 .",
    "this is the same as maximizing the mutual information between outcome and prediction @xmath87 , at least if we do not care how exactly @xmath88 _ _ represents _ _ is equivalent to all lossless ( re-)encodings of @xmath88 , e.g. in other alphabets , amongst others the alphabet of @xmath82 . ] the prediction . under some mild constraints",
    "are sampled appropriately , such that they form a markov chain @xmath89 the data processing inequality here actually states that trying to tackle this problem information theoretically is trivial , as @xmath87 is maximized by @xmath90 , i.e. all the information we can ever hope to exploit for prediction is already in the raw data ( and it is a mere technicality to extract it in a useful way ) .",
    "the whole problem becomes interesting only when there is some kind of bandwidth limitation on @xmath84 , i.e. when for example @xmath91 has to use the same alphabet as @xmath82 , meaning that we have to state our prediction as a single value that @xmath82 will take .",
    "of course , this actually is the typical scenario in neural circuits .",
    "therefore , we state the main goal of predictive coding as maximizing @xmath87 , under the constraint that @xmath82 and @xmath92 have the same `` bandwidth '' ( the same raw bit content to be precise ) . despite of the goal to maximize a simple mutual information this is not an infomax problem , due to the temporal order of the variables ,",
    "i.e. we need the output @xmath88 before the input @xmath82 is available .",
    "thus , we have to find a different solution to our problem .    to this end",
    ", we suggest that a minimal circuit performing predictive coding will have to perform at least three subtasks , ( i ) produce predictions as output , ( ii ) detect whether there were errors in the predictions , ( iii ) use these for learning . in fig.[fig : predictivecodingcircuit_minimal ] we detail a minimalistic circuit performing these tasks , with subtask ( i ) represented in @xmath88 , subtask ( ii ) in @xmath93 and subtask ( iii ) in @xmath84 .",
    "this circuit assumes the following properties for its neural circuits : ( a ) neurons have binary inputs and outputs , ( b ) information passes through a neuron in one direction , and ( c ) information from multiple inputs can be combined into one output only .",
    "the circuit consists of two separate units : ( 1 ) the error detection unit that operates on _ past _ predictions @xmath94 , obtained via a memory buffer , and past inputs @xmath95 , to create the output @xmath2 via an ` xor ` operation , with @xmath96 indicating an erroneous prediction in the past ; ( 2 ) the prediction unit that has the capability to produce output based on a weighted summation over a vector of past inputs @xmath97 via a weighting function in the model @xmath84 .",
    "@xmath84 will update its weights whenever an error was received .    ,",
    "@xmath98 and ( main ) output @xmath93 . ]",
    "we suggest that the information theoretic goal function of this circuit is simply to minimize the entropy of the output of the error unit , i.e. @xmath23 . in principle",
    ", this would drive the binary output of the circuit either to @xmath99 or to @xmath100 .",
    "of these two possibilities , only the second one is stable , as the constant signaling of the presence of an error will lead to incessant changes in @xmath84 , which in turn will change @xmath2 even for unchanging input @xmath3 .",
    "thus , minimizing @xmath23 should enforce @xmath101 .",
    "therefore , we can formulate an information theoretic goal function of the form @xmath0 if we conceive of the whole circuit as being just _ one _ neural processor with inputs @xmath95 and @xmath97 , and as having the error @xmath2 as its main output . in this case",
    ", we find as a goal function for the predictive coding error ( pce ) : @xmath102 with the weights @xmath103 $ ] using the @xmath104-notation from equation [ eq : goalfunction_hy ] where the total output entropy was made explicit , or equivalently , @xmath105 $ ] .",
    "interestingly , this goal function formally translates to @xmath106 $ ] , or @xmath107 .",
    "this gives hope that one can translate the established formalism for @xmath31 to the present case by taking into account that the original architecture behind @xmath31 is augmented here by an additional ` xor ` subunit .",
    "learning of the circuit s goal function may have to proceed in two steps if we do not have subunits able to perform ` xor ` at the beginning . in this case , the `` ` xor ` ''",
    "subunit will first have to learn to perform its function .",
    "this can be achieved by maximizing the synergy of two uniform , random binary inputs and the subunit s output @xmath2 .",
    "after this initial learning the ` xor`-subunit is ` frozen ' and learning of predictions can proceed to minimize @xmath23 .",
    "one conceivable mechanism for this would be to use learning based on coincidences between input bits in @xmath108 and the error bit @xmath2 .",
    "we note that this goal function is not entirely new , as the idea of making the output of a processing unit as constant as possible in learning has been used before in various implementations ( e.g. @xcite ) .",
    "it is also closely related to the homeostatic goals pursued by the free energy minimization principle @xcite .",
    "we have merely added here a generic minimal circuit diagram and the information theoretic interpretation to these previous approaches .",
    "also , note that the actual prediction @xmath85 must be implicitly part of the information theoretic goal function , as the goal function we suggest here would be nonsensical on many other circuits .    as a next level of complication",
    "one may consider that the predictions @xmath1 that are created within our minimal circuit are sent back to the source of the input @xmath3 to interact with it there .",
    "one such interaction scheme will be studied in the next section .",
    "so far the goal functions investigated in our unifying framework @xmath0 had in common that maximization of synergy did not appear as a desirable goal .",
    "this may historically be simply due to the profound mathematical difficulties that had to be overcome in the definition of synergistic information . in this section",
    "we will therefore show how synergy naturally arises in a generalization of ideas from efficient coding by pid .",
    "we will call the goal function simply _ coding with synergy ( cws)_.    the neural coding problem that we will investigate here is closely related to predictive coding discussed in the previous section .",
    "however , in contrast to predictive coding where the creation of predictions was in focus , here we focus on possible uses of prior ( or contextual ) information from @xmath1 , be it derived from predictions or by any other means . in other words , we here simply assume that there is ( valid ) prior information in the system that does not have to be extracted from the ongoing input stream @xmath3 by our neural processor .",
    "moreover , we assume that there is no need to waste bandwidth and energy on communicating @xmath1 as this information is already present in the system .",
    "last , we assume that we want to pass as much of the information in @xmath3 as possible , as well as of the information created synergistically by @xmath3 and @xmath1 .",
    "this synergistic information will arise for example when @xmath1 serves to decode or disambiguate information in @xmath3 .",
    "looking at the pid diagram ( fig .",
    "[ fig : pid ] ) one sees that in this setting it is optimal to minimize @xmath109 and the unused bandwidth @xmath61 while maximizing the other terms .",
    "this leads to :    @xmath110    or @xmath111 $ ] .",
    "the important point here is that this is different from maximizing just @xmath112 , as this would omit the shared information , i.e. we would lose this part of the information in @xmath3 .",
    "the goal function @xmath113 is also different from just maximizing @xmath114 , as this would omit the synergistic information , i.e. the possibility to decode information from @xmath3 by means of @xmath1 .",
    "furthermore , there is no corresponding goal function @xmath31 here in terms of classical information theoretic measures .",
    "this can easily be proven by noting that @xmath111 $ ] has a non - zero projection in @xmath115 ( equation  [ eq : rangecomplement ] ) .",
    "in other words , there is no @xmath116 that satisfies equation [ eq : omegagamma ] .",
    "given there were bandwidth constraints on @xmath2 , one might want to preferentially communicate one or two of the positively weighted terms in equation [ eq : cws ] .",
    "the natural choice here is to favor synergy and unique information about @xmath3 , because the shared information with @xmath1 is already in the system .",
    "if just one contribution can be communicated this leaves us with three choices .",
    "we will quickly discuss the meaning of each here : first , focusing on the unique information @xmath12 emphasizes the surprising information in @xmath3 , because this is the information that is not yet in the system at all ( i.e. not in @xmath1 ) ; second , focusing on the shared information @xmath117 basically leads to coherent infomax ; third , focusing on the synergistic information @xmath118 emphasizes information which can only be obtained when putting together prior knowledge in @xmath1 and incoming information @xmath3 - this would be the extreme case of cws .",
    "this case should arise naturally in binary error computation , e.g. in error units suggested as integral parts of certain predcitive coding architectures ( see @xcite for a discussion of error units , also compare the xor unit in figure [ fig : predictivecodingcircuit_minimal ] ) .    a classic example for this last coding strategy would be cryptographic decoding . here",
    ", the mutual information between cypher text ( serving as input @xmath3 ) and plain text ( serving as output @xmath2 ) is close to zero , i.e. @xmath119 , given randomly chosen keys and a well performing cryptographic algorithm .",
    "nevertheless the mutual information between the two , given keys ( serving as input @xmath1 ) , is the full information of the plain text , i.e. @xmath120 , assuming the unused bandwidth is zero ( @xmath121 ) .",
    "as the mutual information between key and plain text should also be zero ( @xmath122 ) we see that in this case the full mutual information is synergistic : @xmath123 . in a similar vein ,",
    "any task in neural systems that involves an arbitrary key - dependent mapping between information sources  as in the above cryptographic example  will involve cws .",
    "one such task would be to read a newspaper printed in latin characters ( which could be in quite a range of languages ) to get knowledge about the current state of the world ( or at least some aspects of it ) .",
    "visually inspecting the text , without the information incorporated in the rules of the unknown written language used will not reveal information about the world .",
    "yet , having all the information on the rules of written language , without having a specific text will also not reveal anything about the world . to obtain this knowledge",
    "we need , both , the text of the newspaper and the language - specific information how written words map to possible states of the world .",
    "a corollary of the properties of synergistic mutual information is that when a neuron s inputs are investigated individually they will seem unrelated to the output  to the extent that synergistic information is transmitted in the output .",
    "therefore , the minimal configuration of neuronal recordings needed to investigate the synergistic goal fucntion is a triplet of two inputs and one output .",
    "thus , though coding with synergy has not been prominent in empirical reports to date , it might become more frequently detected as dense and highly parallel recordings of neuronal acticity become more widely available .",
    "the general setting of coding under prior knowledge discussed here is also related to barlow s efficient coding hypothesis @xcite if we take the prior information @xmath1 to be information about which inputs to our processor are typical for the environment it lives in .",
    "we here basically generalize barlow s principle by dropping reference to what the input or the prior knowledge are about .",
    "last , this goal function seems significant to us as synergy is seen by some authors as useful in an formal definition of information modification ( e.g. @xcite ) .",
    "thus synergy is a highly useful measure in the description of neural processor with two or more inputs ( or one input and an internal state ) , as it taps into the potential of the processor to genuinely _ modify _ information",
    "in this study we introduced partial information decomposition ( pid ) as a universal framework to describe and compare neural processors in a domain - independent way .",
    "pid is indispensable for the information theoretic analysis of systems where two ( or more ) inputs are combined to one output , because it allows to decompose the information in the output into contributions provided either uniquely by any one of the inputs alone ( unique information ) , by either of them ( shared information ) , or only by both of them jointly ( synergistic information ) . using pid , the information processing principles of the processor can be quantitatively described by specific coefficients @xmath52 for each of the pid contributions in a pid - based goal function @xmath124 , which the processor maximizes .",
    "this framework is useful in several ways .",
    "first , and perhaps most importantly , it allows the principled comparison of existing neural goal functions , such as infomax , coherent infomax , predictive coding , and efficient coding .",
    "second , it aids in the design of novel neural goal functions .",
    "here we presented a specific example , coding with synergy ( cws ) , that exploits synergy to maximize the information that can be obtained from the input when prior information is available in the system .",
    "note , however , that the actual implementation of a neural circuit maximizing the desired goal function is not provided by the new framework and will have to be constructed on a case by case basis at the moment .",
    "this is in contrast to coherent infomax where a working implementation is known .",
    "third , applying this framework to neural recordings may help us understand better how neural circuits that are far away from sensory and motor periphery , and for which we do not have the necessary semantics , function .",
    "currently , the applicability of our framework rests on the assumption that a neural processor with _ two _ inputs is a reasonable approximation of a neuron or microcircuit .",
    "of course , neurons typically have many more inputs than just two .",
    "however , if such inputs naturally fall into two groups , e.g. being first integrated locally in two groups on the dendrites before being brought to interact at the soma , then indeed the two input processor is a useful approximation .",
    "if , moreover , these integrated inputs are measured before their fusion in the soma , then the formalism of goal functions presented here will allow us to assess the function of this neuron in a truly domain independent way , relying only on information that is also available to the neuron itself .",
    "for example , two such spatially segregated and separately integrated inputs can be distinguished on pyramidal cells ( fig .",
    "[ fig : neuralprocessor ] ) .",
    "pyramidal cells are usually highly asymmetric and consist of a cell body with basal dendrites and an elongated apical dendrite that rises to form a distal dendritic tuft in the superficial cortical layers .",
    "thus , the inputs are spatially segregated into basal / perisomatic inputs , and inputs that target the apical tuft .",
    "intracellular recordings indicate that there are indeed separate integration sites for each of these two classes of input , and that there are conditions in which apical inputs amplify ( i.e. modulate ) responses to the basal inputs in a way that closely resembles the schematic two - input processor shown in fig .",
    "[ fig : neuralprocessor ] .",
    "there is also emerging evidence that these segregated inputs have driving and modulatory functions and are combined in a mechanism of apical amplification of basal inputs  resembling the coherent infomax goal function .",
    "direct and indirect evidence on this apical amplification and its cognitive functions is reviewed by phillips [ submitted to this special issue ] .",
    "that evidence shows that apical amplification occurs within pyramidal cells in the superficial layers , as well as in layer 5 cells , and suggests that it may play a leading role in the use of predictive inferences to modulate processing .",
    "which of the goal functions proposed here , e.g infomax , coherent infomax , or coding with synergy a neural processor actually performs is an empirical question that must be answered by analyzing pid footprints of @xmath0 obtained from data recorded in neural processors . at present",
    "this is still a considerable challenge when applied to the level of single cells or microcircuits because this requires the separate recording of at least one output and two inputs , wich must moreover be of different type in the case of coherent infomax .",
    "next , the pid terms have to be estimated from data , instead of distributions that are known .",
    "this type of estimation is still a field of ongoing research at present .",
    "overcoming these challenges will yield in - depth understanding of , for example , the information processing of the layer 5 cell described above in terms of pid , and elucidate which of the potential goal functions is implemented in such a neuron .    in the spirit of the framework proposed here , classical information theoretic techniques",
    "have already been applied to psychophysical data to search for coherent infomax - like processing at this level @xcite .",
    "these studies confirmed for example that attentional influences are modulatory , and showed how modulatory interactions can be distinguished from interactions that integrate multiple driving input streams .",
    "these result are a promising beginning of a more large scale analysis of neuronal data at all levels with information theoretic tools , such as pid .    further information theoretic insight relevant to predictive processing",
    "may also be gained by relating the predictable information in a neural processor s inputs ( measured via local active information storage @xcite ) to the information transmitted to its output ( measured via transfer entropy @xcite , or local transfer entropy @xcite ) to investigate whether principles of predictive coding apply to the information processing in neurons .",
    "this is discussed in more detail in @xcite .",
    "we here argued that the understanding of neural information processing will profit from taking a neural perspective , focusing on the information entering and exiting a neuron , and stripping away semantics imposed by the experimenter ",
    "semantics that is not available to a neuron .",
    "we suggest that the necessary analyses are best carried out in an information theoretic framework , and that this framework must be able to describe the processing in a multiple input system to accommodate neural information processing .",
    "we find that pid provides the necessary measures , and allows to compare most if not all theoretically conceivable neural goal functions in a common framework .",
    "moreover , pid can also be used to design new goal functions from first principles .",
    "we demonstrated the use of this technique in understanding neural goal functions proposed for the integration of contextual information ( coherent infomax ) , the learning of predictions ( predictive coding ) , and introduced a novel one for the decoding of input based on prior knowledge called coding with synergy ( cws ) .",
    "the authors would like to thank nils bertschinger for inspiring discussions on partial information decomposition and for reading an earlier version of the manuscript .",
    "mw received support from loewe grant `` neuronale koordination forschungsschwerpunkt frankfurt ( neff ) '' .",
    "vp received financial support from the german ministry for education and research ( bmbf ) via the bernstein center for computational neuroscience ( bccn ) gttingen under grant no . 01gq1005b .",
    "# 1`#1`urlprefixhref # 1#2#2 # 1#1    m.  wibral , j.  t. lizier , v.  priesemann , http://journal.frontiersin.org/article/10.3389/frobt.2015.00005/abstract[bits from brains for biologically - inspired computing ] , computational intelligence 2 ( 2015 ) 5 , 00000 . http://dx.doi.org/10.3389/frobt.2015.00005 [ ] .",
    "j.  hohwy , the predictive mind , oxford university press , 2013 .",
    "a.  clark , http://journals.cambridge.org/article_s0140525x12000477[whatever next ?",
    "predictive brains , situated agents , and the future of cognitive science ] , behavioral and brain sciences 36  ( 03 ) ( 2013 ) 181204 .",
    "http://dx.doi.org/10.1017/s0140525x12000477 [ ] .",
    "http://journals.cambridge.org/article_s0140525x12000477    t.  s. lee , d.  mumford , http://view.ncbi.nlm.nih.gov/pubmed/12868647 ;    http://www.bibsonomy.org/bibtex/2136bababe707c0ef6f612fdedaeaacfa/meduz[hierarchical bayesian inference in the visual cortex .",
    "] , journal of the optical society of america .",
    "a , optics , image science , and vision 20  ( 7 ) ( 2003 ) 14341448 .",
    "http://view.ncbi.nlm.nih.gov/pubmed/12868647 ;    http://www.bibsonomy.org/bibtex/2136bababe707c0ef6f612fdedaeaacfa/meduz[http://view.ncbi.nlm.nih.gov/pubmed/12868647 ;    http://www.bibsonomy.org/bibtex/2136bababe707c0ef6f612fdedaeaacfa/meduz ]    r.  p. rao , d.  h. ballard , http://dx.doi.org/10.1038/4580[predictive coding in the visual cortex : a functional interpretation of some extra - classical receptive - field effects .",
    "] , nat neurosci 2  ( 1 ) ( 1999 ) 7987 . http://dx.doi.org/10.1038/4580 [ ] .",
    "http://dx.doi.org/10.1038/4580    r.  linsker , self - organisation in a perceptual network , ieee computer ( 1988 ) 105117 .",
    "a.  j. bell , t.  j. sejnowski , http://www.bibsonomy.org/bibtex/20dabc7935b10ef7fa0df72a1ec2f4020/meduz[the ` independent components ' of natural scenes are edge filters ] , vision research 37  ( 23 ) ( 1997 ) 332738 .",
    "http://www.bibsonomy.org/bibtex/20dabc7935b10ef7fa0df72a1ec2f4020/meduz    t.  tanaka , t.  kaneko , t.  aoyagi , http://dblp.uni-trier.de/db/journals/neco/neco21.html#tanakaka09 ;    http://dx.doi.org/10.1162/neco.2008.03-08-727 ;    http://www.bibsonomy.org/bibtex/266091b497b7737276a8d8f2d85327554/dblp[recurrent infomax generates cell assemblies , neuronal avalanches , and simple cell - like selectivity .",
    "] , neural computation 21  ( 4 ) ( 2009 ) 10381067 .",
    "http://dblp.uni-trier.de/db/journals/neco/neco21.html#tanakaka09 ;    http://dx.doi.org/10.1162/neco.2008.03-08-727 ;    http://www.bibsonomy.org/bibtex/266091b497b7737276a8d8f2d85327554/dblp[http://dblp.uni-trier.de/db/journals/neco/neco21.html#tanakaka09 ;    http://dx.doi.org/10.1162/neco.2008.03-08-727 ;    http://www.bibsonomy.org/bibtex/266091b497b7737276a8d8f2d85327554/dblp ]    w.  phillips , j.  kay , d.  smyth , http://www.bibsonomy.org/bibtex/28911c0fa10104682d5ef293908bace70/brian.mingus[the discovery of structure by multi - stream networks of local processors with contextual guidance .",
    "] , network : computation in neural systems 6 ( 1995 ) 225246 .",
    "m.  larkum , a cellular mechanism for cortical associations : an organizing principle for the cerebral cortex . , trends neurosci .",
    "36  ( 3 ) ( 2013 ) 14151 .",
    "http://dx.doi.org/10.1016/j.tins.2012.11.006 [ ] .",
    "l.  von melchner , s.  l. pallas , m.  sur , http://www.bibsonomy.org/bibtex/2aae98ae21e3a4f2ecd83255dec13fdb4/brian.mingus[visual behaviour mediated by retinal projections directed to the auditory pathway ] , nature 404 ( 2000 ) 871876 .",
    "http://www.bibsonomy.org/bibtex/2aae98ae21e3a4f2ecd83255dec13fdb4/brian.mingus    schreiber , measuring information transfer , phys rev lett 85  ( 2 ) ( 2000 ) 461464 .",
    "j.  t. lizier , m.  prokopenko , a.  y. zomaya , local information transfer as a spatiotemporal filter for complex systems . , phys rev e 77  ( 2 pt 2 ) ( 2008 ) 026110 .",
    "j.  t. lizier , m.  prokopenko , a.  y. zomaya , local measures of information storage in complex distributed computation , information sciences 208 ( 2012 ) 3954 .",
    "j.  t. lizier , b.  flecker , p.  l. williams , towards a synergy - based approach to measuring information modification , in : artificial life ( alife ) , 2013 ieee symposium on , ieee , 2013 , pp . 4351 .",
    "e.  t. jaynes , probability theory : the logic of science , cambridge university press , 2003 .",
    "l. williams , r.  d. beer , nonnegative decomposition of multivariate information , arxiv preprint arxiv:1004.2515 .",
    "m.  harder , c.  salge , d.  polani , bivariate measure of redundant information .",
    ", phys rev e stat nonlin soft matter phys 87  ( 1 ) ( 2013 ) 012130 .",
    "n.  bertschinger , j.  rauh , e.  olbrich , j.  jost , n.  ay , http://www.mdpi.com/1099-4300/16/4/2161[quantifying unique information ] , entropy 16  ( 4 ) ( 2014 ) 21612183 , 00002 .",
    "http://www.mdpi.com/1099-4300/16/4/2161    v.  griffith , c.  koch , quantifying synergistic mutual information , in : m.  prokopenko ( ed . )",
    ", guided self - organization : inception , vol .  9 of emergence , complexity and computation , springer ,",
    "berlin / heidelberg , 2014 , pp .",
    "159190 .",
    "h.  cuntz , f.  forstner , a.  borst , m.  husser , http://dblp.uni-trier.de/db/journals/ploscb/ploscb6.html#cuntzfbh10 ;    http://dx.doi.org/10.1371/journal.pcbi.1000877 ;    http://www.bibsonomy.org/bibtex/2c90df6ccace51669cace8cac2cff2204/dblp[one rule to grow them all : a general theory of neuronal branching and its practical application . ] , plos computational biology 6  ( 8) . http://dblp.uni-trier.de/db/journals/ploscb/ploscb6.html#cuntzfbh10 ;    http://dx.doi.org/10.1371/journal.pcbi.1000877 ;    http://www.bibsonomy.org/bibtex/2c90df6ccace51669cace8cac2cff2204/dblp[http://dblp.uni-trier.de/db/journals/ploscb/ploscb6.html#cuntzfbh10 ;    http://dx.doi.org/10.1371/journal.pcbi.1000877 ;    http://www.bibsonomy.org/bibtex/2c90df6ccace51669cace8cac2cff2204/dblp ]    j.  kay , http://dl.acm.org/citation.cfm?id=348227.348246[neural networks for unsupervised learning based on information theory ] , in : j.  w. kay , d.  m. titterington ( eds . ) , statistics and neural networks , oxford university press , inc .",
    ", new york , ny , usa , 1999 , ch . neural networks for unsupervised learning based on information theory , pp .",
    "j.  w. kay , w.  a. phillips , http://dx.doi.org/10.1007/s11538-010-9564-x[coherent infomax as a computational goal for neural systems .",
    "] , bull math biol 73  ( 2 ) ( 2011 ) 344372 . http://dx.doi.org/10.1007/s11538-010-9564-x [ ] .",
    "j.  rauh , n.  bertschinger , e.  olbrich , j.  jost , http://arxiv.org/abs/1404.3146[reconsidering unique information : towards a multivariate information decomposition ] , arxiv:1404.3146 [ cs , math]00001 arxiv : 1404.3146 .",
    "t.  m. cover , j.  a. thomas , elements of information theory , wiley - interscience , new york , ny , usa , 1991 .",
    "j.  kay , d.  floreano , w.  a. phillips , http://www.sciencedirect.com/science/article/pii/s089360809700110x[contextually guided unsupervised learning using local multivariate binary processors ] , neural networks 11  ( 1 ) ( 1998 ) 117140 .",
    "http://dx.doi.org/10.1016/s0893-6080(97)00110-x [ ] .",
    "http://www.sciencedirect.com/science/article/pii/s089360809700110x    w.  a. phillips , a.  clark , s.  m. silverstein , on the functions , mechanisms , and malfunctions of intracortical contextual modulation , neuroscience and biobehavioral reviews 52 ( 2015 ) 120 .",
    "http://dx.doi.org/10.1016/j.neubiorev.2015.02.010 [ ] .",
    "r.  wyss , p.  knig , p.  f. m.  j. verschure , http://dx.doi.org/10.1371/journal.pbio.0040120[a model of the ventral visual system based on temporal stability and local memory .",
    "] , plos biol 4  ( 5 ) ( 2006 ) e120 . http://dx.doi.org/10.1371/journal.pbio.0040120 [ ] .",
    "w.  b. cannon , the wisdom of the body , norton , new york , 1932 .",
    "r.  der , u.  steinmetz , f.  pasemann , homeokinesis - a new principle to back up evolution with learning , in : computational intelligence for modelling , control , and automation , vol .",
    "55 of concurrent systems engineering series , ios press , amsterdam , 1999 , pp .",
    "k.  friston , j.  kilner , l.  harrison , http://dx.doi.org/10.1016/j.jphysparis.2006.10.001[a free energy principle for the brain . ]",
    ", j physiol paris 100  ( 1 - 3 ) ( 2006 ) 7087 .",
    "k.  j. friston , k.  e. stephan , http://dx.doi.org/10.1007/s11229-007-9237-y[free-energy and the brain . ] , synthese 159  ( 3 ) ( 2007 ) 417458 . http://dx.doi.org/10.1007/s11229-007-9237-y    k.  friston , http://dx.doi.org/10.1016/j.tics.2009.04.005[the free - energy principle : a rough guide to the brain ? ] , trends cogn sci 13  ( 7 ) ( 2009 ) 293301 .",
    "h.  b. barlow , possible principles underlying the transformations of sensory messages , in : w.  a. rosenblith ( ed . ) , sensory communication , mit press , 1961 .",
    "l. williams , r.  d. beer , generalized measures of information transfer , arxiv preprint arxiv:1102.1507 .",
    "w.  a. phillips , b.  j. craven , interactions between coincident and orthogonal cues to texture boundaries , perception & psychophysics 62  ( 5 ) ( 2000 ) 10191038 .",
    "a.  b. barrett , http://arxiv.org/abs/1411.2832[an exploration of synergistic and redundant information sharing in static and dynamical gaussian systems ] , arxiv:1411.2832 ( 2014 ) .",
    "http://arxiv.org/abs/1411.2832 [ ] .",
    "e.  olbrich , n.  bertschinger , j.  rauh , http://www.mdpi.com/1099-4300/17/5/3501[information decomposition and synergy ] , entropy 17  ( 5 ) ( 2015 ) 35013517 . http://dx.doi.org/10.3390/e17053501 [ ] .",
    "we write probability distributions of random variables @xmath3 , @xmath1 , @xmath2 as @xmath125 wherever we re talking about the distribution as an object itself , i.e. when we treat a distribution @xmath125 as a point of in the space of all joint probability distributions of these three variables . to signify a value that such a distribution takes for specific realizations @xmath126 , @xmath127 , @xmath128 of these variables",
    ", we either write @xmath129 , or use the shorthand @xmath130 .      to highlight the necessity of the notation used here and to deepen the understanding of the various partial information terms we give the following example where we add explicit set notation for clarity : @xmath131 @xmath132 here , the first expression ( [ eq : pidallshared ] ) asks for the information that _ all _ four right hand side variables share about @xmath2 , while the second expression ( [ eq : pidoneset ] ) asks for the information that the set @xmath133 shares ( with itself ) about @xmath2 . by the self - redundancy axiom .",
    "this axiom becomes important in extensions of pid to more than two input variables . ]",
    "@xcite this is just the mutual information between the set @xmath133 and @xmath2 . in the next example in equations [ eq : pidfoursets ] and [ eq :",
    "pidtwosets ] we ask in equation [ eq : pidtwosets ] for the information shared between the two sets of variables @xmath134 and @xmath135 , meaning that the information about @xmath2 can be obtained from either @xmath136 , or @xmath137 , or from them considered jointly , but must also be found in either @xmath5 or @xmath138 or in the two of them considered jointly .",
    "this means in the latter case information held jointly by @xmath136 and @xmath137 about @xmath2 is considered if it is shared with information about @xmath2 obtained from any combination of @xmath5 , @xmath138 , including their synergistic information .",
    "we here briefly present the learning rules for gradient ascent learning of neural processor learning to maximize the goal function @xmath31 from equation [ eq : f ] .",
    "we only consider the basic case of a single a neural processor with binary output @xmath2 here @xcite . the inputs to this processor",
    "are partitioned into two groups @xmath183 , representing the driving inputs and @xmath184 , representing the contextual inputs .",
    "these inputs enter the information theoretic goal function @xmath185 via their weighted sums per group as : @xmath186    the inputs affect the output probability of the processor via an activation function @xmath187 as : @xmath188 for the sake of deriving general learning rules , @xmath136 may be any general , differentiable nonlinear function of the input .",
    "note that @xmath189 fully determines the information theoretic operation that the processor performs .",
    "@xmath189 is a function of the weights used in the summation of the inputs .",
    "thus , learning a specific information processing goal can only be done via learning these weights  assuming that the input distributions of the processor can not be changed .",
    "learning rules for these weights will now be presented .    to write the learning rules in concise form , the additional definitions : @xmath190 are introduced to abbreviate the expectation of the activation across all input vectors @xmath191 $ ] , @xmath192 $ ] .",
    "these expectations are functions of the input distributions as well as of the weights and have to be recomputed after weight changes . using online learning therefore necessitates computing these expectations over a suitable time window of past inputs . to write the learning rules in concise notation a non - linear floating average @xmath193 of the above expectations",
    "is introduced as :        last , we note that for the specific implementations of cip , the activation function was chosen as : @xmath200~,\\ ] ] with @xmath201 and @xmath202 , and with @xmath126 , @xmath127 being realizations of @xmath3 , @xmath1 from equations [ eq : sumdrive ] , [ eq : sumcontext ] .",
    "this specific activation function @xcite guarantees that :    * zero output activation can only be obtained if the summed driving input @xmath3 is zero .",
    "* for zero summed contextual input @xmath1 , the output equals the summed driving input . * a summed contextual input of the same sign as the summed driving input leads to an amplification of the output .",
    "the reverse holds for unequal signs . *",
    "the sign of the output is equal to the sign of the summed driving input ."
  ],
  "abstract_text": [
    "<S> in many neural systems anatomical motifs are present repeatedly , but despite their structural similarity they can serve very different tasks . a prime example for such a motif is the canonical microcircuit of six - layered neo - cortex , which is repeated across cortical areas , and is involved in a number of different tasks ( e.g.sensory , cognitive , or motor tasks ) . </S>",
    "<S> this observation has spawned interest in finding a common underlying principle , a goal function , of information processing implemented in this structure . by definition </S>",
    "<S> such a goal function , if universal , can not be cast in processing - domain specific language ( e.g. edge filtering , working memory ) . </S>",
    "<S> thus , to formulate such a principle , we have to use a domain - independent framework . </S>",
    "<S> information theory offers such a framework . </S>",
    "<S> however , while the classical framework of information theory focuses on the relation between one input and one output ( shannon s mutual information ) , we argue that neural information processing crucially depends on the combination of _ multiple _ inputs to create the output of a processor . to account for this </S>",
    "<S> , we use a very recent extension of shannon information theory , called partial information decomposition ( pid ) . </S>",
    "<S> pid allows to quantify the information that several inputs provide individually ( unique information ) , redundantly ( shared information ) or only jointly ( synergistic information ) about the output . </S>",
    "<S> first , we review the framework of pid </S>",
    "<S> . then we apply it to reevaluate and analyze several earlier proposals of information theoretic neural goal functions ( predictive coding , infomax and coherent infomax , efficient coding ) . </S>",
    "<S> we find that pid allows to compare these goal functions in a common framework , and also provides a versatile approach to design new goal functions from first principles . building on this , we design and analyze a novel goal function , called coding with synergy , which builds on combining external input and prior knowledge in a synergistic manner . </S>",
    "<S> we suggest that this novel goal function may be highly useful in neural information processing .    </S>",
    "<S> information theory , unique information , shared information , synergy , redundancy , predictive coding , neural coding , coherent infomax , neural goal function </S>"
  ]
}