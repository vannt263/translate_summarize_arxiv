{
  "article_text": [
    "this paper introduces a novel approach to functional data registration within a bayesian hierarchical model . in this model , both registered and warping functions are modeled in terms of gaussian processes with registration described by restrictions on the covariance of the registered functions .",
    "this enables both mcmc and variational bayes methods to be employed for estimation and prediction .",
    "our model for registration extends current inferential procedures to include both 1 ) functional prediction in the context of registration and 2 ) registration and smoothing in one model .",
    "the primary advantage to our proposed registration model in comparison to current registration methods is that it provides a probabilistic framework in which new observations can be considered . assuming a new unregistered function has been partially recorded , with this framework , we can obtain estimates of the registered partial function and also the corresponding partial warping function . using these estimates , the complete registered function , the complete warping function , and the complete unregistered function can be predicted .",
    "details of the prediction model can be found in section [ sec : pred ] . to the authors knowledge",
    "this is the first time functional prediction is considered in the context of registration .",
    "additionally , our model can be extended to allow for noisy observations .",
    "most current registration methods consider functional regularization as a pre - processing step with the exception of @xcite .",
    "however , in the paper by @xcite , the authors maximum likelihood approach to registering latent functions does not provide an easy way to quantify variability in the estimates of the registered functions . in appendix c.3",
    ", we provide an illustration of how smoothing functions in a pre - processing step can significantly underestimate the variability in the estimates of the registered functions .",
    "the simplest definition of functional data registration is any algorithm that aligns functions in a way that eliminates all phase variability between functions ( @xcite ) . without registration ,",
    "basic summary statistics such as the sample mean and covariance are less interpretable as time variation between significant features in the functions tends to dampen the amplitude variation in these features .",
    "furthermore , the average timing of significant features may also be of interest and is difficult to obtain under traditional methods of analyzing functional data",
    ". there has been much recent interest in proper ways to define and measure registration as well as in developing registration methods with desirable statistical properties .    the evolution of registration dates back to @xcite where the authors use a dynamic programming algorithm for landmark registration .",
    "landmark registration was again considered by @xcite and @xcite . in 1997 , a new cost function for functional registration was introduced by @xcite .",
    "a significant advancement in registration literature can be traced to @xcite and @xcite where the authors introduce global registration procedures , and ramsey considers the use of a flexible family of monotone warping functions .",
    "parametric and b - spline base warping functions are considered by @xcite and @xcite , respectively .",
    "nonparametric maximum likelihood approaches to registration are considered by both @xcite and @xcite . a moments based approach to registration",
    "is introduced by @xcite . @xcite",
    "propose pairwise curve synchronization .",
    "the first bayesian approach to registration can be found in @xcite .",
    "registration to principal components is considered by @xcite . finally , with regard to improvements in registration , the recent work by @xcite offers the most comprehensive framework for registration to date .",
    "much of the focus in combining registration with other types of inference in one model has been in the area of functional data clustering and registration .",
    "current work in this area can be found in @xcite , @xcite , and also a bayesian approach in @xcite .",
    "recent work by @xcite includes a model for functional smoothing and registration .",
    "while these extensions to registration procedures offer additional tools for functional data analysis , they tend to focus less on high - quality registration .    in this paper",
    ", we develop bayesian hierarchical models that address both areas of development in registration procedures .",
    "first , a model is proposed to register functional data that gives estimates that compare favorably with those from the best current registration methods available , notably , @xcite .",
    "then , we demonstrate how this model can be extended to incorporate other inferential procedures .",
    "the two examples provided in this paper are extensions for both a functional prediction model and a model for simultaneous registration and smoothing",
    ".    this paper also addresses the computational issues associated with high - dimensional bayesian hierarchical models . to this end",
    ", we propose an alternative algorithm to variational bayes approximation that can be used for models in which the full conditional distributions of a subset of the parameters are not from a known parametric family . to distinguish our algorithm from pure variational bayes ,",
    "in this paper we will refer to this approximate inference procedure as adapted variational bayes ( avb ) .",
    "this paper is organized as follows .",
    "section [ sec : meth ] presents our basic registration model .",
    "the adapted variational bayes algorithm is discussed in detail in section [ sec : vb ] .",
    "a comparison of results from our model to current methods can be found in section [ sec : om ] .",
    "additionally , a comparison of results obtained using avb and those given by mcmc can be found in [ sec : mcmc ] .",
    "the prediction model is presented in section [ sec : pred ] . in section",
    "[ sec : elnino ] , the prediction model is used to forecast the future trajectory of sea - surface temperatures that are associated with the el - nio phenomenon . an adaptation to our model that allows for noisy data",
    "is found in appendix c. finally , a discussion is found in section [ sec : conc ] .",
    "the functional registration models proposed in this paper are foremost designed to extend and improve on the minimum eigenvalue registration criterion for continuous registration first introduced by @xcite .",
    "accordingly , we will consider two functions perfectly registered if the variation between the two functions can be described entirely in terms of one functional direction - the _ target function_. our method of registration improves on ramsay and li s procrustes method , @xcite , by implicitly accounting for vertical shifts between registered functions and by allowing the target curve to evolve throughout the registration procedure . in section [ sec : comp ]",
    ", we will demonstrate how using the minimum eigenvalue criterion under these conditions provides a more complete curve registration .",
    "our results are comparable to those of @xcite .",
    "the theoretical basis for modeling functional data as gaussian processes in a hierarchical bayesian environment is established in @xcite . in our registration model ,",
    "each registered function , @xmath0 , @xmath1 , is the composition of an observed unregistered function , @xmath2 , with an unknown warping function , @xmath3 , over some fixed time domain @xmath4 $ ] .",
    "the function @xmath3 is represented as @xmath5 to enforce its monotonicity where @xmath6 is specified as having a gaussian process distribution , resulting in a functional random effect . in this paper",
    ", we will refer to @xmath6 as the _ base function _ associated with warping function @xmath3 .",
    "the base functions are non - parametrically specified for optimal registration .",
    "we , however , impose the following restrictions on the warping functions :    1 .",
    "@xmath7 , 2 .",
    "@xmath8 , and 3 .",
    "if @xmath9 , then @xmath10 for all @xmath11 .    restrictions ( 1 ) and ( 3 ) are built into the definition of @xmath3 .",
    "restriction ( 2 ) is imposed through the characteristic function in the expression for the prior defined for each base function , @xmath6 .",
    "note that @xmath12 corresponds to the identity warping , @xmath13 .",
    "an important feature of our definition of the warping functions is that it defines an identifiable relationship between @xmath14 and @xmath6 which is necessary for predicting future outcomes of curves that are only partially observed . in section [ sec : pred ] is a more thorough discussion of the prediction model .",
    "we also model each registered function , @xmath0 , @xmath15 , as a gaussian process such that @xmath16 here , @xmath17 is the _ target function_. the target function serves as the primary functional direction in which the registered functions vary . accordingly , the above covariance function , @xmath18 , is defined to penalize all variation in the registered functions beyond a scaling and vertical shifting of the target function ( the function - specific mean ) . in these models we will define @xmath19 as a registration parameter that determines the severity of this penalty .    given a sample of unregistered functions , @xmath2 , @xmath20 , defined over the interval @xmath4 $ ] , we are interested in estimating the warping functions , @xmath3 , the shifting and scaling parameters , @xmath21 and @xmath22 , the target curve , @xmath17 , and the registered functions .    for now , we will assume the functions are recorded without noise .",
    "if the functions are recorded with noise , it is common practice in the current literature to first perform a pre - processing smoothing step .",
    "an undesirable result of this pre - processing step is that the subsequent inference procedure is unable to capture the extra variability associated with the smoothing process .",
    "this phenomenon is illustrated using the berkeley boys growth velocity data in appendix c.3 . appendices",
    "c.1 and c.2 detail how our basic registration model can be modified to both smooth and register functions .",
    "inference is accomplished through a bayesian hierarchical model .",
    "the distributional assumptions and prior specifications for this model are    @xmath23    for this model , the parameters , @xmath24 , allow the registered functions to vary by vertical shifts from a scaling of the target function , @xmath17 . the constraint , @xmath25 , ensures the average vertical shift is estimated to be 0 .",
    "the parameters , @xmath26 , quantify amplitude variation in the registered functions .",
    "note , the gaussian distribution on @xmath27 can be replaced by a dirichlet distribution on @xmath28 .",
    "the result is a slightly more complicated model that has the nice effect of scaling the target function to the empirical mean of the estimated registered functions . priors and would then be omitted .    in the above model specifications ,",
    "all covariance functions are composed of a linear combinations of two bi - variate functions , @xmath29 and @xmath30 .",
    "@xmath29 penalizes variation in constant and linear functions and @xmath30 penalizes function variability in all other directions .",
    "together they define a proper covariance function . for each covariance function above , the specification of the registration and smoothing parameters indicate the extent the two different types of variability should be penalized for each function . for example , for both the registered functions and the base functions , we want to penalize variation in _ any _ direction other than that of the mean function .",
    "the covariance specifications of @xmath18 and @xmath31 reflect these penalties , where the magnitude of the penalty is controlled by registration parameters , @xmath19 and @xmath32 , ( distributional assumptions [ eq : reg],[eq : regcov ] , and [ eq : base ] ) .",
    "we can use @xmath30 to penalize roughness in a given function . here",
    "we would like both the target function and the base functions to be smooth .",
    "this is achieved by the inclusion of @xmath33 and @xmath34 in the priors for these functions ( distributional assumptions [ eq : target ] , [ eq : targcov ] , [ eq : base ] , and [ eq : basecov ] ) where the level of the penalty is controlled by the smoothing parameters @xmath35 and @xmath36 . for the exact definitions of @xmath29 and @xmath30 ,",
    "see @xcite .",
    "given the above model , in practice we will proceed by using finite approximations to each function and functional distribution . in @xcite",
    "we establish some theoretical properties of these types of approximations .",
    "the following finite - dimensional distributions are used in the final model in lieu of their infinite dimensional counterparts above :    @xmath37    section [ sec : comp ] provides several examples that illustrate how allowing the target function to be estimated within the model results in a more complete functional registration in comparison to the procrustes method , @xcite .",
    "however , the gaussian process model does not constrain the timing of a feature in the target function to occur at the average time of the corresponding unregistered features .",
    "although the model for the @xmath6 is centered on zero , it is still possible that the average of the estimated warped time points , @xmath38,@xmath39,@xmath40 , does not correspond to the original time points .",
    "shifting these by an additional registration so that the warped times average to the original time does not affect our prediction model , but it will then allow an explicit comparison of @xmath14 to @xmath41 to tell us whether the process is running ahead or behind `` standard '' time .",
    "@xcite use a similar  correction \" to determine their target function .",
    "details on how to perform this final registration can be found in appendix a.3 .",
    "registration in this model is controlled by three parameters , @xmath19 , @xmath32 , and @xmath36 .",
    "the parameter @xmath19 determines the extent the registered functions will be penalized for varying from a shifting and scaling of the target function .",
    "this penalty for lack of registration is tempered by penalties for roughness in the warping functions .",
    "the parameter @xmath32 determines how far the warping functions can deviate from the identity warping while @xmath36 controls the smoothness of the warping functions .",
    "this model can also be adapted to allow for function specific warping penalties . in section [ sec : elnino ] , we will give an example where function specific penalties for the base functions have been utilized to preserve significant covariance relationships in the estimated registered functions .    for a given statistical analysis ,",
    "the registration parameters are chosen by the user .",
    "this is because variation in registered functions outside of shifts and rescaling @xmath17 ( controlled by @xmath19 ) is nearly non - identifiable from variation in warping functions , as regulated by @xmath32 and @xmath36 .",
    "that is , there are linear directions of variation in the @xmath42 that result in nearly linear directions of variation in the resulting @xmath2 ( a simplified example is the identity @xmath43 which confounds horizontal shifts with vertical variation in periodic functions ) .",
    "an exact description of this form of confounding is beyond the scope of this paper , but it yields unstable estimates when these parameters are not fixed and we therefore choose these by hand . in this model",
    "a large registration penalty , @xmath19 , in comparison to the penalty on warping , @xmath32 , will result in registered functions that no longer retain significant features in the data .",
    "alternatively , a registration parameter that is too small will not properly align features .",
    "desirable values of these parameters can be determined using short runs of the adapted variational bayes algorithm described in section [ sec : avb ] . in practice",
    ", we have found these penalties should be adjusted by powers of ten to see a significant change in estimates of the registered functions .",
    "once determined , @xmath19 , @xmath32 , and @xmath36 are fixed and can be used with the adapted variational bayes estimates to initialize an mcmc sampler .",
    "for our registration model , it is appropriate to use markov chain monte carlo ( mcmc ) methods to sample from the joint posterior distribution of all unknown parameters .",
    "however , for most applications , the dimensionality of the parameter space will require exceptionally long chains that are impractical and expensive to obtain . here ,",
    "we suggest a variational bayes alternative to mcmc sampling to at the very least obtain good starting values for a mcmc sampler . alternatively , we will show in section [ sec : mcmc ] that differences in the estimated parameters obtained through adapted variational bayes and mcmc sampling tend to be small , and estimation via adapted variational bayes alone is likely sufficient for many inferential procedures .",
    "the variational bayes procedure described here is based on the variational methods proposed by @xcite and @xcite .",
    "their proposed method optimizes a lower bound of the marginal likelihood which results in finding an approximate joint posterior density that has the smallest kullback - leibler ( kl ) distance from the true joint posterior density .",
    "both fixed form and nonparametric forms of variational bayes algorithms are currently available .",
    "the variational bayes algorithm that we propose is most closely related to fixed form variational bayes .",
    "a clear explanation of fixed form variational bayes can be found in @xcite where the authors utilize variational bayes for a functional regression model .",
    "suppose @xmath44 is the approximated posterior joint distribution .",
    "the fixed form variational bayes algorithm assumes for some partition of @xmath45 , @xmath46 , where each distribution @xmath47 is of a known parametric form .",
    "traditionally this requirement is satisfied through the use of conditionally conjugate priors .    in our model ,",
    "the gaussian process priors for the base functions , @xmath6 , @xmath48 , are not conditionally conjugate to the likelihood function .",
    "therefore , the fixed form variational bayes optimization method does not apply directly since @xmath49 , @xmath48 are not known parametric distributions .",
    "suppose we order the parameter vector , @xmath50 , so that , @xmath51 , for @xmath52 .",
    "while the @xmath53 distributions on the approximated base functions , @xmath54 , @xmath48 are not of known parametric forms , each @xmath55 , @xmath52 , is a known parametric distribution that can be estimated using the standard fixed form variational bayes algorithm .",
    "the following variational bayes algorithm is adapted to include estimation for parameters without conditionally conjugate priors in addition to all other parameters that typically can be estimated using fixed form variational bayes .",
    "this adaptation is similar to the variational approximation to the em algorithm described by @xcite which performs approximate inference based on the em algorithm for models where the posterior distribution of the latent variables is of an unknown form .",
    "define @xmath56 as the joint distribution of the data , @xmath57 , and parameters of a bayesian hierarchical model .",
    "suppose an approximate joint posterior distribution of the parameters , @xmath51 , is of the form @xmath58 where each @xmath59 , is known only up to a constant of proportionality and the distributions , @xmath60 , are of known parametric forms",
    ". we will define the following estimation procedure for @xmath50 as the adapted variational bayes algorithm .",
    "1 .   initialize @xmath50 .",
    "2 .   for each iteration , @xmath61 , and each @xmath62 , @xmath63 , update the estimate for + @xmath64 so that @xmath65@xmath66 ) .",
    "this is equivalent to setting @xmath67@xmath68 ) .",
    "3 .   for each iteration , @xmath61 , and each @xmath62 , @xmath69 , update @xmath47 so that @xmath70 @xmath71 @xmath72 $ ] , where the expectation is taken with respect to the distributions @xmath73 , @xmath74 , @xmath75 .",
    "4 .   repeat steps ( 2 ) and ( 3 ) until the desired convergence criterion is met .    here",
    "the notation , @xmath76 , denotes the expected value over all parameters except @xmath77 . in the next section",
    ", we will drop the subscript @xmath62 , and @xmath78 will represent the expectation over all parameters except for @xmath77 ( e.g. @xmath79 will represent the expectation taken over all parameters except for @xmath80 ) . + * theorem * _ the adapted variational bayes algorithm converges to estimated parameters , @xmath81 , that minimize the kullback - leibler distance between the approximate posterior distribution , @xmath82 , and the posterior distribution , @xmath83 , for a local optimization of @xmath84 in @xmath85 . _",
    "+ * _ proof _ : * assume @xmath85 is known .",
    "@xcite demonstrate that minimizing the k - l distance between @xmath82 and @xmath83 is equivalent to maximizing the following log of a q - specific lower bound of the joint marginal distribution of @xmath57 and @xmath85 in @xmath53 .",
    "the adapted variational bayes algorithm alternates between : 1 ) maximizing @xmath84 in @xmath85 ( possibly locally ) , and 2 ) fixing @xmath85 at the value determined by the previous step and using traditional variational bayes to maximize @xmath87 . here",
    "we demonstrate this process results in a monotonic increasing sequence in @xmath88 which guarantees the convergence of this algorithm .    for each iteration , @xmath61 of our adapted variational bayes algorithm , @xmath89-\\log[q(\\boldsymbol\\theta^{(m)}_{-\\mathbf w } ) ] ] \\nonumber\\\\      & \\leq &   e_{q(\\boldsymbol\\theta_{-\\mathbf w})}[\\log[f(\\mathbf x,\\mathbf w^{(m+1)},\\boldsymbol\\theta^{(m)}_{-\\mathbf w})]-\\log[q(\\boldsymbol\\theta^{(m)}_{-\\mathbf w } ) ] ] \\label{eq : optstep } \\\\      & \\leq &   e_{q(\\boldsymbol\\theta_{-\\mathbf w})}[\\log[f(\\mathbf x,\\mathbf w^{(m+1)},\\boldsymbol\\theta^{(m+1)}_{-\\mathbf w})]-\\log[q(\\boldsymbol\\theta^{(m+1)}_{-\\mathbf w } ) ] ] \\label{eq : vbstep } \\\\      & = & \\log f(\\mathbf x,\\mathbf w^{(m+1 ) } ; q^{(m+1 ) } ) .",
    "\\nonumber            \\end{aligned}\\ ] ]    the inequality in is guaranteed by step 2 of the adapted variational bayes algorithm , and the inequality in is the result of using the traditional variational bayes algorithm with @xmath85 considered known ( step 3 ) .",
    "+ the lower bound of the marginal distribution of @xmath57 and @xmath85 can be monitored until changes in this function are under some threshold .",
    "the specific form of this function can be found in appendix b.2 .",
    "however , as the algorithm is guaranteed to converge , it is in practice more prudent to instead monitor changes in the parameter estimates from iteration to iteration and stop the algorithm when these changes are below a specified threshold .",
    "convergence of the avb algorithm is guaranteed .",
    "however , convergence to a global maximum is not guaranteed .",
    "in the maximization step of the avb algorithm , a function proportional to the approximate posterior for the base functions is maximized in the base functions .",
    "this function can be multimodal and occasionally the estimated base functions reflect a local maximum of the approximated posterior . to circumvent this problem , in practice",
    "it is sometimes necessary to adjust the registration and warping penalties as the functions become registered .",
    "an unregistered function that requires a substantial amount of warping can cause convergence to a local maximum due to the small penalty on warping .",
    "the flexibility in warping allowed by this small penalty can cause the function to deform rather than register .",
    "this can be remedied in two ways .",
    "the first option is to perform a simple initial warping for this function that prevents the optimization from falling into a local mode .",
    "the second option is to adjust the registration and warping parameters over time .",
    "initially a stronger warping penalty is employed to prevent function deformation .",
    "then , as the functions register , the warping penalty can be reduced to allow for a more complete registration . when initializing an mcmc sampler , the final penalties on warping and registration from the adapted variational bayes algorithm should be used .",
    "while it is common in statistical analysis to perform preprocessing steps before applying a particular inference procedure , failing to account for the variability in parameter estimates due to the preprocessing step leads to overly narrow confidence ( or credible ) regions . in some cases",
    ", the effect may be fairly small , and not much is lost in this oversight .",
    "however , as we show here , the underestimated variability can be substantial when uncertainty in the preprocessing steps is ignored .    in section 4.2",
    "is an illustration of how closely avb and mcmc estimates of the registered functions adhere to one another . not only do these estimates tend to be fairly similar when the functions are recorded without noise , but the uncertainty in these estimates is minimal .",
    "figure [ fig : cred811 ] contains the credible bands for two of the 39 pre - smoothed boys growth velocity functions .",
    "these bands are so narrow the width between them can not be seen .",
    "keep in mind the posterior distributions of the registered functions are certainly multi - modal .",
    "these credible bands result from imposing the restriction that the mean value of the warping functions at each time point over the sample must equal that time point .",
    "even with this restriction , the posterior distributions can be multi - modal .",
    "however , these narrow credible bands reflect that our estimates are in a highly probable area of the posterior distribution with minimal local variance .",
    "figure [ fig : cis ] contains credible bands for both the unregistered and registered functions for the same two functions used in figure [ fig : cred811 ] after noise has been added to the data and accounted for in the model .",
    "the variability due to noise is substantial .",
    "the solid line in all of the plots contains the noiseless version of these estimates ( or observations in the case of the unregistered functions ) .",
    "in addition to providing more accurate credible intervals , this model estimates the noise variance to be .258 ( actual noise variance is .25 ) .",
    "this estimate is obtained using uninformative priors for both the noise variance , @xmath90 and the associated smoothing parameter @xmath91 .",
    "this analysis illustrates how regularizing the data prior to statistical analysis for registration models severely limits inference for these models . if significant noise is present in the data it is prudent to account for the variability in the registration process due to the noise .",
    "our proposed hierarchical model is one way to account for this variability ."
  ],
  "abstract_text": [
    "<S> we propose a model for functional data registration that extends current inferential capabilities for unregistered data by providing a flexible probabilistic framework that 1 ) allows for functional prediction in the context of registration and 2 ) can be adapted to include smoothing and registration in one model . </S>",
    "<S> the proposed inferential framework is a bayesian hierarchical model where the registered functions are modeled as gaussian processes . to address the computational demands of inference in high - dimensional bayesian models , we propose an adapted form of the variational bayes algorithm for approximate inference that performs similarly to mcmc sampling methods for well - defined problems . </S>",
    "<S> the efficiency of the adapted variational bayes ( avb ) algorithm allows variability in a predicted registered , warping , and unregistered function to be depicted separately via bootstrapping . </S>",
    "<S> temperature data related to the el - nio phenomenon is used to demonstrate the unique inferential capabilities for prediction provided by this model .    </S>",
    "<S> # 1    0    0    1    0    * title *    _ keywords : _ bayesian modeling , functional data , functional prediction , registration , smoothing , variational bayes </S>"
  ]
}