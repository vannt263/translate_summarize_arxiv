{
  "article_text": [
    "in the mere 17 years since the first discovery of a `` hot jupiter '' around a main sequence star @xcite , the study of exoplanets has exploded into one of the most vibrant and rapidly developing fields in astronomy today . over 500 exoplanets",
    "have been confirmed , and four times that many candidates have been identified @xcite .",
    "the pace of exoplanet discoveries has consistently increased , with new exoplanet search methods continuously being developed and implemented , and new and surprising classes of planets routinely being uncovered as new regimes of parameter space are explored .",
    "meanwhile , an enormous amount of effort is being put into developing theories of planet formation and evolution that can encompass the astonishing diversity of planetary systems that has emerged .",
    "the transit method has been at the center of this revolution , not only because it has expanded the region of parameter space to which we are sensitive , but more importantly , it can provide a seemingly endless wealth of information about each planet ( see @xcite for a comprehensive review ) .",
    "for example , precise photometry during the primary transit can be used to measure the planet radius and orbital inclination , and when combined with the minimum mass inferred from radial velocity ( rv ) studies , yield the true planet mass and average density , thereby constraining the planet s structure @xcite .",
    "photometric observations during both primary transits and secondary eclipses enable the study of their atmospheres @xcite and thermal emission @xcite .",
    "variations in the timing and shape of the eclipses and transits hint at the existence of other bodies in the system @xcite , constrain orbital evolution due to tides or other effects @xcite , probe `` weather '' in exoplanet atmospheres @xcite , and provide a probe of the interior structure and oblateness of exoplanets @xcite , to name a few . in resonant configurations",
    ", even eris - mass planets may be detectable via such transit timing variations ( ttvs ) using current ground - based technology for favorable systems @xcite .",
    "further , the projected angle between the spin axis of the star and the orbit of the planet can be measured via spectroscopic observations during transit to provide diagnostic information of the physical processes at work in the migration of hot jupiters @xcite .",
    "indeed , the combination of radial velocity and transit data provides the most thorough insights into a planetary system of any demonstrated planet detection and characterization method to date .    because of",
    "the wealth of information that can be derived from these planets , it is important to carefully consider the best ways to extract such information from the data sets we acquire such that results are limited by the data and can be compared in a manner that is as homogeneous and consistent as possible .",
    "the markov chain monte carlo method has become a standard tool in exoplanet research ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , and has recently begun to be replaced with a faster , more elegant flavor : differential evolution mcmc , de - mc ( e.g. , * ? ? ? * ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "many public codes exist to model transit light curves and/or radial velocities , including tap @xcite , jktebop , @xcite , fitsh @xcite , pheobe @xcite , vartools @xcite , nightfall , phos - t @xcite , and systemic @xcite .",
    "the goal of this paper is to present an additional code , exofast , which we believe provides a valuable combination of features not present in any currently - public code : simultaneous and self - consistent radial velocity , transit , and stellar parameter fitting ; fast , robust , de - mc characterization of errors ; intuitive outputs , careful attention to realistic priors ; non - interactive ( easy to pipeline ) ; well documented ; and easy to install , use , and customize . providing a completely general code that can fit any conceivable planetary phenomenon without modification is not practical . rather than attempt to be comprehensive",
    ", our goal was to provide a modular , easily - extensible framework with a relatively straightforward but powerful example implementation for exoplanets that fits a single - planet system which has either or both rv and primary transit data .",
    "this framework and example implementation can be adapted to add additional effects as the data are able to constrain them ( e.g. , ttvs , secondary eclipses ) , impose different priors , or even analyze completely different problems ( e.g. , supernovae , cepheids ) .",
    "while idl is a proprietary language that is generally slower than low - level languages like c and fortran , we chose to use this language because of the large library of existing code , the ease of development , and the fact that well - written idl is comparable in speed to higher level languages for most ( i.e. , non - serial ) applications .",
    "of course , an mcmc code is necessarily serial ( i.e. , one can not calculate an arbitrary step in the markov chain without first calculating the step before it ) , but the vast majority of the time spent is the model evaluation at each step , which has been carefully vectorized whenever possible . for those unable or unwilling to purchase an idl license ,",
    "the gnu data language ( gdl ) is an open - source compiler that claims full syntax compatibility with code up to idl version 7.1 .",
    "our code does not work out of the box with gdl , but some users have gotten the core features working .",
    "future updates will keep compatibility with gdl in mind .    in ",
    "[ sec : overview ] , we provide a brief summary of the general problem of fitting data sets , an overview of how mcmc works , and why it is preferred over alternative methods of fitting data and estimating uncertainties . the discussion here and routines cited are completely general to the problem of fitting any model to a data set and properly characterizing the uncertainties  it is not just applicable to exoplanets or even just astronomy .",
    "next , we describe our specific procedure to fit rv data (  [ sec : rv ] ) , including a detailed discussion of different ways to parameterize eccentricity (  [ sec : eparam ] ) , and two potentially - common mistakes when using mcmc , both of which can inflate the measured eccentricity of intrinsically circular orbits significantly .",
    "we discuss our procedure to fit transit data in  [ sec : transit ] , and combined rv and transit data sets simultaneously in ",
    "[ sec : rvtran ] .",
    "section  [ sec : example ] walks through an example fit of hat - p-3b with real , public data to explain how the code works , what it does , and what its outputs are .",
    "our online interfaces to the most useful codes are presented in  [ sec : online ] . along with these online interfaces ,",
    "all of the source code described here is available online .",
    "those already familiar with mcmc and the basics of light curve and rv modeling may find it most efficient to begin with the discussion on eccentricity parameterization (  [ sec : eparam ] ) , then skip to  [ sec : rvtran ] , where we discuss our unique approach to fitting rv and transit data simultaneously .",
    "appendix  [ sec : analyticld ] demonstrates that the linear and/or quadratic limb darkening coefficients can be fit analytically , which can reduce the dimensionality of a non - linear solver , thereby drastically increasing its speed .",
    "specific improvements to the @xcite code to calculate the quadratically limb - darkened flux during transit , including a factor of @xmath0100 improvement in speed that cuts the run time of typical rv and transit fit from an hour to a few minutes , are described in appendix  [ sec : occultquad ] .",
    "appendix  [ sec : random ] discusses a problem with idl s built - in random number generator and provides an alternative at a moderate increase in the overall run time .",
    "appendix  [ sec : nege ] discusses a way to interpret a negative eccentricity that provides continuous models across the boundary at e=0 .",
    "lastly , appendix  [ sec : runtime ] discusses the execution time and identifies areas for future improvement .",
    "given a data set , @xmath1 , with uncertainties , @xmath2 , we would like to generate a model , @xmath3 , from a set of model parameters that describe the data . if we assume the uncertainties are gaussian , then the probability of the data @xmath1 given the model @xmath3 , or the likelihood , @xmath4 , is given by    @xmath5 \\exp{\\left [ -\\sum_{i=1}^n \\frac{(d_i -                          m_i)^2}{2(\\sigma_i^2 + s^2)}\\right]}\\ ] ]    where the subscript @xmath6 corresponds to each of the @xmath7 data points and @xmath8 is an added scatter term .",
    "if we assume @xmath8 is constant for each model , it can be absorbed by the error in each data point , the likelihood simplifies to    @xmath9    where    @xmath10    therefore , assuming fixed uncertainties , finding the maximum likelihood is equivalent to finding the model with the lowest @xmath11 .",
    "when the model is linear ( i.e. , can be written as a simple linear combination of known quantities with unknown coefficients ) , the @xmath11  can be minimized analytically and exactly to find each of the coefficients ( i.e. , the parameters of the model ) ( see * ? ? ?",
    "* ) .    however , when the model is non - linear , such as for transits and radial velocities , we must determine the best fit parameters which minimize @xmath11  numerically .",
    "unfortunately , there are no generic algorithms to minimize the @xmath11  for a global parameter space ",
    "often , various tricks are required that are specific to the particular problem at hand .",
    "we will discuss the tricks specific to exoplanets in  [ sec : rv ] and  [ sec : transit ] that we use to restrict the region of parameter space close to the global minimum .",
    "once we identify this region , there are many routines that can robustly find a local minimum .",
    "amoeba is a popular non - linear solver that uses the downhill simplex method to find local minima @xcite . given a starting point and stepping scale ( which is approximately the range of parameter space it will consider ) , amoeba will crawl through parameter space to find the minimum , using the @xmath11  at each step to determine its next step .",
    "this routine is very robust at finding local minima .",
    "idl comes with its own built - in amoeba routine , but we discovered a bug that truncates the stepping scale to floating point precision , regardless of the data type initially given .",
    "this is detrimental when fitting parameters that require double precision ( e.g. , julian day ) , since the model will simply oscillate about the minimum and not converge .",
    "we provide a debugged version of this code , which now forces all stepping scales to be double precision , as a new code in this suite , exofast_amoeba .",
    "another popular algorithm to find the local @xmath11  minimum is levenberg - marquardt ( lm ) @xcite , which uses numerical derivatives to predict the minimum more precisely after each evaluation , and therefore requires fewer evaluations of the @xmath11  statistic ( which is generally completely dominates the computation time ) .",
    "the downside is that , if the @xmath11  surface is not smooth , the numerical derivatives may be a poor predictor of the minimum and the fit will not be as robust .",
    "@xcite published an extremely versatile and widely - used idl implementation of the lm algorithm , called mpfit .",
    "as expected , we found our debugged version of amoeba to be more robust than mpfit , routinely finding as good and occasionally better values of @xmath11 , but it also required about 10 times more model evaluations and was therefore about 10 times slower .",
    "once we have the best fit , we check the quality of the fit by examining the probability that our model has the @xmath11  that it does .",
    "if a model is a good description of the data , and the measurement uncertainties are uncorrelated and properly estimated , then the probability of getting the @xmath11  we do , @xmath12 , should be 0.5 . in the the limit of infinite degrees of freedom , this is equivalent to saying the @xmath11  per degree of freedom , @xmath13  is unity . even with as few as 2 degrees of freedom , the difference between @xmath14 and @xmath15 is only 20% , which is already better than we expect this method to be .",
    "while we do not wish to imply more accuracy than that by being too precise , we scale the errors such that @xmath16 because it is precisely correct under the most naive assumption that our uncertainties are gaussian and uncorrelated .    thus , if @xmath17 deviates from 0.5 by an amount that is significantly larger than expected given the number of degrees of freedom , this implies that either the model does not properly describe the data ( e.g. , there is signal of another planet that has not been modeled ) , that the uncertainties have not been properly estimated ( e.g. , because of unrecognized systematics ) , or that the data are not gaussian distributed ( e.g. , there are large , non - gaussian outliers ) .",
    "unfortunately , it is often difficult to distinguish between these possibilities a priori .",
    "if one has reason to believe that the model properly describes the data , but nevertheless finds that the @xmath17 differs significantly from 0.5 , the natural conclusion is that the uncertainties have been misestimated for the bulk of the data ( often underestimated ) or that there are a few non - gaussian outliers .",
    "parameters or parameter uncertainties derived from such data are likely to be biased . in this case",
    ", there is a strong motivation to attempt to `` correct '' the data such that @xmath18 , thereby producing ( hopefully ) less biased parameters and parameter uncertainties .",
    "a plausible procedure for `` correcting '' the uncertainties is as follows .",
    "first , one can search for and identify strong outliers . if , after eliminating these outliers , one still does not find a satisfactory fit , the next step is to modify the uncertainties .",
    "there are generally three ways in which one can modify the uncertainties to @xmath14 : scale the uncertainties by a constant multiplicative factor , add a constant term in quadrature to the uncertainties , or both . the appropriate method to adopt depends in detail on why the uncertainties were misestimated .",
    "however , if this was known , then it is likely that it would have been corrected in the first place . as a general rule ,",
    "if there is an unaccounted systematic that is independent of the signal ( e.g. , stellar jitter ) , then the additional uncertainty term should be added in quadrature .",
    "if there is an error in the normalization or some calibration systematic ( e.g. , an error in the gain ) , it is more appropriate to multiply the uncertainties by a constant factor .",
    "the practical difference between these two approaches is usually minimal , but adding uncertainties in quadrature will tend to even out all of the uncertainties , whereas multiplying will preserve the relative uncertainties .    in @xcite",
    ", we used our code to fit radial velocity data for a brown dwarf candidate from the marvels collaboration .",
    "we found for this candidate that the @xmath13  for the native data and uncertainties was considerably larger than unity .",
    "we then tested many different permutations of eliminating outliers , scaling uncertainties , and adding uncertainty terms in quadrature , to force @xmath15 , and assessed the effect of these different procedures on the resulting best - fit parameters and uncertainties .",
    "we found no statistically significant difference between the various methods of altering the uncertainties .",
    "partly motivated by this experience , our default procedure is to scale the uncertainties by a constant factor and we do not include a jitter term , as is common with rv fits .",
    "however , we recognize that this result is unlikely to be generic , and note that it is relatively straightforward to modify this procedure in our routines .",
    "several alternatives to our method of error scaling have been suggested .",
    "it has been proposed to allow the uncertainty scaling ( e.g. * ? ? ? * ) or another uncertainty term to add in quadrature ( e.g. * ? ? ?",
    "* ) to vary as a free parameter ( e.g. , the @xmath8 term in equation  [ eq : likelihood ] ) .",
    "more recently , @xcite suggests a wavelet analysis method to fit the correlated noise component more robustly , and states that treating correlated noise as white noise like we do systematically underestimates the errors . implementing a wavelet analysis is on our long list of eventual improvements , but in practice , the difference is relatively small as long as the red noise is not dominant ( see @xcite for a discussion ) .    if we consider multiple independent data sets from different sources , they are likely to have different systematics .",
    "therefore , it is a good idea to fit each data set and scale the uncertainties independently , and then ensure that the resulting parameters are consistent with one another before attempting to combine them . if there are large inconsistencies between the data sets , it is indicative of serious problems with either the model ( neglected effects ) or data ( systematic uncertainties ) and a simultaneous fit of all data sets should not be trusted .",
    "if they are consistent , we can find the best fit to the combined data set using a local minimum solver starting at the best - fit values of one of the independent data sets .",
    "so far , we have discussed the process of evaluating the probability that a given data set @xmath1 is described by a given model @xmath3 , @xmath19 .",
    "however , what we are actually interested in is the probability that a given model @xmath3 is correct , given our data @xmath1 , @xmath20 .",
    "this probability depends not only on @xmath19 , but also on our prior beliefs about the probability of a given model @xmath3 , @xmath21 , which are related by bayes theorem    @xmath22    here , @xmath23 is the probability of the data , which is given by the integration of @xmath24 over the parameter space encompassed by the model @xmath3 .",
    "heuristically , bayes theorem can be thought of as providing a rigorous way of incorporating new data to revise initial beliefs .    in principle , one is interested in evaluating @xmath20 for two purposes .",
    "first , a comparison of @xmath20 between two different models for the same data set can determine which model is more likely to be correct given the data , models , and priors .",
    "second , for a given model , @xmath20 can be used to determine the relative probability of different parameters of the model , also called the posterior probability density . in our case , where we are only considering different parameters of the same model",
    ", @xmath23 is constant , so we need not explicitly calculate it .",
    "since there is zero phase space precisely at the best fit , it tells us nothing of the uncertainties of the model parameters .",
    "however , by evaluating the posterior probability density , @xmath20 , we then determine the uncertainties of the model parameters , e.g. , by determining the range of a given parameter that encompasses some set fraction of the probability density .",
    "once the priors , @xmath21 , are specified , the task then becomes evaluating @xmath20 .",
    "the markov chain monte carlo technique provides an efficient method for doing this that allows for easy determination of median values , uncertainties , and covariances for the fitted model parameters , in addition to any parameters that can be derived from the model parameters .",
    "the mcmc technique is also attractive because it is based on the data _ as given _ , as opposed to bootstrap analyses which use simulated data to evaluate the uncertainties .",
    "we adopt the metropolis - hastings algorithm to sample @xmath20 .",
    "we start with a set of trial parameters , and evaluate @xmath11  for this trial set .",
    "we then randomly choose a different set of parameters , and calculate @xmath11  for this set of parameters .",
    "the ratio of the likelihood , assuming the errors are constant ( i.e. , equation  [ eq : likelihoodsimple ] ) for the new set of parameters relative to the initial set is given by    @xmath25/2}.\\ ] ]    we then draw a random number uniformly distributed between 0 and 1 . if the random number is greater than the likelihood ratio , the model is rejected and we do not step there .",
    "instead , we duplicate a copy of the previous position in the chain as the current step .",
    "if the random number is less than the likelihood ratio , we accept the new model .",
    "note , when @xmath26 ( i.e. , the new model is a better fit ) , @xmath27 is always greater than 1 , and so the step will always be accepted .",
    "we repeat this process , stepping to a new region of parameter space until we have a smooth distribution of values for each parameter .",
    "the resultant density of steps is proportional to the posterior probability of each parameter , naturally resulting in a robust estimate of the median value and the 68% confidence interval .",
    "again , we do not consider the absolute normalization , @xmath23 , which one would need to do in order to consider the relative likelihood of models with a different parameterization .",
    "one of the most problematic aspects of using mcmc is determining an appropriate stepping scale and direction . using",
    "the proper length scale is key to speedy convergence .",
    "if the scale is too large , very few trial links will be chosen , and many models will be calculated unnecessarily . if the scale is too small , many links will be accepted but the adjacent links in the chain will be highly correlated with each other , and the resultant chain will not be `` well - mixed , '' discussed below . similarly , if the chains step in a correlated parameter set ( a non - orthogonal direction ) , the chains are very likely to be rejected because the effective step in the orthogonal space will be too large .",
    "the ideal step mimics the posteriori probability distribution precisely @xcite .",
    "of course , this is not known a priori ; it is exactly what we are trying to calculate .",
    "an elegant solution to this problem is the differential evolution markov chain method of @xcite , which runs many chains in parallel ( equal to twice the number of free parameters ) , and uses the difference between the parameter values between two random chains to determine the next step .",
    "since the ensemble of chains should be distributed according to the posterior probability , the difference between two random chains gives us the rough scale and direction of the step ( i.e. , the covariance matrix among all parameters ) , automatically taking into account the correlations between parameters within each step and dramatically decreasing the number of links required for the chains to be well - mixed .",
    "@xcite also adds a small , uniform deviate to each step to ensure the whole parameter space can be reached  otherwise , the steps could be cyclic , depending on the starting positions of each chain , leaving islands of unexplored parameter space . however , we found that , because the dynamic range of the ideal step sizes ( i.e. , the uncertainties ) of different parameters can be arbitrarily large depending on the units of the parameters , adding the same uniform deviate to the steps in each parameter is not general .",
    "that is , the log of the period has typical errors of order @xmath28 , so adding a random deviate of @xmath29 completely dominates its step size , making the step too large and the chain inefficient .",
    "however , adding the same random deviate to the systemic velocity , which has typical errors of order @xmath30 m / s , is completely negligible and does not serve its purpose to adequately mix the steps .    instead , we estimate the stepping scale by starting at the best - fit values and then varying each parameter individually until the @xmath31  is one , in our program exofast_getmcmcscale .",
    "then , we add a uniform deviate equal to a small fraction of that step size ( we somewhat - arbitrarily picked 1/10 ) .",
    "even with highly - correlated parameters , this algorithm yields an acceptance rate of 17% for a large number of parameters  close to the optimal acceptance rate of @xmath32 @xcite .",
    "when done this way , we can step in all parameters simultaneously without having to monitor the acceptance rates of each parameter individually because their step sizes are self - adjusting .    in order to determine when",
    "our mcmc chains have converged , we roughly follow the guidelines set forth by @xcite . each parameter in each of our chains begins at their best - fit value plus 5 times their corresponding step size ( approximately their uncertainty ) times a gaussian - distributed random number .",
    "we take steps as described above in all parameters simultaneously , until the chains have converged .",
    "we consider the chains to be converged when both the number of independent draws , @xmath33 is greater than 1000 and the gelman - rubin statistic , @xmath34 is less than 1.01 for all parameters .",
    "the independent draws and gelman - rubin statistic are calculated in exofast_gelmanrubin and defined by @xcite .",
    "this test must be passed 6 consecutive times  after passing these tests the first time , we take 1 percent more steps and check again . if it fails , we restart the convergence test .",
    "if it passes , we repeat , taking 2 , 3 , 4 , and finally 5 percent more steps .",
    "when all tests have been passed consecutively for all parameters , we consider the chains well - mixed and stop .",
    "finally , we find the first point at which all chains have had a @xmath11  below the median @xmath11  and discard everything before that as the `` burn - in '' ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "this eliminates any bias due to the starting conditions .    due to the limitations of 32-bit operating systems ,",
    "a single program ( e.g. , idl ) can not allocate more than 2 gb of memory ( @xmath0260 million double - precision elements ) . given the number of parameters , links , chains , and redundant copies of each that must be stored",
    ", it is relatively easy to reach this limitation with a moderately - sized chain before it converges . with a 64-bit machine ( and idl )",
    ", it is possible to increase the maximum length of each chain dramatically , but managing that volume of data is slow .",
    "fortunately , due to the autocorrelations within the chains , we can `` thin '' them with little penalty in the accuracy of the results or the efficiency of convergence , but with a huge benefit to the manageability of the data set .",
    "therefore , we include an option to thin the chain by a factor of @xmath35 , which will only keep every @xmath35 link in chain as the markov chain is calculated .",
    "exofast will estimate how many steps will be required for convergence after 5% of the chain has been calculated .",
    "if it is not expected to be well - mixed at the conclusion of program , it will output a warning with a recommended thinning factor . specifying this thinning factor will help ensure that the final chain is converged , but with the unavoidable consequence of increasing the execution time by roughly a factor of @xmath35 .",
    "the resultant parameter distributions ( i.e. , the histogram of steps for each parameter ) are proportional to the posterior probability of each parameter .",
    "we quote the median of the distribution as the final value and 34% confidence interval on either side as the uncertainty . if there are large covariances or non - gaussian distributions of parameters , the ensemble of median values , while individually most probable , may be a poor fit to the data .",
    "this also means that the quoted values for derived parameters are likely to be mathematically inconsistent ( e.g. , the median values of @xmath36 and @xmath37 do not exactly imply the median value of @xmath38 ) , but they are statistically self - consistent .",
    "the true `` best - fit '' set of parameters can be extracted from the program outputs if desired , but we reiterate that the best - fit has zero phase space associated with it , so this is generally not useful .",
    "one of the subtleties of a proper mcmc implementation is the correct choice of priors .",
    "we implicitly impose a prior that is uniform in each parameter we step , so we must be careful to consider our choice of parameterization carefully such that it matches our a priori theoretical expectation .",
    "alternatively , we can weight the stepping probability by the jacobian to transform into our desired parameterzation @xcite , or correct our posterior distributions afterward by importance sampling , as long as the particular prior chosen does not preclude viable regions of parameter space",
    ". supporters of the mcmc method argue that all methods have some implied bias , and that mcmc is a good way to make that bias explicit .",
    "ideally , our prior would represent the underlying distribution of that parameter given the selection effects inherent to the sample . in practice , this is a very difficult quantity to determine , and often times , the reason we are doing the measurement in the first place is to determine such broad statistics .",
    "fortunately , when the data are highly constraining , the prior has little influence on the measured value .",
    "however , when the data are not highly - constraining , the prior can have a large impact on the measured values .",
    "in principle , it is possible to solve exactly for linear parameters during each step of the markov chain , which will reduce the dimensionality of markov chain , making convergence much faster .",
    "unfortunately , we have run our program on the same data set , only changing whether or not we fit some subset of parameters linearly or non - linearly , and the uncertainties in the linear parameters were as much as 10 times smaller when fitted linearly at each step .",
    "this is not too surprising , since the uncertainties in a markov chain come from the distribution of parameters at each step .",
    "if we always find the very best set of linear parameters given the particular set of non - linear parameters , we should expect the width of that distribution to be narrower than we would find if they were allowed to step randomly at each step , as they do when they are treated as non - linear parameters .",
    "even if the fitted parameters are not intrinsic to the problem ( i.e. , their values and uncertainties do not directly affect any physical property we care to measure ) , it may be that their covariances with parameters we care about cause us to underestimate their uncertainties . to deal with this",
    ", we simply fit all parameters non - linearly .",
    "it is likely possible to analytically compensate for this decreased scatter , and therefore recover the speed benefit of analytically fitting many parameters ( e.g. * ? ? ?",
    "* appendix d ) , but we leave this as a potential future improvement .",
    "we also note that this hybrid fitting is similar to detrending data prior to fitting it , as is common in transit fits .",
    "in fact , detrending prior to fitting is significantly worse than this because , rather than underestimating the uncertainties and covariances of detrending parameters , it ignores them altogether .",
    "this may lead to , among other things , spurious claims of transit timing variations ( ttvs ) , as the most commonly - removed trend ( a linear trend with time or airmass ) is highly covariant with the transit time .",
    "the gravitational interaction between a host star and orbiting planet results in a doppler shift of the observed spectrum of the star of the form    @xmath39 + \\gamma + { \\ensuremath{\\dot{\\gamma}}}(t - t_0)\\ ] ]    where @xmath40 is the radial velocity semi - amplitude , and is equal to    @xmath41    in equations [ eq : rv ] and [ eq : kp ] , @xmath42 is the true anomaly as a function of time , @xmath37 is the argument of periastron of the star s orbit measured from the ascending node to its periastron differs from @xmath37 by @xmath43 .",
    "as this definition can be somewhat counter - intuitive , we keep the subscript `` * '' to make it explicit .",
    "] , @xmath36 is the orbital eccentricity , @xmath44 is the systemic velocity ( or often just an arbitrary instrumental offset ) , @xmath45  is a systematic acceleration either due to an additional body in the system with a period much longer than the span of the observations , or systematics in the data , and @xmath46 is an arbitrary zero point for the slope , which we define to be the mean of the of input times .    the true anomaly , which is the angle between periastron and the planet , measured from the barycenter of the system , is    @xmath47}\\ ] ]    where @xmath48 is the eccentric anomaly , given by kepler s equation ,    @xmath49    as a function of the mean anomaly , @xmath50 .",
    "unfortunately , this is a transcendental equation for which no analytic solution for @xmath48 exists .",
    "it must be solved numerically for a given @xmath50 .",
    "we use exofast_keplereq , a slightly improved version of a code written by marc buie and joern wilms , which uses the method by @xcite , and in the case of high eccentricities , uses a newton - raphson method to refine the eccentric anomaly . our improvement handles diabolical inputs that prevent convergence when angles differ by slightly more or less than @xmath51 . while these cases are relatively rare , this routine is called hundreds of millions of times during the mcmc chain , and the original version almost always failed without our fix .    the mean",
    "anomaly simply describes a uniformly flowing time and can be computed from the period of the orbit , @xmath52 , and the time of periastron passage , @xmath53 :    @xmath54    in many instances , the reverse calculation is also required .",
    "that is , we would like to know the time , @xmath55 , that the planet will be at a given true anomaly .",
    "for instance , there are many special times in an orbit that one may be interested in knowing , like the time of periastron , @xmath53 , the time of primary transit center , @xmath56 , the time of secondary eclipse center , @xmath57 , the time when the star is at its ascending node @xmath58  ( when the rv is at a maximum ) , the time when the star is at its descending node , @xmath59  ( when the rv is at a minimum ) , or the time that the @xmath60 and @xmath61 star - planet lagrange points pass in front of the center of the star , @xmath62  and @xmath63 , respectively , which may be a favorable time to look for transiting trojan planets @xcite .",
    "each of these times correspond to a particular value of the true anomaly , given here :    @xmath64    fortunately , this is much easier .",
    "we simply invert equation  [ eq : trueanom ] ,    @xmath65},\\ ] ]    plug e(t ) into equation  [ eq : eccenanom ] , and solve equation  [ eq : meananom ] for @xmath55 .",
    "our routine for the reverse correction , exofast_getphase has keywords that can calculate the phases of each of the true anomalies described in equation  [ eq : specialtrueanoms ] or for an arbitrary true anomaly .",
    "while the @xmath37 is completely degenerate for a planet in a circular orbit , when the orbit is fixed to be circular , we follow the convention that @xmath66 .",
    "this has the virtue that the expected time of conjunction ( or transit ) occurs at `` periastron '' , i.e. , @xmath67 .",
    "the choice of parameterization is extremely important because we implicitly impose priors that are uniform in each of these parameters .",
    "if these priors are not physically motivated , they may introduce biases in the values of the parameters we infer from the mcmc .",
    "often , these biases can be corrected , but if the particular parameterization a priori excludes certain regions of parameter space , it can not .",
    "second , the choice of parameterization is important because highly - correlated parameter sets converge very slowly .",
    "fortunately , this latter concern is largely addressed by the slightly more sophisticated , de - mc algorithm .",
    "the parameterization we favor for radial velocities is @xmath68 , and @xmath45 .",
    "this particular parameterization differs from that suggested in @xcite in a few key ways .",
    "the parameterization of @xmath36 and @xmath37 is discussed in detail at the end of this section .",
    "we both use @xmath69 and @xmath70 , but the notion of imposing strict bounds outside of what we think is possible on each of these parameters to make them `` normalizable '' or `` proper '' priors ( i.e. , the integral over the allowed states is finite ) is unnecessary and somewhat misleading . on page 62 , @xcite state that we can obtain a proper result from an improper prior as long as the posterior distribution ( i.e. , our parameter distribution ) is normalizable , but he warns that such `` distributions must be interpreted with care",
    " one must always check that the posterior distribution has a finite integral and a sensible form . '' artificially imposing boundaries outside of what can be reasonably expected does not free us from this responsibility . in either case , if our posterior distribution is unexpected , we must investigate why .",
    "further , by not imposing strict bounds , we simplify the code and ensure that we do not exclude solutions that may actually be allowed . in practice , this should make no difference as long as the bounds chosen were sufficiently conservative so as not to bias the result .",
    "if the data provide no constraint , we would find that our posterior distribution was unbounded , but this would be obvious because our chains would not converge .",
    "@xcite suggested parameterizing a time in the orbit with a mean anomaly , @xmath71 , at some arbitrary zero - point , @xmath72 .",
    "however , @xmath71 is trivially related to @xmath53 ,    @xmath73    so , modulo comparatively minor covariances with @xmath52 , it is just as covariant with @xmath36 and @xmath37 as @xmath53  is .",
    "imagine , when @xmath36 is very nearly zero , the periastron is poorly defined , and may swing wildly from point to point in the orbit at each step in the chain .",
    "this means that @xmath53 , and therefore @xmath71 , must randomly swing wildly in the same direction or the model would be out of phase with the data and the step would be rejected .",
    "this is the essence of why covariant parameterizations are inefficient .",
    "@xcite suggested many other alternative parameterizations to aide convergence , including @xmath74 for low - eccentricity orbits , and @xmath75 for high eccentricity orbits , where @xmath76 is a an arbitrary zero point in the true anomaly .",
    "these approximate the angular position of the star relative to the plane of sky at a given reference time , and thus are an attempt to compensate for the poorly - constrained @xmath37 ( the angle between the angle on the sky and periastron ) . looking back at equation  [ eq : specialtrueanoms ] , however , we see that each of those special times correspond to a fixed angle along the orbit with respect to the plane of the sky with no approximation whatsoever . indeed , any true anomaly of the form @xmath77 , where @xmath78 is a constant , is stationary on the sky .",
    "since all such times are similar , we use @xmath56  for its practical uses  @xmath56  is the time we want to look for transits ( and is the reason we first considered this family of parameterizations ) .",
    "in addition to the faster convergence time , there are other major advantages to this parameterization .",
    "@xmath56  is usually much better constrained ( smaller uncertainties ) than @xmath53 , we no longer need to tune the parameterization for each system , as @xcite recommends , and @xmath56  is a parameter of general interest  either when the planet transits , or in the case of rv planets , when we may wish to search for transits .",
    "one complication of using @xmath56  is that it could take on any value that differs by integer multiples of the period and it would have no effect on the derived model .",
    "however , the derived uncertainty of @xmath56 , and its covariance with @xmath52 , is strongly dependent on the choice of the zero - point .",
    "it is usually best constrained and least covariant closest to the error - weighted mean of the input times , and that is what we use to run the markov chain .",
    "an alternative parameterization , which we do not adopt but has its appeal , is @xmath79 and @xmath80 , the times of the first and last transit in the data set , instead of @xmath56  and @xmath52 .",
    "since we almost always know the number of periods , @xmath81 , in between with no ambiguity , @xmath52 can be derived trivially at each step ( @xmath82 ) , and unlike @xmath56  and @xmath52 , @xmath79 and @xmath80 are completely uncorrelated .",
    "finding the global minimum @xmath11  to a radial velocity data set can be extremely complicated because of the large volume of parameter space with widely - separated local minima .",
    "fortunately , this process can be greatly simplified if the orbit is circular , in which case the @xmath38  term drops out of equation  [ eq : rv ] and @xmath83 simplifies to the the mean anomaly , @xmath84 . then , the rv equation can be re - written as    @xmath85    where @xmath86 and @xmath87 are arbitrary coefficients . in this formulation , @xmath86 , @xmath87 , @xmath44 , and @xmath45",
    "are all linearly related to the rv , and so for a given @xmath52 , the values of all other parameters that minimize @xmath11  can be found analytically ( e.g. * ? ? ?",
    "thus , there is only one non - linear parameter , @xmath52 , which can be quickly stepped through while the others are solved analytically at each step .",
    "from the best - fit values of @xmath86 and @xmath87 , the parameters of interest can be determined ,    @xmath88    where @xmath89 represents @xmath90 to @xmath91 , but when the sign of the numerator and denominator are known independently , the precise inverse mapping to the full range @xmath92 to @xmath43 can be determined .",
    "this notation is commonly used in computer programming , and is sometimes called atan2 . ] , and @xmath91 comes from the definition that @xmath66 for circular orbits .",
    "this is the basis for the lomb - scargle periodogram @xcite .",
    "the optimal period spacing requires that we sample finely enough such that the difference in phase between the first and last observation changes by @xmath93 radian for each step @xmath94 .",
    "this is because when the phase changes by @xmath43 radians , the rv will have an opposite sign and give a poor fit to the data .",
    "this results in the criteria that @xmath95 , where @xmath96 is the duration of the observations .",
    "typically , a scan through periods in this way will reveal several peaks in likelihood , due to the planetary orbit and aliases thereof .",
    "these provide good starting points for more rigorous , fully non - linear local minimization routines which include eccentricity .",
    "@xcite suggest a parameterization that reduces the number of non - linear parameters of the full , keplerian orbit from five per planet to three per planet , which would help robustly fit the full keplerian solution .",
    "however , since we are only concerned with single - planet systems here , this method is not required to quickly and robustly find the best fit , and our concern about hybrid fitting in ",
    "[ sec : hybrid ] trumps the benefit of this parameterizaton during mcmc fits .    sometimes , because of aliases",
    ", multiple periods will provide similarly good fits .",
    "in such cases , each period should be investigated individually , as amoeba is unable to find minima that are widely separated from the initial values .",
    "we do not yet employ the more sophisticated technique outlined by @xcite to find the best periods among aliases , as this is usually unnecessary .",
    "when there is clearly a unique period associated with the best fit , amoeba will give us a robust result .",
    "given adequate phase coverage over one or more complete orbits , this method can robustly fit most single - planet systems without any special effort .",
    "those with very high eccentricities or poor phase coverage can also often be fit , but sometimes need slight adjustments to the default period range or number of minima to explore .",
    "for example , fitting the data from @xcite for hd 80606b ( @xmath97 ) required searching the 100 likeliest peaks in the lomb - scargle periodogram , whereas the default is 5 .",
    "different groups have chosen to parameterize the eccentricity , @xmath36 , and argument of periastron , @xmath37 , in several different ways .",
    "this is done ostensibly for three different reasons : to apply the appropriate prior , make the chains converge faster , and ease and simplicity in programming .",
    "it is unclear what a good , physically - motivated prior for the eccentricity of a planet is , particularly given the complications of tidal circularization .",
    "still , it is commonly assumed that a uniform prior in @xmath36 is best , and we do the same .",
    "therefore , we explored the advantages of the most common ways to parameterize the eccentricity , specifically , @xmath38  and @xmath98 ; @xmath36 and @xmath37 where @xmath99 and @xmath100 , and @xmath101  and @xmath102 .",
    "we found subtle differences in the different parameterizations , which are worth further exploration .    in appendix",
    "[ sec : nege ] , we discuss a potentially - useful new parameterization , @xmath36 and @xmath37 where @xmath103 and @xmath37 is unbounded .",
    "we explain why the models are continuous across the @xmath104 boundary , but ultimately , we found it inferior to our preferred parameterization and do not endorse it for our particular application .",
    "to ensure our prior distribution is what we expect ( i.e. , uniform ) , we computed the prior distributions of @xmath36 for each of the above parameterizations by running a standard markov chain , but setting the @xmath11  to 1 as long as the chain made an allowed step . if the chain makes a disallowed step , we set the @xmath11  to infinity .",
    "since this results in a uniform likelihood surface in the allowed region , the posterior distribution is equal to the prior distribution .",
    "when parameterizing in @xmath38  and @xmath98 , we solve for @xmath36 and @xmath37 at each step , and set the @xmath11  to infinity if @xmath105 .",
    "as noted by @xcite , and shown in figure  [ fig : eprior ] in red , we clearly see a linear prior in @xmath36 .",
    "ford said that we must correct for the linear prior in @xmath36 during an mcmc fit by weighting the stepping probability by the jacobian of the transformation between the parameters in which we step and the parameters we desire to be uniform . in the case of stepping in @xmath38  and @xmath98 , the jacobian to transform to",
    "@xmath36 and @xmath37 is @xmath36 , so we must weight the stepping probability by @xmath106 , where the subscript @xmath6 denotes the current link in the chain . this will preferentially reject steps to higher eccentricity and nearly recovers the uniform prior in @xmath36 , as shown in green in figure  [ fig : eprior ] .",
    "however , due to the singularity at @xmath104 , there is a very slight overcorrection at @xmath104 .",
    "while stepping in @xmath101  and @xmath102  ( black ) , @xmath38  and @xmath98  without correcting for the linear prior ( red ) , with correcting for the linear prior ( green ) , and stepping in @xmath36 and @xmath37 directly ( blue ) .",
    "we note that the @xmath101 , @xmath102and @xmath107 parameterzations correctly reproduce the uniform prior , but the @xmath38 , @xmath98without correcting for the prior shows a clear linear trend and is obviously wrong . even correcting for the prior",
    ", there is a slight overcorrection at @xmath104.,width=312 ]    another popular parameterization ( e.g. , * ? ? ?",
    "* ) is @xmath101  and @xmath102 .",
    "since the jacobian to @xmath36 and @xmath37 is a constant 1/2 , it can be ignored when computing the stepping probability , and it naturally recovers a uniform prior in @xmath36 and @xmath37 , as shown in black in figure [ fig : eprior ] .",
    "this works by essentially taking smaller steps in eccentricity near zero which exactly compensate for the smaller area at @xmath104 in @xmath38 , @xmath98  space .    stepping directly in @xmath36 and @xmath37 obviously results in a uniform prior ( shown in blue ) , where we set the @xmath11  to infinity if our step took us to any of @xmath108 , @xmath105 , @xmath109 , or @xmath110 .",
    "the primary motivation often cited for using a different parameterization of @xmath36 is that the convergence time is reduced . to test this , we simulated 100 data sets similar to @xcite .",
    "in particular , we chose @xmath111 m / s , @xmath112 days , @xmath113 , and @xmath114 m / s , ( @xmath45  was fixed at zero ) with 80 evenly spaced data points over a span of 30 periods and gaussian random uncertainty with a 1-sigma width of @xmath115 m / s .",
    "the major differences from @xcite are that our times were evenly distributed rather than distributed according to the observing times of the california and carnegie planet search program , and we input hot - jupiter - like parameters for those which were not explicitly stated .",
    "neither of these should have an appreciable impact on our comparison .",
    "we fit these 100 simulated data sets as described in ",
    "[ sec : rvbestfit ] with each of the three eccentricity parameterizations , repeating the procedure for each of the 15 eccentricities listed in table  [ tab : nsteps ] ( for a total of 4500 fits ) .",
    "the uncertainty in the eccentricity , as found by our markov chain , was approximately 0.007 in each case , so our steps in eccentricity roughly correspond to 0 to 10 sigma significant eccentricity . for a more direct comparison with @xcite , we also include eccentricities of 0.01 , 0.1 , 0.5 and 0.8 .",
    "each set of 100 simulated rv curves had the same gaussian `` random '' noise to make the comparison more robust .",
    "we then recorded the number of steps each fit took until the chain was well - mixed , according to the criteria outlined in  [ sec : mcmc ] .",
    "table  [ tab : nsteps ] shows the log of the median number of total accepted steps in all chains until convergence for each of the four parameterizations of eccentricity , as a function of eccentricity , in addition to the best value from all parameterizations proposed by @xcite , where applicable . since the execution time is proportional to the number of steps in the chain ( plus small overheads ) , this is a convenient , computer - independent way of determining how efficient the markov chain is .",
    "for reference , for the @xmath104 case parameterized as @xmath101 , @xmath102 , the fit took about 15 seconds on a standard desktop computer purchased in 2009 .    for moderate eccentricities , @xmath38 ,",
    "@xmath98  was slightly faster than @xmath101 , @xmath102 . both were nearly twice as fast as @xmath36 , @xmath37 at low eccentricities .",
    "even compared to @xcite , which uses a significantly more complicated ( system - dependent ) parameterization , our lowest eccentricity case was about four times faster , and our highest eccentricity case was about 50% faster .",
    "however , at moderate eccentricities , @xcite was 2 - 4 times faster . given the consistency of the number of steps we took as a function of eccentricity and the relatively larger variability of ford s , some of both our observed benefit and deficiency may be due to random variability of ford s chains . to give the reader an idea in the variability of our chains , our best chains ( as opposed to the median values quoted in the table ) at @xmath116 and @xmath117 took log 4.31 and 4.35 steps , respectively , while our worst chains for @xmath118 and @xmath119 took log 4.89 and 4.82 steps , respectively .",
    "c|cccc 0.000 & 4.62 & 4.62 & 4.77 &  + 0.007 & 4.58 & 4.62 & 4.76 & ",
    "+ 0.010 & 4.57 & 4.62 & 4.76 & 5.2 + 0.014 & 4.57 & 4.64 & 4.76 &  + 0.021 & 4.53 & 4.62 & 4.76 &  + 0.028 & 4.53 & 4.57 & 4.62 &  + 0.035 & 4.53 & 4.57 & 4.57 &",
    " + 0.042 & 4.53 & 4.54 & 4.57 &  + 0.049 & 4.53 & 4.53 & 4.57 &  + 0.056 & 4.53 & 4.53 & 4.57 &",
    " + 0.063 & 4.53 & 4.57 & 4.53 &  + 0.070 & 4.53 & 4.53 & 4.57 &",
    " + 0.100 & 4.53 & 4.53 & 4.53 & 3.9 + 0.500 & 4.53 & 4.53 & 4.53 & 4.2 + 0.800 & 4.53 & 4.53 & 4.53 & 4.7 [ tab : nsteps ]    despite requiring more chains ( twice the number of fitted parameters , or 12 in this case ) , a significant fraction of the advantage we observe relative to @xcite ( who used 10 chains ) is due to the de - mc algorithm .",
    "however , using a standard implementation of the markov chain , we were still faster than ford in the low eccentricity cases due to our use of @xmath56  rather than the various combinations of @xmath53 , @xmath71 , and @xmath37 ( see  [ sec : rvparam ] ) .",
    "the final consideration when picking a parameterization are the practical advantages of implementing each .",
    "we did not implement the system - dependent parameterizations suggested by @xcite . from a practical sense , this is clearly the most difficult , though it may be worth it in the moderate - eccentricity cases , given the results in table  [ tab : nsteps ] .",
    "if desired , this can be done with relatively minor changes to the code .    stepping directly in @xmath36 and @xmath37",
    "is intuitive , but dealing with a periodic angular parameter introduces a number of special cases . in particular , the periodic boundary of @xmath37 can confuse the amoeba algorithm and it may not find the best - fit value . during the mcmc fit , we must be careful to take the modulo whenever it crosses a periodic boundary ( i.e. , @xmath120 )  otherwise , it would be free to jump between equivalent , widely - separated minima .",
    "however , the de - mc algorithm would fail if the preferred value were near the boundary so that it could draw steps from both sides to determine the step size .",
    "further , we may get unlucky and find that our probability distribution function lies on a boundary or , when it is poorly constrained , it is possible for a significant amount of power to span the entire range of @xmath51 . in either of these cases ,",
    "the median value , which is required to calculate the convergence criteria and is often used as the final value , would be heavily biased . in order to account for this",
    ", we must first center the distribution about the mode , such that the values are within the range mode @xmath120 before we calculate the median .",
    "additionally , our scheme of finding the appropriate step size would fail if the angle is so poorly - constrained that no value of @xmath37 produced a @xmath121 .    stepping in @xmath38  and @xmath98",
    "eliminates this complicating angular value during the markov chain , but introduces an arguably more complex requirement to deal with the jacobian .",
    "the @xmath11  routine is required to return a determinant ( even if it is 1 for no transformation ) to make the priors more obvious to the end - user .",
    "after this , the mcmc routine handles the determinant weighting transparently in order to transform to the desired prior .",
    "so with our code , using a jacobian is trivial to deal with .",
    "however , as seen in figure  [ fig : eprior ] , this does not completely correct the prior .",
    "stepping in @xmath101  and @xmath102  eliminates the jacobian and frees us from most of the burden of dealing with angular parameters , and is therefore practically the simplest .",
    "additionally , it is comparable or faster than other parameterizations , and recovers a precisely uniform prior in @xmath36 . for these reasons ,",
    "we use it in exofast .",
    "it has long been understood that there is a bias against low eccentricities in binary systems .",
    "such a bias is extremely important to understand , as many of the observed systems are expected to be tidally circularized .",
    "if they are not , it has profound implications for our understanding of tides ( and the tidal q factor ) , the existence of additional bodies in the system which may be perturbing their orbits , and the formation and evolution of planets as a whole .      the bias against @xmath104 that most people are aware of was first quantified by @xcite in the case of binary stars , and is due to the fact that there is zero phase space at exactly @xmath104 , and",
    "therefore observational uncertainties will produce a best fit that is biased toward a positive value , even for intrinsically circular orbits .",
    "they say that , in order to measure a non - zero eccentricity with a 95% confidence , one must measure a result of @xmath122 , rather than the naively - expected @xmath123 , where @xmath124 is the standard deviation of the eccentricities .",
    "we simulated 100,000 data sets of intrinsically circular orbits with different noise like those described in ",
    "[ sec : nsteps ] , and found the best - fit eccentricity using amoeba for each .",
    "the resultant histogram of best - fit eccentricities is shown in figure  [ fig : edist ] in cyan .",
    "this is the eccentricity distribution one would measure from a bootstrap analysis , similar to that described in @xcite , and clearly shows this deficit at @xmath104 .    ) , plus the distribution of best - fit values from 100,000 amoeba fits ( cyan ) , demonstrating the problem with bootstrap analyses of bounded parameters , the linear prior , and the very slight problem with the corrected @xmath38 , @xmath98  paramterization.,width=312 ]    for comparison , we also plot the combined pdfs of the 100 trials of mcmc fits described in  [ sec : nsteps ] for each of the parameterizations of eccentricity .",
    "we clearly see the problem of using a linear prior ( red ) .",
    "the other distributions look similar , but a close inspection shows the @xmath38 , @xmath98  distribution is slightly biased high at @xmath104 because of the slight bias in the prior distribution .",
    "fortunately , the difference between the parameterizations is negligible relative to the width of the gaussian , so it is of little practical importance .",
    "now a huge advantage of mcmc becomes apparent : instead of looking at simulated permutations of the data and finding the best fit ( which has zero likelihood because it is infinitesimally small ) like a bootstrap or prayer bead analysis does , mcmc considers the data as is .",
    "it is clear that the pdfs from the mcmc do not suffer from this bias to nearly the same extent , but there is still the matter of how to summarize such a non - gaussian distribution .",
    "obviously , the standard method of quoting a median value and a 68% confidence interval provides a misleading , marginally - significant , non - zero eccentricity  even the absolute value of a gaussian peaked at zero has a median value of 0.67 sigma .",
    "instead , we could fit a 3-parameter gaussian to the pdf ( normalization , zero point , and width ) and use the zero point as the likely value and the width as its uncertainty . in principle , we can actually infer a negative eccentricity , dramatically reducing the lucy - sweeney bias . alternatively , we can quote an upper limit which does not assume the pdf is gaussian , but does not give us a likely value or uncertainty . in the end , however , there is no substitute for a visual inspection of the pdf . by default",
    ", exofast simply quotes the median values and 68% confidence interval as with all other parameters .",
    "therefore , it is up to the user to inspect the pdf and assess the significance of the output eccentricity .",
    "recently , @xcite introduced a new bayesian method to evaluate the robustness of a measured eccentricity , which may help .    to see how this bias evolves with the significance of eccentricity , we calculated both the median and fitted values of the eccentricity of each of the 100 fits and averaged them together in order to reduce the statistical uncertainty and flesh out the bias .",
    "this mean of medians is plotted as a function of intrinsic eccentricity in figure  [ fig : ebias ] in solid lines , and the fitted gaussian zero - points are shown as dashed lines for each of the the parameterizations described above .",
    "a non - biased result would fall on the dotted line .",
    "as expected from the lucy - sweeney bias , we over - estimate the eccentricity of orbits with intrinsically small eccentricities .",
    "the uncertainty in each value is roughly 0.007 .     for legend ) .",
    "the solid lines are the typical median value , while the dashed lines are the zero point of a 3-parameter gaussian fit to the pdf .",
    "we clearly see the effect of the lucy - sweeney bias at low eccentricities , and the additional bias due to the linear prior for all eccentricities .",
    "the values plotted are the average of the median / fitted values for all 100 fits of simulated data .",
    "the statistical uncertainties for each of the 100 fits was roughly 0.007 at each point .",
    "the small , systematic offset between the measured and intrinsic eccentricities at high eccentricites for all methods is within the uncertainties given we only ran 100 trials.,width=312 ]    the maximum measured eccentricity deviation out the 100 trials was slightly more than @xmath125  slightly more than we would expect by chance . in the real world with real data , systematic uncertainties , unmodeled effects , or outliers will all tend to make the eccentricity appear even larger than this data set simulated with white noise .",
    "@xcite pointed out this bias in the case of planetary systems , though they incorrectly called it a lutz - kelker bias @xcite .",
    "the lutz - kelker bias is actually a volume effect described in the case of trigonometric parallaxes .",
    "while parallax is a positive - definite parameter which suffers from a lucy - sweeney bias too , the lutz - kelker bias exists at all values of parallaxes .",
    "it states that , due to observational uncertainties and the fact that the number density of stars is larger for those with a smaller parallax , more stars tend to have a true parallax that is smaller than what is measured .      as shown in ",
    "[ sec : eprior ] , if we step in @xmath38  and @xmath98 , we introduce a linear prior .",
    "this prior is clearly not supported by the observations of short - period planets to date , so failing to correct for it will therefore lead to a significant ( up to @xmath126 ) bias in the inferred eccentricity , as shown in figure  [ fig : ebias ] in red .",
    "since there is very little difference in the convergence time , many may find it easier to step in @xmath101  and @xmath102 , as we do .",
    "another bias comes from an incorrect , but potentially - common , implementation of the metropolis - hastings algorithm . as we discussed in ",
    "[ sec : mcmc ] , when we reject a step , we must make a copy of the previous step in its place .",
    "since the acceptance rate is ideally around 20% , we will end up with each step copied , on average , five times .",
    "this algorithm is somewhat unintuitive : we might think we would end up with huge spikes in the parameter distributions where the chain got stuck ( fortunately , not true ) , or that making 5 copies of each step is wasteful ( and it is ; see the discussion on thinning in  [ sec : mcmc ] ) .",
    "instead , we might think that we should not copy the previous step .",
    "the effect of making this mistake is minimal for unbounded parameters , which makes it difficult to identify with typical sanity checks .",
    "however , this misleading intuition can introduce a significant bias that guides fits away from any hard boundary , such as @xmath104 .",
    "unfortunately , an obsolete version of an mcmc code which was distributed with the idl astronomy library made this exact mistake . those who use this code , model their own codes from it , or make the same intuitive mistake will all suffer from this bias .    a more subtle way to make",
    "essentially the same error is in the boundary handling .",
    "the proper meaning of a boundary ( e.g. , @xmath127 ) is that the model s likelihood is zero ( or the @xmath11  is infinite ) beyond it .",
    "however , we can not a priori restrict the model to step only in bounds . while it may seem more efficient not to allow the markov chain to step out of bounds",
    ", we must allow the markov chain to go out of bounds , get rejected , and make a copy of its previous step in the process .",
    "we make these boundary conditions intuitive and fast in our @xmath11  routines by checking them first and returning an infinite @xmath11  if it is out of bounds .",
    "the mcmc routine then automatically assigns a zero likelihood to this step and will always reject it , making a copy of the previous step in the process and preserving the meaning of boundaries .    to demonstrate why copying this step is necessary",
    ", we repeat the exercise from  [ sec : eprior ] , but without copying the previous step when a step is rejected .",
    "the resultant prior distributions of eccentricity are shown in figure  [ fig : epriorzoom ] , which shows the prior probability is strongly attenuated near imposed boundaries .",
    "this , in turn , will bias the inferred values away from said boundaries .",
    "the depth of this attenuated region is proportional to the step size , which is typically equal to the uncertainty in the parameter .",
    "we note that the affected region plotted here is exaggerated because the de - mc code automatically chooses a large stepsize to efficiently fill all of the likelihood space .",
    "however , for values near ( @xmath128 times the step size ) a boundary , the prior probabilities are still significantly impacted by this bias .    .",
    "the detailed effect this error has depends on the particular parameterization of eccentricity , and the step size , which is ideally equal to the uncertainty . in this plot , the attenuation region",
    "is exaggerated because the differential evolution code picks a step size that is large enough to efficiently fill all of the allowed likelihood space , and thus is attenuated by both boundaries at once .",
    "however , this attenuation is significant as long as the median value is within a few stepsizes of a boundary .",
    "this shows a strong a priori bias against boundaries like @xmath104 when the mh algorithm is improperly implemented.,width=312 ]    unfortunately , it is difficult to say how prevalent these latter two biases may or may not be in the literature , or , in the case of the improper mh algorithm , even how significant it is if we know it is present , since the bias depends on the parameterization and even the step size .",
    "the larger the step size , the larger the bias ( due to the larger attenuated prior region ) .",
    "while the ideal step size is roughly equal to the one - sigma uncertainty , there is no guarantee that a suboptimal step size was not used , because it is not supposed to matter .",
    "nevertheless , some combination of these latter two biases may at least partly explain the large number of nominally - significant eccentricities , even after accounting for the lucy - sweeney bias , for systems that are expected to be tidally circularized .",
    "a primary transit occurs when a planet passes in front of its star and blocks a portion of its light for a period of time .",
    "we monitor the star s brightness during this time by taking repeated exposures , and then comparing the target star s brightness to an ensemble of comparison stars .",
    "if the star were uniformly bright , the relative flux we would see during transit would be :    @xmath129\\ ] ]    where @xmath130  is the baseline flux and analytic equations for @xmath131 are defined in ( eq . 1 , * ? ? ? * ) .",
    "@xmath131 is solely a function of the transit geometry , @xmath132 , which is the radius of the planet in stellar radii and @xmath133 , which is the projected distance from the center of the planet to the center of the star , in stellar radii and is a function of time .      in reality , stars are not uniformly bright  for typical broad - band optical / nir filters , their apparent brightness falls toward the limb of the star , which is an effect called limb darkening . for main sequence stars ,",
    "the intensity of the star , @xmath134 , is well - described as functions of @xmath135 , where @xmath136 is the angle between the observer and the normal vector on the surface of the star  that is , @xmath137 at the center of the star , where the normal vector points directly at the observer , and @xmath138 at the limb of the star , where the normal vector is perpendicular to the observer s line of sight .",
    "there are many different ways to parameterize the intensity of the star .",
    "we will discuss the most commonly - used laws for transiting planets .",
    "the linear limb darkening law ,    @xmath139    where @xmath140 is a limb - darkening coefficient , was the first obvious choice , but it quickly became clear it was insufficient to describe real surface brightness profiles ( e.g. * ? ? ?",
    "* ) . for the precision of many ground based transits ,",
    "the linear law is still sufficient , but the light curves from _ hubble space telescope _ for hd 209458b showed the linear limb - darkening law to be inadequate for high - precision transit light curves @xcite .",
    "thus , many have adopted a quadratic limb darkening law of the form :    @xmath141    @xcite state that the quadratic limb darkening law is sufficient to describe transit light curves with a precision of @xmath142  a precision that has never been achieved from the ground .",
    "this was confirmed empirically by @xcite , who showed that the quadratic limb darkening law was sufficient for the quality of data then achievable .",
    "however , the accuracy of the quadratic law is worse than the 20 parts per million that is achieved routinely from _ kepler",
    "_  @xcite for large planets ( @xmath143 ) . for these planets , _ kepler _  must therefore use a non - linear limb darkening law of the form ,    @xmath144    unfortunately , our theoretical predictions of the coefficients has proven insufficient for very precise light curves ( e.g * ? ? ?",
    "* ) , and either the limb - darkening needs to be fit by the data , or perhaps newer , more precise models based on 3d hydrodynamical models of stellar atmospheres may be sufficient @xcite .",
    "since the transit flux given by the quadratic limb - darkening law is applicable to all but the most precise light curves and is significantly faster to compute than the non - linearly limb - darkened transit flux , we limit our discussion to the quadratic limb darkening law . note that in the discussion that follows , we can reproduce the linear law precisely by fixing @xmath145 to be zero .",
    "@xcite give the quadratically limb - darkened flux during transit as :    @xmath146 - u_2\\eta^{d}}{1-u_1/3-u_2/6}\\right)\\ ] ]    where @xmath147 , and @xmath148 are given in table 1 of @xcite for all possible geometries .",
    "like @xmath131 , @xmath147 and @xmath148 only depend on @xmath149 and @xmath150 .",
    "@xmath151 is a step function equal to 1 where @xmath152 and 0 elsewhere .    as a side note , in appendix",
    "[ sec : analyticld ] , we show that both the quadratic and linear limb darkening flux during transit can be can be written as linear combinations of analytic functions .",
    "therefore , the limb darkening coefficients can be solved analytically for fixed @xmath149 and @xmath150 .",
    "given these analytic expressions , generating a model lightcurve becomes a matter of computing @xmath150 for all times , which is similar to computing the rv .",
    "first , we calculate the true anomaly in the same way as before ( i.e. , using equations  [ eq : trueanom ] ,  [ eq : eccenanom ] , and  [ eq : meananom ] ) .",
    "then , the three - space coordinates of the planet s position relative to the star , as seen from earth , are    @xmath153    where @xmath154 is the distance from the center of the star to the center of the planet as a function of time , @xmath155 is the semi - major axis , and @xmath156  is the stellar radius . some have opted to mix @xmath37 and @xmath157 at this point  while using @xmath157 makes more intuitive sense , we feel the consistent use of one value for the argument of periastron reduces the chance of accidentally misapplying one or the other , and @xmath37 is already widely in use .",
    "@xmath158 is along the line of sight , where @xmath159 is toward the observer , and the x - y plane is the plane of the sky .",
    "neither transits nor rv can constrain the longitude of the ascending node , @xmath160 , which is the angle from north to the ascending node , measured counterclockwise ( i.e. , the rotation of the x - y plane ) , but for completeness , the orientation with respect to an observer on earth is    @xmath161    where @xmath162 is east and @xmath163 is north . for concreteness , we assume @xmath164 , which means @xmath165 , @xmath166 , and @xmath167 .",
    "this implies that , during primary transit , the planet moves from @xmath168 to @xmath169 and at @xmath170 , @xmath171 is equal to the opposite of the impact parameter , @xmath172 . finally ,",
    "@xmath173    where @xmath150 is in units of stellar radii , exactly as required by the @xcite code .",
    "we must take care to note the sign of @xmath158  both transits and occultations occur when @xmath174 , but it is a primary transit when @xmath175 , and a secondary eclipse when @xmath176 .",
    "the calculation of the planetary path is done in our program exofast_getb and includes the general handling of @xmath160 , which would be useful if one would like to include astrometric measurements .",
    "while we do not support fitting the secondary eclipse , the calculation of its model flux is identical save a couple minor substitutions : @xmath149 becomes @xmath177 , @xmath150 becomes @xmath178 , and the limb darkening of the planet can be ignored . ]",
    "the resultant model is the observed flux from the planet as it is blocked by the star . in general",
    ", this will be some combination of thermal and reflected light , but without knowing the temperature , albedo , and thermal redistribution ( e.g. , from a phase curve ) , the two can not be distinguished , and thus only one additional parameter for each observed bandpass is required for the normalization of the planetary flux ( i.e. , the eclipse depth ) .",
    "this normalization is a linear parameter , but note the warning above about hybrid linear and non - linear fits .",
    "we stress that this is not the same as fitting different values of @xmath149 for the primary transit and secondary eclipse .",
    "the shape of the ingress / egress and the duration of the eclipse require @xmath149 to be the ratio of radii , not just the square root of the depth .",
    "we also need to define the parameterization of the transit light curve , which is much less obvious and has been done many different ways in the literature .",
    "most , however , have agreed upon @xmath56 , @xmath130 , @xmath69 , and the quadratic limb darkening parameters @xmath179 and @xmath145 . while the eccentricity is also required to derive the model , it is often fixed at the best - fit values from the rv .",
    "the remaining parameters , which determine the shape of the transit , have no universally - accepted parameterization , likely because each parameterization has its own advantages and disadvantages .",
    "@xcite suggested @xmath180 , the total duration of the transit ( first to fourth contact ) , @xmath181 , the duration of the flat part of the transit ( second to third contact ) , and the transit depth , @xmath182 ( for non - grazing transits with no limb - darkening , @xmath183 ) .",
    "@xcite suggested that a less - correlated parameterization would be @xmath184 , the duration of ingress or egress ( i.e. , from first to second contact or third to fourth contact ) , @xmath96 , the duration from mid - ingress to mid - egress , and @xmath182 .",
    "unfortunately , both of those parameterizations are undefined for grazing geometries , and therefore it is impossible to correct the prior distributions to be physical for all geometries .",
    "grazing transits would be poorly fit , and near - grazing geometries may be unfairly biased by the parameterization ( see @xcite for a discussion ) . for this reason , we advocate a more physically - motivated parameterization : @xmath185 , @xmath186 , and @xmath149 .",
    "the advantage to this parameterization is that it intuitively imposes reasonable priors on the physical parameters , and is well - defined for all geometries .",
    "the disadvantage is that they are further removed from what is actually measured ( the shape of the transit ) and the covariances between these parameters is large .",
    "fortunately , the de - mc algorithm automatically takes the covariances into account , so this is much less important .      a lucy - sweeney - like bias exists for all bounded parameters , which includes @xmath186 ( in the case of transiting planets when we can not distinguish between @xmath187 ) , and @xmath149 .",
    "unfortunately , the lucy - sweeney - type bias for @xmath186 is unavoidable , but unlike eccentricity , where we expect hot jupiters to be tidally circularized and therefore @xmath36 to be exactly 0 , there is no reason to expect a planet to be exactly edge on .",
    "therefore , we are significantly less likely to encounter such a bias .",
    "further , the theoretical interpretation of such a system does not qualitatively change if we measure a small inclination , whereas a small non - zero eccentricity for a planet that is supposed to be tidally circularized requires exotic explanations , such as anomalous values of the tidal q factor or additional bodies perturbing the system . however , we still need to be careful not to overinterpret nominally - significant impact parameter changes if they are close to 0",
    ".    we can avoid a bias in @xmath149 altogether by thinking about how our model would behave if negative values were allowed .",
    "while unphysical , we can make the identification that a negative planetary radius would add flux during transit .",
    "to that end , we allow negative planetary radii , calculate the flux decrement as if it were positive , and then add the flux to the baseline rather than subtract it .",
    "this avoids the lucy - sweeney - type bias , since a negative value of @xmath149 implies a unique , well - defined likelihood .",
    "if the median @xmath149 is negative with low significance , it is likely there is no transit at all .",
    "if it is negative with high significance , there are likely large systematics in the data .",
    "if , however , we see a small tail at negative values but the result is statistically significant , we can be more confident that the non - zero measurement is real , and not a result of a bias in fitting",
    ". this will be particularly useful when measuring small transits with low significance , whose depths would otherwise tend to be overestimated , just like eccentricity .",
    "we could achieve a similar effect for secondary eclipses by allowing the normalization to be negative .",
    "while we choose to step in @xmath149 , the same trick could be played with @xmath182 . however , since @xmath149 is required to calculate the model transit , and @xmath188 , we would have to redefine @xmath189 to avoid an imaginary @xmath149 .",
    "it is not yet clear which would impose a more physical prior .",
    "one may consider stepping in @xmath190 , which has no such positive - definite requirement , similar to our steps in @xmath191 , @xmath192 , or @xmath193 .",
    "however , in the case of @xmath149 , systematics in the data may be present which mimic a negative @xmath149 , which is not the case for the other parameters .",
    "if our model is not allowed to fit such a systematic , we would unfairly bias the result toward positive values .",
    "transits are first identified in transit surveys using data from relatively small telescopes that monitor large areas of the sky at once ( tres , @xcite ; hatnet , @xcite ; xo , @xcite ; corot , @xcite ; superwasp , @xcite ; kelt , @xcite ; kepler , @xcite ; qes , @xcite ) .",
    "they use a box least - squares ( bls ) algorithm @xcite , which extracts the duration , depth , period , and @xmath56  of the transits . for the high - precision transits which this code was designed for , these quantities",
    "will be already roughly known , either from the literature or bls fits to their own survey data . with these parameters ,",
    "the problem is greatly simplified to finding a local minimum around relatively well - behaved region of parameter space .    with good starting values for @xmath52 and @xmath56",
    ", we can begin with fairly generic guesses for the rest of the parameters and standard local minimization routines like amoeba work well to find the local minimum .",
    "once found , we follow the same procedure as the rv data (  [ sec : rv ] ) and scale the uncertainties to get @xmath16 .",
    "usually , transit fits of survey - quality data are too degenerate to robustly fit for the impact parameter , and it must instead be fixed to zero ( i.e. , central crossing ) . however , because we simultaneously fit the stellar properties ( see  [ sec : rvtran ] ) , our code has been tested on kelt survey data and works well when given a good starting value for @xmath56  and @xmath52 from an independent run of bls .",
    "therefore , exofast may also be a useful tool for vetting grazing eclipsing binaries from survey data .",
    "for simultaneous fits to rv and transit data , the models themselves are the same , but the advantage of fitting the data simultaneously is that they both constrain many of the same parameters , which improves the quality of both fits and ultimately gives us a clearer picture of the system as a whole .",
    "further , we can include additional effects with no penalty , such as the light travel time difference due to the reflex motion of the star (  [ sec : reflex ] ) .",
    "more important , covariances between parameters in the different data sets may be unintuitive and non - negligible . fitting the two data sets",
    "separately assumes the covariances between the parameters in the two data sets are zero , whereas a simultaneous fit naturally takes these covariances into account .      the disadvantage of a simultaneous fit is having to rethink the parameterization of the problem , since the overlapping constraints are not always intuitive .",
    "the parameters @xmath69 , @xmath101 , @xmath102 , and @xmath56  trivially overlap between the two data sets .",
    "@xmath70 , @xmath44 , @xmath45 , are still mostly independent parameters for rv , and @xmath130 , @xmath186 , @xmath194 , and @xmath149 are mostly independent for photometry .",
    "however , the combined parameterization is actually a one - parameter family of solutions , meaning that with an estimate of the mass of the star , radius of the star , or a clever combination of the two , we can solve the entire system precisely , including the stellar mass and radius . of course , the minimum mass of the planet , @xmath195 , can not be determined from rv without estimating the mass of the primary ( even if we assume @xmath196 ) , and all of the physical parameters from the transit scale with @xmath156 ( and we must assume @xmath196 ) , which the transit can not constrain @xcite .",
    "since we must use external information anyway , it behooves us to do it during each step in the markov chain and use all of the information of the two data sets to their full advantage while simultaneously exploring the covariances between all parameters .",
    "the stellar surface gravity , @xmath197 ( often measured as @xmath198 ) , is equal to @xmath199 , and is one clever combination of stellar parameters that allows us to break the degeneracy . with that , kepler s law , and equation  [ eq : kp ] , the semi - major axis of the planet s orbit , true mass of the planet , mass of the star , and radius of the star , all in physical units with no approximation , become a function of observed quantities :    @xmath200    with the approximation that @xmath201 , the latter terms in the equations for @xmath155 and @xmath202  drop out , and @xmath40 ( i.e. , radial velocity ) is no longer required , meaning we could apply this technique generally to transit fits alone ( of course , losing our constraint on @xmath203 ) .",
    "in fact , this is how we actually fit transit - only data sets .",
    "however , we have no constraint on @xmath198  from the transit or radial velocity alone , so simply adding this parameter to the model without additional information will force the markov chain to inefficiently explore this degeneracy .",
    "this problem is most usually solved by iterating between light curve and rv fitting and isochrone modeling ( e.g. , * ? ? ?",
    "* ) to get the model parameters ( e.g. , * ? ? ?",
    "* ) , but that sort of iteration does not properly account for covariances between the stellar and planetary parameters .",
    "worse , those fitting follow - up light curves almost always assume the fixed stellar parameters derived from the discovery paper and simply ignore the inconsistency between the stellar density implied by their new light curve and their assumed stellar parameters .",
    "recently , @xcite determined an empirical polynomial relation between the masses and radii of stars , and their @xmath198 , effective temperatures , @xmath204 , and metallicities , @xmath205 $ ]  ( see their table 4 ) based on a large sample non - interacting binary stars in which all of these parameters were well - measured .",
    "this is essentially a computationally - convenient way of modeling isochrones , which imposes the same mass - radius constraint to break the degeneracy , but is fast enough to incorporate at each step in the markov chain .",
    "therefore , we add @xmath198 , @xmath204 , and @xmath205 $ ]  to our stepping parameters and , at each step , we use equation  [ eq : mrstar ] to derive the self - consistent @xmath202and @xmath156that is used to generate the model . finally , we calculate what the torres relations would predict for @xmath202  and @xmath156 , and apply a prior penalty to the @xmath206 for the difference between the torres values and our model values , using the scatter about their fitted relations ( @xmath207 and @xmath208 ) , as the prior width .",
    "the constraint on @xmath204  and @xmath205$]from the transit data , rv data , and torres relation is very poor , resulting in highly uncertain values for @xmath202  and @xmath156 .",
    "fortunately , @xmath198 , @xmath204  and @xmath205 $ ]  can be easily measured with a high - quality spectrum , and can then be applied as priors during each step of the fit for a precise estimate of the stellar parameters . therefore , our total @xmath11  at each step is    @xmath209}}\\right)}{\\sigma_{m , torres}}\\right)^2 \\\\   & + \\left(\\frac{{\\ensuremath{r_*}}- r_{torres}\\left({\\ensuremath{\\log g}},{\\ensuremath{t_{\\rm eff}}},{\\ensuremath{\\left[{\\rm fe}/{\\rm h}\\right]}}\\right)}{\\sigma_{r , torres}}\\right)^2 \\\\   & + \\left(\\frac{{\\ensuremath{\\log g}}- { \\ensuremath{\\log g}}_{spec}}{\\sigma_{{\\ensuremath{\\log g}},spec}}\\right)^2 + \\left(\\frac{{\\ensuremath{t_{\\rm eff}}}- t_{spec}}{\\sigma_{t , spec}}\\right)^2 \\\\   & + \\left(\\frac{{\\ensuremath{\\left[{\\rm fe}/{\\rm h}\\right]}}- { \\ensuremath{\\left[{\\rm fe}/{\\rm h}\\right]}}_{spec}}{\\sigma_{{\\ensuremath{\\left[{\\rm fe}/{\\rm h}\\right]}},spec}}\\right)^2 , \\end{split}\\ ] ]    plus penalties for any other priors we choose to impose . when done this way , the torres relation , @xmath204 , and @xmath205 $ ] , plus the density of the star from the transit can often constrain @xmath198  better than its spectroscopic counterpart , more directly and more precisely constraining @xmath202  and @xmath156 .",
    "these constraints , in turn , feed back directly to the fundamental planetary parameters we care about most ( e.g. , @xmath203  and @xmath210 ) .",
    "sometimes , @xmath198  can actually be better - constrained by the spectroscopy , in which case that constraint feeds back into the constraint on @xmath211 , and the other transit parameters .",
    "adding the `` prior '' penalty to @xmath202  and @xmath156  from the torres relation in this manner is somewhat unconventional .",
    "typically , priors are static and come from previously - fit data , not model - dependent and derived at each step . to avoid this , we considered stepping in @xmath198 , @xmath204 ,  and @xmath205$]and use the @xmath202  and @xmath156  from the torres relation to break the degeneracy , but then we would be solving a one - parameter degeneracy with two parameters , over - constraining the model and leading to inconsistencies . said another way , the torres relation is not mathematically self - consistent : the input @xmath198  does not precisely equal the @xmath198  derived from the output @xmath202  and @xmath156 , and therefore there would be multiple ways to calculate critical parameters .",
    "additionally , the theoretical scatter in the torres relation would set a floor to how well we could measure @xmath202  and @xmath156 , regardless of other constraints .",
    "finally , if we were to do it this way , @xmath204  and @xmath205 $ ]  would define @xmath202  and @xmath156 , not simply constrain it .",
    "therefore , we would lose the power to influence @xmath204  and @xmath205$]away from their prior values . while typically , the parameters derived in different ways are statistically consistent , as we would expect , if we use the output @xmath156  and @xmath202  to generate any piece of our model , rather than as a prior constraint , the model itself would be mathematically inconsistent . applying these model - dependent priors",
    "uses the information encoded in the torres relation , which may more appropriately be thought of as data which we merely expect to be statistically consistent , while maintaining the mathematical self - consistency of the model .",
    "@xcite recasts the torres relation in terms of @xmath212  instead of @xmath198 , with the idea being that the transit precisely constrains @xmath212  ( ignoring @xmath203 ) , and so it is a more efficient stepping parameter for the markov chain , which is true .",
    "unfortunately , for the same reason the transit precisely constrains @xmath212 , we can no longer derive independent constraints on @xmath202  and @xmath156 .",
    "therefore , we would be required to use the output @xmath202  and @xmath156  to link the rv and transit models , and we are left with a mathematically inconsistent model ( their input @xmath212  does not equal the @xmath212  derived from the output @xmath202  and @xmath156  either ) , and we are left with no additional constraint on @xmath204  and @xmath205 $ ] .",
    "one reasonable criticism of our method is that it assumes the torres mass and radius relations are independent .",
    "to the extent this is not true , we are double - counting the prior imposed from the torres relations , and artificially reducing our errors .",
    "alternatively , we can discard a relation , either for @xmath156  or for @xmath202 , but then we assume the relations are perfectly correlated and we lose information to the extent that they are not .",
    "a more correct method likely lies somewhere in between , explicitly accounting for the covariances between the @xmath202  and @xmath156  relations .",
    "a consequence of using the torres relations ( or any stellar model ) is that we inherit their limitations ",
    "i.e. , exofast , in its current form , should only be applied to `` single ( post- ) main - sequence stars above 0.6 m@xmath213 '' @xcite . for stars outside of this regime , the prior for the torres relation",
    "should be removed , and an independent prior on the stellar mass and/or radius should be imposed by editing the @xmath11  function ( a trivial task assuming another measurement is available ) .",
    "the code will issue a warning if this regime is encountered , which can be safely ignored if it is only issued during the burn - in period .",
    "a final note about this procedure is that the covariance between @xmath198  and @xmath211 is extreme because they only differ by one power of @xmath156 .",
    "if this covariance is not accounted for when stepping in the markov chain ( e.g. , with the de - mc algorithm ) , the markov chain becomes extraordinarily inefficient , taking hundreds of times longer than it otherwise would .      with the values of @xmath198 , @xmath204 ,  and",
    "@xmath205 $ ]  from the steps in the markov chain ( and the observed bandpass ) , we can also derive the limb - darkening parameters by interpolating the tables from @xcite .",
    "this interpolation is done in our module quadld .",
    "however , the tables contain a substantial and poorly - quantified systematic error .",
    "if this error were not taken into account , we would underestimate the errors on any covariant parameters .",
    "worse , if the data were of sufficient quality to constrain the limb - darkening parameters , they would become an implicit constraint on @xmath198 , @xmath204 , and @xmath205 $ ]  ( and therefore the mass and radius of the star ) . since we know the limb darkening tables to be flawed @xcite , this would in turn",
    "bias the stellar parameters and all derived parameters . to account for this",
    ", we do something similar to what we described above with the torres relation .",
    "we estimate the error on the quadratic limb darkening parameters , @xmath214 and @xmath215 to be 0.05 , based on figure 1 of @xcite . then",
    ", we step in both limb darkening parameters , @xmath179 and @xmath145 , calculate what the @xcite tables would predict from the current steps in @xmath198 , @xmath204 , and @xmath205 $ ] , for the limb darkening coefficients , @xmath216 and @xmath217 , and add an additional penalty to the @xmath11  ( equation  [ eq : priors ] ) of the form :    @xmath218    ideally , we would like to have accurate errors for the limb darkening coefficients and use more accurate , non - linear limb darkening tables from 3d hydrodynamical models , but neither are currently available .",
    "additionally , a non - linear limb darkening model is currently impractically slow to calculate for large data sets , as our speed enhancements described in appendix  [ sec : occultquad ] would no longer apply .",
    "if an accurate grid of theoretical limb darkening parameters with well - understood uncertainties were available for a wide range of stars , exofast may be able to work backwards , and use the limb darkening to constrain @xmath198 , @xmath204 , and @xmath205$] and therefore the masses and radii of stars  from a precise light curve alone  a tantalizing possibility . in appendix",
    "[ sec : analyticld ] , we describe how one could linearly fit the quadratic limb - darkening parameters , which may prove useful with such a scheme .      having the semi - major axis in physical units provides an absolute scale to the system , which allows us to include the reflex motion of the star without introducing any additional parameters .",
    "equation  [ eq : xy ] assumes a barycentric coordinate system , but the rvs are measured in the stellar - centric frame , and the transits are measured in the planet - centric frame .",
    "the light travel time ( roemer delay ) between these frames can be large , and is analagous to the standard correction from the heliocentric julian date ( hjd ) or julian date ( jd ) to barycentric julian date ( bjd ) in our own solar system @xcite .",
    "one can attempt to correct the flux of the transit @xcite or the observed radial velocity to account for this , but it is far more straight - forward to transform the observed time stamp into the target - barycentric frame , which is done at each step in the markov chain with our code bjd2target .",
    "this effect is most important when trying to reconcile observations for which the information comes from different points in space , such as the primary transit , secondary eclipse , radial velocity , phase curves , or multiple transits of different planets around the same star . for a typical hot jupiter ( @xmath219 au )",
    "the difference between different reference frames is @xmath220 seconds , and it is obviously much larger for planets farther from their stars .    even in the case of a single observation of a transiting hot jupiter ,",
    "the duration of a transit ( as seen in the solar system barycenter frame ) differs by 0.15 seconds from its true value if this effect is ignored , which is only marginally negligible by today s standards . for hot jupiters ,",
    "the offset between the stellar - centric and barycentric frame is negligible ( @xmath010 ms ) , but grows with the semi - major axis and the mass of the companion .",
    "while the target - barycentric times are important for dynamical studies of transit timing variations ( ttvs ) , they are really only necessary when there are two transiting planets , which our code does not consider . with a single planet ,",
    "the offset between the frames for all primary transits will be constant , so the ttvs will not be different . to avoid confusion , and because the times in the solar system barycentric frame ( i.e. , @xmath221 ) are most precisely known , we only quote the times in @xmath221 .",
    "this conversion back to @xmath221  is done by our module , target2bjd . to be clear , this procedure automatically reconciles the locations of each observation , which for example , naturally constrains the eccentricity from the primary transit and secondary eclipse timing .",
    "we could calculate the roemer delay due to the systemic velocity , @xmath44 , with no additional parameters .",
    "since the system may be receding or approaching us , the systemic velocity will tend to stretch out or compress the arrival time of the photons by a constant factor , @xmath222 .",
    "however , since rvs are often arbitrarily normalized , we do not always have a good measurement of the absolute systemic velocity .",
    "more important , the only practical effect such a correction would be to change the period by a factor of @xmath223 . while this @xmath224 difference in period is already highly statistically significant in many systems , the practical implications of such a difference will be completely obscured by the uncertainties in the other parameters for the foreseeable future , and quoting this corrected period would only make deriving the ephemeris in the observed frame more error - prone",
    "therefore , we actively chose not to correct for the additional roemer delay introduced by @xmath44",
    ".    there are several other minor effects on the arrival time of the photons that we ignore , because they would require additional free parameters which are generally not well - known and the effect is very small .",
    "these include the light travel time from our barycentric frame to the target barycentric frame , proper motion @xcite , parallax @xcite , and general relativistic precession @xcite .",
    "the ignored effects are numerous , and include the rossiter - mclaughlin ( rm ) effect , transit timing variations ( ttvs ) ( if multiple epochs are fit simultaneously , a constant ephemeris is assumed ) , secondary eclipses , a distance constraint on the stellar radius , reflection from the planet , ellipsoidal variations of the star , relativistic beaming , lensing , gravity brightening , non - keplerian gravitational interactions , and tidal effects , to name a few .",
    "it is expected that for a given system , many will want to modify the code slightly to include some of these additional effects .",
    "we have attempted to make the code as modular and comprehensible as possible , such that someone familiar with idl could use exofast as a starting point or template for their own , more specialized code without necessarily needing to master the details of modeling .",
    "future versions of our code may include some of these ignored effects as we have occasion to worry about them , and those codes may be made publicly available .",
    "now , with a mere 16 parameters , @xmath44 , @xmath45 , @xmath56 , @xmath69 , @xmath101 , @xmath102 , @xmath191 , @xmath186 , @xmath149 , @xmath130 , @xmath193 , @xmath198 , @xmath204 , @xmath205 $ ] , @xmath179 , and @xmath145 , we can describe the both the radial velocity and transit data simultaneously , including often neglected effects .    since we have a very good guess for the starting values for each parameter from the individual fits , no global fit is required .",
    "assuming they are consistent with one another , we only need a slight refinement to find the best fit of the combined data sets . therefore , using the starting values for the best fits we found in both individual cases , we run a quick amoeba minimization to find the local minimum of the combined data set , and we are finally ready to use our mcmc code , exofast_demc , to determine the model uncertainties and covariances .    to summarize , our procedure to calculate the @xmath11 , after we have stepped in the 16 parameters above , is as follows :    1 .",
    "check the boundary conditions for each parameter .",
    "if any parameter is out of bounds , we immediately return @xmath225 .",
    "2 .   compute @xmath155 , @xmath156 , @xmath203 , and @xmath202  ( equation  [ eq : mrstar ] ) .",
    "3 .   compute @xmath202  and @xmath156  from the torres relation .",
    "4 .   interpolate the @xcite tables to get the quadratic limb darkening parameters at the given @xmath198 , @xmath204 , and @xmath205 $ ] .",
    "compute @xmath53 .",
    "correct the observed times to the target s barycentric frame (  [ sec : reflex ] ) .",
    "calculate the rv model (  [ sec : rv ] ) .",
    "calculate the transit model (  [ sec : transit ] ) .",
    "compute the total @xmath11  ( equations  [ eq : priors ] and  [ eq : ldchi2 ] ) .",
    "to explain how to use the code , describe its outputs , and validate it at the same time , we now follow an example fit from beginning to end using rvs and transit data of hat - p-3b from @xcite ( t07 hereafter ) , hosted on the exoplanet archive .",
    "this system was chosen nearly randomly  we just looked down the list of hat planets and took the first one with enough public data and only one source for rvs ( since the public version of exofast does not fit multiple rv zero points ) .    since the transit data were in @xmath226  and magnitudes , we converted it to the required format of @xmath221 , and normalized flux , and wrote them to a file called `` hat3.flux.'' we also converted the times of the rv data points from @xmath227  to @xmath221  and wrote them to a file , `` hat3.rv . ''",
    "next , we have to make decisions about what information we have the power to constrain and which parameters should be constrained by priors from external information , or held fixed . as is always the case , we must adopt priors for @xmath204  and @xmath205 $ ]  based on spectra , which t07 quote in their table 2 .",
    "as we explain later , we also choose to use the spectroscopic constraint on @xmath198 .",
    "for all spectroscopic priors , we use the larger uncertainties they describe in the text .",
    "like t07 , due to relatively poor phase coverage , there was no power to constrain the eccentricity , so we fixed it to zero and did not attempt to fit a slope .",
    "the relatively short range of data for the rv meant the transit survey data constrained the period significantly better , so t07 fixed their period at that value .",
    "instead , we use the value from their photometry as a prior .",
    "note that a suitable fit was possible without this prior ; the uncertainties were just larger .",
    "finally , we must specify the band in which the transit was observed , in this case , sloan i.    once we have decided on what to fit , we simply call the code to reflect our wishes :    where the name of the observed bandpass is given as `` band '' , `` pname '' is the case - insensitive name of a planet in exoplanets.org , from which the starting values or spectroscopic priors can be pulled .",
    "this is generally not necessary , but gives the fit a more robust starting point .",
    "the `` priors '' input is a 2d array containing the prior value and 1-sigma uncertainty for each parameter . to specify a parameter with no prior",
    ", a width of infinity should be used .",
    "the parameters `` minp '' and `` maxp '' limit the range of the lomb - scargle periodogram , and is typically not necessary , but these rv data are too sparse to get reliable results from the periodogram .",
    "further details on the calling structure and additional features can be found in the documentation of the code .",
    "exofast then fits the rv data as described in ",
    "[ sec : rv ] , scales the uncertainties , fits the transit data as described in ",
    "[ sec : transit ] , scales its uncertainties , then fits the two data sets combined .",
    "it determines the appropriate stepping scale , and begins the markov chains , giving a status update about the number of accepted steps , and estimates the time to completion , if it were to take the maximum number of steps .",
    "once it has taken 5% of the maximum number of allowed steps , it will estimate if the chains will be well - mixed by the time it is done .",
    "if it is expected to be done before it hits the limit , it will estimate when . if not , it will recommend a thinning factor . in our case ,",
    "the chains were well - mixed in about five minutes on a standard desktop computer .",
    "this is slightly faster than a general run because the orbit was circular .",
    "this eliminates two free parameters and makes the normally expensive solution to kepler s equation trivial .",
    "still , for a similar number of points , ten minutes is fairly typical , even when eccentricity is included .",
    "the estimated time of completion , or the recommended thinning factor is very rough , and should only be trusted to a factor of 2 - 5 . while the thinning factor of @xmath35 means it may take up to @xmath35 times longer , it will stop as soon as it is well mixed , so it pays to be a little conservative  for that reason , the suggested value is twice what it actually calculates it needs .",
    "the mixing criteria described in  [ sec : mcmc ] are very conservative .",
    "indeed , we have run chains with fewer than 100 independent steps ( as opposed to the required 1000 ) that did not differ significantly from chains that were fully mixed . therefore ,",
    "if you find that the recommended thinning value would imply a prohibitively - long execution time , you may wish to proceed with caution in interpreting a chain that gives such a warning . otherwise , restarting the chain with the suggested thinning factor",
    "is highly recommended .",
    "alternatively , if you have a 64-bit machine ( and a 64 bit version of idl ) , you could increase the maximum number of steps by this same factor , but there is very little difference in the quality of the final output or the execution time , and thinning makes the chains smaller and more manageable without throwing away much useful information .",
    "once the program runs to completion , there are several outputs .",
    "first , are publication - ready plots of the data with the best - fit model overplotted , and residuals below , as shown in figure  [ fig : hat3rv ] and  [ fig : hat3transit ] for our example .            by setting the debug flag ,",
    "these plots can be drawn to the screen during each call to the @xmath11  routine . as the name implies , it is useful for debugging if the fit is not working",
    ". it can also be instructive  if , for example , the debug flag is left on during an amoeba fit , it will essentially play a movie of the routine settling into the best fit . in this manner , one can gain an intuitive feel for how the algorithm works ( or why it is failing ) .",
    "it is also instructive to leave it on at the beginning of an mcmc fit , to gain intuition for the difference between the steady convergence toward the best fit of the amoeba algorithm versus the more stochastic mcmc algorithm , which wanders around the vicinity of the best fit .",
    "obviously , this slows down the fit by orders of magnitude and is not intended to be used during a standard fit .",
    "perhaps the most powerful aspect of mcmc is the fact that , with the parameter chains in hand , it is trivial to generate median values , uncertainties , and covariances for derived quantities  we simply perform the appropriate transformation to each step in the entire chain . as such , we quote the median values , along with their 68% confidence intervals for several other derived quantities of interest as a publication - ready latex source code file for the deluxe table shown in table  [ tab : hat - p-3b ] .",
    "exofast_latextab , which generates these tables , automatically rounds the two - sided uncertainties to two significant digits each and rounds the median value to the higher precision uncertainty .",
    "when the uncertainty is symmetric , we use the @xmath228 symbol ; otherwise , we list both uncertainties in the standard fashion . this fast and accurate way to go from parameter arrays to properly - formatted latex source code is not just convenient , but also mitigates the risk of typos introducing catastrophic errors . even if it is not intended to be inserted into a paper , it is often more readable as appropriately - rounded latex code than simply printing the median values , particularly since it reorganizes the fitted and derived quantities into a more intuitive layout , as we have done in table  [ tab : hat - p-3b ] to group parameters into stellar , planetary , rv , transit , and eclipse categories .",
    "lcc  @xmath229&mass ( @xmath230 ) & @xmath231 +  @xmath232&radius ( @xmath233 ) & @xmath234 +  @xmath235&luminosity ( @xmath236 ) & @xmath237 +  @xmath212&density ( cgs ) & @xmath238 +  @xmath239&surface gravity ( cgs ) & @xmath240 +  @xmath241&effective temperature ( k ) & @xmath242 +  @xmath243}}$]&metalicity & @xmath244 +  @xmath52&period ( days ) & @xmath245 +  @xmath155&semi - major axis ( au ) & @xmath246 +  @xmath247&mass ( @xmath248 ) & @xmath249 +  @xmath250&radius ( @xmath251 ) & @xmath252 +  @xmath253&density ( cgs ) & @xmath254 +  @xmath255&surface gravity & @xmath256 +  @xmath257&equilibrium temperature ( k ) & @xmath258 +  @xmath151&safronov number & @xmath259 +  @xmath260&incident flux ( 10@xmath261 erg s@xmath262 @xmath263 ) & @xmath264 +  @xmath40&rv semi - amplitude ( m / s ) & @xmath265 +  @xmath266&minimum mass ( @xmath248 ) & @xmath267 +  @xmath268&mass ratio & @xmath269 +  @xmath44&systemic velocity ( m / s ) & @xmath270 +  @xmath56&time of transit ( @xmath221 ) & @xmath271 +  @xmath272&radius of planet in stellar radii & @xmath273 +  @xmath274&semi - major axis in stellar radii & @xmath275 +  @xmath179&linear limb - darkening coeff & @xmath276 +  @xmath145&quadratic limb - darkening coeff & @xmath277 +  @xmath6&inclination ( degrees ) & @xmath278 +  @xmath279&impact parameter & @xmath280 +  @xmath182&transit depth & @xmath281 +  @xmath282&fwhm duration ( days ) & @xmath283 +  @xmath184&ingress / egress duration ( days ) & @xmath284 +  @xmath285&total duration ( days ) & @xmath286 +  @xmath287&a priori non - grazing transit prob & @xmath288 +  @xmath289&a priori transit prob & @xmath290 +  @xmath130&baseline flux & @xmath291 +  @xmath292&time of eclipse ( @xmath221 ) & @xmath293 [ tab : hat - p-3b ]    however , just because the program will provide an answer that is easy to publish does not necessarily mean it is publication - ready .",
    "it is always wise to inspect the parameter distributions for strange behavior that may compromise the results , and check that the parameters themselves make sense . as such",
    ", we also output the probability distribution functions for each parameter , including derived parameters  a subset of which is shown in figure  [ fig : hat3pdf ] .",
    "the best - fit values ( the lowest @xmath11  achieved by the markov chain ) are shown as a solid vertical line over each distribution .",
    "these plots are not typically expected to be published , but are rather for diagnostic purposes only .",
    "another interesting diagnostic is the plot of covariances , a subset of which are shown in figure  [ fig : hat3covar ] .",
    "we plot contour plots of each parameter against each other parameters , including derived parameters , where the contours show the 68% and 95% confidence intervals .",
    "we hide the numerical values on the axis labels for readability  the shape is the most important diagnostic here . the value above the plot is the correlation statistic .",
    "the solid black dot is the best - fit value of the two parameters .",
    "we note that our routine to generate these contours , exofast_errell , is not a standard idl routine and may be of general interest . again",
    ", the roughness of these covariance plots would smooth out with longer chains .",
    "the parameter chains themselves may be useful for additional diagnostics or analysis not performed by exofast , such as creating publication - quality plots of particular parameter distributions or covariances .",
    "we output an idl save file that includes the array of steps populated by the markov chain , including all derived quantities .",
    "this array includes the burn - in , in order to maintain a complete record of all steps .",
    "the save file also includes the corresponding @xmath11  at each step , the latex - style names of all parameters , and the index of the first useful link in the chain ( before which is considered the `` burn - in '' ) .",
    "generally , our results are in very good agreement with those found by t07 .",
    "all quoted values agree with theirs within @xmath294 , and the vast majority agree to better than @xmath295 .",
    "the determination of the stellar properties is the most fundamental difference between our two methods .",
    "we use the relations from @xcite and the spectroscopic priors , to enforce consistency with our transit and rv data at each step , whereas they fit the transit , then use the fitted density as a constraint to their stellar isochrones @xcite in a later step to derive the stellar properties and iterate @xcite . while this is an attempt to do a similar thing , their process results in statistically consistent , but not identical values for @xmath212  from the stellar parameters ( @xmath296 g @xmath297 ) and the @xmath212  from the fitted transit parameters ( @xmath298 g @xmath297 )",
    ". however , our densities from the two sets of parameters are necessarily equivalent ( @xmath299 g @xmath297 ) .",
    "the uncertainty in the transit time from t07 is almost an order of magnitude larger than what we find .",
    "given the good agreement between everything else , we suspect a typo in t07 is to blame ( e.g. , a missing zero ) , as our uncertainty is more in line with analytic estimates @xcite  thus demonstrating a major benefit of a code that automatically generates the latex source code of the final parameters directly from the data .",
    "other than that , all of our uncertainties agree within 30% , with neither of us finding systematically smaller uncertainties across all parameters . unsurprisingly , the largest differences are for the stellar parameters , for which we used fundamentally different methods .",
    "while we in principle , can derive uncertainties that are smaller than the scatter in the @xcite relation due to the additional constraints of the rv and transit data , our uncertainties are still dominated by that scatter .",
    "these propagate to slightly larger uncertainties in the planetary parameters .",
    "the radial velocity portion of this code has been used to fit the radial velocity data in @xcite , @xcite , and @xcite , though minor modifications were required to fit different zero points for each instrument , as described in the text of those papers .",
    "we used this code to fit the kelt-1b data @xcite , adding the ability to fit an arbitrary number of transits ( with ttvs ) , rv zero points , and the rossiter mclaughlin effect .",
    "we also modified our code to fit the kelt-2ab and kelt-3b @xcite with an arbitrary number of transits , deblend the stars , and constrain the stellar radius through the _ hipparcos _ distance prior .",
    "as described in  [ sec : nsteps ] , we ran 100 fits of simulated rv curves for each of 15 different intrinsic eccentricities ( 1500 total fits in our preferred parameterization ) . with the exception of the lucy - sweeney bias for eccentricity discussed in detail there , all measured values for the other 5 parameters",
    "were statistically consistent with the input values , finding only 312 of the 7500 parameters outside of @xmath300 ( 341 expected ) , and 10 outside of @xmath125 ( 20 expected ) .",
    "further , all of these simulations were robustly fit without any intervention or assumption of the input values .",
    "all of the following routines make use of readexo , an idl program to read the current exoplanets.org database @xcite into an idl structure .",
    "it is available for download and use offline as well .",
    "each header entry , described on their website , becomes a unique tag name for the structure .",
    "the code will automatically adapt to the addition of rows and columns into the exoplanets.org database , and a flag can be set to automatically update the local copy .",
    "this code is useful for more general manipulation and comparison of all exoplanetary data than is allowed by their web interface , and for integration into software suites . in our code , we use it to seed the fits for known planets to make the fits more robust , and for retrieving priors on @xmath198 , @xmath204  and @xmath205 $ ]  to get the host properties , rather than requiring the user to supply them .",
    "however , we caution strongly that the selection effects inherent to this sample of planets are very poorly understood and poorly quantified ",
    "care should be taken not to over - interpret results that these tools make trivial .",
    "if you use this code , please also cite @xcite and acknowledge their efforts in making and maintaining this incredibly useful database .",
    "there are now so many transiting planets that , on any given night from any given location on earth , we are very likely to be able to observe at least one of them @xcite .",
    "it is useful , then , to quickly determine which those are in order to plan observations or fill gaps in observing schedules .",
    "we present an online calculator that determines which planets are transiting or eclipsing from a particular observatory on a particular date .",
    "this tool uses the exoplanets.org database , which is synced daily . for the predicted secondary eclipse times of non - circular orbits and especially the transit times of rv planets ,",
    "the precision is likely to be much worse than what we could derive from the original data because these times were derived from the values of @xmath52 , @xmath36 , @xmath37 given in exoplanets.org and thus do not include the often large covariances between these parameters .",
    "these could be greatly improved if , in the future , published results included @xmath56  and @xmath57  directly from the fits , like hat does ( e.g. * ? ? ?",
    "* ) , and these were included in the exoplanets.org database .",
    "we provide an intuitive online interface to the basic features of exofast , which can fit either or both of the rv and transit , including an arbitrary number of systematics .",
    "priors on @xmath204  and @xmath205 $ ]  are required , but can be automatically pulled from the exoplanets.org database by selecting the appropriate planet from a pull down menu .",
    "this pull down menu will update daily as new planets are added to the exoplanets.org database . when fitted , the transit ( normalized to 1 and with systematics removed ) and the rv will be plotted and both the fitted and derived parameters will be output to the screen .",
    "unfortunately , since these are run on the server side , it is not practical to support the full de - mc code , so only the best - fit values are reported .",
    "therefore , this online fitter is not intended for research - quality fits .",
    "our last online code linearly interpolates the @xcite quadratic limb darkening tables for given values of @xmath198 , @xmath204 , and @xmath205 $ ]  in any of the following bands : johnson / cousins @xmath301 , @xmath87 , @xmath302 , @xmath303 , @xmath134 , @xmath304 , @xmath305 , and @xmath40 ; sloan @xmath306 , @xmath307 , @xmath308 , @xmath309 , and @xmath310 ; _ spitzer _  3.6 @xmath311 , 4.5 @xmath311 , 5.8 @xmath311 , and 8.0 @xmath311 ; _ kepler _ ; _ corot _ ; and stroemgren @xmath312 , @xmath313 , @xmath279 , and @xmath314 .",
    "this can also draw the stellar parameters from the exoplanets.org database , if available .",
    "we would like to thank howard relles , rachel ross , karen collins , thomas beatty , and trey mack for beta tests , bug reports , and excellent suggestions for improvements ; eric ford for cuda translations of bottlenecked routines , robert siverd for useful discussions about the eccentricity parameterization and random number generator ; and wayne landsman for curating the idl astronomy library , from which we use many functions @xcite .",
    "this research has made use of the exoplanet orbit database and the exoplanet data explorer at exoplanets.org .",
    "this research has made use of the nasa exoplanet archive , which is operated by the california institute of technology , under contract with the national aeronautics and space administration under the exoplanet exploration program .",
    "this work was supported in part by nsf career grants ast-1056524 and ast-0645416 .",
    "given the quadratic limb darkening law ( eq .  [ eq : quadld ] ) and the flux during transit ( eq .  [ eq : transitflux ] ) , if we make the following definitions    @xmath315    the flux during transit can be written :    @xmath316    since @xmath317 , @xmath318 , and @xmath319 , are solely functions of the transit geometry ( @xmath149 and @xmath150 ) , they can be calculated independent of the limb darkening and are now optional outputs of exofast_occultquad . using these , we can analytically solve for the coefficients @xmath320 , @xmath321 , and @xmath322 by performing a standard linear @xmath11  minimization ( see * ? ? ?",
    "then the limb darkening coefficients , @xmath179 and @xmath145 simply become :    @xmath323    this analytic fit can greatly increase the speed of any fitting routine by reducing the dimensionality of the non - linear solver , particularly in cases where data are taken with multiple bands . however , in the low signal - to - noise regime where the data have little power to constrain the limb darkening , this analytic fit can give non - physical results , in which case it is better to fit a linear limb darkening law , fix the values at the @xcite values , fit them non - linearly with a prior , or allow them to vary by interpolating the @xcite limb darkening tables with each new @xmath198 , @xmath204 , and @xmath205 $ ]  during the markov chain .",
    "in addition , such a hybrid fit must be used with care , as discussed in  [ sec : hybrid ] .",
    "these relations trivially simplify in the case of linear limb darkening , when @xmath324 :    @xmath325    and then ,    @xmath326    given the same form ( @xmath317 and @xmath318 are the same in the quadratic and linear cases ) , one could even fit both laws to see if the improvement in @xmath11  justifies the addition of the extra parameter .",
    "the original idl code to analytically calculate the flux decrement during transit presented in @xcite was more than an order of magnitude faster than the previous numerical method @xcite .",
    "however , the idl version was a simple line by line translation of the fortran code , which does not take advantage of the fact that idl is a vector language . in our code , exofast_occultquad , we sped it up by a factor of 100 - 500 and fixed many bugs .",
    "the majority of the speed enhancement came from a major restructuring of the code to take advantage of idl s much faster vector operations and careful attention to optimal case ordering .    with the aid of the idl profiler",
    ", we determined the largest remaining bottleneck was in the calculation of the elliptic integral of the third kind , which used translations of the numerical recipes routines rj , rc , and rk .",
    "after testing potential replacements , including idl s separately - licensed routines imsl_elrc , imsl_elrj , and imsl_rlrd , we replaced the numerical recipes routines with an idl translation of an algol program published by @xcite .",
    "this iterative routine is many times faster and more robust than any other alternative we tried .    for our last minor speed enhancement , we combined the routines to calculate hasting s polynomial approximation for the elliptic integrals of the first ( @xmath327 ) and second ( @xmath328 ) kind , which now share the computationally expensive task of computing @xmath329 .",
    "we also tested other replacements for this routine , including fukushima s piecewise polynomial approximation , @xcite , but none was reliably faster .",
    "the calculation of the elliptic integral of the third kind is still the primary bottleneck , but is now comparable to the calculation of the elliptic integrals of the first and second kind , the arc cosine , logarithm , and idl s where function , so additional substantive speed gains will be difficult without a fundamental re - characterization of the problem .",
    "these speed enhancements combined make this idl routine @xmath330 times faster than the previous idl version .",
    "since this calculation is a significant fraction of calculating a transit model , this improves the run time of entire program by a significant factor .",
    "we rewrote the fortran routine with the latter two enhancements , which is now twice as fast as its predecessor , and 2 - 5x faster than the current idl version , depending on the compiler .",
    "lastly , we include an idl wrapper for the fortran version , which uses call_external and is a drop - in replacement for the strictly idl version described above .",
    "after its first use , it is indistinguishable from the native fortran in terms of speed , and is therefore 200 - 500 times faster than the original idl version , and 2 - 5 times faster than the current idl code , depending on fortran the compiler used .",
    "the downside to this wrapper is that the syntax required by idl is technically not legal fortran .",
    "the makefile in the idl example does not implement f77 on linux , likely because of compilation warnings .",
    "some compilers , like gfortran and f95 simply refuse to compile it .",
    "it can be compiled with g77 or f77 , and the warnings they issue can be safely suppressed .",
    "only ifort will compile it without warnings .",
    "this call_external version has only been tested on a 32 bit linux machine .    for those that wish to use a high - level , but non - proprietary language",
    ", we include a python version of exofast_occultquad , which uses numpy and mirrors the vector structure of the idl version .",
    "therefore , we expected this to be similar in execution time to the idl , but it was actually 5 times slower . this may be a general statement about python , or simply a result of the fact that its authors are expert idl programmers and only novice python programmers .",
    "in addition to being relatively slow , the numerical recipes codes to calculate the elliptic integral of the third kind ( rc , rj , rk ) were poorly - behaved where values of @xmath150 were within @xmath331 of special cases ( first , second , third , and fourth contact ) . very near special cases ( @xmath332 ) , the previous codes differed from the true value by as much as @xmath333 . with the @xcite algorithm to calculate the elliptic integral of the third kind , the differences between the calculated value and true value ( calculated with arbitrary precision in mathematica ) are less than @xmath334 .",
    "since any model that suffered from this bug would be unfairly disfavored , the practical implication of this bug is the potential for a bias in the measured times that pushes data points away from these special cases . for a typical hot jupiter",
    ", @xmath331 corresponds to @xmath335 ms in the planetary orbit .",
    "fortunately , this is beyond the current precision attained to date .",
    "however , diabolical alignments of several data points may further skew the inferred value .",
    "we also fixed typos in cases where @xmath336 , as in secondary eclipses .",
    "while these typos would have caused catastrophic failures , the behavior of this bug is sufficiently non - physical they would have been immediately found , so were unlikely to have done any harm .",
    "we also fixed a handful of other minor bugs , such as consistent use of double precision values and ( in fortran ) functions .",
    "the major conceptual enhancements were mentioned previously .",
    "namely , we now allow negative values of @xmath149 , as discussed in ",
    "[ sec : otherbiases ] and we return the coefficients required to analytically fit the quadratic limb darkening parameters as discussed in appendix  [ sec : analyticld ] .",
    "thorough testing at every special case , @xmath337 of every special case , and for a thousand uniformly spaced values of z between @xmath338 and @xmath339 for hundreds of values of p ranging from @xmath332 to 2 have been checked , and no other errors were found .",
    "idl s help states that randomu , their built - in random number generator , is similar to numerical recipes ran1 , which has a periodicity , m , of @xmath340 , and should suffice for a series of random numbers of length , @xmath7 , such that @xmath341 million .    in our sample fit of hat - p-3b",
    ", we generate 16 random numbers per step : one for each of the 13 parameters , two to choose the random chains , and one to compare to the likelihood ratio .",
    "we construct 26 simultaneous chains , each with a maximum of 100,000 steps .",
    "this means that in total , we may generate up to 41.6 million random numbers during the course of the fit .",
    "while this is just below the recommended number , with a few more free parameters ( recall that we fixed the slope to zero and forced the orbit to be circular ) or thinning , we could easily exceed the periodicity of the generator .",
    "initially , we were incredulous that the results could be substantively affected by the periodicity of the generator , given that the chains wander around in n - dimensional parameter space and so are unlikely to take the same step from the same place",
    ". therefore , we constructed another random number generator which cycled through the first 10007 ( a prime number ) random deviates generated by randomu to artificially reduce its periodicity and more readily test this effect .",
    "while the acceptance rate was ideal , the chains were well - mixed according to our criteria , and the resulting probability distribution functions looked acceptable by eye ( i.e. , no obvious signs of any malfunction ) , the median parameters differed significantly ( in some cases by more than @xmath342 ) from the results using randomu .",
    "indeed , it has been shown numerous times that the quality of the random number generator can affect the scientific conclusions in other applications ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "while looking for a suitable replacement , we found that the third edition of numerical recipes @xcite warns `` be cautious about any source earlier than about 1995 , since the field progressed enormously in the following decade . ''",
    "they also state that one should never use a random number generator with a periodicity of less than @xmath343 .",
    "this certainly excludes ran1 , and even excludes their own popularly - used alternative ran2 , published in 1992 , with a periodicity of @xmath344 .",
    "numerical recipes recommends a specific implementation of a random number generator , which was translated into idl by paolo grigis and included with our distribution as pg_ran .",
    "it has a periodicity of @xmath345 .",
    "our program , exofast_random , is a wrapper for this routine to allow it to return an 8-dimensional array of uniform and gaussian random numbers in order to make it a drop - in replacement for randomu .",
    "while this version is @xmath346 times slower than randomu because random number generators are serial by nature , and so not ideally implemented in idl , its contribution to the overall runtime of the program is only about 3% . a fast , built - in random number generator would essentially eliminate this contribution completely .",
    "we repeated the fit of hat - p-3b again , now using this generator . because we did not approach the periodicity of randomu in this fit , it is comforting that we found no statistical difference .",
    "further , we first did this test before implementing de - mc . repeating this test afterward , we found the effect is much less significant when using de - mc , but still noticeable , than in a standard mcmc implementation because it is so much more efficient and therefore we take a small fraction of the maximum number of steps .",
    "in addition , the steps we do take are dominated by the two random numbers that choose the chains , not the uniform random deviate for each parameter .",
    "for generality , and because the overall speed hit is relatively small , we always use our own , slower generator . by setting a simple flag",
    ", the user can override this default behavior to select randomu or any other generator , as long as it has the same calling structure and has the ability to return 1 , 2 , and 3 dimensional uniform and gaussian random deviates .",
    "when investigating eccentricity parameterizations , we explored one that , to the best of our knowledge , has not been tried before : allowing the eccentricity to be bounded between -1 and 1 . while formally undefined , we were motivated to allow a negative eccentricity by considering its definition , @xmath347 , where @xmath348 is the distance to the focus at apoapsis and @xmath349 is the distance to the same focus at periapsis .",
    "therefore , a negative eccentricity implies that we have incorrectly labeled our periapsis and apoapsis . when @xmath108 , we redefine a new eccentricity , @xmath350 . since that flips the periapsis and apoapsis , we also need to change the argument of periastron accordingly : @xmath351 . since changing",
    "the argument of periastron also shifts the time of periastron , we need to make sure we calculate the @xmath53  after changing @xmath37 . in the end , however , we can not allow negative eccentricities in our final distribution because it will create a discrete degeneracy between @xmath36 and @xmath37 , and @xmath352 and @xmath353 . in a properly - sampled markov chain ,",
    "the median value of @xmath36 would always be 0 , and for statistically significant eccentricities , the chain is likely to get stuck at either the negative or positive eccentricity and never sample the other .",
    "we can avoid this by explicitly changing our values of @xmath36 and @xmath37 , following the above prescription , as soon as @xmath36 steps to a negative value .",
    "further , if @xmath37 is outside of the normally allowed bounds , @xmath354 , we can simply add or subtract @xmath51 until it is within bounds .",
    "this scheme does not affect our uniform prior in either @xmath36 or @xmath37 . by eliminating regions of parameter space that are out of bounds and",
    "therefore always rejected , the autocorrelations are smaller and the chains converge faster . unfortunately , because we must rescale the eccentricity , it does not get rid of the lucy - sweeney bias as we had originally hoped .    the resultant prior distributions and a posteriori distributions were indistinguishable from the @xmath101 , @xmath102  parameterization . when using a standard markov chain implementation and for small eccentricities , we found this parameterization to be about 20% faster than any of those discussed in  [ sec : eparam ] . with the de - mc code , it was about 20% faster than stepping directly in @xmath36 , where @xmath99 , but still nearly half as fast as parametrizing it in terms of @xmath38  and @xmath98or @xmath101  and @xmath102 .",
    "so while we generally recommend a de - mc stepping in @xmath101  and @xmath102 , we still present this alternative parameterization in case others find it useful .    one useful application of the same idea would be when incorporating the rm effect , where the projected rotational velocity , @xmath355  is analogous to eccentricity and the projected spin - orbit alignment , @xmath356 is analagous to the argument of periastron .",
    "therefore , we can allow a negative @xmath355 , but when it is negative , take its opposite and add @xmath43 to @xmath356 .",
    "we ran a typical case of a fit of a simulated data set with both transit and rv data with an eccentric orbit , which was well - mixed in about 5 minutes . during this time , we ran idl s profiler to analyze the performance to see which of our routines may be a bottleneck . while the particular mix will necessarily depend on the number of data points and the details of the model , it is instructive to look at this case study in depth .",
    "the solution to kepler s equation is the largest remaining bottleneck , consuming @xmath357 of the total computation time .",
    "since this iterative solution is necessarily serial , re - writing it in fortran or c and calling it via call_external may be a quick way of improving the run time .",
    "however , for low - eccentricity orbits , the number of iterations is small , and for circular orbits , this calculation is trivial .    at 20% ,",
    "the next largest contribution is from the program that generates the parameter distribution and covariance plots , the vast majority going toward the later . because we generate a plot for every parameter covariant with every other parameter , including all derived parameters , this represents over 1000 different plots . for each one",
    ", it bins every point in every chain into a 2d grid and finds the probability contours . in general , this is a useful diagnostic , but not necessary , and can be skipped by setting the appropriate flag .",
    "the next largest contributor is the conversion to the target reference frame ( @xmath358 ) .",
    "it is also an iterative function , and is calculated twice at each step in the mcmc chain ",
    "one for rv and one for photometry .",
    "it is important to know that this routine calls exofast_keplereq , and its contribution is not included in that figure ( so as not to double count it ) .",
    "the total run time of this calculation is @xmath359 .",
    "this is a hefty price to pay to include an effect that is nearly negligible , and this could be skipped altogether ( as has been done to date ) without any significant difference . analyzing the run time excluding",
    "this routine does not qualitatively change the top contributors .    at 6.3%",
    ", the interpolation of the quadratic limb darkening parameters takes the next biggest slice .",
    "the tables are large , and simply loading them into memory takes a long time , though we make use of global variables to make this quicker than it normally would be .",
    "the last major contributor , at 6.1% , is the calculation of the transit light curve ( using the all - idl version ) .",
    "our efforts to optimize this calculation are described in detail in appendix  [ sec : occultquad ] . like the conversion to the target reference frame",
    ", this figure counts the time only spent inside the routine .",
    "the total contribution of this routine is 12% .",
    "had we left the routine alone , the total run time of the mcmc fit would have been completely dominated by this calculation , taking an hour by itself .",
    "we also note that we have not optimized the code to calculate the transit flux using a non - linear limb darkened stellar model , which was many times slower than the original code . therefore , while in principle , it would be easy to modify our code to include non - linear limb darkening , it would increase the total run time of the fit by more than an order of magnitude .    using our comparison between exofast_occultquad and its fortran counterpart as a guide",
    ", we can guess that an all - fortran version of exofast would probably be around 5 times faster .",
    "further , considering the execution time and relative contribution of the fortran version of exofast_occultquad , an improvement of greater than 50 times is unlikely , even if all other contributions became negligible .",
    "recently , however , nvidia graphics processing units ( gpus ) have been used with cuda , the compute unified device architecture on highly parallelizable code for a significant ( factors of 10 - 100 ) gains over their cpu counterparts ( e.g. * ? ? ?",
    "while the links in a de - mc chain are serial , each model calculation within a link and each parallel chain could be computed simultaneously to take advantage of the gpu architecture if we had enough data points to justify the gpu overheads . indeed , preliminary tests have shown a factor of 50 improvement for the calculation of the model light curve for kepler - sized data sets with a relatively inexpensive graphics card ( geforce gtx 480 , $ 200 in 2012 ) and show the potential for even greater improvements .",
    "however , other bottlenecks currently make the overall boost a small fraction of that , making it tough to justify additional hardware . with some effort , these other bottlenecks may also be reduced , making gpus a promising avenue for large data sets .",
    "possibly the easiest way of substantively improving the run time is to decrease the total number of steps required for convergence , and therefore require fewer model calculations .",
    "recently , @xcite suggested an alternative way of stepping , which is worth investigating , though its advantage over de - mc is unclear . to stress how much of a difference this may make , because of the very strong correlation between @xmath211 and @xmath198 , a standard implementation of the mcmc algorithm converged nearly 1000 times slower than its de - mc counterpart  the difference between 3 minutes and 2 days .                                                              , j. , gaudi , b.  s. , siverd , r. , trueblood , m. , & trueblood , p. 2010 , in society of photo - optical instrumentation engineers ( spie ) conference series , vol .",
    "7733 , society of photo - optical instrumentation engineers ( spie ) conference series"
  ],
  "abstract_text": [
    "<S> we present exofast , a fast , robust suite of routines written in idl which is designed to fit exoplanetary transits and radial velocity variations simultaneously or separately , and characterize the parameter uncertainties and covariances with a differential evolution markov chain monte carlo method . </S>",
    "<S> we describe how our code incorporates both data sets to simultaneously derive stellar parameters along with the transit and rv parameters , resulting in more self - consistent results on an example fit of the discovery data of hat - p-3b that is well - mixed in under five minutes on a standard desktop computer . </S>",
    "<S> we describe in detail how our code works and outline ways in which the code can be extended to include additional effects or generalized for the characterization of other data sets  including non - planetary data sets . </S>",
    "<S> we discuss the pros and cons of several common ways to parameterize eccentricity , highlight a subtle mistake in the implementation of mcmc that could bias the inferred eccentricity of intrinsically circular orbits to significantly non - zero results , discuss a problem with idl s built - in random number generator in its application to large mcmc fits , and derive a method to analytically fit the linear and quadratic limb darkening coefficients of a planetary transit . </S>",
    "<S> finally , we explain how we achieved improved accuracy and over a factor of 100 improvement in the execution time of the transit model calculation . </S>",
    "<S> our entire source code , along with an easy - to - use online interface for several basic features of our transit and radial velocity fitting , are available online at http://astroutils.astronomy.ohio-state.edu/exofast . </S>"
  ]
}