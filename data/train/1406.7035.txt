{
  "article_text": [
    "quantum - mechanical uncertainty relations place fundamental limits on the accuracy with which one is able to measure the values of different physical quantities .",
    "this has profound implications not only on the microscopic but also on the macroscopic level of physical systems .",
    "the archetypal uncertainty relation formulated by heisenberg in 1927 describes a trade - off between the error of a measurement to know the value of one observable and the disturbance caused on another complementary observable so that their product should be no less than a limit set by @xmath0 . since heisenberg s intuitive , physically motivated deduction of the error - disturbance uncertainty relations  @xcite , a number of methodologies trying to improve or supersede this result have been proposed .",
    "in fact , over the years it have became steadily clear that the intuitiveness of heisenberg s version can not substitute mathematical rigor and it came as no surprise that the violation of the heisenberg s original relation was recently reported a number of experimental groups , e.g. , most recently by the vienna group in neutron spin measurements  @xcite .",
    "at present it is ozawa s universally valid error - disturbance relation  @xcite that represents a viable alternative to heisenberg s error - disturbance relation .    yet , already at the end of 1920s kennard and independently robertson and schrdinger reformulated the original heisenberg ( single experiment , simultaneous measurement , error - disturbance ) uncertainty principle in terms of a statistical ensemble of identically prepared experiments  @xcite . among other things ,",
    "this provided a rigorous meaning to heisenberg s imprecisions (  ungenauigkeiten `` ) @xmath1 and @xmath2 as standard deviations in position and momenta , respectively , and entirely avoided the troublesome concept of _ simultaneous _ measurement .",
    "the robertson ",
    "schrdinger approach has proven to be sufficiently versatile to accommodate other complementary observables apart from @xmath3 and @xmath4 , such as components of angular momenta , or energy and time . because in the above cases the variance is taken as a measure of uncertainty '' , expressions of this type are also known as variance - based uncertainty relations ( vur ) . since robertson and schrdinger s papers , a multitude of vurs has been devised ; examples include the fourier - type uncertainty relations of bohr and wigner  @xcite , the fractional fourier - type uncertainty relations of mustard  @xcite , mixed - states uncertainty relations  @xcite , the angle - angular momentum uncertainty relation of lvy - leblond  @xcite and carruthers and nietto  @xcite , the time - energy uncertainty relation of mandelstam and tamm  @xcite , luisell s amplitude - phase uncertainty relation  @xcite , and synge s three - observable uncertainty relations  @xcite .",
    "many authors  @xcite have , however , remarked that even vurs have many limitations .",
    "in fact , the essence of a vur is to put an upper bound to the degree of concentration of two ( or more ) probability distributions , or , equivalently impose a lower bound to the associated uncertainties .",
    "while the variance is often a good measure of the concentration of a given distribution , there are many situations where this is not the case .",
    "for instance , variance as a measure of concentration is a dubious concept in the case when a distribution contains more than one peak .",
    "besides , variance diverges in many distributions even though such distributions are sharply peaked .",
    "notorious examples of the latter are provided by heavy - tail distributions such as lvy  @xcite , weibull  @xcite or cauchy  lorentz distributions  @xcite .",
    "for instance , in the theory of bright  wigner shapes it has been known for a long time  @xcite that the cauchy ",
    "lorentz distribution can be freely concentrated into an arbitrarily small region by changing its scale parameter , while its standard deviation remains very large or even infinite",
    ".    another troublesome feature of vurs appears in the case of finite - dimensional hilbert spaces , such as the hilbert space of spin or angular momentum .",
    "the uncertainty product can attain zero minimum even when one of the distributions is not absolutely localized , i.e. , even when the value of one of the observables is not precisely known  @xcite .",
    "in such a case the uncertainty is just characterized by the lower bound of the uncertainty product ( i.e. , by zero ) and thus it only says that this product is greater than zero for some states and equal to zero for others .",
    "this is , however , true also in classical physics .",
    "the previous examples suggest that it might be desirable to quantify the inherent quantum unpredictability in a different , more expedient way .",
    "a distinct class of such non - variance - based uncertainty relations are the uncertainty relations based on information theory . in these the uncertainty",
    "is quantified in terms of various information measures  entropies , which often provide more stringent bound on concentrations of the probability distributions .",
    "the purpose of the present paper is to give a brief account of the existing information - theoretic uncertainty relations ( itur ) and present some new results based on rnyi entropy .",
    "we also wish to promote the notion of rnyi entropy ( re ) which is not yet sufficiently well known in the physics community .",
    "our paper is organized in the following way : in section  [ i ] , we provide some information - theoretic background on the rnyi entropy ( re ) .",
    "in particular , we stress distinctions between the re for discrete probabilities and re for continuous probability density functions ( pdf )  the so - called differential re . in section  [ ii ]",
    "we briefly review the concept of _ entropy power _ both for shannon and rnyi entropy .",
    "we also prove the generalized entropy power inequality . with the help of the riesz  thorin inequality we derive in section  [ sec3 ] the re - based itur for discrete distributions .",
    "in addition , we also propose a geometric illustration of the latter in terms of the _ condition number _ and _ distance to singularity_. in section  [ sec4 ] we employ the beckner ",
    "babenko inequality to derive a continuous variant of the re - based itur .",
    "the result is phrased both in the language of res and generalized entropy powers .",
    "in particular , the latter allows us to establish a logical link with the robertson  schrdinger vur .",
    "the advantage of iturs over the usual vur approach is illustrated in section  [ sec5 ] . in two associated subsections",
    "we first examine the rle of a discrete generalized itur on a simple two - level quantum system . in the second subsection",
    "the continuous itur is considered for quantum - mechanical systems with heavy - tailed distributions and schrdinger cat states .",
    "an improvement of the rnyi itur over both the robertson  schrdinger vur and shannon itur is demonstrated in all the cases discussed . finally in section  [ sec6 ]",
    "we make some concluding remarks and propose some generalizations . for the reader",
    "s convenience we relegate to appendix  a some of the detailed mathematical steps needed in sections  [ ii.a ] and [ sec4 ] .",
    "the basic notion that will be repeatedly used in the following sections is the notion of rnyi entropy . for this reason we begin here with a brief review of some of its fundamental properties .",
    "res constitute a one - parameter family of information entropies labeled by rnyi s parameter @xmath5 and fulfill additivity with respect to the composition of statistically independent systems .",
    "the special case with @xmath6 corresponds to the familiar shannon entropy .",
    "it can be shown that rnyi entropies belong to the class of mixing homomorphic functions  @xcite and that they are analytic for @xmath7 s which lie in @xmath8 quadrants of the complex plane  @xcite . in order to address the uncertainty relations issue",
    "it is important to distinguish two situations .",
    "let @xmath9 be a random variable admitting @xmath10 different events ( be it outcomes of some experiment or microstates of a given macrosystem ) , and let @xmath11 be the corresponding probability distribution .",
    "information theory then ensures that the most general information measures ( i.e. entropy ) compatible with the additivity of independent events are those of rnyi  @xcite : @xmath12 form ( [ ren11 ] ) is valid even in the limiting case when @xmath13 . if , however , @xmath10 is finite then rnyi entropies are bounded both from below and from above : @xmath14 in addition , res are monotonically decreasing functions in @xmath7 , so @xmath15 if and only if @xmath16 .",
    "one can reconstruct the entire underlying probability distribution knowing all rnyi distributions via the widder ",
    "stiltjes inverse formula  @xcite . in this case",
    "the leading order contribution comes from @xmath17 , i.e. from shannon s entropy .",
    "some elementary properties of @xmath18 are as follows :    1 .",
    "re is symmetric : @xmath19  .",
    "re is nonnegative : @xmath20  .",
    "re is decisive : @xmath21  .",
    "4 .   for @xmath22 re",
    "is concave ; for @xmath23 re in neither convex nor concave .",
    "re is bounded , continuous and monotonic in @xmath7  .",
    "re is analytic in @xmath24 @xmath25 for @xmath26 it equals to shannon s entropy , i.e. @xmath27 .    among a myriad of information measures res distinguish themselves by having a firm operational characterization in terms of block coding and hypotheses testing .",
    "parameter @xmath7 is then directly related to so - called @xmath28-cutoff rates  @xcite .",
    "re is used in coding theory  @xcite , cryptography  @xcite , finance  @xcite and in theory of statistical inference  @xcite . in physics one",
    "often uses @xmath29 in the framework of quantum information theory  @xcite .",
    "let @xmath30 be a measurable set on which is defined a continuous probability density function ( pdf ) @xmath31 .",
    "we will assume that the support ( or outcome space ) is a smooth but not necessarily compact manifold . by covering the support with the mesh @xmath32 of @xmath33dimensional ( disjoint )",
    "cubes @xmath34 @xmath35 of size @xmath36 we may define the integrated probability in @xmath37th cube as @xmath38 this defines the mesh probability distribution @xmath39",
    ". infinite precision of measurements ( i.e. , when @xmath40 ) often brings infinite information . as the most junk \" information comes from the uniform distribution @xmath41 , it is more sensible to consider the relative information entropy rather than absolute one . in references",
    "@xcite it was shown that in the limit @xmath13 ( i.e. , @xmath40 ) it is possible to define a finite information measure compatible with information theory axioms .",
    "renormalized _ rnyi entropy , often known as _ differential re entropy _",
    ", reads @xmath42 here @xmath43 is the volume of @xmath30 .",
    "equation ( [ ren55 ] ) can be viewed as a generalization of the kullback ",
    "leibler relative entropy  @xcite . when @xmath30 is compact it is possible to introduce a simpler alternative prescription as @xmath44 in both previous cases @xmath45 represents the euclidean dimension of the support .",
    "rnyi entropies ( [ ren55 ] ) and ( [ ren6 ] ) are defined if ( and only if ) the corresponding integral @xmath46 exists .",
    "equations ( [ ren55 ] ) and ( [ ren6 ] ) indicate that the asymptotic expansion for @xmath47 has the form : @xmath48 here @xmath49 is the covering volume and the symbol @xmath50 is the residual error which tends to @xmath51 for @xmath40 .",
    "in contrast to the discrete case , rnyi entropies @xmath52 are not generally positive .",
    "in particular , a distribution which is more confined than a unit volume has less re than the corresponding entropy of a uniform distribution over a unit volume and hence yields a negative @xmath52 .",
    "a paradigmatic example of this type of behavior is the @xmath53-function pdf in which case @xmath54 , for all @xmath7 .",
    "information measures @xmath55 and @xmath52 are often applied in theory of statistical inference  @xcite and in chaotic dynamical systems  @xcite .",
    "the mathematical underpinning for most uncertainty relations used in quantum mechanics lies in inequality theory .",
    "for example , the wave - packet uncertainty relations are derived from the plancherel inequality , and the celebrated robertson ",
    "schrdinger s vur is based on the cauchy ",
    "schwarz inequality ( and ensuing parseval equality )  @xcite .",
    "similarly , fourier - type uncertainty relations are based on the hausdorff  young inequality  @xcite , etc .    in information theory",
    "the key related inequalities are a ) young s inequality that implies the entropy power inequalities , b ) the riesz  thorin inequality that determines the generalized entropic uncertainty relations and c ) the cramr  rao and logarithmic sobolev inequalities that imply fisher s information uncertainty principle . in this section",
    "we will briefly review the concept of the _ entropy power _ and the ensuing _ entropy power inequality_. both concepts were developed by shannon in his seminal 1948 paper in order to bound the capacity of non - gaussian additive noise channels  @xcite .",
    "the connection with quantum mechanics was established by stam  @xcite , lieb  @xcite and others who used the entropy power inequality to prove standard vur .    in the second part of this section",
    "we show how the entropy power can be extended into the re setting . with the help of young s inequality",
    "we find the corresponding generalized entropy power inequality .",
    "related applications to quantum mechanics will be postponed to section  [ sec5.b ] .",
    "suppose that @xmath56 is a random vector in @xmath57 with the pdf @xmath58 .",
    "the differential ( or continuous ) entropy @xmath59 of @xmath56 is defined as @xmath60 the discrete version of ( [ iia1 ] ) is nothing but the shannon entropy  @xcite , and in such a case it represents an average number of binary questions that are needed to reveal the value of @xmath61 .",
    "actually , ( [ iia1 ] ) is not a proper entropy but rather information gain  @xcite as can be seen directly from ( [ ren6 ] ) when the limit @xmath62 is taken .",
    "we shall return to this point in section  5 .",
    "the _ entropy power _",
    "@xmath63 of @xmath61 is the unique number such that  @xcite @xmath64 where @xmath65 is a gaussian random vector with zero mean and variance equal to @xmath63 , i.e. , @xmath66 .",
    "eq.([3.1.0b ] ) can be equivalently rewritten in the form @xmath67 with @xmath68 representing a gaussian random vector with the zero mean and unit covariance matrix .",
    "the solution of both ( [ 3.1.0b ] ) and ( [ 3.1.0bv ] ) is then @xmath69 let @xmath70 and @xmath71 be two independent continuous vector valued random variables of finite variance . in the case when the shannon differential entropy is measured in",
    "_ nats _ ( and not bits ) we get for the entropy power @xmath72 the differential entropy ( [ a.8.a ] ) ( as well as ( [ a.8.b ] ) ) satisfies the so - called _ entropy power inequality _",
    "@xmath73 where the equality holds iff @xmath70 and @xmath71 are multivariate normal random variables with proportional covariance matrices  @xcite . in general , inequality ( [ 3.1.0 ] ) does not hold when @xmath70 and @xmath71 are discrete random variables and the differential entropy is replaced with the discrete entropy .",
    "shannon originally used this inequality to obtain a lower bound for the capacity of non - gaussian additive noise channels . since shannon s pioneering paper several proofs of the entropy power inequality have become available  @xcite .      in the following",
    "we will show how it is possible to extend the entropy power concept to res . to this end",
    "we first define rnyi entropy power ( for simplicity we use nats as units of information ) .",
    "[ section ]    let @xmath74 and let @xmath61 be a random vector in @xmath75 with probability density @xmath76 .",
    "the @xmath4-th rnyi entropy power of @xmath61 is defined as @xmath77 where @xmath78 is the hlder conjugate of @xmath4 .",
    "the above form of @xmath79 was probably firstly stated by gardner  @xcite who , however , did not develop the analogy with @xmath63 any further .",
    "plausibility of @xmath79 as the entropy power comes from the following important properties :    [ section ]    the @xmath4-th rnyi entropy power @xmath79 is a unique solution of the equation @xmath80 with @xmath68 representing a gaussian random vector with zero mean and unit covariance matrix .",
    "in addition , in the limit @xmath81 one has @xmath82 .",
    "let @xmath70 and @xmath71 be two independent continuous random vectors in @xmath57 with probability densities @xmath83 and @xmath84 , respectively .",
    "suppose further that @xmath85 and @xmath86 , and let @xmath87 then the following inequality holds : @xmath88 additionally , in the limits @xmath89 the inequality ( [ 3.1.0a ] ) reduces to the shannon entropy power inequality ( [ 3.1.0 ] ) and @xmath90.[th1 ]    _ proof of theorem  [ th1 ] .",
    "_    that @xmath79 from definition  iii.1 is the only solution of ( [ 3.1.0k ] ) follows from the scaling property of @xmath91 , namely @xmath92 where @xmath93 .",
    "the above scaling relation follows directly from the definition of @xmath91 and from a change of variable argument .",
    "we can further use the simple fact that @xmath94 to see that ( [ 3.1.0k ] ) leads to the equation @xmath95 this yields @xmath96 which , for @xmath91 measured in _ nats _",
    ", coincides with ( [ 3.1.0e ] ) .    to prove the inequality ( [ 3.1.0a ] )",
    "we first realize that @xmath4 , @xmath97 and @xmath98 represent hlder s triple , i.e. @xmath99 this allows us to use young s inequality ( q.v .",
    "appendix  a ) , which for the case at hand reads @xmath100 where @xmath101 is a constant defined in appendix  a. the left - hand - side of ( [ 3.1.0d ] ) can be explicitly written as @xmath102^{1/r } .",
    "\\label{3.1.0f}\\end{aligned}\\ ] ] the probability @xmath103 is nothing but the joint probability that @xmath104 and @xmath105 . the quantity inside @xmath106 thus represents the density function for the sum of two random variables @xmath107 . with the help of ( [ 3.1.0e ] ) we can rewrite ( [ 3.1.0f ] ) as @xmath108^{-d/2r ' }",
    "r^{-d/2r}. \\label{3.1.0g}\\end{aligned}\\ ] ] on the other hand , the right - hand - side of ( [ 3.1.0d ] ) is @xmath109^{-d/2q'}[2\\pi { n}_{p}(\\mathcal{x}_{2})]^{-d/2p ' } q^{-d/2q } p^{-d/2p}\\ , .",
    "\\label{3.1.0h}\\end{aligned}\\ ] ] plugging ( [ 3.1.0 g ] ) and ( [ 3.1.0h ] ) into the young inequality ( [ 3.1.0d ] ) we obtain @xmath110^{r'/q ' } [ { n}_{p}(\\mathcal{x}_{2})]^{r'/p'}\\nonumber \\\\[2 mm ] & = & \\left(\\frac{{n}_{q}(\\mathcal{x}_{1})}{1-\\lambda } \\right)^{1-\\lambda } \\left(\\frac{{n}_{p}(\\mathcal{x}_{2})}{\\lambda } \\right)^{\\lambda}.\\end{aligned}\\ ] ] this completes the proof of the inequality ( [ 3.1.0a ] ) .",
    "it remains to show that in the limits @xmath89 we regain the shannon entropy power inequality .",
    "firstly , the above limits directly give the inequality @xmath111 which holds without restrictions on @xmath85 .",
    "the best estimate ( the highest lower bound ) is obtained for @xmath112 that extremizes the right - hand - side .",
    "assuming that the right - hand - side is for fixed @xmath70 and @xmath71 a smooth function of @xmath112 , we can take its derivative with respect to @xmath112 .",
    "this equals zero when @xmath113 positivity of @xmath114 then ensures that @xmath112 , which extremizes the right - hand - side of ( [ 3.1.0i ] ) , belongs to the interval @xmath115 .",
    "in addition , the extremum is actually a maximum because the second derivative is @xmath116 ^ 3/{n}(\\mathcal{x}_{1}){n}(\\mathcal{x}_{2})$ ] which is clearly negative . by inserting ( [ 3.1.0j ] ) into ( [ 3.1.0i ] )",
    "we regain the shannon entropy power inequality .    to prove that @xmath63 is a limiting case of @xmath79 for @xmath81",
    ", we just realize that @xmath117 and @xmath118 .",
    "thus indeed in the @xmath81 limit we regain the original shannon entropy power @xmath63 as well as the usual entropy power inequality ( [ 3.1.0 ] ) .",
    "@xmath119 + in passing we may observe that from the definition ( [ 3.1.0e ] ) and eqs .",
    "( [ 14.bb])-([15.bb ] ) it follows that @xmath120 , i.e. the power entropy coincides for gaussian processes with the variance @xmath121 . in case when @xmath122 represents a random gaussian vector of zero mean and covariance matrix @xmath123 , then latexmath:[$n_p(\\mathcal{z}_g ) =    @xmath4-independent and hence valid also for the original shannon entropy power .",
    "to prove the information uncertainty relation based on re we need to prove a particular variant of the riesz  thorin inequality  @xcite upon which our considerations will be based . for this purpose we first state the riesz convexity theorem .",
    "[ riesz convexity theorem]let @xmath125 be a linear operator ( i.e. , @xmath126 ) and @xmath127 .",
    "let , in addition , @xmath128 be the least number  @xmath37 \" satisfying @xmath129 then @xmath130 is convex in triangle @xmath131 , @xmath132 .",
    "_ proof of theorem  [ th2 ] .",
    "_ we shall use the notation @xmath145 , @xmath146 ( and the hlder conjugates @xmath147 , @xmath148 ) . consider the line from @xmath149 to @xmath150 in the @xmath151 plane .",
    "this line lies entirely in the triangle of concavity ( see figure [ concfig ] ) .",
    "let us now define @xmath152 implying @xmath153 , and define @xmath154 implying @xmath155 hence @xmath156 note particularly that because @xmath157 $ ] then @xmath158 $ ] and @xmath140 $ ] . to estimate the right hand side of ( [ proof2 ] ) we first realize that @xmath159 .",
    "this results from the very assumption of the theorem , namely that @xmath160 hence , @xmath161 . to find the estimate for @xmath162",
    "we realize that it represents the smallest @xmath37 in the relation @xmath163 thus @xmath164 so finally we can write that @xmath165      to establish the connection with re let us assume that @xmath56 is a discrete random variable with @xmath10 different values , @xmath166 is the probability space affiliated with @xmath56 and @xmath11 is a sample probability distribution from @xmath166 .",
    "normally the geometry of @xmath166 is identified with the geometry of a simplex . for our purpose",
    "it is more interesting to embed @xmath166 in a sphere .",
    "because @xmath167 is non  negative and summable to unity , it follows that the square ",
    "root likelihood @xmath168 exists for all @xmath169 , and it satisfies the normalization condition @xmath170 hence @xmath171 can be regarded as a unit vector in the hilbert space @xmath172",
    ". then the inner product @xmath173 defines the angle @xmath174 that can be interpreted as a distance between two probability distributions .",
    "more precisely , if @xmath175 is the unit sphere in the @xmath10-dimensional hilbert space , then @xmath174 is the spherical ( or geodesic ) distance between the points on @xmath175 determined by @xmath176 and @xmath177 .",
    "clearly , the maximal possible distance , corresponding to orthogonal distributions , is given by @xmath178 .",
    "this follows from the fact that @xmath176 and @xmath177 are non  negative , and hence they are located only on the positive orthant of @xmath175 ( see figure  [ fig1 ] ) .        the geodesic distance @xmath174 is called the bhattacharyya distance .",
    "the representation of probability distributions as points on a sphere also has an interesting relation to bayesian statistics .",
    "if we use a uniform distribution on the sphere as the prior distribution then the prior distribution on probability vectors in @xmath166 is exactly the celebrated jeffrey s prior that has found new justification via the minimum description length approach to statistics  @xcite .",
    "now , let @xmath179 and @xmath180 denote a pair of probability distributions and @xmath176 and @xmath181 the corresponding elements in hilbert space .",
    "because @xmath176 and @xmath177 are non - negative , they are located only on the positive orthant of @xmath175 .",
    "the transformation @xmath182 then corresponds to a rotation with @xmath183 .    to proceed , we set @xmath184 and @xmath185 ( remembering that @xmath186 )",
    ". then the riesz  thorin inequality reads ( with @xmath187 ) @xmath188 which is equivalent to @xmath189 we raise both sides to the power @xmath190 and get @xmath191 the parameters are limited due to the condition @xmath140 $ ] and @xmath186 implying @xmath192 this implies that @xmath193 $ ] and @xmath194 . combining ( [ ineq ] ) and ( [ iden ] )",
    "we get @xmath195 by applying the negative binary logarithm on both sides of ( [ prae ] ) we get the following theorem .",
    "[ hoved]suppose that @xmath196 and that @xmath197 define @xmath198 if @xmath193 $ ] and @xmath199 and the probability distributions @xmath179 and @xmath180 are related by @xmath200 where @xmath201 , then @xmath202    two immediate comments are in order .",
    "firstly , one can extend the domain of validity of both @xmath98 and @xmath203 by noticing that @xmath179 and @xmath180 are interchangeable in the above derivation without altering  @xcite the actual value of @xmath204 .",
    "this has the consequence that one may phrase both resultant inequalities as a single inequality where both @xmath98 and @xmath203 belong to the interval @xmath205 with @xmath206 . secondly , because the information measure @xmath29 is always non - negative , the inequality ( [ 3.3.1 ] ) can represent a genuine uncertainty relation only when @xmath207 . note that for @xmath208 or @xmath209 ( i.e. for most physically relevant situations ) one always has that @xmath210 .",
    "this is because for such @xmath211 s @xmath212 the last identity results from the fact that all of eigenvalues of @xmath208 or @xmath209 have absolute value @xmath213 .",
    "it needs to be stressed that in the particular case when @xmath214 ( and thus also @xmath215 ) we get @xmath216 this shannon entropy based uncertainty relation was originally found by kraus  @xcite and maassen  @xcite .",
    "a weaker version of this itur was also earlier proposed by deutsch  @xcite .",
    "the reader can see that itur ( [ 3.3.1 ] ) which is based on re provides a natural extension of the shannon itur ( [ kendt ] ) . in section  [ sec5 ]",
    "we shall see that there are quantum mechanical systems where rnyi s itur improves both on robertson ",
    "schrdinger s vur and shannon s itur .",
    "let us close this section by providing a useful geometric understanding of the inequality ( [ 3.3.1 ] ) . to this end",
    "we invoke two concepts known from error analysis .",
    "these are , the _ condition number _ and _ distance to singularity _",
    "( see , e.g. , refs .",
    "@xcite ) .",
    "the _ condition number _ @xmath217 of the non - singular matrix @xmath211 is defined as @xmath218 where , the corresponding ( mixed ) matrix - valued norm @xmath219 is defined as @xmath220 from ( [ a.22 ] ) is nothing but @xmath221 .",
    "note also that @xmath222 , which is the usual @xmath7-matrix norm .",
    "justification for calling @xmath223 a condition number comes from the following theorem :    let @xmath224 be a linear equation and let there be an error ( or uncertainty ) @xmath225 in representing the vector @xmath226 , and let @xmath227 solve the new error - hindered equation @xmath228 .",
    "the relative disturbance in @xmath229 in relation to @xmath225 fulfills @xmath230 [ th4 ]    _ proof of theorem  [ th4 ] . _",
    "the proof is rather simple . using the fact that @xmath224 and @xmath228 we obtain @xmath231 . taking @xmath7-norm on both sides we can write @xmath232 on the other hand , the @xmath28-norm of @xmath224 yields @xmath233 combining ( [ 34a ] ) with ( [ 35a ] ) we obtain ( [ 33a ] ) .",
    "@xmath234 + from the previous theorem we see that @xmath217 quantifies a _ stability _ of the linear equation @xmath224 , or better the extent to which the relative error ( uncertainty ) in @xmath226 influences the relative error in @xmath229 . a system described by @xmath211 and @xmath226 is stable if @xmath217 is not too large ( ideally close to one ) .",
    "it is worth of stressing that @xmath235 .",
    "the latter results from the fact that @xmath236 in the last step we have used the submultiplicative property of mixed matrix norms .    the second concept  the _ distance to singularity _ for a matrix @xmath211 , is defined as latexmath:[\\ ] ] this is clearly solved with @xmath494 . by choosing @xmath495",
    "$ ] we get improvement over the hausdorff  young inequalities .",
    "t.  arimitsu and n.  arimitsu , j.  phys .",
    "a : math .  gen .  * 33 * ( 2000 ) l235 .",
    "t.  arimitsu and n.  arimitsu , physica a  * 295 * ( 2001 ) ( 2001 ) 673 .",
    "t.  arimitsu and n.  arimitsu , physica a  * 305 * ( 2002 ) 218",
    ". t.  arimitsu and n.  arimitsu , condens .",
    "matter * 14 * ( 2002 ) 2237 ."
  ],
  "abstract_text": [
    "<S> uncertainty relations based on information theory for both discrete and continuous distribution functions are briefly reviewed . </S>",
    "<S> we extend these results to account for ( differential ) rnyi entropy and its related entropy power . </S>",
    "<S> this allows us to find a new class of information - theoretic uncertainty relations ( iturs ) . </S>",
    "<S> the potency of such uncertainty relations in quantum mechanics is illustrated with a simple two - energy - level model where they outperform both the usual robertson  </S>",
    "<S> schrdinger uncertainty relation and kraus  maassen shannon entropy based uncertainty relation . in the continuous case </S>",
    "<S> the ensuing entropy power uncertainty relations are discussed in the context of heavy tailed wave functions and schrdinger cat states . </S>",
    "<S> again , improvement over both the robertson  schrdinger uncertainty principle and shannon itur is demonstrated in these cases . </S>",
    "<S> further salient issues such as the proof of a generalized entropy power inequality and a geometric picture of information - theoretic uncertainty relations are also discussed . </S>"
  ]
}