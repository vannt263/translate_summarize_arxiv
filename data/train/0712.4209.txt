{
  "article_text": [
    "in the last few decades it has become apparent that many problems in information theory have analogies to certain problems in the area of statistical physics of disordered systems .",
    "such analogies are useful because physical insights , as well as statistical mechanical tools and analysis techniques can be harnessed in order to advance the knowledge and the understanding with regard to the information ",
    "theoretic problem under discussion .",
    "one important example of such an analogy is between the statistical physics of disordered magnetic materials , a.k.a .",
    "spin glasses , and the behavior of certain ensembles of random codes for source coding ( see , e.g. , @xcite , @xcite , @xcite , @xcite ) and for channel coding ( see , e.g. , @xcite and references therein , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite ) .    among the various models of interaction disorder in spin glasses , one of the most fascinating models is the _ random energy model _ ( rem ) , invented by derrida in the early eighties @xcite , @xcite , @xcite ( see also , e.g. , @xcite , @xcite , @xcite , for later developments ) .",
    "the rem is on the one hand , extremely simple and easy to analyze , and on the other hand , rich enough to exhibit phase transitions .",
    "according to the rem , the different spin configurations are distributed according to the boltzmann distribution , namely , their probabilities are proportional to an exponential function of their negative energies , but the configuration energies themselves are i.i.d .  random variables , hence the name random energy model .. ]    in ( * ? ? ? * chap .",
    "6 ) , mzard and montanari draw an interesting analogy between the rem and the statistical physics pertaining to _ finite temperature decoding",
    "_ @xcite of ensembles of random block codes .",
    "the relevance of the rem here is due to the fact that in this context , the partition function that naturally arises has the log  likelihood function ( of the channel output given the input codeword ) as its energy function ( hamiltonian ) , and since the codewords are selected at random , then the induced energy levels are random variables .",
    "consequently , the phase transitions of the rem are ` inherited ' by ensembles of random block codes , as is shown in @xcite . in @xcite , this subject",
    "was further studied and the free energies corresponding to the various phases were related to random coding exponents of the probability of error at rates below capacity and to the probability of correct decoding at rates above capacity .",
    "while the rem is a very simple and interesting model for capturing disorder , as described above , it is not quite faithful for the description of a real physical system .",
    "the reason is that according to the rem , any two distinct spin configurations , no matter how similar and close to each other , have independent , and hence unrelated , energies .",
    "a more realistic model must take into account the geometry and the structure of the physical system and thus allow dependencies between energies associated with closely related configurations .",
    "this observation has motivated derrida to develop the _ generalized random energy model _ ( grem )",
    "@xcite ( see also , e.g. , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , for later related work ) .",
    "the grem extends the rem in that it introduces an hierarchical structure in the form of a tree , by grouping subsets of ( neighboring ) spin configurations in several levels , where the leaves of this tree correspond to the various configurations . according to the grem , for every branch in this tree",
    ", there is an associated independent randomly chosen energy component .",
    "the total energy of each configuration is then the sum of these energy components along the branches that form the path from the root of the tree to the leaf corresponding to this configuration .",
    "this way , the degree of dependency between the energies of two different configurations depends on the ` distance ' between them on the tree : more precisely , it depends on the number of common branches shared by their paths from the root up to the node at which their paths split .",
    "the grem is somewhat more complicated to analyze than the rem , but not substantially so .",
    "it turns out that the number of phase transitions in the grem depends on the parameters of the model .",
    "if the tree has @xmath0 levels , there can be up to @xmath0 phase transitions , but there can also be a smaller number .",
    "for example , in the case @xmath1 , under a certain condition , there is only one phase transition and the behavior of the free energy in both phases is just like in the ordinary rem .    in analogy to the above described relationship between the rem and the statistical physics of random block codes , the natural question that now arises",
    "is whether the grem and its phase transitions can give us some insights about the behavior of code ensembles with some hierarchical structure ( e.g. , tree ",
    "structured codes , successive refinement codes , etc . ) .",
    "in particular , in what way do these phase transitions guide us in the choice of the design parameters of these codes ?",
    "it is the purpose of this paper to explore these questions and to give at least some partial answers .",
    "we demonstrate that there is indeed an intimate relationship between the grem and certain ensembles of hierarchical codes .",
    "consider , for example , a two  stage rate ",
    "distortion code of block length @xmath2 , where the first @xmath3 components of the reproduction vector , at rate @xmath4 , depend only on the first @xmath5 bits of the compressed bitstream , and the last @xmath6 symbols of the reproduction codeword , at rate @xmath7 , depend on the entire bitstream of length @xmath8 .",
    "the overall rate of this code is , of course , the weighted average of @xmath4 and @xmath7 with weights proportional to @xmath3 and @xmath6 , respectively .",
    "an ensemble of codes with this structure is defined as follows : first , we randomly draw a rate @xmath4 codebook of block length @xmath3 according to some distribution .",
    "then , for each resulting codeword of length @xmath3 , we randomly draw a rate @xmath7 codebook of block length @xmath6 . and",
    "@xmath6 , operating independently . ]",
    "thus , the code has a tree structure with two levels , like a two  level grem .",
    "the overall distortion of the code along the entire @xmath9 symbols is the sum of partial distortions along the two segments , in analogy to the above described additivity of the partial energies along the branches of the tree pertaining to the grem , and since the codewords are random , then so are the distortions they induce .",
    "the motivation for this class of codes , especially when the idea is generalized from two parts to a larger number of @xmath0 parts , say , of equal length ( @xmath10 ) , is that the delay , at least at the decoder , is reduced from @xmath9 to @xmath11 , because the decoder is causal in the level of segments of length @xmath11 .",
    "the following questions now arise : is there any inherent penalty , in terms of performance , for this ensemble of reduced delay decoding codes ?",
    "if so , how can we minimize this penalty ? if not , how should we choose the design parameters ( i.e. , @xmath12 and @xmath13 , @xmath14 , for a given overall average rate @xmath15 ) such that this code will ` behave ' like a full block code of length @xmath9 ?    for simplicity ,",
    "let us return to the case @xmath1 .",
    "for a given @xmath15 and @xmath9 , we have two degrees of freedom : the choices of @xmath4 and @xmath3 ( which will then dictate @xmath7 and @xmath6 ) .",
    "is it better to choose @xmath16 or @xmath17 , if at all it makes any difference ?",
    "a similar question can be asked concerning @xmath3 and @xmath6 .",
    "the answer depends , of course , on our figure of merit .",
    "obviously , if one is interested only in the asymptotic distortion , the question becomes uninteresting , because then by choosing two independent codes for the two parts , both at rate @xmath15 , the overall distortion will be given by the distortion  rate function , @xmath18 , just like that of the full unstructured code . for a given @xmath9 , of course",
    ", the redundancies will correspond to the shorter blocks @xmath3 and @xmath6 , but this is a second order effect . here , we choose to examine performance in terms of the characteristic function of the overall distortion , @xmath19 $ ] .",
    "this is , of course , a much more informative figure of merit than the average distortion , because in principle , it gives information on the entire probability distribution of the distortion .",
    "in particular , it generates all the moments of the distortion by taking derivatives , and it is useful in deriving chernoff bounds on probabilities of large deviations events concerning the distortion . in the context of the analogy with statistical physics and the grem , this characteristic function can easily be related to the partition function whose hamiltonian is given by the distortion .",
    "it turns out that the characteristic function of the distortion behaves in a rather surprisingly interesting manner and with a direct relation to the grem . for @xmath20 ,",
    "when the corresponding grem has @xmath1 phase transitions , the characteristic function of the distortion behaves like that of two independent block codes of lengths @xmath3 and @xmath6 and rates @xmath4 and @xmath7 , thus the dependency between the two parts of the code is not exploited in terms of performance . for @xmath16 , which is the case where the analogous grem has",
    "only one phase transition ( and behaves exactly like the ordinary rem , which is parallel to an ordinary random block code with no structure ) , the characteristic function behaves like that of a full unstructured optimum block code at rate @xmath15 across a certain interval of small @xmath21 , but beyond a certain point , it becomes inferior to that of a full code . for @xmath22 , it behaves like the unstructured code for the _ entire _ range of @xmath23 , but then one might as well use two independent block codes ( and reduce the search complexity at the encoder from @xmath24 to @xmath25 ) .",
    "the choices of @xmath3 and @xmath6 are immaterial in that sense , as long as they both grow linearly with @xmath9 .",
    "thus , the conclusion is that it is best to use @xmath26 , but if communication protocol constraints dictate different rates at different segments , then performance is better when @xmath16 than when @xmath20 .",
    "these results can be extended to the case of @xmath0 stages .",
    "a parallel analysis can be applied to analogous ensembles of ( reduced delay ) channel encoders of block length @xmath2 ( for the case @xmath1 ) , which have a similar tree structure : here , the first @xmath3 channel letters of each block depend only on the first @xmath5 information bits , whereas the other @xmath6 channel symbols depend on the entire information vector of length @xmath8 .",
    "the random codebook is again drawn hierarchically in the same manner as before . if the code performance is judged in terms of the error exponent , then once again , the choice @xmath27 is always better than the choice @xmath28 . here , unlike the source coding problem",
    ", there is an additional consideration : there are two types of incorrect codewords that are competing with the correct one in the decoding process : those for which the first @xmath3 channel inputs agree with those of the correct codeword ( the first segment is the same ) and those for which this is not the case . in this case",
    ", @xmath7 has to be chosen sufficiently small so that the error term contributed by erroneous codewords of the first kind would not dominate the probability of error . considering the case",
    "@xmath29 , if the overall average rate is not too small , it is possible to choose @xmath4 and @xmath7 so that the error exponent of this ensemble of codes is not worse than that of an ordinary random code with no structure .",
    "this idea can be extended to @xmath0 stages in a straightforward manner .",
    "in fact , we propose a systematic procedure to allocate rates to the different stages in a way that guarantees that the error exponent would be at least as good as that of the classical random coding error exponent pertaining to an ordinary random code at rate @xmath15 .",
    "the outline of this paper is as follows . in section 2",
    ", a few notation conventions are described . in section 3",
    ", we provide some more detailed background in statistical physics , with emphasis on the rem and the grem .",
    "finally , in section 4 , we present our main results on hierarchical code ensembles of the type described above , along with their relationship to the grem .",
    "readers who are not interested in the relationship with statistical physics ( although this is one of the main points in the paper ) may skip section 3 and ignore , in section 4 , the comments on the statistical mechanical aspects , all this without essential loss of continuity .",
    "throughout this paper , scalar random variables ( rv s ) will be denoted by capital letters , like @xmath30 and @xmath31 , their sample values will be denoted by the respective lower case letters , and their alphabets will be denoted by the respective calligraphic letters .",
    "a similar convention will apply to random vectors and their sample values , which will be denoted with the same symbols in the boldface font .",
    "thus , for example , @xmath32 will denote a random @xmath9-vector @xmath33 , and @xmath34 is a specific vector value in @xmath35 , the @xmath9-th cartesian power of @xmath36 .",
    "sources and channels will be denoted generically by the letters @xmath37 and @xmath38 .",
    "specific letter probabilities corresponding to a source @xmath38 will be denoted by the corresponding lower case letters , e.g. , @xmath39 is the probability of a letter @xmath40 .",
    "a similar convention will be applied to the channel @xmath37 and the corresponding transition probabilities , @xmath41 , @xmath40 , @xmath42 .",
    "the expectation operator will be denoted by @xmath43 .",
    "the cardinality of a finite set @xmath44 will be denoted by @xmath45 . for two positive sequences @xmath46 and @xmath47",
    ", the notation @xmath48 means that @xmath49 and @xmath50 are asymptotically of the same exponential order , that is , @xmath51 .",
    "similarly , @xmath52 means that @xmath53 , etc .",
    "information theoretic quantities like entropies and mutual informations will be denoted following the usual conventions of the information theory literature .",
    "in this section , we provide some basic background in statistical physics , focusing primarily on the rem , along with its relevance to ordinary ensembles of source and channel block codes , and then we extend the scope to the grem .      consider a physical system with a large number @xmath9 of particles , which can be in a variety of ` microstates ' pertaining to the various combinations of the microscopic physical states ( characterized by position , momentum , spin , etc . )",
    "that these particles may have .",
    "for each such microstate of the system , which we shall designate by a vector @xmath54 , there is an associated energy , given by an energy function ( hamiltonian ) @xmath55 .",
    "one of the most fundamental results in statistical physics ( based on the law of energy conservation and the basic postulate that all microstates of the same energy level are equiprobable ) is that when the system is in equilibrium , the probability of a microstate @xmath54 is given by the boltzmann distribution @xmath56 where @xmath57 is the inverse temperature , that is , @xmath58 , @xmath59 being temperature , , where @xmath0 is boltzmann s constant , but following the common abuse of the notation , we redefine @xmath60 as temperature ( in units of energy ) . ] and @xmath61 is the normalization constant , called the _ partition function _ , which is given by @xmath62 or @xmath63 depending on whether @xmath54 is discrete or continuous .",
    "the role of the partition function is by far deeper than just being a normalization factor , as it is actually the key quantity from which many macroscopic physical quantities can be derived , for example , the free energy is @xmath64 , the average internal energy ( i.e. , the expectation of @xmath55 where @xmath54 drawn is according ( [ bd ] ) ) is given by the negative derivative of @xmath65 , the heat capacity is obtained from the second derivative , etc .",
    "one of the important examples of such a multi  particle physical system is that of a magnetic material , in which each molecule has a magnetic moment , a three  dimensional vector which tends to align with the magnetic field felt by that molecule .",
    "in addition to the influence of a possible external magnetic field , there is also an effect of mutual interactions between the magnetic moments of various ( neighboring ) molecules .",
    "quantum mechanical considerations dictate that the set of possible configurations of each magnetic moment ( spin ) is discrete : in the simplest case , it has only two possible values , which we shall designate by @xmath66 ( spin up ) and @xmath67 ( spin down ) .",
    "thus , a spin configuration , i.e. , the vector of spins of @xmath9 molecules , is designated by a binary vector @xmath68 , where each component @xmath69 takes values in @xmath70 according to the spin of the @xmath71th molecule , @xmath72 .",
    "when the spins of a certain magnetic material tend to align in the same direction , the material is called _ ferromagnetic _ , and a customary model of the hamiltonian , the _ ising model _ ,",
    "is given by @xmath73 where the in first term , pertaining to the interaction , @xmath74 describes the intensity of the interaction with the summation being defined over pairs of neighboring spins ( depending on the geometry of the problem ) , and the second term is associated with an external magnetic field ( proportional to ) @xmath75 . when @xmath76 , the material is _",
    "antiferromagnetic _ ,",
    "namely , neighboring spins ` prefer ' to be antiparallel .",
    "more general models allow interactions not only with immediate neighbors , but also more distant ones , and then there are different strengths of interaction , depending on the distance between the two spins . in this case , the first term is replaced , by the more general form @xmath77 , where now the sum can be defined over all possible pairs @xmath78 . here , in addition to the ferromagnetic case , where all @xmath79 , and the antiferromagnetic case , where all @xmath80 , there is also a situation where some @xmath81 are positive and others are negative , which is the case if a _",
    "spin glass_. here , not all spin pairs can be in their preferred mutual position ( parallel / antiparallel ) , thus the system may be _ frustrated . _    to model situations of disorder , it is common to model @xmath81 as random variables ( rv s ) with , say , equal probabilities of being positive or negative . for example , in the edwards ",
    "anderson ( ea ) model @xcite , @xmath81 are taken to be i.i.d .",
    "zero  mean gaussian rv s when @xmath71 and @xmath82 are neighbors and zero otherwise .",
    "in the sherrington  kirkpatrick ( sk ) model @xcite , all @xmath83 are i.i.d .",
    "zero  mean gaussian rv s .",
    "thus , the system has two levels of randomness : the randomness of the interaction coefficients and the randomness of the spin configuration given the interaction coefficients , according to the boltzmann distribution .",
    "however , the two sets of rv s are normally treated differently .",
    "the random coefficients are considered _ quenched _",
    "rv s in the terminology of physicists , namely , they are considered fixed in the time scale at which the spin configuration may vary .",
    "this is analogous to the situation of coded communication in a random coding paradigm : a randomly drawn code should normally be thought of as a quenched entity , as opposed to the randomness of the source and/or the channel .      in @xcite,@xcite,@xcite , derrida took the above described idea of randomizing the ( parameters of the ) hamiltonian to an extreme , and suggested a model of spin glass with disorder under which the energy levels @xmath84 are simply i.i.d .",
    "rv s , without any structure in the form of ( [ ham ] ) or its above  described extensions .",
    "in particular , in the absence of a magnetic field , the @xmath85 rv s @xmath84 are taken to be zero ",
    "mean gaussian rv s , all with variance @xmath86 , where @xmath87 is a parameter . to match the behavior of the hamiltonian ( [ ham ] ) with a limited number of interacting neighbors and random interaction parameters , which has a number of independent terms that is linear in @xmath9 . ]",
    "the beauty of the rem is in that on the one hand , it is very easy to analyze , and on the other hand , it consists of sufficient richness to exhibit phase transitions .",
    "the basic observation about the rem is that for a typical realization of the configurational energies @xmath84 , the number of configurations with energy about @xmath88 ( i.e. , between @xmath88 and @xmath89 ) , @xmath90 , is proportional ( up to sub  exponential terms in @xmath9 ) to @xmath91 , as long as @xmath92 , whereas energy levels outside this range are typically not populated by spin configurations ( @xmath93 ) , as the probability of having at least one configuration with such an energy decays exponentially with @xmath9 .",
    "thus , the asymptotic ( thermodynamical ) entropy per spin , which is defined by @xmath94 is given by @xmath95 the partition function of a typical realization of a rem spin glass is then @xmath96 whose exponential growth rate , @xmath97 behaves according to @xmath98\\nonumber\\\\ & = & \\max_{|e|\\le e_0}\\left[\\ln 2- \\left(\\frac{e}{nj}\\right)^2-\\beta j \\cdot\\left(\\frac{e}{nj}\\right)\\right].\\end{aligned}\\ ] ] solving this simple optimization problem , we find that @xmath99 is given by @xmath100 which means that the asymptotic free energy per spin , a.k.a .  the _ free energy density _ , which is obtained by @xmath101 is given by ( cf .",
    "* proposition 5.2 ) ) : @xmath102 thus , the free energy density is subjected to a phase transition at the inverse temperature @xmath103 . at high temperatures ( @xmath104 ) , which is referred to as the _ paramagnetic phase _ , the partition function is dominated by an exponential number of configurations with energy @xmath105 and the entropy grows linearly with @xmath9 .",
    "when the system is cooled to @xmath106 and beyond , which is the _ glassy phase _ , the system freezes but it is still in disorder  the partition function is dominated by a subexponential number of configurations of minimum energy @xmath107 .",
    "the entropy , in this case , grows sublinearly with @xmath9 , namely the entropy per spin vanishes , and the free energy density no longer depends on @xmath57 .",
    "further details about the rem can be found in @xcite and the references mentioned in the introduction .",
    "as described in @xcite , there is an interesting analogy between the rem and the partition function pertaining to _ finite temperature decoding _",
    "@xcite of ensembles of channel block codes ( see also @xcite ) .    in particular , consider a codebook @xmath108 of @xmath109 binary codewords of length @xmath9 , @xmath110 , to be used across a binary symmetric channel ( bsc ) with crossover probability @xmath111 . given a binary vector @xmath112 at the channel output ,",
    "consider the generalized posterior parametrized by @xmath57 : @xmath113 where @xmath114 , @xmath115 is the hamming distance between @xmath54 and @xmath112 , and where the real posterior is obtained , of course , for @xmath116 .",
    "this is identified as a boltzmann distribution whose energy function ( which depends on the given @xmath112 ) is @xmath117 .",
    "as described in @xcite and @xcite , there are a few motivations for introducing the temperature parameter @xmath57 here .",
    "first , it allows a degree of freedom in case there is some uncertainty regarding the channel noise level ( small @xmath57 corresponds to high noise level ) .",
    "second , it is inspired by the ideas behind simulated annealing techniques : by sampling from @xmath118 while gradually increasing @xmath57 ( cooling the system ) , the minima of the energy function ( ground states ) can be found .",
    "third , by applying symbolwise map decoding , i.e. , decoding the @xmath119th symbol of @xmath54 as @xmath120 , where @xmath121 we obtain a family of _ finite  temperature decoders _ parametrized by @xmath57 , where @xmath116 corresponds to minimum symbol error probability ( with respect to the true channel ) and @xmath122 corresponds to minimum block error probability . as in @xcite , we will distinguish between two contributions of @xmath123 : one is @xmath124 , where @xmath125 is the actual codeword transmitted , and the other is @xmath126 , pertaining to all incorrect codewords .",
    "the former is typically about @xmath127 since @xmath128 concentrates about @xmath129 .",
    "we next focus on the behavior of @xmath130 .    to this end , consider a random selection of the code @xmath108 , where every bit of every codeword is drawn by an independent fair coin tossing . for a given @xmath112 ,",
    "the energy levels @xmath131 pertaining to all incorrect codewords are rv s ( exactly like in the rem ) because of the random selection of these codewords .",
    "now , the total number of correct codewords is about @xmath24 , and the probability that a randomly chosen @xmath54 would fall at distance @xmath132 from @xmath112 is exponentially @xmath133}$ ] , where @xmath134 then the typical number of codewords at normalized distance @xmath135 is about @xmath136}\\ ] ] as long as @xmath137 and @xmath138 when @xmath139 .",
    "thus , letting @xmath140 denote the small solution to the equation @xmath141 ( the gilbert ",
    "varshamov distance ) , we find that , with a clear analogy to the rem , the corresponding thermodynamical entropy is given by @xmath142 accordingly , the partition function @xmath130 of a typical code is given by @xmath143}\\cdot e^{-\\beta bn\\delta}{\\stackrel{\\cdot } { = } } \\exp\\{n[r-\\ln 2+\\max_{\\delta(r)\\le\\delta\\le 1-\\delta(r)}(h(\\delta)- \\beta b\\delta)]\\},\\ ] ] and the free energy density pertaining to @xmath144 behaves according to @xmath145 where @xmath146 and @xmath147}{b},\\ ] ] and where , again , the first line of @xmath148 corresponds to the paramagnetic phase with exponentially many codewords at distance ( energy ) @xmath149 from @xmath112 , and the second line is the glassy phase with subexponentially many codewords at distance @xmath150 . in @xcite ,",
    "these free energies are related to random coding exponents as mentioned in the introduction .    by the same token , in rate ",
    "distortion source coding , if one defines the partition function as @xmath151 with @xmath54 being the source vector , @xmath152 being the reproduction codevectors , and @xmath115 being the hamming distortion measure , then the same analysis takes place . in the sequel",
    ", we will motivate this definition of the partition function of rate ",
    "distortion coding and use it .",
    "as we have seen , the rem is an extremely simple model to analyze , but its simplicity is also recognized as a drawback from the aspect of faithfully modeling a spin glass .",
    "the reason for this is the lack of structure which is needed to allow dependencies between energy levels of spin configurations that are closely related : for example , if @xmath54 and @xmath153 differ only in a single component , it is conceivable that the respective energies would be close , as suggested by ( [ ham ] ) . to this end , as described in the introduction , derrida proposed a generalized version of the rem  the grem , which introduces dependencies between configurational energies in an hierarchical fashion .",
    "we next briefly review the grem .",
    "a grem with @xmath0 levels can best be thought of as a tree with @xmath85 leaves and depth @xmath0 , where each leaf represents one spin configuration .",
    "this tree is defined by @xmath0 positive parameters , @xmath154 , which are all in the interval @xmath155 , and whose product , @xmath156 , equals @xmath157 .",
    "the construction of this tree is as follows : the root of the tree is connected to @xmath158 distinct nodes , by integers . ] which will be referred to as first ",
    "level nodes .",
    "each first  level node is in turn connected to @xmath159 distinct second  level nodes , thus a total of @xmath160 second  level nodes . in the case",
    "@xmath1 , these second  level nodes are the leaves of the tree and @xmath161 . if @xmath162 , the process continues , and each second  level node is connected to @xmath163 third  level nodes , and so on . at the last step , each one of the @xmath164 nodes at level @xmath165",
    "is connected to @xmath166 distinct leaves , thus a total of @xmath167 leaves .",
    "the rem corresponds to the degenerate special case where @xmath168 .",
    "the random selection of energy levels for the grem is defined by another set of @xmath0 parameters , @xmath169 , which are all positive reals that sum to unity .",
    "the random selection is carried out in the following manner : for each one of the @xmath170 branches emanating from @xmath171th level nodes and connecting them to @xmath71th level nodes ( @xmath172 ) in the tree , we randomly choose an independent rv , henceforth referred to as a _",
    "branch energy _ , which is a zero mean , gaussian rv with variance @xmath173 , where @xmath87 is like in the rem and where @xmath174 are as described above .",
    "finally , the energy level of a given configuration is given by the sum of branch energies along the path from the root to the leaf that represents this configuration .",
    "thus , the total energy , is the sum of @xmath0 independent zero  mean gaussian rv s with variances @xmath173 , and so , it is zero  mean gaussian rv with variance @xmath86 , exactly like in the rem .",
    "however , now the energy levels of different configurations may be clearly correlated if the paths from the root to their corresponding leaves share some common branches before they split .",
    "the degree of statistical dependence is according to their distance along the tree .",
    "for example , if two configurations are first  degree siblings , i.e. , they share the same parent node at level @xmath165 , then all their energy components are the same except their last branch energies , which are independent . on the other extreme , if their paths are completely distinct , then their energies are independent .",
    "the grem for @xmath1 is analyzed in @xcite .",
    "we next present the derivation for this case ( with a few more details than in @xcite ) .",
    "let @xmath175 and @xmath176 be positive numbers whose product equals @xmath157 , and let @xmath177 and @xmath178 be positive numbers whose sum equals @xmath179 .",
    "now , every configuration with energy @xmath88 has some first  level branch energy @xmath180 and second  level branch energy @xmath181 . for a typical realization of this grem ,",
    "the number of first  level branches with energy about @xmath180 is exponentially @xmath182\\right\\},\\ ] ] provided that the expression in the square brackets is non  negative , i.e. , @xmath183 , and @xmath184 otherwise .",
    "therefore , the number of configurations with total energy about @xmath88 is exponentially @xmath185\\right\\},\\ ] ] whose exponential rate ( the entropy per spin ) is given by @xmath186.\\ ] ] note that @xmath187 is an even function , non  increasing in @xmath188 , and it should be kept in mind that beyond the value of @xmath188 at which @xmath187 vanishes , denote it by @xmath189 , we have @xmath190 since @xmath191 is typically zero ( as was the case with the rem ) .",
    "we shall get back to this point shortly , but for a moment , let us ignore it and solve the maximization problem pertaining to the above expression of @xmath187 , as is . denoting the resulting maximum by @xmath192 ( to distinguish from @xmath187 , where @xmath189 and the jump to @xmath193 are taken into account ) , we get : @xmath194 where @xmath195 .",
    "taking now into account the above mentioned observation concerning the criticality of the point @xmath196 , we have to distinguish between two cases .",
    "the first is the case where @xmath197 , namely , the first line of the above expression of @xmath192 vanishes for @xmath188 smaller than @xmath198 .",
    "the first line vanishes for @xmath199 , so the condition for this case to hold is @xmath200 , or equivalently , @xmath201 . in this case",
    ", we then have : @xmath202 which is exactly the same behavior as in the ordinary rem ( @xmath168 ) .",
    "consequently , the exponential rate of the partition function , which is given by @xmath203,\\ ] ] is also the same as in the rem , namely , @xmath204 where @xmath205 is the above defined critical inverse temperature of the rem ( see subsection [ rem ] ) .",
    "we next consider the complementary case where @xmath206 . in this case",
    ", the expression of @xmath187 should take into account the fact that it vanishes ( and then becomes @xmath207 ) according to the second line of ( [ psibasic ] ) .",
    "this amounts to : @xmath208 where @xmath209 . before we compute the corresponding partition function ,",
    "we make the following observation : @xmath210 where the inequality follows from the well  known inequality @xmath211/[\\sum_{i=1}^m b_i]\\le\\max_{1\\le i \\le m}a_i / b_i$ ] for positive @xmath212 and @xmath213 ( * ? ? ?",
    "* lemma 1 ) . in the same manner , using the similar inequality @xmath211/[\\sum_{i=1}^m",
    "b_i]\\ge\\min_{1\\le i \\le m}a_i / b_i$ ] , we get @xmath214 it follows then that the condition @xmath206 is equivalent to the condition @xmath215 . defining @xmath216 we then have @xmath217 .",
    "let us examine how @xmath99 behaves as @xmath57 grows from zero to infinity . for small enough @xmath57 , the achiever of @xmath99 , call it @xmath218 , is still smaller in absolute value than @xmath219 , and then it is obtained from equating to zero the derivative of @xmath220 $ ] , with @xmath187 being according to first line of ( [ 2phasetransitions ] ) , thus @xmath221 .",
    "this remains true as long as @xmath222 , which means @xmath223 . in this case , the partition function is dominated by @xmath224\\}$ ] first ",
    "level branches with energy @xmath225 , each followed by @xmath224\\}$ ] second  level branches with energy @xmath226 , and this is a pure paramagnetic phase . as @xmath57 continues to grow beyond @xmath227 , but is still below @xmath228 , the partition function is dominated by a subexponential number of first ",
    "level branches of energy @xmath229 followed by @xmath224\\}$ ] second  level branches with energy @xmath230 .",
    "this is a `` semi  glassy '' phase , where the first ",
    "level branches are already glassy but the second  level ones are still paramagnetic .",
    "as @xmath57 exceeds @xmath228 , this becomes a pure glassy phase where the partition function is dominated by a subexponential number of first  level branches with energy @xmath229 and a subexponential number of second  level branches with energy @xmath231 .",
    "accordingly , the function @xmath99 exhibits two phase transitions at inverse temperatures @xmath227 and @xmath228 : @xmath232 again , the free energy density is obtained by @xmath233 .",
    "this different behavior of the grem for the two different cases will be pivotal to our later discussion on the parallel behavior of ensembles of codes . when there is a general number @xmath0 of levels , the above analysis of the grem becomes , of course , more complicated and there are more cases to consider , but the concepts remain the same",
    ". there can be up to @xmath0 phase transitions , but there can be less , depending on the parameters of the model @xmath234 . for details ,",
    "the reader is referred to @xcite,@xcite .",
    "in analogy to the relationship between the rem and ordinary ensembles of block codes , as was described in subsection [ remc ] , it is natural to wonder about the possibility of similar relationships between the grem and more general ensembles of block codes , and to ask whether the fact that the grem exhibits different types of behavior ( as we have seen in subsection [ grem ] ) , has implications on the behavior of these ensembles of codes . since the grem is defined by an hierarchical ( tree ) structure , it is plausible to expect that if a relationship to coding exists , it will be in the context of ensembles of codes which have hierarchical structures as well .",
    "hierarchically structured ensembles of codes are encountered in numerous applications in information theory , including block  causal tree ",
    "structured source codes and channel codes of the type described informally in the introduction , successive refinement source codes @xcite,@xcite,@xcite , codes for the broadcast channel ( * ? ? ?",
    "* chap .  15.6 ) and codes based on binning techniques ( see , e.g. , @xcite,@xcite,@xcite ) , just to name a few . in this paper",
    ", we confine our attention to the first above  mentioned class of codes .",
    "the fact that the grem behaves , in some situations , like the rem , and the rem is analogous to an ordinary block code without any hierarchical structure ( cf .",
    "[ remc ] ) , may hint that in the parallel situations in the realm of our coding problem , a typical code from the hierarchical ensemble will perform essentially as well as a typical ( good ) code without the hierarchical structure . in these situations then ( which can be imposed by a clever choice of certain design parameters ) , it would be interesting to explore the question whether we may enjoy the benefit that the hierarchical structure buys us ( in our case , reduced delay ) without essentially paying in terms of performance . as we show in this section",
    ", the answer to this question turns out to be affirmative to a large extent , both in the source coding setting and in the channel coding setting .",
    "finally , in closing this introductory part of section [ main ] , a more technical comment is in order : as in subsection [ remc ] , throughout the sequel , we confine ourselves to the memoryless binary symmetric source ( bss ) with the hamming distortion measure , in the context of source coding , and to the binary symmetric channel ( bsc ) in the context of channel coding",
    ". the random coding distribution in both problems will be i.i.d .  and",
    "uniform , i.e. , each bit of each codeword will be drawn by independent fair coin tossing .",
    "also , we will focus mostly on the case @xmath1 .",
    "the reason for this is that our purpose is this paper is more to demonstrate certain concepts , and so , we prefer to slightly sacrifice generality at the benefit of simplicity , and so , better readability , and a smaller amount of space .",
    "having said that , all the derivations can be extended to apply to more general memoryless sources , channels , and random coding distributions ( as was done in @xcite ) , as well as to a general number @xmath0 of stages .",
    "consider the bss @xmath235 , @xmath236 ( @xmath71  positive integer ) and the hamming distortion measure between two binary @xmath9vectors @xmath54 and @xmath237 : @xmath238 where @xmath239 if @xmath240 and @xmath241 if @xmath242 , @xmath243 . before discussing ensembles of codes with hierarchical structures , let us first confine attention to an ordinary ensemble with no structure .",
    "consider a random selection of a codebook of size @xmath109 ( @xmath15 being the coding rate in nats per source bit ) , @xmath244 , @xmath245 , @xmath246 , where each component of each codeword is drawn randomly by an independent fair coin tossing .",
    "for a given source vector @xmath54 and for a given such randomly drawn codebook @xmath108 , let @xmath247 denote the distortion associated with encoding @xmath54 .    instead of examining the expected distortion , @xmath248 , w.r.t .",
    "both the source and the random codebook selection , as is traditionally done , we will concern ourselves with a more refined and more informative objective function , which is the characteristic function of @xmath249 , namely , @xmath250\\},\\ ] ] or in particular , its exponential rate @xmath251 focusing on the range @xmath23 .",
    "as is well known , the characteristic function provides information not only on the expected distortion , @xmath248 , but also on every moment of @xmath249 ( by taking derivatives of @xmath252 at @xmath253 ) .",
    "it is also intimately related to the tail behavior ( i.e. , large deviations probabilities ) of the distribution of @xmath249 via chernoff bounds .    in order to analyze @xmath252 and then @xmath254 , first , for an ordinary ensemble , and later for an hierarchical structured ensemble , it is convenient to define , for given @xmath54 and @xmath108 , the partition function , the partition function @xmath255 induced by a typical codebook is exactly the same as in ( [ ze ] ) , with the minor modification that here @xmath57 is not scaled by @xmath75 as in ( [ ze ] ) . ]",
    "@xmath256 the function @xmath252 is obtained from the partition function by @xmath257 in the definition of the ensemble behavior of @xmath254 , there are now two options .",
    "the first is to think of the above defined expectation of @xmath258 as being taken w.r.t .  both the source @xmath32 and the code ensemble @xmath259 , and then to define @xmath254 as above .",
    "the second option is to define the above expectation of @xmath258 w.r.t .",
    "the source only , while keeping @xmath108 fixed , and then to define @xmath254 as @xmath260 , where the latter expectation is across the ensemble of codebooks @xmath259 . the difference between meanings of the two approaches is in the point of view : in the former approach the randomness of both @xmath32 and @xmath108 are treated on equal grounds , and this makes sense if @xmath32 and @xmath108 vary on the same time scale ( e.g. , when the codebook varies frequently according to some secret key ) . in the parallel discussion on spin glasses ( cf .",
    "section 3.1 ) , this is analogous to the double randomness of both the spin configuration and the interaction parameters , and in the language of statistical physicists , this is called _ annealed _ averaging .",
    "the second approach , which physicists refer to as _ quenched _ averaging , fits better the paradigm where the code @xmath108 is held fixed over many realizations of the source @xmath32 . in the information theory literature , it is more customary to adopt an approach analogous to annealed averaging and so , we shall do the same here .",
    "let us begin the with the calculation of the annealed version of @xmath254 , first , for a an ordinary non  hierarchical code : @xmath261^{1/\\theta}\\right\\}\\nonumber\\\\ & = & { \\mbox{\\boldmath $ e$}}\\left\\{\\left[\\sum_{d=0}^nn(d)\\cdot e^{-s\\theta d}\\right]^{1/\\theta}\\right\\}\\nonumber\\\\ & { \\stackrel{\\cdot } { = } } & { \\mbox{\\boldmath $ e$}}\\left\\{\\sum_{d=0}^nn^{1/\\theta}(d)\\cdot e^{-sd}\\right\\}\\nonumber\\\\ & = & \\sum_{d=0}^n{\\mbox{\\boldmath $ e$}}\\{n^{1/\\theta}(d)\\}\\cdot e^{-sd}\\end{aligned}\\ ] ] where @xmath262 is the number of codewords whose normalized hamming distance from @xmath32 is exactly @xmath135 , and where the third ( exponential ) equality holds , even before taking the expectation , because the summation over @xmath263 consists of a _ subexponential _ number of terms , and so , both @xmath264^{1/\\theta}$ ] and @xmath265 are of the same exponential order as @xmath266^{1/\\theta}$ ] .",
    "this is different from the original summation over @xmath108 which contains an _ exponential _ number of terms .",
    "now , as is shown in subsection a.1 of the appendix ( see also @xcite ) , @xmath267 } & \\delta < \\delta(r)~~\\mbox{or}~~ \\delta > 1-\\delta(r)\\\\ e^{n[r+h(\\delta)-\\ln 2]/\\theta } & \\delta(r)\\le\\delta\\le 1-\\delta(r )   \\end{array}\\right.\\ ] ] where @xmath140 is defined ( cf .",
    "subsection [ remc ] ) as the small solution to the equation @xmath141 , which is also the distortion ",
    "rate function of the bss .",
    "this gives @xmath268}\\cdot e^{-s\\delta n}+ \\sum_{\\delta\\ge\\delta(r ) } e^{n[r+h(\\delta)-\\ln 2]/\\theta}\\cdot e^{-s\\delta n}\\nonumber\\\\ & { \\stackrel{\\delta } { = } } & a+b\\end{aligned}\\ ] ] now , as @xmath269 , the term @xmath75 tends to @xmath270 , which is of the exponential order of @xmath271 .",
    "the term @xmath272 , which is independent of @xmath273 , is of the exponential order of @xmath274 , where @xmath275= \\left\\{\\begin{array}{ll } s\\delta(r ) & s\\le s_r\\\\ v(s , r ) & s > s_r \\end{array}\\right.\\ ] ] where @xmath276.\\ ] ] and @xmath277 since @xmath278 never exceeds @xmath279 for @xmath280 , the dominant term is @xmath272 , and therefore , for the ordinary block code ensemble , we have : @xmath281 it is not difficult to show also , using sphere covering considerations , that @xmath282 is the best achievable performance in terms of the exponential rate of the characteristic function of the distortion .",
    "the function @xmath282 is depicted qualitatively in fig .",
    "[ gen ] .",
    "we proceed to define the ensemble of hierarchical codes and to analyze its performance with relation to the grem .",
    "let @xmath2 , where @xmath9 , @xmath3 and @xmath6 are positive integers . for a given @xmath4 ,",
    "consider a random selection of a codebook of size @xmath283 , @xmath284 , @xmath285 , @xmath286 , where each component of each codeword is drawn randomly by an independent fair coin tossing .",
    "next , given @xmath7 , for each @xmath286 , consider a similar random selection of a codebook of size @xmath287 , @xmath288 , @xmath289 , @xmath290 .",
    "the encoder works as follows : given a source vector @xmath291 , it finds a pair of indices @xmath292 , @xmath286 , @xmath290 , such that the distortion between @xmath54 and the concatenation of the codewords @xmath293 is minimum .",
    "the index @xmath71 is encoded by @xmath5 nats and the index @xmath82 ( given @xmath71 ) is encoded by @xmath294 nats , thus a total of @xmath295 nats , where @xmath15 is the overall rate , given by @xmath296 the decoder can , of course , generate the first  stage reproduction @xmath297 based on the first @xmath5 nats received , without having to wait for the @xmath294 following ones .",
    "the extension of this hierarchical structure to a larger number of stages @xmath0 should be obvious .",
    "in particular , as mentioned in the introduction , if @xmath0 divides @xmath9 and the @xmath9block is divided to @xmath0 sub  blocks of length @xmath11 each , then the decoder can generate chunks of the reproduction at a reduced delay of @xmath11 instead of @xmath9",
    ".    the analogy of this structure with the grem should also be obvious .",
    "the code has a tree structure and the configurational energies of the grem play the same role as the distortion here , as the overall distortion is the cumulative sum of the per  stage distortions .",
    "also , the coding rate @xmath13 here plays the same role as @xmath298 of the grem ( @xmath299 ) .",
    "thus , it is natural to expect that the partition function @xmath255 of this code ensemble would behave analogously to that of the grem , as we shall see next .    for the sake of simplicity ,",
    "we return to the case @xmath1 , with the understanding that our derivations can be extended without any essential difficulties to a general @xmath0 . before analyzing the characteristic function of the distortion along with its exponential rate , it is instructive to examine the partition function @xmath255 for a given @xmath54 and address the analogy with that of the grem .    for a given @xmath54 and a typical code in the ensemble ,",
    "there are @xmath300}$ ] first - stage codewords @xmath301 at distance @xmath302 from the vector formed by the first @xmath3 components of @xmath54 , provided that @xmath303 and @xmath304 otherwise .",
    "for each one of these first  stage codewords , there are @xmath305}$ ] second  stage codewords @xmath306 at distance @xmath307 from the vector formed by the last @xmath6 components of @xmath54 , provided that @xmath308 .",
    "thus , the total number of concatenated codewords @xmath309 at distance @xmath310 ( that is , @xmath311 ) from @xmath54 is given by @xmath312}\\cdot e^{n_2[r_2+h((\\delta-\\lambda\\delta_1)/(1-\\lambda))-\\ln 2]}\\nonumber\\\\ & { \\stackrel{\\cdot } { = } } & \\exp\\left\\{n\\max_{\\delta(r_1)\\le\\delta_1\\le 1-\\delta(r_1)}\\left[r+\\lambda h(\\delta_1 ) + ( 1-\\lambda)h\\left(\\frac{\\delta-\\lambda\\delta_1}{1-\\lambda}\\right)-\\ln 2\\right]\\right\\}.\\end{aligned}\\ ] ] consequently , the exponential growth rate of @xmath313 is given by @xmath314.\\ ] ] for large @xmath135 , the constraint @xmath315 is inactive and the achiever of @xmath316 is @xmath317 , and then @xmath318 if we now gradually reduce @xmath135 , the behavior depends on whether we first encounter the value @xmath319 , below which @xmath317 no longer satisfies the constraint , or the the value @xmath320 , below which @xmath321 vanishes .",
    "this in turn depends on whether @xmath322 is larger or smaller than @xmath140 , or equivalently , if @xmath323 or @xmath324 .",
    "consider the case @xmath324 first . in this case ,",
    "@xmath325 , and we have : @xmath326 exactly like in the ordinary , non  hierarchical ensemble ( cf .",
    "( [ srem ] ) ) , and then the corresponding exponential rate of the partition function is as in subsection [ remc ] , except that here @xmath57 is not scaled by @xmath75 , i.e. , @xmath327 .",
    "the other case is @xmath323 , which is equivalent to @xmath328 . here , in analogy to the grem with two phase transitions",
    ", we have : @xmath329 & \\beta > \\beta(r_2 ) \\end{array}\\right.\\ ] ] we now identify the first line as the purely paramagnetic phase , the second line  as the `` semi  glassy '' phase ( where @xmath301 are glassy but @xmath306 are paramagnetic ) , and the third line  as the purely glassy phase .",
    "note that the glassy phase here behaves as if the two parts of the code , at rates @xmath4 and @xmath7 , were operating independently , namely , as if @xmath330 were all identical , in which case , the distortion would have been minimized separately over the two segments .",
    "we will get back to this point in the sequel .",
    "we have seen then that the ensemble behaves substantially differently depending on whether @xmath27 or @xmath20 . in the former case",
    ", the above calculation may indicate that the ensemble performance is similar to that of an ordinary block code of length @xmath9 without any structure .",
    "we next carry out a detailed analysis of the characteristic function and its exponential rate , which we shall denote by @xmath331 .    similarly as before",
    ", we first compute @xmath332 : @xmath333^{1/\\theta}\\right\\}\\nonumber\\\\ & { \\stackrel{\\cdot } { = } } & \\sum_{d_1=0}^{n_1}\\sum_{d_2=0}^{n_2}{\\mbox{\\boldmath $ e$}}\\{n^{1/\\theta}(d_1,d_2)\\}\\cdot e^{-s(d_1+d_2)},\\end{aligned}\\ ] ] where @xmath334 is the number concatenated codewords @xmath309 for which the first stage contributes distance @xmath335 and the second stage contributes distance @xmath336 . for the moments @xmath337 , or equivalently , @xmath338 ,",
    "the following is proven in section a.2 of the appendix : @xmath339\\ } &   \\delta_1 \\in{{\\cal i}}^c(r_1),~\\delta_2\\in{{\\cal i}}^c(r_2)\\\\ \\exp\\{n[\\lambda w_1 + ( 1-\\lambda)w_2/\\theta]\\ } & \\delta_1\\in{{\\cal i}}^c(r_1),~\\delta_2\\in{{\\cal i}}(r_2)\\\\ \\exp\\{n[\\lambda w_1+(1-\\lambda)w_2]/\\theta\\ } & \\delta_1 \\in{{\\cal i}}(r_1),~\\delta_2\\in{{\\cal i}}(r_2)\\\\ \\exp\\{n\\eta[\\lambda w_1+(1-\\lambda)w_2]\\ } &   \\delta_1 \\in{{\\cal i}}(r_1),~\\delta_2\\in{{\\cal i}}^c(r_2 ) \\end{array}\\right.\\ ] ] where @xmath340 , @xmath341\\setminus{{\\cal i}}(r)$ ] , @xmath342 , @xmath299 , with @xmath343 being defined as @xmath344 and @xmath345    therefore , @xmath346}\\times\\nonumber\\\\ & & e^{-sn[\\lambda\\delta_1+(1-\\lambda)\\delta_2]}+\\nonumber\\\\ & & \\sum_{\\delta_1\\in{{\\cal i}}^c(r_1)}\\sum_{\\delta_2\\in{{\\cal i}}(r_2 ) } e^{n[\\lambda(r_1+h(\\delta_1)-\\ln 2)+(1-\\lambda)(r_2+h(\\delta_2)-\\ln 2)/\\theta]}\\times\\nonumber\\\\ & & e^{-sn[\\lambda\\delta_1+(1-\\lambda)\\delta_2]}+\\nonumber\\\\ & & \\sum_{\\delta_1\\in{{\\cal i}}(r_1)}\\sum_{\\delta_2\\in{{\\cal i}}(r_2 ) } e^{n[\\lambda(r_1+h(\\delta_1)-\\ln 2)+(1-\\lambda)(r_2+h(\\delta_2)-\\ln 2)]/\\theta}\\times\\nonumber\\\\ & & e^{-sn[\\lambda\\delta_1+(1-\\lambda)\\delta_2]}+\\nonumber\\\\ & & \\sum_{\\delta_1\\in{{\\cal i}}(r_1)}\\sum_{\\delta_2\\in{{\\cal i}}^c(r_2 ) } e^{n\\eta[\\lambda(r_1+h(\\delta_1)-\\ln 2)+(1-\\lambda)(r_2+h(\\delta_2)-\\ln 2)]}\\times\\nonumber\\\\ & & e^{-sn[\\lambda\\delta_1+(1-\\lambda)\\delta_2]}\\nonumber\\\\ & { \\stackrel{\\delta } { = } } & a+b+c+d\\end{aligned}\\ ] ] let us now handle each one of these four terms and take the limit @xmath269 .",
    "this results in : @xmath347}\\right]\\cdot \\left[\\sum_{\\delta_2\\in{{\\cal i}}^c(r_2)}e^{n_2[r_2+h(\\delta_2)-\\ln 2-s\\delta_2]}\\right]\\nonumber\\\\ & { \\stackrel{\\cdot } { = } } & e^{-n_1u(s , r_1)}\\cdot e^{-n_2u(s , r_2)}\\nonumber\\\\ & = & e^{-n[\\lambda u(s , r_1)+(1-\\lambda)u(s , r_2)]},\\end{aligned}\\ ] ] @xmath348}\\right]\\cdot \\left[\\sum_{\\delta_2\\in{{\\cal i}}(r_2)}e^{-n_2s\\delta_2}\\right]\\nonumber\\\\ & { \\stackrel{\\cdot } { = } } & e^{-n_1u(s , r_1)}\\cdot e^{-n_2\\delta(r_2)}\\nonumber\\\\ & = & e^{-n[\\lambda u(s , r_1)+(1-\\lambda)\\delta(r_2)]},\\end{aligned}\\ ] ] @xmath349},\\ ] ] and @xmath350 where @xmath351- \\mu(\\delta_1,\\delta_2)[r+\\lambda h(\\delta_1)+(1-\\lambda)h(\\delta_2)-\\ln 2]\\}\\ ] ] and where @xmath352 among the terms @xmath272 , @xmath75 , and @xmath353 , the term @xmath272 is exponentially the dominant one . to check whether or not @xmath272 dominates also @xmath354",
    ", we will have to investigate the function @xmath355 .",
    "this is done in subsection a.3 of the appendix , where it is shown that this function is as follows : for @xmath16 : @xmath356 and for @xmath20 : @xmath357 & 0\\le s\\le s_{r_2}\\\\ \\lambda s\\delta(r_1)+(1-\\lambda)v(s , r_2 ) & s > s_{r_2}\\end{array}\\right.\\ ] ] finally , the overall exponential rate of the characteristic function , @xmath358 , we have to take into account the contribution of @xmath272 , as mentioned above .",
    "this gives : @xmath359 where @xmath360 .",
    "now , in the case @xmath361 , for small @xmath21 , the function @xmath362 is linear with slope @xmath140 , whereas the function @xmath363 is linear with a slope of @xmath364 which is larger .",
    "thus , @xmath362 is smaller in some interval of small @xmath21 .",
    "however , for larger @xmath21 , @xmath362 continues to have a linear term with slope @xmath365 whereas @xmath363 never exceeds the level of @xmath366 .",
    "thus , there must be a ( unique ) point of intersection @xmath367 .",
    "consequently , for @xmath361 , we have @xmath368 where @xmath355 is as in ( [ fr1gr2 ] ) . concerning the case @xmath20 , both @xmath362 ( of eq .",
    "( [ fr1lr2 ] ) ) and @xmath363 start as linear functions of the same slope of @xmath364 .",
    "however , while the latter begins its curvy part at @xmath369 , the former continues to be linear until the point @xmath370 . in this case , then it is easy to see that @xmath331 is dominated by @xmath363 across the entire range @xmath23 , i.e. , @xmath371    we see then that the ensemble performance is substantially different in the two cases : for @xmath20 , @xmath331 is exactly the same as if we used two _ independent _ block codes of lengths @xmath3 and @xmath6 at rates @xmath4 and @xmath7 , respectively . in particular , the corresponding average distortion is @xmath364 which is , of course , larger than @xmath140 . in other words , we are gaining nothing from the tree structure and the dependence between the two parts of the code . for @xmath16 , on the other hand , there is at least a considerable range of small @xmath21 for which @xmath372 , namely , the ensemble performance is exactly like that of the ordinary ensemble of full block code of length @xmath9 and rate @xmath15 , without any structure ( which is also the best achievable exponential rate ) .",
    "however , beyond a certain value of @xmath21 , there is some loss in comparison to the ordinary ensemble .",
    "the case @xmath22 can be obtained as the limiting behavior of both @xmath373 and @xmath361 , by taking both rates to be arbitrarily close to each other . in this case",
    ", we obtain @xmath372 throughout the _ entire _ range @xmath23 ( cf .  the discussion on this in the introduction ) .",
    "the conclusion then is that if we use an hierarchical structure of the kind we consider in this paper , it is best to assign equal rates at the two stages , but then we might as well abandon the tree structure of the code altogether , and just encode the two parts independently , both at rate @xmath15 ( this will moreover save complexity at the encoder ) . if , however , certain considerations dictate different rates at different segments , then it is better to encode at a larger rate in the first segment and at a smaller rate in the second .",
    "this derivation can be extended , in principle , to any finite number @xmath0 of stages .",
    "the analysis is , of course , more complicated but conceptually , the ideas are the same",
    ". we will not carry out this extension in this paper .      in complete duality to the source coding problem",
    ", one may consider a channel code ( for the bsc ) with a similar hierarchical structure : given a binary information vector of length @xmath374 nats , we encode it in two parts : the first segment , of length @xmath5 nats , is encoded to a binary channel input vector of length @xmath3 , independently of the forthcoming @xmath294 nats ( thus , the channel encoder is of reduced delay ) .",
    "then , the remaining @xmath294 nats are mapped to another binary channel input vector of length @xmath6 and it depends on the entire information vector of length @xmath375 .    the ensemble of codebooks is drawn similarly as before : first , a randomly drawn first  stage codebook of size @xmath376 , and then , for each one of its codewords , another codebook of size @xmath377 is drawn independently . once again , each bit of each codeword is drawn by independent fair coin tossing .    the decoder applies maximum likelihood ( ml ) decoding based on the entire channel output vector @xmath112 of length @xmath2 , pertaining to the input @xmath54 of length @xmath9 .",
    "the analogy with the grem is that here , the energy function is the log ",
    "likelihood , which is additive over the two stages by the memorylessness of the channel .",
    "in full analogy to the grem and the source coding problem of subsection [ lossysourcecoding ] , and as an extension to the derivation in subsection [ remc ] , here too , the partition function @xmath130 has exactly the same two different types of behavior , depending on whether @xmath27 or @xmath28 .",
    "therefore , we will not repeat this here .    concerning the aspect of performance evaluation of this ensemble of codes , and a comparison to the ordinary ensemble , here the natural figure of merit is gallager s random coding error exponent , which can be analyzed using methods similar to those that we used in subsection [ lossysourcecoding ] .",
    "we will not carry out a very refined analysis as we did before , but we will make a few observations in this context , although not quite directly related to the grem .      in the hierarchical case with @xmath1 stages , the probability of error consists of two contributions .",
    "the first pertains to all incorrect codewords @xmath385 whose first segment @xmath153 agrees with that of the correct codeword , and the second one is associated with all other incorrect codewords . as for the former type of codewords ,",
    "the ml decoder actually compares the likelihood scores of the second segment only ( as those of the first segment are the same and hence cancel out ) , and so , these incorrect codewords contribute a term of the order of @xmath386 to the average error probability , where @xmath387 is the gallager s random coding error exponent function ( * ? ? ?",
    "* , eq .  ( 5.6.16 ) ) . concerning the second set of incorrect codewords",
    ", we can apply an upper bound as above , except that the expectations have to be taken w.r.t .  the hierarchical ensemble .",
    "however , it is easy to see that the expectation of @xmath388 is exactly the same as in the ordinary ensemble , and thus , so is the upper bound for this set of codewords , which is then @xmath389 .",
    "the total average error probability is then upper bounded by @xmath390 this gives further motivation why @xmath7 should be chosen smaller than @xmath4 : if @xmath391 , the second term definitely dominates the exponent , because both @xmath392 and @xmath393 and so @xmath394 . for a given @xmath15 and @xmath395 , can we , and if so how , assign the segmental rates @xmath4 and @xmath7 such that the second term would not be dominant , i.e. , @xmath396 ? if @xmath15 is large enough this is possible .",
    "for example , one way to do this is to select @xmath397 , where @xmath353 is the channel capacity . in this case , we have , by the convexity of @xmath398 : @xmath399 for this strategy to be applicable , @xmath15 must be at least as large as @xmath400 .",
    "how does this discussion extend to a general number of stages @xmath0 and is there a more systematic approach to allocate the segmental rates @xmath401 for a given overall rate @xmath15 ?",
    "for simplicity , let us suppose that the segment lengths are all the same , i.e. , @xmath402 .",
    "the extension turns out to be quite straightforward : in the case of @xmath0 stages there are @xmath0 types of incorrect codewords : those that agree with the correct codeword in all stages except the last stage , those that agree in all stages except the last two stages , etc . accordingly , using the same considerations as above , it is easy to see then that the upper bound on the average error probability consists of @xmath0 contributions whose exponents are @xmath403 for convenience , let us denote @xmath404 under what conditions and how can we assign the segmental rates such that @xmath405 for all @xmath406 ? first , we must select @xmath407 sufficiently small such that @xmath408 .",
    "as @xmath15 is given , this will dictate the choice of @xmath4 according to the identity @xmath409 next , we choose @xmath410 small enough such that @xmath411 as @xmath407 has already been chosen , this will dictate the choice of @xmath7 according to the identity @xmath412 and so on .",
    "this procedure continues until in the last step we choose @xmath413 such that @xmath414 , which dictates the choice of @xmath415 via @xmath416 , where @xmath417 was selected in preceeding step .",
    "an obvious condition for this procedure to be applicable is that @xmath15 would be large enough such that @xmath418 .",
    "note that if some of the segmental rates exceed capacity ( or even the log alphabet size ) , this is not a problem , as long as the averages @xmath419 are all small enough .",
    "we begin with a simple large deviations bound regarding the distance enumerator , which appears also in @xcite , but we present here too for the sake of completeness . for @xmath420 $ ] ,",
    "consider the binary divergence @xmath421.\\end{aligned}\\ ] ] to derive a lower bound to @xmath422 , let us use the inequality @xmath423 and then @xmath424 for every given @xmath112 , @xmath425 is the sum of the @xmath426 independent binary random variables , @xmath427 , where the probability that @xmath428 is exponentially @xmath429}$ ] .",
    "the event @xmath430 , for @xmath431 , means that the relative frequency of the event @xmath432 is at least @xmath433 .",
    "thus , by the chernoff bound : @xmath434})\\right\\}\\nonumber\\\\ & { \\stackrel{\\cdot } { \\le } } & \\exp\\left\\{-e^{nr}\\cdot e^{-n(r - a)}(n[(\\ln 2-r- h(\\delta)+a]-1)\\right\\}\\nonumber\\\\ & \\le & \\exp\\left\\{-e^{na}(n[\\ln 2-r - h(\\delta)+a]-1)\\right\\}.\\end{aligned}\\ ] ] denoting by @xmath435 the interval @xmath436 and by @xmath437 , the complementary range @xmath438\\setminus{{\\cal i}}(r)$ ] , we have , for @xmath439 : @xmath440}+ e^{nrs}\\cdot e^{-(n\\epsilon-1)e^{n\\epsilon}}.\\end{aligned}\\ ] ] one can let @xmath180 vanish with @xmath9 sufficiently slowly that the second term is still superexponentially small , e.g. , @xmath441 .",
    "thus , for @xmath439 , @xmath442 is exponentially bounded by @xmath443}$ ] independently of @xmath21 . for @xmath444 , we have : @xmath445}\\cdot \\mbox{pr}\\{n(n\\delta)\\le e^{n[r+h(\\delta)-\\ln 2+\\epsilon]}\\}+\\nonumber\\\\ & & e^{nrs}\\cdot \\mbox{pr}\\{n(n\\delta)\\ge e^{n[r+h(\\delta)-\\ln 2+\\epsilon]}\\}\\nonumber\\\\ & \\le&e^{ns[r+h(\\delta)-\\ln 2+\\epsilon ] } + e^{nrs}\\cdot e^{-(n\\epsilon-1)e^{n\\epsilon}}\\end{aligned}\\ ] ] where again , the second term is exponentially negligible .    to see that both bounds are exponentially tight ,",
    "consider the following lower bounds .",
    "for @xmath439 , @xmath446^{e^{nr}-1}\\nonumber\\\\ & { \\stackrel{\\cdot } { = } } & e^{nr}e^{-n[\\ln 2-h(\\delta)]}\\cdot \\left[1-e^{-n[\\ln 2-h(\\delta)]}\\right]^{e^{nr}}\\nonumber\\\\ & = & e^{n[r+h(\\delta)-\\ln 2]}\\cdot\\exp\\{e^{nr}\\ln[1-e^{-n[\\ln 2-h(\\delta)]}]\\}.\\end{aligned}\\ ] ] using again the inequality in ( [ lnineq ] ) , the second factor is lower bounded by @xmath447}/(1-e^{-n[\\ln 2-h(\\delta)]})\\ } = \\exp\\{-e^{-n[\\ln 2-r - h(\\delta)]}/(1-e^{-n[\\ln 2-h(\\delta)]})\\}\\ ] ] which clearly tends to unity as @xmath448 for @xmath439 .",
    "thus , @xmath442 is exponentially lower bounded by @xmath443}$ ] . for @xmath444 , and an arbitrarily small @xmath449",
    ", we have : @xmath450}\\cdot \\mbox{pr}\\{n(n\\delta)\\ge e^{n[r+h(\\delta)-\\ln 2-\\epsilon]}\\}\\nonumber\\\\ & = & e^{ns[r+h(\\delta)-\\ln 2-\\epsilon]}\\cdot\\left(1- \\mbox{pr}\\{n(n\\delta ) < e^{n[r+h(\\delta)-\\ln 2-\\epsilon]}\\}\\right)\\end{aligned}\\ ] ] where @xmath451}\\}$ ] is again upper bounded , for an internal point in @xmath435 , by a double exponentially small quantity as above . for @xmath135 near the boundary of @xmath435 , namely , when @xmath452 , we can lower bound @xmath442 by slightly reducing @xmath15 to @xmath453 ( where @xmath449 is very small ) .",
    "this will make @xmath135 an internal point of @xmath454 for which the previous bound applies , and this bound is of the exponential order of @xmath455}$ ] . since @xmath456 is still very close to zero",
    ", then @xmath455}$ ] is of the same exponential order as @xmath457}$ ] since both are about @xmath458 .",
    "it should be noted that a similar double  exponential bound can be obtained for the probability of the event @xmath459 , where @xmath460 and @xmath461 .",
    "here we can proceed as above except that the in the lower bound on divergence @xmath422 we should take the second line of ( a.3 ) ( rather than the third ) , which is of the exponential order of @xmath462}$ ] ( observe that here @xmath463 is exponentially larger than @xmath363 , as opposed to the earlier case ) .",
    "thus , we obtain @xmath464 at the second level exponent , and so the decay is double exponential as before .",
    "first , let us write @xmath465 as follows : @xmath466 where @xmath153 and @xmath467 designate @xmath468 and @xmath469 , respectively , and where @xmath470 denotes the indicator function of an event .",
    "we now treat each one of the four cases pertaining to the combinations of both @xmath471 and @xmath472 being or not being members of @xmath473 and @xmath474 , respectively .",
    "+      for a given , arbitrarily small @xmath449 , consider the event @xmath477 .",
    "if both the number of indices @xmath71 for which @xmath478 is less than @xmath479 and for each @xmath71 , @xmath480 , then clearly , the event @xmath481 does not occur .",
    "thus , for @xmath481 to occur , at least one of these events must occur .",
    "in other words , either the number of indices @xmath71 for which @xmath478 is larger than @xmath479 or there exist @xmath71 for which @xmath482 .",
    "the probability of the former event is upper bounded by @xmath483 ( cf .",
    "subsection a.1 ) .",
    "similarly , the probability of the latter , for a given @xmath71 , is bounded by @xmath484 .",
    "thus , the probability of the union of events @xmath485 is upper bounded by @xmath486 , which is still double exponential in @xmath9 .",
    "thus , @xmath487 therefore , @xmath488 which is exponentially upper bounded by @xmath489}$ ] since @xmath180 is arbitrarily small , @xmath490}$ ] , and the last term is double  exponential . to obtain the compatible lower bound , we use @xmath491 now , the event @xmath492 is the event that there is exactly one value of @xmath71 such that @xmath493 , and that for this @xmath71 , there is exactly one @xmath82 such that @xmath494 . as shown in subsection a.1 , the probability of the former is exponentially @xmath495}$ ] and the probability of the latter is exponentially @xmath305}$ ] .",
    "thus , by independence , @xmath496 is the product , which is exponentially @xmath489}$ ] .",
    "define now the event @xmath44 as @xmath498\\}\\right\\}.\\ ] ] as we have argued before , the probability of @xmath44 is doubly exponentially close to unity ( since the probability of @xmath499 is upper bounded by the sum of exponentially many doubly - exponentially small probabilities ) .",
    "now , clearly , if @xmath44 occurs , @xmath500\\}\\cdot\\sum_{i=1}^{m_1 } 1\\{d_h({\\mbox{\\boldmath $ x$}}',{\\hat{\\mbox{\\boldmath $ x$}}}_i)=n_1\\delta_1\\}.\\ ] ] thus , @xmath501\\}\\times\\right.\\right.\\nonumber\\\\ & & \\left.\\left.\\sum_{i=1}^{m_1 } 1\\{d_h({\\mbox{\\boldmath $ x$}}',{\\hat{\\mbox{\\boldmath $ x$}}}_i)=n_1\\delta_1\\}\\right]^{1/\\theta}\\right\\}\\nonumber\\\\ & & + e^{nr/\\theta}\\cdot\\mbox{pr}\\{{{\\cal a}}^c\\},\\end{aligned}\\ ] ] where the second term is again doubly  exponentially small .",
    "as for the first term , we bound @xmath502 by unity and @xmath503\\}\\sum_{i=1}^{m_1 } 1\\{d_h({\\mbox{\\boldmath $ x$}}',{\\hat{\\mbox{\\boldmath $ x$}}}_i)=n_1\\delta_1\\}\\right]^{1/\\theta}\\right\\}\\nonumber\\\\ & = & \\exp\\{n_2[r_2+h(\\delta_2)-\\ln 2+\\epsilon]/\\theta\\}\\cdot{\\mbox{\\boldmath $ e$}}\\left\\{\\left[\\sum_{i=1}^{m_1 } 1\\{d_h({\\mbox{\\boldmath $ x$}}',{\\hat{\\mbox{\\boldmath $ x$}}}_i)=n_1\\delta_1\\}\\right]^{1/\\theta}\\right\\}\\end{aligned}\\ ] ] where the latter expectation ( cf .",
    "subsection a.1 ) is of the exponential order of @xmath495}$ ] if @xmath475 ( case 2 ) and @xmath495/\\theta}$ ] if @xmath504 ( case 3 ) .",
    "thus , in both cases , we obtain the desired exponential order as an upper bound . for the lower bound",
    ", we argue similarly that the probability of the event @xmath505\\}\\right\\}\\ ] ] is doubly  exponentially close to unity , and so , @xmath506\\}\\sum_{i=1}^{m_1 } 1\\{d_h({\\mbox{\\boldmath $ x$}}',{\\hat{\\mbox{\\boldmath $ x$}}}_i)=n_1\\delta_1\\}\\right]^{1/\\theta}\\right\\},\\ ] ] and we again use the above result on the moments of @xmath507 in both cases of @xmath471 .",
    "since @xmath504 , then the event @xmath508}\\le \\sum_{i=1}^{m_1}1\\{d_h({\\mbox{\\boldmath $ x$}}',{\\hat{\\mbox{\\boldmath $ x$}}}_i)=n_1\\delta_1\\ } \\le e^{n_1[r_1+h(\\delta_1)-\\ln 2+\\epsilon]}\\right\\},\\ ] ] has a probability which is doubly  exponentially close to unity .",
    "thus , given that @xmath44 occurs , there are @xmath509}\\le l\\le   e^{n_1[r_1+h(\\delta_1)-\\ln 2+\\epsilon]}\\ ] ] indices @xmath510 for which @xmath478 . given @xmath511 and given these indices , @xmath465 is the sum of @xmath512+n_2r_2}$ ] i.i.d .",
    "bernoulli trials , @xmath513 , whose probability of success is exponentially @xmath514}$ ] . thus , similarly as in the derivation in subsection a.1 , @xmath515 or , equivalently , in the notation of eq .",
    "( [ nd1d2 ] ) : @xmath516\\ } & \\lambda w_1+(1-\\lambda)w_2 < 0\\\\ \\exp\\{n[\\lambda w_1+(1-\\lambda)w_2]/\\theta\\ } & \\lambda w_1+(1-\\lambda)w_2 \\ge 0\\end{array}\\right.\\ ] ] the total expectation should , of course , account for @xmath499 as well , but since the probability of this event is doubly exponentially small , then the contribution of this term is negligible .",
    "first , we observe that the constraints @xmath504 and @xmath476 can be replaced by their one - sided versions @xmath517 and @xmath518 , respectively , since values of @xmath471 and @xmath472 beyond @xmath519 can not be better than their corresponding reflections @xmath520 and @xmath521 .    next observe that @xmath355 can be rewritten as follows : @xmath522 where @xmath523\\ ] ] subject to the constraints @xmath517 , @xmath518 , and @xmath524 , and @xmath525 + ( 1-\\lambda)[s\\delta_2-r_2-h(\\delta_2)+\\ln 2]\\}\\ ] ] subject to the constraints @xmath517 , @xmath518 , and @xmath526 .",
    "note that the optimization problem associated with @xmath527 is a convex problem , but the one pertaining to @xmath528 is not , because of its last constraint which is not convex .        when @xmath361 , we have @xmath532 . as for @xmath533 ,",
    "it is easy to see that @xmath534 is a solution that satisfies the necessary and sufficient kuhn  tucker conditions for optimality of a convex problem , and so , @xmath535 .",
    "consider next the function @xmath528 .",
    "let us ignore , for a moment , the non  convex constraint @xmath536 , and refer only to the constraints @xmath303 and @xmath518 .",
    "denote by @xmath537 the corresponding maximum without the non ",
    "convex constraint .",
    "the maximization problem associated with @xmath538 is now convex and it is to see that @xmath539 and @xmath540 satisfy the necessary and sufficient conditions for optimality , where @xmath541 .",
    "this is also a solution for @xmath542 if it satisfies the non  convex constraint , namely , if @xmath543 whether or not this condition is satisfied depends on @xmath21 .",
    "since we are assuming @xmath361 , we then have @xmath544 , where we remind that @xmath545 . consequently , there are three different ranges of @xmath21 : @xmath546 , @xmath547 , and @xmath548 .",
    "when @xmath549 , this is equivalent to @xmath550 in which case the above necessary condition ( [ cond ] ) becomes @xmath551 to check whether this condition is satisfied , observe that @xmath552 , and so this is equivalent to the condition @xmath553 , which is @xmath554 , in agreement with the assumption on the range of @xmath21 . therefore , the above solution is acceptable for @xmath542 and by substituting it back into the objective function , we get : @xmath555 + ( 1-\\lambda)[s\\nu_s - r_2-h(\\nu_s)+\\ln 2]\\nonumber\\\\ & = & \\lambda s\\delta(r_1)+(1-\\lambda)v(s , r_2)\\end{aligned}\\ ] ] when @xmath556 , this is equivalent to @xmath557 , in which case the condition ( [ cond ] ) becomes @xmath558 , or equivalently , @xmath559 , which is @xmath280 .",
    "however , @xmath560 is between @xmath561 and @xmath562 , and so , the conclusion is that the non  convex constraint is satisfied only in upper part of the interval @xmath563 $ ] , i.e. , @xmath564 $ ] . in this range ,",
    "@xmath565 , and this yields @xmath566 . for @xmath567 , the condition ( [ cond ] ) no longer holds . in this case",
    ", the optimum solution should be sought on the boundary of the non  convex constraint , namely , under the equality constraint @xmath568 , but this coincides then with the solution to @xmath533 which was found on this boundary as well .",
    "thus , for @xmath569 $ ] , we have @xmath570 . summarizing our results for @xmath542 over the entire range of @xmath23",
    ", we have @xmath571 or , equivalently , @xmath572 finally , @xmath362 should be taken as the minimum between @xmath533 and @xmath542 .",
    "now , @xmath533 is linear and @xmath542 is concave ( as it is the minimum of a linear function in @xmath21 ) , coinciding with @xmath533 along @xmath573 $ ] .",
    "thus @xmath542 can not exceed @xmath533 for any @xmath21 , and so , @xmath574 .",
    "thus , @xmath575      in this case , @xmath576 . once again , @xmath533 is associated with a convex program whose conditions for optimality are easily seen to be satisfied by the solution @xmath577 and @xmath578 .",
    "thus , @xmath579.\\ ] ]    as for @xmath542 , let us examine again the various ranges of @xmath21 , where this time , @xmath580 . for @xmath581 , we have @xmath582 and then the condition ( [ cond ] ) is equivalent to @xmath583 , which is @xmath554 , in agreement with the assumption .",
    "this corresponds to @xmath577 and @xmath584 , which yields @xmath585 for @xmath586 , which means @xmath587 , condition ( [ cond ] ) is satisfied with equality , and the corresponding solution is @xmath577 and @xmath578 , which yields @xmath588.\\ ] ] for @xmath589 , eq .",
    "( [ cond ] ) is not satisfied , and we resort again to the boundary solution , which , as mentioned earlier , is the same as @xmath533 . summarizing our findings for the case @xmath20 , and applying similar concavity considerations as before ( telling us that @xmath574 ) , we have : @xmath590 & 0\\le s\\le s_{r_2}\\\\ \\lambda s\\delta(r_1)+(1-\\lambda)v(s , r_2 ) & s > s_{r_2}\\end{array}\\right.\\ ] ]                                            t.  c.  dorlas and j.  r.  wedagedera , `` phase diagram of the random energy model with higher order ferromagnetic term and error correcting codes due to sourlas , '' _ phys .",
    "_ , vol .",
    "83 , no .  21 , pp .",
    "44414444 , november 1999 .",
    "n.  merhav , `` relations between random coding exponents and the statistical physics of random codes , '' submitted to _",
    "ieee trans .  inform .",
    "theory _ , august 2007 .",
    "available on  line at : [ http://www.ee.technion.ac.il/people/merhav/papers/p117.pdf ] .",
    "n.  merhav , `` error exponents of erasure / list decoding revisited via moments of distance enumerators , '' submitted to _",
    "ieee trans .  inform .  theory _ ,",
    "november 2007 . also , available on  line at [ http://www.ee.technion.ac.il/people/merhav/papers/p119.pdf ] ."
  ],
  "abstract_text": [
    "<S> in an earlier work , the statistical physics associated with finite  temperature decoding of code ensembles , along with the relation to their random coding error exponents , were explored in a framework that is analogous to derrida s random energy model ( rem ) of spin glasses , according to which the energy levels of the various spin configurations are independent random variables . </S>",
    "<S> the generalized rem ( grem ) extends the rem in that it introduces correlations between energy levels in an hierarchical structure . in this paper </S>",
    "<S> , we explore some analogies between the behavior of the grem and that of code ensembles which have parallel hierarchical structures . </S>",
    "<S> in particular , in analogy to the fact that the grem may have different types of phase transition effects , depending on the parameters of the model , then the above  mentioned hierarchical code ensembles behave substantially differently in the various domains of the design parameters of these codes . </S>",
    "<S> we make an attempt to explore the insights that can be imported from the statistical mechanics of the grem and be harnessed to serve for code design considerations and guidelines . </S>",
    "<S> +   + * index terms : * spin glasses , grem , phase transitions , random coding , error exponents .    </S>",
    "<S> department of electrical engineering + technion - israel institute of technology + haifa 32000 , israel + </S>"
  ]
}