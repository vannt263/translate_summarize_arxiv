{
  "article_text": [
    "we are interested in finding the global minimum @xmath0 of an objective function @xmath1 over some bounded domain , typically @xmath2 , subject to the non - negativity of a series of constraint functions @xmath3 .",
    "this can be formalized as @xmath4 however , @xmath5 and @xmath3 are unknown and can only be evaluated pointwise via expensive queries to black - boxes that provide noise - corrupted evaluations of @xmath5 and @xmath6 .",
    "we assume that  @xmath5 and each of the constraints  @xmath7 are defined over the entire space  @xmath8 .",
    "we seek to find a solution to ( [ eq : problem ] ) with as few queries as possible .",
    "_ bayesian optimization _",
    "@xcite methods approach this type of problem by building a bayesian model of the unknown objective function and/or constraints , using this model to compute an _ acquisition function _ that represents how useful each input @xmath9 is thought to be as a next evaluation , and then maximizing this acquisition function to select a _ suggestion _ for function evaluation . in this",
    "we work we extend predictive entropy search ( pes ) @xcite to solve ( [ eq : problem ] ) , an approach that we call predictive entropy search with constraints ( pesc ) .",
    "pesc is an acquisition function that approximates the expected information gain about the value of the constrained minimizer  @xmath0 . as we will show below",
    ", pesc is effective in practice and can be applied to a much wider variety of constrained problems than existing methods .",
    "most previous approaches to bayesian optimization with unknown constraints are variants of expected improvement ( ei ) @xcite .",
    "ei measures the expected amount by which observing at @xmath9 leads to improvement over the current best value or _ incumbent _",
    "@xmath10 : @xmath11 where @xmath12 is the collected data .",
    "one way to use ei with constraints works by discounting ei by the posterior probability of a constraint violation .",
    "the resulting acquisition function , which we call expected improvement with constraints ( eic ) , is given by @xmath13 where @xmath14 is the set of objective function observations and @xmath15 is the set of observations for constraint @xmath16 .",
    "initially proposed by @xcite , eic has recently been independently developed in @xcite . in the constrained case , @xmath10 is the smallest value of the posterior mean of @xmath5 such that all the constraints are satisfied at the corresponding location .",
    "@xcite propose a combination of the expected improvement heuristic and the augmented lagrangian ( al ) optimization framework for constrained blackbox optimization .",
    "al methods are a class of algorithms for constrained nonlinear optimization that work by iteratively optimizing the unconstrained al :    @xmath17\\nonumber\\end{aligned}\\ ] ] where @xmath18 is a penalty parameter and @xmath19 is an approximate lagrange multiplier , both of which are updated at each iteration .",
    "the method proposed by @xcite uses bayesian optimization with ei to solve the unconstrained _ inner _ loop of the augmented lagrangian formulation .",
    "al is limited by requiring noiseless constraints so that @xmath20 and @xmath21 can be updated at each iteration . in section [ sec : toy_problem ] we show that pesc and eic perform better than al on the synthetic benchmark problem considered in @xcite , even when the al method has access to the true objective function and pesc and eic do not .",
    "@xcite propose an acquisition function based on the integrated expected conditional improvement ( ieci ) , which is given by @xmath22 h(\\mathbf{x}')d\\mathbf{x } ' \\ ; , \\label{eq : ieci}\\end{aligned}\\ ] ] where @xmath23 is the expected improvement at  @xmath24 and  @xmath25 is the expected improvement at @xmath24 when the objective has been evaluated at @xmath26 , but without knowing the value obtained .",
    "the ieci at @xmath26 is the expected reduction in improvement at @xmath24 under the density @xmath27 caused by observing the objective at that location , where @xmath27 is the probability of all the constraints being satisfied at @xmath24 .",
    "@xcite compare ieci with eic for optimizing the hyper - parameters of a topic model with constraints on the entropy of the per - topic word distribution and show that eic outperforms ieci for this problem .",
    "@xcite proposes to sequentially explore the location that yields that largest the expected volume reduction ( evr ) of the feasible region below the best feasible objective value @xmath10 found so far .",
    "this quantity is given by integrating the product of the probability of improvement and the probability of feasibility .",
    "that is , @xmath28 h(\\x')\\del\\x'\\ , , \\label{eq : ev}\\end{aligned}\\ ] ] where , as in ieci , @xmath29 is the probability that the constraints are satisfied at @xmath30 .",
    "this step - wise uncertainty reduction approach is similar to pesc in that both methods work by reducing a specific type of uncertainty measure ( entropy for pesc and expected volume for evr ) .",
    "ei - based methods for constrained optimization have several issues .",
    "first , when no point in the search space is feasible under the above definition , @xmath10 does not exist and the ei can not be computed .",
    "this issue affects eic , ieci , and evr . to address this issue , @xcite",
    "modify eic to ignore the factor  @xmath31 in ( [ eq : eicacquisition ] ) and only consider the posterior probability of the constraints being satisfied when @xmath10 is not defined .",
    "the resulting acquisition function focuses only on searching for a feasible location and ignores learning about the objective @xmath5 .",
    "furthermore , @xcite identify a pathology with eic when one is able to separately evaluate the objective or the constraints , i.e. , the _ decoupled _ case .",
    "the best solution  @xmath32 must satisfy a conjunction of low objective value _ and _ high ( non - negative ) constraint values . by only evaluating the objective or a single constraint ,",
    "this conjunction can not be satisfied by a single observation under a myopic search policy .",
    "thus , the new observed @xmath9 can not become the new incumbent as a result of a decoupled observation and the expected improvement is zero .",
    "therefore standard eic fails in the decoupled setting .",
    "@xcite circumvent this pathology by treating decoupling as a special case and using a two - stage acquisition function : first , @xmath9 is chosen with eic , and then , given @xmath9 , the task ( whether to evaluate the objective or one of the constraints ) is chosen with the method in @xcite .",
    "this approach does not take full advantage of the available information in the way a joint selection of @xmath9 and the task would . like eic , the methods al , ieci , and evr",
    "are also not easily extended to the decoupled setting .",
    "in addition to this difficulties , evr and ieci are limited by having to compute the integrals in ( [ eq : ieci ] ) and ( [ eq : ev ] ) over the entire domain , which is done numerically over a grid on @xmath30 @xcite .",
    "the resulting acquisition function must then be globally optimized , which also requires a grid on  @xmath9 .",
    "this nesting of grid operations limits the application of this method to small @xmath33",
    ".    our new method , pesc , does not suffer from these pathologies .",
    "first , the pesc acquisition function does not depend on the current best feasible solution , so it can operate coherently even when there is not yet a feasible solution .",
    "second , pesc naturally separates the contribution of each task ( objective or constraint ) in its acquisition function . as a result",
    ", no pathology arises in the decoupled case and , thus , no _ ad hoc _ modifications to the acquisition function are required .",
    "third , likewise evr and ieci , pesc also involves computing a difficult integral ( over the posterior on @xmath0 ) .",
    "however , this can be done efficiently using the sampling approach described in @xcite .",
    "furthermore , in addition to its increased generality , our experiments show that pesc performs favorably when compared to eic and al even in the basic setting of joint evaluations to which these methods are most suited .",
    "we seek to maximize information about the location  @xmath34 , the constrained global minimum , whose posterior distribution is  @xmath35 .",
    "we assume that  @xmath5 and @xmath36 follow independent gaussian process ( gp ) priors ( see , e.g. , * ? ? ?",
    "* ) and that observation noise is i.i.d .",
    "gaussian with zero mean .",
    "gps are widely - used probabilistic models for bayesian nonparametric regression which provide a flexible framework for working with unknown response surfaces .    in",
    "the coupled setting we will let @xmath37 denote all the observations up to step  @xmath38 , where  @xmath39 is a vector collecting the objective and constraint observations at step  @xmath40 .",
    "the next query  @xmath41 can then be defined as that which maximizes the expected reduction in the differential entropy  @xmath42 $ ] of the posterior on  @xmath43 .",
    "we can write the pesc acquisition function as@xmath44- \\mathbb{e}_{\\mathbf y}\\left\\{\\text{h}\\left[\\mathbf{x}_{\\star}|\\data\\cup(\\mathbf x , \\mathbf y)\\right]\\right\\}\\label{eq : originalacquisition}\\end{aligned}\\ ] ] where the expectation is taken with respect to the posterior distribution on the noisy evaluations of  @xmath5 and  @xmath45 at  @xmath26 , that is ,  @xmath46 .    the exact computation of the above expression is infeasible in practice .",
    "instead , we follow @xcite and take advantage of the symmetry of mutual information , rewriting this acquisition function as the mutual information between @xmath47 and @xmath34 given the collected data @xmath48 .",
    "that is , @xmath49- \\mathbb{e}_{\\mathbf x_\\star}\\left\\{\\text{h}\\left [ \\mathbf y| \\data , \\mathbf x , \\mathbf x_\\star \\right]\\right\\ } \\label{eq : newacquisition } \\ ] ] where the expectation is now with respect to the posterior @xmath50 and where @xmath51 is the posterior predictive distribution for objective and constraint values given past data and the location of the global solution to the constrained optimization problem @xmath34 .",
    "we call @xmath51 the _ conditioned predictive distribution _ ( cpd )",
    ".    the first term on the right - hand side of ( [ eq : newacquisition ] ) is straightforward to compute : it is the entropy of a product of independent gaussians , which is given by @xmath52 where @xmath53 and @xmath54 are the predictive variances of the objective and constraints , respectively .",
    "however , the second term in the right - hand side of ( [ eq : newacquisition ] ) has to be approximated . for this",
    ", we first approximate the expectation by averaging over samples of @xmath43 approximately drawn from @xmath50 . to sample @xmath43",
    ", we first approximately draw @xmath5 and @xmath55 from their gp posteriors using a finite parameterization of these functions .",
    "then we solve a constrained optimization problem using the sampled functions to yield a sample of @xmath0 .",
    "this optimization approach is an extension of the approach described in more detail by @xcite , extended to the constrained setting . for each value of @xmath43 generated by this procedure , we approximate the cpd @xmath51 as described in the next section .",
    "let @xmath56^t}$ ] denote the concatenated vector of the noise - free objective and constraint values at @xmath57 .",
    "we can approximate the cpd by first approximating the posterior predictive distribution of @xmath58 conditioned on @xmath48 , @xmath57 , and @xmath34 , which we call the _ noise free cpd _ ( nfcpd ) , and then convolving that approximation with additive gaussian noise of variance @xmath59 .",
    "we first consider the distribution @xmath60 .",
    "the variable @xmath0 is in fact a deterministic function of the latent functions @xmath61 : in particular , @xmath0 is the global minimizer if and only if ( i ) all constraints are satisfied at @xmath0 and ( ii ) @xmath62 is the smallest feasible value in the domain .",
    "we can informally translate these deterministic conditions into a conditional probability :    @xmath63\\right ] \\prod_{\\x'\\in\\domain } \\psi(\\x ' ) \\ , , \\ ] ] where @xmath64 is defined as    @xmath65\\right )      \\theta[f(\\x')-f(\\xopt ) ] +       \\left(1 - \\prod_{k=1}^k\\theta\\left[c_k(\\x')\\right]\\right)\\end{aligned}\\ ] ] and the symbol @xmath66 denotes the heaviside step function with the convention that @xmath67 . the first product in ( [ eq : pesc : x - star - given - functions ] ) encodes condition ( i ) and the infinite product over @xmath64 encodes condition ( ii ) .",
    "note that @xmath64 also depends on @xmath0 and @xmath61 ; we use the notation @xmath64 for brevity . because @xmath68 is simply a vector containing the values of @xmath61 at @xmath9",
    ", @xmath68 is also a deterministic function of @xmath61 and we can write @xmath69 using dirac delta functions to pick out the values at @xmath9 :    @xmath70 \\prod_{k=1}^k \\delta[z_k - c_k(\\x ) ] \\ , .\\ ] ] we can now write the nfcpd by i ) noting that @xmath71 is independent of @xmath72 given @xmath61 , ii ) multiplying the product of ( [ eq : pesc : x - star - given - functions ] ) and ( [ eq : pesc : z - given - functions ] ) by @xmath73 and iii ) integrating out the latent functions @xmath61 :    @xmath74 \\left [      \\textstyle\\prod_{k=1}^k \\delta[z_k - c_k(\\mathbf x)]\\right ] \\\\",
    "\\left[\\textstyle\\prod_{k=1}^k \\theta\\left[c_k(\\xopt)\\right]\\right ]       \\left [ \\textstyle\\prod_{\\mathbf x'\\neq \\x }   \\psi(\\x ' ) \\right ] \\psi(\\x ) \\\\",
    "p(f , c_1,\\dots , c_k|\\data ) \\,df\\,dc_1\\dots\\,dc_k \\ , , \\label{eq : nfcpd}\\end{gathered}\\ ] ] where @xmath73 is an infinite - dimensional gaussian given by the gp posterior on @xmath75 , and we have separated @xmath76 out from the infinite product over @xmath30 .    we find a gaussian approximation to ( [ eq : nfcpd ] ) in several steps .",
    "the general approach is to separately approximate the factors that do and do not depend on @xmath9 , so that the computations associated with the latter factors can be reused rather than recomputed for each @xmath9 . in ( [ eq : nfcpd ] ) , the only factors that depend on @xmath9 are the deltas in the first line , and @xmath76 .",
    "let @xmath77 denote the @xmath78-dimensional vector containing objective function evaluations at @xmath79 and @xmath80 , and define constraint vectors @xmath81 similarly .",
    "then , we approximate ( [ eq : nfcpd ] ) by conditioning only on @xmath77 and @xmath81 , rather than the full @xmath61 .",
    "we first approximate the factors in ( [ eq : nfcpd ] ) that do not depend on @xmath9 as    @xmath82\\right ]       \\left [ \\textstyle\\prod_{n=1}^n \\psi(\\x_n ) \\right ]      p(\\mathbf{f},\\mathbf c_1,\\dots,\\mathbf c_k|\\data )      \\label{eq : nfcpdfinite}\\end{gathered}\\ ] ] where @xmath83 is the gp predictive distribution for objective and constraint values . because ( [ eq : nfcpdfinite ] ) is not tractable , we approximate the normalized version of @xmath84 with a product of gaussians using expectation propagation ( ep ) @xcite . in particular , we obtain @xmath85 where @xmath86 is the normalization constant of @xmath84 and @xmath87 for @xmath88 are the mean and covariance terms determined by ep .",
    "see the supplementary material for details on the ep approximation",
    ". roughly speaking , ep approximates each true ( but intractable ) factor in ( [ eq : nfcpdfinite ] ) with a gaussian factor whose parameters are iteratively refined .",
    "the product of all these gaussian factors produces a tractable gaussian approximation to ( [ eq : nfcpdfinite ] ) .",
    "we now approximate the portion of ( [ eq : nfcpd ] ) that does depend on @xmath9 , namely the first line and the factor @xmath76 , by replacing the deltas with @xmath89 , the @xmath90 dimensional , gaussian conditional distribution given by the gp priors on  @xmath91 .",
    "our full approximation to ( [ eq : nfcpd ] ) is then    where  @xmath92 is a normalization constant . from here",
    ", we analytically marginalize out all integration variables except @xmath93 ; see the supplementary material for the full details .",
    "this calculation , and those that follow , must be repeated for every @xmath9 ; however , the ep approximation in ( [ eq : gaussian - q ] ) can be reused over all @xmath9 . after performing the integration ,",
    "we arrive at    where @xmath94 .",
    "details on how to compute the means @xmath95 and variances @xmath96 , as well as the 2-dimensional mean vector @xmath97 and the @xmath98 covariance matrix @xmath99 can be found in the supplementary material .",
    "we perform one final approximation to ( [ eq : nfcpdapproximationmarginalized ] ) .",
    "we approximate this distribution with a product of independent gaussians that have the same marginal means and variances as ( [ eq : nfcpdapproximationmarginalized ] ) .",
    "this corresponds to a single iteration of ep ; see the supplementary material for details .      by approximating the nfcpd with a product of independent gaussians , we can approximate the entropy in the cpd by performing the following operations .",
    "first , we add the noise variances to the marginal variances of our final approximation of the nfcpd and second , we compute the entropy with ( [ eq : entropy - of - gaussians ] ) .",
    "the pesc acquisition function , which approximates ( [ eq : originalacquisition ] ) , is then    @xmath100 where @xmath101 is the number of samples drawn from @xmath50 , @xmath102 is the @xmath103-th of these samples , @xmath104 and @xmath105 are the predictive variances for the noisy evaluations of @xmath5 and @xmath7 at @xmath26 , respectively , and @xmath106 and @xmath107 are the approximated marginal variances of the cpd for the noisy evaluations of  @xmath5 and  @xmath7 at @xmath26 given that @xmath108 .",
    "marginalization of ( [ eq : pesc_acquisition_function ] ) over the gp hyper - parameters can be done efficiently as in @xcite .",
    "the pesc acquisition function is additive in the expected amount of information that is obtained from the evaluation of each task ( objective or constraint ) at any particular location @xmath26 .",
    "for example , the expected information gain obtained from the evaluation of @xmath5 at @xmath26 is given by the term  @xmath109}$ ] in ( [ eq : pesc_acquisition_function ] ) .",
    "the other  @xmath110 terms in ( [ eq : pesc_acquisition_function ] ) measure the corresponding contribution from evaluating each of the constraints .",
    "this allows pesc to easily address the decoupled scenario when one can independently evaluate the different functions at different locations . in other words ,",
    "equation ( [ eq : pesc_acquisition_function ] ) is a sum of individual acquisition functions , one for each function that we can evaluate .",
    "existing methods for bayesian optimization with unknown constraints ( described in section [ section : related - work ] ) do not possess this desirable property .",
    "finally , the complexity of pesc is of order @xmath111 per iteration in the coupled setting . as with unconstrained pes , this is dominated by the cost of a matrix inversion in the ep step .",
    "we evaluate the performance of pesc through experiments with i ) synthetic functions sampled from the gp prior distribution , ii ) analytic benchmark problems previously used in the literature on bayesian optimization with unknown constraints and iii ) real - world constrained optimization problems .",
    "for case i ) above , the synthetic functions sampled from the gp prior are generated following the same experimental set up as in @xcite and @xcite .",
    "the search space is the unit hypercube of dimension @xmath112 , and the ground truth objective @xmath5 is a sample from a zero - mean gp with a squared exponential covariance function of unit amplitude and length scale @xmath113 in each dimension .",
    "we represent the function @xmath5 by first sampling from the gp prior on a grid of 1000 points generated using a halton sequence ( see * ? ? ?",
    "* ) and then defining @xmath5 as the resulting gp posterior mean .",
    "we use a single constraint function @xmath114 whose ground truth is sampled in the same way as @xmath5 .",
    "the evaluations for @xmath5 and @xmath114 are contaminated with i.i.d .",
    "gaussian noise with variance @xmath115 .",
    "we first analyze the accuracy of the approximation to  ( [ eq : newacquisition ] ) generated by pesc .",
    "we compare the pesc approximation with a ground truth for  ( [ eq : newacquisition ] ) obtained by rejection sampling ( rs ) .",
    "the rs method works by discretizing the search space using a uniform grid .",
    "the expectation with respect to  @xmath116 in ( [ eq : newacquisition ] ) is then approximated by monte carlo . to achieve this , @xmath5 and @xmath55",
    "are sampled on the grid and the grid cell with positive @xmath55 ( feasibility ) and the lowest value of @xmath5 ( optimality ) is selected . for each sample of @xmath43 generated by this procedure ,",
    "@xmath117 $ ] is approximated by rejection sampling : we select those samples of @xmath5 and @xmath55 whose corresponding feasible optimal solution is the sampled @xmath43 and reject the other samples .",
    "we then assume that the selected samples for @xmath5 and @xmath55 are independent and have gaussian marginal distributions . under this assumption ,",
    "@xmath117 $ ] can be approximated using the formula for the entropy of independent gaussian random variables , with the variance parameters in this formula being equal to the empirical marginal variances of the selected samples of @xmath5 and @xmath55 at @xmath26 plus the corresponding noise variances @xmath118 and @xmath119 .",
    "the left plot in figure [ fig : accuracy_approximation ] shows the posterior distribution for @xmath5 and @xmath114 given 5 evaluations sampled from the gp prior with @xmath120 .",
    "the posterior is computed using the optimal gp hyperparameters .",
    "the corresponding approximations to ( [ eq : newacquisition ] ) generated by pesc and rs are shown in the middle plot of figure [ fig : accuracy_approximation ] .",
    "both pesc and rs use a total of 50 samples from @xmath116 when approximating the expectation in ( [ eq : newacquisition ] ) .",
    "the pesc approximation is very accurate , and importantly its maximum value is very close to the maximum value of the rs approximation .",
    "one disadvantage of the rs method is its high cost , which scales with the size of the grid used .",
    "this grid has to be large to guarantee good performance , especially when @xmath112 is large .",
    "an alternative is to use a small dynamic grid that changes as data is collected .",
    "such a grid can be obtained by sampling from @xmath116 using the same approach as in pesc .",
    "the samples obtained would then form the dynamic grid .",
    "the resulting method is called rejection sampling with a dynamic grid ( rsdg ) .",
    "we compare the performance of pesc , rs and rsdg in experiments with synthetic data corresponding to 500 pairs of @xmath5 and @xmath114 sampled from the gp prior with @xmath120 . at each iteration",
    ", rsdg draws the same number of samples of @xmath43 as pesc .",
    "we assume that the gp hyperparameter values are known to each method .",
    "recommendations are made by finding the location with lowest posterior mean for @xmath5 such that @xmath114 is non - negative with probability at least @xmath121 , where @xmath122 .",
    "for reporting purposes , we set the utility @xmath123 of a recommendation @xmath26 to be @xmath124 if  @xmath125 satisfies the constraint , and otherwise a penalty value of the worst ( largest ) objective function value achievable in the search space .",
    "for each recommendation at @xmath26 , we compute the utility gap @xmath126 , where @xmath43 is the true solution of the optimization problem .",
    "each method is initialized with the same three random points drawn with latin hypercube sampling .",
    "[ fig : synthetic - and - toy ]    the right plot in figure [ fig : accuracy_approximation ] shows the median of the utility gap for each method across the 500 realizations of @xmath5 and  @xmath114 .",
    "the @xmath127-axis in this plot is the number of joint function evaluations for @xmath5 and @xmath114 .",
    "we report the median because the empirical distribution of the utility gap is heavy - tailed and in this case the median is more representative of the location of the bulk of the data than the mean .",
    "the heavy tails arise because we are measuring performance across 500 different optimization problems with very different degrees of difficulty . in this and all following experiments , standard errors on the reported plot",
    "are computed using the bootstrap .",
    "the plot shows that pesc and rs are better than rsdg .",
    "furthermore , pesc is very similar to rs , with pesc even performing slightly better at the end of the data collection process since pesc is not limited by a finite grid as rs is .",
    "these results show that pesc yields a very accurate approximation of the information gain . furthermore , although rsdg performs worse than pesc , rsdg is faster because the rejection sampling operation ( with a small grid ) is less expensive than the ep algorithm .",
    "thus , rsdg is an attractive alternative to pesc when the available computing time is very limited .",
    "we also compare the performance of pesc and rsdg with that of eic ( section [ sec : cei ] ) using the same experimental protocol as in the previous section , but with dimensionalities  @xmath128 and  @xmath129 .",
    "we do not compare with rs here because its use of grids does not scale to higher dimensions .",
    "figure [ fig : synthetic - and - toy ] shows the utility gap for each method across 500 different samples of @xmath5 and @xmath114 from the gp prior with  @xmath128 ( a ) and  @xmath129 ( b ) .",
    "overall , pesc is the best method , followed by rsdg and eic .",
    "rsdg performs similarly to pesc when  @xmath128 , but is significantly worse when  @xmath129 .",
    "this shows that , when @xmath33 is high , grid based approaches ( e.g. rsdg ) are at a disadvantage with respect to methods that do not require a grid ( e.g. pesc ) .",
    "we compare pesc with eic and al ( section [ sec : lagrangian ] ) in the toy problem described in @xcite .",
    "we seek to minimize the function @xmath130 , subject to the constraint functions @xmath131 and @xmath132 , given by @xmath133 where @xmath9 is confined to the unit square . the evaluations for @xmath5 , @xmath114 and @xmath134 are noise - free .",
    "we compare pesc and eic with @xmath135 and a squared exponential gp kernel .",
    "pesc uses 10 samples from @xmath116 when approximating the expectation in ( [ eq : newacquisition ] ) .",
    "we use the al implementation provided by @xcite in the r package _ lagp _ which is based on the squared exponential kernel and assumes the objective @xmath5 is known .",
    "thus , in order for this implementation to be used , al has an advantage over other methods in that it has access to the true objective function .",
    "in all three methods , the gp hyperparameters are estimated by maximum likelihood .",
    "figure [ fig : toy - example ] shows the mean utility gap for each method across 500 independent realizations .",
    "each realization corresponds to a different initialization of the methods with three data points selected with latin hypercube sampling . here",
    ", we report the mean because we are now measuring performance across realizations of the same optimization problem and the heavy - tailed effect described in section [ sec : approximation_quality ] is less severe .",
    "the results show that pesc is significantly better than eic and al for this problem .",
    "eic is superior to al , which performs slightly better at the beginning , presumably because it has access to the ground truth objective @xmath5 .      in this experiment",
    ", we tune the hyperparamters of a three - hidden - layer neural network subject to the constraint that the prediction time must not exceed 2 ms on a geforce gtx 580 gpu ( also used for training ) .",
    "the search space consists of 12 parameters : 2 learning rate parameters ( initial and decay rate ) , 2 momentum parameters ( initial and final ) , 2 dropout parameters ( input layer and other layers ) , 2 other regularization parameters ( weight decay and max weight norm ) , the number of hidden units in each of the 3 hidden layers , the activation function ( relu or sigmoid ) .",
    "the network is trained using the _ deepnet _ package , and the prediction time is computed as the average time of 1000 predictions , each for a batch of size 128 .",
    "the network is trained on the mnist digit classification task with momentum - based stochastic gradient descent for 5000 iterations .",
    "the objective is reported as the classification error rate on the validation set .",
    "as above , we treat constraint violations as the worst possible value ( in this case a classification error of 1.0 ) .",
    "figure  [ fig : net - coupled ] shows the results of 50 iterations of bayesian optimization . in this experiment and the next ,",
    "the @xmath136-axis represents observed objective values , @xmath137 , a matrn 5/2 gp covariance kernel is used , and gp hyperparameters are integrated out using slice sampling @xcite as in @xcite .",
    "curves are the mean over 5 independent experiments .",
    "we find that pesc performs significantly better than eic .",
    "however , when the noise level is high , reporting the best objective observation is an overly optimistic metric ( due to `` lucky '' evaluations ) ; on the other hand , ground - truth is not available . therefore , to validate our results further , we used the recommendations made at the final iteration of bayesian optimization for each method ( eic and pesc ) and evaluted the function with these recommended parameters .",
    "we repeated the evaluation 10 times for each of the 5 repeated experiments to compute a ground - truth score averaged of 50 function evaluations .",
    "this procedure yields a score of @xmath138 for pesc and @xmath139 for eic ( as in the figure , constraint violations are treated as a classification error of @xmath140 ) .",
    "this result is consistent with figure  [ fig : net - coupled ] in that pesc performs significantly better than eic , but also demonstrates that , due to noise , figure  [ fig : net - coupled ] is overly optimistic .",
    "while we may believe this optimism to affect both methods equally , the ground - truth measurement provides a more reliable result and a much clearer understanding of the classification error attained by bayesian optimization .",
    "hybrid monte carlo , also known as hamiltonian monte carlo ( hmc ) , is a popular markov chain monte carlo ( mcmc ) technique that uses gradient information in a numerical integration to select the next sample . however , using numerical integration gives rise to new parameters like the integration step size and the number of integration steps .",
    "following the experimental set up in @xcite , we optimize the number of effective samples produced by an hmc sampler limited to 5 minutes of computation time , subject to passing of the geweke @xcite and gelman - rubin @xcite convergence diagnostics , as well as the constraint that the numerical integration should not diverge .",
    "we tune 4 parameters of an hmc sampler : the integration step size , number of integration steps , fraction of the allotted 5 minutes spent in burn - in , and an hmc mass parameter ( see * ? ? ?",
    "we use the _ coda _ r package @xcite to compute the effective sample size and the geweke convergence diagnostic , and the _ pymc _ python package @xcite to compute the gelman - rubin diagnostic over two independent traces .",
    "following @xcite , we impose the constraints that the absolute value of the geweke test score be at most 2.0 and the gelman - rubin score be at most 1.2 , and sample from the posterior distribution of a logistic regression problem using the uci german credit data set @xcite .",
    "figure  [ fig : hmc ] evaluates eic and pesc on this task , averaged over 10 independent experiments . as above ,",
    "we perform a ground - truth assessment of the final recommendations .",
    "the average effective sample size is @xmath141 for pesc and @xmath142 for eic . from these results we draw a similar conclusion to that of figure  [ fig : hmc ] ; namely , that pesc outperforms eic but only by a small margin , and furthermore that the experiment is very noisy .",
    "in this paper , we addressed global optimization with unknown constraints . motivated by the weaknesses of existing methods , we presented pesc , a method based on the theoretically appealing expected information gain heuristic .",
    "we showed that the approximations in pesc are quite accurate , and that pesc performs about equally well to a ground truth method based on rejection sampling . in [ sec : d_2_and_8,sec : toy_problem , sec : nnet , sec : hmc ] , we showed that pesc outperforms current methods such as eic and al over a variety of problems .",
    "furthermore , pesc is easily applied to problems with decoupled constraints , without additional computational cost or the pathologies discussed in @xcite .",
    "one disadvantage of pesc is that it is relatively difficult to implement : in particular , the ep approximation often leads to numerical instabilities .",
    "therefore , we have integrated our implementation , which carefully addresses these numerical issues , into the open - source bayesian optimization package _ spearmint _ at https://github.com/hips/spearmint/tree/pesc .",
    "we have demonstrated that pesc is a flexible and powerful method and we hope the existence of such a method will bring constrained bayesian optimization into the standard toolbox of bayesian optimization practitioners .",
    "jos miguel hernndez - lobato acknowledges support from the rafael del pino foundation .",
    "zoubin ghahramani acknowledges support from google focused research award and epsrc grant ep / i036575/1 .",
    "matthew w. hoffman acknowledges support from epsrc grant ep / j012300/1 .",
    "gramacy , robert  b. , gray , genetha  a. , digabel , sebastien  le , lee , herbert k.  h. , ranjan , pritam , wells , garth , and wild , stefan  m. modeling an augmented lagrangian for improved blackbox constrained optimization , 2014 .",
    "arxiv:1403.4890v2 [ stat.co ] .",
    "villemonteix , julien , vzquez , emmanuel , and walter , eric .",
    "an informational approach to the global optimization of expensive - to - evaluate functions .",
    "_ journal of global optimization _ , 440 ( 4):0 509534 , 2009 ."
  ],
  "abstract_text": [
    "<S> unknown constraints arise in many types of expensive black - box optimization problems . </S>",
    "<S> several methods have been proposed recently for performing bayesian optimization with constraints , based on the expected improvement ( ei ) heuristic . </S>",
    "<S> however , ei can lead to pathologies when used with constraints . </S>",
    "<S> for example , in the case of decoupled constraints  i.e . </S>",
    "<S> , when one can independently evaluate the objective or the constraints  ei can encounter a pathology that prevents exploration . </S>",
    "<S> additionally , computing ei requires a current best solution , which may not exist if none of the data collected so far satisfy the constraints . </S>",
    "<S> by contrast , information - based approaches do not suffer from these failure modes . in this paper </S>",
    "<S> , we present a new information - based method called predictive entropy search with constraints ( pesc ) . </S>",
    "<S> we analyze the performance of pesc and show that it compares favorably to ei - based approaches on synthetic and benchmark problems , as well as several real - world examples . </S>",
    "<S> we demonstrate that pesc is an effective algorithm that provides a promising direction towards a unified solution for constrained bayesian optimization . </S>"
  ]
}