{
  "article_text": [
    "reconstruction of a high dimensional low - rank matrix from a low dimensional measurement vector is a challenging problem .",
    "the low - rank matrix reconstruction ( lrmr ) problem is inherently under - determined and have been receiving attention @xcite due to its generality over popular sparse reconstruction problems along with many application scopes @xcite .",
    "here we consider the lrmr system model @xmath0 where @xmath1 is the measurement vector , @xmath2 is the linear measurement matrix , @xmath3 is the low - rank matrix , @xmath4 is additive noise ( typically assumed to be zero - mean gaussian with covariance @xmath5 ) and @xmath6 is the vectorization operator . with @xmath7 ,",
    "the setup is underdetermined and the task is the reconstruction ( or estimation ) of @xmath8 from @xmath9 . to deal with the underdetermined setup ,",
    "a typical and much used strategy is to use a regularization in the reconstruction cost function .",
    "regularization brings in the information about low rank priors . a typical type i",
    "estimator is @xmath10 where @xmath11 is a regularization parameter and @xmath12 is a fixed penalty function that promotes low rank in @xmath13 .",
    "common low - rank penalties in the literature @xcite are @xmath14 where @xmath15 denotes the matrix trace , @xmath16 denotes determinant , and @xmath17 and @xmath18 .",
    "we mention that the nuclear norm penalty is a convex function .    in the literature ,",
    "lrmr algorithms can be categorized in three types : convex optimization @xcite , greedy solutions @xcite and bayesian learning @xcite .",
    "most of these existing algorithms are highly motivated from analogous algorithms used for standard sparse reconstruction problems , such as compressed sensing where @xmath19 in is replaced by a sparse vector .",
    "using convex optimization we can solve when @xmath20 is the nuclear norm , which is an analogue of using @xmath21-norm in sparse reconstruction problems .",
    "further , greedy algorithms , such as iteratively reweighted least squares @xcite solves by using algebraic approximations . while convex optimization and greedy solutions are popular they often need more a - priori information than knowledge about structure of the signal under reconstruction ; for example , convex optimization algorithms need information about the strength of the measurement noise to fix the parameter @xmath22 , and greedy algorithms need information about rank . in absence of such a - priori information ,",
    "bayesian learning is a preferred strategy to use .",
    "bayesian learning is capable of estimating the necessary information from measurement data . in bayesian learning",
    "we evaluate the posterior @xmath23 with the knowledge of prior @xmath24 .",
    "if @xmath8 has a prior distribution @xmath25 and the noise is distributed as @xmath26 , then the maximum - a - posteriori ( map ) estimate can be interpreted as the type i estimate in . as type",
    "i estimation requires more information ( such as @xmath22 ) , type ii estimators are often more useful .",
    "type ii estimation techniques use hyper - parameters in the form of latent variables with prior distributions . while for sparse reconstruction problems , bayesian learning via type ii estimation in the form of relevance vector machine @xcite and sparse bayesian learning @xcite have gained significant popularity , the endeavor to design type ii estimation algorithms for lrmr is found to be limited . in @xcite , direct use of sparse bayesian learning was used to realize an lrmr reconstruction algorithm .",
    "bayesian approaches were used in @xcite for a problem setup with a combination of low rank and sparse priors , called principal component pursuit @xcite . in @xcite , gaussian and bernoulli variables was used and the parameters were estimated using markov chain monte carlo while in @xcite an empirical bayesian approach was used .",
    "type ii estimation methods are typically iterative where latent variables are usually treated via variational techniques @xcite , evidence approximation @xcite , expectation maximization @xcite and markov chain monte carlo @xcite .",
    "our objective in this paper is to develop new type ii estimation methods for lrmr .",
    "borrowing ideas from type ii estimation techniques for sparse reconstruction , such as the relevance vector machine and sparse bayesian learning algorithms , we model a low - rank matrix by a multiplication of precision matrices and an i.i.d . gaussian matrix .",
    "the use of precision matrices helps to realize low - rank structures .",
    "the precision matrices are characterized by hyper - parameters which are treated as latent variables .",
    "the main contributions of this paper are as follows .    1",
    ".   we introduce one - sided and two - sided precision matrix based models . 2 .",
    "we show how the schatten s - norm and log - determinant penalty functions are related to latent variable models in the sense of map estimation via type i estimator .",
    "3 .   for all new type ii estimation methods ,",
    "we derive update equations for all parameters in iterations .",
    "the methods are based on evidence approximation and expectation - maximization .",
    "the methods are compared numerically to existing methods , such as the bayesian learning method of @xcite and nuclear norm based convex optimization method @xcite .",
    "we are aware that evidence approximation and expectation - maximization are unable to provide globally optimal solutions .",
    "hence we are unable to provide performance guarantees for our methods .",
    "this paper is organized as follows .",
    "we discuss the preliminaries of sparse bayesian learning in section  [ sec : preliminaries ] . in section  [ sec : one_sided ] we introduce one - sided precisions for matrices and derive the relations to type i estimators .",
    "two - sided precisions are introduced in section  [ sec : two_sided ] and in section  [ sec : algorithms ] we derive the update equations for the parameters . in section  [ sec : simulations ] we numerically compare the performance of the algorithms for matrix reconstruction and matrix completion .      in this section",
    ", we explain the relevance vector machine ( rvm ) @xcite and sparse bayesian learning ( sbl ) methods @xcite for a standard sparse reconstruction problem .",
    "the setup is @xmath27 where @xmath28 is the sparse vector to be reconstructed from the measurement vector @xmath29 and @xmath26 is the additive measurement noise .",
    "the approach is to model the sparse signal @xmath30^{\\top}$ ] as @xmath31 where @xmath32 and @xmath33 is the precision of @xmath34 .",
    "this is equivalent to setting @xmath35 the main idea is to use a learning algorithm for which several precisions go to infinity , leading to sparse reconstruction .",
    "alternatively said , the use of precisions allows to inculcate _ dominance _ of a few components over other components in a sparse vector .",
    "note that @xmath36^{\\top}$ ] and @xmath22 are latent variables that also need to be estimated .",
    "we find the posterior @xmath37 if @xmath38 is assumed sharply peaked around @xmath39 , @xmath40 ( this is version of the so - called laplace approximation described in appendix [ appendix : laplace ] ) .",
    "assuming the knowledge of @xmath41 and @xmath42 , the map estimate is @xmath43^{\\top } \\leftarrow \\arg \\max_{\\mathbf{x } } p(\\mathbf{x}|\\mathbf{y } , \\boldsymbol{\\gamma } , \\beta ) = \\beta \\boldsymbol{\\sigma } \\mathbf{a}^\\top \\mathbf{y } \\label{eq : map_x_rvm}\\end{aligned}\\ ] ] where @xmath44 . in the notion of iterative updates , we use @xmath45 to denote the assignment operator .",
    "the precisions @xmath46 and @xmath22 are estimated by @xmath47 where @xmath48 and @xmath49 .",
    "gamma distributions are typically chosen as hyper - priors for @xmath50 and @xmath51 with the form @xmath52 with @xmath53 , @xmath54 and @xmath55 .",
    "the evaluation of @xmath56 leads to coupled equations and are therefore solved approximately as @xmath57 where @xmath58 is the @xmath59th diagonal element of @xmath60 .",
    "the parameters of the gamma distributions for @xmath61 and @xmath51 are typically chosen to be non - informative , i.e. @xmath62 . the update solutions of and are repeated iteratively until convergence . in sparse bayesian learning algorithm a standard expectation - maximization framework is used to estimate @xmath46 and @xmath22 .",
    "finally we mention that the rvm and sbl methods have connection with type i estimation @xcite .",
    "if the precisions have arbitrary prior distributions @xmath50 , then the marginal distribution of @xmath34 becomes @xmath63 for some function @xmath64 .",
    "given and for a known @xmath22 , the map estimate is @xmath65 if @xmath50 is a gamma prior then @xmath66 is a student-@xmath67 distribution with @xmath68 .",
    "one rule of thumb is that a `` more '' concave @xmath69 gives a more sparsity promoting model @xcite , see some example functions in figure  [ fig : illustration ] . in the figure , @xmath70 corresponds to a laplace distributed variable @xmath34 , @xmath71 to a student-@xmath67 and @xmath72 to a generalized normal distribution @xcite .",
    "the relation between the sparsity promoting penalty function @xmath69 and the corresponding prior @xmath50 of the latent variable @xmath73 was discussed in @xcite , see also @xcite and @xcite .",
    "( 3,0 ) node[right ] @xmath34 ; ( 0,0 )  ( 0,2 ) ; ( -2.5,2.5 )  ( 0,0 ) ",
    "( 2.5,2.5 ) node[right ] @xmath74 ; plot ( , ln(+ 1 ) ) node[right ] @xmath75 ; plot ( , sqrt(abs ( ) ) ) node[right ] @xmath72 ;",
    "the structure of a low - rank matrix @xmath8 is characterized by the dominant singular vectors and singular values . like the use of precisions in for the standard sparse reconstruction problem via inculcating dominance ,",
    "we propose to model the low - rank matrix @xmath8 as @xmath76 where the components of @xmath77 are i.i.d . @xmath78 and",
    "@xmath79 is a positive definite random matrix ( which distribution will be described later ) .",
    "this is equivalent to @xmath80 denoting @xmath81 and @xmath82 , we evaluate @xmath83 we note that @xmath20 must have the special form @xmath84 for @xmath24 to hold ( as @xmath85 is integrated out ) . as @xmath86 , the resulting map estimator can be interpreted as the type i estimator .",
    "next we investigate the relation between the priors @xmath24 and @xmath87 .",
    "the motivation is that the relations are necessary for designing practical learning algorithms .",
    "from , we note that @xmath24 is the laplace transform of @xmath88 @xcite , which establishes the relation .",
    "naturally , we can find @xmath87 by the inverse laplace transform @xcite as follows @xmath89 where the integral is taken over all symmetric matrices @xmath90 such that @xmath91 where @xmath92 is a real matrix so that the contour path of integration is in the region of convergence of the integrand . while the laplace transform characterizes the exact relation between priors , the computation is non - trivial and often analytically intractable . in practice ,",
    "a standard approach is to use the laplace approximation @xcite where typically the mode of the distribution under approximation is found first and then a gaussian distribution is modeled around that mode .",
    "let @xmath87 have the form @xmath93 ; then the laplace approximation becomes @xmath94 where @xmath95 is the hessian of @xmath96 evaluated at the minima ( which is assumed to exist ) .",
    "the derivation of the laplace approximation is shown in appendix [ appendix : laplace ] .    denoting @xmath97 and assuming that the hessian is constant ( independent of @xmath98 ) we get that @xmath99 where we absorbed the constants terms into the normalization factor of @xmath24 .",
    "we find that @xmath100 is the concave conjugate of @xmath101 @xcite .",
    "hence , for a given @xmath100 we can recover @xmath101 as @xmath102 if @xmath101 is concave ( which holds under the assumption that @xmath103 is convex ) .",
    "further , we can find @xmath103 from @xmath101 followed by solving the prior @xmath93 .",
    "using the concave conjugate relation , we now deal with the task of finding appropriate @xmath103 for two example low - rank promoting penalty functions , as follows .    1 .   _ for schatten @xmath104-norm : _ the schatten @xmath104-norm based penalty function is @xmath105 .",
    "we here use a regularized schatten @xmath104-norm based penalty function as @xmath106 where the use of @xmath18 helps to bring numerical stability to the algorithms in section  [ sec : algorithms ] . for the penalty function",
    ", we find the appropriate @xmath103 as @xmath107 where @xmath108 .",
    "the derivation of is given in appendix [ appendix2 ] .",
    "note that , for @xmath109 , @xmath20 becomes the regularized nuclear norm based penalty function @xmath110 2 .",
    "_ log - determinant penalty : _ for the log - determinant based penalty function @xmath111 where @xmath112 is a real number , we find @xmath103 as @xmath113 as @xmath93 , we find that the prior @xmath85 is wishart distributed ( wishart is a conjugate prior the distribution ) . for a scalar instead of a matrix ,",
    "the prior distribution becomes a gamma distribution as used in the standard rvm and sbl .",
    "we have discussed a left - sided precision based model   in this section , but the same strategy can be easily extended to form a right - sided precision based model",
    ". then a natural question arises , which model to use ?",
    "our hypothesis is that the user choice stems from minimizing the number of variables to estimate .",
    "if the low - rank matrix is fat then the left - sided model should be used , otherwise the right - sided model .",
    "a further question arises on the prospect of developing a two sided precision based model , which is described in the next section .",
    "in this section , we propose to use precision matrices on both sides to model a random low - rank matrix .",
    "we call this the two - sided precision based model .",
    "our hypothesis is that the two - sided precision helps to enhance dominance of a few singular vectors . for low - rank modeling ,",
    "we make the following ansatz @xmath114 where @xmath115 and @xmath116 are positive definite random matrices . using the relation @xmath117",
    ", we find @xmath118 to promote low - rank , we use a prior distribution @xmath119 .",
    "the marginal distribution of @xmath8 is @xmath120 we have noticed that the use of in evaluating does not bring out suitable connections between the resulting @xmath24 functions and the usual low - rank promoting @xmath20 functions ( such as nuclear norm , schatten s - norm and log - determinant ) .",
    "thus it is non - trivial to establish a direct connection between @xmath121 of and the type i estimator of .    instead of a direct connection we can establish an indirect connection by an approximation . for a given @xmath122 and by marginalizing over @xmath123 , we have @xmath124 and hence the corresponding type i estimator cost function is @xmath125 a similar cost function can be found for a given @xmath126 by marginalizing over @xmath127 .",
    "we discuss the roles of @xmath123 and @xmath127 in the next section .      from",
    ", we can see that the column and row vectors of @xmath8 are in the range spaces of @xmath128 and @xmath129 , respectively .",
    "further let us interpret this in a statistical sense with the note that a skewed precision matrix comprises of correlated components .",
    "let us denote the @xmath130th component of @xmath129 by @xmath131_{ij}$ ] .",
    "if @xmath129 is highly skewed then @xmath131_{ij}$ ] and @xmath131_{ii}$ ] are highly correlated .",
    "suppose @xmath132 denotes the @xmath59th column vector of @xmath8 .",
    "then following , we can write @xmath133 \\sim   \\mathcal{n } \\left ( \\left [ \\begin{array}{c }   \\mathbf{0 } \\\\",
    "\\mathbf{0 } \\end{array } \\right ] , \\left [ \\begin{array}{cc }   [ \\boldsymbol{\\alpha}_r^{-1}]_{ii } \\ , \\boldsymbol{\\alpha}_l^{-1 } & [ \\boldsymbol{\\alpha}_r^{-1}]_{ij } \\ ,",
    "\\boldsymbol{\\alpha}_l^{-1 } \\\\   { [ \\boldsymbol{\\alpha}_r^{-1}]_{ji } \\",
    ", \\boldsymbol{\\alpha}_l^{-1 } } & [ \\boldsymbol{\\alpha}_r^{-1}]_{jj } \\ , \\boldsymbol{\\alpha}_l^{-1 } \\end{array } \\right ] \\right).\\end{aligned}\\ ] ] the above relation shows that a presence of highly skewed @xmath129 leads to the cross - correlation @xmath131_{ij } \\ , \\boldsymbol{\\alpha}_l^{-1}$ ] between @xmath134 and @xmath135 that is comparably strong to the auto - correlations @xmath131_{ii } \\ , \\boldsymbol{\\alpha}_l^{-1}$ ] .",
    "we mention that a low - rank property can be established in a qualitative statistical sense by the presence of columns having strong cross - correlation .",
    "the one - sided precision based model can be seen as the two - sided model where @xmath136 .",
    "hence the one - sided precision based model is unable to capture information about cross - correlation between columns of @xmath8 .",
    "a similar argument can be made for the right sided precision based model where @xmath137 .",
    "considering the potential of two - sided precision matrices , the optimal inference problem is @xmath138 which is the map estimator for amenable priors and often connected with the type i estimator in .",
    "direct handling of the optimal inference problem is limited due to lack of analytical tractability .",
    "therefore various approximations are used to design practical algorithms which are also type ii estimators .",
    "this section is dedicated to design new type ii estimators via evidence approximation ( as used by the rvm ) and expectation - maximization ( as used in sbl ) approaches .      in the evidence approximation , we iteratively update the parameters as@xmath139 the solution of is the standard linear minimum mean square error estimator ( lmmse ) as @xmath140 using a standard approach ( see equations ( 45 ) and ( 46 ) of @xcite or ( 7.88 ) of @xcite ) , the solution of can be found as @xmath141 the standard rvm in @xcite uses the different update rule @xcite @xmath142 which often improves convergence @xcite .",
    "the update rule has the benefit over of having established convergence properties . in simulations we used the update rule since it improved the estimation accuracy",
    ".    finally we deal with as follows .    1 .",
    "_ for schatten @xmath104-norm : _ using the schatten @xmath104-norm prior gives us the update equations @xmath143 where @xmath144 and the matrices @xmath145 and @xmath146 have elements @xmath147_{ij } = \\mathrm{tr}(\\boldsymbol{\\sigma}(\\boldsymbol{\\alpha}_r \\otimes \\mathbf{e}_{ij}^{(l ) } ) ) , \\\\ & [ \\tilde{\\boldsymbol{\\sigma}}_r]_{ij } = \\mathrm{tr}(\\boldsymbol{\\sigma}(\\mathbf{e}_{ij}^{(r ) } \\otimes \\boldsymbol{\\alpha}_l ) ) , \\end{aligned}\\ ] ] and where @xmath148 and @xmath149 are matrices with ones in position @xmath130 and zeros otherwise .",
    "log - determinant penalty : _ for the log - determinant prior the update equations become @xmath150 we see that the update rule for the log - determinant penalty can be interpreted as in the limit @xmath151 .",
    "the derivations of and are shown in appendix [ appendix2 ] and [ appendix3 ] .",
    "the corresponding update equations for the one - sided precision based model are obtained by fixing the other precision matrix to be the identity matrix . in the spirit of the evidence approximation based relevance vector machine , we call the developed algorithms in this section as relevance singular vector machine ( rsvm ) . for schatten - s norm and log - determinant priors ,",
    "the methods are named as rsvm - sn and rsvm - ld , respectively .",
    "[ [ em ] ] em ~~    in expectation - maximization @xcite , the value of the precisions @xmath152 are updated in each iteration by maximizing the cost ( em help function in map estimation ) @xmath153 where @xmath154 are the parameter values from the previous iteration .",
    "the function @xmath155 is defined as @xmath156   = \\text{constant } \\nonumber \\\\ & - \\frac{\\beta}{2 } ||\\mathbf{y - a}\\mathrm{vec}(\\hat{\\mathbf{x } } ) ||_2 ^ 2 - \\frac{1}{2 } \\mathrm{tr}(\\boldsymbol{\\alpha}_l \\hat{\\mathbf{x } } \\boldsymbol{\\alpha}_r \\hat{\\mathbf{x}}^\\top ) -\\frac{1}{2 } \\mathrm{tr}(\\boldsymbol{\\sigma}^{-1 } \\boldsymbol{\\sigma } ' ) \\nonumber \\\\ % &   \\,\\,\\,\\ ,   & + \\frac{q}{2 } \\log |\\boldsymbol{\\alpha}_l| + \\frac{p}{2 } \\log |\\boldsymbol{\\alpha}_r| + \\frac{m}{2 } \\log \\beta , \\label{eq : lowrank : em_help_function}\\end{aligned}\\ ] ] where @xmath157 , and @xmath158 denotes the expectation operator .",
    "the maximization of @xmath159 leads to update equations which are identical to the update equations of evidence approximation .",
    "that means that for the schatten - s norm , the maximization leads to , and , and for log - determinant penalty , the maximization leads to , and . for the noise precision ,",
    "em reproduces the update equation .",
    "the derivation of and update equations are shown in appendix [ appendix : em ] . unlike evidence approximation , em has monotonic convergence properties and hence the derived update equations are bound to improve estimation performance in iterations .",
    "we have found that in practical algorithms , there is a chance that one of the two precisions becomes large and the other small over iterations .",
    "a small precision results in numerical instability in the kronecker covariance structure . to prevent the inbalance we rescale the matrix precisions in each iteration such that @xmath160 the a - priori and a - posteriori squared frobeniun norm of @xmath8 are equal , @xmath161 = \\mathrm{tr}(\\boldsymbol{\\alpha}_l^{-1 } ) \\mathrm{tr}(\\boldsymbol{\\alpha}_r^{-1 } ) \\\\ & = \\mathcal{e}[||\\mathbf{x}||_f^2 | \\boldsymbol{\\alpha}_l , \\boldsymbol{\\alpha}_r,\\beta , \\mathbf{y } ] = ||\\hat{\\mathbf{x}}||_f^2 + \\mathrm{tr}(\\boldsymbol{\\sigma}),\\end{aligned}\\ ] ] and @xmath162 the contribution of the precisions to the norm is equal , @xmath163 the rescaling makes the algorithm more stable and often improves estimation performance .",
    "in this section we numerically verify our two hypotheses , and compare the new algorithms with relevant existing algorithms .",
    "our objectives are to verify :    * the hypothesis that the left - sided precision is better than the right sided precision for a fat low - rank matrix , * the hypothesis that the two - sided precision based model performs better than one - sided precision based model . *",
    "the proposed methods perform better than a nuclear - norm minimization based convex algorithm and a variational bayes algorithm .    in the simulations we considered low - rank matrix reconstruction and also matrix completion as a special case due to its popularity .",
    "to compare the algorithms , the performance measure is the normalized - mean - square - error @xmath164/\\mathcal{e}[||\\mathbf{x}||_f^2].\\end{aligned}\\ ] ]    in experiments we varied the value of one parameter while keeping the other parameters fixed . for given parameter values",
    ", we evaluated the nmse as follows .    1 .   for lrmr ,",
    "the random measurement matrix @xmath2 was generated by independently drawing the elements from @xmath78 and normalizing the column vectors to unit norm .",
    "for low rank matrix completion , each row of @xmath165 contains a 1 in a random position and zero otherwise with the constraint that the rows are linearly independent .",
    "matrices @xmath166 and @xmath167 with elements drawn from @xmath78 were randomly generated and the matrix @xmath8 was formed as @xmath168 .",
    "note that @xmath8 is of rank @xmath169 ( with probability @xmath170 ) .",
    "3 .   generate the measurement @xmath171 , where @xmath172 and @xmath173 is chosen such that the signal - to - measurement - noise ratio is @xmath174}{\\mathcal{e}[||\\mathbf{n}||_2 ^ 2 ] } = \\frac{rpq}{m\\sigma_n^2}.\\end{aligned}\\ ] ] 4 .",
    "estimate @xmath13 using competing algorithms and calculate the error @xmath175 .",
    "repeat steps @xmath176 for each measurement matrix @xmath177 number of times .",
    "repeat steps @xmath178 for the same parameter values @xmath179 number of times .",
    "7 .   then compute the nmse by averaging .    in the simulations we chose @xmath180 , which means that the averaging was done over 625 realizations .",
    "we normalized the column vectors of @xmath165 to make the smnr expression realization independent .",
    "finally we describe competing algorithms . for comparison , we used the following nuclear norm based estimator @xmath181 where we used @xmath182 as proposed in @xcite .",
    "the cvx toolbox @xcite was used to implement the estimator . for matrix completion",
    "we also compared with the variational bayesian ( vb ) developed by babacan et .",
    "al . @xcite . in vb ,",
    "the matrix @xmath8 is factorized as @xmath183 and ( block ) sparsity inducing priors are used for the column vectors of @xmath184 and @xmath185 .",
    "the vb algorithm was developed for matrix completion ( and robust pca ) , but not for matrix reconstruction .",
    "we note that , unlike rsvm and vb , the nuclear norm estimator requires a - priori knowledge of the noise power .",
    "we also compared the algorithms to the cramr - rao bound ( crb ) from @xcite ( as we know the rank a - priori in our experimental setup ) .",
    "we mention that the crb is not always a valid lower bound in this experimental setup because all technical conditions for computing a valid crb are not always fulfilled and the estimators are not always unbiased .",
    "the choice of crb is due to absence of any other relevant theoretical bound .",
    "our first experiment is for verification of the first two hypotheses . for the experiment",
    ", we considered lrmr and fixed @xmath186 , @xmath187 , @xmath188 , @xmath189 db and varied @xmath190 .",
    "the results are shown in figure  [ fig : single_double_rec ] where nmse is plotted against normalized measurements @xmath191 .",
    "we note that rsvm - sn with left precision is better than right precision .",
    "same result also hold for rsvm - ld .",
    "this verifies the first hypothesis .",
    "further we see that rsvm - sn and rsvm - ld with two sided precisions are better than respective one - sided precisions .",
    "this result verifies the second hypothesis . in the experiments we used @xmath192 for rsvm - sn as it was found to be the best ( empirically ) .",
    "henceforth we fix @xmath192 for rsvm - sn .     for low - rank matrix reconstruction . ]",
    "the second experiment considers comparison with nuclear - norm based algorithm and the crb for lrmr .",
    "the objective is robustness study by varying number of measurements and measurement noise power .",
    "we used @xmath193 , @xmath187 and @xmath188 . in figure",
    "[ fig : alpha_snr_rec ] ( a ) we show the performance against varying @xmath191 ; the smnr = 20 db was fixed .",
    "the performance improvement of rsvm - sn is more pronounced over the nuclear - norm based algorithm in the low measurement region .",
    "now we fix @xmath194 and vary the smnr .",
    "the results are shown in figure  [ fig : alpha_snr_rec ] ( b ) which confirms robustness against measurement noise .     and",
    "smnr for low - rank matrix reconstruction .",
    "( a ) smnr = 20 db and @xmath191 is varied .",
    "( b ) @xmath194 and smnr is varied . ]",
    "next we deal with matrix completion where the measurement matrix @xmath165 has a special structure and considered to be inferior to hold information about @xmath8 than the same dimensional random measurement matrix used in lrmr .",
    "therefore matrix completion requires more measurements and higher smnr .",
    "we performed similar experiments as in our second experiment and the results are shown in figure  [ fig : alpha_snr_comp ] . in the experiments the performance of the vb algorithm is included .",
    "it can be seen that rsvm - sn is typically better than the other algorithms .",
    "we find that the the vb algorithm is pessimistic .     and smnr for low - rank matrix completion .",
    "( a ) smnr = 20 db and @xmath191 is varied .",
    "( b ) @xmath194 and smnr is varied . ]    finally in our last experiment we investigated the vb algorithm to find conditions for its improvement and compared it with rsvm - sn . for this experiment , we fixed @xmath193 , @xmath187 , @xmath194 and smnr = 20 db , and varied @xmath195 .",
    "the results are shown in figure  [ fig : q_completion ] and we see that vb provides good performance when @xmath196 .",
    "the result may be attributed to an aspect that vb is highly prone to a large number of model parameters which arises in case @xmath8 is away from a square matrix .     for low - rank matrix completion .",
    "in this paper we developed bayesian learning algorithms for low - rank matrix reconstruction .",
    "the framework relates low - rank penalty functions ( type i estimators ) to the latent variable models ( type ii estimators ) with either left- or right - sided precisions through the matrix laplace transform and the concave conjugate formula .",
    "the model was further extended to the two - sided precision based model .",
    "using evidence approximation and expectation maximization , we derived the update equations for the parameters .",
    "the resulting algorithm was named the relevance singular vector machine ( rsvm ) due to its similarity with the relevance vector machine for sparse vectors .",
    "especially we derived the update equations for the estimators corresponding to the log - determinant penalty and the schatten @xmath104-norm penalty , we named the algorithms rsvm - ld and rsvm - sn , respectively .    through simulations , we showed that the two - sided precision based model performs better than the one - sided model for matrix reconstruction .",
    "the algorithm also outperformed a nuclear - norm based estimator , even though the nuclear - norm based estimator knew the noise power .",
    "the proposed methods also outperformed a variational bayes method for matrix completion when the matrix is not square .",
    "the laplace approximation is an approximation of the integral @xmath197 where the integral is over @xmath198 .",
    "the function @xmath199 is approximated by a second order polynomial around its minima @xmath200 as @xmath201 where @xmath202 is the hessian of @xmath199 at @xmath200 . the term linear in @xmath203 vanishes and @xmath204 at @xmath200 since we expand around a minima . with this approximation ,",
    "the integral becomes @xmath205 in , the integral is given by @xmath206 } d \\boldsymbol{\\alpha}.\\end{aligned}\\ ] ] set @xmath207 , where @xmath208 .",
    "let @xmath209 denote the minima of @xmath199 and @xmath95 the hessian at @xmath210 .",
    "assuming that @xmath210 and @xmath95 are `` large '' in the sense that the integral over @xmath211 can be approximated by the integral over @xmath79 we find that @xmath212 where @xmath213 .      the em help function @xmath155 is given by @xmath214 = c + \\frac{m}{2 } \\log \\ , \\beta \\\\ & - \\frac{\\beta}{2 } \\mathcal{e}[||\\mathbf{y - a}\\mathrm{vec}(\\mathbf{x})||_2 ^ 2 - \\frac{1}{2 } \\mathcal{e}[\\mathrm{tr}(\\boldsymbol{\\alpha}_l \\mathbf{x } \\boldsymbol{\\alpha}_r \\mathbf{x}^\\top ) ] \\\\ & + \\frac{q}{2 } \\log |\\boldsymbol{\\alpha}_l| + \\frac{p}{2 } \\log |\\boldsymbol{\\alpha}_r|,\\end{aligned}\\ ] ] where @xmath215 is a constant . using that @xmath216 = ||\\mathbf{y}||_2 ^ 2 - 2\\mathbf{y}^\\top \\mathbf{a } \\mathrm{vec}(\\hat{\\mathbf{x } } ) \\\\ & + \\mathrm{tr}(\\mathbf{a^\\top a}(\\mathrm{vec}(\\hat{\\mathbf{x } } ) \\mathrm{vec}(\\hat{\\mathbf{x}})^\\top + \\boldsymbol{\\sigma } ' ) ) \\\\ & = ||\\mathbf{y - a}\\mathrm{vec}(\\hat{\\mathbf{x}})||_2 ^ 2 + \\mathrm{tr}(\\mathbf{a}^\\top\\mathbf{a}\\boldsymbol{\\sigma } ' ) , \\end{aligned}\\ ] ] and @xmath217 \\\\ & = \\mathrm{tr}((\\boldsymbol{\\alpha}_r \\otimes \\boldsymbol{\\alpha}_l)(\\mathrm{vec}(\\hat{\\mathbf{x } } ) \\mathrm{vec}(\\hat{\\mathbf{x}})^\\top + \\boldsymbol{\\sigma } ' ) ) \\\\ & = \\mathrm{tr}(\\boldsymbol{\\alpha}_l \\hat{\\mathbf{x } } \\boldsymbol{\\alpha}_r \\hat{\\mathbf{x}}^\\top ) + \\mathrm{tr}((\\boldsymbol{\\alpha}_r \\otimes \\boldsymbol{\\alpha}_l ) \\boldsymbol{\\sigma}'),\\end{aligned}\\ ] ] we recover the expression for the em help function .",
    "we here set @xmath218 to keep the derivation more general .",
    "the regularized schatten @xmath104-norm penalty is given by @xmath219 for the concave conjugate formula we find that the minimum over @xmath98 occurs when @xmath220 solving for @xmath98 gives us that @xmath221 which results in .",
    "the log - determinant penalty is given by @xmath223 for the concave conjugate formula we find that the minimum over @xmath98 occurs when @xmath224 solving for @xmath98 gives @xmath225 by removing the constants we recover .    using , we find that the minimum of with respect to @xmath123 for the log - determinant penalty occurs when @xmath226 solving for @xmath123 gives us for @xmath123 . the derivation of the update equation for @xmath127 is found in a similar way .",
    "k.  yu , j.  lafferty , s.  zhu and y.  gong , `` large - scale collaborative prediction using a nonparametric random effects model , '' proceedings of the 26th annual international conference on machine learning .",
    "acm , 2009 .",
    "m.  tipping and a.  faul , `` fast marginal likelihood maximisation for sparse bayesian models , '' proceedings of the ninth international workshop on artificial intelligence and statistics , vol . 1 , no . 3 , 2003 .    z.  zhang and b.d .  rao , `` extension of sbl algorithms for the recovery of block sparse signals with intra - block correlation , '' ieee transactions on signal processing , vol . 61 , no . 8 , pp .",
    "2009 - 2015 , april 2013 .",
    "z.  zhang and b.d .",
    "rao , `` sparse signal recovery with temporally correlated source vectors using sparse bayesian learning , '' ieee journal of selected topics in signal processing , vol .",
    "912 - 926 , sept . 2011 .",
    "m.  sundin , s.  chatterjee , m.  jansson and c.r .",
    "rojas , `` relevance singular vector machine for low - rank matrix sensing '' , international conference on signal processing and communications ( spcom ) , indian institute of science , bangalore , india , july 2014 .",
    "avaliable online from http://arxiv.org/abs/1407.0013 ."
  ],
  "abstract_text": [
    "<S> we develop latent variable models for bayesian learning based low - rank matrix completion and reconstruction from linear measurements . for under - determined systems , </S>",
    "<S> the developed methods are shown to reconstruct low - rank matrices when neither the rank nor the noise power is known a - priori . </S>",
    "<S> we derive relations between the latent variable models and several low - rank promoting penalty functions . </S>",
    "<S> the relations justify the use of kronecker structured covariance matrices in a gaussian based prior . in the methods </S>",
    "<S> , we use evidence approximation and expectation - maximization to learn the model parameters . </S>",
    "<S> the performance of the methods is evaluated through extensive numerical simulations . </S>"
  ]
}