{
  "article_text": [
    "in many applications in mathematics and statistical data analysis , it is of interest to find an approximate solution to a system of linear equations that has no exact solution .",
    "for example , let a matrix @xmath5 and a vector @xmath6 be given . if @xmath2 , there will not in general exist a vector @xmath7 such that @xmath8 , and yet it is often of interest to find a vector @xmath9 such that @xmath10 in some precise sense .",
    "the method of least squares , whose original formulation is often credited to gauss and legendre  @xcite , accomplishes this by minimizing the sum of squares of the elements of the residual vector , i.e. , by solving the optimization problem @xmath11 it is well - known that the minimum @xmath12-norm vector among those satisfying eqn .",
    "( [ eqn : orig_ls_prob ] ) is @xmath13 where @xmath14 denotes the moore - penrose generalized inverse of the matrix @xmath15  @xcite .",
    "this solution vector has a very natural statistical interpretation as providing an optimal estimator among all linear unbiased estimators , and it has a very natural geometric interpretation as providing an orthogonal projection of the vector @xmath16 onto the span of the columns of the matrix @xmath15 .    recall that to minimize the quantity in eqn .",
    "( [ eqn : orig_ls_prob ] ) , we can set the derivative of @xmath17 with respect to @xmath9 equal to zero , from which it follows that the minimizing vector @xmath18 is a solution of the so - called normal equations @xmath19 geometrically , this means that the residual vector @xmath20 is required to be orthogonal to the column space of @xmath15 , i.e. , @xmath21 . while solving the normal equations squares the condition number of the input matrix ( and thus is not recommended in practice ) , direct methods ( such as the qr decomposition  @xcite ) solve the problem of eqn .",
    "( [ eqn : orig_ls_prob ] ) in @xmath3 time assuming that @xmath22 . finally , an alternative expression for the vector @xmath18 of eqn .",
    "( [ eqn : xopt_orig_ls_prob ] ) emerges by leveraging the singular value decomposition ( svd ) of @xmath15 .",
    "if @xmath23 denotes the svd of @xmath15 , then @xmath24      in this paper , we describe two randomized algorithms that will provide accurate relative - error approximations to the minimal @xmath12-norm solution vector @xmath18 of eqn .",
    "( [ eqn : xopt_orig_ls_prob ] ) faster than existing exact algorithms for a large class of overconstrained least - squares problems .",
    "in particular , we will prove the following theorem .",
    "[ thm : main_result ] suppose @xmath5 , @xmath25 , and let @xmath26 .",
    "then , there exists a randomized algorithm that returns a vector @xmath27 such that , with probability at least @xmath28 , the following two claims hold : first , @xmath29 satisfies @xmath30 and , second , if @xmath31 is the condition number of @xmath15 and if we assume that @xmath32 $ ] is the fraction of the norm of @xmath16 that lies in the column space of @xmath15 ( i.e. , @xmath33 , where @xmath34 is an orthogonal basis for the column space of @xmath15 ) , then @xmath29 satisfies @xmath35 finally , the solution @xmath29 can be computed in @xmath4 time if @xmath0 is sufficiently larger than @xmath1 and less than @xmath36 .",
    "we will provide a precise statement of the running time for our two algorithms ( including the @xmath37-dependence ) in theorems  [ thm : alg_sample_fast ] ( section  [ sxn : sampling ] ) and  [ thm : alg_projection_fast ] ( section  [ sxn : projection ] ) , respectively .",
    "it is worth noting that the claims of theorem  [ thm : main_result ] can be made to hold with probability @xmath38 , for any @xmath39 , by repeating the algorithm @xmath40 times .",
    "for example , one could run ten independent copies of the algorithm and keep the vector @xmath29 that minimizes the residual .",
    "this clearly does not increase the running time of the algorithm by more than a constant factor , while driving the failure probability down to ( approximately ) @xmath41 .",
    "also , we will assume that @xmath0 is a power of two and that the rank of the @xmath42 matrix @xmath15 equals @xmath1 .",
    "( we note that padding @xmath15 and @xmath16 with all - zero rows suffices to remove the first assumption . )",
    "we now provide a brief overview of our main algorithms .",
    "let the matrix product @xmath43 denote the @xmath44 randomized hadamard transform ( see also section  [ sxn : rht ] ) . here",
    "the @xmath44 matrix @xmath45 denotes the ( normalized ) matrix of the hadamard transform and the @xmath44 diagonal matrix @xmath46 is formed by setting its diagonal entries to @xmath47 or @xmath48 with equal probability in @xmath0 independent trials .",
    "this transform has been used as one step in the development of a `` fast '' version of the johnson - lindenstrauss lemma  @xcite .",
    "our first algorithm is a random sampling algorithm .",
    "after premultiplying @xmath15 and @xmath16 by @xmath43 , this algorithm samples uniformly at random @xmath49 constraints from the preprocessed problem .",
    "( see eqn .",
    "( [ eqn : rvaluefinal ] ) , as well as the remarks after theorem  [ thm : alg_sample_fast ] for the precise value of @xmath49 . )",
    "then , this algorithm solves the least squares problem on just those sampled constraints to obtain a vector @xmath27 such that theorem  [ thm : main_result ] is satisfied .",
    "note that applying the randomized hadamard transform to the matrix @xmath15 and vector @xmath16 only takes @xmath50 time .",
    "this follows since we will actually sample only @xmath49 of the constraints from the hadamard - preprocessed problem  @xcite .",
    "then , exactly solving the @xmath51 sampled least - squares problem will require only @xmath52 time .",
    "assuming that @xmath37 is a constant and @xmath53 , it follows that the running time of this algorithm is @xmath54 when @xmath55 .    in a similar manner , our second algorithm also initially premultiplies @xmath15 and @xmath16 by @xmath43 .",
    "this algorithm then multiplies the result by a @xmath56 sparse projection matrix @xmath57 , where @xmath58 .",
    "this matrix @xmath57 is described in detail in section  [ sxn : review_previous : projecting ] .",
    "its construction depends on a sparsity parameter , and it is identical to the `` sparse projection '' matrix in matouek s version of the ailon - chazelle result  @xcite . finally , our second algorithm solves the least squares problem on just those @xmath59 coordinates to obtain @xmath27 such that the three claims of theorem  [ thm : main_result ] are satisfied . assuming that @xmath37 is a constant and @xmath53",
    ", it follows that the running time of this algorithm is @xmath54 when @xmath60 .",
    "it is worth noting that our second algorithm has a ( marginally ) less restrictive assumption on the connection between @xmath0 and @xmath1 .",
    "however , the first algorithm is simpler to implement and easier to describe .",
    "clearly , an interesting open problem is to relax the above constraints on @xmath0 for either of the proposed algorithms .",
    "we should note several lines of related work .    * first",
    ", techniques such as the `` method of averages ''  @xcite preprocess the input into the form of eqn .",
    "( [ eqn : orig_ls_prob_xrotated ] ) of section  [ sxn : precond ] and can be used to obtain exact or approximate solutions to the least squares problem of eqn .",
    "( [ eqn : orig_ls_prob ] ) in @xmath61 time under strong statistical assumptions on @xmath15 and @xmath16 . to the best of our knowledge",
    ", however , the two algorithms we present and analyze are the first algorithms to provide nontrivial approximation guarantees for overconstrained least squares approximation problems in @xmath61 time , while making no assumptions at all on the input data .",
    "* second , ibarra , moran , and hui  @xcite provide a reduction of the least squares approximation problem to the matrix multiplication problem .",
    "in particular , they show that @xmath62 time , where @xmath63 is the time needed to multiply two @xmath64 matrices , is sufficient to solve this problem .",
    "all of the running times we report in this paper assume the use of standard matrix multiplication algorithms , since @xmath65 matrix multiplication algorithms are almost never used in practice .",
    "moreover , even with the current best value for the matrix multiplication exponent , @xmath66  @xcite , our algorithms are still faster . *",
    "third , motivated by our preliminary results as reported in  @xcite and  @xcite , both rokhlin and tygert  @xcite as well as avron , maymounkov , and toledo  @xcite have empirically evaluated numerical implementations of variants of one of the algorithms we introduce .",
    "we describe this in more detail below in section  [ sxn : intro - empirical ] . *",
    "fourth , very recently , clarkson and woodruff proved space lower bounds on related problems  @xcite ; and nguyen , do , and tran achieved a small improvement in the sampling complexity for related problems  @xcite .",
    "in prior work we have empirically evaluated randomized algorithms that rely on the ideas that we introduce in this paper in several large - scale data analysis tasks .",
    "nevertheless , it is a fair question to ask whether our `` random perspective '' on linear algebra will work well in numerical implementations of interest in scientific computation .",
    "we address this question here .",
    "although we do _ not _ provide an empirical evaluation in this paper , in the wake of the original technical report version of this paper in 2007  @xcite , two groups of researchers have demonstrated that numerical implementations of variants of the algorithms we introduce in this paper can perform very well in practice .    * in 2008 , rokhlin and tygert  @xcite describe a variant of our random projection algorithm , and they demonstrate that their algorithm runs in time @xmath67 where @xmath68 is an `` oversampling '' parameter and @xmath69 is a condition number . importantly ( at least for very high - precision applications of this random sampling methodology ) , they reduce the dependence on @xmath37 from @xmath70 to @xmath71 .",
    "moreover , by choosing @xmath72 , they demonstrate that @xmath73 .",
    "although this bound is inferior to ours , they also consider a class of matrices for which choosing @xmath74 empirically produced a condition number @xmath75 , which means that for this class of matrices their running time is @xmath76 their numerical experiments on this class of matrices clearly indicate that their implementations of variants of our algorithms perform well for certain matrices as small as thousands of rows by hundreds of columns . * in 2009 , avron , maymounkov , toledo  @xcite introduced a randomized least - squares solver based directly on our algorithms .",
    "they call it blendenpik , and by considering a much broader class of matrices , they demonstrate that their solver `` beats lapack s direct dense least - sqares solver by a large margin on essentially any dense tall matrix . '' beyond providing additional theoretical analysis , including backward error analysis bounds for our algorithm , they consider five ( and numerically implement three ) random projection strategies ( i.e. , discrete fourier transform , discrete cosine transform , discrete hartely transform , walsh - hadamard transform , and a kac random walk ) , and they evaluate their algorithms on a wide range of matrices of various sizes and various `` localization '' or `` coherence '' properties .",
    "based on these results that empirically show the superior performance of randomized algorithms such as those we introduce and analyze in this paper on a wide class of matrices , they go so far as to `` suggest that random - projection algorithms should be incorporated into future versions of lapack . ''      after a brief review of relevant background in section  [ sxn : review_la ] , section  [ sxn : precond ] presents a structural result outlining conditions on preconditioner matrices that are sufficient for relative - error approximation . then , we present our main sampling - based algorithm for approximating least squares approximation in section  [ sxn : sampling ] and in section  [ sxn : projection ] we present a second projection - based algorithm for the same problem .",
    "preliminary versions of parts of this paper have appeared as conference proceedings in the 17th acm - siam symposium on discrete algorithms  @xcite and in the 47th ieee symposium on foundations of computer science  @xcite ; and the original technical report version of this journal paper has appeared on the arxiv  @xcite . in particular ,",
    "the core of our analysis in this paper was introduced in  @xcite , where an expensive - to - compute probability distribution was used to construct a relative - error approximation sampling algorithm for the least squares approximation problem .",
    "then , after the development of the fast johnson - lindenstrauss transform  @xcite , @xcite proved that similar ideas could be used to improve the running time of randomized algorithms for the least squares approximation problem . in this paper , we have combined these ideas , treated the two algorithms in a manner to highlight their similarities and differences , and considerably simplified the analysis .",
    "we let @xmath77 $ ] denote the set @xmath78 ; @xmath79 denotes the natural logarithm of @xmath9 and @xmath80 denotes the base two logarithm of @xmath9 . for any matrix @xmath5 ,",
    "@xmath81 $ ] denotes the @xmath82-th row of @xmath15 as a row vector and @xmath83 $ ] denotes the @xmath84-th column of @xmath15 as a column vector .",
    "also , given a random variable @xmath85 , we let @xmath86}$ ] denote its expectation and @xmath87}$ ] denote its variance .    we will make frequent use of matrix and vector norms .",
    "more specifically , we let @xmath88 denote the square of the frobenius norm of @xmath15 , and we let @xmath89 denote the spectral norm of @xmath15 . for any vector @xmath90 ,",
    "its @xmath12-norm ( or euclidean norm ) is equal to the square root of the sum of the squares of its elements , while its @xmath91 norm is defined as @xmath92 } \\abs{x_i}$ ] .",
    "we now review relevant definitions and facts from linear algebra ; for more details , see  @xcite . let the rank of @xmath5 be @xmath93 . the singular value decomposition ( svd ) of @xmath15 is denoted by @xmath94 , where @xmath95 is the matrix of left singular vectors",
    ", @xmath96 is the diagonal matrix of non - zero singular values , and @xmath97 is the matrix of right singular vectors . let @xmath98 $ ] , denote the @xmath82-th non - zero singular value of @xmath15 , and @xmath99 and @xmath100 denote the maximum and minimum singular value of @xmath15 .",
    "the condition number of @xmath15 is @xmath101 .",
    "the moore - penrose generalized inverse , or pseudoinverse , of @xmath15 may be expressed in terms of the svd as @xmath102  @xcite .",
    "finally , for any orthogonal matrix @xmath103 , let @xmath104 denote an orthogonal matrix whose columns are an orthonormal basis spanning the subspace of @xmath105 that is orthogonal to the column space of @xmath106 . in terms of @xmath107 , the optimal value of the least squares residual of eqn .",
    "( [ eqn : orig_ls_prob ] ) is @xmath108      we will make frequent use of the following fundamental result from probability theory , known as markov s inequality  @xcite .",
    "let @xmath85 be a random variable assuming non - negative values with expectation @xmath86}$ ] .",
    "then , for all @xmath109 , @xmath110}\\ ] ] with probability at least @xmath111",
    ".    we will also need the so - called union bound .",
    "given a set of random events @xmath112 holding with respective probabilities @xmath113 , the probability that all events hold ( i.e. , the probability of the union of those events ) is upper bounded by @xmath114 .",
    "the randomized hadamard transform was introduced in  @xcite as one step in the development of a fast version of the johnson - lindenstrauss lemma  @xcite .",
    "recall that the ( non - normalized ) @xmath44 matrix of the hadamard transform @xmath115 may be defined recursively as follows : @xmath116    , \\qquad \\mbox{with } \\qquad h_2 = \\left [ \\begin{array}{cc }    + 1 & + 1 \\\\    + 1 & -1 \\end{array}\\right].\\ ] ]",
    "the @xmath44 normalized matrix of the hadamard transform is equal to @xmath117 ; hereafter , we will denote this normalized matrix by @xmath45 . now consider a diagonal matrix @xmath118 such that the diagonal entries @xmath119 are set to + 1 with probability @xmath120 and to @xmath48 with probability @xmath120 in @xmath0 independent trials .",
    "the product @xmath43 is the randomized hadamard transform and has two useful properties .",
    "first , when applied to a vector , it `` spreads out '' its energy , in the sense of providing a bound for its infinity norm ( see section  [ sxn : review_previous : hadamard ] ) .",
    "second , computing the product @xmath121 for any vector @xmath90 takes @xmath122 time .",
    "even better , if we only need to access , say , @xmath49 elements in the transformed vector , then those @xmath49 elements can be computed in @xmath123 time  @xcite .",
    "we will expand on the latter observation in the proofs of theorems  [ thm : alg_sample_fast ] and  [ thm : alg_projection_fast ] .",
    "both of our algorithms may be viewed as preconditioning the input matrix @xmath15 and the target vector @xmath16 with a carefully - constructed data - independent random matrix @xmath85 . for our random sampling algorithm ,",
    "we let @xmath124 , where @xmath125 is a matrix that represents the sampling operation and @xmath43 is the randomized hadamard transform , while for our random projection algorithm , we let @xmath126 , where @xmath57 is a random projection matrix .",
    "thus , we replace the least squares approximation problem of eqn .",
    "( [ eqn : orig_ls_prob ] ) with the least squares approximation problem @xmath127 we explicitly compute the solution to the above problem using a traditional deterministic algorithm  @xcite , e.g. , by computing the vector @xmath128 alternatively , one could use standard iterative methods such as the the conjugate gradient normal residual method ( cgnr , see  @xcite for details ) , which can produce an @xmath37-approximation to the optimal solution of eqn .",
    "( [ eqn : orig_ls_prob_xrotated ] ) in @xmath129 time , where @xmath130 is the condition number of @xmath131 and @xmath49 is the number of rows of @xmath131 .      in this subsection",
    ", we will state and prove a lemma that establishes sufficient conditions on any matrix @xmath85 such that the solution vector @xmath29 to the least squares problem of eqn .",
    "( [ eqn : orig_ls_prob_xrotated ] ) will satisfy relative - error bounds of the form  ( [ eqn : result1_intro ] ) and  ( [ eqn : result2_intro ] ) . recall that the svd of @xmath15 is @xmath132 .",
    "in addition , for notational simplicity , we let @xmath133 denote the part of the right hand side vector @xmath16 lying outside of the column space of @xmath15 .",
    "the two conditions that we will require of the matrix @xmath85 are : @xmath134 for some @xmath26 .",
    "several things should be noted about these conditions .",
    "first , although condition  ( [ eqn : lemma1_ass2 ] ) depends on the right hand side vector @xmath16 , algorithms  [ alg : alg_sample_fast ] and  [ alg : alg_projection_fast ] will satisfy it without using any information from @xmath16 .",
    "second , although condition  ( [ eqn : lemma1_ass1 ] ) only states that @xmath135 , for all @xmath136 $ ] , for both of our randomized algorithms we will show that @xmath137 , for all @xmath136 $ ] .",
    "thus , one should think of @xmath138 as an approximate isometry .",
    "third , condition  ( [ eqn : lemma1_ass2 ] ) simply states that @xmath139 remains approximately orthogonal to @xmath138 .",
    "finally , note that the following lemma is a deterministic statement , since it makes no explicit reference to either of our randomized algorithms .",
    "failure probabilities will enter later when we show that our randomized algorithms satisfy conditions  ( [ eqn : lemma1_ass1 ] ) and  ( [ eqn : lemma1_ass2 ] ) .",
    "[ lem : suff_cond ] consider the overconstrained least squares approximation problem of eqn .",
    "( [ eqn : orig_ls_prob ] ) and let the matrix @xmath140 contain the top @xmath1 left singular vectors of @xmath15 .",
    "assume that the matrix @xmath85 satisfies conditions  ( [ eqn : lemma1_ass1 ] ) and  ( [ eqn : lemma1_ass2 ] ) above , for some @xmath26 .",
    "then , the solution vector @xmath29 to the least squares approximation problem  ( [ eqn : orig_ls_prob_xrotated ] ) satisfies : @xmath141    let us first rewrite the down - scaled regression problem induced by @xmath85 as @xmath142 ( [ eqn : ds1 ] ) follows since @xmath143 and ( [ eqn : ds2 ] ) follows since the columns of the matrix @xmath15 span the same subspace as the columns of @xmath34 .",
    "now , let @xmath144 be such that @xmath145 , and note that @xmath146 minimizes eqn .",
    "( [ eqn : ds2 ] ) .",
    "the latter fact follows since @xmath147 thus , by the normal equations  ( [ eqn : normal_eqn ] ) , we have that @xmath148 taking the norm of both sides and observing that under condition  ( [ eqn : lemma1_ass1 ] ) we have @xmath149 , for all @xmath82 , it follows that @xmath150 using condition  ( [ eqn : lemma1_ass2 ] ) we observe that @xmath151    to establish the first claim of the lemma , let us rewrite the norm of the residual vector as @xmath152 where ( [ eqn : pfceq1 ] ) follows by pythagoras , since @xmath153 , which is orthogonal to @xmath15 , and consequently to @xmath154 ; ( [ eqn : pfceq2 ] ) follows by the definition of @xmath146 and @xmath155 ; and ( [ eqn : pfceq3 ] ) follows by ( [ eqn : z - norm2 ] ) and the orthogonality of @xmath34 .",
    "the first claim of the lemma follows since @xmath156 .    to establish the second claim of the lemma ,",
    "recall that @xmath157 .",
    "if we take the norm of both sides of this expression , we have that @xmath158 where ( [ eqn : pfdeq1 ] ) follows since @xmath159 is the smallest singular value of @xmath15 and since the rank of @xmath15 is @xmath1 ; and ( [ eqn : pfdeq2 ] ) follows by ( [ eqn : z - norm2 ] ) and the orthogonality of @xmath34 . taking the square root , the second claim of the lemma follows .",
    "if we make no assumption on @xmath16 , then  ( [ eqn : lemma1_eq4 ] ) from lemma  [ lem : suff_cond ] may provide a weak bound in terms of @xmath160 .",
    "if , on the other hand , we make the additional assumption that a constant fraction of the norm of @xmath16 lies in the subspace spanned by the columns of @xmath15 , then  ( [ eqn : lemma1_eq4 ] ) can be strengthened .",
    "such an assumption is reasonable , since most least - squares problems are practically interesting if at least some part of @xmath16 lies in the subspace spanned by the columns of @xmath15 .",
    "[ lem : suff_cond2 ] using the notation of lemma  [ lem : suff_cond ] and assuming that @xmath161 , for some fixed @xmath162 $ ] it follows that @xmath163    since @xmath161 , it follows that @xmath164 this last inequality follows from @xmath165 , which implies @xmath166 by combining this with eqn .",
    "( [ eqn : lemma1_eq4 ] ) of lemma  [ lem : suff_cond ] , the lemma follows .",
    "in this section , we present our randomized sampling algorithm for the least squares approximation problem of eqn .",
    "( [ eqn : orig_ls_prob ] ) . we also state and prove an associated quality - of - approximation theorem .",
    "algorithm  [ alg : alg_sample_fast ] takes as input a matrix @xmath5 , a vector @xmath6 , and an error parameter @xmath26 .",
    "this algorithm starts by preprocessing the matrix @xmath15 and the vector @xmath16 with the randomized hadamard transform .",
    "it then constructs a smaller problem by sampling uniformly at random a small number of constraints from the preprocessed problem .",
    "our main quality - of - approximation theorem ( theorem  [ thm : alg_sample_fast ] below ) states that with constant probability over the random choices made by the algorithm , the vector @xmath29 returned by this algorithm will satisfy the relative - error bounds of eqns .",
    "( [ eqn : result1_intro ] ) and  ( [ eqn : result2_intro ] ) and will be computed quickly .",
    "* input : * @xmath5 , @xmath167 , and an error parameter @xmath26 .",
    "* output : * @xmath27 .    1 .",
    "let @xmath49 assume the value of eqn .",
    "( [ eqn : rvaluefinal ] ) .",
    "2 .   let @xmath125 be an empty matrix .",
    "3 .   * for * @xmath168 ( i.i.d .",
    "trials with replacement ) * select uniformly at random * an integer from @xmath169 . * * if * @xmath82 is selected , * then * append the column vector @xmath170 to @xmath125 , where @xmath171 is an all - zeros vector except for its @xmath82-th entry which is set to one .",
    "4 .   let @xmath172 be the normalized hadamard transform matrix .",
    "let @xmath118 be a diagonal matrix with @xmath173 6 .",
    "compute and return @xmath174 .    in more detail , after preprocessing with the randomized hadamard transform of section  [ sxn : rht ] , algorithm  [ alg : alg_sample_fast ] samples exactly @xmath49 constraints from the preprocessed least squares problem , rescales each sampled constraint by @xmath175 , and solves the least squares problem induced on just those sampled and rescaled constraints .",
    "( note that the algorithm explicitly computes only those rows of @xmath176 and only those elements of @xmath177 that need to be accessed . ) more formally , we will let @xmath178 denote a sampling matrix specifying which of the @xmath0 constraints are to be sampled and how they are to be rescaled .",
    "this matrix is initially empty and is constructed as described in algorithm  [ alg : alg_sample_fast ] .",
    "then , we can consider the problem @xmath179 which is just a least squares approximation problem involving the @xmath49 constraints sampled from the matrix @xmath15 after the preprocessing with the randomized hadamard transform .",
    "the minimum @xmath12-norm vector @xmath27 among those that achieve the minimum value @xmath180 in this problem is @xmath181 which is the output of algorithm  [ alg : alg_sample_fast ] .",
    "[ thm : alg_sample_fast ] suppose @xmath5 , @xmath25 , and let @xmath26 .",
    "run algorithm  [ alg : alg_sample_fast ] with @xmath182 and return @xmath29 .",
    "then , with probability at least .8 , the following two claims hold : first , @xmath29 satisfies @xmath183 and , second , if we assume that @xmath184 for some @xmath162 $ ] , then @xmath29 satisfies @xmath185 finally , @xmath186 time suffices to compute the solution @xmath29 .",
    "* remark : * assuming that @xmath187 , and using @xmath188 , we get that @xmath189 thus , the running time of algorithm  [ alg : alg_sample_fast ] becomes @xmath190 assuming that @xmath55 , the above running time reduces to @xmath191 it is worth noting that improvements over the standard @xmath3 time could be derived with weaker assumptions on @xmath0 and @xmath1 .",
    "however , for the sake of clarity of presentation , we only focus on the above setting .",
    "* remark : * the assumptions in our theorem have a natural geometric interpretation . in particular , they imply that our approximation becomes worse as the angle between the vector @xmath16 and the column space of @xmath15 increases . to see this ,",
    "let @xmath192 , and note that @xmath193 .",
    "hence the assumption @xmath194 can be simply stated as @xmath195 the fraction @xmath196 is the sine of the angle between @xmath16 and the column space of @xmath15 ; see page 242 of  @xcite .",
    "thus , @xmath197 is a bound on the tangent between @xmath16 and the column space of @xmath15 ; see page 244 of  @xcite .",
    "this means that the bound for @xmath198 is proportional to this tangent .      in this subsection , we state a lemma that quantifies the manner in which @xmath43 approximately `` uniformizes '' information in the left singular subspace of the matrix @xmath15 .",
    "we state the lemma for a general @xmath42 orthogonal matrix @xmath106 such that @xmath199 , although we will be interested in the case when @xmath2 and @xmath106 consists of the top @xmath1 left singular vectors of the matrix @xmath15 .",
    "[ lem : hu ] let @xmath106 be an @xmath42 orthogonal matrix and let the product @xmath43 be the @xmath44 randomized hadamard transform of section  [ sxn : rht ] . then , with probability at least @xmath200 , @xmath201    .",
    "we follow the proof of lemma 2.1 in  @xcite . in that lemma",
    ", the authors essentially prove that the randomized hadamard transform @xmath43 `` spreads out '' input vectors . more specifically , since the columns of the matrix @xmath106 ( denoted by @xmath202 for all @xmath203 $ ] ) are unit vectors , they prove that for fixed @xmath203 $ ] and fixed @xmath204 $ ] , @xmath205}\\leq 2 e^{-s^2 n/2}.\\ ] ] ( note that we consider @xmath1 vectors in @xmath105 whereas  @xcite considered @xmath0 vectors in @xmath206 and thus the roles of @xmath0 and @xmath1 are inverted in our proof . ) let @xmath207 to get @xmath208}\\leq \\frac{1}{20nd}.\\ ] ] from a standard union bound , this immediately implies that with probability at least @xmath209 , @xmath210 holds for all @xmath204 $ ] and @xmath203 $ ] . using @xmath211 for all @xmath204 $ ]",
    ", we conclude the proof of the lemma .",
    "we now establish the following lemma which states that all the singular values of @xmath212 are close to one .",
    "the proof of lemma  [ lem : sample_lem20pf ] depends on a bound for approximating the product of a matrix times its transpose by sampling ( and rescaling ) a small number of columns of the matrix .",
    "this bound appears as theorem  [ thm : theorem7correct ] in the appendix and is an improvement over prior work of ours in  @xcite .",
    "[ lem : sample_lem20pf ] assume that eqn .",
    "( [ eqn : lem : hu_eqn2 ] ) holds .",
    "if @xmath213 then , with probability at least .95 , @xmath214 holds for all @xmath136 $ ] .",
    "note that for all @xmath136 $ ] @xmath215 in the above , we used the fact that @xmath216 .",
    "we now can view @xmath217 as an approximation to the product of two matrices @xmath218 and @xmath219 by randomly sampling and rescaling columns of @xmath220 .",
    "thus , we can leverage theorem  [ thm : theorem7correct ] from the appendix . more specifically , consider the matrix @xmath220 . obviously , since @xmath45 , @xmath46 , and @xmath34 are orthogonal matrices , @xmath221 and @xmath222 .",
    "let @xmath223 ; since we assumed that eqn .",
    "( [ eqn : lem : hu_eqn2 ] ) holds , we note that the columns of @xmath220 , which correspond to the rows of @xmath219 , satisfy @xmath224    .\\ ] ] thus , applying theorem  [ thm : theorem7correct ] with @xmath225 as above , @xmath226 , and @xmath227 implies that @xmath228 holds with probability at least @xmath229 . for the above bound to hold , we need @xmath49 to assume the value of eqn .",
    "( [ eqn : rvalue ] ) .",
    "finally , we note that since @xmath230 , the assumption of theorem  [ thm : theorem7correct ] on the frobenius norm of the input matrix is always satisfied . combining the above with inequality  ( [ eqn : eqx31 ] )",
    "concludes the proof of the lemma .",
    "we next prove the following lemma , from which it will follow that condition  ( [ eqn : lemma1_ass2 ] ) is satisfied by algorithm  [ alg : alg_sample_fast ] .",
    "the proof of this lemma depends on bounds for randomized matrix multiplication algorithms that appeared in  @xcite .",
    "[ lem : sample_lem40pf ] if eqn .",
    "( [ eqn : lem : hu_eqn2 ] ) holds and @xmath231 , then with probability at least .9 , @xmath232    recall that @xmath133 and that @xmath233 .",
    "we start by noting that since @xmath234 it follows that @xmath235 thus , we can view @xmath236 as approximating the product of two matrices @xmath237 and @xmath238 by randomly sampling columns from @xmath220 and rows / elements from @xmath238 .",
    "note that the sampling probabilities are uniform and do not depend on the norms of the columns of @xmath237 or the rows of @xmath239 .",
    "however , we can still apply the results of table 1 ( second row ) in page 150 of  @xcite . more specifically , since we condition on eqn .",
    "( [ eqn : lem : hu_eqn2 ] ) holding , the rows of @xmath219 ( which of course correspond to columns of @xmath220 ) satisfy @xmath240,\\ ] ] for @xmath223 . applying the result of table 1 ( second row ) of  @xcite we get @xmath241 }     \\leq \\frac{1}{\\beta r}{\\mbox{}\\left\\|hdu_a\\right\\|_f^2}{\\mbox{}\\left\\|hdb^{\\perp}\\right\\|_2 ^ 2 }     =      \\frac{d{\\cal z}^2}{\\beta r}.\\ ] ] in the above we used @xmath242 .",
    "markov s inequality now implies that with probability at least .9 , @xmath243 setting @xmath244 and using the value of @xmath225 specified above concludes the proof of the lemma .",
    "we now complete the proof of theorem  [ thm : alg_sample_fast ] .",
    "first , let @xmath245 denote the event that eqn .",
    "( [ eqn : lem : hu_eqn2 ] ) holds ; clearly , @xmath246 } \\geq .95 $ ] .",
    "second , let @xmath247 denote the event that both lemmas  [ lem : sample_lem20pf ] and  [ lem : sample_lem40pf ] hold conditioned on @xmath245 holding .",
    "then , @xmath248}\\\\ & \\geq & 1 - { \\mbox{}{\\bf{pr}}\\left[\\mbox{lemma \\ref{lem : sample_lem20pf } does not hold $ |$ \\cal e}_{(\\ref{eqn : lem : hu_eqn2})}\\right ] } -{\\mbox{}{\\bf{pr}}\\left[\\mbox{lemma \\ref{lem : sample_lem40pf } does not hold $ |$ \\cal e}_{(\\ref{eqn : lem : hu_eqn2})}\\right]}\\\\ & \\geq & 1 - .05 - .1 = .85 . \\ ] ] in the above , @xmath249 denotes the complement of event @xmath250 . in the first inequality we used the union bound and in the second inequality we leveraged the bounds for the failure probabilities of lemmas  [ lem : sample_lem20pf ] and  [ lem : sample_lem40pf ] given that eqn .",
    "( [ eqn : lem : hu_eqn2 ] ) holds .",
    "we now let @xmath250 denote the event that both lemmas  [ lem : sample_lem20pf ] and  [ lem : sample_lem40pf ] hold , without any a priori conditioning on event @xmath245 ; we will bound @xmath251}$ ] as follows : @xmath252 } & = & { \\mbox{}{\\bf{pr}}\\left[{\\cal e } | { \\cal e}_{(\\ref{eqn : lem : hu_eqn2})}\\right]}\\cdot { \\mbox{}{\\bf{pr}}\\left[{\\cal e}_{(\\ref{eqn : lem : hu_eqn2})}\\right ] } + { \\mbox{}{\\bf{pr}}\\left[{\\cal e } | \\overline{{\\cal e}_{(\\ref{eqn : lem : hu_eqn2})}}\\right]}\\cdot { \\mbox{}{\\bf{pr}}\\left[\\overline{{\\cal e}_{(\\ref{eqn : lem : hu_eqn2})}}\\right]}\\\\ & \\geq & { \\mbox{}{\\bf{pr}}\\left[{\\cal e } | { \\cal e}_{(\\ref{eqn : lem : hu_eqn2})}\\right]}\\cdot { \\mbox{}{\\bf{pr}}\\left[{\\cal e}_{(\\ref{eqn : lem : hu_eqn2})}\\right]}\\\\ & = & { \\mbox{}{\\bf{pr}}\\left[{\\cal e}_{\\ref{lem : sample_lem20pf},\\ref{lem : sample_lem40pf}|(\\ref{eqn : lem : hu_eqn2 } ) } | { \\cal e}_{(\\ref{eqn : lem : hu_eqn2})}\\right]}\\cdot { \\mbox{}{\\bf{pr}}\\left[{\\cal e}_{(\\ref{eqn : lem : hu_eqn2})}\\right]}\\\\ & \\geq & .85\\cdot .95 \\geq .8 . \\ ] ] in the first inequality we used the fact that all probabilities are positive .",
    "the above derivation immediately bounds the success probability of theorem  [ thm : alg_sample_fast ] . combining lemmas  [ lem : sample_lem20pf ] and  [ lem : sample_lem40pf ] with the structural results of lemma  [ lem : suff_cond ] and setting @xmath49 as in eqn .",
    "( [ eqn : rvaluefinal ] ) concludes the proof of the accuracy guarantees of theorem  [ thm : alg_sample_fast ] .",
    "we now discuss the running time of algorithm  [ alg : alg_sample_fast ] .",
    "first of all , by the construction of @xmath125 , the number of non - zero entries in @xmath125 is @xmath49 . in step",
    "@xmath253 we need to compute the products @xmath254 and @xmath255 .",
    "recall that @xmath15 has @xmath1 columns and thus the running time of computing both products is equal to the time needed to apply @xmath256 on @xmath257 vectors .",
    "first , note that in order to apply @xmath46 on @xmath257 vectors in @xmath105 , @xmath258 operations suffice . in order to estimate how many operations",
    "are needed to apply @xmath259 on @xmath257 vectors , we use the results of theorem  @xmath260 ( see also section 7 ) of ailon and liberty  @xcite , which state that at most @xmath261 operations are needed for this operation . here",
    "@xmath262 denotes the number of non - zero elements in the matrix @xmath125 , which is at most @xmath49 . after this preprocessing ,",
    "algorithm  [ alg : alg_sample_fast ] must compute the pseudoinverse of an @xmath51 matrix , or , equivalently , solve a least - squares problem on @xmath49 constraints and @xmath1 variables .",
    "this operation can be performed in @xmath52 time since @xmath263 .",
    "thus , the entire algorithm runs in time @xmath264",
    "in this section , we present a projection - based randomized algorithm for the least squares approximation problem of eqn .",
    "( [ eqn : orig_ls_prob ] ) . we also state and prove an associated quality - of - approximation theorem .",
    "algorithm  [ alg : alg_projection_fast ] takes as input a matrix @xmath5 , a vector @xmath6 , and an error parameter @xmath265 .",
    "this algorithm also starts by preprocessing the matrix @xmath15 and right hand side vector @xmath16 with the randomized hadamard transform .",
    "it then constructs a smaller problem by performing a `` sparse projection '' on the preprocessed problem .",
    "our main quality - of - approximation theorem ( theorem  [ thm : alg_projection_fast ] below ) will state that with constant probability ( over the random choices made by the algorithm ) the vector @xmath29 returned by this algorithm will satisfy the relative - error bounds of eqns .",
    "( [ eqn : result1_intro ] ) and  ( [ eqn : result2_intro ] ) and will be computed quickly .",
    "* input : * @xmath5 , @xmath6 , and an error parameter @xmath265 .    * output : * @xmath27 .    1 .",
    "let @xmath266 and @xmath59 assume the values of eqns .",
    "( [ eqn : valueqfinal ] ) and  ( [ eqn : valuekfinal ] ) .",
    "2 .   let @xmath267 be a random matrix with @xmath268 for all @xmath269 independently .",
    "3 .   let @xmath172 be the normalized hadamard transform matrix .",
    "4 .   let @xmath118 be a diagonal matrix with @xmath173 5 .",
    "compute and return @xmath270 .    in more detail ,",
    "algorithm  [ alg : alg_projection_fast ] begins by preprocessing the matrix @xmath15 and right hand side vector @xmath16 with the randomized hadamard transform @xmath43 of section  [ sxn : rht ] .",
    "this algorithm explicitly computes only those rows of @xmath176 and those elements of @xmath177 that need to be accessed to perform the sparse projection . after this initial preprocessing ,",
    "algorithm  [ alg : alg_projection_fast ] will perform a `` sparse projection '' by multiplying @xmath176 and @xmath177 by the sparse matrix @xmath57 ( described in more detail in section  [ sxn : review_previous : projecting ] ) .",
    "then , we can consider the problem @xmath271 which is just a least squares approximation problem involving the matrix @xmath272 and the vector @xmath273 .",
    "the minimum @xmath12-norm vector @xmath27 among those that achieve the minimum value @xmath180 in this problem is @xmath274 which is the output of algorithm  [ alg : alg_projection_fast ] .",
    "[ thm : alg_projection_fast ] suppose @xmath5 , @xmath25 , and let @xmath265 .",
    "run algorithm  [ alg : alg_projection_fast ] with and @xmath275 are the unspecified constants of lemma  [ lem : matousek ] . ] @xmath276 and return @xmath29 .",
    "then , with probability at least @xmath28 , the following two claims hold : first , @xmath29 satisfies @xmath277 and , second , if we assume that @xmath184 for some @xmath162 $ ] then @xmath29 satisfies @xmath185 finally , the expected running time of the algorithm is ( at most ) @xmath278    * remark : * assuming that @xmath187 we get that @xmath279 thus , the expected running time of algorithm  [ alg : alg_projection_fast ] becomes @xmath280 finally , assuming @xmath60 , the above running time reduces to @xmath281 it is worth noting that improvements over the standard @xmath3 time could be derived with weaker assumptions on @xmath0 and @xmath1 .      in this subsection ,",
    "we state a lemma about the action of a sparse random matrix operating on a vector . recall that given any set of @xmath0 points in euclidean space ,",
    "the johnson - lindenstrauss lemma states that those points can be mapped via a linear function to @xmath282 dimensions such that the distances between all pairs of points are preserved to within a multiplicative factor of @xmath283 ; see  @xcite and references therein for details .",
    "formally , let @xmath265 be an error parameter , @xmath284 be a failure probability , and @xmath285 $ ] be a `` uniformity '' parameter .",
    "in addition , let @xmath266 be a `` sparsity '' parameter defining the expected number of nonzero elements per row , and let @xmath59 be the number of rows in our matrix .",
    "then , define the @xmath56 random matrix @xmath57 as in algorithm [ alg : alg_projection_fast ] .",
    "matouek proved the following lemma , as the key step in his version of the ailon - chazelle result  @xcite .",
    "[ lem : matousek ] let @xmath57 be the sparse random matrix of algorithm  [ alg : alg_projection_fast ] , where @xmath286 for some sufficiently large constant @xmath287 ( but still such that @xmath288 ) , and @xmath289 ) for some sufficiently large constant @xmath275 ( but such that @xmath59 is integral ) . then for every vector @xmath290 such that @xmath291",
    ", we have that with probability at least @xmath38 @xmath292    * remark : * in order to achieve sufficient concentration for all vectors @xmath290 , the linear mapping defining the johnson - lindenstrauss transform is typically `` dense , '' in the sense that almost all the elements in each of the @xmath59 rows of the matrix defining the mapping are nonzero . in this case , implementing the mapping on @xmath1 vectors ( in , e.g. , a matrix @xmath15 ) via a matrix multiplication requires @xmath293 time .",
    "this is not faster than the @xmath3 time required to compute an exact solution to the problem of eqn .",
    "( [ eqn : orig_ls_prob ] ) if @xmath59 is at least @xmath1 .",
    "the ailon - chazelle result  @xcite states that the mapping can be `` sparse , '' in the sense that only a few of the elements in each of the @xmath59 rows need to be nonzero , provided that the vector @xmath9 is `` well - spread , '' in the sense that @xmath294 is close to @xmath295 .",
    "this is exactly what the preprocessing with the randomized hadamard transform guarantees .      in this subsection",
    ", we provide a proof of theorem  [ thm : alg_projection_fast ] .",
    "recall that by the results of section  [ sxn : sampling : proofs : structural ] , in order to prove theorem  [ thm : alg_projection_fast ] , we must show that the matrix @xmath296 constructed by algorithm  [ alg : alg_projection_fast ] satisfies conditions  ( [ eqn : lemma1_ass1 ] ) and  ( [ eqn : lemma1_ass2 ] ) with probability at least @xmath297 .",
    "the next two subsections focus on proving that these conditions hold ; the last subsection discusses the running time of algorithm  [ alg : alg_projection_fast ] .      in order to prove that all the singular values of @xmath298 are close to one , we start with the following lemma which provides a means to bound the spectral norm of a matrix .",
    "this lemma is an instantiation of lemmas that appeared in  @xcite .",
    "[ lem : arorahazenkale ] let @xmath299 be a @xmath64 symmetric matrix and define the grid @xmath300 in words , @xmath301 includes all @xmath1-dimensional vectors @xmath9 whose coordinates are integer multiples of @xmath302 and satisfy @xmath303 .",
    "then , the cardinality of @xmath301 is at most @xmath304 .",
    "in addition , if for every @xmath305 we have that @xmath306 , then for every unit vector @xmath9 we have that @xmath307 .",
    "we next establish lemma  [ lem : project_lem20pf ] , which states that all the singular values of @xmath298 are close to one with constant probability .",
    "the proof of this lemma depends on the bound provided by lemma  [ lem : arorahazenkale ] and it immediately shows that condition  ( [ eqn : lemma1_ass1 ] ) is satisfied by algorithm  [ alg : alg_projection_fast ] .",
    "[ lem : project_lem20pf ] assume that lemma  [ lem : hu ] holds .",
    "if @xmath266 and @xmath59 satisfy : @xmath308 then , with probability at least .95 , @xmath309 holds for all @xmath136 $ ] . here",
    "@xmath287 and @xmath275 are the unspecified constants of lemma  [ lem : matousek ] .",
    "define the symmetric matrix @xmath310 , recall that @xmath311 , and note that @xmath312 holds for all @xmath136 $ ] .",
    "consider the grid @xmath301 of eqn .",
    "( [ eqn : grid ] ) and note that there are no more than @xmath313 pairs @xmath314 , since @xmath315 by lemma  [ lem : arorahazenkale ] .",
    "since @xmath316 , in order to show that @xmath317 , it suffices by lemma  [ lem : arorahazenkale ] to show that @xmath318 , for all @xmath305 .",
    "to do so , first , consider a single @xmath319 pair .",
    "let @xmath320 and note that @xmath321 by multiplying out the right hand side of the above equation and rearranging terms , it follows that @xmath322 in order to use lemma  [ lem : matousek ] to bound the quantities @xmath323 , and @xmath324 , we need a bound on the uniformity ratio @xmath325 .",
    "to do so , note that @xmath326 } \\abs{\\left(hdu_a\\right)_{(i)}x } } { \\vttnorm{hdu_ax } }        \\le \\frac{\\max_{i \\in [ n ] } \\vttnorm{\\left(hdu_a\\right)_{(i)}}\\vttnorm{x}}{\\vttnorm{x } }        \\le \\sqrt{\\frac{2d \\ln(40nd)}{n}}.\\ ] ] the above inequalities follow by @xmath327 and lemma  [ lem : hu ] .",
    "this holds for both our chosen points @xmath9 and @xmath328 and in fact for all @xmath329 .",
    "let @xmath330 and let @xmath331 ( these choices will be explained shortly ) .",
    "then , it follows from lemma  [ lem : matousek ] that by setting @xmath332 and our choices for @xmath59 and @xmath266 , each of the following three statements holds with probability at least @xmath38 : @xmath333 thus , combining the above with eqn .",
    "( [ eqn : pd1 ] ) , for this single pair of vectors @xmath334 , @xmath335 holds with probability at least @xmath336 .",
    "next , recall that there are no more than @xmath313 pairs of vectors @xmath314 , and we need eqn .",
    "( [ eqn : eqx33 ] ) to hold for all of them .",
    "since we set @xmath331 then it follows by a union bound that eqn .",
    "( [ eqn : eqx33 ] ) holds for all pairs of vectors @xmath314 with probability at least .95 .",
    "additionally , let us set @xmath330 , which implies that @xmath337 thus concluding the proof of the lemma .    finally , we discuss the values of the parameters @xmath266 and @xmath59 .",
    "since @xmath338 , @xmath339 , and @xmath332 , the appropriate values for @xmath266 and @xmath59 emerge after elementary manipulations from lemma  [ lem : matousek ] .      in order to prove that condition  ( [ eqn : lemma1_ass2 ] ) is satisfied , we start with lemma  [ lem : tmatmult ] . in words ,",
    "this lemma states that given vectors @xmath9 and @xmath328 we can use the random sparse projection matrix @xmath57 to approximate @xmath340 by @xmath341 , provided that @xmath342 ( or @xmath343 , but not necessarily both ) is bounded .",
    "the proof of this lemma is elementary but tedious and is deferred to section  [ sxn : pf_of_technical_lemma ] of the appendix .",
    "[ lem : tmatmult ] let @xmath319 be vectors in @xmath344 such that @xmath345 .",
    "let @xmath57 be the @xmath56 sparse projection matrix of section  [ sxn : review_previous : projecting ] , with sparsity parameter @xmath266 .",
    "if @xmath346 , then @xmath347 } \\le \\frac{2}{k } { \\mbox{}\\left\\|x\\right\\|_2 ^ 2 } { \\mbox{}\\left\\|y\\right\\|_2 ^ 2 } + \\frac{1}{k } { \\mbox{}\\left\\|y\\right\\|_2 ^ 2}.\\ ] ]    the following lemma proves that condition  ( [ eqn : lemma1_ass2 ] ) is satisfied by algorithm  [ alg : alg_projection_fast ] .",
    "the proof of this lemma depends on the bound provided by lemma  [ lem : tmatmult ] .",
    "recall that @xmath133 and thus @xmath348 .",
    "[ lem : project_lem40pf ] assume that eqn .",
    "( [ eqn : lem : hu_eqn2 ] ) holds .",
    "if @xmath349 and @xmath350 , then , with probability at least .9 , @xmath351    we first note that since @xmath352 , it follows that @xmath353 , for all @xmath354 $ ] .",
    "thus , we have that @xmath355 we now bound the expectation of the left hand side of eqn .",
    "( [ eqn : eqx41 ] ) by using lemma  [ lem : tmatmult ] to bound each term on the right hand side of eqn .",
    "( [ eqn : eqx41 ] ) . using eqn .",
    "( [ eqn : eqpd12 ] ) of lemma  [ lem : hu ] we get that @xmath356 holds for all @xmath357 $ ] . by our choice of the sparsity parameter @xmath266 the conditions of lemma  [",
    "lem : tmatmult ] are satisfied .",
    "it follows from lemma  [ lem : tmatmult ] that @xmath358 }   &   = & \\sum_{j=1}^{d } { \\mbox{}{\\bf{e}}\\left [ \\left ( \\left(\\left(hdu_a\\right)^{(j)}\\right)^{t}t^tthdb^{\\perp } - { u_a^{(j)}}^{t}dh^thdb^{\\perp } \\right)^2 \\right ] } \\\\ & \\leq & \\sum_{j=1}^d \\left(\\frac{2}{k}{\\mbox{}\\left\\|\\left(hdu_a\\right)^{(j)}\\right\\|_2 ^ 2}{\\mbox{}\\left\\|hdb^{\\perp}\\right\\|_2 ^ 2 } + \\frac{1}{k}{\\mbox{}\\left\\|hdb^{\\perp}\\right\\|_2 ^ 2}\\right )    \\\\ & = & \\frac{3d}{k}{\\mbox{}\\left\\|hdb^{\\perp}\\right\\|_2 ^ 2 } = \\frac{3d}{k}{\\cal z}^2.\\end{aligned}\\ ] ] the last line follows since @xmath359 , for all @xmath357 $ ] . using markov s inequality",
    ", we get that with probability at least @xmath360 , @xmath361 the proof of the lemma is concluded by using the assumed value of @xmath59 .      by our choices of @xmath59 and @xmath266 as in eqns .",
    "( [ eqn : valuekfinal ] ) and  ( [ eqn : valueqfinal ] ) , it follows that both conditions  ( [ eqn : lemma1_ass1 ] ) and  ( [ eqn : lemma1_ass2 ] ) are satisfied .",
    "combining with lemma  [ lem : suff_cond ] we immediately get the accuracy guarantees of theorem  [ thm : alg_projection_fast ] .",
    "the failure probability of algorithm  [ alg : alg_projection_fast ] can be bounded using an argument similar to the one used in section  [ sxn : sampling : proofs : complete ] .    in order to complete the proof we discuss the running time of algorithm  [ alg : alg_projection_fast ] .",
    "first of all , by the construction of @xmath57 , the expected number of non - zero entries in @xmath57 is @xmath362 . in step",
    "@xmath363 we need to compute the products @xmath364 and @xmath365 .",
    "recall that @xmath15 has @xmath1 columns and thus the running time of computing both products is equal to the time needed to apply @xmath296 on @xmath257 vectors .",
    "first , note that in order to apply @xmath46 on @xmath257 vectors in @xmath105 , @xmath258 operations suffice . in order to estimate how many operations",
    "are needed to apply @xmath366 on @xmath257 vectors , we use the results of theorem  @xmath260 ( see also section 7 ) of ailon and liberty  @xcite , which state that at most @xmath367 operations are needed for this operation . here",
    "@xmath368 denotes the number of non - zero elements in the matrix @xmath57 , which  in expectation  is @xmath369 . after this preprocessing ,",
    "algorithm  [ alg : alg_projection_fast ] must compute the pseudoinverse of a @xmath370 matrix , or , equivalently , solve a least - squares problem on @xmath59 constraints and @xmath1 variables .",
    "this operation can be performed in @xmath371 time since @xmath372 .",
    "thus , the entire algorithm runs in expected time @xmath373 } + o\\left(kd^2 \\right ) \\leq n(d+1 ) + 2n(d+1 ) \\log_2 \\left(nkq + 1\\right ) + o\\left(kd^2 \\right).\\ ] ]    10    n.  ailon and b.  chazelle .",
    "approximate nearest neighbors and the fast johnson - lindenstrauss transform . in _ proceedings of the 38th annual acm symposium on theory of computing _ ,",
    "pages 557563 , 2006 .",
    "n.  ailon and e.  liberty .",
    "fast dimension reduction using rademacher series on dual bch codes . in _ proceedings of the 19th annual acm - siam symposium on discrete algorithms _ , pages 19 , 2008 .",
    "s.  arora , e.  hazan , and s.  kale . a fast random sampling algorithm for sparsifying matrices . in _ proceedings of the 10th international workshop on randomization and computation _ ,",
    "pages 272279 , 2006 .",
    "h.  avron , p.  maymounkov , and s.  toledo .",
    "blendenpik : supercharging lapack s least - squares solver . manuscript .",
    "( 2009 ) .",
    "h.  avron , p.  maymounkov , and s.  toledo .",
    "blendenpik : supercharging lapack s least - squares solver . , 32:12171236 , 2010 .",
    "a.  ben - israel and t.n.e .",
    "greville . .",
    "springer - verlag , new york , 2003 .",
    "r.  bhatia . .",
    "springer - verlag , new york , 1997 .",
    "clarkson and d.p .",
    "woodruff . numerical linear algebra in the streaming model . in _ proceedings of the 41st annual acm symposium on theory of computing _ , pages 205214 , 2009 .",
    "d.  coppersmith and s.  winograd .",
    "matrix multiplication via arithmetic progressions .",
    ", 9(3):251280 , 1990 .",
    "g.  dahlquist , b.  sjberg , and p.  svensson .",
    "comparison of the method of averages with the method of least squares .",
    ", 22(104):833845 , 1968 .",
    "p.  drineas , r.  kannan , and m.w . mahoney .",
    "fast monte carlo algorithms for matrices i : approximating matrix multiplication .",
    ", 36:132157 , 2006 .",
    "p.  drineas , m.w .",
    "mahoney , and s.  muthukrishnan .",
    "sampling algorithms for @xmath12 regression and applications . in",
    "_ proceedings of the 17th annual acm - siam symposium on discrete algorithms _ , pages 11271136 , 2006 .",
    "p.  drineas , m.w .",
    "mahoney , and s.  muthukrishnan .",
    "relative - error cur matrix decompositions . , 30:844881 , 2008 .",
    "p.  drineas , m.w .",
    "mahoney , s.  muthukrishnan , and t.  sarls .",
    "faster least squares approximation .",
    "technical report .",
    "preprint : arxiv:0710.1435 ( 2007 ) .",
    "u.  feige and e.  ofek .",
    "spectral techniques applied to sparse random graphs . , 27(2):251275 , 2005 .",
    "golub and c.f .  van loan . .",
    "johns hopkins university press , baltimore , 1996 .",
    "ibarra , s.  moran , and r.  hui .",
    "a generalization of the fast lup matrix decomposition algorithm and applications .",
    ", 3:4556 , 1982 .",
    "j.  matouek . on variants of the johnson  lindenstrauss lemma .",
    ", 33(2):142156 , 2008 .",
    "r.  motwani and p.  raghavan . .",
    "cambridge university press , new york , 1995 .",
    "nguyen , t.t .",
    "do , and t.d .",
    "tran . a fast and efficient algorithm for low - rank approximation of a matrix . in _ proceedings of the 41st annual acm symposium on theory of computing _ , pages 215224 , 2009 .",
    "r.  i. oliveira . .",
    "technical report .",
    "preprint : arxiv:1004.3821v1 ( 2010 ) .",
    "v.  rokhlin and m.  tygert . a fast randomized algorithm for overdetermined linear least - squares regression .",
    ", 105(36):1321213217 , 2008 .",
    "m.  rudelson and r.  vershynin . sampling from large matrices : an approach through geometric functional analysis .",
    ", 54(4):article 21 , 2007 .",
    "t.  sarls .",
    "improved approximation algorithms for large matrices via random projections . in _ proceedings of the 47th annual ieee symposium on foundations of computer science _ , pages 143152 , 2006 .",
    "stewart and j.g .",
    "academic press , new york , 1990 .",
    "stigler . .",
    "harvard university press , cambridge , 1986 .",
    "let @xmath374 be any matrix . consider the following algorithm ( which is essentially the algorithm in page 876 of  @xcite ) that constructs a matrix @xmath375 consisting of @xmath376 rescaled columns of @xmath15 .",
    "we will seek a bound on the approximation error @xmath377 , which we will provide in theorem  [ thm : theorem7correct ] .",
    "a variant of this theorem appeared as theorem 7 in  @xcite ; this version modifies and supersedes eqn . ( 47 ) of theorem 7 in the following manner : first , we will assume that the spectral norm of @xmath15 is bounded and is at most one ( this is a minor normalization assumption ) .",
    "second , and most importantly , we will need to set @xmath376 to be at least the value of eqn .",
    "( [ eqn : cboundappendix ] ) for the theorem to hold .",
    "this second assumption was omitted from the statement of eqn . ( 47 ) in theorem 7 of  @xcite .",
    "[ thm : theorem7correct ] let @xmath374 with @xmath380 .",
    "construct @xmath381 using the exactly(@xmath376 ) algorithm and let the sampling probabilities @xmath382 satisfy @xmath383 for all @xmath204 $ ] for some constant @xmath384 $ ] .",
    "let @xmath26 be an accuracy parameter and assume @xmath385 . if @xmath386 then , with probability at least @xmath38 , @xmath387    consider the exactly@xmath388 algorithm .",
    "then @xmath389 similar to  @xcite we shall view the matrix @xmath390 as the true mean of a bounded operator valued random variable , whereas @xmath391 will be its empirical mean .",
    "then , we will apply lemma 1 of  @xcite . to this end , define a random vector @xmath392 as @xmath393 } = p_i \\ ] ] for @xmath204 $ ] .",
    "the matrix @xmath379 has columns @xmath394 , where @xmath395 are @xmath376 independent copies of @xmath328 .",
    "using this notation , it follows that @xmath396 } = aa^t \\ ] ] and @xmath397 finally , let @xmath398 we can now apply lemma 1 , p. 3 of  @xcite .",
    "notice that from eqn .",
    "( [ eqn : expectyyt ] ) and our assumption on the spectral norm of @xmath15 , we immediately get that @xmath399 } } = \\tnorm{aa^t } \\leq \\tnorm{a}\\tnorm{a^t } \\leq 1.\\ ] ] then , lemma 1 of  @xcite implies that @xmath400 with probability at least @xmath401 .",
    "let @xmath402 be the failure probability of theorem  [ thm : theorem7correct ] ; we seek an appropriate value of @xmath376 in order to guarantee @xmath403 .",
    "equivalently , we need to satisfy @xmath404 recall that @xmath405 , and combine eqns .",
    "( [ eqn : defm ] ) and ( [ eqn : defpj ] ) to get @xmath406 .",
    "combining with the above equation , it suffices to choose a value of @xmath376 such that @xmath407 or , equivalently , @xmath408 we now use the fact that for any @xmath409 , if @xmath410 then @xmath411 .",
    "let @xmath412 , let @xmath413 , and note that @xmath409 if @xmath414 , since @xmath225 , @xmath37 , and @xmath402 are at most one .",
    "thus , it suffices to set @xmath415 which concludes the proof of the theorem .",
    "let @xmath267 be the sparse projection matrix constructed via algorithm  [ alg : alg_projection_fast ] ( see section  [ sxn : projection : result ] ) , with sparsity parameter @xmath266 .",
    "in addition , given @xmath416 , let @xmath417 .",
    "we will derive a bound for @xmath418 } = { \\mbox{}{\\bf{e}}\\left[\\left(x^tt^tty - x^ty\\right)^2\\right]}.\\ ] ] let @xmath419 be the @xmath82-th row of @xmath57 _ as a row vector _",
    ", for @xmath420 $ ] , in which case @xmath421 rather than computing @xmath422}$ ] directly , we will instead use that @xmath422}=\\left({\\mbox{}{\\bf{e}}\\left[\\delta\\right]}\\right)^2+{\\mbox{}{\\bf{var}}\\left[\\delta\\right]}$ ] .",
    "we first claim that @xmath423}=0 $ ] . by linearity of expectation ,",
    "@xmath424 }     = \\sum_{i=1}^{k } \\left [ { \\mbox{}{\\bf{e}}\\left[x^tt_{(i)}^tt_{(i)}y\\right]}-\\frac{1}{k}x^ty   \\right ]    .\\ ] ] we first analyze @xmath425 for some fixed @xmath82 ( w.l.o.g .",
    "@xmath426 ) .",
    "let @xmath427 denote the @xmath82-th element of the vector @xmath428 and recall that @xmath429}=0 $ ] , @xmath430}=0 $ ] for @xmath431 , and also that @xmath432}=1/k$ ] .",
    "thus , @xmath433 }     = { \\mbox{}{\\bf{e}}\\left[\\sum_{i=1}^{n}\\sum_{j=1}^{n } x_it_it_jy_j\\right ] }     = \\sum_{i=1}^{n}\\sum_{j=1}^{n } x_i { \\mbox{}{\\bf{e}}\\left[t_it_j\\right ] } y_j     = \\sum_{i=1}^{n } x_i { \\mbox{}{\\bf{e}}\\left[t_i^2\\right ] } y_i     = \\frac{1}{k } x^ty    .\\ ] ] by combining the above with eqn .",
    "( [ eqn : technical_pr_eq10 ] ) , it follows that @xmath423}=0 $ ] , and thus that @xmath422}={\\mbox{}{\\bf{var}}\\left[\\delta\\right]}$ ] . in order to provide a bound for @xmath434}$ ] , note that @xmath435 } & = & \\sum_{i=1}^{k } { \\mbox{}{\\bf{var}}\\left [ x^tt_{(i)}^tt_{(i)}y - \\frac{1}{k}x^ty \\right ] } \\\\ \\label{eqn : technical_pr_eq30 }              & = & \\sum_{i=1}^{k } { \\mbox{}{\\bf{var}}\\left [ x^tt_{(i)}^tt_{(i)}y \\right]}.\\end{aligned}\\ ] ] eqn .",
    "( [ eqn : technical_pr_eq20 ] ) follows since the @xmath59 random variables @xmath436 are independent ( since the elements of @xmath57 are independent ) and eqn .",
    "( [ eqn : technical_pr_eq30 ] ) follows since @xmath437 is constant . in order to bound eqn .",
    "( [ eqn : technical_pr_eq30 ] ) , we first analyze @xmath425 for some @xmath82 ( w.l.o.g .",
    "@xmath426 ) . then , @xmath438 }     & = & { \\mbox{}{\\bf{e}}\\left[(x^tt^tty)^2\\right ] } - \\left({\\mbox{}{\\bf{e}}\\left[x^tt^tty\\right]}\\right)^2     \\\\ \\label{eqn : eqx51 }     & = & { \\mbox{}{\\bf{e}}\\left[(x^tt^tty)^2\\right ] } - \\frac{1}{k^2}(x^ty)^2       .\\end{aligned}\\ ] ] we will bound the @xmath439}$ ] term directly : @xmath440 }     & = & { \\mbox{}{\\bf{e}}\\left[\\sum_{i_1=1}^{n}\\sum_{i_2=1}^{n}\\sum_{j_1=1}^{n}\\sum_{j_2=1}^{n}x_{i_1}x_{i_2}t_{i_1}t_{i_2}t_{j_1}t_{j_2}y_{j_1}y_{j_2 } \\right ] } \\\\ \\label{eqn : star1 }     & = & \\sum_{i_1=1}^{n}\\sum_{i_2=1}^{n}\\sum_{j_1=1}^{n}\\sum_{j_2=1}^{n}x_{i_1}x_{i_2 } { \\mbox{}{\\bf{e}}\\left [ t_{i_1}t_{i_2}t_{j_1}t_{j_2 } \\right ] } y_{j_1}y_{j_2 }     .\\end{aligned}\\ ] ] notice that if any of the four indices @xmath441 appears only once , then the expectation @xmath442}$ ] corresponding to those indices equals zero .",
    "this expectation is non - zero if the four indices are paired in couples or if all four are equal .",
    "that is , non - zero expectation happens if @xmath443 for case  ( a ) , let @xmath444 and let @xmath445 , in which case the corresponding terms in eqn .",
    "( [ eqn : star1 ] ) become : @xmath446}y_p^2     & = & \\sum_{\\ell=1}^{n}\\sum_{p=1:p\\ne\\ell}^{n } x_{\\ell}^2{\\mbox{}{\\bf{e}}\\left[t_{\\ell}^2\\right]}{\\mbox{}{\\bf{e}}\\left[t_p^2\\right]}y_p^2    \\\\     & = & \\frac{1}{k^2 } \\sum_{\\ell=1}^{n}\\sum_{p=1:p\\ne\\ell}^{n } x_{\\ell}^2y_p^2    \\\\     & = & \\frac{1}{k^2 } \\sum_{\\ell=1}^{n}\\sum_{p=1:p\\ne\\ell}^{n } x_{\\ell}^2y_p^2 + \\frac{1}{k^2 } \\sum_{p=1}^{n } x_p^2y_p^2 - \\frac{1}{k^2 } \\sum_{p=1}^{n}x_p^2y_p^2     \\\\     & = & \\frac{1}{k^2 } { \\mbox{}\\left\\|x\\right\\|_2 ^ 2 } { \\mbox{}\\left\\|y\\right\\|_2 ^ 2 } - \\frac{1}{k^2 } \\sum_{p=1}^n x_p^2 y_p^2     .\\end{aligned}\\ ] ] similarly , cases  ( b ) and  ( c ) give : @xmath447}y_{\\ell}y_p     & = & \\frac{1}{k^2}(x^ty)^2 - \\frac{1}{k^2 } \\sum_{p=1}^{n } x_p^2 y_p^2    \\\\     & & \\mbox{(where $ i_1 = j_1 = \\ell$ and $ i_2 = j_2 = p$ ) }   , \\mbox { and } \\\\ \\sum_{\\ell=1}^{n}\\sum_{p=1:p\\ne\\ell}^{n } x_{\\ell}x_p{\\mbox{}{\\bf{e}}\\left[t_{\\ell}^2t_p^2\\right]}y_{\\ell}y_p     & = & \\frac{1}{k^2}(x^ty)^2 - \\frac{1}{k^2 } \\sum_{p=1}^{n } x_p^2 y_p^2    \\\\     & & \\mbox{(where $ i_1 = j_2 = \\ell$ and $ i_2 = j_1 = p$ ) }           .\\end{aligned}\\ ] ] finally , for case  ( d ) , let @xmath448 , in which case : @xmath449}y_{\\ell}^2     = \\frac{1}{k^2q } \\sum_{\\ell=1}^{n } x_{\\ell}^2y_{\\ell}^2    , \\ ] ] where we have used that @xmath450}=1/(k^2q)$ ] . by combining these four terms for each of the @xmath59 terms in the sum , it follows from eqns .",
    "( [ eqn : technical_pr_eq30 ] ) and  ( [ eqn : eqx51 ] ) that @xmath451 }     & = & k\\left ( \\frac{1}{k^2}{\\mbox{}\\left\\|x\\right\\|_2 ^ 2}{\\mbox{}\\left\\|y\\right\\|_2 ^ 2 }               + \\frac{2}{k^2}(x^ty)^2               - \\frac{3}{k^2 } \\sum_{p=1}^n x_p^2 y_p^2               + \\frac{1}{k^2q}\\sum_{p=1}^n x_p^2 y_p^2               - \\frac{1}{k^2}(x^ty)^2          \\right )      \\\\ \\label{eqn : eqx61 }     & \\leq & \\frac{2}{k}{\\mbox{}\\left\\|x\\right\\|_2 ^ 2}{\\mbox{}\\left\\|y\\right\\|_2 ^ 2 }          + \\frac{1}{kq } \\sum_{p=1}^n x_p^2 y_p^2.\\end{aligned}\\ ] ] in the above we used @xmath452 .",
    "since we assumed that @xmath453 , the second term on the right hand side of eqn .",
    "( [ eqn : eqx61 ] ) is bounded by @xmath454 and the lemma follows since we have assumed that @xmath346 ."
  ],
  "abstract_text": [
    "<S> least squares approximation is a technique to find an approximate solution to a system of linear equations that has no exact solution . in a typical setting , one lets @xmath0 be the number of constraints and @xmath1 be the number of variables , with @xmath2 . </S>",
    "<S> then , existing exact methods find a solution vector in @xmath3 time . </S>",
    "<S> we present two randomized algorithms that provide accurate relative - error approximations to the optimal value and the solution vector of a least squares approximation problem more rapidly than existing exact algorithms . </S>",
    "<S> both of our algorithms preprocess the data with the randomized hadamard transform . </S>",
    "<S> one then uniformly randomly samples constraints and solves the smaller problem on those constraints , and the other performs a sparse random projection and solves the smaller problem on those projected coordinates . in both cases , solving the smaller problem provides relative - error approximations , and , if @xmath0 is sufficiently larger than @xmath1 , the approximate solution can be computed in @xmath4 time . </S>"
  ]
}