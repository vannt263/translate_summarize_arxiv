{
  "article_text": [
    "stochastic optimization algorithms play an important role in optimization problems involving objective functions that can not be computed analytically .",
    "these schemes are extensively used in discrete event systems , such as queuing systems , for obtaining optimal or near - optimal performance measures .",
    "gradient descent algorithms are used for stochastic optimization by estimating the gradient of average cost in the long run .",
    "methods for gradient estimation by random perturbation of parameters have been proposed in @xcite .",
    "the smoothed functional ( sf ) scheme , described in @xcite , approximates the gradient of expected cost by its convolution with a multivariate normal distribution .",
    "based on all the above schemes , two - timescale stochastic approximation algorithms have been presented in @xcite , which simultaneously perform cost averaging and parameter updation using different step - size schedules .",
    "the main issue with such algorithms is that , although convergence to a local optimum is guaranteed , the global optimum can not achieved in practice .",
    "hence , new methods are sought .    in this paper",
    ", we propose a new sf technique based on @xmath0-gaussian distribution , which is a generalization of the gaussian distribution .",
    "we show that @xmath0-gaussian satisfies all the conditions for smoothing kernels proposed by rubinstein  @xcite .",
    "we illustrate a method for gradient estimation using @xmath0-gaussian .",
    "we also present a two - timescale algorithm for stochastic optimization using @xmath0-gaussian based sf , and show the convergence of the proposed algorithm .",
    "the rest of the paper is organized as follows .",
    "the framework for the optimization problem and some of the preliminaries are presented in section  [ background ] .",
    "gradient estimation using @xmath0-gaussian sf has been derived in section  [ validity ] .",
    "section  [ algorithm ] presents the proposed algorithm .",
    "numerical experiments comparing our algorithm with a previous algorithm is presented in section  [ results ] .",
    "an outline of convergence analysis of our algorithm is discussed in section  [ convergence ] .",
    "finally , section  [ conclusion ] provides the concluding remarks .",
    "most of the distributions , like normal , uniform , exponential etc .",
    ", can be obtained by maximizing shannon entropy functional defined as @xmath1 , where @xmath2 is a pdf defined on the sample space @xmath3 .",
    "other entropy functions have also been proposed as generalized information measures .",
    "one of the most popular among them is nonextensive entropy , first introduced in  @xcite , and later studied by tsallis  @xcite .",
    "its continuous form entropy functional , which is consistent with the discrete case  @xcite , is defined as @xmath4^q\\mathrm{d}x}{q-1 } , \\qquad{q\\in\\mathbb{r}}.\\ ] ] this entropy functional produces shannon entropy as @xmath5 . corresponding to this generalized measure , @xmath0-expectation of a function @xmath6 can be defined as @xmath7^q\\mathrm{d}x } { \\displaystyle\\int\\limits_{\\mathbb{r}}[p(x)]^q\\mathrm{d}x}.\\ ] ] maximizing tsallis entropy under the following constraints : @xmath8 results in @xmath0-gaussian distribution @xcite , which is of the form @xmath9 where @xmath10 is called tsallis cut - off condition , and @xmath11 is the normalizing constant , which depends on the value of @xmath0 .",
    "the function defined in   is not integrable for @xmath12 , and hence , @xmath0-gaussian is a probability density function only for @xmath13 .",
    "multivariate form of the @xmath0-gaussian distribution @xcite is defined as @xmath14 where @xmath15 is the normalizing constant .",
    "it is easy to verify that the multivariate normal distribution is a special case of   as @xmath16 .",
    "a similar distribution can also be obtained by maximizing rnyi entropy  @xcite .",
    "let @xmath17 be a parameterized markov process , depending on a tunable parameter @xmath18 , where @xmath19 is a compact and convex subset of @xmath20 .",
    "let @xmath21 denote the transition kernel of @xmath22 when the operative parameter is @xmath18 .",
    "let @xmath23 be a lipschitz continuous cost function associated with the process .",
    "[ ergodic ] the process @xmath22 is ergodic for any given @xmath24 as the operative parameter , _",
    "i.e. _ , @xmath25 \\text { as } l\\to\\infty,\\ ] ] where @xmath26 is the stationary distribution of @xmath22 .",
    "our objective is to minimize the long - run average cost @xmath27 by choosing an appropriate @xmath18 .",
    "the existence of the above limit is given by assumption  [ ergodic ] .",
    "in addition , we assume that the average cost @xmath28 satisfies the following condition .",
    "[ differentiable ] @xmath29 is continuously differentiable with respect to any @xmath30 .",
    "we also assume the existence of a stochastic lyapunov function through the following assumption .",
    "[ lyapunov ] let @xmath31 be a sequence of random parameters , obtained using an iterative scheme , controlling the process @xmath22 , and @xmath32 @xmath33 , @xmath34 denote the sequence of associated @xmath35-fields .",
    "+ there exists @xmath36 , @xmath37 compact , and a continuous @xmath38-valued function @xmath39 , with @xmath40 , such that under any non - anticipative @xmath31 ,    a.   @xmath41 < \\infty$ ] and b.   @xmath42 \\leqslant v(y_n ) - \\epsilon_0 $ ] , when @xmath43 , @xmath34 .",
    "assumption  [ differentiable ] is a technical requirement , whereas assumption  [ lyapunov ] is used to show the stability of the scheme .",
    "assumption  [ lyapunov ] will not be required , for instance , if the single - stage cost function @xmath44 is bounded in addition .",
    "given any function @xmath45 , its smoothed functional is defined as @xmath46 = \\int\\limits_{-\\infty}^{\\infty}g_{\\beta}(\\eta)f(\\theta-\\eta)\\mathrm{d}\\eta   = \\int\\limits_{-\\infty}^{\\infty}g_{\\beta}(\\theta-\\eta)f(\\eta)\\mathrm{d}\\eta,\\ ] ] where @xmath47 is a kernel function .",
    "the idea behind using smoothed functionals is that if @xmath48 is not well - behaved , _",
    "i.e. _ , it has a fluctuating character , then @xmath49 $ ] has less fluctuations for appropriate values of @xmath50 .",
    "this ensures that any optimization algorithm with objective function @xmath48 does not get stuck at any local minimum , but converges to the global minimum .",
    "the parameter @xmath50 controls the degree of smoothness .",
    "rubinstein  @xcite has shown that the sf algorithm achieves these properties if the kernel function satisfies the following sufficient conditions :    1 .",
    "@xmath51 , + where @xmath52 .",
    "2 .   @xmath53 is piecewise differentiable in @xmath54 .",
    "3 .   @xmath53 is a probability distribution function , + _",
    "i.e. _ , @xmath49 = \\mathbb{e}_{g_{\\beta}(\\eta)}[f(\\theta-\\eta)]$ ] .",
    "@xmath55 , the dirac delta function .",
    "@xmath56 = f(\\theta)$ ] .",
    "the normal distribution satisfies the above conditions , and has been used as a kernel by katkovnik  @xcite .",
    "based on , a form of gradient estimator has been derived in @xcite which is given by @xmath57\\approx\\frac{1}{\\beta ml}\\sum_{n=0}^{m-1}\\sum_{m=0}^{l-1}{\\eta(n)h(y_m)}\\ ] ] for large @xmath58 , @xmath59 and small @xmath50 .",
    "the process @xmath60 is governed by parameter @xmath61 , where @xmath62 is obtained through an iterative scheme .",
    "@xmath63 is a @xmath64-dimensional vector composed of i.i.d .",
    "@xmath65-distributed random variables .",
    "the @xmath0-gaussian distribution satisfies the kernel properties ( p1 )  ( p5 ) for all @xmath13 , @xmath66 .    1 .   from  ,",
    "it is evident that @xmath67 .",
    "2 .   for @xmath68 , @xmath69 , for all @xmath70 . @xmath71 for @xmath72 , when @xmath73 , we have @xmath74 so ,   holds . on the other hand ,",
    "when @xmath75 , we have @xmath76 , which implies @xmath77 and , @xmath78 . + thus , @xmath79 is differentiable for @xmath80 , and piecewise differentiable for @xmath72 .",
    "3 .   @xmath79 is a distribution for @xmath13 and hence , the corresponding sf @xmath81 , which is parameterized by both @xmath0 and @xmath50 can be written as @xmath82 = \\mathbb{e}_{g_{q,\\beta}(\\eta)}[f(\\theta-\\eta)].\\ ] ] 4 .",
    "as @xmath83 , @xmath84 .",
    "but , we have @xmath85 for @xmath13 .",
    "so , @xmath86 .",
    "it follows from dominated convergence theorem that @xmath87 & = \\int\\limits_{-\\infty}^{\\infty}{\\lim_{\\beta\\to0}g_{q,\\beta}(\\eta)f(\\theta-\\eta)d\\eta } \\\\&=\\int\\limits_{-\\infty}^{\\infty}{\\delta(\\eta)f(\\theta-\\eta)\\mathrm{d}\\eta } = f(\\theta ) .",
    "\\end{aligned}\\ ] ]    @xmath88    our objective is to estimate @xmath89 using the sf approach .",
    "the existence of @xmath89 is due to assumption  [ differentiable ] .",
    "now , @xmath90^t.\\ ] ] + let us define , @xmath91 for @xmath72 , and @xmath92 for @xmath68 .",
    "it is evident that @xmath93 is the support set for the @xmath0-gaussian distribution with @xmath0-variance @xmath94 .",
    "+ define the sf for gradient of average cost as @xmath95 & = \\big[s_{q,\\beta}[\\nabla_{\\theta}^{(1)}j(\\theta ) ] \\quad\\ldots\\quad s_{q,\\beta}[\\nabla_{\\theta}^{(n)}j(\\theta ) ] \\big]^t \\\\&=\\int\\limits_{\\mathbb{r}^n}g_{q,\\beta}(\\theta-\\eta){\\nabla_{\\eta}j(\\eta)}\\mathrm{d}\\eta \\;.\\end{aligned}\\ ] ] it follows from integration by parts and the definition of @xmath93 , @xmath95&=\\int\\limits_{\\omega_q}\\nabla_{\\eta}g_{q,\\beta}(\\eta)j(\\theta-\\eta)\\mathrm{d}\\eta \\;.\\end{aligned}\\ ] ] substituting @xmath96 , we have @xmath95 & = \\int\\limits_{\\omega_q}{\\frac{2}{(3-q)\\beta}}\\frac{\\bar{\\eta}j(\\theta+\\beta\\bar{\\eta})}{\\left(1-\\frac{(1-q)}{(3-q ) } \\vert\\bar{\\eta}\\vert^2\\right)}g_q(\\bar{\\eta})\\mathrm{d}\\bar{\\eta }    \\nonumber \\\\&=\\frac{2}{\\beta(3-q)}\\mathbb{e}_{g_q(\\bar{\\eta})}\\left[\\frac { \\bar{\\eta}j(\\theta+\\beta\\bar{\\eta})}{\\left(1-\\frac{(1-q)}{(3-q)}\\vert\\bar{\\eta}\\vert^2\\right)}\\right]\\enspace .",
    "\\label{d : formula } \\end{aligned}\\ ] ]    we first state the following lemma which will be required to prove the result in proposition  [ d : convergence ] .    [ q - expect ] let @xmath97 be a function defined over a standard @xmath0-gaussian distributed random variable @xmath98 , @xmath99 @xmath100\\enspace,\\quad\\qquad\\ ] ] where @xmath101^q\\mathrm{d}x\\right]$ ] , @xmath15 being the normalizing constant for n - variate @xmath0-gaussian .    from   @xmath102^q\\mathrm{d}x } { \\displaystyle\\int_{\\mathbb{r}^n}[g_{q}(x)]^q\\mathrm{d}x } \\\\&= \\frac{1}{\\lambda_q k_{q , n}}\\int\\limits_{\\mathbb{r}^n}{f(x ) } \\left(1-\\frac{(1-q)\\vert{x}\\vert^2}{(3-q)}\\right)_+^{\\frac{q}{1-q}}\\mathrm{d}x   \\quad \\\\&= \\frac{1}{\\lambda_q}\\int\\limits_{\\omega_q}\\frac{f(x)}{\\left(1-\\frac{(1-q)}{(3-q ) } \\vert{x}\\vert^2\\right)}g_q(x)\\mathrm{d}x \\\\&= \\frac{1}{\\lambda_q}\\mathbb{e}_{g_q(x ) } \\left[\\displaystyle\\frac{f(x)}{1-\\frac{(1-q)}{(3-q ) } \\vert{x}\\vert^2}\\right]\\enspace.\\end{aligned}\\ ] ] @xmath88    [ d : convergence ] for a given @xmath13 , @xmath66 , as @xmath83 , sf for the gradient converges to a scaled version of the gradient , @xmath103-\\frac{2\\lambda_q}{(3-q)}\\nabla_{\\theta}j(\\theta ) \\right \\vert \\to 0 \\text { as   } \\beta \\to 0.\\ ] ]    for small @xmath50 , using taylor series expansion , @xmath104 by lemma  [ q - expect ] , @xmath105   = \\frac{2\\lambda_q}{\\beta(3-q)}\\bigg\\langle\\bar{\\eta}j(\\theta+\\beta\\bar{\\eta})\\bigg\\rangle_q \\\\&= \\frac{2\\lambda_q}{\\beta(3-q)}\\bigg[\\big\\langle{\\bar{\\eta}}\\big\\rangle_q{j(\\theta ) } + \\beta\\big\\langle{\\bar{\\eta}\\bar{\\eta}^t}\\big\\rangle_q{\\nabla_{\\theta}j(\\theta ) } +   \\\\&\\qquad\\qquad\\qquad\\qquad\\qquad\\frac{1}{2}\\beta^2\\big\\langle{\\bar{\\eta}\\bar{\\eta}^t{\\nabla_{\\theta}^2j(\\theta)}\\bar{\\eta}}\\big\\rangle_q + o(\\beta^2)\\bigg ] \\\\&= \\frac{2\\lambda_q}{(3-q)}\\left[{\\nabla_{\\theta}j(\\theta ) } + \\beta\\left(\\frac{1}{2}\\big\\langle{\\bar{\\eta}\\bar{\\eta}^t{\\nabla_{\\theta}^2j(\\theta)}\\bar{\\eta}}\\big\\rangle_q + o(\\beta)\\right)\\right]\\end{aligned}\\ ] ] thus , @xmath106\\to\\left(\\displaystyle\\frac{2\\lambda_q}{(3-q)}{\\nabla_{\\theta}j(\\theta)}\\right)$ ] as @xmath83 .    as a consequence of the proposition [ d : convergence ] , for large @xmath58 and small @xmath50 ,",
    "the form of gradient estimate suggested by   is @xmath107\\approx\\frac{1}{\\lambda_q\\beta m}\\sum_{n=0}^{m-1}\\left[\\frac{\\bar{\\eta}(n ) j(\\theta(n)+\\beta\\bar{\\eta}(n))}{\\left(1-\\frac{(1-q)}{(3-q)}\\vert\\bar{\\eta}(n)\\vert^2\\right)}\\right].\\ ] ] using an approximation of  , for large @xmath59 , we can write the above equation as @xmath108\\approx\\frac{1}{\\lambda_q\\beta ml}\\sum_{n=0}^{m-1}\\sum_{m=0}^{l-1 } \\frac{\\bar{\\eta}(n)h(y_m)}{\\left(1-\\frac{(1-q)}{(3-q)}\\vert\\bar{\\eta}(n)\\vert^2\\right)},\\ ] ] where @xmath60 is governed by parameter @xmath109 .    however , since @xmath110 , @xmath111 need not be explicitly determined as estimating @xmath112 $ ] instead of @xmath89 does not affect the gradient descent approach . as a special case , for @xmath113",
    ", we have @xmath114 from definition .",
    "hence , we obtain the same form as in .",
    "in this section , we propose a two - timescale algorithm corresponding to the estimate obtained in  .    the @xmath0-gaussian distributed parameters ( @xmath54 ) have been generated in the algorithm using the method proposed in @xcite .",
    "+ let \\{@xmath115 } , \\{@xmath116 } be two step - size sequences satisfying    [ stepsize ] @xmath117 , @xmath118 , and @xmath119",
    ".    for @xmath120 , let @xmath121 @xmath122 represent the projection of @xmath24 onto the set @xmath19 .",
    "\\{@xmath123}@xmath124 are quantities used to estimate @xmath112 $ ] via the recursions below .",
    "fix @xmath58 , @xmath59 , @xmath0 and @xmath50 .",
    "set @xmath125 .",
    "fix parameter vector @xmath126 .",
    "generate i.i.d .",
    "standard @xmath0-gaussian distributed random variables @xmath127 and set + @xmath128 . generate the simulation @xmath129 governed with parameter @xmath61 .",
    "+ @xmath130 $ ] . .",
    "set @xmath131 .",
    "output @xmath132 as the final parameter vector .",
    "we consider a two - node network of @xmath133 queues with feedback .",
    "the setting here is somewhat similar to that considered in @xcite .",
    "nodes 1 and 2 are fed with independent poisson external arrival processes with rates @xmath134 and @xmath135 , respectively . after departing from node-1 ,",
    "customers enter node-2 . once the service at node-2 is completed , a customer either leaves the system with probability @xmath136 or joins node-1 .",
    "the service time processes of the two nodes , @xmath137 and @xmath138 , respectively , are defined as @xmath139 where @xmath140 and @xmath141 are constants . here ,",
    "@xmath142 and @xmath143 are independent samples drawn from uniform distribution on ( 0,1 ) .",
    "service time of each node depends on the @xmath144-dimensional tunable parameter vector @xmath145 , whose individual components lie in a certain interval @xmath146 $ ] , @xmath147 , @xmath148 .",
    "@xmath149 represents the @xmath150 update of parameter vector at node-@xmath151 , and @xmath152 represents the target vector .",
    "the cost function is chosen to be the sum of the two queue lengths at any instant .",
    "for the cost to be minimum , @xmath153 should be minimum , and hence , we should have @xmath154 , @xmath148 .",
    "we denote @xmath155 , and @xmath156 , where @xmath64=@xmath157+@xmath158 . for the simulations , we use the following values of parameters : + ( 1 ) @xmath159 , + ( 2 ) @xmath160 , @xmath161 for all @xmath162 , _",
    "i.e. _ , @xmath163^n$ ] . + ( 3 ) @xmath164 , @xmath165 for @xmath166 , + ( 4 ) @xmath167 , @xmath168 , + ( 5 ) @xmath169 , @xmath170 .",
    "simulations are performed by varying the parameters @xmath0 and @xmath50 .",
    "we compare the performance of our algorithm with the sf algorithm proposed in @xcite , which uses gaussian smoothing .",
    "the euclidian distance between @xmath171 and @xmath172 is chosen as the performance measure as this gives the proximity of the updates to the global optimum .",
    "for each case , the results are averaged over @xmath173 independent trials .",
    "figure  1 shows that with same @xmath50 , @xmath0-sf converges faster than sf algorithm for some @xmath0 s .",
    "table  i presents a detailed comparison for different values of @xmath0 and @xmath50 .",
    ".,height=136 ]    [ gqsf1_error ]    .performance ( mean distance from optimum ) . [ cols=\"^,^,^,^,^,^,^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     the cases where @xmath0-sf outperforms sf are highlighted , and for each @xmath50 , the best result is underlined .",
    "it can be observed that for smaller @xmath50 , @xmath0-sf with @xmath80 performs better than sf , but for larger @xmath50 , better performance can be obtained with @xmath72 .",
    "so , as @xmath50 increases , smaller @xmath0 s prove to be better . as per observations , @xmath174 performs better than gaussian in 63% cases , and also gives the least distance in most of the cases ( 50% ) .",
    "the results show that there are some values of @xmath66 for which we can reach closer proximity of the global minimum with the proposed algorithm than the sf case .",
    "this can be contributed to the power - law tail of @xmath0-gaussian which allows better control over the level of smoothing .",
    "there is an additional improvement provided by @xmath111 , which can be expressed as @xmath175\\enspace.\\ ] ] for @xmath80 , the term inside bracket is always less than 1 , which implies @xmath176 , whereas @xmath177 for @xmath72 .",
    "thus the gradient descent is faster for @xmath72 , which leads to faster convergence .",
    "we also note that for high @xmath0 , the algorithm does not converge for larger @xmath50 .",
    "so we may claim that the region of stability of @xmath0-sf , given by @xmath178 ( see theorem  [ thm ] ) , decreases as @xmath0 increases .",
    "here , we give a sketch of the proof of convergence of the proposed algorithm .",
    "we just state the important results .",
    "the proofs will be given in a longer version of the paper .",
    "let @xmath179 , @xmath180 denote the @xmath35-fields generated by the above mentioned quantities , where @xmath181 and @xmath182 for @xmath183 , @xmath184 .",
    "define @xmath185 such that @xmath186)$ ] , where @xmath187 $ ] is the integer part of @xmath188 .",
    "thus , @xmath189 , @xmath190 and @xmath191 .    with the above notation ,",
    "substituting @xmath192 we can rewrite step 9 of our algorithm in terms of @xmath193 , @xmath194 and @xmath195 .",
    "we define the sequences @xmath196 , @xmath183 , @xmath197\\right)\\end{aligned}\\ ] ]    the sequences @xmath198 , @xmath199 @xmath200 are almost surely convergent martingale sequences .",
    "consider the following ordinary differential equations : @xmath201 - z(t).\\end{aligned}\\ ] ]    [ lem ] the sequence of updates @xmath202 is uniformly bounded with probability 1 .",
    "[ z2d ] for a given @xmath13 , @xmath66 , with probability 1 @xmath203 \\right\\vert \\to 0 $ ] as @xmath204 .",
    "the following corollary follows directly from proposition  [ d : convergence ] and lemma  [ z2d ] by triangle inequality .",
    "given a particular @xmath13 , with probability 1 , as @xmath204 and @xmath205 , @xmath206    now , finally considering the ode corresponding to the slowest timescale recursion : @xmath207 where @xmath208 for any bounded , continuous function @xmath209 .",
    "the stable points of lie in the set @xmath210 . given @xmath211 ,",
    "we define @xmath212 .",
    "[ thm ] under assumptions  [ differentiable ]    [ stepsize ] , given @xmath13 , @xmath66 and @xmath211 , @xmath213 such that for all @xmath214 $ ] , the sequence @xmath31 obtained using the @xmath0-sf algorithm converges to a point in @xmath215 with probability @xmath216 as @xmath204 .",
    "the @xmath0-gaussian exhibits power - law behavior , which gives a better control over smoothing of functions as compared to normal distribution .",
    "we have extended the gaussian smoothed functional gradient estimation approach to @xmath0-gaussians , and developed an optimization algorithm based on this .",
    "we have also presented results illustrating that for some values of @xmath0 , our algorithm performs better than the sf algorithm  @xcite ."
  ],
  "abstract_text": [
    "<S> the @xmath0-gaussian distribution results from maximizing certain generalizations of shannon entropy under some constraints . the importance of @xmath0-gaussian distributions stems from the fact that they exhibit power - law behavior , and also generalize gaussian distributions . in this paper , we propose a smoothed functional ( sf ) scheme for gradient estimation using @xmath0-gaussian distribution , and also propose an algorithm for optimization based on the above scheme . </S>",
    "<S> convergence results of the algorithm are presented . </S>",
    "<S> performance of the proposed algorithm is shown by simulation results on a queuing model . </S>"
  ]
}