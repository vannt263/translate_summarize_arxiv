{
  "article_text": [
    "in this paper we introduce a derivative - free linear interpolation - based method for solving constrained optimization problems of the form @xmath7 where @xmath3 and @xmath4 are continuous convex functions defined on a nonempty open convex subset @xmath8 of @xmath2 , and where the constraint set @xmath9 is a nonempty compact convex subset of @xmath8 .",
    "we further assume that we have access to the lower-@xmath10 representations of @xmath3 and @xmath4 and that the problem is feasible i.e. , there exists some @xmath11 such that @xmath12 .",
    "the algorithm is based on the @xmath6-comirror algorithm presented in @xcite .",
    "derivative - free optimization ( dfo ) is a rapidly growing field of research that explores the minimization of a black - box function when first - order information ( derivatives , gradients , or subgradients ) is unavailable .",
    "while the majority of past work in dfo has focused on unconstrained optimization , several methods have recently been introduced for constrained optimization . in constrained optimization , most of the analysis of dfo methods has been done within the framework of _ direct search _ and _ pattern search _ methods .",
    "that is , methods that do not attempt to build interpolation ( or other such ) models of the objective function , but instead use concepts like positive bases to ensure convergence .",
    "such methods can be adapted to constrained optimization through techniques by e.g. projecting search directions onto constraint sets @xcite , `` pulling back '' search directions onto manifolds @xcite , the use of filtering techniques @xcite , or barrier based penalties @xcite .",
    "on the other hand , fairly little research has explored approaching constrained optimization via model - based dfo methods .",
    "notable in this area is @xcite , which extends the uobyqa @xcite to constrained optimization ( in an algorithm named condor ) .",
    "this paper provides a novel model - based dfo method for linearly constrained optimization .",
    "our algorithm is designed for constraints defined by a given convex function .",
    "our algorithm is based on the @xmath6-comirror algorithm @xcite .",
    "the @xmath6-comirror algorithm finds its roots in mirror - descent methods @xcite .",
    "these methods can be viewed as nonlinear projected subgradient methods that use a general distance - like function ( the bregman distance ) instead of the usual euclidean squared distance @xcite .",
    "the @xmath6-comirror algorithm adapts the mirror - descent method to work for convex constrained optimization where the constraint set is provided by a convex function .",
    "it requires that the problem is additionally constrained by a convex compact set and that the subgradients ( of both the constraint function and the objective function ) are bounded over this set .",
    "the algorithm presented here differs from previous research in two other notable ways .",
    "first , unlike past model - based dfo method , we do not assume that the objective function is @xmath13 ; instead , we work with the broader class of lower-@xmath13 functions ( see definition  [ lowerc1 ] ) .",
    "lower-@xmath13 functions include convex ( * ? ? ?",
    "* theorem  10.33 ) and @xmath13 functions ( by definition ) , as well as fully amenable functions ( * ? ? ?",
    "* exercise  10.36 ) and finite max functions ( example  [ ex : finitemax ] below ) . to work with lower-@xmath13 functions ,",
    "we develop a method to approximate subgradients for such functions and analyze it for the derivative - free algorithm .",
    "in particular , in theorem  [ errbdd ] we define the approximate subgradient for an arbitrary lower-@xmath13 function and prove that it satisfies an error bound analogous to the one introduced in ( * ? ? ?",
    "* theorem 2.11 ) for the class of @xmath14 functions .",
    "the second major difference from previous dfo research is that we present a convergence result that quantifies the difference between the function values of the iterates and the optimal function value . to the best of our knowledge ,",
    "this provides the first results of this kind for a multivariable dfo method .",
    "it is remarkable that the dfo algorithm we develop has the same convergence result as the original gradient - based algorithm presented in @xcite .",
    "( a quadratically convergent dfo method is developed in @xcite , but only for functions defined on @xmath15 .",
    "furthermore , in @xcite , a superlinearly convergent algorithm is presented . )",
    "the remainder of this paper is organized as follows .",
    "section  [ s:2 ] is a brief introduction to the main building blocks we use .",
    "first , we provide the definition of the class of lower-@xmath13 functions and some properties .",
    "second , we provide the definition of the linear interpolation model of a function @xmath3 over a subset @xmath16 of @xmath2 and a sufficient condition to be well - defined .",
    "finally , we give the definition and the main properties of bregman distances . in section  [ s:3 ]",
    "we give the first key result in theorem  [ errbdd ] , on which we build our convergence results . in section  [ s:4 ]",
    "we describe our derivative - free @xmath17comirror algorithm . in theorem  [ t :",
    "conv ] we establish the convergence analysis . in section  [ s:5 ]",
    "we provide some numerical results that confirm the practical feasibility of the algorithm .",
    "section  [ s : conc ] summarizes some concluding remarks . to make",
    "the presentation self - contained we add appendix  [ s : app ] which includes the proofs of two basic inequalities .",
    "we shall work in @xmath2 , equipped with the usual euclidean norm @xmath18 . throughout the remainder of the paper , we suppose that    @xmath8 is a nonempty open convex subset of @xmath2 .",
    "recall that for a convex function @xmath19 , the subdifferential @xmath20 at a point @xmath21 is defined by @xmath22 we denote the _ closed _ ball in @xmath2 centred at @xmath23 with radius @xmath24 by @xmath25 and the set of _ natural numbers _ by @xmath26 given @xmath27 , we abbreviate the _ unit simplex _ in @xmath28 by @xmath29 } , i\\in\\{1,\\ldots , r\\}\\big\\}.\\ ] ] finally , we shall use @xmath30 to denote the spectral norm of a matrix @xmath31 .",
    "we next introduce the class of lower-@xmath13 functions .",
    "[ lowerc1 ]  ( * ? ? ? * definition 10.29 ) a function @xmath19 is said to be a _",
    "lower-@xmath32 function _ at @xmath33",
    "if there exists a neighbourhood @xmath34 and a representation @xmath35 in which all functions @xmath36 are of class @xmath37 on @xmath38 , the index set @xmath39 is a compact topological space , and @xmath36 and the first @xmath40 derivatives of @xmath36 depend continuously not just on @xmath41 but even on @xmath42 . in this case",
    "we say that provides a _ lower-@xmath32 representation _ of @xmath3 at @xmath21 .",
    "the function @xmath3 is said to be _",
    "lower-@xmath37 on @xmath8 _ if @xmath3 is lower-@xmath37 at every point @xmath21 .",
    "the next lemma provides details regarding when a convex function is lower-@xmath13 .",
    "[ con_lc2 ] ( * ? ? ?",
    "* theorem 10.33 ) let @xmath19 be convex . then @xmath3 is lower-@xmath10 on @xmath8 .",
    "although the class of lower-@xmath13 functions includes many convex functions ( * ? ?",
    "* theorem  10.33 ) , it should be noted that our algorithm will require access to a lower-@xmath13 representation of the objective and constraint functions .",
    "the next example shows that any finite max function is not only lower-@xmath13 , but also provides a natural lower-@xmath13 representation .",
    "[ ex : finitemax ] let @xmath19 be defined as @xmath43 , where each @xmath44 is of class @xmath32 on @xmath8 .",
    "then @xmath3 is lower-@xmath32 on @xmath8 .",
    "( this is the case where @xmath45 is @xmath46 equipped with the discrete topology . )",
    "the value of working with lower-@xmath13 functions is seen in lemma  [ subgd ] , which demonstrates how to compute the subdifferential of a lower-@xmath13 function .    [ subgd ]",
    "let @xmath19 be a convex function that has a lower-@xmath13 representation @xmath47 at @xmath48 and set @xmath49 .",
    "then @xmath50    combine ( * ? ? ?",
    "* theorem 10.31 ) and ( * ? ? ?",
    "* proposition 8.12 ) .",
    "* proposition  10.54)[save ] let @xmath19 be a lower-@xmath13 function , and let @xmath1 be a nonempty compact subset of @xmath8 .",
    "then there exists an open set @xmath51 with @xmath52 , such that @xmath3 has a common lower-@xmath13 representation valid at all points @xmath53 , i.e. , there exists a compact topological space @xmath45 , and a family of functions @xmath54 defined on @xmath51 such that @xmath55 and the functions @xmath56 , @xmath57 , and @xmath58 are continuous on @xmath59 .    to prove convergence of the algorithm introduced in this paper , we require bounds on the subgradients of the objective and the constraint functions",
    ". lemma  [ bound : aux ] provides a proof of the existence of this bound .",
    "[ bound : aux ] let @xmath19 be convex , and let @xmath1 be a nonempty compact subset of @xmath8 .",
    "then @xmath60    since @xmath3 is convex , lemma [ con_lc2 ] implies that @xmath3 is lower-@xmath13 on @xmath8 .",
    "since @xmath1 is a nonempty compact subset of @xmath8 , theorem  [ save ] guarantees the existence of an open subset @xmath51 with @xmath61 such that @xmath3 has a common lower-@xmath13 representation valid at all points @xmath53 .",
    "let @xmath62 be as stated in theorem  [ save ] .",
    "the definition of lower-@xmath13 implies that the mapping @xmath63 is continuous on @xmath59 . by the weierstrass theorem , @xmath64 .",
    "now , let @xmath65 , and let @xmath66 . using lemma  [ subgd ] we know that @xmath67 for some @xmath68 where @xmath27 is the number of elements in @xmath69 .",
    "therefore @xmath70 and the proof is complete .",
    "( alternatively , one may consider either the lower semicontinuous hull of @xmath3 and apply ( * ? ? ?",
    "* theorem  24.7 ) , or use ( * ? ? ?",
    "* corollary  12.38 ) after extending @xmath71 to a maximally monotone operator . )",
    "[ coffee ] let @xmath19 be a lower-@xmath13 function , and let @xmath1 be a nonempty compact convex subset of @xmath8 .",
    "let @xmath51 , @xmath45 , and @xmath54 be as in theorem  [ save ] .",
    "then there exists @xmath72 such that @xmath73 is @xmath74-lipschitz on @xmath51 for every @xmath75 .    by theorem",
    "[ save ] , @xmath76 is continuous on the compact set @xmath77",
    ". therefore , by the weierstrass theorem , @xmath78 .",
    "now apply the mean value theorem ( * ? ? ?",
    "* theorem  5.1.12 ) .      in our method",
    "we use a derivative - free model - based technique .",
    "therefore , in this section we introduce the definition of the linear interpolation model and related facts .",
    "let @xmath19 be a function , and let @xmath79 .",
    "if the matrix @xmath80    is invertible , then @xmath16 is said to be _ a poised tuple centred at @xmath81_. moreover , if @xmath82 then @xmath16 is said to be _ a poised tuple centred at @xmath81 with respect to @xmath3_. in this case the linear system @xmath83 has a unique solution @xmath84 , and the _ linear interpolation model _ of the function @xmath3 over @xmath16 is the unique ( well defined ) function    @xmath85    note that in this case @xmath86 satisfies the interpolation conditions @xmath87    the following theorem provides the error bound satisfied by the approximate gradient of the linear interpolation model .",
    "* theorem 2.11)[lin_int_err ] suppose that @xmath19 is @xmath13 function on @xmath8 .",
    "let @xmath88 .",
    "assume that @xmath89 is a poised tuple of sample points centred at @xmath81 with respect to @xmath3 .",
    "set @xmath90 .",
    "suppose that @xmath91 .",
    "let @xmath92 be @xmath93 lipschitz over @xmath94 .",
    "then the gradient of the linear interpolation model @xmath86 satisfies an error bound of the form @xmath95 where @xmath96      the last building block used in our analysis is the bregman distance .",
    "@xcite let @xmath97 be a convex differentiable function .",
    "the corresponding _",
    "bregman distance _",
    "@xmath98 is @xmath99    ( * ? ? ?",
    "* section  3.5 ) let @xmath100 be a nonempty convex subset of @xmath2 .",
    "let @xmath101 .",
    "then @xmath102 is said to be _ strongly convex _ with convexity parameter @xmath103 , if for all @xmath104 , @xmath105}$ ] we have @xmath106    throughout the next arguments we shall assume that @xmath102 is a strongly convex and differentiable function on a nonempty convex subset of @xmath107 , with a convexity parameter @xmath108 . in this paper we shall be interested in bregman distances that are created from strongly convex functions .",
    "the following result is part of the folklore ( and established in much greater generality in e.g. , ( * ? ? ?",
    "* section  3.5 ) ) ; for completeness we include the proof .",
    "[ equi ] let @xmath97 be a differentiable function .",
    "let @xmath1 be a nonempty subset of @xmath8",
    ". then the following are equivalent :    1 .",
    "[ i ] @xmath109 for all @xmath110 and @xmath1110,1\\right[}$ ] .",
    "[ ii ] @xmath112 for all @xmath110 and @xmath1130,1\\right[}$ ] .",
    "[ iii ] @xmath114 for all @xmath110 and @xmath1110,1\\right[}$ ] .",
    "`` @xmath115 '' : rewrite ( [ i ] ) as @xmath116 hence @xmath117 taking the limit as @xmath118 and using the assumption that @xmath102 is differentiable we see that @xmath119 hence holds .",
    " @xmath120 \" .",
    "suppose that ( [ ii ] ) holds for all @xmath121 .",
    "let @xmath1220,1\\right[}$ ] .",
    "set @xmath123 . applying ( [ ii ] ) to @xmath124 and @xmath125 yields @xmath126 similarly , applying ( [ ii ] ) to @xmath127 and @xmath125 yields @xmath128 multiplying by @xmath129 and by @xmath130 , and adding we get @xmath131 notice that @xmath132 and @xmath133 .",
    "thus , substituting in the last inequality we get @xmath134\\\\   & =   \\lambda { \\omega}(x ) + ( 1-\\lambda ) { \\omega}(y ) -\\lambda ( 1-\\lambda ) { \\langle{\\nabla}{\\omega}(z ) , x - y\\rangle}+\\lambda(1-\\lambda){\\langle{\\nabla}{\\omega}(z),x - y\\rangle}\\\\ &   -\\frac{\\alpha}{2}\\lambda ( 1-\\lambda){\\left ( ( 1-\\lambda){{\\lvertx - y\\rvert}}^2+{\\lambda}{{\\lvertx - y\\rvert}}^2\\right)}\\\\   & =   \\lambda { \\omega}(x ) + ( 1-\\lambda ) { \\omega}(y ) -\\frac{\\alpha}{2}\\lambda ( 1-\\lambda){{\\lvertx - y\\rvert}}^2.\\end{aligned}\\ ] ] substituting for @xmath135 gives ( [ i ] ) .     @xmath120 \" .",
    "suppose that ( [ ii ] ) holds @xmath136 .",
    "then we have @xmath137 @xmath138 adding and we get ( [ iii ] ) .",
    " @xmath120 \" . by the fundamental theorem of calculus we have for @xmath1390,1\\right[}$ ]",
    "@xmath140 subtracting @xmath141 , noting that @xmath142 and using ( iii ) we get @xmath143 which completes the proof .    following @xcite , we give the definition of the bregman diameter of an arbitrary set @xmath1 .",
    "let @xmath97 be a convex differentiable function .",
    "let @xmath1 be a nonempty subset of @xmath8 .",
    "the _ bregman diameter of the set @xmath1 _ is defined as @xmath144    in the following lemma we prove that , if @xmath102 is differentiable and strongly convex , then the bregman diameter is finite for every compact subset of @xmath2 .",
    "[ diam ] let @xmath145 be a differentiable convex function .",
    "let @xmath1 be a nonempty compact subset of @xmath8 .",
    "then @xmath146 is bounded on @xmath147 .",
    "consequently , the bregman diameter of the set @xmath1 is finite .",
    "since @xmath102 is convex and differentiable , therefore @xmath102 is continuously differentiable on @xmath8 ( * ? ? ?",
    "* corollary  25.5.1 ) .",
    "thus , @xmath102 and @xmath148 are continuous on @xmath1 , and therefore @xmath98 is continuous on @xmath147 .",
    "now , @xmath147 is a nonempty compact subset of @xmath149 , and therefore @xmath146 is bounded on @xmath147 and the bregman diameter of the set @xmath1 is finite .",
    "recall that we are interested in the general convex problem of the form @xmath150    in the sequel , we shall consider the following assumptions on @xmath3 , @xmath4 and @xmath1 .",
    "a1 : :    @xmath19 and    @xmath151 are continuous convex functions . a2 : :    @xmath1 is a nonempty compact convex subset of    @xmath8 , and @xmath1 is not a singleton . a3 : :    we have access to lower-@xmath10    representations ( see theorem  [ save ] ) of @xmath3 and    @xmath4 on some open subset @xmath51 of    @xmath8 such that @xmath152 and    @xmath153 a4 : :    the set of optimal solutions of problem @xmath154 is nonempty .    [ assum ] under assumption * a1 * , the functions @xmath3 and @xmath4 are lower-@xmath10 functions on @xmath8 ( by lemma  [ con_lc2 ] ) . assumption * a3 * provides the stronger statement that we have access to lower-@xmath13 representations of these functions .",
    "[ bdsbgd ] suppose that assumptions * a1 * and * a2 * hold .",
    "then @xmath155    combine remark  [ assum ] , assumption  * a2 * , and lemma  [ bound : aux](ii ) .    in the following theorem , we give an error bound for the approximate subgradient .",
    "[ errbdd ] suppose that * a1 * , * a2 * , * a3 * , and * a4 * hold .",
    "let @xmath89 be a poised tuple of sample points centred at @xmath156 with respect to @xmath3 .",
    "set @xmath90 .",
    "suppose that @xmath157 .",
    "let @xmath158 .",
    "let @xmath159 , and @xmath160 , where @xmath27 .",
    "define @xmath161 .",
    "then there exists @xmath162 such that the following error bound holds : @xmath163 where @xmath74 is as in lemma  [ coffee ] , and @xmath164 is as defined in theorem  [ lin_int_err ] .    by assumption @xmath165 .",
    "lemma  [ subgd ] implies that @xmath166 .",
    "using the triangle inequality , the error bound given in theorem  [ lin_int_err ] ( applied to @xmath51 instead of @xmath8 ) and lemma  [ coffee ] , we have @xmath167 as claimed .",
    "our next corollary relates theorem  [ errbdd ] to the algorithm presented later .",
    "let us note that the function @xmath168 in corollary  [ cor:11 ] is the same as the one used in the algorithm .",
    "we also note that , although in corollary  [ cor:11 ] we provide the error bound for the approximate gradient function in a general format , in practice we shall use @xmath169 .",
    "[ cor:11 ] suppose that * a1 * , * a2 * , * a3 * and * a4 * hold .",
    "let @xmath170 be a poised tuple of sample points centered at @xmath156 with respect to @xmath3 .",
    "set @xmath171 and suppose that @xmath172 . for every @xmath173 ,",
    "let @xmath174 , @xmath175 , @xmath176 , @xmath177 , @xmath178 and @xmath179 and @xmath180 then :    1 .",
    "the following error bound holds @xmath181 where @xmath182 , @xmath74 is defined as in lemma  [ coffee ] and @xmath183 is obtained by replacing @xmath3 by @xmath4 in lemma  [ coffee ] , and @xmath184 is as defined in theorem  [ lin_int_err ] .",
    "the function @xmath168 induced by satisfies @xmath185 where @xmath186 and @xmath187 are defined as in lemma  [ bdsbgd ] .",
    "( i ) : use and , and apply theorem  [ errbdd ] to @xmath3 and @xmath4 .",
    "( ii ) : let @xmath65 . using the triangle inequality , , and we have @xmath188 .",
    "in this section we introduce the derivative - free @xmath17comirror algorithm and present a convergence analysis .",
    ": : initialization : :    input    +    * @xmath189 ,    * @xmath190 .",
    "general step : :    for every @xmath191    +    * select @xmath192    * select a poised tuple    @xmath193    centred at @xmath81 with respect to @xmath3 such that    the set    @xmath194 ,    @xmath195 and    @xmath196 , where    @xmath197 is as defined    in theorem  [ lin_int_err ] .    *",
    "set @xmath198    where @xmath199 @xmath200 and    where @xmath103 is the strong convexity parameter of the    strongly convex function    @xmath201 , @xmath202 is    the corresponding bregman diameter of the set @xmath1 , and    @xmath203 and @xmath204 are defined as in    corollary  [ cor:11 ] .       1 .   in generating the points of the tuple @xmath205 we need to check that @xmath196 .",
    "if this inequality fails , then we resample .",
    "it is always possible to generate the tuple @xmath206 for all @xmath207 provided that @xmath208 is set to be sufficiently large @xcite . for a detailed discussion on how to choose @xmath208",
    "we refer the reader to @xcite .",
    "2 .   the poised tuple @xmath193 must satisfy @xmath209 to guarantee that the error bound in theorem  [ errbdd ] still holds true .",
    "this does not create a conflict ( i ) because by the definition of the matrix @xmath184 in , the value of @xmath210 remains unchanged under scaling or shifting .",
    "the update of @xmath211 in is well defined , since that the function @xmath212 is strongly convex and differentiable over @xmath1 , and therefore it has a unique minimizer over @xmath1 .",
    "the step length @xmath213 is well defined for all @xmath191 except when @xmath214 in which case either we have a local minimum , or we change the search radius @xmath215 to get a better approximation of the gradients .",
    "moreover , the bregman diameter @xmath202 is finite by lemma  [ diam ] .",
    "finally , by lemma  [ equi ]  ( ii ) , we have that @xmath216 , and therefore , since @xmath1 is not a singleton , the bregman diameter @xmath202 is strictly positive .",
    "5 .   in general ,",
    "the bregman diameter @xmath217 is not easy to calculate .",
    "however , if the set @xmath1 is simple and the function @xmath102 is separable , calculating @xmath202 becomes simpler .",
    "for example , if @xmath218\\times\\cdots\\times [ \\alpha_m,\\beta_m]$ ] and @xmath219 , then @xmath220 .",
    "we devote this subsection to study the convergence of the algorithm .",
    "lemma  [ key ] and its proof are only a minor adaptation of ( * ? ? ?",
    "* lemma  2.2 ) . for the sake of completeness",
    ", we include the adapted proof .",
    "[ key ] let @xmath221 be the sequence generated by @xmath222 .",
    "let @xmath223 be two strictly positive integers .",
    "then for all @xmath224    @xmath225    for every @xmath226 .    by the optimality condition in @xmath227 we have    @xmath228    hence , @xmath229    the three - point property of the bregman distance ( * ? ? ?",
    "* lemma 3.1 ) tells us @xmath230    combining ( [ 2.7 ] ) and ( [ 2.8 ] ) yields @xmath231 that is @xmath232 adding @xmath233 to both sides of the above inequality and using lemma  [ equi ] ( ii ) and the cauchy - schwarz inequality we get @xmath234 notice that , @xmath235 is a quadratic function of @xmath236 that has a maximum value of @xmath237 , i.e. , @xmath238 .",
    "this yields @xmath239 summing the last inequality over @xmath240 we obtain    @xmath241    using the definition of @xmath202 we note that @xmath242 , from which we get ( [ 2.6 ] ) .",
    "the following theorem presents the efficiency estimate for the derivative - free @xmath17comirror method . in proving theorem  [ t : conv ]",
    "we are motivated by the techniques used in the proof of ( * ? ? ?",
    "* theorem   2.1 ) . given @xmath243",
    ", we denote the set of indices of the @xmath17feasible solutions among the first @xmath244 iterations by @xmath245    [ t : conv ] suppose that assumptions * a1 * , * a2 * , * a3 * and * a4 * hold .",
    "let @xmath246 and let @xmath221 be the sequence generated by @xmath222",
    ". denote by @xmath247 the optimal function value of .",
    "then for every @xmath248 @xmath249 where @xmath250 @xmath251 @xmath252 @xmath253 @xmath186 and @xmath187 are as defined in , @xmath254 is as defined in corollary  [ cor:11 ] , and @xmath255 satisfies that @xmath196 for all @xmath224 .    using assumption * a4 *",
    ", suppose that @xmath256 is an optimal solution of ( [ p ] ) .",
    "fix @xmath257 , and @xmath258 .",
    "we begin by considering the following two cases :    case i : : :    @xmath259 .",
    "then    @xmath260 , and , by    @xmath261 , @xmath262 , and we have    @xmath263    and @xmath264 , and hence    @xmath265    therefore , using cauchy - schwarz inequality and the error bound in    equation @xmath266 @xmath267    hence @xmath268 case ii : : :    @xmath269 . then    @xmath270 . using    @xmath261 , @xmath262 , and we have    @xmath271    and @xmath272 , and hence    @xmath273    since @xmath274 we have    @xmath275    hence , using cauchy - schwarz inequality , the assumption that    @xmath196 for all    @xmath224 , and the error bound in    equation @xmath266 we have @xmath276    by combining case i and case ii , we have @xmath277    using ( [ 2.15 ] ) we have for all @xmath278 , with @xmath279 @xmath280 let",
    "@xmath281 , then using @xmath282    substituting @xmath283 in lemma  [ key ] we see that @xmath284 on the other hand , since @xmath1 is not a singleton , we have @xmath285 for every @xmath286 , and thus @xmath287    combining and yields @xmath288 using , we have @xmath289    and @xmath290 we recall that @xmath196 for all @xmath224 , @xmath291 and @xmath292 . now ,",
    "for every @xmath293 using corollary  [ cor:11 ] and we have @xmath294 using and we get @xmath295 using equations @xmath296 and @xmath297 , inequality becomes @xmath298 now , set @xmath299 .",
    "on the one hand , using and lemma  [ inequalities ] we get @xmath300 where @xmath301 . on the other hand , using the fact that @xmath302 we have @xmath303 where @xmath304 . using and @xmath305",
    "we deduce that @xmath306 which completes the proof .",
    "in this section we provide some numerical results of the @xmath222   algorithm .",
    "the @xmath222   algorithm was implemented in matlab . to begin we examine three academic test problems from @xcite .",
    "we then apply the @xmath222   algorithm to a simulation test problem from @xcite .",
    "we first consider three academic test problems from @xcite . in working with these problems ,",
    "we rewrite the constraint functions as a single constraint via a max function . for example , in @xmath307 the constraint functions are rewritten as @xmath308 , where @xmath309 , @xmath310 and @xmath311 .    1",
    ".   @xmath307 @xmath312 2 .",
    "@xmath313 @xmath314 3 .",
    "@xmath315 @xmath316    in @xcite and @xcite , the authors mention that their algorithms could not find an optimal solution to @xmath315 .",
    "this is due to them incorrectly stating that the optimal value is @xmath317 .",
    "the correct optimal value is @xmath318 , which we demonstrate below .",
    "define @xmath3 , @xmath319 , and @xmath320 as follows , @xmath321 notice that @xmath322 , so @xmath3 is strictly convex .",
    "+ the constraint set @xmath323 is also convex .",
    "let @xmath324 be the positive real root of @xmath325 .",
    "then at @xmath326 , and @xmath327 , with @xmath328 we have @xmath329 , @xmath330 and @xmath331 ; that is first order optimality holds . as the objective function and constraint set",
    "are convex , this implies optimality .",
    "the corresponding optimal value is @xmath318 .",
    "approximate values of @xmath332 and @xmath333 .",
    "we test @xmath222   on each of these three test problems using two options for creating the bregman distance . in the results of these test problems",
    "we shall use @xmath334 , and @xmath335 to denote the ( negative ) entropy @xmath336 . in table",
    "[ table : t1 ] we compare our results of the first three test problems to the results obtained by the pattern search method and simplex search method introduced in @xcite .",
    "note that , although in test problems 2 and 3 the constraint functions are non convex , the generated constraint set is convex .",
    "this is not covered by theorem  [ t : conv ] , however ; the @xmath222   still gives a good fit .",
    ".comparing results for test problems 1 , 2 , and 3 . [ cols=\"^,<,^,^,^,^ \" , ]     [ table : t8 ]    in table  [ table : t8 ] we see that with 500 function calls , @xmath222   is able to achieve a significantly better fit than the dps . while the fit for @xmath222   never quite achieves the quality of the drs+sa method , it comes quite close after 3000 function calls .",
    "this difference could be explained by the fact that the drs+sa method employs heuristics to break free of local minimizers .",
    "in this paper we developed the convergence analysis required to generate a derivative - free comirror algorithm , @xmath222 .",
    "furthermore , we provided some numerical results from the implementation of the algorithm in matlab .",
    "one natural line of future research is to adapt the algorithm to deal with the problem @xmath337 i.e. , @xmath338 , and to prove convergence .",
    "another line of future research is examining the convergence in the case where @xmath4 is not necessarily convex , but the constraint set remains convex .",
    "results from test problems 2 and 3 suggest that this is possible .",
    "[ inequalities ] for any integer @xmath339 the following inequalities hold true @xmath340 @xmath341    to see inequality , notice @xmath342 we now consider two cases ( @xmath244 is even and @xmath244 is odd ) .",
    "case i : suppose @xmath343 with @xmath344 .",
    "then @xmath345 case ii : suppose @xmath346 with @xmath344 .",
    "then @xmath347 moreover , for @xmath348 direct computation shows that @xmath349 , which together with , and proves the first inequality for all @xmath350 .",
    "finally , @xmath351 which proves inequality",
    "hhb was partially supported by the natural sciences and engineering research council of canada and by the canada research chair program .",
    "wlh was partially supported by the natural sciences and engineering research council of canada and ubc internal research funding .",
    "wmm was partially supported by the natural sciences and engineering research council of canada and ubc internal research funding .",
    "w.l . hare . using derivative free optimization for constrained parameter selection in a home and community",
    "care forecasting model . in _",
    "international perspectives on operations research and health care , proceedings of the 34th meeting of the euro working group on operational research applied to health sciences _ , pages 6173 , 2010 ."
  ],
  "abstract_text": [
    "<S> we consider @xmath0 where @xmath1 is a compact convex subset of @xmath2 , and @xmath3 and @xmath4 are continuous convex functions defined on an open neighbourhood of @xmath1 . </S>",
    "<S> we work in the setting of derivative - free optimization , assuming that @xmath3 and @xmath4 are available through a black - box that provides only function values for a lower-@xmath5 representation of the functions . </S>",
    "<S> we present a derivative - free optimization variant of the @xmath6-comirror algorithm @xcite . </S>",
    "<S> algorithmic convergence hinges on the ability to accurately approximate subgradients of lower-@xmath5 functions , which we prove is possible through linear interpolation . we provide convergence analysis that quantifies the difference between the function values of the iterates and the optimal function value . </S>",
    "<S> we find that the dfo algorithm we develop has the same convergence result as the original gradient - based algorithm . </S>",
    "<S> we present some numerical testing that demonstrate the practical feasibility of the algorithm , and conclude with some directions for further research .    </S>",
    "<S> * keywords : * convex optimization , derivative - free optimization , lower-@xmath5 , approximate subgradient , non - euclidean projected subgradient , bregman distance .    * </S>",
    "<S> 2010 mathematics subject classification : * primary 90c25 , 90c56 ; secondary 49m30 , 65k10 . </S>"
  ]
}