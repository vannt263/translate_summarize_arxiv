{
  "article_text": [
    "we consider a state - space models , that is , a pair of discrete - time stochastic processes , @xmath1 and @xmath2 . the hidden process @xmath3 is a  markov chain and the joint density for @xmath4 observations of the hidden process and the observations is @xmath5 where @xmath6 , @xmath7 , and @xmath8 , @xmath9",
    ", @xmath10 is a static parameter with prior @xmath11 . in particular , it is of interest to infer the posterior on @xmath12 , conditional upon @xmath13 ( @xmath14 ) as the time parameter @xmath4 grows .",
    "this problem is of interest in a wide variety of applications , including econometrics , finance and engineering ; see for instance @xcite .    in general ,",
    "even if @xmath15 is fixed , the posterior can not be computed exactly and one often has to resort to numerical methods , for example by using sequential monte carlo ( smc ) ( see e.g.  @xcite ) .",
    "smc makes use of a collection of proposal densities and sequentially simulates from these @xmath16 samples , termed particles . in most scenarios",
    "it is not possible to use the distribution of interest as a proposal .",
    "therefore , one must correct for the discrepancy between proposal and target via importance weights . in the majority of cases of practical interest , the variance of these importance weights increases with algorithmic time .",
    "this can , to some extent , be dealt with via a resampling procedure consisting of sampling with replacement from the current weighted samples and resetting them to @xmath17 .",
    "however , as is well known in the literature , due to the path degeneracy problem for particle filters , when @xmath15 is a random variable smc methods do not always work well ; see the review of @xcite for details .",
    "this has lead to a wide variety of techniques being developed , including @xcite ; see @xcite for a full review .",
    "the method which one might consider to be the state - of - the - art for the bayesian online static parameter estimation for state - space models , is that in @xcite .",
    "this approach combines the methods of smc samplers @xcite and particle markov chain monte carlo ( pmcmc ) @xcite .",
    "the method provides consistent estimates ( that is , as the number of samples grows ) of expectations w.r.t .",
    "the posterior on @xmath12 , as the time parameter grows . the method has been shown to work well in practice , but has one major issue ; the computational cost of the application of pmcmc kernels grows as the time parameter grows ; whilst the amount of times that the application of the kernel may stablize , one will still need to apply the kernel during the algorithm ( although a variety of tricks can be used to reduce the cost ; see @xcite ) .",
    "we present a version of the sm@xmath0 algorithm which has computational cost that does not grow with the time parameter .",
    "in addition , under assumptions , the algorithm is shown to provide consistent estimates of expectations w.r.t .  the posterior .",
    "however , the cost to achieve this consistency can be exponential in the dimension of the parameter space ; if this exponential cost is avoided , typically the algorithm is biased .",
    "the bias is investigated from a theoretical perspective and , under assumptions , we find that the bias does not accumulate as the time parameter grows .    our approach is based upon breaking up the observations into blocks of @xmath18 observations ; an approach used , differently , in other articles such as @xcite . the idea is simply to develop a ` perfect ' sm@xmath0 algorithm which has computational cost that does not grow with the time parameter ; in practice , one can not implement this algorithm and so one must approximate it .",
    "our approach simply uses the approximation of an appropriate target from the previous block .",
    "it is remarked that if the posterior exhibits concentration / bernstein - von mises properties as the time parameter grows , then alternative schemes are relevant , which could be prefered to the ideas here ; however , such properties do not always hold .",
    "see for instance the work of @xcite in the context of mle for relatively weak conditions ; the properties of the mle can impact on the concentration / bernstein - von mises properties of the posterior ( see e.g.  @xcite ) .    in section [ sec : algo ] , sm@xmath0 is reviewed and our algorithm is presented . in section [ sec : theory ] our theoretical results are presented , with proofs housed in the appendix . in section [ sec : simos ] our simulation results are presented . in section [ sec : summary ]",
    "the article is concluded and several possible extensions are discussed .",
    "let @xmath19 be a measurable space .",
    "the notation @xmath20 denotes the class of bounded and measurable real - valued functions .",
    "@xmath21 dentes the continuous , bounded measurable real - valued functions on @xmath22 .",
    "the supremum norm is written as @xmath23 .",
    "@xmath24 is the set of probability measures on @xmath19 .",
    "we will consider non - negative operators @xmath25 such that for each @xmath26 the mapping @xmath27 is a finite non - negative measure on @xmath28 and for each @xmath29 the function @xmath30 is measurable ; the kernel @xmath31 is markovian if @xmath32 is a probability measure for every @xmath26 . for a finite measure @xmath33 on @xmath19 , real - valued and measurable @xmath34 @xmath35 we also write @xmath36 . @xmath37",
    "denotes the total variation distance .",
    "smc@xmath39 uses smc to sample the following sequence of targets , for a fixed @xmath40 @xmath41 with @xmath42 , where @xmath43 with @xmath44 for any @xmath45 . throughout , we write the normalizing constant of @xmath46 as @xmath47 .",
    "we make the following defintion for notational convenience in the sequel : @xmath48    we provide a feynman - kac representation of the smc@xmath39 algorithm which facilitates theoretical analysis and will assist our subsequent notations .",
    "let @xmath49 and set @xmath50 and for any @xmath9 @xmath51 where @xmath52 is @xmath53invariant ( i.e.  typically a particle marginal metropolis - hastings ( pmmh ) kernel @xcite ) .",
    "then we set @xmath54 and @xmath55 the smc algorithm which approximates the @xmath56time marginal @xmath57 will have joint law : @xmath58 where @xmath59 is the usual selection - mutation operator and @xmath60 is the empirical measure of the particles .",
    "it is easily shown that by approximating @xmath61 one can approximate the posterior on @xmath15 and @xmath62 given @xmath63 ; see @xcite .",
    "we note that , typically one will apply the kernel @xmath52 when dynamic resampling is performed ; however the form of the algorithm is simple to describe with resampling at each time step .      one issue with the above algorithm is that whenever the kernel @xmath52 is applied , one must sample trajectories of the hidden states which grow with the time parameter .",
    "thus , even if resampling is dynamically performed , leading to an application of @xmath52 , the cost of the algorithm will increase with the time parameter .",
    "so one can say whilst sm@xmath38 is a very powerful algorithm , it is not an online algorithm . here",
    "we present an ` ideal ' or perfect version of the algorithm that has computational cost which does not grow with time , but can not be implemented in general .",
    "this algorithm will provide the basis for our biased algorithm in the next section",
    ".    to begin , we set @xmath64 which in principle can grow and @xmath65 which is fixed .",
    "the parameter @xmath18 will represent the maximum length of the trajectory of the hidden state , which one wants to sample .",
    "we define the following target probabilities : @xmath66 and @xmath67 @xmath68 @xmath69 where @xmath70 is as and @xmath71 if one can approximate the targets above one can approximate the posterior on @xmath15 and @xmath62 given @xmath63 . as we will see below , our algorithm to achieve this can not be implemented in practice , but has the benefit that the computational cost per - time step can not grow beyond a given bound .      for @xmath72 one can run the sm@xmath38 algorithm as in section [ sec : smc2 ] .",
    "that is to run the algorithm with law until time @xmath73 .",
    "we will add a final time step that will resample the @xmath74 particles according to @xmath75 .",
    "that is , defining @xmath76 as the dirac measure , we sample from @xmath77    for the subsequent blocks , we make the following definitions .",
    "let @xmath78 , @xmath79 , @xmath80 , @xmath81 , and @xmath82 .",
    "now let @xmath83 set @xmath84 define @xmath85 as a markov kernel of invariant density @xmath86 , @xmath87 such as a pmmh kernel . then define @xmath88 where @xmath89 and @xmath90 is as",
    ". set @xmath91 as a dirac mass .",
    "finally , for @xmath92 , @xmath93 and probability density @xmath94 on @xmath95 define @xmath96    then the perfect algorithm has joint law for @xmath78 , @xmath97 @xmath98 where @xmath99 is the empirical measure of the particles at time @xmath100 .",
    "set , for @xmath78 , @xmath93 @xmath101 @xmath102 and @xmath103 , then we note that for @xmath104 , @xmath105integrable @xmath106 @xmath107 so that one can estimate expectations w.r.t .",
    "@xmath108 via @xmath109      the problem with the previous algorithm is that one can seldom evaluate @xmath110 nor sample from it perfectly .",
    "we introduce the following algorithm , which will sample from the following targets . for the first block",
    ", one can run the sm@xmath38 algorithm to target the sequence .",
    "at the subsequent blocks , one is unable to evaluate the target , nor sample from the proposals .",
    "we propose the following approximate targets to replace : @xmath111 @xmath112 with @xmath113 , @xmath114 samples from the algorithm at the previous block , which we shall describe how to obtain and @xmath115 a kernel density whose bandwidth may depend on @xmath74 . at the end of a block of the algorithm , we just take @xmath116 , @xmath117 as the samples we have obtained , taking the first of the @xmath118tuples of @xmath119 ( as the samples are exchangeable ) . using standard smc theory , which we will expand upon",
    ", one can prove that if the bandwidth of @xmath31 appropriately depends on @xmath74 that at least @xmath120 will converge almost surely ( in an appropriate sense ) to @xmath121 as @xmath74 grows , hence providing the justification of the approximation introduced .",
    "set @xmath122 then the approximate algorithm has joint law for @xmath78 , @xmath97 @xmath123 where @xmath99 is the empirical measure of the particles at time @xmath100 .",
    "an estimate of the form can be used to estimate the targets . note that the cost of the algorithm is not @xmath124 as one does not need to evaluate @xmath125 , even in the pmmh steps .",
    "similar , but different , ideas have appeared in several articles including @xcite .",
    "these ideas are considered in the context of hidden markov models and partially observed point processes respectively .",
    "the key differences of our work to @xcite ( @xcite is for maximum likelihood estimation ( mle ) ) are as follows . in the context of @xcite we do not use a type of ` sequential mcmc ' , in that our approach can be used explicitly for online bayesian parameter estimation .",
    "the approach of @xcite is less general , where the particles are not updated with pmcmc , and one has a block length of 1 .",
    "alternatives to the approach outlined above are ; ( i ) a form of sequential pmcmc in the spirit of @xcite or ( ii ) to use an over - lapping or sliding window . for ( i ) , one expects that the cost is higher than the above algorithm , and online ( filtered ) estimates are not available . for ( ii )",
    "the cost may be higher , but , in simulation studies , we did not find any obvious improvement in practice .",
    "we also remark that our approximate algorithm uses kernel density estimation , but , in principle , any approximation scheme could be used ; this is demonstrated in section [ sec : simos ] where the method in @xcite is used in place of a kernel density estimate .",
    "in practice , the algorithm is implemented with dynamic resampling according to the effective sample size .",
    "throughout , it is supposed that for any @xmath126 , @xmath127 .      we will now show that , under a specific choice of the bandwidth @xmath128 and under some mathematical assumptions , the algorithm just presented is consistent .",
    "we set @xmath129 ; it is supposed that for any fixed @xmath15 , @xmath130 . for simplicity",
    "we denote @xmath131 ( resp .",
    "@xmath132 ) , @xmath72 as @xmath133 ( resp .  @xmath134 ) .",
    "we have    * @xmath135 , @xmath136 * @xmath137 * @xmath138 .    for each @xmath139 , @xmath140 , @xmath97 , there exists a @xmath141 such that for every @xmath142 @xmath143    [ hyp:3 ] we have @xmath144 , in addition there exist a @xmath145 such that for every @xmath74 @xmath146 .",
    "[ hyp:4 ] for any @xmath78 , @xmath97 , @xmath147 the function @xmath148 is continuous at @xmath149 uniformly in @xmath150 ; also @xmath151 .",
    "the first three assumptions are from @xcite to allow convergence of the smc estimate of the kernel density and the final assumption is from @xcite .",
    "we set @xmath152 let @xmath153 denote convergence in probability as @xmath154 .",
    "we now have the following result whose proof is in appendix [ app : a ] .",
    "[ theo : consis ] assume ( a1 - 4 ) .",
    "then for each @xmath139 , @xmath140 , @xmath97 and @xmath155 we have @xmath156 and in particular for every @xmath157 @xmath158    the result here is essentially qualitative . in order to obtain consistency , one must adopt an exponential effort in @xmath159 and so one would only run ( with the choice of @xmath128 as in ( a[hyp:3 ] ) ) over the exact algorithm if the time horizon is long and @xmath159 is small . in practice one will decouple @xmath128 and @xmath74 , leading to estimates which are biased , even if @xmath154 .",
    "we will now study the bias .",
    "we now investigate the asymptotic ( in @xmath74 ) bias of the approach .",
    "we make the following hypothesis :    [ hyp : bias ] there exists a @xmath160 such that for any @xmath161 : @xmath162 there exists an @xmath163 and for any @xmath78 , @xmath87 there exists a @xmath164 such that for any probability density @xmath94 on @xmath95 @xmath165 @xmath166    we write the @xmath167limiting version of as @xmath168 ( which one can easily prove exists , for any @xmath78 ) . for a bounded @xmath169",
    "we define the bias as @xmath170\\big|.\\ ] ] we have the following result whose proof is in appendix [ app : bias ] .",
    "[ theo : bias ] assume ( a[hyp : bias ] ) .",
    "then there exist a @xmath171 , @xmath172 such that for any @xmath78 : @xmath173    the result indicates that some of the bias can fall at a geometric rate as @xmath18 grows .",
    "note , that the bias can not disappear once , one block is wrong ( which is by assumption in the statement of the theorem ) .",
    "the result also provides the reassuring point that bias do not accumulate as the number of blocks grow , albeit under strong assumptions .",
    "we consider the following gaussian model , @xmath174 where @xmath175 is the density function of a normal distribution with mean @xmath33 and variance @xmath176 .",
    "the data is simulated from @xmath177 , with 10,000 time steps .",
    "the algorithms are implemented with @xmath178 fixed to @xmath179 and the other two being estimated .",
    "that is , the parameter is @xmath180 .",
    "we consider three algorithms .",
    "first , due to the simple structure of the model , we can obtain exact evaluation of @xmath181 through a kalman filter .",
    "we will replace the particle filter in the sm@xmath38 algorithm with the kalman filter ; this results in a regular smc algorithm .",
    "this is used to provide accurate unbiased estimators when comparing algorithms .",
    "the second is the sm@xmath38 algorithm and the third is the proposed new algorithm , which is termed sm@xmath38fw .",
    "the pmcmc step in each algorithm is constructed as a two - block metropolis random walk , with a normal kernel on the logarithm scale .    with the kalman filter ,",
    "the computational cost of @xmath181 is negligible and thus we use a large number of @xmath15-particles , @xmath182 .",
    "this allows us to obtain unbiased posterior estimators with very small variance . as a result",
    ", this provides a good baseline for the comparison of the other two algorithms .    for the sm@xmath38 algorithm",
    ", we use @xmath183 particles for the smc algorithm and @xmath184 particles . for simplicity",
    ", the value of @xmath185 is fixed through the time line .",
    "for the sm@xmath38fw algorithm , we set @xmath186 and @xmath184 . as we will see later , despite the increased number of the @xmath15-particles , the computational cost is still significantly lower than sm@xmath38 , in addition to be upper bounded per time step . the kernel density estimate ( kde ) is bivariate normal on the logarithm scale , with covariance matrix taken a diagonal form , @xmath187 where @xmath188 is the identity matrix of rank two .",
    "we consider @xmath189 , @xmath190 and @xmath191 .",
    "three widths of each fixed window @xmath192 , @xmath193 and @xmath194 are also considered .      in figure",
    "[ fig : gl_mean_x ] to  [ fig : gl_mean_lambda ] we show the average of estimates over 30 simulations , given different bandwidth @xmath128 and window width @xmath18 .",
    "the first 125 time steps are cutoff from the graphs , during which time the sm@xmath38fw algorithm is exactly the same as the sm@xmath38 algorithm .     for the gaussian linear model .",
    "]     ( transition precision ) for the gaussian linear model . ]",
    "( observation precision ) for the gaussian linear model . ]",
    "algorithms labeled with `` kalman '' means a kalman filter is used in place of a particle filter and thus exact values of @xmath181 are calculated instead of approximations . in this case , the results show the behavior of the sm@xmath38fw algorithm when @xmath195 .",
    "however , in this particular example , errors introduced by the particle filter approximation is minimal as shown in the graphs .",
    "it is as expected that the longer the window width , the better the sm@xmath38fw algorithms perform .",
    "the choice of the bandwidth @xmath128 has a more dramatic effect on the performance .",
    "with @xmath196 the algorithms give almost exact results even for @xmath192 .",
    "on the other hand , with @xmath197 , the errors are numerous . for the state @xmath198 , neither the bandwidth nor the window width affect the results in any observable way .",
    "for the parameters , @xmath196 and @xmath199 provides the best results .    in figure  [",
    "fig : gl_mse ] the mse of the two algorithms are shown , using the results obtained with a smc algorithm using kalman filters ( sm@xmath38 ( kalman ) in previous figures ) as an unbiased , accurate estimate of the true posterior means .",
    "it can be seen that for the state @xmath198 , the results are mixed . for the @xmath200 parameter",
    "the sm@xmath38 algorithm has a smaller mse while the opposite is true for the @xmath201 parameter .",
    "it shall be noted that , the sm@xmath38fw algorithm use @xmath186 while the sm@xmath38 algorithm only use @xmath183 . however , despite the large difference of the numbers of @xmath15-particles , the sm@xmath38fw algorithm is still significantly more computationally cost efficient .",
    "it took about thirty minutes for a single run of the sm@xmath38fw algorithm under this setting while it took about five hours for the sm@xmath38 algorithm .",
    "more importantly , the cost of sm@xmath38fw is bounded per time step , and thus it is possible to obtain better results than sm@xmath38 for all parameters while having a bounded , smaller cost in the long run .",
    "fw algorithm uses @xmath196 and @xmath202 . ]",
    "we consider the -driven stochastic volatility model , applied to 1,000 recent s&p 500 data from september 17 , 2010 to september 8 , 2014 .",
    "the data are obtained as logarithm return of the daily adjusted close price , and then normalized to unity variance .",
    "the data is plotted in figure  [ fig : levy_data ] .",
    "there are considerably much larger volatility at the beginning of the series .",
    "it becomes much more stable in the middle and slightly larger at the end .",
    "the model we employed to analyze this data is the same as in @xcite and we use the same notations and formulation as in that paper .",
    "the model has four parameters @xmath203 and a two - dimensional state @xmath204 where @xmath205 is the time interval and in this example it set to constant @xmath179 .",
    "we consider three algorithms .",
    "first , the sm@xmath38 algorithm with @xmath206 and @xmath184 .",
    "second , the sm@xmath38fw algorithm with the same number of particles .",
    "the same normal kde approximation is used as in the last example .",
    "various bandwidths of the kernel were considered , and @xmath196 is chosen .",
    "the last , the idea coupled with the parallel particle filter algorithm ( ppf ) @xcite , instead of the kde approximation , is considered .",
    "the latter two algorithms use a window width @xmath207 .",
    "we found that further increasing the window width does not improve results in a significant manner .",
    "the algorithms again use normal kernels on logarithm scales for the pmmh proposals . using results from the particle filter paper",
    ", we believe that @xmath208 and @xmath209 are strongly correlated and they are updated in one block of pmcmc move while @xmath210 and @xmath201 are updated individually in their own block .",
    "the proposal scales are calibrated on - line using the moments estimates from the sampler .      in figure",
    "[ fig : levy_avg ] we show the average of estimates over 20 simulations .",
    "all three algorithms give similar results .",
    "compared to results from the sm@xmath38 algorithm , the sm@xmath38fw algorithm give slightly better result than that of the sm@xmath38fw - ppf one .",
    "one feature that was mentioned in section [ sec : theory ] , but not shown clearly in the early simple gaussian linear example , is that the bias of the online algorithms does not accumulate over time .",
    "for instance , consider the @xmath176 state , though non - trivial errors can be observed for both on - line algorithms in the early time steps , they were not carried on to later times .",
    "in addition to the estimates , prediction of the variance ( square of volatility ) , @xmath211 is also calculated as @xmath212 = { \\mathbb{e}}[{\\mathbb{e}}[x_n|x_{n-1}]|y_{1:n-1}]$ ] . since",
    "the transition density is not of a closed form , its expectation is estimated with 100 samples of @xmath213 generated for each value of @xmath214 in the particle system .",
    "the outer expectation is approximated with the particle system at time @xmath215 .",
    "the results for the three algorithms are plotted in figure  [ fig : levy_vol ] against the squared log - returns .          through two examples",
    ", it is shown that with some careful choice of the kde bandwidth , or using the parallel particle filter algorithm , which does not require this layer of tuning and an appropriate window width , it is possible to obtain results competitive to the sm@xmath38 algorithm with only a fraction of computational cost . in the -driven stochastic model example , the theoretical feature that the bias does not accumulate over time is demonstrated .",
    "in this article we have presented a method for bayesian online static parameter estimation for state - space models .",
    "our method is such that the computational cost does not grow with the time parameter and moreover , the algorithm can be shown to be consistent , although , the cost is then exponential in the dimension of the static parameter .",
    "we have additionally shown that the asymptotic bias , under strong assumptions , does not grow with time .",
    "there are several avenues for future work .",
    "first is the use of alternative approximation schemes in our algorithm ; we have relied on kernel density estimation , but there are other schemes which could be used .",
    "second , in our work , we have investigated the asymptotic bias . however , as is clear in the proofs , then the consecutive blocks are independent and one thus expects that the study of the finite sample bias is significantly more challenging ; an investigation of this is warranted .",
    "this research was supported by a singapore ministry of education academic research fund tier 1 grant ( r-155 - 000 - 156 - 112 ) .",
    "we thank alex beskos & alex thiery for many useful conversations on this work .",
    "for @xmath72 the result follows by standard theory ; see @xcite .",
    "we first consider @xmath216 .",
    "denote by @xmath217 the @xmath218algebra generated by the particle system up - to time @xmath18 and expectations w.r.t .",
    "the law of the simulated algorithm as @xmath219 .",
    "then we have @xmath220 @xmath221 + \\mathbb{e}[\\check{\\eta}_{t , t}^{n}(k_n(\\theta)f_{\\theta}(x_{t+1}|\\cdot))|\\mathscr{f}_{t } ] - \\zeta(\\theta , x_{t+1}).\\ ] ] for the first term on the r.h.s .",
    "one has by the ( conditional ) marcinkiewicz - zygmund inequality ( conditional on @xmath217 the samples are generated independently ) : @xmath222)^2 ] \\leq \\frac{c'}{\\sqrt{n}}\\ ] ] where @xmath223 depends on @xmath224 in ( a[hyp:3 ] ) ; thus @xmath225 $ ] converges to zero in probability .",
    "now , for @xmath226 - \\zeta(\\theta , x_{t+1})\\ ] ] @xmath227=\\phi_t(\\eta_{t-1}^n)(k_n(\\theta)f_{\\theta}(x_{t+1}|\\cdot))$ ] , and the denominator converges in probability and by the arguments in @xcite[theorem 4.1 ] the numerator will converge in probability to the appropriate quantity ; that is @xmath228 .    for @xmath229",
    ", this converges in probability to @xmath230 by centering by the conditional expectation ( given @xmath231 ) and applying the ( conditional ) marcinkiewicz - zygmund inequality for @xmath232 $ ] . the term @xmath233 $ ] will converge to @xmath230 using the conditional i.i.d .",
    "property and the fact that @xmath216 converges in probability to @xmath234 .    for @xmath235",
    ", we consider : @xmath236 the first term is dealt with via the ( conditional ) marcinkiewicz - zygmund inequality as above . for @xmath237 as @xmath238 converges in probability to @xmath239 , we need only consider @xmath240 @xmath241 @xmath242 the last term on the r.h.s",
    ".  converges to zero by the above calculations .",
    "so we focus on the first term on the r.h.s .",
    "we have @xmath243 \\leq\\ ] ] @xmath244|]\\ ] ] let @xmath245 be given . by ( a[hyp:4 ] ) there exists @xmath246 independent of @xmath247 such that for any probability density @xmath248 with @xmath249 we have that @xmath250 .",
    "consider the event : @xmath251 then @xmath252| ] = \\ ] ] @xmath252|\\mathbb{i}_{a(n,\\delta ) } ] + \\ ] ] @xmath252|\\mathbb{i}_{a(n,\\delta)^c } ] \\leq\\ ] ] @xmath253 by the convergence in probability of @xmath216 here is an @xmath254 such that for each @xmath255 we have @xmath256\\leq \\frac{\\epsilon}{2}$ ] .",
    "hence , for any @xmath255 : @xmath257| ] < c\\epsilon\\ ] ] and as @xmath245 was arbitrary , the term of interest goes to zero in @xmath258 ; this completes the proof .",
    "the proof can also be repeated if one considers a @xmath259 as part of the function ( the argument is almost the same ) .",
    "the proofs at subsequent times follow the above arguments and are omitted for brevity .",
    "in the context of the proof for the bias , we need only consider one block ( as will become apparent in the proof ) , as blocks are independent in the asymptotic bias .",
    "in addition , one significantly simplify the notations by simply considering two feynman - kac formula of @xmath18 steps , with different initial distributions , the same potentials and different markov kernels on measurable spaces @xmath260 .",
    "thus , for @xmath261 the two feynman - kac @xmath56time marginals : @xmath262 with @xmath263 this corresponds to our case , as the potentials are the same , with the markov kernels and initial distributions different .",
    "our proofs will depend a lot on the bayes rule , which we now recall , for @xmath264 @xmath265 we use the notation @xmath266 , @xmath267 ( with the convention when @xmath268 , one returns @xmath33 ) .",
    "our assumption ( a[hyp : bias ] ) under the modified notation is        we have that , under our modified notations @xmath276 by lemma [ lem : forget ] @xmath277 then @xmath278 as @xmath279 for any @xmath280 .",
    "now @xmath281(g_{t-1}\\varphi)}{\\phi_{t-1}^1(\\eta_0 ^ 2)(g_{t-1 } ) } + \\\\ & & \\frac{\\phi_{t-1}^2(\\eta_0 ^ 2)(g_{t-1}\\varphi)}{\\phi_{t-1}^1(\\eta_0 ^ 2)(g_{t-1})\\phi_{t-1}^2(\\eta_0 ^ 2)(g_{t-1})}\\times \\\\ & & [ \\phi_{t-1}^2(\\eta_0 ^ 2)(g_{t-1})-\\phi_{t-1}^1(\\eta_0 ^ 2)(g_{t-1})].\\end{aligned}\\ ] ] then , by application of proposition [ prop : bias ] along with ( a[hyp : bias ] ) it follows that @xmath282 @xmath283 which allows one to conclude the proof .",
    "as one can see from inspection of the proof , the difference in initial distribution does not impact the bound .",
    "moreover , the difference in markov kernels is controlled , leading to a control of the bias ; such a latter property is not obvious _ a priori _ and needs to be proved .",
    "as is evident from the proof , it does not matter which block one considers , under our assumptions .",
    "we have the standard telescoping sum , for @xmath287 $ ] @xmath288(\\varphi)\\ ] ] @xmath289(\\varphi).\\ ] ] now , by lemma [ lem : forget]@xmath290 @xmath291 by lemma [ lem : tech_res_bias ] @xmath292 so we have proved that @xmath293 from which one can easily conclude .",
    "we have @xmath297(\\varphi ) = \\frac{1}{\\mu(g_{p-1})}\\mu(g_{p-1}[m_p^1-m_p^2](\\varphi)).\\ ] ] now , by ( a[hyp : bias ] ) , for any @xmath298 one can write @xmath299 where @xmath300 is a markov kernel : @xmath301 thus @xmath302(\\varphi)(x ) = ( 1-\\epsilon)[r_p^1-r_p^2](\\varphi)(x)\\ ] ] from which one easily concludes .",
    "the following lemma just collects some results of @xcite into a convenient form for use in the above proofs .",
    "we make the following defintions for @xmath303 , @xmath261 : @xmath304 , @xmath305 and @xmath306 . for a bounded and measurable real - valued function",
    "@xmath307 we denote @xmath308      by ( * ? ?",
    "* theorem 4.3.1 ) @xmath312 where @xmath313 is the dobrushin coefficient of @xmath314 .",
    "then by @xcite ( fourth displayed equation ) and ( a[hyp : bias ] ) @xmath315 note that via ( a[hyp : bias ] ) and ( * ? ? ?",
    "* lemma 4.1 ) one has @xmath316 and as a result via @xcite , for any pair of probabilities @xmath310 @xmath317 the result thus follows ."
  ],
  "abstract_text": [
    "<S> we consider bayesian online static parameter estimation for state - space models . </S>",
    "<S> this is a very important problem , but is very computationally challenging as the state - of - the art methods that are exact , often have a computational cost that grows with the time parameter ; perhaps the most successful algorithm is that of sm@xmath0 @xcite . </S>",
    "<S> we present a version of the sm@xmath0 algorithm which has computational cost that does not grow with the time parameter . </S>",
    "<S> in addition , under assumptions , the algorithm is shown to provide consistent estimates of expectations w.r.t .  the posterior . </S>",
    "<S> however , the cost to achieve this consistency can be exponential in the dimension of the parameter space ; if this exponential cost is avoided , typically the algorithm is biased . </S>",
    "<S> the bias is investigated from a theoretical perspective and , under assumptions , we find that the bias does not accumulate as the time parameter grows . </S>",
    "<S> the algorithm is implemented on several bayesian statistical models . + * keywords * : state - space models ; bayesian inference ; sequential monte carlo .    * biased online parameter inference for state - space models *    by yan zhou & ajay jasra    department of statistics & applied probability , national university of singapore , singapore , 117546 , sg . </S>",
    "<S> + e-mail:`stazhou@nus.edu.sg , staja@nus.edu.sg ` + </S>"
  ]
}