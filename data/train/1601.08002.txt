{
  "article_text": [
    "orthogonal transform such as discrete wavelet transform is an important tool in statistical signal processing and analysis .",
    "especially , wavelet denoising is a popular application of discrete wavelet transform . in wavelet denoising",
    ", noisy signal is transformed into wavelet domain in which wavelet coefficients are obtained . by applying a thresholding method ,",
    "noise - related parts of coefficients are removed in a sense ; e.g. some of coefficients are set to zero .",
    "the inverse wavelet transform of the modified coefficients yields a denoised signal .",
    "the most popular and simple methods of thresholding is hard and soft - thresholding in @xcite .",
    "both thresholding methods have a parameter . in hard - thresholding method , the parameter works purely as a threshold level ; i.e. coefficients less than the parameter value are removed and un - removed coefficients are harmless . on the other hand , in soft - thresholding ,",
    "the parameter works as a threshold level as in hard - thresholding and simultaneously as an amount of shrinkage for un - removed components .",
    "coefficients less than the parameter value are removed and un - removed coefficients are shrunk toward zero by the parameter . for a better denoising performance , we need to determine an optimal value of the parameter .",
    "for example , in hard - thresholding , if the parameter value is too large then most of coefficients are removed even when those are significant .",
    "this results in an excess smoothing that yields a large bias between estimated output and target function output .",
    "on the other hand , if the parameter value is too small then most of coefficients are un - removed even when those are not significant .",
    "this results in a large variance of output estimate and , thus useless for denoising .",
    "a problem of choice of an optimal parameter value is often referred as a model selection problem .",
    "there are several model selection methods under thresholding .",
    "@xcite has proposed universal hard and soft - thresholding in which a theoretically significant constant value is employed as a parameter value .",
    "also , @xcite has derived a criterion for determining an optimal parameter value of soft - thresholding by applying stein s lemma@xcite . the soft - thresholding method with this criterion",
    "is called as sure ( stein s unbiased risk estimator ) shrink in @xcite .",
    "unfortunately , there is no such a theoretically supported criterion for hard - thresholding while modified cross validation approaches have been proposed @xcite .",
    "we focus on a soft - thresholding method in this paper . as previously mentioned , soft - thresholding is a combination of hard - thresholding and shrinkage in which both of threshold level and amount of shrinkage are simultaneously controlled by a single parameter .",
    "the parameter is a threshold level for removing un - necessary components and is also an amount of shift by which estimators of coefficients of un - removed components are shrunk toward to zero .",
    "if the parameter value is large then threshold level is large .",
    "therefore , the number of un - removed components is small .",
    "however , at the same time , the amount of shrinkage is automatically large .",
    "this can be an excess shrinkage amount which may yields a large bias of output estimate in representing a target function .",
    "this may cause a high prediction error at a relatively small model even when it can represent a target function ; i.e. even when it can obtain a sparse representation .",
    "therefore , the number of un - removed components in soft - thresholding tends to be large if we choose the parameter value based on a substitution of prediction error such as sure and cross - validation error .",
    "this is an inevitable problem of soft - thresholding , which is brought about by an introduction of a single parameter for controlling both of threshold level and amount of shrinkage simultaneously .",
    "note that , in the implementation of thresholding methods for wavelet denoising in @xcite , thresholding is recommended to apply only to detail coefficients .",
    "this heuristics may be actually valid to avoid the problem mentioned here .",
    "on the other hand , in machine learning and statistics , there are several model selection methods by using regularization , in which coefficient estimators are obtained by minimizing a regularized cost that consists of error term plus regularization term .",
    "a regularization method has a parameter that is multiplied by regularizer in the regularization term and determines a balance between error and regularization .",
    "lasso ( least absolute shrinkage and selection operator ) is a very popular regularization method for variable selection@xcite .",
    "it employs sum of absolute values of coefficients as a regularizer ; i.e. @xmath0 norm of a coefficient vector .",
    "lasso is known to be useful for obtaining a sparse representation of a target function ; i.e. the number of components for representing a target function is very small . in lasso ,",
    "extra components are automatically removed by setting their coefficients to zero .",
    "this property is clearly understood when it applied to orthogonal regression problems . in this case",
    ", lasso reduces to a soft - thresholding method in which a parameter of soft - thresholding is a regularization parameter divided by 2 .",
    "hence , a sparseness obtained by lasso comes from a sof - thresholding property . and , thus , lasso encounters the above mentioned problem of soft - thresholding .",
    "this dilemma between sparsity and prediction of lasso has already been discussed in @xcite and @xcite .",
    "@xcite has proposed scad ( smoothly clipped absolute deviation ) penalty which is a nonlinear modification of @xmath0 penalty .",
    "@xcite has proposed adaptive lasso that employs weighted @xmath0 penalties .",
    "an @xmath0 penalty term is modified by different ways ( functions ) in scad and adaptive lasso while shrinkage is suppressed for large values of estimators in both methods .",
    "this may reduce an excess shrinkage at a relatively small model .",
    "especially , in case of orthogonal regression , weights of adaptive lasso are effective for directly and adaptively reducing a shrinkage amount that is represented as a shift in soft - thresholding . in these methods ,",
    "cross validation is used as a model selection method for choosing parameter values such as a regularization parameter .",
    "unfortunately , usual cross validation can not be used in orthogonal regression unless it is heuristically modified as in @xcite .    in this paper",
    ", we introduce a scaling of soft - thresholding estimators ; i.e. a soft - thresholding estimator is multiplied by a scaling parameter . unlike adaptive lasso , introduction of scaling is intended to control threshold level and amount of shrinkage independently .",
    "it is thus a direct solution for a problem of parametrization of soft - thresholding .",
    "if the scaling parameter value is less than one then it works as shrinkage of soft - thresholding estimator . for an orthogonal regression problem",
    ", this is equivalent to elastic net@xcite in machine learning .",
    "however , the scaling parameter can be larger than one by which the above mentioned excess shrinkage in soft - thresholding is expected to be relaxed ; i.e. scaling expands a shrinkage estimator obtained by soft - thresholding . especially in this paper , we propose a component - wise and data - dependent scaling method ; i.e. scaling parameter value can be different for each coefficient and is calculated from data .",
    "we refer the proposed scaling as adaptive scaling . in this paper",
    ", we derive a risk under adaptive scaling and construct a model selection criterion as an unbiased risk estimate .",
    "therefore , our work establishes a denoising method in which a drawback of a naive soft - thresholding is improved by the introduction of adaptive scaling and an optimal model is automatically selected according to a derived criterion under the adaptive scaling .",
    "in section 2 , we state a setting of orthogonal non - parametric regression that includes a problem of wavelet denoising . in this section",
    ", we also give a naive soft - thresholding method and several related methods . in this paper ,",
    "especially , we employ a soft - thresholding method based on lars ( least angle regression)@xcite in these methods . in lars - based soft - thresholding , a model selection problem reduces to the determination of the number of un - removed components . in section 3",
    ", we define an adaptive scaling and derive a risk under lars - based soft - thresholding with the adaptive scaling .",
    "we then give a model selection criterion as an unbiased estimate of the risk .",
    "we here also consider the properties of risk curve and reveals the model selection property .",
    "the proofs of theorems in this section are included in appendix with some lemmas . in section 4 , the proposed adaptive scaling method",
    "is examined for toy artificial problems including applications to wavelet denoising .",
    "section 5 is devoted to conclusions and future works .",
    "let @xmath1 and @xmath2 be input variables and an output variable , for which we have @xmath3 i.i.d .",
    "samples : @xmath4 , where @xmath5 .",
    "we assume that @xmath6 , @xmath7 , where @xmath8 are i.i.d additive noise sequence according to @xmath9 ; i.e. normal distribution with mean @xmath10 and variance @xmath11 .",
    "@xmath12 is a target function .",
    "we assume that @xmath13 are fixed below .",
    "we define @xmath14 , @xmath15 and @xmath16 , where @xmath17 denotes a matrix transpose .",
    "we then have @xmath18 and @xmath19=\\h$ ] , where @xmath20 denotes the expectation with respect to the joint probability distribution of @xmath21 .",
    "let @xmath22 be a series of functions on @xmath23 .",
    "we consider to estimate a target function by a linear combination of @xmath3 functions in this series : @xmath24 where @xmath25 is a coefficient vector .",
    "this is a non - parametric regression problem .",
    "we call @xmath26 a component or basis function .",
    "we assume that there exists @xmath27 and @xmath28 such that @xmath29 for any @xmath30 when @xmath31 .",
    "@xmath32 can be zero for some @xmath33 .",
    "we define @xmath34 and denote the complement of @xmath35 by @xmath36 .",
    "we call @xmath26 with @xmath37 true component or non - zero component .",
    "we also define @xmath38 which is the number of true components or non - zero components .",
    "we assume that @xmath39 ; i.e. true components are always included in a model .",
    "we also assume that @xmath40 is very small compared to @xmath3 .",
    "these two assumptions say that there exists a sparse representation of a target function in terms of a set of @xmath3 components .",
    "let @xmath41 be an @xmath42 matrix whose @xmath43 element is @xmath44 .",
    "we assume that the orthogonality condition : @xmath45 where @xmath46 denotes an @xmath42 identity matrix .",
    "we thus consider a non - parametric orthogonal regression problem ; e.g. discrete fourier transform and discrete wavelet transform for typical examples .",
    "the least squares estimator under the orthogonality condition is given by @xmath47 note that we have @xmath48 here .",
    "since there exists a @xmath49 such that @xmath50 , @xmath51 holds by the assumption on additive noise ; i.e. multivariate normal distribution with a mean vector @xmath49 and a unit covariance matrix multiplied by @xmath52 . in other words ,",
    "@xmath53 , @xmath54 and @xmath55 are independent .",
    "we define @xmath56 , @xmath54 , where @xmath57 is a sign function .",
    "we define @xmath58 as an index sequence for which @xmath59 holds .",
    "note that we can exclude the case of ties in our probabilistic evaluations in this paper since this is guaranteed with probability one by ( [ eq : dist - of - ecv ] ) .",
    "let @xmath60 with a parameter @xmath61 be a soft - thresholding estimator , in which @xmath62 where @xmath63 .",
    "@xmath64 determines both of a threshold level and amount of shrinkage . under the orthogonality condition",
    ", several sparse modeling methods can be reduced to soft - thresholding estimator .    for a fixed @xmath65 , cost function of lasso",
    "is given by @xmath66 where @xmath67 is the euclidean norm and @xmath68 ; i.e. lasso introduces an @xmath0 regularizer .",
    "@xmath69 is a regularization parameter .",
    "a minimizer of ( [ eq : cost - lasso ] ) under the orthogonality condition is known to be a soft - thresholding estimator with @xmath70 ; i.e. it is @xmath71 . on the other hand , for fixed @xmath65 and @xmath72 , cost function of elastic net",
    "is given by @xmath73 thus , elastic net introduces both of an @xmath0 regularizer and @xmath74 regularizer .",
    "as shown in @xcite , a minimizer of ( [ eq : cost - elnet ] ) under the orthonormality condition is given by @xmath75 , @xmath76 . since @xmath72 ,",
    "the solution of elastic net is obtained by shrinking lasso estimator which is a soft - thresholding estimator .",
    "on the other hand , lars ( least angle regression ) @xcite is a greedy iterative algorithm in which a component is appended to a model at each step .",
    "this can be viewed as a sparse modeling method if we can find an optimal step .",
    "for this purpose , a @xmath77 type criterion is derived under a mild condition in @xcite . as shown in @xcite and lemma 1 in @xcite , under the orthonormality condition , lars",
    "is also reduced to soft - thresholding estimator in which the parameter value is given by @xmath78 at the @xmath79th step ; i.e. it is the @xmath80th largest absolute value among the least squares estimators .",
    "therefore , a set of candidates of parameter values is @xmath81 in lars . by this choice of threshold level , the number of un - removed components at the @xmath79th step is equal to @xmath79 .",
    "therefore , a model selection problem of lars - based soft - thresholding is the determination of the number of un - removed components .",
    "we refer to lars - based soft - thresholding as lst .    as a modification of lasso , adaptive",
    "lasso@xcite introduces a weighted @xmath0 regularizer , in which a weight for the @xmath33th component is @xmath82 and a choice of @xmath83 with @xmath84 is especially considered in @xcite .",
    "the solution of adaptive lasso under the orthonormality condition is given by @xmath85 it is regarded as a soft - thresholding estimator with a component - wise and data - dependent parameter . if @xmath86 is large then @xmath82 is small . in this case ,",
    "threshold level and amount of shrinkage for the corresponding estimator is small .",
    "this reduces a bias , or equivalently , an excess shrinkage of estimator especially when the estimator is actually valid ; i.e. the corresponding component is needed . in other words ,",
    "adaptive lasso avoids an excess shrinkage on estimators of un - removed components by an adaptive manner ; i.e. by controlling a component - wise and estimator - dependent `` shift '' in soft - thresholding estimator .",
    "this relaxes the problem of employing a single parameter value for both of threshold level and amount of shrinkage in soft - thresholding .",
    "we can choose a small parameter value for valid components and a large value for non - essential components ; i.e. the parameters mainly work as threshold levels for removing non - essential components .    in this paper , by introducing scaling for soft - thresholding estimator",
    ", we consider to control threshold level and amount of shrinkage independently .",
    "our approach is different from adaptive lasso while they serves the same purpose .",
    "as seen in later sections , the advantage of employing scaling is that we can construct a model selection criterion that is required in applications .",
    "let @xmath87 be a vector of the above mentioned lst estimators that are defined by @xmath88 where @xmath89 .",
    "we define @xmath90 for @xmath91 . in this paper",
    ", we consider to employ @xmath92 in which @xmath93 we call @xmath94 , @xmath54 component - wise scaling parameters .",
    "let @xmath95 be an @xmath42 diagonal matrix whose @xmath96 element is @xmath94 .",
    "we can write @xmath97 .",
    "we define @xmath98 .",
    "note that , in a matrix formulation , @xmath99 and @xmath100 are used as vertical vectors .",
    "as in the previous discussion , if we restrict @xmath101 then the method is elastic net which yields shrinkage of soft - thresholding estimator .",
    "therefore , introduction of scaling parameter can be viewed as an extension of elastic net .",
    "however , we expect that scaling is used for expanding soft - thresholding estimator ; i.e. @xmath102 is desirable .",
    "note that @xmath103 is a two stage estimate in which lst is firstly applied and then scaling is applied . scaling re - adjusts only amount of shrinkage . a risk for lst with component - wise scaling",
    "is defined by @xmath104 where the latter definition is due to the orthogonality condition ( [ eq : ot - cond ] ) .",
    "a naive lst is a case of @xmath105 , where @xmath106 is an @xmath3-dimensional vector of one s . for this case , we have @xmath107 as a special case of @xcite . more generally , in case of introducing a single common scaling parameter @xmath108 on all components",
    ", @xcite has shown that @xmath109 therefore , an unbiased risk estimate is given by @xmath110 which can be used as a model selection criterion for choosing an optimal @xmath79 if we replace @xmath11 with its estimate @xmath111 . for this case ,",
    "an optimal scaling value that minimizes the risk is given by @xmath112+\\sigma^2k / n } { \\e\\left[\\sum_{j\\in\\ek_k}\\eb_{k , j}^2\\right]}.\\ ] ] in practical application , for example , @xmath113 can be an estimate of the optimal value .",
    "we state some definitions used below .",
    "we define @xmath114 and @xmath115 .",
    "we define @xmath116 and @xmath117 , by which @xmath118 ; i.e. @xmath119 and @xmath120 are mutually independent .",
    "we define @xmath121 in applying lst . correspondingly , by ( [ eq : def - eb - k - j ] ) , we define @xmath122 and @xmath123 .",
    "we also define @xmath124 , by which @xmath125 are i.i.d . according to @xmath126 by the definition of @xmath120 . for an event @xmath127",
    ", we denote the complement of @xmath127 by @xmath128 and indicator function of @xmath127 by @xmath129 .",
    "we define @xmath130 and @xmath131 .",
    "we also define @xmath132 .",
    "we denote @xmath133 distribution with one degree of freedom by @xmath134 .",
    "the purpose of scaling is to avoid excess shrinkage of coefficients of un - removed components .",
    "then , it is reasonable to choose @xmath94 so as to satisfy @xmath135 .",
    "this yields @xmath136 when @xmath137 is small .",
    "this approximation is valid since an un - removed component may have a coefficient estimate that is sufficiently larger than an appropriate threshold level . in this paper",
    ", we hence employ @xmath138 as empirical values , where @xmath108 is a finite constant that is defined to avoid @xmath139 when @xmath140 .",
    "we define @xmath141 .",
    "( [ eq : component - wise - scaling ] ) gives data - dependent and component - wise scaling value .",
    "we refer this scaling method as adaptive scaling . by ( [ eq : component - wise - scaling ] )",
    ", the adaptive scaling value is always larger than one .",
    "note also that @xmath142 is valid only to @xmath143 since @xmath144 for @xmath145 .",
    "let @xmath146 be an @xmath42 diagonal matrix whose @xmath96 element is @xmath142 .",
    "we define a risk for our adaptive scaling estimator by @xmath147      we state three theorems whose proofs are given in appendix with some lemmas .",
    "[ theorem : rnk - evalpha ] for @xmath148 defined in ( [ eq : component - wise - scaling ] ) , @xmath149\\ ] ] holds .",
    "[ theorem : ealpha_j - convergence ] we define @xmath150 with @xmath151 . for @xmath37 , @xmath152=0\\ ] ] holds .",
    "this implies that , for @xmath37 , @xmath153=0\\ ] ] holds for any @xmath154 . on the other hand , we assume that @xmath155 . then , for @xmath156 , @xmath157=0\\ ] ] holds for any @xmath154 .",
    "[ theorem : r(1)-r(evalpha ) ] @xmath158 holds for a sufficiently large @xmath3 .",
    "we give some remarks .",
    "* by theorem [ theorem : rnk - evalpha ] , @xmath159 is an unbiased estimator of risk under adaptive scaling with @xmath148 defined by ( [ eq : component - wise - scaling ] ) .",
    "therefore , this can be a model selection criterion for choosing an optimal @xmath79 if we can set an appropriate estimate of noise varinace @xmath11 in ( [ eq : ernk_evalpha ] ) . * by lemma [ lemma : p - oen*-bound ] , the probability that all of true components are un - removed is high when @xmath160 and @xmath3 is sufficiently large ; i.e. lst has a kind of consistency in selecting true components if those exist .",
    "note that since our adaptive scaling is applied to lst estimator , this consistency result applies to adaptive scaling estimators .",
    "* theorem [ theorem : ealpha_j - convergence ] says that , in a large sample situation , scaling values are larger than @xmath161 for components that are not true .",
    "some of non - true components are selected when @xmath162 .",
    "this excess expansion of coefficient estimators for non - true components may cause a high risk for @xmath162 .",
    "therefore , @xmath163 may hold for @xmath162 even though @xmath164 holds by theorem [ theorem : r(1)-r(evalpha ) ] .",
    "this fact seems to be disadvantage of introducing our adaptive scaling .",
    "however , it may not be so from the viewpoint of model selection since this property allows us to identify the minimum of risk curve ; i.e. risk is small at @xmath165 while it is large when @xmath166 .",
    "hence , nearly optimal @xmath79 is expected to be found according to a model selection criterion given by ( [ eq : ernk_evalpha ] ) . and , at such an optimal @xmath79 , a consistent choice of a set of true components and a low risk value are guaranteed by lemma [ lemma : p - oen*-bound ] and theorem [ theorem : r(1)-r(evalpha ) ] respectively .",
    "this speculation is verified in numerical experiments in the next section .",
    "* since the least squares estimators of coefficients of true components tend to be large , approximation in ( [ eq : approx - in - adaptive - scaling ] ) is valid for them . therefore , lst estimators for true components are nearly the least squares estimators . and , as mentioned above , true components may be consistently selected according to ( [ eq : ernk_evalpha ] ) if variance estimate is suitable .",
    "therefore , a model estimated by our adaptive scaling scheme may be close to one estimated by a hard thresholding for which it is difficult to establish a model selection procedure .",
    "consider a set of @xmath3 functions @xmath167 in which @xmath168 the design matrix constructed by @xmath169 satisfies the orthogonality condition of ( [ eq : ot - cond ] ) if @xmath170 for @xmath7 and @xmath3 is even .",
    "these two conditions are satisfied in our experiment here .",
    "we set @xmath171 and @xmath172 , by which @xmath173 ; i.e. @xmath174 , @xmath175 are true components .",
    "we set @xmath176 for gaussian noise variance .",
    "we also set @xmath177 and the maximum number of components that is included in a model is @xmath178 . for an artificially generated data , we apply lst , lst - ssp(lst with single scaling parameter ) and lst - as(lst with an adaptive scaling ) . we employ ( [ eq : lst - ssp - ealpha ] ) as an empirical scaling value for lst - ssp .",
    "adaptive scaling values for lst - as are given by ( [ eq : component - wise - scaling ] ) . for each method , we calculate the approximated risk that is the mean - squared error between a true function output and estimated output on data points .",
    "we also calculate the risk estimate ( unbiased estimate of risk ) .",
    "it is given by ( [ eq : risk - for - lst ] ) with @xmath179 for lst , ( [ eq : risk - for - lst - ssp ] ) with @xmath180 in ( [ eq : lst - ssp - ealpha ] ) for lst - ssp and ( [ eq : ernk_evalpha ] ) with @xmath142 in ( [ eq : component - wise - scaling ] ) for lst - as .",
    "we need to estimate noise variance in calculating a risk estimate that is employed as a model selection criterion in applications .",
    "we here estimate it by the unbiased estimate of noise variance under a linear regression with a set of @xmath181 components that includes true components .",
    "we repeat this procedure for @xmath182 times .",
    "we show averages of ( approximated ) risks and risk estimates for lst - as in figure [ fig : risk - and - risk - estimate - lst - as ] .",
    "we also show averages of risks for lst , lst - ssp and lst - as in figure [ fig : risk - for - lst - ssp - as ] . in figure",
    "[ fig : risk - and - risk - estimate - lst - as ] , we can see that ( [ eq : ernk_evalpha ] ) is actually valid as an unbiased risk estimate under lst - as even when noise variance is replaced with its estimator . in figure",
    "[ fig : risk - for - lst - ssp - as ] , at around a small number of components , risk of lst - as is minimized and is smaller than those of lst and lst - ssp . however , risk of lst - as tends to be larger than those of lst and lst - ssp as the number of components increases .",
    "this is consistent with the remark on theorem [ theorem : r(1)-r(evalpha ) ] and theorem [ theorem : ealpha_j - convergence ] . in other words , an optimal number of components",
    "can clearly be identified in risk curve of lst - as while risk curves of lst and lst - ssp are nearly flat around the minimum value in figure [ fig : risk - for - lst - ssp - as ] .",
    "we emphasize two important points in this result .",
    "the first one is that , as guaranteed by theorem [ theorem : r(1)-r(evalpha ) ] , risk value of lst - as is smaller than that of lst at around an optimal number of components .",
    "the second one is that it can be found via a model selection based on risk estimate . in table",
    "[ tbl : risk - and - un - removed - components ] , we show the averaged risk value and the average numbers of un - removed components for models that are selected by risk estimates . from table [ tbl : risk - and - un - removed - components ] , we can say that lst - as gives low risk at a sparse representation .",
    ".average of risk and the number of un - removed components [ tbl : risk - and - un - removed - components ] selected according to risk estimate .",
    "the standard deviation is showed in the bracket . [ cols=\"^,^,^\",options=\"header \" , ]      discrete wavelet transform is a popular tool for analysis , de - noising and compression of signals and images ; e.g. see @xcite .",
    "we here consider an application of lst with adaptive scaling to a problem of wavelet denoising@xcite .",
    "let @xmath183 , @xmath184 $ ] be a signal .",
    "@xmath3 samples of @xmath183 is denoted by @xmath185 , @xmath186 , @xmath7 .",
    "we define @xmath187 .",
    "we assume that @xmath188 for a natural number @xmath189 .",
    "let @xmath190 and @xmath191 be approximation and detail coefficients at a level @xmath189 in discrete wavelet transform , where @xmath192 .",
    "we define @xmath193 in which we set @xmath194 for @xmath195 . by setting @xmath196 ,",
    "the decomposition algorithm with pre - determined wavelets calculates @xmath197 from @xmath198 by decreasing @xmath199 , where @xmath200 is a fixed level determined by user .",
    "this procedure can be written by @xmath201 where @xmath202 is an @xmath42 orthonormal matrix that is determined by coefficients of scaling and wavelet function ; e.g. see @xcite . on the other hand ,",
    "the reconstruction algorithm calculates @xmath203 from @xmath204 by increasing @xmath205 .",
    "this can be written by @xmath206 since @xmath202 is an orthonormal matrix .",
    "let @xmath207 be an operator on @xmath208 into @xmath208 such as a thresholding operator . in wavelet denoising ,",
    "@xmath209 is processed by using @xmath207 and obtain @xmath210 .",
    "we then obtain a denoised signal by @xmath211 .",
    "note that , in applications , a simple and fast decomposition / reconstruction algorithm is used instead of the above matrix calculation ; e.g. see @xcite .",
    "we here compare the prediction accuracy and sparseness of lst - as to those of lst , lst - ssp and also universal soft - thresholding ( ust ) in @xcite .",
    "note that sure shrink of @xcite is almost equivalent to lst here . in an application of ust , a threshold level on the absolute values of coefficients at the @xmath200th level",
    "is given by @xmath212 where @xmath111 is an estimate of noise variance .",
    "in wavelet denoising , the median absolute deviation ( mad ) is a standard robust estimate of noise variance .",
    "it is given by @xmath213 where @xmath214 , @xmath215 is the smallest scale wavelet coefficients that are heuristically known to be noise dominated components . for lst , lst - ssp and lst - as",
    ", we also employ this estimator in a model selection criterion that is an unbiased risk estimate .",
    "we choose `` heavisine '' and `` blocks '' given in @xcite as test signals .",
    "the former is almost smooth and the latter has many discontinuous points .",
    "additive noise has a normal distribution with mean @xmath10 and variance @xmath176 . as in @xcite",
    ", signals are rescaled so that signal - to - noise ratio is @xmath216 .",
    "the number of samples is @xmath217 .",
    "we set @xmath218 . in @xcite , in practical application , it is employed a heuristic method which applies soft - thresholding only for detail coefficients at a determined level .",
    "we do not obey this heuristics and apply soft - thresholding to all coefficients in orthogonal transformation for a fair comparison .",
    "this is because the choice of a level at which thresholding applies largely depends on the performance as in @xcite and there is no systematic choice of such level .",
    "we employ the orthogonal daubechies wavelet with @xmath219 wavelet / scaling coefficients . for given samples ,",
    "we apply lst , lst - ssp , lst - as and ust , in which the maximum number of un - removed components is set to @xmath220 ; i.e. the maximum value of @xmath79 to be examined .",
    "we then calculate the mean squared error between true signal outputs and estimated outputs on the sampling points as an approximation of risk . for lst , lst - ssp and lst - as , the mean squared error and risk estimate are obtained at each @xmath79 .",
    "for ust , the number of un - removed components and risk value at a selected size are obtained .",
    "we repeat this procedure @xmath221 times .",
    "we show averages of ( approximated ) risk and risk estimate of lst , lst - ssp and lst - as in figure [ fig : wl - risk - curve - heavisine ] for `` heavisine '' and figure [ fig : wl - risk - curve - blocks ] for `` blocks '' respectively .",
    "we also show box plots of risk values at the selected number of components and those of the number of un - removed components in figure [ fig : wl - box - plot - heavisine ] for `` heavisine '' and figure [ fig : wl - box - plot - blocks ] for `` blocks '' respectively . by figure",
    "[ fig : wl - risk - curve - heavisine ] ( b ) and figure [ fig : wl - risk - curve - blocks ] ( b ) , risk estimate approximates risk well for both signals even when noise variance is estimated by mad . by figure [ fig :",
    "wl - risk - curve - heavisine ] and figure [ fig : wl - risk - curve - blocks ] , we can expect that a model estimated by lst - as shows a low risk and high sparsity compared to lst and lst - ssp ; i.e. this result leads to the same conclusions as in the previous numerical example . by comparing figure [ fig : wl - risk - curve - heavisine ] to figure [ fig : wl - risk - curve - blocks ] , the optimal number of components for `` blocks '' is larger than for `` heavisine '' , which is due to a degree of smoothness of signals . by figure",
    "[ fig : wl - box - plot - heavisine ] and figure [ fig : wl - box - plot - blocks ] , for both signals , lst - as outperforms the other methods in terms of prediction accuracy and sparsity , in which especially it shows a nice sparseness property .",
    "note that the worse results of lst and ust may be improved by applying a heuristics that thresholding methods are applied only to detail coefficients at a determined level while there is no systematic choice of the appropriate level .",
    "\\(a ) averaged risk curves of lst , lst - ssp and lst - as .",
    "\\(b ) averaged risk and risk estimate of lst - as .",
    "\\(a ) risk value at the selected number of components .",
    "\\(b ) the number of un - removed components .",
    "\\(a ) averaged risk curve of lst , lst - ssp and lst - as .",
    "\\(b ) averaged risk curve and risk estimate of lst - as .",
    "\\(a ) risk value at the selected number of components .",
    "\\(b ) the number of un - removed components .",
    "soft - thresholding is a key modeling tool in statistical signal processing such as wavelet denoising .",
    "it has a parameter that simultaneously controls threshold level and amount of shrinkage .",
    "this parametrization is possible to suffer from an excess shrinkage for un - removed valid components at a sparse representation ; i.e. there is a dilemma between prediction accuracy and sparsity . in this paper ,",
    "to overcome this problem , we introduced a component - wise and data - dependent scaling method for soft - thresholding estimators in a context of non - parametric orthogonal regression including discrete wavelet transform .",
    "we refer this method as an adaptive scaling method . here , we employed a lars - based soft - thresholding method ; i.e. a soft - thresholding method that is implemented by lars under an orthogonality condition . in lars - based soft - thresholding , a parameter value is selected by a data - dependent manner by which a model selection problem reduces to the determination of the number of un - removed components .",
    "we firstly derived a risk given by lar - based soft - thresholding estimate with our adaptive scaling .",
    "for determining an optimal number of un - removed components , we then gave a model selection criterion as an unbiased estimate of the risk .",
    "we also analyzed some properties of the risk curve and found that the model selection criterion is possible to select a model with low risk and high sparsity compared to a naive soft - thresholding .",
    "this was verified by a simple numerical experiment and an application to wavelet denoising . as a future work ,",
    "we need more application results . in doing this ,",
    "estimate of noise variance should be established in general applications while mad was found to be a good choice for a wavelet denoising application .",
    "although we gave scaling values in a top down manner in this paper , we may need to test the other forms of adaptive scaling values ; e.g. scaling values which are estimates of optimal values in some senses .",
    "moreover , development of adaptive scaling for non - orthogonal case may be expected for more general applications .",
    "00    abramovich , f. , b. yoav , 1996 .",
    "adaptive thresholding of wavelet coefficients .",
    "computational statistics & data analysis 22 , 351 - 361 .",
    "burrus , c.s . ,",
    "gopinath , r.a . ,",
    "guo , h. , 1998 .",
    "introduction to wavelets and wavelet transform .",
    "prentice hall .",
    "donoho , d.l . , johnstone , i.m . , 1994 .",
    "ideal spatial adaptation via wavelet shrinkage .",
    "biometrika 81 , 425 - 455 .    donoho , d.l . ,",
    "johnstone , i.m . ,",
    "adapting to unknown smoothness via wavelet shrinkage .",
    "90 , 1200 - 1224 .",
    "efron , b. , hastie , t. , johnstone , i. , tibshirani , r. , 2004 .",
    "least angle regression .",
    "32 , 407 - 499 .",
    "fan , j. and li , r. , 2001 .",
    "variable selection via nonconcave penalized likelihood and its oracle properties .",
    "96 , 1348 - 1360 .",
    "hagiwara , k. , 2006 .",
    "on the expected prediction error of orthogonal regression with variable components .",
    "ieice trans .",
    "fundamentals e89-a , 3699 - 3709 .",
    "hagiwara , k. , 2014 .",
    "least angle regression in orthogonal case , in : proceedings of iconip 2014 , part ii , lncs 8835 , springer , 540 - 547 .",
    "hagiwara , k. , 2015 . on scaling of soft - thresholding estimator ,",
    "submitted to neurocomputing .",
    "hurvich c.m . and",
    "tsai c. 1998 .",
    "a crossvalidatory aic for hard wavelet thresholding in spatially adaptive function estimation .",
    "biometrika 85 , 701 - 710 .",
    "knight , k. , fu , w. , 2000 .",
    "asymptotics for lasso - type estimators .",
    "28 , 1356 - 1378 .",
    "leadbetter , m.r . ,",
    "lindgren , g. , rootzn , h. , 1983 .",
    "extremes , and related properties of random sequences and processes .",
    "springer - verlag .",
    "nason , g.p . , 1996 .",
    "wavelet shrinkage using cross - validation .",
    "j. r. statist .",
    "b 58 , 463 - 79 .",
    "resnick , s.i . , 1987 .",
    "extreme values , regular variation , and point processes .",
    "springer - verlag .",
    "stein , c. , 1981 .",
    "estimation of the mean of a multivariate normal distribution .",
    "ann . stat . 9 , 1135 - 1151 .",
    "tibshirani , r. , 1996 .",
    "regression shrinkage and selection via the lasso .",
    "j. r. stat .",
    ". methodol .",
    "58 , 267 - 288 .",
    "zhao , p. , yu , b. , 2006 . on model selection consistency of lasso .",
    "res . 7 , 2541 - 2563 .",
    "zou , h. , 2006 .",
    "the adaptive lasso and its oracle properties .",
    "assoc . 101 , 1418 - 1492 .",
    "zou , h. , hastie , t. , tibshirani , r. , 2007 . on the degrees of freedom of lasso .",
    "35 , 2173 - 2192 .",
    "zou , h. , hastie , t. , 2005 .",
    "regularization and variable selection via the elastic net .",
    "j. r. stat .",
    "67 , 301 - 320 .",
    "we here give some lemmas that is used for proving the main theorems .",
    "let @xmath222 be random variables .",
    "we define the @xmath223th largest value among @xmath222 by @xmath224 .",
    "[ lemma : max - chi2-e ] let @xmath222 be i.i.d .",
    "random variables from @xmath134 .",
    "we define @xmath225 . then , at each fixed @xmath226 , @xmath227 hold , where @xmath228 is the @xmath79th derivative of the gamma function at @xmath229 .",
    "( [ eq : lemma - max - chi2-e ] ) implies that @xmath230=1.\\end{aligned}\\ ] ]    by slightly modifying example 3 , pp.72 - 73 in @xcite , we can show that @xmath231 converges to the double exponential distribution .",
    "then , ( [ eq : lemma - max - chi2-e ] ) is a direct conclusion of proposition 2.1 ( iii ) in @xcite .",
    "[ lemma : prob - bound - mth - largest - chi2 ] let @xmath222 be i.i.d .",
    "random variables from @xmath134 . at each fixed @xmath223 , @xmath232&=0\\\\ \\label{eq : prob - bound - mth - largest - chi2-upper }   \\lim_{n\\to\\infty}\\p\\left[x_{(m)}>2\\log n)\\right]&=0\\end{aligned}\\ ] ] hold , where @xmath233 is an arbitrary positive constant .",
    "we denote the probability distribution function of @xmath134 by @xmath234 .",
    "the probability density function of @xmath134 is given by @xmath235 .",
    "we have @xmath236 .",
    "thus , we have @xmath237 as @xmath238 by applying @xmath239 and lhospital s rule . therefore , for a @xmath240 random variable @xmath241 , @xmath242\\sim 2f_1(x)\\ ] ] holds for a sufficiently large @xmath243 .    by ( [ eq : lemma - chi2-p - bound-1 ] )",
    ", we obtain @xmath244 & \\le\\sum_{i=1}^n\\p\\left[x_i>2\\log n\\right]\\notag\\\\ & \\sim 2nf_1\\left(2\\log n\\right)\\notag\\\\ & = \\frac{1}{\\pi}\\frac{1}{\\sqrt{\\log n}}\\to 0~(n\\to\\infty).\\end{aligned}\\ ] ] since @xmath245 for any @xmath223 , we have ( [ eq : prob - bound - mth - largest - chi2-upper ] ) . on the other hand , by ( [ eq : lemma - chi2-p - bound-1 ] ) , we have @xmath246 for a sufficiently large @xmath3 . since this goes to @xmath247 , we obtain ( [ eq : prob - bound - mth - largest - chi2-lower ] ) by theorem 2.2.1 in @xcite .",
    "[ lemma : cj > maxci ] for any @xmath37 and any @xmath248 , @xmath249\\le 2\\pi^{-1/2}\\rho^{-1/2}n^{-\\rho}\\ ] ] holds for a sufficiently large @xmath3 .",
    "we define @xmath250 .",
    "we obtain @xmath251 & \\ge\\p\\left[\\left[\\tc_j^2>\\tau_{n,\\rho}\\right ] \\bigcap\\left[\\tau_{n,\\rho}>\\max_{i\\in \\ok^*}\\tc_i^2\\right]\\right]\\notag\\\\ & = 1-\\p\\left[\\left[\\tc_j^2\\le\\tau_{n,\\rho}\\right ] \\bigcup\\left[\\tau_{n,\\rho}\\le\\max_{i\\in \\ok^*}\\tc_i^2\\right]\\right]\\notag\\\\ & \\ge 1-\\p\\left[\\tc_j^2\\le\\tau_{n,\\rho}\\right]- \\p\\left[\\max_{i\\in \\ok^*}\\tc_i^2\\ge \\tau_{n,\\rho}\\right].\\end{aligned}\\ ] ]    by the definition of @xmath252 , we have @xmath253&=\\p\\left[|\\tc_j|\\le\\sqrt{\\tau_{n,\\rho}}\\right]\\notag\\\\ & = \\p\\left[|\\sqrt{n}\\beta_j/\\sigma+\\oc_j|\\le\\sqrt{\\tau_{n,\\rho}}\\right]\\notag\\\\ & \\le\\p\\left[\\sqrt{n}|\\beta_j|/\\sigma-|\\oc_j|\\le \\sqrt{\\tau_{n,\\rho}}\\right]\\notag\\\\ & = \\p\\left[|\\oc_j|\\ge\\sqrt{n}|\\beta_j|/\\sigma-\\sqrt{\\tau_{n,\\rho}}\\right]\\notag\\\\ & \\le\\p\\left[|\\oc_j|\\ge\\sqrt{\\tau_{n,\\rho}}\\right]\\notag\\\\ & = \\p\\left[\\oc_j^2\\ge\\tau_{n,\\rho}\\right]\\end{aligned}\\ ] ] for a sufficiently large @xmath3 .",
    "note that this evaluation is not tight but is enough in this paper . since @xmath254 by the definition of @xmath252 , by ( [ eq : lemma - chi2-p - bound-1 ] ) and ( [ eq : cj > maxci-1 ] )",
    ", we have @xmath255\\le \\pi^{-1/2}\\rho^{-1/2}n^{-\\rho}\\ ] ] for a sufficiently large @xmath3 .    on the other hand",
    ", @xmath256 holds for @xmath257 since @xmath258 holds for @xmath257 . by ( [ eq : lemma - chi2-p - bound-1 ] )",
    ", we thus have @xmath259 & \\le\\sum_{j\\in\\ok^*}\\p[\\tc_i^2\\ge\\tau_{n,\\rho}]\\notag\\\\ & \\sim ( n - k^*)\\pi^{-1/2}(\\rho+1)^{-1/2}n^{-(\\rho+1)}\\notag\\\\ & \\le\\pi^{-1/2}\\rho^{-1/2}n^{-\\rho}\\end{aligned}\\ ] ] for a sufficiently large @xmath3 . by ( [ eq : cj > maxci-0 ] ) , ( [ eq : cj > maxci-2 ] ) and ( [ eq : cj > maxci-3 ] ) , we obtain ( [ eq : cj > maxci ] ) as desired .",
    "[ lemma : p - oen*-bound ] @xmath260\\le k^*\\pi^{-1/2}\\rho^{-1/2}n^{-\\rho}\\ ] ] holds for any @xmath248 and a sufficiently large @xmath3 .",
    "if @xmath261 does not occur then there exist @xmath262 such that @xmath263 .",
    "this implies that there exist @xmath37 and @xmath264 that satisfy @xmath265 .",
    "therefore , we have @xmath266 . by lemma",
    "[ lemma : cj > maxci ] , we then obtain ( [ eq : p - oen*-bound ] ) .",
    "[ lemma : e - cp1 ^ 2-ioe * ] @xmath267 holds for a fixed @xmath268 .",
    "we define @xmath269 and @xmath270 .",
    "we also define an event @xmath271 . by the cauchy - schwarz inequality",
    ", we have @xmath272\\notag\\\\ & \\le\\e[(\\oc_{p_1}+(\\beta_{p_1}/\\sigma)\\sqrt{n})^{2m}i_{\\oe_n^*}]\\notag\\\\ & \\le\\e[(\\oc+(\\obeta/\\sigma)\\sqrt{n})^{2m}i_{\\oe_n^*}]\\notag\\\\ & \\le\\e[(\\oc+(\\obeta/\\sigma)\\sqrt{n})^{2m}i_fi_{\\oe_n^ * } ] + \\e[(\\oc+(\\obeta/\\sigma)\\sqrt{n})^{2m}i_{\\of}i_{\\oe_n^*}]\\notag\\\\ & \\le 2^{2m}\\e[\\oc^{2m}i_fi_{\\oe_n^ * } ] + 2^{2m}(\\obeta/\\sigma)^{2m}n^m\\e[i_{\\of}i_{\\oe_n^*}]\\notag\\\\ & \\le 2^{2m}\\e[(\\oc^2)^{m}i_{\\oe_n^ * } ] + 2^{2m}(\\obeta/\\sigma)^{2m}n^m\\e[i_{\\oe_n^*}]\\notag\\\\ & \\le 2^{2m}\\sqrt{\\e[(\\oc^2)^{2m}]}\\sqrt{\\p[\\oe_n^ * ] } + 2^{2m}(\\obeta/\\sigma)^{2m}n^m\\p[\\oe_n^*].\\end{aligned}\\ ] ] by lemma [ lemma : p - oen*-bound ] with @xmath273 , the second term of ( [ eq : e - cp1 ^ 2-ioe*-2 ] ) goes to zero as @xmath274 . since @xmath275 is the largest value among i.i.d . @xmath134 sequence with size @xmath3 , the first term of ( [ eq : e - cp1 ^ 2-ioe*-2 ] ) goes to zero as @xmath274 by lemma [ lemma : max - chi2-e ] and lemma [ lemma : p - oen*-bound ] with the above choice of @xmath276 .",
    "[ lemma : e - ttheta_k^2-bound ] if @xmath160 then @xmath277\\le ( 2\\log n)^m\\ ] ] holds for a fixed @xmath268 and sufficiently large @xmath3 .    we can write @xmath278=\\e\\left[\\ttheta_k^{2m}i_{\\oe_n^*}\\right ] + \\e\\left[\\ttheta_k^{2m}i_{e_n^*}\\right].\\ ] ] by lemma [ lemma : e - cp1 ^",
    "2-ioe * ] and the definition of @xmath279 , @xmath280\\le\\e\\left[\\tc_{p_1}^{2m}i_{\\oe_n^*}\\right]\\to   0~(n\\to\\infty).\\ ] ] we define @xmath281 .",
    "if @xmath261 occurs then @xmath282 and @xmath283 is the largest value among i.i.d . @xmath240 random sequence with length @xmath284 .",
    "therefore , by lemma [ lemma : max - chi2-e ] , @xmath285}{(2\\log n)^m } \\le\\frac{\\e\\left[\\tc^{2m}\\right]}{(2\\log n)^m}\\to 1~(n\\to\\infty).\\ ] ]",
    "we give the proofs of the main theorems below .    for an @xmath148",
    ", the risk is reformulated as @xmath286 where we used ( [ eq : dist - of - ecv ] ) at the third line and the orthogonality condition at the last line .",
    "the last term is often called the degree of freedom ; see e.g. @xcite .",
    "let @xmath287 be an @xmath288-dimensional vector that is constructed by removing @xmath289 from @xmath290 .",
    "we define @xmath291 .",
    "although @xmath292 is a function of @xmath290 , we regard this as a function @xmath289 under a fixed @xmath287 and denote it by @xmath293 .",
    "let @xmath294 be the @xmath79th largest value in @xmath295 . by ( [ eq : component - wise - scaling ] ) , we have @xmath296 note here that @xmath293 is well - defined even when @xmath297 under the definition of @xmath142 in ( [ eq : component - wise - scaling ] ) .",
    "this is lipschitz continuous as a function of @xmath289 when @xmath287 is fixed .",
    "it is thus absolutely continuous . on the other hand , we denote expectation with respect to @xmath290 by @xmath298 .",
    "we have @xmath299 and @xmath300 , where @xmath301 denotes the determinant of a matrix .",
    "therefore , @xmath20 is always replaced with @xmath298 by change of variables .",
    "we also denote a conditional expectation with respect to @xmath289 given @xmath287 by @xmath302 .",
    "we define @xmath303}(\\tc_j|\\tcv_{-j})$ ] by @xmath303}(\\tc_j|\\tcv_{-j})=1 $ ] when @xmath304 $ ] and @xmath10 otherwise under a fixed @xmath287 .",
    "then , by applying this change of variables and stein s lemma@xcite under the above absolutely continuity , we obtain @xmath305\\notag\\\\ & = \\sum_{j=1}^n\\e_{\\tcv}\\left[d_j(\\tcv)(\\tc_j - c_j)\\right]\\notag\\\\ & = \\sum_{j=1}^n\\e_{\\tcv_{-j}}\\e_{\\tc_j|\\tcv_{-j } } \\left[d_j(\\tc_j|\\tcv_{-j})(\\tc_j - c_j)\\right]\\notag\\\\ & = \\sum_{j=1}^n\\e_{\\tcv_{-j}}\\e_{\\tc_j|\\tcv_{-j } } \\left[\\frac{\\partial d_j(\\tc_j|\\tcv_{-j})}{\\partial   \\tc_j}\\right]\\notag\\\\ & = \\e_{\\tcv}\\left[\\sum_{j=1}^n ( \\theta_j^2/\\tc_j^2)(1-i_{[-\\theta_j,\\theta_j]}(\\tc_j|\\tcv_{-j}))\\right ] -\\e_{\\tcv}\\left [ \\sum_{j=1}^ni_{[-\\theta_j,\\theta_j]}(\\tc_j|\\tcv_{-j})\\right]\\notag\\\\ & = \\e\\left[\\sum_{j\\in\\ek_k}(\\ealpha_j-1)^2\\right]-(n - k),\\end{aligned}\\ ] ] where the last line is obtained by the definition of @xmath294 and @xmath142 .",
    "( [ eq : risk ] ) and ( [ eq : a - tbv - tcv - tcv - c ] ) yield ( [ eq : theorem - rnk - evalpha ] ) .",
    "we show that @xmath306\\to 0~(n\\to\\infty)\\end{aligned}\\ ] ] for @xmath37 .",
    "we define @xmath307 for which @xmath308=0 $ ] . by the definition of @xmath142 in ( [ eq : component - wise - scaling ] )",
    ", we then have @xmath309\\notag\\\\ & = \\p\\left[\\left\\{\\ealpha_j>1+\\epsilon_{j , n}\\right\\}\\bigcap \\oe_0\\right]\\notag\\\\ & = \\p\\left[\\left\\{\\frac{\\ttheta_k}{|\\tc_j|}>\\epsilon_{j , n}\\right\\}\\bigcap \\oe_0\\right]\\notag\\\\ & \\le\\p\\left[\\frac{\\ttheta_k}{\\sqrt{2\\log n } } -\\frac{|\\tc_j|}{(|\\beta_j|+\\delta)\\sqrt{n}}>0\\right]\\notag\\\\ & \\le \\p\\left[\\ttheta_k>\\sqrt{2\\log n}\\right ] + \\p\\left[|\\tc_j|<(|\\beta_j|+\\delta)\\sqrt{n}\\right].\\end{aligned}\\ ] ] for the first term of ( [ eq : ealpha_j - bound - in*-2 ] ) , we have @xmath310\\notag\\\\ & = \\p\\left[\\ttheta_k^2>2\\log n\\right]\\notag\\\\ & = \\p\\left[\\ttheta_k^2>2\\log n|e_n^*\\right]\\p\\left[e_n^*\\right]+ \\p\\left[\\ttheta_k^2>2\\log n|\\oe_n^*\\right]\\p\\left[\\oe_n^*\\right]\\notag\\\\ & \\le\\p\\left[\\ttheta_k^2>2\\log n|e_n^*\\right]+ \\p\\left[\\oe_n^*\\right].\\end{aligned}\\ ] ] the second term of ( [ eq : ealpha_j - bound - in*-3 ] ) goes to zero as @xmath274 by lemma [ lemma : p - oen*-bound ] . if @xmath261 occurs then @xmath311 is the @xmath312th largest value among i.i.d . @xmath240",
    "random sequence with size @xmath313 .",
    "therefore , by ( [ eq : prob - bound - mth - largest - chi2-upper ] ) in lemma [ lemma : prob - bound - mth - largest - chi2 ] , the first term of ( [ eq : ealpha_j - bound - in*-3 ] ) goes to zero as @xmath274 .",
    "thus , the first term of ( [ eq : ealpha_j - bound - in*-2 ] ) goes to zero as @xmath274 . recall that @xmath314 for @xmath37 , where @xmath315 .",
    "then , for the second term of ( [ eq : ealpha_j - bound - in*-2 ] ) , we obtain @xmath316 & = \\p\\left[|\\sqrt{n}\\beta_j+\\oc_j|<(|\\beta_j|+\\delta)\\sqrt{n}\\right]\\notag\\\\ & \\le\\p\\left[\\sqrt{n}|\\beta_j|-|\\oc_j|<(|\\beta_j|+\\delta)\\sqrt{n}\\right]\\notag\\\\ & = \\p\\left[|\\oc_j|>\\delta\\sqrt{n}\\right]\\to 0~(n\\to\\infty).\\end{aligned}\\ ] ] since @xmath317 holds , we obtain ( [ eq : ealpha_j - lower - bound - k*-0 ] ) as desired .",
    "on the other hand , we consider ( [ eq : ealpha_j - lower - bound - ok * ] ) . for any @xmath318",
    ", we have @xmath319 & = \\p\\left[\\{\\ealpha_j\\le 2-\\epsilon\\}\\bigcap\\oe_0\\right]\\notag\\\\ & \\le\\p\\left[\\ttheta_k\\le ( 1-\\epsilon)|\\tc_j|\\right]\\notag\\\\ & \\le\\p\\left[\\ttheta_k\\le\\delta_n\\right ] + \\p\\left[(1-\\epsilon)|\\tc_j|>\\delta_n\\right].\\end{aligned}\\ ] ] for the first term of ( [ eq : ealpha_j - bound - notin*-1 ] ) , we have @xmath320 & = \\p\\left[\\ttheta_k^2\\le\\delta_n^2\\right]\\notag\\\\ & = \\p\\left[\\ttheta_k^2\\le\\delta_n^2|e_n^*\\right]\\p[e_n^*]+ \\p\\left[\\ttheta_k\\le\\delta_n^2|\\oe_n^*\\right]\\p[\\oe_n^*]\\notag\\\\ & \\le\\p\\left[\\ttheta_k^2\\le\\delta_n^2|e_n^*\\right]+\\p[\\oe_n^*].\\end{aligned}\\ ] ] by lemma [ lemma : p - oen*-bound ] , the second term of ( [ eq : ealpha_j - bound - notin*-2 ] ) goes to zero as @xmath274 .",
    "we set @xmath321 .",
    "if @xmath261 occurs then @xmath311 is the @xmath312th largest value among i.i.d . @xmath240",
    "random sequence with size @xmath313 .",
    "therefore , by ( [ eq : prob - bound - mth - largest - chi2-lower ] ) in lemma [ lemma : prob - bound - mth - largest - chi2 ] and the choice of @xmath318 , the first term of ( [ eq : ealpha_j - bound - notin*-2 ] ) goes to zero as @xmath274 .",
    "we define @xmath281 . for the second term of ( [ eq : ealpha_j - bound - notin*-1 ] ) , we have @xmath322\\le\\p\\left[\\tc^2>2\\log n\\right]\\end{aligned}\\ ] ] since @xmath37 .",
    "here , @xmath283 is the largest value among i.i.d . @xmath240",
    "random sequence with size @xmath313 by the definitions of @xmath323 and @xmath35 .",
    "hence , by ( [ eq : prob - bound - mth - largest - chi2-upper ] ) in lemma [ lemma : prob - bound - mth - largest - chi2 ] , ( [ eq : ealpha_j - bound - notin*-3 ] ) goes to zero as @xmath274 .    by",
    "( [ eq : risk - for - lst ] ) and ( [ eq : theorem - rnk - evalpha ] ) , we have @xmath324 -\\e\\left[(\\ealpha_{p_j}-1)^2\\ttheta_{k^*}^2\\right ] -2\\e\\left[(\\ealpha_{p_j}-1)^2\\right]\\right)\\notag\\\\ & = \\frac{\\sigma^22\\log n}{n } \\sum_{j=1}^{k^*}\\left(\\frac{\\e\\left[\\ttheta_{k^*}^2\\right]}{2\\log n } -\\frac{\\e\\left[(\\ealpha_{p_j}-1)^2\\ttheta_{k^*}^2\\right]}{2\\log n } -2\\frac{\\e\\left[(\\ealpha_{p_j}-1)^2\\right]}{2\\log n}\\right)\\end{aligned}\\ ] ] through a simple calculation .",
    "we evaluate the three terms in the sum of ( [ eq : theorem - r(1)-r(evalpha)-1 ] ) .",
    "we first have @xmath325/(2\\log n)=1\\ ] ] by lemma [ lemma : e - ttheta_k^2-bound ] .",
    "hence , the proof is completed by showing that the second and third terms of ( [ eq : theorem - r(1)-r(evalpha)-1 ] ) goes to zero as @xmath274 .",
    "we define @xmath326 and @xmath327 , where @xmath328 is defined in ( [ eq : def - epsilon_n ] ) .",
    "we have @xmath329 for @xmath330 by the definition of @xmath331 . and , if @xmath261 occurs then @xmath332 for any @xmath333 .",
    "we then obtain @xmath334 & = \\e\\left[(\\ealpha_{p_j}-1)^2i_{g_j\\bigcap e_n^*}\\right ] + \\e\\left[(\\ealpha_{p_j}-1)^2i_{\\og_j\\bigcup\\oe_n^*}\\right]\\notag\\\\ & \\le\\e\\left[i_{g_j\\bigcap\\oe_n^*}\\right]+\\epsilon_n^2+\\p[\\oe_n^*]\\notag\\\\ & \\le\\sum_{l\\in k^*}\\p\\left[(\\ealpha_l-1)^2>\\epsilon_n^2\\right]+\\epsilon_n^2+\\p[\\oe_n^*].\\end{aligned}\\ ] ] ( [ eq : theorem - r(1)-r(evalpha)-3 ] ) goes to zero as @xmath274 by ( [ eq : ealpha_j - lower - bound - k*-0 ] ) in theorem [ theorem : ealpha_j - convergence ] , the definition of @xmath335 and lemma [ lemma : p - oen*-bound ] .",
    "we also have @xmath336\\notag\\\\ & = \\e\\left[(\\ealpha_{p_j}-1)^2\\ttheta_{k^*}^2i_{g_j\\bigcap e_n^*}\\right ] + \\e\\left[(\\ealpha_{p_j}-1)^2\\ttheta_{k^*}^2i_{\\og_j\\bigcup\\oe_n^*}\\right]\\notag\\\\ & \\le\\e\\left[\\ttheta_{k^*}^2i_{g_j}i_{e_n^*}\\right ] + \\epsilon_n^2\\e[\\ttheta_{k^*}^2]+\\e[\\ttheta_{k^*}^2i_{\\oe_n^*}].\\end{aligned}\\ ] ] for the first term of ( [ eq : theorem - r(1)-r(evalpha)-4 ] ) , by the cauchy - schwarz inequality , we have @xmath337}{2\\log n } & \\le\\frac{\\sqrt{\\e\\left[\\ttheta_{k^*}^4\\right]}}{2\\log n } \\sqrt{\\e\\left[i_{g_j}i_{e_n^*}\\right]}\\notag\\\\ & \\le\\frac{\\sqrt{\\e\\left[\\ttheta_{k^*}^4\\right]}}{2\\log n } \\sqrt{\\sum_{l\\in k^*}\\p\\left[(\\ealpha_l-1)^2>\\epsilon_n^2\\right]}.\\end{aligned}\\ ] ] ( [ eq : theorem - r(1)-r(evalpha)-5 ] ) goes to zero as @xmath274 by lemma [ lemma : e - ttheta_k^2-bound ] and ( [ eq : ealpha_j - lower - bound - k*-0 ] ) in theorem [ theorem : ealpha_j - convergence ] .",
    "the second term of ( [ eq : theorem - r(1)-r(evalpha)-4 ] ) goes to zero as @xmath274 by lemma [ lemma : e - ttheta_k^2-bound ] and the definition of @xmath335 . by the cauchy - schwarz inequality",
    ", the third term of ( [ eq : theorem - r(1)-r(evalpha)-4 ] ) is bounded above by @xmath338}\\sqrt{\\p[\\oe_n^*]}$ ] .",
    "this goes to zero as @xmath274 by lemma [ lemma : e - ttheta_k^2-bound ] and lemma [ lemma : p - oen*-bound ] .",
    "we thus obtain ( [ eq : theorem - r(1)-r(evalpha ) ] ) as desired ."
  ],
  "abstract_text": [
    "<S> soft - thresholding is a sparse modeling method that is typically applied to wavelet denoising in statistical signal processing and analysis . </S>",
    "<S> it has a single parameter that controls a threshold level on wavelet coefficients and , simultaneously , amount of shrinkage for coefficients of un - removed components . </S>",
    "<S> this parametrization is possible to cause excess shrinkage , thus , estimation bias at a sparse representation ; i.e. there is a dilemma between sparsity and prediction accuracy . to relax this problem </S>",
    "<S> , we considered to introduce positive scaling on soft - thresholding estimator , by which threshold level and amount of shrinkage are independently controlled . </S>",
    "<S> especially , in this paper , we proposed component - wise and data - dependent scaling in a setting of non - parametric orthogonal regression problem including discrete wavelet transform . </S>",
    "<S> we call our scaling method adaptive scaling . </S>",
    "<S> we here employed soft - thresholding method based on lars(least angle regression ) , by which the model selection problem reduces to the determination of the number of un - removed components . </S>",
    "<S> we derived a risk under lars - based soft - thresholding with the proposed adaptive scaling and established a model selection criterion as an unbiased estimate of the risk . </S>",
    "<S> we also analyzed some properties of the risk curve and found that the model selection criterion is possible to select a model with low risk and high sparsity compared to a naive soft - thresholding method . </S>",
    "<S> this theoretical speculation was verified by a simple numerical experiment and an application to wavelet denoising .    </S>",
    "<S> non - parametric orthogonal regression , soft - thresholding , shrinkage , adaptive scaling , wavelet denoising </S>"
  ]
}