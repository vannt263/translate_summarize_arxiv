{
  "article_text": [
    "in the early 1980s , a number of authors suggested that certain computations might be accelerated with computers making use of quantum resources @xcite .",
    "feynman s 1981 proposal @xcite suggested that quantum systems themselves might be more efficiently modelled with quantum computers . over a decade later , peter shor devised a polynomial - time quantum method for factoring large integers . despite this theoretical promise ,",
    "progress towards experimental quantum computing platforms remained limited .",
    "it was not until 1998 , with the introduction of quantum annealing ( qa ) @xcite , that a path to scalable quantum hardware emerged . while existing qa machines are not computationally universal , qa machines are available now at large scales and offer significant speedups for certain problem classes @xcite . here",
    ", we explore the potential of qa to accelerate training of probabilistic models .",
    "the qa heuristic operates in a manner analogous to simulated annealing ( sa ) , but relies on quantum , rather than thermal , fluctuations to foster exploration through a search space .",
    "just as thermal fluctuations are annealed in sa , quantum fluctuations are annealed in qa .    with the exception of @xcite , most applications run on qa hardware have used the optimization potential of quantum annealing . in @xcite ,",
    "the focus is on training a 4-layer deep belief network .",
    "pre - training of each layer uses restricted boltzmann machines ( rbms ) trained using qa via a complete bipartite graph embedding .",
    "@xcite tested their approach against 1-step contrastive divergence ( cd ) samples on a coarse - version of mnist and concluded that qa sped up training significantly . in @xcite ,",
    "the authors consider training a fully connected boltzmann machine ( bm ) using qa via a complete graph embedding on the hardware graph .",
    "they report a training speed - up compared to training with simulated annealing directly on the complete graph .",
    "these studies assume that the quantum hardware produces a classical boltzmann distribution .",
    "in contrast , in this paper we do not assume the qa samples are boltzmann .",
    "we demonstrate the differences between classical boltzmann and qa hardware samples , and explore the impact of these differences in training fully - visible bms in small density estimation tasks .",
    "training of bms is a natural application domain because available qa hardware realizes boltzmann - like distributions , inference in bms is known to be very hard @xcite , and bms are a building block of many generative probabilistic models @xcite .",
    "we begin with background on qa on the annealing - based quantum system , highlighting its practical constraints .",
    "we characterize the sampling done by the hardware , which in some cases is boltzmann and in other cases differs significantly from boltzmann .",
    "we then describe the challenge of learning probabilistic models with bms , and how qa might accelerate such training .",
    "we provide benchmark results on the learning of multimodal distributions , and quantify the benefits that qa can offer .",
    "lastly , we show the impact of the non - boltzmann nature of the d - wave system , and how this impacts learning .",
    "we conclude with directions for future work .",
    "qa uses quantum - mechanical processes to minimize and sample from energy - based models .",
    "the d - wave machine implements the ising model energy function : @xmath2 with variable connectivity defined by a graph @xmath3 .",
    "the 2000-qubit d - wave system allows for up to @xmath4 variables with sparse bipartite connectivity .",
    "the connectivity graph of the d - wave device is called chimera , and denoted @xmath5 .",
    "@xmath5 consists of an @xmath6 array of @xmath7 unit cells with connection between unit cells as in fig .",
    "[ fig : chimera ] , which shows a @xmath8 graph .",
    "0.6        0.35    [ cols= \" < , > \" , ]     the tree - width of @xmath8 graph is 48 , so exact inference is practically impossible .",
    "it is simple to convert @xmath9 valued spins @xmath10 to boolean - valued variables @xmath11 so that @xmath12 also defines a bm with energy @xmath13 and the same sparse bipartite connectivity .",
    "quantum mechanics replaces the energy function with a linear operator acting on states @xmath14 and returning new states @xmath15 .",
    "this energy operator is described by the hamiltonian , a @xmath16 matrix @xmath17 whose components are indexed by @xmath18 .",
    "the diagonal elements of @xmath17 record the energy of the corresponding states , _",
    "i.e. _ , @xmath19 , and the off - diagonal elements of @xmath17 act to transform states . in the d - wave machine",
    "the only allowed off - diagonal contributions are those which flip bits , _",
    "i.e. _ for @xmath20 @xmath21 quantum processes favor states corresponding to the eigenvectors of low - energy eigenvalues of @xmath17 .",
    "thus , at zero temperature when @xmath22 , quantum evolution corresponds to uniform sampling within the eigenspace corresponding to the lowest eigenvalue ( energy ) of @xmath17 .",
    "however , @xmath23 gives rise to eigenvectors that are linear combinations of basis vectors .",
    "these states are called superpositions , and are interpreted as follows .",
    "an arbitrary superposition is written as @xmath24 where @xmath25 is a weight ( often called an amplitude ) , and @xmath26 is the basis vector corresponding to state @xmath14 . in superposition @xmath27 any particular state ,",
    "@xmath14 , is observed with probability proportional to @xmath28 .",
    "thus , the quantum state @xmath27 implicitly encodes @xmath29 degrees of freedom .",
    "superposition states are unavailable in non - quantum devices , and are a source of the speedups seen in quantum computations . in hardware like the d - wave annealer ,",
    "superposition states are generated by physical processes and do not need to be simulated .    in qa algorithms ,",
    "@xmath17 is varied over time so that$ ] is iverson s bracket defined to be 1 if predicate @xmath30 is true , and 0 otherwise . ]",
    "@xmath31 + \\mathcal{b}(t/\\tau ) e({{\\pmb{s } } } ) [ { { \\pmb{s}}}={{\\pmb{s } } } ' ] .",
    "\\label{ht}\\ ] ] the time - dependent weightings @xmath32 are monotonically decreasing / increasing and satisfy @xmath33 and @xmath34 , so that we evolve from @xmath35  which has no diagonal energy contribution , and which assigns equal probability to all states ( @xmath36 )  to the ising energy function @xmath37 .",
    "the decreasing quantum effects mediated by @xmath38 give rise to the name quantum annealing . for certain classes of optimization problems ,",
    "quantum annealing can be dramatically faster than simulated annealing @xcite .    on the 2000-qubit d - wave system , the annealing time @xmath39 is programmable ( the default anneal time is 20 @xmath40s ) .",
    "a single sample is then measured ( drawn ) at time @xmath39 , and the process is repeated in an i.i.d .",
    "fashion for subsequent anneals . on the first anneal ,",
    "the parameters @xmath41 and @xmath42 must be specified , requiring a programming time around @xmath43 ms .",
    "further timing data of the 2000-qubit d - wave system are listed in fig .",
    "[ tab : timing ] .",
    "the ising hamiltonian described above is a zero temperature ( @xmath44 ) idealization of real - world complexities .",
    "important deviations from ideality arise from :    * _ finite temperature : _",
    "qa hardware does not operate at zero temperature . in units where the parameters lie in the interval @xmath45 and @xmath46 , the effective hardware temperature @xmath47 is problem dependent and usually between @xmath48 and @xmath49 . * _ parameter misspecification : _ during programming the @xmath50 parameters are subject to additive gaussian noise having standard deviations @xmath51 and @xmath52 respectively .",
    "additionally , in spite of calibration of the device , small systematic deviations from the idealized ising model arise because the ising model is only an approximation to the true low - energy physics . * _ dynamics : _ the quantum mechanical evolution of the annealing process can not be simulated at large scales ( even for idealized models ) , and quantum effects can cause significant deviations from the classical boltzmann distribution .",
    "a better approximation is obtained using the density matrix of the quantum boltzmann distribution @xmath53 , but even this approximation fails to capture the out - of - equilibrium effects of rapid annealing within the d - wave device @xcite .    in spite of these complexities , it remains true that qa hardware rapidly produces i.i.d .",
    "low energy samples from programmable chimera - structured energy models . here",
    ", we explore whether this capability can be harnessed for efficient learning of chimera - structured bms . as our interest is on the sampling aspects of learning , we focus on fully visible models to avoid the confounding influence of multimodal likelihood functions .",
    "to begin , we explore the qa sampling distributions . as a rough characterization , we might expect a boltzmann distribution @xmath54 , and indeed for some problems this is a good description .",
    "however , the boltzmann distribution assumes classical statistics , and numerous experiments have confirmed the quantum nature of the d - wave systems @xcite . with different choices of energy functions",
    "we can clearly expose its quantum properties .",
    "consider the ising model illustrated on fig .",
    "[ fig : flc1 ] .",
    "the model consists of 4 unit cells .",
    "the variables within each unit cell are strongly ferromagnetically ) connections induce neighbouring spins to take the same value in low energy states . ]",
    "connected with connection weights of  @xmath55 .",
    "the connections between unit cells form a frustrated loop , and have weights @xmath56 10 times weaker in magnitude than the intra - cell connections .",
    "the @xmath57 weights on all variables are zero .",
    "we call this a frustrated loop of clusters problem , and reference it as fcl-1 .    0.3        0.65     the energy landscape of fcl-1 has 16 local minima corresponding to the @xmath58 possible configurations of four unit cells ( each variable within a unit cell takes the same value in low - energy states ) .",
    "these 16 local minima are separated by high - energy barriers . to cross a barrier ,",
    "an entire unit cell must be flipped , incurring an energy penalty of @xmath59 . among the 16 local minima ,",
    "8 are ground states and 8 are excited states , and the energy gap between ground and excited states is 4 .",
    "the energy barriers make it very difficult for any single - spin - flip markov chain monte carlo ( mcmc ) algorithm to move between valleys . to draw approximate boltzmann samples from fcl-1",
    ", we ran @xmath60 mcmc chains from random initializations , and updated each using blocked gibbs sampling with @xmath61 gibbs updates , annealed over 1000 temperature steps .",
    "the inverse temperature steps were set uniformly over the interval @xmath62 $ ] so there are 10 blocked gibbs updates at each @xmath63 .    under fcl-1 , we also generated @xmath60 qa samples , each obtained with a @xmath64 annealing process .",
    "to adjust for physical temperature of the hardware , we scaled down the values of @xmath42 by a factor of @xmath65 , which is a crude estimate of the @xmath66 parameter for this problem . as a result ,",
    "the model programmed on hardware had all @xmath42 values within the @xmath67 $ ] range as required by the 2000-qubit d - wave system .",
    "the resulting empirical probabilities of 16 local minima under both mcmc ( green ) and qa ( blue ) sampling are shown in fig .",
    "[ fig : sampling_flc1 ] .",
    "the abscissa represents the 16 local minima .",
    "the ordinate records the probability of each local minimum .",
    "red bars show the probabilities of local minima under a classical boltzmann distribution .",
    "qa empirical probabilities follow the exact boltzmann probabilities closely , with a kullback  leibler ( kl ) divergence of empirical distribution from exact boltzmann distribution of @xmath68 .",
    "in contrast , mcmc annealing substantially over - samples excited states with corresponding @xmath69 .",
    "mcmc chains become trapped in excited minima during the anneal , and are not able to cross barriers between states as the temperature decreases .",
    "the failure of the mcmc annealing process is shown more in detail in fig .",
    "[ fig : dynamics ] . here , the abscissa records inverse temperature , and the ordinate records probability .",
    "the solid green , red , and blue curves represent the exact combined probabilities of all 16 local minima , all 8 ground states , and all 8 excited states respectively . the dashed lines represent corresponding empirical probabilities derived from mcmc chains at each temperature step .",
    "notably , the exact probabilities of excited states change non - monotonically during the annealing process . at early stages of the anneal at low @xmath63 values ,",
    "the probability of excited states increases as a function of @xmath63 as probability flows from the entire solution space into the local minima .",
    "as @xmath63 increases further , the dynamics alter .",
    "probability transitions from excited states to ground states , and the total probability of excited states decreases as a function of @xmath63 . the mcmc process is able to accurately model probabilities of all states at early stages of the anneal , but when the energy barriers between states grow sufficiently large , the process freezes , and the probabilities of local minima do not change . as a result , mcmc over - samples excited minima .        it might be argued that a single parameter , @xmath63 , can be adjusted to provide a close match between the qa distribution and the corresponding boltzmann distribution , since there are only two relevant distinct energies within fcl-1 . to address this concern , we modified fcl-1 by breaking symmetry within the inter - cell frustrated loop connections . the modified problem , fcl-2 , is shown on fig .",
    "[ fig : flc2 ] .",
    "0.3        0.65     fcl-2 has the same 16 low - energy local optima , but 4 of these are ground states , and the remaining 12 excited states have diverse energy values .",
    "we repeated the sampling procedures described above using the same value of @xmath70 to adjust the @xmath71 values programmed on hardware .",
    "the results are presented in fig .",
    "[ fig : sampling_flc2 ] .",
    "again we see that the empirical qa samples closely follow the exact boltzmann distribution , with kl divergence of 0.006 , while mcmc annealing continues to over - sample excited states , only reaching a kl divergence of 0.28 .",
    "thus far , the qa distributions closely approximate the classical boltzmann distribution .",
    "a little digging into the physics yields the reason . during quantum annealing",
    ", there is a freeze - out analogous to the classical freeze - out seen in fig .",
    "[ fig : dynamics ] . for fcl-1 and fcl-2",
    ", the equivalence of all intra - cell interactions means that quantum effects at the freeze - out point affect all ferromagnetically connected clusters equally .",
    "this freeze - out translates to a simple energy shift in the classical spectrum , so that the quantum boltzmann distribution is very similar to the classical distribution . in general however , clusters might _ not _ freeze at the same point .",
    "next , we consider ising models where the qa distribution deviates from the classical boltzmann .",
    "such models can be obtained by differentiating among the @xmath72 couplings .",
    "thus , we consider the fcl-3 problem of fig .  [",
    "fig : flc3 ] .    0.3        0.65",
    "the results of the same sampling procedure applied to fcl-3 are presented in fig .",
    "[ fig : sampling_flc3 ] . again",
    ", red bars represent the classical boltzmann probabilities of energy local minima , and blue and green bars represent empirical probabilities of local minima derived from qa and mcmc samples respectively .",
    "now we see that the qa distribution deviates substantially from classical boltzmann with a kl divergence similar to that obtained by a mcmc and anneal procedure ( 0.11 ) . clusters with large ( strong ) @xmath73 freeze earlier in the quantum annealing process compared to weak ones @xcite .",
    "hence , qubits in strong clusters equilibrate under a quantum boltzmann distribution at a lower energy scale than qubits in weak clusters .",
    "the result is a distorted distribution that deviates from the classical boltzmann . to confirm this explanation , we applied a classical redfield simulation of the quantum dynamics @xcite .",
    "orange bars in fig .",
    "[ fig : sampling_flc3 ] show empirical probabilities of local minima derived using this simulation agree closely with probabilities derived from qa samples .",
    "lastly , we modified cluster strengths for an fcl-2 problem ( with a broken symmetry between excited states ) and denoted the resulting problem fcl-4 ( fig .  [ fig : flc4 ] ) .",
    "the sampling results are shown in fig .",
    "[ fig : sampling_flc4 ] .",
    "the qa distribution again deviates significantly from the classical boltzmann , but agrees closely with the quantum simulation .    0.3        0.65     from a machine learning perspective",
    ", these asymmetric cluster problems may appear discouraging , as they suggest that the general qa distribution has a complicated form that depends on unknown factors , e.g. freeze - out points for different qubits . in the next section , however , we show that at least in considered cases it is possible to adjust ( with simple learning rules ) hardware parameters to match classical boltzmann distributions of interest .",
    "a boltzmann machine defines a probability distribution over @xmath74-valued variables @xmath14 as @xmath75 where the partition function is @xmath76 . for chimera - structured bms",
    "the vector of sufficient statistics is given by @xmath77 $ ] .",
    "often , hidden variables are introduced to increase the modeling flexibility of bms , but we defer the study of hidden variable models because the likelihood surfaces that result become multimodal .",
    "bms play an important role in many machine learning algorithms , and serve as building blocks for undirected generative models such as deep bms @xcite .    in fully visible bms",
    ", the parameters @xmath78 are learned from training data @xmath79 by maximizing the expected log - likelihood @xmath80 of @xmath81 : @xmath82 where @xmath83 / |d|$ ] is the training data distribution .",
    "though @xmath80 is a concave function ( making maximization straightforward in principle ) , neither @xmath84 nor @xmath85 can be determined exactly for models at large scale .",
    "thus , training of practically relevant bms is typically very difficult .",
    "the dominant approach to training bms is stochastic gradient ascent , where approximations to @xmath85 are used @xcite .",
    "mcmc ( specifically gibbs sampling ) is used to estimate @xmath86 needed for @xmath87 at parameter setting @xmath88 , and @xmath88 is updated ( most simply ) according to the estimated gradient as @xmath89 .",
    "a variety of methods are available for the gradient step size @xmath90 .",
    "the efficacy of stochastic gradient ascent depends on the quality of the gradient estimates , and two methods are commonly applied to seed the mcmc chains with good starting configurations .",
    "contrastive divergence ( cd ) @xcite initializes the markov chains with the data elements themselves since ( at least for well - trained models ) these are highly likely states .",
    "persistent contrastive divergence ( pcd ) @xcite , improves upon cd by initializing the markov chains needed for @xmath88 with samples from the previous chain at @xmath91 .",
    "if gradient steps on @xmath78 are small , it is hoped that samples from @xmath92 rapidly equilibrate under @xmath93 .",
    "the approaches used in cd and pcd to foster rapid equilibration acutely fail in multimodal probability distributions that have high - energy barriers .",
    "however , even simple problems at modest sizes can show the effects of poor equilibration under pcd as the problem size grows . to demonstrate this",
    ", we generated 20 chimera - structured ising models with @xmath94 and @xmath95 randomly sampled from @xmath96 at sizes @xmath97 ( 72 variables ) , @xmath98 ( 128 variables ) , and @xmath99 ( 200 variables ) .",
    "pcd - estimated gradients used 1000 chains with either 2 , 10 , or 50 blocked gibbs updates , and all models were trained for 500 iterations using nesterov - accelerated gradients @xcite .",
    "the nesterov method uses momentum ( past gradients ) , and is more susceptible to noisy gradients than stochastic gradient descent @xcite .",
    "the learned model @xmath100 results are presented on fig .  [ fig : perfect_learning ] ( @xmath101 is learned , and @xmath102 is fixed to zero ) .",
    "the abscissa represents problem size , and the ordinate represents the log - likelihood - ratio @xmath103 $ ] averaged on test data . note",
    "that this ratio is a sampling - based estimate of @xmath104 .",
    "the exact model is recovered when the kl divergence is zero .",
    "as expected , models trained using exact samples achieve a kl divergence close to 0 on all instances , but pcd requires progressively more gibbs updates as the problem size increases .",
    "models trained with exact samples minimize the kl divergence , but models trained with approximate pcd sampling require progressively more gibbs updates to perform well .",
    "solid lines represent the mean value across 20 random instances , and dashed lines represent 25th and 75th percentiles.,scaledwidth=65.0% ]    in subsequent experiments , we explore whether qa may improve upon cd and pcd by providing mcmc seeds that more accurately sample low - energy states of @xmath93 thus allowing for faster equilibration and better gradient estimates .      in training models on qa hardware , it is important to distinguish @xmath105 from the d - wave qa sampling distribution . by @xmath106",
    "we denote the distribution formed by sampling the qa hardware at parameter @xmath107 followed by @xmath108 sweeps of blocked gibbs updates at parameter @xmath78 . in particular , @xmath109 is the raw hardware distribution at @xmath107 , and @xmath110 . in the experiments we report , we use @xmath0 blocked gibbs sweeps .    to test qa for bm learning",
    "we train fully visible multimodal chimera - structured models . for a variety of problems up to @xmath99 scale ( 200 variables ) , we specify @xmath111 , draw exact boltzmann samples is 20 .",
    "] from @xmath111 , and try to recover @xmath78 from the samples .",
    "we compare the efficacy of cd , pcd , and qa - seeded mcmc chains . in all cd / pcd / qa cases ,",
    "each chain is run for 50 blocked gibbs updates . to assess the accuracy of the learned models",
    ", we measure the log likelihood on both training and held out test data , and compare these results to known optimal values .    for each fcl problem",
    ", we generate a training and a test set of size @xmath112 using an exact boltzmann sampler .",
    "all fcl problems have @xmath113 and only @xmath114 parameters are learned . during training ,",
    "gradients are estimated from 1000 monte carlo chains seeded with cd , pcd , or qa initializations .",
    "the qa seeds are obtained by calling the quantum hardware with the standard @xmath64 anneal . in all cases ,",
    "50 block gibbs updates are performed on the seeds . to speed training ,",
    "we used nesterov accelerated gradients .",
    "the results for fcl-1 are presented in fig .",
    "[ fig : training_nesterov ] .",
    "after about 30 iterations , the cd and pcd procedures collapse , and the corresponding log likelihoods deteriorate . this occurs when the energy barriers between local optima in the learned model energy landscape become too large for the mcmc chains to cross efficiently with 50 gibbs updates . as a result , mcmc - based procedures obtain biased gradients and the cd / pcd models drift away from the optimal region . in contrast , qa - seeded gradients consistently improve the log - likelihood value for about 70 updates and stagnate within @xmath115 of @xmath116 .     in the reformulation of @xcite ) .",
    "both cd and pcd procedures become unstable , but qa - seeded gradients exhibit stable learning.,scaledwidth=65.0% ]    the poor performance of cd and pcd is due in part to the choice of the nesterov accelerated gradient updates , which , as mentioned earlier , are more sensitive to noisy gradients than stochastic gradient descent updates . interestingly , increasing the number of gibbs steps ( up to @xmath117 ) does not help either cd or pcd significantly . as expected , we found training cd / pcd with simple stochastic gradient updates to be more effective over a wide range of iteration - independent learning rates @xmath118 .",
    "a smaller learning rate effectively corresponds to a larger number of gibbs updates at a larger learning rate , and therefore improves the quality of estimated gradients , but takes more time .",
    "we trained cd / pcd models for 10,000 iterations , and compared to 200 iterations of training using qa with nesterov - accelerated gradients .",
    "the cd / pcd learning rates were varied from @xmath119 , where learning rapidly goes unstable , to @xmath120 where learning was impractically slow within 10,000 iterations .",
    "the results are shown in fig .  [",
    "fig : training_sgd ] .",
    "we found that some of the cd and pcd trained models achieved @xmath121 values similar to that of qa - based learning , but required @xmath122 times as many model updates .",
    "it is reassuring that qa samples are able to improve upon cd / pcd in fcl-1 and fcl-2 where the qa distribution closely follows the classical boltzmann distribution ( see figs . [",
    "fig : sampling_flc1 ] and [ fig : sampling_flc2 ] ) .",
    "however , what about training on fcl-3 where qa exhibits strongly non - boltzmann behavior ( see fig . [",
    "fig : sampling_flc3 ] ) ? in order for the difference in cluster strengths to be reflected in the data , we scaled down all @xmath72 in fcl-3 by a factor of 3 .",
    "weights are strong enough that there are negligibly few broken intracluster bonds , and therefore training data generated for fcl-3 and fcl-1 are almost identical . ]",
    "we train a bm using qa - seeded gradients and fixed learning rate @xmath123 on the resulting problem to learn parameters @xmath100 .    to characterize @xmath100 , we determine the occupation of local minima under @xmath124 ( in red ) and @xmath125 ( in blue ) . in fig .",
    "[ fig : flc3_learning ] green bars represent the local minima occupation probabilities in the scaled - fcl-3 training data .",
    "parameters are scaled down from fcl-3 by a factor of 3 .",
    "the bars indicate the local minimum probabilities derived from the learned model @xmath100 using a boltzmann distribution ( red ) , and the hardware distribution @xmath125 ( blue ) .",
    "green bars are the probabilities in the training data.,scaledwidth=65.0% ]    the occupation probabilities do not sum to 1 as there is significant probability of occupying states with broken intracluster bonds .",
    "the boltzmann distribution @xmath124 fits the data poorly , but @xmath125 fits the data well .",
    "more detailed examination reveals that @xmath124 over - samples the states that are under - sampled when the qa hardware is used to sample from fcl-3 ( fig .",
    "[ fig : sampling_flc3 ] ) .",
    "the learning procedure therefore adjusts the model to compensate for the deviation of qa distribution from classical boltzmann .",
    "this suggests two important conclusions .",
    "firstly , the gradients of the loss function eq .   used in the training procedure and derived under the assumption of classical boltzmann distribution remain useful in optimizing the model under non - boltzmann qa distribution .",
    "secondly , the parameters of the hardware distribution in this case are flexible enough to closely approximate a classical boltzmann distribution of interest .",
    "the results of the previous section suggest that the learned models @xmath100 may not be good fits to training data under boltzmann assumptions , but may be when sampling according to @xmath126 .",
    "ideally , we would quantify this by measuring log likelihood on test data , but this is not directly possible because a closed form expression of the hardware distribution is unavailable .",
    "instead , we fit a density estimate to data sampled from @xmath126 , and evaluate test set log - likelihood using the tractable fit .    0.9        0.9",
    "let @xmath127 represent a tractable fit obtained from samples of @xmath106 , which approximates @xmath106 .",
    "we require that @xmath127 can be evaluated for any @xmath14 so that the log likelihood of test data may be computed .",
    "one choice for @xmath127 is the neural autoregressive density estimator ( nade ) @xcite .",
    "nade decomposes the joint distribution into a product of conditional density estimates , one for each dimension of @xmath14 .",
    "nade often outperforms other density estimators , but it suffers from slow training and the necessity of hyperparameter tuning .",
    "we made some effort to optimize hyperparameters , but improved values are likely possible .",
    "consider again the fcl-3 problem .",
    "we denote the fcl-3 parameters by @xmath111 , and the parameters of the model learned under qa gradients as @xmath100 .",
    "let @xmath124 and @xmath125 represent the boltzmann and hardware probability distributions for parameters @xmath100 .",
    "we compile three data sets each consisting of @xmath61 samples from @xmath128 ( data ) , @xmath124 , and @xmath125 .",
    "the data sets are further split into 5000 training and 5000 test points .",
    "to apply nade to the datasets , we use an rbm with 200 hidden units with a learning rate initialized to @xmath129 and decreased over time @xmath130 as @xmath131 .",
    "the nade optimization is terminated when the algorithm sees no performance improvement for 10 consecutive epochs .",
    "we validate the quality of the resultant nade models by showing scatter plots of log probability of each test point with respect to its energy ( first three panels of fig .",
    "[ fig : nade_data ] ) .",
    "the nade models are all roughly boltzmann with log probability decreasing approximately linearly with @xmath132 as expected . in fig .",
    "[ fig : nade_data ] we show the average test set log - likelihood of the nade models trained on samples from @xmath128 , @xmath125 and @xmath124 . for comparison , the horizontal blue line denotes the likelihood of test data under the true model @xmath128 . according to nade , the hardware model @xmath125 is a better fit to test data than @xmath124 .",
    "the nade algorithm is heuristic and introduces its own error in estimating the test set log likelihoods , and",
    "our hope is that the nade error is smaller than the differences in test set log likelihoods . for models of unknown structure",
    ", we have no better alternative than a blackbox approach like nade , but on these problems where we know the training data is boltzmann distributed we can do better .",
    "as all three distributions should be either boltzmann or close to boltzmann , we fit a boltzmann distribution to each set of samples .",
    "[ fig : orang_data ] shows analogous results but under a boltzmann fit rather than a nade fit . in this case",
    "we see that @xmath133 on test data is an excellent fit , and almost matches the true test set log likelihood .",
    "thus , the qa - enabled training procedure learns a very good data model under the hardware distribution despite the fact that the hardware distribution is significantly non - boltzmann . in the rest of the paper , we assume that @xmath127 is calculated using boltzmann estimates .",
    "lastly , we characterize the relative computational effort of learning on larger problems .",
    "these problems consist of 200 variables arranged as a @xmath134 array of unit cell clusters with @xmath135 , and with inter - cell couplings that are randomly @xmath136 .",
    "these problems have many local minima due to the frustrated loops between clusters , and have high - energy barriers between local minima .",
    "we indicate a particular realization of this model as @xmath111 and create test and training states of 500,000 each by sampling from @xmath128 .",
    "parameters @xmath100 are learned from the training data using pcd and qa seeded gradients , and approximate kl divergence is measured using the test data . in all cases ,",
    "we use 1000 monte carlo chains and apply 50 blocked gibbs updates . in figs .",
    "[ fig : sgd ] and [ fig : nesterov ] we show the number of gradient updates required by pcd and qa - seeded gradients to achieve a specified @xmath137 under stochastic gradient ( sgd ) and nesterov updates .",
    "we ran pcd at 9 different learning rates ranging from @xmath138 down to @xmath139 , and qa - seeded gradients at learning rates @xmath138 , @xmath140 , and @xmath115 . at each @xmath121 divergence",
    ", we counted the number of gradient updates in the method requiring the fewest number of updates to attain that @xmath121 . for comparison",
    ", we also indicate the rate of learning under exact gradients using a step size of 0.1 .",
    "the curves labeled @xmath133 and @xmath124 are the two variants of hardware - trained models . curves that terminate at finite @xmath121 values indicate that no lower @xmath121 divergence was found .",
    "we see that nesterov updates using qa gradients result in the most rapid learning .",
    "we have seen that qa - seeded mcmc can speed training of some classical bms .",
    "the learning rule we employ , @xmath141 ( which assumes a classical boltzmann sampling distribution ) , results in models that adapt to the biases arising from deviations between the qa sampling distribution and the classical boltzmann distribution . as a consequence ,",
    "@xmath142 is usually a better model than @xmath124 . in light of this , it is natural to explore a training procedure that avoids blocked gibbs postprocessing entirely , namely @xmath143 , and evaluate the generalization of @xmath144 on test data .",
    "this may seem a strange learning rule as it is motivated by assuming the qa sampling distribution is boltzmann , which it clearly is not .",
    "however , as we show next it can be theoretically motivated .",
    "when annealing classically , the dynamics can freeze as the temperature drops below the size of relevant energy barriers .",
    "we provided an example of this in fig .",
    "[ fig : dynamics ] for classical annealing on fcl-1 .",
    "a similar effect can occur during quantum annealing where dynamics freeze at time @xmath130 prior to the end of the quantum anneal at @xmath145 .",
    "thus , a more accurate model of qa distribution is described in @xcite using a transverse ising hamiltonian @xmath146 for the hamiltonian of eq .   and some @xmath147 .",
    "the density matrix of the distribution defined by @xmath148 is @xmath149 where the partition function @xmath150 is simply the trace of the matrix @xmath151 , and the probability of state @xmath14 is the @xmath152 diagonal entry of @xmath153 . maximizing the log likelihood @xmath154 of this distribution is difficult .",
    "instead , @xcite proposes to maximize a lower bound @xmath155 obtained using the golden - thompson inequality : @xmath156 the gradient of this lower bound can be estimated exactly as in using the raw qa samples , that is , using @xmath157 : @xmath158    0.9   frustrated cluster loop problems.,title=\"fig : \" ]       0.9   frustrated cluster loop problems.,title=\"fig : \" ]      we generated problems as in section [ bm - experiments ] .",
    "we focus on the problem class where qa sampling shows the largest deviation from classical boltzmann sampling , namely a @xmath134 array of clusters with randomly assigned cluster strengths from @xmath159 as in fcl-4 , and where all 4 cycles are frustrated , and have otherwise random couplings from @xmath160 . training and",
    "test sets had size @xmath112 points each , generated by an exact boltzmann sampler . on these problems , @xmath142 provides better fits than @xmath124 . as mentioned before , we used raw hardware samples ( postprocessing offered no improvement ) and used @xmath161 to measure performance .",
    "we tested annealed learning using gradient step sizes decaying as @xmath162 $ ] .",
    "both cd and pcd used 10,000 blocked gibbs updates at each parameter update .",
    "our findings for @xmath134 cluster problems are summarized in fig .",
    "[ fig : etat ] .",
    "these plots show the evolution , over the sgd iterations , of test set kl divergences @xmath163 for software runs and @xmath164 for qa runs ( the dotted red line is the performance of qa using @xmath165 , for reference ) .",
    "the @xmath166 values shown are the best for each algorithm where @xmath167 .",
    "for these examples , only cd with 10,000 blocked gibbs updates was competitive with qa .",
    "in this work , we have studied the utility of quantum annealing in training hard fully visible boltzmann distributions .",
    "we have empirically characterized the sampling distribution of the d - wave qa device on a number of problem classes , and shown that , while the device is effective at sampling low - energy configurations , the sampling distribution can differ significantly from classical boltzmann . in spite of this , a learning procedure that updates model parameters _ as if the sampling distribution were boltzmann _ results in excellent models as long as samples are drawn from the qa hardware followed by @xmath108 gibbs updates .",
    "we tested several values of @xmath108 and we noticed improvements over cd and pcd .",
    "interestingly , raw qa samples ( i.e. , @xmath1 ) provided similar improvements .",
    "we justify this by relating learning in classical bms and quantum bms as described in @xcite .",
    "we have demonstrated computational benefits over pcd and cd by measuring the decrease in the number of parameter updates required for training , and shown benefits under both fixed and decaying learning rates .",
    "these promising results justify further exploration .",
    "firstly , the computational benefits of qa over cd / pcd were demonstrated in artificial problems constructed to have high - energy barriers between modes , but which were small enough to yield exact results .",
    "we anticipate that more realistic problems also having large energy barriers would show similar qa improvement , but this should be validated .",
    "secondly , we would like to have further evidence that the qa model of @xcite or an extension of it can be used to justify the parameter update rule of eq .",
    "to raw qa samples .",
    "our motivation is heuristic , and a deeper understanding might provide more effective learning updates .",
    "thirdly , the sparsity of connections on current qa hardware limits the expressiveness of models , and hidden variables are required to model distributions of practical interest .",
    "thus , studies similar to this one should characterize performance for qa - based learning in models with hidden variables .",
    "lastly , qa hardware is continuously being improved , and new parameters that control the quantum annealing path ( the @xmath168 and @xmath169 functions of eq .  )",
    "have recently been developed .",
    "learning to exploit these additional controls for improved training is an important and challenging task .",
    "m.  benedetti , j.  realpe gomez , r.  biswas , and a.  perdomo ortiz .",
    "quantum - assisted learning of graphical models with arbitrary pairwise connectivity .",
    "see https://arxiv.org/abs/1609.02542 , september 2016 .",
    "v.  s. denchev , s.  boixo , s.  v. isakov , n.  ding , r.  babbush , v.  smelyanskiy , j.  martinis , and h.  neven .",
    "what is the computational value of finite range tunneling ?",
    "see http://arxiv.org/abs/1512.02206 , december 2015 .",
    "h.  larochelle and i.  murray .",
    "the neural autoregressive distribution estimator . in _ proc . of the 14th aistats _ , pages 2937 , 2011 .",
    "see http://jmlr.csail.mit.edu/proceedings/papers/v15/larochelle11a/larochelle11a.pdf .",
    "t.  lanting , a.j .",
    "przybysz , a.  yu .",
    "smirnov , f.m .",
    "spedalieri , m.h .",
    "amin , a.j .",
    "berkley , r.  harris , f.  altomare , s.  boixo , p.  bunyk , n.  dickson , c.  enderud , j.p .",
    "hilton , e.  hoskinson , m.w .",
    "johnson , e.  ladizinsky , n.  ladizinsky , r.  neufeld , t.  oh , i.  perminov , c.  rich , m.c .",
    "thom , e.  tolkacheva , s.  uchaikin , a.b .",
    "wilson , and g.  rose .",
    "entanglement in a quantum annealing processor .",
    ", 4:021041 , 2014 .",
    "see https://arxiv.org/abs/1401.3500 .",
    "p.  m. long and r.  servedio .",
    "restricted boltzmann machines are hard to approximately evaluate or simulate . in johannes",
    "frnkranz and thorsten joachims , editors , _ proceedings of the 27th international conference on machine learning ( icml-10 ) _ , pages 703710 .",
    "omnipress , 2010 .",
    "see http://www.cs.columbia.edu/~rocco/public/final-camera-ready-icml10.pdf .",
    "r.  salakhutdinov and g.  hinton .",
    "deep boltzmann machines . in",
    "_ proceedings of the international conference on artificial intelligence and statistics _ , volume  5 , pages 448455 , 2009 .",
    "see http://www.cs.toronto.edu/~fritz/absps/dbm.pdf .",
    "i.  sutskever , j.  martens , g.  e. dahl , and g.  e. hinton . on the importance of initialization and momentum in deep learning . in sanjoy dasgupta and david mcallester , editors ,",
    "_ proceedings of the 30th international conference on machine learning ( icml-13 ) _ , volume  28 , pages 11391147 .",
    "jmlr workshop and conference proceedings , may 2013 .",
    "t.  tieleman .",
    "training restricted boltzmann machines using approximations to the likelihood gradient . in _ proceedings of the 25th international conference on machine learning _ ,",
    "pages 10641071 .",
    "acm new york , ny , usa , 2008 . see http://www.cs.toronto.edu/~tijmen/pcd/pcd.pdf ."
  ],
  "abstract_text": [
    "<S> quantum annealing ( qa ) is a hardware - based heuristic optimization and sampling method applicable to discrete undirected graphical models . while similar to simulated annealing , qa relies on quantum , rather than thermal , effects to explore complex search spaces . for many classes of problems , </S>",
    "<S> qa is known to offer computational advantages over simulated annealing . here </S>",
    "<S> we report on the ability of recent qa hardware to accelerate training of fully visible boltzmann machines . </S>",
    "<S> we characterize the sampling distribution of qa hardware , and show that in many cases , the quantum distributions differ significantly from classical boltzmann distributions . in spite of this difference , training ( which seeks to match data and model statistics ) using standard classical gradient updates </S>",
    "<S> is still effective . </S>",
    "<S> we investigate the use of qa for seeding markov chains as an alternative to contrastive divergence ( cd ) and persistent contrastive divergence ( pcd ) . using @xmath0 gibbs steps , </S>",
    "<S> we show that for problems with high - energy barriers between modes , qa - based seeds can improve upon chains with cd and pcd initializations . for these hard problems , </S>",
    "<S> qa gradient estimates are more accurate , and allow for faster learning . furthermore , and interestingly , </S>",
    "<S> even the case of raw qa samples ( that is , @xmath1 ) achieved similar improvements . </S>",
    "<S> we argue that this relates to the fact that we are training a quantum rather than classical boltzmann distribution in this case . </S>",
    "<S> the learned parameters give rise to hardware qa distributions closely approximating classical boltzmann distributions that are hard to train with cd / pcd . </S>"
  ]
}