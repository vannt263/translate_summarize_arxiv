{
  "article_text": [
    "_ compressed ( or compressive ) sensing _ ( cs ) is currently one of the most popular topics in information science , and has been used for applications in various engineering fields , such as audio and visual electronics , medical imaging devices , and astronomical observations @xcite .",
    "typically , smooth signals , such as natural images and communications signals , can be represented by a sparsity - inducing basis , such as a fourier or wavelet basis @xcite .",
    "the goal of cs is to reconstruct a high - dimensional signal from its lower - dimensional linear transformation data , utilizing the prior knowledge on the sparsity of the signal @xcite .",
    "this results in time , cost , and precision advantages .",
    "mathematically , the cs problem can be expressed as follows : an @xmath4-dimensional vector @xmath5 is linearly transformed into an @xmath6-dimensional vector @xmath7 by an @xmath8-dimensional measurement matrix @xmath9 , as @xmath10 @xcite .",
    "the observer is free to choose the measurement protocol . given @xmath9 and @xmath7 , the central problem is how to reconstruct @xmath5 . when @xmath11 , due to the loss of information , the inverse problem has infinitely many solutions",
    "however , when it is guaranteed that @xmath5 has only @xmath12 nonzero entries in some convenient basis ( i.e. , when the signal is sparse enough ) and the measurement matrix is incoherent with that basis , there is a high probability that the inverse problem has a unique and exact solution .",
    "considerable efforts have been made to clarify the condition for the uniqueness and correctness of the solution , and to develop practically feasible signal reconstruction algorithms @xcite .",
    "recently , a scheme called _",
    "1-bit compressed sensing ( 1-bit cs ) _ was proposed . in 1-bit cs ,",
    "the signal is recovered from only the sign data of the linear measurements @xmath13 , where @xmath14 for @xmath15 is a component - wise operation when @xmath16 is a vector @xcite .",
    "discarding the amplitude information can significantly reduce the amount of data to be stored and/or transmitted .",
    "this is highly advantageous for most real - world applications , particularly those in which the measurement is accompanied by the transmission of digital information @xcite . in 1-bit cs ,",
    "the amplitude information is lost during the measurement stage , making perfect recovery of the original signal impossible .",
    "thus , we generally need more measurements to compensate for the loss of information .",
    "the scheme is considered to have practical relevance in situations where perfect recovery is not required , and measurements are inexpensive but precise quantization is expensive .",
    "these features are very different from those of general cs .",
    "the most widely used signal reconstruction scheme in cs is @xmath3-norm minimization , which searches for the vector with the smallest @xmath3-norm @xmath17 under the constraint @xmath18 .",
    "this is based on the work of cands et al .",
    "@xcite@xcite , who also suggested the use of a random measurement matrix @xmath1 with independent and identically distributed entries . because the optimization problem is convex and",
    "can be solved using efficient linear programming techniques , these ideas have led to various fast and efficient algorithms .",
    "the @xmath3-reconstruction is now widely used , and is responsible for the surge of interest in cs over the past few years . against this background ,",
    "@xmath3-reconstruction was the first technique attempted in the development of the 1-bit cs problem . in @xcite ,",
    "an approximate signal recovery algorithm was proposed based on the minimization of the @xmath3-norm under the constraint @xmath19 , and its utility was demonstrated by numerical experiments . in @xcite ,",
    "the capabilities of this method were analyzed , and a new algorithm based on the cavity method was presented .",
    "however , the significance of the @xmath3-based scheme may be rather weak for 1-bit cs , because the loss of convexity prevents the development of mathematically guaranteed and practically feasible algorithms .",
    "therefore , we propose another approach based on bayesian inference for 1-bit cs , focused on the case that each entry of @xmath1 is independently generated from a standard gaussian distribution , and the output @xmath2 is noisless . although the bayesian approach is guaranteed to achieve the optimal performance when the actual signal distribution is given , quantifying the performance gain is a nontrivial task .",
    "we accomplish this task utilizing the replica method , which shows that when non - zero entries of the signal follow zero mean gaussian distributions , the bayesian optimal inference asymptotically saturates the mean squared error ( mse ) performance obtained when the positions of non - zero signal entries are known as @xmath20 .",
    "this means that , in such cases , at least in terms of mses , the correct prior knowledge of the sparsity asymptotically becomes as informative as the knowledge of the exact positions of the non - zero entries .",
    "unfortunately , performing the exact bayesian inference is computationally difficult .",
    "this difficulty is resolved by employing the generalized approximate message passing technique , which is regarded as a variation of belief propagation or the cavity method @xcite .",
    "this paper is organized as follows .",
    "the next section sets up the 1-bit cs problem . in section 3",
    ", we examine the signal recovery performance achieved by the bayesian scheme utilizing the replica method . in section 4 ,",
    "an approximate signal recovery algorithm based on belief propagation is developed .",
    "the utility of this algorithm is tested and its asymptotic performance is analyzed in section 5 .",
    "the final section summarizes our work .",
    "let us suppose that entry @xmath21 @xmath22 of an @xmath4-dimensional signal ( vector ) @xmath23 is independently generated from an identical sparse distribution : @xmath24 where @xmath25 $ ] represents the density of nonzero entries in the signal , and @xmath26 is a distribution function of @xmath27 that has a finite second moment and does not have finite mass at @xmath28 . in 1-bit cs , the measurement is performed as @xmath29 where @xmath30 operates in a component - wise manner , and for simplicity we assume that each entry of the @xmath8 measurement matrix @xmath1 is provided as a sample of a gaussian distribution of zero mean and variance @xmath31 .",
    "we shall adopt the bayesian approach to reconstruct the signal from the 1-bit measurement @xmath7 assuming that @xmath1 is correctly known in the recovery stage .",
    "let us denote an arbitrary recovery scheme for the measurement @xmath2 as @xmath32 , where we impose a normalization constraint @xmath33 to compensate for the loss of amplitude information by the 1-bit measurement .",
    "equations ( [ sparse ] ) and ( [ measurement ] ) indicate that , for a given @xmath1 , the joint distribution of the sparse vector and its 1-bit measurement is @xmath34 where @xmath35 for @xmath36 , and vanishes otherwise .",
    "this generally provides @xmath37 with the mean square error , which is hereafter handled as the performance measure for the signal reconstruction - norm , can also be chosen as the performance measure .",
    "the argument shown in this section holds similarly even when such measures are used .",
    "] , as follows : @xmath38 the following theorem forms the basis of our bayesian approach .",
    "[ theorem1 ] @xmath39 is lower bounded as @xmath40 where @xmath41 is the marginal distribution of the 1-bit measurement @xmath2 and @xmath42 generally denotes the posterior mean of an arbitrary function of @xmath0 , @xmath43 , given @xmath2 .",
    "the equality holds for the bayesian optimal signal reconstruction @xmath44    employing the bayes formula @xmath45 in ( [ msedef ] ) yields the expression @xmath46 inserting the cauchy ",
    "schwarz inequality @xmath47 into the right - hand side of ( [ mse2 ] ) yields the lower bound of ( [ mse theorem ] ) , where the equality holds when @xmath48 is parallel to @xmath49 .",
    "this , in conjunction with the normalization constraint of @xmath48 , leads to ( [ bayesian_reconstruction ] ) .",
    "the above theorem guarantees that the bayesian approach achieves the best possible performance in terms of mse .",
    "therefore , we hereafter focus on the reconstruction scheme of ( [ bayesian_reconstruction ] ) , quantitatively evaluate its performance , and develop a computationally feasible approximate algorithm .",
    "in statistical mechanics , the macroscopic behavior of the system is generally analyzed by evaluating the partition function or its negative logarithm , free energy . in our signal reconstruction problem",
    ", the marginal likelihood @xmath50 of ( [ marginal_y ] ) plays the role of the partition function .",
    "however , this still depends on the quenched random variables @xmath2 and @xmath1 . therefore , we must further average the free energy as @xmath51_{{\\boldsymbol{y}},{\\boldsymbol{\\phi}}}$ ] to evaluate the typical performance , where @xmath52_{{\\boldsymbol{y}},{\\boldsymbol{\\phi}}}$ ] denotes the configurational average concerning @xmath7 and @xmath9 .    unfortunately , directly averaging the logarithm of random variables is , in general , technically difficult .",
    "thus , we resort to the replica method to practically resolve this difficulty @xcite . for this",
    ", we first evaluate the @xmath53-th moment of the marginal likelihood @xmath54_{{\\boldsymbol{\\phi}},{\\boldsymbol{y}}}$ ] for @xmath55 using the formula @xmath56 which holds only for @xmath55 . here , @xmath57",
    "( @xmath58 ) denotes the @xmath59-th replicated signal . averaging ( [ eq : expansion ] ) with respect to @xmath1 and @xmath2 results in the saddle - point evaluation concerning the macroscopic variables @xmath60 and @xmath61 ( @xmath62 ) .",
    "although ( [ eq : expansion ] ) holds only for @xmath63 , the expression @xmath64_{{\\boldsymbol{\\phi}},{\\boldsymbol{y}}}$ ] obtained by the saddle - point evaluation under a certain assumption concerning the permutation symmetry with respect to the replica indices @xmath65 is obtained as an analytic function of @xmath53 , which is likely to also hold for @xmath66 .",
    "therefore , we next utilize the analytic function to evaluate the average of the logarithm of the partition function as @xmath67_{{\\boldsymbol{y}},{\\boldsymbol{\\phi}}}.   \\label{free energy density}\\end{aligned}\\ ] ] in particular , under the replica symmetric ansatz , where the dominant saddle - point is assumed to be of the form @xmath68 the above procedure expresses the average free energy density as @xmath69 here , @xmath70 , @xmath71 , @xmath72 is a gaussian measure , @xmath73 denotes the extremization of a function @xmath74 with respect to @xmath75 , @xmath76 , and @xmath77 the derivation of @xmath78 is provided in [ replicaderivation ] .    in evaluating the right - hand side of ( [ free energy density ] ) ,",
    "@xmath50 not only gives the marginal likelihood ( the partition function ) , but also the conditional density of @xmath2 for taking the configurational average .",
    "this accordance between the partition function and the distribution of the quenched random variables is generally known as the nishimori condition in spin glass theory @xcite , for which the replica symmetric ansatz ( [ rsanzats ] ) is supported by other schemes than the replica method @xcite , yielding the identity @xmath79_{{\\boldsymbol{y}},{\\boldsymbol{\\phi}}}=\\int { \\rm d}{\\boldsymbol{\\phi } } p({\\boldsymbol{\\phi } } ) \\left ( \\sum_{{\\boldsymbol{y } } } p^{n+1}({\\boldsymbol{y}}|{\\boldsymbol{\\phi } } ) \\right ) $ ] .",
    "this indicates that the true signal , @xmath80 , can be handled on an equal footing with the other @xmath53 replicated signals @xmath81 in the replica computation . as @xmath82 , this higher replica symmetry among the @xmath83 replicated variables allows us to further simplify the replica symmetric ansatz ( [ rsanzats ] ) by imposing four extra constraints : @xmath84 , @xmath85 , @xmath86 , and @xmath87 . as a consequence",
    ", the extremization condition of ( [ eq : free energy ] ) is summarized by the non - linear equations @xmath88    in physical terms , the value of @xmath89 determined by these equations is the typical overlap @xmath90_{{\\boldsymbol{y}},{\\boldsymbol{\\phi } } } $ ] between the original signal @xmath80 and the posterior mean @xmath91 .",
    "the law of large numbers and the self - averaging property guarantee that both @xmath92 and @xmath93 converge to @xmath94 with a probability of unity for typical samples .",
    "this indicates that the typical value of the direction cosine between @xmath80 and @xmath95 can be evaluated as @xmath96_{{\\boldsymbol{y}},{\\boldsymbol{\\phi } } } \\simeq \\left [ ( { \\boldsymbol{x}}^0 \\cdot \\left \\langle { \\boldsymbol{x } } \\right \\rangle_{|{\\boldsymbol{y}},{\\boldsymbol{\\phi } } } ) \\right ] _ { { \\boldsymbol{y}},{\\boldsymbol{\\phi}}}/ \\left ( \\left [ \\left |{\\boldsymbol{x}}^0 \\right |\\right ] _ { { \\boldsymbol{x}}^0 } \\left   [ \\left |\\left \\langle { \\boldsymbol{x } } \\right \\rangle_{|{\\boldsymbol{y}},{\\boldsymbol{\\phi } } } \\right |\\right ] _ { { \\boldsymbol{y}},{\\boldsymbol{\\phi } } } \\right ) = n m /(n\\sqrt{\\rho m})=\\sqrt{m/\\rho}$ ] . therefore , the mse in ( [ msedef ] ) can be expressed using @xmath89 and @xmath94 as @xmath97    the symmetry between @xmath80 and the other replicated variables @xmath57 @xmath98 provides @xmath99 with further information - theoretic meanings .",
    "inserting @xmath100 into the definition of @xmath99 gives @xmath101 , which indicates that @xmath99 accords with the entropy density of @xmath2 for typical measurement matrices @xmath1 .",
    "the expression @xmath102 guarantees that the conditional entropy of @xmath2 given @xmath0 and @xmath1 , @xmath103 , always vanishes .",
    "these indicate that @xmath99 also implies a mutual information density between @xmath2 and @xmath0 .",
    "this physically quantifies the optimal information gain ( per entry ) of @xmath0 that can be extracted from the 1-bit measurement @xmath2 for typical @xmath1 .",
    "equation ( [ mse ] ) represents the potential performance of the bayesian optimal signal reconstruction of 1-bit cs .",
    "however , in practice , exploiting this performance is a non - trivial task , because performing the exact bayesian reconstruction ( [ bayesian_reconstruction ] ) is computationally difficult . to resolve this difficulty",
    ", we now develop an approximate reconstruction algorithm following the framework of belief propagation ( bp ) .",
    "actually , bp has been successfully employed for standard cs problems with linear measurements , showing excellent performance in terms of both reconstruction accuracy and computational efficiency @xcite . to incorporate the non - linearity of the 1-bit measurement",
    ", we employ a variant of bp known as generalized approximate message passing ( gamp ) @xcite , which can also be regarded as an approximate bayesian inference algorithm for perceptron - type networks @xcite .",
    "in general , the canonical bp equations for the probability measure @xmath104 are expressed in terms of @xmath105 messages , @xmath106 and @xmath107 , which represent probability distribution functions that carry posterior information and output measurement information , respectively .",
    "they can be written as @xmath108 here , @xmath109 and @xmath110 are normalization factors ensuring that @xmath111 , and we also define @xmath112 . using ( [ m_mu_i ] ) , the approximation of marginal distributions @xmath113 , which are often termed beliefs , are evaluated as @xmath114 where @xmath115 is a normalization factor for @xmath116 . to simplify the notation",
    ", we hereafter convert all measurement results to @xmath117 by multiplying each row of the measurement matrix @xmath118 by @xmath119 @xmath120 , giving @xmath121 , and denote the resultant matrix as @xmath118 . in the new notation , @xmath122 .",
    "next , we introduce means and variances of @xmath123 in the posterior information message distributions as @xmath124 we also define @xmath125 and @xmath126 for notational convenience .",
    "similarly , the means and variances of the beliefs , @xmath127 and @xmath128 , are introduced as @xmath129 and @xmath130 .",
    "note that @xmath131 represents the approximation of the posterior mean @xmath91 .",
    "this , in conjunction with a consequence of the law of large numbers @xmath132 , indicates that the bayesian optimal reconstruction is approximately performed as @xmath133 .    to enhance the computational tractability ,",
    "let us rewrite the functional equations of ( [ m_mu_i ] ) and ( [ m_i_mu ] ) into algebraic equations using sets of @xmath134 and @xmath135 . to do this",
    ", we insert the identity @xmath136 into ( [ m_mu_i ] ) , which yields @xmath137 the smallness of @xmath138 allows us to truncate the taylor series of the last exponential in equation ( [ m_mu_i_1 ] ) up to the second order of @xmath139 . integrating @xmath140 for @xmath141",
    ", we obtain the expression @xmath142 and carrying out the resulting gaussian intergral of @xmath143 , we obtain @xmath144 since @xmath145 vanishes as @xmath146 while @xmath147 , we can omit @xmath148 in ( [ m_mu_i_2 ] ) .",
    "in addition , we replace @xmath149 in @xmath150 with its expectation @xmath31 , utilizing the law of large numbers .",
    "this removes the dependence on the index @xmath151 , making all @xmath152 equal to their average @xmath153 the smallness of @xmath154 again allows us to truncate the taylor series of the exponential in ( [ m_mu_i_2 ] ) up to the second order .",
    "thus , we have a parameterized expression of @xmath155 : @xmath156 where the parameters @xmath157 and @xmath158 are evaluated as @xmath159 using @xmath160 the derivation of these is given in [ gamp_derivation ] .",
    "equations ( [ a ] ) and ( [ b ] ) act as the algebraic expression of ( [ m_mu_i ] ) . in the sign output channel ,",
    "inserting @xmath122 into ( [ g_out ] ) gives @xmath161 and @xmath162 for 1-bit cs as @xmath163    to obtain a similar expression for ( [ m_i_mu ] ) , we substitute the last expression of ( [ m_mu_i_3 ] ) into ( [ m_i_mu ] ) , which leads to @xmath164   e^{-(x_{i}^2 /2)\\sum\\limits_{\\gamma\\neq\\mu}a_{\\gamma\\rightarrow i } + x_i\\sum\\limits_{\\gamma\\neq\\mu}b_{\\gamma\\rightarrow i } } .\\end{aligned}\\ ] ] this indicates that @xmath165 in ( [ m_i_mu ] ) can be expressed as a gaussian distribution with mean @xmath166 and variance @xmath167 .",
    "inserting these into ( [ a_imu ] ) and ( [ nu_imu ] ) provides the algebraic expression of ( [ m_i_mu ] ) as @xmath168 where @xmath169 and @xmath170 stand for the mean and variance of an auxiliary distribution of @xmath16 @xmath171   \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x - r)^2}{2\\sigma^2}}\\end{aligned}\\ ] ] where @xmath172 is a normalization constant , respectively . for instance , when @xmath173 is a gaussian distribution of mean @xmath174 and variance @xmath175 , we have @xmath176\\nonumber\\\\                      \\times\\exp{\\bigl\\{-\\frac{r^2}{2\\sigma^2}-\\frac{(r-\\bar{x})^2}{2(\\sigma^2+\\sigma^2)}\\bigr\\ } }                      + \\rho^2\\exp{\\bigl\\{-\\frac{(r-\\bar{x})^2}{\\sigma^2+\\sigma^2}\\bigr\\}}\\frac{\\sigma^2\\sigma^4}{(\\sigma^2+\\sigma^2)^2}\\biggr\\ } \\nonumber\\\\                      \\times\\biggl\\{(1-\\rho)\\exp{\\bigl\\{-\\frac{r^2}{2\\sigma^2}\\bigr\\}}+\\rho\\frac{\\sigma}{\\sqrt{\\sigma^2+\\sigma^2}}\\exp{\\bigl\\ { -\\frac{(r-\\bar{x})^2}{2(\\sigma^2+\\sigma^2)}\\bigr\\ } } \\biggr\\}^{-2}.                      \\label{fc}\\end{aligned}\\ ] ] for the signal reconstruction , we need to evaluate the moments of @xmath177 .",
    "this can be performed by simply adding back the @xmath151 dependent part to ( [ vstepa ] ) and ( [ vstepnu ] ) as @xmath178 where @xmath179 , @xmath180 . for large @xmath4",
    ", @xmath181 typically converges to a constant , independent of the index , as @xmath182 .",
    "this , in conjunction with ( [ a ] ) and ( [ b ] ) , yields @xmath183    bp updates @xmath105 messages using ( [ a ] ) , ( [ b ] ) , ( [ vstepa ] ) , and ( [ vstepnu ] ) ( @xmath184 ) in each iteration .",
    "this requires a computational cost of @xmath185 per iteration , which may limit the practical utility of bp to systems of relatively small size . to enhance the practical utility ,",
    "let us rewrite the bp equations into those of @xmath186 messages for large @xmath4 , which will result in a significant reduction of computational complexity to @xmath187 per iteration . to do this",
    ", we express @xmath188 by applying taylor s expansion to ( [ vstepa ] ) around @xmath189 as @xmath190 where @xmath191 and @xmath192 is approximated as @xmath193 , because of the smallness of @xmath194 . multiplying this by @xmath138 and summing the resultant expressions over @xmath195 yields @xmath196 where we have used @xmath197 , which can be confirmed by ( [ fa ] ) and ( [ fc ] ) .",
    "let us assume that @xmath198 and @xmath199 are initially set to certain values .",
    "inserting these into ( [ v ] ) and ( [ omega ] ) gives @xmath200 and @xmath201 .",
    "substituting these into equations ( [ g_out_1bit ] ) and ( [ g_out_p_1bit ] ) yields a set of @xmath199 , which , in conjunction with @xmath202 , offers @xmath182 and @xmath203 through ( [ sigma ] ) and ( [ r ] ) .",
    "inserting these into ( [ a ] ) and ( [ nu ] ) offers a new set of @xmath198 . in this way",
    ", the iteration of ( [ v ] ) , ( [ omega ] ) @xmath204 ( [ g_out_1bit ] ) , ( [ g_out_p_1bit ] ) @xmath204 ( [ sigma ] ) , ( [ r ] ) @xmath204 ( [ a ] ) , ( [ nu ] ) @xmath204 ( [ v ] ) , ( [ omega ] ) @xmath205 constitutes a closed set of equations to update the sets of @xmath198 and @xmath199 .",
    "this is the generic gamp algorithm given a likelihood function @xmath206 and a prior distribution @xmath207 @xcite .",
    "we term the entire procedure the approximate message passing for 1-bit compressed sensing ( 1bitamp ) algorithm .",
    "the pseudocode of this algorithm is summarized in figure [ proposedalgorithm ] .",
    "three issues are noteworthy .",
    "first , for relatively large systems , e.g. , @xmath208 , the iterative procedure converges easily in most cases .",
    "nevertheless , since it relies on the law of large numbers , some divergent behavior appears as @xmath4 becomes smaller . even for such cases , however , employing an appropriate damping factor in conjunction with a normalization of @xmath209 at each update",
    "considerably improves the convergence property .",
    "second , the most time - consuming parts of this iteration are the matrix - vector multiplications @xmath210 in ( [ r ] ) and @xmath211 in ( [ omega ] ) .",
    "this indicates that the computational complexity is @xmath212 per iteration .",
    "finally , @xmath127 in equation ( [ r ] ) and @xmath213 in equation ( [ omega ] ) correspond to what is known as the _ onsager reaction term _ in the spin glass literature @xcite .",
    "these terms stabilize the convergence of 1bitamp , effectively canceling the self - feedback effects .",
    "approximate message passing for 1-bit cs ^ * , ^ * , ^ *    1 )  : + : _ 0 ^ * + : _ 0 ^ * + : _ 0 ^ * + : k0 + 2 )  : + k k+1 + 3 )  : + _ kn^-1 ( ( _ k-1 ) ) + 4 )  : + _ k _ k-1-_kg_out(_k-1 , _ k ) + 5 )  : + _ k^2 n ( ( g_out^(_k , _ k)))^-1 + 6 )  : + ( ) _ k_k-1+(g_out(_k , _ k))_k^2 + 7 )  : + _ kf_a ( _ k^2 , _ k ) + 8)  : + _ kf_c ( _ k^2 , _ k ) + 9 )  :",
    "c    mse ( in decibels ) versus measurement bit ratio @xmath214 for 1-bit cs for gauss - bernoulli prior .",
    "( a ) , ( b ) , ( c ) , and ( d ) correspond to @xmath215 , and @xmath216 , respectively .",
    "red curves represent the theoretical prediction of @xmath3-norm minimization @xcite ; blue curves represent the theoretical prediction of the bayesian optimal approach ; green curves represent the theoretical prediction of the bayesian optimal approach when the positions of all nonzero components in the signal are known , which is obtained by setting @xmath217 and @xmath218 in ( [ m_result ] ) and ( [ mhat_result ] ) .",
    "crosses represent the average of 1000 experimental results by the 1bitamp algorithm in figure [ algorithm ] for a system size of @xmath219 .",
    "circles show the average of 1000 experimental results by an @xmath3-based algorithm rfpi proposed in @xcite for 1-bit cs in the system size of @xmath220 . although the replica symmetric prediction for the @xmath3-based approach is thermodynamically unstable , the experimental results of rfpi are numerically consistent with it very well.,width=302 ]    mse ( in decibels ) versus measurement bit ratio @xmath214 for 1-bit cs for gauss - bernoulli prior .",
    "( a ) , ( b ) , ( c ) , and ( d ) correspond to @xmath215 , and @xmath216 , respectively .",
    "red curves represent the theoretical prediction of @xmath3-norm minimization @xcite ; blue curves represent the theoretical prediction of the bayesian optimal approach ; green curves represent the theoretical prediction of the bayesian optimal approach when the positions of all nonzero components in the signal are known , which is obtained by setting @xmath217 and @xmath218 in ( [ m_result ] ) and ( [ mhat_result ] ) .",
    "crosses represent the average of 1000 experimental results by the 1bitamp algorithm in figure [ algorithm ] for a system size of @xmath219 .",
    "circles show the average of 1000 experimental results by an @xmath3-based algorithm rfpi proposed in @xcite for 1-bit cs in the system size of @xmath220 . although the replica symmetric prediction for the @xmath3-based approach is thermodynamically unstable , the experimental results of rfpi are numerically consistent with it very well.,width=302 ]     +    mse ( in decibels ) versus measurement bit ratio @xmath214 for 1-bit cs for gauss - bernoulli prior .",
    "( a ) , ( b ) , ( c ) , and ( d ) correspond to @xmath215 , and @xmath216 , respectively .",
    "red curves represent the theoretical prediction of @xmath3-norm minimization @xcite ; blue curves represent the theoretical prediction of the bayesian optimal approach ; green curves represent the theoretical prediction of the bayesian optimal approach when the positions of all nonzero components in the signal are known , which is obtained by setting @xmath217 and @xmath218 in ( [ m_result ] ) and ( [ mhat_result ] ) .",
    "crosses represent the average of 1000 experimental results by the 1bitamp algorithm in figure [ algorithm ] for a system size of @xmath219 .",
    "circles show the average of 1000 experimental results by an @xmath3-based algorithm rfpi proposed in @xcite for 1-bit cs in the system size of @xmath220 . although the replica symmetric prediction for the @xmath3-based approach is thermodynamically unstable , the experimental results of rfpi are numerically consistent with it very well.,width=302 ]    mse ( in decibels ) versus measurement bit ratio @xmath214 for 1-bit cs for gauss - bernoulli prior .",
    "( a ) , ( b ) , ( c ) , and ( d ) correspond to @xmath215 , and @xmath216 , respectively .",
    "red curves represent the theoretical prediction of @xmath3-norm minimization @xcite ; blue curves represent the theoretical prediction of the bayesian optimal approach ; green curves represent the theoretical prediction of the bayesian optimal approach when the positions of all nonzero components in the signal are known , which is obtained by setting @xmath217 and @xmath218 in ( [ m_result ] ) and ( [ mhat_result ] ) .",
    "crosses represent the average of 1000 experimental results by the 1bitamp algorithm in figure [ algorithm ] for a system size of @xmath219 .",
    "circles show the average of 1000 experimental results by an @xmath3-based algorithm rfpi proposed in @xcite for 1-bit cs in the system size of @xmath220 . although the replica symmetric prediction for the @xmath3-based approach is thermodynamically unstable , the experimental results of rfpi are numerically consistent with it very well.,width=302 ]     and @xmath221 , and the errorbar , which are evaluated from 10000 experiments .",
    "red , blue , magenta , and green represent @xmath215 , and @xmath216 , respectively.,width=377 ]    to examine the utility of 1bitamp , we carried out numerical experiments for gauss - bernoulli prior , @xmath222 with system size @xmath219 .",
    "we set initial conditions of @xmath223 , and @xmath224 , where @xmath225 is the @xmath4-dimensional vector whose entries are all unity , and stopped the algorithm after @xmath226 iterations ( figure [ err_iteration ] ) .",
    "the mse results for various sets of @xmath214 and @xmath94 are shown as crosses in figures [ figure2 ] ( a)(d ) .",
    "each cross denotes an experimental estimate obtained from 1000 experiments .",
    "the standard deviations are omitted , as they are smaller than the size of the symbols .",
    "the convergence time is short , which verifies the significant computational efficiency of 1bitamp .",
    "for example , in a matlab@xmath227 environment , for @xmath228 , one experiment takes around 0.2 s.    to test the consistency of 1bitamp with respect to replica theory , we solved the saddle - point equations ( [ m_result ] ) and ( [ mhat_result ] ) for gauss - bernoulli prior for each set of @xmath214 and @xmath94 . the blue curves in figures [ figure2 ] ( a)(d ) show the theoretical mse evaluated by ( [ mse ] ) against @xmath214 for @xmath215 , and @xmath216 .",
    "the excellent agreement between the numerical experiments and the theoretical prediction indicates that 1bitamp nearly saturates the potentially achievable mse of the signal recovery scheme based on the bayesian optimal approach .    for comparison , figures [ figure2 ] ( a)(d ) also plot the replica symmetric prediction of mses for the @xmath3-norm minimization approach ( red curves ) to the gauss - bernoulli signal , which was examined in an earlier study  @xcite . although the replica symmetric prediction is thermodynamically unstable , it is numerically consistent with the experimental results ( circles ) given by the algorithm proposed in @xcite",
    ". therefore , the prediction at least serves as a good approximation .",
    "we also plot the mses of the bayesian optimal approach when the positions of the non - zero components of @xmath0 are known ( green curves ) .",
    "these act as lower bounds for the mses of the bayesian optimal approach . when the positions of non - zero components of @xmath0 are known , we need not consider the part containing zero components . therefore , the problem can be seen as that defined when a @xmath229-dimensional signal @xmath0 is measured by an @xmath230-dimensional matrix . in such situations ,",
    "performance can be evaluated by setting @xmath231 and replacing @xmath232 with @xmath233 in ( [ m_result ] ) and ( [ mhat_result ] ) , as the dimensionality of @xmath0 is reduced from @xmath4 to @xmath234 . solving ( [ m_result ] ) and ( [ mhat_result ] ) for @xmath235 shows that the mses of the bayesian optimal approach can be asymptotically expressed as @xmath236 for @xmath235 , which accords exactly with the asymptotic form of the green curves ( figure  [ fig.asymptotic ] : left panel , see [ app3 ] ) . since we defined mse with the normalized signal , this holds for all zero mean gauss - bernoulli distributions of any variance . on the other hand , the asymptotic form of the mse for the @xmath3-norm approach",
    "is evaluated as @xmath237 ^ 2}{\\alpha^2 } , \\label{mse_l1}\\end{aligned}\\ ] ] where @xmath238 is the value of @xmath239 for the @xmath3-norm approach obtained for @xmath240 ( see [ app4 ] ) .",
    "c     for bayesian optimal signal reconstruction of 1-bit cs for gauss - bernoulli prior .",
    "red , blue , magenta , and green correspond to @xmath215 , and @xmath216 , respectively .",
    "the solid curves represent the theoretical prediction obtained by ( [ m_result ] ) and ( [ mhat_result ] ) ; dashed curves show the performance when the positions of non - zero entries are known , and dotted curves denote the asymptotic forms ( [ asymptotics ] ) , which are indistinguishable from the dashed curves because they closely overlap . right : ratio of mse between @xmath241-norm and bayesian approaches when @xmath235 versus sparsity @xmath94 of the signal .",
    "the inset shows a log - log plot for @xmath242 .",
    "the least - squares fit implies that the ratio diverges as @xmath243 as @xmath244 .",
    ", width=302 ]     for bayesian optimal signal reconstruction of 1-bit cs for gauss - bernoulli prior .",
    "red , blue , magenta , and green correspond to @xmath215 , and @xmath216 , respectively .",
    "the solid curves represent the theoretical prediction obtained by ( [ m_result ] ) and ( [ mhat_result ] ) ; dashed curves show the performance when the positions of non - zero entries are known , and dotted curves denote the asymptotic forms ( [ asymptotics ] ) , which are indistinguishable from the dashed curves because they closely overlap . right : ratio of mse between @xmath241-norm and bayesian approaches when @xmath235 versus sparsity @xmath94 of the signal .",
    "the inset shows a log - log plot for @xmath242 .",
    "the least - squares fit implies that the ratio diverges as @xmath243 as @xmath244 .",
    ", width=302 ]    equation ( [ asymptotics ] ) means that , at least in terms of mses , correct prior knowledge of the sparsity asymptotically becomes as informative as the knowledge of the exact positions of the non - zero components . in most statistical models ,",
    "the accuracy of asymptotic inference is expressed as a function of the ratio @xmath70 between the number of data @xmath6 and the dimensionality of the variables to be inferred @xmath4 @xcite .",
    "equation ( [ asymptotics ] ) indicates that , in the current problem , the dimensionality @xmath4 is replaced with the actual degree of the non - zero components @xmath234 , which originates from the singularity of the prior distribution ( [ sparse ] ) .",
    "this implies that caution is necessary in testing the validity of statistical models when sparse priors are employed , since conventional information criteria such as akaike s information criterion @xcite and the minimum description length @xcite mostly handle objective statistical models that are free of singularities , so that the model complexity is naively incorporated as the number of parameters @xmath4 @xcite .",
    "equation ( [ mse_l1 ] ) indicates that , even if prior knowledge of the sparsity is not available , optimal convergence can be achieved in terms of the `` exponent ( decay of @xmath245 ) '' as @xmath240 using the @xmath3-norm approach .",
    "however , the performance can differ considerably in terms of the `` pre - factor ( coefficient of @xmath246 ) '' the right panel of figure [ fig.asymptotic ] plots the ratio @xmath247 , which diverges as @xmath243 as @xmath244 .",
    "this indicates that prior knowledge of the sparsity of the objective signal is more beneficial as @xmath94 becomes smaller .",
    "c     for 1-bit cs in the case of laplace - bernoulli prior .",
    "solid lines represent the theoretical prediction and the markers represent the experiment results by 1bitamp algorithm for a signal size of @xmath219 and averaged from 1000 experiments .",
    "red , blue , magenta , and green represent @xmath215 , and @xmath216 , respectively .",
    "right : asymptotic behavior of mse for laplace - bernoulli prior , when the positions of zero entries of the signal are known and unknown .",
    "this implies that mse of these two cases are different even asymptotically .",
    ", width=302 ]     for 1-bit cs in the case of laplace - bernoulli prior .",
    "solid lines represent the theoretical prediction and the markers represent the experiment results by 1bitamp algorithm for a signal size of @xmath219 and averaged from 1000 experiments .",
    "red , blue , magenta , and green represent @xmath215 , and @xmath216 , respectively .",
    "right : asymptotic behavior of mse for laplace - bernoulli prior , when the positions of zero entries of the signal are known and unknown .",
    "this implies that mse of these two cases are different even asymptotically .",
    ", width=302 ]     for checking the generality of the results obtained for gauss - bernoulli prior , we also carried out similar analysis for laplace - bernoulli prior @xmath248 the left panel of fig .",
    "[ laplace_msedb ] shows the comparison between the replica prediction and the experimental results by gamp , which supports that the replica and gamp correspondence does hold for general priors .",
    "the right panel of fig .",
    "[ laplace_msedb ] compares the performance with that achieved when the positions of non - zero entries are known .",
    "unlike the case of gauss - bernoulli prior , the two performances do not get close even asymptotically .",
    "this implies that the significance of utility of the bayesian approach depends considerably on the statistical property of the objective signal .",
    "in summary , we have examined the typical performance of the bayesian optimal signal recovery for 1-bit cs using methods from statistical mechanics . for gauss - bernoulli prior , using the replica method to compare the performance of the bayesian optimal approach to the @xmath3-norm minimization , we have shown that the utility of correct prior knowledge on the objective signal , which is incorporated in the bayesian optimal scheme , becomes more significant as the density of non - zero entries @xmath94 in the signal decreases .",
    "in addition , we have clarified that , for this particular prior , the mse performance asymptotically saturates that obtained when the exact positions of non - zero entries are exactly known as the number of 1-bit measurements increases .",
    "we have also developed a practically feasible approximate algorithm for bayesian signal recovery , which can be regarded as a special case of the gamp algorithm .",
    "the algorithm has a computational cost of the square of the system size per update , exhibiting a fairly good convergence property as the system size becomes larger .",
    "the experimental results for both gauss - bernoulli prior and laplace - bernoulli prior show excellent agreement with the predictions made by the replica method .",
    "these indicate that almost - optimal reconstruction performance can be attained with a computational complexity of the square of the signal length per update for general priors , which is highly beneficial in practice .",
    "obtaining the correct prior distribution of the sparse signal may be an obstacle to applying the current approach in practical problems .",
    "one possible solution is to estimate hyper - parameters that characterize the prior distribution in the reconstruction stage , as has been proposed for normal cs @xcite .",
    "it was reported that orthogonal measurement matrices , rather than those of statistically independent entries , enhance the signal reconstruction performance for several problems related to cs @xcite .",
    "such devices may also be effective for 1-bit cs .",
    "yx is supported by jsps research fellowships dc2 .",
    "this study was also partially supported by the jsps core - to - core program `` non - equilibrium dynamics of soft matter and information , '' jsps kakenhi nos .",
    "26011287 ( yx ) , 25120013 ( yk ) , and the grant dyspan of triangle de la physique ( lz ) .",
    "useful discussions with chistophe schlke are also acknowledged .",
    "averaging ( [ eq : expansion ] ) with respect to @xmath1 and @xmath2 gives the following expression for the @xmath53-th moment of the partition function : @xmath249_{{\\boldsymbol{\\phi}},{\\boldsymbol{y } } } = \\int \\prod_{a=1}^n \\left ( d \\textrm{\\boldmath $ x$}^a p\\left ( \\textrm{\\boldmath $ x^a$}\\right)\\right )   \\times\\left[\\prod_{a=1}^n \\prod_{\\mu=1}^m \\theta\\left ( ( { \\boldsymbol{y}})_\\mu   ( { \\boldsymbol{\\phi } } { \\boldsymbol{x}}^a)_\\mu \\right ) \\right]_{{\\boldsymbol{\\phi}},{\\boldsymbol{y}}}.   \\label{zmoment}\\end{aligned}\\ ] ] we insert @xmath250 trivial identities @xmath251 where @xmath252 , into ( [ zmoment ] ) .",
    "furthermore , we define a joint distribution of @xmath83 vectors @xmath253 as @xmath254 where @xmath255 is an @xmath256 symmetric matrix whose @xmath257 and other diagonal entries are fixed as @xmath94 and @xmath258 , respectively .",
    "@xmath259 denotes the distribution of the original signal @xmath80 , and @xmath260 is the normalization constant that ensures @xmath261 holds .",
    "these indicate that ( [ zmoment ] ) can also be expressed as @xmath249_{{\\boldsymbol{\\phi}},{\\boldsymbol{y } } } = \\int d{\\boldsymbol{q } } \\left ( v\\left ( { \\boldsymbol{q}}\\right ) \\times \\xi\\left ( { \\boldsymbol{q}}\\right ) \\right ) ,   \\label{zn}\\end{aligned}\\ ] ] where @xmath262 and @xmath263_{{\\boldsymbol{\\phi}}}.   \\label{xi}\\end{aligned}\\ ] ]    equation ( [ xi ] ) can be regarded as the average of @xmath264 with respect to @xmath265 and @xmath1 over distributions of @xmath266 and @xmath267 . in computing this ,",
    "note that the central limit theorem guarantees that @xmath268 can be handled as zero - mean multivariate gaussian random numbers whose variance and covariance are given by @xmath269_{{\\boldsymbol{\\phi}},\\{{\\boldsymbol{x}}^a\\ } } = \\delta_{\\mu \\nu } q_{ab } , \\end{aligned}\\ ] ] when @xmath1 and @xmath265 are generated independently from @xmath270 and @xmath266 , respectively .",
    "this means that ( [ xi ] ) can be evaluated as @xmath271    on the other hand , expressions @xmath272 and @xmath273 and use of the saddle - point method , offer @xmath274 here , @xmath275 and @xmath276 is an @xmath277 symmetric matrix whose @xmath257 and other diagonal components are given as @xmath278 and @xmath279 , respectively .",
    "the off - diagonal entries are @xmath280",
    ". equations ( [ logxi ] ) and ( [ logv ] ) indicate that @xmath281_{{\\boldsymbol{\\phi}},{\\boldsymbol{y}}}$ ] is correctly evaluated by the saddle - point method with respect to @xmath282 in the assessment of the right - hand side of ( [ zn ] ) , when @xmath4 and @xmath6 tend to infinity and @xmath70 remains finite .",
    "let us assume that the relevant saddle - point for assessing ( [ zn ] ) is of the form ( [ rsanzats ] ) and , accordingly , @xmath283 the @xmath83-dimensional gaussian random variables @xmath284 whose variance and covariance are given by ( [ rsanzats ] ) can be expressed as @xmath285 utilizing @xmath286 independent standard gaussian random variables @xmath287 and @xmath288 .",
    "this indicates that ( [ logxi ] ) is evaluated as @xmath289 on the other hand , substituting ( [ rshatq ] ) into ( [ logv ] ) , in conjunction with the identity @xmath290 provides @xmath291_{x^0,z } \\right \\}.",
    "\\label{newv}\\end{aligned}\\ ] ] although we have assumed that @xmath63 , the expressions of ( [ newxi ] ) and ( [ newv ] ) are likely to hold for @xmath66 as well .",
    "therefore , the average free energy @xmath292 can be evaluated by substituting these expressions into the formula @xmath293_{{\\boldsymbol{\\phi}},{\\boldsymbol{y } } } \\right ) $ ] .",
    "furthermore , employing the expressions that hold for @xmath294 , @xmath295 @xmath296 and @xmath297 , where @xmath298 is an arbitrary function , we obtain the form @xmath299 and we have @xmath300 using these in the resultant expression of @xmath292 gives ( [ eq : free energy ] ) .",
    "expanding the exponential in ( [ m_mu_i_2 ] ) up to the second order of @xmath301 and performing the integration with respect to @xmath302 gives @xmath303 where @xmath304 and @xmath305 equations ( [ c1 ] ) and ( [ c2 ] ) imply that @xmath306 and @xmath307 can be expressed as @xmath308 and @xmath309 , respectively . inserting this into ( [ a_appendix ] ) and ( [ b_appendix ] ) , we obtain ( [ m_mu_i_3])([g_out_p ] ) .",
    "the behavior as @xmath311 and @xmath312 is obtained as @xmath240 .",
    "this implies that , for gauss - bernoulli distribution , equations  ( [ m_result ] ) and ( [ mhat_result ] ) can be evaluated as @xmath313^{-1 } \\cr & \\simeq & \\rho ( 1-\\hat{m}^{-1 } ) \\label{app3m}\\end{aligned}\\ ] ] and @xmath314 respectively . here ,",
    "the integration variables have been changed to @xmath315 and @xmath316 in ( [ app3 m ] ) and ( [ app3mhat ] ) , respectively , and we set @xmath317 .",
    "equations ( [ app3 m ] ) and ( [ app3mhat ] ) yield an asymptotic expression for @xmath89 : @xmath318 inserting this into ( [ mse ] ) gives ( [ asymptotics ] ) .",
    "the performance when the positions of non - zero entries are known can be evaluated by setting @xmath231 and replacing @xmath232 with @xmath233 in ( [ m_result ] ) and ( [ mhat_result ] ) as the dimensionality of @xmath0 is reduced from @xmath4 to @xmath234 .",
    "this reproduces ( [ asymptotics ] ) in the asymptotic region of @xmath235 .",
    "the saddle - point equations of the @xmath3-norm minimization approach under a normalization constraint of @xmath320 are as follows @xcite : @xmath321 \\right .",
    "\\nonumber\\\\            & & + \\rho\\left[\\left(\\hat{q}+\\hat{m}^2 + 1\\right)h\\left(\\frac{1}{\\sqrt{\\hat{q}+\\hat{m}^2}}\\right ) \\right . \\nonumber\\\\            & & \\left .",
    "-\\sqrt{\\frac{\\hat{q}+\\hat{m}^2}{2\\pi}}e^{-\\frac{1}{2\\left(\\hat{q}+\\hat{m}^2\\right)}}\\right]\\right\\}\\label{qh2_l1 } , \\\\",
    "! \\frac{2}{\\hat{q}}\\!\\left[\\!\\left(\\!1\\!-\\!\\rho\\right)h\\left(\\frac{1}{\\sqrt{\\hat{q}}}\\right )       \\!+\\!\\rho h\\!\\left(\\!\\frac{1}{\\sqrt{\\hat{q}\\!+\\!\\hat{m}^2}}\\!\\right)\\!\\right]\\label{chi_l1 } , \\\\ m\\!&=&\\!\\frac{2\\rho\\hat{m}}{\\hat{q}}h\\left(\\frac{1}{\\sqrt{\\hat{q}+\\hat{m}^2}}\\right)\\label{m_l1}. \\end{aligned}\\ ] ]    the behavior as @xmath322 and @xmath312 is obtained as @xmath240 .",
    "this implies that  ( [ qh2_l1 ] ) can be evaluated as @xmath323 , \\label{qh_l1_asym}\\end{aligned}\\ ] ] where @xmath324 $ ] . inserting  ( [ qh_l1_asym ] ) into  ( [ m_l1 ] )",
    ", we obtain @xmath325 where @xmath326 ^ 2/(2\\alpha^2 ) .",
    "\\label{delta_l1}\\end{aligned}\\ ] ] inserting ( [ mh_l1 ] ) , ( [ qh_l1_asym ] ) , ( [ m_l1_asym ] ) , and @xmath327/\\hat{q}$ ] into ( [ qh_l1 ] ) yields a closed equation with respect to @xmath239 : @xmath328^{-1}.   \\label{qh_l1_asym}\\end{aligned}\\ ] ] this determines the value of @xmath239 for @xmath240 , @xmath238 . combining ( [ delta_l1 ] ) and @xmath329 gives ( [ mse_l1 ] ) in the asymptotic region of @xmath330 .",
    "99 ` https://sites.google.com/site/igorcarron2/compressedsensinghardware ` elad m , 2010 _ sparse and redundant representations : from theory to applications in signal and image processing _",
    "( new york : springer ) starck",
    "j - l , murtagh f and fadili j m , 2010 _ sparse image and signal processing : wavelets , curvelets , morphological diversity _ ( new york : cambridge university press ) cands e j and wakin m b , 2008 _ ieee signal processing magazine _ march 2008 , 21 donoho d l , 2006 _ ieee trans .",
    "inform . theory _ * 52 * 1289 cands e j , romberg j and tao t , 2006 _ ieee trans .",
    "inform . theory _",
    "* 52 * 489 kabashima y , wadayama t and tanaka t , _ j. stat .",
    "( 2009 ) l09003 ; _ j. stat .",
    "( 2012 ) e07001 ganguli s and sompolinsky h , 2010 _ phys .",
    "lett . _ * 104 * 188701 krzakala f , mzard m , sausset f , sun y f and zdeborov l , 2012 _ phys .",
    "x _ * 2 * 021005 boufounos p t and baraniuk r g 2008 _ in proceedings of ciss2008 _ 16 lee d , sasaki t , yamada t , akabane k , yamaguchi y and uehara k , 2012 _ in proceedings of ieee vehicular technology conference ( vtc spring ) _ xu y and kabashima y , 2013 _ j. stat .",
    "mech . _ p02041 rangan , s. _ generalized approximate message passing for estimation with random linear mixing _ , information theory proceedings ( isit ) , 2011 ieee international symposium on ; arxiv : 1010.5141v1 [ cs.it ] , 2010 kabashima y and uda s , 2004 _ a bp - based algorithm for performing bayesian inference in large perceptron - type networks _ , s. ben - david , j. case , and maruoka ( eds . ) , alt 2004 , lecture notes in ai , springer , vol.3244 , pp.479 - 493 .",
    "dotsenko v s , 2001 _ introduction to the replica theory of disordered statistical systems _ , ( cambridge : cambridge university press ) nishimori h , 2001 _ statistical physics of spin glasses and information processing _ , ( oxford : oxford university press ) h. nishimori and d. sherrington , _ absence of replica symmetry breaking in a region of the phase diagram of the ising spin glass _ , in  disordered and complex systems \" , ed .",
    "sollich et al , aip conf .",
    "553 , p. 67",
    "( 2001 ) a. montanari , _ estimating random variables from random sparse observations _ , european transactions on telecommunications 19 , 385403 ( 2008 ) donoho d l , maleki a and montanari a , 2009 _ message - passing algorithms for compressed sensing _ , proc .",
    "106 18914 thouless d j , anderson p w and palmer r g , 1977 _ phil . mag .",
    "_ * 35 * 593 shiino m and fukai t , 1992 _ j. phys .",
    "a _ * 25 * l375 seung h s , sompolinsky h and tishby n , 1992 _ phys .",
    "rev . a _ * 45 * 6056 watkin t l h , rau a and biehl m , 1993 _ rev . mod",
    ". phys . _ * 65 * 499 akaike h , 1974 _ ieee trans . on ac _",
    "* 19 * 716 rissanen j , 1978 _ automatica _ * 14 * 465 watanabe s , 2009 algebraic geometry and statistical learning theory ( cambridge university press , cambridge , uk ) mzard m , parisi g and virasoro m a , 1987 _ spin glass theory and beyond _ ( singapore : world scientific ) mzard m and montanari m , 2009 _ information , physics , and computation_(new york : oxford university press ) mackay d j c , 1999 _ ieee trans .",
    "inform . theory _ * 45 * 399 ; mackay d j c and neal r m , 1997 _ elect .",
    "_ 33 457 kabashima y and saad d , 1998 _ europhys .",
    "lett . _ * 44 * 668 de almeida j r l and thouless d j , 1978 _ j. phys . a _ * 11 * 983 shizato t and kabashima y , 2009 _ j. phys .",
    "a _ * 42 * 015005 kabashima y , vehkaper m and chatterjee s , 2012 _ j. stat .",
    "_ p12003 vehkaper m , kabashima y and chatterjee s , 2013 _ analysis of regularized ls reconstruction and random matrix ensembles in compressed sensing _ , arxiv:1312.0256 kabashima y and vehkaper m , 2014 _ signal recovery using expectation consistent approximation for linear observations _ ,",
    "arxiv:1401.5151 oymak s and hassibi b , 2014 _ a case for orthogonal measurements in linear inverse problems _ , preprint ` http://www.its.caltech.edu/  soymak / ohunitary.pdf ` wen c - k and wong k - k , 2014 _ analysis of compressed sensing with spatially - coupled orthogonal matrices _ , arxiv:1402.3215"
  ],
  "abstract_text": [
    "<S> the 1-bit compressed sensing framework enables the recovery of a sparse vector @xmath0 from the sign information of each entry of its linear transformation . discarding the amplitude information </S>",
    "<S> can significantly reduce the amount of data , which is highly beneficial in practical applications . in this paper </S>",
    "<S> , we present a bayesian approach to signal reconstruction for 1-bit compressed sensing , and analyze its typical performance using statistical mechanics . as a basic setup </S>",
    "<S> , we consider the case that the measuring matrix @xmath1 has i.i.d entries , and the measurements @xmath2 are noiseless . </S>",
    "<S> utilizing the replica method , we show that the bayesian approach enables better reconstruction than the @xmath3-norm minimization approach , asymptotically saturating the performance obtained when the non - zero entries positions of the signal are known , for signals whose non - zero entries follow zero mean gaussian distributions . </S>",
    "<S> we also test a message passing algorithm for signal reconstruction on the basis of belief propagation . </S>",
    "<S> the results of numerical experiments are consistent with those of the theoretical analysis . </S>"
  ]
}