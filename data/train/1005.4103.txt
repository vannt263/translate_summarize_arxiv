{
  "article_text": [
    "real - time detection of various categories of objects in images is one of the key tasks in computer vision .",
    "this topic has been extensively studied in the past a few years due to its important applications in surveillance , intelligent video analysis _",
    "etc_. viola and jones proffered the first real - time face detector @xcite . to date , it is still considered one of the state - of - the - art , and their framework is the basis of many incremental work afterwards .",
    "object detection is a highly asymmetric classification problem with the exhaustive scanning - window search being used to locate the target in an image .",
    "only a few are true target objects among the millions of scanned patches .",
    "cascade classifiers have been proposed for efficient detection , which takes the asymmetric structure into consideration . under the assumption of each node of the cascade classifier",
    "makes independent classification errors , the detection rate and false positive rate of the entire cascade are : @xmath0 and @xmath1 , respectively . as pointed out in @xcite , these two equations suggest a _ node learning objective _ : each node should have an extremely high detection rate @xmath2 ( _ e.g. _ , @xmath3 ) and a moderate false positive rate @xmath4 ( _ e.g. _ , @xmath5 ) . with the above values of @xmath6 and @xmath4 ,",
    "assume that the cascade has @xmath7 nodes , then @xmath8 and @xmath9 , which is usually the design goal .",
    "a drawback of standard boosting like adaboost is that it does not take advantage of the cascade classifier .",
    "adaboost only minimizes the overall classification error and does not minimize the number of false negatives . in this sense ,",
    "the features selected are not optimal for the purpose of rejecting negative examples . at the feature selection and classifier training level , viola and jones",
    "leveraged the asymmetry property , to some extend , by replacing adaboost with asymboost @xcite .",
    "asymboost incurs more loss for misclassifying a positive example by simply modifying adaboost s exponential loss .",
    "better detection rates were observed over the standard adaboost .",
    "nevertheless , asymboost addresses the node learning goal _ indirectly _ and still may not be the optimal solution .",
    "_ explicitly studied the node learning goal and they proposed to use linear asymmetric classifier ( lac ) and fisher linear discriminant analysis ( lda ) to adjust the linear coefficients of the selected weak classifiers @xcite .",
    "their experiments indicated that with this post - processing technique , the node learning objective can be better met , which is translated into improved detection rates . in viola and jones framework , boosting is used to select features and at the same time to train a strong classifier .",
    "s work separates these two tasks : they still use adaboost or asymboost to select features ; and at the second step , they build a strong classifier using lac or lda . since there are two steps here , in wu _",
    "_ s work @xcite , the node learning objective is only considered at the second step . at the first step  feature selection  the node learning objective is not explicitly considered .",
    "we conjecture that _ further improvement may be gained if the node learning objective is explicitly taken into account at both steps_. we design new boosting algorithms to implement this idea and verify this conjecture .",
    "our major contributions are as follows .    1 .",
    "we develop new boosting - like algorithms via directly minimizing the objective function of linear asymmetric classifier , which is termed as lacboost ( and fisherboost from fisher lda ) .",
    "both of them can be used to select features that is optimal for achieving the node learning goal in training a cascade classifier .",
    "to our knowledge , this is the first attempt to design such a feature selection method .",
    "2 .   lacboost and fisherboost share similarities with lpboost @xcite in the sense that both use column generation  a technique originally proposed for large - scale linear programming ( lp ) .",
    "typically , the lagrange dual problem is solved at each iteration in column generation .",
    "we instead solve the primal quadratic programming ( qp ) problem , which has a special structure and entropic gradient ( eg ) can be used to solve the problem very efficiently .",
    "compared with general interior - point based qp solvers , eg is much faster .",
    "considering one needs to solve qp problems a few thousand times for training a complete cascade detector , the efficiency improvement is enormous .",
    "compared with training an adaboost based cascade detector , the time needed for lacboost ( or fisherboost ) is comparable .",
    "this is because for both cases , the majority of the time is spent on weak classifier training and bootstrapping .",
    "we apply lacboost and fisherboost to face detection and better performances are observed over the state - of - the - art methods @xcite .",
    "the results confirm our conjecture and show the effectiveness of lacboost and fisherboost .",
    "lacboost can be immediately applied to other asymmetric classification problems .",
    "we also analyze the condition that makes the validity of lac , and show that the multi - exit cascade might be more suitable for applying lac learning of @xcite ( and our lacboost ) rather than viola - jones standard cascade .",
    "besides these , the lacboost / fisherboost algorithm differs from traditional boosting algorithms in that lacboost / fisherboost does not minimize a loss function .",
    "this opens new possibilities for designing new boosting algorithms for special purposes .",
    "we have also extended column generation for optimizing nonlinear optimization problems .",
    "next we review some related work that is closest to ours .",
    "* related work * there is a large body of previous work in object detection @xcite ; of particular relevance to our work is boosting object detection originated from viola and jones framework .",
    "there are three important components that make viola and jones framework tremendously successful : ( 1 ) the cascade classifier that efficiently filters out most negative patches in early nodes ; and also contributes to enable the final classifier to have a very high detection rate ; ( 2 ) adaboost that selects informative features and at the same time trains a strong classifier ; ( 3 ) the use of integral images , which makes the computation of haar features extremely fast .",
    "most of the work later improves one or more of these three components . in terms of the cascade classifier , a few different approaches such as soft cascade @xcite , dynamic cascade @xcite , and multi - exit cascade @xcite .",
    "we have used the multi - exit cascade in this work .",
    "the multi - exit cascade tries to improve the classification performance by using all the selected weak classifiers for each node .",
    "so for the @xmath10-th strong classifier ( node ) , it uses all the weak classifiers in this node as well as those in the previous @xmath11 nodes .",
    "we show that the lac post - processing can enhance the multi - exit cascade .",
    "more importantly , we show that the multi - exit cascade better meets lac s requirement of data being gaussian distributions .",
    "the second research topic is the learning algorithm for constructing a classifier .",
    "_ use fast forward feature selection to accelerate the training procedure @xcite .",
    "they have also proposed lac to learn a better strong classifier @xcite .",
    "pham and cham recently proposed online asymmetric boosting with considerable improvement in training time @xcite . by exploiting the feature statistics ,",
    "they have also designed a fast method to train weak classifiers @xcite .",
    "advocated floatboost to discard some redundant weak classifiers during adaboost s greedy selection procedure @xcite .",
    "liu and shum proposed klboost to select features and train a strong classifier @xcite .",
    "other variants of boosting have been applied to detection .    *",
    "notation * the following notation is used .",
    "a matrix is denoted by a bold upper - case letter ( @xmath12 ) ; a column vector is denoted by a bold lower - case letter ( @xmath13 ) .",
    "the @xmath14th row of @xmath12 is denoted by @xmath15 and the @xmath16-th column @xmath17 .",
    "the identity matrix is @xmath18 and its size should be clear from the context . @xmath19 and",
    "@xmath20 are column vectors of @xmath21 s and @xmath22 s , respectively .",
    "we use @xmath23 to denote component - wise inequalities .",
    "let @xmath24 be the set of training data , where @xmath25 and @xmath26 , @xmath27 .",
    "the training set consists of @xmath28 positive training points and @xmath29 negative ones ; @xmath30 .",
    "let @xmath31 be a weak classifier that projects an input vector @xmath13 into @xmath32 .",
    "here we only consider discrete classifier outputs .",
    "we assume that the set @xmath33 is finite and we have @xmath10 possible weak classifiers .",
    "let the matrix @xmath34 where the @xmath35 entry of @xmath36 is @xmath37 .",
    "@xmath38 is the label predicted by weak classifier @xmath39 on the training datum @xmath40 .",
    "we define a matrix @xmath41 such that its @xmath42 entry is @xmath43 .",
    "before we propose our lacboost and fisherboost , we briefly overview the concept of lac .",
    "_ @xcite have proposed linear asymmetric classification ( lac ) as a post - processing step for training nodes in the cascade framework .",
    "lac is guaranteed to get an optimal solution under the assumption of gaussian data distributions .",
    "suppose that we have a linear classifier @xmath44 , if we want to find a pair of @xmath45 with a very high accuracy on the positive data @xmath46 and a moderate accuracy on the negative @xmath47 , which is expressed as the following problem : @xmath48 where @xmath49 denotes a symmetric distribution with mean @xmath50 and covariance @xmath51 .",
    "if we prescribe @xmath52 to @xmath53 and assume that for any @xmath54 , @xmath55 is gaussian and @xmath56 is symmetric , then can be approximated by @xmath57 is similar to lda s optimization problem @xmath58 can be solved by eigen - decomposition and a close - formed solution can be derived : @xmath59 on the other hand , each node in cascaded boosting classifiers has the following form : @xmath60 we override the symbol @xmath61 here , which denotes the output vector of all weak classifiers over the datum @xmath13 .",
    "we can cast each node as a linear classifier over the feature space constructed by the binary outputs of all weak classifiers . for each node in cascade classifier",
    ", we wish to maximize the detection rate as high as possible , and meanwhile keep the false positive rate to an moderate level ( _ e.g. _ , @xmath62 ) .",
    "that is to say , the problem expresses the node learning goal .",
    "therefore , we can use boosting algorithms ( _ e.g. _ , adaboost ) as feature selection methods , and then use lac to learn a linear classifier over those binary features chosen by boosting .",
    "the advantage is that lac considers the asymmetric node learning explicitly .",
    "however , there is a precondition of lac s validity .",
    "that is , for any @xmath54 , @xmath55 is a gaussian and @xmath56 is symmetric . in the case of boosting classifiers , @xmath55 and @xmath56 can be expressed as the margin of positive data and negative data . empirically",
    "wu _ et al . _",
    "@xcite verified that @xmath63 is gaussian approximately for a cascade face detector .",
    "we discuss this issue in the experiment part in more detail .",
    "in kernel methods , the original data are nonlinearly mapped to a feature space and usually the mapping function @xmath64 is not explicitly available .",
    "it works through the inner product of @xmath65 . in boosting @xcite ,",
    "the mapping function can be seen as explicitly known through : @xmath66 .",
    "$ ] let us consider the fisher lda case first because the solution to lda will generalize to lac straightforwardly , by looking at the similarity between and .",
    "fisher lda maximizes the between - class variance and minimizes the within - class variance . in the binary - class case , we can equivalently rewrite into @xmath67 where @xmath68 and @xmath69 are the between - class and within - class scatter matrices ; @xmath70 and @xmath71 are the projected centers of the two classes .",
    "the above problem can be equivalently reformulated as @xmath72 for some certain constant @xmath73 and under the assumption that @xmath74 .",
    "now in the feature space , our data are @xmath75 , @xmath76 .",
    "we have @xmath77 where @xmath78 is the @xmath16-th row of @xmath79 . @xmath80 here the @xmath16-th entry of @xmath81 is defined as @xmath82 if @xmath83 , otherwise @xmath84 .",
    "similarly @xmath85 if @xmath86 , otherwise @xmath87 .",
    "we also define @xmath88 . for ease of exposition ,",
    "we order the training data according to their labels .",
    "so the vector @xmath89 : @xmath90^\\t ,                    \\label{eq : e}\\ ] ] and the first @xmath91 components of @xmath92 correspond to the positive training data and the remaining ones correspond to the @xmath93 negative data .",
    "so we have @xmath94 , @xmath95 with @xmath96 the covariance matrices . by noticing that @xmath97 we can easily rewrite the original problem into : @xmath98 here @xmath99 is a block matrix with @xmath100 and @xmath101 is similarly defined by replacing @xmath91 with @xmath29 in @xmath102 . also note that we have introduced a constant @xmath103 before the quadratic term for convenience .",
    "the normalization constraint @xmath104 removes the scale ambiguity of @xmath105 .",
    "otherwise the problem is ill - posed .    in the case of lac ,",
    "the covariance matrix of the negative data is not involved , which corresponds to the matrix @xmath101 is zero .",
    "so we can simply set @xmath106 and becomes the optimization problem of lac .    at this stage , it remains unclear about how to solve the problem because we do not know all the weak classifiers .",
    "the number of possible weak classifiers could be infinite  the dimension of the optimization variable @xmath105 is infinite .",
    "so is a semi - infinite quadratic program ( siqp ) .",
    "we show how column generation can be used to solve this problem . to make column generation applicable , we need to derive a specific lagrange dual of the primal problem .    * the lagrange dual problem",
    "* we now derive the lagrange dual of the quadratic problem .",
    "although we are only interested in the variable @xmath105 , we need to keep the auxiliary variable @xmath107 in order to obtain a meaningful dual problem .",
    "the lagrangian of is @xmath108 with @xmath109 .",
    "@xmath110 gives the following lagrange dual : @xmath111 in our case , @xmath112 is rank - deficient and its inverse does not exist ( for both lda and lac ) .",
    "we can simply regularize @xmath112 with @xmath113 with @xmath114 a very small constant .",
    "one of the kkt optimality conditions between the dual and primal is @xmath115 , which can be used to establish the connection between the dual optimum and the primal optimum .",
    "this is obtained by the fact that the gradient of @xmath116 w.r.t .",
    "@xmath92 must vanish at the optimum , @xmath117 , @xmath118 .",
    "problem can be viewed as a regularized lpboost problem .",
    "compared with the hard - margin lpboost @xcite , the only difference is the regularization term in the cost function . the duality gap between the primal and the dual is zero . in other words , the solutions of and coincide . instead of solving directly , one calculates the most violated constraint in iteratively for the current solution and adds this constraint to the optimization problem . in theory , any column that violates dual feasibility can be added . to speed up the convergence ,",
    "we add the most violated constraint by solving the following problem : @xmath119 this is exactly the same as the one that standard adaboost and lpboost use for producing the best weak classifier .",
    "that is to say , to find the weak classifier that has minimum weighted training error .",
    "we summarize the lacboost / fisherboost algorithm in algorithm  [ alg : qpcg ] . by simply changing @xmath101 , algorithm  [ alg : qpcg ]",
    "can be used to train either lacboost or fisherboost .",
    "note that to obtain an actual strong classifier , one may need to include an offset @xmath120 , _ i.e. _ the final classifier is @xmath121 because from the cost function of our algorithm , we can see that the cost function itself does not minimize any classification error .",
    "it only finds a projection direction in which the data can be maximally separated .",
    "a simple line search can find an optimal @xmath120 .",
    "moreover , when training a cascade , we need to tune this offset anyway as shown in .",
    "the convergence of algorithm  [ alg : qpcg ] is guaranteed by general column generation or cutting - plane algorithms , which is easy to establish .",
    "when a new @xmath122 that violates dual feasibility is added , the new optimal value of the dual problem ( maximization ) would decrease .",
    "accordingly , the optimal value of its primal problem decreases too because they have the same optimal value due to zero duality gap .",
    "moreover the primal cost function is convex , therefore in the end it converges to the global minimum .",
    "* initialization * : @xmath123 ; @xmath124 ; and @xmath125 , @xmath126@xmath127@xmath128 .",
    "[ alg : qpcg ]    at each iteration of column generation , in theory , we can solve either the dual or the primal problem .",
    "however , in practice , it could be much faster to solve the primal problem because ( i ) generally , the primal problem has a smaller size , hence faster to solve .",
    "the number of variables of is @xmath129 at each iteration , while the number of variables is the number of iterations for the primal problem .",
    "for example , in viola - jones face detection framework , the number of training data @xmath130 and @xmath131 .",
    "in other words , the primal problem has at most @xmath132 variables in this case ; ( ii ) the dual problem is a standard qp problem .",
    "it has no special structure to exploit .",
    "as we will show , the primal problem belongs to a special class of problems and can be efficiently solved using entropic / exponentiated gradient descent ( eg ) @xcite .",
    "a fast qp solver is extremely important for training a object detector because we need to the solve a few thousand qp problems .",
    "we can recover both of the dual variables @xmath133 easily from the primal variable @xmath134 : @xmath135 the second equation is obtained by the fact that in the dual problem s constraints , at optimum , there must exist at least one @xmath136 such that the equality holds .",
    "that is to say , @xmath137 is the largest _ edge _ over all weak classifiers .",
    "we give a brief introduction to the eg algorithm before we proceed .",
    "let us first define the unit simplex @xmath138 .",
    "eg efficiently solves the convex optimization problem @xmath139 under the assumption that the objective function @xmath140 is a convex lipschitz continuous function with lipschitz constant @xmath141 w.r.t . a fixed given norm @xmath142 .",
    "the mathematical definition of @xmath141 is that @xmath143 holds for any @xmath144 in the domain of @xmath145 .",
    "the eg algorithm is very simple :    1 .",
    "initialize with @xmath146 ; 2 .",
    "generate the sequence @xmath147 , @xmath148 with : @xmath149   }               { \\sum_{j=1}^n   { \\mathbf w}^{k-1}_j \\exp [ - \\tau_k f'_j ( { \\mathbf w}^{k-1 } ) ] } .\\ ] ] here @xmath150 is the step - size .",
    "@xmath151 ^\\t $ ] is the gradient of @xmath140 ; 3 .",
    "stop if some stopping criteria are met .",
    "the learning step - size can be determined by @xmath152 following @xcite . in @xcite , the authors have used a simpler strategy to set the learning rate .",
    "eg is a very useful tool for solving large - scale convex minimization problems over the unit simplex .",
    "compared with standard qp solvers like mosek @xcite , eg is much faster .",
    "eg makes it possible to train a detector using almost the same amount of time as using standard adaboost as the majority of time is spent on weak classifier training and bootstrapping .    in the case",
    "that @xmath153 , @xmath154 similarly , for lda , @xmath155 when @xmath156 .",
    "hence , @xmath157 therefore , the problems involved can be simplified when @xmath153 and @xmath156 hold .",
    "the primal problem equals @xmath158 we can efficiently solve using the eg method . in eg",
    "there is an important parameter @xmath141 , which is used to determine the step - size .",
    "@xmath141 can be determined by the @xmath159-norm of @xmath160 . in our case",
    "@xmath161 is a linear function , which is trivial to compute .",
    "the convergence of eg is guaranteed ; see @xcite for details .    in summary ,",
    "when using eg to solve the primal problem , line @xmath162 of algorithm  [ alg : qpcg ] is :     @xmath163 _ solve the primal problem using eg , and update the dual variables @xmath164 with , and @xmath165 with . _",
    "d artificial data ( positive data represented by @xmath166 s and negative data by @xmath167 s ) . weak classifiers are decision stumps . in this case , fisherboost intends to correctly classify more positive data in this case .",
    ", title=\"fig : \" ] d artificial data ( positive data represented by @xmath166 s and negative data by @xmath167 s ) .",
    "weak classifiers are decision stumps . in this case , fisherboost intends to correctly classify more positive data in this case .",
    ", title=\"fig : \" ]    first , let us show a simple example on a synthetic dataset ( more negative data than positive data ) to illustrate the difference between fisherboost and adaboost .",
    "[ fig : toy ] demonstrates the subtle difference of the classification boundaries obtained by adaboost and fisherboost .",
    "we can see that fisherboost seems to focus more on correctly classifying positive data points .",
    "this might be due to the fact that adaboost only optimizes the overall classification accuracy .",
    "this finding is consistent with the result in @xcite .    , @xmath168 , @xmath169 .",
    "the @xmath170 nodes contains @xmath171 , @xmath172 , @xmath173 weak classifiers respectively .",
    "curves close to a straight line mean close to a gaussian .",
    ", title=\"fig : \" ] , @xmath168 , @xmath169 .",
    "the @xmath170 nodes contains @xmath171 , @xmath172 , @xmath173 weak classifiers respectively .",
    "curves close to a straight line mean close to a gaussian .",
    ", title=\"fig : \" ] , @xmath168 , @xmath169 .",
    "the @xmath170 nodes contains @xmath171 , @xmath172 , @xmath173 weak classifiers respectively .",
    "curves close to a straight line mean close to a gaussian .",
    ", title=\"fig : \" ]    * face detection * in this section , we compare our algorithm with other state - of - art face detectors .",
    "we first show some results about the validity of lac ( or fisher lda ) post - processing for improving node learning in object detection .",
    "[ fig : normplot ] illustrates the normal probability plot of margins of positive training data , for the first three nodes in the multi - exit with lac cascade .",
    "clearly , the larger number of weak classifiers being used , the more closely the margin follows gaussian distribution . in other words , lac may achieve a better performance if a larger number of weak classifiers are used .",
    "the performance could be poor with too fewer weak classifiers .",
    "the same statement applies to fisher lda , and lacboost , fisherboost , too .",
    "therefore , we do not apply lac / lda in the first eight nodes because the margin distribution could be far from a gaussian distribution . because the late nodes of a multi - exit cascade contain more weak classifiers , we conjecture that the multi - exit cascade might meet the gaussianity requirement better .",
    "we have compared multi - exit cascades with lda / lac post - processing against standard cascades with lda / lac post - processing in @xcite and slightly improved performances were obtained .",
    "six methods are evaluated with the multi - exit cascade framework @xcite , which are adaboost with lac post - processing , or lda post - processing , asymboost with lac or lda post - processing @xcite , and our fisherboost , lacboost .",
    "we have also implemented viola - jones face detector as the baseline @xcite . as in @xcite ,",
    "five basic types of haar - like features are calculated , which makes up of a @xmath174 dimensional over - complete feature set on an image of @xmath175 pixels . to speed up the weak classifier training , as in @xcite",
    ", we uniformly sample @xmath176 of features for training weak classifiers ( decision stumps ) .",
    "the training data are @xmath177 mirrored @xmath175 face images ( @xmath178 for training and @xmath179 for validation ) and @xmath180 large background images , which are the same as in @xcite .",
    "multi - exit cascades with @xmath181 exits and @xmath182 weak classifiers are trained with various methods .",
    "for fair comparisons , we have used the same cascade structure and same number of weak classifiers for all the compared learning methods .",
    "the indexes of exits are pre - set to simplify the training procedure . for our fisherboost and lacboost",
    ", we have an important parameter @xmath183 , which is chosen from @xmath184 @xmath185 @xmath186 .",
    "we have not carefully tuned this parameter using cross - validation .",
    "instead , we train a @xmath187-node cascade for each candidate @xmath188 , and choose the one with the best _ training _ accuracy .- node cascade and choose the best @xmath73 on cross - validation data may give better detection rates . ] at each exit , negative examples misclassified by current cascade are discarded , and new negative examples are bootstrapped from the background images pool .",
    "totally , billions of negative examples are extracted from the pool .",
    "the positive training data and validation data keep unchanged during the training process .",
    "our experiments are performed on a workstation with @xmath189 intel xeon e@xmath190 cpus and @xmath191 gb ram .",
    "it takes about @xmath169 hours to train the multi - exit cascade with adaboost or asymboost . for fisherboost and lacboost , it takes less than @xmath192 hours to train a complete multi - exit cascade .",
    "in other words , our eg algorithm takes less than @xmath193 hour for solving the primal qp problem ( we need to solve a qp at each iteration ) .",
    "a rough estimation of the computational complexity is as follows .",
    "suppose that the number of training examples is @xmath194 , number of weak classifiers is @xmath10 , at each iteration of the cascade training , the complexity for solving the primal qp using eg is @xmath195 with @xmath196 the iterations needed for eq s convergence .",
    "the complexity for training the weak classifier is @xmath197 with @xmath198 the number of all haar - feature patterns . in our experiment , @xmath199 , @xmath200 , @xmath201 , @xmath202 .",
    "so the majority of the training computation is on the weak classifier training .",
    "we have also experimentally observed the speedup of eg against standard qp solvers .",
    "we solve the primal qp defined by using eg and mosek @xcite .",
    "the qp s size is @xmath203 variables . with the same accuracy tolerance",
    "( mosek s primal - dual gap is set to @xmath204 and eg s convergence tolerance is also set to @xmath204 ) , mosek takes @xmath205 seconds and eg is @xmath206 seconds .",
    "so eg is about @xmath207 times faster .",
    "moreover , at iteration @xmath208 of training the cascade , eg can take advantage of the last iteration s solution by starting eg from a small perturbation of the previous solution .",
    "such a warm - start gains a @xmath162 to @xmath209 speedup in our experiment , while there is no off - the - shelf warm - start qp solvers available yet .",
    "we evaluate the detection performance on the mit+cmu frontal face test set .",
    "two performance metrics are used here : each node and the entire cascade .",
    "the node metric is how well the classifiers meet the node learning objective .",
    "the node metric provides useful information about the capability of each method to achieve the node learning goal .",
    "the cascade metric uses the receiver operating characteristic ( roc ) to compare the entire cascade s peformance .",
    "multiple issues have impacts on the cascade s performance : classifiers , the cascade structure , bootstrapping _",
    "etc_.    we show the node comparison results in fig . [",
    "fig : node1 ] .",
    "the node performances between fisherboost and lacboost are very similar . from fig .",
    "[ fig : node1 ] , as reported in @xcite , lda or lac post - processing can considerably reduce the false negative rates . as expected",
    ", our proposed fisherboost and lacboost can further reduce the false negative rates significantly .",
    "this verifies the advantage of selecting features with the node learning goal being considered .    from the roc curves in fig .",
    "[ fig : roc1 ] , we can see that fisherboost and lacboost outperform all the other methods .",
    "in contrast to the results of the detection rate for each node , lacboost is slightly worse than fisherboost in some cases .",
    "that might be due to that many factors have impacts on the final result of detection .",
    "lac makes the assumption of gaussianity and symmetry data distributions , which may not hold well in the early nodes .",
    "this could explain why lacboost does not always perform the best .",
    "et al . _ have observed the same phenomenon that lac post - processing does not outperform lda post - processing in a few cases .",
    "however , we believe that for harder detection tasks , the benefits of lacboost would be more impressive .    the error reduction results of fisherboost and lacboost in fig .  [ fig : roc1 ] are not as great as those in fig .",
    "[ fig : node1 ] . this might be explained by the fact that the cascade and negative data bootstrapping remove of the error reducing effects , to some extend .",
    "we have also compared our methods with the boosted greedy sparse lda ( bgslda ) in @xcite , which is considered one of the state - of - the - art .",
    "we provide the roc curves in the supplementary package .",
    "both of our methods outperform bgslda with adaboost / asymboost by about @xmath210 in the detection rate .",
    "note that bgslda uses the standard cascade .",
    "so besides the benefits of our fisherboost / lacboost , the multi - exit cascade also brings effects .",
    "by explicitly taking into account the node learning goal in cascade classifiers , we have designed new boosting algorithms for more effective object detection .",
    "experiments validate the superiority of our fisherboost and lacboost .",
    "we have also proposed to use entropic gradient to efficiently implement fisherboost and lacboost .",
    "the proposed algorithms are easy to implement and can be applied other asymmetric classification tasks in computer vision .",
    "we are also trying to design new asymmetric boosting algorithms by looking at those asymmetric kernel classification methods ."
  ],
  "abstract_text": [
    "<S> object detection is one of the key tasks in computer vision . </S>",
    "<S> the cascade framework of viola and jones has become the _ de facto _ standard . a classifier in each node of the cascade is required to achieve extremely high detection rates , instead of low overall classification error . </S>",
    "<S> although there are a few reported methods addressing this requirement in the context of object detection , there is no a principled feature selection method that explicitly takes into account this asymmetric node learning objective . </S>",
    "<S> we provide such a boosting algorithm in this work . </S>",
    "<S> it is inspired by the linear asymmetric classifier ( lac ) of @xcite in that our boosting algorithm optimizes a similar cost function . the new totally - corrective boosting algorithm is implemented by the column generation technique in convex optimization . </S>",
    "<S> experimental results on face detection suggest that our proposed boosting algorithms can improve the state - of - the - art methods in detection performance .    </S>",
    "<S> = 1 </S>"
  ]
}