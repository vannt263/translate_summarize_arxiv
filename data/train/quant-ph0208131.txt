{
  "article_text": [
    "a theorem of shannon  @xcite basic to all information theory describes the optimum compression of a discrete memoryless source , showing that the minimum achievable rate is the entropy of the source distribution .",
    "the situation is the following :    let @xmath0 be a probability distribution on the finite set @xmath1 .",
    "we call @xmath2 an _ @xmath3code _ for the _ discrete memoryless source @xmath0 _ , if @xmath4 are stochastic maps , with a finite set @xmath5 , such that @xmath6 where @xmath7 denoting the minimal @xmath8 such that an @xmath3 code exists , by @xmath9 , shannon  @xcite shows that for @xmath10 @xmath11 with the entropy @xmath12 of the distribution .    motivated by the work  @xcite , and by a construction in  @xcite ( in footnote 4 ) , we study here the following modification of this problem :    to each @xmath13 is associated a probability distribution @xmath14 on the finite set @xmath15 ( thus @xmath16 is a stochastic map , or channel , form @xmath1 to @xmath15 ) .",
    "an @xmath3code is now a pair @xmath2 of stochastic maps @xmath17 ( compare with eq .",
    "( [ eq : det : code ] ) ) , and instead of condition ( [ eq : det : condition ] ) we impose @xmath18 where @xmath19 is the @xmath20norm on function on @xmath21 : @xmath22 .",
    "note that for two probability distributions @xmath0 and @xmath23 , @xmath24 equals their _ total variational distance _",
    "@xmath25 of the two .",
    "we define @xmath9 to be the minimal @xmath8 of an @xmath3code .",
    "note that for @xmath26 , and @xmath14 the point ",
    "mass @xmath27 in @xmath28 , the new notion of @xmath3code coincides with the previous one . notice further , that we allow probabilistic choices in the encoding and decoding . while it is easy to see that this freedom does not help in shannon s problem , it is crucial for the more general form , that we will study in this paper .",
    "the basic problem of course is to find the optimum rate @xmath29 of compression ( if the limit exists ; otherwise @xmath30 is to be considered ) , and especially the behaviour of this function at @xmath31 .    for the case",
    "@xmath32 , i.e. perfect restitution of the distributions @xmath14 , these definitions in principle make sense , but we do nt expect a neat theory to emerge .",
    "instead we define @xmath33 the minimal entropy of the distribution on @xmath5 induced by the encoder @xmath34 ( with the idea that blocks of these @xmath35blocks we may data compress to this rate ) .",
    "obviously @xmath36 , so the limit @xmath37 exists , and is equal to the infimum of the sequence . to evaluate this quantity is another problem we would like to solve .",
    "the structure of this paper is as follows : first we find lower bounds ( section  [ sec : lower ] ) , then discuss upper bounds , preferrably by constructing codes : in section  [ sec : cr : trick ] we show how the lower bound is approached by using the additional resource of common randomness , in section  [ sec : local : fid ] we prove achievability of it under a letterwise fidelity criterion as a consequence of this result , section  [ sec : howards ] presents a constructions to upper bound @xmath38 and @xmath39 . in section  [ sec : applications ] applications of the results and conjectures are presented : first , we make it plausible that the distillation procedure of  @xcite is asymptotically reversible , second we show that shannon s coding theorem allows an `` inverse '' ( at least in situations where unlimited common randomness is around ) , third we give a simple proof that feedback does not increase the rate of a discrete memoryless channel , and fourth demonstrate , how shannon s rate  distortion theorem follows as a corollary .",
    "the compression result ( with or without common randomness ) thus reveals a great unifying power in classical information theory .",
    "finally , in section  [ sec : quantum ] we discuss extensions of our results to the case of a source of mixed quantum states : the present discussion fits into this models as probability distributions are just commuting mixed state density operators .",
    "let us mention here the previous work on the problem : the major initiating works are  @xcite and  @xcite .",
    "the latter introduced the distinction between blind and visible coding , and between the block and letterwise fidelity criterion .",
    "in contrast to the pure state case the four possible combinations of these conditions seem to lead to rather different answers . the case of blind coding with either the letter or blockwise fidelity criterion was solved recently by koashi and imoto  @xcite . otherwise in this paper",
    ", we will only address the visible case .",
    "an attempt on the letterwise fidelity case with either blind or visible encoding was made in  @xcite .",
    "however , an examination of the approach of this work shows that it does not fit into any of the the classes of fidelity criteria proposed by  @xcite : for a code @xmath2 one could either apply the _ global _ criterion , which is essentially our eq .",
    "( [ eq : condition ] ) , that is definitely not what is considered in  @xcite , there being employed rate distortion theory .",
    "or one could impose that the output @xmath40 is good on the average letterwise ( the _ local _ criterion of  @xcite ) : @xmath41 \\leq \\lambda,\\ ] ] where @xmath42 denotes the marginal distribution of @xmath43 on the @xmath44 factor in @xmath21 , and @xmath45 is any distance measure on probability distributions ( that we require only to be convex in the second variable ) . for @xmath46",
    "this is implied by eq .",
    "( [ eq : condition ] ) .",
    "this , too , is not met in  @xcite , as there @xmath34 and @xmath47 are constructed as deterministic maps , while to satisfy eq .",
    "( [ eq : l : condition ] ) one needs at least a small amount of randomness .    to achieve this",
    "one could base the fidelity condition on looking at individual letter positions of _ source and output simultaneously _ : @xmath48\\",
    "! \\leq \\lambda.\\ ] ] condition  ( [ eq :",
    "l : condition ] ) being weaker than  ( [ eq : condition ] ) , this one is still weaker .",
    "however , this , too , does not coincide with the criterion of  @xcite : denoting by @xmath49 the joint distribution of @xmath28 and @xmath50 according to @xmath0 and @xmath16 , i.e. @xmath51 , one considers @xmath52 ( this is implied by eq .  ( 1 ) of  @xcite for @xmath53 , which in turn is implied by eq .",
    "( [ eq : ll : condition ] ) for @xmath54 ) .",
    "it is not at all clear how to connect this with any of the above : eq .",
    "( [ eq : ks : condition ] ) is about the _ empirical joint distribution _ of letters in @xmath55 and @xmath43 ( assume for simplicity , as indeed the authors of  @xcite do , that @xmath34 and @xmath47 are deterministic ) , that is about a distribution created by selecting a position @xmath56 randomly , while eqs .",
    "( [ eq : condition ] ) to  ( [ eq : ll : condition ] ) are about distributions created either by the coding process alone or in conjunction with the source .",
    "our view is confirmed in an independent recent analysis of  @xcite by soljanin  @xcite , to the same effect .",
    "an interesting new twist was added when in  @xcite ( and later in a more extended way in  @xcite and the recent  @xcite ) the use of unlimited common randomness between the sender and receiver was allowed in the visible coding model with blockwise fidelity criterion . as already mentioned , we reproduce this result here in detail , with special attention to the resource of common randomness : we present a protocol for which we prove that it has minimum common randomness consumption in the class of protocols which even simulate full passive feedback of the received signal to the sender .",
    "let the random variable @xmath57 be distributed according to @xmath58",
    ". then we can define @xmath59 by @xmath60 by ( [ eq : condition ] ) we have the markov chain @xmath61 using data processing inequality as follows : @xmath62 with @xmath63 for @xmath31 . to be precise",
    ", one may choose ( for @xmath64 ) @xmath65 employing the following well known result with eq .",
    "( [ eq : condition ] ) .",
    "[ lemma : h : cont ] let @xmath0 and @xmath23 be probability distributions on a set with finite cardinality @xmath66 , such that @xmath67 . then @xmath68    see e.g.  @xcite .",
    "thus we arrive at    [ satz : lower ] for any @xmath35 and @xmath69 : @xmath70 where @xmath71 is the mutual information of the channel @xmath16 between the input distribution @xmath0 and the output distribution @xmath72 .",
    "@xmath73    by using slightly stronger estimates , we even get    [ satz : strong : lower ] for every @xmath10 @xmath74    let @xmath2 be an optimal @xmath3code . from eq .",
    "( [ eq : condition ] ) we find ( by a markov inequality argument ) that @xmath75 denote the intersection of this set with the typical sequences @xmath76 ( see eq .",
    "( [ eq : typical ] ) below ) by @xmath77 , with @xmath78",
    ". then @xmath79 and there exists an @xmath80transmission code @xmath81 for the channel @xmath82 with @xmath83 , see  @xcite ( the case of a classical  quantum channel @xmath16 was done in  @xcite ) . by construction",
    "this is a @xmath84code for the channel @xmath85 .",
    "we want now view @xmath34 as belonging to the message encoder , and @xmath47 as belonging to the message decoder , the resulting code being one for the identical channel on @xmath5 .",
    "let us denote the concatenation of the map @xmath47 with the channel decoder by @xmath86 .",
    "on the other hand , we may replace @xmath34 by a deterministic map @xmath87 , because randomization at the encoder never decreases error probabilities : @xmath88 still is an @xmath84code .",
    "it is now obvious that @xmath89 for every @xmath90 , hence @xmath91 and we are done .",
    "it might be a bit daring to formulate conjectures at this point , so we content ourselves with posing the following questions :    [ quest : main ] is it true that for all @xmath10 @xmath92    in fact , we would like to go present a slightly stronger statement :    _ question  [ quest : main] _ : for every @xmath10 , @xmath93 , @xmath94 , and large enough @xmath35 does there exists a @xmath3code with @xmath95 and with the additional property that @xmath96    here @xmath76 is the set of _ typical sequences _ : @xmath97 where @xmath98 counts the number of occurences of @xmath28 in @xmath55 , and @xmath99 .",
    "observe that by chebyshev s inequality @xmath100 in fact , by employing the chernoff bound we even obtain @xmath101 with these bounds it is easily seen that a positive answer to the latter question implies the same to the former .",
    "but also conversely , it is not difficult to show that a `` yes '' to question  [ quest : main ] implies a `` yes '' to question  [ quest : main].",
    "the following construction is a generalization and refinement of the one by bennett et  al .",
    "@xcite ( footnote 4 ) , found independently by dr , vidal , and cirac  @xcite . the idea there is to use common randomness between the sender and the receiver of the encoded messages .",
    "formally this means that @xmath34 and @xmath47 also depend on a common random variable @xmath102 , uniformly distributed and independent of all others .",
    "note that this has a nice expression when viewing @xmath34 and @xmath47 as map valued random variables : here we allow dependence ( via @xmath102 ) between @xmath34 and @xmath47 , while in the initial definition , eq .  ( [ eq : code ] ) , @xmath34 and @xmath47 are independent ( as random variables ) .",
    "it seems that the power of allowing the use of common randomness can be understood from this point of view : it is a `` convexification '' of the theory with deterministic or independent encoders and decoders .",
    "it is easy to see that the lower bound of theorem  [ satz : lower ] still applies here .",
    "we only have to modify the derivation a little bit : @xmath103 with a slight variant @xmath104 of @xmath105 .",
    "we shall apply an explicit large deviation estimate for sampling probability distributions from  @xcite ( extended to density operators in  @xcite ) , which we state separately without proof :    [ lemma : large : deviation ] let @xmath106 be independent identically distributed ( i.i.d . )",
    "random variables with values in the function algebra on the finite set @xmath107 , which are bounded between @xmath108 and @xmath109 , the constant function with value @xmath110 .",
    "assume that the average @xmath111 .",
    "then for @xmath112 @xmath113\\right\\ }                   \\leq 2|{{\\mathcal{k}}}|\\exp\\left(-m\\frac{\\eta^2 s}{2\\ln 2}\\right)\\!,\\ ] ] where @xmath114=[(1-\\eta)\\sigma;(1+\\eta)\\sigma]$ ] is an interval in the value  wise order of functions : @xmath115=\\{x:\\forall k\\ a(k)\\leq x(k)\\leq b(k)\\}$ ] .",
    "@xmath73    before we prove our main theorem , we need three lemmas on exact types and conditional types .",
    "the first is a simple yet crucial observation :    [ lemma : types : blurb ] let @xmath16 be a channel from @xmath1 to @xmath15 , @xmath0 a p.d",
    ". on @xmath1 , @xmath116 the induced distribution on @xmath15 and @xmath117 the transpose channel from @xmath15 to @xmath1 .",
    "let @xmath118 , @xmath119 be exact @xmath35types of @xmath1 , @xmath15 , respectively that are marginals of a joint exact @xmath35type @xmath120 of @xmath121 .",
    "consider the uniform distribution @xmath122 on @xmath123 on @xmath123 , which has the property @xmath124 and the channel from @xmath123 to @xmath125 , @xmath126 where @xmath127 is the set of _ conditional exact typical sequences _ of @xmath55 .",
    "then the induced distribution @xmath128 on @xmath125 is the uniform distribution , i.e. @xmath129 and the transpose channel to @xmath130 is indeed @xmath131 , defined by @xmath132 with @xmath133 .",
    "straightforward .",
    "[ lemma : cardinalities ] there is an absolute constant @xmath134 such that for all distributions @xmath0 on @xmath1 , @xmath135 , channels @xmath136 and @xmath94 @xmath137 for @xmath138 , consider a joint @xmath35type @xmath120 on @xmath121 with marginals @xmath118 on @xmath1 and @xmath119 of @xmath15 . then , introducing the channel @xmath139 with @xmath140 : @xmath141    see  @xcite .",
    "the third contains the central insight for our construction :    [ lemma : covering ] with the hypotheses and notation of lemma  [ lemma : types : blurb ] there exist families @xmath142 , @xmath143 , from @xmath144 such that for all @xmath102 @xmath145,\\tag{i${}_\\nu$}\\ ] ] and @xmath146,\\tag{ii}\\ ] ] for all @xmath147 and @xmath148 that satisfy @xmath149    introduce i.i.d .",
    "random variables , distributed on @xmath125 according to @xmath150 ( i.e. uniformly ) .",
    "then for all @xmath151 : @xmath152 hence lemma  [ lemma : large : deviation ] applies and we find @xmath153 by choosing @xmath148 and @xmath147 according to the lemma we enforce that the sum of these probabilities is less than @xmath110 , hence there are actual values of the @xmath154 such that all ( i@xmath155 ) and ( ii ) are satisfied .",
    "with this we are ready to prove :    [ satz : sim : feedback : channel ] there exists an @xmath3code @xmath156 with @xmath157 and common randomness consumption @xmath158 in fact , not only the condition ( [ eq : condition ] ) is satisfied but the even stronger @xmath159    suppose @xmath55 is seen at the source , and that its type is @xmath118 .",
    "for each joint @xmath35type @xmath120 of @xmath121 we assume that families @xmath160 as described in lemma  [ lemma : covering ] are fixed throughout .",
    "then the protocol the sender follows is :    1 .",
    "choose a joint type @xmath120 on @xmath121 with probability @xmath161 and send it .",
    "note that @xmath120 can be written @xmath140 , with the marginal @xmath118 on @xmath1 and a channel @xmath162 .",
    "2 .   if @xmath118 is not typical or @xmath120 is not jointly typical then terminate . 3 .   use the common randomness to choose @xmath102 uniformly .",
    "4 .   choose @xmath163 according to @xmath164 and send it .",
    "the receiver chooses @xmath165 , using the common randomness sample @xmath102 .",
    "let us first check that this procedure works correctly :    for typical @xmath55 we can calulate the distribution of @xmath166 conditional on the event that their joint type is @xmath120 : this is then a distribution on @xmath167 , and we assume @xmath120 to be typical .",
    "@xmath168 with the `` big ",
    "notation : @xmath169 signifies any function whose modulus is bounded by @xmath170 . here we have used the definition of the protocol , then lemma  [ lemma : types : blurb ] ( for the definition of @xmath171 and the fact that @xmath172 does not depend on @xmath173 ) , then lemma  [ lemma : covering ] .",
    "so , the induced distribution is , up to a factor between @xmath174 and @xmath175 , equal to the correct output distribution @xmath176 . now averaging over the typical @xmath120 gives eq .",
    "( [ eq : individual : condition ] ) .",
    "what is the communication cost ?",
    "sending @xmath120 is asymptotically for free , as the number of joint types is bounded by the polynomial @xmath177 .",
    "sending @xmath163 costs @xmath178 bits , with @xmath147 bounded according to lemma  [ lemma : covering ] .",
    "that is , @xmath179 on the other hand @xmath180 and we are done .",
    "[ rem : chernoff : better ] in the above statement of theorem  [ satz : sim : feedback : channel ] we assumed @xmath181 to be a constant , absorbed into the `` @xmath182 '' in the code length estimate . using the chernoff estimate  ( [ eq : typical : prob : exp ] ) on the probabilities of typical sets in the above proof in fact shows the existence of an @xmath3code satisfying  ( [ eq : individual : condition ] ) @xmath183    in the line of  @xcite , the interpretation of this result is that investing common randomness at rate @xmath184 , one can simultate the noisy channel @xmath16 by a noiseless one of rate @xmath185 , when sending only @xmath0typical words .",
    "considering the construction again , we observe that in fact not only it provides a simulation of the channel @xmath16 , but additionally of the _ noiseless passive feedback_. simply because the sender can read off from his random choices the @xmath166 obtained by the receiver , too .",
    "this observation is the key to show that our above construction is optimal under the hypothesis that the channel _ with noiseless passive feedback _ is simulated : in fact , since both sender and receiver can observe the very output sequence @xmath166 of the channel , which has entropy @xmath186 , they are able to generate common randomness at this rate . since communication was only at rate @xmath185 , the difference must by invested in prepared common randomness : otherwise we would get more of it out of the system than we could have possibly invested .",
    "formally this insight is captured by the following result :    [ satz : feedback : channel : opt ] if the decoder of a @xmath3code @xmath2 with common randomness consumption @xmath187 $ ] ( with distribution @xmath188 ) depends deterministically on @xmath102 and @xmath90 ( which is precisely the condition that the encoder can recover the receiver s output ) then @xmath189    for the first inequality introduce the channels @xmath190 , and their induced distributions @xmath191 on @xmath21 and transpose channels @xmath192 with respect to @xmath58 , i.e. @xmath193 then we can rewrite eq .",
    "( [ eq : condition ] ) as @xmath194 this inequality oviously remains valid if we restrict the sum to @xmath195 and replace @xmath196 and @xmath192 by their restrictions to @xmath197 : @xmath198 and @xmath199 , respectively .",
    "on the other hand , choosing @xmath200 , we have @xmath201 which yield @xmath202 hence there exists at least one @xmath102 such that @xmath203 note that , as functions on @xmath204 , @xmath205 so , when we introduce the support @xmath206 of the left hand side , we arrive at @xmath207 from which our claim follows by a standard trick  @xcite : let @xmath208 , with @xmath209 .",
    "then @xmath210 and using the fact that @xmath211 this implies @xmath212 now only note that ( since @xmath213 is deterministic ) @xmath214 and by @xmath215 we are done",
    ".    now for the second inequality : from the definition we get , by summing over @xmath55 , @xmath216 because the @xmath213 are all deterministic , the distributions @xmath217 are all supported on sets of cardinality @xmath8 .",
    "hence the support @xmath206 of @xmath218 can be estimated @xmath219 .",
    "on the other hand , we deduce @xmath220 which , by the same standard trick  @xcite as before , yields our estimate : with @xmath221 , the set @xmath222 satisfies @xmath223 but since for all @xmath224 @xmath225 we can conlude @xmath226    collecting these results we can state    [ cor : feedback : channel : opt ] for any simulation of the channel @xmath16 together with its noiseless passive feedback with error @xmath227 , at rate @xmath118 and common randomness consumption rate @xmath228 : @xmath229 conversely , these rates are also achievable",
    ".    a simulation of the channel must be in the error bound for _ every _ input @xmath55 , hence eq .",
    "( [ eq : condition ] ) will be satisfied for every distribution @xmath0 .",
    "the lower bounds follow now from theorem  [ satz : feedback : channel : opt ] by choosing @xmath0 to maximize @xmath185 and @xmath186 , respectively .    to achieve this , the encoder , on seeing @xmath55 reports its type to the receiver ( asymptotically free ) and then they use the protocol of theorem  [ satz : sim : feedback : channel ] for @xmath230 , the empirical distribution of @xmath55 .",
    "possibly they have to use the channel at rate @xmath231 to set up additional common randomness beyond the given @xmath232 .    at this point",
    "we would like to point out a remarkable parallel of methods and results to the work  @xcite : our use of lemma  [ lemma : covering ] is the classical case of of the use of its quantum version from  @xcite , and the main result of the cited paper is the quantum analog of the present theorem  [ satz : sim : feedback : channel ] .",
    "the optimality result there has its classical case formulated in theorems  [ satz : lower ] ( and  [ satz : strong : lower ] ) and  [ satz : feedback : channel : opt ] , and even the construction of the following section has its counterpart there .",
    "the use of common randomness turned out to be remarkably powerful , and it is known in various occasions to make problems more tractable : a major example is the arbitrarily varying channel ( see for example the review  @xcite ) . while for discrete memoryless channels it does not lead to improved rates or error bounds , it there allows for a `` reverse '' of shannon s coding theorem  @xcite in the sense of simulating efficiently a noisy channel by a noiseless one .",
    "this viewpoint seems to extend to quantum channels as well , assisted by entanglement rather than common randomness : see  @xcite .",
    "we shall expand on the power of the `` randomness assisted '' viewpoint in section  [ sec : applications ] .",
    "here we show that from the theorem of the previous section a solution to the compression problem under a slightly relaxed distance criterion follows : whereas previously we had to employ common randomness to achieve the lower bound @xmath185 , this will turn out to be unnecessary now .    specifically",
    ", our condition will be eq .",
    "( [ eq : l : condition ] ) :    [ satz : coding : local ] there exists an @xmath35block code @xmath2 with @xmath233 such that @xmath234    choose an @xmath3code @xmath235 as in theorem  [ satz : sim : feedback : channel ] .",
    "obviously this code meets the condition of the theorem , except for the use of common randomness .",
    "we will show that a uniformly random choice among a _ small _ ( subexponential ) number of @xmath102 is sufficient for this to hold .",
    "then the protocol simply is :    1 .",
    "the sender choses @xmath102 uniformly random ( among the chosen few ) , and sends it to the receiver ( at asymptotic rate @xmath108 ) .",
    "2 .   she uses @xmath236 to encode , and the receiver uses @xmath213 to decode .    by construction",
    "this meets the requirements of the theorem .    to prove our claim , note that from theorem  [ satz : sim : feedback : channel ] we can infer @xmath237 introduce i.i.d .",
    "random variables @xmath238 , distributed according to @xmath239 . with the notations @xmath240 and @xmath241 we have",
    "@xmath242 denote the minimal nonzero entry of @xmath16 by @xmath243 , and choose @xmath170 so small that for all typical @xmath55 and all @xmath56 @xmath244 by lemma  [ lemma : large : deviation ] we obtain @xmath245\\text { on } { { \\operatorname{supp}\\,}}w_{x_k}\\right\\ }       \\\\          & \\hspace{4.7 cm } \\leq 2|{{\\mathcal{y}}}|\\exp\\left(-q\\frac{\\epsilon^2 u}{4\\ln 2}\\right ) .",
    "\\end{split}\\ ] ] hence the sum of these probabilities is upper bounded by @xmath246 which is less than @xmath110 for @xmath247 hence there exist actual values @xmath238 such that @xmath248 which is what we wanted to prove : observe that @xmath23 grows only polynomially .",
    "as we remarked already in the introduction ,  @xcite proposed to prove this result ( and indeed more , being interested in the tradeoff between rate and error ) , but eventually turned to the much softer condition  ( [ eq : ks : condition ] ) , which originates from the traditional model of rate distortion theory .",
    "nice though the idea of the previous section is , the lower bound results show that on this road we can not hope to approach the conjectured bound , because without common randomness at hand we have to spend communication at the same rate to establish it ( compare  @xcite , appendix , for this rather obvious  looking fact ) .    in this section",
    "we want to study the _ perfect _ restitution of the probability distributions @xmath14",
    "( i.e.  @xmath32 ) :    recall that here we want to minimize @xmath249 , and this minimum we call @xmath250 .",
    "obviously @xmath36 , so the limit @xmath37 exists , and is equal to the infimum of the sequence",
    ".    then we have    [ satz : old : new ] for all @xmath10 @xmath251    it is sufficient to prove the inequality for @xmath252 in place of @xmath253 :    fix a @xmath110code @xmath254 with @xmath255 .",
    "then , for @xmath256 choose any @xmath3source code @xmath257 for @xmath258 , which is possible at rate @xmath259 .",
    "then @xmath2 with @xmath260 and @xmath261 is an @xmath3code for the mixed state source with limiting rate @xmath252",
    ".    it would be nice if we could prove also an inequality in the other direction , but it seems that a direct reduction like in the previous proof does not exist : for this we would need to take an @xmath3code and convert it to an @xmath262code , increasing the entropy only slightly .",
    "[ fig : network ] . note that we included a sink , edges leading to the sink obviously having probability @xmath110.,title=\"fig : \" ]    a nice picture to think about the problem of finding @xmath252 is the following in the spirit of flow networks :    from the source we go to one of the nodes @xmath13 , with probability @xmath263 .",
    "then , with a probability of @xmath264 we go to @xmath90 , and from there with a probability of @xmath265 to @xmath266 .",
    "then the condition is that @xmath267 examples of this constructions are discussed in  @xcite ( where it was in fact invented ) , and here we want to add some general remarks on optimizing it , as well thoughts on a possible algorithm to do that .",
    "we begin with a general observation on the number of intermediate nodes :    [ satz : cd ] an optimal zero error code for @xmath16 requires at most @xmath268 intermediate nodes , with @xmath269 , @xmath270 .    for a fixed set @xmath5",
    "the problem is the following : + under the constraints @xmath271 minimize the entropy @xmath272 , where @xmath273 .",
    "observe that for each fixed set of @xmath274 the constraints define a convex admissible region for the @xmath275 , of which a _ concave _ function is to be minimized .",
    "hence , the minimum will be achieved at an extreme point of the region , that we rewrite as follows : @xmath276 an extreme point must be extremal in every of the summand convex bodies @xmath277 .",
    "on the other hand , an extreme point of @xmath277 must meet @xmath278 many of the inequalities ( @xmath279 ) with equality .",
    "since @xmath280 there remain only at most @xmath47 nonzero @xmath275 for every @xmath28 . in particular , only at most @xmath281 many @xmath90 are accessed at all .",
    "in fact , to minimize @xmath272 , at most @xmath268 , otherwise @xmath282 would contain full information about @xmath28 .",
    "[ rem : cd : more ] the last argument can be improved : for @xmath283 we can even assume @xmath284 .",
    "the argument of the proof gives us the idea that maybe by an alternating minimization we can find the optimal code :    indeed , conditions  ( [ eq : cd : e ] ) and  ( [ eq : cd : ed ] ) for fixed @xmath47 are linear in @xmath34 , and the target function is concave ( entropy of a linear function of @xmath34 ) , so we can find it s minimum at an extreme point of the admissible region .",
    "this part is solved by standard convex optimization methods . on the other hand , for fixed @xmath34 , eqs .",
    "( [ eq : cd : d ] ) and  ( [ eq : cd : ed ] ) are linear in @xmath47 .",
    "however , variation does not change the aim function .",
    "still we have freedom to choose , and this might be a good rule : let @xmath47 _ maximize _ the conditional entropy @xmath285 .",
    "the rationale is that this entropy signifies the ignorance of the sender about the actual output .",
    "if it does not approach @xmath184 in the limit this means that the protocol simulates partial feedback of the channel @xmath16 , which could be used to extract common randomness .",
    "this amount is a lower bound to what the protocol has to communicate in excess of @xmath185 .",
    "we have , however , no proof that this rule converges to an optimum .",
    "in this section we point out three important connections to other questions , some of which depend on positive answers to the questions  [ quest : main ] and  [ quest : main].      it is known that if two parties ( say , alice and bob ) have access to many inpendent copies of the pair of random variables @xmath286 ( which are supposed to be correlated ) , then they can , by public discussion ( which is overheard by an eavesdropper ) , create common randomness at rate @xmath287 , almost independent of the eavesdropper s information . for details",
    "see  @xcite , where this is proved , and also the optimality of the rate .",
    "one might turn around the question and ask , how much common randomness is required to create the pair @xmath288 approximately .",
    "this question , in the vein of that of the previous subsection , is really about reversibility of transformations between different appearances of correlation .",
    "note that this was confirmed in  @xcite for the case of deterministic correlation between @xmath289 and @xmath290 , i.e.  @xmath291 , which there was parallelled to entanglement concentration and dilution for pure states .",
    "an affirmative answer to question  [ quest : main ] , surprisingly implies that a rate of @xmath287 of common randomness is sufficient , _ with no further public discussion _ to create pairs @xmath292 .",
    "this is done by first creating the distribution @xmath23 of @xmath293 on @xmath5 from the common randomness ( this alice and bob do each on their own ! ) : this may be not altogether obvious as the common randomness is assumed in pure form ( i.e. a uniform distribution on @xmath148 alternatives ) , while the distribution @xmath23 may have no regularity . to overcome this",
    "difficulty fix an @xmath93 and let @xmath294 now we partition the unit interval into the subintervals @xmath295,\\end{aligned}\\ ] ] and define @xmath296 , @xmath297 .",
    "notice that for @xmath298 the probabilities for @xmath282 s belonging to the same set @xmath299 differ from each other only by a factor between @xmath300 and @xmath301 , and that @xmath302 , because of @xmath303 , by definition of @xmath56 .",
    "hence , defining uniform distributions @xmath304 on @xmath299 for @xmath298 , it is immediate that @xmath305 now the distribution on the @xmath306 in this formula can be approximated to within @xmath307 by a @xmath308type distribution , which in turn can be obtained directly from a uniform distribution on @xmath308 alternatives . in this way we reduced everything to a number of uniform distributions , maybe on differently sized sets ,",
    "all bounded by @xmath8 and a helper uniform distribution on a set of size @xmath308 .",
    "however , it is well known that these can be obtained from a uniform distribution on @xmath309 items within arbitrarily small error .    given this distribution on @xmath5 , bob applies @xmath47 , whereas alice applies the transpose channel @xmath310 to @xmath34 .",
    "one readily checks that this produces the joint distribution of @xmath311 , up to arbitrarily small disturbance in the total variational norm .",
    "note that this result would imply a new proof of the optimality of of the rate @xmath312 of common randomness distillation from @xmath311 : because we can simulate the latter pair of random variables with this rate of common randomness , we would obtain a net increase of common randomness after application of the distillation , which clearly can not be .",
    "it was already pointed out that this study has the paper  @xcite as one motiviation , with its idea to prove the optimality of shannon s coding theorem by showing that every noisy channel @xmath16 can be simulated by a binary noiseless one operating at rate @xmath313 .",
    "shannon s theorem is understood as saying that the noisy channel can simulate a binary noiseless one of rate @xmath313 .",
    "both simulations are allowed to perform with small error .",
    "note that an affirmation of question  [ quest : main] , implies that this can be done , without the common randomness consumption like in section  [ sec : cr : trick ] .",
    "as indicated , this provides a proof of the converse to shannon s coding theorem :    the idea is that otherwise we could , given a rate of @xmath313 noiseless bits simulate the channel , which in turn could be used to transmit at a rate @xmath314 . the combination of simulation and coding yields a coding method for transmitting @xmath118 bits over a channel providing @xmath313 noisless bits , which is absurd ( in  @xcite this reasoning is called `` causality argument '' ) .",
    "theorem  [ satz : sim : feedback : channel ] allows us to prove even more :    [ satz : feedback : capacity ] for the channel @xmath16 with noisless feedback ( i.e. after each symbol @xmath28 transmitted the sender gets a copy of the symbol @xmath50 read by the receiver , and may react in her encoding ) the capacity is given by @xmath313 .",
    "in fact , for the maximum size @xmath315 of an @xmath3feedback code @xmath316    let an optimal @xmath3feedback code for the channel @xmath82 with noiseless feedback be given .",
    "we will construct an @xmath317code with shared randomness , as follows :    choose a simulation of the channel @xmath16 on @xmath35blocks sending @xmath318 bits , and using shared randomness , and with error bounded by @xmath319 ( this is possible by the construction of theorem  [ satz : sim : feedback : channel ]  see remark  [ rem : chernoff : better ] ) .",
    "we shall use @xmath35 independent copies of the feedback code in parallel : in each round @xmath35 inputs symbols are prepared , sent through the channel , yielding @xmath35 respective feedback symbols .",
    "obviously , each round can be simulated with an error in the output distribution bounded by @xmath170 , using our simulation of the channel w ( which , as we remarked earlier , simulates even the feedback ) . in each of the parallel executions of the feedback code",
    "thus accumulates an error of at most @xmath320 , increasing the error probability of the code to @xmath321 .",
    "hence on the block of all the @xmath35 feedback codes we can bound the error probability by @xmath322 .",
    "but this is subexponentially ( in @xmath323 ) close to @xmath110 , so a standard argument applies :    first , by considering average error probability we can get rid of the shared randomness : there exists one value of the shared random variable for which the average error probability is bounded by @xmath324 .",
    "then we can argue that there is a subset @xmath325 of the constructed code s message set @xmath326 which has _",
    "maximal _ error probability bounded by @xmath327 and @xmath328    what we achieved so far hence is this : a code of @xmath329 messages with error probability @xmath330 and using @xmath331 noiseless bits .",
    "clearly , we may assume the encoder to be deterministic without losing in error probability .",
    "but then at most @xmath332 messages can be mapped to the same codeword without violating the error condition .    collecting everything we conclude",
    "@xmath333^n ,    \\end{split}\\ ] ] implying the theorem .",
    "[ rem : feedback ] the _ weak converse _ ( i.e.  the statement that the rate for codes with error probability approaching @xmath108 is bounded by @xmath313 ) is much easier to obtain , by simply keeping track of the mutual information between the message and the channel output through the course of operating a feedback code , using some well  known information identities , and finally estimating the code rate employing fano s inequality .",
    "let @xmath334 be any _ distortion measure _",
    ", i.e. a non  negative real function .",
    "this function is extended to words @xmath335 by letting @xmath336 shannon s rate distortion theorem is about the following problem : construct an @xmath35block code @xmath2 ( which my be chosen to be deterministic ) such that for a given @xmath337 @xmath338 i.e. , the average distortion between source and output word is bounded by @xmath339 .    a pair @xmath340 of non  negative real numbers is said to be _ achievable _ if there exist @xmath35block codes with code rate tending to @xmath118 and distortion rate asymptotically bounded by @xmath45 .",
    "define the _ rate  distortion function _",
    "@xmath341 as the minimum @xmath118 such that @xmath340 is achievable .",
    "[ satz : shannon : r : d ] the rate distortion function is given by the following formula : @xmath342 where @xmath343 is the expected ( single  letter ) distortion when using the channel @xmath16 .",
    "the proof of `` @xmath344 '' here is a simple exercise using convexity of mutual information in the channel and standard entropy inequalities .",
    "we can give a simple proof of the `` @xmath345''part of this result , using theorem  [ satz : feedback : channel : opt ] :    choose some channel @xmath16 satisfying the distortion constraint .",
    "then mapping @xmath55 to @xmath346 obviously satisfies the distortion constraint on the code in the sense that the expected distortion between input and output , over source and channel , is bounded by @xmath339 .",
    "of course , sampling @xmath346 at the encoder and sending some @xmath166 will not meet the bound @xmath185 .",
    "however , we can apply theorem  [ satz : feedback : channel : opt ] to approximately simulate the joint distribution of @xmath55 and @xmath166 by using some common randomness @xmath102 and a deterministic code @xmath347 sending @xmath348 bits .",
    "hence , invoking linearity of the definition of @xmath349 , @xmath350 so there must be one @xmath102 such that @xmath351 , which ends our proof .    at this point",
    "we would like to advertise our point of view that theorem  [ satz : coding : local ] , and even more so theorem  [ satz : sim : feedback : channel ] , is what rate ",
    "distortion is actually about : the former theorem shows how to simulate a given channel on all individual positions of a transmission , and this is what we need in rate  distortion .",
    "in fact , rate  distortion theory is unchanged when instead of the one convex condition ( `` distortion bound '' ) on the code we have several , effectively restricting the admissible approximate joint types of input and output to any prescribed convex set  in particular a single point .    the strength of theorem  [ satz : coding : local ] in comparison to such a development of rate ",
    "distortion theory lies in the fact that with its help we satisfy the convex conditions in _ every letter _ , not just in the block average . and theorem  [ satz : sim : feedback : channel ] gives the analogue of this even with the condition imposed on the whole block , yielding results that are not obtainable by simply applying rate ",
    "distortion tools ( see e.g.  @xcite ) .",
    "the problem studied in this paper has a natural extension to quantum information theory : now the source emits ( generally mixed ) quantum states @xmath14 on the hilbert space @xmath352 ( @xmath13 ) , with probabilities @xmath263 , and an @xmath3code is a pair @xmath2 of maps @xmath353 where @xmath354 is the set of states on the code hilbert space @xmath355 and @xmath47 is completely positive , trace preserving , and linear .",
    "the condition to satisfy is @xmath356 with the trace norm @xmath19 on density operators .",
    "define , like before , @xmath9 as the minimum @xmath357 of an @xmath3code .",
    "sometimes , the stronger condition @xmath358 will be applied .",
    "notice that this contains our original problem as the special case of a _ quasiclassical _ ensemble , when all the @xmath359 commute ( which means they can be interpreted as probability distributions on a set of common eigenstates ) .",
    "this problem ( with a number of variations , which we explained in the introductory section  [ sec : pdsources ] for the classical case ) is studied in  @xcite . there ( and previously in  @xcite ) it is shown that the lower bound theorem  [ satz : lower ] holds in the quantum case , too , with understanding @xmath360 as von neumann entropy :    [ satz : mixed : lower ] for all @xmath35 , @xmath181 @xmath361 with a function @xmath63 for @xmath31 .",
    "@xmath73    let us improve this slightly by proving the strong version of this result :    [ satz : mixed : strong : lower ] for all @xmath10 @xmath362    by much the same method as the proof of theorem  [ satz : strong : lower ] : the changes are that we need the more general code selection result of  @xcite , thm .",
    "ii.4 , instead of the classical theorem  @xcite , and which we state separately below : if @xmath2 is an optimal @xmath3code , define @xmath363 obviously @xmath364 , so we can apply lemma  [ lemma : max : code ] and find an @xmath365transmission code @xmath81 for @xmath82 such that @xmath366 this is an @xmath80code for the channel @xmath85 , with @xmath367 , if we choose @xmath170 small enough . combining @xmath34 with the transmission encoder , and @xmath47 with the transmission decoder , we obtain an @xmath3transmission code for @xmath329 many messages over a noiseless system with hilbert space @xmath355 of dimension @xmath9 .    to each message @xmath368 there belongs a decoding operator @xmath369 on the coding space @xmath355 , forming together a povm : @xmath370 . now",
    "to decode correctly with probability @xmath371 , for each @xmath243 we must have @xmath372 on the other hand , by @xmath373 , we conclude @xmath374 and we are done .",
    "[ lemma : max : code ] for @xmath375 there is a constant @xmath376 and @xmath94 such that for every discrete memoryless quantum channel @xmath16 and distributions @xmath0 on @xmath1 the following holds : if @xmath377 is such that @xmath378 then there exists an @xmath3transmission code @xmath2 with the properties @xmath379 @xmath380    see  @xcite , thm .",
    "progress on the problem of achievability of this bound is not known to us .",
    "it is remarkable that koashi and imoto  @xcite could obtain the exact optimal bound in the case of _ blind _ coding .",
    "it is indirectly defined via a canonical joint decomposition of the source states , but it can be derived from their result that generically the optimum rate is @xmath186 , which is achieved by simply schumacher encoding the ensemble @xmath381 .    nevertheless , the results obtained in the classical case are very encouraging , so we state two conjectures :    [ conj : qmain : cr ] for @xmath69 there exist @xmath3codes with common randomness , asymptotically achieving transmission rate @xmath185 and common randomness consumption @xmath184 .",
    "if it turns out true , and also question  [ quest : main ] has a positive answer , we might even hope that also    [ quest : qmain ] for @xmath69 , is @xmath382 [ note that , as in the case of question  [ quest : main ] , codes achieving the optimal bound may also be constructed to satisfy eq .",
    "( [ eq : qu : condition : strong ] ) . ]",
    "answers `` yes '' .",
    "the implications of these statements , if they are true , would be of great significance to quantum information theory : not only would we get a new proof of the capacity of a classical  quantum channel being bounded by the maximum of the holevo information and for the optimality of common randomness extraction from a class of bipartite quantum sources  @xcite , but also the achievability of @xmath185 in the quantum rate distortion problem  @xcite with visible coding would follow , that until now has escaped all attempts .",
    "we demonstrated the current state of knowledge in the problem of visible compression of sources of probability distributions and its extension to mixed state sources in quantum information theory .",
    "apart from reviewing the currently known constructions we contributed a better understanding of the resources involved : in particular the use of common randomness in some of them , and providing strong converses . also we showed the numerous applications the result ( and sometimes the conjectures ) have throughout information theory , making the matter an eminent unifying building block within the theory .",
    "we would like to draw the attention of the reader once more to our questions  [ quest : main ] and  [ quest : qmain ] , and especially the conjecture  [ conj : qmain : cr ] offering them as a challenge to continue this work .",
    "research partially supported by sfb 343 `` diskrete strukturen in der mathematik '' of the deutsche forschungsgemeinschaft , by fakultt fr mathematik , universitt bielefeld , by the university of bristol , and by the u.k . engineering and physical sciences research council .                c. h. bennett , p. w. shor , j. a. smolin , a. v. thapliyal , `` entanglement  assisted classical capacity of noisy quantum channels '' , phys .",
    "letters , vol .",
    "30813084 , 1999 .",
    "by the same authors : `` entanglement  assisted capacity of a quantum channel and the reverse shannon theorem '' , e  print quant - ph/0106052 , 2001 .",
    "g. kramer , s. a. savari , `` quantum data compression of ensembles of mixed states with commuting density operators '' , e  print quant - ph/0101119 . presented at the @xmath383 ams meeting ( hoboken , nj ) , april 2829 , 2001 ."
  ],
  "abstract_text": [
    "<S> we study the problem of efficient compression of a stochastic source of probability distributions . it can be viewed as a generalization of shannon s source coding problem . </S>",
    "<S> it has relation to the theory of common randomness , as well as to channel coding and rate  distortion theory : in the first two subjects `` inverses '' to established coding theorems can be derived , yielding a new approach to proving converse theorems , in the third we find a new proof of shannon s rate  distortion theorem .    after reviewing the known lower bound for the optimal compression rate </S>",
    "<S> , we present a number of approaches to achieve it by code constructions . </S>",
    "<S> our main results are : a better understanding of the known lower bounds on the compression rate by means of a strong version of this statement , a review of a construction achieving the lower bound by using common randomness which we complement by showing the optimal use of the latter within a class of protocols . </S>",
    "<S> then we review another approach , not dependent on common randomness , to minimizing the compression rate , providing some insight into its combinatorial structure , and suggesting an algorithm to optimize it .    </S>",
    "<S> the second part of the paper is concerned with the generalization of the problem to quantum information theory : the compression of mixed quantum states . here , after reviewing the known lower bound we contribute a strong version of it , and discuss the relation of the problem to other issues in quantum information theory . </S>"
  ]
}