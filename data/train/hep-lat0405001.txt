{
  "article_text": [
    "the recent advocacy of the use of bayesian statistics for the analysis of data from lattice simulations , in the guise of the methods of constrained curve fitting  @xcite , or maximum entropy  @xcite , has eased considerably the ambiguity and irritation associated with estimating the systematic errors due to curve fitting , especially when extracting masses , spectral weights and matrix elements from monte carlo estimates of correlation functions .",
    "previously , monte carlo estimates , @xmath6 , of two - point hadronic correlators had been fit to a theoretical model , such as @xmath7 where @xmath8 is the spectral weight of the @xmath9 state , by the maximum - likelihood procedure of minimizing the @xmath0 @xmath10 with covariance matrix @xmath11 traditionally , these had been fit only at large euclidean times @xmath12 , where contributions from excited states are exponentially damped .",
    "the art had been to choose a value of @xmath13 which compromises between unnecessarily high statistical errors for large @xmath13 and high systematic errors ( from contamination from excited states ) for small @xmath14 .",
    "lattice alchemy provided various recipes for making the compromise and estimating the systematic errors , but the procedures were often suspect and always frustrating .",
    "the truncation of the data set to only a few large @xmath15 was deemed necessary because the alternative ( of including more time slices but also more terms in the fit model ) resulted in unacceptably unstable fits to the sum of decaying exponentials ( traditionally a bane of numerical analysts ) .",
    "success was achieved in some cases by enlarging the data set by including more channels , e.g.  diagonalization of multi - source multi - exponential fits .",
    "indeed when correlators from very many sources could be calculated cheaply , such as for glueballs or static quarks , the improvement was dramatic .",
    "but most often , when only a couple of channels at best could be fit simultaneously , the competition between increased statistical errors for large @xmath13 and large systematic errors for small @xmath13 remained ; although the final statistical and systematic errors were reduced , the effort and uncertainty in obtaining a reliable systematic error remained .",
    "constrained curve fitting  @xcite offers the alternative of minimizing an augmented @xmath0 , @xmath16 where @xmath17 denotes the collective parameters of the fit ( e.g.  @xmath18 for a sum of exponentials ) , as a way of achieving stability by `` guiding the fit '' with the use of bayesian priors , that is , values of the parameters obtained from _ a priori _ estimates @xmath19 . with",
    "improved stability , the data sets can be enlarged to include small @xmath15 and the theory can be enlarged by including many more terms in the fit model until convergence is obtained . the systematic error associated with the choice of @xmath13 is thereby largely absorbed into the statistical error .",
    "the advantage that the constrained curve fitting of lattice data has over a typical data set that a numerical generalist would consider , is that often we have reliable estimates of , or at least constraints on , the fit parameters from outside the data ( for example , the masses must be positive , or the level spacing is expected to be such - and - such from reliable models ) which can then be used as bayesian priors .",
    "examples , such as upsilon spectroscopy  @xcite where the level spacing can be reliably estimated from quark models and experiments , are impressive . remarkably , constrained curve fitting with bayesian priors on such data has been able to give satisfactory fits for local - local correlation functions , i.e.  when multi - source fits are unavailable ( presumably due to prohibitive cost ) .",
    "but with our recent data  @xcite , we enter previously unexplored territory .",
    "we work with overlap fermions with exact chiral symmetry at unprecedented small quark mass and large spatial volume . the literature , from which to obtain estimates to be used as priors , is limited .",
    "furthermore , the details of the level spacings ( e.g.  the roper resonance and the @xmath20 ) are hotly debated between advocates of quark models versus those of chiral models .",
    "the use of priors in standard constrained curve fitting tends to `` lock in '' the fit ( within a sigma or so ) ; if one gets them badly wrong , then the fitted results may be misleading .",
    "furthermore , the stability of the fit results against choice of prior must be tested  this reintroduces an element of subjectivity . as a modification of the basic bayesian - prior constrained - curve fitting ( augmented @xmath0 ) procedure",
    ", we propose to make it more automatic , and to further absorb systematic errors associated with choice of prior into statistical errors .    in section",
    "ii , we give an overview of the `` sequential empirical bayes method '' detailing our extension of constrained curve - fitting . in section iii , we add some further improvements to better assess and reduce systematic errors , and study fits to artificial data where the true values of the parameters are known . in section iv we give , as an illustration of the efficacy of the algorithm , some results from our low quark mass overlap fermion data for the excited states of the pion , present preliminary fits for the @xmath1 where ghost states ( a quenched artifact ) must be dealt with , and comment on the details of fits of the roper resonance and @xmath2 previously presented elsewhere  @xcite . our summary and conclusions follow in section v.",
    "bayesian statistics is an entire field in itself , with an old and broad history ; for an introduction , see  @xcite . empirical bayes methods are intermediate between classical ( `` frequentist '' ) and bayesian methods .",
    "the core ideas of a sequential analysis of the data originate with robbins  @xcite .",
    "we propose the sequential empirical bayes ( seb ) method as a refinement especially well suited for the special properties of lattice monte carlo correlation functions .",
    "we begin in subsection  [ sect : synopsis ] by giving a brief description of the standard constrained - curve fitting approach emphasizing its basis in bayesian probability theory .",
    "our sketch relies heavily on a few recent and relevant summaries  @xcite ; indeed , our notation is an amalgam of theirs .",
    "we follow with a descriptive overview of our method in subsection  [ sect : overview ] and a more detailed rendering of the basic algorithm in subsection  [ sect : basicalgorithm ] .      to account for the more general case of multi - source fits , as well as to allow the time dependence to be non - exponential ( periodic boundary conditions result in @xmath21 or @xmath22 , and quenched artifacts can give rise to more complicated temporal dependence ) ,",
    "we write @xmath23 where @xmath24 are the collective parameters of the fit ( e.g.  @xmath18 for a sum of exponentials ) , the indices @xmath25 , @xmath26 distinguish different values of the independent variable ( time for correlation functions ) and different interpolating fields , @xmath27 are the monte carlo data , @xmath28 is the fit model , and @xmath29 is the covariance matrix .    minimizing the @xmath0 in eq .  [ generalchisquare ] is the solution of the problem of determining the set of fit parameters @xmath24 which maximizes @xmath30 , the conditional probability of measuring the data @xmath27 given a set of parameters , also known as the `` likelihood '' of the data .",
    "bayesian inference turns this question around and demands that the solution of the curve - fitting problem consist of determining the set of parameters @xmath24 which maximizes @xmath31 , the conditional probability that @xmath24 is correct given the measured data @xmath27 .",
    "that is , bayesian inference asks which fit - model parameters are most likely given the data .",
    "the computation of the latter conditional probability is possible because of the celebrated bayes theorem @xmath32 which follows directly from the elementary properties of probability theory @xmath33 the unconditional probability @xmath34 is the plausibility one assigns to the parameters @xmath24 before the additional information of the fit is provided , and is known as the `` bayesian prior distribution '' . the conditional probability @xmath31 is known as the `` posterior probability distribution '' ; it is the reassessment of the likelihood of the parameters after the fit incorporates the data .",
    "the unconditional probability @xmath35 , known as the `` prior predictive probability '' of the data , is independent of @xmath24 and thus serves only as a normalization constant ; it is determined from eq .",
    "[ bayestheorem ] and the normalization condition @xmath36 .",
    "heuristically , bayes theorem can be thought of as `` posterior probability '' @xmath37 `` likelihood '' @xmath38 `` prior probability '' .",
    "traditional inference ( `` frequentist theory '' ) uses the likelihood @xmath30 to quantify all statistical measures of inference ( mean , standard deviation , @xmath0 , confidence limits , @xmath39 ) , while bayesian inference uses the posterior distribution @xmath31 to determine the statistics . from eq .",
    "[ bayestheorem ] , bayesian theory requires augmenting the information that the frequentist theory uses with the additional estimate of the prior distribution @xmath34 .",
    "of course , the two methods agree when the prior distribution is chosen to be constant .",
    "the likelihood @xmath30 , needed by both frequentists and bayesians , can be estimated if the monte carlo data set is sufficiently large .",
    "then , regardless of the statistics of the underlying population distribution ( the ensemble of all possible configurations ) , the sample distribution ( monte - carlo - generated averages over finite sets of configurations ) will have gaussian statistics , as assured by the central limit theorem .",
    "thus @xmath40 with the @xmath0 given by eq .",
    "[ generalchisquare ] .",
    "the bayesian prior distribution @xmath34 is the probability that a particular set of parameters @xmath24 is correct , _ a priori _ to the data analysis .",
    "it is implicit in that the bayesian probabilities are conditional on some background information .",
    "operationally , @xmath34 serves as a constraint on the parameters @xmath24 . for the physicist",
    ", these might be constraints on the ranges of parametric values that are physically feasible ( e.g `` positive energy splittings '' ) , or perhaps something stronger as dictated by experience from previous similar fits or guidance from models of qcd or experiments .",
    "a simple choice for the prior distribution is gaussian @xmath41 with @xmath42 defined in eq .",
    "[ chiprior ]",
    ". then @xmath43 of eq .",
    "[ bayestheorem ] is maximized by minimizing the augmented @xmath0 ( @xmath44 ) of eq .",
    "[ augmentedchisquare ] .",
    "this is the approach outlined in  @xcite where they choose values for the priors @xmath45 and @xmath46 based on physicists intuition .",
    "the opposite extreme is the use of the `` entropic prior '' , based on the view that for some fits where the number of parameters is larger than the number of measured data ( such as for the spectral density function  @xcite , or in a wider context image restoration ) only minimal information about the priors is available .",
    "we seek an approach between the two extremes .",
    "we use the language of augmented @xmath0 but seek to use a subset of the available data to estimate the priors in an orderly fashion , which will then be used on new data .",
    "so how do we obtain our priors ?",
    "for concreteness , consider again the fit model of the sum of decaying exponentials in eq .",
    "[ exp ] . apportion the data into a nested set ( picture the shells in an onion or a kewpie doll ) and",
    "extract estimates for priors in a progressive manner , at each stage obtaining estimates of one or two new priors upon each expansion of the data set .",
    "an especially natural and well - suited nesting is to partition the hadronic correlator data via euclidean time slices .",
    "that is , the first and smallest data set includes all configurations but with times slices @xmath15 restricted to @xmath47 ( and perhaps @xmath48 depending on boundary conditions ) .",
    "subsequently , the @xmath49 data set includes all time slices such that @xmath50 , where @xmath51 and @xmath52 is most simply chosen to be the constant @xmath53 .",
    "( a more general choice is made in practice , as described later . )",
    "the time @xmath54 is chosen by the same criteria used in traditional fits so that an unconstrained fit is well approximated by a single exponential .",
    "the output values for two parameters , the ground state mass and spectral weight , are then used as priors for the next fit on the augmented data set . for the second fit , two ( or more , in general ) additional time slices are included , as are two more parameters in the fit model , the weight and mass of the first excited state .",
    "the new fit is constrained with regard to the ground state but unconstrained with regard to the first excited state .",
    "the four fitted parameters are then used as priors in a third fit on a larger data set , and so on until all desired time slices and/or terms in the fit are included .",
    "in this way , we estimate the priors from a subset of the data .",
    "furthermore , we choose that subset of the data which is best suited for making the estimation .",
    "the choice is determined with the same compromise as is used for traditional unconstrained fits , namely between minimizing statistical errors ( which grow as @xmath15 increases ) and minimizing the contamination of higher excited states ( which grows as @xmath15 decreases ) .",
    "thus we propose an adaptive self - contained constrained curve - fitting procedure , dubbed the `` sequential empirical bayes '' method . in a nutshell",
    ", we obtain the priors gradually ( allowing them to change as needed ) , from the ground state up , as the data set is monotonically enlarged by including earlier and earlier time slices .",
    "its advantages include that it is usable whenever external reliable estimates of the priors are not available , and it is as automatic as one could hope for , thereby reducing the potential to introduce bias and of course decreasing the frustrating busy - work of fitting . especially for the low - lying states ,",
    "if the initial priors are estimated incorrectly , there are several subsequent steps by which they may change ( by about a sigma each time ) .      from experience",
    "we have discovered that for some data , either `` real '' ( actual lattice gauge theory data ) or artificially constructed , a very basic algorithm which incorporates the seb philosophy is adequate for producing reliable priors .",
    "however , some of our data have exposed deficiencies in the basic algorithm . these have been corrected without violating the spirit of the seb , but make the resulting final algorithm rather mysterious and complicated to describe in one pass . accordingly , in this paper , we begin here by outlining the simplest algorithm as a template . in sect .",
    "[ sect : systematic ] we will introduce modifications as necessary .    [",
    "cols=\"^,^,^,^,^ \" , ]     table  [ table : parameters ] describes for simplicity the very basic `` fixed @xmath52 '' algorithm , in which after each pair of steps , @xmath55 new ( earlier ) time slices are added to the data to be fitted .",
    "the algorithm is as follows :    * choose @xmath56 and @xmath13 , the maximum window over which the fits will be done .",
    "the window is to be chosen as large as possible . for correlation functions expected , on theoretical grounds , to be positive",
    "then if a value of a correlation function at a time slice is not within one sigma of being positive , then that time slice and all greater time slices are eliminated from consideration .",
    "this prevents noisy correlation functions at large @xmath15 from giving grossly inaccurate estimates of the fitted values to be subsequently used as priors .",
    "* determine the number of the terms we want to use in the fit model , and determine @xmath54 as the starting point for the fit .",
    "ensure that @xmath57 * choose central trial values @xmath58 and @xmath59 ( or @xmath60 ) equal to those obtained from the effective mass relations .",
    "loop on various trial values around these central values . for each , use an unconstrained fit on the one - mass - term model to fit the correlator data including time slices @xmath61 to @xmath56 and obtain @xmath62 and @xmath63 .",
    "choose as input for the next step those values which yield the lowest ( but reasonable ) @xmath64 .",
    "* using these values of @xmath65 and @xmath66 as both priors and initial values , do a constrained curve fit ( using the one - mass - term model on the data set enlarged to include @xmath67 ) to obtain @xmath68 and @xmath69 .",
    "* loop on a wide range of trial values for @xmath70 and @xmath71 . with a two - mass - term model ,",
    "constrain the first mass and weight ( using the previous output as both priors and initial values ) but leave the second mass and weight unconstrained . loop on various trial values for the latter .",
    "do this half - constrained fit on the data set enlarged to include @xmath72 and obtain @xmath73 and @xmath74 .",
    "choose as input for the next step those values which yield the lowest ( but reasonable ) @xmath75 .",
    "* using these values of @xmath76 , @xmath66 , @xmath77 , @xmath78 as both priors and initial values , do a fully - constrained fit ( using the two - mass - term model on the data set enlarged to include @xmath79 ) to obtain @xmath80 , @xmath81 , @xmath82 , and @xmath83 .",
    "* repeat the last two steps until all desired mass terms and time slices are included .",
    "one thus obtains a complete set of priors . *",
    "add the final time slice @xmath13 and do a fully - constrained fit using previously obtained values for priors and initial guesses .    all fits are correlated using the full covariance matrix . furthermore , the entire process is bootstrapped ( or jackknifed ) .",
    "final quoted errors are bootstrap ( or jackknife ) errors . within a bootstrap sample , at intermediate steps in the algorithm",
    ", the sigmas of the priors , @xmath84 , are obtained from the fitting errors of the previous step .    as a variation , rather than deciding _ a priori _ on the number of terms in the fit and adding time slices one at time",
    ", one can let the data decide how many time slices to include with each enlargement of the data by choosing the minimum @xmath75 over a range of reasonable possibilities .",
    "thus , for example , if the data is dominated by the ground state for many time slices , then many time slices will be automatically added before an attempt is made to fit the first - excited state .",
    "we will refer to this improvement as the `` variable @xmath52 '' approach , and to the basic template described in this section as the `` fixed @xmath52 '' approach .",
    "the @xmath85 minimization can be cast more generally as minimizing a functional @xmath86 of the vector @xmath24 ( see , for example ,  @xcite ) . in the extreme case , with more unknown parameters than data points",
    ", the minimization procedure becomes degenerate and there is enough freedom to drive @xmath85 down to unreasonably small values . with more data points than unknown parameters , although the agreement with the data typically is good ( often too good to believe ) ,",
    "the solution is unstable and often wildly oscillating .",
    "this signals a proximity to the degeneracy of the underlying minimization problem . constrained",
    "curve fitting can be cast more generally as the problem of minimizing @xmath87 @xmath88 if @xmath89 is degenerate , but @xmath90 is not , the degeneracy is lifted .",
    "the stability is improved in general .",
    "the solution @xmath91 varies along a `` trade - off curve '' ( see , for example ,  @xcite ) .",
    "@xmath92 ( e.g.  @xmath0 ) measures the agreement of the model to the data , while @xmath93 ( e.g.  @xmath94 ) plays the role of a stabilizing functional .",
    "the standard constrained - curve fitting procedure selects @xmath95 .",
    "to estimate the systematic errors associated with the choice of priors , it is common to repeat the procedure with all @xmath84 replaced by @xmath96 where @xmath97 is a typical choice .",
    "this moves one along the trade - off curve from @xmath95 to @xmath98 , giving less weight to the priors .",
    "more generally , one could have an independent weighting factor , @xmath99 , multiplying the @xmath100 of each prior .        for concreteness , we return to the simple case of a sum of decaying exponentials of eqs .",
    "[ exp ] ,  [ chisquare ] @xmath101 by choosing @xmath102 , one relaxes the constraint of the priors and thus tests the sensitivity of the fitted values to the choice of prior .",
    "we present three ways , in order of increasing sophistication , to implement the choice of each @xmath103 .    1 .",
    "_ global static weight : _ the canonical approach is to keep it unchanged as a global constant for the fit , @xmath104 .",
    "the variation of the dependence of the output fitted results on this global input parameter may be used to estimate the systematic error .",
    "local variable weight : _ allow each @xmath103 to be a local variable . at each step ,",
    "loop on various values of @xmath103 and choose the value which minimizes the @xmath75 .",
    "monitor against runaway solutions .",
    "+   plot of ground - state pion mass , @xmath105 ( left ) and spectral weight @xmath106 ( right ) , versus global static weight , @xmath107 , for bare quark mass @xmath108.,title=\"fig : \" ] plot of ground - state pion mass , @xmath105 ( left ) and spectral weight @xmath106 ( right ) , versus global static weight , @xmath107 , for bare quark mass @xmath108.,title=\"fig : \" ] +   same as in fig .",
    "[ fig : pionlambdatest ] but for the nucleon for bare quark mass @xmath109 ( @xmath110).,title=\"fig : \" ] same as in fig .",
    "[ fig : pionlambdatest ] but for the nucleon for bare quark mass @xmath109 ( @xmath110).,title=\"fig : \" ] 3 .",
    "_ local dynamical weight : _ for some quantities , such as for the ground - state mass and spectral weight of the pion displayed in fig .",
    "[ fig : pionlambdatest ] , the dependence of the fitted results on the value of @xmath107 is quite mild . for others , such as for the nucleon of fig .",
    "[ fig : nucleonlambdatest ] , the dependence is stronger .",
    "accordingly , we propose an adaptive procedure of absorbing any systematic error associated with the choice of @xmath111 into the statistical error , by upgrading each @xmath99 to a dynamical variable to be determined by the fit",
    ". it is incorporated as a dynamical fit variable by further augmenting the @xmath85 : @xmath112 the choice of @xmath113 and @xmath114 is also somewhat arbitrary , and results in some uncertainty in assessing the systematic error ; however , the dependence on these parameters is gentler than on the corresponding static global values and any remaining systematic error is masked by statistical error .",
    "global dynamical weight : _ in our fits presented here , we made the simplest choice of a special case of ( c ) : we took all @xmath103 for different parameters to be the same global @xmath111 , and took @xmath115 and @xmath116 .",
    "+ often , the dependence of the results of the fit on the global static weight is sufficiently smooth that the use of global dynamical weight is unnecessary .",
    "a way to mitigate the potentially aggressive nature of the fully - constrained fit in the sequential empirical bayes method is the following : when one is interested in estimating a particular parameter ( i.e.  the mass or weight of the ground state or @xmath49 excited state ) one takes the priors from the previous method and does a partially constrained fit : all other parameters are constrained except the one in question which is unconstrained ( or perhaps only very lightly constrained ) .",
    "we say that the parameter to be estimated is `` released '' ( from the constraint ) .",
    "the statistical error estimate obtained this way is larger and a more conservative choice",
    ". we will , by default , release the constraint for all of the final results presented .      referring back to our algorithm ,",
    "if a prior is available , we use it as the value of the initial guess .",
    "but as a new parameter is introduced there is no prior , and an initial guess must be obtained by some other criterion . in practice",
    ", it may happen that different initial guesses lead to different values for the fit parameters , especially for the unconstrained fit .",
    "this may signal the presence of multiple local minima in the complicated @xmath85 topography . to address this",
    "we introduce and advocate the use of `` scanning '' over a grid of reasonable initial values and selecting that choice which leads to a descent to the lowest @xmath85 . by extending the scan to a rather large region of parameter space ,",
    "we ensure that the initial guesses for the parameters will not confine the @xmath85 minimization search to a single basin of local attraction , but rather will extend over many , including that of the global minimum .",
    "recovery of parameters from artificially constructed data.,title=\"fig : \" ] recovery of parameters from artificially constructed data.,title=\"fig : \" ]    as there are a lot of ingredients to our modification of the standard constrained fit method ( `` onion - shell data '' , `` scanning '' , `` global dynamical weight '' , `` releasing the constraint '' ) it is comforting to learn that the method can successfully reconstruct the parameters of artificially - constructed data where the true results are known independently of the fit .",
    "we created a sample of artificial data as a sum of five decaying exponentials , with means for masses and weights fixed at values close to those extracted from real data .",
    "then , for simplicity , given this function , @xmath117 , we added an independent gaussian noise at each value of @xmath15 . when run through our fitting procedure",
    ", we were able to reconstruct the masses and weights for the ground state , first - excited state , and second - excited state ( see fig .",
    "[ fig : fakedata ] ) ; for these , the actual values were within one measured standard deviation of the measured masses .",
    "test of partitioning the data .",
    "ground and first excited state pion mass , @xmath105 , as a function of the bare quark mass @xmath118 . ]",
    "we have constructed an automated and natural way of obtaining the priors from a subset of the data . however , one worries that there is a significant amount of `` data snooping '' which may make it difficult to estimate the systematic errors .",
    "( strictly speaking , using any of the data to obtain the priors violates the bayesian approach . ) to alleviate these worries , we have implemented the following test : we partition the data into two non - intersecting sets of configurations , @xmath119 and @xmath120 , with an equal number , @xmath121 of configurations in each set ( which must still be large enough to permit stable covariant fits ) . using the set @xmath119 of configurations",
    ", we perform our procedure outlined above of obtaining the priors gradually , from the ground state up , as the data set is monotonically enlarged by including earlier and earlier time slices . now , regarding this entire procedure as a black box solely for the purposes of obtaining the priors , we next use this fixed set of priors in the canonical way  @xcite to perform a constrained fit separately on data set @xmath120 ( for which there is no data snooping ) , on data set @xmath119 ( maximal data snooping ) , and on the full set @xmath122 ( partial data snooping but with greater statistics ) .",
    "any disagreement beyond statistical errors can help assess systematic errors due to data snooping .",
    "in our case we found no appreciable differences beyond expected statistical fluctuations .",
    "figure  [ fig : partition ] shows a plot of the ground and first excited state pion mass , @xmath105 , as a function of the bare quark mass @xmath118 .",
    "however priors are obtained , they can then be used in a standard constrained curve fitting procedure , wherein the fit window @xmath13@xmath56 is held fixed ( and as big as feasible ) , while the number of terms in the fit model is increased one - by - one until the fit results converge for the lowest few parameters of interest .",
    "to test whether the sequential empirical bayes method has produced reliable priors , we subsequently use its priors in the standard way  @xcite as described above .",
    "figure  [ fig : stability ] illustrates the passing of this test .",
    "in a test of stability , priors had previously been selected by the sequential empirical bayes method .",
    "now they are used in the standard constrained fit .",
    "the figure shows fit values for the lowest two masses from constrained fits ( for all @xmath15 s ) with different numbers of terms in the fit model .",
    "the data is from the pion two - point correlation function @xmath123 at bare quark mass @xmath109 ( @xmath110 ) . ]",
    "given the posterior probability distribution , @xmath31 , all statistics may be evaluated by computing integrals . in practice , such evaluation is often difficult to perform directly ; monte carlo methods such as simulated annealing  @xcite can be used .",
    "even so , the computational cost is often daunting and so approximations are sought .",
    "most commonly , one may assume that the data set is sufficiently large that the central limit theorem implies that @xmath124 is approximately quadratic about its minimum .",
    "we in fact use this approximation , and furthermore use the resulting fitting errors from a fit as the priors @xmath125 for the next fit .",
    "to test this , we plot in fig .",
    "[ fig : fittingerrors ] the @xmath124 as a function of @xmath24 in the neighborhood of the minimum .",
    "superimposed is the parabola which is the quadratic approximation about the minimum .",
    "the two agree very well . also calculated and plotted as ranges are the `` fit error '' , obtained from the quadratic approximation , and the second moment of the actual distribution .",
    "these too agree very well , and indicate that using the naive fit error as provided by the minimization routine is quite adequate .",
    "although fit errors are used to produce priors for the inner loops of the algorithm , all final errors reported are bootstrap errors .",
    "plot of the augmented @xmath124 in the vicinity of its minimum .",
    "it agrees well with a quadratic approximation , ensuring that the fit error provided by the minimization routine is adequate .",
    "the data is from the pion two - point correlation function at bare quark mass @xmath108 ( @xmath126 ) . ]",
    "it is crucial to be extremely conservative when making fits , especially when relying on bayes inference methods .",
    "one must be sure to avoid announcing a `` false positive '' , that is making claim which can not be guaranteed to be correct .",
    "we can understand in more detail how a false positive may emerge and how to design the procedure to avoid them , by testing variants of the seb algorithm against a number of `` toy models '' where the actual values of the function s parameters are known .",
    "suppose the artificial data is created with the three - state model function @xmath127 and we want to perform a fit of the data sample generated by this function with fixed values of input parameters and gaussian statistical error @xmath128 at each time slice @xmath15",
    ". then seb should work if the following holds :    1 .",
    "there exists a @xmath129 such that in the time range @xmath130 , @xmath131 so that in this range , the data can be fitted by @xmath132 .",
    "that is , there is a `` plateau '' in the effective mass plot for large time . 2 .",
    "there exists a @xmath133 such that in the time range @xmath134 , @xmath135 so that in this range , the data can be well fitted by @xmath136 .",
    "3 .   in the time range @xmath137",
    ", the third state becomes important , and the full three - state model must be used in the fit .",
    "now the seb is very adaptive ; in the course of the iterative procedure , the ground state is fitted several times and allowed to float within a sigma or so of its current prior with each new fit .",
    "thus the original criterion of the existence of a clear plateau need not be strictly enforced .",
    "but if the condition for the plateau is badly violated , or if the similar conditions for the excited states are badly violated , there is a possible danger of the algorithm gravitating toward a local minimum in the @xmath0 which is not the true value .    to illustrate , consider artificial data , constructed from the following three - state toy model @xmath138 in the time range @xmath139 , with relative errors ( uncorrelated and gaussian ) increasing with time @xmath15 to mimic actual lgt data .",
    "in keeping with the spirit of the seb , we fit first to a single exponential over the time interval @xmath140 $ ] , then use the fitted ground state values as priors for a two - state partially - constrained fit in the time interval @xmath141 $ ] , and then use the fitted ground and first - excited state fit values as priors for a final three - state partially - constrained fit in the time interval @xmath142 $ ] , where @xmath143 . but to stress the caveat about the dangers of `` false positives '' , we explore all values for the pair @xmath144 .",
    "[ [ lowest - precision - case ] ] `` lowest precision '' case : + + + + + + + + + + + + + + + + + + + + + + + + + +    with statistical errors ranging from @xmath145 at @xmath146 , to @xmath147 at @xmath148 , the lowest @xmath149 fit is for @xmath150 with @xmath151 , @xmath152 , @xmath153 , @xmath154 , @xmath155 , @xmath156 , that is , the fit fails to produce the input parameters of the toy model .",
    "[ [ low - precision - case ] ] `` low precision '' case : + + + + + + + + + + + + + + + + + + + + + + +    if the gaussian noise is reduced by about a factor of two to @xmath157 at @xmath146 , @xmath158 at @xmath159 , then there are two solutions with the lowest @xmath160 : a false positive with @xmath161 and @xmath162 , @xmath163 , @xmath164 , @xmath165 , @xmath166 , @xmath167 , and a fit which agrees with the input model : @xmath168 with @xmath169 , @xmath170 , @xmath171 , @xmath172 , @xmath173 , @xmath174 .",
    "[ [ high - precision - case ] ] `` high precision '' case : + + + + + + + + + + + + + + + + + + + + + + + +    if the gaussian noise is further reduced to @xmath175 at @xmath146 , @xmath176 at @xmath177 then the lowest @xmath178 does reproduce the input parameters : @xmath179 , @xmath180 , @xmath181 , @xmath182 , @xmath183 , @xmath184 , @xmath185 .",
    "let @xmath186 be the absolute value of the difference of the function @xmath187 of input parameters and the function @xmath188 of fitted parameters @xmath189 a fit @xmath188 with reasonable small @xmath64 implies that the relation @xmath190 roughly holds at most times @xmath15 . in other words , with this statistical error @xmath191 , we can not distinguish the fitted function @xmath188 from the original @xmath187 .     the relative errors of three data sets of increasing precision ( `` lowest '' , `` low '' , and `` high '' ) are plotted with points .",
    "the curved solid ( black ) curve is the plot of the function @xmath192 , which is the relative difference between original function @xmath193 and the false - positive fitted function @xmath194 from the data set ( `` lowest precision '' ) . ]    from fig .",
    "[ fig : relativeerror ] we see that the statistical errors of the data set ( `` lowest precision '' ) are all larger than the difference of the two functions in the full time range .",
    "this is the reason why the false positive can be chosen by the fit routine .",
    "the dashed ( blue ) straight line ( @xmath195 ) and the dotted ( red ) straight line ( @xmath196 ) intersect with the curve of relative errors at @xmath129 and @xmath133 , respectively .",
    "this shows that the ground state dominates in the range @xmath197 , the first - excited state plays a role in the range @xmath198 , and the three states should all be included in the time range @xmath199 . when the precision is higher , the statistical errors are smaller than the difference of two functions , in some time range , so that the fit can reject the false positive @xmath200 .    returning to the false positive of the `` lowest precision '' case ,",
    "notice that when forced to fit to a three - state model , the fit prefers a two - state solution with the one weight essentially zero , and the remaining weight consistent with the sum of the true second and third weights and the remaining mass interpolating between the true masses .",
    "in short , the fit is content with averaging the two higher states into one .",
    "also notice that it is the second weight which is zero .",
    "this is because , at the intermediate stage , a two - state model was forcing a fit to data completely dominated by a single state .",
    "subsequently , as earlier times were added , the data could be described by two states , but since the would - be second state weight was by now constrained near zero , its role was supplanted by the third state in the model .    that is , by introducing the new ( second ) state into the model before the data was precise enough to discern it , spurious results were obtained .",
    "this suggests the cure : postpone the introduction of new states in the model until the data demands it ( as evidenced by a sudden increase in the @xmath149 ) .    in more detail ,",
    "if the time range is large enough there exists a largest time @xmath129 beyond which the contamination of higher states will be obscured by the statistical errors and can be neglected so that in the time range @xmath197 the ground state dominates and the data can be well described by the single exponential @xmath201 . thus the time range @xmath197 is a good choice for the first step of seb , and we can get a reasonably small @xmath64 over this ground - state effective - mass plateau . if we include more time slices with @xmath134 , the contribution of the first excited state becomes significant , but the second excited state contributes little to the correlation function in this range .",
    "the presence of the first excited state results in a noticeable increase of the @xmath64 if we continue to force a single exponential as a fit model ; the inclusion of one more term in the fit model is necessary .",
    "continue this procedure until the time - slices are exhausted .    _ the penultimate algorithm : `` not - too - soon '' _    1 .",
    "the available data are in the time range @xmath202 .",
    "2 .   from effective mass plots , choose an initial time range @xmath203 $ ] to do an unconstrained one - mass fit .",
    "use `` scanning ''  see sect .",
    "[ sect : scanning ] .",
    "3 .   include one more time slice and repeat an ( independent ) unconstrained one - mass fit , and monitor the fitted parameters @xmath204 and the @xmath64 .",
    "4 .   if the fitted parameters and the @xmath64 do not change much , include one more time slice and repeat the previous step .",
    "this iteration stops if there is a noticeable change of @xmath64 and the values of the fitted parameters indicating a breakdown of the one - state model .",
    "then set @xmath205 equal to the time at which the @xmath64 jumps , indicating the necessity of a two - mass fit for @xmath206 .",
    "set the priors for the ground state mass and weight equal to the fitted values from the last low@xmath64 fit over @xmath207 $ ] .",
    "5 .   include one more mass term in the fit model and do a partially - constrained two - mass fit ( with scanning ) for @xmath208 $ ] .",
    "the ground state priors are fixed at the values determined by the previous step .",
    "the first - excited state is unconstrained but scanned .",
    "repeat , adding time slices until the two - state model breaks down as indicated by a jump in the @xmath64 . then set @xmath209 equal to the time at which the @xmath64 jumps , indicating the necessity of a three - mass fit for @xmath199 . set the priors for the ground state and first - excited state mass and weight equal to the fitted values from the last low@xmath64 fit over @xmath210 $ ] .",
    "note that the ground - state priors are refreshed .",
    "repeat , adding more time slices and more fit - model terms ( one at a time and only when necessary ) and more until all time slices are used . 8 .",
    "the highest state in the fit model will be absorbing all the contributions from higher states in the true function , and thus its fitted parameters will differ from the true values .",
    "thus the highest state in the fit model must be rejected .",
    "now we return to the artificial data from the toy model `` lowest precision '' . recall that when we assume a three - mass fit and independently try all pairs @xmath144",
    ", we obtained a false positive .",
    "but the statistical errors are so large that they obscure the contributions from excited states at all but the earliest time slices . adding terms to the fit model",
    "prematurely led to spurious results .",
    "now we apply the new method outlined immediately above to the same data .",
    "a one - state fit works well for @xmath211 $ ] until the @xmath0 suddenly jumps at the ( proposed value of ) @xmath212 .",
    "thus we set @xmath213 .",
    "the values of the one - mass term fit over time range @xmath214 $ ] are used to set priors for @xmath215 and @xmath216 , and a second term is added to the fit model .",
    "the two - term model works well for a fit in the range @xmath217 $ ] with @xmath133 varying from 3 to 0 .",
    "the two - mass model fit the data very well ( @xmath218 ) in the whole time range with fitted parameters @xmath219 , @xmath220 , @xmath221 , @xmath222 .",
    "since the two - mass fit exhausts all the time slices and does not show a noticeable increase of @xmath64 , we do nt go ahead to include the third mass term in the fit model .",
    "but this fit is a `` false positive '' ! since the statistical errors are larger than the difference between the false positive and the true solution ( eq .  [ deltag ] ) , the false positive",
    "can not be rejected on the basis of its @xmath64 , by this or any other method .",
    "so what have we gained ?",
    "the difference this time is that this solution is exposed as a two - mass - term fit rather than a spurious three - term fit . and",
    "as always , the highest state in the fit must routinely be dropped since in general it can be contaminated by contributions from higher excited states .",
    "we conclude that with the `` lowest precision '' data set , we can not get an unambiguous estimate of the second and the third state .",
    "the fit is comfortable predicting the ground state parameters only ( and they are correct ) .",
    "thus the algorithm has not made a `` false positive '' claim .",
    "now , we illustrate this technique with another set of artificial data for which it will be possible to extract excited states . it will be instructive to monitor at the behavior of the @xmath64 , but also to see how the fit parameters of each term in the fit model stabilize as time slices are added .",
    "[ fig : toychisq1 ] ( left ) plots the @xmath64 for data artificially constructed as a sum of four decaying exponentials .",
    "a one - term fit is adequate for time slice 13 ( @xmath223 ) .",
    "but it is exposed as being entirely inadequate as time slice 12 is added to the data set to be fit .",
    "( the @xmath64 jumps tremendously because we have made the statistical errors on the data to be fit quite small , to illustrate the point . )",
    "so for time slices 6 - 12 a two - term fit model is used , and the fit is quite adequate . at time slice 5 , however , the @xmath64 jumps tremendously , exposing the two - term fit as inadequate .",
    "thus a third term is added at time slice 5 .",
    "the three - term fit works down through time slice 2 . at time slice 1",
    ", a fourth term must be added to keep the @xmath64 from being unreasonably large .",
    "behavior of the @xmath64 of the fit as earlier time slices are added to the fit range .",
    "a sudden jump indicates that the fit model is inadequate and that a new mass term should be added to the model .",
    "accordingly , then the @xmath64 drops only to increase again as further time slices are added .",
    "`` not - too - soon '' addition of terms to the fit model prevents a single state being erroneously identified as two .",
    "also shown are the behavior of the ground and first few excited states , which stabilize as more terms are added .",
    ", title=\"fig : \" ] behavior of the @xmath64 of the fit as earlier time slices are added to the fit range .",
    "a sudden jump indicates that the fit model is inadequate and that a new mass term should be added to the model .",
    "accordingly , then the @xmath64 drops only to increase again as further time slices are added .",
    "`` not - too - soon '' addition of terms to the fit model prevents a single state being erroneously identified as two .",
    "also shown are the behavior of the ground and first few excited states , which stabilize as more terms are added .",
    ", title=\"fig : \" ]    meanwhile , fig .",
    "[ fig : toychisq1 ] ( right ) monitors the masses of the lowest few states as more time slices are added to the data set and more terms are added ( as necessary ) to the fit model . at time slice 13 ,",
    "the one - term model is fitted with a ground - state mass ( open circle ) .",
    "( its value is consistent with the value obtained much later when time slices 112 have been added to the fit model and three more terms are added to the fit model .",
    "that is , time slices above 13 are sufficient to determine the ground - state mass , i.e the effective mass plot has `` plateaued '' .",
    "but more importantly for the efficacy of the algorithm , this fitted value remains stable later as the data and fit model are expanded . ) at time slice 12",
    ", the second term is added to the fit model ( solid diamonds ) . as each earlier time",
    "is added , the first - excited state mass changes by a sigma or so , as the augmented data is refit .",
    "this is important .",
    "it is saying that if a relatively small amount of data from time slices 12 and above suggested a somewhat inaccurate guess for the prior for this mass , then subsequent fits including data at time slices 8 - 11 can influence the value and bring it toward the correct value ( at the horizontal line ) . by time slice 8 ,",
    "the first excited state mass has stabilized ( at the correct value ) and will not subsequently deviate through time slice 6 . at time slice 5 ,",
    "the first - excited state mass deviates from the correct answer .",
    "but this is where the jump in the @xmath64 warns that the two - state fit model is inadequate ; the first - excited state fitted mass is contaminated from higher states in the data . adding a third term returns the fitted first - excited state mass to its correct value .",
    "it remains stably at this value with the addition of earlier time slices 24 to the three - state fit model ( open triangles ) . before the second - excited state has stabilized , we have run out of earlier time slices to add , and we can make no claim as to whether its fitted value is correct .",
    "( it is nt . ) but the algorithm has successfully fitted the first - excited state ( and of course the ground state ) .",
    "same as in fig .",
    "[ fig : toychisq1 ] but the artificial data is less precise ( 1% ) .",
    "accordingly , the @xmath64 does not jump to such extreme values when the number of the terms in the fit model becomes inadequate.,title=\"fig : \" ] same as in fig .",
    "[ fig : toychisq1 ] but the artificial data is less precise ( 1% ) .",
    "accordingly , the @xmath64 does not jump to such extreme values when the number of the terms in the fit model becomes inadequate.,title=\"fig : \" ]    figs .",
    "[ fig : toychisq2 ] and  [ fig : toychisq3 ] tell a similar story for artificial data which is less precise ( 1% and 5% respectively ) . the @xmath64 jump ( indicating that another term should be added to the fit model ) but , as one would expect , not as extremely as for the more precise data of figs .",
    "[ fig : toychisq1 ] .",
    "same as in fig .",
    "[ fig : toychisq1 ] but the artificial data is even less precise ( 5%).,title=\"fig : \" ] same as in fig .",
    "[ fig : toychisq1 ] but the artificial data is even less precise ( 5%).,title=\"fig : \" ]      as we saw in the last section , the `` not - too - soon '' algorithm avoids `` false positives '' by delaying the addition of a new term to the fit model before the data were able to discern it .",
    "however , there is the logical possibility of the opposite danger : if the new term is added too late , then there could potentially be an erroneous @xmath224-state fit of data which is in fact better described by @xmath225 states .",
    "if so , then the highest two states will be `` averaged '' into one .",
    "( so , for example , a fitted first excited state mass might take a value intermediate between the true first and second excited state masses . )",
    "the addition of further states to the fit model might then cover up this misidentification , and allow for this different kind of false positive .",
    "one would think that an increased @xmath64 would warn of this danger ; however , since the @xmath64 is an average over many time slices , it s warning may come a time slice or two too late . if the data is such that a new term is discernible every couple of slices , then this may indeed lead to erroneous results .",
    "ratio of the contribution to the fit model from a proposed new term , @xmath226 , compared to the statistical error @xmath227 ) at the newest time slice @xmath228 added to the data set .",
    "the new term is deemed not needed at time slices @xmath229 and @xmath230 . at time slice @xmath231 , the ratio exceeds @xmath232 and the new term is conditionally accepted . the ratio remains above @xmath232 for time slice @xmath233 and below , confirming that the new term is needed .",
    "the data is from a fit of the roper resonance at the same quark mass as in fig .",
    "[ fig : roper_stability_previous ] . ]    a final tweak of the algorithm addresses this issue : the `` just - in - time '' algorithm is the same as the `` not - too - soon '' algorithm outlined in the last section , except that the previous criterion for adding a new term to the fit model , namely that without this new term the @xmath64 would suddenly increase , is replaced by a more sensitive criterion : as a new time slice @xmath228 is added to the data set , a tentative fit is made with a proposed extra term , \\{@xmath234,@xmath235}. then the contribution of this fitted term is compared to the statistical error at the new time slice @xmath236 ( see fig .",
    "[ fig : relativecontribution ] . ) if eq .",
    "[ acceptcriterion ] does not hold , then the new term is deemed not needed .",
    "if it does hold , then the new term is accepted ( but only conditionally since the equation may hold only by statistical fluctuation ) .",
    "as new time slices are added , the new term must continue to pass this test .",
    "( this is stable : typically , if it passes the test once , it is likely to continue to pass the test at earlier time slices , since for these the relative error gets smaller and the fractional contribution gets larger . )",
    "if the test is failed before a higher - order term is conditionally added , then one returns to time slice @xmath228 and it is deemed that the @xmath224th term is not included in the fit . then time @xmath237 is added to the data , a new proposal is made to add the @xmath224th term , and the process continues .",
    "we have presented a list of algorithms with steadily increasing complexity but with steadily increasing robustness .",
    "the earliest and simplest `` fixed @xmath238 '' algorithm , most easily described and presented first here as a template , was originally used ( prematurely in retrospect ) for some preliminary conference presentations .",
    "we now deem it too unsophisticated and do not use it anymore .",
    "the plain vanilla `` variable @xmath52 '' algorithm is adequate provided that the level spacings are well separated and each term is saturated by several time slices .",
    "it is perfectly adequate to describe , for instance , the pion ground and excited state , as explained in sect .",
    "[ sect : pion ] .",
    "the plain `` variable @xmath52 '' was used in the first version of our roper paper and caused some erroneous results at high quark mass .",
    "this has been rectified by the use of the `` just - in - time '' algorithm for the updated version of our roper paper and is briefly described in sect .  [",
    "sect : roper ] .      on a @xmath239 lattice ,",
    "we use the overlap fermion  @xcite and the iwasaki gauge action  @xcite with @xmath240 .",
    "the lattice spacing , determined from the measured pion decay constant @xmath241 , is determined to be @xmath242 , and thus the lattice has spatial size of @xmath243 .",
    "we adopt the following form for the massive dirac operator  @xcite @xmath244 where @xmath245 so that @xmath246 where @xmath247 is the matrix sign function and @xmath248 is taken to be the hermitian wilson - dirac operator , i.e. @xmath249 . here",
    "@xmath250 is the usual wilson fermion operator , except with a negative mass parameter @xmath251 in which @xmath252 .",
    "we take @xmath253 in our calculation which corresponds to @xmath254 .",
    "the massive overlap action is defined so that the tree - level renormalization of mass and wavefunction is unity .",
    "we adopt the zolatarev implementation  @xcite of the optimal rational approximation  @xcite to approximate the matrix sign function .",
    "the inversion of the quark matrix involves nested do loops in this approximation .",
    "further details of the procedure are given elsewhere  @xcite .",
    "two - point correlation function @xmath255 for the pion for three bare quark masses . ]",
    "figure  [ fig : picor ] shows the correlation function for the correlator @xmath123 .",
    "one can see that it is dominated by the ground state of the pseudoscalar channel ( pion ) over all but the few earliest time slices .",
    "this presents a problem for the default `` fixed @xmath52 '' approach . referring back to the algorithm of section  [ sect : basicalgorithm ] ,",
    "we see that at each step of the algorithm , @xmath52 new time slices are added to the data while a new excited state is added to the fit model .",
    "thus for the pion correlator , one would be trying to fit to a model with many states when the data is saturated by just the ground state . forcing",
    "a fit may give misleading estimates of excited state parameters which are subsequently used as priors .",
    "with the `` variable @xmath52 '' refinement of the algorithm , rather than deciding _ a priori _ on the number of terms in the fit and adding time slices a fixed number at a time , one lets the data decide how many time slices to include with each enlargement of the data by choosing the minimum @xmath85 over a range of reasonable possibilities .",
    "thus since the pion correlator is dominated by the ground state for many time slices , then many time slices will be automatically added before an attempt is made to fit the first - excited state .",
    "in fact , we find that the `` variable @xmath52 '' method works very well for the pion correlators . figure  [ fig : pionmass ] shows the results of the fit , the ground- and first - excited - state weight and mass as a function of the bare quark mass .",
    "ground and first - excited state pion mass @xmath105 , as a function of the bare quark mass @xmath118 ( left ) . ground and",
    "first - excited stated pion weight , as a function of the bare quark mass @xmath118 ( right ) .",
    "notice that the ground state weight does not diverge as the quark mass approaches zero for this @xmath123 correlator as it would for the @xmath256 correlator , where @xmath257 is the pseudoscalar density.,title=\"fig : \" ] ground and first - excited state pion mass @xmath105 , as a function of the bare quark mass @xmath118 ( left ) .",
    "ground and first - excited stated pion weight , as a function of the bare quark mass @xmath118 ( right ) .",
    "notice that the ground state weight does not diverge as the quark mass approaches zero for this @xmath123 correlator as it would for the @xmath256 correlator , where @xmath257 is the pseudoscalar density.,title=\"fig : \" ]      studies using standard curve fitting have heretofore failed to satisfactorily identify the `` roper resonance '' @xmath258 of the nucleon on the lattice  @xcite .",
    "our analysis is the first lattice calculation to obtain the masses of the roper and @xmath259 at low quark masses well in the chiral regime ( with a pion mass as low as @xmath260 ) .",
    "however , the effects of quenched artifacts , specifically the presence of ghost @xmath261 states , complicates the physics as well as the functional form of fit model , and so the details of the calculation are presented elsewhere  @xcite . to be sure , our calculation benefits from going to unprecedented low quark mass with the full chiral symmetry provided by overlap fermions , but the sequential empirical bayes method plays a crucial role .",
    "it also presented a challenge and led to the refinement of the seb from the `` variable @xmath52 '' version to the `` just - in - time '' version .",
    "the first version of our roper results  @xcite used the `` variable @xmath238 '' method which , as we ve seen , had until then been perfectly adequate to expose excited states ( such as for the pion in sec .  [",
    "sect : pion ] ) where the density of excited states is not too high . compared to our final analysis with the `` just - in - time '' version  @xcite , the results for the nucleon and @xmath262 did not change .",
    "the only change is for the roper state at medium and heavy quark masses .",
    "indeed our first results at medium - heavy masses passed the `` stability plot '' test ( fig .",
    "[ fig : roper_stability_previous ] ) and our focus returned to the more interesting light quark mass regime . here",
    "the `` variable @xmath52 '' criterion for when to add new states remains perfectly adequate for pion masses below @xmath263 .",
    "stability test for the roper resonance at bare quark mass @xmath264 ( @xmath265 ) .",
    "priors had previously been selected by the plain `` variable @xmath52 '' version of the sequential empirical bayes method .",
    "now they are used in the standard constrained fit .",
    "the figure shows fit values for the lowest two masses ( nucleon and roper ) from constrained fits with different numbers of terms in the fit model .",
    "( at this mass , ghost states make no contribution , and so the fit model contains only exponentials . ) ]    subsequently , we came to realize that the `` variable @xmath52 '' criterion that we adopted to introduce the excited state was not adequate for medium and heavy quarks , although it is adequate for the light quarks . in fitting someone else s proprietary charmonium data",
    ", we realized that the crucial difference between the heavy quark spectrum and that of the light is that the ratio of the excited state mass to that of the ground state is much smaller in the heavy system , that is , the excitation spectrum in the heavy quark system is more dense .",
    "the earlier criterion erroneously resulted in obtaining the average of the higher excited states as the first excited state for the heavy system . in view of this",
    ", we devised the new and final `` just - in - time '' criterion ( sec .",
    "[ sect : finalalgorithm ] ) , wherein a new term is added if its contribution is statistically significant at the current time slice .",
    "in fact , the penultimate `` not - too - soon '' algorithm ( sec .  [ sect : finalalgorithm ] ) , where the criterion for adding a new term to the fit model depends on a sudden jump in the @xmath75 , works just as well as `` just - in - time '' for the roper resonance at both medium - high and low quark masses .",
    "but for some charmonium data , or other test data constructed with a dense excited - state spectrum , `` just - in - time '' is somewhat better than `` not - to - soon '' , and because of its potential theoretical advantages is our method of choice .",
    "[ fig : roper_stability ] shows that our final algorithm passes the `` stability test '' .",
    "however , so did the earlier `` variable @xmath52 '' algorithm .",
    "this and the fact that the extracted excited state mass changed alarmingly means that the `` stability test ''  @xcite is only a necessary but not a sufficient test for the reliability of constrained - curve fitting .",
    "we emphasize , however , that the source of the discrepancy has been identified and cured ; the final algorithm can robustly protect against the averaging of excited states .",
    "same as for fig .",
    "[ fig : roper_stability_previous ] but for our best and final `` just - in - time '' algorithm . ]      in the quenched approximation , there are artifacts associated with the absence of quark loops .",
    "one of the more interesting consequences is that the would - be @xmath266 propagator involves only double @xmath111 poles in `` hairpin diagrams '' .",
    "these lead to the chiral - log terms contributing to hadron masses ; we see these clearly in our recent lattice calculation of pion and nucleon masses  @xcite .",
    "another quenched artifact is the contribution of ghost states in the hadron propagators as first seen in the @xmath1 meson channel  @xcite where the ghost @xmath267-wave @xmath268 state lies lower in mass than the @xmath1 ( for sufficiently small quark mass ) .",
    "we have seen a similar effect in analysis of the excited nucleon spectrum where a @xmath257-wave @xmath261 lies close in mass to the roper resonance  @xcite , and its presence must be carefully disentangled by the fitting code .",
    "more dramatically , in the negative - parity channel @xmath262 ( @xmath269 ) , the lowest @xmath267-wave @xmath261 state has a mass lower than that of the @xmath262 for small quark mass .",
    "since the @xmath111@xmath111 coupling in the hairpin diagram is negative  @xcite , the @xmath262 correlator changes sign with increasing time separation  @xcite .",
    "this effect is only seen at small enough quark masses , and thus is not seen in most lattice simulations at much higher masses .    as a result ,",
    "the form of the fitting function changes ; there are extra non - exponential terms .",
    "nevertheless , the seb can successfully handle this situation .",
    "it is crucial to enforce the physical constraint that the weight of the ghost state be negative ; otherwise there would be no way to distinguish it from the physical state ( e.g. roper ) lying nearby .",
    "the details of the fitting model and the results of the fitting ( using the `` just - in - time '' seb algorithm ) for the roper and @xmath262 are in  @xcite .",
    "@xmath1 correlators for our lowest quark mass for which @xmath270 ( left ) and @xmath271 ( right ) .",
    "the negative dip of the correlators is an indication of the domination of the ghost @xmath267-wave @xmath268 state over the physical @xmath272 .",
    "the curves are contributions to the fit model and are explained in the text.,title=\"fig : \" ] @xmath1 correlators for our lowest quark mass for which @xmath270 ( left ) and @xmath271 ( right ) .",
    "the negative dip of the correlators is an indication of the domination of the ghost @xmath267-wave @xmath268 state over the physical @xmath272 .",
    "the curves are contributions to the fit model and are explained in the text.,title=\"fig : \" ]    another example is displayed in fig .",
    "[ fig : a0_prop_1 ] where we show the @xmath1 propagator at two low quark masses , for which @xmath270 ( left ) and @xmath271 ( right ) .",
    "the most dramatic feature of the data in each figure is the negative dip of the correlator at time slice 3 and above .",
    "the solid ( red ) line passing through the data is the result of the seb fit .",
    "the two dominant contributions to the fit model are negative and are indicated by the dashed ( green ) line labeled `` term 1 '' and by the solid ( cyan ) curve labeled `` term 2 '' .",
    "they are modeled by the expression @xmath273 where @xmath216 is constrained to be negative and the @xmath274 factor reflects the double - pole nature of the hairpin diagram  @xcite .",
    "we fit @xmath275 , the mass of the interacting would - be @xmath266 and @xmath276 state .",
    "since we work in a finite box , the @xmath277 states are discrete and they are constrained to be near the energy of the two non - interacting particles , each with @xmath278 for discrete lattice momentum @xmath279 where @xmath224 is an integer . for `` term 1 '' , @xmath280 and for `` term 2 '' , @xmath281 .",
    "the contribution of the physical state @xmath1 is positive and is indicated by dotted magenta curve in each figure . in summary ,",
    "our seb algorithm is quite capable of handling non - standard forms of the fit model including negative - normed ghost - state contributions and can fit successfully the data .",
    "we have advocated refinements of bayesian - inspired constrained - curve fitting which we believe better stabilizes fits at low quark mass .",
    "this permits analysis where the values of fit parameters are not reliably known _ a priori _ , such that their use as priors might be dangerously biased .    in the `` sequential empirical bayes method '' ,",
    "we have constructed an automated and natural way of reliably obtaining the priors from naturally - nested subsets of the data ( `` onion shells '' ) . for the prototype described here , a time - dependent two - point correlation function @xmath117",
    ", we first obtain estimates of the ground state mass and weight from unconstrained fits to a subset of data restricted to large times .",
    "these are then used as the priors in a subsequent constrained fit .",
    "a sequence of ( fully or partially ) constrained fits follow . at each step , more data from adjacent earlier times are added and another term is added to the fit model .",
    "when a new parameter is added , its first estimate is obtained from a fit in which it is unconstrained but previously introduced parameters are constrained .",
    "after each fit , the fitted values of each parameter are used as the priors for the next fit .",
    "thus the value of each prior is free to float from one fit to the next , and thus is not prevented from changing significantly over the course of the several steps of the iteration as more and more data are added .",
    "this mitigates any bias which may have been introduced wherein the first estimates of a prior are inaccurate due to statistical fluctuations in the smaller initial data set . in the final step , as the last data are added , no new terms are added to the fit model . a final fit for each parameter leaves it unconstrained ( or very weakly constrained ) with all other parameters constrained by priors set equal to the latest fitted values .",
    "our final bootstrapped errors obtained from `` releasing the constraint '' in this way are larger than for a completely - constrained fit and are selected as a conservative estimate .",
    "we have found some further refinements to be fruitful : ( a ) we use a `` scanning '' technique to automate the selection of an initial value for a new parameter for its first unconstrained fit .",
    "we thus avoid the pitfall of the @xmath85 minimization routine becoming trapped in the attraction basin of a local , but not global minimum .",
    "( b ) the weight factor ( essentially a lagrange multiplier ) , @xmath107 , balances the influence of the data versus the priors in determining the output of a fit .",
    "decreasing @xmath107 from its canonical bayesian value of 1 , can be used to assess the systematic error associated with the choice of prior(s ) .",
    "we advocate absorbing this systematic error into a statistical error by promoting @xmath107 to be `` global dynamical weight '' , that is , a fit parameter with its own prior mean and standard deviation .",
    "we have tested that our algorithm can successful recover the correct fit parameters of an artificial data set , constructed as a sum of decaying exponentials with realistic values of the parameters . for one channel ( corresponding to using only the local - local two - point correlation function ) , the masses and weights of the ground and first - excited states are fit to within a measured standard deviation of the true values .",
    "thus , for our real data , we restrict ourselves to measuring only the lowest two states , even though we use four or five states in the fit model .",
    "this is sufficient for our present purposes .",
    "our method is not in strict accord with the bayesian philosophy , as we use subsets of the data to guide the selection of the priors .",
    "nevertheless , the following mitigates bias from such data snooping : ( a ) the data is naturally nested in that a subset of data restricted to large times can provide a fair estimate of the lowest state parameters .",
    "adiabatically increasing the data set with the introduction of new terms in the fit model gives fair estimates of excited - state parameters to be used as priors .",
    "( b ) the priors are given ample opportunity to change in accordance with the data in the several steps of the algorithm .",
    "is this enough ? to test this we `` partition the data '' by configuration into equally - sized but disjoint sets and apply our algorithm to one half to obtain priors and then use these priors in a standard constrained fit on the other half ( in strict accord with the bayesian philosophy ) and on the full data set .",
    "we see no difference beyond expected statistical errors ; that is , no bias is seen .    in further studies using",
    "artificially constructed data with known means and errors ( `` toy models '' ) we have presented some caveats against the careless application of the algorithm which might result in a state being spuriously identified as a combination of two , leading to a `` false positive '' prediction .",
    "these false positives were avoided in the toy model studies by insisting that a new term be added to the fit model `` just - in - time '' , that is only after a fit with fewer terms is deemed inadequate .    for this pilot study , we have analyzed the ground and first - excited masses and weights for the pion from overlap fermions on a quenched @xmath3 lattice with spatial size @xmath4 and pion mass as low as @xmath282 , commented on the history of the use and suitability of variations of the seb algorithm for extracting the roper resonance  @xcite , and given another example ( @xmath1 ) where the method can handle ghost states ."
  ],
  "abstract_text": [
    "<S> we introduce the `` sequential empirical bayes method '' , an adaptive constrained - curve fitting procedure for extracting reliable priors . </S>",
    "<S> these are then used in standard augmented-@xmath0 fits on separate data . </S>",
    "<S> this better stabilizes fits to lattice qcd overlap - fermion data at very low quark mass where _ a priori _ values are not otherwise known . </S>",
    "<S> lessons learned ( including caveats limiting the scope of the method ) from studying artificial data are presented . as an illustration , from local - local two - point correlation functions , </S>",
    "<S> we obtain masses and spectral weights for ground and first - excited states of the pion , give preliminary fits for the @xmath1 where ghost states ( a quenched artifact ) must be dealt with , and elaborate on the details of fits of the roper resonance and @xmath2 previously presented elsewhere . </S>",
    "<S> the data are from overlap fermions on a quenched @xmath3 lattice with spatial size @xmath4 and pion mass as low as @xmath5 . </S>"
  ]
}