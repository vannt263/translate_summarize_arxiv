{
  "article_text": [
    "the extent to which text makes sense by introducing , explaining and linking its concepts and ideas through a sequence of semantically and logically related units of discourse is called _",
    "coherence_. several models of text coherence exist . on a high level , these models capture text regularities or patterns predicted by a theory or otherwise hypothesised to indicate coherence . for instance , according to early discourse theories @xcite , three core factors of text coherence are ( i ) the discourse purpose ( _ intentional structure _ ) , ( ii ) the specific discourse items discussed ( _ attentional structure _ ) , and ( iii ) the organisation of the _ discourse segments_. of these factors , coherence models have been presented for both intentional structure @xcite , and discourse segments @xcite , but attentional structure has received the most attention and is the basis for most existing models of text coherence .",
    "one approach to attentional structure that has been used extensively in coherence models is _ centering theory _",
    "@xcite ( ct ) , which posits that a reader s attention is centered on a few salient entities in text , and that these exhibit patterns signalling the reader to switch or retain attention .",
    "a widely used application of ct is the _ entity grid _",
    "model  @xcite , which assumes that coherence can be measured from sequences of repeated discourse entities , such as the subject and object of a sentence . in this work",
    ", we use the entity grid as our basis to propose two novel classes of models for document coherence .",
    "our first class of models calculates coherence using the information _ * entropy * _  @xcite of the salient discourse entities in the entity grid",
    ". the information entropy rate of a document can be thought of as the rate at which new information appears as one reads the document .",
    "the main idea is that , as more new information appears , the document becomes less focussed on a single topic , so its coherence decreases .",
    "this assumption , which is already verified in the literature @xcite , is illustrated in figure [ fig : cat ] , which juxtaposes the discourse flow between three entities ( ` cat(s ) , mat(s ) , wet ` ) and six entities ( ` cat , mat , everything , wet , no one , rain ` ) in texts a and b respectively .",
    "as entities are linked between sentences , i.e. repeated , text becomes more topically focussed and hence more coherent .",
    "however , when many new entities are introduced , since they are by definition not linked to previous discourse , the topic of the text becomes less clear and overall coherence decreases .",
    "to the best of our knowledge , discourse entropy has not been used for text coherence estimation before ( there is however work on _ lexical entropy _ for text readability , which we discuss in section [ sec : discoursetheories ] ) .",
    "our second class of document coherence models maps the entity grid to _ * graphs * _ following guinaudeau & strube  , where entities are vertices , linked by various relations between them , e.g. distance or adjacency .",
    "the topologies of these graphs model the flow of discourse .",
    "we investigate whether graph properties , such as the clustering coefficient , or iterative graph ranking algorithms , such as pagerank , can approximate document coherence .",
    "this class of models extends work by guinaudeau & strube that used only a single such metric , namely outdegree , to calculate text coherence .",
    "we present several instantiations of the above two classes of coherence models and we evaluate their effectiveness , first as coherence models per se , and second when integrated to information retrieval ( ir ) . in the first case",
    ", we evaluate our coherence models in the standard _ sentence reordering _ task .",
    "we find that our models are more accurate than two well known baselines in coherence modelling ( one of them being the entity grid  @xcite that we extend ) , even without any parameter tuning or training . in the second case",
    ", we investigate whether factoring coherence in the ir process improves retrieval precision , on the basis that coherence should be a reasonable predictor of relevance .",
    "experiments with standard trec web track data show that reranking retrieval results according to their coherence scores improves retrieval precision , especially for the top 20 retrieved results .    in the rest of the paper , section  [ sec : discoursetheories ] overviews related work ; sections  [ sec : ecohmodel ] &  [ sec : graphcomodel ] present our entropy and graph based coherence models",
    "respectively ; section  [ s : eval ] discusses the experimental evaluation of these models , and section [ s : discussion ] their limitations and future extensions .",
    "sections  [ s : conc ] summarises our conclusions .",
    "[ sec : discoursetheories ] we overview the natural language processing ( nlp ) literature on models of text coherence ( section [ ss : localglobal ] ) , focussing in particular on those using discourse entities ( section [ ss : disc ] ) .",
    "we also discuss related work on applying text coherence to improve nlp tasks and ir ( section [ ss : quality ] )",
    ".      a text can be coherent at a _ local _ and _ global _ level  @xcite .",
    "_ local _ coherence is measured by examining the similarity between neighbouring text spans , e.g. , the well - connectedness of adjacent sentences through lexical cohesion @xcite , or entity repetition @xcite . _ global _ coherence , on the other hand , is measured through discourse - level relations connecting remote text spans across a whole text , e.g. sentences @xcite .",
    "there is extensive work on local coherence that uses different approaches , including bag of words methods at sentence level @xcite , sequences of content words ( of length @xmath1 ) at paragraph level @xcite , local lexical cohesion information @xcite , local syntactic cues @xcite , and combining local lexical and syntactic features , e.g. , term co - occurrence @xcite .",
    "overall , various aspects of ct have long been used to model local coherence @xcite , including the well - known entity approaches that rank the repetition and syntactic realisation of entities in adjacent sentences @xcite .",
    "there is also work on _ global _ coherence , focussing on the structure of a document as a whole @xcite .",
    "however , not many coherence models represent both local and global coherence , even though those two are connected : local coherence is a prerequisite for global coherence  @xcite , and there is psychological evidence that coherence on both local and global levels is manifested in text comprehension @xcite . among the few models that capture both local and global text coherence",
    "is the sentence ordering model of zhang : on a local level , sentences are represented as concept vectors where concepts are equivalent to content words ; on a global level , sentences are represented as vertices , and sentence relations as edges in a document graph .    our work captures both local and global coherence as explained in sections [ sec : ecohmodel ] & [ sec : graphcomodel ] , which practically means that it can model document coherence both in the simple case of capturing adjacent discourse transitions like those illustrated in figure [ fig : cat ] , but also in the more complex ( yet more realistic ) case of capturing non - adjacent discourse transitions like those illustrated in figure [ fig : natfmetrics ] .",
    "the basis of our two coherence models is the entity grid @xcite , which represents a document by its salient discourse entities and their syntactic roles , see table [ tbl : examplegrid ] for example . the entity grid indicates the location of each discourse entity in a document , which is important for coherence modelling because mentions of an entity tend to appear in clusters of neighbouring sentences in coherent documents .",
    "this last assumption is adapted from ct where consecutive utterances are regarded as more coherent if they keep mentioning the same entities  @xcite .",
    "there are several extensions and variations of the entity grid , including adaptations for german @xcite , coupled with extensions integrating high level sentence structure @xcite ; extensions integrating writing quality features ( e.g. word variety and style indicators ) @xcite ; extensions of the original entity grid ( which captures sentence - to - sentence entity transitions ) to capture term occurrences in sentence - to - sentence relation sequences @xcite ; extensions focussing on syntactic regularities @xcite ; a _ topical entity grid _ that considers topical information instead of only discourse entities @xcite ; and extensions incorporating modifiers and named entity types into the entity grid to distinguish important from less important entities @xcite . a recent extension @xcite maps the entity grid into a graph where coherence is calculated as the average outdegree of the nodes in the graph .",
    "while this model is only presented as a local coherence model , it does represent both local and global coherence : entities are represented as nodes in a graph representing the document , allowing to connect entities occurring in non - adjacent sentences , hence spanning globally in the document .",
    "the coherence models we present in this paper employ the entity grid of @xcite , but use several novel computations based on this grid : first entropy , and second an extension of the work of guinaudeau & strube   with several more complex graph metrics than outdegree .",
    "apart from being an interesting problem in itself , coherence models and hence coherence prediction are important to a variety of nlp tasks and applications , such as summarisation @xcite , machine translation @xcite , separating conversational threads @xcite , text fluency / reading difficulty detection  @xcite , grammars for natural language generation @xcite , genre classification @xcite , sentence insertion @xcite and sentence ordering @xcite . in ir in particular ,",
    "the document coherence scores produced by our models can be seen as similar to several document quality measures that have been used in the past to improve retrieval .",
    "examples of document quality include , for instance , document complexity , which mikk @xcite predicts using a corrected term frequency measure based on word commonness . in the context of web search in particular ,",
    "document quality has been studied extensively . for instance , bendersky et al .",
    "@xcite estimate the quality of web documents by features indicating readability , layout and ease - of - navigation , such as : the number of terms that are rendered visible by a web browser ; the number of terms in the title ; the average number of characters of visible terms on the page ( used also as an estimate of readability by @xcite ) ; the fraction of anchor text on the page ( used also as discriminative of content by @xcite ) ; the fraction of text that is rendered visible by a web browser , compared to the full source of the page ( also known as information - to - noise ratio and used as feature of document quality in @xcite ) ; the stopword / non - stopword ratio of the page ; and the lexical entropy of the page , computed over the terms occurring in the document .",
    "in fact , bendersky et al . use this type of lexical entropy as an estimate of document cohesiveness , reasoning that documents with lower entropy will tend to be more cohesive and more focussed on a single topic . this inversely proportional relation between text entropy and its cohesiveness is also used in our model ; the difference is that we compute the discourse entropy ( entropy of discourse entity @xmath0-grams ) , whereas bendersky et al .",
    "compute the lexical entropy ( entropy of individual words ) .",
    "tan et al .",
    "@xcite also present a model of text comprehensibility using lexical features , such as bag of words , word length and sentence length , to approximate semantic and syntactic complexity .",
    "they build a classifier that uses these features to assign a comprehensibility score to each document , and then rerank retrieved documents according to this comprehensibility score .",
    "we also rerank results using our coherence scores , and like @xcite , find this to be effective .",
    "the difference of our work to that of tan et al .",
    "is that ( i ) our coherence scores are not produced by a classifier but they are computed either as entropy or graph centrality approximations without tuning parameters ; and ( ii ) we do not use lexical frequency statistics , but solely discourse entities , e.g. subject , object , which we consider better approximations of semantic complexity than lexical frequency features .",
    "[ sec : ecohmodel ] our first coherence model uses information entropy .",
    "entropy has been used in various areas e.g. , lossless data compression  @xcite , or cryptography  @xcite .",
    "see berger et al .   for an older but comprehensive introduction to entropy for nlp .",
    "information entropy is the expected value of the information content of a random variable . if a document is seen as a sequence of @xmath2 i.i.d .",
    "events @xmath3 , the entropy is , roughly , a measure of the average `` surprise '' of observing an event @xmath4 .",
    "for example , if all events occur with equal probability , the entropy is _ high _ ; however if a single event occurs much more frequently than others , the entropy is _ low _ ( the average surprise is low as a single event will occur very often , as expected ) .    for a positive integer @xmath0",
    ", we may consider the probability @xmath5 of observing event @xmath4 given the preceding @xmath0 events . in this case , the entropy , roughly , measures the average surprise of seeing event @xmath4 given the history of @xmath0-preceding events ( a so - called @xmath0-_gram _ ) . in particular ,",
    "if the events are _ discourse entities _ , _ low _ entropy will occur if only a few distinct discourse entities can occur , on average , after each @xmath0-gram of other discourse entities .",
    "in contrast , if new discourse entities are being introduced throughout the text independently of the preceding entities , e.g. , recall the example in figure [ fig : cat ] , high entropy will occur .",
    "based on these observations , we compute a document coherence score as the reciprocal entropy of a random variable constructed from the probability of discourse entities in the document .",
    "we next describe how we compute this probability of discourse entities ( section [ ss : prob ] ) and the final coherence score per document ( section [ ss : score ] ) .",
    "we build an entity grid as per barzilay and lapata : the rows correspond to the sentences of a document @xmath6 , in order , and each column corresponds to a salient entity occurring in the sentence , in order of their occurrence in @xmath6 .",
    "the entry in each row and column of the grid is the syntactic role of the corresponding salient entity .",
    "we use the syntactic roles of ` subject ` ( * s * ) and ` object ` ( * o * ) because they are the most important discourse items , denoting respectively the _ actor _ and the _ entity that is acted upon_. table  [ tbl : examplegrid ] displays an example of the entity grid built from an excerpt of hemmingway s ` the old man and the sea ` . for instance ,",
    "` man ` is a discourse entity occurring as a subject in sentences @xmath7 , @xmath8 , and @xmath9 .",
    "we propose to measure coherence using entropy in the following way . from the entity grid",
    ", we extract @xmath0-grams of entities in the order they occur in the text , i.e. per row in the grid .",
    "we then compute the probability of each entity @xmath0-gram using the standard maximum likelihood language modelling frequency approximations ( more elaborate approximations are also possible , see for instance @xcite , but we choose this for simplicity in this preliminary work ) .",
    "that is , for an 1-gram , we compute the probability @xmath10 of a discourse entity @xmath11 in document @xmath6 as : @xmath12 where @xmath13 is the frequency of @xmath11 in @xmath6 , and @xmath14 is the total frequency of all discourse entities in @xmath6 .",
    "similarly , for a 2-gram , we compute the probability @xmath15 of entity @xmath11 following entity @xmath16 as : @xmath17 where @xmath18 is the number of times that entity @xmath11 occurs as the first entity after entity @xmath16 in @xmath6 .",
    "the resulting probability distributions may be smoothed by standard methods ( e.g. , dirichlet or good - turing ) . for simplicity",
    "we do not do so in this basic model .",
    "we use these probabilities to compute entropy as explained next .",
    "formally , the entropy , @xmath19 , of a discrete random variable variable @xmath20 with sample space @xmath21 is : @xmath22 where @xmath23 for all @xmath24 .",
    "we model each document as a markov process generating discourse entities . for a @xmath25-order markov process",
    ", the probability of generating a discourse entity @xmath11 depends solely on the preceding @xmath0-gram of discourse entities . using equation  [ eqn : orgentropy ] it is straightforward to obtain explicit expressions for the entropy of the probability distribution generating events .",
    "for instance , for @xmath26 , the resulting expressions are , respectively : @xmath27 where @xmath21 is the set of all discourse entities @xmath28 in the document , @xmath10 is computed using equation [ eq:1gram ] , and @xmath15 is computed using equation [ eq:2gram ] .",
    "subscripts @xmath29 denote the order of the entropy model , which increases as the value @xmath0 of the @xmath0-gram increases .",
    "our document coherence score is then the reciprocal of this entropy value : @xmath30 to boost low entropy scoring documents according to our assumption that coherence and entropy are inversely related .",
    "the value of @xmath31 is always non - negative , but has no upper bound .",
    "so we normalise it as follows : @xmath32 where @xmath33 is the maximum value of @xmath31 across all documents in the collection for a @xmath25-order model .",
    "the division by @xmath33 is a simple normalisation of the coherence scores of the documents in the collection .",
    "other normalisation approaches can also be used , which , if parameterised , can result in better performing models than the ones we report . in this preliminary work",
    ", we use this basic normalisation .",
    "in addition , different values of @xmath25 give different orders of the markov process , hence different coherence model instantiations , and hence distinct coherence scores . in this work ,",
    "we experiment with 0-order , 1-order and 2-order models , as explained in section [ s : eval ] .",
    "our entropy coherence model captures only local coherence through entity transitions occurring within sentences .",
    "we next present a family of models that capture both local and global coherence through entity transitions between both adjacent and non - adjacent sentences in a document .",
    "we represent a document @xmath6 as a directed bipartite graph where each node in the first partition is a sentence , and each node in the second partition is a discourse entity .",
    "there is an edge from a sentence - node to an entity - node iff the sentence contains the entity .",
    "this representation was first suggested by guinaudeau & strube  . using this directed bipartite graph",
    ", we can build an undirected graph whose nodes are sentences and where there is an edge between two distinct nodes iff they share at least one common entity .",
    "this undirected graph can be unweighted or weighted ; if weighted , the weight can reflect salient properties of the document and sentences , e.g. the distance of the sentences in the text .",
    "an example of a bipartite graph and its corresponding undirected graph is shown in figure  [ fig : bipartitegraph ] .",
    "the intuition is that as these graphs model relationships between sentences and entities , properties of the graphs will reflect the coherence of a document , as in coherent text , sentences occurring close to each other should have closely related entities .",
    "guinaudeau and strube use only the outdegree of such graphs constructed from discourse entities to calculate a coherence score .",
    "note that while guinaudeau and strube write that their model only takes _",
    "coherence into account , the bipartite graph can also be used to reason about _ global _ coherence , as it models connections between non - adjacent sentences .",
    "we extend the approach of guinaudeau and strube by experimenting with a number of other graph metrics that capture various aspects of the topology of the graphs ( and accordingly , we conjecture , the narrative flow of the text ) .",
    "our methodology consists of two main steps : ( i ) we build , for each @xmath6 , a discourse entity graph ( section [ ss : build ] ) , and ( ii ) we compute our proposed graph topology measures and use these as coherence scores for each document @xmath6 ( section [ ss : metrics ] ) .",
    "we next describe these steps in detail .      for a document @xmath6 containing @xmath2 sentences @xmath34 ,",
    "we denote by @xmath35 the labelled directed bipartite graph with @xmath36 and @xmath37 nodes where @xmath38 , @xmath39 , @xmath40 .",
    "@xmath41 consists of ordered pairs of nodes from @xmath42 .",
    "a node @xmath38 denotes a sentence and @xmath39 an entity found in the entity grid that can be shared by multiple nodes in @xmath43 .",
    "@xmath44 is called the _ bipartite graph _ of @xmath6 .",
    "the labelling of the nodes is used to retain the _ order _ in which the sentences represented by nodes occur in @xmath6 .",
    "we denote by @xmath45 the undirected graph with @xmath43 nodes and @xmath28 edges where @xmath46 are nodes in @xmath43 and @xmath47 is an edge between pairs of nodes @xmath46 with a non - zero real - valued weight .",
    "a node @xmath38 denotes a sentence in @xmath6 and an edge @xmath47 between nodes @xmath48 corresponds to an entity shared by @xmath49 and @xmath50 .",
    "@xmath51 is called the _ projection graph _ of @xmath6 .",
    "we use several graph metrics applied to the graphs @xmath44 and @xmath51 associated to document @xmath6 .",
    "each metric produces a separate coherence score for @xmath6 .",
    "we present these next .",
    "_ pagerank _",
    "@xcite is a vertex ranking metric that gives higher scores to the best connected vertices in a graph .",
    "the pagerank score of a node in a graph depends on its indegree and the pagerank scores of those nodes linking to it ( the latter being considered as recommendation ) .",
    "formally , the pagerank of @xmath51 is given by :    @xmath52    where @xmath53 is the pagerank of @xmath50 , @xmath54 is the number of edges incident on node @xmath49 , and @xmath55 is a damping factor ( typically @xmath56 ) .",
    "we hypothesise that a lower median pagerank score is indicative of a more coherent document : as @xmath51 becomes more connected , the relative importance of each node or sentence decreases in the pagerank score .",
    "we use the median rather than the average because the average pagerank score does not distinguish between star - graphs ( where all but one node are connected to the single remaining node , and no other edges occur ) and path graphs ( where the nodes occur in sequence ) ; however , path graphs intuitively correspond to coherent discourse flow whereas star graphs do not .",
    "so we propose that the final coherence score of @xmath6 is the _",
    "median _ pagerank of nodes of @xmath51 : @xmath57      the clustering coefficient ( cc ) measures the extent to which the neighbours of a node in @xmath51 are connected to each other .",
    "we hypothesise that a lower clustering coefficient score is indicative of a more coherent document : the fact that the neighbours of any given node in @xmath51 are themselves connected suggests that this node s importance to the overall discourse is very low ; if no neighbours are connected , all nodes are equally important for coherence .",
    "we compute the coherence score of @xmath6 as the global clustering coefficient of @xmath51 : @xmath58 where @xmath59 is the number of closed triplets containing @xmath50 , and @xmath60 is the total number of open and closed triplets containing @xmath50 ( a _ triplet _ is a subset of @xmath43 containing exactly three nodes .",
    "a triplet is _ open _ if exactly two of its three nodes are connected , and _ closed _ if all three nodes are connected ) .",
    "the betweenness of a node @xmath50 measures the fraction of shortest paths in @xmath51 that contain @xmath50 .",
    "we hypothesise that a higher average betweenness score for the nodes of @xmath51 is indicative of a more coherent document : a coherent document @xmath6 in our context should resemble a path graph where a sentence is connected only to the preceding and next sentence .",
    "we compute the coherence score of @xmath6 as the average of all betweenness scores in @xmath51 : @xmath61 where @xmath62 is the number of shortest paths between @xmath63 and @xmath64 in @xmath43 containing @xmath50 , and @xmath65 is the total number of shortest paths between @xmath63 and @xmath64 .",
    "the entity distance between two sentences @xmath50 and @xmath49 is the smallest number of words occurring between @xmath50 and @xmath49 that do not contain an entity shared by @xmath50 and @xmath49 . we reason that a short entity distance implies that shared entities between @xmath50 and @xmath49 aid the flow of the text , whereas a high distance means that many other entities are mentioned in between @xmath50 and @xmath49 , implying topic drift , and hence lower coherence .",
    "we compute the coherence score of @xmath6 as the inverse of the average entity distance over all entities shared by two or more sentences : @xmath66 where @xmath67 is the number of sentences in @xmath6 , @xmath68 is the set of entities of @xmath6 occurring in at least two distinct sentences , and @xmath69 and @xmath70 are the locations of entity @xmath71 in the document .",
    "for example , for sentence @xmath72 , the location of entity @xmath71 could be the 20th term in @xmath6 , and for @xmath73 , @xmath71 could be the 30th term in @xmath6 .",
    "we propose an adjacent topic flow ( atf ) metric , which is , roughly , the average of the reciprocal number of shared entities between adjacent sentences . in a coherent document , consecutive sentences should share common entities between them to relate current discourse to past discourse : the fewer shared entities ( minimum one ) between adjacent sentences , the more focused the discourse and the more coherent the text .",
    "we compute the coherence score of @xmath6 as the average reciprocal of the union of entities between adjacent sentences : @xmath74 where @xmath75 are the sentences in the document and @xmath76 is the union of entities from sentences @xmath72 and @xmath73 .",
    "the intuition is that adjacent sentences should share entities , but if the union of their entity sets is large this strains the focus of the reader as these entities need not be necessarily linked to previous discourse .",
    "the atf metric determines if a pair of adjacent sentences share a single entity irrespective of whether several entities are shared .",
    "thus , a text with two ( or more ) main topics ( e.g.  ` security ` and ` heartbleed ` ) mentioned in adjacent sentences would not be detected . for coherence ,",
    "more shared entities can be seen as more salient words aiding the reader to retain focus as more links exist between current and past discourse .",
    "we present a version of atf called adjacent weighted topic flow ( awtf ) where we _ weigh _ a document according to the number of entities shared by adjacent sentences .",
    "the resulting coherence score is : @xmath77 where @xmath78 is the number of shared entities .",
    "atf and awtf rely on _ local _ coherence between adjacent sentences to capture global coherence .",
    "however , a coherent text may contain discourse gaps where several topics are treated locally , then abandoned , yet later on picked up again .",
    "for example , consider a document with three paragraphs of which two are on the same topic as illustrated in figure  [ fig : natfmetrics ] .",
    "figure  [ fig : natfmetrics](b ) shows that the application of an adjacent - based topic flow metric would find no coherence between the first two and last two sentences .",
    "conversely , figure 1(c ) shows that a _",
    "non_-adjacent topic flow metric would find some coherence between the first and last sentence in the second comparison ( indicated by the number 2 ) as they are on the same topic .",
    "we define two coherence models ( i ) _ non - adjacent topic flow ( natf ) _ and ( ii ) on _ non - adjacent weighted topic flow ( nawtf ) _ , with corresponding coherence scores : @xmath79 and @xmath80 where @xmath81 is the number of all entities in @xmath6 shared between at least two sentences . if @xmath82 , we set @xmath83 reflecting that there is no coherence in the document .",
    "strictly speaking , equations [ eqn : atf][eqn : nawtf ] are not graph centrality metrics but as our bipartite graph is a labelled graph all are invariant under graph isomorphism .",
    "we next evaluate the above graph - based models , as well as the entropy models of document coherence presented in section [ sec : ecohmodel ] .",
    "first we evaluate the accuracy of our coherence models in section [ ss : accuracy ] , and then retrieval precision when reranking documents according to their coherence scores in section [ ss : precision ] .",
    "we evaluate our coherence models in the _ sentence reordering _ task , which is standard for coherence evaluation .",
    "the main idea is that we scramble the order of the sentences in each document , and subsequently then determine the number of times the original document is deemed more coherent than its permutations .",
    "specifically , for each original document @xmath84 in a dataset , we reorder its sentences 20 times , producing 20 permutations @xmath85 $ ] .",
    "we assume that the more sentences are reordered , the less coherent @xmath86 becomes on average ; this assumption has been validated with human assessments @xcite .",
    "we run our coherence model on all @xmath87 pairs , and measure accuracy by considering as _ true positive _ each case where @xmath84 gets a higher coherence score than its @xmath86 permutation .",
    "we use the earthquakes and accidents dataset , which has been used in previous work on coherence prediction  @xcite , so that we can directly compare our coherence model to the state of the art .",
    "this dataset contains relatively clean , curated articles about earthquakes from the north american news corpus and narratives from the national transportation safety board , of relatively short length and not much variation in document length ( statistics in table [ tbl : datasets ] ) .",
    "we identify entities in documents using the stanford parser . due to the steep increase in parsing time as sentence length increases",
    ", we only consider sentences of 60 terms or less , which is approximately 3 times the average length of a sentence in english  @xcite .",
    "if a sentence is longer than 60 terms , we do not exclude it , but rather cut it at length 60 , to speed up processing .",
    "our baselines are ( i ) barzilay and lapata s original entity grid model @xcite , and ( ii ) barzilay and lee s well - known hmm - based model @xcite .",
    "we do not use guidnaudeau and strube s indegree model @xcite as they use a different dataset . for the remaining baselines ,",
    "we compare the _ tuned _ scores reported in @xcite with _ untuned _ scores of our models .",
    ".statistics of the earthquakes and accidents dataset .",
    "average document length is measured in terms ; mad denotes mean absolute deviation . [ cols=\"<,^,^\",options=\"header \" , ]      table  [ tbl : acc ] presents the accuracy and statistical significance of the sentence reordering experiments .",
    "our models are overall comparable to the baselines .",
    "our 0-order entropy model outperforms both baselines for both datasets . as its order increases ,",
    "its performance decreases ( but still outperforms both baselines for the accidents dataset ) .",
    "this happens because as the order of our model grows , we expect to find fewer higher - frequency @xmath0-grams because our model will rely on increasingly longer sequences of entities .",
    "this results in overall higher entropy scores .",
    "for example , a 2-gram and 3-gram coherence model of the example entity grid in table  [ tbl : examplegrid ] would result in these respective entropy scores @xmath88 and @xmath89 , which grow as the value of @xmath0 in the @xmath0-gram increases .",
    "looking at cases where our entropy model errs , we see that there is a potentially negative bias of our model against shorter documents . as our entropy model scores document coherence using the frequency of entity transitions that occur in a document , short documents are at a disadvantage . for example , a very short document of only one sentence where all @xmath0-grams occur exactly once will receive a very high entropy score ( hence very low coherence score ) .",
    "the minimum number of sentences found in documents in our data were 2 and 3 for the earthquakes and accidents datasets respectively .",
    "our entropy model will very likely deem these documents as incoherent , even if they are not .",
    "regarding our graph coherence metrics , the pagerank , betweenness , entity distance , atf , natf and nawtf variants also outperform the baselines at all times .",
    "entity distance and betweenness are overall best , outperforming our entropy model too . both entity distance and betweenness measure the same feature , that is distance between discourse entities , but in different ways : betweenness in terms of shortest path in the graph ; entity distance in terms of absolute number of terms separating the entities .",
    "it seems that this distance , measured in either way , is more discriminative of coherence than e.g. any clustering or recommendation of discourse entities captured by the clustering coefficient or pagerank respectively .",
    "overall , manual inspection of the documents where most of our models failed reveals two main reasons for this : ( i ) these documents contain sentences of very large length , exceeding the 60 term limit we have defined . as we artificially cut these sentences at 60 terms ,",
    "the discourse flow of the text is disturbed . moreover ,",
    "several very long sentences tend to introduce a large number of new entities into the discourse , which may be topically relevant but not necessarily identical , e.g. @xmath90__accommodation with a bed , table , cupboard , attached washroom , television , broadband internet facility , telephone , small kitchen with facilities to prepare tea , coffee , etc . _ _ ( ii ) the versions of our models that concentrate on entities shared by adjacent sentences risk underperforming , because about 50% of the sentences in english are estimated not to share any entities and 60% of them to be related by weak discourse relations  @xcite .",
    "e.g. , for journalistic texts like the earthquake dataset , the spatial proximity of sentences does not necessarily correlate with their semantic relatedness , as related sentences can be placed at the beginning and the end for emphasis @xcite . indeed in table [ tbl : acc ]",
    "our two adjacent topic flow metrics atf and awtf have lower accuracy in the earthquakes than accident dataset , and also compared to their non - adjacent versions ( natf and nawtf ) .",
    "note that this spatial proximity limitation is a major disadvantage of _ all _ models of local text coherence , not only ours .",
    "we discuss this point in section [ s : discussion ] .",
    "overall , our _ untuned _ models perform on par with the _ tuned _ baselines , occasionally outperforming them .",
    "motivated by the good performance of our coherence models , we next study their potential usefulness to retrieval .",
    "we now test whether our document coherence scores can be useful to retrieval .",
    "our assumption is that more coherent documents are likely to be more relevant . to test this",
    "we rerank the top 1000 documents retrieved by a baseline model according to their coherence scores .",
    "our baseline ranking model is a unigram , query likelihood , dirichlet - smoothed , language model .",
    "let @xmath91 be the baseline retrieval status value of a document , and @xmath92 be the coherence score of a document computed as per any of our coherence models ( equations [ eq : coh][eqn : nawtf ] ) . for our entropy coherence model",
    "we use only the 0-order variant , as this performed best in the sentence reranking task .",
    "we use a simple _ linear _ combination @xcite to compute the reranked @xmath91 of each document , denoted @xmath93 : @xmath94 where @xmath95 is a smoothing parameter controlling the effect of @xmath91 over @xmath92 .",
    "we use indri 5.8 for indexing and retrieval without stemming or stopword removal .",
    "we use the clueweb09 cat.b test collection with queries 150 - 200 from the web adhoc track of trec 2012 .",
    "clueweb09 contains free text crawled from the web , likely to be more noisy and containing documents that are overall longer and with more variation in document length than in the earthquakes and accidents dataset .",
    "we remove spam from clueweb09 using the spam rankings of cormack et al . with a percentile - score @xmath96 indicating spam . this threshold is stricter than the one recommended in , practically meaning that we remove many more documents assumed to be spam .",
    "this reduces the number of documents from ca .",
    "50 million to ca . 16 million .",
    "we apply such strict spam filtering for the following reason : as we hypothesise that a lower entropy scoring document will be more coherent , documents containing the same repeated entities will be judged more coherent . for example , a document containing only the sentence ` free domains , free domains , free domains ` would receive an entropy score of @xmath97 ( i.e. , highest coherence ) but is , arguably , not very coherent .",
    "while this problem does not occur in the earthquakes and accidents dataset , spam documents are plentiful in clueweb09  @xcite which is why apply such a strict threshold when removing spam from this collection .",
    "we evaluate retrieval with mean reciprocal rank ( mrr ) of the first relevant result , precision at 10 ( p@10 ) , mean average precision ( map ) of the top 1000 results , and expected reciprocal rank at 20 ( err@20 ) .",
    "the baseline and our reranking method include parameters @xmath98 and @xmath99 that we tune using 5-fold cross - validation .",
    "we report the average of the five test folds .",
    "we vary the ranking baseline s @xmath100 \\{100,500,800,1000,2000,3000,4000,5000,8000 , + 10000 } ; and the reranking parameter @xmath101 in steps of 0.05 .",
    "table [ tab : retrieval ] displays the retrieval results .",
    "we see that reranking documents by their coherence score overall outperforms the baseline in terms of retrieval precision , with few exceptions . specifically , the mrr gains vary between + 66.2% and + 170.9% , which practically translates to a boosting of the first relevant document by moving it approximately one position higher in the ranking on average .",
    "the p@10 gains vary between + 13.1% and + 83.8% , which practically translates to an increase of the portion of relevant documents found in the top 10 from less than 2 in the baseline to over 3 documents on average , except once ( for entity distance ) .",
    "this gaining trend also applies to the top 20 retrieved results , as can be seen by the improvements in err@20 .",
    "however , looking at map for the top 1000 retrieved documents we see only marginal gains and twice drops in performance ( for entropy and entity distance ) .",
    "this means that coherence improves mainly early precision , i.e. has the potential to refine the very top of the ranked list , but not alter it significantly at more depth .",
    "interestingly , while entity distance was among the best performing models in the sentence reranking task ( table [ tbl : acc ] ) , it is the weakest performing model when integrated to retrieval .",
    "this complements previous findings in the literature showing that , when using an nlp component in ir , higher nlp accuracy does not necessarily result in higher retrieval performance @xcite .",
    "the reason why entity distance underperforms when integrated to retrieval compared to sentence reranking could be the different dataset characteristics : earthquakes and accidents include relatively short documents ( roughly 240 terms long on average ) that are fairly uniform ( on average 65 terms of mean absolute deviation ) , see table [ tbl : datasets ] .",
    "clueweb09 on the other hand includes documents that are longer ( 1461 terms long on average ) and more heterogeneous in style and size .",
    "in fact , quite a few of the clueweb09 documents are wikipedia articles , which are fairly long and organised in thematic subsections , meaning that salient entities might be introduced , then dropped for a while , and later on picked up again in different subsections .",
    "this is likely to affect negatively entity distance , much more than our other coherence metrics , as it is the only metric that measures the actual distance in words between occurrences of the same entity in different sentences .",
    "so , for those documents where this distance varies wildly , reaching both very low and very high numbers , the entity distance will produce notably different coherence scores for different documents , rendering a comparison between documents on the basis of these scores difficult .    finally , note that the baseline scores in table [ tab : retrieval ] are not directly comparable to the ones reported in the literature when using no spam filter or another spam filter threshold , because the stricter the spam filtering , the smaller the final dataset used for retrieval and the higher the risk of removing relevant documents .",
    "indeed , it has been reported that for clueweb09 cat.b , removing spam results in overall lower precision @xcite .",
    "we also anecdotally report that several documents among the ones we removed as spam were assessed as relevant in the trec relevance judgements .",
    "overall we conclude that the improvements in table [ tab : retrieval ] show that coherence can be a discriminative feature of relevance .",
    "we now discuss interesting caveats of our coherence models and potential solutions .",
    "one weakness of our coherence models , and of the entity grid in fact , is in capturing the contribution of newly introduced entities to discourse : it is long assumed that repeated mentions of the same entities contribute to coherence ; however , related - yet - not - identical entities also contribute to text coherence . to our knowledge , this is something that occurs frequently in text but that is not yet captured by automatic models of coherence prediction .",
    "doing so would require a richer set of metrics and representations , involving for instance methods long used in ir for detecting near - synonyms , such as wordnet - type ontologies , or term associations extracted from large scale query logs or other relevant corpora .",
    "this is an interesting research direction that we intend to follow in the future .",
    "a further limitation of our coherence models is the selection of salient discourse entities : following @xcite , we consider all subjects and objects as salient discourse entities .",
    "this assumption implies that ( a ) all subjects and objects are equally salient in text , and ( b ) all subjects and objects are topical ( as opposed to having a modifying or periphrastic role in the discourse ) .",
    "however , neither of these assumptions is true .",
    "assumption ( a ) could be removed by including some salience weighting component in the selection of entities . in the past , this has been attempted through the use of linguistic features , such as modifiers and named entity types @xcite .",
    "it will be interesting to see if we can also use statistical approximations to this end , looking into the rich ir literature in term weighting .",
    "assumption ( b ) could be removed by including a similar topical weighting component , such as in @xcite . in the case where coherence scores are used for ir",
    ", it makes sense to align this topical detection to the query topic , so that , for instance , coherence is computed only for documents whose topic is relevant to the query topic .",
    "such a tight integration of coherence modelling to ranking would be an interesting direction to explore in future work .",
    "modelling text coherence is an area that has received a lot of interest , with a surge of automatic methods in the last decade .",
    "we presented two novel classes of models of document coherence which extend the well - known _ entity grid _ representation @xcite .",
    "our first model approximates coherence as the inverse of the discourse entropy in text , based on the assumption that repeated entities in text , which have low entropy , indicate higher coherence .",
    "our second class of coherence models map the entity grid of a document into a graph , as proposed by guinaudeau & strube  , and then approximate document coherence using different metrics of graph topology and their variations .",
    "the assumption is that the discourse flow of a document can be reflected in the topology of its entity graph , hence any regularities of the former may be measured from the latter .",
    "we report two experiments . in the first",
    ", we find that our coherence models are comparable to two well - known text coherence models . given that our models are completely untuned",
    ", their performance may be further improved through smoothing . in the second experiment",
    ", we find that using our coherence scores to rerank retrieved documents improves retrieval precision , especially for the top 1 - 20 results .",
    "overall , this work contributes two novel classes of document coherence models that approximate coherence from discourse entity @xmath0-grams .",
    "their application to retrieval complements recent work in ir showing a positive relation between relevance and document cohesiveness or comprehensibility @xcite ( albeit computed differently than we do ) .",
    "this is a promising research direction for ir that we intend to pursue in the future .",
    "a.  ntoulas , m.  najork , m.  manasse , and d.  fetterly . detecting spam web pages through content analysis . in l.",
    "carr , d.  d. roure , a.  iyengar , c.  a. goble , and m.  dahlin , editors , _ www _ , pages 8392 .",
    "acm , 2006 ."
  ],
  "abstract_text": [
    "<S> we present two novel models of document coherence and their application to information retrieval ( ir ) . </S>",
    "<S> both models approximate document coherence using discourse entities , e.g. the subject or object of a sentence . </S>",
    "<S> our first model views text as a markov process generating sequences of discourse entities ( entity @xmath0-grams ) ; we use the entropy of these entity @xmath0-grams to approximate the rate at which new information appears in text , reasoning that as more new words appear , the topic increasingly drifts and text coherence decreases . </S>",
    "<S> our second model extends the work of guinaudeau & strube   that represents text as a graph of discourse entities , linked by different relations , such as their distance or adjacency in text . </S>",
    "<S> we use several graph topology metrics to approximate different aspects of the discourse flow that can indicate coherence , such as the average clustering or betweenness of discourse entities in text . </S>",
    "<S> experiments with several instantiations of these models show that : ( i ) our models perform on a par with two other well - known models of text coherence even without any parameter tuning , and ( ii ) reranking retrieval results according to their coherence scores gives notable performance gains , confirming a relation between document coherence and relevance . </S>",
    "<S> this work contributes two novel models of document coherence , the application of which to ir complements recent work in the integration of document cohesiveness or comprehensibility to ranking @xcite . </S>"
  ]
}