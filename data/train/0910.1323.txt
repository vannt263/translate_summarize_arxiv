{
  "article_text": [
    "in newtonian physics , physical processes are understood with respect to a fixed spatial coordinate system and a time parameter , which is absolute and ever increasing .",
    "predictions are entirely deterministic .",
    "quantum theory and general relativity depart from this classical picture in opposing manners .",
    "quantum theory gives probabilistic predictions as to the outcomes of measurements , but retains fixed space and time coordinates . on the other hand ,",
    "general relativity is deterministic , but shows that space and time form a dynamical structure .",
    "reconciling these fundamental philosophical differences is one of the many challenges one is faced with in trying to construct a theory of quantum gravity .",
    "there have been many different approaches to this problem with many different results @xcite .",
    "one way of moving forward is to dismiss classical assumptions and create a probabilistic theory that has a dynamic causal structure .",
    "however , what results is indefinite casual structure .",
    "this is more radical than either probabilistic predictions or dynamical space - time structure . in general relativity ,",
    "a separation between space - time locations is either space - like or time - like .",
    "an indefinite causal structure would allow for a separation between space - time locations to be something like a quantum superposition of a space - like and a time - like separation . while we may be uncertain of the causal structure of the path between measurements , we know where in space - time we make measurements , what measurements we have made , and what outcomes we get . with this data",
    ", we can examine probabilistic correlations for information .",
    "the causaloid framework ( @xcite,@xcite,@xcite ) provides us with the necessary structure",
    ". we will outline the essentials of this framework in section 2 .",
    "it is natural in discussions of causal structure to raise the question of entropy .",
    "the second law of thermodynamics tells us that in an isolated system , entropy can increase or remain the same , but it can never decrease . in information",
    "theory , entropy is viewed as being a measure of uncertainty before we measure a state or equivalently , the amount of information gained by upon learning the state of a system .",
    "inherent in both concepts of entropy is an assumed causal structure , specifically that there exists a background time .",
    "the standard definition of entropy is in the context of a definite causal structure with reference to absolute time . in order to make sense of entropy in an indefinite causal structure",
    ", a clear definition must be established . to do so requires consideration of the following questions :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ what are the concepts from the usual picture of entropy in a definite causal structure that are necessary to define entropy ?",
    "what are the analogues to these concepts in a picture with indefinite causal structure ? _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    using the formalism introduced in the causaloid framework , we are able to provide answers to these questions and then , define a causally - unbiased entropy .    in section 2 , we will review the relevant aspects of the causaloid framework .",
    "we then proceed with the new developments . in section 3",
    ", we define a new type of product that is utilized in the work on entropy .",
    "the definition of causally - unbiased entropy and resulting features are developed in section 4 .",
    "every experiment results in a set of data from making measurements on a system . each piece of data",
    "could be thought of as a card with three pieces of information on it ; where the measurement is made in space - time , what is measured , and what the result of the measurement is .",
    "we will represent each card ( or piece of data ) as @xmath0 where @xmath1 denotes the space - time information , @xmath2 denotes the information pertinent to a choice of measurement or action , and @xmath3 denotes the information regarding an observation or outcome of a measurement .",
    "the set of all possible cards ( i.e. all possible measurements with all possible outcomes with every space - time configuration ) is denoted @xmath4 .",
    "we can imagine running an experiment an infinite number of times so as to be able to obtain relative frequencies . in order for the cards to tell us the relative frequencies , we must systematically sort them .",
    "each distinct @xmath1 is defined as an _ elementary region _ of space - time .",
    "a _ composite region _",
    ", denoted @xmath5 , is a set of elementary regions .",
    "( note : these definitions of  elementary region \" and  composite region \" differ from those in @xcite . ) therefore , these cards can be sorted according to their associated space - time region .",
    "the set of all possible cards with the same space - time information @xmath1 written on them is the _ measurement information for elementary region @xmath1_. we denote this set as @xmath6 .",
    "the measurement information for composite region @xmath5 is the union of all sets of measurement information for the elementary regions contained within the composite region .",
    "more concisely , @xmath7 we can further sort the measurement information in a region . the _ procedure in a region _ , denoted @xmath8 , is the set of all distinct choices of measurement recorded for the region @xmath1 .",
    "@xmath9 for composite regions , we define the following set : @xmath10 similarly , the _ outcome set in a region _ , denoted @xmath11 is defined to be the set of all distinct outcomes of a measurement recorded for the region @xmath1 .",
    "@xmath12 again , for composite regions , we define @xmath13 notice that @xmath14 and @xmath15 .",
    "the composite structure we expected of our space - time regions is reflected structure throughout these sets .",
    "a set of cards with the measurement information for a region has no more or less structure than an elementary region of space - time .",
    "therefore , without adding structure or losing generality , we can take the sets @xmath6 to be elementary regions , at least , for the purposes of this paper .",
    "from this point forward , the term _ region _ will be used interchangeably to refer to objects of type @xmath1 or @xmath5 and type @xmath6 or @xmath16 .",
    "notice that the set of all cards @xmath4 can be viewed as all the cards from all ( elementary ) regions .",
    "@xmath17 so @xmath4 is the largest of all regions that can be considered .",
    "these definitions provide a firm foundation on which the causaloid framework rests both mathematically and conceptually .",
    "the most basic quantity that we would want to be able to calculate is the probability that a certain ( set of ) outcome(s ) is observed given that a certain ( set of ) measurement(s ) has been performed at a ( set of ) location(s ) in space and time .",
    "suppose that the set of locations we are interested in is @xmath5 .",
    "the set of all the cards corresponding to these locations called @xmath16 .",
    "we write pairings of measurements and corresponding outcomes in @xmath16 as @xmath18 . a specific outcome and measurement pair",
    "is denoted as @xmath19 ( or equivalently , @xmath20 ) .",
    "the set of all @xmath19 in region @xmath5 is @xmath21 .",
    "the set comprised of all the cards not in @xmath16 is @xmath22 .",
    "we call @xmath23 the _ generalized preparation _ because it is the information that surrounds @xmath16 not only from the immediate past , but from the future and the rest of space - time as well . by the choices we make in setting up the experiment",
    ", we can put conditions on the generalized preparation such that @xmath24 is well - defined .",
    "( see ref.@xcite for details . )",
    "then we can write @xmath25 for a specific pair @xmath26 , we can write this probability as @xmath27 we use the short - hand @xmath28 to denote the probability defined in eq.([p_alpha1 w gp ] ) . one way to specify the state of a system is to list all the possible @xmath28 for elements of @xmath16 .",
    "@xmath29 however , this over - specifies the state .",
    "we do not usually need to know the probability of every outcome of every measurement in order to determine what the complete state of the system is .",
    "physical theories tell us what relationships exist between variables and what constraints those relationships place on the variables of the system . these relationships and constraints",
    "can be used to determine a reduced set of probabilities from which all other probabilities can be represented .",
    "the reduced set of probabilities is defined such that any probability can be written as a linear combination of the probabilities in the reduced set .",
    "let us denote the reduced or fiducial set in @xmath16 as @xmath30 .",
    "this process of going from the set of all the probabilities to the smallest essential set we call _ first level physical compression_. this can be expressed as @xmath31 such that @xmath32 where @xmath33 encodes the physical compression and therefore , is determined by the details of the physical theory .",
    "we can define a _ decompression matrix _",
    ", @xmath34 such that @xmath35 where @xmath36 means the @xmath37 component of @xmath33 .",
    "let us consider two distinct regions @xmath38 . in a similar fashion to the single region case",
    ", @xmath39 we specify the state of the system by listing all @xmath40 .",
    "@xmath41 where @xmath42 is the cartesian product .",
    "it can be shown that @xmath43 which implies that the following list of probabilities is sufficient .",
    "@xmath44 this is effectively first level compression on each index .",
    "but if a physical theory has some connection between the two regions , @xmath45 may no longer be the smallest set that is sufficient to represent all possible states .",
    "then _ second level physical compression _ is possible .",
    "it is defined to be @xmath46 such that @xmath47 when @xmath48 , second level compression is trivial .",
    "but it is proven in @xcite that it is possible that @xmath49 .",
    "now we can define a second level decompression matrix . by comparing eq.([1st level ] ) and eq.([2nd level ] ) , we infer that @xmath50 where @xmath51 which is the desired second level decompression matrix .",
    "this matrix encodes how we move from @xmath52 s to @xmath53 s .",
    "using the definition of the first level decompression matrix , eq.([2nd level matrix ] ) becomes @xmath54 this defines the _ causaloid product _ , denoted @xmath55 which unifies the different causal structure - specific products .",
    "explicitly , @xmath56 it is this product that allows us to look at the probabilistic correlations between arbitrary locations in space - time without specifying the causal relationship .",
    "we have shown second level compression for the case where we have two regions .",
    "this is easily generalized for any number of regions .",
    "the object that would encode the compression for three regions would be @xmath57 , for four regions would be @xmath58 , etc .",
    "after second level compression over multiple regions , we have @xmath59 there is a third level of physical compression that compresses these multi - region @xmath60-matrices to give the _ causaloid _ , @xmath61 , which is defined as @xmath62 where @xmath63 is determined by the rules of the physical theory ( for detailed discussion of how this works see [ 2 ] ) .",
    "by decompressing the set @xmath63 , we can obtain the @xmath60-matrix for any set of regions .",
    "this means that the causaloid gives us the ability to perform any calculation that the physical theory allows for .      up to this point",
    "we have exclusively dealt with probabilities conditioned on procedures .",
    "it is more useful to also be able to condition on outcomes .",
    "specifically , we d like an expression for the following : @xmath64 using bayes theorem , this becomes @xmath65 where @xmath66 denotes that the sum is over all possible outcomes corresponding to the measurement @xmath67 ( in @xmath68 ) .",
    "( for simplicity , we have suppressed the part of the notation denoting the generalized preparation . ) in the causaloid framework , this becomes @xmath69 where @xmath70 .",
    "( the sum being over @xmath71 in this notation has the same meaning as the sum being over all outcomes consistent with @xmath72 . ) in order for this probability to be considered well - defined , the right hand side can not depend on @xmath73 .",
    "since @xmath74 and @xmath75 are determined exclusively by the physical theory , neither has any dependence on @xmath73 .",
    "however , @xmath76 does depend on @xmath73 .",
    "this implies that in order for the probability eq.([probdef ] ) to be well defined ( i.e. not depend on @xmath73 ) , it must vary with @xmath77 .",
    "the dependence on @xmath77 can be removed altogether by requiring that @xmath74 be parallel to @xmath75 .",
    "therefore , the above probability is well defined if and only if @xmath78 with this condition , we get @xmath79",
    "consider two distinct regions ; @xmath81 and @xmath82 . by definition @xmath83",
    "suppose we wanted to take the dot product between two vectors of the above form . using decompression matrices , we can write @xmath84 where @xmath85 , @xmath86 , and @xmath87 . notice that we can write @xmath88 as @xmath89\\ ] ] similarly , @xmath90\\ ] ] define @xmath91 and , similarly , @xmath92 using this , eq.([devdot ] ) becomes @xmath93 where @xmath94 and @xmath95 .",
    "this suggests that the essence of @xmath96 is a relationship between @xmath97 and @xmath98 mediated by matrices that depend on @xmath99 .",
    "therefore , we can view eq.([devdot ] ) as kind of product of @xmath97 and @xmath98 .",
    "dot products of this form come up frequently enough that we will define this as the @xmath100-dot product and denote it as @xmath101 we will make use of this product later in the paper .",
    "standard definitions of entropy assume fixed causal structure . here",
    "we develop a causally - unbiased definition of entropy in the causaloid formalism .",
    "shannon entropy for a classical state is defined as @xmath102 the definition of @xmath103 used in this equation requires that the structure of space - time be organized with the following features :    * a region of interest , @xmath104 * an immediate past space - time region , @xmath105 * sufficient data about what happened in @xmath105 * a measurement @xmath106 * a set of outcomes , @xmath107 , corresponding to @xmath106    this allows us to write @xmath108 removing all time bias from these features of space - time structure , we get    * a region of interest , @xmath104 * a reference region @xmath105 * an outcome / measurement pair in @xmath105 , @xmath109 * a measurement @xmath106 * a set of outcomes , @xmath107 , corresponding to @xmath106    the reference region can be thought of as a kind of preparation region that is not limited to being in the causal past .",
    "in fact , the choice of reference region is arbitrary as illustrated in fig .",
    "[ indefinitecs ] .",
    "the definition of @xmath103 in a causally - unbiased structure is @xmath110 ( since @xmath105 is arbitrary , we should technically say ` @xmath103 with respect to the reference region @xmath105 ' .",
    "however , for the sake of brevity , we will assume that ` with respect to @xmath105 ' is implied much as ` with respect to the past ' is taken as implied in the causally - biased situation . )    using the above definition of @xmath103 , we define the entropy relative to the reference data @xmath111 as @xmath112 notice that this reduces to the causally - biased definition of entropy when @xmath105 is the past ; @xmath106 measures the microstate in the classical case or measures in the basis where @xmath113 is diagonal in the quantum case .",
    "taking the probability to be well - defined , eq.([parallel r s ] ) and eq.([theo s ] ) give the following definition of entropy : @xmath114 of course , this equation requires that @xmath115 . loosening this condition slightly",
    ", we can consider what happens when @xmath116 is nearly parallel to @xmath117 , using the definition of the probability from eq.([probdef ] ) .",
    "the entropy associated with this is @xmath118 it becomes necessary to shorten the notation for the following work so @xmath116 will be denoted as @xmath119 ( where the index @xmath120 is represented by @xmath121 ) and @xmath117 will be denoted as @xmath122 . as with any vector",
    ", @xmath119 can be decomposed into a component parallel to @xmath122 and a component perpendicular to @xmath122 ( i.e. components in @xmath123 and @xmath124 , respectively ) .",
    "that is , @xmath125 using the unit vectors as defined , @xmath76 can be decomposed as @xmath126 where @xmath127 is the component of @xmath76 that is perpendicular to the plane defined by @xmath122 and @xmath119 .",
    "the probability of interest , @xmath103 , then becomes @xmath128 where @xmath129 .",
    "notice that the first term is equivalent to a well - defined probability ( eq . [ parallel r s ] ) .",
    "we require the second term to be small since the deviation from well - defined should be small .",
    "since we have already required that @xmath130 be small , we need only place restrictions on @xmath131 .      for the purposes of this subsection",
    ", we will work in the plane defined by @xmath122 and @xmath119 .",
    "define the angle between @xmath122 and the projection of @xmath76 into the plane to be @xmath132 .",
    "define the length of the projection of @xmath76 into the plane to be @xmath133 . using basic trigonometry",
    ", we get @xmath134 therefore , k can be written in a form that is dependent on only one variable , as follows : @xmath135 as @xmath132 tends towards @xmath136 , @xmath131 tends to infinity .",
    "therefore , to ensure that the second term of ( [ notwdp ] ) is small , we require that @xmath131 be finite . assume it to be a property of the state space for @xmath77 that there exists some @xmath137 . clearly , @xmath138 in order for @xmath131 to be finite .",
    "so @xmath132 is bounded as follows : @xmath139 the @xmath131 corresponding to @xmath140 will be denoted as @xmath141 .",
    "further bounds can be placed on @xmath131 by the state space of the physical theory . for our purposes , it is sufficient that @xmath131 is finite .      in light of ( [ notwdp ] ) , entropy , as defined in ( [ generalentropy ] ) , becomes @xmath143 \\label{deventropy0}\\end{aligned}\\ ] ] since @xmath144 is very small ( as is implied by the fact that @xmath145 and @xmath146 are nearly parallel ) and @xmath131 is finite , we can take a taylor expansion ( to leading order ) of the first @xmath147 term .",
    "doing this gives @xmath148 \\nonumber \\\\",
    "& = & -\\sum_i \\left(\\frac{v_i^{\\parallel}}{u}\\right ) \\log_2 \\left(\\frac{v_i^{\\parallel}}{u}\\right ) + k\\left(\\frac{{v_i^{\\perp}}}{u}\\right ) \\log_2 \\left(e\\frac{v_i^{\\parallel}}{u}\\right ) + { \\cal o}\\left({v_i^{\\perp}}^2\\right)\\end{aligned}\\ ] ] notice that the first term is equivalent to the definition of entropy where @xmath149 and that @xmath150 reduces to this definition when @xmath151 . that is ,",
    "when @xmath149 ( or equivalently , @xmath151 ) @xmath152 for @xmath153 , we will define @xmath154 using @xmath141 as defined in the previous section , we can regard @xmath155 as a kind of correction to the causally - biased entropy .",
    "then , to leading order @xmath156      @xmath142 is an entirely new quantity with no direct classical analogue so understanding its physical interpretation is a non - trivial matter .",
    "if we consider entropy as a measure of uncertainty , then @xmath157 is the measure of our uncertainty that the measurement @xmath106 in region @xmath104 will yield the specific outcome @xmath158 , given the data we have from the reference region @xmath105 .",
    "since our reference region @xmath105 is arbitrary , one way to view @xmath142 is that it measures how completely the region @xmath105  prepares \" region @xmath104 . in this sense",
    ", preparation influences our uncertainty . in a definite causal structure ,",
    "an immediate past region would completely prepare our region of interest and @xmath142 would be zero .",
    "however , in the causally - indefinite picture , we can not require a priori if the reference region that we have chosen will completely prepare our region of interest .",
    "if there are no influences on our uncertainty from outside region @xmath105 , then the probability will be well - defined and @xmath142 will be zero .",
    "but if there are influences on our uncertainty from outside region @xmath105 , then the magnitude of @xmath142 will reflect that .      for the sake of completeness",
    "the @xmath159 s and @xmath160 s must be translated into @xmath161 s and @xmath162 s .",
    "notice that @xmath163 substituting @xmath162 for @xmath160 and @xmath161 for @xmath159 gives @xmath164 using the @xmath100-dot product the above equations simplify to @xmath165 this allows us to completely specify the entropy of @xmath81 relative to a preparation @xmath82 in the causaloid framework .",
    "it is straightforward to generalize this to define the joint entropy of @xmath81 and @xmath166 with reference to a  preparation \" @xmath82 .",
    "simply redefine @xmath146 and @xmath167 as @xmath168 where @xmath169 and @xmath170 using the same procedure as for one region , we get @xmath171 in this manner , we can define causally - unbiased entropy in the causaloid framework for any number of regions .",
    "in a definite causal structure , the only thing required for a definition of entropy that is not in an indefinite causal structure is an immediate past region .",
    "since there is no reason in an indefinite causal structure to choose any reference region over any other , we simply choose an arbitrary region .",
    "this ensures that we do not hold on to any pre - conceived notions of space - time and its connection to causality .",
    "the definition of the causally - unbiased entropy resulted in a correction to the causally - biased definition of entropy . in a sense ,",
    "the q factor gives us an emergent idea of causality .",
    "it is a measure of the extent to which our region of interest is causally connected to our reference ( or  preparation \" ) region .",
    "if it is zero , the traditional ideas of causality are recovered .",
    "the next step would be determining how the q factor could potentially be physically observed . to do so",
    "may require us to know more of the theoretical and mathematical properties of q. which mathematical properties of shannon entropy hold for causally - unbiased entropy ? what is the status of the second law of thermodynamics in an indefinite causal structure ? to go about answering this , we could consider how @xmath150 `` evolves '' along tubes through indefinite space - times .",
    "these questions will be the subjects of continuing work in the near future .",
    "this work was supported by ogs .",
    "research at perimeter institute for theoretical physics is supported in part by the government of canada through nserc and by the province of ontario through mri .",
    "r.  penrose and m.  a.  h.  maccallum , `` twistor theory : an approach to the quantization of fields and space - time , '' phys .",
    "* 6 * , 241 ( 1972 ) .",
    "s.  w.  hawking , `` quantum gravity and path integrals , '' phys .",
    "d * 18 * , 1747 ( 1978 ) . c.  rovelli and l.  smolin , `` loop space representation of quantum general relativity , '' nucl .",
    "b * 331 * , 80 ( 1990 ) .",
    "r.  d.  sorkin , `` on the role of time in the sum over histories framework for gravity , '' int .",
    "j.  theor .",
    "phys .   * 33 * , 523 ( 1994 ) .",
    "j.  ambjorn , j.  jurkiewicz and r.  loll , `` quantum gravity , or the art of building spacetime , '' hep - th/0604212 .",
    "l.  hardy , `` probability theories with dynamic causal structure : a new framework for quantum gravity , '' gr - qc/0509120 .",
    "l.  hardy , `` towards quantum gravity : a framework for probabilistic theories with non - fixed causal structure , '' j.  phys .  a * 40 * , 3081 ( 2007 ) , gr - qc/0608043 .",
    "l.  hardy , `` formalism locality in quantum theory and quantum gravity , '' gr - qc/0804.0054 .",
    "l. d. landau and e. m. lifshitz , _ course of theoretical physics vol . 5 : statistal physics pt . 1 _ 3rd ed .",
    "( butterworth heinemann , oxford , 1980 ) ."
  ],
  "abstract_text": [
    "<S> entropy is a concept that has traditionally been reliant on a definite notion of causality . </S>",
    "<S> however , without a definite notion of causality , the concept of entropy is not all lost . </S>",
    "<S> indefinite causal structure results from combining probabilistic predictions and dynamical space - time . combining the probabilistic nature of quantum theory and dynamical treatment space - time from general relativity </S>",
    "<S> is an approach to the problem of quantum gravity . </S>",
    "<S> the causaloid framework lays the mathematical groundwork to be able to treat indefinite causal structure . in this paper , we build on the causaloid mathematics and define a causally - unbiased entropy for an indefinite causal structure . in defining a causally - unbiased entropy , there comes about an emergent idea of causality in the form of a measure of causal connectedness , termed the q factor . </S>"
  ]
}