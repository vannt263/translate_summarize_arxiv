{
  "article_text": [
    "in the last few years , statistical problems in large dimension received a lot of attention .",
    "that is , estimation problems where the dimension of the parameter to be estimated , say @xmath1 , is larger than the size of the sample , usually denoted by @xmath2 .",
    "this setting is motivated by modern applications such as genomics , where we often have @xmath3 the number of patients with a very rare desease , and @xmath1 of the order of @xmath4 or even @xmath5 ( cgh arrays ) , see for example @xcite and the references therein .",
    "other examples appear in econometrics , we refer the reader to belloni and chernozhukov @xcite .",
    "probably the most famous example is high dimensional regression estimation : one observes pairs @xmath6 for @xmath7 with @xmath8 , @xmath9 and one wants to find a @xmath10 such that for a new pair @xmath11 , @xmath12 would be a good prediction for @xmath13 .",
    "if @xmath14 , it is well known that a good estimation can not be performed unless we make an additional assumption . very often , it is quite natural to assume that there exists such a @xmath15 that is sparse : most of its coordinates are equal to @xmath16 . if we let @xmath17 denote the number of non - zero coordinates in @xmath15 , this means that @xmath18 . in the genomics example , it means that only a few genes are relevant to explain the desease .",
    "early examples of estimators introduced to deal with this kind of problems include the now famous aic @xcite and bic @xcite . both can be written @xmath19 where @xmath20 differs in aic and bic . despite aic and bic may give poor results when @xmath14 ( see @xcite ) , taking @xmath21 leads to estimators with very satisfying statistical properties ( @xmath22 being the variance of the noise ) .",
    "see for example @xcite for such results , and @xcite in the case of unknown variance .",
    "the main problem with this so - called @xmath23 penalization approach is that the effective computation of the estimators defined in is very time consuming . in practice , these estimators can not be used for @xmath1 more than a few tens .",
    "this motivated the study of the lasso introduced by tibshirani @xcite .",
    "this estimator is defined by @xmath24 the convexity of this minimization problem ensures that the estimator can be computed for very large @xmath1 , see efron _",
    "@xcite for example .",
    "this motivated a lot of theoretical studies on the statistical performances of this estimator .",
    "the results with the weakest hypothesis can be found in the work of bickel _ et al .",
    "_ @xcite or koltchinksii @xcite .",
    "see also very nice reviews in the paper by van de geer and bhlmann @xcite or in the phd thesis of hebiri @xcite . also note that a quantity of variants of the idea of @xmath0-penalization were studied simultaneously to the lasso : among others the basis pursuit @xcite , the dantzig selector @xcite , the elastic net @xcite ...    another problem of estimation in high dimension is the so - called problem of sparse density estimation . in this setting ,",
    "we observe @xmath2 random variables with ( unknown ) density @xmath25 and the purpose is to estimate @xmath25 as a linear combination of some functions @xmath26 ,  , @xmath27 . if @xmath14 and @xmath28 we can use the spades ( for sparse density estimator ) by bunea _ et al .",
    "_ @xcite or the iterative feature selection procedure in @xcite .",
    "one of the common features of all the theoretical studies of sparse estimators is that they focus only on the case where the observations are independent .",
    "for example , for the density estimation case , in @xcite and @xcite the observations are assumed to be iid .",
    "the purpose of this paper is to propose a unified framework .",
    "namely , we define a general stochastic optimization problem that contains as a special case regression and density estimation .",
    "we then define a general @xmath29-penalized estimator for this problem , in the special case of regression estimation this estimator is actually the lasso and in the case of density estimation it is spades .",
    "finally , we provide guarantees on the statistical performances of this estimator in the spirit of @xcite , but we do not only consider independent observations : we want to study the case of dependent observations , and prove that we can still recover the target @xmath15 in this case , under various hypothesis .",
    "we now give the general setting and notations of our paper .",
    "note that the cases of regression and density estimation will appear as particular cases .",
    "we observe @xmath2 random variables in @xmath31 .",
    "let @xmath32 be the distribution of @xmath33 .",
    "we have a function @xmath34 such that for any @xmath35 , @xmath36 is a quadratic function .",
    "the objective is the estimation of a value @xmath37 that minimizes the following expression which only depends on @xmath2 and @xmath15 : @xmath38 all the results that will follow are intended to be interesting in the case @xmath39 on the condition that @xmath40 is small .",
    "we use the following estimator : @xmath41\\ ] ] and @xmath42 denotes any solution of this minimization problem .",
    "we now detail the notations in the two examples of interest :    1 .   in the regression example , @xmath43 with the @xmath44 deterministic , and @xmath45 where @xmath46 ( the @xmath47 are not necessarily iid , they may be dependent and have different distribution ) . here",
    "we take @xmath48 . in this example",
    ", @xmath42 is known as the lasso estimator @xcite .",
    "2 .   in the density estimation case , @xmath49 have the same density wrt lebesgue measure ( but they are not necessarily independent ) .",
    "we have a family of functions @xmath50 and we want to estimate the density @xmath25 of @xmath51 by functions of the form @xmath52 in this case we take @xmath53 and note that this leads to @xmath54 then @xmath42 is the estimator known as spades @xcite .      in section [ sectionmainresult ]",
    "we provide a sparsity inequality that extend the one of bickel _ et al . _",
    "@xcite to the case of non iid variables .",
    "this result involves two assumptions : the first one is about the function @xmath55 and is already needed in the iid case .",
    "it is usually refered as restricted eigenvalue property .",
    "the other hypothesis is more involved , it is specific to the non iid case .",
    "it roughly says that we are able to control the deviations of empirical means of dependent variables around their expectations .    in section [ examples",
    "] , we provide several examples of classical assumptions on the observations that can ensure that we have such a control .",
    "these assumptions are expressed in terms of weak dependence coefficients , so in the beginning of this section we briefly introduce weak dependence .",
    "we also provide some references .",
    "we apply the results of sections [ sectionmainresult ] and [ examples ] to regression estimation in section [ regression ] and to density estimation in section [ density ] .",
    "finally the proofs are given in section [ proofs ] .",
    "first , we need an assumption on the quadratic form @xmath56 .    _",
    "* assumption * @xmath57 with @xmath58 . as @xmath59 is a quadratic form , we have the matrix @xmath60 that does not depend on @xmath15 , and we assume that the matrix @xmath61 has only @xmath62 on its diagonal ( actually , this just means that we renormalize the observations @xmath63 in the regression case , or the function @xmath64 in the density estimation case ) , that it is non - random ( here again , this is easily checked in the two examples ) and that it satisfies _",
    "@xmath65 note that this condition , usually referred as restricted eigenvalue property ( rep ) , is already required in the iid setting , see @xcite for example . in these paper",
    "it is also discussed why we can not hope to get rid of this hypothesis .",
    "we set for simplicity @xmath66 recall that as @xmath67 is a quadratic function it may be written as @xmath68 for a @xmath69-matrix valued function @xmath70 on @xmath71 and a vector function @xmath72 so that @xmath73    [ mainresult ] let us assume that assumption @xmath57 is satisfied .",
    "let us assume that the distribution @xmath32 of @xmath33 is such that there is a constant @xmath74 $ ] and a decreasing continuous function @xmath75 with @xmath76 let us put @xmath77 then @xmath78    the arguments of the proof of theorem [ mainresult ] are taken from @xcite .",
    "the proof is given in section [ proofs ] , page .    note that the hypothesis in this theorem heavily depend on the distribution of the variables @xmath79 , ",
    ", @xmath80 , and particulary on their type of dependence .",
    "section [ examples ] will provide some examples of situations where this hypothesis is satisfied .    also note that the upper bound in the inequality is minimized if we make the choice @xmath81 .",
    "then @xmath82^{2}}{n^{1 - 2\\alpha } } } \\\\ \\\\ { \\rm and } \\\\ \\\\ \\displaystyle{\\|\\hat{\\theta}_{\\lambda}-\\overline{\\theta}\\|_{1 }       \\leq \\frac{8}{\\kappa } \\frac{\\|\\overline{\\theta}\\|_{0}\\left[\\psi^{-1}\\left({\\varepsilon}/p\\right)\\right ] ^{2}}{n^{\\frac{1}{2}-\\alpha } } } \\end{array } \\right\\}\\geq 1-\\varepsilon.\\ ] ] it is important to remark that the choice @xmath83 may be impossible in practice , as the practitionner may not know @xmath84 and @xmath75 . moreover ,",
    "this choice is not necessarily the best one in practice : in the regression case with iid noise @xmath85 , we will see that this choice leads to @xmath86 .",
    "this choice requires the knowledge of @xmath87 .",
    "moreover it is not usually the best choice in practice , see for example the simulations in @xcite . even in the iid case ,",
    "the choice of a good @xmath88 in practice is still an open problem .",
    "however , note that    1 .   the question is in some sense meaningless . for example",
    "the value of @xmath88 that minimizes the quadratic risk @xmath89 is not the same than the value of @xmath88 that may ensure , under some supplementary hypothesis , that @xmath42 identifies correctly the non - zero coordinates in @xmath37 , see for example leeb and ptscher @xcite on that topic .",
    "one has to be careful to what one means when one say _ a good choice for _ @xmath88 .",
    "2 .   some popular methods like cross - validation seem to give good results for the quadratic risk , at least in the iid case .",
    "an interesting open question is to know if one can prove theoretical results for cross validation in this setting .",
    "see also the bootstrap method proposed in @xcite .",
    "3 .   the lars algorithm @xcite compute @xmath42 for any @xmath90 in a very short time ( coordinate descent algorithms @xcite are valuable alternative to lars ) .      first , note that in the regression setting ( equation [ eqreg ] ) , for any @xmath91 and @xmath92 we have @xmath93 then , in the density estimation context , @xmath94-\\varphi_{j}(z_{i}).\\end{gathered}\\ ] ]    so , in both cases , the assumption given by equation [ conditionthm ] is satisfied if we have a control of the deviation of empirical means to their expectation . in the next sections , we discuss some conditions to obtain such controls with dependent variables .",
    "in this section , we give some results that allow to control the deviation of empirical means to their expectations for general ( non iid ) obsrevations .",
    "the idea will be , in the next sections , to apply these results to the processes @xmath95 for @xmath96 . for the sake of simplicity , in this section",
    ", we deal with a generic process @xmath97 and the applications are given in the next sections .",
    "various examples of pairs @xmath98 are given .",
    "we will use the classical notation @xmath99      we are going to introduce some coefficients in order to control the dependence of the @xmath101 .",
    "the first example of such coefficients are the @xmath84-mixing coefficients first introduced by rosenblatt @xcite , @xmath102 the idea is that the faster @xmath103 decreases to @xmath16 , the less dependent are @xmath101 and @xmath104 for large @xmath105 .",
    "assumptions on the rate of decay allows to prove laws of large numbers and central limit theorems .",
    "different mixing coefficients were then studied , we refer the reader to @xcite for more details .",
    "the main problem with mixing coefficients is that they exclude too many processes .",
    "it is easy to build a process @xmath106 satisfying a central limit theorem with constant @xmath103 , see @xcite chapter 1 for an example .",
    "this motivated the introduction of _ weak dependence _ coefficients .",
    "the monograph @xcite provides a comprehensive introduction to the various weak dependence coefficients .",
    "our purpose here is not to define all these coefficients , but rather to introduce some examples that allow to satisfy condition   in theorem [ mainresult ] .",
    "we put , for any process @xmath107 , @xmath108    we precise in  -[sectmom ] and in  -[sectexp ] that suitable decays of those coefficients yield  .",
    "those two sections will provide quite different forms of the function  @xmath109 .",
    "let us assume that for any @xmath110 , for any @xmath111 and @xmath112 respectively @xmath113 and @xmath114-lipschitz , where eg .",
    ", @xmath115 we also assume that for any @xmath116 with @xmath117 , @xmath118\\right| \\leq \\psi(l_1,l_2,\\ell , m-\\ell)\\eta_{v}(r)\\ ] ] with @xmath119",
    ". then @xmath106 is said to be @xmath120-dependent with @xmath120-dependence coefficients @xmath121 .",
    "other functions @xmath122 allow to define the @xmath88 , @xmath123 and @xmath124-dependence , see @xcite .",
    "we finally provide some basic properties , proved in @xcite .",
    "the following result allows a comparison between different type of coefficients .",
    "if @xmath125 then @xmath126    finally the following property will be useful in this paper .",
    "[ lipeta ] if @xmath106 is @xmath120-dependent and @xmath25 is @xmath127-lipschitz and bounded , then @xmath128 is also @xmath120-dependent with @xmath129      in doukhan and louhichi @xcite it is proved that if for an even integer @xmath130 we have @xmath131 then marcinkiewicz - zygmund inequality follows : @xmath132 and thus @xmath133 and @xmath134 is of the order of @xmath135 in  .",
    "however , explicit constants are needed in theorem [ mainresult ] .",
    "we actually have the following result .    [ momcomb ]",
    "assume that coefficients ( [ coef ] ) fit the relation ( [ bornec ] ) for some integer @xmath136 , then marcinkiewicz - zygmund inequality follows @xmath137\\le c^qd_{2q}(2q)!n^q\\ ] ] where @xmath138    the proof follows @xcite , it is given in section [ proofs ] .",
    "sharper constants @xmath139 are also derived in the proof ( equation , page ) , one may replace the constants @xmath140 by 1 , 4 and 17 and using the recursion ( [ recur ] ) also improves the above mentioned bounds .",
    "various inequalities of this type where derived for alternative dependences ( see doukhan @xcite , rio @xcite and dedecker _ et al . _",
    "@xcite for an extensive bibliography which also covers the case of non integer exponents ) .      using the previous inequality , doukhan and louhichi",
    "@xcite proved exponential inequalities that would lead to @xmath134 in @xmath141 .",
    "doukhan and neumann @xcite use alternative cumulant techniques to get @xmath134 in @xmath142 for suitable bounds of the previous covariances ( [ coef ] ) .",
    "[ t2.1 ] @xcite let us assume that @xmath125 .",
    "let @xmath143 be one of the following functions :    * @xmath144 , * @xmath145 , * @xmath146 , * @xmath147 , for some @xmath148 .",
    "we assume that there exist constants @xmath149 , @xmath150 , and a nonincreasing sequence of real coefficients @xmath151 such that , for all @xmath152-tuples @xmath153 and all @xmath154-tuples @xmath155 with @xmath156 the following inequality is fulfilled : @xmath157 where @xmath158 then @xmath159 where @xmath160 can be chosen as any number greater than or equal to @xmath161 and @xmath162    [ remarquedn ] one can easily check that if @xmath106 is @xmath120-dependent then is satisfied with @xmath163 as in _ ( b ) _ , @xmath164 and @xmath165 , see remark 9 page 9 in @xcite .",
    "so if @xmath106 is @xmath120-dependent and @xmath166 decreases fast enough to @xmath16 then we have an exponential inequality .",
    "this result yields convienient bounds for the function @xmath109 .",
    "a recent paper by olivier wintenberger @xcite is also of interest : it directly yields alternative results from our main result . in this paper",
    ", we do not intend to provide the reader with encyclopedic references but mainly to precise some ideas and techniques so that this will be developed in further papers .",
    "assume now that @xmath106 is a centered series satisfies @xmath168 then @xmath169 may occur , eg . if @xmath170 for @xmath1710,1]$ ] then @xmath172 ; then @xmath173 holds .      in the special case of gaussian processes @xmath174 , tails of @xmath175 are classically described because @xmath176 and here @xmath177 .",
    "we thus may obtain simultaneously subgaussian tails and @xmath178 .",
    "assume that that for each @xmath179 , @xmath180 and @xmath181 is a stationary gaussian processes with , for some @xmath182 , @xmath183 , @xmath184 let @xmath185 for a function with hermite rank @xmath186 , and since @xmath187 their covariance series is non @xmath188-th summable in case @xmath171\\frac1m,1[$ ] .",
    "the case @xmath189 and @xmath171\\frac12,1[$ ] is investigated by using the following expansion in the seminal work by rosenblatt @xcite .    set @xmath190 for the covariance matrix of the gaussian random vector @xmath191 :",
    "@xmath192 quoting that @xmath193 with latexmath:[\\[c_k = b^k\\int_{0}^1\\cdots\\int_{0}^1     ( @xmath182 is given by equation  ) , this is thus clear that for small enough @xmath195 , @xmath196 here the conditions in the main theorem hold with @xmath197 and @xmath198 for any @xmath199 .",
    "in this section we apply theorem [ mainresult ] and the various examples of section [ examples ] to obtain results for regression estimation .",
    "note that the results in the iid setting are already known , they are only given here for the sake of completeness , in order to provide comparison with the other cases .",
    "let us remind that in the regression case , we want to apply the results of section [ examples ] to @xmath200 for the sake of simplicity , in this whole session dedicated to regression , let us put @xmath201      under the usual assumption that the @xmath47 are iid and subgaussian ,",
    "@xmath202 \\leq \\exp\\left(\\frac{s^{2}\\sigma^{2}}{2}\\right)\\ ] ] for some known @xmath203 , then we have @xmath204 so we can apply theorem [ mainresult ] in order to obtain the following well known result :    in the context of equation [ eqreg ] , under assumption @xmath57 , if the @xmath205 are iid and subgaussian with variance upper bounded by @xmath203 , the choice @xmath206 leads to @xmath207        let us remark that , for any @xmath96 , @xmath208 thus , we apply theorem [ mainresult ] and proposition [ momcomb ] to obtain the following result .    in the context of equation [ eqreg ] , under assumption",
    "@xmath57 , if the @xmath205 satisfy , for some even integer @xmath130 , @xmath209 the choice @xmath210 leads to @xmath211    this result aims at filling a gap for non subgaussian and non iid random variables .",
    "the result still allows to deal with the _ sparse _ case @xmath39 in case @xmath212 . in this case",
    "we deal with the case @xmath213 and we get a rate of convergence in probability @xmath214 .    if @xmath215 and @xmath216 the least squares methods apply which make such sparsity algorithms less relevant .    moreover if @xmath217 the present method is definitely not efficient .",
    "hence in the case of heavy tails , such as considered in the paper by bartkiewicz _",
    "@xcite , our results are useless . anyway , using least squares for heavy tailed models ( without second order moments ) does not look to be a good idea !      using theorem [ mainresult ] and theorem [ t2.1 ] we prove the following result .",
    "let us assume that the @xmath205 satisfy the hypothesis of theorem [ t2.1 ] : let @xmath143 be one of the functions of theorem [ t2.1 ] , we assume that there are constants @xmath149 , @xmath150 , and a nonincreasing sequence of real coefficients @xmath151 such that , for all @xmath152-tuples @xmath153 and all @xmath154-tuples @xmath155 with @xmath156 the following inequality is fulfilled : @xmath218 where @xmath158 let @xmath219 be a positive constant and let us put @xmath220 let us assume that @xmath221 , @xmath1 and @xmath2 are such that @xmath222 then for @xmath223 we have @xmath224    so the rate is the same than in the iid case .",
    "the only difference is in the constant , and a restriction for very large values of @xmath1 .",
    "for the sake of shorteness , let us put @xmath225 and note that @xmath226 .",
    "first , note that for any @xmath92 , @xmath227 if we put @xmath228 and @xmath229 . using theorem [ t2.1 ]",
    ", we obtain for any @xmath230 , @xmath231 where @xmath232 and @xmath233 in other words : @xmath234 now , let us put @xmath235 , we obtain @xmath236 remark that we can not in general compute explicitely the inverse of this function but we can upper - bound the range for @xmath152 : @xmath237 in this case , @xmath238 and so @xmath239 so we can take , following theorem [ mainresult ] , @xmath240 as soon as @xmath241 . for example , for a fixed number of observations @xmath2 and a fixed confidence level @xmath242 , we have the restriction : @xmath243 under this condition we have , by theorem [ mainresult ] , @xmath244 this ends the proof .      in order to illustrate the results",
    ", we propose a very short simulation study .",
    "the purpose of this study is not to show the good performances of the estimator in practice or to give recipes for the choice of @xmath88 .",
    "the aim is more to show that the performances of the iid setting are likely to be obtained in the dependent setting if the dependence coefficients are small .",
    "we use the following model : @xmath245 where the @xmath63 s will be treated as fixed design , but in practice will be iid vectors in @xmath246 with @xmath247 , with distribution @xmath248 where @xmath249 is given by @xmath250 .",
    "results of the experiments .",
    "the @xmath251-axis gives the value @xmath252 where @xmath253 .",
    "the @xmath13-axis gives @xmath254 the error of reconstruction of the signal .",
    "the lines code is the following : @xmath255 : solid line , @xmath256 : short dashed line , @xmath257 : dotted line , @xmath258 : dot / dash , @xmath259 : long dash.,width=415,height=245 ]    the parameter is given by @xmath260 .",
    "this is the toy example used by tibshirani @xcite .",
    "let @xmath261 - 1,1[$ ] .",
    "the noise satisfies @xmath262 , for @xmath263 , where the @xmath264 are iid @xmath265 and @xmath266 .",
    "note that this ensure that @xmath267 for any @xmath268 , so the noise level does not depent on @xmath269 . in the experiments , @xmath270 we fixed a grid of values @xmath2710,1.5[$ ] and we computed , for every experiment , the lasso estimator with @xmath272 for all @xmath273 . we have repeated the experiment @xmath274 times for every value of @xmath269 and report the results in figure [ courbes ] .",
    "we can remark that all the curves are very similar .",
    "the minimum reconstruction error is obtained for @xmath275 , that corresponds to @xmath276 . note that in the iid case , it is smaller than the theoretical value given by theorem [ mainresult ] , @xmath277 for @xmath278 , that would correspond to @xmath279 , a value that would not event stand in the figure !",
    "here we apply theorem [ mainresult ] and section [ examples ] to the context of density estimation .",
    "let us remind that in this setting , @xmath280-\\varphi_{j}(z_{i}).\\ ] ]      if the @xmath51 are iid with density @xmath25 and if @xmath281 for any @xmath92 then we can apply hoeffding inequality @xcite to upper bound @xmath282 we obtain @xmath283 so we can apply theorem [ mainresult ] .    in the context of density estimation , under assumption @xmath57 , if the @xmath51 are iid with density @xmath25 and if @xmath281 for any @xmath92 , the choice @xmath284 leads to @xmath285    this result is essentially known , see @xcite .      note that if as previously we work with bounded @xmath286 , we automatically have moments of any order .",
    "so we will only state a result based on exponential inequality .",
    "so , using theorem [ mainresult ] and theorem [ t2.1 ] we obtain :    let us assume that there are @xmath287 and @xmath288 such that @xmath286 is @xmath127-lipschitz and @xmath281 for any @xmath92 .",
    "let us assume that @xmath79 ,  , @xmath80 satisfy @xmath289 for some @xmath290 .",
    "let us put a @xmath291 , define @xmath292 and assume that @xmath1 , @xmath2 and the confidence level @xmath242 are such that @xmath293 then @xmath294    the assumption that the @xmath64 are all @xmath127-lipschitz for a constant @xmath127 excludes a lot of interesting dictionaries .",
    "if we assume that the @xmath64 are @xmath295-lipschitz ( this would be the case if we used the first @xmath2 functions in the fourier basis for example ) , then we will suffer a loss in when compared to the iid case .",
    "however , note that equation   below is the starting point of our proof , so we can not hope to find a simple way to remove this hypothesis when using @xmath120-weak dependence .",
    "this will be the object of a future work .    as @xmath296 is @xmath297-lipschitz ,",
    "using proposition [ lipeta ] we have : @xmath298",
    "so we have @xmath299 moreover , following remark  [ remarquedn ] , @xmath300 so we can apply theorem [ t2.1 ] with @xmath301 and we obtain @xmath302 with @xmath303 and @xmath304 in other words @xmath305 we then put @xmath306 to obtain @xmath307 here again , if we have @xmath308 then @xmath309",
    "so we take , following theorem [ mainresult ] , @xmath310 and we obtain , with probability at least @xmath311 , @xmath312",
    "in this paper , we showed how the lasso and other @xmath0-penalized methods can be extended to the case of dependent random variables .",
    "an open and ambitious question to be adressed later is to find a good data - driven way to calibrate the regularization parameter @xmath88 when we do nt know in advance the dependence coefficients of our observations .",
    "anyway this first step with sparsity in the dependent setting is done for accurate applications and our brief simulations let us think that such techniques are reasonable for time series .    here again extensions to random fields or to dependent point processes seem plausible .",
    "by definition , @xmath313 and so @xmath314\\right\\ } d\\mathbb{p}(z_{1},\\ldots , z_{n } ) \\\\     - \\frac{1}{n}\\sum_{i=1}^{n } \\left[q(z_{i},\\hat{\\theta}_{\\lambda})-q(z_{i},\\overline{\\theta})\\right ] + \\lambda \\left(\\|\\overline{\\theta}\\|_{1}-\\|\\hat{\\theta}_{\\lambda}\\|_{1}\\right).\\end{gathered}\\ ] ] now , as @xmath55 is quadratic wrt @xmath15 we have , for any @xmath315 , @xmath316 moreover , as @xmath37 is the minimizer of @xmath317 , we have the relation @xmath318 pluging and into leads to @xmath319 and then @xmath320 now , we remind that we have the hypothesis @xmath321 that becomes , with a simple union bound argument , @xmath322 and so , if we put @xmath323 , @xmath324 also remark that @xmath325 .",
    "so until the end of the proof , we will work on the event @xmath326 true with probability at least @xmath311 .",
    "going back to , we have @xmath327 and then @xmath328 & = \\lambda \\left ( \\sum_{j=1}^{p }                      + \\sum_{j:\\overline{\\theta}_{j}\\neq 0}(|\\overline{\\theta}_{j}|-|(\\hat{\\theta}_{\\lambda})_{j}| )            \\right)\\end{aligned}\\ ] ] that leads to the following inequality that will play a central role in the end of the proof : @xmath329 , leads to @xmath330 in assumption @xmath331 .",
    "so , leads to @xmath332 \\nonumber & \\leq 2 \\lambda \\left ( \\|\\overline{\\theta}\\|_{0 }                       \\sum_{j:\\overline{\\theta}_{j}\\neq 0 } [ ( \\hat{\\theta}_{\\lambda})_{j}-\\overline{\\theta}_{j}]^{2 }                             \\right)^{\\frac{1}{2 } } \\\\[-1pt ] \\nonumber & \\leq 2 \\lambda \\left ( \\frac{\\|\\overline{\\theta}\\|_{0}}{\\kappa } ( \\hat{\\theta}_{\\lambda}-\\overline{\\theta})'\\frac{\\mathbf{m}}{2}(\\hat{\\theta } _ { \\lambda}-\\overline{\\theta } )                      \\right)^{\\frac{1}{2 } } \\\\[-1pt ] & = 2 \\lambda \\left ( \\frac{\\|\\overline{\\theta}\\|_{0}}{\\kappa }                      \\left[r(\\hat{\\theta}_{\\lambda})-r(\\overline{\\theta})\\right ]                      \\right)^{\\frac{1}{2}}. \\label{step5}\\end{aligned}\\ ] ] we conclude that @xmath333 now remark that to states that a convex quadratic function of @xmath334 $ ] is negative , so both roots of that quadratic are real .",
    "this leads to @xmath335 this ends the proof .",
    "first @xmath336 the same combinatorial arguments as in @xcite yield for @xmath337 @xmath338 let us now assume the condition ( [ bornec ] ) then @xmath339 a rough bound is thus @xmath340 and we thus derive @xmath341 now using precisely condition ( [ bornec ] ) with the relation ( [ eqrec ] ) we see that if @xmath342 , and @xmath343 then the sequence recursively defined as @xmath344 satisfies @xmath345 } n^{[\\frac m2]}. $ ] remember that @xmath346 hence as in @xcite we quote that @xmath347 is less that the @xmath188-th catalan number , @xmath348 and this ends the proof .",
    "h.  akaike .",
    "information theory and an extension of the maximum likelihood principle . in b.",
    "n. petrov and f.  csaki , editors , _",
    "2nd international symposium on information theory _ , pages 267281 .",
    "budapest : akademia kiado , 1973 .",
    "a.  belloni and v.  chernozhukov .",
    "high dimensional sparse econometric models : an introduction . in p.",
    "alquier , e.  gautier , and g.  stoltz , editors , _ inverse problems and high - dimensional estimation_. springer lecture notes in statistics , 2011 ."
  ],
  "abstract_text": [
    "<S> the aim of this paper is to provide a comprehensive introduction for the study of @xmath0-penalized estimators in the context of dependent observations . </S>",
    "<S> we define a general @xmath0-penalized estimator for solving problems of stochastic optimization . </S>",
    "<S> this estimator turns out to be the lasso @xcite in the regression estimation setting . </S>",
    "<S> powerful theoretical guarantees on the statistical performances of the lasso were provided in recent papers , however , they usually only deal with the iid case . here </S>",
    "<S> , we study this estimator under various dependence assumptions .    * * </S>"
  ]
}