{
  "article_text": [
    "recurrent neural networks ( rnns ) are able to take a data sequence of arbitrary length as input , map it to a hidden state , and then use this hidden state to make a prediction .",
    "this prediction can be a single value , but it can also be a entire new sequence . in this paper",
    "we will study a ` character - level rnn ' , i.e.  a language model that learns to predict the next character of a text given the history of characters seen . at every time step a new character is fed into the rnn , which then transforms its hidden state to be able to predict the next character . such a type of rnn is also used in applications such as speech to text translation @xcite and video frame tagging @xcite . in the next section will illustrate how such a model can be learned using different training and prediction schedules , and how this affects the model efficiency .",
    "in all experiments from this section we will use the following neural network architecture :    input ( 65 dimensions )  lstm layer ( 50 dimensions , input / output / forget gate non - linearity : sigmoid , cell non - linearity : tanh )  dense layer ( 65 dimensions , softmax non - linearity ) .",
    "the number of input and output dimensions is 65 , since there are 65 different characters in our text corpus , and we encode every character in a one - hot representation .",
    "we choose to use a long short - term memory ( lstm ) layer above a simple recurrent connection as this is currently state of the art in many text mining tasks @xcite .",
    "to train this neural network we use truncated backprogratation through time ( tbptt ) .",
    "every @xmath0 time steps the gradients are backpropagated for @xmath1 time steps in the past ( we use the same parameter notation as in @xcite ) . to allow for temporal dependencies in the input sequence between every gradient update",
    ", we can additionally ensure that @xmath2 . the choice of these parameters can be a trade - off between computation time and model effectiveness . to calculate the gradients in the tbptt",
    ", we use the categorical cross entropy as loss function .    apart from choosing these parameters ,",
    "there is a choice of how to implement the training and prediction procedures , which can influence both model effectiveness and training speed .",
    "below we list four different schedules for the character - level rnn .",
    "a visualization of each of the schedules is shown in figure [ fig : schedules ] .",
    "schedule 1  in this schedule we take sequences of fixed length @xmath1 as input for training , and for every input character in such a sequence we predict the next character .",
    "this leads to @xmath1 losses that we backpropagate through the network .",
    "the hidden and cell states in the lstm layer are reset to their initial values for every input sequence .",
    "these initial states are learned through backpropagation . at test time , we take an entire sequence of length @xmath1 as input , and we ask to predict the single next character .    schedule 2  this schedule is similar to the first one , but instead of making a prediction for every input character at train time , we now only predict the final next character . we therefore only have a single loss that is backpropagated . at test time , the procedure is the same as in schedule 1 .",
    "schedule 3  here , the training procedure is the same as in the first schedule . at test time",
    "we start predicting using the learned initial hidden state .",
    "after that the subsequent character is predicted for every input character , after which the hidden state is updated to be used in the next prediction .",
    "schedule 4  at train time , the initial hidden state for a new input sequence is reused from the previous input sequence . more specifically , as initial hidden state for the input character at time @xmath3 we use the hidden state produced by input character at time @xmath4 from the previous input sequence .",
    "the prediction procedure in this final schedule is the same as in schedule 3 . to evaluate the predictive capacity of a character - level rnn",
    ", we use the perplexity measure , traditionally used to measure the effectiveness of language models .",
    "we will evaluate the four schedules explained above in terms of perplexity vs.  training time and training input sequences : the most efficient implementations should reach a lower perplexity faster than the other ones . every implementation is run a 100 times with random settings for the @xmath1 and @xmath0 parameters : @xmath5 .",
    "this is all done on the same hardware , and for a total of 500,000 input train sequences .",
    "we employ adam gradient updates @xcite with a batch size of 50 across all experiments , and we use the lasagne implementation framework ( github.com/lasagne/lasagne ) .",
    "our dataset consists of excerpts from shakespearian plays ; the train set has around 1,100,000 characters , and the test set around 11,000 .",
    "it is clear from the figures that all settings for schedules 1 and 2 converge smoothly towards an optimum , but schedule 1 is more efficient . schedules 3 and 4 have noisy behaviour in the beginning of the training phase .",
    "after that all settings for schedule 4 seem to converge to the same optimum , but some parameter settings for schedule 3 continue to behave very noisily .",
    "schedule 1 , however , seems to be performing best and most consistently .",
    "we observe that the settings for which the lowest perplexity is reached the fastest , all have a small @xmath6 .",
    "since @xmath2 , this means that frequent model updates over short input sequences are preferred .",
    "experimental code and data can be found on : https://github.com/cedricdeboom/charrnn .",
    "we tested the efficiency of multiple training and prediction schedules of a character - level recurrent neural network , in terms of model effectiveness as a function of training time and the number of training input sequences .",
    "we observed that the choice of a particular schedule can considerably impact the efficiency of the model .",
    "it also turns out that training over short input sequences and with frequent model updates is most efficient .",
    "further research is however required to verify if these conclusions hold for more complex models and other datasets .",
    "we thank pontus stenetorp for his useful feedback on drafts of this paper .",
    "we also acknowledge nvidia for its generous hardware support .",
    "cedric de boom is funded by a phd  grant of the flanders research foundation ( fwo ) .",
    "steven bohez is funded by a phd  grant of the agency for innovation by science and technology in flanders ( iwt ) ."
  ],
  "abstract_text": [
    "<S> we present four training and prediction schedules from the same character - level recurrent neural network . </S>",
    "<S> the efficiency of these schedules is tested in terms of model effectiveness as a function of training time and amount of training data seen . </S>",
    "<S> we show that the choice of training and prediction schedule potentially has a considerable impact on the prediction effectiveness for a given training budget . </S>"
  ]
}