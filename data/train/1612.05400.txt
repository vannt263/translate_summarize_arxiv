{
  "article_text": [
    "content - based image retrieval ( cbir ) aims at effectively indexing and mining large image databases such that given an unseen query image we can effectively retrieve images that are similar in content . with the deluge in medical imaging data , there is a need to develop cbir systems that are both fast and efficient .",
    "however , in practice , it is often infeasible to exhaustively compute similarity scores between the query image and each image within the database .",
    "adding to the challenge of scalability of cbir systems is the less understood semantic gap between the visual content of the image and the associated expert annotations  @xcite .",
    "to address these challenges , hashing based cbir systems have come to a forefront where the system indexes each image with a compact similarity preserving binary code that could be potentially leveraged for very fast retrieval .    towards this end",
    ", we propose an end - to - end one - stage deep residual hashing ( drh ) network to directly generate hash codes from input images .",
    "specifically , the drh model constitutes of a sub - network with multiple residual convolutional blocks for learning discriminative image representations followed by a fully - connected hashing layer to generate compact binary embeddings . through extensive validation",
    ", we demonstrate that drh learns discriminative hash codes in an end - to - end fashion and demonstrates high retrieval quality on standard chest x - ray image databases .",
    "the existing hashing methods proposed for efficient encoding and searching approaches have been proposed for large scale retrieval in machine learning and medical image computing can be categorised into : * ( 1 ) * shallow learning based hashing methods like locality sensitive hashing ( lsh )  @xcite ) , data - driven methods _",
    "e.g. _ iterative quantization ( itq )  @xcite , kernel sensitive hashing  @xcite , circulent binary embedding ( cbe )  @xcite , metric hashing forests ( mhf )  @xcite ; * ( 2 ) * hashing using deep architectures ( only binarization without feature learning ) including restricted boltzmann machines in semantic hashing  @xcite , autoencoders in supervised deep hashing  @xcite _ etc . _ and * ( 3 ) * application - specific hashing methods including weighted hashing for histopathological image search  @xcite , binary code tagging for chest x - ray images  @xcite , forest based hashing for neuron images  @xcite , to name a few .",
    "the ultimate objective of earning similarity preserving hashing functions is to generate embeddings in a latent hamming space such that the class - separability is preserved while embedding and local neighborhoods are well defined and semantically relevant .",
    "this can be visualized in 2d by generating the t - stochastic neighborhood embedding ( t - sne )  @xcite of unseen test data post learning like shown in fig .",
    "[ fig : nettsne ] . starting from fig ..  [ fig : nettsne](a ) which is generated by a purely un - superivsed setting we aim at moving towards fig ..  [ fig : nettsne](d ) which is closer to an ideal embedding .",
    "in fact , fig .  [",
    "fig : nettsne ] represents the results of our proposed drh approach in comparison to other methods and baselines .",
    "* hand - crafted features * : conventional hashing methods including lsh , itq , ksh , mhf _ etc . _ perform encoding in two stages : firstly , generating a vector of hand - crafted descriptors and a second stage involving hashing learning to preserve the captured semantics in a latent hamming space .",
    "these two independent stages may lead to sub - optimal results as the image descriptors may not be tailored for hashing .",
    "moreover , hand - crafting requires significant domain knowledge and extensive parameter tuning which is particularly undesirable .    * conventional deep learning * : using point - wise loss - functions like cross - entropy , hinge loss _ etc . _ for training ( / finetuning ) deep networks may not lead to feature representations that are sufficiently optimal for the task of retrieval as they do not consider crucial pairwise relationships between instances  @xcite .    * simultaneous feature learning and hashing * : recently , with the advent of deep learning for hashing we are able to perform effective end - to - end learning of binary representations directly from input images .",
    "these include deep hashing for compact binary code learning  @xcite , deep hashing network for effective similarity retrieval  @xcite , simultaneous feature learning and hashing  @xcite _ etc . _",
    "to name a few .",
    "however , a crucial disadvantage of these deep learning for hashing methods is that with very deep versions of these networks accuracy gets saturated and often degrades  @xcite .",
    "in addition to this , the continuous relaxation of hash codes to train deep networks to be able to learn with more viable continuous optimisation methods ( gradient - descent based methods ) could potentially lead to uncontrolled quantization and distance approximation errors during binarization .    in an attempt to redress the above short - comings of the existing approaches ,",
    "we make the following contributions with our work : * 1 ) * we , for the first lime , design a novel deep hash function learning framework using deep residual networks for representation learning ; * 2 ) * we introduced a neighborhood component analysis - inspired loss suitably tailored for learning discriminative hash codes ; * 3 ) * we leverage multiple hashing related losses and regularizations to control the quantization error while binarization of hash codes and to encourage hash codes to be maximally independent of each other ; and * 4 ) * clinically , to the best of our knowledge , this is the first retrieval work on medical images ( specifically , chest x - ray images ) to discuss co - morbidities _",
    "i.e. _ co - occuring manifestations of multiple diseases .",
    "the paper also aims at encouraging further discussion on the following aspects of cbir through drh :    1 .",
    "* trainability * : how do we train very deep neural networks for hashing ? does introducing residual connections aid in this process ?",
    "* representability * : do networks tailored for the dataset at hand learn better representations over transfer learning ? 3 .",
    "* compactness * : do highly compact binary representations effectively compress the desired semantic content within an image ? do loss functions to control quantization error while binarzing aid in improved hash coding ?",
    "* semantic - similarity preservation * : do we learn hash codes such that neighbourhoods in the hamming space comprise of semantically similar instances ?",
    "* joint optimisation * : does end - to - end implicit learning of hash codes work better than a two stage learning process where the images are embedded to a latent space and then quantized explicitly _ via _ hashing ?",
    "an ideal hashing method should generate codes that are compact , similarity preserving and easy to compute representations ( typically , binary in nature ) , which can be leveraged for accurate search and fast retrieval  @xcite .",
    "the desired similarity preserving aspect of the hashing function implies that _ semantically similar images are encoded with similar hash codes_. mathematically , hashing aims at learning a mapping @xmath0 , such that an input image @xmath1 can be encoded into a @xmath2 bit binary code @xmath3 . in hashing for image retrieval , we typically define a similarity matrix @xmath4 , where @xmath5 implies images @xmath6 and @xmath7 are similar and @xmath8 indicates they are dissimilar .",
    "similarity preserving hashing aims at learning an encoding function @xmath9 such that the similarity matrix @xmath10 is maximally - preserved in the binary hamming space .",
    "we start with a deep convolutional neural network architecture inspired in part by the seminal resnet architecture proposed for image classification by he _",
    "et al . _",
    "@xcite . as shown in fig .",
    "[ fig : netarchdrh ] , the proposed architecture consists of the a convolutional layer ( conv 1 ) followed by a sequence of residual blocks ( conv 2 - 5 ) and terminating in a final fully connected hashing ( fch ) layer for hash code - generation .",
    "the unique advantages offered by the proposed resnet architecture for hashing over a typical convolutional neural network are as follows :    * * training of very deep networks * : the representational power of deep networks should ideally increase with increased depth .",
    "it is empirically observed that in deep feed - forward nets beyond a certain depth , adding additional layers results in higher training and validation error ( despite using batch normalization )  @xcite .",
    "residual networks seamlessly solves this _ via _ adding short cut connections that are summed with the output of the convolutional blocks . * * ease of optimization * : a major issue to training deep architectures is the problem of vanishing gradients during training ( this is in part mitigated with the introduction of rectified linear units ( relu ) , input batch normalisation and layer normalisation ) .",
    "residual connections offer additional support _ via _ a no - resistance path for the flow of gradients along the shortcut connections to reach the shallow learning layers .      in order to learn feature embeddings tailored for retrieval and specifically for the scenario at hand where the pairwise similarity matrix @xmath10 should be preserved , we propose our supervised retrieval loss drawing inspiration from the neighbourhood component analysis  @xcite . to encourage the learnt embedding to be binary in nature , we squash the output of the residual layers to be within @xmath11 $ ] by passing it through a hyperbolic tangent ( tanh ) activation function .",
    "the final binary hash codes @xmath12 are generated by quantizing the output of the tanh activation function ( say , @xmath13 ) as follows : @xmath14 . given @xmath15 instances and the corresponding similarity matrix is defined as @xmath16 , the proposed supervised retrieval loss is formulated as : @xmath17    where @xmath18 is the probability that any two instances ( @xmath19 and @xmath20 ) can be potential neighbours .",
    "inspired by knn classification , where the decision of an unseen test sample is determined by the semantic context of its local neighbourhood in the embedding space , we define @xmath18 as a softmax function of the hamming distance ( indicated as @xmath21 ) between the hash codes of two instances and is derived as : @xmath22    as gradient based optimisation of @xmath23 in a binary embedding space is infeasible due to its non - differentiable nature , we use a continuous domain relaxation and substitute non - quantized embeddings @xmath24 in place of hash code @xmath25 and euclidean distance as as surrogate of hamming distance between binary codes .",
    "this is derived as : @xmath26 .",
    "it must be noted that such an continuous relaxation could potentially result in uncontrollable quantization error and large approximation errors in distance estimation . with continuous relaxation , eq .",
    "is now differentiable and continuous thus suited for backpropagation of gradients during training .",
    "generation of high quality hash codes requires us to control this quantization error and bridge the gap between the hamming distance and its continuous surrogate . in this paper , we jointly optimise for @xmath23 and improve hash code generation by imposing additional loss functions as follows :    * quantization loss * : in the seminal work on iterative quantization ( itq ) for hashing  @xcite , gong and lazebnik introduced the notion of quantization error @xmath27 as @xmath28 .",
    "optimising for @xmath27 required a computation intensive alternating optimisation procedure and is not compatible with back propagation which is used to train deep neural nets ( due to non - differentiable sgn function within the formulation ) . towards this end , we use a modified point - wise quantization loss function proposed by zhu _ et al .",
    "_ sans the sgn function as @xmath29  @xcite .",
    "they establish that @xmath30 is an upper bound over @xmath27 , therefore can be deemed as a reasonable loss function to control quantization error . for ease of back - propagation , we propose to use a differentiable smooth surrogate to @xmath31 norm @xmath32 and derived the proposed quantization loss function as:@xmath33 . with the incorporation of the quantization loss , we hypothesise that the final binarization step would incur significantly less quantization error and the loss of retrieval quality (",
    "also empirically validated in section  [ sec : results ] ) .",
    "* bit balance loss * : in addition to @xmath34 , we introduce an additional bit balance loss @xmath35 to maximise the entropy of the learnt hash codes and in effect create balanced hash codes . here",
    ", @xmath35 is derived as : @xmath36 .",
    "this loss aims at encouraging maximal information storage within each hash bit .",
    "* regularisation * : inspired by itq  @xcite , we also introduce a relaxed orthogonality regularisation constraint @xmath37 on the convolutional weights ( say , @xmath38 ) connecting the output of the final residual block of the network to the hashing block .",
    "this weakly enforces that the generated codes are not correlated and each of the hash bits are independent .",
    "here , @xmath37 is formulated as : @xmath39 . in additon to @xmath37 , we also impose weight decay regularization @xmath40 to control the scale of learnt weights and biases .      in this section , we detail on the training procedure for the proposed drh network with respect to the supervised retrieval and hashing related loss functions .",
    "we learn a single - stage end - to - end deep network to generate hash codes directly given an input image .",
    "we formulate the optimisation problem to learn the parameters of our network ( say , @xmath41 ) :    @xmath42    where @xmath43 , @xmath44 , @xmath45 and @xmath46 are four parameters to balance the effect of different contributing terms . to solve this optimisation problem",
    ", we employ stochastic gradient descent to learn optimal network parameters .",
    "differentiating @xmath47 with respect to @xmath48 and using chain rule , we derive : @xmath49 the second term @xmath50 is computed through gradient back - propagation . the first term ( @xmath51 )",
    "is the gradient of the composite loss function @xmath47 with respect to the output hash codes of the drh network .",
    "we differentiate the continuous relaxation of the supervised retrieval loss function with respect to the hash code of a single example ( @xmath52 ) as follows  @xcite :    = 2 ( _ l : s_li > 0 p_lid_li - _ l i ( _ q : s_lq > 0 p_lq ) p_lid_li ) - 2 ( _ j : s_ij > 0 p_ijd_ij - _ j : s_ij > 0 p_ij ( _ z i p_izd_iz ) ) [ eq : derjs ]    where @xmath53 .",
    "the derivatives of hashing related loss functions ( @xmath34 and @xmath35 ) are derived as : @xmath54 and @xmath55 the regularisation function @xmath37 acts on the convolutional weights corresponding to the hash layer ( @xmath38 ) and its derivative with respect to @xmath38 is derived as follows : @xmath56 .",
    "having computed the gradients of the individual components of the loss function with respect to the parameters of drh , we apply gradient - based learning rule to update @xmath48 .",
    "we use mini - batch stochastic gradient descent ( sgd ) with momentum .",
    "sgd incurs limited memory requirements and reduces the variance of parameter updates .",
    "the addition of the momentum term @xmath57 leads to stable convergence .",
    "the update rule for the weights of the hash layer is derived as :    @xmath58    the convolutional weights and biases of the other layers are updated similarly .",
    "it must be noted that the learning rate @xmath59 in eq  [ eq : updatewh ] is an important hyper - parameter .",
    "for faster learning , we initialise it the largest learning rate that stably decreases the objective function ( typically , at @xmath60 or @xmath61 ) . upon convergence at a particular setting of @xmath59 ,",
    "we scale the learning rate multiplicatively by a factor of @xmath62 and resume training.this is repeated until convergence or reaching the maximum number of epochs .",
    "* database * : we conducted empirical evaluations on the publicly available indiana university chest x - rays ( cxr ) dataset archived from their hospital s picture archival systems  @xcite .",
    "the fully - anonymized dataset is publicly available through the openi image collection system  @xcite . for this paper ,",
    "we use a subset of 2,599 frontal view cxr images that have matched radiology reports available for different patients . following the label generation strategy published in  @xcite for this dataset , we extracted nine most frequently occurring unique patterns of medical subject headings ( mesh ) terms related to cardiopulmonary diseases from these expert - annotated radiology report  @xcite .",
    "these include normal , opacity , calcified granuloma , calcinosis , cardiomegaly , granulomatous disease , lung hyperdistention , lung hypoinflation and nodule .",
    "the dataset was divided into non - overlapping subsets for training ( 80% ) and testing ( 20% ) with patient - level splits .",
    "the semantic similarity matrix @xmath10 is contructed using the mesh terms _ i.e. _ a pair of images are considered similar if they share atleast one mesh term .",
    "* comparative methods and baselines * : we evaluate and compare the retrieval performance of the proposed drh network with nine state - of - the art methods including five unsupervised shallow - learning methods : lsh  @xcite , itq  @xcite , cbe  @xcite ; two supervised shallow - learning methods : ksh  @xcite and mhf  @xcite and two deep learning based methods : alexnet - ksh ( a - ksh )  @xcite and vggf - ksh ( v - ksh )  @xcite . to justify the proposed formulation ,",
    "we include simplified four variants of the proposed drh network as baselines : dph ( deep plain net hashing ) by removing the residual connections , drhnq ( deep residual hashing without quantization ) by removing the hashing related losses and generating binary codes only through tanh activation , drn - ksh by training a deep residual network with only the supervised retrieval loss and quantizing through ksh post training and drh - nb which is a variant of drh where continuous embeddings are used sans quantization , which may act as an upper bound on performance .",
    "we used the standard metrics for evaluating retrieval quality as proposed by lai _",
    "et al . _",
    "@xcite : mean average precision ( map ) and precision - recall curves varying the code size(16 , 32 , 48 and 64 bits ) . for fair comparison , all the methods were trained and tested on identical data folds . the retrieval performance of methods involving residual learning and baselines is evaluated for two variants varying the number of layers : @xmath63 and @xmath64 .",
    "for the shallow learning methods , we represent each image as a 512 dimensional gist vector  @xcite . for the drh and associated baselines ,",
    "the input image is resized to @xmath65 and normalized to a dynamic range of 0 - 1 using the pre - processing steps discussed in  @xcite . for a - ksh and v - ksh ,",
    "the image normalization routines were identical to that reported in the original works  @xcite  @xcite .",
    "we implement all our deep learning networks ( including drh ) on the open - source matconvnet framework  @xcite .",
    "the hyper - parameters @xmath43 , @xmath44 and @xmath66 were set at 0.05 , 0.025 and 0.01 empirically .",
    "the momentum term @xmath57 was set at 0.9 , the initial learning rate @xmath59 at @xmath60 and batchsize at 128 .",
    "the training data was augmented on - the - fly extensively through jittering , rotation and intensity augmentation by matching histograms between images sharing similar co - morbidities .",
    "all the comparative deep learning methods were also trained with similar augmentation .",
    "furthermore , for a - ksh and v - ksh variants , we pre - initialized the network parameters from the pre - trained models by removing the final probability layer  @xcite  @xcite .",
    "these network learnt a @xmath67-dimensional embedding by fine - tuning it with cross - entropy loss .",
    "the hashing was performed explicitly through ksh upon convergence of the network .    * results * : [ sec : results ] the results of the map of the hamming ranking for varying",
    "code sizes of all the comparative methods are listed in table  [ wrap - tab:1 ] .",
    "we report the precision - recall curves for the comparative methods at a code size of 64 bits in fig .",
    "[ fig : prcomp ] . to justify the proposed formulation for drh , several variants of drh (",
    "namely , drn - ksh , dph , drh - nq and drh - nb ) were investigated and compare their retrieval results are tabulated in table  [ wrap - tab:2 ] .",
    "in addition to map , we also report the retrieval precision withing hamming radius of 2 ( p @ h2 ) .",
    "the associated precision - recall curves are shown in fig .",
    "[ fig : prbl ] .",
    "[ fig : prcomp ]     [ fig : prbl ]",
    "within this section , we present our discussion answering the questions posed in section  [ sec : intro ] , w.r.t . to the results and observations we reported in section  [ sec : results ] .",
    "* trainability * : the introduction of residual connections offers short - cut connections which act as zero - resistance paths for gradient flow thus effectively mitigating vanishing of gradients as network depth increases .",
    "this is strongly substantiated by comparing the performance of drh - 34 to drh - 18 _ vs. _ the plain net variants of the same depth dph - 34 to dph - 18 .",
    "there is a strong improvement in map with increasing depth for drh of about 9.3% . on the other hand ,",
    "we observe a degradation of 2.2% map performance on increasing layer depth in dph .",
    "the performance of drh-18 is fractionally better than dph - 18 indicating that drh exhibits better generalizability and the degradation problem is addressed well as we have significant map gains from increased depth . with the introduction of batch normalisation and residual connections ,",
    "we ensure that the signals during forward pass have non - zero variances and that the back propagated gradients exhibit healthy norms .",
    "therefore , neither forward nor backward signals vanish within the network .",
    "this is substantiated by the differences in map observed in table  [ wrap - tab:1 ] between methods using bn ( drh , dph and v - ksh ) in comparison to a - ksh which does not use bn .",
    "* representability * : ideally , the latent embeddings in the hamming space should be such that similar samples are mapped closer while simultaneously mapping dissimilar samples further apart .",
    "we plot the t - stochastic neighbourhood embeddings ( t - sne )  @xcite of the hash codes for four comparative methods ( gist - itq , vggf - ksh , dph - 18 and drh - 34 ) in fig .",
    "[ fig : nettsne ] to visually assess the quality of the hash codes generated .",
    "visually , we observe that hand - crafted gist features with unsupervised hashing method itq fail to sufficiently induce semantic separability . in comparison , though vggf - ksh improves significantly owing to network fine - tuning , better embedding results from drh - 34 ( dph-18 is highly comparable to drh-34 ) .",
    "additionally , the significant differences in map reported in table  [ wrap - tab:1 ] between these methods substantiates our hypothesis that in scenarios of limited training data it is better to train smaller models from scratch over finetuning to avoid overfitting ( drh - 34 has 0.183 m in comparison to vggf with 138 m parameters ) .",
    "also the significant domain shift between natural images ( imagenet - vggf ) and cxr poses a significant challenge for generalizability of networks finetuned from pre - trained nets .",
    "l5.6 cm    * compactness * : hashing aims at generating compact representations preserving the semantic relevance to the maximal extent .",
    "varying the code sizes , we observe from table  [ wrap - tab:1 ] that the map performance of majority of the supervised hashing methods improves significantly .",
    "in particular for drh - 34 , we observe that the improvement in the performance from 48 bits to 64 bits is only fractional .",
    "the performance of drh - 34 at 32 bits is highly comparable to drh - 18 at 64 bits .",
    "this testifies that with increasing layer depth drh learns more compact binary embeddings such that shorter codes can already result in good retrieval quality .    * semantic similarity preservation * : visually assessing the t - sne representation of gist - itq ( fig .",
    "[ fig : nettsne](a ) ) we can observe that it fails to sufficiently represent the underlying semantic relevance within the cxr images in the latent hamming space , which retestifies the concerns over hand - crafted features that were raised in section  [ sec : motivation ] .",
    "vggf - ksh ( fig .",
    "[ fig : nettsne](b ) ) improves over gist - itq substantially , however it fails to induce sufficient class - separability . despite ksh considering pair - wise relationships while learning to hash ,",
    "the feature representation generated by fine - tuned vgg - f is limited in representability as the cross - entropy loss is evaluated point - wise . finally , the tsne embedding of drh - 34 shown in fig .",
    "[ fig : nettsne ] visually reaffirms that semantic relevance remains preserved upon embedding and the method generates clusters well separated within the hamming space .",
    "the high degree of variance associated with the tsne embedding of normal class ( red in color ) is conformal with the high population variability expected within that class .",
    "[ fig : netresults ] demonstrates the first five retrieval results sorted according to their hamming rank for four randomly selected cxr images from the testing set . in particular , for case ( d ) , where we observe that the top neighbours ( d 1 - 5 ) share atleast one co - occurring pathology .",
    "for cases ( a ) , ( b ) and ( c ) , all the top five retrieved neighbours share the same class .",
    "r0.5    * joint optimisation * : the main contribution of the work hinges on the hypothesis that performing an end - to - end learning of hash codes is better than a two stage learning process .",
    "comparative validations against the two - stage deep learning methods ( a - ksh , v - ksh and baseline variant drn - ksh ) strongly support this hypothesis .",
    "in particular , we observe over 14.2% improvement in map comparing drn - ksh ( 34 - l ) to drh - 34 .",
    "this difference in performance may be owed to a crucial disadvantage of drn - ksh that the generated feature representation is not optimally compatible to binararization .",
    "we can also observe that , drh - 18 and drh - 34 incur very small average map decrease fo 1.8% and 0.7% when binarizing hash codes against non - binarized continuous embeddings in drh - b- 18 and drh - b - 34 respectively .",
    "in contrast , drh - nq suffers from very large map decreases of 6.6% and 10.8% in comparison to drh - b. these observations validate the need for the proposed quantization loss as it leads to nearly lossless binarization .",
    "in this paper , we have presented a novel deep learning based hashing approach leveraging upon residual learning , termed as deep residual hashing ( drh ) .",
    "drh integrates representation learning and hash coding into a joint optimisation framework with dedicated losses for improving retrieval performance and hashing related losses to control the quantization error and improve the hash code quality .",
    "our approach demonstrated very promising results on a challenging chest x ray dataset with co - occurring morbidities .",
    "taking insights from this pilot study on retrieval of cxr images with cardiopulmonary diseases , we believe gives rise to the following open questions for further discussion : how deep is deep enough ? how does drh extend to include an additional anatomical view ( like the dorsal view for cxr ) improve retrieval performance ?",
    "does drh generalize to unseen disease manifestations ? ; and can we visualize what drh learns ? in conclusion , we believe that our paper strongly supports our initial premise of using drh for retrieval but also opens up questions for future discussions .                          mesbah s , conjeti s , kumaraswamy a , rautenberg p , navab n , katouzian a. hashing forests for morphological search and retrieval in neuroscientific image databases . in miccai 2015 , pp .",
    "135 - 143 , springer international publishing .",
    "demner - fushman d , kohli md , rosenman mb , shooshan se , rodriguez l , antani s , thoma gr , mcdonald cj .",
    "preparing a collection of radiology examinations for distribution and retrieval . in jamia .",
    "2015 jul 1:ocv080 ."
  ],
  "abstract_text": [
    "<S> hashing aims at generating highly compact similarity preserving code words which are well suited for large - scale image retrieval tasks . </S>",
    "<S> most existing hashing methods first encode the images as a vector of hand - crafted features followed by a separate binarization step to generate hash codes . </S>",
    "<S> this two - stage process may produce sub - optimal encoding . in this paper , for the first time , we propose a deep architecture for supervised hashing through residual learning , termed deep residual hashing ( drh ) , for an end - to - end simultaneous representation learning and hash coding . </S>",
    "<S> the drh model constitutes four key elements : ( 1 ) a sub - network with multiple stacked residual blocks ; ( 2 ) hashing layer for binarization ; ( 3 ) supervised retrieval loss function based on neighbourhood component analysis for similarity preserving embedding ; and ( 4 ) hashing related losses and regularisation to control the quantization error and improve the quality of hash coding . </S>",
    "<S> we present results of extensive experiments on a large public chest x - ray image database with co - morbidities and discuss the outcome showing substantial improvements over the latest state - of - the art methods . </S>"
  ]
}