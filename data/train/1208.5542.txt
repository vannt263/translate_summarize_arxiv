{
  "article_text": [
    "recently , graph has been extensively used to abstract complex systems and interactions in emerging `` big data '' applications , such as social network analysis , world wide web , biological systems and data mining . with the increasing growth in these areas ,",
    "petabyte - sized graph datasets are produced for knowledge discovery  @xcite , which could only be solved by distributed machines ; benchmarks , algorithms and runtime systems for distributed graph have gained much popularity in both academia and industry  @xcite .",
    "one of the most widely used graph - searching algorithms is breadth - first search ( bfs ) , which serves as a building block for a great many graph algorithms such as minimum spanning tree , betweenness centrality , and shortest paths  @xcite .",
    "implementing a distributed bfs with high performance , however , is a challenging task because of its expensive communication cost  @xcite .",
    "generally , algorithms have two kinds of costs : arithmetic and communication . for distributed algorithms , communication often costs significantly more than arithmetic .",
    "for example , on a 512-node cluster , the baseline bfs algorithm in graph 500 spends about 70% time on communication during its traversal on a scale - free graph with 8 billion vertices ( figure  [ fig : intro - percent ] ) .",
    "therefore the most critical task in a distributed bfs algorithm is to minimize its communication .",
    "0.45ll approach & category + two - dimensional partitioning  @xcite & algorithm + bitmap & sparse vector  @xcite & data structure + pgas with communication coalescing  @xcite & runtime + _ this work _ : compression & sieve & data structure +    several different approaches are proposed to optimize communication in distributed bfs ( table  [ table : comparison ] ) : using two - dimensional partitioning of the graph to reduce communication overhead  @xcite , using bitmap or sparse vector to reduce the size of messages  @xcite , or applying communication coalescing in pgas implementation to minimize message overhead  @xcite .",
    "these approaches attack the problem from different angles : algorithm , data structure and runtime . in this paper",
    ", we will focus on reducing the size of communication messages ( the optimization of data structures ) .",
    "the main techniques we use are _ compression _ and _ sieve_. overall , we make the following contributions :    * by compressing the messages , we reduce the communication time by @xmath0 and improved its overall performance by @xmath1 compared to the baseline bfs algorithm . * by sieving the messages with a novel distributed directory before compression .",
    "we further reduce the communication by @xmath2 and improved the performance by another @xmath3 , achieving a total @xmath4 reduction in communication and @xmath5 performance improvement over the baseline implementation . *",
    "we implement and analyse several compression methods for bitmap compression .",
    "our experiment shows the space - time tradeoff of different compression methods .    in the next section",
    "we will introduce the problem with an example .",
    "section  [ sec : baseline - bfs ] will describe the baseline bfs algorithm .",
    "section  [ sec : spmv - bfs ] and section  [ sec : dir - bfs ] will describe our bfs algorithms with compression and sieve .",
    "the analysis and experiment results are presented in section  [ sec : ana ] and section  [ sec : exp ] , followed by related works and concluding remarks in section  [ sec : related ] and section  [ sec : cls ] .",
    "is represented as a vector.,scaledwidth=48.0% ]    we start with an example illustrating the breadth - first search ( bfs ) algorithm . given a graph @xmath6 and a distinguished source vertex @xmath7 , breadth - first search systematically explores the edges of @xmath8 to `` discover '' every vertex that is reachable from @xmath7 . in figure",
    "[ fig : bfs - example - all ] , the source vertex @xmath9 is painted black when the algorithm begins .",
    "then it explores its adjacent vertices : @xmath10 , @xmath11 and @xmath12 , and paints them black .",
    "the exploration goes on until all vertices are visited .",
    "vertices discovered the first time is painted black ; discovered vertices are painted solid grey ; vertices to be discovered are painted grey with black edge .",
    "the frontier @xmath13 of the graph is the set of the vertices which are discovered the first time .    for distributed bfs ,",
    "the vertices as well as the frontier are divided among processors : @xmath14 , @xmath15 , @xmath16 , @xmath17 . and",
    "the global information of the frontier can only be retrieved through communication .",
    "for @xmath18 in this example , it only `` owns '' the information of whether vertex @xmath9 and @xmath19 are visited .",
    "if it want to identify whether vertex @xmath12 is visited , it needs to ask this information from @xmath20 .",
    "the common way to update the global @xmath13 is to use mpi collective communication like allgather at the end of each level  @xcite .",
    "the most critical task for distributed bfs is to reduce the size of the frontier , which directly influence the size of the messages communicated . to reduce it , bitmap or sparse vector is commonly used to represent the frontier .",
    "bitmap use a vector of size @xmath21 to represent the frontier , each bit of the vector representing a vertex : @xmath19 means it is included in the frontier , @xmath9 means it is not .",
    "sparse vector includes the frontier vertices only , each is represented using 64 bits . for graphs of diameter @xmath22 ,",
    "bitmap is generally better when @xmath23 .",
    "table  [ table : sparsity ] provides an example of the size of the frontier represented as bitmap or sparse vector , for a scale - free graph of 1.6 billion vertices . in this case , for @xmath24 , the total size of messages using bitmap is 1.4 gb , much less than the sparse vector s 12.4 gb .",
    "0.45lzzz level & # vertices & bitmap & sparse vector + 1 & 2 & 196.9 mb & 16b + 2 & 20842 & 196.9 mb & 162.8 kb + 3 & 235274348 & 196.9 mb & 2.0 gb + 4 & 1377666413 & 196.9 mb & 10.2 gb + 5 & 38582585 & 196.9 mb & 294.4 mb + 6 & 88639 & 196.9 mb & 692.4 kb + 7 & 211 & 196.9 mb & 1.69 kb + total & 1651633040 & 1.4 gb & 12.4 gb +    despite the huge space saved by bitmap , there remains two problems :    * the problem of bitmap is that it need to contain _ all _ the vertices to keep the position information of each vertex . for the above example , to represent 2 vertices at level 1 ,",
    "the size of the bitmap frontier is still 196.9 mb , where most of the elements are zero .",
    "fortunately , these zeros can be condensed .",
    "_ we leverage lossless compression to reduce the size of the bitmap_. * the other problem is the expensive broadcast cost of the allgather collective communication , which broadcasts _ all _ vertices to _ all _ processors . in fact , each processor needs only a small fractions of the frontier .",
    "for example , in figure  [ fig : bfs - example - all ] ( b ) , @xmath20 does not need to send the information of vertex @xmath12 to @xmath25 , because vertex @xmath12 does not has a direct edge connecting to the vertices of @xmath25 .",
    "_ we propose a distributed directory to sieve the bitmap vectors before compression , further reducing its message size_.",
    "let @xmath26 denote the adjacency matrix of the graph @xmath8 , @xmath27 denote the frontier at level @xmath28 , and @xmath29 denote the visited information of previous frontiers .",
    "the exploration of level @xmath28 in bfs is algebraically equivalent to a sparse matrix vector multiplication ( spmv ) : @xmath30 ( we will omit the transpose and assume that the input is pre - transposed for the rest of this section ) .",
    "for example , traversing from level one ( figure  [ fig : bfs - example - all ] ( a ) ) to level two ( figure  [ fig : bfs - example - all ] ( b ) ) is equivalent to the linear algebra below . @xmath31",
    "the syntax @xmath32 denotes the matrix - vector multiplication operation , @xmath33 denotes element - wise multiplication , @xmath34 , and overline represents the complement operation . in other words , @xmath35 for @xmath36 and @xmath37 for @xmath38 .    in figure  [",
    "fig : bfs - example - all ] , bfs starts from vertex @xmath39 , thus @xmath40 .",
    "if we use a vector of size @xmath41 to represent the corresponding frontier @xmath27 , for example , @xmath42 . this algorithm becomes deterministic with the use of ( select , max)-semiring , because the parent is always chose to be the vertex with the highest label .",
    "[ t ] [ alg : spmv - bfs ]    @xmath43    algorithm [ alg : spmv - bfs ] describes the baseline bfs .",
    "each loop block ( starting in line 3 ) performs a single level traversal .",
    "@xmath13 represents the current frontier , which is initialized as an empty bitmap ; @xmath44 is an bitmap that holds the temporary parent information for that iteration only ; @xmath45 is the visited information of previous frontiers .",
    "the computational step ( line 4,5,6 ) can be efficiently parallelized with multithreading . for spmv operation in line 4",
    ", the matrix data is naturally splitted into pieces for multithreading . at the end of each loop , allgather updates @xmath13 with mpi collective communication .",
    "for large graphs , the communication time of distributed bfs algorithms can take as much as seventy percent of the total execution time . to reduce it",
    ", we need to reduce the size of the messages .",
    "one simple way is to use lossless compression , trading computation for bandwidth .",
    "[ t ] [ alg : bfs - compress ]    @xmath43    algorithm  [ alg : bfs - compress ] describe the distributed bfs with compression .",
    "the difference between algorithm  [ alg : bfs - compress ] and algorithm  [ alg : spmv - bfs ] are line 7 and 9 . at line 7 the frontier vector @xmath13 is first compressed into @xmath46 before communication . at line",
    "9 @xmath46 is uncompressed back to @xmath13 after communication",
    ".    .a wah compressed bitmap .",
    "[ cols= \" < , < \" , ]     figure  [ fig : time - profiling ] is the time breakdown of the algorithms in figure  [ fig : weak - scaling ] : `` traversing '' time is the time spent on local computing ; `` reducing '' time is the time spent on a mpi reduction operation to get the total vertex count of the frontier ; `` communicatoin '' time is the time spent on communication ; `` compression & sieve '' time is the time spent on compression and sieve .",
    "for all three algorithms , as the number of nodes increases , `` communication '' times increase exponentially . for _",
    "bit _ , it accounts for as much as @xmath47 of the total time for 512 node .",
    "the `` reducing ''",
    "times also increases because the imbalance of a graph become more severe as the graph becomes larger ; the local `` traversing '' times remain more or less the same because the problem size per node is fixed . at 512 node , _ wah _ reduces the `` communication '' time by @xmath0 compared to _ bit _ ; _ dir - wah _ reduces the `` communication '' time by another @xmath2 compared to _ wah _ , achieving a total @xmath4 reduction compared to _ bit _ , from 18.6 seconds to 3.9 seconds .",
    "on one hand , the `` compression & sieve '' time of _ wah _ ( only compression time is counted for _ wah _ ) at 512 nodes is less than @xmath48 of the total run time and not shown in the figure .",
    "this means the benefit of compression is at very little cost . on the other hand , the time of `` compression & sieve '' in _ dir - wah _ ,  the computing time traded for bandwidth  accounts for @xmath49 of the total .",
    "this is because algorithm  [ alg : bfs - dir - compress ] ( line 9 ) needs to copy the frontier for each process before sieve .",
    "this copying time is expensive because it is in direct proportion to the number of processes .",
    "overall , comparing _ dir - wah _ to _ wah _ ( 512 nodes ) , sieve costs about 1.3 seconds but saves 5.0 seconds in communication  the saving is worth the cost .",
    "figure  [ fig : diff - scales ] plots the performance of different bfs algorithms at different scales .",
    "the experiment runs on 512 nodes .",
    "we can learn from this plot that the compression and sieve method favours larger messages .",
    "the size of messages will affect the results : at scale 26 , _ dir - wah _ , _ wah _ and _ bit _ need to exchange 8 mb bitmap globally using mpi collective communications ; at scale 33 , 1 gb . _ dir - wah _ is the slowest when the scale is small , but it gradually catches up and surpasses all other algorithms when scale gets bigger .            as mentioned in section  [ sec : spmv - bfs ] , different methods could be used for compression .",
    "we did not implement all of them but choose two , zlib library  @xcite and wah , based on following reasons : zlib library is famous for good compression on a wide variety of data and provides different compression levels ; wah is dedicated to bitmap compression , simpler than plwah and faster than bbc .",
    "we use zlib 1.2.6 , and three different compression levels : best compression ( _ zlb - bc _ ) , best speed ( _ zlb - bs _ ) and default ( _ zlb - df _ ) .",
    "the results are plotted in figure  [ fig : zlib - weak - scaling ] and figure  [ fig : zlib - time - profiling ] .",
    "figure  [ fig : zlib - weak - scaling ] shows the weak scaling performance of bfs algorithms with different compression and sieve methods .",
    "bfs with zlib best compression _ zlb - bc _ is the slowest . with 512 nodes ,",
    "_ dir - wah _ provides the best performance , followed by _ zlb - bs _ ( 69.9% of _ dir - wah _ ) , _ dir - zlb - bs _ ( 66.7% ) , _ zlb - bc _",
    "( 53.5% ) , and _ dir - zlb - df _ ( 39.7% ) respectivelly .",
    "figure  [ fig : zlib - time - profiling ] shows the time breakdown of these algorithms . at scale 33 with 512 nodes , _ dir - zlb - df _",
    "`` communication '' time is the smallest , @xmath50 of _ dir - wah _ , followed by _ dir - zlb - bs _ ( @xmath51 ) , _ dir - zlb - bc _ ( @xmath52 ) , _ zlb - bs _ ( @xmath53 ) and _ zlb - bc _ ( @xmath54 ) . although _ dir - zlb - df _ and _ dir - zlb - bs",
    "_ s communication times are less than _ dir - wah _ , their `` compression and sieve '' times are @xmath55 and @xmath56 of _ dir - wah_. so the overall performance of _ dir - zlb - df _ and _ dir - zlb - bs _ are worse than _ dir - wah_. for all three compression levels in zlib we tested , default method , not the best compression method , provides the best compression ratio .",
    "in fact , the zlib best compression method is not suited for bitmap compression : it is not only the slowest , but also provides the worst compression ratio .",
    "several different approaches are proposed to reduce the communication in distributed bfs .",
    "yoo et al .",
    "@xcite run distributed bfs on ibm bluegene / l with 32,768 nodes .",
    "its high scalability is achieved through a set of memory and communication optimizations , including a two - dimensional partitioning of the graph to reduce communication overhead .",
    "bulu and madduri  @xcite improved yoo et al.s work by adding hybrid mpi / openmp programming to optimize computation on state - of - the - art multicore processors , and managed to run distributed bfs on a 40,000-core machine .",
    "the method of two - dimensional partitioning reduces the number of processes involved in collective communications .",
    "our algorithm reduces the communication overhead in a different way : minimizing the size of messages with compression and sieve .",
    "moreover , these two optimizations could be combined together to further reduce the communication cost in distributed bfs .",
    "a preliminary result is presented in section  [ sec : cls ] to demonstrate its potential .",
    "beamer et al .",
    "@xcite use a hybrid top - down and bottom - up approach that dramatically reduces the number of edges examined .",
    "the sample code in graph 500  @xcite use bitmap ( bitset array ) in communication , reducing its message size .",
    "cong et al .",
    "@xcite applying communication coalescing in pgas implementation to minimize message overhead .",
    "benchmarks , algorithms and runtime systems for graph algorithms have gained much popularity in both academia and industry .",
    "earlier works on cray xmt / mta  @xcite and ibm cyclops-64  @xcite prove that both massive threads and fine - grained data synchronization improve bfs performance .",
    "bader and madduri @xcite designed a fine - grained parallel bfs which utilizes the support for hardware threading and synchronization provided by mta-2 , and ensures that the graph traversal is load - balanced to run on thousands of hardware threads .",
    "mizell and maschhoff @xcite discussed an improvement on cray xmt . using massive number of threads to hide latency has long be employed in these specialized multi - threaded machines . with the recent progress of multi - core and smt",
    ", this technique can be popularized to more commodity users .",
    "both core - level parallelism and memory - level parallelism are exploited by agarwal et al .",
    "@xcite for optimized parallel bfs on intel nehalem ep and ex processors .",
    "they achieved performances comparable to special purpose hardwares like cray xmt and cray mta-2 and first identified the capability of commodity multi - core systems for parallel bfs algorithms .",
    "scarpazza et al .",
    "@xcite use an asynchronous algorithm to optimize communication between spe and spu for running bfs on sti cell processors .",
    "leiserson and schardl @xcite use cilk++ runtime model to implement parallel bfs .",
    "cong et al .",
    "@xcite present a fast pgas implementation of distributed graph algorithms .",
    "another trend is to use gpu for parallel bfs , for they provide massively parallel hardware threads , and are more cost - effective than the specialized hardwares .",
    "generally , gpus are good at regular problems with contiguous memory accesses .",
    "the challenge of designing an effective bfs algorithm on gpu is to solve the imbalance between threads and to hide the cost of data transfer between cpu and gpu .",
    "there are several works @xcite working on this direction .",
    "the main purpose of this paper is to reduce the communication cost in distributed breadth - first search ( bfs ) , which is the bottleneck of the algorithm .",
    "we found two problems in previous distributed bfs algorithms : first , their message formats are not condensed enough ; second , broadcasting messages causes waste .",
    "we propose to reduce the message size by compressing and sieving . by compressing the messages",
    ", we reduce the communication time by @xmath0 . by sieving the messages with a distributed directory before compression , we reduce the communication time by another @xmath2 , achieving a total @xmath4 reduction in communication time and @xmath5 performance improvement over the baseline implementation .    for future works",
    ", we would like to combine our optimization of message size with other methods such as two - dimensional partitioning  @xcite and hybrid top - down and bottom - up algorithm  @xcite .",
    "the potential is clear .",
    "a preliminary optimization of the distributed bfs algorithm in combinational blas library  @xcite , compressing the sparse vector using zlib library , reduces the communication time by @xmath57 and increases overall performance by @xmath58 . by using compressed bitmap and adding sieve",
    ", we expect to further improve its performance .",
    "d.  a. bader , `` petascale computing for large - scale graph problems , '' in _ proceedings of the 7th international conference on parallel processing and applied mathematics _",
    "ppam07.1em plus 0.5em minus 0.4emberlin , heidelberg : springer - verlag , 2008 , pp .",
    "166169 .",
    "a.  yoo , e.  chow , k.  henderson , w.  mclendon , b.  hendrickson , and u.  catalyurek , `` a scalable distributed parallel breadth - first search algorithm on bluegene / l , '' in _ proceedings of the 2005 acm / ieee conference on supercomputing _ , ser . sc 05.1em plus 0.5em minus 0.4em washington , dc , usa : ieee computer society , 2005 , pp .",
    "25.    a.  bulu and k.  madduri , `` parallel breadth - first search on distributed memory systems , '' in _ proceedings of 2011 international conference for high performance computing , networking , storage and analysis _ ,",
    "sc 11.1em plus 0.5em minus 0.4emnew york , ny , usa : acm , 2011 , pp . 65:165:12 .",
    "g.  malewicz , m.  h. austern , a.  j. bik , j.  c. dehnert , i.  horn , n.  leiser , and g.  czajkowski , `` pregel : a system for large - scale graph processing - `` abstract '' , '' in _ proceedings of the 28th acm symposium on principles of distributed computing _ , ser .",
    "podc 09.1em plus 0.5em minus 0.4em new york , ny , usa : acm , 2009 , pp .",
    "66 .",
    "a.  chan , f.  dehne , and r.  taylor , `` cgmgraph / cgmlib : implementing and testing cgm graph algorithms on pc clusters and shared memory machines , '' in _ international journal of high performance computing applications_.1em plus 0.5em minus 0.4emspringer , 2005 .",
    "g.  cong , g.  almasi , and v.  saraswat , `` fast pgas implementation of distributed graph algorithms , '' in _ proceedings of the 2010 acm / ieee international conference for high performance computing , networking , storage and analysis _",
    "sc 10.1em plus 0.5em minus 0.4emwashington , dc , usa : ieee computer society , 2010 , pp .",
    "111 .",
    "g.  antoshenkov , `` byte - aligned bitmap compression , '' in _ proceedings of the conference on data compression _ , ser .",
    "dcc 95.1em plus 0.5em minus 0.4emwashington , dc , usa : ieee computer society , 1995 , pp .",
    "476.    f.  delige and t.  b. pedersen , `` position list word aligned hybrid : optimizing space and performance for compressed bitmaps , '' in _ proceedings of the 13th international conference on extending database technology _ , ser .",
    "edbt 10.1em plus 0.5em minus 0.4emnew york , ny , usa : acm , 2010 , pp .",
    "228239 .",
    "r.  thakur , `` improving the performance of collective operations in mpich , '' in _ recent advances in parallel virtual machine and message passing interface .",
    "number 2840 in lncs , springer verlag ( 2003 ) 10th european pvm / mpi user s group meeting_.1em plus 0.5em minus 0.4emspringer verlag , 2003 , pp .",
    "257267 .",
    "j.  leskovec , d.  chakrabarti , j.  kleinberg , and c.  faloutsos , `` realistic , mathematically tractable graph generation and evolution , using kronecker multiplication , '' in _ knowledge discovery in databases : pkdd 2005 _ , ser .",
    "lecture notes in computer science , a.  jorge , l.  torgo , p.  brazdil , r.  camacho , and j.  gama , eds.1em plus 0.5em minus 0.4emspringer berlin / heidelberg , 2005 , vol . 3721 , pp .",
    "133145 .",
    "s.  beamer , k.  asanovi , and d.  a. patterson , `` searching for a parent instead of fighting over children : a fast breadth - first search implementation for graph500 , '' eecs department , university of california , berkeley , tech .",
    "ucb / eecs-2011 - 117 , nov 2011 .",
    "[ online ] .",
    "available : http://www.eecs.berkeley.edu/pubs/techrpts/2011/eecs-2011-117.html    d.  a. bader and k.  madduri , `` designing multithreaded algorithms for breadth - first search and st - connectivity on the cray mta-2 , '' in _ proceedings of the 2006 international conference on parallel processing _ , ser .",
    "icpp 06.1em plus 0.5em minus 0.4em washington , dc , usa : ieee computer society , 2006 , pp",
    ". 523530 .",
    "d.  mizell and k.  maschhoff , `` early experiences with large - scale cray xmt systems , '' in _ proceedings of the 2009 ieee international symposium on parallel&distributed processing_.1em plus 0.5em minus 0.4em washington , dc , usa : ieee computer society , 2009 , pp",
    "v.  agarwal , f.  petrini , d.  pasetto , and d.  a. bader , `` scalable graph exploration on multicore processors , '' in _ proceedings of the 2010 acm / ieee international conference for high performance computing , networking , storage and analysis _",
    "sc 10.1em plus 0.5em minus 0.4em washington , dc , usa : ieee computer society , 2010 , pp .",
    "111 .",
    "c.  e. leiserson and t.  b. schardl , `` a work - efficient parallel breadth - first search algorithm ( or how to cope with the nondeterminism of reducers ) , '' in _ proceedings of the 22nd acm symposium on parallelism in algorithms and architectures _",
    "spaa 10.1em plus 0.5em minus 0.4emnew york , ny , usa : acm , 2010 , pp . 303314 .",
    "s.  hong , s.  k. kim , t.  oguntebi , and k.  olukotun , `` accelerating cuda graph algorithms at maximum warp , '' in _ proceedings of the 16th acm symposium on principles and practice of parallel programming _ , ser .",
    "ppopp 11 .",
    "1em plus 0.5em minus 0.4emnew york , ny , usa : acm , 2011 , pp . 267276 .",
    "p.  harish and p.  j. narayanan , `` accelerating large graph algorithms on the gpu using cuda , '' in _ proceedings of the 14th international conference on high performance computing _ ,",
    "0.5em minus 0.4emberlin , heidelberg : springer - verlag , 2007 , pp .",
    "197208 .",
    "l.  luo , m.  wong , and w .- m .",
    "hwu , `` an effective gpu implementation of breadth - first search , '' in _ proceedings of the 47th design automation conference _ ,",
    "dac 10.1em plus 0.5em minus 0.4emnew york , ny , usa : acm , 2010 , pp ."
  ],
  "abstract_text": [
    "<S> for parallel breadth first search ( bfs ) algorithm on large - scale distributed memory systems , communication often costs significantly more than arithmetic and limits the scalability of the algorithm . in this paper </S>",
    "<S> we sufficiently reduce the communication cost in distributed bfs by compressing and sieving the messages . </S>",
    "<S> first , we leverage a bitmap compression algorithm to reduce the size of messages before communication . </S>",
    "<S> second , we propose a novel distributed directory algorithm , cross directory , to sieve the redundant data in messages . </S>",
    "<S> experiments on a 6,144-core smp cluster show our algorithm outperforms the baseline implementation in graph500 by 2.2 times , reduces its communication time by 79.0% , and achieves a performance rate of 12.1 gteps ( billion edge visits per second ) . </S>"
  ]
}