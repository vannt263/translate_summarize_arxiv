{
  "article_text": [
    "parameter estimation for dynamic systems of nonlinear differential equations from noisy measurements of some components of the solution at discrete times is a common problem in many applications . in the bayesian statistical framework ,",
    "the particle filter ( pf ) is a popular sequential monte carlo ( smc ) method for estimating the solution of the dynamical system and the parameters defining it in a sequential manner . among the different variants of pf proposed in the literature , the algorithm of @xcite estimates the state variable along with the model parameters by combining an auxiliary particle technique @xcite with approximation of the posterior density of the parameter vector by gaussian mixtures or an ensemble of particles drawn from the density @xcite .",
    "efficient time integration is crucial in the implementation of pf algorithms , particularly when the underlying dynamical system is stiff and can not be solved analytically , therefore requiring the use of specialized numerical solvers . in @xcite ,",
    "suitable linear multistep methods ( lmms ) @xcite for stiff problems are used within a @xcite - type pf , and the variance of the innovation term in the pf is assigned according to estimates of the local error introduced by the numerical solver . in the present work , we explain how to organize the calculations efficiently on multicore desktop computers , making it possible to follow a large number of particles .",
    "computed examples show that significant speedups can be obtained over a naive implementation .",
    "the inherently parallel nature of pf algorithms is well - known ( see , e.g. , @xcite ) , and software packages have been made available for implementing parallel pfs on various platforms ( e.g. , @xcite ) . in this paper , we show how to reformulate the pf with lmm time integrators proposed in @xcite to make it most amenable to parallel and vectorized environments .",
    "the general approach can be straightforwardly adapted to different computing languages .",
    "computational advantages of the new formulations are illustrated with two sets of computed examples in matlab .",
    "the derivation of the lmm pf - smc method that we are interested in , inspired by the algorithm proposed in @xcite , can be found in @xcite . for sake of completeness , the lmm pf - smc procedure is outlined in algorithm 1 , where @xmath0 denotes the lmm of choice .",
    "+    ' '' ''    * algorithm 1 : lmm pf - smc sampler *    ' '' ''    given the initial probability density @xmath1 :    1 .",
    "_ initialization : _ draw the particle ensemble from @xmath1 : @xmath2 compute the parameter mean and covariance : @xmath3 set @xmath4 .",
    "propagation : _ shrink the parameters @xmath5 by a factor @xmath6 .",
    "compute the state predictor using lmm : @xmath7 3 .",
    "_ survival of the fittest : _ for each @xmath8 : * compute the fitness weights @xmath9 * draw indices with replacement @xmath10 using probabilities @xmath11 ; * reshuffle @xmath12 4 .   _",
    "proliferation : _ for each @xmath8 : * proliferate the parameter by drawing @xmath13 * using lmm error control , estimate @xmath14 * draw @xmath15 ; * repropagate using lmm and add innovation : @xmath16 5 .   _ weight updating : _ for each @xmath8 , compute @xmath17 6 .   if @xmath18 , update @xmath19 increase @xmath20 and repeat from 2 .    ' '' ''    the main computational bottlenecks in the implementation of the algorithm come from the numerical time integrations in step 2 and step 4 , in particular when , due to the stiffness of the system , either extremely small time steps or the use of specially designed numerical schemes are needed to avoid the amplification of unstable modes . indeed , the need to use tiny time steps has been identified as a major bottleneck for pf algorithms ; see , e.g. , @xcite . among the available ode solvers for stiff systems ,",
    "lmms have the advantage being well - understood when it comes to stability properties and local truncation error estimates . the latter ,",
    "which in turn defines the accuracy of the integrator , and for which classical estimation methods exist , provides a natural way to assign the variance of the innovation term @xmath21 in step 4 ; we refer to @xcite for the details .",
    "we illustrate how the organization of the computations affects the computing time of the proposed lmm pf - smc algorithm on a system of nonlinear odes , which could arise , e.g. , from multi - compartment cellular metabolism models @xcite : @xmath22 the parameters @xmath23 and @xmath24 are known , and @xmath25 is the input function , where @xmath26 is the non - negative part of @xmath27 , and @xmath28 , @xmath29 , @xmath30 , and @xmath31 are given . in applications arising from metabolic studies ,",
    "the components @xmath32 , @xmath33 and @xmath34 of the solution of the ode system , which will be referred to as the states of the system , are typically concentrations of substrates and intermediates . in our computed examples ,",
    "the data @xmath35 consist of noisy observations of all three state components @xmath32 , @xmath33 and @xmath34 at 50 time instances , and the goal is to estimate the states at all time instances as well as the unknown parameters @xmath36 and @xmath37 , @xmath38 , which are , respectively , the maximum reaction rates and affinity constants in the michaelis - menten expressions of the reaction fluxes . since the system of odes ( [ diff eq system ] ) is stiff for some values of the unknown parameters , we propagate and repropagate the particles using implicit lmms , e.g. , from the adams - moulton ( am ) or backward differentiation formula ( bdf ) families .",
    "implicit methods require the solution of a nonlinear system of equations at each time step , which is done with a newton - type scheme . by carefully organizing the calculations",
    "so as to take maximal advantage of the multicore environment , it turns out that the time required by implicit lmm time integrators is comparable to that required by the explicit adams - bashforth ( ab ) integrators , which are not suitable for stiff problems .",
    "many desktop computers and programming languages provide vectorized and multicore environments which can significantly reduce the execution time of pf methods when they are formulated to take advantage of these features .",
    "all of the computed examples in this paper were produced using a dell alienware aurora r4 desktop computer with 16 gb ram and an intel core^^ i7 - 3820 processor ( cpu @ 3.60ghz ) with 8 virtual cores , i.e. , 4 cores and 8 threads with hyper - threading capability , using the matlab r2013a programming language .",
    "when testing the parallel performance , we set the local cluster to have a maximum of 8 matlab workers , and we took as baseline the execution time of the lmm pf - smc algorithm on a single processor .",
    "it is straightforward to see that the propagation and re - propagation steps of algorithm 1 are naturally suited to parallelization by subdividing the particles among the different processors .",
    "this can be done by reorganizing the ` for ` loops in the algorithm so that they are partitioned and distributed among the available processors ( or workers ) in the pool , which is achieved in matlab with the commands ` matlabpool ` ( or ` parpool ` ) and ` parfor ` .",
    "not surprisingly , the best parallel performance occurs when all workers take approximately the same time to complete the task , because the slowest execution time determines the speed of the parallel loop .",
    "this can be achieved by prescribing the same time step for all particles in the time integration procedure .",
    "we remark that most ode solvers , including the matlab built - in time integrators , guarantee a requested accuracy in the solution by adapting the time step , a practice which may cause the propagation of two different particles to take very different times , depending on the stiffness induced by different parameter values .",
    "the spread of the computing times needed for the numerical integration of a particle ensemble is rather wide for systems , like the one in this example , whose stiffness is highly sensitive to the values of the unknown parameters .",
    "this violates the principle of equal load on the workers which is essential for a good parallel performance .",
    "propagation of all particles by lmms with the same fixed time step , on the other hand , ensures that the time required for each particle is the same , eliminating idle time .    to present the results of our computed examples , we introduce two key concepts in parallel computing : speedup and parallel efficiency .",
    "the _ speedup _ using @xmath39 processors is the ratio @xmath40 , where @xmath41 is the execution time of the sequential algorithm and @xmath42 is the execution time of the parallel algorithm on @xmath39 processors , while the _ efficiency _ using @xmath39 processors is defined as @xmath43 .",
    "efficiency is a performance measure used to estimate how well the processors are utilized in running a parallel code : @xmath44 , trivially , for algorithms run sequentially , i.e. , on a single processor . for further details ,",
    "see , e.g. , @xcite .",
    ".[par_table5]cpu times ( in seconds ) sequentially and in parallel with 8 workers , along with the corresponding speedup @xmath45 and efficiency @xmath46 , for applying the pf - smc algorithm to solve the parameter estimation problem for system ( [ diff eq system ] ) using the first three lmm time integrators of each family with fixed time step @xmath47 and two sample sizes , @xmath48 and @xmath49 particles , respectively .",
    "[ cols=\"^,^,^,^,^,^,^,^,^ \" , ]     time series estimates of parameters @xmath50 , @xmath51 , @xmath52 , @xmath53 and @xmath54 for problem ( [ eq : advdiff ] ) when @xmath55.,title=\"fig:\",width=115 ] time series estimates of parameters @xmath50 , @xmath51 , @xmath52 , @xmath53 and @xmath54 for problem ( [ eq : advdiff ] ) when @xmath55.,title=\"fig:\",width=115 ] time series estimates of parameters @xmath50 , @xmath51 , @xmath52 , @xmath53 and @xmath54 for problem ( [ eq : advdiff ] ) when @xmath55.,title=\"fig:\",width=115 ] time series estimates of parameters @xmath50 , @xmath51 , @xmath52 , @xmath53 and @xmath54 for problem ( [ eq : advdiff ] ) when @xmath55.,title=\"fig:\",width=115 ] time series estimates of parameters @xmath50 , @xmath51 , @xmath52 , @xmath53 and @xmath54 for problem ( [ eq : advdiff ] ) when @xmath55.,title=\"fig:\",width=115 ]",
    "the use of stable , fixed time step lmm solvers in pf - smc algorithms lends itself in a natural way to both parallelizing and vectorizing the computations , thus providing a competitive alternative to running independent parallel chains in monte carlo simulations @xcite . in this paper",
    ", we consider these two different implementation strategies for a recently proposed pf - smc algorithm , and we illustrate the advantages with computed examples using two stiff test problems with different features .    the results in tables [ par_table5 ] show that in the case where the stiffness of the dynamical system is very sensitive to the parameters to be estimated , as for system ( [ diff eq system ] ) , vectorizing the lmm pf - smc algorithm results in significant speedup over the sequential and even parallel implementations . moreover , for the vectorized version , the cpu times when using implicit and explicit methods are closer , and increasing the order of the method has little effect .    in the case of a large system where the stiffness is an intrinsic feature and",
    "does not depend much on the values of the unknown parameters , as for system ( [ eq : odesys ] ) , on the other hand , vectorization of the pf - smc using implicit lmms does not perform better than the sequential implementation , but parallelization of the algorithm speeds up the calculations .",
    "our results suggest that both the size and structure of the problem determine whether the parallelized or vectorized version of the algorithm is more efficient .",
    "this work was partly supported by grant number 246665 from the simons foundation ( daniela calvetti ) and by nsf dms project number 1312424 ( erkki somersalo ) .",
    "a. lee , c. yau , m. b. giles , a. doucet and c. c. holmes , on the utility of graphics cards to perform massively parallel simulation of advanced monte carlo methods , _ j. comput . graph . statist .",
    "_ , * 19 * ( 2010 ) , 769789 .",
    "j. liu and m. west , combined parameter and state estimation in simulation - based filtering , in _ sequential monte carlo methods in practice _ ( eds .",
    "a. doucet , j. f. g. de freitas and n. j. gordon ) , springer , new york ( 2001 ) , 197223 .                m. west , mixture models , monte carlo , bayesian updating and dynamic models , in _ computing science and statistics : proceedings of the 24th symposium on the interface _ ( ed",
    ". j. h. newton ) , interface foundation of america , fairfax station , va ( 1993 ) , 325333 ."
  ],
  "abstract_text": [
    "<S> particle filter ( pf ) sequential monte carlo ( smc ) methods are very attractive for the estimation of parameters of time dependent systems where the data is either not all available at once , or the range of time constants is wide enough to create problems in the numerical time propagation of the states . </S>",
    "<S> the need to evolve a large number of particles makes pf - based methods computationally challenging , the main bottlenecks being the time propagation of each particle and the large number of particles . </S>",
    "<S> while parallelization is typically advocated to speed up the computing time , vectorization of the algorithm on a single processor may result in even larger speedups for certain problems . in this paper </S>",
    "<S> we present a formulation of the pf - smc class of algorithms proposed in @xcite , which is particularly amenable to a parallel or vectorized computing environment , and we illustrate the performance with a few computed examples in matlab . + * keywords * : parallel computing , vectorization , particle filters , sequential monte carlo , linear multistep methods . + * msc - class * : 65y05 , 65y10 ( primary ) ; 62m20 , 65l06 , 62m05 ( secondary ) . </S>"
  ]
}