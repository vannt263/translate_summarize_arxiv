{
  "article_text": [
    "given data @xmath1 , where @xmath2 is the response and @xmath3 is the @xmath4-dimensional covariate , the goal in many analyses is to approximate the unknown function @xmath5 by minimizing a  specified loss function @xmath6 [ a common choice is @xmath0-loss , @xmath7 . in trying to estimate @xmath8 ,",
    "one strategy is to make use of a large system of possibly redundant functions @xmath9 .",
    "if @xmath9 is rich enough , then it is reasonable to expect @xmath8 to be well approximated by an additive expansion of the form @xmath10 where @xmath11 are base learners parameterized by @xmath12 . to estimate @xmath8 , a  joint multivariable optimization over @xmath13 may be used .",
    "but such an optimization may be computationally slow or even infeasible for large dictionaries .",
    "overfitting may also result . to circumvent this problem , iterative descent",
    "algorithms are often used .",
    "one popular method is the gradient descent algorithm described by @xcite , closely related to the method of `` matching pursuit '' used in the signal processing literature [ @xcite ] .",
    "this algorithm is applicable to a wide range of problems and loss functions , and is now widely perceived to be a generic form of boosting . for the @xmath14th step , @xmath15 ,",
    "one solves @xmath16 where @xmath17 ^ 2\\ ] ] identifies the closest base learner to the gradient @xmath18 in @xmath0-distance , where @xmath19 is the gradient evaluated at the current value @xmath20 , and is defined by @xmath21_{f_{m-1}(\\mathbf{x}_i ) } = -l'(y_i , f_{m-1}(\\mathbf{x}_i)).\\ ] ] the @xmath14th update for the predictor of @xmath8 is @xmath22 where @xmath23 is a regularization ( learning ) parameter .    in this paper",
    ", we study friedman s algorithm under @xmath0-loss in linear regression settings assuming an @xmath24 design matrix @xmath25 $ ] , where @xmath26 denotes the @xmath27th column . here",
    "@xmath28 represents the @xmath27th base learner ; that is , @xmath29 where @xmath30 and @xmath31 .",
    "it is well known that under @xmath0-loss the gradient simplifies to the residual @xmath32 .",
    "this is particularly attractive for a theoretical treatment as it allows one to combine the line - search and the learner - search   into a single step because the @xmath0-loss function can be expressed as @xmath33 .",
    "the optimization problem becomes @xmath34    it is common practice to standardize the response by removing its mean which eliminates the issue of whether an intercept should be included as a  column of  @xmath35 .",
    "it is also common to standardize the columns of @xmath35 to have a  mean of zero and squared - length of one .",
    "thus , throughout , we assume the data is standardized according to @xmath36 the condition @xmath37 leads to a particularly useful simplification : @xmath38 thus , the search for the most favorable direction is equivalent to determining the largest absolute value @xmath39 .",
    "we refer to @xmath40 as the _ gradient - correlation _ for @xmath27 .",
    "we shall refer to friedman s algorithm under the above settings as 2boost .",
    "algorithm  [ a : l2boost ] provides a formal description of the algorithm [ we use @xmath41 for notational convenience ] .",
    "initialize @xmath42 for @xmath43 @xmath44 , where @xmath45 @xmath46 , where @xmath47    properties of stagewise algorithms similar to 2boost have been studied extensively under the assumption of an infinitesimally small regularization parameter .",
    "@xcite considered a forward stagewise algorithm @xmath48 , and showed under a convex cone condition that the least angle regression ( lar ) algorithm yields the solution path for @xmath49 , the limit of @xmath48  as @xmath50 .",
    "this shows that @xmath48 , a  variant of boosting , and the lasso [ @xcite ] are related in some settings .",
    "@xcite showed in general that the solution path of @xmath49 is equivalent to the path of the monotone lasso .",
    "however , much less work has focused on stagewise algorithms assuming an arbitrary learning parameter @xmath23 .",
    "an important exception is @xcite who studied 2boost with componentwise linear least squares , the same algorithm studied here , and proved consistency for arbitrary @xmath51 under a sparsity assumption where @xmath4 can increase at an exponential rate relative to @xmath52 .",
    "as pointed out in @xcite , the @xmath48algorithm studied by @xcite bears similarities to 2boost .",
    "it is identical to algorithm  [ a : l2boost ] , except for line 4 , where @xmath53 is used in place of @xmath51 and @xmath54.\\ ] ] thus , @xmath48  replaces the gradient - correlation @xmath55 with the sign of the gradient - correlation @xmath56 . for infinitesimally small @xmath51",
    "this difference appears to be inconsequential , and it is generally believed that the two limiting solution paths are equal [ @xcite ] . in general , however , for arbitrary @xmath23 , the two solution paths are different .",
    "indeed , @xcite indicated certain unique advantages possessed by 2boost .",
    "other related work includes @xcite , who described a bias - variance decomposition of the mean - squared - error of a variant of 2boost .      in this paper",
    ", we investigate the properties of 2boost assuming an arbitrary learning parameter @xmath23 . during 2boost s descent along a fixed coordinate direction ,",
    "a new coordinate becomes more favorable when it becomes closest to the current gradient .",
    "but when does this actually occur ? we provide an exact simple closed form expression for this quantity : the number of iterations to favorability ( theorem  [ criticalpoint.theorem ] of section  [ s : fixeddescent ] ) .",
    "this core identity is used to describe 2boost s solution path ( theorem  [ full.path.solution.general ] ) , to introduce new tools for studying its path and to study and characterize some of the algorithm s unique properties .",
    "one of these is active set cycling , a property where the algorithm spends lengthy periods of time cycling between the same coordinates when  @xmath51 is small ( section  [ s : cyclingbehavior ] ) .",
    "our fixed descent identity also reveals how correlation affects 2boost s ability to select variables in highly correlated problems .",
    "we identify a _",
    "repressible condition _ that prevents a new variable from entering the active set , even though that variable may be highly desirable ( section  [ s : repressibility ] ) .",
    "using a  data augmentation approach , similar to that used for calculating the elastic net [ @xcite ] , we describe a simple method for adding @xmath0-penalization to 2boost ( section  [ s : elasticboost ] ) . in combination with decorrelation , this reverses the repressible condition and improves 2boost s performance in correlated problems . because 2boost is known to approximate forward stagewise algorithms for arbitrarily small @xmath51 , it is natural to expect these results to apply to such algorithms like lar and lasso , and thus our results provide a new explanation for why these algorithms may perform poorly in correlated settings and why methods like the elastic net , which makes use of @xmath0-penalization , are more adept in such settings .",
    "all proofs in this manuscript can be found in the supplemental article [ @xcite ] .",
    "to analyze 2boost we introduce the following notation useful for describing its solution path . let @xmath57 be the @xmath58 nonduplicated values in order of appearance of the selected coordinate directions @xmath59 .",
    "we refer to these ordered , nonduplicated values as _ critical directions _ of the path . for example , if @xmath60 , the critical directions are @xmath61 and @xmath62 . to formally describe the solution path we introduce the following nomenclature.=-1    [ path.def ] the descent length along a critical direction @xmath63",
    "is denoted by  @xmath64 .",
    "the critical point @xmath65 is the step number at which the descent along  @xmath63 ends .",
    "thus , following step @xmath66 , the descent is along @xmath63 for a total of @xmath64 steps , ending at step @xmath65 .",
    "the set of values @xmath67 can be used to formally describe the solution path of 2boost : the algorithm begins by descending along direction  @xmath68 ( the first critical direction ) for @xmath69 steps , after which it switches to a descent along direction @xmath70 ( the second critical direction ) for a total of @xmath0 steps .",
    "this continues with the last descent along @xmath71 ( the final critical direction ) for a total of @xmath72 steps .",
    "see figure  [ figure1 ] for illustration of the notation .    .",
    "the @xmath73 critical directions are @xmath74 with critical descent step lengths @xmath75 and critical points @xmath76 . _",
    "a key observation is that 2boost s behavior along a given descent is deterministic except for its descent length @xmath64 ( number of steps ) .",
    "if we could determine the descent length , a quantity we show is highly amenable to analysis , then an exact description of the solution path becomes possible as 2boost can be conceptualized as collection of such fixed paths .",
    "imagine then that we are at step @xmath77 of the algorithm and that in the following step a new critical direction @xmath27 is formed .",
    "let us study the descent along @xmath27 for the next @xmath78 steps .",
    "thus , in the @xmath14th step of the descent along @xmath27 , the predictor is @xmath79 consider then algorithm  [ a : incrementall2boost ] which repeatedly boosts the predictor along the @xmath27th direction for a total of @xmath80 steps .",
    "@xmath81 @xmath82 , where @xmath83    the following result states a closed form solution for the @xmath14-step predictor of algorithm  [ a : incrementall2boost ] and will be crucial to our characterization of 2boost .",
    "[ incremental.operator.theorem ] @xmath84 , where @xmath85 and @xmath86 .",
    "theorem  [ incremental.operator.theorem ] shows that taking a single step with learning parameter @xmath87 yields the same limit as taking @xmath14 steps with the smaller learning parameter  @xmath51 .",
    "the result also sheds insight into how @xmath51 slows the descent relative to stagewise regression .",
    "notice that the @xmath14-step predictor can be written as @xmath88 the first term on the right is the predictor from a greedy stagewise step , while the second term represents the effect of slow - learning .",
    "this latter term is what slows the descent relative to a greedy step .",
    "when @xmath89 this term vanishes , and we end up with stagewise fitting , @xmath90 .      theorem  [ incremental.operator.theorem ] shows how to take a  large boosting step in place of many small steps , but it does not indicate how many steps must be taken along @xmath27 before a new variable enters the solution path . if this were known , then the entire @xmath27-descent could be characterized in terms of a single step .    to determine the descent length ,",
    "suppose that 2boost has descended along @xmath27 for a total of @xmath14 steps . at step @xmath91",
    "the algorithm must decide whether to continue along @xmath27 or to select a new direction @xmath92 .",
    "to determine when to switch directions , we introduce the following definition .",
    "[ favorable.def ] a direction @xmath92 is said to be more favorable than @xmath27 at step @xmath91 if @xmath93 and @xmath94 .",
    "thus , if @xmath92 is more favorable at @xmath91 , the descent switches to @xmath92 for step @xmath91 .    to determine when @xmath92 becomes more favorable , it will be useful to have a  closed form expression for @xmath95 and @xmath96 . by theorem  [ incremental.operator.theorem ] , @xmath97 \\\\ & = & \\rho_{j,1 } -\\nu_{m}\\rho_{k,1}r_{j , k},\\end{aligned}\\ ] ] where @xmath98 . setting @xmath99 yields @xmath100 .",
    "therefore , @xmath94 if and only if @xmath101 dividing throughout by @xmath102 , with a little bit of rearrangement , this becomes @xmath103 ^ 2,\\ ] ] where @xmath104 .",
    "notice importantly that @xmath105 because @xmath27 is the direction with maximal gradient - correlation at the start of the descent .",
    "it is also useful to keep in mind that @xmath106 is the sample correlation of @xmath107 and @xmath28 due to , and thus @xmath108 .",
    "the following result states the number of steps taken along @xmath27 before @xmath92 becomes more favorable .",
    "[ criticalpoint.theorem ] the number of steps @xmath109 taken along @xmath27 so that @xmath92 becomes more favorable than @xmath27 at @xmath110 is the largest integer @xmath14 such that @xmath111 it follows that for @xmath112 @xmath113,\\hspace*{-30pt}\\ ] ] where @xmath114 is the largest integer less than or equal to @xmath115 .",
    "[ repressible.remark ] in particular , notice that @xmath116 when @xmath117 [ adopting the standard convention that @xmath118 and assuming that @xmath119 .",
    "we call @xmath117 the repressible condition",
    ". section  [ s : repressibility ] will show that repressibility plays a key role in 2boost s behavior in correlated settings .",
    "when @xmath90 we obtain @xmath120 from which corresponds to greedy stagewise fitting .",
    "because this makes the @xmath90 case uninteresting , we shall hereafter assume that @xmath121 .",
    "theorem  [ criticalpoint.theorem ] immediately shows that the problem of determining the next variable to enter the solution path can be recast as finding the direction requiring the fewest number of steps @xmath109 to favorability . when combined with theorem  [ incremental.operator.theorem ] , this characterizes the entire descent and can be used to characterize 2boost s solution path",
    ".    as before , assume that @xmath27 corresponds to the first critical direction of the path , that is , @xmath122 . by theorem  [ criticalpoint.theorem ] , 2boost descends along @xmath27 for a total of @xmath123 steps , where @xmath124 and @xmath70 is the coordinate requiring the smallest number of steps to become more favorable than @xmath27 . by theorem  [ incremental.operator.theorem ] ,",
    "the predictor at step @xmath125 is @xmath126 applying theorem  [ incremental.operator.theorem ] once again , but now using a descent along @xmath70 initialized at @xmath127 , and continuing this argument recursively , as well as using the representation for the number of steps from theorem  [ criticalpoint.theorem ] , yields theorem  [ full.path.solution.general ] , which presents a recursive description of 2boost s solution path .    [ full.path.solution.general ] @xmath128 , where @xmath129 are determined recursively from @xmath130 , \\\\",
    "l_r&=&m_{l_{r+1}}^{(r)},\\qquad s_r = s_{r-1 } + l_r,\\qquad s_0=0 , \\\\",
    "d_j^{(r ) } & = & \\frac{\\rho_j^{(r)}}{\\rho_{l_r}^{(r)}},\\qquad \\rho_{j}^{(r+1 ) } = { \\mathbf{x}}_j^t({\\mathbf{y}}-{\\mathbf{f}}_{s_{r } } ) = \\rho_{j}^{(r ) } - \\nu_{l_r}\\rho_{l_r}^{(r)}r_{j , l_r}.\\end{aligned}\\ ] ]    [ step.number.tie.remark ] a technical issue arises in theorem  [ full.path.solution.general ] when @xmath131 is not unique .",
    "non - uniqueness can occur due to rounding which is caused by the floor function used in the definition of @xmath109 .",
    "this is why line 1 selects the next critical value , @xmath132 , by maximizing the absolute gradient - correlation @xmath133 and not by minimizing the step number @xmath131 .",
    "this definition for  @xmath132 is equivalent to the two - step solution @xmath134    [ equal.gradient.remark ] another technical issue arises when there is a tie in the absolute gradient - correlation . in line 3 of algorithm  [",
    "a : l2boost ] it may be possible for two coordinates , say @xmath92 and @xmath27 , to have equal gradient - correlations at step @xmath135 .",
    "theorem  [ full.path.solution.general ] implicitly deals with such ties due to definition  [ favorable.def ] .",
    "for example , suppose that the first @xmath136 steps are along @xmath27 with the tie occurring at step @xmath14 .",
    "in the language of theorem  [ criticalpoint.theorem ] , because @xmath92 becomes more favorable than @xmath27 at @xmath91 , where @xmath137 , we have @xmath138 resolves the tie at @xmath14 by continuing to descend along @xmath27 , then switching to @xmath92 at step @xmath91 .",
    "although algorithm  [ a : l2boost ] does not explicitly address this issue , the potential discrepancy is minor because such ties should rarely occur in practice .",
    "this is because for @xmath139 to hold , the value inside the floor function of used to define @xmath109 must be an integer ( a careful analysis of the proof of theorem  [ criticalpoint.theorem ] shows why ) .",
    "a tie can occur only when this value is an integer which is numerically unlikely to occur.=-1    theorem  [ full.path.solution.general ] immediately yields a recursive solution for the coefficient vector , @xmath140 . the solution path for @xmath140 is the piecewise solution @xmath141 where @xmath142 is the vector with one in coordinate @xmath63 and zero elsewhere .      aside from the technical issue of ties , theorem",
    "[ full.path.solution.general ] and algorithm  [ a : l2boost ] are equivalent . for convenience ,",
    "we state theorem  [ full.path.solution.general ] in an algorithmic form to facilitate comparison with algorithm  [ a : l2boost ] ; see algorithm  [ a : l2boostpath ] .",
    "computationally , algorithm  [ a : l2boostpath ] improves upon algorithm  [ a : l2boost ] by avoiding taking many small steps along a given descent .",
    "however , the difference is not substantial because the benefits only apply when @xmath51 is small , and as we will show later ( section  [ s : cyclingbehavior ] ) , this forces the algorithm to cycle between its variables following the first descent , thus mitigating its ability to take large steps .",
    "thus , strictly speaking , the benefit of algorithm  [ a : l2boostpath ] is confined primarily to the first descent .",
    "@xmath143 ; @xmath144 ; @xmath145 @xmath146 ; @xmath147 @xmath148 ; @xmath149    @xmath128    to investigate the differences between the two algorithms we analyzed the diabetes data used in @xcite .",
    "the data consists of @xmath150 patients in which the response of interest , @xmath151 , is a quantitative measure of disease progression for a patient . in total",
    "there are 64 variables , that includes 10 baseline measurements for each patient , 45 interactions and 9 quadratic terms .    in order to compare results ,",
    "we translated each iteration , @xmath152 , used by algorithm  [ a : l2boostpath ] into its corresponding number of steps , @xmath14 .",
    "thus , while we ran algorithm  [ a : l2boostpath ] for @xmath153 iterations , this translated into @xmath154 steps .",
    "as expected , this difference is primarily due to the first iteration @xmath155 which took @xmath156 steps along the first critical direction ( first panel of figure  [ figure2 ] ; the rug indicates critical points , @xmath65 ) .",
    "there are other instances where algorithm  [ a : l2boostpath ] took more than one step ( corresponding to the light grey tick marks on the rug ) , but these were generally steps of length 2 .",
    "the standardized gradient - correlation is plotted along the @xmath151-axis of the figure .",
    "the standardized gradient - correlation for step @xmath14 was defined as ( using the notation of algorithm  [ a : l2boost ] ) @xmath157 the middle panel displays the results using algorithm  [ a : l2boost ] with @xmath158 steps . clearly",
    ", the greatest gains from algorithm  [ a : l2boostpath ] occur along the @xmath155 descent .",
    "one can see this most clearly from the last panel which superimposes the first two panels .     against step number @xmath14 for algorithms  [ a : l2boostpath ] and  [ a : l2boost ] , respectively .",
    "only coordinates in the solution path are displayed ( a total of four ) .",
    "the third panel superimposes the first two panels .",
    "all analyses used @xmath159 . ]    note a potential computational optimization exists in algorithm  [ a : l2boostpath ] .",
    "it is possible to calculate the correlation values only once as each new variable enters the active set , then cache these values for future calculations .",
    "thus , when @xmath132 is a new variable in the active set , we calculate @xmath160 .",
    "the updated gradient - correlation is calculated efficiently by using addition and scalar multiplication using the previous gradient - correlation and the cached correlation coefficients @xmath161 this is in contrast to algorithm  [ a : l2boost ] which requires a vector multiplication of dimension @xmath4 at each step @xmath14 to update the gradient - correlation : @xmath162 .",
    "[ active.set.remark ] above , when we refer to the `` active set , '' we mean the unique set of critical directions in the current solution path .",
    "this term will be used repeatedly throughout the paper .      throughout the paper",
    "we illustrate different ways of utilizing @xmath109 of theorem  [ criticalpoint.theorem ] to explore 2boost .",
    "so far we have confined the use of theorem  [ criticalpoint.theorem ] to determining the descent length along a fixed direction , but another interesting application is determining how far a given variable is from the active set . note that although theorem  [ criticalpoint.theorem ] was described in terms of an active set of only one coordinate , it applies in general , regardless of the size of the active set .",
    "thus , @xmath109 can be calculated at any step @xmath14 to determine the number of steps required for @xmath92 to become more favorable than the current direction , @xmath27 .",
    "this value represents the distance of @xmath92 to the solution path and can be used to visualize it .    to demonstrate this , we applied algorithm  [ a : l2boost ] to the diabetes data for @xmath163 steps and recorded @xmath109 for each of the @xmath164 variables . figure  [ figure3 ] records these values .",
    "each `` jagged path '' in the figure is the trace over the 10,000 steps for a variable @xmath92 .",
    "each point on the path equals the number of steps @xmath109 to favorability relative to the current descent @xmath165 .",
    "the patterns are quite interesting .",
    "the top variables have @xmath109 values which quickly drop within the first 1000 steps .",
    "another group of variables have values which take much longer to drop , doing so somewhere between 2000 to 4000 steps , but then increase almost immediately .",
    "these variables enter the solution path but then quickly become unattractive regardless of the descent direction .     of each variable @xmath92 to favorability relative to the current descent  @xmath27",
    "( results based on algorithm  [ a : l2boost ] where @xmath159 ) . for visual clarity the @xmath109 values have been smoothed using a running median smoother . ]",
    "it has become popular to visualize the solution path of forward stagewise algorithms by plotting their gradient - correlation paths and/or their coefficient paths .",
    "figure  [ figure3 ] is a similar tool .",
    "a unique feature of @xmath109 is that it depends not only on the gradient - correlation ( via @xmath166 ) , but also the correlation in the @xmath167-variables ( via @xmath106 ) and the learning parameter @xmath51 . in this manner ,",
    "figure  [ figure3 ] offers a new tool for understanding and exploring such algorithms .",
    "it has been widely observed that decreasing the regularization parameter slows the convergence of stagewise descent algorithms .",
    "@xcite showed that the @xmath48  algorithm tracks the equiangular direction of the lar path for arbitrarily small @xmath53 . to achieve what lar does in a single step ,",
    "the @xmath48  algorithm may require thousands of small steps in a direction tightly clustered around the equiangular vector , eventually ending up at nearly the same point as lar .",
    "we show that 2boost exhibits this same phenomenon .",
    "we do so by describing this property as an active set cycling phenomenon . using results from the earlier fixed descent analysis",
    ", we show in the case of an active set of two variables that 2boost systematically switches ( cycles ) between its two variables when @xmath51 is small . for an arbitrarily small @xmath51 this forces the absolute gradient - correlations for the active set variables to be nearly equal .",
    "this point of equality represents a  singularity point that triggers a  near - perpetual deterministic cycle between the variables , ending only when a  new variable enters the active set with nearly the same absolute gradient - correlation .",
    "our insight will come from looking at theorem  [ criticalpoint.theorem ] in more depth .",
    "as before , assume the algorithm has been initialized so that @xmath27 is the first critical step . previously the descent along  @xmath27 was described in terms of steps , but this can be equivalently expressed in units of the `` step size '' taken .",
    "define @xmath168 recall that theorem  [ incremental.operator.theorem ] showed that a single step along @xmath27 with @xmath51 replaced with @xmath169 yields the same limit as @xmath109 steps along @xmath27 using @xmath51 .",
    "we call @xmath169 the step size taken along @xmath27 . because @xmath92 becomes more favorable than @xmath27 at @xmath110 , the gradient following a step size of @xmath169 along @xmath27 satisfies @xmath170 , and in particular holds for the second critical direction , @xmath70 , which rephrased in terms of step size , is the smallest  @xmath169 value , @xmath171",
    "although inequality is strict , it becomes arbitrarily close to equality with shrinking @xmath51 . with a little bit of rearranging , implies that @xmath172 we will show @xmath173 is the step size making the absolute gradient - correlation between @xmath92 and @xmath27 equal @xmath174 converges to the smallest @xmath173 satisfying  ; thus , becomes an equality in the limit . for convenience ,",
    "we define @xmath175 .",
    "[ dynamic.nu.size ] let @xmath176 . then @xmath177 . furthermore ,",
    "if @xmath178 and @xmath179 , then @xmath180 and @xmath181 as @xmath182 .    therefore , for arbitrarily small @xmath51 , @xmath183 and @xmath27 and @xmath70 will have near - equal absolute gradient - correlations .",
    "this latter property triggers two - cycling . to see why ,",
    "let us assume for the moment that the active set variables have equal absolute gradient - correlations .",
    "then by a direct application of theorem  [ criticalpoint.theorem ] , one can show that the number of steps taken along @xmath70 before @xmath27 becomes more favorable is @xmath184 .",
    "thus , following the descent along @xmath27 , the algorithm switches to @xmath70 , but then immediately switches back to @xmath27 . if @xmath51 is small enough , this process is repeated , setting off a two - cycling pattern .",
    "the next result is a formal statement of these arguments .",
    "define @xmath185 for notational convenience , let @xmath186 and @xmath137 . for technical reasons we shall assume @xmath187 .",
    "recall remark  [ repressible.remark ] showed that @xmath188 , the repressible condition , yields an infinite number of steps to favorability .",
    "thus , for @xmath27 to be even eligible for favorability we must have @xmath187 .",
    "[ long.descent.followed.two.cycle ] if the first two critical directions are @xmath189 and @xmath190 , then @xmath27 is favored over @xmath92 for the next step after @xmath92 if @xmath187 .",
    "theorem  [ long.descent.followed.two.cycle ] assumes that @xmath190 . while this only holds in the limit , the two values should be nearly equal for arbitrarily small @xmath51 , and thus the assumption is reasonable .",
    "notice also that theorem  [ long.descent.followed.two.cycle ] only shows that @xmath27 is more favorable than  @xmath92 , and not that the algorithm switches to @xmath27 .",
    "however , we can see that this must be the case . for arbitrarily small",
    "@xmath51 , @xmath27 s gradient - correlation should be nearly equal to @xmath92 s , and by definition , @xmath92 has maximal absolute gradient - correlation along the second descent .",
    "indeed , the following result shows that the absolute gradient - correlations for @xmath27 and @xmath92 can be made arbitrarily close for small enough @xmath51 for any step @xmath191 following the descent along @xmath27 .",
    "the result also shows that the sign of the gradient - correlation is preserved when @xmath51 is arbitrarily small , a fact that we shall use later .    [ gradient.correlation.equality ]",
    "@xmath192 as @xmath182 for each @xmath191 .    combining theorems  [ long.descent.followed.two.cycle ] and  [ gradient.correlation.equality ] , we see that if @xmath51 is small enough , the first three critical directions of the path must be @xmath193 with critical points @xmath194 . and",
    "once the descent switches back to @xmath27 , it is clear from the same argument that the next critical direction , @xmath195 , will be @xmath92 , and so forth .",
    "we present a numerical example demonstrating two - cycling . for our example , we simulated data according to @xmath196 where @xmath197 , and @xmath198 .",
    "the first 10 coordinates of @xmath140 were set to 5 , with the remaining coordinates set to 0 .",
    "the design matrix @xmath35 was simulated by drawing its entries independently from a standard normal distribution .    .",
    "top left panel details the path through the first three active variables , the remaining panels detail each active variable descent . ]",
    "figure  [ figure4 ] plots the standardized gradient - correlations from algorithm  [ a : l2boostpath ] using @xmath199 . as done earlier , we have converted iterations @xmath152 into step numbers @xmath14 along the @xmath167-axis .",
    "the plots show the behavior of each coordinate within an active set descent .",
    "the rug marks show each step @xmath14 for clarity , and dashed vertical lines indicate the step @xmath109 where the next step adds a new critical direction to the solution path .",
    "the top left panel shows the complete descent along the first three active variables .",
    "the remaining panels detail the coordinate behavior as the active set increases from one to three coordinates .",
    "the top right panel shows repeated selection of the @xmath68 direction shown in black .",
    "the last step along @xmath68 occurs at @xmath109 marked with the vertical dashed line , where the next step is along the @xmath70 direction shown in red .",
    "this point marks the beginning of the two - cycling behavior , which continues in the lower left panel . at each step",
    ", the algorithm systematically switches between the @xmath68 and @xmath70 directions , until an additional direction becomes more favorable .",
    "the cycling pattern is @xmath200 .",
    "the lower right panel demonstrates three - cycling behavior .",
    "here it is instructive to note that the order of selection within three - cycling is nondeterministic . in this panel",
    "the order starts as @xmath201 , but changes near @xmath202 to @xmath203 . as discussed later , nondeterministic cycling patterns are typical behavior of higher order cycling ( active sets of size greater than two ) .      here",
    "we provide a formal limiting result of two - cycling .",
    "the result can be viewed as the analog of theorem  [ dynamic.nu.size ] when the active set involves two variables . using a slightly modified version of 2boost",
    "we show that for arbitrarily small @xmath51 , if the algorithm cycles between its two active variables , it does so until a new variable enters the active set with the same absolute gradient - correlation .",
    "assume the active set is @xmath204 and that @xmath27 and @xmath92 are cycling according to @xmath205 .",
    "the @xmath14-step predictor for @xmath15 is @xmath206 where @xmath207 .",
    "the cycling pattern is assumed to persist for a minimum length of @xmath208 .",
    "it will simplify matters if the cycling is assumed to be initialized with strict equality of the gradient correlations : @xmath209 . with an arbitrarily small @xmath51 ,",
    "this will force near equal absolute gradient - correlations at each step and by theorem  [ gradient.correlation.equality ] will preserve the sign of the gradient - correlation .",
    "we assume @xmath210 it should be emphasized that the above assumptions represent a simplified version of 2boost . in practice",
    ", we would have @xmath211 where @xmath212 .",
    "however , for convenience we will not concern ourselves with this level of detail here .",
    "readers can consult @xcite for a more refined analysis .",
    "one way to ensure @xmath209 is to initialize the algorithm with the limiting predictor @xmath213 of theorem  [ dynamic.nu.size ] obtained by letting along the @xmath27-descent . with a slight abuse of notation",
    "denote this initial estimator by @xmath214 .",
    "however , the fact that this specific @xmath214 is used does not play a direct role in the results . under the above assumptions , the following closed form expression for the @xmath14-step predictor under two - cycling holds .",
    "[ two.cycle.predictor ] assume that @xmath215 for @xmath216 .",
    "if @xmath217 , then for any @xmath218 satisfying @xmath219 , we have for each @xmath216 , @xmath220 , & \\quad if $ m$ is odd,\\vspace*{2pt } \\cr { \\mathbf{f}}_{0 } + v_{m}\\rho_{k,1 } [ { \\mathbf{x}}_k+(s-\\nu r_{j , k}){\\mathbf{x}}_j ] , & \\quad if $ m$ is even , } \\ ] ] where @xmath221 $ ] and @xmath222 .",
    "note that @xmath223 under the asserted conditions .",
    "to determine the above limit requires first determining when a new direction @xmath224 becomes more favorable . for @xmath225 to be more favorable at @xmath91 , we must have @xmath226 when @xmath14 is odd , or @xmath227 when @xmath14 is even .",
    "the following result determines the number of steps to favorability .",
    "for simplicity only the case when @xmath14 is odd is considered , but this does not affect the limiting result .",
    "[ criticalpoint.twocycle.theorem ] assume the same conditions as theorem  [ two.cycle.predictor ] .",
    "then @xmath225 becomes more favorable than @xmath92 at step @xmath91 where @xmath14 is the largest odd integer @xmath228 such that @xmath229 where @xmath230 and @xmath231    clearly shares common features with .",
    "this is no coincidence .",
    "the bounds are similar in nature because both are derived by seeking the point where the absolute gradient - correlation between sets of variables are equal . in the case of two - cycling , this is the singularity point where @xmath27 , @xmath92 and  @xmath225 are all equivalent in terms of absolute gradient - correlation .",
    "the following result states the limit of the predictor under two - cycling .",
    "[ dynamic step.two.cycles ] under the conditions of theorem  [ two.cycle.predictor ] , the limit of @xmath232 as @xmath182 at the next critical direction @xmath233 equals @xmath234,\\ ] ] where @xmath235 , @xmath179 , @xmath236 and @xmath237 .",
    "furthermore , @xmath238 , where for each @xmath225 , @xmath239 .",
    "this shows that the predictor moves along the combined direction @xmath240 taking a step size @xmath241 that makes the absolute gradient - correlation for @xmath233 equal to that of the active set @xmath204 .",
    "theorem  [ dynamic step.two.cycles ] is a direct analog of theorem  [ dynamic.nu.size ] to two - cycling .",
    "not surprisingly , one can easily show that this limit coincides with the lar solution . to show this , we rewrite @xmath242 in a form comparable to lar , @xmath243.\\ ] ] recall that lar moves the shortest distance along the equiangular vector defined by the current active set until a new variable with equal absolute gradient - correlation is reached .",
    "the term in square brackets above is proportional to this equiangular vector .",
    "thus , since @xmath242 is obtained by moving the shortest distance along the equiangular vector such that @xmath244 have equal absolute gradient - correlation , @xmath242 must be identical to the lar solution .",
    "analysis of cycling in the general case where the active set @xmath245 is comprised of @xmath246 variables is more complex . in two - cycling",
    "we observed cycling patterns of the form @xmath247 , but when @xmath248 , 2boost s cycling patterns are often observed to be nondeterministic with no discernible pattern in the order of selected critical directions .",
    "moreover , one often observes some coordinates being selected more frequently than others .",
    "a study of @xmath249-cycling has been given by @xcite .",
    "however , the analysis assumes deterministic cycling of the form @xmath250 which is the natural extension of the two - cycling just studied . to accommodate this framework ,",
    "a modified 2boost procedure involving coordinate - dependent step sizes was used .",
    "this models 2boost s cycling tendency of selecting some coordinates more frequently by using the size of a step to dictate the relative frequency of selection . under constraints to the coordinate step sizes , equivalent to solving a system of linear equations defining the equiangular vector used by lar , it was shown that the modified 2boost procedure yields the lar solution in the limit .",
    "interested readers should consult @xcite for details .",
    "now we turn our attention to the issue of correlation .",
    "we have shown that regardless of the size of the active set a new direction @xmath92 becomes more favorable than the current direction @xmath27 at step @xmath110 where @xmath109 is the smallest integer value satisfying @xmath251 using our previous notation , let @xmath173 and @xmath169 denote the left and right - hand sides of the above inequality , respectively .",
    "generally , large values of @xmath109 are designed to hinder noninformative variables from entering the solution path .",
    "if @xmath92 requires a large number of steps to become favorable , it is noninformative relative to the current gradient and therefore unattractive as a candidate .",
    "surprisingly , however , such an interpretation does not always apply in correlated problems .",
    "there are situations where @xmath92 is informative , but @xmath109 can be artificially large due to correlation .    to see why , suppose that @xmath92 is an informative variable with a relatively large value of @xmath166 .",
    "now , if @xmath92 and @xmath27 are correlated , so much so that @xmath252 , then @xmath253",
    ". hence , @xmath254 and @xmath255 due to . thus , even though @xmath92 is promising with a large gradient - correlation , it is unlikely to be selected because of its high correlation with @xmath27 .",
    "the problem is that @xmath92 becomes an unlikely candidate for selection when  @xmath166 is close to @xmath106 .",
    "in fact , @xmath116 when @xmath117 so that @xmath92 can never become more favorable than @xmath27 when the two values are equal . we have already discussed the condition @xmath117 several times now , and have referred to it as the _",
    "repressible condition_. repressibility plays an important role in correlated settings .",
    "we distinguish between two types of repressibility : weak and strong repressibility .",
    "weak repressibility occurs in the trivial case when @xmath256 .",
    "weak repressibility implies that @xmath257 .",
    "hence the gradient - correlation for  @xmath92 and  @xmath27 are equal in absolute value and @xmath92 , and  @xmath27 are perfectly correlated .",
    "this trivial case simply reflects a numerical issue arising from the redundancy of the @xmath92 and @xmath27 columns of the @xmath35 design matrix . the stronger notion of repressibility , which we refer to as strong repressibility ,",
    "is required to address the nontrivial case @xmath258 in which @xmath92 is repressed without being perfectly correlated with @xmath27 .",
    "the following definition summarizes these ideas .",
    "[ repressible.def ] we say @xmath92 has the strong repressible condition if @xmath117 and @xmath259 .",
    "we say that @xmath92 is ( strongly ) repressed by @xmath27 when this happens . on the other hand , @xmath92 has the weak repressible condition if @xmath92 and  @xmath27 are perfectly correlated ( @xmath256 ) and @xmath117 .",
    "we present a numerical example of how repressibility can hinder variables from being selected . for our illustration",
    "we use example ( d ) of section 5 from @xcite .",
    "the data was simulated according to @xmath260 where @xmath261 , @xmath262 and @xmath263 .",
    "the first 15 coordinates of @xmath140 were set to  3 ; all other coordinates were 0 .",
    "the design matrix @xmath264_{100\\times40}$ ] was simulated according to @xmath265 \\\\[-8pt ] { \\mathbf{x}}_j & = & { \\mathbf{z}}_3 + \\tau{\\boldsymbol{{{\\varepsilon}}}}_j,\\qquad j = 11,\\ldots,15,\\nonumber \\\\ { \\mathbf{x}}_j & = & { \\boldsymbol{{{\\varepsilon}}}}_j,\\qquad j > 15,\\nonumber\\end{aligned}\\ ] ] where @xmath266 and @xmath267 were i.i.d.@xmath268 and @xmath269 . in this simulation , only coordinates 1 to 5 , 6 to 10 and 11 to 15 have nonzero coefficients .",
    "these @xmath167-variables are uncorrelated across a group , but share the same correlation within a group . because the within group correlation is high , but less than  1",
    ", the simulation is ideal for exploring the effects of strong repressibility .",
    "for the first 5 coefficients from simulation : red points are iterations @xmath152 where the descent direction @xmath270 .",
    "variables 2 and 3 are never selected due to their excessively large @xmath169 step sizes : an artifact of the correlation between the 5 variables .",
    "the last panel ( bottom right ) displays @xmath271 for those iterations  @xmath152 where @xmath272 . ]",
    "figure  [ figure5 ] displays results from fitting algorithm  [ a : l2boostpath ] for @xmath273 iterations with @xmath274 .",
    "the first 5 panels are the values @xmath275 against the iteration @xmath276 , with points colored in red indicating iterations @xmath152 where @xmath270 and @xmath27 is used generically to denote the current descent direction .",
    "notationally , the descent at iteration @xmath152 is along @xmath27 for a step size of @xmath277 , at which point @xmath225 becomes more favorable than @xmath27 and the descent switches to @xmath225 , the next critical direction .",
    "the value plotted , @xmath278 , is the step size for @xmath279 .",
    "whenever the selected coordinate is from the first group of variables ( we are referring to the red points ) one of the coordinates @xmath280 achieves a  small @xmath169 value .",
    "however , coordinates @xmath281 and @xmath282 maintain very large values throughout all iterations .",
    "this is despite the fact that the two coordinates generally have large values of @xmath166 , especially during the early iterations ( see the bottom right panel ) .",
    "this suggests that 1 , 4 and 5 become active variables at some point in the solution path , whereas coordinates 2 and 3 are never selected ( indeed , this is exactly what happened ) .",
    "we can conclude that coordinates 2 and 3 are being strongly repressed by @xmath272 .",
    "interestingly , coordinate 4 also appears to be repressed at later iterations of the algorithm .",
    "observe how its @xmath166 values decrease with increasing @xmath152 ( blue line in bottom right panel ) , and that its @xmath169 values are only small at earlier iterations .",
    "thus , we can also conclude that coordinates @xmath283 eventually repress coordinate 4 as well .",
    "we note that the number of iterations @xmath273 used in the example is not very large , and if 2boost were run for a longer period of time , coordinates 2 and  3 will eventually enter the solution path ( panels 2 and  3 of figure  [ figure5 ] show evidence of this already happening with @xmath169 steadily decreasing as @xmath152 increases ) .",
    "however , doing so leads to overfitting and poor test - set performance ( we provide evidence of this shortly ) . using different values of @xmath51",
    "also did not resolve the problem .",
    "thus , similar to the lasso , we find that 2boost is unable to select entire groups of correlated variables . like the lasso",
    "this means it also will perform suboptimally in highly correlated settings . in the next section",
    "we introduce a simple way of adding @xmath0-regularization as a way to correct this deficiency .",
    "the tendency of the lasso to select only a handful of variables from among a group of correlated variables was noted in @xcite . to address this deficiency",
    ", @xcite described an optimization problem different from the classical lasso framework .",
    "rather than relying only on @xmath69-penalization , they included an additional @xmath0-regularization parameter designed to encourage a ridge - type grouping effect , and termed the resulting estimator `` the elastic net . '' specifically , for a fixed @xmath284 ( the ridge parameter ) and a fixed @xmath285 ( the lasso parameter ) , the elastic net was defined as @xmath286 to calculate the elastic net , @xcite showed that could be recast as a lasso optimization problem by replacing the original data with suitably constructed augmented values .",
    "they replaced @xmath287 @xmath288 and @xmath35 @xmath289 with augmented values @xmath290 and @xmath291 , defined as follows : @xmath292_{(n+p)\\times1},\\qquad { { \\mathbf{x}}^*}= \\frac{1}{\\sqrt{1+\\lambda } } \\left[\\matrix { { \\mathbf{x}}\\cr \\sqrt{\\lambda } { \\mathbf{i } } } \\right]_{(n+p)\\times p } = [ { { \\mathbf{x}}^*}_1,\\ldots,{{\\mathbf{x}}^*}_p].\\hspace*{-35pt}\\ ] ] the elastic net optimization can be written in terms of the augmented data by reparameterizing @xmath140 as @xmath293 . by lemma  1 of @xcite , it follows that can be expressed as @xmath294 which is an @xmath69-optimization problem that can be solved using the lasso .",
    "one explanation for why the elastic net is so successful in correlated problems is due to its decorrelation property .",
    "let @xmath295 .",
    "because the data is standardized such that @xmath296 [ recall ] , we have @xmath297 one can see that @xmath298 is a decorrelation parameter , with larger values reducing the correlation between coordinates .",
    "@xcite argued that this effect promotes a `` grouping property '' for the elastic net that overcomes the lasso s inability to select groups of correlated variables .",
    "we believe that decorrelation is an important component of the elastic net s success .",
    "however , we will argue that in addition to its role in decorrelation , @xmath298 has a surprising connection to repressibility that further explains its role in regularizing the elastic net .",
    "the argument for the elastic net follows as a special case ( the limit ) of a  generalized 2boost procedure we refer to as elasticboost .",
    "the elasticboost  algorithm is a modification of 2boost applied to the augmented problem .",
    "to implement elasticboost  one runs 2boost on the augmented data , adding a post - processing step to rescale the coefficient solution path : see algorithm  [ a : eboost ] for a precise description . for arbitrarily small @xmath51 ,",
    "the solution path for elasticboost  approximates the elastic net , but for general @xmath23 , elasticboost  represents a novel extension of 2boost .",
    "we study the general elasticboost  algorithm , for arbitrary @xmath23 , and present a detailed explanation of how @xmath298 imposes @xmath0-regularization .",
    "augment the data .",
    "set @xmath299 for @xmath300 .",
    "run algorithm [ a : l2boostpath ] for @xmath301 iterations using the augmented data .",
    "let @xmath302 denote the @xmath301-step predictor ( discard @xmath302 for @xmath303 ) .",
    "let @xmath304 denote the @xmath301-step coefficient estimate .",
    "rescale the regression estimates : @xmath305 .      to study the effect @xmath298 has on elasticboost s solution path we consider in detail how @xmath298 effects @xmath306 , the number of steps to favorability [ defined as in but with @xmath287 and @xmath35 replaced by their augmented values @xmath290 and @xmath291 ] .",
    "at initialization , the gradient - correlation for @xmath308 is @xmath309 in the special case when @xmath299 , corresponding to the first descent of the algorithm , @xmath310 therefore , @xmath311 , and hence @xmath312.\\ ] ] this equals the number of steps in the original ( nonaugmented ) problem but where @xmath35 is replaced with variables decorrelated by a factor of @xmath313 . for large values of @xmath298",
    "this addresses the problem seen in figure  [ figure5 ] .",
    "recall we argued that @xmath109 can became inflated due to the near equality of @xmath166 with  @xmath106 .",
    "however , @xmath314 shrinks to zero with increasing @xmath298 , which keeps @xmath306 from becoming inflated .",
    "this provides one explanation for @xmath298",
    "s role in regularization , at least for the case when @xmath298 is large .",
    "but we now suggest another theory that applies for both small and large @xmath298 .",
    "we argue that regularization is imposed not just by decorrelation , but through a combination of decorrelation and reversal of repressibility .",
    "s role is more subtle than our previous argument suggests .    to show this ,",
    "let us suppose that near - repressibility holds .",
    "we assume therefore that @xmath315 for some small @xmath316 .",
    "then , @xmath317}_{\\mathrm{repressibility\\ effect } } \\\\ & & \\hphantom{\\qquad= } \\underbrace{- \\log\\biggl(1-\\frac{r_{j , k}}{\\sqrt{1+\\lambda}}{\\operatorname{sgn}}\\biggl(r_{j , k } \\biggl[\\frac{1}{1+{\\delta}}-\\frac{1}{\\sqrt{1+\\lambda}}\\biggr ] \\biggr)\\biggr)}_{\\mathrm { decorrelation\\ effect}}.\\nonumber\\end{aligned}\\ ] ] the first term on the right captures the effect of repressibility .",
    "when @xmath318 is small , @xmath298 plays a crucial role in controlling its size . if @xmath319 , the expression reduces to @xmath320 which converges to @xmath321 as @xmath322 ; thus precluding  @xmath92 from being selected [ keep in mind that is divided by @xmath323 , which is negative ; thus @xmath324 . on the other hand , any @xmath284 , even a  relatively small value ,",
    "ensures that the expression remains small even for arbitrarily small @xmath318 , thus reversing the effect of repressibility .    the second term on the right of is related to decorrelation .",
    "if @xmath325 ( which holds if @xmath298 is large enough when @xmath326 , or for all @xmath284 if @xmath327 ) , the term reduces to @xmath328 which remains bounded when @xmath284 if @xmath329 . on the other hand , if @xmath330 , the term reduces to @xmath331 which remains bounded if @xmath329 and shrinks in absolute size as @xmath298 increases .    taken together , these arguments show @xmath298 imposes @xmath0-regularization through a combination of decorrelation and the reversal of repressibility which applies even when @xmath298 is relatively small .",
    "these arguments apply to the first descent .",
    "the general case when @xmath332 requires a detailed analysis of @xmath333 . in general ,",
    "@xmath334 we break up the analysis into two cases depending on the size of @xmath298 .",
    "suppose first that @xmath298 is small .",
    "then @xmath335 which is the ratio of gradient correlations based on the original @xmath35 without pseudo - data .",
    "if @xmath92 is a promising variable , then @xmath333 will be relatively large , and our argument from above applies . on the other hand",
    "if @xmath298 is large , then the third term in the numerator and the denominator of @xmath333 become the dominating terms and @xmath336 the growth rate of @xmath337 for the pseudo data is @xmath338 for a group of variables that are actively being explored by the algorithm .",
    "thus @xmath339 and our previous argument applies .",
    "as evidence of this , and to demonstrate the effectiveness of elasticboost , we re - analyzed using algorithm  [ a : eboost ] .",
    "we used the same parameters as in figure  [ figure5 ] ( @xmath273 and @xmath274 ) .",
    "we set @xmath340 .",
    "the results are displayed in figure  [ figure6 ] .",
    "in contrast to figure  [ figure5 ] , notice that all 5 of the first group of correlated variables achieve small @xmath341 values ( and we confirmed that all 5 variables enter the solution path ) .",
    "it is interesting to note that @xmath333 is nearly 1 for each of these variables .    ) .",
    "now each of the first 5 coordinates are selected and each has @xmath333 values near one . ]",
    "( top ) and @xmath342 ( bottom ) based on 250 independent learning samples .",
    "the distribution of coefficient estimates are displayed as boxplots ; mean values are given in red . ]    to compare 2boost and elasticboost  more evenly , we used 10-fold cross - validation to determine the optimal number of iterations ( for elasticboost , we used doubly - optimized cross - validation to determine both the optimal number of iterations and the optimal @xmath298 value ; the latter was found to equal @xmath343 )",
    ". figure  [ figure7 ] displays the results .",
    "the top row displays 2boost , while the bottom row is elasticboost  ( fit under the optimized @xmath298 ) .",
    "the minimum mean - squared - error ( mse ) is slightly smaller for elasticboost  ( 217.9 ) than 2boost ( 231.7 ) ( first panels in top and bottom rows ) .",
    "curiously , the mse is minimized using about same number of iterations for both methods ( 190 for 2boost and 169 for elasticboost ) .",
    "the middle panels display the coefficient paths .",
    "the vertical blue line indicates the mse optimized number of iterations . in the case of 2boost",
    "only 4 nonzero coefficients are identified within the optimal number of steps , whereas elasticboost  finds all 15 nonzero coefficients .",
    "this can be seen more clearly in the right panels which show coefficient estimates at the optimized stopping time .",
    "not only are all 15 nonzero coefficients identified by elasticboost , but their estimated coefficient values are all roughly near the true value of 3 .",
    "in contrast , 2boost finds only 4 coefficients due to strong repressibility .",
    "its coefficient estimates are also wildly inaccurate .",
    "while this does not overly degrade prediction error performance ( as evidenced by the first panel ) , variable selection performance is seriously impacted .",
    "the entire experiment was then repeated 250 times using 250 independent learning sets .",
    "figure  [ figure8 ] displays the coefficient estimates from these 250 experiments for elasticboost  ( left side ) and 2boost ( right side ) as boxplots .",
    "the top panel are based on the original sample size of @xmath261 and the bottom panel use a larger sample size @xmath342 .",
    "the results confirm our previous finding : elasticboost  is consistently able to group variables and outperform 2boost in terms of variable selection .",
    "finally , the left panel of figure  [ figure9 ] displays the difference in test set mse for 2boost and elasticboost  as a function of @xmath298 over the 250 experiments ( @xmath261 ) .",
    "negative values indicate a lower mse for elasticboost , which is generally the case for larger @xmath298 .",
    "the right panel displays the mse optimized number of iterations for 2boost compared to elasticboost .",
    "generally , elasticboost  requires fewer steps as @xmath298 increases .",
    "this is interesting , because as pointed out , this generally coincides with better mse performance .",
    "a key observation is that 2boost s behavior along a  fixed descent direction is fully specified with the exception of the descent length , @xmath64 . in theorem",
    "[ criticalpoint.theorem ] , we described a closed form solution for @xmath109 , the number of steps until favorability , where @xmath344 is the currently selected coordinate direction and @xmath345 is the next most favorable direction .",
    "theorem  [ criticalpoint.theorem ] quantifies 2boost s descent length , thus allowing us to characterize its solution path as a series of fixed descents where the next coordinate direction , chosen from all candidates @xmath346 , is determined as that with the minimal descent length @xmath109 ( assuming no ties ) .",
    "since we choose from among all directions @xmath346 , @xmath109 , and equivalently the step length @xmath169 , can be characterized as measures to favorability , a property of each coordinate at any iteration @xmath152 .",
    "these measures are a function of @xmath51 and the ratio of gradient - correlations @xmath166 and the correlation coefficient @xmath106 relative to the currently selected direction @xmath27 .",
    "characterizing the 2boost solution path by @xmath109 provides considerable insight when examining the limiting conditions .",
    "when @xmath347 , 2boost exhibits active set cycling , a property explored in detail in section  [ s : cyclingbehavior ] .",
    "we note that this condition is fundamentally a result of the optimization method which drives @xmath348 when @xmath51 is arbitrarily small .",
    "this virtually guarantees the notorious slow convergence seen with infinitesimal forward stagewise algorithms .",
    "the repressibility condition occurs in the alternative limiting condition @xmath349 .",
    "repressibility arises when the gradient correlation ratio @xmath166 equals the correlation @xmath106 .",
    "when @xmath259 , @xmath92 is said to be strongly repressed by @xmath27 , and while descending along @xmath27 , the absolute gradient - correlation for @xmath92 can never be equal to or surpass the absolute gradient - correlation for  @xmath27 .",
    "strong repressibility plays a crucial role in correlated settings , hindering variables from being actively selected .",
    "adding @xmath0 regularization reverses repressibility and substantially improves variable selection for elasticboost , an 2boost implementation involving the data augmentation framework used by the elastic net ."
  ],
  "abstract_text": [
    "<S> we consider @xmath0boosting , a special case of friedman s generic boosting algorithm applied to linear regression under @xmath0-loss . </S>",
    "<S> we study @xmath0boosting for an arbitrary regularization parameter and derive an exact closed form expression for the number of steps taken along a fixed coordinate direction . </S>",
    "<S> this relationship is used to describe @xmath0boosting s solution path , to describe new tools for studying its path , and to characterize some of the algorithm s unique properties , including active set cycling , a property where the algorithm spends lengthy periods of time cycling between the same coordinates when the regularization parameter is arbitrarily small . </S>",
    "<S> our fixed descent analysis also reveals a _ repressible condition _ that limits the effectiveness of @xmath0boosting in correlated problems by preventing desirable variables from entering the solution path . as a simple remedy , </S>",
    "<S> a data augmentation method similar to that used for the elastic net is used to introduce @xmath0-penalization and is shown , in combination with decorrelation , to reverse the repressible condition and circumvents @xmath0boosting s deficiencies in correlated problems . in itself , this presents a new explanation for why the elastic net is successful in correlated problems and why methods like lar and lasso can perform poorly in such settings .    .    </S>"
  ]
}