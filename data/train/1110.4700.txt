{
  "article_text": [
    "in @xcite , the authors showed that the now popular abc ( approximate bayesian computation ) method @xcite is not necessarily validated when applied to bayesian model choice problems , in the sense that the resulting bayes factors may fail to pick the correct model even asymptotically .",
    "the abc algorithm is progressively getting accepted as a necessary component of the bayesian toolbox for handling intractable likelihoods . since it is not the central topic of this article , but rather both a motivation and an immediate application domain for our derivation , we do not embark upon a complete description of its implementation , referring to @xcite and @xcite for details .",
    "we simply recall here that the core feature of this approximation technique is to run simulations @xmath0 from the prior distribution and the corresponding sampling distribution until a statistic @xmath1 of the simulated pseudo - data @xmath2 is close enough to the corresponding value of the statistic @xmath3 at the observed data @xmath4 .",
    "the degree of proximity ( also called the tolerance ) can be improved by an increase in the computational power .",
    "however the choice of the statistic @xmath5 is particularly crucial in that the resulting ( approximately bayesian ) inference relies on this statistic and only on this statistic .",
    "it thus impacts the resulting inference much more than the choices of the tolerance distance and of the tolerance value .",
    "when conducting abc model choice @xcite , the outcome of the ideal algorithm associated with zero tolerance and zero monte carlo error is the bayes factor @xmath6 namely the bayes factor for testing @xmath7 versus @xmath8 based on the sole observation of @xmath3 .",
    "this value most often differs from the bayes factor @xmath9 based on the whole data @xmath4 . as discussed in @xcite and @xcite , in the specific case",
    "when the statistic @xmath3 is sufficient for both @xmath7 and @xmath8 , the difference between both bayes factors can be expressed as [ eq : summayes ] b_12 ( ) = b^_12 ( ) , where the ratio of the @xmath10 s often behaves like a likelihood ratio of the same order as the data size @xmath11 .",
    "the discrepancy revealed by the above is such that abc model choice can not be trusted without further checks .",
    "indeed , even in the limiting ideal case , i.e.  when the abc algorithm achieves a zero tolerance , the abc odds ratio does not take into account the features of the data besides the value of @xmath3 .",
    "@xcite warn that this difference can be such that @xmath12 leads to an inconsistent model choice .",
    "( the same is obviously true for point estimation , e.g.  when considering the extreme case of an ancillary @xmath3 . )",
    "this is also the reason why @xcite consider the alternative approach of assessing each model on its own under several divergence measures , defining a new algorithm they denote @xmath13 .    beyond abc applications ,",
    "note that many fields report summary statistics in their publications rather than the raw data , for various reasons ranging from confidentiality to storage , to proprietary issues .",
    "for instance , a dataset may be replaced by several @xmath14-values , @xmath15 , against several specific hypotheses .",
    "handling a model choice problem based solely on @xmath16 is therefore a relevant issue , with the coherence of the corresponding bayes factor at stake .",
    "another relevant instance outside the abc domain is provided in @xcite , who exhibit the above differences in the bayes factors when using a non - sufficient statistic , including an example where the limiting bayes factor , as the sample size grows to infinity , is @xmath17 or @xmath18 .",
    "similarly , @xcite compare point processes via bayes factors constructed on summary statistics .",
    "they discuss those summary statistics ( second order statistics and some based on vorono tesselations ) depending on the misclassification rates of the corresponding bayes factors through a simulation study .",
    "however , the connection with the genuine bayes factor is not pursued .",
    "( a connection with the abc setting appears in the conclusion of the paper , though , with a reference to @xcite which is often credited as one originator of the method . )",
    "the purpose of the current paper is to study asymptotic conditions on the statistic @xmath5 under which the bayes factor @xmath12 either converges to the correct answer or it does not .",
    "we obtain a precise characterisation of consistency in terms of the limiting distributions of the summary statistic @xmath3 under both models , namely that the true asymptotic mean of the summary statistic @xmath3 can not be recovered under the wrong model , except for nested models .",
    "as explained below , this characterisation shows that using point estimation statistics as summary statistics is rarely pertinent for testing while ancillary statistics are more likely candidates , at least formally .",
    "once stated , the condition on the statistic @xmath5 is quite natural in that the bayes factor will otherwise favour the simplest model .",
    "our main result implies that a validation of summary statistics providing convergent model choice is available for abc algorithms .",
    "the practical side is computational in that the mean values of the summary statistics can be checked by simulation .",
    "further properties of the vector of summary statistics can also be tested via these simulations , including the comparison of several summary statistics or , equivalently , the selection of the most discriminant components of the above vector .",
    "the above connection between the bayes factor based on the whole data @xmath4 and the bayes factor based on the summary @xmath3 is only valid when the latter is sufficient for both models . in this",
    "setting , and only in this setting , the ratio of the @xmath19 s in is equal to one solely when the statistic @xmath5 is furthermore sufficient across models @xmath7 and @xmath8 , i.e.  for the collection @xmath20 of the model index and of the parameter . a rather special instance where this occurs is the case of gibbs random fields @xcite",
    "otherwise , the conclusion drawn from using @xmath3 necessarily differs from the conclusion drawn from using @xmath4 .",
    "the same is obviously true outside the sufficient case , which implies that the selection of a summary statistic must be evaluated against its performances for model choice , because it is not guaranteed per se .",
    "the following example illustrates this point :    [ laplace - vs - gaussian ] to illustrate the impact of the choice of a summary statistic on the bayes factor , we consider the comparison of model @xmath7 : @xmath21 with model @xmath8 : @xmath22 , the laplace or double exponential distribution with mean @xmath23 and scale parameter @xmath24 , which has a variance equal to one .",
    "since it is irrelevant for consistency issues , we assume throughout the paper that the prior probabilities of both models @xmath7 and @xmath8 are equal to @xmath25 .    in this formal setting , we considered the following statistics :    1 .",
    "the sample mean @xmath26 ; 2 .   the sample median @xmath27 ; 3 .   the sample variance @xmath28 ; 4 .",
    "the median absolute deviation @xmath29 ; 5 .   the sample fourth moment @xmath30 ; 6 .   the sample sixth moment @xmath31 .",
    "given the models under comparison , the first statistic is sufficient only for the gaussian model , the second , fifth and sixth statistics are not sufficient but their distributions depend on @xmath32 in both models , while both the sample variance and the median absolute deviation are ancillary statistics .    as explained later in section [ sec : consequence ] , the most important feature of those statistics is that all statistics but the fourth one have the same expectation under both models ( when using appropriate values of the @xmath32 s ) while the median absolute deviation always has a different expectation under model @xmath33 and model @xmath34 .    since we are facing standard models in this artificial example , the analytic computation of the true bayes factor would be possible , even in the laplace case .",
    "however , if we base our inference only on one or several of the above statistics , the computation of the corresponding bayes factors requires an abc step .",
    "[ fig : norla1 ] shows the distribution of the posterior probability that the model is normal ( as opposed to laplace ) when the data is either normal or laplace and when the summary statistic in the abc algorithm is the collection of the first three statistics above .",
    "the outcome is thus that the estimated posterior probability has roughly the same predictive distribution under both models , hence abc based on those summary statistics is not discriminative . fig .",
    "[ fig : norla2 ] represents the same outcome when the summary statistic used in the abc algorithm is only made of the median absolute deviation of the sample . in this second case ,",
    "the two distributions of the estimated posterior probability are quite opposed under each model , concentrating near zero and one as the number of observations @xmath11 increases , respectively .",
    "hence , this summary statistic is highly discriminant for the comparison of the two models .",
    "from an abc perspective , this means that using the median absolute deviation is then satisfactory , as opposed to the first three statistics .",
    "finally , fig .",
    "[ fig:4th ] and [ fig:4 + 6th ] represents the same outcome when the summary statistics used in the abc algorithm are respectively the empirical fourth moment and both the empirical fourth and sixth moments .",
    "when using solely the empirical fourth moment , the posterior probability for the normal model is highly concentrated near 1 when the observations are normally distributed , while the posterior probability for the normal model slowly decreases to zero with the number of observations when they are laplace distributed .",
    "when using both the fourth and the sixth moments , the convergence ( to zero ) in the laplace case occurs faster .",
    "we note that the distance used for the latter case is an euclidean distance with weights @xmath33 and @xmath35 on the fourth and the sixth components , in order to compensate for the one - hundred - fold larger values of the square differences of the sixth moments . using a regular euclidean distance led to account only for the empirical sixth moment statistic . in section [ subsec :",
    "galap ] , the experimental results obtained in fig .",
    "[ fig:4th ] and [ fig:4 + 6th ] will be analysed in terms of theoretical results of [ sec : consequence ] .     when the data is made of @xmath36 observations _",
    "( left , centre , right , resp . ) _ either from a gaussian or laplace distribution with mean equal to zero and when the summary statistic in the abc algorithm is the vector made of the collection of * the sample mean , median , and variance*. the abc algorithm uses a reference table of @xmath37 simulations ( @xmath38 for each model ) from the prior @xmath39 and selects the tolerance @xmath40 as the @xmath41 distance quantile over those simulations . ]     when the abc algorithm is based on * the median absolute deviation * of the sample as its sole summary statistic . ]    , for @xmath42 observations , when the abc algorithm is based on * the empirical fourth moment * as its sole summary statistic . ]",
    "when the abc algorithm is based on both * the fourth and sixth empirical moments * as summary statistics . ]    the above example illustrates very clearly the major result of this paper , namely that the mean behaviour of the summary statistic @xmath3 under both models under comparison is fundamental for the convergence of the bayes factor , i.e.  of the bayesian model choice based on @xmath3 .",
    "this result , described in the next section , thus brings an answer to the question raised in @xcite about the validation of abc model choice , although it may require additional simulation experiments in realistic situations .",
    "the paper is organised as follows : section [ sec : zesection ] contains the theoretical derivation of the asymptotic behaviour of the bayes factor @xmath12 , section [ sec : asmpt ] covering our main assumptions and exhibiting the asymptotic behaviour of the marginal likelihoods , section [ sec : consequence ] detailing the consequences of this result for model choice based on summary statistics .",
    "section [ sec : illustre ] illustrates the relevance of our criterion for evaluating summary statistics , including a non - trivial population genetics example .",
    "section [ sec : checkstat ] details the practical implementation of a validation mechanism based on the above results .",
    "section [ seccon ] concludes the paper with a short discussion .",
    "let @xmath43 be the observed sample , not necessarily iid .",
    "we denote by @xmath44 the true distribution of the sample , and by @xmath45 a @xmath46-dimensional vector of summary statistics , @xmath47 .",
    "the distribution @xmath48 is the projection of @xmath49 under the map @xmath50 and we denote its density by @xmath51 .",
    "there are two competing models @xmath7 and @xmath8 that we wish to compare :    * under @xmath7 , @xmath52 where @xmath53 , * under @xmath8 , @xmath54 where @xmath55 .",
    "the distributions of @xmath56 under @xmath7 and @xmath8 are denoted by @xmath57 and @xmath58 , respectively .",
    "we also assume that the distribution functions @xmath59 , @xmath60 have densities @xmath61 and @xmath62 with respect to some dominating measures @xmath63 , respectively . under the respective prior distributions @xmath64 and @xmath65 on @xmath66 and @xmath23 ,",
    "the posterior distributions given @xmath56 are denoted by @xmath67 and @xmath68 .      before stating the main result in the paper , we detail theoretical assumptions on both the models and the summary statistics under which the main result holds .",
    "we start with a brief primer on our notations .",
    "the letter @xmath69 denotes a generic positive constant ( independent of @xmath11 ) , whose value may change from one occurrence to the next , but is of no consequence .",
    "we write @xmath70 to denote @xmath71 . for two sequences @xmath72 of real numbers , @xmath73 ( resp .",
    "@xmath74 ) means @xmath75 ( resp .",
    "similarly , @xmath77 means that @xmath78 the symbol @xmath79 denotes convergence in distribution .",
    "technical assumptions that are necessary for establishing the main result of the paper are as follows :    1 .",
    "[ a1 ] there exist a sequence of positive real numbers @xmath80 converging to @xmath81 , a distribution @xmath82 on @xmath83 , and a vector @xmath84 , such that @xmath85 2 .   [ a3 ] for @xmath86 , there exist sieves @xmath87 and constants @xmath88 , such that @xmath89 for all @xmath90 , the asymptotic means @xmath91 of @xmath56 under this model satisfy : for all @xmath92 @xmath93 \\leq c_i x^{-\\alpha_i}\\,.\\ ] ] we define the sets @xmath94 @xmath95 as @xmath96    we say that @xmath97 is _ compatible with _",
    "@xmath56 if @xmath98 meaning that the asymptotic mean of @xmath56 is found within the range of the means of @xmath56 in model @xmath97 .    1 .",
    "[ a4 ] if @xmath97 is compatible with @xmath56 , then there exists a constant @xmath99 such that @xmath100 2 .",
    "[ a5 ] if @xmath97 is compatible with @xmath56 , then for any @xmath101 there exist @xmath102 and a set @xmath103 such that for all @xmath104 @xmath105    even though these assumptions might appear overwhelming , we claim that ( [ a1])([a5 ] ) are both mild and relatively easy to check in applications .",
    "a detailed discussion on those assumptions is provided in section [ subsec : ass ] .",
    "furthermore , we will later illustrate why they hold in both the gaussian versus laplace example ( section [ subsec : galap ] ) and a realistic population example ( section [ sec : popgenx ] ) .    the following result provides a fundamental control on the convergence rate of the marginal likelihoods . in lemma",
    "[ thm : normasympt ] , @xmath106 and @xmath107 denote the marginal densities of @xmath56 under models @xmath7 and @xmath8 , respectively , namely @xmath63 @xmath108    [ thm : normasympt ] under assumptions ( [ a1])([a5 ] ) , for @xmath109 , there exist constants @xmath110 such that , if @xmath97 is compatible with @xmath56 , and @xmath111 , @xmath112 , @xmath113 and , if @xmath97 is not compatible with @xmath56 , @xmath114.\\ ] ]    the above lemma , or more precisely , provides an equivalence result for the marginal densities @xmath115 when @xmath116 but it does not specifically require that @xmath48 belongs to model @xmath97 .",
    "appendix 2 details the proof of lemma [ thm : normasympt ] .",
    "the following result is a corollary on the use of @xmath56 for inference purposes other than model choice :    [ corollary:1 ] under the assumptions of lemma [ thm : normasympt ] , if @xmath97 is compatible with @xmath56 , the posterior distribution @xmath117 concentrates at the rate @xmath118 on @xmath119 , provided @xmath111 and @xmath112 .",
    "hence , under the posterior distribution @xmath120 , @xmath121 converges to @xmath122 at the rate @xmath118 .",
    "equation of lemma [ thm : normasympt ] yields that @xmath123 with large probability .",
    "for all sequences @xmath124 converging to @xmath125 , calculations performed in the proof of lemma [ thm : normasympt ] ( see appendix 2 ) yield that with probability going to 1 under @xmath48 , @xmath126 therefore the posterior distribution of @xmath121 has its tail probability given by @xmath127 and the corollary follows .",
    "lemma [ thm : normasympt ] helps in understanding the meaning of the parameter @xmath128 in assumption ( [ a4 ] ) when @xmath97 is compatible with @xmath56 .",
    "indeed , we then have @xmath129 thus @xmath130 and @xmath131 appears as a penalisation factor resulting from integrating @xmath32 out in the very same spirit as the effective number of parameters appears in the dic @xcite criterion and in the discussions in @xcite and @xcite . in regular models",
    ", @xmath128 corresponds to the dimension of @xmath132 , leading to the usual bic approximation ; however , in non - regular models , which may occur with the kind of applications where abc methods are required , @xmath128 can be different .",
    "this is illustrated in the examples of section [ sec : illustre ] .",
    "we now present the major implication of these results on the relevance of some summary statistics to compute bayes factors .",
    "lemma [ thm : normasympt ] implies that the asymptotic behaviour of the bayes factor is driven by the asymptotic mean value of @xmath56 under both models .",
    "it is usual to assume that one of the competing models is true , when studying the behaviour of testing procedures ( here posterior probabilities and bayes factors ) . here , in full generality , it is actually enough that one of the models is compatible with the statistic @xmath56 .",
    "hence , without loss of generality we assume that the true distribution belongs to model @xmath7 and we first consider the case where model @xmath8 is _ also compatible _ with @xmath56 , i.e. @xmath133 under assumptions ( [ a1])([a5 ] ) , @xmath134 where @xmath110 , irrespective of the true model .",
    "thus the asymptotic behaviour of the bayes factor depends solely on the difference @xmath135 .",
    "for instance , if @xmath136 ( as in the embedded case ) and @xmath48 is in @xmath7 , the bayes factor goes to 0 , instead of infinity .",
    "if instead @xmath137 , the bayes factor is bounded from below and from above and is thus useless to separate the two models .",
    "note that the asymptotic ( non - convergent ) behaviour remains the same even when @xmath48 is in neither model , provided @xmath138    on the contrary , assume that the true distribution is in model @xmath7 and that model @xmath8 is not compatible with @xmath56 , then the bayes factor , under assumptions ( [ a1])([a5 ] ) , satisfies @xmath139 and if @xmath140 , @xmath141 which leads to choosing the right model asymptotically .",
    "the above then implies the following consistency result , which is the core derivation of our paper , providing a characterisation of relevant summary statistics :    [ thm : bfcon ] if , under assumptions ( [ a1])([a5 ] ) , models @xmath7 and @xmath8 are both compatible with @xmath56 , and @xmath142 , then the bayes factor @xmath143 has the same asymptotic behaviour as @xmath144 , irrespective of the true model .",
    "therefore , it always asymptotically selects the model with the smallest effective dimension @xmath128 .",
    "if model @xmath7 is compatible with @xmath56 and model @xmath145 is incompatible with @xmath56 , then @xmath146 and if @xmath147 , then the bayes factor @xmath148 is consistent .",
    "note that , for ancillary statistics , the condition @xmath149 is vacuous since @xmath150 and @xmath151 .",
    "the theorem therefore also applies to compatible ancillary statistics .",
    "an essential practical consequence of theorem [ thm : bfcon ] is that the bayes factor is merely driven by the means @xmath121 and the relative position of @xmath122 in both sets @xmath152 , @xmath109 .",
    "if @xmath48 is in neither model but @xmath122 belongs to @xmath153 and not to @xmath154 , then the bayes factor will asymptotically select @xmath7 .",
    "note that the result does not cover the behaviour of the bayes factor when neither model is compatible with @xmath56 , since there is no simple characterisation in this case .",
    "the following heuristic argument sheds some light on why the above results hold .",
    "suppose the summary statistics ( appropriately rescaled ) are asymptotically normal under each model .",
    "assume that the kullback - leibler divergence between the distributions of @xmath155 can be approximated by the kullback - leibler divergence between the respective asymptotic gaussian distributions @xmath156 and @xmath157 where @xmath158 , @xmath159 , and @xmath160 denote the asymptotic variances under the various models , where @xmath161 denotes the determinant of the matrix @xmath162 , and where @xmath163 is the pdf of the standard gaussian distribution .",
    "then @xmath164 in that case a usual laplace argument would imply that @xmath165 so that the difference between @xmath122 and @xmath121 is the key measure to evaluate the distance between @xmath166 and @xmath167 .",
    "the above argument is purely illustrative since requiring is very strong and not realistic in most cases .",
    "formally , an ideal statistics @xmath56 would be an ancillary statistics for both models with different expectation under both models . indeed , in this case , the sets @xmath168 @xmath63 are singletons and they only have to differ for the bayes factor to be consistent .",
    "for instance , in example 1 , both the empirical variance and the empirical mad statistic are ancillary . in the first case , the expectation is the _ same _ under both distribution , which explains why the bayes factor can not discriminate between models ( fig .",
    "[ fig : norla1 ] ) . in the second case ,",
    "the expectations differ , hence a consistent bayes factor as exhibited in fig .",
    "[ fig : norla2 ] .",
    "concerning the assumptions ( [ a1])([a5 ] ) , some simplifications occur under ancillarity :    * assumption ( [ a3 ] ) must hold for a single distribution and @xmath169 ; * assumption ( [ a4 ] ) holds automatically since @xmath170 ; * assumption ( [ a5 ] ) must also hold for the fixed distribution of @xmath56 under model @xmath97 ( and obviously holds when @xmath97 is the true model ) .",
    "unfortunately , it is very hard to extract useful ancillary statistics from complex models : while examples of ancillary statistics abound , for instance rank statistics @xcite , they either do not apply to non - iid settings or have identical means under different models .",
    "example 1 is thus truly a toy example in that it constitutes the exception to this remark .",
    "when considering the population genetics models of section [ sec : illustre ] , we can not provide such solutions .    in the special case of @xmath7 being a submodel of @xmath8 , and",
    "if the true distribution belongs to the smaller model @xmath7 , any summary statistic satisfies @xmath171 so that the bayes factor is of order @xmath172 . if the summary statistic is informative merely on a parameter which is the same under both models , _",
    "i.e. _ , if @xmath137 , then the bayes factor is not consistent .",
    "else , @xmath173 and the bayes factor is consistent under @xmath7 .",
    "if the true distribution does not belong to @xmath7 , then the same phenomenon as described above occurs and the bayes factor is consistent only if @xmath174 .",
    "this case will be illustrated for a quantile distribution in section [ sec : kant ] .",
    "assumptions ( [ a1])([a5 ] ) may appear too stringent or too abstract to be of any practical relevance and we now discuss why they make perfect sense .",
    "assumption ( [ a1 ] ) is quite natural .",
    "it is often the case that summary statistics @xmath56 are chosen as empirical versions of quantities of interest ( under second order moment conditions ) and it is natural to assume that they concentrate since they are chosen to be both low dimensional and informative on some aspects of the model ( even though the result also applies to ancillary statistics ) . for instance , when the summary statistics are empirical means or empirical quantiles , ( [ a1 ] ) is satisfied with @xmath175 and the gaussian distribution being the limiting @xmath82 ( a most common occurrence ) .",
    "however , if @xmath56 is a distance ( e.g. , of the type induced by chi - square like statistics ) then @xmath82 will be the chi - square distribution .",
    "we also note that ( [ a1 ] ) holds for some ancillary statistics , like those of example 1 .",
    "assumption ( [ a3 ] ) requires that under each model @xmath56 concentrates around the model asymptotic mean values @xmath121 at rate @xmath176 , even though it is not necessary to have convergence in distribution .",
    "more precisely , ( [ a3 ] ) controls the moderate deviations of the estimator @xmath56 from the asymptotic mean @xmath177 under each model .",
    "for instance , when @xmath56 is an empirical mean , i.e. , @xmath178 for a given function @xmath179 , markov inequality implies that for every @xmath180 , @xmath181 \\leq \\frac { \\mathbb{e}\\big [   \\left| \\sum_{i=1}^n \\{h(y_i )   - \\mu_i(\\theta_i)\\ } \\big| \\theta_i \\right|^p\\big ] } { u^p \\,n^{p/2 } } \\lesssim u^{-p } , \\end{aligned}\\ ] ] for large values of @xmath14 ( typically , larger than @xmath182 ) and under very weak assumptions ( much weaker than being in an i.i.d .  setting ) .",
    "assumption ( [ a4 ] ) describes the behaviour of the prior distribution of the mean of @xmath56 near the true asymptotic value @xmath122 .",
    "this assumption needs only hold on a compatible model and it is often found in the bayesian asymptotic literature , see for instance condition ( 2.5 ) of theorem 1 in @xcite",
    ". usually referred to as _ the prior mass condition _ , it corresponds to the fact that if the prior vanishes in regions where the likelihood is not too small ( i.e. , near @xmath122 in our case ) then the marginal becomes very small .",
    "the exponents @xmath128 can be viewed as effective dimensions of the parameter under the posterior distributions , as discussed after corollary [ corollary:1 ] .",
    "if the maps @xmath183 are locally invertible near @xmath122 , under the usual continuity conditions on the maps @xmath184 , for any @xmath185 , there exists a finite collection of points @xmath186 such that the sets @xmath187 can be bounded both from above and from below by sets of the form @xmath188 thus if the prior density @xmath189 is bounded from above and below near the points @xmath190 , we immediately deduce that @xmath191 and @xmath192 , verifying ( [ a4 ] ) . in most cases",
    "we will have @xmath193 , since assuming that @xmath194 would imply that the prior density of @xmath195 explodes at @xmath122 .",
    "assumption ( [ a5 ] ) states that , if there are @xmath32 s such that @xmath196 , then uniformly in @xmath197 close to one of those @xmath32 s , @xmath198 is bounded from below by @xmath199 on a set having large probability in terms of @xmath48 .",
    "there are various instances under which this assumption is satisfied .",
    "first , if @xmath97 is the true model and @xmath56 is ancillary under this model , it automatically holds since for all @xmath200 @xmath201 . secondly ,",
    "if @xmath202 converges in distribution to @xmath82 and if the densities are close , then ( [ a5 ] ) is satisfied .",
    "this requires in particular that @xmath56 has the same support under @xmath48 and @xmath203 , but not necessarily that @xmath203 or @xmath48 are continuous distributions .",
    "assumption ( [ a5 ] ) may become difficult to check when the sets @xmath204 are not compact , which is typically the case when the sets @xmath205 are not compact .",
    "the important point here is that , in such cases , the posterior distribution @xmath206 is not informative on the whole parameter @xmath32 ( at least no further than the prior ) but instead informative on a fraction of it , summarised by @xmath121 .",
    "re - parametrising @xmath207 into @xmath208 where @xmath209 represents the part of @xmath32 which is not informed by the asymptotic distribution of @xmath56 , @xmath56 is asymptotically ancillary for @xmath209 . in such a case , ( [ a5 ] ) will still hold in situations where the prior distribution does not assign too much mass near the tails , so that the sieves @xmath210 can be chosen not too large to ensure that the distributions @xmath211 of @xmath56 hardly depend on @xmath209 .",
    "recall that in the setting of example [ laplace - vs - gaussian ] , we denote by @xmath7 the gaussian model and by @xmath8 the laplace model . in each model ,",
    "the prior on the mean @xmath32 is a centred gaussian distribution with variance @xmath212 and in each case the data are simulated under @xmath213 . for illustrating our main result on consistency , we consider the summary statistics made of the empirical fourth moment , @xmath214 , such that @xmath215 and @xmath216 .",
    "we now endeavour to check that assumptions ( [ a1])([a5 ] ) hold for that statistic .",
    "given that this is an empirical moment , ( [ a1 ] ) is trivially satisfied as a consequence of the central limit theorem , with @xmath217 .",
    "for assumption ( [ a3 ] ) , @xmath121 is already defined above . for both models , we set @xmath218 for ( [ a3 ] ) to hold , so that @xmath219 under a gaussian prior on @xmath197 under both models , which implies @xmath220 . the second part of ( [ a3 ] ) is verified using markov inequality .",
    "indeed , @xmath221    & \\leq \\frac{\\mathbb{e}_i [ ( y^4 - \\mu_i(\\theta))^4|\\theta ] } { n^2 x^4}\\ , .",
    "\\end{split}\\ ] ] which implies @xmath222 .    addressing ( [ a4 ] ) , in model @xmath7",
    "if the mean is equal to zero then @xmath223 and , in model @xmath8 if @xmath224 , then @xmath225 . thus @xmath226 and @xmath227 can be bounded from above and below by balls of the form @xmath228 so that @xmath229 in those cases .",
    "note that , if the mean is different from zero , @xmath230 in model @xmath7 and @xmath231 in model @xmath8 .",
    "then @xmath232 and @xmath227 can be bounded from above and below by balls in the form @xmath233 so that @xmath234 in those cases .",
    "+ addressing ( [ a5 ] ) , since both distributions satisfy cramer condition , the empirical fourth moment allows for an edgeworth expansion under both models , which can be made uniform in sets in the form @xmath235 , see ( * ? ? ?",
    "* theorem 19.1 ) .",
    "hence , ( [ a5 ] ) is satisfied .    in conclusion , if the true distribution belongs to model @xmath8 and the mean is equal to zero , @xmath236 for both @xmath109 and we have @xmath237 and @xmath238 . on the other hand ,",
    "if the true distribution belongs to model @xmath7 then @xmath239 and @xmath240 following from theorem [ thm : bfcon ] , the bayes factor is then consistent but at the rate @xmath241 under model @xmath8 .",
    "this is to some extent an accidental result , merely due to the fact that , in that very special case when the mean is equal to zero , @xmath136 .",
    "[ fig:4th ] presented in section [ subsec : insu ] illustrates the above discussion . finally , note",
    "that , if the mean is different from zero , then a similar argument leads to the lack of consistency of the bayes factor , since then @xmath234 and @xmath242 , for both @xmath109 .",
    "we now consider the example of a four - parameter quantile distribution , defined through its quantile function @xmath243 where @xmath244 is the @xmath14th standard normal quantile and the parameters @xmath245 and @xmath246 represent location , scale , skewness and kurtosis , respectively @xcite .",
    "while the quantile function is well - defined , and the distribution easy to simulate , there is no closed - form expression for the corresponding density function , which makes the implementation of an mcmc algorithm quite delicate .",
    "@xcite introduce a abc procedure that uses the order statistics as summary statistics .",
    "we consider here a model choice perspective .    in this experiment",
    ", we set @xmath247 and @xmath248 .",
    "we then oppose two models :    * model @xmath7 , in which @xmath249 , with a single unknown parameter @xmath250 and a prior @xmath251 $ ] . in the simulation process , when @xmath7 is true , we choose @xmath252 . * model @xmath8 , with two unknown parameters @xmath253 and a prior @xmath254\\otimes\\mathcal{u}[-1/2,5]$ ] . in the simulation process ,",
    "when @xmath8 is true , we choose @xmath255 and @xmath256 .",
    "this obviously is a case of embedded models , since @xmath7 is a sub - model of @xmath8 . as in the previous experiments",
    ", we use an abc procedure relying on @xmath37 proposals from the prior and a tolerance set at the @xmath41 quantile of the @xmath257 distances between some empirical quantiles . in the comparison",
    "below , we first use the empirical quantile of order @xmath258 as sole summary statistic .",
    "then , we consider the empirical quantiles of order @xmath258 and @xmath259 , and , at last , the empirical quantiles of order @xmath258 , @xmath260 , @xmath261 and @xmath259 .",
    "the results are summarised in fig .",
    "[ fig : quantiles ] .",
    "they show complete agreement with theorem 1 .",
    "when the summary statistic @xmath56 is restricted to the empirical quantile of order @xmath258 , the bayes factor is not consistent .",
    "indeed , in such a case , we have @xmath262 and @xmath263 when @xmath7 is true with @xmath264 , then @xmath265 and @xmath266 .",
    "similarly , when @xmath8 is true with @xmath267 and @xmath264 , then @xmath268 and @xmath269 therefore , the bayes factor has the same asymptotic behaviour as @xmath270 , irrespective of the true model .",
    "we can prove that @xmath234 in this case , therefore that the bayes factor is not consistent .    when the summary statistics @xmath56 is the vector made of the empirical quantiles of order @xmath258 and @xmath259 , the bayes factor is consistent .",
    "indeed , in such a case , we have @xmath271\\,,\\ ] ] and @xmath272 @xmath273\\,.\\ ] ] when @xmath7 is true with @xmath264 , then @xmath274 $ ] and @xmath269 we can prove that @xmath275 and hence that the bayes factor is consistent . moreover , when @xmath8 is true with @xmath267 and @xmath264 , then @xmath276 $ ] , and @xmath277 we can prove that @xmath278 and therefore that the bayes factor is again consistent .    finally , when the summary statistics @xmath56 is the larger vector made of the empirical quantiles of order @xmath258 , @xmath260 , @xmath261 and @xmath259 , the bayes factor is obviously consistent .",
    "however , the results obtained in fig .  [ fig : quantiles ] are very similar to the ones obtained with only two empirical quantiles .     when the data is made of 100 observations _",
    "( left column ) _ , 1000 observations _ ( central column ) _ and 10,000 observations _",
    "( right column ) _ either from @xmath7 _",
    "( m1 ) _ or @xmath8 _ ( m2 ) _ when the summary statistics in the abc algorithm are made of the empirical quantile at level @xmath258 _",
    "( first row ) _ , the empirical quantiles at levels @xmath258 and @xmath259 _ ( second row ) _ , and the empirical quantiles at levels @xmath258 , @xmath260 , @xmath261 and @xmath259 _",
    "( third row ) _ , respectively .",
    "the boxplots rely on @xmath279 replicas and the abc algorithms are based on @xmath37 proposals ( @xmath38 for each model ) from the prior , with the tolerance being chosen as the @xmath41 quantile on the distances . ]",
    "we now examine a monte carlo experiment that more directly relates to the genesis of abc , namely population genetics . as in @xcite , we consider two populations ( 1 and 2 ) having diverged at a fixed time @xmath280 in the past and a third population ( 3 ) having diverged from one of those two populations ( models 1 and 2 , respectively ) .",
    "times are set to @xmath281 generations for the first divergence and @xmath282 generations for the second one .",
    "the effective population size is assumed to be identical for all three populations and equal to @xmath283 .",
    "recall that the effective size of a population is defined as the size of an ideal ( wright - fisher ) population that would show the same behaviour as the population of interest , in terms of loss of genetic variation due to random drift .",
    "+ we assume we observed 50 diploid individuals per population genotyped at 5 , 50 or 100 independent microsatellite loci , this number acting as a proxy to the sample size . these loci are assumed to evolve according to the stepwise mutation model : when a mutation occurs , the number of repetitions of the mutated gene increases or decreases by one unit with equal probability .",
    "for each configuration ( defined in terms of loci numbers ) , we generate 100 observations for which the mutation rate @xmath197 is common to all loci and set to @xmath284 . in these experiments ,",
    "both scenarios have a single parameter , the mutation rate @xmath197 .",
    "we chose a uniform prior distribution @xmath285 $ ] on this parameter @xmath197 .",
    "for the abc analysis , we use three summary statistics associated to the @xmath286 distances @xcite .",
    "let @xmath287 be the repeated number of allele in locus @xmath288 ( @xmath289 ) for individual @xmath290 ( corresponding to @xmath291 diploid individuals ) within population @xmath292 .",
    "the @xmath286 distance between population @xmath293 and @xmath294 , denoted by @xmath295 , is : @xmath296    let us consider two copies of the locus @xmath297 with allele sizes @xmath298 and @xmath299 , and assume that the most recent time in the past for which they have a common ancestor , defined as the coalescence time @xmath300 , is known .",
    "the two copies are then separated by a branch of gene genealogy of total length @xmath301 .",
    "as explained in @xcite , according to the coalescent process , during that time the number of mutations is a random variable distributed from a poisson distribution with parameter @xmath302 .",
    "therefore , if the stepwise mutation model is adopted , we get ( under models 1 and 2 ) @xmath303 in addition , if @xmath304 and @xmath305 , we have ( under models 1 and 2 ) @xmath306 and @xmath307 moreover , @xmath308 and @xmath309 the coalescent process associated to the stepwise mutation model gives @xmath310 and then @xmath311 we can apply the same type of reasoning to the other @xmath286 distances , and if @xmath312 denotes the mutation rate under model @xmath313 , we get the results given in table [ tab : tab1 ] .",
    ".[tab : deltamus ] theoretical expectations of the @xmath314 statistics under both models [ tab : tab1 ] [ cols=\"<,^,^ \" , ]     given the complexity of this genetic model , it provides a realistic example of relevant statistics satisfying the assumptions ( [ a1])([a5 ] ) . let us consider the associated statistics @xmath315 .",
    "this is an empirical mean of variables @xmath316 which are independent and identically distributed .",
    "moreover , since for each couple @xmath317 , @xmath318 is bounded by a poisson random variable , then , under each model , @xmath319 has moments of all orders and @xmath320 .",
    "thus , using theorem 2.2.1 of @xcite , we obtain that for all @xmath321 and all @xmath32 @xmath322 where    1 .   the order @xmath323 is uniform over @xmath324 and over compact subsets of @xmath325 , 2 .",
    "@xmath326 is the edgeworth expansion of the density of @xmath315 : @xmath327 for some @xmath328 .",
    "if under the true distribution @xmath319 also has at least @xmath329 moments , then @xmath330 for some @xmath331 and @xmath332 thus ( [ a1 ] ) is satisfied with @xmath333 . introducing @xmath334 , @xmath335 from and implies that , if @xmath336 , @xmath337 for all @xmath338 by choosing @xmath339 small enough .",
    "hence ( [ a5 ] ) is verified .",
    "moreover , if model @xmath97 is compatible , @xmath340 and @xmath341 , for @xmath342 , @xmath343 and ( [ a4 ] ) is satisfied with @xmath344 .",
    "it is straightforward to verify ( [ a3 ] ) . indeed , if model @xmath97 is not compatible , choosing @xmath345 , for all @xmath346 , using we get @xmath347 = o(n^{-(s-1)/2})\\ ] ] uniformly in @xmath32 for all @xmath329 . if model @xmath97 is compatible , using markov inequality , @xmath348",
    "\\leq    \\frac {   { \\mathbb{e}}_{\\theta_i}\\left [ |(\\delta\\mu)^2_{1,2 } - \\mu_i(\\theta_i ) |^4\\right ] } {   x^4 } \\ ] ] and ( [ a3 ] ) is satisfied with @xmath222 .",
    "we can use the same arguments to show that assumptions ( [ a1])([a5 ] ) holds also if @xmath349 .",
    "table [ tab : deltamus ] indicates that whatever model the data originates from ( whether @xmath7 or @xmath8 ) , the bayes factor based only on the distance @xmath315 as the summary statistic does not converge .",
    "indeed , if @xmath350 , we get the same expectation on the first line of table [ tab : deltamus ] .",
    "the same occurs when only @xmath351 ( resp .",
    "@xmath352 ) is used . indeed , in that case , if @xmath353 ( resp .",
    "@xmath354 ) we get the same expectation on the second ( resp .",
    ", the third ) row of table [ tab : deltamus ] . now , if either two or three of the distances are used , the bayes factors do converge .",
    "indeed , in these settings , no value of @xmath66 and @xmath23 can produce equal expectations .",
    "[ fig : popgen ] shows how the empirical results confirm this theoretical analysis .",
    "even the medium case of 50 loci indicates whether the use of the corresponding summary statistic(s ) is valid or not . under both models ,",
    "the abc computations have been performed using the diy - abc software @xcite",
    ".     distances .",
    "the abc algorithm uses @xmath355 proposals ( @xmath356 for each model ) from the prior and selects the tolerance @xmath40 as the @xmath357 distance quantile . ]",
    "while theorem 1 operates in an asymptotic and theoretical framework , it is nonetheless possible to find a methodological consequence from this characterisation of consistent summary statistics for testing .",
    "this result states that the summary statistic @xmath56 is not consistent ( and thus unacceptable ) for testing between models when both models are compatible with @xmath56 , in other words when @xmath358 based on this our asymptotic result , we propose to run a practical check of the relevance ( or non - relevance ) of @xmath56 .",
    "the null hypothesis of this test is expressed as both models are compatible with the statistic @xmath56 .",
    "the testing procedure then provides estimates of the mean of @xmath56 under each model and checks whether or not those means are equal . for the sake of clarity ,",
    "we assume without loss of generality that @xmath7 is the true model ( recall that it is enough to have this model compatible with the statistic @xmath56 ) , so checking the relevance of @xmath56 means testing for @xmath359 against @xmath360    corollary [ corollary:1 ] implies that , when model @xmath97 is compatible with @xmath56 , the predictive value of the summary statistic , @xmath361 $ ] , is approximately equal to @xmath122 ( @xmath4 denotes the observed summary statistic ) : @xmath362 & = \\int        t g_{i ,",
    "n}(t | \\theta_i ) dt d\\pi_i ( \\theta_i| { \\boldsymbol{t}}^n({\\mathbf{y } } ) )   \\\\    & = \\int_{\\theta_i } \\mu_i(\\theta_i ) d\\pi_i ( \\theta_i| { \\boldsymbol{t}}^n({\\mathbf{y } } ) ) \\\\    & = \\mu_0 + \\int_{\\theta_i } ( \\mu_i(\\theta_i ) - \\mu_0 ) d\\pi_i ( \\theta_i| { \\boldsymbol{t}}^n({\\mathbf{y}}))\\ , .",
    "\\end{split}\\ ] ] when @xmath363 is bounded on @xmath364 ( for instance when @xmath364 is compact and @xmath365 is continuous ) @xmath366 thus , under the null ( non - relevance of @xmath56 ) , we have @xmath367 =    { \\mathbb{e}}^\\pi\\left[{\\boldsymbol{t}}^n({\\mathbf{y}}^{\\rm{new}})|{\\boldsymbol{t}}^n({\\mathbf{y } } ) ,   { \\mathfrak{m}}_2 \\right ] + o_p(1 ) = \\mu_0 + o_p(1)\\ ] ] and the proximity of both predictive values indicates that the statistic @xmath56 is not discriminant .    to quantify what this notion of proximity means we advocate using the following practical procedure . under each model @xmath97 , @xmath109 ,",
    "run an abc sample producing a sample @xmath368 from the approximate posterior distribution of @xmath32 given @xmath369 .",
    "note that @xmath370 can be chosen to be arbitrarily large .",
    "for each value @xmath371 , generate @xmath372 , derive @xmath373 and compute @xmath374 conditionally on @xmath369 , we have @xmath375 ) \\rightsquigarrow \\mathcal n ( 0 , v_i),\\ ] ] for some @xmath376 , as @xmath370 goes to infinity .",
    "therefore , we propose to test for a common mean @xmath377 against the alternative of different means @xmath378 this test is implemented using the fact that asymptotically the decision statistic @xmath379 converges to a chi - squared distribution even in the case the covariance matrices @xmath380 and @xmath381 are estimated by convergent estimators , e.g.  empirical covariances .",
    "if the null hypothesis @xmath382 can not be rejected , we conclude that the statistic @xmath369 is not adequate for model choice .      in the case of the normal versus laplace toy problem , we ran a hundred evaluations based on three and four statistics , i.e.  the empirical mean , median and variance , without and with the empirical mad .",
    "the two choices of the summary statistic vector led to two different abc approximations of the posterior distribution . under each model ,",
    "the abc procedure is based on a fixed reference table of @xmath383 proposals from the prior @xmath39 and the respective model , and it selects the tolerance @xmath40 as the @xmath41 quantile of the deduced simulation distances .",
    "then , under each model , we get a @xmath384-sample as an approximation of the posterior distributions . for each of those values , we simulated samples from the models , with the same size as the original sample , producing samples @xmath373 , and we derived from those samples @xmath385 tests about the equality of the means . fig .",
    "[ fig : verif - gl ] evaluates the impact of including the empirical mad within those summary statistics on the result of the @xmath385 test .",
    "the result of this simulation experiment ( based on 100 replications ) is quite satisfactory in that in approximately @xmath386 of cases the difference between the two empirical means falls within the null hypothesis acceptance interval associated with a @xmath387 error when the empirical mad is not included , therefore concluding on the inappropriateness of the summary statistics to conduct the abc model comparison . on the opposite",
    ", the difference always is outside the null hypothesis acceptance interval when the empirical mad is included .          for the population genetics example",
    ", we ran a comparison experiment between the case when we use @xmath315 as sole summary statistic and when we use instead the vector @xmath388 .",
    "the results are presented in fig .",
    "[ fig : verif - popgen ] . for each of the @xmath279 points used for the boxplots ,",
    "the data is made of @xmath389 loci and the @xmath390-statistics are based on @xmath384 samples under each model . in fact , the abc algorithm relies on a fixed reference table of @xmath356 proposals from the prior and the respective model , and it selects the tolerance @xmath40 as the @xmath357 quantile of the deduced simulation distances . as for the previous example , the result of this simulation experiment is quite satisfactory .",
    "it highlights the ability of our empirical procedure to detect inappropriate summary statistics .",
    "we can truly compare the expectation of a summary statistics under both models using parameter values drawn from the abc posterior approximation .     and",
    "@xmath388.,title=\"fig : \" ]   and @xmath388.,title=\"fig : \" ]",
    "this paper has produced sufficient conditions for a summary statistics to produce a consistent or an inconsistent bayesian model choice .",
    "it thus brings an answer to the question raised in @xcite , which was warning the abc community about the potential pitfalls of an uncontrolled use of abc approximations to bayes factors . the central condition that the true asymptotic mean of the summary statistic should not be recovered under the wrong model",
    "if model choice is to take place ( in a convergent manner ) is both natural , in that the asymptotic normality implies that only first moments matter , and fundamental , in that it drives the choice of summary statistics in practical settings , first and foremost for the abc algorithm . indeed ,",
    "theorem [ thm : bfcon ] implies that estimation statistics should not be used in abc algorithms aiming at model comparison , unless their expectation can be shown to differ under both models .",
    "this means that ( a ) different statistics should be used for estimation and for testing and ( b ) that they should not be mixed in a single summary statistic .",
    "note that the distinction differs from the sufficient versus ancillary opposition found in classical statistics @xcite in that it is enough that the summary statistic @xmath56 has a different asymptotic mean under both models .",
    "in addition , and as shown in the normal - laplace example in section [ subsec : galap ] , some ancillary statistics may not be appropriate for testing .    at a methodological level , the classification of summary statistics resulting from the present study is paramount : when comparing models with a given range of potential summary statistics , the expectations of the various summary statistics can be evaluated by simulation under all models .",
    "for instance , in abc settings , the production of pseudo - data is a requirement for the implementation of the method ; it is therefore quite straightforward to test via a preliminary experiment whether the condition of theorem [ thm : bfcon ] holds .    neither the final choice of summary statistics as in @xcite , nor the comparison with alternative model comparisons techniques such as @xmath13 @xcite are covered in the current paper .",
    "these obviously are issues worth investigating and they constitute seeds for future development in the area .",
    "part of this work was done when the second author ( nsp ) was visiting dauphine and crest and he thanks both institutions for their warm hospitality .",
    "all authors but the second author are partially supported by the agence nationale de la recherche ( anr , 212 , rue de bercy 75012 paris ) through the 20092013 projects bandhits and emile .",
    "the second author is partially supported by the nsf grant 1107070 .",
    "the authors are grateful to the whole editorial panel for their supportive and constructive comments throughout the editorial process .",
    "discussions with dennis prangle at various stages of the paper were quite helpful .",
    "recall that @xmath48 is the true distribution of @xmath56 .",
    "let us first assume that @xmath391 and let @xmath392 be defined as in ( [ a4 ] ) .",
    "note that from ( [ a1 ] ) , for all @xmath393 , there exists @xmath394 such that for @xmath11 large enough @xmath395 and note that @xmath396 goes to infinity as @xmath397 goes to 0 .",
    "let @xmath398 and consider the set @xmath399 , and the positive constants @xmath400 and @xmath401 defined in ( [ a5 ] ) . from we have for all @xmath402 , @xmath403 for some positive constant @xmath404 , where the second inequality follows from the definition of @xmath399 and the last from ( [ a4 ] ) .",
    "formally , since @xmath405 , there exists @xmath406 such that for @xmath11 large enough @xmath407\\geq   1 - \\epsilon.\\end{aligned}\\ ] ]    we now obtain an upper bound for @xmath115 .",
    "using we write , @xmath408 as before fix @xmath409 and let @xmath410 be defined as in . applying markov inequality and fubini s theorem , we obtain that , for all @xmath398 , @xmath411 \\\\ &   +   \\int_{\\mathcal f_{n , i}^c } \\int_{v_n|t - \\mu_0|\\leq m_\\delta } \\frac{1}{\\epsilon\\,g_n(t)\\,\\pi_i(s_{n , i } ) }   g_n(t ) g_i(t|\\theta_i ) dt   \\,\\pi_i(\\theta_i)\\,d\\theta_i    \\\\ & \\leq g_n [ |{\\boldsymbol{t}}^n -\\mu_0| > m_\\delta v_n^{-1 } ] \\\\ &   +   \\int_{\\mathcal f_{n , i}^c } \\frac{1}{\\epsilon\\,\\pi_i(s_{n , i } ) } \\int_{{\\mathbb{r}}^d }",
    "g_i(t|\\theta_i ) dt   \\,\\pi_i(\\theta_i)\\,d\\theta_i    \\\\ & \\leq \\delta + \\frac{\\pi(\\mathcal f_{n , i}^c ) } { \\epsilon \\pi_i(s_{n , i } ) } \\leq 2 \\delta , \\end{split}\\ ] ] given the conditions imposed by ( [ a3 ] ) and ( [ a4 ] ) , when @xmath11 is large enough .",
    "we can represent @xmath210 as a finite disjoint union of the following sets : @xmath412 now we have @xmath413 set @xmath414 since @xmath415 .",
    "then , if @xmath416 , @xmath417 and if @xmath418 is a constant such that @xmath419 we obtain @xmath420 & \\leq &   \\frac{1 } { m_\\delta^k v_n^{-d_i } } \\pi_i(s_{n , i}(m_\\delta ) ) + \\delta \\nonumber \\\\ & = & o ( m_\\delta^{d_i - k } ) + \\delta , \\label{eqn : mdeltadi}\\end{aligned}\\ ] ] where the last inequality follows from in ( [ a4 ] ) . since @xmath421 , the bound in goes to @xmath17 as @xmath397 goes to zero . using ( [ a3 ] ) and following exactly the same argumentation as for , i.e.  markov inequality and fubini s theorem",
    ", we obtain that for @xmath422 , @xmath423 for @xmath11 large enough , and similarly @xmath424 for @xmath11 large enough , under assumption ( [ a4 ] ) . combining the above inequalities , , and with , we obtain for @xmath11 large enough , @xmath425 which can be made arbitrarily small by choosing @xmath426 small enough . combining",
    "the above with implies that @xmath427 the above estimate together with the lower bound obtained in proves the first claim ( equation ) of lemma [ thm : normasympt ] .",
    "now suppose @xmath428 .",
    "then there exists @xmath429 such that @xmath430 for all @xmath431 . a computation identical to , together with , yields that for all sequences @xmath432 going to infinity and all @xmath398 , @xmath433 for @xmath11 large enough .",
    "this proves the second claim ( equation ) of lemma [ thm : normasympt ] ."
  ],
  "abstract_text": [
    "<S> the choice of the summary statistics used in bayesian inference and in particular in abc algorithms has bearings on the validation of the resulting inference . </S>",
    "<S> those statistics are nonetheless customarily used in abc algorithms without consistency checks . </S>",
    "<S> we derive necessary and sufficient conditions on summary statistics for the corresponding bayes factor to be convergent , namely to asymptotically select the true model . </S>",
    "<S> those conditions , which amount to the expectations of the summary statistics differing asymptotically under the two models , are quite natural and can be exploited in abc settings to infer whether or not a choice of summary statistics is appropriate , via a monte carlo validation . </S>"
  ]
}