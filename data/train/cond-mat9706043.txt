{
  "article_text": [
    ""
  ],
  "abstract_text": [
    "<S> exact solutions for the learning problem of autoassociative networks with binary couplings are determined by a new method : the use of a branch - and - bound algorithm leads to a substantial saving of computing time compared to complete enumeration . as a result </S>",
    "<S> , fully connected networks with up to 40 neurons could be investigated . </S>",
    "<S> the network capacity is found to be close to 0.83 .    </S>",
    "<S> [ exact learning in binary neuronal networks ]    the training of neural networks with binary couplings is believed to belong to the class of np - complete problems i.e. the average computing time required to find a solution scales exponentially with the number of couplings to determine . </S>",
    "<S> this exponential scaling is due to the discrete structure of the space of couplings and is obvious in the case of complete enumeration . </S>",
    "<S> however , theoretical @xcite and numerical @xcite studies showed that it holds as well for heuristic approaches ( e.g. simulated annealing ) . </S>",
    "<S> training by complete enumeration has been carried out for small networks with up to 25 neurons @xcite . </S>",
    "<S> heuristic algorithms @xcite were used for networks with up to thousand neurons . </S>",
    "<S> still , the main disadvantage of heuristic algorithms consists in the uncertainty about the existence of solutions not found by the algorithm .    our aim has been to develop an exact algorithm guaranteed to find all possible solutions in considerably less computing time than complete enumeration . in @xcite </S>",
    "<S> , gardner showed that the space of interactions in neural network models can be treated in a way similar to the phase space of spin glass models . </S>",
    "<S> accordingly , it should be possible to use the branch - and - bound method , already successfully applied to the search for ground states of a ising spin glass model @xcite , for the training of neural networks with binary couplings .    </S>",
    "<S> consider an autoassociative network built of @xmath0 two - state neurons @xmath1 and fully connected by binary synaptic couplings that can take on the values @xmath2 . </S>",
    "<S> the self couplings @xmath3 should be set to zero . </S>",
    "<S> the task of the network would be to store a set of patterns @xmath4 with elements @xmath5 . </S>",
    "<S> a training procedure determines couplings that make these patterns attractors of the discrete network dynamics @xmath6 the capacity of the network specifies the number of different patterns that can be stored simultaneously . </S>",
    "<S> it is normally expressed as a critical load @xmath7 .    for good retrieval one </S>",
    "<S> is interested in large basins of attraction . as discussed in @xcite , </S>",
    "<S> these correspond to large values of the pattern stability @xmath8 the maximally stable rule therefore formulates the learning problem as an optimization task : for a given set of patterns , one has to determine an optimal set of couplings that maximises the network stability @xmath9 as long as there is no symmetry constraint on the matrix of couplings , the optimization task separates into the training of @xmath0 simple perceptrons with @xmath10 input neurons , corresponding to the individual rows of the matrix with the self - coupling excluded . </S>",
    "<S> the network stability @xmath11 emerges as the minimum of the `` perceptron stabilities '' @xmath12 .    </S>",
    "<S> the new learning algorithm was developed using the branch - and - bound method , a standard tool of combinatorial optimization theory @xcite : to find a row of the matrix of couplings with maximal stability @xmath12 , complete enumeration would check the @xmath13 possible configurations for optimal ones . </S>",
    "<S> branch - and - bound starts with a division into a hierarchy of subproblems : each single coupling is tested with both possible values yet taking into account the state of the previously ( on a trial basis ) determined couplings , thus forming a binary tree of  incomplete \" configurations . only the final level of the tree would contain the  complete \" solutions . this division is the ` branching ' part of the algorithm . standing alone </S>",
    "<S> it would double the necessary computing time . here </S>",
    "<S> the ` bounding ' ( and subsequently cutting ) part comes into action : for each node of the binary tree an upper bound for the best possible solution of the remaining subproblem is evaluated . starting point is an ideal stability , @xmath14 , which is obtained if one takes all terms in the sum ( [ eq : stability ] ) to be positive . </S>",
    "<S> ( generally , the maximal stability lies below this ideal stability which can only be achieved if there is just one pattern to store . ) when testing a coupling @xmath15 , this bound will be corrected , taking into account the already fixed part of the configuration . </S>",
    "<S> if it falls under a pre - set value , e.g. the stability attained by the use of the clipped hebb rule , the binary tree is  cut \" at this node , i.e. the subtree of this node does not need to be considered . as a result , only a small percentage of the nodes has to be checked . </S>",
    "<S> for @xmath16 we found that only @xmath17 to @xmath18 percent of the nodes were evaluated , depending on , e.g. , the number of patterns to store .    assuming that the evaluation of a node of the binary tree is approximately as time consuming as checking one possible configuration during complete enumeration , a comparison of these two methods has been done : as predicted by theory , we still have an exponential scaling of the algorithm . </S>",
    "<S> however , if we set the load @xmath19 to 0.5 and look for one solution with positive stability , the algorithm scales no longer with @xmath20 but with @xmath21 . in the ( worst ) case of determining all optimal solutions at @xmath22 , </S>",
    "<S> the scaling is @xmath23 . </S>",
    "<S> ( that would mean , the algorithm still optimizes a 30-neuron network in approximately the computing time needed for the complete enumeration of a 25-neuron network . )    </S>",
    "<S> we used the branch - and - bound algorithm to determine the capacity of the network storing random uncorrelated patterns . </S>",
    "<S> only one row of the coupling matrix was considered assuming the stability value to be self - averaging in the thermodynamic limit ( cf . </S>",
    "<S> @xcite ) .    </S>",
    "<S> the procedure resembles the one used in @xcite : for a given value of @xmath0 , the stability @xmath24 is determined for an increasing number of patterns until its value becomes negative , signifying that it is no longer possible to store all patterns . </S>",
    "<S> then the capacity @xmath25 is determined by a linear interpolation between the last positive @xmath26 and the negative @xmath27 . if the patterns are binary - valued , @xmath28 , @xmath24 takes on discrete values with a spacing of @xmath29 . for @xmath0 odd , </S>",
    "<S> this discreteness results in two values of @xmath25 corresponding to the first and last occurrence of @xmath30 . </S>",
    "<S> this procedure was carried out for networks with @xmath31 . to reduce finite size ( discretization and parity ) effects , we also used continuous distributed patterns ( cf . </S>",
    "<S> @xcite ) . </S>",
    "<S> we considered a normalized gaussian distribution as well as patterns with elements evenly distributed in the interval @xmath32 ( box constraint ) to examine the influence of the pattern distribution .     </S>",
    "<S> +     +    the figures [ fig : ar ] and [ fig : arg ] show the dependence of this capacity on the network size as well as on the nature of the patterns . </S>",
    "<S> the error bars correspond to twice the mean deviation of the average value ( statistical error ) . </S>",
    "<S> sample size varied between 10 000 for small systems and 100 for @xmath33 . </S>",
    "<S> the @xmath34-patterns exhibit a strong parity effect which should however vanish in the thermodynamic limit . in </S>",
    "<S> , the values for the gaussian patterns show a periodicity which is probably a result of the linear interpolation as the period of six corresponds to the passing the zero - line of a stability value @xmath24 . </S>",
    "<S> ( remember that the critical capacity is approximately 5/6 and @xmath35 is restricted to rationals @xmath36 ) . </S>",
    "<S> quadratic fits are given as a guideline to the eye ( cf . </S>",
    "<S> @xcite ) .    </S>",
    "<S> there is no scaling theory for this problem , however , our numerical data suggest that the extrapolation to @xmath37 could not be a linear one . a tentative quadratic extrapolation yields @xmath38 for gaussian distributed patterns , @xmath39 for the box constraint and @xmath40 for @xmath34-patterns and @xmath0 even . in the case of @xmath34-patterns and </S>",
    "<S> @xmath0 odd , a quadratic fit is clearly inadmissible .    </S>",
    "<S> a second approach followed the procedure by krauth and opper @xcite in determining @xmath41 for different values of @xmath35 and a subsequent extrapolation to @xmath42 . </S>",
    "<S> the capacity for gaussian distributed patterns is determined as @xmath43 .    </S>",
    "<S> we aimed to examine the possibilities and limits of combinatorial optimization when used for the training of autoassociative neural networks with binary couplings . </S>",
    "<S> the developed branch - and - bound algorithm allowed us to extend the exact investigation to systems with up to 40 neurons . </S>",
    "<S> we were not able to leave the region of strong finite size effects but could confirm theoretical @xcite and numerical @xcite studies with additional numerical evidence . </S>",
    "<S> the possibility to determine all solutions of the learning problem also opens the way for an analysis of the space of solutions similar to the one already done for the ground states of the ising spin glass model @xcite .    </S>",
    "<S> thanks go to w kinzel for drawing our attention to the problem , to t klotz , a hartwig and j weibarth for many helpful discussions and to m opper for additional information . </S>",
    "<S> gm appreciated a scholarship of the hans - bckler - stiftung . </S>"
  ]
}