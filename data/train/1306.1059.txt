{
  "article_text": [
    "classical statistical theory grants validity of statistical tests and confidence intervals assuming a wall of separation between the selection of a model and the analysis of the data being modeled . in practice , this separation rarely exists , and more often a model is `` found '' by a data - driven selection process . as a consequence inferential guarantees derived from classical theory are invalidated . among model selection methods that are problematic for classical inference , _ variable selection _ stands out because it is regularly taught , commonly practiced and highly researched as a technology .",
    "even though statisticians may have a general awareness that the data - driven selection of variables ( predictors , covariates ) must somehow affect subsequent classical inference from @xmath0- and @xmath1-based tests and confidence intervals , the practice is so pervasive that it appears in classical undergraduate textbooks on statistics such as @xcite .",
    "the reason for the invalidation of classical inference guarantees is that a data - driven variable selection process produces a model that is itself stochastic , and this stochastic aspect is not accounted for by classical theory .",
    "models become stochastic when the stochastic component of the data is involved in the selection process .",
    "( in regression with fixed predictors the stochastic component is the response . )",
    "models are stochastic in a well - defined way when they are the result of formal variable selection procedures such as stepwise or stagewise forward selection or backward elimination or all - subset searches driven by complexity penalties ( such as @xmath2 , aic , bic , risk - inflation , lasso@xmath3 ) or prediction criteria such as cross - validation , or more recent proposals such as lars and the dantzig selector ; for an overview see , for example , @xcite .",
    "models are also stochastic but in an ill - defined way when they are informally selected through visual inspection of residual plots or normal quantile plots or other regression diagnostics .",
    "finally , models become stochastic in an opaque way when their selection is affected by human intervention based on post - hoc considerations such as `` in retrospect only one of these two variables should be in the model '' or `` it turns out the predictive benefit of this variable is too weak to warrant the cost of collecting it . '' in practice , all three modes of variable selection may be exercised in the same data analysis : multiple runs of one or more formal search algorithms may be performed and compared , the parameters of the algorithms may be subjected to experimentation and the results may be critiqued with graphical diagnostics ; a  round of fine - tuning based on substantive deliberations may finalize the analysis .",
    "posed so starkly , the problems with statistical inference after variable selection may well seem insurmountable . at a minimum",
    ", one would expect technical solutions to be possible only when a formal selection algorithm is ( 1 ) well - specified ( 1a ) in advance and ( 1b ) covering all eventualities , ( 2 ) strictly adhered to in the course of data analysis and ( 3 ) not `` improved '' on by informal and post - hoc elements .",
    "it may , however , be unrealistic to expect this level of rigor in most data analysis contexts , with the exception of well - conducted clinical trials .",
    "the real challenge is therefore to devise statistical inference that is valid following _ any _ type of variable selection , be it formal , informal , post hoc or a combination thereof .",
    "meeting this challenge with a relatively simple proposal is the goal of this article .",
    "this proposal for valid _",
    "_ po__st-__s__election _ _ i__nference , or `` _ _ posi _ _ '' for short , consists of a large - scale family - wise error guarantee that can be shown to account for all types of variable selection , including those of the informal and post - hoc varieties . on the other hand , the proposal is no more conservative than necessary to account for selection , and in particular it can be shown to be less conservative than scheff s simultaneous inference .",
    "the framework for our proposal is in outline as follows ",
    "details to be elaborated in subsequent sections : we consider linear regression with predictor variables whose values are considered fixed , and with a response variable that has normal and homoscedastic errors .",
    "the framework does not require that any of the eligible linear models is correct , not even the full model , as long as a valid error estimate is available .",
    "we assume that the selected model is the result of some procedure that makes use of the response , but the procedure does not need to be fully specified .",
    "a  crucial aspect of the framework concerns the use and interpretation of the selected model : we assume that , after variable selection is completed , the selected predictor variables  and only they  will be relevant ; all others will be eliminated from further consideration .",
    "this assumption , seemingly innocuous and natural , has critical consequences : it implies that statistical inference will be sought for the coefficients of the selected predictors only and in the context of the selected model only .",
    "thus the appropriate targets of inference are the best linear coefficients within the selected model , where each coefficient is adjusted for the presence of all other included predictors but not those that were eliminated .",
    "therefore the coefficient of an included predictor generally requires inference that is specific to the model in which it appears . summarizing in a motto , a difference in adjustment implies a difference in parameters and hence in inference .",
    "the goal of the present proposal is therefore simultaneous inference for all coefficients within all submodels .",
    "such inference can be shown to be valid following any variable selection procedure , be it formal , informal , post hoc , fully or only partly specified .",
    "problems associated with post - selection inference were recognized long ago , for example , by @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite .",
    "more recently specific problems have been the subject of incisive analyses and criticisms by the `` vienna school '' of ptscher , leeb and schneider ; see , for example , leeb and ptscher ( @xcite ) , @xcite , @xcite , @xcite , ptscher and schneider ( @xcite ) , as well as @xcite and @xcite .",
    "important progress was made by @xcite and @xcite .",
    "this article proceeds as follows : in section  [ secreinterpretation ] we first develop the `` submodel view '' of the targets of inference after model selection and contrast it with the `` full model view '' ( section  [ secsubmodel ] ) ; we then introduce assumptions with a view toward valid inference in `` wrong models '' ( section  [ secassumptions ] ) .",
    "section  [ secestimation ] is about estimation and its targets from the submodel point of view .",
    "section  [ secvalidcis ] develops the methodology for posi confidence intervals ( cis ) and tests . after some structural results for the posi problem in section  [ secstruct ] , we show in section  [ secasymp ] that with increasing number of predictors @xmath4 the width of posi cis can range between the asymptotic rates @xmath5 and @xmath6 .",
    "we give examples for both rates and , inspired by problems in sphere packing and covering , we give upper bounds for the limiting constant in the @xmath6 case .",
    "we conclude with a discussion in section  [ secconc ] .",
    "some proofs are deferred to the , and some elaborations to the online appendix in the supplementary material [ @xcite ] .",
    "computations will be described in a separate article .",
    "simulation - based methods yield satisfactory accuracy specific to a design matrix up to @xmath7 , while nonasymptotic universal upper bounds can be computed for larger @xmath4 .",
    "it is a natural intuition that model selection distorts inference by distorting sampling distributions of parameter estimates : estimates in selected models should tend to generate more type i errors than conventional theory allows because the typical selection procedure favors models with strong , hence highly significant predictors .",
    "this intuition correctly points to a multiplicity problem that grows more severe as the number of predictors subject to selection increases .",
    "this is the problem we address in this article .",
    "model selection poses additional problems that are less obvious but no less fundamental : there exists an ambiguity as to the role and meaning of the parameters in submodels . in one view , the relevant parameters are always those of the full model , hence the selection of a submodel is interpreted as estimating the deselected parameters to be zero and estimating the selected parameters under a zero constraint on the deselected parameters . in another view , the submodel has its own parameters , and the deselected parameters are not zero but nonexistent .",
    "these distinctions are not academic as they imply fundamentally different ideas regarding the targets of inference , the measurement of statistical performance , and the problem of post - selection inference .",
    "the two views derive from different purposes of equations :    * underlying the full model view of parameters is the use of a full equation to describe a `` data generating '' mechanism for the response ; the equation hence has a causal interpretation . *",
    "underlying the submodel view of parameters is the use of any equation to merely describe association between predictor and response variables ; no data generating or causal claims are implied .    in this article",
    "we address the latter use of equations .",
    "issues relating to the former use are discussed in the online appendix of the supplementary material [ @xcite , section b.1 ] .      in what follows we elaborate three points that set the submodel interpretation of coefficients apart from the full model interpretation , with important consequences for the rest of this article :    the full model has no special status other than being the repository of available predictors .",
    "the coefficients of excluded predictors are not zero ; they are not defined and therefore do not exist .",
    "the meaning of a predictor s coefficient depends on which other predictors are included in the selected model .",
    "\\(1 ) the full model available to the statistician often can not be argued to have special status because of inability to identify and measure all relevant predictors .",
    "additionally , even when a large and potentially complete suite of predictors can be measured , there is generally a question of predictor redundancy that may make it desirable to omit some of the measurable predictors from the final model .",
    "it is a common experience in the social sciences that models proposed on theoretical grounds are found on empirical grounds to have their predictors entangled by collinearities that permit little meaningful statistical inference .",
    "this situation is not limited to the social sciences : in gene expression studies it may well occur that numerous sites have a tendency to be expressed concurrently , hence as predictors in disease studies they will be strongly confounded .",
    "the emphasis on full models may be particularly strong in econometrics where there is a `` notion that a longer regression  has a causal interpretation , while a shorter regression does not '' [ @xcite , page 59 ] . even in causal models ,",
    "however , there is a possibility that included adjuster variables will `` adjust away '' some of the causal variables of interest . generally , in any creative observational study involving novel predictors",
    ", it will be difficult a priori to exclude collinearities that might force a rethinking of the predictors . in conclusion , whenever predictor redundancy is a potential issue , it can not a priori be claimed that the full model provides the parameters of primary interest .",
    "\\(2 ) in the submodel interpretation of parameters , claiming that the coefficients of deselected predictors are zero does not properly describe the role of predictors .",
    "deselected predictors have no role in the submodel equation ; they become no different than predictors that had never been considered .",
    "the selected submodel becomes the vehicle of substantive research irrespective of what the full model was .",
    "as such the submodel stands on its own .",
    "this view is especially appropriate if the statistician s task is to determine which predictors are to be measured in the future .",
    "\\(3 ) the submodel interpretation of parameters is deeply seated in how we teach regression .",
    "we explain that the meaning of a regression coefficient depends on which of the other predictors are included in the model : `` the slope is the average difference in the response for a unit difference in the predictor , _ at fixed levels of all other predictors in the model_. '' this `` ceteris paribus '' clause is essential to the meaning of a slope . that there is a difference in meaning when there is a difference in covariates is most drastically evident when there is a case of simpson s paradox . for example , if purchase likelihood of a high - tech gadget is predicted from _ age _ , it might be found against expectations that younger people have lower purchase likelihood , whereas a regression on _ age _ and _ income _ might show that at fixed levels of income younger people have indeed higher purchase likelihood .",
    "this case of simpson s paradox would be enabled by the expected positive collinearity between _ age _ and _ income_. thus the marginal slope on _ age _ is distinct from the _ income_-adjusted slope on _ age _ as the two slopes answer different questions , apart from having opposite signs . in summary ,",
    "_ different models result in different parameters with different meanings_.    must we use the full model with both predictors ? not if _ income _",
    "data is difficult to obtain or if it provides little improvement in @xmath8 beyond _ age_. the model based on _ age _ alone can not be said to be a priori `` wrong . '' if , for example , the predictor and response variables have jointly multivariate normal distributions , then every linear submodel is `` correct . ''",
    "these considerations drive home , once again , that sometimes no model has special status .    in summary ,",
    "a range of applications call for a framework in which the full model is not the sole provider of parameters , where rather each submodel defines its own .",
    "the consequences of this view will be developed in section  [ secestimation ] .",
    "we state assumptions for estimation and for the construction of valid tests and cis when fitting arbitrary linear equations .",
    "the main goal is to prepare the ground for valid statistical inference after model selection_not _ assuming that selected models are correct .",
    "we consider a quantitative response vector @xmath9 , assumed random , and a full predictor matrix @xmath10 , @xmath11 , assumed fixed .",
    "we allow @xmath12 to be of nonfull rank , and @xmath13 and @xmath4 to be arbitrary .",
    "in particular , we allow @xmath14 . throughout the article we let @xmath15 due to frequent reference we call @xmath16 ( @xmath17 ) `` _ _ the classical case__. ''    it is common practice to assume the full model @xmath18 to be correct . in the present framework , however , first - order correctness , @xmath19 = { { \\mathbf{x}}}{{\\bolds{\\beta}}}$ ] , will not be assumed . by implication",
    ", first - order correctness of any submodel will not be assumed either .",
    "effectively , @xmath20 \\in{\\mathbb{r}}^n\\ ] ] is allowed to be unconstrained and , in particular , need not reside in the column space of @xmath12 .",
    "that is , the model given by @xmath12 is allowed to be `` first - order wrong , '' and hence we are , in a well - defined sense , serious about g.e.p .",
    "box s famous quote .",
    "what he calls `` wrong models , '' we prefer to call `` approximations '' : all predictor matrices @xmath12 provide approximations to @xmath21 , some better than others , but the degree of approximation plays no role in the clarification of statistical inference .",
    "the main reason for elaborating this point is as follows : after model selection , the case for `` correct models '' is clearly questionable , even for `` consistent model selection procedures '' [ @xcite , page 101 ] ; but if correctness of submodels is not assumed , it is only natural to abandon this assumption for the full model also , in line with the idea that the full model has no special status . as we proceed with estimation and inference guarantees in the absence of first - order correctness we will rely on assumptions as follows :    * for estimation ( section  [ secestimation ] ) , we will only need the existence of @xmath22 $ ] . * for testing and ci guarantees ( section  [ secvalidcis ] )",
    ", we will make conventional second - order and distributional assumptions , @xmath23    the assumptions ( [ eqassumption ] ) of homoscedasticity and normality are as questionable as first - order correctness , and we will report elsewhere on approaches that avoid them .",
    "for now we follow the vast model selection literature that relies on the technical advantages of assuming homoscedastic and normal errors .    accepting the assumption ( [ eqassumption ] )",
    ", we address the issue of estimating the error variance @xmath24 , because the valid tests and cis we construct require a valid estimate @xmath25 of @xmath24 that is independent of ls estimates . in the classical case ,",
    "the most common way to assert such an estimate is to assume that the full model is first - order correct , @xmath26 in addition to ( [ eqassumption ] ) , in which case the mean squared residual ( msr ) @xmath27 of the full model will do .",
    "however , other possibilities for producing a valid estimate @xmath25 exist , and they may allow relaxing the assumption of first - order correctness :    * exact replications of the response obtained under identical conditions might be available in sufficient numbers .",
    "an estimate @xmath25 can be obtained as the msr of the one - way anova of the groups of replicates . * in general , a larger linear model than the full model might be considered as correct ; hence @xmath25 could be the msr from this larger model .",
    "* a different possibility is to use another dataset , similar to the one currently being analyzed , to produce an independent estimate @xmath25 by whatever valid estimation method .",
    "* a special case of the preceding is a random split - sample approach whereby one part of the data is reserved for producing @xmath25 and the other part for estimating coefficients , selecting models and carrying out post - model selection inference . *",
    "a different type of estimate , @xmath25 , may be based on considerations borrowed from nonparametric function estimation [ @xcite ] .",
    "the purpose of pointing out these possibilities is to separate , at least in principle , the issue of first - order model incorrectness from the issue of error estimation under assumption ( [ eqassumption ] ) .",
    "this separation puts the case @xmath28 within our framework as the valid and independent estimation of @xmath24 is a problem faced by all `` @xmath28 '' approaches .",
    "following section  [ secsubmodel ] , the value and meaning of a regression coefficient depends on what the other predictors in the model are .",
    "an exception occurs , of course , when the predictors are perfectly orthogonal , as in some designed experiments or in function fitting with orthogonal basis functions . in this case",
    "a coefficient has the same value and meaning across all submodels .",
    "this article is hence a story of ( partial ) collinearity",
    ".      we will give meaning to ls estimators and their targets in the absence of any assumptions other than the existence of @xmath22 $ ] , which in turn is permitted to be entirely unconstrained in @xmath29 .",
    "besides resolving the issue of estimation in `` first - order wrong models , '' the major purpose here is to elaborate the idea that the slope of a predictor generates different parameters in different submodels . as each predictor appears in @xmath30 submodels ,",
    "the @xmath4 regression coefficients of the full model generally proliferate into a plethora of as many as @xmath31 distinct regression coefficients according to the submodels they appear in .",
    "to describe the situation we start with notation .",
    "to denote a submodel we use the ( nonempty ) index set @xmath32 of the predictors @xmath33 in the submodel ; the size of the submodel is @xmath34 and that of the full model is @xmath35 .",
    "let @xmath36 denote the @xmath37 submatrix of @xmath12 with columns indexed by  @xmath38 .",
    "we will only allow submodels @xmath38 for which @xmath39 is of full rank , @xmath40 we let @xmath41 be the unique least squares estimate in @xmath38 , @xmath42    now that @xmath41 is an estimate , what is it estimating ? following section  [ secsubmodel ] , we will not interpret @xmath41 as estimates of the full model coefficients and , more generally , of any model other than @xmath38 .",
    "thus it is natural to ask that @xmath41 define its own target through the requirement of unbiasedness , @xmath43 = \\bigl({{\\mathbf{x}}}_{\\mathrm{m}}^t { { \\mathbf{x}}}_{\\mathrm{m}}\\bigr)^{-1 } { { \\mathbf{x}}}_{\\mathrm{m}}^t { { \\mathbf{e}}}[{{\\mathbf{y } } } ] = { \\mathop{\\arg\\min}}_{{{\\bolds{\\beta } } } ' \\in{\\mathbb{r}}^m } \\bigl\\| { { \\bolds{\\mu}}}- { { \\mathbf{x}}}_{\\mathrm{m}}{{\\bolds{\\beta } } } ' \\bigr\\| ^2.\\ ] ] this definition requires no other assumption than the existence of @xmath22 $ ] .",
    "in particular there is no need to assume first - order correctness of @xmath38 or @xmath44 . nor does it matter to what degree @xmath38 provides a good approximation to @xmath21 in terms of approximation error @xmath45 .    in the classical case @xmath46",
    ", we can define the target of the full - model estimate @xmath47 as a special case of ( [ eqtarget ] ) with @xmath48 , @xmath49 = \\bigl({{\\mathbf{x}}}^t { { \\mathbf{x}}}\\bigr)^{-1 } { { \\mathbf{x}}}^t { { \\mathbf{e}}}[{{\\mathbf{y}}}].\\ ] ] in the general ( including the nonclassical ) case , let @xmath50 be any ( possibly nonunique ) minimizer of @xmath51 ; the link between @xmath50 and @xmath52 is as follows : @xmath53 thus the target @xmath52 is an estimable linear function of @xmath50 , without first - order correctness assumptions",
    ". equation ( [ eqcontrast ] ) follows from @xmath54 .",
    "_ notation _ : to distinguish the regression coefficients of the predictor @xmath55 relative to the submodel it appears in , we write @xmath56 $ ] for the components of @xmath57 $ ] with @xmath58 .",
    "an important convention is that indexes are always elements of the full model , @xmath59 , for what we call `` full model indexing . ''",
    "@xmath60the regression coefficient @xmath61 is conventionally interpreted as the `` average difference in the response for a unit difference in @xmath62 , ceteris paribus in the model @xmath38 . ''",
    "this interpretation no longer holds when the assumption of first - order correctness is given up . instead",
    ", the phrase `` average difference in the response '' should be replaced with the unwieldy phrase `` average difference in the response approximated in the submodel @xmath38 . ''",
    "the reason is that the target of the fit @xmath63 in the submodel @xmath38 is @xmath64 , hence in @xmath38 we estimate unbiasedly not the true @xmath21 but its ls approximation @xmath65 .",
    "a second interpretation of regression coefficients is in terms of adjusted predictors : for @xmath58 define the @xmath38-adjusted predictor @xmath66 as the residual vector of the regression of @xmath55 on all other predictors in @xmath38 .",
    "multiple regression coefficients , both estimates @xmath67 and parameters @xmath61 , can be expressed as simple regression coefficients with regard to the @xmath38-adjusted predictors , @xmath68 the left - hand formula lends itself to an interpretation of @xmath69 in terms of the well - known leverage plot which shows @xmath70 plotted against @xmath66 and the line with slope @xmath67 .",
    "this plot is valid without first - order correctness assumption .",
    "a third interpretation can be derived from the second : to unclutter notation let @xmath71 be any adjusted predictor @xmath72 , so that @xmath73 and @xmath74 are the corresponding @xmath67 and @xmath61 .",
    "introduce ( 1 ) case - wise slopes through the origin , both as estimates @xmath75 and as parameters @xmath76 , and ( 2 ) case - wise weights @xmath77 .",
    "equations ( [ eqadjusted ] ) are then equivalent to the following : @xmath78 hence regression coefficients are weighted averages of case - wise slopes , and this interpretation holds without first - order assumptions .",
    "we consider inference for @xmath41 and its target @xmath52 . following section  [ secassumptions ]",
    "we require a normal homoscedastic model for @xmath70 , but we leave its mean @xmath22 $ ] entirely unspecified : @xmath79 .",
    "we then have equivalently @xmath80 again following section  [ secassumptions ] we assume the availability of a valid estimate @xmath25 of @xmath24 that is independent of all estimates @xmath67 , and we further assume @xmath81 for @xmath82 degrees of freedom . if the full model is assumed to be correct , @xmath83 and @xmath84 , then @xmath85 . in the limit @xmath86",
    "we obtain @xmath87 , the case of known  @xmath88 , which will be used starting with section  [ secasymp ] .",
    "let @xmath89 denote a @xmath1-ratio for @xmath61 that uses @xmath90 irrespective of @xmath38 , @xmath91 where @xmath92 refers to the diagonal element corresponding to @xmath55 .",
    "the quantity @xmath93 has a central @xmath1-distribution with @xmath82 degrees of freedom .",
    "essential is that the standard error estimate in the denominator of ( [ eqt ] ) does _ not _ involve the msr @xmath94 from the submodel @xmath38 , for two reasons :    * we do not assume that the submodel @xmath38 is first - order correct ; hence @xmath95 would , in general , have a distribution that is a multiple of a noncentral @xmath96 distribution with unknown noncentrality parameter . *",
    "more disconcertingly , @xmath95 would be the result of selection , @xmath97 ; see section  [ secmodel - selection ] .",
    "not much of real use is known about its distribution ; see , for example , @xcite and @xcite .",
    "these problems are avoided by using one valid estimate @xmath25 that is independent of all submodels .    with this choice of @xmath90 , confidence intervals for @xmath98",
    "take the form @xmath99_{jj}^{{1/2 } } { { \\hat{\\sigma}}}\\bigr ] \\nonumber\\\\[-8pt]\\\\[-8pt ] & = & \\bigl [ { { \\hat{\\beta}}}_{j \\cdot{\\mathrm{m } } } \\pm k { { \\hat{\\sigma}}}/\\|{{\\mathbf{x}}}_{j \\cdot{\\mathrm{m}}}\\| \\bigr ] . \\nonumber\\end{aligned}\\ ] ] if @xmath100 is the @xmath101 quantile of a @xmath1-distribution with @xmath82 degrees of freedom , then the interval is marginally valid with a @xmath102 coverage guarantee @xmath103 \\stackrel{(\\ge ) } { = } 1-\\alpha.\\ ] ] this holds if the submodel @xmath38 is _ not _ the result of variable selection .      in practice",
    ", the model @xmath38 tends to be the result of some form of model selection that makes use of the stochastic component of the data , which is the response vector  @xmath70 ( @xmath12 being fixed , section  [ secassumptions ] ) .",
    "this model should therefore be expressed as @xmath104 . in general",
    "we allow a variable selection _ procedure _ to be any ( measurable ) map @xmath105 where @xmath106 is the set of all full - rank submodels @xmath107 thus the procedure @xmath108 is a discrete map that divides @xmath29 into as many as @xmath109 different regions with shared outcome of model selection .",
    "data dependence of the selected model @xmath108 has strong consequences :    * most fundamentally , the selected model @xmath104 is now random .",
    "whether the model has been selected by an algorithm or by human choice , if the response @xmath70 has been involved in the selection , the resulting model is a random object because it could have been different for a different realization of the random vector @xmath70 . *",
    "associated with the random model @xmath110 is the parameter vector of coefficients @xmath111 , which is now randomly chosen also : * * it has a random dimension @xmath112 : @xmath113 . * * for any fixed @xmath114 , it may or may not be the case that @xmath115 . *",
    "* conditional on @xmath115 , the parameter @xmath116 changes randomly as the adjuster covariates in @xmath110 vary randomly .",
    "thus the set of parameters for which inference is sought is random also .      with randomness of the selected model and its parameters in mind , what is a desirable form of post - selection coverage guarantee for confidence intervals ?",
    "a natural requirement would be a @xmath102 confidence guarantee for the coefficients of the predictors that are selected into the model , @xmath117 \\ge1-\\alpha.\\ ] ] several points should be noted :    * the guarantee is family - wise for all selected predictors @xmath118 , though the sense of `` family - wise '' is unusual because @xmath119 is random . * the guarantee has nothing to say about predictors @xmath120 that have been deselected , regardless of the substantive interest they might have .",
    "predictors of overarching interest should be protected from variable selection , and for these one can use a modification of the posi approach which we call `` posi1 ; '' see section  [ secposi1 ] .",
    "* because predictor selection is random , @xmath104 , two realized samples @xmath121 from @xmath70 may result in different sets of selected predictors , @xmath122 .",
    "it would be a fundamental misunderstanding to wonder whether the guarantee holds for both realizations .",
    "instead , the guarantee ( [ eqposiguarantee ] ) is about the _ procedure _",
    "@xmath123 for the long run of independent realizations of @xmath70 ( by the lln ) , and not for any particular realizations @xmath124 .",
    "a standard formulation used to navigate these complexities after a realization @xmath125 of @xmath70 has been analyzed is the following : `` for @xmath118 we have @xmath102 _ confidence _ that the interval @xmath126 contains @xmath127 . ''",
    "* marginal guarantees for individual predictors require some care because @xmath128 does not exist for @xmath120 .",
    "this makes @xmath129 an incoherent statement that does not define an event .",
    "guarantees are possible if the condition @xmath118 is added with a conjunction or is being conditioned on : the marginal and conditional probabilities @xmath130 \\quad\\mbox{and}\\quad { { \\mathbf{p}}}\\bigl[\\beta_{j \\cdot{{\\hat{\\mathrm{m } } } } } \\in{\\operatorname{ci}}_{j \\cdot{{\\hat{\\mathrm{m}}}}}(k_{j \\cdot})| j \\in{{\\hat{\\mathrm{m}}}}\\bigr],\\ ] ] respectively , are both well - defined and can be the subject of coverage guarantees ; see the online appendix of the supplementary material [ @xcite , section  b.4 ] .",
    "finally , we note that the smallest constant @xmath131 that satisfies the guarantee ( [ eqposiguarantee ] ) is specific to the procedure @xmath108 .",
    "thus different variable selection procedures would require different constants .",
    "finding procedure - specific constants is a challenge that will be intentionally bypassed by the present proposals .",
    "the `` posi '' procedure proposed here produces a constant @xmath131 that provides universally valid post - selection inference _ for all model selection procedures _",
    "@xmath108 , @xmath132 \\ge1-\\alpha\\qquad\\forall{{\\hat{\\mathrm{m}}}}.\\ ] ] universal validity irrespective of the model selection procedure @xmath108 is a strong property that raises questions of whether the approach is too conservative .",
    "there are , however , some arguments in its favor :    \\(1 ) universal validity may be desirable or even essential for applications in which the model selection procedure is not specified in advance or for which the analysis involves some ad hoc elements that can not be accurately pre - specified .",
    "even so , we should think of the actually chosen model as part of a `` procedure '' @xmath133 , and though the ad hoc steps are not specified for @xmath70 other than the observed one , this is not a problem because our protection is irrespective of what a specification might have been .",
    "this view also allows data analysts to change their minds , to improvise and informally decide in favor of a model other than that produced by a formal selection procedure , or to experiment with multiple selection procedures .",
    "\\(2 ) there exists a model selection procedure that requires the full strength of universally valid posi , and this procedure may not be entirely unrealistic as an approximation to some types of data analytic activities : `` significance hunting , '' that is , selecting that model which contains the statistically most significant coefficient ; see section  [ secspar ] .",
    "\\(3 ) there is a general question about the wisdom of proposing ever tighter confidence and retention intervals for practical use when in fact these intervals are valid only under tightly controlled conditions",
    ". it might be realistic to suppose that much applied work involves more data peeking than is reported in published articles .",
    "with inference that is universally valid after any model selection procedure , we have a way to establish which rejections are safe , irrespective of unreported data peeking as part of selecting a model .",
    "\\(4 ) related to the previous point is the fact that today there is a realization that a considerable fraction of published empirical work is unreproducible or reports exaggerated effects ; well known in this regard is @xcite .",
    "a factor contributing to this problem might well be liberal handling of variable selection and absent accounting for it in subsequent inference .",
    "the concerns over posi s conservative nature can be alleviated somewhat by introducing a degree of flexibility to the posi problem with regard to the universe of models being searched .",
    "such flexibility is additionally called for from a practical point of view because it is not true that all submodels in @xmath106 ( [ eqmall ] ) are always being searched .",
    "rather , the search is often limited in a way that can be specified a priori , without involvement of @xmath70 . for example",
    ", a predictor of interest may be forced into the submodels of interest , or there may be a restriction on the size of the submodels . indeed , if @xmath4 is large , a  restriction to a manageable set of submodels is a computational necessity . in much of what follows we can allow the universe @xmath134 of allowable submodels to be an ( almost ) arbitrary but pre - specified nonempty subset of @xmath106 ; w.l.o.g .",
    "we can assume @xmath135 . because we allow only nonsingular submodels [ see ( [ eqmall ] ) ] we have @xmath136 @xmath137 , where as always @xmath138 .",
    "selection procedures are now maps @xmath139 the following are examples of model universes with practical relevance ; see also @xcite , section 1.1 , example 1 .",
    "submodels that contain the first @xmath140 predictors ( @xmath141 ) : @xmath142 .",
    "classical : @xmath143 .",
    "example : forcing an intercept into all models .",
    "submodels of size @xmath144 or less ( `` sparsity option '' ) : @xmath145 .",
    "classical : @xmath146",
    ".    submodels with fewer than @xmath144 predictors dropped from the full model : @xmath147 .",
    "classical : @xmath148 .",
    "nested models : @xmath149 .",
    "example : selecting the degree up to @xmath151 in a polynomial regression .    models dictated by an anova hierarchy of main effects and interactions in a factorial design .",
    "this list is just an indication of possibilities . in general , the smaller the set @xmath152 is , the less conservative the posi approach is , and the more computationally manageable the problem becomes . with sufficiently strong restrictions , in particular using the sparsity option ( 2 ) and assuming the availability of an independent valid estimate @xmath90 , it is possible to apply posi in certain nonclassical @xmath153 situations .    further reduction of the posi problem is possible by pre - screening adjusted predictors _ without the response _",
    "@xmath70 . in a fixed - design regression , any variable selection procedure that does _",
    "not _ involve @xmath70 does _ not _ invalidate statistical inference .",
    "for example , one may decide not to seek inference for predictors in submodels that impart a `` variance inflation factor '' ( @xmath154 ) above a user - chosen threshold : @xmath155 if @xmath55 is centered , hence does not make use of @xmath70 , and elimination according to @xmath156 does not invalidate inference .",
    "we show that universally valid post - selection inference ( [ eqbciuniv ] ) follows from simultaneous inference in the form of family - wise error control for all parameters in all submodels .",
    "the argument depends on the following lemma that may fall into the category of the `` trivial but not immediately obvious . ''",
    "[ lemtrivial ] for any model selection procedure @xmath157 , the following inequality holds for all @xmath9 : @xmath158    this is a special case of the triviality @xmath159 , where @xmath160 .    the right - hand max-@xmath161 bound of the lemma is sharp in the sense that there exists a variable selection procedure @xmath108 that attains the bound ; see section  [ secspar ] .",
    "next we introduce the @xmath162 quantile of the right - hand max-@xmath161 statistic of the lemma : let @xmath131 be the minimal value that satisfies latexmath:[\\[\\label{eqkdef } { { \\mathbf{p}}}\\bigl[\\max_{{\\mathrm{m}}\\in{{\\mathcal{m } } } } \\max_{j \\in{\\mathrm{m } } }    this value will be called `` the posi constant . ''",
    "it does not depend on any model selection procedures , but it does depend on the design matrix @xmath12 , the universe @xmath134 of models subject to selection , the desired coverage @xmath162 , and the degrees of freedom @xmath82 in @xmath90 , hence @xmath164 .",
    "[ thmsim ] for all model selection procedures @xmath165 we have @xmath166 \\ge1-\\alpha,\\ ] ] where @xmath164 is the posi constant .",
    "this follows immediately from lemma  [ lemtrivial ] .",
    "although mathematically trivial we give the above the status of a theorem as it is the central statement of the reduction of universal post - selection inference to simultaneous inference .",
    "the following is just a repackaging of theorem  [ thmsim ] :    [ corcov ] `` simultaneous post - selection confidence guarantees '' hold for any model selection procedure @xmath167 , @xmath168 \\ge1-\\alpha,\\ ] ] where @xmath164 is the posi constant .",
    "simultaneous inference provides strong family - wise error control , which in turn translates to strong error control for tests following model selection .",
    "[ corstrong ] `` strong post - selection error control '' holds for any model selection procedure @xmath169 , @xmath170 \\le\\alpha,\\ ] ] where @xmath164 is the posi constant and @xmath171 is the @xmath1-statistic for the null hypothesis @xmath172 .",
    "the proof is standard ; see the online appendix of the supplementary material [ @xcite , section b.3 ] .",
    "the corollary states that , with probability @xmath162 , in a selected model _ all _ posi - significant rejections have detected true alternatives .",
    "several portions of the following treatment are devoted to a better understanding of the structure and value of the posi constant @xmath173 . except for very special choices it does not seem possible to provide closed form expressions for its value .",
    "however , the structural geometry and other properties to be described later do enable a reasonably efficient computational algorithm .",
    "r - code for computing the posi constant for small to moderate values of @xmath4 is available on the authors web pages .",
    "this code is accompanied by a manuscript that will be published elsewhere describing the computational algorithm and generalizations . for the basic",
    "setting involving @xmath106 the algorithm will conveniently provide values of @xmath174 for matrices @xmath12 of rank @xmath175 , or slightly larger depending on available computing speed and memory .",
    "it can also be adapted to compute @xmath131 for some other families contained within @xmath106 , such as some discussed in section  [ secrestrictions ] .",
    "realizing the idea that the ls estimators in different submodels are generally unbiased estimates of different parameters , we generated a simultaneous inference problem involving up to @xmath176 linear contrasts @xmath61 .",
    "in view of the enormous number of linear combinations for which simultaneous inference is sought , one should wonder whether the problem is not best solved by scheff s method [ @xcite ] which provides simultaneous inference for _ all _ linear combinations . to accommodate rank - deficient @xmath12 , we cast scheff s result in terms of @xmath1-statistics for arbitrary nonzero @xmath177 : @xmath178 the @xmath1-statistics in ( [ eqt ] ) are obtained for @xmath179 .",
    "scheff s guarantee is @xmath180 = 1-\\alpha,\\ ] ] where the scheff constant is @xmath181 it provides an upper bound for _ all _ posi constants :    [ propscheffe ] @xmath182 .",
    "thus for @xmath118 a parameter estimate @xmath183 whose @xmath1-ratio exceeds @xmath184 in magnitude is universally safe from having the rejection of `` @xmath185 '' invalidated by variable selection .",
    "the universality of the scheff constant is a tip - off that it may be too loose for some predictor matrices @xmath12 , and obtaining the sharper constant @xmath186 may be worthwhile .",
    "an indication is given by the following comparison as @xmath187 :    * for the scheff constant it holds @xmath188 . * for orthogonal designs it holds @xmath189 .",
    "( for orthogonal designs see section  [ secorthopt ] . )",
    "thus the posi constant @xmath190 is much smaller than @xmath184 .",
    "the large gap between the two suggests that the scheff constant may be too conservative at least in some cases",
    ". we will study certain nonorthogonal designs for which the posi constant is @xmath191 in section  [ secexch ] . on the other hand",
    ", the posi constant can approach the order @xmath192 of the scheff constant @xmath184 as well , and we will study an example in section  [ secrootp ] .",
    "even though in this article we will give asymptotic results for @xmath193 and @xmath194 only , we mention another kind of asymptotics whereby @xmath82 is held constant while @xmath193 : in this case @xmath184 is in the order of the product of @xmath195 and the @xmath102 quantile of the inverse - root - chi - square distribution with @xmath82 degrees of freedom . in a similar way ,",
    "the constant @xmath190 for orthogonal designs is in the order of the product of @xmath196 and the @xmath102 quantile of the inverse - chi - square distribution with @xmath82 degrees of freedom .",
    "there exists a model selection procedure that requires the full protection of the simultaneous inference procedure ( [ eqkdef ] ) .",
    "it is the `` significance hunting '' procedure that selects the model containing the most significant `` effect '' : @xmath197 we name this procedure `` spar '' for `` _ _ single predictor adjusted regression__. '' it achieves equality with the `` significant triviality bound '' in lemma  [ lemtrivial ] and is therefore the worst case procedure for the posi problem . in the submodel @xmath198",
    ", the less significant predictors matter only in so far as they boost the significance of the winning predictor by adjusting it accordingly .",
    "this procedure ignores the quality of the fit to @xmath70 provided by the model .",
    "while our present purpose is to point out the existence of a selection procedure that requires full posi protection , spar could be of practical interest when the analysis is centered on strength of `` effects , '' not quality of model fit .",
    "sometimes a regression analysis is centered on a predictor of interest , @xmath55 , and on inference for its coefficient @xmath199 .",
    "the other predictors in @xmath38 act as controls , so their purpose is to adjust the primary predictor for confounding effects and possibly to boost the primary predictor s own `` effect . ''",
    "this situation is characterized by two features :    * variable selection is limited to models that contain the primary predictor .",
    "we therefore define for any model universe @xmath134 a sub - universe @xmath200 of models that contain the primary predictor @xmath55 , @xmath201 so that for @xmath202 we have @xmath58 iff @xmath203 .",
    "* inference is sought for the primary predictor @xmath55 only , hence the relevant test statistic is now @xmath204 and no longer @xmath205 .",
    "the former statistic is coherent because it is assumed that @xmath206 .",
    "we call this the `` posi1 '' situation in contrast to the unconstrained posi situation . similar to posi , posi1 starts with a `` significant triviality bound '' :    [ lemtrivial1 ] for a fixed predictor @xmath55 and model selection procedure @xmath207 , it holds that @xmath208    for a `` proof , '' the only thing to note is @xmath209 by the assumption @xmath210 . we next define the `` posi1 '' constant for the predictor @xmath55 as the @xmath102 quantile of the max-@xmath161 statistic on the right - hand side of the lemma : let @xmath211 be the minimal value that satisfies @xmath212 \\ge1 - \\alpha.\\ ] ] importantly , this constant is dominated by the general posi constant , @xmath213 for the obvious reason that the present max-@xmath161 is smaller than the general posi max-@xmath161 due to @xmath214 and the restriction of inference to @xmath55 .",
    "the constant @xmath215 provides the following `` posi1 '' guarantee shown as the analog of theorem  [ thmsim ] and corollary  [ corcov ] folded into one :    [ thmsim1r ] let @xmath216 be a selection procedure that always includes the predictor @xmath55 in the model",
    ". then we have @xmath217 \\ge1 - \\alpha,\\ ] ] and accordingly we have the following post - selection confidence guarantee : @xmath218 \\ge1-\\alpha.\\ ] ]    inequality ( [ eqsimjr ] ) is immediate from lemma  [ lemtrivial1 ] .",
    "the `` triviality bound '' of the lemma is attained by the following variable selection procedure which we name `` spar1 '' : @xmath219 it is a potentially realistic description of some data analyses when a predictor of interest is determined a priori , and the goal is to optimize _ this _ predictor s `` effect . ''",
    "this procedure requires the full protection of the posi1 constant  @xmath215 .",
    "in addition to its methodological interest , the posi1 situation addressed by theorem  [ thmsim1r ] is of theoretical interest : even though the posi1 constant  @xmath215 is dominated by the unrestricted posi constant @xmath131 , we will construct in section  [ secrootp ] an example of predictor matrices for which the posi1 constant increases at the scheff rate and is asymptotically more than 63% of the scheff constant @xmath184 .",
    "it follows that near - scheff protection can be needed even for spar1 variable selection .",
    "we can reduce the dimensionality of the posi problem from @xmath220 to @xmath221 , where @xmath222 , by introducing scheff s canonical coordinates .",
    "this reduction is important both geometrically and computationally because the posi coverage problem really takes place in the column space of @xmath12 .",
    "let @xmath223 be any orthonormal basis of the column space of @xmath12 .",
    "note that @xmath224 is the orthogonal projection of @xmath70 onto the column space of @xmath12 even if @xmath12 is not of full rank .",
    "we call @xmath225 and @xmath226 canonical coordinates of @xmath12 and @xmath227",
    ".    we extend the notation @xmath39 for extraction of subsets of columns to canonical coordinates @xmath228 .",
    "accordingly slopes obtained from canonical coordinates will be denoted by @xmath229 to distinguish them from the slopes obtained from the original data @xmath230 , if only to state in the following proposition that they are identical .",
    "[ propcancoord ] properties of canonical coordinates :    @xmath231 .",
    "@xmath232 and @xmath233 .",
    "@xmath234 for all submodels @xmath235 .",
    "@xmath236 , where @xmath237 .",
    "@xmath238 , where @xmath239 and @xmath240 is the residual vector of the regression of @xmath241 onto the other columns of @xmath228 .",
    "@xmath242 .    in the classical case",
    "@xmath243 , @xmath244 can be chosen to be an upper triangular or a symmetric matrix .",
    "the proofs of ( 1)(6 ) are elementary . as for ( 7 ) , an upper triangular @xmath244 can be obtained from a qr - decomposition based on a gram  schmidt procedure , @xmath245 , @xmath246 . a symmetric @xmath244",
    "is obtained from a singular value decomposition , @xmath247 , @xmath248 , @xmath249 .",
    "canonical coordinates allow us to analyze the posi coverage problem in  @xmath250 . in what follows",
    "we will freely assume that all objects are rendered in canonical coordinates and write @xmath12 and @xmath70 for @xmath244 and @xmath251 , implying that the predictor matrix is of size @xmath252 and the response is of size @xmath253 .",
    "we simplify the posi coverage problem ( [ eqkdef ] ) as follows : due to pivotality of @xmath1-statistics , the problem is invariant under translation of @xmath21 and rescaling of @xmath88 ; see equation ( [ eqt ] ) . hence it suffices to solve coverage problems for @xmath254 and @xmath255 .",
    "in canonical coordinates this implies @xmath256={{\\mathbf{0}}}_d$ ] , hence @xmath257 .",
    "for this reason we use the more familiar notation @xmath258 instead of @xmath251 .",
    "the random vector @xmath259 has a @xmath260-dimensional @xmath1-distribution with @xmath82 degrees of freedom , and any linear combination @xmath261 with a unit vector @xmath262 has a one - dimensional @xmath1-distribution .",
    "letting @xmath72 be the adjusted predictors in canonical coordinates , estimates ( [ eqadjusted ] ) and their @xmath1-statistics ( [ eqt ] ) simplify to @xmath263 which are linear functions of @xmath258 and @xmath259 , respectively , with `` posi coefficient vectors '' @xmath264 and @xmath265 that equal @xmath66 up to scale @xmath266 as we now operate in canonical coordinates , we have @xmath267 and @xmath268 , the unit sphere in @xmath250 . to complete the structural description of the posi problem",
    "we let @xmath269 if @xmath270 , we omit the second argument and write @xmath271 .    [ propmaxlin ] the posi problem ( [ eqkdef ] ) is equivalent to a @xmath260-dimensional coverage problem for linear functions of the multivariate @xmath1-vector @xmath259 , @xmath272 = { { \\mathbf{p}}}\\bigl[\\max _ { { { \\bar{\\mathbf{l}}}}\\in{\\mathcal{l}}({{\\mathbf{x}}},{{\\mathcal{m } } } ) } \\bigl|{{\\bar{\\mathbf{l}}}}^t{{\\mathbf{z}}}/{{\\hat{\\sigma}}}\\bigr| \\leq k \\bigr ] \\stackrel { ( \\ge ) } { = } 1 - \\alpha.\\ ] ]      the set @xmath273 of unit vectors @xmath274 has interesting geometric structure which is the subject of this and the next subsection .",
    "the following proposition ( proof in appendix  [ appproporth ] ) elaborates the fact that @xmath275 is essentially the predictor vector @xmath55 orthogonalized with regard to the other predictors in the model @xmath38 .",
    "vectors will always be assumed in canonical coordinates and hence @xmath260-dimensional .",
    "[ proporth ] orthogonalities in @xmath273 : the following statements hold assuming that the models referred to are in @xmath134 ( hence are of full rank ) .",
    "adjustment properties : @xmath276    the following vectors form an orthonormal `` gram  schmidt '' series : @xmath277 other series are obtained using @xmath278 in place of @xmath279 .",
    "vectors @xmath274 and @xmath280 are orthogonal if @xmath281 , @xmath58 and .",
    "classical case @xmath243 and @xmath270 : each vector @xmath274 is orthogonal to @xmath282 vectors @xmath280 ( not all of which may be distinct ) .    the cardinality of orthogonalities in the classical case and @xmath270 is as follows : if the predictor vectors @xmath55 have no orthogonal pairs among them , then @xmath283 .",
    "if there exist orthogonal pairs , then @xmath284 is less .",
    "for example , if there exists exactly one orthogonal pair , then @xmath285 .",
    "when @xmath12 is a fully orthogonal design , then @xmath286 .",
    "coverage problems can be framed geometrically in terms of probability coverage of polytopes in @xmath250 . for the posi problem the polytope with half - width @xmath131",
    "is defined by @xmath287 henceforth called the `` posi polytope . '' the posi coverage problem ( [ eqtcoverage ] ) is equivalent to calibrating @xmath131 such that @xmath288 = 1-\\alpha.\\ ] ] the simplest case of a posi polytope , for @xmath289 , is illustrated in figure 1 in the online appendix of the supplementary material [ @xcite , section  b.7 ] .",
    "more general polytopes are obtained for arbitrary sets @xmath290 of unit vectors , that is , subsets @xmath291 of the unit sphere in @xmath250 .",
    "for the special case @xmath292 the `` polytope '' is the `` scheff ball '' with coverage @xmath293 as @xmath194 : @xmath294 = f_{{{\\mathrm{f}}}_{d , r } } \\bigl(k^2/d\\bigr).\\ ] ]    many properties of the polytopes @xmath295 are not specific to posi because they hold for polytopes ( [ eqposipolytope ] ) generated by simultaneous inference problems for linear functions with arbitrary sets @xmath290 of unit vectors .",
    "these polytopes :    form scale families of geometrically similar bodies : @xmath296 ;    are point symmetric about the origin : @xmath297 ;    contain the scheff ball : @xmath298 ;    are intersections of `` slabs '' of width @xmath299 : @xmath300    have @xmath301 faces ( assuming @xmath302 ) , and each face is tangent to the scheff ball @xmath303 with tangency points @xmath304 @xmath305 .",
    "specific to posi are the orthogonalities described in proposition  [ proporth ] .      in orthogonal designs , adjustment has no effect : @xmath306 for all @xmath206 , hence @xmath307 and @xmath308 .",
    "the polytope @xmath295 is therefore a hypercube .",
    "this observation implies an optimality property of orthogonal designs if the submodel universes @xmath134 are sufficiently rich to force @xmath273 to contain an orthonormal basis of @xmath250 : the polytope generated by an orthonormal basis is a hypercube ; hence the polytope @xmath309 is contained in this hypercube ; thus @xmath309 has maximal extent if and only if it is equal to this hypercube , which is the case if and only if @xmath273 is this orthonormal basis and nothing more ; that is , @xmath12 is an orthogonal design .",
    "a  simple sufficient condition for @xmath134 to grant the existence of an orthonormal basis in @xmath273 is the existence of a maximal nested sequence of submodels such as @xmath310 , @xmath311 in @xmath134 .",
    "it follows according to item ( 2 ) in proposition  [ proporth ] that there exists an orthonormal gram ",
    "schmidt basis in @xmath273 .",
    "we summarize :    [ proporthopt ] among predictor matrices with @xmath312 and model universes @xmath134 that contain at least one maximal nested sequence of submodels , orthogonal designs with @xmath313 columns yield :    * the maximal coverage probability @xmath314 $ ] for fixed @xmath131 and * the minimal posi constant @xmath131 satisfying @xmath315 = 1-\\alpha$ ] for fixed  @xmath316 , @xmath317 .",
    "the proposition holds not only for multivariate @xmath1-vectors and their gaussian limits but for arbitrary spherically symmetric distributions .",
    "optimality of orthogonal designs translates to optimal asymptotic behavior of their constant @xmath318 for large @xmath260 :    [ proporthasy ] consider the gaussian limit @xmath194 . for @xmath12 and @xmath134 as in proposition  [ proporthopt ] , the asymptotic lower bound for the constant @xmath131 as @xmath319",
    "is attained for orthogonal designs for which the asymptotic rate is @xmath320    by proposition  [ proporthopt ] the posi problem is bounded below by orthogonal designs , and by proposition  [ propscheffe ] it is loosely bounded above by the scheff ball ( both for all @xmath316 , @xmath260 , and @xmath82 ) .",
    "the question of how close to the scheff bound posi problems can get for @xmath194 will occupy us in section  [ secrootp ] . unlike the infimum problem , the supremum problem does not appear to have a unique optimizing design @xmath12 uniformly in @xmath316 , @xmath260 and @xmath82 .      in the classical case @xmath243 and @xmath270 there",
    "exists a duality for posi vectors @xmath271 which we will use in section  [ secexch ] below but which is also of independent interest .",
    "some preliminaries : letting @xmath321 be the full model , we observe that the ( unnormalized ) posi vectors @xmath322 form the rows of the matrix @xmath323 ; see ( [ eqadjusted ] ) and ( [ eqcontrast ] ) . in a change of perspective",
    ", we interpret the transpose matrix @xmath324 as a predictor matrix , to be called the `` dual design '' of @xmath12 .",
    "it is also of size @xmath325 in canonical coordinates , and its columns are the posi vectors @xmath326 .",
    "it turns out that @xmath327 and @xmath12 pose identical posi problems if @xmath270 :    [ thmdual ] @xmath328 .",
    "recall that @xmath271 and @xmath329 contain the normalized versions of the respective adjusted predictor vectors .",
    "the theorem follows from the following lemma which establishes the identities of vectors between @xmath329 and @xmath271 .",
    "we extend obvious notation from @xmath12 to @xmath327 as follows : @xmath330 submodels for @xmath327 will be denoted @xmath331 , but they , too , will be given as subsets of @xmath332 which , however , refer to columns of @xmath327 .",
    "finally , the normalized version of @xmath333 will be written as @xmath334 .",
    "[ lemdual ] for two submodels @xmath38 and @xmath331 that satisfy @xmath335 and @xmath336 , we have @xmath337    the proof is in appendix  [ applemdual ] .",
    "the assertion about norms is really only needed to exclude collapse of @xmath338 to zero .",
    "a special case arises when the predictor matrix ( in canonical coordinates ) is chosen to be symmetric according to proposition  [ propcancoord](7 ) : if @xmath339 , then @xmath340 , and hence :    [ symdualk ] if @xmath12 is symmetric in canonical coordinates , then @xmath341",
    "we consider examples in the classical case @xmath243 and @xmath270 .",
    "also , we work with the gaussian limit @xmath187 , that is , @xmath24 known , and w.l.o.g . @xmath342 .      in exchangeable designs",
    "all pairs of predictor vectors enclose the same angle . in",
    "canonical coordinates a convenient parametrization of a family of symmetric exchangeable designs is @xmath343 where @xmath344 , and @xmath345 is a matrix with all entries equal to @xmath346 .",
    "the range restriction on @xmath347 assures that @xmath348 is positive definite .",
    "we will write @xmath349 depending on which parameter matters in a given context .",
    "we will make use of the fact that @xmath350 is also an exchangeable design .",
    "the function @xmath351 maps the interval @xmath352 onto itself , and it holds @xmath353 , @xmath354 as @xmath355 , and vice versa .",
    "exchangeable designs include orthogonal designs for @xmath356 , and they extend to two types of strict collinearities : for @xmath357 the predictor vectors collapse to a single dimension @xmath358 , and for @xmath359 they collapse to a subspace @xmath360 of dimension @xmath361 , where @xmath362 .",
    "as collinearity drives the fracturing of the regression coefficients into model - dependent quantities @xmath61 , it is of interest to analyze @xmath363 as @xmath364 moves from orthogonality at @xmath356 toward either of the two types of collinearity . here is what we find : unguided intuition might suggest that the collapse to rank 1 calls for larger @xmath186 than the collapse to rank @xmath151 .",
    "this turns out to be entirely wrong : collapse to rank 1 or rank @xmath151 has identical effects on @xmath186 .",
    "the reason is duality ( section  [ secdual ] ) : for exchangeable designs , @xmath364 collapses to rank 1 if and only if @xmath365 collapses to rank @xmath151 , and vice versa , while @xmath366 according to corollary  [ symdualk ] .",
    "we next address the asymptotic behavior of @xmath367 for increasing  @xmath4 . as noted in section  [ secscheffe ] ,",
    "there is a wide gap between orthogonal designs with @xmath368 and the full scheff protection with @xmath369 .",
    "the following theorem shows how exchangeable designs fall into this gap :    [ thmex ] posi constants of exchangeable design matrices @xmath370 [ defined in ( [ eqex ] ) above ] have the following limiting behavior : @xmath371    the proof can be found in appendix  [ appthmex ] .",
    "the theorem shows that for exchangeable designs the posi constant remains much closer to the orthogonal case than the scheff case .",
    "thus , for this family of designs it is possible to improve on the scheff constant by a considerable margin .",
    "the following detail of geometry for exchangeable designs has a bearing on their posi constants : the angle between pairs of predictor vectors as a function of @xmath347 is @xmath372 .",
    "as the vectors fall into the rank-@xmath373 collinearity at @xmath359 , the cosine becomes @xmath374 , which converges to zero as @xmath375 .",
    "thus , as @xmath376 , exchangeable designs approach orthogonal designs even at their most collinear extreme . for further illustrative materials related to exchangeable designs ,",
    "see figures  2 and 3 in the online appendix of the supplementary material [ @xcite , section b.7 ] .",
    "the following is a situation in which the asymptotic upper bound for @xmath377 is @xmath6 , hence equal to the rate of the scheff constant @xmath378 .",
    "perhaps surprisingly , it is sufficient to consider posi1 ( section  [ secposi1 ] ) whose constant is dominated by that of full posi .",
    "let the posi1 predictor of interest be @xmath379 , so the search is over all models @xmath380 , but inference is sought only for @xmath381 . consider the following upper triangular @xmath325 design matrix in canonical coordinates : @xmath382 where @xmath383 is the primary predictor , and the canonical basis vectors @xmath384 are the controls .",
    "the vector @xmath385 has unit length , and hence the parameter @xmath386 is the correlation between the primary predictor and the controls .",
    "it is constrained to @xmath387 , so @xmath388 has full rank .",
    "for @xmath389 the primary predictor @xmath385 becomes fully collinear with the controls , and it is on the approach to this boundary where the rate of the following theorem is attained :    [ thmrootp ] for @xmath24 known , the designs ( [ eqppcdesign ] ) have posi__1 _ _ constants @xmath390 with the following asymptotic rate : @xmath391    the proof is in appendix  [ appthmrootp ] . as @xmath392 the theorem provides a lower bound on the rate of the full posi constant .",
    "the value 0.6363   is not maximal , and we have indications that the supremum over all designs may exceed  0.78 . together with the upper bound of corollary  [ corbound ] , this would provide a narrow asymptotic range for worst - case posi .",
    "most importantly , the example shows that for some designs posi constants can be much larger than the @xmath393 @xmath161-quantiles used in common practice .",
    "the following is a rough asymptotic upper bound on all posi constants @xmath394 .",
    "it has the scheff rate but with a multiplier that is strictly less than scheff s .",
    "the bound is loose because it ignores the rich structure of the sets @xmath273 ( section  [ secorthgeo ] ) and only uses their cardinality @xmath395 ( @xmath31 in the classical case @xmath243 and ) .",
    "[ thmbound ] denote by @xmath396 arbitrary finite sets of @xmath260-dimensional unit vectors , @xmath397 , such that @xmath398 where @xmath399 .",
    "denote by @xmath400 the @xmath401-quantile of @xmath402 .",
    "then the following describes an asymptotic worst - case bound for @xmath400 and its attainment : @xmath403    the proof of theorem  [ thmbound ] ( see appendix  [ appthmbound ] ) is an adaptation of wyner s ( @xcite ) techniques for sphere packing and sphere covering . the worst - case bound ( @xmath404 )",
    "is based on a surprisingly crude bonferroni - style inequality for caps on spheres .",
    "attainment of the bound ( @xmath405 ) makes use of the artifice of picking the vectors @xmath406 randomly and independently .",
    "applying the theorem to posi sets @xmath407 in the classical case @xmath243 , we have @xmath408 , hence @xmath409 , so the theorem applies with @xmath410 :    [ corbound ] in the classical case @xmath16 a universal asymptotic upper bound for the posi constant @xmath411 is @xmath412    the corollary shows that the asymptotic rate of the posi constant , if it reaches the scheff rate , will always have a multiplier that is strictly below that of the scheff constant .",
    "we do not know whether there exist designs for which the bound of the corollary is attained , but the theorem says the bound is sharp for unstructured sets @xmath290 .",
    "we investigated the post - selection inference or `` posi '' problem for linear models whereby valid statistical tests and confidence intervals are sought after variable selection , that is , after selecting a subset of the predictors in a data - driven way .",
    "we adopted a framework that does _ not _ assume any of the linear models under consideration to be correct .",
    "we allowed the response vector to be centered at an arbitrary mean vector but with homoscedastic and gaussian errors .",
    "we further allowed the full predictor matrix @xmath413 to be rank - deficient , @xmath414 , and we also allowed the set @xmath134 of models @xmath38 under consideration to be largely arbitrary . in this framework",
    "we showed that valid post - selection inference is possible via simultaneous inference .",
    "an important enabling principle is that submodels have their own regression coefficients ; put differently , @xmath61 and @xmath415 are generally different parameters if @xmath416 .",
    "we showed that simultaneity protection for all parameters @xmath61 provides valid post - selection inference . in practice",
    "this means enlarging the constant @xmath417 used in conventional inference to a constant @xmath418 that provides simultaneity protection for up to @xmath31 parameters @xmath61 .",
    "we showed that the constant depends strongly on the predictor matrix @xmath12 as the asymptotic bound for @xmath173 with @xmath419 ranges between the minimum of @xmath420 achieved for orthogonal designs on the one hand , and a large fraction of the scheff bound @xmath195 on the other hand .",
    "this wide asymptotic range suggests that computation is critical for problems with large numbers of predictors . in the classical case",
    "@xmath243 our current computational methods are feasible up to about @xmath7 .",
    "we carried out post - selection inference in a limited framework .",
    "several problems remain open , and many natural extensions are desirable :    * among open problems is the quest for the largest fraction of the asymptotic scheff rate @xmath195 attained by posi constants .",
    "so far we know this fraction to be at least 0.6363 , but no more than 0.8660   in the classical case @xmath243 .",
    "when the size of models @xmath421 is limited as a function of @xmath4 ( `` sparse models '' ) , better rates can be achieved , and we will report these results elsewhere . *",
    "computations for @xmath422 are a challenge .",
    "straight enumeration of the set of up to @xmath31 linear combinations should be replaced with heuristic shortcuts that yield practically useful upper bounds on @xmath423 , @xmath134 , @xmath424 that are specific to @xmath12 and the set of submodels @xmath134 , unlike the 0.8660 fraction of the scheff bound which is universal .",
    "* situations to which the posi framework should be extended include generalized linear models , mixed effects models , models with random predictors , as well as prediction problems .",
    "results for the last two situations will be reported elsewhere .",
    "* it would be desirable to devise post - selection inference for specific selection procedures for cases in which a strict model selection protocol is being adhered to .",
    "@xmath425 code for computing the posi constant for up to @xmath426 can be obtained from the authors web pages ( a manuscript describing the computations is available from the authors ) .",
    "\\(1 ) the matrix @xmath427 has the vectors @xmath264 as its columns .",
    "thus @xmath428 .",
    "orthogonality @xmath429 for @xmath430 follows from @xmath431 .",
    "the same properties hold for the normalized vectors @xmath274 .",
    "\\(2 ) the vectors @xmath432 form a gram ",
    "schmidt series with normalization , hence they are an o.n .",
    "basis of @xmath433 .",
    "\\(3 ) for @xmath281 , @xmath58 , @xmath434 , we have @xmath435 because they can be embedded in an o.n .",
    "basis by first enumerating @xmath38 and subsequently @xmath436 , with @xmath114 being last in the enumeration of @xmath38 and @xmath437 last in the enumeration of @xmath438 .",
    "\\(4 ) for any @xmath439 , @xmath440 , there are @xmath441 ways to choose a partner @xmath442 such that either @xmath443 or @xmath444 , both of which result in @xmath445 by the previous part .",
    "the proof relies on a careful analysis of orthogonalities as described in proposition  [ proporth ] , part  ( 3 ) . in what follows we write",
    "@xmath446 $ ] for the column space of a matrix @xmath447 , and @xmath446^\\perp$ ] for its orthogonal complement .",
    "we show first that , for @xmath448 , @xmath336 , the vectors @xmath449 and @xmath274 are in the same one - dimensional subspace , hence are a multiple of each other . to this end",
    "we observe : @xmath450,\\qquad { { \\bar{\\mathbf{l}}}}_{j \\cdot{\\mathrm{m } } } \\in [ { { \\mathbf{x}}}_{{\\mathrm{m}}\\setminus j}]^\\perp , & \\\\ & \\displaystyle { { \\bar{\\mathbf{l}}}}^*_{j \\cdot{\\mathrm{m}}^ * } \\in\\bigl[{{\\mathbf{x}}}^*_{{\\mathrm{m}}^*}\\bigr],\\qquad { { \\bar{\\mathbf{l}}}}^*_{j \\cdot{\\mathrm{m}}^ * } \\in\\bigl[{{\\mathbf{x}}}^*_{{\\mathrm{m}}^ * \\setminus j}\\bigr]^\\perp , & \\\\ \\label{dualproof3 } & \\displaystyle \\bigl[{{\\mathbf{x}}}^*_{{\\mathrm{m}}^*}\\bigr ] = [ { { \\mathbf{x}}}_{{\\mathrm{m}}\\setminus j}]^\\perp,\\qquad \\bigl[{{\\mathbf{x}}}^*_{{\\mathrm{m}}^ * \\setminus j}\\bigr]^\\perp= [ { { \\mathbf{x}}}_{{\\mathrm{m}}}].&\\end{aligned}\\ ] ] the first two lines state that @xmath274 and @xmath449 are in the respective column spaces of their models , but orthogonalized with regard to all other predictors in these models . the last line , which can also be obtained from the orthogonalities implied by @xmath451 ,",
    "establishes that the two vectors fall in the same one - dimensional subspace , @xmath452 \\cap[{{\\mathbf{x}}}_{{\\mathrm{m}}\\setminus j}]^\\perp = \\bigl[{{\\mathbf{x}}}^*_{{\\mathrm{m}}^*}\\bigr ] \\cap\\bigl[{{\\mathbf{x}}}^*_{{\\mathrm{m}}^ * \\setminus j } \\bigr]^\\perp\\ni{{\\bar{\\mathbf{l}}}}^*_{j \\cdot{\\mathrm{m}}^*}.\\ ] ] since they are normalized , it follows @xmath453 .",
    "this result is sufficient to imply all of theorem  [ thmdual ] .",
    "the lemma , however , makes a slightly stronger statement involving lengths which we now prove . in order to express @xmath264 and @xmath333",
    "according to ( [ eqposivecs ] ) , we use @xmath454 as before and we write @xmath455 for the analogous projection onto the space spanned by the columns @xmath456 of @xmath327 .",
    "the method of proof is to evaluate @xmath457 .",
    "the main argument is based on @xmath458 which follows from these facts : @xmath459 which in turn are consequences of ( [ dualproof3 ] ) and @xmath460 .",
    "we also know from ( [ eqposivecs ] ) that @xmath461 putting together ( [ dualproof5 ] ) , ( [ dualproof6 ] ) and ( [ eqposivecs ] ) , we obtain @xmath462 because the two vectors are scalar multiples of each other , we also know  that @xmath463 putting together ( [ dualproof7 ] ) and ( [ dualproof8 ] ) we conclude @xmath464 this proves the lemma and the theorem .",
    "the parameter @xmath347 in equation ( [ eqex ] ) can range from @xmath465 to @xmath466 , but because of duality there is no loss of generality in considering only the case in which @xmath467 , and we do so in the following .",
    "let @xmath468 and @xmath58 .",
    "consider first the case @xmath469 , hence @xmath470 : we have @xmath471 , the @xmath114th column of @xmath12 , and @xmath472 . for any @xmath473 it follows=1 @xmath474=0    consider next the case @xmath475 , and for notational convenience let @xmath476 and @xmath477 where @xmath478 .",
    "the following results can then be applied to arbitrary @xmath38 and @xmath58 by permuting coordinates . the projection of @xmath479 on the space spanned by @xmath480 must be of the form @xmath481 where the constant @xmath386 satisfies @xmath482 .",
    "this follows from symmetry , and no calculation of projection matrices is needed to verify this .",
    "let @xmath483 .",
    "then @xmath484 some algebra starting from @xmath485 yields @xmath486 the term @xmath487 is nonnegative , maximal wrt @xmath488 for @xmath489 , and thereafter maximal wrt @xmath347 for @xmath490 , whence @xmath491 and finally @xmath492 this fact will make the term @xmath487 in ( [ eqbdexl ] ) asymptotically irrelevant . using and @xmath493 as well as ( [ eqbdexl ] ) and ( [ eqbdexda ] ) we obtain @xmath494\\\\[-8pt ] & \\le & \\|{{\\mathbf{z}}}\\|_\\infty+ \\|{{\\mathbf{z}}}\\|_\\infty+ \\biggl{\\vert}\\frac{1}{2 \\sqrt{p } } \\sum_{j=1}^p z_j \\biggr{\\vert}.\\nonumber\\end{aligned}\\ ] ] combining ( [ eqbdexs1 ] ) and ( [ eqbdexs2 ] ) we obtain for @xmath495 the following :",
    "@xmath496 this verifies that @xmath497    it remains to prove that equality holds in ( [ eqbdexineq ] ) . to this end",
    "let @xmath498 denote the order statistics of @xmath499 , @xmath500 .",
    "fix @xmath488 .",
    "we have in probability @xmath501 note that @xmath502 for a given @xmath258 we choose @xmath503 such that @xmath504 is the index of @xmath505 and @xmath506 includes @xmath507 as well as the set of indices of @xmath508 for @xmath509 . from ( [ eqbdexl ] ) we then obtain in probability @xmath510 choosing @xmath488 arbitrarily large and combining this with ( [ eqbdexineq ] ) yields the desired conclusion .",
    "recall from ( [ eqppcdesign ] ) the designs @xmath511 where @xmath512 is the primary predictor .",
    "the matrix @xmath12 will be treated according to posi1 ( section  [ secposi1 ] ) , and hence we will examine the distribution of @xmath513 ( assuming @xmath342 known ) .",
    "we determine @xmath514 for a fixed model @xmath38 ( @xmath4 ) with @xmath515 , @xmath516 therefore , @xmath517 for fixed @xmath488 we can explicitly maximize the sum on the right - hand side , @xmath518 where @xmath519 is the @xmath114th order statistic of @xmath499 , @xmath520 , omitting @xmath521 .",
    "we can also explicitly maximize the factor @xmath522 in ( [ eqrootp1 ] ) , @xmath523 and equality is attained as @xmath524 .",
    "therefore , for fixed @xmath488 , we can continue from ( [ eqrootp1 ] ) as follows : @xmath525 the reason for writing the two sums in this manner is that we will interpret them as approximations to riemann sums . to this end",
    "we borrow from @xcite the following approximations for @xmath526 : @xmath527 reparametrizing @xmath528 , the anticipated riemann approximation is @xmath529 therefore , @xmath530 and similarly @xmath531 summarizing , @xmath532 the function @xmath533 is maximized at @xmath534 with@xmath535 .",
    "therefore , @xmath536 the bound is sharp because it is attained by the models that include the first or last @xmath537 order statistics of @xmath258 when @xmath538 and @xmath539 . from ( [ eqk1 ] )",
    "we conclude that @xmath540 .",
    "we show that if @xmath541 , then :    * we have a uniform asymptotic worst - case bound , @xmath542 * which is attained when @xmath543 and @xmath544 are i.i.d .",
    "@xmath545 independent of @xmath258 , @xmath546    these facts imply the assertions about @xmath401-quantiles @xmath547 of@xmath548 in theorem  [ thmbound ] .",
    "we decompose @xmath549 where @xmath550 and @xmath551 are independent . due to",
    "@xmath552 it is sufficient to show the following :    * uniform asymptotic worst - case bound , @xmath553 * attainment of the bound when @xmath554 and @xmath544 are i.i.d .",
    "@xmath545 independent of @xmath555 , @xmath556    to show ( [ equbound ] ) , we upper - bound the noncoverage probability and show that it converges to zero for @xmath557 . to this end",
    "we start with a bonferroni - style bound , as in @xcite , @xmath558 & = & { { \\mathbf{p}}}\\bigcup_{{{\\bar{\\mathbf{l}}}}\\in{\\mathcal{l } } } \\bigl [ \\bigl|{{\\bar{\\mathbf{l}}}}^t { { \\mathbf{u}}}\\bigr| > k ' \\bigr ] \\nonumber \\\\ & \\le & \\sum_{{{\\bar{\\mathbf{l}}}}\\in{\\mathcal{l } } } { { \\mathbf{p}}}\\bigl [ \\bigl|{{\\bar{\\mathbf{l}}}}^t { { \\mathbf{u}}}\\bigr| > k'\\bigr ] \\\\ & = & |{\\mathcal{l}}_p| { { \\mathbf{p}}}\\bigl [ |u| > k'\\bigr],\\nonumber\\end{aligned}\\ ] ] where @xmath559 is any coordinate of @xmath555 or projection of @xmath555 onto a unit vector .",
    "we will show that bound ( [ eqbonf ] ) converges to zero .",
    "we use the fact that @xmath560 , hence @xmath561 = \\frac{1}{{{\\mathrm{b}}}(1/2,(p-1)/2 ) } \\int _ { k'^2}^1 x^{-1/2 } ( 1-x)^{(p-3)/2 } \\,dx.\\ ] ] we bound the beta function and the integral separately , @xmath562 where we used @xmath563 ( a good approximation , really ) and @xmath564 .",
    "@xmath565 where we used @xmath566 on the integration interval . continuing with the chain of bounds from ( [ eqbonf ] ) we have @xmath567 \\le\\frac{1}{k ' }",
    "\\biggl(\\frac{2}{(p-1)\\pi } \\biggr)^{1/2 } \\bigl ( |{\\mathcal{l}}_p|^{1/(p-1 ) } \\sqrt{1-k'^2 } \\bigr)^{p-1}.\\ ] ] since @xmath568 , the right - hand side converges to zero at geometric speed if @xmath569 , that is , if @xmath557 .",
    "this proves ( [ equbound ] ) .    to show ( [ eqlbound ] ) , we upper - bound the coverage probability and show that it converges to zero for @xmath570 .",
    "we make use of independence of @xmath544 , as in @xcite , @xmath571 & = & \\prod_{{{\\bar{\\mathbf{l}}}}\\in{\\mathcal{l}}_p } { { \\mathbf{p}}}\\bigl [ \\bigl| { { \\bar{\\mathbf{l}}}}^t { { \\mathbf{u}}}\\bigr| \\le k ' \\bigr ] = { { \\mathbf{p}}}\\bigl [ | u | \\le k ' \\bigr]^{|{\\mathcal{l}}_p| } \\nonumber \\\\ & = & \\bigl(1 - { { \\mathbf{p}}}\\bigl [ | u | > k ' \\bigr ] \\bigr)^{|{\\mathcal{l}}_p| } \\\\ & \\le & \\exp\\bigl ( - |{\\mathcal{l}}_p| { { \\mathbf{p}}}\\bigl [ | u | > k ' \\bigr ] \\bigr).\\nonumber\\end{aligned}\\ ] ] we will lower - bound the probability @xmath572 $ ] recalling ( [ eqbeta ] ) and again deal with the beta function and the integral separately , @xmath573 where we used @xmath574 ( again , a good approximation ) .",
    "@xmath575 where we used @xmath576 . putting it all together we bound the exponent in  ( [ eqexp ] ) , @xmath567 \\ge\\frac{\\sqrt{p/2 - 3/4}}{\\sqrt{\\pi } ( p-1)/2 } \\bigl ( |{\\mathcal{l}}_p|^{1/(p-1 ) } \\sqrt{1-k'^2 } \\bigr)^{p-1}.\\ ] ] since @xmath568 , the right - hand side converges to @xmath577 at nearly geometric speed if @xmath578 , that is , @xmath570 .",
    "this proves  ( [ eqlbound ] ) .",
    "@xmath579we thank e. candes , l. dicker , m. freiman , e. george , a. krieger , m. low , z. ma , e. pitkin , l. shepp , n. sloane , p. shaman and m. traskin for very helpful discussions .",
    "the acronym `` spar '' is due to m. freiman .",
    "we are indebted to an anonymous reviewer for extensive and constructive criticism that influenced the positioning of this article ."
  ],
  "abstract_text": [
    "<S> it is common practice in statistical data analysis to perform data - driven variable selection and derive statistical inference from the resulting model . </S>",
    "<S> such inference enjoys none of the guarantees that classical statistical theory provides for tests and confidence intervals when the model has been chosen a priori . </S>",
    "<S> we propose to produce valid `` post - selection inference '' by reducing the problem to one of simultaneous inference and hence suitably widening conventional confidence and retention intervals . </S>",
    "<S> simultaneity is required for all linear functions that arise as coefficient estimates in all submodels . by purchasing `` simultaneity insurance '' for all possible submodels , </S>",
    "<S> the resulting post - selection inference is rendered universally valid under all possible model selection procedures . </S>",
    "<S> this inference is therefore generally conservative for particular selection procedures , but it is always less conservative than full scheff protection . </S>",
    "<S> importantly it does _ not _ depend on the truth of the selected submodel , and hence it produces valid inference even in wrong models . </S>",
    "<S> we describe the structure of the simultaneous inference problem and give some asymptotic results .    ,    ,    , + </S>"
  ]
}