{
  "article_text": [
    "dedicated computers have had a long history vis - a - vis several physics problems demanding huge computing resources , where conventional computers have not allowed to reach conclusive results .",
    "a non exhaustive list of these problems includes monte carlo simulations of lattice qcd  @xcite , numerical studies of large self - gravitating systems @xcite and high - resolution numerical solutions of the navier - stokes equations in the turbulent regime @xcite .",
    "statistical mechanics is another area , relevant for our plans , where dedicated computers have also been heavily used  @xcite , starting from the pioneering efforts dating as far back as the late seventies  @xcite for the simulation of the ising model .",
    "several dedicated machines have been developed more recently to study spin - glass systems .",
    "a dedicated machine , _ rtn _ , built in zaragoza in 1991  @xcite , was designed to perform computationally intensive calculations that make limited use of floating point arithmetics , such as spin glasses and lattice gauge - higgs model simulations .",
    "rtn was based on transputer processors : it had 8 identical boards , each with 8 transputers , one connection chip , and one controller board .",
    "rtn was at the time a very effective and reliable computational device .",
    "_ sue _ ( spin update engine ) has been the second generation spin - glass machine  @xcite .",
    "sue was completed in the year @xmath0 , and it has been used to simulate three dimensional ising spin glasses .",
    "it consists of twelve identical boards : each board simulates at the same time @xmath1 different systems .",
    "the update speed of the whole machine is @xmath2 ps / spin ( when running at a @xmath3 mhz clock frequency ) .",
    "field programmable gate arrays ( fpga ) made it possible to design a low cost , high performance , reliable dedicated machine .",
    "the fpga - based architecture allows to reprogram the structure of the hardware connections of the computer : in sue this feature was used to perform simple changes , like studying systems of different sizes , or modifying the updating dynamical scheme , or to change some details of the hamiltonian of the system .",
    "sue was very different from rtn : instead of `` usual '' processors like transputers ( but remember that even transputers were not usual at all in their extreme optimization toward easy inter - communication ) it adopted fpga s ( of a generation which , looked at from today , seems prehistoric ) : these devices give the possibility of `` programming '' the hardware and of configuring the processor at best . in short ,",
    "they contain logical gates that can be activated or deactivated or connected as needed , allowing in this way to select the desired functionalities .    in this paper",
    "we describe yet another generation spin - glass computing engine , called _ ianus _ ( after the name of the two - faced ancient roman god of doors and _ gates _ ) , whose development is well underway .",
    "the new project has basically two goals : i ) developing an extremely high - performance spin - glass simulation engine , able to update ( on average ) one spin in less than @xmath4 ps and ii ) developing a software infrastructure that drastically reduces the effort to customize an array of fpga s to a specific computational algorithm .",
    "this paper is structured as follows : in section @xmath5 we discuss the advantages and disadvantages of building dedicated computers instead of buying commercial clusters , and in section @xmath6 we describe as a practical example our computational goals for spin - glass simulations and the performance of some non - dedicated computers when handling it .",
    "section @xmath7 describes the overall architecture of the new machine and section @xmath8 discusses the hardware main issues .",
    "section @xmath9 presents performance estimations , and the paper is ended by some concluding remarks that also cover longer term plans to make _ ianus _ more general purpose .",
    "the decision to invest time and money in developing a new dedicated computer has to follow an accurate balancing between real benefits and development burden ; also the possibility of seeing the project _ aging _ when compared to actual scientific progress and technological improvements has to be taken into account .",
    "optimized computers provide much larger computing power when used in solving problems they are designed for .",
    "this can show up either as a much better `` price to performance '' ratio , so that the computing power available within the limits of a given budget is greatly increased , or as a much higher sheer performance , technically not achievable at a given point in time on the given problem with commercial computers .",
    "this is not the only way dedicated computers shorten the time needed to produce scientific results : usually the development of computer applications for scientific research is centered in squeezing performance out of very generic hardware .",
    "code optimization is always a hard and time - eating task and often the gain is not comparable to the effort . a dedicated machine is built _ around _ a specific computational task , and therefore the final user always accesses its maximum performance .",
    "all computational problems for which dedicated machines have been proposed have a very large degree of regularity , that typically translates into heavy use of _ unusual _ mathematical sequences and into extreme levels of parallelization .",
    "both features are key factors for performance :    * traditional computers are optimized for execution of short linear sequences of code performing irregular memory accesses , and integer arithmetic or logic manipulations ( adds / subtract ) of long data words ( @xmath10 or @xmath11 bits ) .",
    "recent designs also focus on floating point arithmetics .",
    "every time the mix of required operations is at variance with the above list , there is room for dedicated processing .",
    "for instance , the grape processor is centered around an hardware block able to compute on the fly the inverse of a square - root . in lattice",
    "qcd virtually all processing amounts to manipulation of complex matrices so that hardware pipelines computing @xmath12 ( with @xmath13 complex numbers ) are extremely effective . in our case , as discussed in details later on , we leverage on the fact that our typical operations are logical operations on a small number of bits and that the data - base to be processed is small enough that we store it within the processor , fully removing the bottle - neck of memory access . *",
    "traditional computers are not usually designed to be effective for massive , regular and tightly coupled parallelism . rather , typical design goals in commercial parallel systems are extreme flexibility in possible communication patterns , at the price of interconnection bandwidth and latency .",
    "many computationally intensive applications on the other hand have intrinsically high levels of parallelism , so each processing node handles a very small subset of the physical system .",
    "this brings inevitably to a large ratio of information exchanged among nodes over performed arithmetic / logic operation .",
    "parallel efficiency requires that the time @xmath14 needed by a processor to handle ( e.g. , on one iteration ) its system subset is roughly equal to the time @xmath15 needed to exchange needed data with other nodes .",
    "we will see that this requirement is badly violated in traditional computer systems , while dedicated structures may leverage on regularity of communication patterns to achieve the goal of @xmath16 with limited resources .",
    "all this has costs .",
    "* large investment in time : a new concept dedicated machine always requires a long development time , due to designing , building and testing it . * large investment in man - power : specific high - level knowledge in electronics ,",
    "digital design and topics in the target machine s application fields are needed .",
    "working groups made of engineers , physicists and biologists have to be formed and coordinated . in universities",
    ", the investment in the training of collaborating students has to be taken into account .",
    "the relatively long time frame associated to development carries two additional risks .    *",
    "possible loss of relevance of the optimized algorithms .",
    "a lot of care has to be taken when judging what shall be still interesting to the scientific community in the following years , to avoid that the specific problems on which the new machine is unbeaten becomes obsolete before the machine does .",
    "we may say that it is the machine that has to make the problem grow old and not the opposite .",
    "* loss of competitiveness in time with respect to commercial computers .",
    "computer performance has been growing steadily according to moore s law ( performance doubles every approximately 18 months ) , while the performance of a dedicated machine remains constant , till a new generation is developed .",
    "[ power_evol ] sketches the situation : one must be sure that the performance gain associated to the dedicate system , _ when _ it comes into operation is large enough to offset power improvements of traditional processors working in parallel with a reasonable parallel factor , and provide a window of opportunity of at least a few years .        in the next sections we analyze these problems with a specific regard to our application .",
    "the first area of application for _ ianus _ is the accurate and detailed study of spin - glass systems . later on",
    "we will describe other contexts in which we expect _ ianus _ to become a key player .",
    "we consider here the the edwards - anderson spin glass model  @xcite in three dimensions .",
    "we define as usual a collection of variables @xmath17 corresponding to sites of a cubic lattice of size @xmath18 ; each variable takes values @xmath19 .",
    "a collection of static links @xmath20 between first neighbor sites determines the properties of the system , and we are interested in randomly ( quenched ) chosen ones .",
    "when studying equilibrium properties of the system , averages on a large number of realizations of the @xmath21 ensemble are appropriate , while an analysis of the system away from equilibrium requires very long time histories over a smaller set of @xmath21 . in the former case , there is an additional opportunity for trivial parallelization , as simulations are performed in parallel over different sets of @xmath21 , that possibly use the same random numbers .",
    "this option is much less effective in the latter case , whose physical interest is growing with time , and for which we plan to optimize our new processor .",
    "we discuss here two different algorithms , namely the demon  @xcite and the heat bath algorithm  @xcite .",
    "the main advantage of the former algorithm is that random number are not needed , so it was widely applied in earlier dedicated machines . for the latter algorithm ,",
    "on the other hand , performance depends strongly on the efficiency of ( multiple ) random number generators .",
    "the energy of the system is defined by @xmath22 where @xmath23 indicates that sums have to be performed over all pairs of neighboring sites in the physical lattice . in the heat bath algorithm spins",
    "are updated by extracting their values with a probability given by : @xmath24 updates may be done sequentially ( one at a time , eventually sweeping the whole lattice ) or in parallel , being careful to respect balance .",
    "the highest level of achievable parallelism is obtained when all sites are divided into two groups in a checkerboard configurations and all elements of each group are updated at the same time ( use of checkerboard structure is needed to ensure that detailed balance is respected during the simulation ) .    in the demon algorithm ,",
    "the system is coupled to a set of energy reservoirs ( `` demons '' ) @xmath25 .",
    "the energy of this enlarged system is given by @xmath26 where all the demons may be initially set to zero .",
    "the demons can not lower their energy below zero and also an upper limit may be assigned .",
    "consider the case @xmath27 with a unique reservoir coupled to the system .",
    "spins are updated sequentially depending on the actual values of the reservoir : if inverting the value of a spin is energetically convenient , the demon receives the energy variation and the spin is flipped , otherwise the spin flips only if the reservoir may supply the needed energy .",
    "both algorithms can be coded in traditional processors with reasonable efficiency .",
    "for instance , the heat - bath procedure updates a single spin every 10 nanoseconds on a pentium iv processor at 3.2 ghz by means of a reasonably programmed multi - spin coding updating routine , in which one random number is used for each try .",
    "if multiple spins ( belonging to independent ensembles ) use the same random number performance increase by a factor of roughly @xmath6 .",
    "both algorithms can be parallelized very easily .",
    "typically the physical lattice is divided in homogeneous region and computations in each of them are carried out by one computational node ; only boundaries of each partition have to be transported between nodes after each step of the algorithm .",
    "we have parallelized the heat - bath algorithm on a set of 4 pentium iv nodes at 3.2 ghz with gigabit ethernet obtaining a parallel efficiency of 42% , when considering a lattice of global size @xmath28 .",
    "the small amount of information to be transmitted per each complete monte carlo lattice update makes latency overhead very large for each communication .",
    "the actual spin update time ( within each node of the cluster ) to consider when comparing to our new dedicated machine is then about 24 nanoseconds .",
    "a properly coded routine for the demon algorithm updates a spin in less than 4 nanoseconds .",
    "the parallel efficiency in this case is even lower , about 25% .",
    "sustained parallel update time in this case is therefore of the order of @xmath29 nanoseconds in this case .",
    "very recently , new computer options have become available , in terms of very massively parallel systems with nearest neighbor connections ( blue gene / l @xcite ) and recently announced processors with extremely powerful on - board processing resources ( like the clearspeed csx600 processor @xcite or the ibm / sony / toshiba cell processor @xcite ) .    in order to estimate the performance of spin - updates on these new machines , we may build a very simple model , in which we define the computational load as @xmath30 ( @xmath31 is the linear size of the lattice handled by each processor ) , and the information - exchange between neighboring nodes @xmath32 . for a balanced spin - update engine , we will have @xmath33 where we have introduced average sustained processing power @xmath34 and average sustained interconnection bandwidth @xmath35 ( both quantities can be very conveniently written in terms of operations ( or bits exchanged ) per processor clock cycle ) .",
    "considering for definiteness the heat - bath algorithm , we have @xmath36 ( in @xmath37 we have lumped a few",
    "short - word adds , the computation of one random number and access to one entry of a small look - up table ) , and @xmath38 .",
    "comparing with the performance figures for the pentium iv cluster , we have effective values of @xmath39 , and @xmath40 .    for the new machines , we use published data for @xmath35 and an approximate estimate for @xmath34 based on the assumption that the ( clock - frequency normalized ) performance of each integer processing unit available on these new processors is roughly the same as that available on a pentium 4 ( probably an upper limit , since these processors are all geared to floating point performance ) .",
    "we list relevant properties in table [ tab : procs ] , where we also estimate the update time per site and the smallest value of @xmath31 , @xmath41 , for which performance is not limited by communications , that we derive from eq .",
    "( [ eq : balance1 ] ) .",
    ".architectural parameters for blue gene / l and for the ibm - cell , and clearspeed csx600 processors and their corresponding single - processor estimated spin - update time . [ cols=\"^,^,^,^\",options=\"header \" , ]     [ tab : procs ]    note that these processors and systems are strongly optimized for floating - point calculations , not particularly useful in our context .",
    "this is worsened by the fact that fast floating point is single precision for blue - gene and the cell processors ( the latter also has less accurate rounding circuits ) so its use for random number generation is questionable .",
    "for this reason sustainable @xmath34 values are smaller than would be achievable in floating point intensive computation .",
    "inspection of table [ tab : procs ] shows that the update - speed of one blue - gene / l node is smaller than on a conventional processor , while both the cell and csx600 processors promise a boost of almost one order of magnitude more than traditional high - end microprocessors .",
    "the real advantage potentially offered by all these new architectures however is that all the intrinsic parallelism of the underlying problem can be exploited without performance penalties .",
    "_ ianus _ is a massively parallel machine , optimized for the simulation of spin - glass systems and based on latest generation fpga components .    in recent years",
    ", fpga s have increased dramatically their processing speed and , above all , the logic complexity that they can incorporate . at present , logical systems of the order of millions of gates",
    "can be built inside just one fpga device . at the same time",
    ", memory blocks of the order of millions of bits are built into high - end fpga s .",
    "as already stressed , we expect to make much better than conventional processors in application domain , leveraging on three main cornerstones :    * logic operations associated to spin - glass simulations differ from the arithmetic operations in which traditional computers focus . * our data - base",
    "can be fully contained by memories inside the processor . as a consequence",
    "an extremely high bandwidth is available to feed processing blocks .",
    "* fpga s have very large resources for off - chip communication that we exploit for processor to processor data - transfer , with simple protocols that help reduce latency .    at the same time",
    ", fpga s should help reduce development efforts , enlarging the window of opportunity for our machine , and allow , at a later stage , efficient tailoring of the developed hardware in different application areas .    in short",
    ", we are developing a _ bi - programmable _ machine , that we call _ ianus_. _ ianus _ is based on the following main components :    * a hardware layer , based on a module containing several high - end fpga s . * a communication fabric that allows high bandwidth data exchange among the fpga s and connection of the hardware module with a traditional host computer . * a software layer that allows to load a specific simulation model onto the hardware layer , set appropriate simulation parameters , run actual simulations and collect results .",
    "* at a later stage , we plan to add a model - development layer that allows to develop an ad - hoc fpga based simulation engine for a large and growing class of interesting models with reasonable effort ( we refer to this layer with the term bi - programmability ) .",
    "the natural focus of _ ianus _ will be toward discrete variables .",
    "binary variables are the easiest example ( and at first glance they can be seen as occupation variables , or as magnetic spins , or as boolean truth statements ) , but we plan and demand that _ ianus _ should be as effective when dealing with @xmath42 or @xmath43 state variables .",
    "typical fields we believe will be able to use _ ianus _ effectively are :    * lattice models for physical systems .",
    "* slow dynamics on simplified models : that is very important when discussing the former issue but is also very relevant for the next two issues . *",
    "biological issues : go models , statistical mechanics of dna and rna , docking . * optimization and cryptography problems .",
    "the hardware architecture of _ ianus _ has already been developed : the basic computational block will contain 16 high - end fpga chips ( e.g. , of the altera stratix or xilinx virtex families ) that we call sp , ( for simulation processors ) .",
    "we believe that using a powerful state - of - the - art core for the sp is an important feature of the machine we have in mind .",
    "notice that each sp processor will be able to deal with of the order of @xmath44 logical elements at the same time ( at least a @xmath45 lattice of boolean variables when using binary link coupling ) .    the sp processors will be arranged and connected in a two dimensional array of @xmath46 processors : periodic boundary conditions will be enforced by hardware .",
    "a seventeenth processor will be used as an input / output processor ( iop ) for communicating with other boards .",
    "it will play the role of a cross - bar switch : each of the @xmath47 sp will have a ( fast ) link to the iop .",
    "the iop will perform two main functions , by allowing effective long range sp to sp communication and by acting as interface between the processing board and the host computer , a traditional linux - based system .    in the long term , this hardware structure will be enabled by a growing set of fpga - optimized building blocks ( random number generators , metropolis update engines , address - generators ... ) to be used to make the sp s the dedicated / optimized processors for each target application .",
    "we also plan a user friendly framework interface that helps to assemble the wished set of computational functionalities , as well as a run - time support system that controls the configuration of the sp s and the actual simulation steps .",
    "we have already developed and tested preliminary versions of the fpga based implementation of both the demon and heat - bath algorithms .",
    "they are being tested on simple test boards using recent fpga devices ( altera ep1s60 and xilinx virtex 4 lx25 ) . for the final version of the machine we are considering either devices of the altera stratix ep2 or xilinx virtex 4 series .",
    "these more recent components have roughly up to 100% more available gates ( and memory elements ) and their speed is some 20% 30% higher .",
    "our preliminary results are therefore lower limits on achievable performance .",
    "we have developed and tested a simulating engine for the demon algorithm that tries to extract all possible parallelism .",
    "our implementation of the algorithm is as follows : inside each system every site may be labeled as _ black _ or _",
    "white _ following a checkerboard scheme , and each of the two subset may be updated in parallel , all neighbors of a black site being white ones .",
    "we define @xmath48 demon reservoirs , the first one of them coupled to the first black spin and the first white one and so on ; then we proceed with parallel update within each set , alternating the simultaneous update of @xmath49 black spins and @xmath49 white ones .",
    "when simulating spin glasses one usually defines two identical replicas of the system , with same fixed @xmath50 $ ] coupling configuration and different independent initial spin configurations ; this leads to an immediate improvement of the algorithm : we put all black spins of replica 1 and all white spins of replica 2 in the respective positions of an `` artificial '' lattice system , say the _",
    "p lattice _ , while in the _ q lattice _ we put black spins of replica 2 together with white spins of replica 1 .",
    "this way , each spin in p has its neighborhood in q and vice versa . also we double the number of demons to @xmath51 .",
    "this way we can update in parallel a whole lattice of @xmath52 spins , the p or the q one in turn .",
    "preliminary tests show that we need only a 5 ns clock cycle to perform the simultaneous update of up to 1000 spins , that corresponds to a lattice size @xmath53 . in doing this ,",
    "we only made use of flip - flop registers to store variables and the connectivity has been `` hard - wired '' ; an `` update engine '' module is defined ( taking as input a spin variable , its demon and neighborhood and outputting updated spin and demon ) and the updating logic is replicated once for each site of the lattice ; each variable register is directly connected to the appropriate ports of the appropriate engines .",
    "this approach is very efficient but pays the cost of being logic consuming : the `` finiteness '' of an fpga strongly limits the maximum size @xmath18 .",
    "this way we can simulate up to @xmath53 on an altera stratix ep1s60 and up to @xmath54 on a xilinx virtex 4 lx25 .    in order to improve the size on these preliminary tests , we can hardwire connections only on one plane of the cubic lattice , implementing @xmath55 demons and updating engines , saving logic to store systems of size up to @xmath53 on the relatively small xilinx virtex4 xc4vlx25 device and up to @xmath56 on the altera stratix ep1s60 . even if this way we need @xmath18 clock cycles to update the whole lattice",
    ", we can update a spin every @xmath57 .",
    "also a wise use of memory blocks inside fpga s guarantees raising sizes by a factor 3 and more , with a small cost in speed .    on the final machine , we can easily subdivide a @xmath58 system among the 16 sp s of the dedicated machine board in slices of @xmath59 .",
    "cautiously , we update a @xmath60 slice at a time , taking no more than 64 cycles to update in parallel all the system ( a quarter of million spins ! ) . exploiting low latencies",
    ", the algorithm may be tailored to send computed boundary data ( two faces of @xmath61 data bits ) while the sequential update is running inside each sp .",
    "@xmath62 would provide a sustained speed of @xmath63 spins per clock cycle , with a parallel efficiency of 100% . with a clock cycle of 5 nanoseconds",
    ", it corresponds to less than @xmath64 picoseconds per spin update .    for the heat bath algorithm",
    "we need a random number generator for each site that we want to update in parallel .",
    "we have implemented an fpga version that includes 128 update engines , each equipped with its own random number generator ( based on the shift register algorithm of @xcite , with 32-bit words ) . at present , for test purposes , we use a lattice of @xmath45 sites completely contained within the processor .",
    "the system updates one two - dimensional plane at a time . using 128 update - engines in parallel , each plane",
    "is updated in 8 clock cycles , then the following planes are processed .    if we consider the same machine structure as above , and the same physical lattice , we may split the physical lattice in several ways .",
    "one possible solution , that minimizes bandwidth requirement has each processor handling a sublattice of @xmath65 lattice points .",
    "our processors update @xmath66 spin per clock cycles , corresponding to an update time of approximately @xmath67 ps .",
    "this performance is sustained by a bandwidth as small as 32 bits per clock cycle .",
    "extrapolating our preliminary results , we can conclude that _ ianus _ ( with , e.g. , 16 processing boards ) will be equivalent to a cluster of @xmath68 pentium iv processors or to a partition of @xmath69 blue - gene / l processors , or to an assembly of @xmath70 cell or clearspeed processors .    at present",
    "we are controlling that the random generators we will use are correct , and also that the parallel updating scheme does not introduce larger correlations between configurations : eventually the plane - sequential version of the implementation should be safer . at the same time we are finalizing the fpga design and starting to design the actual hardware infrastructure and the software structure needed to interface to traditional linux - based host computers .",
    "probably what is more important to stress here is that _ ianus _ wants to be a machine that can solve a large number of problems . we believe that even models defined on random graphs , or where variables can take a finite number of values , will be very well suited for _ ianus",
    "_ : problems like the study of go models for protein folding and , more in general , biological issues , optimization issues and more will fit very well this pattern .",
    "we already showed that it is possible to set up a computing device with an absolutely favorable performances - to - costs ratio , which is likely to be long - lived in the field of ising spin glasses simulations .",
    "keeping in mind that several topics in the field of complex systems are strictly related to the physics of ising spin systems ( among the classes of problems cited above , for example , error correction codes and the satisfiability of boolean formulae ) , and that disposable technology is giving us many more possibilities with respect to the previous ( and fruitful ) sue experience , we are confident in _ ianus _ turning out as a `` dedicated '' and `` multi - purpose '' machine at the same time : another two - faced aspect justifying the choice of its name .",
    "we wish to thank piero vicini , alessandro lonardo , davide rossetti and sergio de luca for very useful discussions .",
    "we also thank liliana arrachea , pierpaolo bruscolini , kenneth dawson , giacomo marchiori , yamir moreno vega , giorgio parisi , ilenia pedron and federico ricci - tersenghi for sharing with us interesting ideas .",
    "this work has been partially supported by dga , mec ( bfm2003-c08532-c03 , fis04 - 5073-c04 , fises2004 - 01399 , fpa04 - 2602 ) and the european community s human potential programme , contracts hprn - ct-2002 - 00307 ( dyglagemem ) and hprn - ct-2002 - 00319 ( stipco ) .",
    "j. kahle , m. suzuoki , y. masubuchi , _ cell microprocessor briefing _",
    ", san francisco ( 2005 ) + s. mueller et al .",
    ", _ the vector floating - point unit in a synergistic processor element of a cell processor _ , proc .",
    "17th int . symp . on computer arithmetic ( 2005 )",
    "to appear ."
  ],
  "abstract_text": [
    "<S> dedicated machines designed for specific computational algorithms can outperform conventional computers by several orders of magnitude . in this note we describe _ ianus _ , a new generation fpga based machine and its basic features : hardware integration and wide reprogrammability . </S>",
    "<S> our goal is to build a machine that can fully exploit the performance potential of new generation fpga devices . </S>",
    "<S> we also plan a software platform which simplifies its programming , in order to extend its intended range of application to a wide class of interesting and computationally demanding problems . </S>",
    "<S> the decision to develop a dedicated processor is a complex one , involving careful assessment of its performance lead , during its expected lifetime , over traditional computers , taking into account their performance increase , as predicted by moore s law . </S>",
    "<S> we discuss this point in detail .    </S>",
    "<S> -25pt    ,    ,    ,    ,    ,    ,    ,    ,    ,    ,    ,    ,    ,    ,    ,    ,    ,    ,    ,    . -10pt    </S>",
    "<S> -10pt    -15pt    spin - glass , special purpose machine , special computers , programmable logic . </S>"
  ]
}