{
  "article_text": [
    "from a methodological viewpoint , testing a null hypothesis @xmath2 versus the alternative @xmath3 in a bayesian framework requires the introduction of two prior distributions , @xmath4 and @xmath5 , that are defined on the respective parameter spaces . in functional terms , the core object of the bayesian approach to testing and model choice",
    ", the bayes factor @xcite , is indeed a ratio of two marginal densities taken at the same observation @xmath6 , @xmath7 ( this quantity @xmath8 is then compared to @xmath9 in order to decide about the strength of the support of the data in favour of @xmath10 or @xmath11 . )",
    "it is thus mathematically clearly and uniquely defined , provided both integrals exist and differ from both @xmath12 and @xmath13 . the practical computation of the bayes factor has generated a large literature on approximative ( see , e.g. * ? ? ? * ; * ? ? ? * ; * ? ? ? * ; * ? ? ? * ) , seeking improvements in numerical precision .",
    "the savage  dickey @xcite representation of the bayes factor is primarily known as a special identity that relates the bayes factor to the posterior distribution which corresponds to the more complex hypothesis . as described in @xcite and @xcite ( 2000 ,",
    "pages 164 - 165 ) , this representation has practical implications as a basis for simulation methods .",
    "however , as stressed in @xcite and @xcite , the foundation of the savage ",
    "dickey representation is clearly theoretical .",
    "more specifically , when considering a testing problem with an embedded model , @xmath14 , and a nuisance parameter @xmath15 , i.e.  when @xmath16 can be decomposed as @xmath17 and when @xmath18 , for a sampling distribution @xmath19 , the plug - in representation @xmath20 with the obvious notations for the marginal distributions @xmath21 holds under dickey s ( 1971 ) assumption that the conditional prior density of @xmath15 under the alternative model , given @xmath22 , @xmath23 , is equal to the prior density under the null hypothesis , @xmath24 , @xmath25 therefore , dickey s ( 1971 ) identity reduces the bayes factor to the ratio of the posterior over the prior marginal densities of @xmath26 under the alternative model , taken at the tested value @xmath1 . the bayes factor is thus expressed as an amount of information brought by the data and this helps in its justification as a model choice tool .",
    "( see also @xcite . )    in order to illustrate the savage  dickey representation , consider the artificial example of computing the bayes factor between the models @xmath27 and @xmath28 which is equivalent to testing the null hypothesis @xmath29 against the alternative @xmath30 when @xmath31 . in that case ,",
    "model @xmath32 clearly is embedded in model @xmath33 .",
    "we have @xmath34 and therefore @xmath35 dickey s assumption on the prior densities is satisfied , since @xmath36 therefore , since @xmath37 and @xmath38 we clearly recover the savage ",
    "dickey representation @xmath39    while the difficulty with the representation is usually addressed in terms of computational aspects , given that @xmath40 is rarely available in closed form , we argue in the current paper that the savage  dickey representation faces challenges of a deeper nature that led us to consider it a ` paradox ' .",
    "first , by considering both prior and posterior marginal distributions of @xmath26 uniquely _ under the alternative model , _ seems to indicate that the posterior probability of the null hypothesis @xmath14 is contained within the alternative hypothesis posterior distribution , even though the set of @xmath41 s such that @xmath22 has a zero probability under this alternative distribution .",
    "second , as explained in section [ sec : measure ] , an even more fundamental difficulty with assumption is that it is meaningless when examined ( as it should ) within the mathematical axioms of measure theory .    having stated those mathematical difficulties with the savage  dickey representation , we proceed to show in section [ sec : montecarl ] that similar identities hold under no constraint on the prior distributions . in section",
    "[ sec : montecarl ] , we derive computational algorithms that exploit these representations to approximate the bayes factor , in an approach that differs from the earlier solution of @xcite .",
    "the paper concludes with an illustration in the setting of variable selection within a probit model .",
    "when considering a standard probabilistic setting where the dominating measure on the parameter space is the lebesgue measure , rather than a counting measure , the conditional density @xmath42 is rigorously @xcite defined as the density of the conditional probability distribution or , equivalently , by the condition that @xmath43 for all measurable sets @xmath44 , when @xmath45 is the associated marginal density of @xmath26 .",
    "therefore , this identity points out the well - known fact that the conditional density function @xmath42 is defined up to a set of measure zero both in @xmath15 for _ every _ value of @xmath26 _ and _ in @xmath26 .",
    "this implies that changing arbitrarily the value of the _ function _",
    "@xmath46 for a negligible collection of values of @xmath26 does not impact the properties of the conditional distribution .    in the setting where the savage ",
    "dickey representation is advocated , the value @xmath1 to be tested is not determined from the observations but it is instead given in advance since this is a testing problem . therefore the density function @xmath47 may be chosen in a _ completely arbitrary _ manner and there is no possible reason for a unique representation of @xmath23 that can be found within measure theory .",
    "this implies that there always is a version of the conditional density @xmath23 such that dickey s ( 1971 ) condition is satisfied  as well as , conversely , there are an infinity of versions for which it is _ not _ satisfied. as a result , from a mathematical perspective , condition can not be seen as an _ assumption _ on the prior @xmath48 without further conditions , contrary to what is stated in the original @xcite and later in @xcite , @xcite and @xcite .",
    "this difficulty is the first part of what we call the _ savage  dickey paradox _",
    ", namely that , as stated , the representation relies on a mathematically void constraint on the prior distribution . in the specific case of the artificial example introduced above",
    ", the choice of the conditional density @xmath23 is therefore arbitrary : if we pick for this density the density of the @xmath49 distribution , there is agreement between @xmath23 and @xmath24 , while , if we select instead the function @xmath50 , which is not a density , there is no agreement in the sense of condition .",
    "the paradox is that this disagreement has no consequence whatsoever in the savage ",
    "dickey representation .    the second part of the savage ",
    "dickey paradox is that the representation is solely valid for a specific and unique choice of a version of the density for both the conditional density @xmath23 and the joint density @xmath51 . when looking at the derivation of , the choices of some specific versions of those densities are indeed noteworthy : in the following development , @xmath52 } \\\\",
    "& = \\dfrac{\\int \\pi_1(\\psi|\\theta_0 ) f(x|\\theta_0,\\psi)\\,\\text{d}\\psi\\,\\pi_1(\\theta_0 ) }                { \\int \\pi_1(\\theta,\\psi)f(x|\\theta,\\psi)\\,\\text{d}\\psi\\text{d}\\theta\\,\\pi_1(\\theta_0 ) }            & & \\text{[using a specific version of $ \\pi_1(\\psi|\\theta_0)$]}\\\\            & = \\dfrac{\\int \\pi_1(\\theta_0,\\psi ) f(x|\\theta_0,\\psi)\\,\\text{d}\\psi}{m_1(x)\\pi_1(\\theta_0 ) }            & & \\text{[using a specific version of $ \\pi_1(\\theta_0,\\psi)$]}\\\\            & = \\dfrac{\\pi_1(\\theta_0|x)}{\\pi_1(\\theta_0)}\\,,&&\\text{[using a specific version of $ \\pi_1(\\theta_0|x)$]}\\end{aligned}\\ ] ] the second equality depends on a specific choice of the version of @xmath23 but not on the choice of the version of @xmath53 , while the third equality depends on a specific choice of the version of @xmath54 as equal to @xmath55 , thus related to the choice of the version of @xmath53 . the last equality leading to the savage ",
    "dickey representation relies on the choice of a specific version of @xmath56 as well , namely that the constraint @xmath57 holds , where the right hand side is equal to the bayes factor @xmath8 and is therefore independent from the version .",
    "this rigorous analysis implies that the savage ",
    "dickey representation is tautological , due to the availability of a version of the posterior density that makes it hold .    as an illustration , consider once again the artificial example above .",
    "as already stressed , the value to be tested @xmath58 is set prior to the experiment .",
    "thus , without modifying either the prior distribution under model @xmath33 or the marginal posterior distribution of the parameter @xmath26 under model @xmath33 , and in a completely rigorous measure - theoretic framework , we can select @xmath59 for that choice , we obtain @xmath60 hence , for this specific choice of the densities , the savage  dickey representation does not hold .",
    "@xcite have proposed a generalisation of the savage ",
    "dickey density ratio when the constraint ( [ eq : savage ] ) on the prior densities is not verified ( we stress again that this is a mathematically void constraint on the respective prior distributions ) .",
    "@xcite state that @xmath61}\\\\             & = \\pi_1(\\theta_0|x)\\dfrac{\\int",
    "\\pi_0(\\psi ) f(x|\\theta_0,\\psi)\\,\\text{d}\\psi}{m_1(x)\\pi_1(\\theta_0|x ) }           & \\qquad & \\text{[for any version of $ \\pi_1(\\theta_0|x)$]}\\\\            & = \\pi_1(\\theta_0|x)\\int\\dfrac{\\pi_0(\\psi ) f(x|\\theta_0,\\psi)}{m_1(x)\\pi_1(\\theta_0|x ) }          \\dfrac{\\pi_1(\\psi|\\theta_0)}{\\pi_1(\\psi|\\theta_0)}\\,\\text{d}\\psi & \\qquad & \\text{[for any version of $ \\pi_1(\\psi|\\theta_0)$]}\\\\            & = \\pi_1(\\theta_0|x)\\int\\dfrac{\\pi_0(\\psi)}{\\pi_1(\\psi|\\theta_0)}\\,\\dfrac{f(x|\\theta_0,\\psi )           \\pi_1(\\psi|\\theta_0)\\,\\text{d}\\psi}{m_1(x)\\pi_1(\\theta_0|x)}\\,\\dfrac{\\pi_1(\\theta_0)}{\\pi_1(\\theta_0 ) }           & \\qquad & \\text{[for any version of $ \\pi_1(\\theta_0)$]}\\\\            & = \\dfrac{\\pi_1(\\theta_0|x)}{\\pi_1(\\theta_0)}\\,\\int\\dfrac{\\pi_0(\\psi)}{\\pi_1(\\psi|\\theta_0)}\\ , \\pi_1(\\psi|\\theta_0,x)\\,\\text{d}\\psi          & \\qquad & \\text{[for a specific version of $ \\pi_1(\\psi|\\theta_0,x)$]}\\\\            & = \\dfrac{\\pi_1(\\theta_0|x)}{\\pi_1(\\theta_0)}\\,\\mathbb{e}^{\\pi_1(\\psi|x,\\theta_0 ) }            \\left[\\dfrac{\\pi_0(\\psi)}{\\pi_1(\\psi|\\theta_0)}\\right]\\,.\\end{aligned}\\ ] ] this representation of @xcite therefore remains valid for any choice of versions for @xmath56 , @xmath53 , @xmath23 , provided the conditional density @xmath62 is defined by @xmath63which obviously means that the verdinelli  wasserman representation @xmath64\\ ] ] is dependent on the choice of a version of @xmath53 .",
    "we now establish that an alternative representation of the bayes factor is available and can be exploited towards approximation purposes . when considering the bayes factor @xmath65 where the right hand side obviously is independent of the choice of the version of @xmath53 , the numerator can be seen as involving a specific version in @xmath22 of the marginal posterior density @xmath66 which is associated with the alternative prior @xmath67 .",
    "indeed , this density @xmath68 appears as the marginal posterior density of the posterior distribution defined by the density @xmath69 where @xmath70 is the proper normalising constant of the joint posterior density . in order to guarantee a savage  dickey - like representation of the bayes factor , the appropriate version of the marginal posterior density in @xmath22 , @xmath71 ,",
    "is obtained by imposing @xmath72 where , once again , the right hand side of the equation is uniquely defined .",
    "this constraint amounts to imposing that bayes theorem holds in @xmath22 instead of almost everywhere ( and thus not necessarily in @xmath22 ) .",
    "it then leads to the alternative representation @xmath73 which holds for any value chosen for @xmath53 provided condition applies .",
    "this new representation may seem to be only formal , since both @xmath74 and @xmath70 are usually unavailable in closed form , but we can take advantage of the fact that the bridge sampling identity of @xcite ( see also @xcite ) gives an unbiased estimator of @xmath75 since @xmath76 =    \\mathbb{e}^{\\pi_1(\\theta,\\psi|x ) } \\left [ \\dfrac{\\pi_0(\\psi)}{\\pi_1(\\psi|\\theta ) } \\right ] = \\dfrac{\\tilde m_1(x)}{{m}_1(x)}\\,.\\ ] ] in conclusion , we obtain the representation @xmath77\\ , , \\label{eq : mr09}\\ ] ] whose expectation part is uniquely defined ( in that it does not depend on the choice of a version of the densities involved therein ) , while the first ratio must satisfy condition .",
    "we further note that this representation clearly differs from verdinelli and wasserman s ( @xcite ) representation : @xmath78\\ , , \\label{eq : vw05}\\ ] ] since uses a specific version of the marginal posterior density on @xmath26 in @xmath1 , as well as a specific version of the full conditional posterior density of @xmath15 given @xmath1",
    "in this section , we consider the computational implications of the above representation in the specific case of latent variable models , namely under the practical possibility of a data completion by a latent variable @xmath79 such that @xmath80 when @xmath81 is available in closed form , including the normalising constant .",
    "we first consider a computational solution that approximates the bayes factor based on our novel representation ( [ eq : mr09 ] ) .",
    "given a sample @xmath82 simulated from ( or converging to ) the augmented posterior distribution @xmath83 , the sequence @xmath84 converges to @xmath71 in @xmath85 under the following constraint on the selected version of @xmath86 used therein : @xmath87 which again amounts to imposing that bayes theorem holds in @xmath22 for @xmath88 rather than almost everywhere .",
    "( note once more that the right hand side is uniquely defined , i.e.  that it does not depend on a specific version . ) therefore , provided iid or mcmc simulations from the joint target @xmath83 are available , the converging approximation to the bayes factor @xmath8 is then @xmath89 ( we stress that the simulated sample is produced for the artificial target @xmath83 rather than the true posterior @xmath90 if @xmath91 . )",
    "moreover , if @xmath92 is a sample independently simulated from ( or converging to ) @xmath93 , then @xmath94 is a convergent and unbiased estimator of @xmath95 .",
    "therefore , the computational solution associated to our representation of @xmath8 leads to the following unbiased estimator of the bayes factor : @xmath96 note that @xmath97 =    \\mathbb{e}^{\\tilde\\pi_1(\\theta,\\psi|x ) } \\left [ \\dfrac{\\pi_1(\\psi|\\theta ) } { \\pi_0(\\psi ) } \\right ] = \\dfrac{m_1(x)}{\\tilde{m}_1(x)}\\ ] ] implies that @xmath98 is another convergent ( if biased ) estimator of @xmath95 .",
    "the availability of two estimates of the ratio @xmath95 is a major bonus from a computational point of view since the comparison of both estimators may allow for the detection of infinite variance estimators , as well as for coherence of the approximations .",
    "the first approach requires two simulation sequences , one from @xmath99 and one from @xmath93 , but this is a void constraint in that , if @xmath10 is rejected , a sample from the alternative hypothesis posterior will be required no matter what .",
    "although we do not pursue this possibility in the current paper , note that a comparison of the different representations ( including verdinelli and wasserman s , 1995 , as exposed below ) could be conducted by expressing them in the bridge sampling formalism @xcite .",
    "we now consider a computational solution that approximates the bayes factor and is based on @xcite s representation ( [ eq : vw05 ] ) . given a sample @xmath100 simulated from ( or converging to ) @xmath90",
    ", the sequence @xmath101 converges to @xmath56 under the following constraint on the selected version of @xmath102 used there : @xmath103 moreover , if @xmath104 is a sample generated from ( or converging to ) @xmath105 , the sequence @xmath106 is converging to @xmath107\\ ] ] under the constraint @xmath108therefore , the computational solution associated to the @xcite s representation of @xmath8 ( [ eq : vw05 ] ) leads to the following unbiased estimator of the bayes factor : @xmath109 although , at first sight , the approximations and may look very similar , the simulated sequences used in both approximations differ : the first average involves simulations from @xmath83 and from @xmath90 , respectively , while the second average relies on simulations from @xmath90 and from @xmath105 , respectively .",
    "although our purpose in this note is far from advancing the superiority of the savage  dickey type representations for bayes factor approximation , given the wealth of available solutions for embedded models @xcite , we briefly consider an example where both verdinelli and wasserman s ( 1995 ) and our proposal apply .",
    "the model is the bayesian posterior distribution of the regression coefficients of a probit model , following the prior modelling adopted in @xcite that extends @xcite s ( 1971 ) @xmath110-prior to generalised linear models .",
    "we take as data the pima indian diabetes study available in r @xcite dataset with 332 women registered and build a probit model predicting the presence of diabetes from three predictors , the glucose concentration , the diastolic blood pressure and the diabetes pedigree function , assessing the impact of the diabetes pedigree function , i.e.  testing the nullity of the coefficient @xmath26 associated to this variable . for more details on the statistical and computational issues , see @xcite since this paper relies on the pima indian probit model as benchmark .",
    "this probit model is a natural setting for completion by a truncated normal latent variable @xcite .",
    "we can thus easily implement a gibbs sampler to produce output from all the posterior distributions considered in the previous section . besides , in that case , the conditional distribution @xmath111 is a normal distribution with closed form parameters .",
    "it is therefore straightforward to compute the unbiased estimators and .",
    "figure [ fig : bfbsmrvwchiis ] compares the variation of this approximation with other standard solutions covered in @xcite for the same example , namely the regular importance sampling approximation based on the mle asymptotic distribution , chib s version based on the same completion , and a bridge sampling @xcite solution completing @xmath112 with the full conditional being derived from the conditional mle asymptotic distribution .",
    "the boxplots are all based on 100 replicates of @xmath113 simulations . while the estimators and are not as accurate as chib s version and as the importance sampler in this specific case , their variabilities remain at a reasonable order and are very comparable .",
    "the r code and the reformated datasets used in this section are available at the following address : ` http://www.math.univ-montp2.fr/~marin/savage/dickey.html ` .",
    "comparison of the variabilities of five approximations of the bayes factor evaluating the impact of the diabetes pedigree covariate upon the occurrence of diabetes in the pima indian population , based on a probit modelling .",
    "the boxplots are based on @xmath114 replicas and the savage  dickey representation proposed in the current paper is denoted by mr , while verdinelli and wasserman s ( 1995 ) version is denoted by vw . ]",
    "the authors are grateful to h.  doss and j.  rousseau for helpful discussions , as well as to m.  kilbinger for bringing the problem to their attention .",
    "comments from the editorial team were also most useful to improve our exposition of the savage  dickey paradox .",
    "the second author also thanks geoff nicholls for pointing out the bridge sampling connection at the crism workshop at the university of warwick , may 31 , 2010 .",
    "this work had been supported by the agence nationale de la recherche ( anr , 212 , rue de bercy 75012 paris ) through the 2009 - 2012 project bigmc ."
  ],
  "abstract_text": [
    "<S> when testing a null hypothesis @xmath0 in a bayesian framework , the savage  </S>",
    "<S> dickey ratio @xcite is known as a specific representation of the bayes factor @xcite that only uses the posterior distribution under the alternative hypothesis at @xmath1 , thus allowing for a plug - in version of this quantity . </S>",
    "<S> we demonstrate here that the savage  </S>",
    "<S> dickey representation is in fact a generic representation of the bayes factor and that it fundamentally relies on specific measure - theoretic versions of the densities involved in the ratio , instead of being a special identity imposing some mathematically void constraints on the prior distributions . </S>",
    "<S> we completely clarify the measure - theoretic foundations of the savage  </S>",
    "<S> dickey representation as well as of the later generalisation of @xcite . </S>",
    "<S> we provide furthermore a general framework that produces a converging approximation of the bayes factor that is unrelated with the approach of @xcite and propose a comparison of this new approximation with their version , as well as with bridge sampling and chib s approaches . </S>"
  ]
}