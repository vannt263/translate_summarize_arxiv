{
  "article_text": [
    "how a coherent lexicon can emerge in a group of interacting agents is a major open issue in the language evolution and acquisition research area ( hurford , 1989 ; nowak & krakauer , 1999 ; steels , 2002 ; kirby , 2002 ; smith , kirby , & brighton , 2003 ) .",
    "in addition , the dynamics in the self - organization of shared lexicons is one of the issues to which computational and mathematical modeling can contribute the most , as the emergence of a lexicon from scratch implies some type of self - organization and , possibly , threshold phenomenon .",
    "this can not be completely understood without a thorough exploration of the parameter space of the models ( baronchelli , felici , loreto , caglioli , & steels , 2006 ) .",
    "there are two main research avenues to investigate the emergence or bootstrapping of a lexicon .",
    "the first approach , inspired by the seminal work of pinker and bloom ( 1990 ) who argued that natural selection is the main design principle to explain the emergence and complex structure of language , resorts to evolutionary algorithms to evolve the shared lexicon .",
    "the key element here is that an improvement on the communication ability of an individual results , in average , in an increase of the number of offspring it produces ( hurford , 1989 ; nowak & krakauer , 1999 ; cangelosi , 2001 ; fontanari & perlovsky , 2007 , 2008 ) .",
    "the second research avenue , which we will follow in this paper , argues for a culturally based view of language evolution and so it assumes that the lexicons are acquired and modified solely through learning during the individual s lifetime ( steels , 2002 ; smith , kirby , & brighton , 2003 ) .    of course , if there is a fact about language which is uncontroversial , it is that the lexicon must be learned from the active or passive interaction between children and language - proficient adults",
    ". the issue of whether this ability to learn the lexicon is due to some domain - general learning mechanism , or is an innate ability , unique to humans , is still on the table ( bates & elman , 1996 ) . in the problem",
    "we address here , there is simply no language - proficient individuals , so it is not so far - fetched to put forward a biological rather than a cultural explanation for the emergence of a self - organized lexicon .",
    "nevertheless , in this contribution we will use many insights produced by research on language acquisition by children ( see , e.g. , gleitman , 1990 ; bloom , 2000 ) to study different learning strategies .    from a developmental perspective , there are basically two competing schemes for lexicon acquisition by children ( rosenthal & zimmerman , 1978 ) . the first scheme , termed cross - situational or observational learning , is based on the intuitive idea that one way that a learner can determine the meaning of a word is to find something in common across all observed uses of that word ( pinker , 1984 ; gleitman , 1990 ; siskind , 1996 ) .",
    "hence learning takes place through the statistical sampling of the contexts in which a word appears .",
    "since the learner receives no feedback about its inferences , we refer to this scheme as unsupervised learning .",
    "the second scheme , known generally as operant conditioning , involves the active participation of the agents in the learning process , with exchange of non - linguistic cues to provide feedback on the hearer inferences .",
    "this supervised learning scheme has been applied to the design of a system for communication by autonomous robots  the so - called language game in the talking heads experiments ( steels , 2003 ) .",
    "despite the technological appeal , the empirical evidence is that most part of the lexicon is acquired by children as a product of unsupervised learning ( pinker , 1984 ; gleitman , 1990 ; bloom , 2000 ) .",
    "interestingly , from the perspective of evolving or bootstrapping a lexicon , the unsupervised scheme is very attractive too , since it eliminates altogether the issue of honest signaling ( dawkins & krebs , 1978 ) , as no signaling is involved in the learning process , which requires only observation and some elements of intuitive psychology ( e.g. theory of mind ) .",
    "many different computational implementations and variants of these two schemes for bootstrapping a lexicon have been proposed in the literature .",
    "for example , smith ( 2003a , 2003b ) , smith , smith , blythe , & vogt ( 2006 ) , and de beule , de vylder , & belpaeme ( 2006 ) have addressed the unsupervised learning scheme , whereas steels & kaplan ( 1999 ) , ke , minett , au , wang ( 2002 ) , smith , kirby , & brighton , ( 2003 ) , and lenaerts , jansen , tuyls , & de vylder ( 2005 ) , the supervised scheme . however , except for the extensive statistical analysis of a variant of the supervised learning algorithm which reduces the problem to that of naming a single object ( baronchelli , felici , loreto , caglioli , & steels , 2006 ) , the study of the effects of changing the parameters of those models have been usually limited to the display of the time evolution of some measure of the communication accuracy of the population . although at first sight the supervised learning scheme may seem to be clearly superior to the unsupervised one ( albeit less realistic in the context of language acquisition by children ) , we are not aware of any thorough comparison between the performances of these two learning scenarios .",
    "in fact , in this contribution we show that in a realistic limit of very large lexicon sizes the supervised and unsupervised learning performances are essentially identical .",
    "in this paper we study minimal models of the supervised and unsupervised learning schemes which preserve the main ingredients of these two classical language acquisition paradigms . for the sake of simplicity , here we interpret the lexicon as a mapping between objects and words ( or sounds ) rather than as a mapping between meanings ( conceptual structures ) and sounds .",
    "a more complete scenario would involve first the creation of meanings , i.e. , the bootstrapping of an object - meaning mapping ( steels , 1996 ; fontanari , 2006 ) and then the emergence of a meaning - sound mapping ( see , e.g. , smith , 2003a , 2003b ; fontanari & perlovsky , 2006 ) .",
    "following a common assumption in lexicon bootstrapping models , such as the popular iterated learning model ( smith , kirby , & brighton , 2003 ; brighton , smith , & kirby , 2005 ) , we consider here only two agents who play in turns the roles of speaker and hearer .",
    "the agents live in a fixed environment composed of @xmath0 objects and have @xmath1 words available to name these objects .",
    "as we are interested in the limit where @xmath0 and @xmath1 are very large with the ratio @xmath2 finite we do not need to account for the possibility of creation of new words as in some variants of the supervised learning scheme ( baronchelli , felici , loreto , caglioli , & steels , 2006 ) .",
    "we assume that each agent is characterized by a @xmath3 verbalization matrix @xmath4 the entries of which @xmath5 $ ] , with @xmath5 $ ] for all values of @xmath6 , being interpreted as the probability that object @xmath7 is associated with word @xmath8 .",
    "this assumption rules out the existence of objects without names , but it allows for words which are never used to name objects . to describe the communicative behavior of the agents through the verbalization matrix ( i.e. , the associations between objects and words for use both in production and interpretation ) we need to specify how the speaker chooses a word for any given object as well as how the hearer infers the object the speaker intended to name by that word .    to name an object , say object @xmath7",
    ", the speaker simply chooses the word @xmath9 which is associated to the largest entry of row @xmath7 of the matrix @xmath4 , i.e. , @xmath10 .",
    "in addition , to guess which object the speaker named by word @xmath8 the hearer selects the object that corresponds to the largest of the @xmath0 entries @xmath11 , @xmath12 .",
    "in other words , the hearer chooses the object that it itself would be most likely to associate with word @xmath8 ( smith , 2003a , 2003b ) .",
    "this amounts to assuming that the agents are endowed with a ` theory of mind ' ( tom ) , i.e. , that the hearer is somehow able to understand that the speaker thinks similar to itself and hence would behave likewise when facing the same situation ( donald , 1991 ) .",
    "we note that the original inference scheme , termed `` obverter '' ( oliphant & batali , 1997 ) , assumed that the hearer has access to the verbalization matrix of the speaker ( through mind reading , as the critics were ready to point out ) . here",
    "we follow the more reasonable scheme , dubbed `` introspective obverter '' ( smith , 2003a ) , which requires endowing the agents with a theory of mind rather than with telepathic abilities .",
    "effective communication takes place when the two agents reach a consensus on which word must be assigned to each object . to achieve this",
    ", we must provide a prescription to modify their initially random verbalization matrices . here",
    "we will consider two learning procedures that differ basically on whether the agents receive feedback ( supervised learning ) or not ( unsupervised learning ) about the success of a communication episode .",
    "but before doing this we need to set up the language game scenario where the agents interact .    from the list of @xmath0 objects , the agent who plays the speaker role chooses randomly @xmath13 objects without replacement .",
    "this set of @xmath13 objects forms the context .",
    "then the speaker chooses randomly one object in the context and produces the word associated to that object , according to the procedure sketched before .",
    "the hearer has access to that word as well as to the @xmath13 objects that comprise the context .",
    "its task is to guess which object in the context is named by that word .",
    "this is then an ambiguous language acquisition scenario in which there are multiple object candidates for any word .",
    "once the verbalization matrices are updated the two agents interchange the roles of speaker and hearer and a new context is generated following the same procedure .    to control the convergence properties of the learning algorithms described",
    "next we assume that the entries @xmath11 are discrete variables that can take on the values @xmath14 . in our simulations",
    "we choose @xmath15 .",
    "the reciprocal of @xmath16 can be interpreted as the algorithm learning rate .",
    "in addition , as there are two agents who alternate in the roles of speaker and hearer , henceforth we will add the superscripts i or j to the verbalization matrix in order to identify the agent it corresponds to . at the beginning of the language game",
    "each agent has a different , randomly generated verbalization matrix .",
    "more pointedly , to generate the row @xmath7 of @xmath17 we distribute with equal probability @xmath16 balls among @xmath1 slots and set the value of entry @xmath18 as the ratio between the number of balls in slot @xmath8 and the total number of balls @xmath16 .",
    "an analogous procedure is used to set the initial value of @xmath19 .      in this scheme ,",
    "the list of objects in the context @xmath20 and the accompanying word @xmath9 is the only information fed to the learning algorithm .",
    "hence , in the unsupervised scheme , only the hearer s verbalization matrix is updated .",
    "of course , since the agents change roles at each learning episode , the verbalization matrices of both agents are updated during the learning stage . for concreteness ,",
    "let us assume that agent @xmath21 is the speaker and so agent @xmath22 is the hearer in a particular learning episode . as pointed out before , the idea here is to model the cross - situational learning scenario ( siskind , 1996 ) in which the agents infer the meaning of a given word by monitoring its occurrence in a variety of contexts .",
    "accordingly , the learning procedure increases the entries @xmath23 by the amount @xmath24 .",
    "in addition , for each object in the context , say @xmath25 , a word , say @xmath8 , is chosen randomly and the entry @xmath26 is decreased by the same amount @xmath24 , thus keeping the correct normalization of the rows of the verbalization matrix .",
    "( the possibility that @xmath27 is not ruled out . )",
    "this procedure which is inspired by moran s model of population genetics ( ewens , 2004 ) guarantees a minimum disturbance in the verbalization matrix and can be interpreted as the lateral inhibition of the competing word - object associations .",
    "we note that during the learning stage the agent playing the hearer role does not need to guess which object in the context is named by word @xmath9 .",
    "an extra rule is needed to keep the entries @xmath28 within the unit interval @xmath29 $ ] : we assume that once an entry reaches the values @xmath30 or @xmath31 it becomes fixed , so the extremes of the unit interval act as absorbing barriers for the stochastic dynamics of the learning algorithm .",
    "the setting is identical to that described before except that now the hearer must guess which object in the context the speaker named by @xmath9 and then communicate its choice to the speaker ( using some nonlinguistic means , such as pointing to the chosen object ) . in turn",
    ", the speaker must provide another nonlinguistic hint to indicate which object in the context it named by word @xmath9 .",
    "let us assume that the speaker associates word @xmath9 to object @xmath25 .",
    "if the hearer s guess happens to be the correct one , then both entries @xmath32 and @xmath33 are incremented by the amount @xmath24 .",
    "furthermore , two words , say @xmath34 and @xmath35 , are chosen randomly and the entries @xmath36 and @xmath37 are decreased by @xmath24 so the normalization of row @xmath25 is preserved in both verbalization matrices .",
    "suppose now the hearer s guess is wrong , say , object @xmath38 instead of @xmath25",
    ". then both entries @xmath32 and @xmath39 are decreased by the amount @xmath24 and , as before , two words @xmath34 and @xmath35 are chosen randomly and the entries @xmath36 and @xmath40 are increased by @xmath24 . as in the unsupervised case , the extremes @xmath41 and @xmath42 are absorbing barriers .",
    "the weak point of this learning scheme is the need for nonlinguistic hints to communicate the success or failure of the communication episode .",
    "this implies that , prior to learning , the agents are already capable to communicate ( and understand ) sophisticated meanings such as success and failure and behave ( by updating their verbalization matrices ) accordingly .",
    "in fact , feedback about the outcome of the communication episode may be seen as a form of telepathic meaning transfer .",
    "simulation experiments of the two learning algorithms described above show , not surprisingly , that after a transient the two agents become identical , in the sense that they are described by the same verbalization matrix .",
    "in addition , in the case of unsupervised learning the stochastic dynamics always leads to binary verbalization matrices , i.e. , matrices whose entries @xmath11 can take on the values 1 or 0 only .",
    "of course , once the dynamics produces a binary matrix it becomes frozen .",
    "this same outcome characterizes the supervised case as well , except in the cases that the lexicon size @xmath1 is on the same order of the context size @xmath13 .",
    "however , as we focus on the regime where @xmath13 is finite and @xmath0 and @xmath1 are large we can guarantee that the stochastic dynamics leads to binary verbalization matrices regardless of the learning procedure .",
    "once the dynamics becomes frozen ( and so the learning stage is over ) we measure the average communication error @xmath43 as follows .",
    "the speaker chooses object @xmath7 from the list of @xmath0 objects and emits the corresponding word ( there is a unique word assigned to any given object , i.e. , there is a single entry 1 in any row of the verbalization matrix ) .",
    "the hearer must then infer which object is named by that word . since the same word can name many objects ( i.e. , there may be many entries 1 in a given column ) , the probability @xmath44 that the hearer s guess is correct is simply the reciprocal of the number of objects named by that word .",
    "this probability is the communication accuracy regarding object @xmath7 .",
    "the procedure is repeated for the @xmath0 objects , so the average communication error is defined as @xmath45 where @xmath46 is the average communication accuracy of the algorithm .",
    "as already pointed out , the normalization condition on the rows of the verbalization matrix @xmath4 allows for the possibility that a certain number of words are not used by the lexicon acquisition algorithms .",
    "let @xmath47 stand for the actual number of words used by those algorithms .",
    "then we can easily convince ourselves that @xmath48 simply by noting that @xmath49 when the sum is restricted to objects that are associated to the same word .",
    "finally , we note that in the definitions of these communication measures the context plays no role at all ; indeed the context is relevant only during the learning stage .",
    "it is important to estimate the optimal ( minimum ) communication error @xmath50 in our learning scenario since , in addition to being a lower bound to the communication error produced by the learning algorithms , it allows us to rate their absolute performances .",
    "for @xmath51 the optimal communication error is obtained by making a one - to - one assignment between @xmath52 words and @xmath52 objects , and then assigning the single remaining word to the remaining @xmath53 objects .",
    "this procedure yields @xmath54 .",
    "for @xmath55 we can obtain @xmath56 simply by discarding @xmath57 words and making a one - to - one word - object assignment with the other @xmath0 words .",
    "in fact , using our finding that @xmath58 we see that , as expected , the optimal performance is obtained by setting @xmath59 if @xmath51 and @xmath60 if @xmath61 .",
    "figure [ fig:1 ] shows the comparison between the optimal performance and the actual performances of the two learning algorithms as function of the ratio @xmath62 . in this , as well as in the other figures of this paper",
    ", each symbol stands for the average over @xmath63 independent samples or language games .",
    "the performance of the supervised algorithm deteriorates as the number of objects @xmath0 increases , in contrast to that of the unsupervised algorithm which actually shows a slight improvement in this case . for @xmath64 ,",
    "both algorithms produce the same communication error ( see fig .  [ fig:2 ] ) , which is shown by the solid line in fig .  [ fig:1 ] .",
    "we note that a preliminary comparative analysis of these algorithms for @xmath65 led to an incorrect claim about the general superiority of the supervised learning scheme ( fontanari & perlovsky , 2006 ) . for small values of @xmath62",
    "the performances of the two learning algorithms are practically indistinguishable from the optimal performance , but as we will argue below the algorithms actually never achieve that performance , except for @xmath66 .",
    "it is instructive to calculate the communication error in the case that the @xmath0 objects are assigned randomly to the @xmath1 words .",
    "this is a classical occupancy problem discussed at length in the celebrated book by feller ( 1968 ) . in this occupancy problem , the probability @xmath67 that the number of words @xmath68 not used in the assignment of the @xmath0 objects to the @xmath1 words ( i.e. , @xmath69 ) is @xmath70 which in the limits @xmath71 and @xmath72 reduces to the poisson distribution @xmath73 where @xmath74 remains bounded ( feller , 1968 ) .",
    "hence the average communication accuracy resulting from the random assignment of objects to words is simply @xmath75 , which yields the communication error @xmath76 surprisingly , this equation describes perfectly the communication error of the two learning algorithms in the limit @xmath64 ( solid line in fig .",
    "[ fig:1 ] ) .",
    "we note that the ( small ) discrepancy observed in fig .",
    "[ fig:2 ] for the extrapolated data of the unsupervised algorithm and the analytical prediction can be reduced to zero by decreasing the learning rate @xmath24 .",
    "equation ( [ er ] ) explains also why the performances of the algorithms are practically indistinguishable from the optimal performance for small @xmath62 , since the difference between them vanishes as @xmath77 .",
    "in addition , eq .  ( [ er ] )",
    "shows that in the limit of large @xmath62 , the communication error vanishes as @xmath78 .",
    "a word is in order about the effect of the context size @xmath13 on the performance of the two learning algorithms , since figs .",
    "[ fig:1 ] and [ fig:2 ] exhibit the results for @xmath79 only .",
    "simulations for larger values of @xmath13 show that this parameter is completely irrelevant for the performance of the supervised algorithm .",
    "of course , this is expected since regardless of the context size , at most two rows ( object labels ) of the verbalization matrices are updated .",
    "but the situation is far from obvious for the unsupervised algorithm since @xmath13 determines the number of rows to be updated in each round of the game .",
    "however , the results summarized in fig .  [ fig:3 ] for @xmath80 indicate that , despite strong finite - size effects particularly for small @xmath62 , the communication error ultimately tends to @xmath81 in the limit of large @xmath0 .",
    "in this paper we have unveiled two remarkable results . first , the supervised and unsupervised schemes for bootstrapping a lexicon yield the same communication accuracy in the limit of very large lexicon sizes . for",
    "finite lexicon sizes the supervised scheme always outperforms the unsupervised one , but its performance degrades as the lexicon size increases , whereas the performance of the unsupervised learning algorithm improves slightly with increasing lexicon size ( see fig .",
    "[ fig:1 ] ) .",
    "second , those performances tend to the communication accuracy obtained by a random occupancy problem in which the @xmath0 objects are assigned randomly to the @xmath1 words .",
    "these findings reveal a surprising inefficiency of traditional lexicon bootstrapping scenarios when evaluated in the realistic regime of very large lexicon sizes",
    ". it would be most interesting to devise sensible scenarios that reproduce the optimal communication performance or , at least , that exhibit an communication error that decays faster than the random occupancy result , @xmath82 , in the case the number of available words is much greater than the number of objects ( @xmath83 ) .",
    "the scenarios studied here are easily adapted to model the problem of lexicon acquisition ( rather than bootstrapping ) : we have just to assume that one of the agents , named the master in this case , knows the correct lexicon and so its verbalization matrix is kept fixed during the entire learning procedure ; the verbalization matrix of the other agent  the pupil  is allowed to change following the update algorithms described before ( see , e.g. , fontanari , tikhanoff , cangelosi , ilin , & perlovsky , 2009 ) .",
    "most interestingly , in this context , statistical world learning has been observed in controlled experiments involving infants ( smith & yu , 2008 ) and adults ( yu & smith , 2007 ) .",
    "similar experiments , but now aiming at bootstrapping a lexicon , could be easily carried out by replacing our virtual agents by two adults , who would then resort to some conscious or unconscious mechanism to track the co - occurrence of words and objects .",
    "of course , the very emergence of pidgin - a means of communication between two or more groups which lack a common language ( thomason & kaufman , 1988 ) - can be seen as a realization of such an experiment and serves as additional justification for the study of lexicon bootstrapping .",
    "the research at so carlos was supported in part by cnpq , fapesp and soard grant fa9550 - 10 - 1 - 0006 .",
    "j.f.f . thanks the hospitality of the adaptive behaviour & cognition research group , university of plymouth , where this research was initiated .",
    "the visit was supported by eucognition.org travel grant na-097 - 6 .",
    "cangelosi also acknowledges the contribution of the italk project from the european commission ( fp7 ict cognitive systems and robotics ) .",
    "baronchelli , a. , felici , m. , loreto , v. , caglioli , e. , & steels , l. ( 2006 ) .",
    "sharp transition towards shared vocabularies in multi - agent systems .",
    "_ journal of statistical mechanics _ ,",
    "p06014 .",
    "de beule , j. , de vylder , b. , & belpaeme , t. ( 2006 ) . a cross - situational learning algorithm for damping homonymy in the guessing game . in l.m .",
    "rocha , m. bedau , d. floreano , r. goldstone , a. vespignani , & l. yaeger ( eds . ) , _ proceedings of the xth conference on artificial life _ ( pp .",
    "466 - 472 ) .",
    "cambridge , ma : mit press .",
    "dawkins , r. , & krebs , j.r .",
    "animal signals : information or manipulation ? in :",
    "j.r . krebs , & n. b. davies ( eds . ) , _ behavioural ecology : an evolutionary approach _ ( pp . 282 - 309 ) .",
    "oxford , uk : blackwel scientific publications .",
    "fontanari , j.f . , & perlovsky , l.i .",
    "( 2006 ) . meaning creation and communication in a community of agents . in _ proceedings of the 2006 international joint conference on neural networks _",
    "2892 - 2897 ) .",
    "piscataway , nj : ieee press .",
    "steels , l. , & kaplan , f. ( 1999 ) . situated grounded word semantics . in",
    "_ proceedings of the sixteenth international joint conferences on artificial intelligence _",
    "( pp . 862 - 867 ) . san francisco , ca : morgan kauffman ."
  ],
  "abstract_text": [
    "<S> scenarios for the emergence or bootstrap of a lexicon involve the repeated interaction between at least two agents who must reach a consensus on how to name @xmath0 objects using @xmath1 words . </S>",
    "<S> here we consider minimal models of two types of learning algorithms : cross - situational learning , in which the individuals determine the meaning of a word by looking for something in common across all observed uses of that word , and supervised operant conditioning learning , in which there is strong feedback between individuals about the intended meaning of the words . despite the stark differences between these learning schemes , </S>",
    "<S> we show that they yield the same communication accuracy in the realistic limits of large @xmath0 and @xmath1 , which coincides with the result of the classical occupancy problem of randomly assigning @xmath0 objects to @xmath1 words . </S>"
  ]
}