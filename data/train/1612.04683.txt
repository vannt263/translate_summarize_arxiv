{
  "article_text": [
    "in translation industry , a new frontier for mt is to implement generic systems that adapt on - the - fly to any context . once fuelled with enough data and bootstrapped , they work without any further training iterations and translates sentences in a domain sensitive way , without any need of prior identification of or adaptation to the domain",
    ".    the mt architecture we are working on does not rely on the current paradigm of handling and using generic and domain - specific training data in a machine translation system .",
    "it assumes that domain information is not defined a priori or attached to the data , but instead that any subset of data collected and ingested by the system might become relevant at some point during the use of the system .",
    "domain relevance is based on matching the input sentence to translate , together with some of its context , against all available data .",
    "the result is a probability distribution over the domains , which is used to activate , with proper weights , the underlying domain specific models .    even holding the prompt reaction constraint guaranteed to users by our architecture , any background processing",
    "is allowed in order to further boost the system behaviour .",
    "in fact , among the others , two relevant issues are still open .",
    "one is the scalability over the number of `` domains '' that can be efficiently handled ; the number can grow very fast since for us a `` domain '' is the specific data provided by each customer .",
    "another is related to those customers who provide not enough training data for bootstrapping reliable models . in order to keep manageable the cardinality of domains and , if legally possible , exploit at best all the data previously ingested by the system ,",
    "a natural choice is to aggregate similar domains .    in this paper",
    ", we report on domain clustering in the ambit of the mt architecture sketched above .",
    "we will provide an empirical positive answer to the two questions induced by the just mentioned issues : ( i )  is clustering able to aggregate so many domains that with just few cluster - specific models the mt quality remains adequate ? ( ii )  is clustering able to aggregate domains with few training data such that the overall performance improves ?",
    "after the overviews on domain adaptation literature ( section  [ sec : related ] ) and on consolidated scientific knowledge on data clustering ( section  [ sec : clstalg ] ) , the hierarchical agglomerative clustering actually implemented is described in section  [ sec : hac ] ; five different similarity measures of clusters are proposed in section  [ sec : distances ] and experimentally compared in section  [ sec : exp ] , according to the framework defined in section  [ sec : mmt ] . a discussion , the summary and the list of investigations planned for the future end the paper .",
    "for optimising performance , the models of machine translation systems are often specialised on specific domains , like legal , information technology or medicine .",
    "the specialisation is obtained by training models on text from the specific domain .",
    "specialised texts can be gathered either by exploiting supervision or through automatic selection from general texts . deciding if a sentence belongs to a given domain",
    "can be done by checking how well it is predicted by a domain specific language model ( through the perplexity ) , by the tf / idf method commonly employed in information retrieval , or by the cross - entropy difference method presented in  @xcite .    instead of discarding part of the training data ,",
    "multiple models can be trained on the various domains .",
    "multiple domain - specific models can be loaded in mt engines that receive input from different domains ; the input is then classified and the proper model activated ; for example , in  @xcite the classification is done using either language models or information retrieval methods .    another way to exploit multiple domain - specific models is the mixture - model approach which combines the various models , properly weighting each of them  @xcite .",
    "typically , the combination is realised as a linear or log - linear model and can involve language , translation and even alignment models .",
    "the interpolation weights can be estimated off - line on a development set or on the source side of the whole test set . on - line estimation",
    "is also feasible on the current sentence ( or bunch of sentences ) to translate  @xcite ; in the ambit of cat and interactive mt , the availability of user corrections allows a promptly and really effective adaptation of the weights  @xcite .",
    "if proper meta - information is available , a supervised partitioning of the training data into domains is allowed .",
    "unfortunately , that is a rather rare case .",
    "more commonly , unsupervised clustering is needed , as in  @xcite , where self - organizing map is used to create auxiliary language models , the most appropriate of which is selected on - the - fly for each document to translate .",
    "in the following , some excerpts from  @xcite are fused to provide a brief introduction to clustering algorithms .",
    "clustering algorithms group a set of documents into subsets or clusters .",
    "the algorithms goal is to create clusters that are coherent internally , but clearly different from each other . in other words , documents within a cluster",
    "should be as similar as possible ; and documents in one cluster should be as dissimilar as possible from documents in other clusters .",
    "clustering is the most common form of unsupervised learning .",
    "no supervision means that there is no human expert who has assigned documents to classes .",
    "clustering algorithms can be flat or hierarchical .",
    "flat clustering creates a flat set of clusters without any explicit structure that would relate clusters to each other .",
    "hierarchical clustering creates a hierarchy of clusters .",
    "flat clustering algorithms are efficient and conceptually simple , but have a number of drawbacks .",
    "in addition to return a flat unstructured set of clusters , they typically require a pre - specified number of clusters as input and are nondeterministic . on the contrary ,",
    "hierarchical clustering outputs a hierarchy , a structure that is more informative than the unstructured set of clusters returned by flat clustering .",
    "hierarchical clustering does not require to pre - specify the number of clusters and most hierarchical algorithms are deterministic .",
    "these advantages of hierarchical clustering come at the cost of lower efficiency .",
    "the most common hierarchical clustering algorithms have a complexity that is at least quadratic in the number of documents compared to the linear complexity of k - means , one widely used flat clustering algorithm .",
    "hierarchical clustering algorithms are either top - down or bottom - up .",
    "top - down clustering requires a method for splitting a cluster and proceeds by splitting clusters recursively until individual documents are reached .",
    "bottom - up algorithms treat each document as a singleton cluster at the outset and then successively merge ( or agglomerate ) pairs of clusters until all clusters have been merged into a single cluster that contains all documents .",
    "bottom - up hierarchical clustering is therefore called hierarchical agglomerative clustering ( hac ) .",
    "hac algorithms employ a similarity measure for deciding which clusters to merge ; common similarity measures are : single - link , complete - link , group - average , and centroid similarity .    in single - link clustering ,",
    "the similarity of two clusters is the similarity of their most similar members .",
    "this single - link merge criterion is local .",
    "attention is solely paid to the area where the two clusters come closest to each other .",
    "other , more distant parts of the cluster and the clusters overall structure are not taken into account .    in complete - link clustering ,",
    "the similarity of two clusters is the similarity of their most dissimilar members .",
    "this is equivalent to choosing the cluster pair whose merge has the smallest diameter .",
    "this complete - link merge criterion is non - local ; the entire structure of the clustering can influence merge decisions .",
    "this results in a preference for compact clusters with small diameters over long , straggly clusters , but also causes sensitivity to outliers .",
    "group - average agglomerative clustering evaluates cluster quality based on all similarities between documents , thus avoiding the pitfalls of the single - link and complete - link criteria , which equate cluster similarity with the similarity of a single pair of documents .",
    "group - average similarity computes the average similarity of all pairs of documents , including pairs from the same cluster ( but self - similarities ) .    in centroid clustering ,",
    "the similarity of two clusters is defined as the similarity of their centroids .",
    "centroid similarity is equivalent to average similarity of all pairs of documents from different clusters .",
    "thus , the difference between the group - average similarity and the centroid similarity is that the former considers all pairs of documents in computing average pairwise similarity whereas the latter excludes pairs from the same cluster .",
    "an unsupervised clustering can be evaluated in two ways : intrinsically , according to properties of the clusters , or extrinsically , according to the performance on a task which uses the clustering .    for intrinsic evaluation",
    ", the silhouette coefficient  @xcite can be used , which measures how similar an object is to its own cluster ( cohesion ) compared to other clusters ( separation ) .    for each datum",
    "_ i _ , let _",
    "a(i ) _ be the average dissimilarity of _ i _ with all other data within the cluster which _ i _ belongs to .",
    "_ a(i ) _ can be interpreted as how well _ i _ is assigned to its cluster ( the smaller the value , the better the assignment ) .",
    "the average dissimilarity of _ i _ to a generic cluster _",
    "c _ is defined as the average distance from _",
    "i _ to all points in _",
    "b(i ) _ be the lowest average dissimilarity of _ i _ to any other cluster , of which _",
    "i _ is not a member .",
    "the cluster with this lowest average dissimilarity is said to be the `` neighbouring cluster '' of  _ i _ because it is the next best fit cluster for   _ i_. the silhouette value for _ i _ is defined as :    @xmath0    from the definition , it results that : @xmath1    for _ s(i ) _ to be close to 1 it is required that @xmath2 . as _",
    "a(i ) _ is a measure of how dissimilar _",
    "i _ is to its own cluster , a small value means it is well matched .",
    "furthermore , a large _ b(i ) _ implies that _ i _ is badly matched to its neighbouring cluster .",
    "s(i ) _ close to one means that the datum is appropriately clustered . if _",
    "s(i ) _ is close to negative one , then by the same logic it is seen that _",
    "i _ would be more appropriate if it was clustered in its neighbouring cluster .",
    "s(i ) _ near zero means that the datum is on the border of the two clusters .",
    "note that when a cluster contains only a single object _ i _ , _",
    "a(i ) _ can not be defined ; following  @xcite , we simply set _",
    "s(i ) _ to zero , an arbitrary but neutral choice .",
    "the average _",
    "s(i ) _ over all data of a cluster is a measure of how tightly grouped all the data in the cluster are , while the average _",
    "s(i ) _ over all data of the entire dataset is a measure of how appropriately the data have been clustered .",
    "algorithm  [ alg : clstrng ] shows the pseudo - code of the hierarchical agglomerative procedure we have implemented .",
    "two functions play the main role , namely d ( ) , which computes somehow the `` distance '' between two clusters , and evaluate ( ) , which , given a clustering , provides a score of its quality .",
    "q [ ] is a data structure for storing triples @xmath3 where @xmath4 is the distance between the @xmath5 and the @xmath6 clusters ; if d ( ) is really a distance ( and hence identity of indiscernibles and symmetry are satisfied conditions ) , q [ ] is a strict ( upper or lower ) triangular matrix .",
    "output @xmath7 +    the algorithm takes as input the set of @xmath8 documents to cluster .",
    "each of them is considered as a single cluster , hence the size of the initial clustering is @xmath8 .",
    "the distances between any pair of documents are computed and stored in the entries above the main diagonal of the matrix q [ ] . before starting to iterate ,",
    "the quality of the initial clustering is output .    at each iteration ,",
    "the two closest clusters are identified ( @xmath9 operation performed on @xmath10 $ ] , the first component of triples @xmath11 ) ; their rows and columns in q [ ] are removed ; then , they are merged ( @xmath12 ) and the smallest of their two indexes is assigned to the new cluster ; finally , the distance of the new cluster from any other cluster is computed and stored in the upper part of q [ ] . before ending the iteration , the quality of the new clustering is output .    the algorithm is suboptimal , since the local decision to merge the two current closest clusters can not be backtracked .",
    "moreover , no stopping criterion is designed , hence at the end all initial clusters are merged into a single cluster ; that allows to have plots covering the whole range of clusterings in between the two extremes ( @xmath8 and @xmath13 clusters ) , as we will see . on the other side ,",
    "an ending rule could be easily devised looking at the value of @xmath14 , for example comparing it to some threshold .      in our context , the single points to be clustered are translation memories ( tms ) , that is collections of sentence pairs .",
    "the distance @xmath15 between the target sides of two tms @xmath16 and @xmath17 can be measured by means of the cross - perplexity : @xmath18 where @xmath19 is the target text in the tm @xmath20 , @xmath21 is the language model estimated on @xmath19 and @xmath22 is the perplexity of @xmath23 measured on @xmath24 , which indicates how well the probability distribution @xmath23 predicts the text @xmath24 .",
    "@xmath25 indicates the proper sum of perplexities .    as seen in section  [ sec : clstalg ] , the distance between clusters with more than one tm can be computed by measuring the similarity of two single tms : the closest in the single - link case , the farthest in the complete - link .",
    "hereafter , they will be indicated as @xmath26 and @xmath27 , respectively .",
    "group - average and centroid are instead similarity measures which involve all the points of clusters . given the peculiarity of our case , instead of group - average or centroid , in order to involve all data of clusters , a natural choice is to really merge tms : when @xmath16 and @xmath17 are clustered , instead of considering the new cluster as a collection of two separate tms , it is though as a new single tm @xmath28 which is the concatenation of @xmath16 and @xmath17 .",
    "this way , the distance @xmath29 defined above can be computed for any pair of clusters generated by the agglomerative algorithm .    a variant of @xmath30 consists in looking ahead the impact of merging a pair of clusters by computing :    @xmath31    @xmath32    that is the difference between the perplexity on @xmath28 of the lm estimated on it and the cumulative perplexity on @xmath16 and @xmath17 of the two corresponding lms .",
    "the smaller the difference , the more convenient is to merge the two clusters .",
    "@xmath33 is more expensive than @xmath30 because at each iteration it requires to train and evaluate @xmath34 for any pair @xmath35 , while the latter just for the pair selected by the @xmath9 operation .    as described in section  [ sec : mmt ] , given a source document , the module of our system named context analyser ( ca ) generates a domain distribution vector .",
    "it can then be exploited to measure the distance among clusters by means of : @xmath36    where @xmath37 is the discrete distribution provided by the ca over the document @xmath24 .",
    "the rationale behind @xmath38 is that the more @xmath17 ( @xmath16 ) suits the context @xmath16 ( @xmath17 ) , the higher @xmath39 and then the lower @xmath40 .",
    "note that all the above @xmath41 are defined over either the target or the source side , but they hold for the opposite side as well .",
    "moreover , the computation of the distance @xmath42 in algorithm  [ alg : clstrng ] involves the training of new lms in all the cases but @xmath43 and @xmath27 , for which the values stored in @xmath44 during the initialisation phase can be reused in any iteration , making those two distances definitely more efficient than the others .",
    "the mmt project , described in  @xcite , features an on - line domain adaptation .",
    "a context analyser is employed whose training consists in the creation of a database built on the source side of training data alongside the domain provenance meta - information . at translation time , given a source text window or an entire document , the context analyser generates a domain distribution vector including the top matching domains available in the training data .",
    "the vector is passed on to the mt engine that will properly adapt the translation and language models to the input document : in the former case , by biasing the sampling of translation pairs in the suffix array  @xcite , while in the latter case , by linearly combining domain specific language models .",
    "textual data in mmt is divided into `` domains '' . from the commercial translation service provider",
    "s point of view , the most straightforward manifestation of `` domain '' is the customer - specific tm : the archive of all documents translated by the provider for a specific customer .",
    "large translation clients can use product- or business - area - specific tms , but it happens that tm contents are heterogeneous ; therefore , the mmt concept of `` domain '' differs from the usual meaning given to the word `` domain '' .",
    "documents were collected from the two major sources of tms available to mmt : the taus data cloud and translated s mymemory .",
    "details are provided in  @xcite .    for mt evaluation purposes ,",
    "an english - italian benchmark was built .",
    "it includes the 30 largest tms from the mymemory database ; the provenance of the documents varies from software documentation to legal documents and advertising . from the taus data cloud , 33 further tms were also added .",
    "this benchmark will be referred to henceforth as benchmark 1.1 .",
    "data in benchmark 1.1 was split into training , development and test sets . in order to evaluate the performance of the translation engine in a real scenario ,",
    "the final composition of development and test sets includes all 30 domains ( i.e. translation memories ) from mymemory and a selection of 10 domains ( translation memories ) from taus data cloud .",
    "table  [ tab : resource : bilingual ] provides statistics on bilingual data sets ; figures refer to untokenized texts .",
    ".statistics on bilingual resources .",
    "[ cols=\"^,^,^,^\",options=\"header \" , ]",
    "different instances of algorithm  [ alg : clstrng ] , one for each distance defined in section  [ sec : distances ] , were run to cluster the 40 tms of the test set ; processing times are reported in table  [ tab : proctime ] . in the following sections , first the dendrograms",
    "visualize how the various instances of the algorithm perform the clustering step by step ; then , intrinsic and extrinsic evaluations are provided .",
    "c14mmc19mmc15mmc14 mm @xmath45 & @xmath46 & @xmath47 & @xmath48 + 15 & 10 & 100 & 50 +      figures  [ fig : dnd : pp ] and  [ fig : dnd : singlelink ] show the dendrograms relative to four ( out of five ) distances .",
    "each horizontal line segment indicates the merging of two clusters ; the length of the vertical line segments incident to the extremes of the horizontal line segment is proportional to the distance between the merged cluster and each cluster to merge : the shorter , the more convenient the merging ; viceversa , the longer , the better to avoid the merging . with this key",
    ", the dendrograms can be read as follows : @xmath49 , @xmath45 and @xmath47 show a growing , but anyway interesting , ability in grouping original tms in few , compact clusters .",
    "it is a matter of fact that such ability increases with the computational cost ( cf .",
    "table  [ tab : proctime ] ) . on the contrary",
    ", @xmath50 does not work at all : the well known _ chaining _ effect",
    "is here observed , likely because the low number of points to cluster .",
    "a similar behaviour is seen with @xmath51 , for which we then omit the dendrogram .      as discussed in section",
    "[ sec : evaluation ] , different clusterings generated during the run of a clustering algorithm or by different clustering algorithms can be intrinsically evaluated through the silhouette value .",
    "figure  [ fig : sc ] plots the silhouette for the clusterings generated at each iteration by our hac algorithm instantiated with the five proposed distances .",
    "the number of clusters in each clustering is reported on the abscissa ; hence , in order to see how the algorithm proceeds from the first to the last iteration , the plot should be seen right - to - left .",
    "first of all , we observe that the values are quite low , exceeding rarely even 0.25 , not a really high value .",
    "this is due to the co - occurrence on the one hand of the low number of points to cluster ( 40 ) and on the other of the arbitrary setting to 0 of the silhouette value for the single - point clusters ( section  [ sec : evaluation ] ) .",
    "in fact , in early hac iterations there are many 0-valued clusters  fact that lowers the overall silhouette of clusterings  that disappear altogether only at the cost of straggly clusters  which still keep the coefficient low .",
    "apart that , the plot confirms the ineffectiveness of @xmath52 predicted by the corresponding dendrogram .",
    "the other distances show similar values for the first 10 - 15 iterations ( i.e. clusterings with 25 - 30 to 40 clusters ) ; after that , they start to diversify : @xmath53 , @xmath54 and @xmath55 initially generate good clusterings that tend to gradually worsen with further aggregations , until an abrupt drop occurs . on the contrary",
    ", @xmath56 keeps the same coefficient even with very few clusters .",
    "the peaks reached at the last but one iteration ( clusterings with just two clusters ) derive from the disappearance of single - point , i.e. 0-valued , clusters .",
    "clusterings can also be indirectly compared by looking at performance of tasks where they are used ; this is the so called extrinsic evaluation ( section  [ sec : evaluation ] ) . since here",
    "clusterings are used for inducing a decomposition of smt models in domains , two straightforward extrinsic evaluations are the perplexity of the induced lms and the final mt quality measured in terms , for example , of bleu score",
    ".    figures  [ fig : pp - bleu : d_delta ] and  [ fig : pp - bleu : d_pp ] plot the perplexity and the bleu score of the clusterings generated during the 40 iterations of our algorithm instantiated with the two most promising distances , according to the dendrograms , that is @xmath57 and @xmath45 .",
    ".,height=264 ]    .,height=264 ]    the values measured in correspondence of the two extreme clusterings ( at the beginning when each domain is a cluster in its own and at the end when all domains have been agglomerated into one single cluster ) are of course equal whatever the instance of the hac algorithm .",
    "concerning the perplexity , the two distances behave very similarly , @xmath47 being a bit smoother than @xmath58 thanks to the possibility to choose the best local merging in a more reliable way . in particular , a slight improvement is observed after early aggregations ; successively , a gradual degradation occurs which becomes more severe in the last 10 iterations .",
    "it is worth to note that the perplexity trend resembles quite closely that of the silhouette coefficients of figure  [ fig : sc ] , apart the outlier peaks of the latter in correspondence of clusterings with 2 clusters .    also the bleu curves are quite well predicted by both the silhouette and the perplexity : a tiny improvement at the beginning ; then , a plateau for @xmath47 and a slight degradation for @xmath45 ; finally , a rather sharp fall for both distances .    we can now answer the two questions posed in the introduction . in fact , as shown , @xmath47 and @xmath58 allow to improve the bleu score of the original domain - specific models by merging few , very close domains ( question ii ) , while , more importantly , @xmath47 is even able to keep the degradation of the bleu score under 0.2 absolute points ( from 57.8 to 57.6 ) employing just 5 specialised models instead of 40 ( question i ) .    on the other side",
    ", it should be said that the blue score does not vary too much , being the difference between the highest and the lowest values lesser than 1 absolute point ; this calls for an assessment on a more challenging benchmark .",
    "often we read that `` clustering is an art , not a science '' and that choosing the right way to measure the distance between the points of the task at hand is even more important than the clustering algorithm actually employed . those remarks",
    "are confirmed by our investigation .",
    "the same hac algorithm was instantiated with five distances and its behaviour observed from different points of view : dendrograms , intrinsic and extrinsic evaluations .",
    "the outcomes of such views are different : for example , according to silhouette , @xmath48 is effective while its dendrogram is very bad ; again , the perplexity curves of @xmath59 and @xmath45 are practically indistinguishable , while the dendrogram of the former appears to be better than the dendrogram of the latter .",
    "each single view can also be affected by critical aspects that should be taken into account .",
    "for example , in our particular set - up , the silhouette coefficient is highly affected by the 0-valued clusters , while the dendrograms by having taken the entire tms as atomic points to aggregate .",
    "hence , only an overall view of all measures can suggest reliable conclusions ; and our measures , as a whole , suggest that @xmath59 is the most effective distance out of those tested .",
    "in this paper we have summarised our investigation on domain clustering in the ambit of an adaptive mt architecture .",
    "a standard bottom - up hierarchical clustering algorithm has been instantiated with five different distances , which have been compared , on an mt benchmark with 40 commercial domains , in terms of dendrograms , intrinsic and extrinsic evaluations .",
    "the main outcome is that the most expensive distance is also the only one which allows the mt engine with just few cluster - specific models to perform as well as the 40-domains adapted mt engine .    in the close future",
    ", we are going to extend the here reported investigation as follows .",
    "first of all , instead of considering each original tm as an indivisible , single point , a finer granularity will be considered to both overcome the 0-valued clusters issue ( section  [ sec : intreval ] ) and improve the performance of single - link instances of the hac algorithm .",
    "unfortunately , no further meta - information is provided inside our tms in addition to the identity of the customer who provided it . anyway",
    ", finer straightforward single points to aggregate could be : ( i )  single segments inside tms ; ( ii )  automatic clusters of sentences inside each tm .",
    "second , our evaluations treated equally all words , but a customer could consider more valued the proper translation of domain - specific terminology than of other words . for this reason",
    ", we are manually annotating domain specific terms in benchmark 1.1 for comparing the instances of the hac algorithm with respect to them .",
    "finally , we will test the clustering on much more challenging benchmarks with hundred to even thousand domains .",
    "fbk authors were supported by the mmt project which received funding from the eu s horizon 2020 research and innovation programme under grant agreement no 645487 .",
    "lars bungum and bjrn gambck .",
    "multi - domain adapted machine translation using unsupervised text clustering . in henning christiansen , isidora stojanovic , and a.  george papadopoulos , editors , _ in proc . of modeling and using context : 9th international and interdisciplinary conference , context _ , pages 201213 , lanarca , cyprus ,",
    "september .",
    "davide caroselli , nicola bertoldi , mauro cettolo , and marcello federico .",
    "public deliverable , the mmt project ( horizon 2020 grant agreement no 645487 ) .",
    "/ deliverables / mmt - d1 - 2-second - design - and - specifications - report/.    marcello federico , nicola bertoldi , davide caroselli , roldano cattoni , mauro cettolo , ullrich germann , luca mastrostefano , and marco trombetti .",
    "public deliverable , the mmt project ( horizon 2020 grant agreement no 645487 ) .",
    "www.modernmt.eu / deliverables / mmt - d3 - 1-first - report - on - database - and - mt - infrastructure/.    andrew finch and eiichiro sumita .",
    "dynamic model interpolation for statistical machine translation . in _ proceedings of the third workshop on statistical machine translation _ , pages 208215 , columbus , ohio , june .",
    "association for computational linguistics .",
    "george foster and roland kuhn .",
    "2007 . mixture - model adaptation for smt . in _ proceedings of the second workshop on statistical machine translation _ , pages 128135 , prague , czech republic , june .",
    "association for computational linguistics .",
    "ullrich germann , anna samiotou , achim ruopp , nicola bertoldi , mauro cettolo , roldano cattoni , marcello federico , david madl , davide caroselli , and luca mastrostefano .",
    "public deliverable , the mmt project ( horizon 2020 grant agreement no 645487 ) .",
    "/ deliverables/97 - 2/.        prashant mathur , mauro cettolo , and marcello federico .",
    "2013 . . in _ proceedings of the eighth workshop on statistical machine translation",
    "_ , pages 301308 , sofia , bulgaria , august .",
    "association for computational linguistics ."
  ],
  "abstract_text": [
    "<S> in this paper , we report on domain clustering in the ambit of an adaptive mt architecture . </S>",
    "<S> a standard bottom - up hierarchical clustering algorithm has been instantiated with five different distances , which have been compared , on an mt benchmark built on 40 commercial domains , in terms of dendrograms , intrinsic and extrinsic evaluations . </S>",
    "<S> the main outcome is that the most expensive distance is also the only one able to allow the mt engine to guarantee good performance even with few , but highly populated clusters of domains . </S>"
  ]
}