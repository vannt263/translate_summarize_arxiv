{
  "article_text": [
    "numerical studies of gravitational @xmath0-body problem have made enormous advances during the last few decades due to algorithmic design , developments in hardware @xcite , and refinements to approximate methods .",
    "a number of interesting books have been written on the subject @xcite .",
    "the main reasons for the popularity of the direct evaluation techniques are its very high accuracy and lack of prior assumptions .",
    "approximate methods , by contrast , often deal with idealized circumstances , which , regardless of their advances in performance , may be hard to justify . regardless of all the advances over the last 40 years the evaluation of the mutual forces between all stars remains the bottleneck in the calculation . without the use of hybrid force evaluation techniques , as pioneered by @xcite",
    ", the @xmath1 performance complexity will remain a problem for the forseeable future . nevertheless , it is to be expected that the direct integration method for gravitational @xmath0-body simulations will remain popular for the next decade .",
    "the main technique adopted to increase the performance ( and accuracy ) of direct @xmath0-body methods in stellar dynamics applications is individual time stepping , providing orders of magnitude increase in computational speed . in the early 1990s",
    "this technique enabled astronomers to study star clusters with up to @xmath2 stars @xcite .",
    "the main development in the 1990s was the development of special - purpose hardware to provide the required teraflop / s raw performance .",
    "in particular grape-4 @xcite , which enables simulation of several tens of thousands of stars , and grape-6 which raised the bar above @xmath3 stars for the first time @xcite .",
    "for the forseeable future , special purpose hardware may still take the lead in advancing the study of self gravitating systems , particularly if that hardware is multi - purpose @xcite and therefore able to serve a larger group of users and investors .",
    "the use of graphical processor units ( gpu ) , is an interesing parallel development with a promising performance / price ratio @xcite .",
    "another likely developmental pathway is the more general use of large parallel computers .",
    "the latter , however , requires efficient parallel kernels for force evaluation .    in the last decade",
    "the number of processors per computer has increased dramatically . even today",
    "s personal computers are equipped with multiple cores , and large supercomputers often carry many thousands of processors , for example the ibm bluegene / l which has 131072 processors .",
    "efficient interprocess communication for such large clusters is far from trivial , and requires significant development efforts .",
    "the communication overhead and development challenges increase even more once large calculations are performed on the grid , as shown in the pioneering study of @xcite .",
    "the parallelization of direct @xmath0-body kernels , however , has not kept pace with the increased speed of multiprocessor computers .",
    "whereas the long - range of the gravitational interaction introduces the need to perform full @xmath4 force evaluations , it also requres all - to - all memory access between processors in the computer , giving a large communication overhead .",
    "this overhead increases linearly with the number of processors @xcite , although algorithms with reduced communication characteristics have been proposed @xcite .",
    "in fact , many algorithmic advances , such as individual time steps @xcite and neighbor schemes @xcite , appear to hamper the parallelization of direct @xmath0-body codes .",
    "the first efficiently parallelized production @xmath0-body code was nbody6++ @xcite , for which the systolic - ring algorithm was adopted @xcite .",
    "in this research note we report on our endevour to parallelize the kira direct gravitational @xmath0-body integrator , the workhorse of the starlab environment .",
    "the source code of the package , including the parallelized version , here reported , is available online at http://www.manybody.org/. we now briefly report on the implementation of the gravitational @xmath0-body methodology (  [ sect : integrator ] ) and the adopted parallelization topology (  [ sect : parallelization ] ) . in ",
    "[ sect : performance ] we report the performance of the package when ported to several parallel systems .",
    "the gravitational evolution of a system consisting of @xmath0 stars with masses @xmath5 and positions @xmath6 is computed by the direct summation of the newtonian force between all pairs of stars .",
    "the force @xmath7 on particle @xmath8 is obtained by summation over the other @xmath9 particles : @xmath10 where @xmath11 is the gravitational constant .",
    "a cluster consisting of @xmath0 stars evolves dynamically due to the mutual gravity of the individual stars . for the force calculation on each star ,",
    "a total of @xmath12 partial forces have to be computed .",
    "the resultant @xmath1 operation is the bottleneck for the gravitational @xmath0-body problem .",
    "several approximate techniques have been designed to speedup the force evaluation , but these can not compete with brute force in terms of precision .",
    "an alternative to improve the performance is by parallelizing the force evaluation for use on a beowulf or linux cluster ( with or without dedicated hardware)@xcite ; the use of graphical processing units @xcite ; a large parallel supercomputer @xcite ; or for grid operations @xcite . for distributed hardware",
    "it is crucial to implement an algorithm that limits communication as much as possible , otherwise the bottleneck shifts from the force evaluation to interprocessor communication .",
    "the parallelization scheme described in this paper is implemented in the kira @xmath0-body integrator , which is a part of the starlab package @xcite . in kira",
    "the particle motion is calculated using a fourth - order , individual time step hermite predictor - corrector scheme ( makino and aarseth 1992 ) .",
    "this scheme works as follows : during a time step the positions ( @xmath13 ) and velocities ( @xmath14 ) are first predicted to fourth order using the acceleration ( @xmath15 ) and the `` jerk '' ( @xmath16 ) which are known from the end of the previous step .",
    "the predicted position ( @xmath17 ) and velocity ( @xmath18 ) are calculated for all particles @xmath19    the acceleration ( @xmath20 ) and jerk ( @xmath21 ) are then recalculated at the predicted time from @xmath22 and @xmath23 using direct summation . finally , a correction is based on the estimated higher - order derivatives : @xmath24 here @xmath25 . which then leads to the new position and velocity at time @xmath26 .",
    "@xmath27 the value of @xmath28 and @xmath29 are computed by direct summation .",
    "the new timestep is calculated using a new predicted second derivative of the acceleration @xmath30 for each particle @xmath8 individually with @xcite @xmath31 here we use for accuracy parameter @xmath32 .",
    "a single integration step in the integrator thus proceeds as follows :    * determine which stars are to be updated .",
    "each star has associated with it an individual time ( @xmath33 ) at which it was last advanced , and an individual time step ( @xmath34 ) .",
    "the list of stars to be integrated consists of those with the smallest @xmath35 .",
    "time steps are constrained to be powers of 2 , allowing `` blocks '' of many stars to be advanced @xcite .",
    "* before the step is taken , check for system reinitialization , diagnostic output , escape removal , termination of the run , storing data , etc . *",
    "perform low - order prediction of all particles to the new time @xmath35 .",
    "this operation may be performed on the grape , if present . *",
    "recompute the acceleration and jerk on all stars in the current block ( using the grape or gpu , if available ) , and correct their positions and velocities .",
    "* check for and initiate unperturbed motion .",
    "* check for close encounters and stellar collisions .",
    "* check for reorganisations in the data structure . * apply stellar and/or binary evolution , and correct the dynamics as necessary . a more detailed discussion of starlab s stellar and binary evolution packages may be found in .",
    "an n - body system in starlab is represented as a linked - list structure , in the form of a mainly `` flat '' tree , individual stars being the `` leaves . ''",
    "the tree is flat in the sense that single stars ( i.e. stars that are not members of any multiple system ) are represented by top - level nodes , having the root node as parent .",
    "binary , triple , and more complex multiple systems are represented as binary trees below their top - level center of mass nodes .",
    "the tree structure determines both how node dynamics is implemented and how the long - range gravitational force is computed .",
    "the motion of every node relative to its parent is followed using the hermite predictor - corrector scheme described above .",
    "the use of relative coordinates at every level ensures that high numerical precision is maintained at all times , even during very close encounters without the need for ks - regularization @xcite .",
    "how the acceleration ( and jerk ) on a particle or node is computed depends on its location in the tree .",
    "top - level nodes feel the force due to all other top - level nodes in the system .",
    "forces are computed using direct summation over all other particles in the system ; no tree or neighbor - list constructs are used .",
    "( the integrator is designed specifically to allow efficient computation of these forces using grape or gpu hardware , if available . )",
    "nearby binary and multiple systems are resolved into their components , as necessary .",
    "the internal motion of a binary component is naturally decomposed into two parts : ( 1 ) the dominant contribution due to its companion , and ( 2 ) the perturbative influence of the rest of the system .",
    "( again , this decomposition is applied recursively , at all levels in a multiple system . )",
    "since the perturbation drops off rapidly with distance from the binary center of mass , in typical cases only a few near neighbors are significant perturbers of even a moderately hard binary .",
    "these neighbors are most efficiently handled by maintaining lists of perturbers for each binary , recomputed at each center of mass step , thereby greatly reducing the computational cost of the perturbation calculation .",
    "a further efficiency measure is the imposition of unperturbed motion for binaries whose perturbation falls below some specified value .",
    "unperturbed binaries are followed analytically for many orbits as strictly two - body motion ; they are also treated as point masses , from the point of view of their influence on other stars . because unperturbed binaries are followed in time steps that are integer multiples of the orbit period , we relax the perturbation threshold for unperturbed motion , relative to that for a perturbed step , as illustrated in fig.[fig : perturbers ] .",
    "perturbed binaries are resolved into their components , both for purposes of determining their center of mass motion and for determining their effect on other stars .",
    "unperturbed treatments of multiple systems also are used , based on empirical studies of the stability of their internal motion .",
    "we have parallelized the above scheme ( see  [ sect : integrator ] ) by allowing each processor to compute the force between a subset of the top - level node particles ( so called @xmath36-parallelization ) . in order to guarantee the integrity of the data across processors and to be able to deal efficiently with neighbor lists",
    "we maintain a copy of the entire system on each processor .    in fig.[fig",
    ": strategy ] we illustrate the parallalization strategy in the case of a distributed memory computer with 4 processors , named p00 , p01 , p10 and p11 .",
    "each processor holds a complete copy of the system in memory , but only a subset ( typically @xmath37 ) is used by the local processor for force calculations .",
    "the subsets of particles are indicated in fig.[fig : strategy ] as the grey area for each processor . in order to integrate the equations of motion for a particular particle",
    ", its total force with all other particles needs to be computed first .",
    "each processor then performs the force calculation between the particle in need for an update and the set of locally owned @xmath37 particles . rather than the usual @xmath9 force evaluations",
    ", each processor then performes @xmath38 force evaluations if the updated particle is part of its local subset of particles , and @xmath37 force evaluations if it is not local .",
    "the partial forces are subsequently added across the network ; all processes keep thus the same view of the system and as a consequence load balance is guaranteed .",
    "after the total force on a particle is calculated , each processor calculates its new position and velocity .",
    "the pseudo - code for this operation is shown in the flowchart .",
    "0.5truein    flowchart    ....    read system    copy system to all processors    identify locally owned subset of particles for each processor    do      identify particle(s ) needing a force evaluation      for all processors :        calculate force between each particle and local particles        across network :        sum all partial forces .",
    "calculate new position and velocity    until the end of the simulation ....    [ tab : flowchart ]      for the efficient integration of strongly perturbed single stars , binaries and higher - order multiples , for each star the @xmath0-body integrator maintains a list of stars that have the strongest effect on its motion .",
    "each particle , therefore , has a linked list of perturbers , consisting of stars which exert a force of @xmath39 ( see  [ fig : perturbers ] ) .",
    "generally , perturbers are geometrically close to the perturbed star , but this is not necessarily the case . given that the force is proportional to mass , a very massive object can still excert a considerable perturbation even if it is far away .",
    "we could have decided to carefully select which stars should be local to a processor making the following special treatment for perturbers unnecessary , but the heuristics would be non - trivial and may depend quite sensitively on the problem studied .    in principle",
    "it is not necessary to parallelize the perturber treatment , but parallelizing the perturber list warrants consistency between the parallel and the sequential implementation of the code .",
    "parallelization of the perturber list is hard because the order in which the perturbers are identified must be the same in the serial version of the code as in the parallel version , to guarantee identical results when compared to the sequential code .",
    "it will turn out that the parallelization of the perturber treatment gives rise to a performance hit for simulations with many binaries on large parallel clusters . in the serial version",
    "the pertuber list is filled in the order by which perturbers are identified , until a maximum @xmath40 is reached ( in practice @xmath41 ) .",
    "we prevent round - off for propagating through the integrator by maintaining the same order in the perturber list on the parallel version of the code .",
    "this is achieved by broadcasting the entire perturber list for each particle , and then sorting them to the same order as the serial code would have identified them .",
    "we have tested pkira on four quite distinct parallel computers . for initial conditions we selected a @xcite density profile with from 1024 to 65536 equal - mass particles .",
    "we integrate this system for one eighth of an @xmath0-body time unit @xcite .    for our performance measurements",
    ", we chose aster ( fig.[fig : speedup_aster ] ) , lisa ( figs.[fig : speedup_lisa ] and [ fig : speedup_lisa_2ppn ] ) and das-3 uva ( figs.[fig : speedup_das3 ] and [ fig : speedup_das3_wbinaries ] ) . in table[tab : hardware ] we present an overview of the computers used for the performance measurements .",
    "the lisa supercomputer was used in two different configurations , singe cpu and dual cpu .",
    ".hardware specifications for the computers used for the performance tests .",
    "[ cols=\"<,^,^,^\",options=\"header \" , ]     we find that the runs across clusters are slower than single - cluster runs with the same number of processors , due to the long network latency ( of up to 2ms ) and relatively low bandwidth . the efficiency ,",
    "however , is above 0.9 when we compare a run with 32 processes on one cluster , or over two clusters with 16 processes each . running pkira over two clusters with 32 processes each , rather than 32 processes on a single cluster , provides a speedup of @xmath42 .",
    "the results of our simulations on the grid are summarized in fig.[fig : grid_das3 ] , where we show the execution time for a plummer sphere with @xmath43 as a function of the number of processors in the testbed .",
    "the same number of processors are divided across a single cluster ( open squares ) , two clusters ( filled squares ) and 4 clusters ( circles ) .",
    "these simulations were performed on the das-3 . for @xmath44 processors",
    "the performance across two sites is not much affected by the long baseline between the two clusters . for 4 clusters the execution time is dominated by the farthest site , which has the longest latency and somewhat lower bandwidth . for grid - based simulations it appears useless to go beyond 60 processors for @xmath45 , since the total performance drops , resulting in a lower total execution time .",
    "note that for @xmath43 on 128 processors at a single site doen not result in a speedup either .",
    "if relatively few processors are available on a single site , it may be worth considering using a grid , if that allows the user to utilize a larger number of processors , as long as the number of processors @xmath44 .",
    "we report the results of our parallelization on the kira @xmath0-body integrator in the starlab package . in order to maintain a platform",
    "independent source code we opted for the message passing interface standard .",
    "the parallelization scheme is standard @xmath36-parallelization with a copy of the entire system on each node to guarantees that the perturber lists are consistent with the serial version of the code . in this way",
    "we achieve that the numerical results are independent of the number of processor units .",
    "we had to include an additional broadcast step to guarantee that the neighbour list is consistent between the sequential and the parallel code .",
    "this extra overhead , is particularly noticeable for small numbers of particles , and does not seriously hamper the scaling of the code with the number of processors . for simulations with a large number of binaries or when running on clusters with more than 128 processors we also found a drop in performance .",
    "the code scales well in the domain tested , which ranges from 1024 to 65536 stars on 1 to 128 processors .",
    "a super linear speed - up was achieved in several cases for 409632768 stars on 1664 processors .",
    "the overall performance for clusters with single stars is rather satisfactory , irrespective of the density profile of the star cluster .",
    "but for primordial binaries the performance drops considerably . with 100% binaries",
    "the performance of a large simulation on many processors hardly results in a reduction of the wall - clock time compared to a few processors",
    ". runs with a large number of primordial binaries still benefit from parallelization for @xmath46 processors .",
    "this performance hit in the presence of binaries is mainly the results of the additional communication required to synchronize the perturber list and of the sequential part in the binary treatment .",
    "we are grateful to douglas heggie , piet hut and jun makino for discussions .",
    "this research was supported in part by the netherlands supercomputer facilities ( ncf ) , the netherlands organization for scientific research ( nwo grant no . 635.000.001 and 643.200.503 ) , the netherlands advanced school for astronomy ( nova ) , the royal netherlands for arts and sciences ( knaw ) , the leids kerkhoven - bosscha fonds ( lkbf ) by nasa atp grant nng04gl50 g and by the national science foundation under grant no .",
    "phy99 - 07949 and deisa .",
    "the calculations for this work were done on the lisa workstation cluster , the aster supercomputer , de das-3 wide area computer in the netherlands and the modesta computer in amsterdam , which are hosted by sara computing and networking services , amsterdam .",
    "ag is supported by grant nnx07ah15 g from nasa ."
  ],
  "abstract_text": [
    "<S> we describe source code level parallelization for the kira direct gravitational @xmath0-body integrator , the workhorse of the starlab production environment for simulating dense stellar systems . </S>",
    "<S> the parallelization strategy , called `` j - parallelization '' , involves the partition of the computational domain by distributing all particles in the system among the available processors . </S>",
    "<S> partial forces on the particles to be advanced are calculated in parallel by their parent processors , and are then summed in a final global operation . </S>",
    "<S> once total forces are obtained , the computing elements proceed to the computation of their particle trajectories . </S>",
    "<S> we report the results of timing measurements on four different parallel computers , and compare them with theoretical predictions . </S>",
    "<S> the computers employ either a high - speed interconnect , a numa architecture to minimize the communication overhead or are distributed in a grid . </S>",
    "<S> the code scales well in the domain tested , which ranges from 1024 - 65536 stars on 1 - 128 processors , providing satisfactory speedup . </S>",
    "<S> running the production environment on a grid becomes inefficient for more than 60 processors distributed across three sites .    </S>",
    "<S> gravitation  stellar dynamics  </S>",
    "<S> methods : n - body simulation  methods : numerical  </S>"
  ]
}