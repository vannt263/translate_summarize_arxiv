{
  "article_text": [
    "a basic ( or _ vanilla _ ) option is a financial product which provides the holder of the option with the right to buy or sell a specified quantity of an underlying asset at a fixed price on or before the expiration date of the option .",
    "there are many more complex options ( called _ exotic _ options ) in use today ; these are often of more practical interest and are harder to deal with . in many scenarios of practical interest and",
    "as we shall assume in this article , the value of the underlying asset can be described by a diffusion process ; in a complete market , the value of the option can be expressed as the expectation under the risk neutral probability of a functional of the paths of the underlying diffusion process . in general",
    "these expectations can not be calculated analytically .",
    "the monte carlo ( mc ) method is a standard approach used to approximate these quantities and it has been extensively used in option pricing since @xcite .",
    "subsequently , a wide variety of monte carlo approaches have been applied ( @xcite provides a thorough introduction ) .",
    "+ the importance of monte carlo for option pricing against other numerical approaches is its ability to deal with high - dimensional integrals .",
    "this is either in the time parameter of the option ( path - dependent options ) or in the dimension of the underlying ( basket of options ) , and more generally in both .",
    "however , it has been noted in the option pricing literature that standard monte carlo estimates can suffer from high variability .",
    "it has been seen in @xcite that , in many situations of practical interest , sequential monte carlo ( smc ) approaches can vastly improve over more standard monte carlo techniques . + sequential monte carlo methods are a general class of methods to sample from a sequence of distributions of increasing dimensions which have extensively been used in engineering , statistics , physics and other domains .",
    "@xcite provides an introduction and shows how essentially all methods for particle filtering can be interpreted as some special instances of a generic smc algorithm .",
    "smc methods make use of a sequence of proposal densities to sequentially approximate the targets via a collection of @xmath0 samples , termed particles . in most scenarios",
    "it is not possible to use the distribution of interest as a proposal .",
    "therefore , one must correct for the discrepancy between proposal and target via importance weights . in the majority of cases of practical interest , the variance of these importance weights increases with algorithmic time .",
    "this can , to some extent , be dealt with a resampling procedure consisted of sampling with replacement from the current weighted samples and resetting them to @xmath1 ( adaptive resampling ) .",
    "the variability of the weights is often measured by the effective sample size ( ess ) .",
    "several convergence results , as @xmath0 grows , have been proved @xcite .",
    "smc methods have also recently been proven to be stable in certain high - dimensional contexts @xcite .",
    "+ the main contributions of this paper are as follows .",
    "we develop the formal framework of _ weighting functions _ ; this technique has already been used , implicitly or explicitly , in @xcite . exploiting this framework ,",
    "we develop tailored methods for pricing of barrier options in high dimensional settings .",
    "it is also applied to the pricing of target accrual redemption note ( tarn ) which are another widely traded kind of path dependent options that are notoriously difficult to accurately value . on the theoretical side , we provide with a proof of the unbiasedness of the smc",
    "estimates when an adaptive resampling scheme is used .",
    "+ this paper is structured as follows . in section",
    "[ sec : options ] we provide background details on option pricing . in section [ sec : smc ] we give a basic summary of smc methods . in section [ sec : weighting ] we give the weighting functions framework and its application in the context of our option pricing problems . in section [ sec : numerics ] our methods are illustrated numerically .",
    "the appendix gives the proof of unbiasedness of our smc estimate in the adaptive resampling case .",
    "+ in the remainder of this article , we use the notation @xmath2 to denote the @xmath3-dimensional euclidean space and @xmath4 . a normal distribution with mean @xmath5 and variance @xmath6 is denoted by @xmath7 and its density at @xmath8 is denoted by by @xmath9 .",
    "@xmath10 denotes the @xmath3-dimensional identity matrix .",
    "@xmath11 denotes expectation .",
    "options come in two basic kinds - call and put .",
    "call options give the right to buy and put options give the right to sell . in this context , there are two main kinds of options - american and european .",
    "american options can be exercised at any time prior to expiration whereas european options can be exercised only at expiration .",
    "we focus on european options in this paper .",
    "european call / put options are known as vanilla options since they are relatively simple in structure .",
    "an exotic option is an option which has features making it more complex than commonly traded vanilla options .",
    "path - dependent options are an example , in which case the payoff depends on the value of the underlying at some ( or all ) time points prior to the expiration date .",
    "we consider two kinds of path - dependent options in this paper , namely barrier options and tarn s , which we shall describe shortly .",
    "consider a collection of @xmath3 underlying assets ; this is also known as a basket .",
    "we denote by @xmath12 the value of the assets in the basket .",
    "@xmath13 is typically modelled by a diffusion process .",
    "one such process is a black - scholes model with a drift and a volatility @xmath14 where @xmath15 is the drift function , @xmath16 is the volatility and @xmath17 denotes a brownian motion in @xmath2 with mean @xmath18 and covariance matrix @xmath19 .",
    "it is reasonable to assume that @xmath17 is normalized , that is , @xmath20 for all @xmath21 ; this assumption is valid because the scale factor can be included in the volatility term .",
    "there is an interest rate @xmath22 , which can depend on time as well .",
    "+ in general , it is hard to analytically work with except in simple scenarios .",
    "this has lead to several discretization methods being available in literature ( @xcite ) with varying levels of accuracy and complexity .",
    "one of the most widely used discretization methods is the euler - maruyama discretization and we work with it in this paper ; however other discretization schemes could also be used which could lead to a lower bias .",
    "consider discretized time points @xmath23 . by letting @xmath24",
    "denote the logarithm @xmath25 and writing @xmath26 in place of @xmath27 , the euler - maruyama discretization of is @xmath28 for @xmath29 , where @xmath30 , @xmath18 denotes the origin in @xmath2 , and @xmath31 and @xmath32 to denote @xmath33 and @xmath34 respectively . ] .",
    "+ we do not focus on the level of discretization here .",
    "given a particular discretization , we apply our methods to it .",
    "the methods developed here could however be used in a multilevel setup ( as in @xcite ) ; we do not explore this further in this paper .",
    "+ we assume the drift @xmath35 in order to keep things simple . is a constant other than @xmath18 , then it is trivial to extend the methods we propose .",
    "if it is a function of the asset value , we could do things similar to what we do in the local volatility model considered later .",
    "] we work with two cases , one where the volatility @xmath36 is a constant , and another where it depends on the price of the underlying .",
    "we now describe two kinds of path - dependent options and we shall later demonstrate our methods on these .",
    "a barrier option is an exotic derivative , typically an option , on the underlying asset(s ) whose value(s ) on reaching pre - set barrier levels either springs the option into existence or extinguishes an already existing option .",
    "barrier options exist for baskets as well and the barrier conditions may in general be defined as a function of the underlying assets .",
    "for example , the function of the underlying assets could be the mean or could be the maximum of their values .",
    "there are two kinds of barrier options :    * when the option springs into existence if a function of the underlying asset values breaches prespecified barriers , it is referred to as being ` knocked - in ' , and * when the option is extinguished if a function of the underlying asset values breaches prespecified barriers , it is referred to as being ` knocked - out ' .",
    "we consider knocked - out options . these are options that are ` alive ' as long as the values of the underlying satisfy barrier conditions at some ( or all ) time points prior to the expiration .",
    "if the barrier condition is breached , the option ` dies ' leading to a zero payoff .",
    "if the option is still ` alive ' at the expiration date , then it gives a payoff that is akin to either a call or a put option ( depending on the option type ) .",
    "+ we consider a basket of @xmath3 underlying assets and a barrier option based on these .",
    "barrier options are hard to price using standard mc in the sense that most ( if not all ) of the particles lead to a zero payoff and this contributes to a high variance of the final estimate . because of the sequential nature of the evolution of the asset values over time , this is a natural example of a setting where smc methods can be applied ; indeed @xcite talks about how smc methods can be used in this context .",
    "we extend their method and show that one can obtain significant gains by choosing the @xmath37 s in section 5 of their paper even by heuristic methods .      a target accumulation redemption note ( tarn )",
    "provides a capped sum of payments over a period with the possibility of early termination ( knockout ) determined by target levels imposed on the accumulated amounts .",
    "a certain amount of payment is made on a series of cash flow dates ( referred to as fixing dates ) until the target level is breached .",
    "the payoff function of a tarn is path dependent in that the payment on a fixing date depends on the spot value of the asset as well as on the accumulated payment amount up to the fixing date .",
    "typically , commercial software solutions for pricing tarn s are based on the mc method .",
    "+ there are different versions of tarn products used in fx trading . for simplicity , we consider here a specific form of tarn s .",
    "consider a sequence of fixing dates @xmath38 and a function @xmath39 .",
    "the function @xmath40 is decomposed into its positive and negative parts as @xmath41 .",
    "gain and loss processes are defined as follows : @xmath42 these are the amounts of positive and negative cashflows respectively .",
    "there are two cashflow cutoffs @xmath43 and @xmath44 . stopping times @xmath45 and @xmath46",
    "are defined as @xmath47 these are the first times when the positive and negative cash flows cross their cut - offs respectively .",
    "the overall stopping time @xmath48 is defined as @xmath49 which is the first time either the positive or the negative cash flows cross a cut - off .",
    "the price of the tarn is the expected value of the overall cash flow , @xmath50 $ ] .",
    "here we have assumed that the interest rate is @xmath51 , if it was nt then the expectation would be a weighted sum with the weights corresponding to discounting factors .",
    "the reason we assume the interest rate is @xmath51 will be explained in section 4.3 .",
    "the main difficulty in applying standard mc methods in this scenario arises from the fact that the function @xmath40 may be discontinuous .",
    "smc methods can be used instead , which we shall show later .",
    "smc methods are a general class of mc methods that sample sequentially from a sequence of target probability densities @xmath52 of increasing dimension , where each distribution @xmath53 is defined on the product space @xmath54 .",
    "writing    @xmath55    it is required only that @xmath56 be known pointwise . in particular , the normalizing constants @xmath57 may be unknown and an estimate is obtained by the smc method .",
    "smc provides an approximation of @xmath58 and an estimate of @xmath59 at time 1 , then an estimate of @xmath60 and an estimate of @xmath61 at time 2 , and so on .",
    "these methods work by propagating a collection of @xmath0 particles using a sequence of importance sampling and resampling steps . in what follows",
    ", superscript @xmath62 shall denote the @xmath63-th particle .",
    "when we write an operation with superscript @xmath62 , we mean that it happens for all @xmath0 particles . a proposal density @xmath64 is selected and particles are proposed from this .",
    "the proposal density has the following structure : @xmath65 after proposing particles , associated _ unnormalized _ weights are computed recursively using the decomposition @xmath66 these can be written in the form @xmath67 where the _ incremental weight _",
    "function @xmath68 is given by @xmath69 these are computed for each particle .",
    "the weights @xmath70 s are unnormalized because they do not add up to 1 .",
    "they are normalized by dividing by their sum and the normalized weights are denoted by @xmath71 s .",
    "+ once we obtain a collection of @xmath0 weighted particles , they are _ resampled _ according to their weights @xmath72 .",
    "this is a crucial step which ensures that the system does nt collapse to very few particles with very high weights .",
    "resampling indices @xmath73 are chosen such that @xmath74 .",
    "the system is then updated by setting @xmath75 and @xmath76 for @xmath77 .",
    "resampling is expensive and in practice is done only if the variance of the weights is high .",
    "one such method is to use the effective sample size ( ess ) , defined at time @xmath78 as @xmath79 and to perform resampling at time @xmath78 if the ess falls below a certain threshold , usually taken to be @xmath80 or @xmath81 .",
    "algorithm [ algo : smc ] describes a general smc algorithm .    set initial weights @xmath82 and initial estimate of normalizing constant @xmath83 .",
    "sample @xmath84 ,    compute unnormalized weights @xmath85 ,    update estimate of normalizing constant @xmath86 ,    compute normalized weights @xmath87 ,    resample @xmath88 to obtain @xmath0 equally - weighted particles @xmath89 and set @xmath90 ,    set @xmath91 .",
    "sample @xmath92 and set @xmath93 ,    compute incremental weights @xmath94 and unnormalized weights @xmath95 ,    update estimate of normalizing constant @xmath96 ,    compute normalized weights @xmath97 ,    resample @xmath98 to obtain @xmath0 equally - weighted particles @xmath99 and set @xmath100 ,    set @xmath101 .",
    "we have two approximations of @xmath102 : @xmath103 which are equal if no resampling is used at time @xmath78 . here",
    "@xmath104 denotes the dirac delta function .",
    "we can also estimate @xmath105 through @xmath106",
    "the goal of using weighting functions is to create a sequence of intermediate target densities that try to approximate the optimal importance sampling density to guide particles towards regions of interest .",
    "consider a discrete time stochastic process @xmath107 on a general space @xmath108 .",
    "suppose that we want to estimate the expectation of a function of this process @xmath109 .",
    "the basic monte carlo method simulates @xmath0 independent realizations of the process @xmath110 , where @xmath111 denotes the @xmath63-th realization .",
    "@xmath112}$ ] is estimated by @xmath113 if @xmath114 is of the form where @xmath109 is zero over most of the space @xmath108 and non - zero only on a small subset , most of the @xmath115 s would be zero .",
    "this subsequently leads to a high variance of the resulting estimate .",
    "+ more specifically , since @xmath108 is the sample space , we write @xmath116 where @xmath117 and @xmath118 if @xmath119 is much larger than @xmath120 , simulating @xmath121 independent realizations will lead to most of them lying in @xmath122 .",
    "our goal is to use smc to simulate more particles from @xmath123 . in order to do that",
    ", we consider a sequence of positive potential functions @xmath124 , @xmath125 , such that @xmath126 and write @xmath127 h(x_{1:n}).\\ ] ] + the goal is to choose the @xmath128 s such that they guide the particles towards being in @xmath123 through the weighting and resampling steps of smc .",
    "+ when this is done on a path space , the resulting algorithm is sometimes known as tempering .",
    "@xcite considers this and shows how one can construct an artificial sequence of intermediate target densities on the path space which guide particles towards regions of interest .",
    "doing it on the path space however makes it computationally expensive and we do not work on the path space .",
    "we consider a discretely monitored barrier option monitored on a series of monitoring dates @xmath129 and a sequence of lower and upper barriers @xmath130 and @xmath131 respectively .",
    "we suppose that the barrier conditions are that all the underlying asset values lie inside their respective barriers at the monitoring times .",
    "this is a simplistic assumption and makes it easier for us to demonstrate our methods ; more complicated barrier conditions could also be used .",
    "+ for ease of notation , we remove the @xmath132 from the barriers and replace it simply by @xmath21 .",
    "let @xmath133 for @xmath134 and let @xmath114 denote the payoff function at time @xmath135 .",
    "the sequence of random variables @xmath107 then forms a markov chain .",
    "the price of the barrier option is    @xmath136 } \\footnote{we have assumed here that the interest rate is 0 .",
    "if the interest rate was $ r$ , then there would be a factor of $ e^{\\int_{0}^{t } r(t ) d t } $ multiplied with $ q_{d}$. this is a constant and affects the variance of the estimate only upto a ( known ) scale factor.},\\ ] ] where @xmath137 @xmath138 and @xmath139 denote the @xmath140-th components of @xmath141 and @xmath142 respectively . in this case , @xmath143    the authors in @xmath144 introduce an smc algorithm to estimate the price .",
    "the proposal density @xmath145 is chosen to be density with respect to the underlying discretization . for each time interval",
    "@xmath146 they simulate forward till @xmath132 and then resample particles that breach the barrier condition at time @xmath132 from among the particles that are still inside the barrier .",
    "their algorithm is algorithm [ algo : smc_barrier ] .",
    "it is commented that they do not use an adaptive version of resampling and instead always resample",
    ".    set initial weights @xmath82 and initial estimate of normalizing constant @xmath83",
    ".    sample @xmath84 .",
    "sample @xmath147 and set @xmath93 .",
    "compute unnormalized weights @xmath148 ,    update estimate of normalizing constant @xmath149 ,    compute normalized weights @xmath150 ,    resample @xmath98 to obtain @xmath0 equally - weighted particles @xmath99 and set @xmath100 .",
    "the estimated price is @xmath151 moreover , since essentially only the normalizing constant is being computed , there is no issue of path degeneracy .. this causes estimates based on the entire paths being unreliable . ]",
    "resampling paths outside the barriers at the monitoring times from paths which are inside the barriers improves the efficiency of the estimator with respect to the standard mc estimator .",
    "it is remarked that for constant volatility , in a black - scholes context , one can sample the price to ensure , in one step , that the process always survives ; see @xcite .",
    "+ if , however , very few particles satisfy the barrier condition at time @xmath132 , then this estimate will also have a high variance .",
    "this can happen for example if : the dimension @xmath3 is high ; the barrier condition is a narrow one , i.e , @xmath152 and @xmath142 are close to each other ; the volatility @xmath36 is high ; the time intervals @xmath146 are large .",
    "our goal is to introduce a sequence of positive weighting functions so that they resolve this issue . in order to do so ,",
    "we write    @xmath153 \\label{eq : h_n } \\nonumber \\\\ & = & h_{0}({\\mathbf s}_{0 } ) \\times { { \\ensuremath{\\operatorname{e}}}\\left [   \\prod_{n=1}^{n } g_{n}(x_{n } )    \\right ] } , \\label{eq : price_potential_functions}\\end{aligned}\\ ] ]    where @xmath154    the @xmath37 s , @xmath155 , are the sequence of positive weighting functions . in algorithm",
    "[ algo : smc_barrier ] , they were simply 1 .",
    "we attempt to choose them more carefully in order to approximate the optimal importance sampling densities .",
    "the algorithm is algorithm [ algo : smc_barrier_wt_fn ] .",
    "+    set initial weights @xmath82 and initial estimate of normalizing constant @xmath83 .",
    "sample @xmath84 ,    compute unnormalized weights @xmath156    update estimate of normalizing constant @xmath157 ,    compute normalized weights @xmath158 ,    resample @xmath88 to obtain @xmath0 equally - weighted particles @xmath89 and set @xmath90 ,    set @xmath91 .",
    "sample @xmath147 and set @xmath93 ,    compute unnormalized weights @xmath159    update estimate of normalizing constant @xmath160 ,    compute normalized weights @xmath161 ,    resample @xmath98 to obtain @xmath0 equally - weighted particles @xmath99 and set @xmath100 ,    set @xmath101 .",
    "the estimated price is @xmath162    path degeneracy is again not an issue because we are still essentially estimating the normalizing constant .",
    "this estimate is unbiased and a proof is provided in the appendix .",
    "+ since @xmath163 is the indicator of @xmath164 being inside the barriers at time @xmath132 , paths which are outside the barriers at the monitoring times are discarded with probability 1 in this case as well .",
    "however unlike in algorithm [ algo : smc_barrier ] , here we seek to give higher weights to particles which we think have a higher chance of being inside the barriers at the monitoring times .",
    "what is being sought while using the weighting functions is an approximation to the optimal importance sampling density , that is , the density of @xmath165 ( at time @xmath166 ) conditional on it surviving at times @xmath132 , @xmath167 . in the case of the barrier options being considered , this corresponds to @xmath168 .",
    "this is aachieved by giving higher weights to particles which have a higher chance of surviving at times @xmath132 .",
    "for example , particles which are far away from the barriers have a lower chance of survival than particles which are closer to the barriers .",
    "this is the intuitive idea behind our choice of weighting functions , and this is illustrated in section [ numerics : barrier ] .",
    "we consider a tarn based on a single underlying asset .",
    "since the main problem arises when the function @xmath40 is discontinuous , we consider a discontinuous @xmath40 to illustrate our methods .",
    "let    @xmath169 + this function has two big jumps at @xmath170 and @xmath171 .",
    "the negative and positive cashflow cutoffs are @xmath172 and @xmath173 .",
    "we recall that we had earlier assumed the interest rate is @xmath51 .",
    "this is now justified .",
    "the main reason why standard mc can not be used efficiently in this scenario is as follows . using mc ,",
    "most of the particles stay inside @xmath174 for the first five fixing dates .",
    "this leads to the contribution of the particle in the mc estimate being @xmath175 . however , an occasional particle escapes @xmath174 within the first five fixing dates and contributes a value that is significantly different from @xmath175 because of the big jumps in @xmath40 .",
    "this causes the variance of the mc estimate to be high .",
    "even if the interest rate was positive , this difficulty would still remain .",
    "therefore for simplicity we assume the interest rate to be @xmath51 .",
    "+ in order to go back to the previous notation , let @xmath176 and define @xmath177 .",
    "then let    @xmath178    be the new payoff function , where we have written @xmath179 in place of @xmath180 . by defining @xmath181 in this way , the problem has been transformed into the format that was being using before . in this case , @xmath182 a particle lies in @xmath122 if ( and only if ) it stays within @xmath174 for the first five fixing dates .",
    "@xmath183 if , for example , the volatility is low or the time intervals @xmath184 are small . it is noted that this is the opposite of the barrier option case ( in which we considered the time intervals and volatility to be large ) .",
    "we again consider a sequence of positive weighting functions @xmath185 and write    @xmath186 \\times \\frac { g(s_{1:n } ) } { h_{t_{5 } } ( s_{t_{5 } } ) } , \\ ] ]    where @xmath187 ; we are basically doing the same thing as in the barrier option case .",
    "the goal is again to guide particles towards being in @xmath123 , and we show that this can be achieved through the usage of some simple weighting functions in algorithm [ algo : smc_barrier_wt_fn ] .",
    "in this section we demonstrate numerically the benefit of using weighting functions . in order to compare the standard deviations of algorithms [ algo : smc_barrier ] and [ algo : smc_barrier_wt_fn ] , we run them 100 times with @xmath188 particles in each run .",
    "we then look at the standard deviations of the 100 estimates and report the relative standard deviations ( the ratio of the standard deviations ) .",
    "we consider a barrier option whose asset values evolve independently of each other .",
    "this translates to @xmath189 .",
    "the option type is call .",
    "we assume that we can simulate forward one day at a time .",
    "a common monitoring strategy is to monitor the underlying assets after every @xmath190 days for a total of @xmath191 time periods . in that case ,",
    "@xmath192 for @xmath193 and @xmath194 .",
    "we choose @xmath195 and @xmath196 .",
    "algorithm [ algo : smc_barrier ] resamples at the end of @xmath191 days and so resets the system of particles by choosing all resampled particles being inside the barriers .",
    "therefore the gain that we expect by using algorithm [ algo : smc_barrier_wt_fn ] can only be before the system is reset and this is why we choose @xmath197 . in what follows",
    ", we refer to algorithm [ algo : smc_barrier ] as ` mc ' and algorithm [ algo : smc_barrier_wt_fn ] as ` smc ' .",
    "+ the algorithms are run on different values of the dimension @xmath3 and the volatility @xmath198 . recalling , the targeted density at any time @xmath78 is proportional to @xmath199 , where @xmath200 denotes the density of @xmath201 .",
    "this implies that the targeted marginal density at time @xmath78 is proportional to @xmath202 , where @xmath203 denotes the marginal density of @xmath204 at time @xmath78 .",
    "we denote the targeted density at time @xmath78 by @xmath205 .",
    "since our basket consists of @xmath3 independent assets , we choose the marginal targeted densities ( at different time points ) to be the product of @xmath3 ( unnormalized ) densities .",
    "that is , @xmath206 is such that @xmath207 , where @xmath208 . in the constant volatility case ,",
    "the marginal density of @xmath209 at time @xmath78 is known and is denoted by @xmath210 ; this is simply a product of @xmath3 gaussians .",
    "therefore the weighting functions are @xmath211 $ ] .",
    "+ we choose @xmath212 .",
    "since our goal is to push particles towards being inside the barriers at time @xmath190 , we consider a ( one dimensional ) brownian bridge with volatility @xmath213 tied down at @xmath214 and @xmath215 .",
    "however , since the variance of the brownian bridge goes to 0 as the time approaches @xmath190 , we can not directly use it as it would lead to a high variance .",
    "so we add @xmath216 to the standard deviation of the brownian bridge .",
    "it is remarked that @xmath216 is somewhat an arbitrary choice .",
    "this is not necessarily the best choice , and we do not claim so ; this choice just serves to illustrate the benefit of using weighting functions .",
    "we have noticed gains even while using other values , all we need to do is to ensure that the targeted densities do nt tend towards being degenerate . in practice",
    ", we introduce the weighting functions only after time @xmath217 because we want to let particles initially explore the space and then give higher weights to particles which are more likely to be inside the barriers at time @xmath190 .",
    "+    ) for a barrier option with @xmath218 and @xmath195 for a constant volatility model when we target a brownian bridge ; in the left figure , the dimension @xmath219 ; in the right figure , the volatility @xmath220.,scaledwidth=100.0% ]    the results are in figure [ fig : fig2 ] .",
    "we observe a gain in the standard deviation by using these functions . in general ,",
    "if the volatilities , initial prices and strike prices were different for the different chains , we could have considered a product of @xmath3 dissimilar brownian bridge densities as the weighting functions .",
    "+ since the actual goal is to estimate the optimal importance density , we try to do it in our simple setup to see how much better it does .",
    "the marginal densities of a particle given that it survives at time @xmath190 can be approximated by gaussian densities .",
    "in fact , this is what we were doing earlier when we choose the targeted distribution to be a brownian bridge . instead of working with a brownian bridge",
    ", we can try to estimate the means and the variances of the gaussian densities by first simulating particles in one dimension and then looking at the means and variance of those that survive .",
    "+ in this regard , we simulate @xmath221 particles in one dimension with volatility @xmath220 .",
    "we then look at the particles that survive at the end . given the particles that survive , we look at their marginal means and variances .",
    "this approximates the marginal densities ( in one dimension ) of a particle given that it survives .",
    "since we are in one dimension , the number of particles surviving is much higher than what we would have in higher dimensions ( about 3900 out of 10000 in our simulations ) .",
    "this means that the probability of survival in one dimension is about @xmath222 for @xmath220 , which for example in 10 dimensions would mean a probability of survival of @xmath223 ( since the chains are independent ) .",
    "we call this the ` optimal ' target .",
    "+ the results are in figure [ fig : fig3 ] .",
    "we can observe an improvement against the standard mc method , relative to figure [ fig : fig2 ] .",
    "however , the gains are not very drastic and serve to illustrate that even the simple intuitive use of a brownian bridge does well .",
    "it is worth noting that even by choosing the volatility to be @xmath224 and running the chain in one dimension , we observe a gain for other values of the volatility as well .",
    "this suggests that even if the volatilities were different for the different chains , we could have simply run a chain in one dimension with a value of the volatility that is in the range of the volatilities and still get decent results .",
    "+    ) for a barrier option with @xmath218 and @xmath195 for a constant volatility model when we target the ` optimal ' target ; in the left figure , the dimension @xmath219 ; in the right figure , the volatility @xmath220.,scaledwidth=100.0% ]    a note on running times : we have observed that the running time for mc is less than three times that of smc ( even without having optimized the codes too much ) . thus if we were to take running times into account , we could have run mc with @xmath225 particles in the same time in which we run smc with @xmath0 particles . the standard deviation for mc using @xmath225 particles would be @xmath226 times less than that of mc with @xmath0 particles .",
    "so even by taking running times into account , smc outperforms mc . in the last example ,",
    "in addition we run mc with @xmath227 particles .",
    "we should take this time into account as well , but we run it for only one value of the volatility and it is fast .",
    "the reason we do not consider the running times in our figures is that we believe it might be possible to optimize the codes even further .",
    "the intention is to demonstrate the benefit of using weighting functions in smc .",
    "local volatility models are in practice used more frequently as they are typically more accurate . the local volatility function considered here",
    "is a linear interpolation between the values of the volatility @xmath213 at @xmath228 and the values of the underlying @xmath229 at @xmath230 . in this case",
    ", we do not know the marginal densities of @xmath209 . for simplicity",
    ", we still assume that the volatility functions are the same for all the chains and approximate it in the following way .",
    "consider a single chain .",
    "if we knew @xmath231 , @xmath232 , then we could approximate @xmath233 by @xmath234 .",
    "we approximate @xmath235 iteratively as follows ( assuming that the time step of the discretization @xmath236 is constant ) :    @xmath237 } & = & s_{n-1 } - \\frac{1}{2 } \\sigma^{2}(s_{n-1 } ) \\delta t \\nonumber \\\\ \\rightarrow { \\ensuremath{\\operatorname{e}}}s_{n } & = & { \\ensuremath{\\operatorname{e}}}s_{n-1 } - \\frac{1}{2 } \\delta t { { \\ensuremath{\\operatorname{e}}}\\left [   \\sigma^{2}(s_{n-1 } )   \\right ] } \\nonumber \\\\ & \\approx & { \\ensuremath{\\operatorname{e}}}s_{n-1 } - \\frac{1}{2 } \\delta t \\sigma^{2 } \\left ( { \\ensuremath{\\operatorname{e}}}s_{n-1 } \\right ) ; \\label{eq : approx_expectation}\\end{aligned}\\ ] ]    therefore we approximate @xmath235 and @xmath238 iteratively .",
    "we first approximate @xmath231 by ; given this estimate , we approximate @xmath239 by and keep doing this .",
    "we approximate @xmath240 by @xmath241 iteratively .    1 .",
    "we begin with a brownian bridge weighting function using the previously estimated values of @xmath242 in the target density and add @xmath243 to the standard deviation as before .",
    "2 .   as in the constant volatility model ,",
    "we simulate @xmath221 particles ( for the local volatility model ) in one dimension and look at the marginal means and variance of the particles that survive .",
    "we call this the ` optimal ' target and target it in the smc algorithm .",
    "the results are in figure [ fig : barrier_lv ] .",
    "we observe a significant gain , which as expected , gets more significant as the dimension increases .",
    "this is because the probability of a particle surviving gets smaller as the dimension increases .",
    "the gains are more significant when we target the ` optimal ' .",
    "as far as running times are concerned , the observations are the same as in the case of the constant volatility model .",
    "thus algorithm [ algo : smc_barrier_wt_fn ] outperforms algorithm [ algo : smc_barrier ] even upon taking running times into consideration .    ) for a barrier option with @xmath218 and @xmath195 for local volatility model ; the left figure is when we target a brownian bridge ; the right figure is when we target the ` optimal'.,scaledwidth=100.0% ]      we again assume that we can simulate forward one day at a time .",
    "we take the number of fixing dates @xmath244 and the interval between fixing dates @xmath245 days .",
    "this roughly corresponds to a tarn where there is a cash flow at the end of every month for a maximum of two years .",
    "we consider different values of the volatility .",
    "a particle leads to a zero payoff if it stays within @xmath246 for the first five months and to a positive payoff if it escapes by the end of the fourth month .",
    "we again compare standard mc and algorithm [ algo : smc_barrier_wt_fn ] ( referring to this as ` smc ' ) .",
    "we note that contrary to barrier options , in this case we expect smc to do better than usual mc for _ low _ values of the volatility .",
    "this is because in this case the region @xmath123 has low probability for lower values of the volatility unlike in the barrier option case where @xmath123 had low probability for higher values of the volatility .",
    "this is why we restrict ourselves to lower values of the volaitility in this section than in section [ numerics : barrier ] .",
    "when the underlying follows black - scholes dynamics with a constant volatility @xmath213 , we can directly simulate forward @xmath190 days at a time using the euler - maruyama discretization .",
    "we run the algorithms different values of the volatility and report the results .",
    "we try three ( increasingly more sophisticated , yet intuitive ) weighting functions to try and get particles to escape .",
    "+ in the most naive version , we choose @xmath247 to simply be @xmath248 . this just gives higher weights to particles that are further away from @xmath249 at the end of the @xmath78-th month .",
    "we obtain the results in figure [ fig : tarn_cv ] . +    ) using most naive weighting functions for tarn in the constant volatility case.,scaledwidth=100.0% ]    we then notice that the targeted density at time @xmath78 is proportional @xmath250 for @xmath251 .",
    "if we choose @xmath247 to be of the form @xmath252 , then the marginal of the targeted density at time @xmath78 is proportional to @xmath253 , where @xmath254 is the marginal density of @xmath179 at time @xmath78 .",
    "since we target @xmath248 , we choose @xmath255 and this leads to the results in figure [ fig : tarn_cv1 ] .",
    "it is seen that smc with weighting functions does much better in most cases , except in the last few cases .",
    "the last few cases correspond to higher volatilities , in which case the probability of a particle escaping increases .",
    "smc does better when the probability of a particle escaping is low , which is what we wanted . +    ) using simple weighting functions for tarn in the constant volatility case.,scaledwidth=100.0% ]    looking into the problem",
    "further , we observe that particles which escape can do so in one of two ways : either to the left or to the right .",
    "we fix a value of @xmath256 and run @xmath227 copies of the chain .",
    "our goal is to look at the exact marginal distribution of particles of particles which have escaped .",
    "we observe that approximately @xmath257 of particles escape to the left and @xmath258 escape to the right .",
    "this is of course for a fixed @xmath213 , but we use these marginal approximations as our target ( we use a mixture of normals ) and run the smc algorithm for different values of @xmath213 .",
    "the results are in figure [ fig : tarn_cv2 ] .",
    "significant gains are observed in this case as well , and are more than the gains before . in this case",
    "as well , we are trying to approximate the optimal importance density . +    ) using a mixture of normal densities as the target for tarn in the constant volatility case.,scaledwidth=100.0% ]    in all of the above three examples in this section ,",
    "we have observed that the running time for smc is less than twice that of mc .",
    "we can thus have similar conclusions as in the case of barrier options , which shows that smc outperforms mc even upon taking running times into consideration .      in this case , the volatility @xmath213 is a function of the price of the underlying @xmath259 .",
    "we can not simulate forward @xmath190 days at a time any more and we simulate forward one day at a time now . in this case the weighting functions are introduced each day for the first 5 time periods . + we consider a volatility function which is minimum at @xmath260 and increases on either side of @xmath260 .",
    "the value at @xmath260 is @xmath261 .",
    "the values in the @xmath174 are less than @xmath262 and are higher outside it .",
    "we choose this because a low value of @xmath213 would mean less particles escaping , but for the ones that do escape we let explore the space more freely .",
    "the local volatility function is a linear interpolation between the values of the volatility @xmath213 at @xmath263 and the values of the underlying @xmath229 at @xmath264 .",
    "+ we start off with the simplest weighting function @xmath265 . in this case , we observe that algorithm [ algo : smc_barrier_wt_fn ] has a standard deviation which is 1.83 times lower than that of mc .",
    "+ since the marginal density of @xmath266 is unknown , we choose a crude approximation of the marginal and use it .",
    "we choose @xmath267 to be the approximation .",
    "this is the density of @xmath268 if the volatility had been a constant @xmath262 .",
    "we run @xmath227 copies of the chain and look at the ones which escape towards the left and towards the right .",
    "we then choose a mixture of normals ( in this case , the proportions @xmath269 and @xmath270 ) and repeat the experiments .",
    "we observe that smc has a standard deviation which is 2.33 times less than that of mc .",
    "+ the running times for smc are twice that of usual mc in this case .",
    "this implies that while there are some gains to be achieved in using smc in this example , they are not as significant as before .",
    "this is because of the fact that the local volatility function is low ( in the region of around @xmath271 ) only for a few values of the underlying , which suggests that one shouldd actually give even higher weights to force particles to escape .",
    "nevertheless , this does serve to illustrate the potential benefits of using smc over mc .",
    "we have formally presented the idea of weighting functions in smc .",
    "the main idea is to try to estimate the sequence of optimal importance densities using a relatively heuristic technique of weighting functions , giving more weights to potentially favourable particles .",
    "we demonstrated this on two examples from finance , but this can be extended to other cases ( for instance those in @xcite ) .",
    "we have also seen that as we get closer to the optimal importance density the gains keep increasing .",
    "however even using an approximation leads to significant gains .",
    "since the idea is quite general , it can potentially be used in finance to price other kinds of path dependent options , for instance asian options as in @xcite .",
    "it can also be used in the context of the estimation of marginal expectations w.r.t",
    ".  laws of jump - diffusions ( and hence some control problems ) ; see for instance @xcite .",
    "the basic notion of weighting functions is also applied in @xcite for high - dimensional filtering problems . in summary the ideas in the article provide a simple formalisation of many that have previously appeared in the literature .",
    "+ there are many extensions of the work in the article . for instance",
    "one can adaptively determine the potential functions in the sequence , albeit at the cost of statistical bias .",
    "other ideas include further applications and adaptation to problems where the multi - level monte carlo method can be used ( e.g.  @xcite ) .",
    "we provide a proof of the unbiasedness of the estimate when the resampling is done adaptively and multinomially .",
    "let @xmath272 be the successive resampling times and let @xmath273 be the most recent resampling time before time @xmath78 , with @xmath274 if no resampling has been carried out before time @xmath78 . also define @xmath275 and @xmath276 .",
    "define @xmath277 where @xmath278",
    "if resampling is carried out at time @xmath78 , then @xmath279 are the resampling weights . +   + for any test function @xmath280 , we estimate @xmath281 } } $ ] by @xmath282 and we estimate the normalizng constant by @xmath283 .",
    "define : @xmath284 where @xmath285 + observe that @xmath286 define the likelihod ratio as @xmath287 and let @xmath288 \\nonumber \\\\ & = & \\frac { \\overline{v}_{1 } \\cdots \\overline{v}_{\\tau_{r } } \\overline{v}_{\\tau_{r+1 } } } { z_{n } } \\sum_{{m}=1}^{{m } } \\frac { v_{n } \\left ( x_{1:n}^{({m})}\\right ) } { { m}\\overline{v}_{n } } } {   \\psi \\left ( x_{1:n}^{({m } ) } \\right ) \\nonumber \\\\ & = & \\frac { z_{n}^{{m } } } { z_{n } } \\widehat{\\psi}_{or}. \\nonumber \\end{aligned}\\ ] ] the third equality is because @xmath276 .",
    "define @xmath289 iteratvely as @xmath290 and @xmath291 , where we recall that @xmath292 is the resampled index of the @xmath63-th particle at the @xmath293-th resampling step .",
    "let @xmath294 these are the @xmath213-fields generated by the random variables associated with the @xmath121 particles just before and just after the @xmath166-th resampling step respectively .",
    "let @xmath295 and define for @xmath296 , @xmath297 } } , \\ ] ] where @xmath298 denotes expectation under the proposal density . then @xmath299 } } $ ]",
    "let @xmath300 h_{\\tau_{s-1}}^{({m } ) } , \\nonumber \\\\",
    "z_{2s}^{({m } ) } & = & \\widetilde{f}_{\\tau_{s } } \\left ( \\overline{x}_{1:\\tau_{s}}^{({m } ) } \\right ) h_{\\tau_{s}}^{({m } ) } - \\sum_{j=1}^{{m } } v_{\\tau_{s}}^{(j ) } \\widetilde{f}_{\\tau_{s } } \\left ( x_{1:\\tau_{s}}^{(j ) } \\right ) \\widetilde{h}_{\\tau_{s}}^{(j)}. \\nonumber \\end{aligned}\\ ] ]        it follows from the proposition that @xmath303 } & = & \\psi_{n } \\nonumber \\\\ \\rightarrow { { \\ensuremath{\\operatorname{e}}}\\left [   \\frac { z_{n}^{{m } } } { z_{n } } \\widehat{\\psi}_{or }    \\right ] } & = & \\psi_{n } \\nonumber \\\\ \\rightarrow { { \\ensuremath{\\operatorname{e}}}\\left [    z_{n}^{{m } } \\widehat{\\psi}_{or }    \\right ] } & = & z_{n } \\psi_{n}. \\nonumber\\end{aligned}\\ ] ] taking @xmath304 yields the desired result .",
    "we observe that @xmath305 \\nonumber \\\\   & & + \\hspace{0.1 in } \\sum_{s=1}^{r+1 } \\left [ \\widetilde{f}_{\\tau_{s } } \\left ( x_{1:\\tau_{s}}^{({m } ) } \\right ) - \\widetilde{f}_{\\tau_{s-1 } } \\left ( \\overline{x}_{1:\\tau_{s-1}}^{({m } ) } \\right ) \\right ] h_{\\tau_{s-1}}^{({m } ) } \\nonumber \\\\ & = & - \\widetilde{f}_{\\tau_{0 } } \\left ( \\overline{x}_{\\tau_{0}}^{({m } ) } \\right ) h_{\\tau_{0}}^{({m } ) } + \\sum_{s=1}^{r+1 }   \\widetilde{f}_{\\tau_{s } } \\left ( x_{1:\\tau_{s}}^{({m } ) } \\right ) h_{\\tau_{s-1}}^{({m } ) } - \\sum_{s=1}^{r } \\sum_{j=1}^{{m } } v_{\\tau_{s}}^{(j ) } \\widetilde{f}_{\\tau_{s } } \\left ( x_{1:\\tau_{s}}^{(j ) } \\right ) \\widetilde{h}_{\\tau_{s}}^{(j ) }   \\nonumber\\end{aligned}\\ ] ] thus , @xmath306 @xmath307 @xmath308 @xmath309 @xmath310    the second inequality is from [ eq:3.new ] , and the last equality is because @xmath275 and @xmath311 } } = \\psi \\left ( x_{1:\\tau_{r+1 } } \\right ) l_{n } \\left ( x_{1:\\tau_{r+1 } } \\right )   \\nonumber \\\\",
    "\\rightarrow \\widetilde{f}_{\\tau_{r+1 } } \\left ( x_{1:\\tau_{r+1}}^{({m } ) } \\right ) h_{\\tau_{r}}^{({m } ) } & = & l_{n } \\left ( x_{1:n}^{({m } ) } \\right ) \\psi \\left ( x_{1:n}^{({m } ) } \\right ) h^{({m})}_{\\widetilde{\\tau}(n ) } \\hspace{0.2 in } \\textrm{as } x_{1:\\tau_{r+1}}^{({m } ) } = x_{1:n}^{({m } ) } \\textrm { and } \\widetilde{\\tau}(n ) = \\tau_{r}. \\nonumber\\end{aligned}\\ ] ]      @xmath313 }   & = &   { { \\ensuremath{\\operatorname{e}}}_{{m}}\\left[\\left .",
    "\\widetilde{f}_{\\tau_{1 } } \\left ( \\overline{x}_{1:\\tau_{1}}^{({m } ) } \\right ) h_{\\tau_{1}}^{({m } ) } - \\sum_{j=1}^{{m } } v_{\\tau_{1}}^{(j ) } \\widetilde{f}_{\\tau_{1 } } \\left ( x_{1:\\tau_{1}}^{(j ) } \\right ) \\widetilde{h}_{\\tau_{1}}^{(j ) }   \\ , \\right|   { \\mathcal{f}}_{1 }   \\right ] } \\nonumber \\\\ & = & { { \\ensuremath{\\operatorname{e}}}_{{m}}\\left[\\left .",
    "\\widetilde{f}_{\\tau_{1 } } \\left ( \\overline{x}_{1:\\tau_{1}}^{({m } ) } \\right ) h_{\\tau_{1}}^{({m } ) } - \\frac{1}{{m } } \\sum_{j=1}^{{m } } \\widetilde{f}_{\\tau_{1 } } \\left ( x_{\\tau_{1}}^{(j ) } \\right ) h_{\\tau_{0}}^{(j ) }   \\ , \\right|   { \\mathcal{f}}_{1 }   \\right ] } \\hspace{0.3 in } \\textrm{from } \\eqref{eq:3.new } \\nonumber \\\\ & = &   { { \\ensuremath{\\operatorname{e } } } _ { { m}}\\left[\\left .",
    "\\widetilde{f}_{\\tau_{1 } } \\left ( \\overline{x}_{1:\\tau_{1}}^{({m } ) } \\right ) h_{\\tau_{1}}^{({m } ) } - \\frac{1}{{m } } \\sum_{j=1}^{{m } } \\widetilde{f}_{\\tau_{1 } } \\left ( x_{\\tau_{1}}^{(j ) } \\right )   \\ , \\right|   { \\mathcal{f}}_{1 }   \\right ] }   \\hspace{0.57 in } \\textrm{as } h_{0}^{(j ) } = 1 \\nonumber \\\\ & = & 0 . \\nonumber\\end{aligned}\\ ] ]    the last equality is because the conditional distribution of @xmath314 given @xmath315 is that of @xmath0 i.i.d .",
    "random vectors which take the value @xmath316 with probability @xmath317 .",
    "also , @xmath318 } & = & { \\ensuremath{\\operatorname{e}}}\\left [ \\left \\ { \\widetilde{f}_{\\tau_{2 } } \\left ( x_{1:\\tau_{2}}^{({m } ) } \\right ) - \\widetilde{f}_{\\tau_{1 } } \\left ( \\overline{x}_{1:\\tau_{1}}^{({m } ) } \\right ) \\right \\ } h_{\\tau_{1}}^{({m } ) } \\bigg| { { \\mathcal{f}}_{2 } } \\right ] \\nonumber \\\\ & = & { \\ensuremath{\\operatorname{e}}}\\left [ \\left \\ { \\widetilde{f}_{\\tau_{2 } } \\left ( x_{1:\\tau_{2}}^{({m } ) } \\right ) - \\widetilde{f}_{\\tau_{1 } } \\left ( \\overline{x}_{1:\\tau_{1}}^{({m } ) } \\right ) \\right \\ } \\bigg| { { \\mathcal{f}}_{2 } } \\right ] h_{\\tau_{1}}^{({m } ) } \\nonumber \\\\ & = & 0 . \\nonumber\\end{aligned}\\ ] ] the last equality is because @xmath319 } & = &   { \\ensuremath{\\operatorname{e}}}_{{m } } \\left [ { \\ensuremath{\\operatorname{e}}}_{q } \\left ( \\psi ( x_{1:n } ) l_{n}(x_{1:n } ) \\bigg| x_{1:\\tau_{s } } = x_{1:\\tau_{s}}^{({m } ) } \\right ) \\bigg| { \\mathcal{f}}_{2(s-1 ) } \\right ] \\nonumber \\\\ & = & { \\ensuremath{\\operatorname{e}}}_{q } \\left ( \\psi \\left ( x_{1:n } \\right ) l_{n } \\left ( x_{1:n } \\right ) \\bigg| x_{1:\\tau_{s-1 } } = x_{1:\\tau_{s-1}}^{({m } ) } \\right ) \\nonumber \\\\ & = & \\widetilde{f}_{\\tau_{s-1 } } \\left ( \\overline{x}_{1:\\tau_{s-1}}^{({m } ) } \\right ) .",
    "\\nonumber \\end{aligned}\\ ] ] the last equality is by the tower property of conditional expectations .",
    "proceeding in this way , it is seen that @xmath320 is a martingale difference sequence ."
  ],
  "abstract_text": [
    "<S> pricing options is an important problem in financial engineering . in many scenarios of practical interest , financial option prices associated to an underlying asset </S>",
    "<S> reduces to computing an expectation w.r.t .  a diffusion process . </S>",
    "<S> in general , these expectations can not be calculated analytically , and one way to approximate these quantities is via the monte carlo method ; monte carlo methods have been used to price options since at least the 1970 s . </S>",
    "<S> it has been seen in @xcite that sequential monte carlo ( smc ) methods are a natural tool to apply in this context and can vastly improve over standard monte carlo . in this article , in a similar spirit to @xcite we show that one can achieve significant gains by using smc methods by constructing a sequence of artificial target densities over time . in particular , we approximate the optimal importance sampling distribution in the smc algorithm by using a sequence of weighting functions . </S>",
    "<S> this is demonstrated on two examples , barrier options and target accrual redemption notes ( tarn s ) . </S>",
    "<S> we also provide a proof of unbiasedness of our smc estimate . </S>",
    "<S> + * key words : * diffusions ; sequential monte carlo ; option pricing + * ams subject classification : * primary 91g60 ; secondary 65c05 .    * some contributions to sequential monte carlo methods for option pricing </S>",
    "<S> *    by deborshee </S>",
    "<S> sen , ajay jasra & yan zhou    department of statistics & applied probability , national university of singapore , singapore , 117546 , sg . </S>",
    "<S> email:`deborshee.sen@u.nus.edu ; staja@nus.edu.sg ; stazhou@nus.edu.sg ` +      aj was supported by a singapore ministry of education academic research fund tier 1 grant ( r-155 - 000 - 156 - 112 ) and is affiliated with the rmi and cqf at nus . </S>",
    "<S> yz was supported by a singapore ministry of education academic research fund tier 2 grant ( r-155 - 000 - 143 - 112 ) . </S>"
  ]
}