{
  "article_text": [
    "the efficient and accurate modelling of data is crucial to support reliable analyses and to improve the solution to related problems .",
    "mixture probability distributions are commonly used in machine learning applications to model the underlying , often unknown , distribution of the data @xcite .",
    "they are widely used to describe data arising in various domains such as astronomy , biology , ecology , engineering , and economics , amongst many others @xcite . in order to describe the given data , the problem of selecting a suitable statistical model has to be carefully addressed .",
    "the problem of mixture modelling is associated with the difficult task of selecting the optimal number of mixture components and estimating the parameters of the constituent probability distributions .",
    "mixtures with varying number of component distributions differ in their model complexities and their goodness - of - fit to the data .",
    "an increase in the complexity of the mixture model , corresponding to an increase in the model parameters , leads to better quality of fit to the data .",
    "various criteria have been proposed to address the trade - off arising due to these two conflicting objectives @xcite .",
    "as explained in @xcite , these methods are not completely effective in addressing this trade - off as the model complexity is approximated as a function of the _ number _ of parameters and not the actual parameters themselves . while some of the methods aim to tune the criteria used to evaluate a mixture model @xcite , they do not provide an associated search strategy to infer the optimal number of mixture components .",
    "the methods that incorporate a rigorous search method for the mixture components are based on dynamic perturbations of the mixture model @xcite .",
    "a thorough review of the various approaches to mixture modelling methods and their limitations is outlined in @xcite .",
    "the strategies based on bayesian inference , and more specifically , using the minimum message length ( mml ) framework have increasingly found support in mixture modelling tasks @xcite .",
    "the mml - based inference framework decomposes a modelling problem into two parts : the first part determines the model complexity by encoding all the parameters of the model , and the second part corresponds to encoding of the observed data using the chosen parameters .",
    "thus , a two - part message length is obtained for a model under consideration .",
    "a model that results in the least total message length is then determined to be the optimal model under this framework @xcite .",
    "the mml - based search method developed by @xcite is demonstrated to outperform the traditionally used approaches and is the current state - of - the art . @xcite",
    "have designed the mixture modelling apparatus to include gaussian distributions to model data in the euclidean space and von mises - fisher ( vmf ) distributions to model directional data distributed on the surface of a sphere .",
    "while gaussian mixtures are ubiquitously used because of their computational tractability @xcite , they are ineffective to model directional data . in this context , analogues of the gaussian distribution defined on the surfaces of the appropriate reimannian manifolds are typically considered .",
    "the vmf is the most fundamental directional probability distribution defined on the spherical surface .",
    "it is the spherical analogue of a symmetrical gaussian wrapped around a unit hypersphere @xcite and is demonstrated to be useful in large - scale text clustering @xcite and gene expression analyses @xcite .",
    "a general form of the vmf distribution is the fisher - bingham ( @xmath0 ) distribution which is used to model asymmetrically distributed data on the spherical surface @xcite .",
    "mixtures of @xmath0  distributions have been employed by @xcite to identify joint sets in rock masses , and by @xcite to sample random protein conformations .",
    "the @xmath0  distribution has increasingly found support in machine learning tasks for structural bioinformatics @xcite .",
    "a two - dimensional version of the vmf distributions called the von mises circular is used to model data distributed on the boundary of a circle .",
    "each data point on the circle has a domain @xmath1 .",
    "if such data occur as pairs , then the resulting manifold in three dimensions would be a torus .",
    "the bivariate von mises ( bvm ) distributions are used to model such data distributed on the toroidal surface and serve as the gaussian analogue .",
    "motivated by its practical applications in bioinformatics , the bvm distributions are widely studied .",
    "the mixtures of bvm distributions have been previously used in modelling protein dihedral angles @xcite .",
    "however , these approaches have some limitations .",
    "@xcite treat the pairs of angles to be independent of each other and do not account for their correlation .",
    "this is akin to conflating two von mises circular distributions together . as explained in section  [ sec : bvm_sine_mixtures ] , such an approximation leads to inefficient mixtures .",
    "although @xcite use bvm distributions that account for the correlation between the angular pairs , they do not have a rigorous search method to determine the optimal mixtures .",
    "this limits their ability to correctly distinguish among models that , while being of different type , have the same number of model parameters .",
    "this paper develops the mixture modelling apparatus to address these limitations using the mml framework .",
    "further , different variants of the bvm distribution obtained by constraining some of its characterizing parameters ( see section  [ sec : bivariate_vonmises ] ) . @xcite",
    "have evaluated the utility of these variants in the context of modelling the protein dihedral angles .",
    "we adopt the mml principle in objectively assessing the mixture distributions of these variants ( see section  [ sec : bvm_sine_mixtures ] ) .",
    "we have developed a search method to determine the optimal number of mixture components and their parameters that describe the given data in a completely unsupervised setting .",
    "the use of the mml modelling paradigm and our proposed search method is explored on real - world data corresponding to the dihedral , that is , torsion angles of protein structures .",
    "we demonstrate that mixtures of bvm distributions facilitate the design of reliable computational models for protein structural data .",
    "in addition to determining the optimal number of mixture components , the parameters of the individual component distributions need to be estimated . traditionally , the optimum parameters are obtained by maximum likelihood ( ml ) or bayesian maximum _ a posteriori _ probability ( map ) estimation . for a mixture distribution ,",
    "the parameters are estimated by maximizing the likelihood of the data by employing an expectation - maximization ( em ) algorithm that iteratively updates the mixture parameters @xcite .",
    "the key differences between ml , map and mml - based estimation is : ( 1 ) unlike ml , mml uses a prior over the parameters and considers their precision while encoding ; ( 2 ) unlike map , mml estimators are invariant under non - linear transformations of the parameters @xcite .",
    "the estimation of parameters using ml ignores the cost of stating the parameters , and map based estimation uses the probability _ density _ of parameters instead of their probability measure .",
    "in contrast , the mml inference process takes into account the optimal precision to which parameters should be stated and uses it to determine a corresponding probability value .",
    "parameter estimation using the mml framework has been carried out on various probability distributions @xcite .",
    "@xcite have demonstrated that the mml estimators outperform the traditionally used estimators in the case of gaussian and vmf distributions .",
    "furthermore , for a @xmath0  distribution , @xcite have shown that the mml estimators have lower bias and error as compared to the ml and map estimators .",
    "* contributions : * the main contributions of this paper are as follows :    * we derive the mml - based estimates of the parameters of a bvm distribution .",
    "the mml estimators are demonstrated to have lower bias and mean squared error when compared to their traditional counterparts .",
    "we consider two variants of the bvm distribution , namely the independent @xcite and the sine variant @xcite .",
    "* we design a search method to infer the optimal number of bvm mixture components that best describe the angular data distributed on the toroidal surface . *",
    "we demonstrate the utility of the mml framework in determining the suitability of the two variants of the bvm distribution in modelling the protein dihedral angle data .",
    "we show that the sine variant that includes the correlation term explains the data much more effectively than the independent version .",
    "* we demonstrate the effectiveness of the mixture modelling method by applying it to cluster protein dihedral angles .",
    "we demonstrate that the resulting mixtures closely correspond to the commonly observed secondary structural regions in protein structures .",
    "the rest of the chapter is organized as follows : section  [ sec : bivariate_vonmises ] describes the bvm distribution , the independent and the sine variants , and their relevance in modelling data distributed on the toroidal surface .",
    "section  [ sec : mml_framework ] describes the mml framework and outlines the differences between parameter estimation using ml , map and mml methods .",
    "it includes the derivation of the mml estimators of the parameters of the bvm distribution .",
    "we empirically demonstrate that the mml estimators outperform the traditionally used ml and map estimators by having lower bias and mean squared error .",
    "section  [ sec : bvm_sine_mixtures ] discusses the search and inference of mixtures of bivariate von mises ( bvm ) distributions using the mml framework . as a specific application",
    ", we employ the mixtures to model protein dihedral angles .",
    "we demonstrate that our search method is able to infer meaningful clusters that directly correspond to frequently occuring conformations in protein structures .",
    "the class of bivariate von mises ( bvm ) distributions was introduced by @xcite to model data distributed on the surface of a 3d torus .",
    "the study of these distributions has been partly motivated by biological research , where it is required to model the protein dihedral angles ( see section  [ sec : bvm_modelling_dihedrals ] ) .",
    "the probability density function of the bvm distribution has the general form @xmath2 where @xmath3 , such that @xmath4 and the parameter vector @xmath5 , such that @xmath6 are the mean angles , @xmath7 and @xmath8 are the concentration parameters , and @xmath9 is a @xmath10 real - valued matrix .",
    "the term @xmath11 corresponds to a von mises distribution on a circle characterized by the parameters @xmath12 and @xmath13 , hence , the bvm distribution ( equation  [ eqn : bvm_density ] ) can be explained as a product of two von mises circular distributions , with an additional exponential term involving @xmath9 , that accounts for the correlation .",
    "the general form of the bvm distribution has 8 free parameters . in order to draw an analogy to the bivariate gaussian distribution ( with 5 free parameters ) , sub - models of the bvm distribution",
    "have been proposed by restricting the values that @xmath9 can take @xcite .",
    "a 6-parameter version was explored by @xcite and has the form @xmath14 in particular , when @xmath15 and @xmath16 , the above density reduces to the following 5-parameter version , which is called the bvm _ sine _ model @xcite .",
    "@xmath17 where @xmath18 is the normalization constant of the distribution defined as @xmath19 and @xmath20 is the modified bessel function of first kind and order @xmath21",
    ". the 5-parameter vector will be @xmath22 where @xmath23 is a real number . if @xmath24 , the probability density function ( equation  [ eqn : bvm_sine ] ) will just be the product of two independent von mises circular distributions , and corresponds to the case when there is no correlation between the two variables @xmath25 and @xmath26 .",
    "the probability density function in such a case is given as @xmath27 where @xmath28 is the normalization constant defined as @xmath29 . and corresponds to the product of the normalization constants for the respective von mises circular distributions .",
    "alternatively , when @xmath30 , the form of equation  [ eqn : bvm_rivest ] results in a different reduced form called the bvm _ cosine _ model @xcite .",
    "the sine and the cosine models serve as natural analogues of the bivariate gaussian distribution on the 3d torus .",
    "in fact , for huge concentrations , @xcite approximate the sine model to a bivariate gaussian distribution with the @xmath10 covariance matrix @xmath31 , i , j\\in\\{1,2\\}$ ] , whose elements are given by @xmath32 the limiting case approximation is valid when @xmath33 .",
    "also , from the covariance matrix , the correlation coefficient @xmath34 can be determined as @xcite : @xmath35    in order to better understand the interaction of @xmath36 and the correlation coefficient @xmath34 , we provide an example in figure  [ fig : torus_diff_rho ] , where the distribution is shown for values of @xmath37 ( low correlation ) , @xmath38 ( moderate correlation ) , and @xmath39 ( high correlation ) .",
    "note that @xmath34 can take negative values , in which case the resultant distribution will just be a reflection in some axis @xcite .",
    "the modelling of directional data using the bvm sine and cosine models has been previously explored by @xcite . for estimating the parameters of the distribution , maximum likelihood based optimization is used .",
    "we discuss ml and map based estimators , which are the traditionally used methods of parameter estimation .      in applications involving modelling directional data using the bvm sine distributions , the maximum likelihood ( ml ) estimates",
    "are typically used @xcite . for bvm sine distributions , the moment and ml estimates are the same , as the bvm sine distribution belongs to the exponential family of distributions @xcite .",
    "given data @xmath40 , where @xmath41 , the ml estimates of the parameter vector @xmath22 are obtained by minimizing the negative log - likelihood expression of the data given by @xmath42    the ml estimates satisfy @xmath43 .",
    "however , as no closed form soultions exist because of the complicated form of @xmath18 , an optimization library is used .",
    "we use nlopt , a non - linear optimization library , to compute the parameter estimates .      for an independent and identically",
    "distributed sample @xmath44 , the map estimates are obtained by maximizing the posterior density @xmath45 .",
    "this requires the definition of a reasonable prior @xmath46 on the parameter space .",
    "the map estimates are sensitive to the nature of parameterization of the probability distribution and this limitation is discussed here .",
    "we demonstrate that the map estimators are inconsistent and are subjective to the parameterization .",
    "we consider two alternative parameterizations in the case of the bvm sine distribution .",
    "+ _ prior on the angular parameters @xmath12 and @xmath47 : _ since @xmath48 , a uniform prior can be assumed in this range for each of the means .",
    "further , assuming @xmath12 and @xmath47 to be independent of each other , their joint prior will be @xmath49 .",
    "+ _ prior on the scale parameters @xmath50 , and @xmath23 : _ as discussed for equation  [ eqn : bvm_density ] , the bvm density function can be regarded as a product of two von mises circular distributions with an additional term that captures the correlation . in the bayesian analysis of the von mises circular distribution , @xcite used the prior on the concentration parameter @xmath51 as @xmath52 .",
    "in the current context of defining priors on @xmath13 and @xmath53 for a bvm distribution , we use the prior @xmath54 . assuming @xmath13 and @xmath53 to be independent of each other , the joint prior is given by @xmath55    in order to define a reasonable prior on @xmath23",
    ", we use the fact that @xmath56 ( see equation  [ eqn : bvm_sine_rho ] ) . hence",
    ", the conditional probability density of @xmath23 is given as : @xmath57 .",
    "therefore , the joint prior density of the scalar parameters @xmath50 and @xmath23 is @xmath58    using the product of the priors for the angular and the scale parameters , that is , @xmath59 and @xmath60 , the joint prior of the parameter vector @xmath61 , is given by @xmath62 the prior density @xmath46 can be used along with the likelihood function to formulate the posterior density as the product of the prior and the likelihood function , that is , @xmath63 .",
    "the map estimates correspond to the maximized value of the posterior @xmath45 .",
    "we consider non - linear transformations of the parameter space , in order to demonstrate that the map estimates are not invariant in different parameterizations of the probability distribution .",
    "we discuss a simple non - linear transformation of the parameter space involving the correlation parameter @xmath23 .",
    "additionally , we also describe a parameterization that transforms all the five parameters . + _ an alternative parameterization involving @xmath23 : _ the bvm sine probability density function ( equation  [ eqn : bvm_sine ] ) can be reparameterized in terms of the correlation coefficient @xmath34 , instead of @xmath23 , by using the relationship @xmath64 ( as per equation  [ eqn : bvm_sine_rho ] ) .",
    "if @xmath65 denotes the modified vector of parameters , the modified prior density @xmath66 is obtained by dividing @xmath46 with the jacobian of the transformation @xmath67 as follows @xmath68 with this transformation , the posterior density @xmath69 can be computed , and subsequently used to determine the map estimates .",
    "+ _ an alternative parameterization involving @xmath61 : _ in addition to the transformation of the correlation parameter @xmath23 , we study another transformation that was proposed by @xcite .",
    "the method transforms a given continuous @xmath70-variate probability distribution into the uniform distribution on the @xmath70-dimensional _ unit _ hypercube .",
    "such a transformation applied on the prior density of the parameter vector @xmath61 results in the prior transforming to a uniform distribution .",
    "hence , estimation in this transformed parameter space is equivalent to the corresponding maximum likelihood estimation .",
    "for the 5-parameter vector @xmath22 , the @xcite transformation to @xmath71 involves computing the cumulative densities @xmath72 as follows @xmath73 as the cumulative densities are bounded by 1 , the above transformation results in @xmath74 .",
    "further , @xcite argue that each @xmath75 is uniformly and independently distributed on @xmath76 $ ] , so that the prior density in this transformed parameter space is @xmath77 in order to achieve such a transformation , we need to express @xmath75 in terms of the original parameters . based on the assumptions made in the formulation of the prior @xmath46 , we derive the following relationships : @xmath78 based on the independence assumption in the formulation of priors of angular and scale parameters , we have @xmath79 , and therefore we have @xmath80 further , @xmath81 , as @xmath23 is independent of @xmath12 and @xmath47 . hence , the invertible transformation corresponding to @xmath23 is as follows @xmath82 so that @xmath23 can be expressed as a function of @xmath83 , and @xmath84 .",
    "the transformed bvm sine probability density function @xmath85 is obtained by substituting the expressions of @xmath61 in terms of @xmath86 in @xmath87 ( equation  [ eqn : bvm_sine ] ) .    in summary , we considered two additional parameterizations of the bvm sine probability density",
    ". for statistical invariance , the estimates of the parameters should also be affected by the same transformation in alternative parameterizations .",
    "the map estimation does not satisy this property , as illustrated by the following example .",
    "an example of estimating parameters using the posterior distributions resulting from the various prior densities ( equations  [ eqn : bvm_sine_prior1 ] - [ eqn : bvm_sine_prior3 ] ) is described here .",
    "a random sample of size @xmath88 is generated from a bvm sine distribution @xcite .",
    "the true parameters of the distribution are @xmath89 , @xmath90 and @xmath91 ( corresponding to a correlation coefficient of @xmath92 ) .",
    "the map estimators are obtained by maximizing the posterior densities using the non - linear optimization library nlopt @xcite in conjunction with derivative - free optimization @xcite .",
    "the differences in the estimates are explained below .",
    "we observe that the estimates of the angular parameters , @xmath12 and @xmath47 , are similar across the different parameterizations , with values close to 1.730 and 1.695 radians respectively . in the case of using @xmath93 , the estimated values @xmath94 and @xmath95",
    "are transformed back into @xmath96 and @xmath97 to allow comparison of similar quantities .",
    "@xmath98    the estimation of the scale parameters , @xmath99 and @xmath23 however , results in different values .",
    "we observe that , in the case of @xmath100 , which translates to @xmath101 .",
    "this is different from the estimated value of @xmath102 using @xmath46 .",
    "the values of @xmath103 and @xmath104 are also different .",
    "further , with @xmath105 , the transformation of estimated @xmath75 into the @xmath61 parameter space result in different estimates . @xmath106    the above example demonstrates a drawback of the map - based estimation with respect to parameter invariance .",
    "the map estimator corresponds to the mode of the posterior distribution .",
    "the mode is , however , not invariant under varying parameterizations . we use the above parameterizations in analyzing the behaviour of the various estimators in the experiments section ( section  [ subsec : bvm_sine_experiments ] ) .",
    "in this section , we describe the model selection paradigm using the minimum message length criterion and proceed to give an overview of mml - based parameter estimation for any distribution .",
    "@xcite developed the first practical criterion for model selection based on information theory .",
    "as per bayes s theorem : @xmath107 where @xmath108  denotes observed data , and @xmath109  some hypothesis about that data .",
    "further , @xmath110 is the joint probability of data @xmath108  and hypothesis @xmath109 , @xmath111 and @xmath112 are the prior probabilities of hypothesis @xmath109  and data @xmath108  respectively , @xmath113 is the posterior probability , and @xmath114 is the likelihood .    as per @xcite ,",
    "given an event @xmath115 with probability @xmath116 , the length of the optimal lossless code to represent that event requires @xmath117 bits .",
    "applying shannon s insight to bayes s theorem , @xcite got the following relationship between conditional probabilities in terms of optimal message lengths : @xmath118 the above equation can be intrepreted as the _ total _ cost to encode a message comprising of the following two parts :    1 .   _ first part : _ the hypothesis @xmath119 , which takes @xmath120 bits , 2 .   _",
    "second part : _ the observed data @xmath44 using knowledge of @xmath119 , which takes @xmath121 bits .    as a result , given two competing hypotheses @xmath109  and @xmath122",
    ", @xmath123 gives the log - odds posterior ratio between the two hypotheses .",
    "the framework provides a rigorous means to objectively compare two competing hypotheses .",
    "clearly , the message length can vary depending on the complexity of @xmath109  and how well it can explain @xmath108 .",
    "a more complex @xmath119 may explain @xmath44 better but takes more bits to be stated itself .",
    "the trade - off comes from the fact that ( hypothetically ) transmitting the message requires the encoding of both the hypothesis and the data given the hypothesis , that is , the model complexity @xmath120 and the goodness of fit @xmath121 .",
    "@xcite introduced a generalized framework to estimate a set of parameters @xmath124 given data @xmath108 .",
    "the method requires a reasonable prior @xmath125 on the hypothesis and evaluating the _ determinant _ of the fisher information matrix @xmath126 of the _ expected _ second - order partial derivatives of the negative log - likelihood function , @xmath127 .",
    "the parameter vector @xmath124 that minimizes the message length expression ( given by equation  [ eqn : two_part_msg ] ) is the mml estimate according to @xcite .",
    "@xmath128 where @xmath129 is the number of free parameters in the model , and @xmath130 is the @xmath129-dimensional lattice quantization constant @xcite .",
    "the total message length @xmath131 , therefore , comprises of two parts : ( 1 )  the cost of encoding the parameters , @xmath132 , and ( 2 )  the cost of encoding the data given the parameters , @xmath133 .",
    "a concise description of the mml method is presented in @xcite .",
    "the key differences between ml , map , and mml estimation techniques are as follows : in ml estimation , the encoding cost of parameters is , in effect , considered constant , and minimizing the message length corresponds to minimizing the negative log - likelihood of the data ( the second part ) . in map",
    "based estimation , a probability _ density _ rather than the probability is used .",
    "it is self evident that continuous parameter values can only be stated to some finite precision ; mml incorporates this in the framework by determining the region of uncertainty in which the parameter is located .",
    "the value of @xmath134 gives a measure of the volume of the region of uncertainty in which the parameter @xmath124 is centered .",
    "this multiplied by the probability density @xmath125 gives the _ probability _ of a particular @xmath124 as @xmath135 .",
    "this probability is used to compute the message length associated with encoding the continuous valued parameters ( to a finite precision ) .      in this section ,",
    "we outline the derivation of the mml - based parameter estimates of a bvm sine distribution .",
    "as explained in section  [ subsec : mml_parameter_estimation ] , the derivation of the mml estimates requires the formulation of the message length expression ( equation  [ eqn : two_part_msg ] ) for encoding some observed data using the bvm sine distribution .",
    "the formulation requires the use of a suitable prior density on the parameters .",
    "we use the parameterization @xmath61 and the corresponding prior @xmath46 that was formulated in the map analyses in section  [ subsec : bvm_sine_map ] .",
    "it is to be noted that the mml estimation is invariant to the parameterization used @xcite .",
    "+ * notations : * [ subsec : bvm_sine_notation ] before describing the mml approach , the following notations are defined as these are used in the following discussion .",
    "the partial derivatives of the normalization constant @xmath18 of the bvm sine distribution would be required later on .",
    "the following are the notations adopted to represent them .",
    "@xmath136 we also require the determinant of the fisher information for the mml estimation of parameters .",
    "we use the above notations in the following computation of the fisher information .",
    "the computation of these partial derivatives is explained in section  [ subsec : bvm_sine_norm_constant_derivatives ] .      in order to proceed with the derivation of the fisher information",
    ", we first outline the derivation of some of the required _ expectation _ quantities . for random variables @xmath137 sampled from the bvm sine distribution ( equation  [ eqn : bvm_sine ] )",
    ", we compute the following quantities : @xmath138 $ ] , @xmath139 $ ] , @xmath140 $ ] , and @xmath141 $ ] .",
    "@xcite derived the normalization constant as an infinite series expansion given by equation  [ eqn : bvm_sine_norm_constant ] .",
    "we use the following _ integral _ form of the normalization constant to derive the above mentioned expectations , as a function of @xmath36 and @xmath23 .",
    "@xmath142 on differentiating the above integral with respect to @xmath13 , we get    c(_1,_2 , ) = _ -^ _ -^ ( _ 1 - _ 1 ) \\{_1 ( _ 1 - _ 1 ) + _ 2 ( _ 2 - _ 2 ) + ( _ 1 - _ 1)(_2 - _ 2 ) } d_2d_1 = c(_1,_2 , ) @xmath143[(_1-_1 ) ]    hence , the expectation can be represented using the above defined notation as @xmath144 = 0 = { \\ensuremath{\\mathbb{e}}}[\\sin(\\theta_2-\\mu_2 ) ] \\notag\\\\ { \\ensuremath{\\mathbb{e}}}[\\cos(\\theta_1-\\mu_1 ) ] = \\frac{1}{c(\\kappa_1,\\kappa_2,\\lambda ) } \\frac{\\partial c(\\kappa_1,\\kappa_2,\\lambda)}{\\partial\\kappa_1 } = \\frac{c_{\\kappa_1}}{c } \\notag\\\\ \\text{similarly,}\\quad { \\ensuremath{\\mathbb{e}}}[\\cos(\\theta_2-\\mu_2 ) ] = \\frac{c_{\\kappa_2}}{c } \\quad\\text{and}\\quad { \\ensuremath{\\mathbb{e}}}[\\sin(\\theta_1-\\mu_1)\\sin(\\theta_2-\\mu_2 ) ] = \\frac{c_{\\lambda}}{c }   \\label{eqn : expect_sincos1}\\end{gathered}\\ ] ] on differentiating twice the integral form of @xmath18 with respect to @xmath36 and @xmath23 , we get the following relationships @xmath145 = \\frac{c_{\\kappa_1\\kappa_2}}{c } , \\notag\\\\ { \\ensuremath{\\mathbb{e}}}[\\cos(\\theta_1-\\mu_1)\\sin(\\theta_2-\\mu_2 ) ] = 0 = { \\ensuremath{\\mathbb{e}}}[\\sin(\\theta_1-\\mu_1)\\cos(\\theta_2-\\mu_2 ) ] \\label{eqn : expect_sincos2}\\end{gathered}\\ ] ]      as described in section  [ subsec : mml_parameter_estimation ] , the computation of the _ determinant _ of the fisher information matrix requires the evaluation of the second order partial derivatives of the negative log - likelihood function with respect to the parameters of the distribution . as per the density function ( equation  [ eqn : bvm_sine ] ) ,",
    "the negative log - likelihood of a datum @xmath146 is given by @xmath147 where @xmath22 as indicated before .",
    "let @xmath148 denote the fisher information for a _ single _ observation .",
    "the fisher information matrix @xmath148 in the case of an @xmath0  distribution is a @xmath149 _ symmetric _ matrix .",
    "further , the determinant @xmath150 is decomposed as a product of @xmath151 and @xmath152 , where @xmath153 is the fisher matrix associated with the angular parameters @xmath12 and @xmath47 , and @xmath154 is the fisher matrix associated with the scale parameters @xmath155 , and @xmath23 . + _ fisher matrix ( @xmath156 ) associated with @xmath157 : _",
    "@xmath153 is a @xmath10 symmetric matrix whose elements are the expected values of the second order partial derivatives of @xmath158 with respect to @xmath12 and @xmath47 . on differentiating equation  [ eqn : bvm_sine_negloglike ] with respect to @xmath12 , we get @xmath159 & =   \\kappa_1\\ , { \\ensuremath{\\mathbb{e}}}[\\cos(\\theta_{1 } - \\mu_1 ) ] + \\lambda \\,{\\ensuremath{\\mathbb{e}}}[\\sin(\\theta_{1 } - \\mu_1)\\sin(\\theta_{2 } - \\mu_2)]\\notag\\\\ & = \\kappa_1 \\frac{c_{\\kappa_1}}{c } + \\lambda \\frac{c_{\\lambda}}{c}\\notag\\\\ \\text{similarly,}\\quad { \\ensuremath{\\mathcal{f}}}_{\\mu_2\\mu_2 } = { \\ensuremath{\\mathbb{e}}}\\left[\\frac{\\partial^2{\\ensuremath{\\mathcal{l}}}}{\\partial\\mu_2 ^ 2}\\right ] = & = \\kappa_2 \\frac{c_{\\kappa_2}}{c } + \\lambda \\frac{c_{\\lambda}}{c }    \\label{eqn : bvm_sine_fisher_angles1}\\end{aligned}\\ ] ] on taking the derivative of equation  [ eqn : bvm_sine_dl_dmu1 ] with respect to @xmath47 , we get @xmath160 & =   -\\lambda { \\ensuremath{\\mathbb{e}}}[\\cos(\\theta_{1 } - \\mu_1)\\cos(\\theta_{2 } - \\mu_2 ) ] = -\\lambda \\frac{c_{\\kappa_1\\kappa_2}}{c } \\label{eqn : bvm_sine_fisher_angles2}\\end{aligned}\\ ] ]    _ fisher matrix ( @xmath154 ) associated with @xmath161 : _",
    "@xmath154 is a @xmath162 symmetric matrix whose elements are the expected values of the second order partial derivatives of @xmath158 with respect to @xmath36 and @xmath23 . on differentiating equation  [ eqn : bvm_sine_negloglike ] with respect to @xmath36 and @xmath23 , we get @xmath163    _ fisher matrix @xmath164 associated with the 5-parameter vector @xmath61 : _ on differentiating equation  [ eqn : bvm_sine_dl_dmu1 ] with respect to @xmath13 and computing the expectation of the differential , we get @xmath165 = 0 = { \\ensuremath{\\mathcal{f}}}_{\\kappa_1\\mu_1 }   \\quad\\text{and}\\quad { \\ensuremath{\\mathbb{e}}}\\left [ \\frac{\\partial^2{\\ensuremath{\\mathcal{l}}}}{\\partial\\lambda\\partial\\mu_1 } \\right ] = 0 = { \\ensuremath{\\mathcal{f}}}_{\\lambda\\mu_1 } \\end{gathered}\\ ] ] this allows for the computation of @xmath150 as the product of @xmath151 and @xmath152 , that is , @xmath166 then , the fisher information for some observed data @xmath167 is given by @xmath168 as each element in @xmath150 is multiplied by the sample size @xmath169 .      the message length to encode some observed data @xmath44 can now be formulated by substituting the prior density @xmath46 ( equation  [ eqn : bvm_sine_prior1 ] ) , the fisher information @xmath126 and the negative log - likelihood of the data ( equation  [ eqn : bvm_sine_negloglike_data ] ) in the message length expression ( equation  [ eqn : two_part_msg ] ) .",
    "the mml parameter estimates are the ones that minimize the total message length . as there is no analytical form of the mml estimates ,",
    "the solution is obtained , as for the maximum likelihood and map cases , by using the nlopt optimization library @xcite . at each stage of the optimization routine",
    ", the fisher information needs to be calculated .",
    "however , this involves the computation of complex entities such as the normalization constant @xmath170 and its partial derivatives .",
    "the computation of these intricate mathematical forms using numerical methods is discussed next in section  [ subsec : bvm_sine_norm_constant_derivatives ] .",
    "the computation of the negative log - likelihood and the message length requires the normalization constant and its associated derivatives . in this section",
    ", the description of the methods that can be employed to efficiently compute these complex functions is explored .",
    "we will utilize the properties of bessel functions to implement the normalization constant and the necessary partial derivatives as limiting order summations for the bvm sine distribution .",
    "the expressions of @xmath174 , and @xmath175 are related to each other .",
    "these expressions are explained by defining the quantity @xmath176 , a logarithm sum , @xmath177 where @xmath178 , @xmath179 , and @xmath180 ( by definition ) .",
    "_ computation of the series @xmath176 : _ we first establish that @xmath181 and show that @xmath176 converges to a finite sum as @xmath182 .",
    "consider the logarithm of the ratio of consecutive terms @xmath183 and @xmath184 in @xmath176 @xmath185 for @xmath186 , @xmath187 , and the ratio @xmath188 as @xmath189 @xcite .",
    "further , @xmath190 implies the above equation is the sum of negative terms .",
    "hence , @xmath191 , which means @xmath192 .",
    "also , @xmath193 hence , as @xmath194 , @xmath176 is a convergent series .    for a practical implementation of the sum",
    ", we need to express @xmath176 as the modified summation @xmath195 where each @xmath183 is divided by the _ maximum _",
    "term @xmath196 . for each @xmath197",
    "is calculated using the previous term @xmath198 ( equation  [ eqn : bvm_sine_ratio_series1 ] ) . the new term @xmath199 is then computed can get very large and can result in overflow when calculating the exponent @xmath200",
    "however , dividing by @xmath196 results in @xmath201 . ] as @xmath202 .",
    "this is because computing the difference with the maximum value and then taking the exponent ensures numerical stability .",
    "the summation is terminated when the ratio @xmath203 ( a small threshold @xmath204 ) .",
    "* let @xmath205 : substituting @xmath206 and @xmath207 in equation  [ eqn : bvm_sine_series1 ] gives the logarithm of the normalization constant ( given in equation  [ eqn : bvm_sine_norm_constant ] ) .",
    "hence , @xmath208 .",
    "* let the @xmath209 term dependent on @xmath13 in equation  [ eqn : bvm_sine_norm_constant ] be represented as @xmath210 , where @xmath211 implicitly refers to @xmath212 .",
    "based on the relationship between the bessel functions @xmath213 , and the derivative @xmath214 in equation  [ eqn : bvm_sine_bessel_identity ] @xcite , the expressions for the first and second derivatives of @xmath215 ( equation  [ eqn : gk1_derivatives ] ) are derived as @xmath216 * let @xmath217 : because of the similar forms of @xmath215 and @xmath218 , the expression for @xmath219 will be similar to that of @xmath220 with a change in _ order _ of the bessel functions from @xmath206 in equation  [ eqn : bvm_sine_series1 ] to @xmath221 .",
    "hence , @xmath222 and an expression similar to equation  [ eqn : bvm_sine_series1_modified ] can be derived for @xmath219 .",
    "* let @xmath223 : similar to the computation of @xmath219 above , if we substitute @xmath224 in equation  [ eqn : bvm_sine_series1_modified ] , we obtain the expression for @xmath225 . *",
    "let @xmath226 : similar to the above computations of @xmath219 and @xmath227 , if we substitute @xmath228 in equation  [ eqn : bvm_sine_series1_modified ] , we obtain the expression for @xmath229 . *",
    "let @xmath230 : substituting @xmath231 in equation  [ eqn : bvm_sine_series1 ] gives the logarithm sum @xmath232 corresponding to the series with terms @xmath233 . based on the nature of @xmath234 ( equation  [ eqn : gk1_derivatives ] ) , and noting",
    "that @xmath235 ( as @xmath236 ) , @xmath237 is formulated as @xmath238 * let @xmath239 : based on the same reasoning as above , we have @xmath240      the expressions of @xmath245 and @xmath246 are related and are explained using the logarithm sum @xmath247 @xmath248 where @xmath249 , @xmath250 , and @xmath251 .",
    "note that @xmath247 is a convergent series ( the proof is based on the same reasoning as for @xmath176 ) .",
    "let the @xmath209 term dependent on @xmath252 in equation  [ eqn : bvm_sine_norm_constant ] be represented as @xmath253 .",
    "its partial derivatives are given below .",
    "these derivatives are the terms in the series @xmath247 ( after factoring out the common elements as @xmath254 ) .",
    "* let @xmath256 : this is obtained by substituting @xmath206 and @xmath207 in equation  [ eqn : bvm_sine_series2 ] .",
    "hence , @xmath257 . *",
    "similarly , @xmath258 and @xmath259 .",
    "* the expression to compute @xmath260 is given by @xmath261    the practical implementation of @xmath262 and @xmath263 is similar to that of @xmath176 given by equation  [ eqn : bvm_sine_series1_modified ] .",
    "however , in these cases , the expressions of @xmath183 and consequently @xmath264 , are modified depending on their specific forms .",
    "also , the series begin from @xmath265 and , hence , the respective maximum terms will correspond to @xmath266 .      for a given bvm sine distribution characterized by concentration parameters",
    "@xmath155 and correlation coefficient @xmath34 , a random sample of size @xmath169 is generated using the method proposed by @xcite .",
    "the angular parameters of the true distribution are set to @xmath267 .",
    "the scale parameters @xmath36 and @xmath34 are varied to obtain different bvm sine distributions and corresponding random samples .",
    "the parameters are estimated using the sampled data and the different estimation methods .",
    "the procedure is repeated 1000 times for each combination of @xmath268 and @xmath34 .      for every",
    "randomly generated sample from a bvm sine distribution , we compute the the ml , map , and mml estimators of the parameters , and these are compared with each other across all the simulations .",
    "the results include the three versions of map estimates resulting from the three forms of the posterior distributions ( equations  [ eqn : bvm_sine_prior1]-[eqn : bvm_sine_prior3 ] ) : _ map1 _ corresponds to the posterior with parameterization @xmath22 , _",
    "map2 _ corresponds to the posterior with parameterization @xmath269 , and _",
    "map3 _ corresponds to the posterior with parameterization @xmath71 . as noted in section  [ subsec : bvm_sine_map ]",
    ", the map3 estimator will be the same as the ml estimator due to the @xcite transformation of @xmath61 to @xmath93 .    in order to compare the various estimators",
    ", we use the mean squared error ( mse ) and kullback - leibler ( kl ) distance as the objective evaluation metrics .",
    "the estimates are also compared using statistical hypothesis testing . for a parameter vector @xmath61 characterizing a true bvm sine distribution , and its estimate @xmath270",
    ", we analyze the mse and kl distance of @xmath270 with respect to the true parameter vector @xmath61 .",
    "the analytical form of the kl distance between two bvm distributions is derived in appendix  [ app : bvm_sine_kldiv ] .",
    "we analyze the percentage of times ( _ wins _ ) the kl distance of a particular estimator is smaller than that of others . when the kl distance of different estimates is compared , because of three different versions of map estimation , three separate frequency plots are presented .",
    "corresponding to the map1 , map2 , and map3 estimators .",
    "with respect to statistical hypothesis testing , the likelihood ratio test statistic is asymptotically approximated as an @xmath271 distribution with five degrees of freedom for the various parameter estimates compared here , it is expected that at especially large sample sizes , the estimates are close to the ml estimate . in other words ,",
    "the empirically determined test statistic is expected to be lower than the critical value @xmath272 , corresponding to a p - value greater than 0.01 .      as per the experimental setup",
    ", we present the results for when the original distribution from which the data is sampled has @xmath273 and @xmath274 .",
    "the correlation coefficient @xmath34 is varied between 0 and 1 , so that we obtain different values for the correlation parameter @xmath23 ( equation  [ eqn : bvm_sine_rho ] ) .",
    "we discuss the results for varying values of sample sizes @xmath169 , and @xmath275 , corresponding to a low , moderate , and high correlation , respectively .",
    "+ * for * @xmath276 the results are presented in figure  [ fig : k1_1_k2_10_r_1 ] .",
    "compared to the ml estimators , the map and mml estimators result in lower bias and mse for all values of @xmath169 .",
    "both the bias and mse continue to decrease as the sample size increases , as the estimation improves with more evidence for all methods .",
    "when compared with map1 and map2 , the mml estimators have greater bias and greater mse . as with the @xmath0  distribution",
    ", we observe that that map1 and map2 result in different estimators , and therefore , result in different bias and mse values .    the kl distance with respect",
    "to map1 is in favour of the map1 estimators .",
    "the map1 estimates result in lower kl distance as compared to the other estimators almost 50% of the 1000 simulations for each @xmath169 ( figure  [ fig : k1_1_k2_10_r_1]c ) .",
    "however , the mml estimators win when the map2 and map3 versions are used .",
    "when map3 is used , the mml estimators have a smaller kl distance in close to 70% of the simulations ( figure  [ fig : k1_1_k2_10_r_1]e ) .",
    "further analysis using statistical hypothesis testing illustrates that the null hypotheses corresponding to the map and mml estimators are accepted ( p - values greater than 0.01 in figure  [ fig : k1_1_k2_10_r_1]f ) . at the 1% significance level . +     +   +    * for * @xmath277 similar to when @xmath37 , we observe that the bias and mse of the map and mml estimators are lower than the ml estimators for different values of @xmath169 .",
    "in contrast to @xmath37 , the bias of the mml estimator is lower than the map1 estimator but higher than the map2 estimator ( figure  [ fig : k1_1_k2_10_r_5]a ) . as with the previous case , map - based estimation result in different estimators .",
    "further analysis of the estimators using kl distance and statistical hypothesis testing follow the same pattern as when @xmath37",
    ". +     +   +    * for * @xmath278 the results are presented in figure  [ fig : k1_1_k2_10_r_9 ] . as with the previous two cases , we observe that the ml estimators have the greatest bias and mse for all values of @xmath169 . the bias of the mml estimators is lower than all the map estimators . however , the mse of the mml estimators is greater compared to the map1 or map2 estimators .",
    "contrary to the previous two cases , we observe that the frequency of wins of kl distance for the mml estimators is lower when compared to map2 estimation ( figure  [ fig : k1_1_k2_10_r_9]e ) .",
    "further , the results following the statistical hypothesis testing follow the same trend as the previous two cases . as the same size increases ,",
    "the different estimators converge to the ml estimators as seen from the high p - values ( figure  [ fig : k1_1_k2_10_r_9]g ) .",
    "+   +    the empirical analyses of the controlled experiments discussed above indicate that the ml estimators of the parameters of a bvm distribution are biased .",
    "the same was observed with other directional probability distributions such as the vmf @xcite and @xmath0  @xcite .",
    "also , we observe that the map estimation method result in different estimators depending on how the distribution is parameterized . we have shown that the map estimators are not invariant under non - linear transformations of the parameter space . in this context",
    ", the mml estimators are empirically demonstrated to have lower bias than the traditional ml estimators and are invariant to alternative parameterizations unlike the map estimators .",
    "we consider two kinds of bivariate von mises ( bvm ) distributions in mixture modelling .",
    "in addition to the sine variant ( equation  [ eqn : bvm_sine ] ) that has the correlation parameter @xmath23 , we also consider the independent variant obtained when @xmath279 .",
    "the independent version assumes zero correlation between the data distributed on the torus ( see equation  [ eqn : bvm_ind ] ) .",
    "we provide a comparison for the mixture models obtained using both versions of the bvm distributions .    previous work on mml - based modelling of protein dihedral angles used independent bvm distributions @xcite .",
    "their work used the snob mixture modelling software @xcite .",
    "as pointed out by @xcite , snob does not have the functionality to account for the correlation between the data .",
    "we therefore study the bvm sine distributions and demonstrate how they can be integrated with our generalized mml - based mixture modelling method .",
    "we extend the search method described in @xcite to infer mixtures of bvm distributions . to infer the optimal number of mixture components , the mixture modelling apparatus is now modified to handle the directional data distributed on the surface of a torus . as in the case of the vmf and @xmath0  distributions , the split operation detailed in @xcite",
    "is tailored for the bvm mixtures . the basic idea behind splitting",
    "a parent component is to identify the means of the child components so that they are on either side of the parent mean and are reasonably apart from each other .",
    "recall that for a gaussian parent component , we computed the direction of maximum variance and selected the initial means , along this direction , that are one standard deviation away on either side of the parent mean .",
    "we employ the same strategy for bvm distributions . for data",
    "@xmath167 , where @xmath280 such that @xmath281 , we compute the direction of maximum variance in the @xmath282-space .",
    "this allows us to compute the initial means of the child components .",
    "the delete and merge operations are carried out in the same spirit . during merging bvm components ,",
    "the kl distance is evaluated to determine the closest pair .",
    "we derive the kl distance for bvm sine and bvm independent distributions as shown in appendix  [ app : bvm_sine_kldiv ] .",
    "further , in all the operations , the mml estimators of the bvm sine distribution , derived in section  [ subsec : bvm_sine_mml ] are used in the update step of the em algorithm .",
    "we consider the spatial orientations resulting from the interactions of the main chain atoms in protein structures .",
    "a protein main chain is comprised of a chain of amino acids , each os which is characterized by a central carbon @xmath283 .",
    "the angular data corresponds to the spatial orientations of the the planes containing the atoms from successive amino acids .",
    "a protein main chain is characterized by a sequence of @xmath284 and @xmath285 angles .",
    "these angles uniquely determine the geometry of the protein backbone structure @xcite .",
    "however , in a majority of protein structures , @xmath286 and , hence the sequence of @xmath287-c - n-@xmath287 atoms lie in a plane ( see the dotted planar representation in figure  [ fig : protein_dihedrals]a ) . as a result ,",
    "the angles @xmath288 and @xmath289 are typically analyzed @xcite .",
    "the angles @xmath288 and @xmath289 are called the dihedral angle pair corresponding to an amino acid residue with a central carbon atom @xmath287 along the protein main chain . geometrically , a dihedral angle is the angle between any two planes defined using four non - collinear points . in figure  [ fig : protein_dihedrals](a ) , @xmath288 is the angle between the two planes formed by c - n-@xmath283  and n-@xmath283-c .",
    "similarly , @xmath289 is the angle between the two planes formed by n-@xmath283-c and @xmath283-c - n .    the dihedral angles @xmath288 and @xmath289 are measured in a consistent manner .",
    "for example , in order to measure @xmath288 , the four atoms c - n-@xmath283-c are arranged such that @xmath288 is calculated as the deviation between n - c and @xmath283-c when viewed in some consistent orientation . as an illustration , in figure  [ fig : protein_dihedrals](b ) , view the arrangement of the four atoms through the n-@xmath283bond such that @xmath283  is behind the plane of the paper and n eclipses the @xmath283  atom .",
    "also , the c atom directly attached to n is at the 12 o clock position . in this orientation",
    ", @xmath288 is given as the angle of rotation required to align the n - c bond with the @xmath283-c bond in the plane of the paper .",
    "further , if it is a clockwise rotation , it is considered a positive value .",
    "this ensures that @xmath290 .",
    "the dihedral angle @xmath289 is measured by following the same convention with the four atoms being n-@xmath283-c - n .",
    "the @xmath282 pair measured in this way can be plotted on the surface of a 3d torus .",
    "each @xmath282 pair corresponds to a point on the toroidal surface .",
    "the angle @xmath288 is used to identify a particular cross - section ( circle ) of a torus , while @xmath289 locates a point on this circle ( see figure  [ fig : torus_phi_psi ] ) .",
    "we generate the entire set of dihedral angle data from the 1802 experimentally determined protein structures in the astral scop-40 ( version 1.75 ) database @xcite representing the `` @xmath291 class '' proteins .",
    "the number of @xmath282 dihedral angle pairs resulting from this data set is 253,165 .",
    "we model this generated set of dihedral angles using bvm sine distributions .",
    "a random sample from this empirical distribution consisting of 10,000 points is shown in figure  [ fig : empirical_distribution_torus ] .",
    "the plot is a heat map showing the density of the data distribution on the toroidal surface .",
    "note that there are regions on the torus which are highly concentrated ( yellow ) , corresponding to the helical regions in the protein .",
    "the ellipse - like patches ( mostly in blue ) roughly correspond to the @xmath291 strands in proteins .",
    "furthermore , the data is multimodal which motivates its modelling using mixtures of bvm distributions .",
    "we consider the effects of using the bvm sine distribution as compared to the bvm independent variant in this context .    )",
    "the figure shows the random sample from different viewpoints.,title=\"fig:\",scaledwidth=40.0% ] ) pairs .",
    "the figure shows the random sample from different viewpoints.,title=\"fig:\",scaledwidth=40.0% ]      the search method inferred a 32-component bvm independent mixture and terminated after 42 iterations involving split , delete , and merge operations . in the case of modelling using bvm sine distributions ,",
    "our search method inferred 21 components and terminated after 29 iterations . in each of these iterations , for every intermediate @xmath292-component mixture , each constituent component is split , deleted , and merged ( with an appropriate component ) to generate improved mixtures .    the progression of the search method for the optimal bvm independent mixture begins with a single component .",
    "the search method results in continuous split operations until the @xmath293 iteration when a 17-component mixture is inferred ( see figure  [ fig : protein_bvm_mix_evolution]a ) .",
    "this corresponds to a progressive increase in the first part of the message ( red curve ) . between the @xmath293 and the @xmath294 iterations ,",
    "we observe a series of delete / merge and split operations leading to a stable 19-component mixture .",
    "the search method again continues to favour the split operations until the @xmath295 iteration when a 26-component mixture is inferred .",
    "thereafter , a series of deletions and splits yield a stable 29-component mixture at the end of the @xmath296 iteration .",
    "the search method eventually terminates when a 32-component mixture is inferred with a characteristic step - like behaviour towards the end indicating perturbations involving split and delete / merge operations ( see figure  [ fig : protein_bvm_mix_evolution]a ) .    in the case of searching for the optimal bvm sine mixture , our proposed search method continues to split the components thereby increasing the mixture size .",
    "this occurs until 21 iterations . at this stage ,",
    "there are 21 mixture components .",
    "this can be observed in figure  [ fig : protein_bvm_mix_evolution](b ) , when the first part of the message ( red curve ) continually increases until the @xmath294 iteration . during this period ,",
    "observe that the second part ( blue ) and the total message length ( green ) continually decrease signifying an improvement to the mixtures .",
    "after the @xmath294 iteration , we observe a step - like behaviour as in the case of mixture modelling using the bvm independent distributions .",
    "the behaviour characterizes the reduction or increase in the number of mixture components corresponding to a decrease or increase to the first part of the message .",
    "after the @xmath297 iteration , we observe that the mixture has 22 components .",
    "however , the final mixture stabilizes in the subsequent iterations to a 21-component mixture . after the @xmath298 iteration , there is no further improvement to the total message length and the search method terminates .    we observe a characteristic increase in the mixture size initially followed by some perturbations that stabilize the intermediate mixture ( step - like behaviour ) , eventually resulting in an optimal mixture ( see figure  [ fig : protein_bvm_mix_evolution ] ) .",
    "there is an initial sharp decrease in the total message length until about 7 iterations for bvm mixtures .",
    "because of the multimodal nature of the directional data ( see figure  [ fig : empirical_distribution_torus ] ) , the initial increase in the number of components would explain the data distribution corresponding to those modes that are clearly distinguishable .",
    "this leads to a substantial improvement to the total message length as the minimal increase in the first part is dominated by the gain in the second part .",
    "however , towards the end of the search , when the increase in first part dominates the reduction in second part , the method stops .",
    "thus , we see the trade - off of model complexity ( as a function of the number of components and their parameters ) , and the goodness - of - fit being balanced using the search based on the mml inference framework .",
    "the existing work of mml - based mixture modelling of protein dihedral angles by @xcite inferred 27 clusters using the bvm independent distributions .",
    "in contrast , our search method inferred 32 clusters . however , their data consists of only 41,731 @xmath282 pairs generated from the protein structures known at that time . in contrast , we have used 253,165 pairs of dihedral angles along with a different search method as explained previously ( see section  [ sec : bvm_modelling_dihedrals ] ) .",
    "so , there is some consensus on the rough number of component distributions if the protein dihedral angles were modelled using bvm distributions assuming no correlation between @xmath288 and @xmath289 .",
    "the visualization of the dihedral angles is commonly done by the ramachandran plot @xcite who first analyzed the various possible protein configurations and represented them as a two - dimensional plot .",
    "an example of one such plot is provided in @xcite and reproduced here ( figure  [ fig : dihedral_angles_lovell ] ) .",
    "such a plot is indicative of the allowed conformations that protein structures can adopt .",
    "there are vast spaces in the dihedral angle space where few data are present .",
    "the conformations corresponding to those regions are not possible .",
    "we consider the plot to explain the similarities between our inferred mixture models and the one that is traditionally used .",
    "and @xmath289 are in degrees ) . plot taken from @xcite .",
    ", scaledwidth=75.0% ]    our resulting mixtures of bvm independent and the sine variants are shown in figure  [ fig : dihedral_angles_bvm_mix ] .",
    "the contours of the constituent components plotted in the @xmath282-space can be seen in the diagram . for visualization purposes , we display the contour of each component that corresponds to 80% of the data distribution .",
    "the data in figure  [ fig : dihedral_angles_bvm_mix ] corresponds to a random sample drawn from the empirical distribution ( same as in figure  [ fig : empirical_distribution_torus ] ) visualized in the @xmath282-space .     +    in figure  [ fig : dihedral_angles_lovell ] , we observe that the top - left region corresponds to the @xmath291 strands in protein structures .",
    "the empirical distribution of dihedral angles we generated also has this characterstic .",
    "we observe a concentrated mass in the top - left in figure  [ fig : dihedral_angles_bvm_mix ] .",
    "furthermore , our inferred mixtures are able to model this region using the appropriate components .",
    "note that smaller or highly compact contours correspond to bvm distributions that have greater concentration parameters ( @xmath13 and @xmath53 in equation  [ eqn : bvm_sine ] ) .",
    "we note that components numbered 1 - 11 ( figure  [ fig : dihedral_angles_bvm_mix]a ) and components 1 - 8 ( figure  [ fig : dihedral_angles_bvm_mix]b ) are used to describe this region .",
    "these correspond to the components of the bvm independent and bvm sine models respectively .",
    "clearly , more number of components are required to model roughly the same amount of data ( corresponding to the @xmath291 strands ) using the bvm independent mixture .    similarly , in figure",
    "[ fig : dihedral_angles_lovell ] , we observe another concentration of mass in the middle - left portion of the figure .",
    "this corresponds mainly to _ right - handed _",
    "@xmath299-helices , which are very frequent in protein structures . in figure",
    "[ fig : dihedral_angles_bvm_mix ] , we have the corresponding mass and also note the dense region ( bright yellow ) . as per our inferred mixtures , component 17 ( figure  [ fig : dihedral_angles_bvm_mix]a ) and component 12 ( figure  [ fig : dihedral_angles_bvm_mix]b )",
    "are used to predominantly describe this dense region .",
    "the other surrounding regions in the dihedral angle space of the right - handed helices are described by components 12 - 19 ( figure  [ fig : dihedral_angles_bvm_mix]a ) and by components 9 - 13 ( figure  [ fig : dihedral_angles_bvm_mix]b ) .",
    "again , we observe that the similar data is described using 8 components by the bvm independent mixture as opposed to 5 components by the bvm sine mixture .",
    "@xcite display another region of concentrated mass in the middle - right of figure  [ fig : dihedral_angles_lovell ] .",
    "this region corresponds to the infrequent _ left - handed _ helices in protein structures .",
    "we see a corresponding mass in the empirical distribution in figure  [ fig : dihedral_angles_bvm_mix ] .",
    "the components 20 - 25 of our inferred bvm indenpendent mixture describe this region ( figure  [ fig : dihedral_angles_bvm_mix]a ) .",
    "the same region is described by components 14 - 16 of our inferred bvm sine mixture ( figure  [ fig : dihedral_angles_bvm_mix]b ) .",
    "notice how this region is described by components 15 and 16 .",
    "these two components describe the dense mass within this region while component 14 is responsible for mainly modelling the data that is further away from this clustered mass .",
    "we again observe that the same region is modelled by greater number of components when using bvm independent distributions .",
    "the remaining mixture components describe the insignificant mass present in other regions of the dihedral angle space .",
    "the ability of our inferred mixtures to identify and describe specific regions of the protein conformational space in a completely unsupervised setting is remarkable .",
    "further , we have qualified the effects of using the bvm distributions which do not account for the correlation between the dihedral angle pairs . in this regard ,",
    "the bvm sine mixtures fare better when compared to mixtures of bvm independent distributions .",
    "we now quantify these effects in terms of the total message length .",
    "our proposed search method to infer an optimal mixture involves evaluating the encoding cost of the mixture parameters or the first part ( model complexity ) , and encoding the data using those parameters or the second part ( goodness - of - fit ) .",
    "the progression of the search method continues until there is no improvement to the total message length .",
    "we observe that the resulting 21-component bvm sine mixture has a first part of 966 bits and a corresponding second part of 5.735 million bits ( see table  [ tab : bvm_mml_mix ] ) .",
    "a bvm independent mixture with the same number of components has a first part of 872 bits and a corresponding second part of 5.751 million bits .",
    "although the model complexity is lower for the bvm independent mixture ( difference of @xmath300 bits ) , the bvm sine mixture has an additional compression of @xmath301 bits in its goodness - of - fit .",
    "thus , the significant gain in the second part dominates the minimal increase in the first part of the bvm sine mixture .",
    "further , if we compare the 21-component bvm independent mixture with the inferred 32-component bvm independent mixture , we observe that the first part is more in the 32-component case .",
    "this is expected because there are more number of mixture parameters to encode in the 32-component mixture .",
    "there is a difference of @xmath302 bits ( see table  [ tab : bvm_mml_mix ] ) .",
    "however , the 32-component mixture results in an extra compression of @xmath303 bits .",
    "so , the total message length is lower for the 32-component mixture , and is therefore , preferred to the 21-component bvm independent mixture .    [ tab : bvm_mml_mix ]    when the inferred 32-component bvm independent and the 21-component bvm sine mixtures are compared , we observe that the total message length is lower for the bvm sine mixture . in this case , both the first and second parts are lower for the sine mixture leading to an overall gain of about @xmath304 bits .",
    "thus , the bvm sine mixture is more appropriate as compared to the bvm independent mixture in describing the protein dihedral angles .",
    "this exercise shows how an optimal mixture model is selected by achieving a balance between the trade - off due to the complexity and the goodness - of - fit to the data .",
    "furthermore , as in the case of the vmf and @xmath0  distributions , we can devise null model descriptions of protein dihedral angles based on the bvm mixtures . for comparison",
    ", we consider a uniform distribution on the torus , which is referred to as the uniform null model in the equation below .",
    "@xmath305 where @xmath306 and @xmath307 are the radii that define the size of the torus ( see figure  [ fig : torus_phi_psi ] ) . when @xmath308 , the surface area of the torus is @xmath309 .",
    "the null models based on the bvm mixtures have the same form as the vmf and @xmath0  mixtures given as mixture distributions @xcite with the number of respective components being @xmath310 and @xmath311 corresponding to the independent and the sine variants respectively .    compared to the uniform model , both the bvm mixtures result in additional compression ( see table  [ tab : bvm_sine_null_models ] ) . the message length to encode the entire collection of 253,165 dihedral angle pairs using",
    "the uniform null model is 6.388 million bits which amounts to 25.234 bits per residue . in comparison , the bvm independent mixture results in a compression of 5.735 million bits which amounts to 22.656 bits per residue .",
    "the additional compression is therefore , close to 2.58 bits per residue ( on average ) .",
    "the bvm sine mixture further leads to an additional compression of 323 bits over the bvm independent mixture .",
    "this is equivalent to an additional saving of 0.0013 bits per residue ( on average ) .",
    ".comparison of the null model encoding lengths based on the uniform distribution on the torus , the 32-component bvm independent and the 21-component bvm sine mixtures . [ cols=\"^,^,^ \" , ]     [ tab : bvm_sine_null_models ]    these results indicate that the bvm mixtures are superior compared to the uniform model .",
    "this can be argued from the fact that the empirical distribution ( see figure  [ fig : empirical_distribution_torus ] ) has empty regions in the dihedral angle space .",
    "this is also confirmed from the ramachandran plot ( figure  [ fig : dihedral_angles_lovell ] ) .",
    "however , the bvm independent and the bvm sine variants are in close competition with each other . noting that we need more mixture components in the independent case and because the sine mixture can describe the data more effectively , we conclude that the bvm sine mixture supersedes the bvm independent mixture .",
    "the ability of the bvm sine mixture to model correlated data leads to improved description of the protein dihedral angles .",
    "we have considered the problem of modelling directional data using the bivariate von mises distributions .",
    "we have demonstrated that the mml - based estimation results in parameters that have a lower bias and mse compared to the traditional ml estimators , and contrast to map estimators , they are invariant to transformations of the parameter space . to model",
    "empirically distributed data with multiple modes , we have used mixtures of bvm distributions .",
    "we have addressed the important problems of selecting optimal number of mixture components along with their parameters using the mml inference framework .",
    "we employed the designed framework to model protein dihedral angles using mixtures of bvm distributions .",
    "the empirical distribution of the pairs of dihedral angles represented on a toroidal surface clearly suggests correlation between the angle pairs . as such ,",
    "the bvm sine mixtures are shown to be appropriate .",
    "both the bvm independent and the sine mixtures effectively model the dihedral angle space .",
    "the ability of the search method to correctly identify components corresponding to the regions of critical protein configurations is remarkable .",
    "this is more so because our search method does not rely on any prior information and infers the mixtures in a completely unsupervised setting .",
    "53 [ 1]#1 [ 1]#1 urlstyle [ 1]doi  # 1    [ 2][]#2    abramowitz m , stegun ia ( 1965 ) handbook of mathematical functions .",
    "dover , new york    akaike h ( 1974 ) a new look at the statistical model identification .",
    "ieee transactions on automatic control 19(6):716723    amos de ( 1974 ) computation of modified bessel functions and their ratios .",
    "mathematics of computation 28(125):239251    banerjee a , dhillon i , ghosh j , sra s ( 2003 ) generative model - based clustering of directional data . in : proceedings of the ninth international conference on knowledge discovery and data mining , new york , pp 1928    banerjee a , dhillon i , ghosh j , sra s ( 2005 ) clustering on the unit hypersphere using von mises - fisher distributions .",
    "journal of machine learning research 6:13451382    biernacki c , celeux g , govaert g ( 2000 ) assessing a mixture model for clustering with the integrated completed likelihood .",
    "ieee transactions on pattern analysis and machine intelligence 22(7):719725    boomsma w , kent jt , mardia kv , taylor cc , hamelryck t ( 2006 ) graphical models and directional statistics capture protein structure .",
    "interdisciplinary statistics and bioinformatics 25:9194    bozdogan h ( 1993 ) choosing the number of component clusters in the mixture - model using a new informational complexity criterion of the inverse - fisher information matrix . in : information and classification , studies in classification , data analysis and knowledge organization , springer berlin heidelberg , pp 4054    conway jh , sloane nja ( 1984 ) on the voronoi regions of certain lattices . siam journal on algebraic and discrete methods 5:294305    dempster ap , laird nm , rubin db ( 1977 ) maximum likelihood from incomplete data via the em algorithm .",
    "journal of the royal statistical society : series b ( methodological ) 39(1):138    dowe dl , allison l , dix ti , hunter l , wallace cs , edgoose t ( 1996 ) circular clustering of protein dihedral angles by minimum message length . in : pacific symposium on biocomputing ,",
    "vol  96 , pp 242255    figueiredo mat , jain ak ( 2002 ) unsupervised learning of finite mixture models .",
    "ieee transactions on pattern analysis and machine intelligence 24(3):381396    fisher r ( 1953 ) dispersion on a sphere",
    ". proceedings of the royal society of london a : mathematical , physical and engineering sciences 217(1130):295305    hamelryck t ( 2009 ) probabilistic models and machine learning in structural bioinformatics .",
    "statistical methods in medical research 18(5):505526    hamelryck t , kent jt , krogh a ( 2006 ) sampling realistic protein conformations using local structural bias .",
    "plos computational biology 2(9):e131    jain ak , duin rpw , mao j ( 2000 ) statistical pattern recognition : a review .",
    "ieee transactions on pattern analysis and machine intelligence 22(1):437    johnson sg ( 2014 ) the nlopt nonlinear - optimization package .",
    "http://ab-initio.mit.edu/nlopt    jupp pe , mardia kv ( 1980 ) a general correlation coefficient for directional data and related regression problems .",
    "biometrika 67(1):163173    kasarapu p ( 2015 ) modelling of directional data using kent distributions .",
    "kasarapu p , allison l ( 2015 ) minimum message length estimation of mixtures of multivariate gaussian and von mises - fisher distributions .",
    "machine learning 100(2 - 3):333378    kent jt ( 1982 ) the fisher - bingham distribution on the sphere .",
    "journal of the royal statistical society : series b ( methodological ) 44(1):7180    kent jt , hamelryck t ( 2005 ) using the fisher - bingham distribution in stochastic models for protein structure .",
    "quantitative biology , shape analysis , and wavelets 24:5760    lovell sc , davis iw , arendall wb , de  bakker piw , word jm , prisant mg , richardson js , richardson dc ( 2003 ) structure validation by c@xmath299 geometry : @xmath288 , @xmath289 and c@xmath291 deviation .",
    "proteins : structure , function , and genetics 50(3):437450    mardia kv ( 1975 ) characterizations of directional distributions .",
    "in : a modern course on statistical distributions in scientific work , vol  17 , springer , netherlands , pp 365385    mardia kv ( 1975 ) statistics of directional data ( with discussion ) .",
    "journal of the royal statistical society : series b ( methodological ) 37:349393    mardia kv , taylor cc , subramaniam gk ( 2007 ) protein bioinformatics and mixtures of bivariate von mises distributions for angular data .",
    "biometrics 63(2):505512    mardia kv , hughes g , taylor cc , singh h ( 2008 ) a multivariate von mises distribution with applications to bioinformatics .",
    "canadian journal of statistics 36(1):99109    mclachlan gj , basford ke ( 1988 ) mixture models : inference and applications to clustering ( statistics : textbooks and monographs ) .",
    "dekker , new york    mclachlan gj , peel d ( 2000 ) finite mixture models .",
    "wiley , new york    murzin ag , brenner se , hubbard t , chothia c ( 1995 ) scop : a structural classification of proteins database for the investigation of sequences and structures",
    ". journal of molecular biology 247(4):536540    oliver jj , baxter ra ( 1994 ) mdl and mml : similarities and differences ( introduction to minimum encoding inference ) .",
    "tech . rep .",
    ", monash university    oliver jj , baxter ra , wallace cs ( 1996 ) unsupervised learning using mml . in : proceedings of the thirteenthth international conference on machine learning , morgan kaufmann publishers , pp 364372    pearson k ( 1895 ) note on regression and inheritance in the case of two parents .",
    "proceedings of the royal society of london 58(347 - 352):240242    peel d , whiten wj , mclachlan gj ( 2001 ) fitting mixtures of kent distributions to aid in joint set identification .",
    "journal of the american statistical association 96(453):5663    powell mjd ( 1994 ) a direct search optimization method that models the objective and constraint functions by linear interpolation . in : advances in optimization and numerical analysis , kluwer academic publishers , dordrecht , netherlands , pp 5167    ramachandran gn , ramakrishnan ct , sasisekharan v ( 1963 ) stereochemistry of polypeptide chain configurations .",
    "journal of molecular biology 7(1):9599    richardson js ( 1981 ) the anatomy and taxonomy of protein structure .",
    "advances in protein chemistry , vol  34 , pp 167  339    rissanen j ( 1978 ) modeling by shortest data description .",
    "automatica 14(5):465471    rivest lp ( 1988 ) a distribution for dependent unit vectors .",
    "communications in statistics - theory and methods 17(2):461483    roberts sj , husmeier d , rezek i , penny w ( 1998 ) bayesian approaches to gaussian mixture modeling .",
    "ieee transactions on pattern analysis and machine intelligence 20(11):11331142    rosenblatt m ( 1952 ) remarks on a multivariate transformation .",
    "the annals of mathematical statistics 23(3):470472    schwarz g ( 1978 ) estimating the dimension of a model .",
    "the annals of statistics 6(2):461464    shannon ce ( 1948 ) a mathematical theory of communication .",
    "the bell system technical journal 27:379423    singh h , hnizdo v , demchuk e ( 2002 ) probabilistic model for two dependent circular variables .",
    "biometrika 89(3):719723    titterington dm , smith afm , makov ue ( 1985 ) statistical analysis of finite mixture distributions .",
    "wiley , new york    ueda n , nakano r , ghahramani z , hinton ge ( 2000 ) smem algorithm for mixture models .",
    "neural computation 12(9):21092128    wallace cs ( 1986 ) an improved program for classification . in : proceedings of the ninth australian computer science conference , pp 357366    wallace cs ( 2005 ) statistical and inductive inference using minimum message length .",
    "springer - verlag , secaucus , nj , usa    wallace cs , boulton dm ( 1968 ) an information measure for classification .",
    "the computer journal 11(2):185194    wallace cs , dowe dl ( 1994 ) estimation of the von mises concentration parameter using minimum message length . in : proceedings of the 12th australian statistical society conference , monash university , australia    wallace cs , dowe dl ( 1994 )",
    "intrinsic classification by mml  the snob program . in : proceedings of the seventh australian joint conference on artificial intelligence , world scientific , pp 3744    wallace cs , freeman pr ( 1987 ) estimation and inference by compact coding .",
    "journal of the royal statistical society : series b ( methodological ) 49(3):240265    watson gs , williams ej ( 1956 ) on the construction of significance tests on the circle and the sphere .",
    "biometrika 43(3 - 4):344352",
    "the analytical form of the kl distance between two bvm sine distributions is derived below . for a datum @xmath146 , where @xmath312 , let @xmath313 and @xmath314 be two bvm sine distributions whose probability density functions are given by equation  [ eqn : bvm_sine ] .",
    "let @xmath315 and @xmath316 be their respective normalization constants , whose expressions are given by equation  [ eqn : bvm_sine_norm_constant ] .",
    "the computation of the bvm sine normalization constant is presented in section  [ subsec : bvm_sine_norm_constant_derivatives ] .",
    "the kl distance between two probability distributions @xmath317 and @xmath318 is defined by @xmath319 $ ] . using the density function in equation  [ eqn : bvm_sine ] , we have @xmath320 = -\\log",
    "c_a   + \\kappa_{a1 } { \\ensuremath{\\mathbb{e}}}_a [ \\cos(\\theta_1 - \\mu_{a1 } ) ]   + \\kappa_{a2 } { \\ensuremath{\\mathbb{e}}}_a [ \\cos(\\theta_2 - \\mu_{a2 } ) ] + \\lambda_a { \\ensuremath{\\mathbb{e}}}_a [ \\sin(\\theta_1 - \\mu_{a1})\\sin(\\theta_2 - \\mu_{a2})]\\ ] ] the expressions for the above expectation terms @xmath321 $ ] , @xmath322 $ ] and @xmath323 $ ] can be computed and are given by equation  [ eqn : expect_sincos1 ] .",
    "similarly , the expectation of @xmath324 is @xmath325 = -\\log c_b   + \\kappa_{b1 } { \\ensuremath{\\mathbb{e}}}_a [ \\cos(\\theta_1 - \\mu_{b1 } ) ]   + \\kappa_{b2 } { \\ensuremath{\\mathbb{e}}}_a [ \\cos(\\theta_2 - \\mu_{b2 } ) ] + \\lambda_b { \\ensuremath{\\mathbb{e}}}_a [ \\sin(\\theta_1 - \\mu_{b1})\\sin(\\theta_2 - \\mu_{b2})]\\ ] ]    in order to compute @xmath326 $ ] , we express @xmath327 as @xmath328 given that @xmath329 = 0 $ ] ( equation  [ eqn : expect_sincos1 ] ) , we have @xmath330   & = \\cos(\\mu_{a1 } - \\mu_{b1})\\,{\\ensuremath{\\mathbb{e}}}_a[\\cos(\\theta_1 - \\mu_{a1 } ) ] \\\\",
    "\\text{similarly,}\\,\\ , { \\ensuremath{\\mathbb{e}}}_a [ \\cos(\\theta_2 - \\mu_{b2 } ) ]   & = \\cos(\\mu_{a2 } - \\mu_{b2})\\,{\\ensuremath{\\mathbb{e}}}_a[\\cos(\\theta_2 - \\mu_{a2 } ) ] \\end{aligned}\\ ] ]    in order to compute @xmath331 $ ] , we express the product of the sine terms as @xmath332 further , using the property that @xmath333 = { \\ensuremath{\\mathbb{e}}}[\\sin(\\theta_1-\\mu_{a1})\\cos(\\theta_2-\\mu_{a2 } ) ] = 0 $ ] ( equation  [ eqn : expect_sincos2 ] ) , we have @xmath334   & = \\cos(\\mu_{a1 } - \\mu_{b1})\\cos(\\mu_{a2 } - \\mu_{b2 } ) { \\ensuremath{\\mathbb{e}}}_a [ \\sin(\\theta_1 - \\mu_{a1})\\sin(\\theta_2 - \\mu_{a2 } ) ] \\\\ & + \\sin(\\mu_{a1 } - \\mu_{b1})\\sin(\\mu_{a2 } - \\mu_{b2 } ) { \\ensuremath{\\mathbb{e}}}_a [ \\cos(\\theta_1 - \\mu_{a1})\\cos(\\theta_2 - \\mu_{a2 } ) ] \\end{aligned}\\ ] ]    then , the kl distance between the two distributions @xmath317 and @xmath318 is derived as @xmath335 & =   \\log\\frac{c_b}{c_a } + \\{\\kappa_{a1 } - \\kappa_{b1}\\cos(\\mu_{a1 } -\\mu_{b1})\\}\\,{\\ensuremath{\\mathbb{e}}}_a[\\cos(\\theta_1 - \\mu_{a1 } ) ] \\notag\\\\ & + \\{\\kappa_{a2 } - \\kappa_{b2}\\cos(\\mu_{a2 } -\\mu_{b2})\\}\\,{\\ensuremath{\\mathbb{e}}}_a[\\cos(\\theta_2 - \\mu_{a2 } ) ] \\notag\\\\ & + \\{\\lambda_a - \\lambda_b \\cos(\\mu_{a1 } -\\mu_{b1 } ) \\cos(\\mu_{a2 } -\\mu_{b2})\\ } \\,{\\ensuremath{\\mathbb{e}}}_a[\\sin(\\theta_1 - \\mu_{a1})\\sin(\\theta_2 - \\mu_{a2})]\\notag\\\\ & - \\lambda_b\\sin(\\mu_{a1 } -\\mu_{b1 } ) \\sin(\\mu_{a2 } -\\mu_{b2 } ) \\label{eqn : bvm_sine_kldiv}\\end{aligned}\\ ] ] gives the analytical form of the kl distance of two bvm sine distributions .",
    "+ _ special case ( @xmath24 ) : _ the bvm sine distribution reduces to the product of two individual von mises circular distributions given by equation  [ eqn : bvm_ind ] . to compute the kl distance between two bvm independent distributions , we can use equation  [ eqn : bvm_sine_kldiv ] , with @xmath24 . note that for the von mises circular distribution",
    ", the normalization constant is @xmath336 , where @xmath337 and @xmath338 are the modified bessel functions .",
    "the kl distance between the bvm independent distributions @xmath317 and @xmath318 is then given by @xmath335 & =   \\log\\frac{i_0(\\kappa_{b1})}{i_0(\\kappa_{a1 } ) }   + \\frac{i_1(\\kappa_{a1})}{i_0(\\kappa_{a1 } ) }   \\ { \\kappa_{a1 } -\\kappa_{b1 } \\cos ( \\mu_{a1 } - \\mu_{b1 } ) \\ } \\notag\\\\ & + \\log\\frac{i_0(\\kappa_{b2})}{i_0(\\kappa_{a2 } ) }   + \\frac{i_1(\\kappa_{a2})}{i_0(\\kappa_{a2 } ) }   \\ { \\kappa_{a2 } -\\kappa_{b2 } \\cos ( \\mu_{a2 } - \\mu_{b2 } ) \\ }   \\label{eqn : bvm_ind_kldiv}\\end{aligned}\\ ] ]"
  ],
  "abstract_text": [
    "<S> the modelling of empirically observed data is commonly done using mixtures of probability distributions . in order to model angular data , directional probability distributions such as the bivariate von mises ( bvm ) </S>",
    "<S> is typically used . </S>",
    "<S> the critical task involved in mixture modelling is to determine the optimal number of component probability distributions . </S>",
    "<S> we employ the bayesian information - theoretic principle of minimum message length ( mml ) to distingush mixture models by balancing the trade - off between the model s complexity and its goodness - of - fit to the data . </S>",
    "<S> we consider the problem of modelling angular data resulting from the spatial arrangement of protein structures using bvm distributions . </S>",
    "<S> the main contributions of the paper include the development of the mixture modelling apparatus along with the mml estimation of the parameters of the bvm distribution . </S>",
    "<S> we demonstrate that statistical inference using the mml framework supersedes the traditional methods and offers a mechanism to objectively determine models that are of practical significance . </S>"
  ]
}