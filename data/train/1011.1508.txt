{
  "article_text": [
    "models are abstractions of reality and the goodness of a model is often judged by its ability to explain the observations which are the reflections of the underlying reality .",
    "the difference between the actual observation and the model counterpart of the observation is known as the _ prediction error _ or _",
    "forecast bias_. this error / bias is a result of either an `` inadequate '' model or a result of misspecifications of initial / boundary conditions and/or parameters in the model .",
    "recently , lakshmivarahan and lewis ( 2008 ) ( ll(2008 ) , hereafter ) have developed a framework for estimating the error in initial / boundary conditions and/or parameters that will account for the observed forecast bias . using the ( first - order ) sensitivity of the solution with respect to the initial conditions and parameters and the observed forecast errors , this framework recasts the bias estimation problem as an inverse problem , in particular as a linear least - squares problem . in ll(2008 )",
    ", they demonstrated the power of this idea using a simplified model consisting of a system of three coupled nonlinear ordinary differential equations that describes the evolution of the mixed - layer over the gulf of mexico and actual observations obtained from an earlier field experiment .",
    "for details , refer to ll(2008 ) and references therein .",
    "in this paper we further analyze the power of this framework using a scalar model for population growth known as the logistic model in continuous time .",
    "but instead of the first - order method used in ll(2008 ) , in this paper we compare the performance of first - order and second - order methods . for simplicity it is assumed that the model is adequate and the forecast errors are due only to errors in the initial condition and the parameter .",
    "as will become evident , the first - order method leads to a linear least - squares problem and the second - order method leads to a nonlinear least - squares problem .",
    "analysis of model inadequacies will be explored in a companion paper .",
    "section 2 contains a summary of the framework and is adapted from ll(2008 ) .",
    "properties of the logistic model are described in section 3 .",
    "numerical experiments related to the estimation of errors using the first - order and second - order methods along with a comparison of these methods are contained in section 4 .",
    "concluding observations are contained in section 5 .",
    "let @xmath0 denote the state of a dynamical system at time @xmath1 where @xmath2 is called the initial state .",
    "let @xmath3 and the state evolve according to a scalar nonlinear ordinary differential equation given by @xmath4 where @xmath5 is called the parameter .",
    "it is tacitly assumed that the function @xmath6 in satisfies all of the conditions required for the existence , uniqueness , and smoothness of the solution @xmath7 for all @xmath1 .",
    "let @xmath8 be a smooth function that defines the model counterpart of the observation @xmath9    let @xmath10 be the actual observation obtained from the field measurement . then @xmath11 denotes the prediction error or the forecast bias .",
    "it is assumed that the model in does not have any deficiencies and the forecast bias is mainly due to the misspecification of the initial condition , @xmath2 , and/or the parameter @xmath12 .",
    "let @xmath13 be the actual change in @xmath14 induced by the perturbations @xmath15 in @xmath2 and @xmath16 in @xmath12 .",
    "the goal is to find @xmath17 , the vector of perturbations such that @xmath18 let @xmath19 be the actual change in @xmath20 induced by the change @xmath13 in @xmath14 .",
    "then @xmath21 combining - , our goal is to find @xmath22 such that @xmath23 recall that @xmath24 where @xmath25 is called the @xmath26 variation of @xmath27 , the fraction of the induced change that is attributable to the @xmath28 derivative of @xmath27 and the change in @xmath13 in @xmath14 . similarly @xmath29 where @xmath30 , called the @xmath28 variation of @xmath31 , is the fraction of the total change in @xmath14 that is attributable to the @xmath28 partial derivatives of @xmath14 with respect to @xmath12 and @xmath2 and @xmath22 .    in practice",
    "we approximate the infinite sum by taking only the first @xmath32 terms , resulting in a @xmath33-order approximation given by @xmath34      setting @xmath35 , we get @xmath36 let @xmath37 . from first principles and",
    ", it follows that @xmath38 and @xmath39 the first derivative @xmath40 is called the first - order sensitivity of @xmath41 with respect to @xmath42 ( cacuci ( 2003 ) and cruz(1973 ) ) . setting @xmath43 \\in { \\ensuremath{\\mathbb{r}}}^{1\\mathsf{x}2},\\ ] ]",
    "can be rewritten as @xmath44 combining and with we obtain an underdetermined linear least - squares problem @xmath45 where @xmath46 suppose that there are @xmath47 observations @xmath48 , @xmath49 , ... @xmath50 available at times @xmath51 . then , at each time @xmath52 , @xmath53 , we have the forecast error @xmath54 define @xmath55 \\in { \\ensuremath{\\mathbb{r}}}^{1\\mathsf{x}2 } \\notag\\end{aligned}\\ ] ] and a diagonal matrix @xmath56 where @xmath57 for simplicity in notation .",
    "let @xmath58 \\in { \\ensuremath{\\mathbb{r}}}^{n\\mathsf{x}2}\\ ] ] @xmath59 and @xmath60 for this case of @xmath47 observations , in place of , we obtain a linear least - squares problem @xmath61 where @xmath62 , @xmath63 , and @xmath64 .",
    "the unknown @xmath22 is obtained by minimizing @xmath65 where it is tacitly assumed that the matrix @xmath66 is of full rank . under this assumption , @xmath22 is given by ( lewis , et al ( 2006 ) ) @xmath67 when @xmath66 is not of full rank , we invoke the tikhonov regularization ( lewis , et al ( 2006 ) ) using which @xmath22 is obtained by minimizing @xmath68 for some real constant @xmath69 , called the regularization parameter . the minimizing @xmath22 in this case",
    "is given by @xmath70 in place of tikhonov regularization , one could use the generalized inverse of the matrix @xmath66 to obtain @xmath71 where @xmath72 is the moore - penrose inverse ( lewis et al ( 2006 ) ) .      setting @xmath73 , in",
    ", it follows that @xmath74 where @xmath75 is given by is linear in @xmath22 . from first principles , we obtain @xmath76 which is quadratic in @xmath22 and @xmath77",
    "is the hessian of @xmath14 with respect to the entries of @xmath22 , that is @xmath78\\ ] ] where @xmath79 and @xmath80 .",
    "the second partial derivatives of @xmath41 are also called the second - order sensitivities of @xmath41 with respect to @xmath42 and @xmath81 . similarly , from , @xmath82 substituting in and using for the case of the single observation , we obtain a nonlinear least - squares problem . @xmath83 the unknown @xmath22 is then obtained by minimizing @xmath84 in the special case where the state is directly observable , @xmath85 and so @xmath86 and @xmath87 . substituting these into , in view of and",
    ", we get @xmath88 clearly , @xmath89 is a fourth degree polynomial in the components of @xmath22 and in general can have multiple minima which could further complicate the minimization problem .",
    "when there are @xmath47 @xmath90 observations at times @xmath91 , then we get @xmath47 versions of the relation , one for each time .",
    "the unknown @xmath22 is then obtained by minimizing @xmath92-\\frac{1}{2}d_{x(t_i)}^2(h)[{\\ensuremath{\\delta x}}(t_i)+\\delta^2x(t_i)]^2\\right\\|^2.\\ ] ] in the special case when @xmath85 , @xmath86 and @xmath87 and reduces to @xmath93 +",
    "consider the standard logistic equation that describes the growth of a certain population in an environment with carrying capacity 1 and growth rate parameter @xmath94 given by @xmath95 with @xmath96 as the initial condition .",
    "the solution of this nonlinear ordinary differential equation is given by @xmath97 notice that @xmath14 depends nonlinearly on @xmath98 and @xmath12 .",
    "it can be verified that @xmath99 as @xmath100 and @xmath101 $ ] for all @xmath102 whenever @xmath103 $ ] , for @xmath94 . since @xmath104 for all @xmath102 , and so is an increasing function which increases from @xmath98 to @xmath105 as @xmath102 increases from @xmath106 to @xmath107 .",
    "a typical plot of @xmath14 in for @xmath108 and @xmath109 is given in figure 1 .",
    "the various first - order sensitivities of the the solution with respect to @xmath110 are given by @xmath111 ^ 2 } \\label{dxox}\\\\ d_\\alpha(x ) & = \\frac{x_o(1-x_o)te^{\\alpha t}}{[1-x_o+x_o e^{\\alpha t}]^2 } = x_o(1-x_o)td_{x(0)}(x ) \\label{dax}\\end{aligned}\\ ] ] that is , @xmath112 is a multiple of @xmath113 .",
    "further it can be verified for large @xmath102 that @xmath114 thus , @xmath112 and hence @xmath115 tend to zero as @xmath100 .",
    "a plot of @xmath116 and @xmath115 versus @xmath102 for @xmath117 and @xmath109 is given in figure 2 .",
    "this in turn implies that @xmath14 becomes less sensitive with respect to @xmath12 and @xmath2 for large @xmath102 , and that for our framework to be effective , the observations have to be taken during the transient phase of the solution .",
    "the hessian of @xmath14 with respect to @xmath22 whose elements are the second - order sensitivity functions are given by @xmath118\\ ] ] where @xmath119 ^ 3}\\ ] ] @xmath120}{[1-x_o+x_o e^{\\alpha t}]^3}\\ ] ] and @xmath121}{[1-x_o+x_o e^{\\alpha t}]^3}.\\ ] ] it is assumed that @xmath14 is directly observable , and so @xmath85 . accordingly @xmath122 and @xmath123 .",
    "in this case reduces to and from and we get @xmath124 \\in { \\ensuremath{\\mathbb{r}}}^{1\\mathsf{x}2}.\\ ] ] two cases arise .",
    "the first case is when there are less observations than parameters .",
    "the second is when the number of observations is at least as great as the number of unknown parameters . in this paper",
    ", we only address the latter case . in this case , @xmath125 and @xmath126\\ ] ] it can be verified that matrix @xmath62 is of rank two and the least - square solution is given by @xmath127      expanding the right hand side of and dropping the terms of degree 3 and 4 , we obtain a quadratic approximation @xmath128 to @xmath89 given by @xmath129\\beta\\right].\\ ] ] the @xmath22 that minimizes this is given by @xmath130\\overline{h}(t_i)^{\\ensuremath{\\mathsf{t}}}e_{t_i}.\\ ] ]",
    "in this section we compare the performance of the first - order and the second - order methods using @xmath47 @xmath90 observations .      it is assumed that @xmath85 , that is , the state of the system @xmath14 is directly observable and that @xmath131 .",
    "we use the model solution @xmath132 starting from @xmath133 and @xmath134 to generate different sets of @xmath47 observations .",
    "the distribution of the @xmath47 observation times @xmath135 are given in table 1 where @xmath136 with @xmath137 ( fixed ) , @xmath32 = 1 , 4 , 8 and 12 and @xmath138 . here",
    "@xmath139 denotes the starting time and @xmath140 denotes the time between successive observations .",
    "thus , for example , for @xmath141 and @xmath142 , we generate @xmath143 observations at times @xmath144 , @xmath145 , @xmath146 , @xmath147 , @xmath148 , and @xmath149 , which corresponds to the second row of table 1 .",
    "the actual observations @xmath150 where @xmath151 is the solution of starting from @xmath152 and @xmath153 used in the estimation of @xmath22 are given in table 2 .",
    "let @xmath14 be the solution of the model starting from @xmath154 and @xmath155 .",
    "our goal is to estimate @xmath156 and @xmath157 using the set of observations in table 2 .",
    "we divide the experiment into three parts based on @xmath47 , the number of observations .",
    "+ : @xmath158 .",
    "we compute the estimates of @xmath22 using two observations at times @xmath139 and @xmath159 for @xmath32 = 1 , 4 , 8 and 12 and for @xmath139 = 0 , 4 and 8 .",
    "estimates of @xmath22 using the first - order and the second - order methods are given in the top part of tables 3 and 4 respectively .",
    "for example , referring to the top part of table 3 , the estimate of @xmath160 using two observations @xmath161 and @xmath162 and the first - order method is given by @xmath163 . since the actual value of @xmath164 and @xmath165 , the relative error in the estimate of @xmath166 and that of @xmath167 .",
    "similarly for all other patterns of @xmath158 observations .",
    "+ : @xmath168 .",
    "results of the estimation of @xmath22 using 4 observations at times @xmath139 , @xmath159 , @xmath169 and @xmath170 for @xmath32 = 1 , 4 , 8 and 12 and @xmath139 = 0 , 4 and 8 are given in the middle parts of tables 3 and 4 .",
    "+ : @xmath143 .",
    "results of the estimation of @xmath22 using 4 observations at times @xmath139 , @xmath159 , @xmath169 , ... , @xmath171 for @xmath32 = 1 , 4 , 8 and 12 and @xmath139 = 0 , 4 and 8 are given in the bottom parts of tables 3 and 4 .",
    "+      a number of observations are in order .    1 .",
    "when two observations are too close to each other , say by a distance @xmath137 , the solutions at these two times are very close to each other and consequently the sensitivity functions at time @xmath139 @xmath172 , @xmath173 and at time @xmath174 namely @xmath175 , @xmath176 are also very close to each other . this in turn implies that the two rows in @xmath177 are very nearly colinear , which leads to bad conditioning of the matrix @xmath178 .",
    "hence there is a greater chance for the estimates to be less reliable .",
    "2 .   from and",
    "figure 1 , it follows that the solution @xmath14 attains the steady state value which is independent of @xmath2 and @xmath12 .",
    "in other words , the sensitivity of @xmath14 with respect to @xmath2 and @xmath12 decrease to zero as @xmath102 increases .",
    "thus , from and it follows that @xmath179 and @xmath180 both tend to zero as @xmath102 increases .",
    "thus , the row of @xmath177 for large @xmath102 becomes zero leading to ill - conditioning of @xmath177 which makes the estimates less reliable .",
    "3 .   since the solution @xmath14 depends on @xmath2 and @xmath12 nonlinearly , we can not hope to recover the actual perturbations @xmath181 and @xmath16 that led to the forecast bias in the first place .",
    "while in principle the second - order method is better than the first - order counterpart , to improve the overall accuracy of the estimate , one may have to resort to an iterative improvement of the estimate .",
    "a detailed comparison of the performance of the iterative versions of the first and second order methods for small values of the perturbations ( @xmath182 and @xmath165 ) are given in tables 5 and 6 respectively .",
    "tables 7 and 8 contain similar results for large values of @xmath181 and @xmath16 .",
    "+ it turns out that the iterative versions of the first - order method peform at least as well as the iterative versions of the second - order counterpart .",
    "given that second - order methods require larger computation , from this exercise it follows that the iterative first - order method would be a good choice for estimating the forecast bias .",
    "cacuci , d.g . ( 2003 ) _ sensitivity and uncertainty analysis _ , chapman and hall / crc press , boca raton , fl +   + cruz , jr . j.b .",
    "( 1973 ) _ system sensitivity analysis _ , stroudsberg , pa . ( editor ) +   + lakshmivarahan , s. and j. lewis ( 2008 ) `` finding sources of bias error in forecast models : a framework , '' _ monthly weather review_. ( submitted ) +   + lewis , j. , s. lakshmivarahan , and s.k .",
    "dhall ( 2006 ) _ dynamic data assimilation _ , cambridge university press , 654 pages +       @xmath183 & @xmath184 & 4@xmath184 & 8@xmath184 & 12@xmath184 + @xmath139 & 0.0 & 0.0 & 0.0 & 0.0 + @xmath185 & 0.5 & 2.0 & 4.0 & 6.0 + @xmath186 & 1.0 & 4.0 & 8.0 & 12.0 + @xmath187 & 1.5 & 6.0 & 12.0 & 18.0 + @xmath188 & 2.0 & 8.0 & 16.0 & 24.0 + @xmath189 & 2.5 & 10.0 & 20.0 & 30.0 + @xmath183 & @xmath184 & 4@xmath184 & 8@xmath184 & 12@xmath184 + @xmath139 & 4.0 & 4.0 & 4.0 & 4.0 + @xmath185 & 4.5 & 6.0 & 8.0 & 10.0 + @xmath186 & 5.0 & 8.0 & 12.0 & 16.0 + @xmath187 & 5.5 & 10.0 & 16.0 & 22.0 + @xmath188 & 6.0 & 12.0 & 20.0 & 28.0 + @xmath189 & 6.5 & 14.0 & 24.0 & 34.0 + @xmath183 & @xmath184 & 4@xmath184 & 8@xmath184 & 12@xmath184 + @xmath139 & 8.0 & 8.0 & 8.0 & 8.0 + @xmath185 & 8.5 & 10.0 & 12.0 & 14.0 + @xmath186 & 9.0 & 12.0 & 16.0 & 20.0 + @xmath187 & 9.5 & 14.0 & 20.0 & 26.0 + @xmath188 & 10.0 & 16.0 & 24.0 & 32.0 + @xmath189 & 10.5 & 18.0 & 28.0 & 38.0 +     & + @xmath141 & 1 & 4 & 8 & 12 + @xmath190 & 0.50000000000000 & 0.50000000000000 & 0.50000000000000 & 0.50000000000000 + @xmath191 & 0.62245933120185 & 0.88079707797788 & 0.98201379003791 & 0.99752737684337 + @xmath192 & 0.73105857863000 & 0.98201379003791 & 0.99966464986953 & 0.99999385582540 + @xmath193 & 0.81757447619364 & 0.99752737684337 & 0.99999385582540 & 0.99999998477002 + @xmath194 & 0.88079707797788 & 0.99966464986953 & 0.99999988746484 & 0.99999999996225 + @xmath195 & 0.92414181997876 & 0.99995460213130 & 0.99999999793885 & 0.99999999999991 + & + @xmath196 & 1 & 4 & 8 & 12 + @xmath190 & 0.98201379003791 & 0.98201379003791 & 0.98201379003791 & 0.98201379003791 + @xmath191 & 0.98901305736941 & 0.99752737684337 & 0.99966464986953 & 0.99995460213130 + @xmath192 & 0.99330714907572 & 0.99966464986953 & 0.99999385582540 & 0.99999988746484 + @xmath193 & 0.99592986228410 & 0.99995460213130 & 0.99999988746484 & 0.99999999972105 + @xmath194 & 0.99752737684337 & 0.99999385582540 & 0.99999999793885 & 0.99999999999931 + @xmath195 & 0.99849881774326 & 0.99999916847197 & 0.99999999996225 & 1.00000000000000 + & + @xmath197 & 1 & 4 & 8 & 12 + @xmath190 & 0.99966464986953 & 0.99966464986953 & 0.99966464986953 & 0.99966464986953 + @xmath191 & 0.99979657302194 & 0.99995460213130 & 0.99999385582540 & 0.99999916847197 + @xmath192 & 0.99987660542401 & 0.99999385582540 & 0.99999988746484 & 0.99999999793885 + @xmath193 & 0.99992515377249 & 0.99999916847197 & 0.99999999793885 & 0.99999999999489 + @xmath194 & 0.99995460213130 & 0.99999988746484 & 0.99999999996225 & 0.99999999999999 + @xmath195 & 0.99997246430889 & 0.99999998477002 & 0.99999999999931 & 1.00000000000000 +    .estimates of @xmath22 using the first - order method .",
    "@xmath198 denotes the condition number of the matrix @xmath199 , where @xmath47 is the number of observations used in the estimation . [",
    "cols=\"^,^,^,^,^,^ \" , ]        & & & + @xmath139&@xmath200&@xmath12&@xmath181&@xmath16&iterations + 0&0.3 & 0.8 & 0.2000 & 0.2000 & 5 + & 0.3 & 0.9 & 0.2000 & 0.1000 & 5 + & 0.3 & 1.1 & 0.2000&-0.1000 & 5 + & 0.3 & 1.2 & 0.2000&-0.2000 & 5 + & 0.4 & 0.8 & 0.1000 & 0.2000 & 5 + & 0.4 & 1.1 & 0.1000&-0.1000 & 4 + & 0.4 & 0.9 & 0.1000 & 0.1000 & 4 + & 0.4 & 1.2 & 0.1000&-0.2000 & 4 + & 0.6 & 0.8&-0.1000 & 0.2000 & 4 + & 0.6 & 0.9&-0.1000 & 0.1000 & 4 + & 0.6 & 1.1&-0.1000&-0.1000 & 5 + & 0.6 & 1.2&-0.1000&-0.2000 & 5 + & 0.7 & 0.8&-0.2000 & 0.2000 & 4 + & 0.7 & 0.9&-0.2000 & 0.1000 & 5 + & 0.7 & 1.1&-0.2000&-0.1000 & 5 + & 0.7 & 1.2&-0.2000&-0.2000 & 5 + 4 & 0.3 & 0.8 & 0.2000 & 0.2000 & 7 + & 0.3 & 0.9 & 0.2000 & 0.1000 & 6 + & 0.3 & 1.1 & 0.2000&-0.1000 & 5 + & 0.3 & 1.2 & 0.2000&-0.2000 & 5 + & 0.4 & 1.1 & 0.1000&-0.1000 & 4 + & 0.4 & 1.2 & 0.1000&-0.2000 & 5 + & 0.4 & 0.8 & 0.1000 & 0.2000 & 6 + & 0.4 & 0.9 & 0.1000 & 0.1000 & 5 + & 0.6 & 0.8&-0.1000 & 0.2000 & 5 + & 0.6 & 0.9&-0.1000 & 0.1000 & 4 + & 0.6 & 1.1&-0.1000&-0.1000 & 6 + & 0.6 & 1.2&-0.1000&-0.2000 & 8 + & 0.7 & 0.8&-0.2000 & 0.2000 & 5 + & 0.7 & 0.9&-0.2000 & 0.1000 & 5 + & 0.7 & 1.1&-0.2000&-0.1000 & 7 + & 0.7 & 1.2&-0.2000&-0.2000 & 10 + 8 & 0.3 & 0.8 & 0.2000 & 0.2000 & 7 + & 0.3 & 1.1 & 0.2000&-0.1000 & 5 + & 0.3 & 0.9 & 0.2000 & 0.1000 & 7 + & 0.3 & 1.2 & 0.2000&-0.2000 & 7 + & 0.4 & 0.8 & 0.1000 & 0.2000 & 7 + & 0.4 & 0.9 & 0.1000 & 0.1000 & 6 + & 0.4 & 1.1 & 0.1000&-0.1000 & 5 + & 0.4 & 1.2 & 0.1000&-0.2000 & 11 + & 0.6 & 0.8&-0.1000 & 0.2000 & 6 + & 0.6 & 1.1&-0.1000&-0.1000 & 6 + & 0.6 & 1.2 & nan & nan&100 + & 0.6 & 0.9&-0.1000 & 0.1000 & 5 + & 0.7 & 1.1&-0.2000&-0.1000 & 9 + & 0.7 & 1.2 & nan & nan&100 + & 0.7 & 0.8&-0.2000 & 0.2000 & 6 + & 0.7 & 0.9&-0.2000 & 0.1000 & 5 +     & & & + @xmath139&@xmath200&@xmath12&@xmath181&@xmath16&iterations + 0 & 0.3 & 0.8&0.2000&0.2000 & 5 + & 0.3 & 0.9&0.2000&0.1000 & 5 + & 0.3 & 1.1&0.2000&-0.1000 & 5 + & 0.3 & 1.2&0.2000&-0.2000 & 5 + & 0.4 & 0.8&0.1000&0.2000 & 5 + & 0.4 & 0.9&0.1000&0.1000 & 5 + & 0.4 & 1.1&0.1000&-0.1000 & 4 + & 0.4 & 1.2&0.1000&-0.2000 & 4 + & 0.6 & 0.8&-0.1000&0.2000 & 4 + & 0.6 & 0.9&-0.1000&0.1000 & 4 + & 0.6 & 1.1&-0.1000&-0.1000 & 4 + & 0.6 & 1.2&-0.1000&-0.2000 & 4 + & 0.7 & 0.8&-0.2000&0.2000 & 4 + & 0.7 & 0.9&-0.2000&0.1000 & 4 + & 0.7 & 1.1&-0.2000&-0.1000 & 5 + & 0.7 & 1.2&-0.2000&-0.2000 & 5 + 4 & 0.3 & 0.8 & nan & nan&101 + & 0.3 & 0.9&0.2000&0.1000 & 5 + & 0.3 & 1.1&0.2000&-0.1000 & 5 + & 0.3 & 1.2&0.2000&-0.2000 & 7 + & 0.4 & 0.8&0.1000&0.2000 & 7 + & 0.4 & 0.9 & nan & nan&100 + & 0.4 & 1.1&0.1000&-0.1000 & 4 + & 0.4 & 1.2&0.1000&-0.2000 & 5 + & 0.6 & 0.8&-0.1000&0.2000 & 7 + & 0.6 & 0.9&-0.1000&0.1000 & 4 + & 0.6 & 1.1&-0.1000&-0.1000 & 5 + & 0.6 & 1.2&-0.1000&-0.2000 & 5 + & 0.7 & 0.8&-0.2000&0.2000 & 5 + & 0.7 & 0.9&-0.2000&0.1000 & 4 + & 0.7 & 1.1&-0.2000&-0.1000 & 5 + & 0.7 & 1.2&-0.2000&-0.2000 & 5 + 8 & 0.3 & 0.8 & nan & nan&100 + & 0.3 & 1.1&0.2000&-0.1000 & 6 + & 0.3 & 1.2&0.2000&-0.2000 & 5 + & 0.3 & 0.9&0.2000&0.1000 & 16 + & 0.4 & 0.8 & nan & nan&100 + & 0.4 & 0.9 & nan & nan&100 + & 0.4 & 1.1&0.1000&-0.1000 & 5 + & 0.4 & 1.2&0.1000&-0.2000 & 4 + & 0.6 & 0.8 & nan & nan&100 + & 0.6 & 0.9&-0.1000&0.1000 & 7 + & 0.6 & 1.1&-0.1000&-0.1000 & 5 + & 0.6 & 1.2&-0.1000&-0.2000 & 6 + & 0.7 & 0.8&-0.2000&0.2000 & 12 + & 0.7 & 0.9&-0.2000&0.1000 & 5 + & 0.7 & 1.1&-0.2000&-0.1000 & 5 + & 0.7 & 1.2&-0.2000&-0.2000 & 9 +"
  ],
  "abstract_text": [
    "<S> in this paper we describe a methodology for determining corrections to parameters and initial conditions in order to improve model forecasts in the presence of data . </S>",
    "<S> this methodology is grounded in the variational problem of minimizing the norm of the errors between forecast and data . </S>",
    "<S> the general method is presented , and then the method is applied to the specific example of a scalar model , the logistic equation , where it is shown that the method produces satisfactory results .    acknowledgement : we wish to record our sincere thanks to john lewis for his interest in this work . </S>",
    "<S> +   + </S>"
  ]
}