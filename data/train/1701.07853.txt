{
  "article_text": [
    "the internet is the main channel for dissemination of information in the 21st century .",
    "information of any kind is posted online and is spread via recommendations or advertisement@xcite .",
    "studying the dynamics of the spread of information through the internet , is a very relevant and challenging activity , since it can help the understanding of the factors which determine how far and how fast a given information can go . the most common way to observe information flow on the web ,",
    "is by tracking how many times a given piece is replicated by different users over a period of time .",
    "sometimes the content is modified as it is replicated , making it harder to track .",
    "in the specific case of news articles , a number of factors influence their spread . among the most important",
    "are the reputation of the original publisher  though that is not easy to measure , and the size of readership of a particular publisher , which will determine the initial spread of any piece .",
    "however the topology of the resulting network associated with dissemination of news can not be anticipated and will depend on the subject of each news piece and its resonance with public interests .    in this work we decided to look at the spread of news stories over the internet characterizing the resulting spread network and the dynamics of the spread .",
    "we start by looking at an actual case of news spread , and estimate the spread network by applying ideas of temporal networks and topic modeling , connecting similar articles within the bounds of temporal window of influence .",
    "then we postulate that the spread dynamics approximates an epidemic process and model it using a network sir model@xcite .",
    "the spread of ideas as an epidemic process is not a new idea@xcite , but here we propose new tools to estimate the spread network from data and compare it with simulated networks produced by an sir epidemic model .",
    "the data used for this study was obtained from the media cloud brasil project ( mcb ) which collects news articles from thousands of sources in the brazilian internet since 2013 . from the mcb database we obtained 2129 articles talking about the charlie hebdo terrorist attack in february 2015 .",
    "the articles span from the day of the attack to the end of march of 2015 .",
    "the data include the full text of the article , the url of publication and the date and time of the publication .          in order to calculate a measure of similarity between text documents one",
    "can rely on a number of metrics for textual distance described in the literature@xcite .",
    "most of these metrics are based on a bag - of - words representation of texts , meaning that texts are defined in terms of which words they contain and their frequency in the documents .",
    "such representation completely disregards higher level linguistic features of texts such as syntactics and semantics . in this analysis ,",
    "we want to use semantically similarity to describe the association between articles . in order for a news article to influence another",
    ", they must talk about the same concepts .    in order to capture the semantics of the articles we started by building a word vector representation for every word in our corpus vocabulary , taking into account the coocurrence of words within a sentence .",
    "this model is built from a larger corpus of news articles ( approximately 2.6 million articles ) according to the skip - gram model , which has been shown to map the words to a vector space where semantic similarity is highly correlated with the cosine distance between word vectors  @xcite .",
    "this larger corpus corresponded to the total collection of article of the mcb project .",
    "the importance of training the word vector model on a corpus as large as possible , is that one gets a more accurate semantic representation of each word as a vector .",
    "it is important that the larger corpus represents a similar informational space as the sample we are trying to analyze .",
    ".parameters of the skip - gram model . [ cols=\"<,^,^\",options=\"header \" , ]     the word vector model , was trained with the parameters described in table [ tab : pars ] .",
    "the fitted word vector model consists of a matrix of @xmath0 word vectors ( @xmath1 ) as rows .",
    "each row represents am @xmath2-dimensional feature vector , with @xmath3 : @xmath4 } w_1 & a_{11 } & a_{12 } & \\hdots & a_{1n } \\\\ w_2 & a_{21 } & a_{22 } & \\hdots & a_{2n } \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\ w_m &   a_{m1 } & a_{m2 } & \\hdots & a_{mn}\\\\ \\end{block } \\end{blockarray}\\ ] ]    from the word vectors obtained , we created document vectors defined as a weighted sum of word vectors . for a document @xmath5 containing @xmath6 distinct words ,",
    "its vector representation @xmath7 is given by    @xmath8    where @xmath9 is the weight of the word @xmath1 in the document @xmath5 .",
    "this weight can be calculated in different ways , for this work we used the tfidf score  @xcite of the word in the document .",
    "another possibility would be to use the frequency of the word in the document .    from the weighted sum we obtain document vectors which can be represented by the matrix below    @xmath10 } d_1 & a_{11 } & a_{12 } & \\hdots & a_{1n }",
    "\\\\ d_2 & a_{21 } & a_{22 } & \\hdots & a_{2n } \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\ d_m &   a_{m1 } & a_{m2 } & \\hdots & a_{mn}\\\\ \\end{block } \\end{blockarray}\\ ] ]    now we can define the similarity between two documents @xmath11 as the cosine of the angle @xmath12 between their vector representations : @xmath13      once the similarity of two articles is calculated , their temporal association must be determined in order to consider the probability of the older article being the _ infector _ of the other . in order to determine the most - likely _ infector _ of an article",
    ", we ranked all articles by date of publications and looked within a fixed time window preceding the publication of each article , for the articles which are most semantically similar .",
    "the choice of the size of the time window was determined in order encompass the majority ( @xmath14% ) of previous similar articles ( see figure [ fig : influence_range ] ) .      to reconstruct the spread network of the news",
    ", we defined the nodes of our network as the articles published on the subject chosen , and the edges as the infection events , i.e. , for every article after the first one , it must have been influenced ( infected ) by a previously published article . to qualify as an infector",
    ", an article must precede the infected article by less than @xmath15 hours , and have a score of similarity ( defined by ( [ eq : cos_sim ] ) ) to the infected article of at least @xmath16 .",
    "the reconstruction procedure is summarized in the four steps below .    1 .   rank all articles in ascending publication time .",
    "let @xmath17 denote the publication date of article @xmath18 .",
    "2 .   create upper triangular matrix @xmath19 , where @xmath20 and @xmath21 .",
    "@xmath22 is the heaviside function .",
    "3 .   create similarity matrix @xmath23 .",
    "where @xmath24 is the similarity defined by equation ( 2 ) whenever @xmath25 and @xmath26 otherwise .",
    "4 .   for each article @xmath27",
    ", we define its influencer @xmath18 as the article corresponding to @xmath28 ( see matrix [ fig : sim_matrix ] ) .",
    ", it shows the first 6 articles . in red",
    "are the maximum similarity score for each column , which we use to define it s infector , per example , the article 4 has been infected by the article 2 . ]      to test the hypothesis that news spread follows an epidemic process , we proposed an sir model for the spread , following the formalism of @xcite . in this formalism , instead of modeling the status of a given individual as",
    "susceptible ( s ) , infectious ( i ) or recovered ( r ) , we model the probability of each article being in each of the states , in this case , an * s * article would be one which has yet to be published , an * i * one which is published and has been infected by the story and an * r * is one which is too old to influence new articles .",
    "this modeling leads us to equations [ eq : sir ] .",
    "align & = -^i_i(t ) + _",
    "i^s(t)_j=1^n a_ij_j^i(t ) + & = - _ i^s(t ) _ j=1^n",
    "a_ij_j^i(t )    [ eq : sir ]    in equations [ eq : sir ] , @xmath29 is the probability of article @xmath18 being in the infectious state at time @xmath30 , similarly for @xmath31 ; @xmath32 is the probability of article @xmath27 being influenced by @xmath18 and comes from the adjacency matrix of the network .",
    "@xmath33 is an adimensional transmission parameter given by @xmath34 .",
    "time ( @xmath30 ) in these equations is also adimensional as it is scaled by @xmath35 .",
    "the network for the simulation is built from the same node set of the empirical data .",
    "the adjacency matrix @xmath36 is given by @xmath37    where @xmath38 is the number of times an article from publisher @xmath39 ( the publisher of article @xmath18 ) , has infected an article from publisher @xmath40 ( the publisher of article @xmath27 ) and @xmath41 is the total number of articles from publisher @xmath40 that have been infected , regardless of publisher .",
    "these counts are derived from the empirical dataset .",
    "the solution of this model generates the temporal dynamics of the probabilities described in ( [ eq : sir ] ) . from the solutions , @xmath42 and @xmath43 we can derive realizations of states for each article , @xmath44 , @xmath45 , and @xmath46 .    to reconstruct the states , we must sample from the probability distribution the states at each time @xmath30 , conditioning on the previous state .",
    "we follow the procedure :    1 .",
    "let @xmath47 , @xmath48 and @xmath49 be binary state vectors from article states at time @xmath30 , where 1 means the article is in that state .",
    "iterate from @xmath50 until the final time step available .",
    "3 .   for each time",
    "@xmath51 generate a newly infected @xmath52 vector , in which each element @xmath18 is a realization of a bernoulli event with probability given by @xmath53 $ ] .",
    "4 .   similarly to the previous step , sample a new @xmath49 vector , in which each element @xmath18 is a realization of a bernoulli event with probability given by @xmath54 $ ] .",
    "update @xmath55 6 .",
    "update @xmath56      from the state matrix @xmath57 we have which articles get infected at each time @xmath30 . to create a spread network for the simulation we need to define the infectors for each time . for",
    "that we used the probability matrix @xmath36 defined by equation ( [ eq : a ] ) .",
    "the following steps describe the entire procedure .    1 .",
    "let @xmath48 be binary state vectors for articles at time t , where 1 means the article is infected .",
    "2 .   iterate from @xmath58 until the final time step available .",
    "3 .   for each article",
    "@xmath18 infected at @xmath48 , obtain its probable infectors , @xmath59 , by multiplying @xmath60 by the column @xmath27 of matrix @xmath36 , where @xmath61 . where the values are the probability of each article @xmath27 from @xmath62 has to infect @xmath18 ( @xmath32 of the matrix a ) .",
    "4 .   define the infector of @xmath18 by sampling from a multinomial distribution with @xmath63 .",
    "the figure [ fig : sim_net ] shows the procedure :",
    "the dataset used is the result of a very specific search on a news articles database , therefore we can expect to the articles to display a great similarity among themselves .",
    "figure [ fig : sim_pair ] , shows the distribution of pairwise similarities that were used to construct the empirical influence network .",
    "when we look to the most similar pair for each article we can notice that for almost every article there is at least one other with similarity equal or greater than 0.8 , as we can see in figure [ fig : sim_most ] .",
    "identical articles ( similarity equals to 1 ) were not considered for edge formation .",
    "figure [ fig : sim_thresh ] shows the similarity threshold of the influence network . in order to have a giant component in the network that contains at least @xmath64 of our articles , we need to consider a minimum of @xmath65 score similarity .",
    "therefore , we defined @xmath66 .",
    "0 432 288        to determine the optimal time window @xmath15 in which to search for influencers , we looked at the distribution of time lags from the most similar article ( most likely influencer ) at various window lengths ( figure [ fig : influence_range ] ) . even for time windows as long as 15 days , 95% of the influencers where within 7 days of the articles they influenced .        to create the spread network , we defined influence based on the time lag from each pair of articles and from their similarity . following the previous analysis , we defined @xmath66 and @xmath67 , that means , the infector must preceded the infected article by less than 168 hours ( 7 days ) and have at least a 0.8 score of similarity .",
    "we can see in figure [ fig : net ] how our network looks like",
    ".          looking at the publication date distribution ( figure [ fig : data_dist ] ) we notice that the maximum number of articles published in a day was between 250 and 300 .",
    "we derive the simulation parameters from this distribution .",
    "for example , on figure [ fig : lambda ] we plot the peak of the infection for a range(@xmath68 $ ] ) of @xmath33 values . from that distribution of peak magnitudes we selected a lambda to match the empirical peak : @xmath69 .        .",
    "the blue area is the area where the peak of the simulation is the same as the peak of the dataset distribution , threfore is the area where the @xmath33 values were tested for our simulation.,title=\"fig : \" ] 432 [ fig : lambda ]    from the simulation ( figure [ fig : simu ] ) we obtain the state matrix , which we use to compare the simulated infection distribution with the original data .",
    "then we ran 10 thousand simulations to show that the model proposed matches the real world dynamics of news articles influences ( figure [ fig : sim_emp ] ) .    .",
    "each curve represents the n the infectious state as a function of time , for every article .",
    "the time units are @xmath70 . ]",
    "in this paper , we presented a methodology for reconstructing the network representing the spread of news in digital media .",
    "the results proposed started from a well defined subset of articles with high semantic similarities .",
    "however we believe the criteria of similarity used to reconstruct the network would work even on a random sample or articles , provided that it was large enough to contain a good portion of the putative spread network one is trying to characterize . in other words , the reconstruction algorithm can be used to detect contagious structures within any large enough collection of news articles .",
    "we also demonstrated that a classical sir process over the network is driving the spread dynamics .",
    "this means that the if one is able to observe the start of the spread , the overall reach and time of persistence in the media can be predicted from analytical results available for the sir model .",
    "the news subject selected , `` charlie hebdo attack '' , represents a very spontaneous media coverage given the great surprise with which it happened , but also due the homogeneous response of the global media condemning the cowardly attack .",
    "we believe that deviations from the classical sir dynamics shown here can hint at some form of media manipulation , but that hypothesis remains to be tested based on well defined cases , such as purchased media coverage during political campaigns , etc . with the current concerns about  fake \" media pieces  @xcite , perhaps the methods presented here can help to discriminate authentic media articles from fake ones based on their spread dynamics or influence patterns . we already began to see some attempts to automatically detect fake news  @xcite , but they mostly rely on linguistic cues .",
    "we believe that qualitative and quantitative aspects of the spread networks can also be of use ."
  ],
  "abstract_text": [
    "<S> news spread in internet media outlets can be seen as a contagious process generating temporal networks representing the influence between published articles . in this article </S>",
    "<S> we propose a methodology based on the application of natural language analysis of the articles to reconstruct the spread network . from the reconstructed network , </S>",
    "<S> we show that the dynamics of the news spread can be approximated by a classical sir epidemiological dynamics upon the network . from the results </S>",
    "<S> obtained we argue that the methodology proposed can be used to make predictions about media repercussion , and also to detect viral memes in news streams .    </S>",
    "<S> news , sir model , epidemics , temporal networks </S>"
  ]
}