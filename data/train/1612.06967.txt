{
  "article_text": [
    "the likelihood function for a complex multivariate model may not be available or very difficult to evaluate , and a composite likelihood function constructed from low - dimensional marginal or conditional distributions has become a popular alternative ( varin , 2008 ; varin , reid & firth , 2011 ) .",
    "suppose @xmath0 is a @xmath1-dimensional random vector with probability density function @xmath2 , with a @xmath3-dimensional parameter vector @xmath4 .",
    "given a set of likelihood functions @xmath5",
    ", @xmath6 , defined by the joint or conditional densities of some sub - vectors of @xmath0 , the composite likelihood function ( lindsay , 1988 ) is defined as @xmath7 where the @xmath8 s are nonnegative weights and the component likelihood @xmath5 might depend only on a sub - vector of @xmath9 .",
    "the choice of @xmath5 and the weights @xmath10 is critical for improving the efficiency of the resulting statistical inference ( lindsay , 1988 ; joe & lee , 2009 ; lindsay , yi & sun , 2011 ) . in this paper",
    "we focus on the two most commonly used composite likelihood functions in literature , independence likelihood and pairwise likelihood , which are defined as @xmath11 and @xmath12 , respectively . given a random sample @xmath13 , where each @xmath14 is a @xmath1-dimensional vector , the composite log - likelihood function is @xmath15 and the maximum composite likelihood estimator ( mcle ) is @xmath16 .",
    "in addition to the computational simplicity , the composite likelihood function has many appealing theoretical properties . in particular , under some regularity conditions , @xmath17 is consistent and asymptotically normally distributed with variance equal to the inverse of the godambe information matrix : @xmath18 ( lindsay , 1988 ; varin , 2008 ; xu & reid , 2011 ) . here",
    "@xmath19 is the sensitivity matrix , and @xmath20 is the variability matrix , with the composite score function @xmath21 . throughout this paper",
    "we use @xmath22 to denote the fisher information matrix of the full likelihood function . given two composite likelihood functions @xmath23 and @xmath24 , @xmath24",
    "is said to be _ more efficient _ than @xmath23 if @xmath24 has a greater godambe information matrix than @xmath23 in the sense of matrix inequality .",
    "it is well known that the full likelihood function is more efficient than any other composite likelihood function under regularity conditions ( godambe , 1960 ; lindsay , 1988 ) , i.e. @xmath25 is non - negative definite .",
    "in general , the second bartlett identity does not hold for composite likelihood functions , i.e. @xmath26 . after lindsay ( 1982 ) , we call a composite likelihood @xmath27 _ information - unbiased _ if @xmath28 , and _ information - biased _ , otherwise",
    ". composite likelihood - based inferential tools have been developed for hypothesis testing ( chandler & bate , 2007 ; pace , salvan , & sartori , 2011 ) and model selection ( varin & vidoni , 2005 ; gao & song , 2010 ) .",
    "information bias of a composite likelihood can make the resulting inference more difficult .",
    "for example , if the composite likelihood is information - unbiased , the likelihood ratio statistic has the same asymptotic chi - square distribution as its full likelihood counterpart . on the other hand",
    ", if it is information - biased the likelihood ratio statistic converges in distribution to a weighted sum of some independent @xmath29 random variables ( kent , 1982 ) .",
    "adjustments have been proposed to the information - biased composite likelihood ratio statistic such that the adjusted statistic has an asymptotic chi - square distribution ( e.g. , chandler & bate , 2007 ; pace , salvan , & sartori , 2011 ) .",
    "the full likelihood function is information - unbiased , but an information - unbiased composite likelihood is not necessarily fully efficient .",
    "in fact , any component likelihood function @xmath5 is information - unbiased .",
    "more generally , any composite likelihood function as the product of component likelihoods with mutually uncorrelated score functions is information - unbiased . as an example , consider a @xmath1-dimensional vector @xmath30 with density function @xmath31 , and defining @xmath32 .",
    "it is easy to show that the covariance between the score function of @xmath33 and the score function of @xmath34 is zero for any @xmath35 .",
    "hence any composite likelihood of the form @xmath36 where @xmath37 , is information - unbiased .",
    "conversely , an information - biased composite likelihood function can be fully efficient .",
    "the pairwise likelihood function for the equal - correlated multivariate normal model in section 2 is fully efficient when estimating the common variance @xmath38 and the correlation coefficient @xmath39 ( cox and reid , 2004 ) , but it is not information - unbiased ( pace , salvan , & sartori , 2011 ) .",
    "a sufficient and necessary condition for a composite likelihood to be fully efficient is given in the following theorem .",
    "theorem 1.suppose the full likelihood function @xmath40 has the score function @xmath41 and fisher information @xmath22 .",
    "then , for any composite likelihood function @xmath42 with the score function @xmath43 , sensitivity matrix @xmath44 , variability matrix @xmath45 and godambe information @xmath46 , @xmath47 if and only if @xmath48 with probability @xmath49 for a constant vector @xmath50 with respect to the random vector @xmath51 .    proofit is easy to show that @xmath52cov@xmath53 ( lindsay , 1988 ) .",
    "as @xmath54var@xmath55 and @xmath56var@xmath57 , the result follows as the difference @xmath58 is the covariance matrix of @xmath59 .",
    "theorem 1 with @xmath60 gives a sufficient condition for the maximum composite likelihood estimator to coincide with the mle ( kenne pagui , salvan & sartori , 2014 ) , which is satisfied by the pairwise likelihood for closed exponential family models ( mardia et al .",
    ", 2009 ) . in particular",
    ", the equicorrelated multivariate normal model with unknown variance @xmath38 and correlation coefficient @xmath39 belongs to the closed exponential family , and it has been shown that @xmath60 and @xmath61 or equivalently , @xmath62 for the pairwise likelihood function ( pace , salvan , & sartori , 2011 ) .",
    "its application in more general exponential family models has been studied in kenne pagui , salvan , & sartori ( 2014b ) .    in this paper",
    "we explore the impact of information bias on the composite likelihood inference in more detail . in section 2",
    "we show through the equicorrelated multivariate normal model that an information - biased composite likelihood may lead to less efficient estimates of the parameters of interest when the nuisance parameters are known .",
    "a sufficient condition is also provided for the occurrence of such a paradoxical phenomenon .",
    "we would expect that a more efficient composite likelihood can be obtained by incorporating additional independent component likelihoods or using higher dimensional component likelihoods .",
    "however such strategies do not always work for information - biased composite likelihood functions , as shown in section 3 .",
    "we conclude with a discussion in section 4 .",
    "in the presence of nuisance parameters , it is well known that the maximum likelihood estimator of the parameter of interest will have a smaller asymptotic variance when the nuisance parameters are known .",
    "it is easy to check that this also holds for information - unbiased composite likelihood functions .",
    "suppose the @xmath3-dimensional parameter vector @xmath9 is partitioned as @xmath63 , where @xmath64 is a @xmath65-dimensional parameter vector of interest and @xmath66 is a @xmath67-dimensional nuisance parameter vector , @xmath68 .",
    "the godambe information matrix of a information - unbiased composite likelihood is @xmath69 , and @xmath70 where @xmath71 is the @xmath72 submatrix of @xmath46 pertaining to @xmath64 , and @xmath73 the @xmath74 sub - matrix of @xmath46 pertaining to @xmath66 .",
    "when @xmath66 is unknown , the asymptotic variance of the mcle of @xmath64 is given by @xmath75 ; when @xmath66 is known , the asymptotic variance of the mcle of @xmath64 can be shown to be @xmath76 .",
    "since @xmath77 is a nonnegative matrix , we have @xmath78 .",
    "however , the reverse relationship may be observed for an information - biased composite likelihood , which is illustrated through the equicorrelated multivariate normal model in the rest of this section . from previous section we know that the pairwise likelihood @xmath79 is information - biased for this model .",
    "* example 1 .",
    "* suppose @xmath80 are @xmath81 independent observations from the same @xmath1-dimensional multivariate normal distribution with zero mean and covariance matrix @xmath82 , where @xmath83 is identity matrix and @xmath84 is a @xmath85 matrix with all entries equal to @xmath49 .",
    "the common correlation coefficient @xmath39 is the parameter of interest .",
    "the equicorrelated multivariate normal model has been well studied to compare the efficiency of pairwise likelihood and full likelihood in different settings ( arnold & strauss , 1991 ; cox & reid , 2004 ; mardia et al . , 2009 ) : when @xmath38 is unknown , the maximum pairwise likelihood estimator of @xmath39 , denoted as @xmath86 , is identical to the mle of @xmath39 and hence fully efficient ; when @xmath38 is known , the maximum pairwise likelihood estimator , denoted as @xmath87 , is less efficient than the maximum likelihood estimator of @xmath39 .",
    "here we are interested in comparing the asymptotic variances of @xmath86 and @xmath87 .",
    "the asymptotic variance of @xmath87 is ( cox & reid , 2004 ) @xmath88 where @xmath89 .",
    "the asymptotic variance of @xmath86 can be shown to be @xmath90    comparing the equations ( [ eq : known ] ) and ( [ eq : unknown ] ) , we find that as @xmath39 approaches its lower bound @xmath91 , @xmath92 decreases to zero while @xmath93 does not .",
    "the ratio of the asymptotic variances , avar@xmath94 , as a function of @xmath39 is plotted in figure  [ fig2 ] for @xmath95 .",
    "we can see that when @xmath39 is positive , @xmath87 is more efficient than @xmath86 ; when @xmath96 , the opposite phenomenon is observed , and when @xmath39 approaches the lower bound @xmath97 , this ratio diverges to infinity .",
    "we performed the comparisons for different @xmath1 and observed the same phenomenon .",
    "= avar@xmath94 at @xmath95",
    ". the vertical and horizontal dashed line denotes @xmath98 and @xmath99 respectively.,height=283 ]    to see that information - biasedness is not a sufficient condition for the paradox to occur , we consider another information - biased composite likelihood function , the full conditional likelihood for the same model : @xmath100 where @xmath101 denotes the random vector excluding @xmath102 .",
    "when @xmath38 is unknown , the maximum full conditional likelihood estimator of @xmath39 , @xmath103 is identical to @xmath104 and fully efficient ( mardia et al . , 2009 )",
    "; when @xmath38 is known , the maximum full conditional likelihood estimator , @xmath105 is less efficient than the maximum likelihood estimator for @xmath106 . using the formula in mardia , hughes , & taylor ( 2007 ) ,",
    "the ratio of the asymptotic variances , avar@xmath107 , as a function of @xmath39 is plotted in figure  [ fig3.3 ] for @xmath95 .",
    "we can see that the ratio is less than @xmath49 for all @xmath108 $ ] .     at @xmath95 .",
    "the horizontal dashed line denotes @xmath99.,height=302 ]      for the inference based on unbiased estimating equations , henmi & eguchi ( 2004 ) provided a sufficient condition for the occurrence of paradox in the presence of nuisance parameters , which also applies to the information - biased composite likelihood since its score function is a special unbiased estimating equation :    proposition 1.suppose the composite likelihood function @xmath42 is information - biased and @xmath63 with the maximum composite likelihood estimators @xmath109 .",
    "we define @xmath110 , the mcle of @xmath64 when @xmath66 is known .",
    "then , @xmath111 has a smaller asymptotic variance than @xmath110 if @xmath112 and @xmath111 are asymptotically independent but @xmath112 and @xmath113 are dependent .    in the example of equicorrelated multivariate normal model , denote by @xmath114 the maximum pairwise likelihood estimator of @xmath38 , we can show that the asymptotic covariance between @xmath104 and @xmath114 is @xmath115 which goes to @xmath116 as @xmath39 approaches @xmath91 while the asymptotic covariance between @xmath87 and @xmath114 is not equal to zero at @xmath117 .",
    "this may explain why the paradox occurs when @xmath39 is close to its lower bound @xmath91 .",
    "in this section we consider the inference on a single parameter only , and the simple illustrative examples allow us to calculate the godambe information matrices or asymptotic variances analytically .",
    "consider a set of information - unbiased composite likelihood functions @xmath118 with mutually uncorrelated score functions , it is easy to show that the product @xmath119 is also information - unbiased and has godambe information matrix @xmath120 where @xmath121 is the information matrix of @xmath122 .",
    "however , as shown in the example below , if any composite likelihood @xmath122 is information - biased , then information additivity may not hold for the product of uncorrelated composite likelihoods .",
    "* example 2 .",
    "* suppose the random vector @xmath123 follows a normal distribution with mean vector @xmath124 and covariance matrix @xmath125    assume @xmath38 is known , @xmath126 and @xmath39 are unknown , and @xmath126 is the only parameter of interest .",
    "suppose we use the independence likelihood function @xmath127 , which is free of the nuisance parameter @xmath39 , to estimate @xmath126 . to incorporate the information contained in the independent variable @xmath128",
    ", we also consider the composite likelihood function @xmath129 .",
    "it is easy to show that the maximum composite likelihood estimators for @xmath130 and @xmath131 are @xmath132 and @xmath133 with variances @xmath134 and @xmath135/n(1 + 2\\sigma^2)^2 $ ] respectively .",
    "we can compare the variances of the two maximum composite likelihood estimators directly .",
    "for example if @xmath136 , the variance of @xmath137 is @xmath138 which is smaller than @xmath139 if and only if @xmath140 .",
    "note that if @xmath141 this result is expected as @xmath142 determines @xmath126 exactly with @xmath143 ; but the dependence on @xmath38 of the range of @xmath39 over which @xmath128 degrades the inference is surprising ; as @xmath38 increases this range approaches @xmath144 .",
    "intuitively , a composite likelihood with higher dimensional component likelihoods should achieve a higher efficiency , although it usually demands more computational cost . in this subsection",
    "we focus on comparing the independence likelihood @xmath145 and the pairwise likelihood @xmath146 .",
    "@xmath79 can be written as the product of @xmath147 and some pairwise conditional likelihood functions . under independence ,",
    "@xmath148 is identical to the full likelihood , and @xmath149 , which is also fully efficient . for a multivariate normal model with continuous responses ,",
    "zhao & joe ( 2005 ) proved that the maximum pairwise likelihood estimator of the regression coefficient has a smaller asymptotic variance than the maximum independence likelihood estimator .",
    "in fact , within the family of information - unbiased composite likelihood functions pairwise likelihood is at least as efficient as independence likelihood : each bivariate density @xmath150 has a larger information matrix than @xmath151 and the total number of the bivariate densities in @xmath79 is @xmath152 when @xmath153 .",
    "however , the two composite likelihoods are information - biased in general especially for complex dependent data and we may observe the reverse relationship .",
    "a bivariate binary model was used in arnold & strauss ( 1991 ) to show that the pairwise conditional likelihood could be less efficient than the independence likelihood . for a bivariate model",
    "the pairwise likelihood is the full likelihood and hence fully efficient .",
    "here we consider a four dimensional binary model which has a complex dependence structure but also allows us to compare the ( asymptotic ) variances of different composite likelihood estimators analytically .",
    "* example 3 .",
    "* suppose @xmath154 follows a multinomial@xmath155 , where @xmath156 is a positive constant and @xmath157 .",
    "the parameter @xmath9 controls both the mean and covariance structures , and we can change the value of @xmath156 to adjust the strength of dependence .",
    "the value of @xmath158 is completely determined by @xmath159 . given a random sample of size @xmath81 from this model , we estimate @xmath9 based on the independent triplets @xmath160 , @xmath161 .    the full likelihood for the model of @xmath123 is @xmath162 solving the score equation we get the maximum likelihood estimator of @xmath9 , @xmath163 .",
    "the exact variance of @xmath164 is @xmath165 the independence likelihood function for the model of @xmath123 is @xmath166 and we can calculate its sensitivity matrix and variability matrix as @xmath167 the pairwise likelihood function is @xmath168 and we can calculate its sensitivity matrix and variability matrix as @xmath169 where @xmath170 , @xmath171 .    for @xmath172 , the asymptotic variances of the maximum composite likelihood estimators for ( [ eq : eg2full ] ) , ( [ eq : eg2ind ] ) and ( [ eq : eg2pair ] ) multiplied by @xmath81 are plotted as a function of @xmath9 in figure  [ fig3.1 ] .",
    "we can see that when @xmath173 , the three estimators perform almost equally well ; when @xmath174 , the full likelihood becomes more efficient than the independence likelihood , and the independence likelihood estimator is more efficient than the pairwise likelihood estimator .",
    "we also carried out the comparisons for different values of @xmath156 and found that at @xmath175 , both the independence likelihood and the pairwise likelihood are fully efficient , but when @xmath176 , the independence likelihood is more efficient than the pairwise likelihood and the ratio of asymptotic variances approaches @xmath49 when @xmath177 .",
    "when @xmath178 , the pairwise likelihood estimator is more efficient than the independence likelihood estimator and the ratio of asymptotic variances approaches @xmath49 when @xmath179 .",
    "this example suggests that in practical applications of composite likelihood inference , where the models will usually have more complex dependence structure and incomplete data ( e.g. , yi , zeng , & cook , 2011 ) , some care is required for the use of higher dimensional composite likelihood to obtain more efficient estimators .",
    "a hybrid composite likelihood combining lower dimensional marginal and conditional likelihoods with different weights is suggested to guarantee the improvement of efficiency ( cox & reid , 2004 ; kenne pagui , salvan , & sartori , 2014a ) .",
    "as a complement to the discussion on composite likelihood inference in reid ( 2012 ) , in this paper we explored the impact of information bias on the composite likelihood based inference in different scenarios .",
    "an information - unbiased composite likelihood behaves somewhat like the ordinary likelihood but it can be very inefficient . on the other hand an information - biased likelihood brings extra difficulty to the computation and is more likely to exhibit undesirable inferential properties , although the information loss can be minimized with a set of carefully selected weights .",
    "one way to avoid the paradoxical phenomenon in section 2 is to convert the composite score function @xmath180 to an unbiased estimating function by projecting ( henmi & eguchi , 2004 ; lindsay , yi , & sun , 2011 ) : @xmath181 where @xmath182 is the score function of full likelihood , @xmath183 ranges over all @xmath184 matrices , @xmath44 and @xmath45 are the sensitivity matrix and variability matrix .",
    "it is easy to check that @xmath185 is information - unbiased .",
    "since @xmath44 and @xmath45 are constant matrices , this projection does not change the point estimator of @xmath9 , and @xmath185 has the same godambe information as @xmath180 . in the equicorrelated multivariate normal model with @xmath186 ,",
    "the score function of the pairwise likelihood is @xmath187 ( pace , salvan & sartori , 2011 ) . from equation ( [ eq : project ] ) , the projected estimating funtion of @xmath188 is equal to the score function of full likelihood , @xmath182 . in complex models ,",
    "the required computation for the projected estimating function @xmath185 can be intractable and it may be a better idea to design a nuisance - parameter - free composite likelihood function carefully for practical use . as an example , a pairwise difference likelihood that eliminates nuisance parameters in a neyman ",
    "scott problem is described in hjort & varin ( 2008 ) ."
  ],
  "abstract_text": [
    "<S> does the asymptotic variance of the maximum composite likelihood estimator of a parameter of interest always decrease when the nuisance parameters are known ? will a composite likelihood necessarily become more efficient by incorporating additional independent component likelihoods , or by using component likelihoods with higher dimension ? in this note </S>",
    "<S> we show through illustrative examples that the answer to both questions is no , and indeed the opposite direction might be observed . </S>",
    "<S> the role of information bias is highlighted to understand the occurrence of these paradoxical phenomenon . </S>",
    "<S> + * key words : * pairwise likelihood ; estimating function ; bartlett s second identity ; godambe information matrix ; nuisance parameter .    2.0 </S>"
  ]
}