{
  "article_text": [
    "bayesian signal processing , which has become very popular over the last years in statistical signal processing , requires computing distributions of unknowns conditioned on observations ( and moments of them ) .",
    "unfortunately , these distributions are often impossible to obtain analytically in many real - world challenging problems .",
    "an alternative is then to resort to monte carlo ( mc ) methods , which approximate the target distributions with random measures composed of samples and associated weights @xcite .",
    "a well - known class of mc methods are those based on the adaptive importance sampling ( ais ) mechanism , such as population monte carlo ( pmc ) algorithms @xcite , which have been used in missing data , tracking , and biological applications , among others @xcite . in these methods ,",
    "a population of probability density functions ( pdfs ) is adapted for approximating a target distribution through an iterative importance sampling procedure .",
    "ais is often preferred to other mc schemes , such as markov chain monte carlo ( mcmc ) , since they present several advantages .",
    "on the one hand , all the generated samples are employed in the estimation ( e.g. , there is no `` burn - in '' period ) . on the other hand ,",
    "the corresponding adaptive schemes are more flexible , since they present fewer theoretical issues than adaptive mcmc algorithms .",
    "namely , the convergence of ais methods can usually be guaranteed under mild assumptions regarding the tails of the distributions and the stability of the adaptive process , whereas adaptive mcmc schemes must be designed very carefully , since the adaptation procedure can easily jeopardize the ergodicity of the chain ( e.g. , see @xcite or ( * ? ? ?",
    "* section 7.6.3 ) ) .",
    "the most characteristic feature in pmc @xcite is arguably the use of resampling procedures for adapting the proposal pdfs ( see for instance @xcite for a review of resampling methods in particle filtering ) . the resampling step is a fast , often dimensionality - free , and easy way of adapting the proposal pdfs by using information about the target .",
    "however , resampling schemes present some important drawbacks , such as the sample impoverishment . at the resampling step , the proposal pdfs with poor performance ( i.e. , with low associated weights ) are likely to be removed , thus yielding a reduction of diversity . since the publication of the standard pmc @xcite , several variants have been considered , partly in an attempt to mitigate this issue . in the d - kernel algorithm @xcite ,",
    "the pmc kernel is a mixture of different kernels and the weights of the mixture are iteratively adapted in an implicit expectation - maximization ( em ) algorithm .",
    "this procedure is refined through a double rao - blackwelization in @xcite .",
    "the mixture population monte carlo algorithm ( m - pmc ) proposed in @xcite also adapts a mixture of proposal pdfs ( weights and parameters of the kernels ) .",
    "the m - pmc belongs to the family of ais methods , since it iteratively draws the samples from the mixture that is updated at every iteration without an explicit resampling step .",
    "since drawing from the mixture can be interpreted as an implicit multinomial resampling , this method retains some similarities with the standard pmc scheme .",
    "a nonlinear transformation of the importance weights in the pmc framework has also been proposed in @xcite .",
    "other sophisticated ais schemes , such as the amis @xcite and the apis @xcite algorithms , have been recently proposed in the literature .    in this paper , we study three novel pmc schemes that improve the performance of standard pmc approach by allowing a better exploration of the space of unknowns and by reducing the variance of the estimators .",
    "these alternatives can be applied within some other sophisticated ais approaches as well , such as the smc samplers @xcite . for this reason",
    ", we mainly compare them with the standard pmc @xcite , since the novel schemes could be automatically combined with the more sophisticated ais techniques .",
    "first of all , we introduce an alternative form of the importance weights , using a mixture of the proposal pdfs in the denominator of the weight ratio .",
    "we provide an exhaustive theoretical analysis , proving the unbiasedness and consistency of the resulting estimator , and showing the reduction in the variance of the estimator w.r.t .",
    "the estimator obtained using the standard weights .",
    "we also prove that the use of this mixture decreases the averaged mismatch between the numerator ( target ) and the function in the denominator of the is weight in terms of @xmath0 distance .",
    "moreover , we test this alternative scheme in different numerical simulations , including an illustrative toy example in section [ sec_ex1 ] , showing its practical benefit .    in the second proposed scheme ,",
    "we generate several samples from every proposal pdf ( not only one , as in pmc ) and then we resample them jointly ( all the samples at once , keeping fixed the total number of proposal pdfs ) . in the third proposed scheme ,",
    "we consider again the generation of several samples from every proposal pdf , but the resampling is performed separately on the set of samples coming from each proposal , therefore guaranteeing that there will be exactly one representative from each of the individual mixture components in the random measure .",
    "we show , through extensive computer simulations in several different scenarios , that the three newly proposed variants provide a substantial improvement compared to the standard pmc .",
    "in addition , we test the proposed variants on a standard implementation of the smc samplers @xcite , showing also an improvement of the performance . on the one hand",
    ", they yield unbiased estimators with a reduced variance , as also proved theoretically . on the other hand",
    ", they outperform the standard pmc in terms of preservation of sample diversity and robustness w.r.t initialization and parameter choice .",
    "let us consider the variable of interest , @xmath1 , and let @xmath2 be the observed data . in a bayesian framework ,",
    "the posterior probability density function ( pdf ) , here referred as _ target _ , contains all the information about the parameters of interest and is defined as @xmath3 where @xmath4 is the likelihood function , @xmath5 is the prior pdf , and @xmath6 is the model evidence or partition function ( useful in model selection ) . in order to simplify the notation . ]",
    "the goal is to compute some moment of @xmath7 , i.e. , an integral measure w.r.t .",
    "the target pdf , @xmath8 where @xmath9 can be any square integrable function of @xmath7 w.r.t .",
    "@xmath10 , and @xmath11 .",
    "is square integrable w.r.t . @xmath10",
    "if @xmath12 , i.e. , if @xmath13 . ]    in many practical applications , both the integral and @xmath14 can not be obtained in closed form and must be approximated .",
    "importance sampling methods allow for the approximation of both quantities by a set of properly weighted samples .",
    "the pmc method @xcite is a well - known iterative adaptive importance sampling technique . at each iteration",
    "it generates a set of @xmath15 samples @xmath16 , where @xmath17 denotes the iteration number and @xmath18 denotes the sample index . in order to obtain the samples",
    ", the original pmc algorithm makes use of a collection of proposal densities @xmath19 , with each sample being drawn from a different proposal , @xmath20 for @xmath21 .",
    "then , they are assigned an importance weight , computed as @xmath22 , i.e. , the weight of a particular sample represents the ratio between the evaluation , at the sample value , of the target distribution and the evaluation at the sample value of the proposal used to generate it .",
    "the method proceeds iteratively ( up to the maximum iteration step considered , @xmath23 ) , building a global importance sampling estimator using different proposals at every iteration .",
    "the new proposals are obtained by updating the set of proposals in the previous iteration .",
    "there are two key issues in the application of pmc methods : the adaptation of the proposals from iteration to iteration and the way resampling is applied .",
    "the latter is critical to avoid the degeneracy of the random measure , i.e. , to avoid a few particles having extremely large weights and the rest negligible ones @xcite . through the resampling procedure one selects the most promising streams of samples from the first iteration up to the current one .",
    "several resampling procedures have been proposed in the literature @xcite . in the standard pmc @xcite ,",
    "multinomial resampling is the method of choice , and consists of sampling @xmath15 times from the discrete probability mass defined by the normalized weights . as a result of this procedure , the new set of parameters used to adapt the proposals for the generation of samples in the next iteration",
    "is selected . in summary ,",
    "the standard pmc technique consists of the steps shown in table [ pmcalg ] .        1 .   *",
    "[ initialization ] * : select the parameters defining the @xmath15 proposals : * the adaptive parameters @xmath24 .",
    "* the set of static parameters , @xmath25 .",
    "+ e.g. , if the proposals were gaussian distributions one could select the adapting parameters in @xmath26 as the means of the proposals ( that would be updated through the iterations ) and the static parameters @xmath25 as their covariances @xcite .",
    "[ for @xmath27 to @xmath28 * : 1 .",
    "draw one sample from each proposal pdf , @xmath29 2 .",
    "compute the importance weights , @xmath30 and normalize them , @xmath31 3 .",
    "perform multinomial resampling by drawing @xmath15 independent parameters @xmath32 from the discrete probability random measure , @xmath33 the new set of adaptive parameters defining the next population of proposals becomes @xmath34 3 .   *",
    "[ output , @xmath35 * : return the pairs @xmath36 , with @xmath37 given by eq .",
    ", for @xmath38 and @xmath39 .",
    "+    [ pmcalg ]       all the generated samples can be used to build a global approximation of the target .",
    "this can be done by first normalizing all the weights from all the iterations , @xmath40 and then providing the pairs @xmath36 for @xmath38 and @xmath39 .",
    "this procedure to compute the weights is equivalent to applying a static importance sampling technique that considers @xmath41 different proposal pdfs and all the corresponding samples .",
    "if the normalizing constant @xmath14 is known , the integral in eq .",
    "is approximated by the unbiased estimator @xmath42 when the normalizing constant is unknown , the unbiased estimate of @xmath14 is substituted in eq . , yielding the self - normalized estimator @xmath43 where @xmath44 is the unbiased estimate of the normalizing constant .",
    "in the following , we introduce several alternative strategies that decrease the variance of the estimators by exploiting the mixture perspective , and improve the diversity of the population w.r.t . to the standard pmc .",
    "more specifically , we study three different pmc schemes : one related to the strategy for calculating the weights and the other two based on modifying the way in which the resampling step is performed .",
    "although we concentrate on the standard pmc , we remark that these alternative schemes can be directly applied or combined in other more sophisticated pmc algorithms .",
    "moreover , the alternative schemes can be easily implemented in other monte carlo methods with resampling steps , such as the sequential monte carlo ( smc ) samplers @xcite , as we show in .",
    "the underlying idea of pmc is to perform a good adaptation of the location parameters @xmath45 , i.e. , where the proposals of the next iteration will be centered ( e.g. , if @xmath46 is a gaussian pdf , then @xmath45 is its mean ) .",
    "these parameters are obtained at each iteration by sampling from @xmath47 in eq .",
    "( i.e. , via resampling ) , which is a random measure that approximates the target distribution , i.e. , @xmath48 . as a direct consequence of the strong law of large numbers , @xmath49 almost surely ( a.s . ) as @xmath50 under very weak assumptions @xcite ( the support of the proposal includes the support of the target and @xmath51 ) . furthermore , by setting @xmath52 , where @xmath53 $ ] , @xmath54 $ ] , and @xmath55 is defined as @xmath56 where @xmath57 denotes the indicator function for the @xmath58-th component ( @xmath59 ) of the variable of interest",
    ", @xmath60 then @xmath61 becomes the multi - variate cumulative distribution function ( cdf ) of @xmath62 .",
    "consequently , since @xmath63 a.s . for any value of @xmath64 as @xmath50 [ geweke,1989 ] , @xmath65 a.s . as @xmath50 . in short",
    ", since the cdf associated to @xmath66 ( which is the pdf used for resampling ) converges to the target cdf ( i.e. , the cdf associated to @xmath10 ) as @xmath50 , then the outputs of the resampling stage ( i.e. , the means @xmath45 ) are asymptotically distributed as the target .    therefore , the equally - weighted mixture of the set of proposals at the @xmath17-th iteration , given by @xmath67 can be seen as a kernel density approximation of the target pdf , where the proposals , @xmath68 , play the role of the kernels ( * ? ? ?",
    "* chapter 6 ) . in general , this estimator has non - zero bias and variance , depending on the choice of @xmath69 , @xmath70 , and the number of samples , @xmath15 .",
    "however , for a given value of @xmath15 , there exists an optimal choice of @xmath71 which provides the minimum mean integrated square error ( mise ) estimator @xcite . using this optimal covariance matrix @xmath71",
    ", it can be proved that @xmath72 pointwise as @xmath73 @xcite .",
    "hence , resampling naturally leads to a concentration of the proposals around the modes of the target for large values of @xmath15 .",
    "therefore , since the performance of an importance sampling method relies on the discrepancy between the numerator ( the target ) and the denominator ( usually , the proposal pdf ) , a reasonable choice for calculating the importance weights is @xmath74 where , as opposed to eq .",
    ", the complete mixture of proposals @xmath75 is accounted for in the denominator .",
    "the first justification for using these _ deterministic mixture _ ( dm ) weights is merely mathematical , since the estimator @xmath76 of eq . with these weights",
    "is also unbiased ( see the proof in [ dm_unbiased_appendix ] ) .",
    "the main advantage of this new scheme is that it yields more efficient estimators , i.e. with less variance , combining the deterministic mixture sampling ( as in standard pmc ) with the weight calculation that accounts for the whole mixture .",
    "namely , the estimator @xmath77 in eq . , computed using the dm approach , has less variance than the estimator obtained by the standard pmc , as proved in [ dm_variance_appendix ] for any target and set of proposal pdfs .",
    "these dm weights have been explored in the literature of multiple importance sampling ( see for instance the balance heuristic strategy of ( * ? ? ?",
    "* section 3.3 . ) or the deterministic mixture approach of ( * ? ? ?",
    "* section 4.3 . ) ) .",
    "the intuition behind the variance reduction is clear in a multi - modal scenario , where different proposals have been successfully adapted covering the different modes , and therefore , the whole mixture of proposals has less mismatch w.r.t .",
    "the target than each proposal separately .",
    "indeed , it can be easily proved that the mismatch of the whole mixture w.r.t to the target is always less than the average mismatch of each proposal .",
    "more precisely , let us consider the @xmath78 functional distance ( with @xmath79 ) among the target and an arbitrary function @xmath80 , @xmath81^{1/p } , \\label{eq : lp_dist}\\ ] ] and let us recall jensen s inequality @xcite , @xmath82 which is valid for any convex function @xmath83 , any set of non - negative weights @xmath84 such that @xmath85 , and any collection of points @xmath86 in the support of @xmath87 .",
    "then , by using jensen s inequality in eq . with @xmath88^{1/p}$ ] , @xmath89 and @xmath90 , it is straightforward to show that @xmath91 indeed , although we have focused on the @xmath78 distance , the proof is valid for any distance function which is based on a norm ( i.e. , any distance s.t .",
    "@xmath92 for some norm @xmath93 ) , since every norm is a convex function .",
    "another benefit of the dm - pmc scheme is the improvement in the exploratory behavior of the algorithm .",
    "namely , since the weights in dm - pmc take into account all the proposals ( i.e. , the complete mixture ) for their calculation , they temper the overrepresentation of high probability areas of the target .",
    "note that , as a consequence of the variance reduction of the dm weights , the effective sample size in dm - pmc in a specific iteration is larger than with the standard is weights .",
    "samples used in the importance sampling estimator . ]",
    "the expression @xmath94 is widely used as a sample approximation of the effective sample size ( see its derivation in @xcite ) . therefore , if the true underlying effective sample size ( the ratio of variances ) is larger with the dm weights ( than with the standard is weights ) , a similar behavior can be considered for @xmath95 . as a consequence ,",
    "the diversity loss associated to the resampling step is reduced by using with the dm weights .",
    "see @xcite for a more detailed discussion about effective sample size in static multiple importance sampling schemes .      in this dm - pmc scheme , the performance is improved at the expense of an increase in the computational cost ( in terms of proposal evaluations ) in the calculation of the weights .",
    "however , it is crucial to note that all the proposed schemes keep the same number of evaluations of the target as in the standard pmc .",
    "hence , if the target evaluation is much more costly than the evaluation of the proposal pdfs ( as it often happens in practical applications ) , the increase in computational cost can be negligible in many scenarios of interest .",
    "note that other adaptive multiple is algorithms , e.g. @xcite , also increase the number of proposal evaluations , and they state that the most significant computational cost is associated to the evaluation of the target ( see this argument in ( * ? ? ?",
    "* section 2.2 . ) ) .",
    "finally , note that the variant _ partial_-dm proposed in @xcite within the static multiple",
    "is framework , could be easily adapted to the dm - pmc . in this weighting scheme ,",
    "a partition ( forming subsets ) of the set of proposals is a priori performed .",
    "the weight of each sample only accounts at the denominator for a subset of proposals , i.e. , reducing the number of proposal evaluations .",
    "this variant achieves an intermediate point in the complexity - performance tradeoff , between the standard weights and the dm weights .",
    "note that other methods also use a mixture of proposals at the denominator of the weights .",
    "for instance , in the d - kernel of @xcite , each sample is drawn from a mixture of @xmath96 kernels ( proposals ) , and this same mixture is evaluated at the denominator of each weight .",
    "nevertheless , note that these @xmath96 kernels are centered at the same position , and the weight of each sample ignores the locations of the @xmath97 proposals . in the m - pmc of @xcite ,",
    "a single mixture is used for sampling and weighting the @xmath15 samples at each iteration . note that this method does not use an explicit resampling step , and the mixture is completely adapted ( weights , means , and covariances ) .    in the sequel",
    ", we adopt the weights of eq . for the other two proposed pmc schemes due to their theoretical and practical advantages discussed above .",
    "we propose to draw @xmath98 samples per individual proposal or mixand , instead of only one as done in the standard pmc algorithm .",
    "namely , @xmath99 for @xmath38 and @xmath100 .",
    "then , we compute the corresponding dm weights as in , @xmath101 therefore , at each iteration we have a set of @xmath102 generated samples , i.e. , @xmath103 resampling is performed in the same way as in standard pmc , although now the objective is to downsample , from @xmath102 samples to @xmath15 samples , according to the normalized weights , @xmath104 we refer to this type of resampling as _",
    "global _ resampling , since all the samples , regardless of the proposal used to generate them , are resampled together .",
    "after resampling , a new set of adapted parameters for the next iteration , @xmath105 , is obtained .",
    "note that , through this paper , for sake of simplicity in the explanation of the proposed improvements , we use the standard multinomial resampling , but other resampling schemes that reduce the path - degeneracy problem can be considered instead , e.g. the residual or stratified resampling , ( see @xcite ) .",
    "the pmc algorithms suffer from sample impoverishment , which is a side effect inherent to adaptive algorithms with resampling steps such as smc samplers or particle filters ( see for instance ( * ? ? ?",
    "* section v - c ) or ( * ? ? ?",
    "* section 2 ) ) .",
    "in other words , there is a diversity reduction of the samples after the resampling step ( in a very adverse scenario , the @xmath15 resampled samples can be @xmath15 copies of the sample ) .",
    "the sample impoverishment of the standard pmc is illustrated in fig .",
    "[ figtrellis ] , fig .",
    "[ fig_survival ] , and fig .",
    "[ fig_positions_evolutions ] , where the increase of diversity of the algorithms proposed in this paper is shown by numerical simulations .",
    "these figures correspond to the example of section [ sec_toy_example ] and will be properly introduced below . in multimodal scenarios , proposals of the standard pmc that are exploring areas with negligible probability masses",
    "are very likely to be removed before they find unexplored relevant areas .",
    "if we draw @xmath98 samples per proposal , the samples of a well - placed proposal will have similarly high weights , but as for the explorative proposals , increasing @xmath98 also increases their chances of discovering local relevant features of the target @xmath106 .",
    "then , the gr - pmc promotes the local exploration of the explorative proposals , increasing the chances of not being removed in the resampling step .",
    "figures [ figtrellis ] and [ fig_survival ] show the reduction of path - degeneracy of gr - pmc in a multimodal scenario , and they will be properly explained in the example of section [ sec_toy_example ] .",
    "note that using @xmath107 does not entail an increase in the computational cost w.r.t .",
    "the standard pmc or dm - pmc ( where @xmath108 ) if the number of evaluations of the target is fixed to @xmath109 . indeed , since the number of resampling stages is reduced to @xmath110 , the computational cost decreases , although at the expense of performing less adaptation steps than for @xmath108 .",
    "therefore , for a fixed budget of target evaluations @xmath111 and a fixed number of proposals @xmath15 , one must decide whether to promote the local exploration ( possibly reducing the path degeneracy ) by increasing @xmath98 , or performing more adaptation steps @xmath23 .",
    "thus , there is a trade - off between local and global exploration as the numerical experiments will also show in section [ sec_results ] .",
    "this suggests that , for a fixed computational budget @xmath111 , there exists an optimal value of samples per proposal and iteration , @xmath112 , which will also depend on the target and can not be found analytically .",
    "this issue can be partially addressed through the use of local resampling , as shown in the following section .",
    "consider again @xmath98 samples generated from each proposal pdf . in this alternative scheme ,",
    "the estimators are built as in gr - pmc , i.e. , with the weights of eq . .",
    "nevertheless , unlike the previous method , here the resampling step is performed independently for each proposal .",
    "namely , at the @xmath17-th iteration , @xmath98 samples are drawn from each of the @xmath15 proposal pdfs , and @xmath15 _ parallel _ resampling procedures are independently performed within each subset of @xmath98 samples ( see fig .",
    "[ fig_globloc ] for a visual comparison of both resampling schemes ) .",
    "more precisely , the adaptive parameter for the next iteration of the @xmath18-th proposal , @xmath32 for @xmath38 , is resampled from the set @xmath113 using the multinomial probability mass function with probabilities @xmath114 where the unnormalized weights @xmath115 are given by eq . .",
    "note that again we can use any resampling technique , including the standard multinomial or other advanced schemes @xcite . in lr - pmc ,",
    "there is no loss of diversity in the population of proposals , since each proposal at the current iteration yields another proposal in the next iteration . in other words , exactly one particle per proposal survives after the resampling step .",
    "the adaptation scheme of lr - pmc can be intuitively understood as follows .",
    "let us consider for a moment a modified version of lr - pmc where the weights used in the resampling are those of standard pmc of eq . instead of the dm weights of eq .",
    "this modified scheme is equivalent to @xmath15 parallel pmc samplers , where the @xmath18-th pmc draws @xmath98 samples from the @xmath18-th proposal , applying a resampling step independently from the other @xmath97 pmc samplers . by using the dm weights in lr - pmc",
    ", we incorporate cooperation among the @xmath15 proposals .",
    "when the proposal pdfs are close to each others , the local resampling scheme ( with dm weights ) adds a `` repulsive '' interaction : among the @xmath98 samples of a specific proposal , the resampling promotes the samples in areas that are less covered by the other @xmath97 proposals ( and where , at the same time , the target evaluation is high ) .",
    "therefore , this scheme performs a cooperative exploration of the state space by the @xmath15 proposals .",
    "note that , when the proposal pdfs are located far away from each other , the weights of the @xmath98 samples of a specific proposal are in practice not affected by other @xmath97 proposals . in this case , the lr - pmc works as the @xmath15 parallel pmc samplers described above .    finally , let us remark that a mixed global - local resampling strategy ( e.g. , performing local resampling on clusters of proposals ) could also be devised in order to obtain the advantages of both global and local resampling .",
    "[ box/.style = rectangle , draw = black , fill = gray!30,scale=0.75 , transform shape , label/.style = rectangle ] ( inf ) at ( 0,0 ) [ box ] @xmath116 ; ( med ) at ( 0,2 ) [ box ] @xmath117 ; ( sup ) at ( 0,4 ) [ box ] @xmath118 ; ( local ) at ( 5,2 ) [ label ] local resampling ; ( qn ) at ( -4,0 ) [ label ] @xmath119 ; ( qi ) at ( -4,2 ) [ label ] @xmath120 ; ( q1 ) at ( -4,4 ) [ label ] @xmath121 ; ( global ) at ( 0,5 ) [ label ] global resampling ; ( local.north west )  ( sup.east ) ; ( local.west ) ",
    "( med.east ) ; ( local.south west )  ( inf.east ) ; ( q1.east )  ( sup.west ) ; ( qi.east )  ( med.west ) ; ( qn.east )  ( inf.west ) ; ( -2.7,-0.7 ) rectangle ( 2.7,4.7 ) ;",
    "let us consider , as a target pdf , a bimodal mixture of gaussians @xmath122 with @xmath123 and @xmath124 , and @xmath125 and @xmath126 .",
    "the proposal pdfs are also gaussians : @xmath127 and @xmath128 . at this point",
    ", we consider two scenarios :    * scenario 1 : in this case , @xmath129 , @xmath130 , @xmath131 , and @xmath132 .",
    "then , both proposal pdfs can be seen as a whole mixture that exactly replicates the target , i.e. , @xmath133 .",
    "this is the desired situation pursued by an adaptive importance sampling algorithm : each proposal is centered at a different mode of the target , and their scale parameters perfectly match the scales of the modes .",
    "[ fig_ex_z_sc1](a ) shows the target pdf in solid black line , and both proposal pdfs in blue and red dashed lines , respectively .",
    "note that the proposals are scaled ( each one integrates up to @xmath134 so we can see the perfect matching between the target and the mixture of proposal densities ) . * scenario 2 : in this case , @xmath135 , @xmath136 , @xmath137 , and @xmath138 .",
    "therefore , there is a mismatch between the target and the two proposals .",
    "[ fig_ex_z_sc2](a ) shows the target pdf in solid black line , and both proposal pdfs in blue and red dashed lines , respectively .",
    "the goal is estimating the normalizing constant using the estimator @xmath139 of eq . with @xmath140 samples , one from each proposal , and @xmath141 .",
    "we use the standard pmc weights of eq .",
    "( estimator @xmath142 ) and the dm - pmc weights of eq .",
    "( estimator @xmath143 ) .",
    "in order to characterize the two estimators , we run @xmath144 simulations for each method . note that the true value is @xmath145 .",
    "figure [ fig_ex_z_sc1](b ) shows a boxplot of the distribution of the estimator @xmath146 , obtained with both methods for scenario 1 .",
    "the blue lower and upper edges of the box correspond to the 25th and 75th percentiles , respectively , while the red line represents the median .",
    "the vertical black dashed whiskers extend to the minimum and maximum obtained values .",
    "since the maxima can not be appreciated in the figure , they are displayed in table [ tabla_maxima ] , altogether with the variance of the estimators .",
    "note that even in this extremely simple and idealized scenario ( perfect adaptation ) , the estimator obtained using the standard is weights ( i.e. , the estimator used in standard pmc ) has a poor performance . in most of the realizations ,",
    "@xmath147 because each proposal ( which integrates up to one ) is adapted to one of the two modes ( which contain roughly half of the probability mass ) . in most of the runs ; the lack of information exchange between the two samples , makes it impossible to know whether the target mass reported by the weight of each sample is the same and should be accounted `` once '' , or whether it is from another area and it should be accounted `` twice '' . ] since @xmath148=z=1 $ ] , in a few runs the value the @xmath142 is extremely high as shown in table [ tabla_maxima ] .",
    "these huge values occur when a sample drawn from the tail of the proposal falls close to the other mode of the target ( where actually the other proposal is placed ) .",
    "on the other hand , note that the dm estimator has a perfect performance ( i.e. , @xmath149 always , thus implying zero variance ) .",
    "hence , this simple example shows that a substantial variance reduction can be attained by using the mixture at the denominator .",
    "figure [ fig_ex_z_sc2](b ) shows an equivalent boxplot for scenario 2 . in this case",
    ", the mismatch between proposals and target pdfs worsens both schemes .",
    "note that the estimator @xmath143 now does not perfectly approximates @xmath14 , but still largely outperform the estimator @xmath142 . in particular ,",
    "the median is still around the true value , and its variance is smaller .    [ cols=\"^,^,^,^,^,^,^,^\",options=\"header \" , ]     ) * mse of the normalizing constant @xmath14 , using @xmath150 proposals and a scale parameter @xmath151 , as the dimension of the state space @xmath152 increases.,scaledwidth=60.0% ]      we consider the use of an autoregressive ( ar ) model contaminated by a non - gaussian noise .",
    "this kind of filters is often used for modeling some financial time series ( see for instance ( * ? ? ? * section 5 ) and @xcite ) , where the noise is assumed to follow the so - called _ generalized hyperbolic distribution _",
    "namely , we consider the following observation model , @xmath153 where @xmath154 is a time index , and @xmath155 is a heavy - tailed driving noise : @xmath156 where @xmath157 denotes the _ modified bessel function _ @xcite .",
    "the vector of unknowns , @xmath158^{\\top}$ ] , contains the coefficients of the ar model .",
    "given a set of observations @xmath159^{\\top}$ ] , the inference problem consists of obtaining statistical information about @xmath160 , by studying the corresponding posterior distribution @xmath161 .",
    "more specifically , we have synthetically generated @xmath162 observations , @xmath159^{\\top}$ ] , setting @xmath163^\\top$ ] , @xmath164 , @xmath165 , @xmath166 , @xmath167 , and @xmath168 .",
    "assuming improper uniform priors over the unknown coefficients , the objective is computing the expected value @xmath169 .",
    "since we are using @xmath162 observations ( a large number for this example ) , we assume that the posterior pdf is quite sharp and concentrated around the true value , @xmath163^\\top$ ] .",
    "nevertheless , in practice we assume that the inference algorithms have no clue of which is that true value ( i.e. , we assume no a priori information ) . therefore , @xmath160 is only used for evaluating the performance of the different methods in terms of mse .",
    "all the methods use gaussian proposals , with the initial adaptive parameters of the individual proposals selected uniformly within the @xmath170 ^ 4 $ ] square , i.e. , @xmath171\\times[-6,6]\\times[-6,6]\\times[-6,6])$ ] , and the covariance matrices for all the gaussians selected as @xmath172 , with @xmath173 for @xmath38 . as in the previous examples",
    ", we have tested different combinations of parameters , keeping the total number of evaluations of the target fixed to @xmath174 .",
    "we have evaluated different values of @xmath175 and @xmath176 .",
    "we ran 500 independent simulations and computed the mse in the estimation of @xmath177 w.r.t .",
    "the true value @xmath160 .",
    "the results obtained by the different methods , in terms of mse averaged over all the components of @xmath7 , are shown in table [ tableexar ] .",
    "note that some combinations of @xmath98 and @xmath15 would yield a number of iterations @xmath178 , since we set @xmath179 .",
    "therefore , those simulations can not be performed and are indicated in the table with the symbol @xmath180 .",
    "note that , for any choice of @xmath15 , the alternative schemes proposed in the paper largely outperform the standard pmc .",
    "furthermore , the advantage of using @xmath107 can again be clearly seen for the three values of @xmath15 tested .",
    "more specifically , the smallest the value of @xmath15 the largest the value of @xmath98 that should be used to attain the best results . note also that m - pmc behaves particularly well in this scenario for high values of @xmath15 , but its performance is very poor for @xmath150 ( unlike gr - pmc and lr - pmc , which can still provide a good performance for the right value of @xmath98 ) .      let us consider a static target in a two - dimensional space .",
    "the goal consists on positioning the target within a wireless sensor network using only range measurements acquired by some sensors .",
    "this example appears in the signal processing literature for localization applications , e.g. in @xcite in particular , let @xmath181^{\\top}$ ] denote the random vector representing the position of the target in @xmath182 plane .",
    "the measurements are obtained from @xmath183 range sensors located at @xmath184^{\\top}$ ] , @xmath185^{\\top}$ ] , @xmath186^{\\top}$ ] , @xmath187^{\\top}$ ] , @xmath188^{\\top}$ ] and @xmath189^{\\top}$ ] .",
    "the measurements are related to the target position through the following expression : @xmath190 where @xmath191 , with @xmath192 for all @xmath193 .",
    "note that the total number of data is @xmath194 .",
    "we consider a wide gaussian prior pdf with mean @xmath195^{\\top}$ ] and covariance matrix @xmath196^{\\top}$ ] with @xmath197 ,    we simulate @xmath198 measurements from the model ( @xmath199 observations from each sensor ) , fixing @xmath200 and @xmath201 .",
    "the goal consists in approximating the mean of the posterior distribution @xmath202 , through the improved pmc techniques proposed in this paper . in order to compare the different techniques , we computed the value of interest by using an extremely thin grid , yielding @xmath203\\approx [ 3.415 , 3.539]^{\\top}$ ] .",
    "we test the proposed methods and we compare them with the standard pmc @xcite and the m - pmc @xcite . in all cases ,",
    "gaussian proposals are used , with initial mean parameters selected uniformly within the @xmath204\\times[1,5]$ ] square , i.e. , @xmath205\\times[-4,4])$ ] for @xmath38 .",
    "all the methods use the same isotropic covariance matrices for all the gaussian proposals , @xmath206 with @xmath207 .",
    "we have tried @xmath208 proposals . in the proposed methods , we test the values @xmath209 .",
    "note that again , we keep fixed the total number of evaluations to @xmath210",
    ".    table [ table_sensors ] shows the mse in estimation of the expected value of the posterior , with the different pmc methods .",
    "again , the proposed methods largely beat the standard pmc for all the sets of parameters .",
    "the m - pmc algorithm is again competitive ( especially with @xmath211 ) , but the proposed algorithms obtain better performance ( in particular , the lr - pmc with a high @xmath98 ) .",
    "the population monte carlo ( pmc ) method is a well - known and widely used scheme for performing statistical inference in many signal processing problems .",
    "three improved pmc algorithms are proposed in this paper .",
    "all of them are based on the deterministic mixture ( dm ) approach , which provides estimators with a reduced variance ( as proved in this paper ) and increases the exploratory behavior of the resulting algorithms .",
    "additionally , two of the methods draw multiple samples per mixand ( both with local and global resampling strategies ) to prevent the loss of diversity in the population of proposals .",
    "the proposed approaches are shown to substantially outperform the standard pmc on three numerical examples .",
    "the proposed improvements can be applied to other existing pmc implementations and other importance sampling techniques , to achieve similar benefits .",
    "this work has been supported by the spanish government s projects ages ( s2010/bmd-2422 ) , alcit ( tec2012 - 38800-c03 - 01 ) , comprehension ( tec2012 - 38883-c02 - 01 ) , dissect ( tec2012 - 38058-c03 - 01 ) , and otosis ( tec2013 - 41718-r ) ; by the bbva foundation through project mg - fiar ( `` i convocatoria de ayudas fundacin bbva a investigadores , innovadores y creadores culturales '' ) ; by erc grant 239784 and aof grant 251170 ; by the national science foundation under award ccf-0953316 ; and by the european union s 7th fp through the marie curie itn mlpm2012 ( grant no . 316861 ) .",
    "10 url # 1`#1`urlprefixhref # 1#2#2 # 1#1    c.  p. robert , g.  casella , monte carlo statistical methods , springer , 2004 .",
    "y.  iba , population monte carlo algorithms , transactions of the japanese society for artificial intelligence 16 ( 2001 ) 279286 .",
    "o.  capp , a.  guillin , j.  m. marin , c.  p. robert , population monte carlo , journal of computational and graphical statistics 13  ( 4 ) ( 2004 ) 907929 .",
    "g.  celeux , j.  m. marin , c.  p. robert , iterated importance sampling in missing data problems , computational statistics & data analysis 50 ( 2006 ) 33863404 .",
    "m.  c. a.  m. bink , m.  p. boer , c.  j.  f. braak , j.  jansen , r.  e. voorrips , w.  e. van  de weg , bayesian analysis of complex traits in pedigreed plant populations , euphytica 161 ( 2008 ) 8596 .    c.  bi , a monte carlo em algorithm for de novo motif discovery in biomolecular sequences , ieee / acm transactions on computational biology and bioinformatics 6 ( 2009 ) 370386 .",
    "g.  e. barter , l.  k. purvis , n.  p. teclemariam , t.  h. west , analysis of detection systems for outdoor chemical or biological attacks , in : proc .",
    "ieee conf . on technologies for homeland security , 2009 .",
    "h.  bhaskar , l.  mihaylova , s.  maskell , population based particle filtering , in : iet seminar on target tracking and data fusion : algorithms and applications , 2008 .    c.  andrieu , j.  thoms , a tutorial on adaptive mcmc , statistics and computing 18  ( 4 ) ( 2008 ) 343373 .",
    "t.  li , m.  bolic , p.  m. djuric , resampling methods for particle filtering : classification , implementation , and strategies , ieee signal processing magazine 32  ( 3 ) ( 2015 ) 7086 .",
    "r.  douc , a.  guillin , j.  m. marin , c.  p. robert , convergence of adaptive mixtures of importance sampling schemes , annals of statistics 35 ( 2007 ) 420448 .",
    "r.  douc , a.  guillin , j.  m. marin , c.  p. robert , minimum variance importance sampling via population monte carlo , esaim : probability and statistics 11 ( 2007 ) 427447 .",
    "a.  iacobucci , j .-",
    "m . marin , c.  robert , on variance stabilisation in population monte carlo by double rao - blackwellisation , computational statistics & data analysis 54  ( 3 ) ( 2010 ) 698710 .",
    "o.  capp , r.  douc , a.  guillin , j.  m. marin , c.  p. robert , adaptive importance sampling in general mixture classes , statistics and computing 18 ( 2008 ) 447459 .",
    "e.  koblents , j.  mguez , a population monte carlo scheme with transformed weights and its application to stochastic kinetic models , statistics and computing ( 2013 ) 119 .",
    "j.  m. cornuet , j.  m. marin , a.  mira , c.  p. robert , adaptive multiple importance sampling , scandinavian journal of statistics 39  ( 4 ) ( 2012 ) 798812 .",
    "l.  martino , v.  elvira , d.  luengo , j.  corander , an adaptive population importance sampler : learning from the uncertanity , ieee transactions on signal processing 63  ( 16 ) ( 2015 ) 44224437 .",
    "d. moral , a.  doucet , a.  jasra , sequential monte carlo samplers , journal of the royal statistical society : series b ( statistical methodology ) 68  ( 3 ) ( 2006 ) 411436 .",
    "j.  s. liu , monte carlo strategies in scientific computing , springer , 2004 .",
    "r.  douc , o.  capp , e.  moulines , comparison of resampling schemes for particle filtering , in : proc .",
    "4@xmath212 int . symp . on image and",
    "signal processing and analysis , 2005 , pp .",
    "6469 .    p.",
    "del moral , a.  doucet , a.  jasra , on adaptive resampling strategies for sequential monte carlo methods , bernoulli 18  ( 1 ) ( 2012 ) 252278 .",
    "j.  geweke , bayesian inference in econometric models using monte carlo integration , econometrica : journal of the econometric society ( 1989 ) 13171339 .",
    "d.  w. scott , multivariate density estimation : theory , practice , and visualization , john wiley & sons , 2009 .",
    "m.  wand , m.  jones , kernel smoothing , chapman and hall , 1994 .",
    "e.  veach , l.  guibas , optimally combining sampling techniques for monte carlo rendering , in siggraph 1995 proceedings ( 1995 ) 419428 .",
    "a.  owen , y.  zhou , safe and effective importance sampling , journal of the american statistical association 95  ( 449 ) ( 2000 ) 135143 .",
    "g.  h. hardy , j.  e. littlewood , g.  plya , inequalities , cambridge univ . press , 1952 .",
    "a.  kong , a note on importance sampling using standardized weights , university of chicago , dept . of statistics ,",
    "rep 348 .",
    "v.  elvira , l.  martino , d.  luengo , m.  f. bugallo , generalized multiple importance sampling , arxiv preprint arxiv:1511.03095 .",
    "v.  elvira , l.  martino , d.  luengo , m.  f. bugallo , efficient multiple importance sampling estimators , signal processing letters , ieee 22  ( 10 ) ( 2015 ) 17571761 .",
    "o.  cappe , t.  rydeen , e.  moulines , inference in hidden markov models , springer , 2005 .",
    "m.  s. arulampalam , s.  maskell , n.  gordon , t.  clapp , a tutorial on particle filters for online nonlinear / non - gaussian bayesian tracking , signal processing , ieee transactions on 50  ( 2 ) ( 2002 ) 174188 .",
    "t.  li , s.  sun , t.  p. sattar , j.  corchado , fight sample degeneracy and impoverishment in particle filters : a review of intelligent approaches , expert systems with applications 41  ( 8) ( 2014 ) 39443954 .",
    "r.  h. shumway , d.  s. stoffer , time series analysis and its applications , springer science & business media , 2013 .",
    "j.  d. hamilton , r.  susmel , autoregressive conditional heteroskedasticity and changes in regime , journal of econometrics 64  ( 1 ) ( 1994 ) 307333 .",
    "e.  eberlein , application of generalized hyperbolic lvy motions to finance , in : lvy processes , springer , 2001 , pp . 319336 .",
    "m.  abramowitz , i.  a. stegun , handbook of mathematical functions : with formulas , graphs , and mathematical tables , no .  55 , dover pub . , 1972 .",
    "l.  martino , h.  yang , d.  luengo , j.  kanniainen , j.  corander , a fast universal self - tuned sampler within gibbs sampling , digital signal processing , in press , 2015 .",
    "a.  m. ali , k.  yao , t.  c. collier , e.  taylor , d.  blumstein , l.  girod , an empirical study of collaborative acoustic source localization , proc .",
    "information processing in sensor networks ( ipsn07 ) , boston .",
    "a.  t. ihler , j.  w. fisher , r.  l. moses , a.  s. willsky , nonparametric belief propagation for self - localization of sensor networks , ieee transactions on selected areas in communications 23  ( 4 ) ( 2005 ) 809819 .",
    "l.  martino , j.  mguez , generalized rejection sampling schemes and applications in signal processing , signal processing 90  ( 11 ) ( 2010 ) 29812995 .",
    "j.  geweke , bayesian inference in econometric models using monte carlo integration , econometrica 24 ( 1989 ) 13171399 .",
    "in this appendix we review the is estimators , analyzing the properties ( unbiasedness and variance ) of the estimator with the dm weights . for the sake of clarity",
    ", we remove the temporal indexes .",
    "let us consider the estimator of eq .",
    "when we have a set of @xmath15 proposal pdfs , @xmath213 .",
    "we draw exactly @xmath214 sample from each proposal , i.e. , @xmath215 for @xmath38 .",
    ", with @xmath216 , for the sake of clarity , but the analysis can be straightforwardly extended to any @xmath217 . ] if the normalizing constant @xmath14 is known , the is estimator is then @xmath218 the difference between the standard and deterministic mixture ( dm ) is estimators lies in the computation of the unnormalized weights . on the one hand",
    ", we recall the standard is weights are given by @xmath219 where @xmath220 is the target evaluated at the @xmath18-th sample ( drawn from the @xmath18-th proposal ) . substituting into",
    ", we obtain the standard is estimator , @xmath221 on the other hand , the weights in the dm approach are given by @xmath222 substituting into we obtain the dm estimator @xmath223      it is well known that @xmath224 in eq . is an unbiased estimator of the integral @xmath225 define in eq . @xcite . in this section , we prove that the dm - is estimator in eq . is also unbiased . since @xmath215 , we have @xmath226   & = & \\frac{1}{nz } \\sum_{i=1}^n   e_{q_i}\\left[\\frac{f({{\\bf x}}_i ) \\pi({{\\bf",
    "x}}_i)}{\\frac{1}{n}\\sum_{j=1}^{n}{q_j({{\\bf x}}_i)}}\\right]\\\\ & = & \\frac{1}{nz } \\sum_{i=1}^n   \\int \\frac{f({{\\bf",
    "x}}_i ) \\pi({{\\bf x}}_i)}{\\frac{1}{n}\\sum_{j=1}^{n}{q_j({{\\bf x}}_i ) } } q_i({{\\bf x}}_i)d{{\\bf x}}_i \\label{eq_i_2}\\\\ & = &    \\frac{1}{z } \\int \\frac{f({{\\bf x } } ) \\pi({{\\bf x}})}{\\frac{1}{n}\\sum_{j=1}^{n}{q_j({{\\bf x } } ) } }   \\left[\\frac{1}{n}\\sum_{i=1}^n q_i({{\\bf x}})\\right]d{{\\bf x}}\\label{eq_i_3}\\\\ & = &   \\frac{1}{z } \\int f({{\\bf x } } ) \\pi({{\\bf x } } ) d{{\\bf x}}= i.\\end{aligned}\\ ] ] @xmath227      in this section , we prove that the dm - is estimator in eq .",
    "always has a lower or equal variance than the standard is estimator of eq . .",
    "we also consider the standard mixture ( sm ) estimator @xmath228 , where @xmath15 samples are independently drawn from the mixture of proposals , i.e. , @xmath229 , and @xmath230 note that obtaining an is estimator with finite variance essentially amounts to having a proposal with heavier tails than the target .",
    "see @xcite for sufficient conditions that guarantee this finite variance .    for any target distribution , @xmath10 , any square integrable function w.r.t .",
    "@xmath10 , @xmath231 , and any set of proposal densities , @xmath232 , such that the variance of the corresponding estimators is finite , the variance of the dm estimator is always lower or equal than the variance of the corresponding standard is and mixture ( sm ) estimators , i.e. , @xmath233 [ theorem_1 ]          _ proof : _ the variance of the is estimator is given by @xmath235 where @xmath236 is the true value of the integral that we want to estimate @xcite .",
    "the variance of the sm estimator is given by @xmath237 where @xmath238 .",
    "substracting and , we get @xmath239 hence , since @xmath240 , in order to prove the theorem it is sufficient to show that @xmath241 now , let us note that the left hand side of is the inverse of the arithmetic mean of @xmath242 , @xmath243 whereas the right hand side of is the inverse of the harmonic mean of @xmath242 , @xmath244 therefore , the inequality in is equivalent to stating that @xmath245 , or equivalently @xmath246 , which is the well - known arithmetic mean  harmonic mean inequality for positive real numbers @xcite .",
    "note that can also be proved using jensen s inequality in eq . with @xmath247 , @xmath248 and @xmath249 for @xmath38 .",
    "@xmath250        _ proof : _ the variance of @xmath252 is computed @xmath253    - e_{q_i}^2\\left [ \\frac{f({{\\bf x}}_i ) { \\pi}({{\\bf x}}_i)}{\\psi({{\\bf x}}_i ) }   \\right ] \\right ) \\nonumber \\\\   & = & \\frac{1}{n^2z^2 } \\sum_{i=1}^n \\left ( \\int \\frac{f^2({{\\bf x}}){\\pi}^2({{\\bf x}})}{\\psi^2({{\\bf x}})}q_i({{\\bf x}})d{{\\bf x}}\\right )   - \\frac{1}{n^2z^2 } \\sum_{i=1}^n \\left(\\int \\frac{f({{\\bf x}}){\\pi}({{\\bf x}})}{\\psi({{\\bf x}})}q_i({{\\bf x}})d{{\\bf x}}\\right)^2 \\nonumber \\\\     & = & \\frac{1}{nz^2 } \\left ( \\int \\frac{f^2({{\\bf x}}){\\pi}^2({{\\bf x}})}{\\psi^2({{\\bf x } } ) } \\left [ \\frac{1}{n } \\sum_{i=1}^n q_i({{\\bf x } } ) \\right]d{{\\bf x}}\\right )   - \\frac{1}{n^2z^2 } \\sum_{i=1}^n \\left(\\int \\frac{f({{\\bf x}}){\\pi}({{\\bf x}})}{\\psi({{\\bf x}})}q_i({{\\bf x}})d{{\\bf x}}\\right)^2 \\nonumber \\\\     & = & \\frac{1}{nz^2 } \\int \\frac{f^2({{\\bf x}}){\\pi}^2({{\\bf x}})}{\\psi({{\\bf x}})}d{{\\bf x}}- \\frac{1}{n^2z^2 } \\sum_{i=1}^n \\left(\\int \\frac{f({{\\bf x}}){\\pi}({{\\bf x}})}{\\psi({{\\bf x}})}q_i({{\\bf x}})d{{\\bf x}}\\right)^2 \\label{eq_var_dm}\\end{aligned}\\ ] ] analyzing eqs . and , we see that proving @xmath254 is equivalent to proving that @xmath255 by defining @xmath256 , can be expressed more compactly as @xmath257 the inequality in eq .",
    "( [ eq_cs_proof ] ) holds , since it corresponds to the definition of the cauchy - schwarz inequality @xcite , @xmath258 with @xmath259 for @xmath216 .",
    "once more , can also by proved by using jensen s inequality in with @xmath260 , @xmath248 and @xmath261 for @xmath38"
  ],
  "abstract_text": [
    "<S> population monte carlo ( pmc ) sampling methods are powerful tools for approximating distributions of static unknowns given a set of observations . </S>",
    "<S> these methods are iterative in nature : at each step they generate samples from a proposal distribution and assign them weights according to the importance sampling principle . </S>",
    "<S> critical issues in applying pmc methods are the choice of the generating functions for the samples and the avoidance of the sample degeneracy . in this paper </S>",
    "<S> , we propose three new schemes that considerably improve the performance of the original pmc formulation by allowing for better exploration of the space of unknowns and by selecting more adequately the surviving samples . </S>",
    "<S> a theoretical analysis is performed , proving the superiority of the novel schemes in terms of variance of the associated estimators and preservation of the sample diversity . </S>",
    "<S> furthermore , we show that they outperform other state of the art algorithms ( both in terms of mean square error and robustness w.r.t . </S>",
    "<S> initialization ) through extensive numerical simulations .    </S>",
    "<S> population monte carlo , adaptive importance sampling , proposal distribution , resampling . </S>"
  ]
}