{
  "article_text": [
    "since introduced by howard @xcite , the concept of the expected value of information has long been studied in the context of decision analysis @xcite and applied to various areas , such as medical decision making @xcite , environmental science @xcite and petroleum engineering @xcite .",
    "the expected value of information is defined as the expected increase in monetary value brought from reducing some degree of uncertainty on unknown parameters involved in a decision model by obtaining additional information .",
    "there are several definitions of the expected value of information depending on the type of information , which includes perfect information , partial perfect information and sample information .",
    "in particular , the expected value of partial perfect information ( evppi ) , or sometimes called the partial expected value of perfect information , denotes the value of eliminating uncertainty on a subset of unknown parameters completely , and has been advocated and used as a decision - theoretic sensitivity index for identifying relatively important unknown parameters @xcite .    for many problems encountered in practice , calculating the evppi analytically is not possible . the simplest and most often - used method",
    "to approximately evaluate the evppi is the nested monte carlo computation @xcite .",
    "as pointed out in @xcite , however , the standard nested monte carlo computation of the evppi results in biased estimates , which directly follows from jensen s inequality .",
    "moreover , it can be inferred from ( * ? ? ?",
    "* section  2 ) that the standard nested monte carlo computation can not achieve the square - root convergence rate in the total computational budget .",
    "in fact , the author of this paper empirically observed a deteriorated convergence rate for a simple toy problem in @xcite .",
    "therefore , an unbiased and efficient computation of the evppi might be of particular interest to practitioners . in this line of investigation",
    ", there have been some recent attempts to construct such computational algorithms @xcite .",
    "as far as the author knows , however , every algorithm proposed in the literature has its own restrictions , for instance , on a decision model , and there is no general algorithm with mild assumptions .    in this paper",
    "we construct general unbiased monte carlo estimators for the evppi as well as the expected value of perfect information ( evpi ) .",
    "our estimators for the evppi on a certain subset of unknown parameters only assume that i.i.d .",
    "random sampling from the conditional distribution of the complement of unknown parameters should be possible .",
    "if this is not the case , it might be necessary to incorporate markov chain monte carlo sampling into our estimators , although such an investigation is beyond the scope of this paper . for a decision model which satisfies the above assumption ,",
    "our estimators are quite simple and straightforward to implement .",
    "our approach to construct unbiased estimators is based on the multilevel monte carlo ( mlmc ) method , which was first introduced by heinrich @xcite for parametric integration and by giles @xcite for path simulation , and was later extended by rhee and glynn @xcite .",
    "we refer to @xcite for a state - of - the - art review on the mlmc method .",
    "the idea of the mlmc method can be simply described as follows : for a dimension @xmath0 , let @xmath1^s)$ ] , and @xmath2^s)$ ] be a sequence of functions which approximates @xmath3 with increasing accuracy ( in the @xmath4 norm ) but also with increasing computational cost .",
    "we denote by @xmath5 the true integral of @xmath3 , i.e. , @xmath6^s}f(x){\\,\\mathrm{d}}x .    \\end{aligned}\\ ] ] the naive monte carlo computation chooses @xmath7 points @xmath8 independently and randomly from @xmath9^s$ ] to approximate @xmath5 by the average @xmath10 for some @xmath11 .",
    "note that the former is an unbiased estimator of @xmath5 whereas the latter contains the bias @xmath12 .",
    "the mlmc method , on the other hand , uses the telescopic representation @xmath13 , and then each term is independently approximated by the naive monte carlo computation , i.e. , @xmath14 where we set @xmath15 and @xmath16 .",
    "for the level @xmath17 such that @xmath18 , the corresponding average is set to 0 . the original mlmc method in @xcite",
    "considers the case @xmath19 , that is , the telescopic representation of @xmath3 is truncated up to @xmath11 terms .",
    "the resulting estimator contains the bias @xmath12 .",
    "the extended mlmc method in @xcite introduces a probability mass function @xmath20 such that @xmath21 for all @xmath22 , where @xmath23 denotes the set of positive integers , and considers the _ single term estimator _",
    "@xmath24 or the _ coupled sum estimator _",
    "@xmath25 where @xmath26 and @xmath8 are chosen independently and randomly according to @xmath20 and @xmath27^s)$ ] , respectively .",
    "these estimators are shown to be unbiased @xcite .",
    "in this setting , the superiority of the mlmc method over the naive monte carlo method depends on the balance between the growth rate of the computational costs for @xmath28 and the decay rate of the variances of @xmath29 .",
    "an application of the mlmc method to the nested monte carlo computation in a different context has been done , for instance , in @xcite and also mentioned in ( * ? ? ?",
    "* section  9 ) .",
    "however , the mlmc method has never been applied to computations of the expected value of information . in this paper , we show that the framework of the mlmc method actually fits quite well into constructing unbiased estimators both for the evpi and the evppi .",
    "because of their simplicity and efficiency , we believe that our unbiased estimators will be one of the most standard choices particularly for evaluating the evppi .",
    "finally , it should be remarked that an unbiased estimator for optimization of expectations has been constructed very recently by blanchet and glynn @xcite in a general context , whose main approach is commonly used in this paper .",
    "the remainder of this paper is organized as follows . in the next section ,",
    "we introduce the definitions of the evpi and the evppi , and then discuss the standard nested monte carlo computations . in section  [ sec:3 ] , we construct unbiased estimators for the evpi and the evppi based on the mlmc method , and also briefly discuss some practical issues relating to implementation .",
    "we conclude this paper with numerical experiments in section  [ sec:4 ] .",
    "let @xmath30 be a finite set of decision options .",
    "the task of a decision maker is to decide which option @xmath31 is optimal under uncertainty of @xmath32 . here",
    "@xmath32 is assumed to be a continuous random variable defined on the @xmath0-dimensional domain @xmath33 with density @xmath34 , and a monetary value function @xmath35 is assigned for each option @xmath31 . throughout this paper , we assume @xmath36 . under the risk neutrality assumption ,",
    "the optimal option is one which maximizes the expected monetary value @xmath37:=\\int_{\\omega_x}f_d(x)p_x(x){\\,\\mathrm{d}}x .",
    "\\end{aligned}\\ ] ] thus the expected monetary value without additional information is given by @xmath38 $ ] .",
    "suppose that perfect information is available .",
    "in this situation , a decision maker can decide an optimal option after eliminating uncertainty of @xmath32 completely .",
    "therefore , the monetary value for a decision maker after @xmath39 is indicated by perfect information is simply given by @xmath40 . as a result , the expected monetary value with perfect information becomes @xmath41 $ ] .",
    "the evpi denotes how much the expected monetary value is increased by eliminating uncertainty of @xmath32 .",
    "thus the evpi is defined by @xmath42 - \\max_{d\\in d}{\\mathbb{e}}_x\\left[f_d\\right ] .",
    "\\end{aligned}\\ ] ] note that the evpi is equivalent to how much a decision maker is willing to pay for obtaining perfect information .",
    "assume that the random variable @xmath32 is separable into ( possibly correlated ) two random variables as @xmath43 with @xmath44 , and that available information is perfect only for @xmath45 .",
    "in this situation , a decision maker can decide an optimal option under uncertainty of @xmath46 after eliminating uncertainty of @xmath45 completely .",
    "therefore , the monetary value for a decision maker after @xmath47 is indicated by partial perfect information is given by @xmath48 $ ] , where the expectation is taken with respect to @xmath46 given @xmath47 . as a result ,",
    "the expected monetary value with partial perfect information for @xmath45 becomes @xmath49 \\right]$ ] .",
    "thus , similarly to the evpi , the evppi on @xmath45 is defined by @xmath50 \\right ] - \\max_{d\\in d}{\\mathbb{e}}_x\\left[f_d\\right ] .",
    "\\end{aligned}\\ ] ] here we recall that the marginal density function of @xmath45 and the conditional density function of @xmath46 given @xmath47 are given by @xmath51 and @xmath52 respectively .",
    "since both the evpi and the evppi are often difficult to calculate analytically , the monte carlo computations are used in practice .",
    "let us consider the evpi first .",
    "for @xmath53 , let @xmath8 and @xmath54 be i.i.d .",
    "random samples generated from @xmath34 .",
    "the evpi is approximated by @xmath55 we can approximate the evppi in a similar way .",
    "let @xmath56 .",
    "let @xmath54 be i.i.d .",
    "random samples generated from @xmath34 , and @xmath57 i.i.d .",
    "random samples generated from @xmath58 . for each @xmath59 , let @xmath60 be i.i.d .",
    "random samples generated from @xmath61 .",
    "then the evppi is approximated by the following nested form @xmath62 in the case where there is no correlation between @xmath45 and @xmath46 , random samples @xmath60 used in the inner sum can be replaced by @xmath63 i.i.d .",
    "random samples generated from @xmath64 for all @xmath59 .    here",
    "we would emphasize that both the monte carlo estimators @xmath65 and @xmath66 are biased .",
    "that is , @xmath67 \\neq { \\mathrm{evpi}}\\quad \\text{and}\\quad { \\mathbb{e}}\\left[\\overline{{\\mathrm{evppi}}}_{x^{(1)}}\\right ] \\neq { \\mathrm{evppi}}_{x^{(1)}}.    \\end{aligned}\\ ] ] this result follows directly from jensen s inequality . in case of the evpi , we have @xmath67 & = { \\mathbb{e}}\\left[\\frac{1}{n}\\sum_{n=1}^{n}\\max_{d\\in d}f_d(x_n)\\right ] - { \\mathbb{e}}\\left[\\max_{d\\in d}\\frac{1}{l}\\sum_{l=1}^{l}f_d(x'_l)\\right ] \\\\     & \\leq { \\mathbb{e}}\\left[\\frac{1}{n}\\sum_{n=1}^{n}\\max_{d\\in d}f_d(x_n)\\right ]   - \\max_{d\\in d}{\\mathbb{e}}\\left[\\frac{1}{l}\\sum_{l=1}^{l}f_d(x'_l)\\right ] \\\\     & = { \\mathbb{e}}_x\\left[\\max_{d\\in d}f_d \\right ] - \\max_{d\\in d}{\\mathbb{e}}_x\\left[f_d\\right ] = { \\mathrm{evpi } } ,    \\end{aligned}\\ ] ] where the inequality stems from jensen s inequality .",
    "it is clear that the first term of the evpi can be estimated without any bias , whereas the second term is estimated with a positive bias .",
    "we refer to @xcite for a possible bounding technique to quantify the bias .",
    "therefore , in total , @xmath65 is a downward biased estimator of @xmath68 . in case of the evppi ,",
    "both the first and second terms of the evppi are estimated with positive biases .",
    "thus it is difficult to conclude whether the estimator @xmath66 is biased either upward or downward . nevertheless , these two biases are not cancelled out , so that we have @xmath69 \\neq { \\mathrm{evppi}}_{x^{(1)}}$ ] .",
    "moreover , there is a limitation in the asymptotic convergence rate of the estimator @xmath66 . for simplicity ,",
    "let us focus on the first term of the evppi , which is approximated as @xmath70 \\right ] \\sim",
    "\\frac{1}{n}\\sum_{n=1}^{n}\\max_{d\\in d}\\frac{1}{m}\\sum_{m=1}^{m}f_d(x^{(1)}_n , x^{(2)}_{m , n } ) .",
    "\\end{aligned}\\ ] ] it is clear from jensen s inequality that the operator @xmath71 yields a positive bias .",
    "assuming that the bias depends on @xmath45 and decays uniformly at a certain rate @xmath72 for @xmath73 , it follows from the heuristic argument in ( * ? ? ?",
    "* section  2 ) that the approximation error itself decays at the rate @xmath74 notice that the total number of samples used is @xmath75 . by putting @xmath76 and @xmath77 for @xmath78 ,",
    "the convergence rate of the standard nested monte carlo estimator in the total computational budget @xmath79 is given by the form @xmath80 , where @xmath81    let us assume that the bias decays at a faster rate than the canonical monte carlo rate , i.e. , @xmath82 .",
    "when @xmath83 , that is , when @xmath84 , the resulting value for @xmath85 is @xmath86 .",
    "this convergence rate is also observed empirically in @xcite .",
    "the best possible value for @xmath85 is actually @xmath87 , which is strictly less than @xmath88 and is attained when @xmath89 .",
    "this result suggests fewer samples for the inner sum and more for the outer sum , which is opposite from what is empirically recommended in @xcite .",
    "nevertheless , the above argument concludes that the square - root convergence rate in @xmath79 can not be achieved by the standard nested monte carlo estimator .",
    "we now move on to constructing unbiased monte carlo estimators for the evpi and the evppi based on the mlmc method .",
    "we first construct unbiased monte carlo estimators for the evpi . for @xmath90 ,",
    "let us denote @xmath91 where @xmath8 are i.i.d .",
    "random samples generated from @xmath34 .",
    "then the following simple but interesting properties hold : @xmath92 = { \\mathbb{e}}\\left[\\max_{d\\in d}f_d(x_1)\\right ] = { \\mathbb{e}}_x\\left[\\max_{d\\in d}f_d \\right ] ,    \\end{aligned}\\ ] ] and @xmath93 ,    \\end{aligned}\\ ] ] where the latter property stems from the law of large numbers .",
    "hence it is trivial that @xmath94 = \\max_{d\\in d}{\\mathbb{e}}_x\\left[f_d\\right ] .",
    "\\end{aligned}\\ ] ] therefore , the evpi can be rewritten into @xmath95 - \\lim_{n\\to \\infty}{\\mathbb{e}}\\left[q(n)\\right ] .",
    "\\end{aligned}\\ ] ] based on this finding and the idea of the mlmc method , unbiased monte carlo estimators for the evpi can be constructed . for a fixed integer @xmath96",
    ", we have the telescopic representation for the right - hand side of ( [ eq : evpi_rewrite ] ) @xmath92 - \\lim_{n\\to \\infty}{\\mathbb{e}}\\left[q(n)\\right ]      & = \\left\\ { { \\mathbb{e}}\\left[q(1)\\right]-{\\mathbb{e}}\\left[q(b)\\right]\\right\\}+\\left\\ { { \\mathbb{e}}\\left[q(b)\\right]-{\\mathbb{e}}\\left[q(b^2)\\right]\\right\\}+\\cdots \\\\     & = \\sum_{l=1}^{\\infty}\\left\\ { { \\mathbb{e}}\\left[q(b^{l-1})\\right]-{\\mathbb{e}}\\left[q(b^l)\\right]\\right\\ } \\\\     & = \\sum_{l=1}^{\\infty}{\\mathbb{e}}\\left[q(b^{l-1})-q(b^l)\\right ] ,    \\end{aligned}\\ ] ] where the last equality stems from the linearity of expectation .",
    "now let us introduce a positive integer - valued independent random variable @xmath11 with a probability mass function @xmath97 such that @xmath98 for all @xmath22 .",
    "then the first _ single term estimator _ of the evpi is given by @xmath99 where @xmath26 are i.i.d .",
    "random samples generated from @xmath97 , and for each @xmath22 we define @xmath100 ,    \\end{aligned}\\ ] ] where @xmath101 are i.i.d .",
    "random samples generated from @xmath34 .",
    "note that the same samples @xmath101 are commonly used in the first and second terms in @xmath102 .",
    "it can be seen that this estimator is unbiased : @xmath103 & = { \\mathbb{e}}\\left[\\sum_{l=1}^{\\infty}p_l(l)y_{l}^{{\\mathrm{single } } } \\right ] = \\sum_{l=1}^{\\infty}p_l(l){\\mathbb{e}}\\left[y_{l}^{{\\mathrm{single } } } \\right ] \\\\     & = \\sum_{l=1}^{\\infty}{\\mathbb{e}}\\left[q(b^{l-1})-q(b^l)\\right ] = { \\mathrm{evpi}}.    \\end{aligned}\\ ] ]    the second _ coupled sum estimator _ of the evpi is given by @xmath104 where @xmath26 are i.i.d .",
    "random samples generated from @xmath97 , and for each @xmath22 we define @xmath105 ,    \\end{aligned}\\ ] ] where @xmath101 are i.i.d .",
    "random samples generated from @xmath34 .",
    "note that the same samples @xmath101 are commonly used in every term in @xmath106 .",
    "it can be seen that this estimator is also unbiased : @xmath107 & = { \\mathbb{e}}\\left[\\sum_{l=1}^{\\infty}p_l(l)y_{l}^{{\\mathrm{coupled } } } \\right ] = \\sum_{l=1}^{\\infty}p_l(l){\\mathbb{e}}\\left[y_{l}^{{\\mathrm{coupled } } } \\right ] \\\\     & = \\sum_{l=1}^{\\infty}\\sum_{j=1}^{l}\\frac{p_l(l)}{\\sum_{k = j}^{\\infty}p_l(k)}{\\mathbb{e}}\\left[q(b^{j-1})-q(b^j ) \\right ] \\\\     & = \\sum_{j=1}^{\\infty}\\sum_{l = j}^{\\infty}\\frac{p_l(l)}{\\sum_{k = j}^{\\infty}p_l(k)}{\\mathbb{e}}\\left[q(b^{j-1})-q(b^j ) \\right ] \\\\     & = \\sum_{j=1}^{\\infty}{\\mathbb{e}}\\left[q(b^{j-1})-q(b^j)\\right ] = { \\mathrm{evpi } } ,    \\end{aligned}\\ ] ] where the fourth equality is given by swapping the order of sums .      in a way similar to that for the evpi , it is possible to construct unbiased monte carlo estimators for the evppi .",
    "since unbiased estimators for the evpi have been constructed already , it suffices to construct unbiased estimators for @xmath108 - { \\mathbb{e}}_{x^{(1)}}\\left[\\max_{d\\in d}{\\mathbb{e}}_{x^{(2)}\\mid x^{(1)}}\\left[f_d\\right ] \\right ] .",
    "\\end{aligned}\\ ] ] for @xmath90 and @xmath109 , let us denote @xmath110 where @xmath111 are generated independently and randomly from @xmath112 .",
    "then the following properties hold : @xmath113 = { \\mathbb{e}}_{x^{(2)}\\mid x^{(1)}}\\left[\\max_{d\\in d}f_d(x^{(1)},\\cdot ) \\right ] ,    \\end{aligned}\\ ] ] and @xmath114 = \\max_{d\\in d}{\\mathbb{e}}_{x^{(2)}\\mid x^{(1)}}\\left[f_d(x^{(1)},\\cdot)\\right ] ,    \\end{aligned}\\ ] ] where the latter property stems again from the law of large numbers . using these results , we have @xmath115 - { \\mathbb{e}}_{x^{(1)}}\\left[\\max_{d\\in d}{\\mathbb{e}}_{x^{(2)}\\mid x^{(1)}}\\left[f_d\\right ] \\right ] \\\\     & =   { \\mathbb{e}}_{x^{(1)}}{\\mathbb{e}}\\left[q(1;\\cdot)\\right ] - { \\mathbb{e}}_{x^{(1)}}\\left[\\lim_{n\\to \\infty}{\\mathbb{e}}\\left[q(n;\\cdot)\\right]\\right ] \\\\     & = { \\mathbb{e}}_{x^{(1)}}\\left[{\\mathbb{e}}\\left[q(1;\\cdot)\\right ] - \\lim_{n\\to \\infty}{\\mathbb{e}}\\left[q(n;\\cdot)\\right]\\right ] .",
    "\\end{aligned}\\ ] ] for a fixed integer @xmath96 , we have the telescopic representation @xmath116 - \\lim_{n\\to \\infty}{\\mathbb{e}}\\left[q(n;\\cdot)\\right]\\right ] & = { \\mathbb{e}}_{x^{(1)}}\\left [ \\sum_{l=1}^{\\infty}\\left\\ { { \\mathbb{e}}\\left[q(b^{l-1};\\cdot)\\right]-{\\mathbb{e}}\\left[q(b^l;\\cdot)\\right]\\right\\}\\right ] \\\\     & = { \\mathbb{e}}_{x^{(1)}}\\left [ \\sum_{l=1}^{\\infty}{\\mathbb{e}}\\left[q(b^{l-1};\\cdot)-q(b^l;\\cdot)\\right ] \\right ] .",
    "\\end{aligned}\\ ] ]    now let us introduce a positive integer - valued independent random variable @xmath117 with a probability mass function @xmath118 such that @xmath119 for all @xmath120 .",
    "then the first _ single term estimator _ of @xmath121 is given by @xmath122 where @xmath26 and @xmath57 are i.i.d .",
    "random samples generated from @xmath97 and @xmath58 , respectively , and for each @xmath22 and @xmath109 we define @xmath123 ,    \\end{aligned}\\ ] ] where @xmath124 are i.i.d .",
    "random samples generated from @xmath112 .",
    "it can be seen that this estimator is unbiased : @xmath125 & = { \\mathbb{e}}\\left[\\sum_{l=1}^{\\infty}p_l(l){\\mathbb{e}}_{x^{(1)}}\\left[z_{l_n , \\cdot}^{{\\mathrm{single}}}\\right ]   \\right ] = { \\mathbb{e}}_{x^{(1)}}\\left[\\sum_{l=1}^{\\infty}p_l(l){\\mathbb{e}}\\left[z_{l_n , \\cdot}^{{\\mathrm{single } } } \\right]\\right ] \\\\     & = { \\mathbb{e}}_{x^{(1)}}\\left[\\sum_{l=1}^{\\infty}{\\mathbb{e}}\\left[q(b^{l-1};\\cdot)-q(b^l;\\cdot)\\right]\\right ] = { \\mathrm{evpi}}.    \\end{aligned}\\ ] ]    the second _ coupled sum estimator _ of @xmath121 is given by @xmath126 where @xmath26 and @xmath57 are i.i.d .",
    "random samples generated from @xmath97 and @xmath58 , respectively , and for each @xmath22 and @xmath109 we define @xmath127 ,    \\end{aligned}\\ ] ] where @xmath124 are i.i.d .",
    "random samples generated from @xmath112 .",
    "note that the same samples @xmath101 are commonly used in every term in @xmath128 .",
    "this estimator can be shown unbiased in a similar way as above .",
    "therefore , @xmath129 can be estimated without any bias by @xmath130 where @xmath131 , and @xmath26 and @xmath132 are i.i.d .",
    "random samples generated from @xmath97 .",
    "note that even if one randomly chosen level @xmath133 is used commonly for each @xmath59 as @xmath134 the resulting estimator is still unbiased .",
    "so far , we do not specify the probability mass function @xmath118 in both the estimators for the evpi and the evppi .",
    "it should be chosen so as for both the variance of the estimator and the expected computational cost to be finite @xcite .",
    "let us consider the estimator ( [ eq : est_evpi_single ] ) of the evpi as an example .",
    "the variance and the expected computational cost per one sample of ( [ eq : est_evpi_single ] ) are given by @xmath135}{p_l(l)}\\quad \\text{and } \\quad \\sum_{l=1}^{\\infty}p_l(l)b^l ,    \\end{aligned}\\ ] ] respectively , where the expectation is taken with respect to random samples @xmath136 for every @xmath22 .",
    "it can be easily shown that the optimal choice for @xmath97 , which minimizes @xmath137}{p_l(l ) } + \\lambda p_l(l)b^l\\right ) ,    \\end{aligned}\\ ] ] for some lagrange multiplier @xmath138 , is given by @xmath139}{b^l}}\\left ( \\sum_{j=1}^{\\infty}\\sqrt{\\frac{{\\mathbb{e}}\\left[(y_j^{{\\mathrm{single}}})^2\\right]}{b^j}}\\right)^{-1 } ,    \\end{aligned}\\ ] ] if the sum over @xmath140 is finite . in order to achieve the root mean square error less than @xmath141 , the required expected total computational cost is @xmath142 b^j}\\right)^{2 } ,    \\end{aligned}\\ ] ] again if the sum over @xmath140 is finite , see ( * ? ? ?",
    "* section  2.2 ) .",
    "note that the above argument also holds for other estimators ( [ eq : est_evpi_coupled ] ) , ( [ eq : est_evppi_single ] ) and ( [ eq : est_evppi_coupled ] ) .",
    "therefore , in preferable cases , our unbiased estimators can achieve the square - root convergence rate in @xmath79 , which is not possible for the nested monte carlo computation as heuristically discussed in the last section .",
    "note that if the sum over @xmath140 is not finite , on the other hand , there is no theoretical foundation on the convergence behavior of our estimators , so that one may observe a deteriorated convergence rate in practical computations .    in practice , we need a more reasonable choice for @xmath97 in some sense since the expectations @xmath143 $ ] are not known in advance . as an alternative approach ,",
    "let us specify the form of @xmath97 as @xmath144 for all @xmath145 with @xmath146 .",
    "let us assume that the expectations @xmath143 $ ] decay at a rate @xmath147 for some @xmath148 .",
    "in order for both the variance of the estimator , in which @xmath143 $ ] is replaced by @xmath147 , and the expected computational cost to be finite , it suffices that @xmath149 holds .",
    "moreover , the optimal choice for @xmath97 obtained in the last paragraph gives @xmath150 .",
    "the same value of @xmath151 can be obtained , as done in @xcite where the special case with @xmath152 and @xmath153 is considered , by minimizing the work - normalized variance @xmath154    furthermore , in practical applications , one may set the total computational cost @xmath79 instead of @xmath7 , i.e. , the number of i.i.d .",
    "copies used in the estimators . in this case , we first generate a random sequence @xmath155 independently from @xmath97 and then define @xmath7 by @xmath156",
    "finally , we conduct numerical experiments for a simple toy problem . in order to evaluate the approximation error quantitatively",
    ", we design a toy problem such that the evpi and the evppi can be calculated analytically .",
    "let us consider the following setting .",
    "let @xmath157 be a set of two possible actions which can be taken by a decision maker under uncertainty of @xmath158 .",
    "for @xmath159 we define @xmath160 where @xmath161 and @xmath162 .",
    "the prior probability density of @xmath32 is given by @xmath163 for given @xmath164 and @xmath165 .",
    "then we have @xmath166 : = \\max\\left\\ { \\int_{{\\mathbb{r}}^s}f_{d_1}(x)p(x){\\,\\mathrm{d}}x , 0\\right\\ } = \\max\\left\\ { w_0+\\sum_{j=1}^{s}w_j\\mu_j , 0\\right\\}.\\end{aligned}\\ ] ]    now let @xmath167 be a subset of @xmath168",
    ". for simplicity , let us focus on the case @xmath169 .",
    "we write @xmath170 , @xmath171 and @xmath172 .",
    "the evppi on @xmath173 can be calculated analytically as follows .",
    "since there is no correlation between @xmath173 and @xmath174 , we have @xmath175 \\right ] \\\\ & \\quad = \\int_{{\\mathbb{r}}^{|u|}}\\max\\left\\ { \\int_{{\\mathbb{r}}^{s-|u|}}f_{d_1}(x_u , x_{-u})\\prod_{j\\in -u}p_{x_j}(x_j){\\,\\mathrm{d}}x_{-u } , 0\\right\\}\\prod_{j\\in u}p_{x_j}(x_j){\\,\\mathrm{d}}x_u \\\\ & \\quad = \\int_{{\\mathbb{r}}^{|u|}}\\max\\left\\ { \\sum_{j\\in u}w_jx_j+w_0+\\sum_{j\\in -u}w_j\\mu_j , 0\\right\\}\\prod_{j\\in u}p_{x_j}(x_j){\\,\\mathrm{d}}x_u \\\\ & \\quad = \\int_{\\omega_{\\geq 0}}\\left ( \\sum_{j\\in u}w_jx_j+w_0+\\sum_{j\\in -u}w_j\\mu_j\\right ) \\prod_{j\\in u}p_{x_j}(x_j){\\,\\mathrm{d}}x_u , \\end{aligned}\\ ] ] where we write @xmath176 . by changing the variables according to @xmath177 where @xmath178 \\in { \\mathbb{r}}^{|u|\\times |u| } , \\end{aligned}\\ ] ] we have @xmath179 i.e.",
    ", the sum of @xmath180 s is nothing but the last component of @xmath181 .",
    "moreover , the probability density of @xmath182 is the normal density with the mean @xmath183 and the variance @xmath184 , the above integral can be written into @xmath185 \\right ] & = \\int_{\\omega_{\\geq 0}}\\left ( \\sum_{j\\in u}w_jx_j+w_0+\\sum_{j\\in -u}w_j\\mu_j\\right ) \\prod_{j\\in u}p_{x_j}(x_j){\\,\\mathrm{d}}x_u \\\\ & = \\int_{a}^{\\infty}\\left ( y_{|u|}-a\\right ) p(y_{|u|}){\\,\\mathrm{d}}y_{|u| } , \\end{aligned}\\ ] ] where we write @xmath186 finally , the last integral equals @xmath185 \\right ] = \\left [ 1-\\phi\\left ( -\\frac{\\mu_{{\\mathrm{all}}}}{\\sigma_u}\\right)\\right]\\mu_{{\\mathrm{all } } } + \\phi\\left ( -\\frac{\\mu_{{\\mathrm{all}}}}{\\sigma_u}\\right ) \\sigma_u,\\end{aligned}\\ ] ] where we write @xmath187 and @xmath188 further , @xmath189 denotes the standard normal density function and @xmath190 does the cumulative distribution function for @xmath189 .",
    "thus , the evppi on @xmath173 is given by @xmath191\\mu_{{\\mathrm{all } } } + \\phi\\left ( -\\frac{\\mu_{{\\mathrm{all}}}}{\\sigma_u}\\right ) \\sigma_u -\\max\\left\\ { \\mu_{{\\mathrm{all } } } , 0\\right\\}.\\end{aligned}\\ ] ] note that the analytical expression for the evpi is given by setting @xmath192 .",
    "in what follows , we focus on the case where @xmath193 and @xmath194 , @xmath195 , @xmath196 for all @xmath197 for the sake of simplicity .",
    "first , let us consider the evpi computation .",
    "the analytical calculation of the evpi is given by @xmath198 .",
    "we use the three estimators ( [ eq : evpi_mc ] ) , ( [ eq : est_evpi_single ] ) and ( [ eq : est_evpi_coupled ] ) of the evpi for approximate evaluations .",
    "when the total computational budget equals @xmath79 , the naive monte carlo estimator ( [ eq : evpi_mc ] ) is set by @xmath199 , whereas our proposed estimators ( [ eq : est_evpi_single ] ) and ( [ eq : est_evpi_coupled ] ) are set as described in the last paragraph of subsection  [ subsec : imple ] with @xmath152 and @xmath144 where @xmath200 .",
    "for a given total computational budget , 100 independent computations are conducted for each estimator .",
    "figure  [ fig : evpi ] compares the boxplots of the evpi computations obtained by three estimators as functions of @xmath201 with @xmath202 .",
    "it can be seen that the naive monte carlo estimator gives more accurate results than our estimators . in case of the evpi ,",
    "the naive monte carlo estimator is not of the nested form so that the approximation error decays at a rate of @xmath203 if the bias decays at a faster rate than the canonical monte carlo rate .",
    "moreover , it can be expected that the variances of our estimators are much larger than that of the naive monte carlo estimator , which yields wider variations among the independent evpi computations by our estimators as well as the difficulty in confirming the unbiasedness of our estimators when the total computational budget is small .",
    "let us move on to the evppi computation , in which case the situation changes significantly .",
    "because of the invariance of parameters , we focus on computing the evppi s on @xmath204 .",
    "the analytical calculations of the evppi s are given by @xmath205 , @xmath206 , @xmath207 , and @xmath208 , respectively .",
    "we use the three estimators ( [ eq : evppi_mc ] ) , ( [ eq : evppi_mlmc ] ) with the single term estimator ( @xmath209 ) , and ( [ eq : evppi_mlmc ] ) with the coupled sum estimator ( @xmath210 ) of the evpi for approximate evaluations .",
    "when the total computational budget equals @xmath79 , the nested monte carlo estimator ( [ eq : evppi_mc ] ) is set by @xmath211 , @xmath212 and @xmath213 as heuristically suggested in subsection  [ subsec : nested_mc ] with @xmath214 .",
    "in fact , although we also conducted the same numerical experiments by setting both @xmath215 and @xmath216 , we obtained similar results to those with @xmath217 , which are thus omitted in this paper .",
    "our proposed estimators ( [ eq : evppi_mlmc ] ) with @xmath131 are set as described in the last paragraph of subsection  [ subsec : imple ] with @xmath152 and @xmath144 where @xmath200 .",
    "for a given total computational budget , 100 independent computations are conducted for each estimator .",
    "figure  [ fig : evppi ] compares the boxplots of the evppi computations on @xmath204 ( from upper panels to lower panels ) obtained by three estimators as functions of @xmath201 with @xmath202 .",
    "it is obvious that the convergence behavior of the nested monte carlo estimator is much worse than that of the monte carlo estimator used for computing the evpi , as can be expected from the heuristic argument in subsection  [ subsec : nested_mc ] . on the other hand , the convergence behaviors of our estimators",
    "do not differ so much whether they are used for computing either the evpi or the evppi , and the length of the boxes for both of our estimators decays to 0 much faster than that for the nested monte carlo estimator .",
    "hence , as can be seen , both of our estimators give more accurate results than that of the nested monte carlo estimator as @xmath79 increases . in practice , we recommend to use the coupled sum estimator since there are several outliers with large evppi values found in case of the single term estimator .",
    "99 andradttir , s. , glynn , p.  w , : computing bayesian means using simulation , acm trans . model .",
    "* 26 * , 2 , article no .  10 ( 2016 ) .",
    "bates , m.  e. , sparrevik , m. , lichy , n. , linkov , i. : the value of information for managing contaminated sediments , environ .",
    "technol . * 48 * , 94789485 ( 2014 ) .",
    "bickel , j.  e. , gibson , r.  l. , mcvay , d.  a. , pickering , s. , waggoner , j. : quantifying the reliability and value of 3d land seismic , spe res .",
    "eval . & eng .",
    "* 11 * , 832841 ( 2008 ) .",
    "blanchet , j.  h. , glynn , p.  w. : unbiased monte carlo for optimization and functions of expectations via multi - level randomization , in : proc .",
    "2015 winter simulation conference ( 2015 ) .",
    "bratvold , r.  b. , bickel , j.  e. , lohne , h.  p. : value of information in the oil and gas industry : past , present , and future , spe res .",
    "eval . & eng .",
    "* 12 * , 630638 ( 2009 ) .",
    "brennan , a. , kharroubi , s. , ohagan , a. , chilcott , j. : calculating partial expected value of perfect information via monte carlo sampling algorithms , med .",
    "decis . making * 27",
    "* , 448470 ( 2007 ) .",
    "bujok , k. , hambly , b.  m. , reisinger , c. : multilevel simulation of functionals of bernoulli random variables with application to basket credit derivatives , methodol .",
    "* 17 * , 579604 ( 2015 ) .",
    "claxton , k. : bayesian approaches to the value of information : implications for the regulation of new health care technologies , health econ .",
    "* 8 * , 269274 ( 1999 ) .",
    "coyle , d. , oakley , j.  e. : estimating the expected value of partial perfect information : a review of methods , eur . j. health econ .",
    "* 9 * , 251259 ( 2008 ) .",
    "dakin , m.  e. , toll , j.  e. , small , m.  j. , brand , k.  p. : risk - based environmental remediation : bayesian monte carlo analysis and the expected value of sample information , risk analysis * 16",
    "* , 6779 ( 1996 ) .",
    "delqui , p. : the value of information and intensity of preference , decision analysis * 5 * , 129139 ( 2008 ) .",
    "felli , j.  c. , hazen , g.  b. : sensitivity analysis and the expected value of perfect information , med .",
    "decis . making * 18 * , 95109 ( 1998 ) .",
    "giles , m.  b. : multilevel monte carlo path simulation , operations research * 56 * , 607617 ( 2008 ) .",
    "giles , m.  b. : multilevel monte carlo methods , acta numer . * 24 * , 259328 ( 2015 ) .",
    "heinrich , s. : monte carlo complexity of global solution of integral equations , j. complexity * 14 * , 151175 ( 1998 ) .",
    "howard , r.  a. : information value theory , ieee trans .",
    "* 2 * , 2226 ( 1966 ) .",
    "madan , j. , ades , a.  e. , price , m. , maitland , k. , jemutai , j. , revill , p. , welton , n.  j. : strategies for efficient computation of the expected value of partial perfect information , med .",
    "decis . making * 34 * , 327342 ( 2014 ) .",
    "mak , w .- k . ,",
    "morton , d.  p. , wood , r.  k. : monte carlo bounding techniques for determining solution quality in stochastic programs , operations research letters * 24 * , 4756 ( 1999 ) .",
    "nakayasu , m. , goda , t. , tanaka , k. , sato , k. : evaluating the value of single - point data in heterogeneous reservoirs with the expectation - maximization algorithm , spe econ . & mgmt .",
    "* 8 * , 110 ( 2016 ) .",
    "oakley , j.  e. : decision - theoretic sensitivity analysis for complex computer models , technometrics * 51 * , 121129 ( 2009 ) .",
    "oakley , j.  e. , brennan , a. , tappenden , p. , chilcott , j. : simulation sample sizes for monte carlo partial evpi calculations , j. health econ .",
    "* 29 * , 468477 ( 2010 ) .",
    "raiffa , h. : decision analysis : introductory lectures on choices under uncertainty .",
    "addison - wesley publishing company , massachusetts ( 1968 ) .",
    "rhee , c .- h . ,",
    "glynn , p.  w. : a new approach to unbiased estimation for sdes , in : proc .",
    "2012 winter simulation conference ( 2012 ) .",
    "rhee , c .- h .",
    ", glynn , p.  w. : unbiased estimation with square root convergence for sde models , operations research * 63 * , 10261043 ( 2015 ) .",
    "sadatsafavi , m. , bansback , n. , zafari , z. , najafzadeh , m. , marra , c. : need for speed : an efficient algorithm for calculation of single - parameter expected value of partial perfect information , value health * 16 * , 438448 ( 2013 ) .",
    "samson , d. , wirth , a. , rickard , j. : the value of information from multiple sources of uncertainty in decision analysis , eur .",
    "39 * , 254260 ( 1989 ) .",
    "sato , k. : value of information analysis for adequate monitoring of carbon dioxide storage in geological reservoirs under uncertainty , int .",
    "j. greenh .",
    "gas control * 5 * , 12941302 ( 2011 ) .",
    "strong , m. , oakley , j.  e. : an efficient method for computing single - parameter partial expected value of perfect information , med .",
    "decis . making * 33 * , 755766 ( 2013 ) .",
    "( from upper to lower ) by the naive nested monte carlo estimator ( left ) , the single term estimator ( middle ) , and the coupled sum estimator ( right ) with the total computational budgets @xmath218.,title=\"fig:\",scaledwidth=32.0% ]    ( from upper to lower ) by the naive nested monte carlo estimator ( left ) , the single term estimator ( middle ) , and the coupled sum estimator ( right ) with the total computational budgets @xmath218.,title=\"fig:\",scaledwidth=32.0% ]    ( from upper to lower ) by the naive nested monte carlo estimator ( left ) , the single term estimator ( middle ) , and the coupled sum estimator ( right ) with the total computational budgets @xmath218.,title=\"fig:\",scaledwidth=32.0% ] +    ( from upper to lower ) by the naive nested monte carlo estimator ( left ) , the single term estimator ( middle ) , and the coupled sum estimator ( right ) with the total computational budgets @xmath218.,title=\"fig:\",scaledwidth=32.0% ]    ( from upper to lower ) by the naive nested monte carlo estimator ( left ) , the single term estimator ( middle ) , and the coupled sum estimator ( right ) with the total computational budgets @xmath218.,title=\"fig:\",scaledwidth=32.0% ]    ( from upper to lower ) by the naive nested monte carlo estimator ( left ) , the single term estimator ( middle ) , and the coupled sum estimator ( right ) with the total computational budgets @xmath218.,title=\"fig:\",scaledwidth=32.0% ] +    ( from upper to lower ) by the naive nested monte carlo estimator ( left ) , the single term estimator ( middle ) , and the coupled sum estimator ( right ) with the total computational budgets @xmath218.,title=\"fig:\",scaledwidth=32.0% ]    ( from upper to lower ) by the naive nested monte carlo estimator ( left ) , the single term estimator ( middle ) , and the coupled sum estimator ( right ) with the total computational budgets @xmath218.,title=\"fig:\",scaledwidth=32.0% ]    ( from upper to lower ) by the naive nested monte carlo estimator ( left ) , the single term estimator ( middle ) , and the coupled sum estimator ( right ) with the total computational budgets @xmath218.,title=\"fig:\",scaledwidth=32.0% ] +    ( from upper to lower ) by the naive nested monte carlo estimator ( left ) , the single term estimator ( middle ) , and the coupled sum estimator ( right ) with the total computational budgets @xmath218.,title=\"fig:\",scaledwidth=32.0% ]    ( from upper to lower ) by the naive nested monte carlo estimator ( left ) , the single term estimator ( middle ) , and the coupled sum estimator ( right ) with the total computational budgets @xmath218.,title=\"fig:\",scaledwidth=32.0% ]    ( from upper to lower ) by the naive nested monte carlo estimator ( left ) , the single term estimator ( middle ) , and the coupled sum estimator ( right ) with the total computational budgets @xmath218.,title=\"fig:\",scaledwidth=32.0% ]"
  ],
  "abstract_text": [
    "<S> the expected value of partial perfect information ( evppi ) denotes the value of eliminating uncertainty on a subset of unknown parameters involved in a decision model . </S>",
    "<S> the evppi can be regarded as a decision - theoretic sensitivity index , and has been widely used for identifying relatively important unknown parameters . </S>",
    "<S> it follows from jensen s inequality , however , that the standard nested monte carlo computation of the evppi results in biased estimates . in this paper </S>",
    "<S> we introduce two unbiased monte carlo estimators for the evppi based on multilevel monte carlo method , introduced by heinrich ( 1998 ) and giles ( 2008 ) , and its extension by rhee and glynn ( 2012 , 2015 ) . </S>",
    "<S> our unbiased estimators are simple and straightforward to implement , and thus are of highly practical use . </S>",
    "<S> numerical experiments show that even the convergence behaviors of our unbiased estimators are superior to that of the standard nested monte carlo estimator .    </S>",
    "<S> _ keywords _ : value of information , expected value of partial perfect information , unbiased estimation , multilevel monte carlo </S>"
  ]
}