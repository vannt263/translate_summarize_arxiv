{
  "article_text": [
    "linear regression is perhaps the most widely used example of parameter fitting throughout the sciences . yet , the traditional ordinary least - squares ( or weighted least - squares ) approach to regression neglects some features that are practically ubiquitous in astrophysical data , namely the existence of measurement errors , often correlated with one another , on _ all _ quantities of interest , and the presence of residual , intrinsic scatter ( i.e.  physical scatter , not the result of measurement errors ) about the best fit . takes on this problem ( see that work for a more extensive overview of the prior literature ) by devising an efficient algorithm for simultaneously constraining the parameters of a linear model and the intrinsic scatter in the presence of such heteroscedastic and correlated measurement errors .",
    "in addition , the approach corrects a bias that exists when the underlying distribution of covariates in a regression is assumed to be uniform , by modeling this distribution as a flexible mixture of gaussian ( normal ) distributions and marginalizing over it .",
    "the model is considerably more complex , in terms of the number of free parameters , than traditional regression .",
    "nevertheless , it can be efficiently constrained using a fully conjugate gibbs sampler , as described in that work .",
    "briefly , the approach takes advantage of the fact that , for a suitable model , the fully conditional posterior of certain parameters ( or blocks of parameters ) may be expressible as a known distribution which can be sampled from directly using standard numerical techniques .",
    "if all model parameters can be sampled this way , then a gibbs sampler , which simply cycles through the list of parameters , updating or block - updating them in turn , can move efficiently through the parameter space . by repeatedly gibbs sampling ,",
    "a markov chain that converges to the joint posterior distribution of all model parameters is generated ( see , e.g. , @xcite for theoretical background ) .",
    "the individual pieces ( e.g. , the model distributions of measurement error , intrinsic scatter , and the covariate prior distribution ) of the model are conjugate , making it suitable for this type of efficient gibbs sampling .",
    "this is a key advantage in terms of making the resulting algorithm widely accessible to the community , since conjugate gibbs samplers , unlike more general and powerful markov chain monte carlo samplers , require no a priori tuning by the user .",
    "while argue against the assumption of a uniform prior for covariates , it should be noted that the alternative of a gaussian mixture model ( or the dirichlet process generalization introduced below ) is not necessarily applicable in every situation either .",
    "when a well motivated physical model of the distribution of covariates exists , it may well be preferable to use it , even at the expense of computational efficiency . in the general case",
    ", we can hope that a flexible parametrization like the gaussian mixture is adequate , although it is always worth checking a posteriori that the model distribution of covariates provides a good description of the data . and @xcite discuss real applications in which a gaussian distribution of covariates turns out to be adequate , despite the underlying physics being non - gaussian .",
    "this work describes two useful generalizations to the algorithm .",
    "first , the number of response variables is allowed to be greater than one .",
    "second , the prior distribution of covariates may be modeled using a dirichlet process rather than as a mixture of gaussians with a fixed number of components .",
    "a dirichlet process describes a probability distribution over the space of probability distributions , and ( in contrast to the many parameters required to specify a large mixing model ) is described only by a concentration parameter and a base distribution . for the choice of a gaussian base distribution , used here , the dirichlet process can be thought of as a gaussian mixture in which the number of mixture components is learned from the data and marginalized over as the fit progresses ( see more discussion , in a different astrophysical context , by @xcite ) .",
    "this makes it a very general and powerful alternative to the standard fixed - size gaussian mixture , as well as one that requires even less tuning by the user , since the number of mixture components need not be specified .",
    "crucially , both of these generalizations preserve the conjugacy of the model , so that posterior samples can still be easily obtained by gibbs sampling .    of course ,",
    "( or this paper ) does not provide the only implementation of conjugate gibbs sampling , nor is that approach the only one possible for linear regression in the bayesian context .",
    "indeed , there exist more general statistical packages capable of identifying conjugate sampling strategies ( where possible ) based on an abstract model definition ( e.g. , bugs , jags , and stan ) .",
    "the use of more general markov chain sampling techniques naturally allow for more general ( non - conjugate ) models and/or parametrizations ( e.g. , @xcite ) . nevertheless , there is something appealing in the relative simplicity of implementation and use of the conjugate gibbs approach , particularly as it applies so readily to the commonly used linear model with gaussian scatter .",
    "section  [ sec : model ] describes the model employed in this work in more detail , and introduces notation .",
    "section  [ sec : sampler ] outlines the changes to the sampling algorithm needed to accomodate the generalizations above .",
    "since this work is intended to extend that of , i confine this discussion only to steps which differ from the that algorithm , and do not review the gibbs sampling procedure in its entirety",
    ". however , the level of detail is intentionally high ; between this document and , it should be straightforward for the interested reader to create his or her own implementation of the entire algorithm .",
    "section  [ sec : examples ] provides some example analyses , including one with real astrophysical data , and discusses some practical aspects of the approach .",
    "the complete algorithm described here ( with both gaussian mixture and dirichlet process models ) has been implemented in the r language .",
    "the package is named linear regression by gibbs sampling ( lrgs ) , the better to sow confusion among extragalactic astronomers .",
    "the code can be obtained from github or the comprehensive r archive network .",
    "here i review the model described by , introducing the generalization to multiple response variables ( section  [ sec : mvmodel ] ) and the use of the dirichlet process to describe the prior distribution of the covariates ( section  [ sec : dproc ] ) .",
    "the notation used here is summarized in table  [ tab : notation ] ; it differs slightly from that of , as noted . in this document",
    ", @xmath0 denotes a stochastic relationship in which a random variable @xmath1 is drawn from the probability distribution @xmath2 , and boldface distinguishes vector- or matrix - valued variables .    [ cols=\"<,^,<,^\",options=\"header \" , ]     [ tab : toy ]         suppose we had a physical basis for a 3-component model ( or suspected 3 components , by inspection ) , but wanted to allow for the possibility of more or less structure than a strict gaussian mixture provides .",
    "the dirichlet process supplies a way to do this . for a given @xmath3 and @xmath4 ,",
    "the distribution of @xmath5 is known , , where @xmath6 is an unsigned stirling number of the first kind @xcite .",
    "] so in principle a prior expectation for the number of clusters , say @xmath7 , can be roughly translated into a gamma prior on @xmath3 . here",
    "i instead adopt an uninformative prior on @xmath3 @xcite , and compare the results to those of a gaussian mixture model with @xmath8 .    using the dirichlet process model , results from a chain of 1000 gibbs samples ( discarding the first 10 )",
    "are shown as shaded histograms in figure  [ fig : toydata ] .",
    "statistic is @xmath9 for every parameter .",
    "the autocorrelation length is also very short , @xmath10 steps for every parameter . ]",
    "the results are consistent with the input model values ( vertical , dashed lines ) for the parameters of interest ( @xmath11 , @xmath12 and @xmath13 ) .",
    "the latent parameters describing the base distribution of the dirichlet process are also consistent with the toy model , although they are poorly constrained .",
    "the right panel of figure  [ fig : toydata ] shows the cluster assignments for a sample with @xmath14 ( the median of the chain ) ; the clustered nature of the data is recognized , although the number of clusters tends to exceed the number of components in the input model .",
    ", we see that 2 of the 6 clusters are populated by single data points that are not outliers .",
    "the reverse is not true , and here it is interesting that the dirichlet process can not fit the data using fewer than @xmath8 clusters ( figure  [ fig : toyres ] ) . ]",
    "an equivalent analysis using a mixture of 3 gaussians rather than a dirichlet process model produces very similar constraints on the parameters of interest ( hatched histograms in figure  [ fig : toyres ] ) .",
    "+    +        as a real - life astrophysical example , i consider the scaling relations of dynamically relaxed galaxy clusters , using measurements presented by @xcite .",
    "note that there are a number of subtleties in the interpretation of these results that will be discussed elsewhere ; here the problem is considered only as an application of the method presented in this work .",
    "briefly , the data set comprises x - ray measurements of 40 massive , relaxed clusters .",
    "the x - ray observables are total mass , @xmath15 ; gas mass , @xmath16 ; average gas temperature , @xmath17 ; and luminosity , @xmath18 .",
    "in addition , spectroscopically measured redshifts are available for each cluster .",
    "a simple model of cluster formation by spherical collapse under gravity , neglecting gas physics , predicts self - similar power - law scaling relations among these quantities : to be measured in a soft x - ray band , in practice 0.12.4kev . since the emissivity in this band is weakly dependent on temperature for hot clusters such as those in the data set , the resulting scaling relation has a shallower dependence on mass than the more familiar bolometric luminosity  mass relation , @xmath19^{4/3}$ ] .",
    "the exponents in the @xmath18 scaling of equation  [ eq : selfsim ] are specific to the chosen energy band . ]",
    "@xmath20^{2/3 } , \\nonumber\\\\    l & \\propto & e(z)^{1.92}\\,m^{0.92 } , \\nonumber\\end{aligned}\\ ] ] where @xmath21 is the normalized hubble parameter at the cluster s redshift .",
    "the aim of this analysis is to test whether the power - law slopes above are accurate , and to characterize the joint intrinsic scatter of @xmath22 , @xmath17 and @xmath18 at fixed @xmath15 and @xmath23 .",
    "taking the logarithm of these physical quantities , and assuming log - normal measurement errors and intrinsic scatter , this becomes a linear regression with @xmath24 and @xmath25 . for brevity , and neglecting units , @xmath26 and @xmath27 ; i also approximately center the covariates for convenience .",
    "figure  [ fig : cldata ] shows summary plots of these data . although measurement errors are shown as orthogonal bars for clarity , the analysis will use a full @xmath28 covariance matrix accounting for the interdependence of the x - ray measurements ( this covariance is illustrated for one cluster in the figure ) .",
    "+      because the redshifts are measured with very small uncertainties , this problem is not well suited to the dirichlet process prior ; intuitively , the number of clusters in the dirichlet process must approach the number of data points because the data are strongly inconsistent with one another ( i.e.  are not exchangeable ) .",
    "instead , i use a gaussian mixture prior with @xmath8 , and verify that in practice the results are not sensitive to @xmath5 ( the parameters of interest differ negligibly from an analysis with @xmath29 ) .    marginalized 2-dimensional constraints on the power - law slopes of each scaling relation are shown in the top row of figure  [ fig : clres ] ( 68.3 and 95.4 per cent confidence ) . on inspection",
    ", only the luminosity scaling relation appears to be in any tension with the expectation in equation  [ eq : selfsim ] , having a preference for a weaker dependence on @xmath30 and a stronger dependence on @xmath15 .",
    "these conclusions are in good agreement with a variety of earlier work ( e.g.  @xcite ; see also the review of @xcite ) .",
    "the posterior distributions of the elements of the multi - dimensional intrinsic covariance matrix are shown in the bottom row of figure  [ fig : clres ] , after transforming to marginal scatter ( square root of the diagonal ) and correlation coefficients ( for the off - diagonal elements ) .",
    "the intrinsic scatters of @xmath22 and @xmath17 at fixed @xmath15 and @xmath23 are in good agreement with other measurements in the literature ( see @xcite , and references therein ) ; the scatter of @xmath18 is lower than the @xmath31 per cent typically found , likely because this analysis uses a special set of morphologically similar clusters rather than a more representative sample .",
    "the correlation coefficients are particularly challenging to measure , and the constraints are relatively poor .",
    "nevertheless , the ability to efficiently place constraints on the full intrinsic covariance matrix is an important feature of this analysis . within uncertainties ,",
    "these results agree well with the few previous contraints on these correlation coefficients in the literature @xcite .",
    "the best - fitting intrinsic covariance matrix is illustrated visually in figure  [ fig : clresid ] , which compares it to the residuals of @xmath32 with respect to the best - fitting values of @xmath33 and the best - fitting scaling relations .",
    "i have generalized the bayesian linear regression method described by to the case of multiple response variables , and included a dirichlet process model of the distribution of covariates ( equivalent to a gaussian mixture whose complexity is learned from the data ) .",
    "the algorithm described here is implemented independently of the linmix_err idl code of as an r package called lrgs , which is publicly available .",
    "two examples , respectively using a toy data set and real astrophysical data , are presented .",
    "a number of further generalizations are possible . in principle",
    ", significant complexity can be added to the model of the intrinsic scatter in the form of a gaussian mixture or dirichlet process model ( with a gaussian base distribution ) while maintaining conjugacy of the conditional posteriors , and thereby the efficiency of the gibbs sampler .",
    "the case censored data ( upper limits on some measured responses ) is discussed by .",
    "this situation , or , more generally , non - gaussian measurement errors , can be handled by rejection sampling ( at the expense of efficiency ) but is not yet implemented in lrgs .",
    "also of interest is the case of truncated data , where the selection of the data set depends on one of the response variables , and the data are consequently an incomplete and biased subset of a larger population .",
    "this case can in principle be handled by modeling the selection function and imputing the missing data @xcite .",
    "lrgs is shared publicly on github , and i hope that users who want more functionality will be interested in helping develop the code further .",
    "the addition of dirichlet process modeling to this work was inspired by extensive discussions with michael schneider and phil marshall .",
    "anja von der linden did some very helpful beta testing .",
    "i acknowledge support from the national science foundation under grant ast-1140019 ."
  ],
  "abstract_text": [
    "<S> ( * ? ? ? </S>",
    "<S> * hereafter ) described an efficient algorithm , using gibbs sampling , for performing linear regression in the fairly general case where non - zero measurement errors exist for both the covariates and response variables , where these measurements may be correlated ( for the same data point ) , where the response variable is affected by intrinsic scatter in addition to measurement error , and where the prior distribution of covariates is modeled by a flexible mixture of gaussians rather than assumed to be uniform . here </S>",
    "<S> i extend the algorithm in two ways . </S>",
    "<S> first , the procedure is generalized to the case of multiple response variables . </S>",
    "<S> second , i describe how to model the prior distribution of covariates using a dirichlet process , which can be thought of as a gaussian mixture where the number of mixture components is learned from the data . </S>",
    "<S> i present an example of multivariate regression using the extended algorithm , namely fitting scaling relations of the gas mass , temperature , and luminosity of dynamically relaxed galaxy clusters as a function of their mass and redshift . </S>",
    "<S> an implementation of the gibbs sampler in the r language , called lrgs , is provided .    </S>",
    "<S> methods : data analysis  x - rays : galaxies : clusters </S>"
  ]
}