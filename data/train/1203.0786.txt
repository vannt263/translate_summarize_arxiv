{
  "article_text": [
    "several years ago , i had the opportunity to give in several venues a keynote talk and to write an associated overview article on the general topic of `` algorithmic and statistical perspectives on large - scale data analysis ''  @xcite . by the _ algorithmic perspective",
    "i meant roughly the approach that someone trained in computer science might adopt ; and by the _ statistical perspective _",
    ", i meant roughly the approach that someone trained in statistics , or in some area such as scientific computing where strong domain - specific assumptions about the data are routinely made , might adopt .",
    "my main thesis was twofold .",
    "first , motivated by problems drawn from a wide range of application domains that share the common feature that they generate very large quantities of data , we are being forced to engineer a union between these two extremely different perspectives or worldviews on what the data are and what are interesting or fruitful ways to view the data .",
    "second , rather than _ first _ making statistical modeling decisions , independent of algorithmic considerations , and _ then _ applying a computational procedure as a black box  which is quite typical in small - scale and medium - scale applications and which is more natural if one adopts one perspective or the other  in many large - scale applications it will be more fruitful to understand and exploit what may be termed the statistical properties _ implicit _ in worst - case algorithms .",
    "i illustrated these claims with two examples from genetic and internet applications ; and i noted that this approach of more closely coupling the computational procedures used with a statistical understanding of the data seems particularly appropriate more generally for very large - scale data analysis problems .    here , i would like to revisit these questions , with an emphasis on describing in more detail particularly fruitful directions to consider in order to `` bridge the gap '' between the theory and practice of modern massive data set ( mmds ) analysis .",
    "on the one hand , very large - scale data are typically stored in some sort of database , either a variant of a traditional relational database or a filesystem associated with a supercomputer or a distributed cluster of relatively - inexpensive commodity machines . on the other hand ,",
    "it is often noted that , in large part because they are typically generated in automated and thus relatively - unstructured ways , data are becoming increasingly ubiquitous and cheap ; and also that the scarce resource complementary to large - scale data is the ability of the analyst to understand , analyze , and extract insight from those data .",
    "as anyone who has `` rolled up the sleeves '' and worked with real data can attest , real data are messy and noisy and poorly - structured in ways that can be hard to imagine before ( and even sometimes after ) one sees them .",
    "indeed , there is often quite a bit of very practical `` heavy lifting , '' _",
    "e.g. _ , cleaning and preparing the data , to be done before starting to work on the `` real '' problem  to such an extent that many would say that big data or massive data applications are basically those for which the preliminary heavy lifting _ is _ the main problem .",
    "this clearly places a premium on algorithmic methods that permit the analyst to `` play with '' the data and to work with the data interactively , as initial ideas are being tested and statistical hypotheses are being formed . unfortunately ,",
    "this is not the sort of thing that is easy to do with traditional databases .    to address these issues ,",
    "i will discuss a notion that lies at the heart of the disconnect between the algorithmic perspective and the statistical perspective on data and data analysis .",
    "this notion , often called _ regularization _ or _ statistical regularization _",
    ", is a traditional and very intuitive idea . described in more detail in section  [ sxn : thoughts : regularization ] , regularization basically has to do with how robust is the output of an algorithm to the noise properties of the input data .",
    "it is usually formulated as a tradeoff between `` solution quality '' ( as measured , _",
    "e.g. _ , by the value of the objective function being optimized ) and `` solution niceness '' ( as measured , _ e.g. _ , by a vector space norm constraint , a smoothness condition , or some other related measure of interest to a downstream analyst ) . for this reason ,",
    "when applied to noisy data , regularized objectives and regularized algorithms can lead to output that is `` better '' for downstream applications , _",
    "e.g. _ , for clustering or classification or other things of interest to the domain scientist , than is the output of the corresponding unregularized algorithms .",
    "thus , although it is nearly completely absent from computer science , which historically has taken the input data as given and modeled algorithms discretely , regularization in one form or another is central to nearly every application domain that applies algorithms to noisy data .",
    "i will also discuss how , by adopting a very non - traditional perspective on approximation algorithms ( or , equivalently , a non - traditional perspective on statistical regularization ) , one can in many cases satisfy the bicriteria of having algorithms that are scalable to very large data sets and that also have good statistical or inferential or predictive properties . basically , the non - traditional perspective is that approximate computation  either in the sense of approximation algorithms in theoretical computer science or in the sense of heuristic design decisions ( such as binning , pruning , and early stopping ) that practitioners must make in order to implement their algorithms in real systems  often _ implicitly _ leads to some sort of regularization .",
    "that is , approximate computation , _ in and of itself _ , can implicitly lead to statistical regularization .",
    "this is very different than the usual perspective in approximation algorithms , where one is interested in solving a given problem , but since the problem is intractable one `` settles for '' the output of an approximation .",
    "in particular , this means that , depending on the details of the situation , approximate computation can lead to algorithms that are both faster _ and _ better than are algorithms that solve the same problem exactly .",
    "while particular examples of this phenomenon are well - known , typically heuristically and amongst practitioners , in my experience the general observation is quite surprising to both practitioners and theorists of both the algorithmic perspective and the statistical perspective on  data .",
    "thus , i will use three `` case studies '' from recent mmds analysis to illustrate this phenomenon of _ implicit regularization via approximate computation _ in three somewhat different ways .",
    "the first involves computing an approximation to the leading nontrivial eigenvector of the laplacian matrix of a graph ; the second involves computing , with two very different approximation algorithms , an approximate solution to a popular version of the graph partitioning problem ; and the third involves computing an approximation to a locally - biased version of this graph partitioning problem . in each case",
    ", we will see that approximation algorithms that are run in practice implicitly compute smoother or more regular answers than do algorithms that solve the same problems exactly .    characterizing and exploiting the implicit regularization properties underlying approximation algorithms for large - scale data analysis problems is not the sort of analysis that is currently performed if one adopts a purely algorithmic perspective or a purely statistical perspective on the data .",
    "it is , however , clearly of interest in many mmds applications , where anything but scalable algorithms is out of the question , and where ignoring the noise properties of the data will likely lead to meaningless output . as such",
    ", it represents a challenging interdisciplinary research front , both for theoretical computer science  and for database theory in particular  as well as for theorists and practitioners of statistical data analysis more generally .",
    "before proceeding further , i would like to present in this section some general thoughts .",
    "most of these observations will be `` obvious '' to at least some readers , depending on their background or perspective , and most are an oversimplified version of a much richer story . nevertheless , putting them together and looking at the `` forest '' instead of the `` trees '' should help to set the stage for the subsequent discussion .",
    "it helps to remember that data are whatever data are  records of banking and other financial transactions , hyperspectral medical and astronomical images , measurements of electromagnetic signals in remote sensing applications , dna microarray and single - nucleotide polymorphism measurements , term - document data from the web , query and click logs at a search engine , interaction properties of users in social and information networks , corpora of images , sounds , videos , etc . to do something useful with the data",
    ", one must first model them ( either explicitly or implicitly - based objectives often has an interpretation in terms of underlying gaussian processes . thus",
    ", performing that computation in some sense implicitly amounts to assuming that that is what the data `` look like . '' ] ) in some way . at root ,",
    "a _ data model _ is a mathematical structure such that  given hardware , communication , input - output , data - generation , sparsity , noise , etc",
    ". considerations ",
    "one can perform computations of interest to yield useful insight on the data and processes generating the data . as such , choosing an appropriate data model has algorithmic , statistical , and implementational aspects that are typically intertwined in complicated ways .",
    "two criteria to keep in mind in choosing a data model are the following .    * first , on the _ data acquisition or data generation side _ , one would like a structure that is `` close enough '' to the data , _ e.g. _ , to the processes generating the data or to the noise properties of the data or to natural operations on the data or to the way the data are stored or accessed , that modeling the data with that structure does not do too much `` damage '' to the  data . *",
    "second , on the _ downstream or analysis side _",
    ", one would like a structure that is at a `` sweet spot '' between descriptive flexibility and algorithmic tractability .",
    "that is , it should be flexible enough that it can describe a range of types of data , but it should not be so flexible that it can do `` anything , '' in which case computations of interest will likely be intractable and inference will be problematic .    depending on the data and applications to be considered ,",
    "the data may be modeled in one or more of several ways .",
    "* _ flat tables and the relational model . _",
    "particularly common in database theory and practice , this model views the data as one or more two - dimensional arrays of data elements .",
    "all members of a given column are assumed to be similar values ; all members of a given row are assumed to be related to one another ; and different arrays can be related to one another in terms of predicate logic and set theory , which allows one to query , _",
    "e.g. _ , with sql or a variant , the data . * _ graphs , including special cases like trees and expanders .",
    "_ this model is particularly common in computer science theory and practice ; but it is also used in statistics and machine learning , as well as in scientific computation , where it is often viewed as a discretization of an underlying continuous problem .",
    "a graph @xmath0 consists of a set of vertices @xmath1 , that can represent some sort of `` entities , '' and a set of edges @xmath2 , that can be used to represent pairwise `` interactions '' between two entities .",
    "there is a natural geodesic distance between pairs of vertices , which permits the use of ideas from metric space theory to develop algorithms ; and from this perspective natural operations include breadth - first search and depth - first search .",
    "alternatively , in spectral graph theory , eigenvectors and eigenvalues of matrices associated with the graph are of interest ; and from this perspective , one can consider resistance - based or diffusion - based notions of distance between pairs of vertices . * _ matrices , including special cases like symmetric positive semidefinite matrices .",
    "_ an @xmath3 real - valued matrix @xmath4 provides a natural structure for encoding information about @xmath5 objects , each of which is described by @xmath6 features ; or , if @xmath7 , information about the correlations between all @xmath5 objects . as such",
    ", this model is ubiquitous in areas of applied mathematics such as scientific computing , statistics , and machine learning , and it is of increasing interest in theoretical computer science . rather than viewing a matrix simply as an @xmath3 array of numbers",
    ", one should think of it as representing a linear transformation between two euclidean spaces , @xmath8 and @xmath9 ; and thus vector space concepts like dot products , orthogonal matrices , eigenvectors , and eigenvalues are natural .",
    "in particular , matrices have a very different semantics than tables in the relational model , and euclidean spaces are much more structured objects than arbitrary metric spaces .",
    "of course , there are other ways to model data_e.g .",
    "_ , dna sequences are often fruitfully modeled by strings  but matrices and graphs are most relevant to our discussion  below .",
    "database researchers are probably most familiar with the basic flat table and the relational model and its various extensions ; and there are many well - known advantages to working with them . as a general rule , these models and their associated logical operations provide a powerful way to process the data at hand ; but they are much less well - suited for understanding and dealing with imprecision and the noise properties in that data .",
    "( see  @xcite and references therein . ) for example , historically , the focus in database theory and practice has been on business applications , _",
    "e.g. _ , automated banking , corporate record keeping , airline reservation systems , etc . , where requirements such as performance , correctness , maintainability , and reliability ( as opposed to prediction or inference ) are crucial .",
    "the reason for considering more sophisticated or richer data models is that much of the ever - increasing volume of data that is currently being generated is either relatively - unstructured or large and internally complex in its original form ; and many of these noisy unstructured data are better - described by ( typically sparse and poorly - structured ) graphs or matrices than as dense flat tables . while this may be obvious to some , the graphs and matrices that arise in mmds applications are very different than those arising in classical graph theory and traditional numerical linear algebra ; and thus modeling large - scale data by graphs and matrices poses very substantial challenges , given the way that databases ( in computer science ) have historically been constructed , the way that supercomputers ( in scientific computing ) have historically been designed , the tradeoffs that are typically made between faster cpu time and better io and network communication , etc .      before the advent of the digital computer , the natural sciences ( and to a lesser extent areas such as social and economic sciences ) provided a rich source of problems ; and statistical methods were developed in order to solve those problems . although these statistical methods typically involved computing something , there was less interest in questions about the nature of computation _ per se_. that is , although computation was often crucial , it was in some sense secondary to the motivating downstream application .",
    "indeed , an important notion was ( and still is ) that of a _ well - posed problem_roughly , a problem is well - posed if : a solution exists ; that solution is unique ; and that solution depends continuously on the input data in some reasonable topology .",
    "especially in numerical applications , such problems are sometimes called _ well - conditioned problems_. from this perspective , it simply does nt make much sense to consider algorithms for problems that are not well - posed ",
    "after all , any possible algorithm for such an ill - posed problem will return answers that are not be meaningful in terms of the domain from which the input data are drawn .    with the advent of the digital computer",
    ", there occurred a split in the yet - to - be - formed field of computer science .",
    "the split was loosely based on the application domain ( scientific computing and numerical computation versus business and consumer applications ) , but relatedly based on the type of tools used ( continuous mathematics like matrix analysis and probability versus discrete mathematics like combinatorics and logic ) ; and it led to two very different perspectives ( basically the statistical and algorithmic perspectives ) on the relationship between algorithms and data .    on the one hand , for many numerical problems that arose in applications of continuous mathematics , a two - step approach was used .",
    "it turned out that , even when working with a given well - conditioned problem , , a form of regularization . ] certain algorithms that solved that problem `` exactly '' in some idealized sense performed very poorly in the presence of `` noise '' introduced by the peculiarities of roundoff and truncation errors .",
    "roundoff errors have to do with representing real numbers with only finitely - many bits ; and truncation errors arise since only a finite number of iterations of an iterative algorithm can actually be performed .",
    "the latter are important even in `` exact arithmetic , '' since most problems of continuous mathematics can not even in principle be solved by a finite sequence of elementary operations ; and thus , from this perspective , fast algorithms are those that converge quickly to approximate answers that are accurate to , _ e.g _ , @xmath10 or @xmath11 or @xmath12 digits of  precision .",
    "this led to the notion of the _ numerical stability _ of an algorithm .",
    "let us view a numerical algorithm as a function @xmath13 attempting to map the input data @xmath14 to the `` true '' solution @xmath15 ; but due to roundoff and truncation errors , the output of the algorithm is actually some other @xmath16 . in this case , the _ forward error _ of the algorithm is @xmath17 ; and the _ backward error _ of the algorithm is the smallest @xmath18 such that @xmath19 .",
    "thus , the forward error tells us the difference between the exact or true answer and what was output by the algorithm ; and the backward error tells us what input data the algorithm we ran actually solved exactly .",
    "moreover , the forward error and backward error for an algorithm are related by the condition number of the problem  the magnitude of the forward error is bounded above by the condition number multiplied by the magnitude of the backward error . in general ,",
    "a backward stable algorithm can be expected to provide an accurate solution to a well - conditioned problem ; and much of the work in numerical analysis , continuous optimization , and scientific computing can be seen as an attempt to develop algorithms for well - posed problems that have better stability properties than the `` obvious '' unstable  algorithm .    on the other hand",
    ", it turned out to be much easier to study computation _ per se _ in discrete settings ( see  @xcite for a partial history ) , and in this case a simpler but coarser one - step approach prevailed .",
    "first , several seemingly - different approaches ( recursion theory , the @xmath20-calculus , and turing machines ) defined the same class of functions .",
    "this led to the belief that the concept of computability is formally captured in a qualitative and robust way by these three equivalent processes , independent of the input data ; and this highlighted the central role of logic in this approach to the study of computation .",
    "then , it turned out that the class of computable functions has a rich structure  while many problems are solvable by algorithms that run in low - degree polynomial time , some problems seemed not to be solvable by anything short of a trivial brute force algorithm .",
    "this led to the notion of the complexity classes p and np , the concepts of np - hardness and np - completeness , etc . ,",
    "the success of which led to the belief that the these classes formally capture in a qualitative and robust way the concept of computational tractability and intractability , independent of any posedness questions or any assumptions on the input data .",
    "then , it turned out that many problems of practical interest are intractable  either in the sense of being np - hard or np - complete or , of more recent interest , in the sense of requiring @xmath21 or @xmath22 time when only @xmath23 or @xmath24 time is available . in these cases , computing some sort of approximation is typically of interest .",
    "the modern theory of approximation algorithms , as formulated in theoretical computer science , provides forward error bounds for such problems for `` worst - case '' input .",
    "these bounds are worst - case in two senses : first , they hold uniformly for all possible input ; and second , they are typically stated in terms of a relatively - simple complexity measure such as problem size , independent of any other structural parameter of the input data . while there are several ways to prove worst - case bounds for approximation algorithms , a common procedure is to take advantage of relaxations_e.g .",
    "_ , solve a relaxed linear program , rather than an integer program formulation of the combinatorial problem  @xcite .",
    "this essentially involves `` filtering '' the input data through some other `` nicer , '' often convex , metric or geometric space .",
    "embedding theorems and duality then bound how much the input data are distorted by this filtering and provide worst - case quality - of - approximation guarantees  @xcite .",
    "the term _ regularization _ refers to a general class of methods  @xcite to ensure that the output of an algorithm is meaningful in some sense_e.g .",
    "_ , to the domain scientist who is interested in using that output for some downstream application of interest in the domain from which the data are drawn ; to someone who wants to avoid `` pathological '' solutions ; or to a machine learner interested in prediction accuracy or some other form of inference .",
    "it typically manifests itself by requiring that the output of an algorithm is not overly sensitive to the noise properties of the input data ; and , as a general rule , it provides a tradeoff between the quality and the niceness of the solution .",
    "regularization arose in integral equation theory where there was interest in providing meaningful solutions to ill - posed problems  @xcite .",
    "a common approach was to assume a smoothness condition on the solution or to require that the solution satisfy a vector space norm constraint .",
    "this approach is followed much more generally in modern statistical data analysis  @xcite , where the posedness question has to do with how meaningful it is to run a given algorithm , given the noise properties of the data , if the goal is to predict well on unseen data .",
    "one typically considers a loss function @xmath25 that specifies an `` empirical penalty '' depending on both the data and a parameter vector @xmath14 ; and a regularization function @xmath26 that provides `` geometric capacity control '' on the vector  @xmath14 .",
    "then , rather than minimizing @xmath25 exactly , one exactly solves an optimization problem of the  form : @xmath27 where the parameter",
    "@xmath20 intermediates between solution quality and solution niceness . implementing regularization explicitly in this manner leads to a natural interpretation in terms of a trade - off between optimizing the objective and avoiding over - fitting the data ; and it can often be given a bayesian statistical interpretation .",
    "encodes `` prior assumptions '' about the input data , and regularizing with this @xmath28 is the `` right '' thing to do  @xcite . ] by optimizing exactly a combination of two functions , though , regularizing in this way often leads to optimization problems that are harder ( think of @xmath29-regularized @xmath30-regression ) or at least no easier ( think of @xmath30-regularized @xmath30-regression ) than the original problem , a situation that is clearly unacceptable in many mmds  applications .    on the other hand ,",
    "regularization is often observed as a side - effect or by - product of other design decisions .",
    "for example , `` binning '' is often used to aggregate the data into bins , upon which computations are performed ; `` pruning '' is often used to remove sections of a decision tree that provide little classification power ; taking measures to improve numerical properties can also penalize large weights ( in the solution vector ) that exploit correlations beyond the level of precision in the data generation process ; and `` adding noise '' to the input data before running a training algorithm can be equivalent to tikhonov regularization .",
    "more generally , it is well - known amongst practitioners that certain heuristic approximations that are used to speed up computations can also have the empirical side - effect of performing smoothing or regularization . for example , working with a truncated singular value decomposition in latent factor models can lead to better precision and recall ; `` truncating '' to zero small entries or `` shrinking '' all entries of a solution vector is common in iterative algorithms ; and `` early stopping '' is often used when a learning model such as a neural network is trained by an iterative gradient descent algorithm .",
    "note that in addition to its use in making ill - posed problems well - posed  a distinction that is not of interest in the study of computation _ per se _ , where a sharp dividing line is drawn between algorithms and input data , thereby effectively assuming away the posedness problem  the use of regularization blurs the rigid lines between algorithms and input data in other ways .",
    "for example , in addition to simply modifying the objective function to be optimized , regularization can involve adding to it various smoothness constraints  some of which involve modifying the objective and then calling a black box algorithm , but some of which are more simply enforced by modifying the steps of the original algorithm . similarly , binning and pruning",
    "can be viewed as preprocessing the data , but they can also be implemented inside the algorithm ; and adding noise to the input before running a training algorithm is clearly a form of preprocessing , but empirically similar regularization effects are observed when randomization is included inside the algorithm , _",
    "e.g. _ , as with randomized algorithms for matrix problems such as low - rank matrix approximation and least - squares approximation  @xcite . finally , truncating small entries of a solution vector to zero in an iterative algorithm and performing early stopping in an iterative algorithm are clearly heuristic approximations that lead an algorithm to compute some sort of approximation to the solution that would have been computed had the truncation and early stopping not been performed .",
    "in this section , i will discuss three case studies that illustrate the phenomenon of implicit regularization via approximate computation in three somewhat different ways . for each of these problems ,",
    "there exists strong underlying theory ; and there exists the practice , which typically involves approximating the exact solution in one way or another",
    ". our goal will be to understand the differences between the theory and the practice in light of the discussion from section  [ sxn : thoughts ] .",
    "in particular , rather than being interested in the output of the approximation procedure insofar as it provides an approximation to the exact answer , we will be more interested in what the approximation algorithm actually computes , whether that approximation can be viewed as a smoother or more regular version of the exact answer , and how much more generally in database theory and practice similar ideas can be applied .",
    "the problem of computing eigenvectors of the laplacian matrix of a graph arises in many data analysis applications , including ( literally ) for web - scale data matrices .",
    "for example , the leading nontrivial eigenvector , _",
    "i.e. _ , the eigenvector , @xmath31 , associated with the smallest non - zero eigenvalue , @xmath32 , is often of interest : it defines the slowest mixing direction for the natural random walk on the graph , and thus it can be used in applications such as viral marketing , rumor spreading , and graph partitioning ; it can be used for classification and other common machine learning tasks ; and variants of it provide `` importance , '' `` betweenness , '' and `` ranking '' measures for the nodes in a graph . moreover ,",
    "computing this eigenvector is a problem for which there exists a very clean theoretical characterization of how approximate computation can implicitly lead to statistical regularization .",
    "let @xmath4 be the adjacency matrix of a connected , weighted , undirected graph @xmath0 , and let @xmath33 be its diagonal degree matrix . that is , @xmath34 is the weight of the edge between the @xmath35 node and the @xmath36 node , and @xmath37 .",
    "the _ combinatorial laplacian _ of @xmath38 is the matrix @xmath39 .",
    "although this matrix is defined for any graph , it has strong connections with the laplace - beltrami operator on riemannian manifolds in euclidean spaces .",
    "indeed , if the graph is a discretization of the manifold , then the former approaches the latter , under appropriate sampling and regularity assumptions .",
    "in addition , the _ normalized laplacian _ of  @xmath38 is @xmath40 .",
    "this degree - weighted laplacian is more appropriate for graphs with significant degree variability , in large part due to its connection with random walks and other diffusion - based processes . for an @xmath6 node graph",
    ", @xmath41 is an @xmath42 positive semidefinite matrix , _",
    "i.e. _ , all its eigenvalues @xmath43 are nonnegative , and for a connected graph , @xmath44 and @xmath45 . in this case , the degree - weighted all - ones vector , _",
    "i.e. _ , the vector whose @xmath35 element equals ( up to a possible normalization ) @xmath46 and which is often denoted @xmath47 , is an eigenvector of @xmath41 with eigenvalue zero , _",
    "i.e. _ , @xmath48 .",
    "for this reason , @xmath47 is often called trivial eigenvector of @xmath41 , and it is the next eigenvector that is of interest .",
    "this leading nontrivial eigenvector , @xmath31 , is that vector that optimizes the rayleigh quotient , defined to be @xmath49 for a unit - length vector @xmath14 , over all vectors perpendicular to the trivial eigenvector .",
    "can be related to generalized eigenvectors of @xmath50 : if @xmath51 , then @xmath52 , where @xmath53 . ]    in most applications where this leading nontrivial eigenvector is of interest , other vectors can also be used .",
    "for example , if @xmath32 is not unique then @xmath31 is not uniquely - defined and thus the problem of computing it is not even well - posed ; if @xmath54 is very close to @xmath32 , then any vector in the subspace spanned by @xmath31 and @xmath55 is nearly as good ( in the sense of forward error or objective function value ) as @xmath31 ; and , more generally , _ any _ vector can be used with a quality - of - approximation loss that depends on how far it s rayleigh quotient is from the rayleigh quotient of @xmath31 . for most small - scale and medium - scale applications ,",
    "this vector @xmath31 is computed `` exactly '' by calling a black - box solver .",
    ", that any numerical computation can be performed `` exactly . '' ]",
    "it could , however , be approximated with an iterative method such as the power method symmetric matrix @xmath4 and returns as output a number @xmath20 and a vector @xmath56 such that @xmath57 .",
    "it starts with an initial vector , @xmath58 , and it iteratively computes @xmath59 . under weak assumptions , it converges to @xmath60 , the dominant eigenvector of @xmath4 .",
    "the reason is clear : if we expand @xmath61 in the basis provided by the eigenfunctions @xmath62 of @xmath4 , then @xmath63 .",
    "vanilla versions of the power method can easily be improved ( at least when the entire matrix @xmath4 is available in ram ) to obtain better stability and convergence properties ; but these more sophisticated eigenvalue algorithms can often be viewed as variations of it .",
    "for instance , lanczos algorithms look at a subspace of vectors generated during the iteration . ] or by running a random walk - based or diffusion - based procedure ; and in many larger - scale applications this is  preferable .",
    "perhaps the most well - known example of this is the computation of the so - called pagerank of the web graph  @xcite . as an example of a spectral ranking method  @xcite , pagerank provides a ranking or measure of importance for a web page ; and the power method has been used extensively to perform very large - scale pagerank computations  @xcite .",
    "although it was initially surprising to many , the power method has several well - known advantages for such web - scale computations : it can be implemented with simple matrix - vector multiplications , thus not damaging the sparsity of the ( adjacency or laplacian ) matrix ; those matrix - vector multiplications are strongly parallel , permitting one to take advantage of parallel and distributed environments ( indeed , mapreduce was originally developed to perform related web - scale computations  @xcite ) ; and the algorithm is simple enough that it can be `` adjusted '' and `` tweaked '' as necessary , based on systems considerations and other design constraints . much more generally , other spectral ranking procedures compute vectors that can be used instead of the second eigenvector @xmath31 to perform ranking , classification , clustering , etc .  @xcite .    at root , these random walk or diffusion - based methods assign positive and/or negative `` charge '' ( or relatedly probability mass ) to the nodes , and then they let the distribution evolve according to dynamics derived from the graph structure .",
    "three canonical evolution dynamics are the following .",
    "* * heat kernel . * here , the charge evolves according to the heat equation @xmath64 .",
    "that is , the vector of charges evolves as @xmath65 , where @xmath66 is a time parameter , times an input seed distribution vector .",
    "* * pagerank .",
    "* here , the charge evolves by either moving to a neighbor of the current node or teleporting to a random node .",
    "that is , the vector of charges evolves as @xmath67 where @xmath68 is the natural random walk transition matrix associated with the graph and where @xmath69 is the so - called teleportation parameter , times an input seed vector .",
    "* * lazy random walk .",
    "* here , the charge either stays at the current node or moves to a neighbor .",
    "that is , if @xmath70 is the natural random walk transition matrix , then the vector of charges evolves as some power of @xmath71 , where @xmath72 represents the `` holding probability , '' times an input seed vector .",
    "in each of these cases , there is an input `` seed '' distribution vector , and there is a parameter ( @xmath73 , @xmath74 , and the number of steps of the lazy random walk ) that controls the `` aggressiveness '' of the dynamics and thus how quickly the diffusive process equilibrates . in many applications ,",
    "one chooses the initial seed distribution carefully , then this seed vector could have randomly positive entries or could be a vector with entries drawn from @xmath75 uniformly at random ; while if one is interested in local spectral graph partitioning  @xcite , as in section  [ sxn : local ] , then this vector could be the indicator vector of a small `` seed set '' of nodes . ] and/or prevents the diffusive process from equilibrating to the asymptotic state .",
    "( that is , if one runs any of these diffusive dynamics to a limiting value of the aggressiveness parameter , then under weak assumptions an exact answer is computed , independent of the initial seed vector ; but if one truncates this process early , then some sort of approximation , which in general depends strongly on the initial seed set , is computed . ) the justification for doing this is typically that it is too expensive or not possible to solve the problem exactly ; that the resulting approximate answer has good forward error bounds on it s rayleigh quotient ; and that , for many downstream applications , the resulting vector is even better ( typically in some sense that is not precisely described ) than the exact answer .    to formalize this last idea in the context of classical regularization theory ,",
    "let s ask what these approximation procedures actually compute . in particular , do these diffusion - based approximation methods exactly optimize a regularized objective of the form of problem  ( [ eqn : reg - gen ] ) , where @xmath28 is nontrivial , _",
    "e.g. _ , some well - recognizable function or at least something that is `` little - o '' of the length of the source code , and where @xmath76 is the rayleigh quotient ?    to answer this question , recall that @xmath31 exactly solves the following optimization problem .",
    "@xmath77 the solution to problem  ( [ eqn : mo - unreg - vp ] ) can also be characterized as the solution to the following sdp ( semidefinite program ) .",
    "@xmath78 where @xmath79 stands for the matrix trace operation .",
    "problem  ( [ eqn : mo - unreg - sdp ] ) is a relaxation of problem  ( [ eqn : mo - unreg - vp ] ) from an optimization over unit vectors to an optimization over distributions over unit vectors , represented by the density matrix @xmath80 .",
    "these two programs are equivalent , however , in that the solution to problem  ( [ eqn : mo - unreg - sdp ] ) , call it @xmath81 , is a rank - one matrix , where the vector into which that matrix decomposes , call it @xmath82 , is the solution to problem  ( [ eqn : mo - unreg - vp ] ) ; and vice versa .    viewing @xmath31 as the solution to an sdp makes it easier to address the question of what is the objective that approximation algorithms for problem  ( [ eqn : mo - unreg - vp ] ) are solving exactly . in particular , it can be shown that these three diffusion - based dynamics arise as solutions to the following regularized  sdp .",
    "@xmath83 where @xmath84 is a regularization function , which is the generalized entropy , the log - determinant , and a certain matrix-@xmath85-norm , respectively  @xcite ; and where @xmath86 is a parameter related to the aggressiveness of the diffusive process  @xcite .",
    "conversely , solutions to the regularized sdp of problem  ( [ eqn : mo - reg - sdp ] ) for appropriate values of @xmath86 can be computed _ exactly _ by running one of the above three diffusion - based approximation algorithms .",
    "intuitively , @xmath84 is acting as a penalty function , in a manner analogous to the @xmath30 or @xmath29 penalty in ridge regression or lasso regression , respectively ; and by running one of these three dynamics one is _ implicitly _ making assumptions about the functional form of @xmath84 .",
    "more formally , this result provides a very clean theoretical characterization of how each of these three approximation algorithms for computing an approximation to the leading nontrivial eigenvector of a graph laplacian can be seen as exactly optimizing a regularized version of the same problem .",
    "graph partitioning refers to a family of objective functions and associated approximation algorithms that involve cutting or partitioning the nodes of a graph into two sets with the goal that the cut has good quality ( _ i.e. _ , not much edge weight crosses the cut ) as well as good balance ( _ i.e. _ , each of the two sets has a lot of the node weight).-balanced cut problem , and quotient cut formulations . in this article",
    ", i will be interested in conductance , which is a quotient cut formulation , but variants of most of what i say will hold for the other formulations . ] as such , it has been studied from a wide range of perspectives and in a wide range of applications . for example , it has been studied for years in scientific computation ( where one is interested in load balancing in parallel computing applications ) , machine learning and computer vision ( where one is interested in segmenting images and clustering data ) , and theoretical computer science ( where one is interested in it as a primitive in divide - and - conquer algorithms ) ; and more recently it has been studied in the analysis of large social and information networks ( where one is interested in finding `` communities '' that are meaningful in a domain - specific context or in certifying that no such communities exist ) .",
    "given an undirected , possibly weighted , graph @xmath0 , the _ conductance @xmath87 of a set of nodes @xmath88 _ is : @xmath89 where @xmath90 denotes the set of edges having one end in @xmath91 and one end in the complement @xmath92 ; where @xmath93 denotes cardinality ( or weight ) ; where @xmath94 ; and where @xmath4 is the adjacency matrix of a graph .",
    "of a set of nodes @xmath95 _ is @xmath96 , the conductance is simply a degree - weighted version of the expansion . ] in this case , the _ conductance of the graph @xmath38 _ is : @xmath97    although exactly solving the combinatorial problem  ( [ eqn : conductance_graph ] ) is intractable , there are a wide range of heuristics and approximation algorithms , the respective strengths and weaknesses of which are well - understood in theory and/or practice , for approximately optimizing conductance . of particular interest here",
    "spectral methods _ and _ flow - based methods_.    spectral algorithms compute an approximation to problem  ( [ eqn : conductance_graph ] ) by solving problem  ( [ eqn : mo - unreg - vp ] ) , either exactly or approximately , and then performing a `` sweep cut '' over the resulting vector .",
    "several things are worth noting .",
    "* first , problem  ( [ eqn : mo - unreg - vp ] ) is a relaxation of problem  ( [ eqn : conductance_graph ] ) , as can be seen by replacing the @xmath98 constraint in the corresponding integer program with the constraint @xmath99 subject to @xmath100 , _",
    "i.e. _ , by satisfying the combinatorial constraint `` on average '' .",
    "* second , this relaxation effectively embeds the data on the one - dimensional ) ; and this is of interest since a complete graph is like a constant - degree expander  namely , a metric space that is `` most unlike '' low - dimensional euclidean spaces such as one - dimensional lines  in terms of its cut structure  @xcite .",
    "this provides tighter duality results , and the reason for this connection is that the identity on the space perpendicular to the degree - weighted all - ones vector is the laplacian matrix of a complete graph  @xcite . ]",
    "span of @xmath31although , since the distortion is minimized only on average , there may be some pairs of points that are distorted a lot .",
    "* third , one can prove that the resulting partition is `` quadratically good , '' in the sense that the cut returned by the algorithm has conductance value no bigger than @xmath101 if the graph actually contains a cut with conductance @xmath102  @xcite .",
    "this bound comes from a discrete version of cheeger s inequality , which was originally proved in a continuous setting for compact riemannian manifolds ; and it is parameterized in terms of a structural parameter of the input , but it is independent of the number @xmath6 of nodes in the graph . * finally , note that the worst - case quadratic approximation factor is _ not _ an artifact of the analysis ",
    "it is obtained for spectral methods on graphs with `` long stringy '' pieces  @xcite , basically since spectral methods confuse `` long paths '' with `` deep cuts''and that it is a very `` local '' property , in that it is a consequence of the connections with diffusion and thus it is seen in locally - biased versions of the spectral method  @xcite .",
    "flow - based algorithms compute an approximation to problem  ( [ eqn : conductance_graph ] ) by solving an all - pairs multicommodity flow problem .",
    "several things are worth noting .    * first , this multicommodity flow problem is a relaxation of problem  ( [ eqn : conductance_graph ] ) , as can be seen by replacing the @xmath98 constraint ( which provides a particular semi - metric ) in the corresponding integer program with a general semi - metric constraint . *",
    "second , this procedure effectively embeds the data into an @xmath29 metric space , _ i.e. _ , a real vector space @xmath8 , where distances are measured with the @xmath29 norm . *",
    "third , one can prove that the resulting partition is within an @xmath103 factor of optimal , in the sense that the cut returned by the algorithm has conductance no bigger than @xmath103 , where @xmath6 is the number of nodes in the graph , times the conductance value of the optimal conductance set in the graph  @xcite .",
    "this bound comes from bourgain s result which states that any @xmath6-point metric space can be embedded into euclidean space with only logarithmic distortion , a result which clearly depends on the number @xmath6 of nodes in the graph but which is independent of any structural parameters of the  graph . * finally ,",
    "note that the worst - case @xmath103 approximation factor is _ not _ an artifact of the analysis ",
    "it is obtained for flow - based methods on constant - degree expander graphs  @xcite  and that it is a very `` global '' property , in that it is a consequence of the fact that for constant - degree expanders the average distance between all pairs of nodes is  @xmath103 .",
    "thus , spectral methods and flow - based methods are complementary in that they relax the combinatorial problem of optimizing conductance in very different ways ; they succeed and fail for complementary input ( _ e.g. _ , flow - based methods do not confuse `` long paths '' with `` deep cuts , '' and spectral methods do not have problems with constant - degree expanders ) ; and they come with quality - of - approximation guarantees that are structurally very different .",
    "worst - case approximation guarantees , etc.although they would note that spectral methods are better for expanders , basically since the quadratic of a constant is a constant . on the other hand , nearly everyone outside of computer science would say spectral methods do pretty well for the data in which they are interested , and they would wonder why anyone would be interested in partitioning a graph without any good  partitions . ] for these and other reasons , spectral and flow - based approximation algorithms for the intractable graph partitioning problem provide a good `` hydrogen atom '' for understanding more generally the disconnect between the algorithmic and statistical perspectives on data .    providing a precise statement of how spectral and flow - based approximation algorithms",
    "implicitly compute regularized solutions to the intractable graph partitioning problem ( in a manner , _",
    "e.g. _ , analogous to how truncated diffusion - based procedures for approximating the leading nontrivial eigenvector of a graph laplacian exactly solve a regularized version of the problem ) has _ not _ , to my knowledge , been accomplished .",
    "nevertheless , this theoretical evidence_i.e .",
    "_ , that spectral and flow - based methods are effectively `` filtering '' the input data through very different metric and geometric places  suggests that this phenomenon exists .    to observe this phenomenon empirically , one should work with a class of data that highlights the peculiar features of spectral and flow - based methods , _",
    "e.g. _ , that has properties similar to graphs that `` saturate '' spectral s and flow s worst - case approximation guarantees .",
    "empirical evidence  @xcite clearly demonstrates that large social and information networks have these properties  they are strongly expander - like when viewed at large size scales ; their sparsity and noise properties are such that they have structures analogous to stringy pieces that are cut off or regularized away by spectral methods ; and they often have structural regions that at least locally are meaningfully low - dimensional .",
    "thus , this class of data provides a good `` hydrogen atom '' for understanding more generally the regularization properties implicit in graph approximation algorithms .    in light of this , let s say that we are interested in finding reasonably good clusters of size @xmath104 or @xmath105 nodes in a large social or information network .",
    "( see  @xcite for why this might be interesting . ) in that case , figure  [ compactness - vs - cuts - fig ] presents very typical results .",
    "figure  [ compactness - vs - cuts - fig : obj ] presents a scatter plot of the size - resolved conductance of clusters found with a flow - based approximation algorithm ( in red ) and a spectral - based approximation algorithm ( in blue ) . or @xmath105 nodes ( but  @xcite provides details on this ) ; and do nt worry about the details of the flow - based and spectral - based procedures , except to say that there is a nontrivial theory - practice gap ( again ,  @xcite provides details ) . ] in this plot , lower values on the y - axis correspond to better values of the objective function ; and thus the flow - based procedure is unambiguously better than the spectral procedure at finding good - conductance clusters . on the other hand ,",
    "how useful these clusters are for downstream applications is also of interest .",
    "since we are not explicitly performing any regularization , we do not have any explicit `` niceness '' function , but we can examine empirical niceness properties of the clusters found by the two approximation procedures .",
    "figures  [ compactness - vs - cuts - fig : nice1 ] and  [ compactness - vs - cuts - fig : nice2 ] presents these results for two different niceness measures . here , lower values on the y - axis correspond to `` nicer '' clusters , and again we are interested in clusters with lower y - axis values .",
    "thus , in many cases , the spectral procedure is clearly better than the flow - based procedure at finding nice clusters with reasonably good conductance values .",
    "formalizations aside , this empirical tradeoff between solution quality and solution niceness is basically the defining feature of statistical regularization  except that we are observing it here as a function of two different approximation algorithms for the same intractable combinatorial objective function . that is , although we have not explicitly put any regularization term anywhere , the fact that these two different approximation algorithms essentially filter the data through different metric and geometric spaces leaves easily - observed empirical artifacts on the output of those approximation algorithms .",
    "one possible response to these empirical results is is to say that conductance is not the `` right '' objective function and that we should come up with some other objective to formalize our intuition ; but of course that other objective function will likely be intractable , and thus we will have to approximate it with a different spectral - based or flow - based ( or some other ) procedure , in which case the same implicit regularization issues will arise  @xcite .      in many applications",
    ", one would like to identify locally - biased graph partitions , _",
    "i.e. _ , clusters in a data graph that are `` near '' a prespecified set of nodes . for example , in nearly every reasonably large social or information network , there do not exist large good - conductance clusters , but there are often smaller clusters that are meaningful to the domain scientist  @xcite ; in other cases , one might have domain knowledge about certain nodes , and one might want to use that to find locally - biased clusters in a semi - supervised manner  @xcite ; while in other cases , one might want to perform algorithmic primitives such as solving linear equations in time that is nearly linear in the size of the graph  @xcite .",
    "one general approach to problems of this sort is to modify the usual objective function and then show that the solution to the modified problem inherits some or all of the nice properties of the original objective .",
    "for example , a natural way to formalize the idea of a locally - biased version of the leading nontrivial eigenvector of @xmath41 that can then be used in a locally - biased version of the graph partitioning problem is to modify problem  ( [ eqn : mo - unreg - vp ] ) with a locality constraint as follows .",
    "@xmath106 where @xmath107 is a vector representing the `` seed set , '' and where @xmath108 is a locality parameter .",
    "this _ locally - biased _ version of the usual spectral graph partitioning problem was introduced in  @xcite , where it was shown that solution inherits many of the nice properties of the solution to the usual global spectral partitioning problem . in particular , the exact solution can be found relatively - quickly by running a so - called personalized pagerank computation ; if one performs a sweep cut on this solution vector in order to obtain a locally - biased partition , then one obtains cheeger - like quality - of - approximation guarantees on the resulting cluster ; and if the seed set consists of a single node , then this is a relaxation of the following _ locally - biased graph partitioning problem _ : given as input a graph @xmath0 , an input node @xmath109 , and a positive integer @xmath110 , find a set of nodes @xmath95 achieving @xmath111 _ i.e. _ , find the best conductance set of nodes of volume no greater than @xmath110 that contains the input node  @xmath56  @xcite .",
    "this `` optimization - based approach '' has the advantage that it is explicitly solving a well - defined objective function , and as such it is useful in many small - scale to medium - scale applications  @xcite . but this approach has the disadvantage , at least for web - scale graphs , that the computation of the locally - biased eigenvector `` touches '' all of the nodes in the graph  and this is very expensive , especially when one wants to find small clusters .",
    "an alternative more `` operational approach '' is to do the following : run some sort of procedure , the steps of which are similar to the steps of an algorithm that would solve the problem exactly ; and then either use the output of that procedure in a downstream application in a manner similar to how the exact answer would have been used , or prove a theorem about that output that is similar to what can be proved for the exact answer . as an example of this approach , @xcite",
    "take as input some seed nodes and a locality parameter and then run a diffusion - based procedure to return as output a `` good '' cluster that is `` nearby '' the seed nodes . in each of these cases ,",
    "the procedure is similar to the usual procedure , : @xcite performs truncated random walks ; @xcite approximates personalized pagerank vectors ; and  @xcite runs a modified heat kernel procedure . ] except that at each step of the algorithm various `` small '' quantities are truncated to zero ( or simply maintained at zero ) , thereby minimizing the number of nodes that need to be touched at each step of the algorithm .",
    "for example ,  @xcite sets to zero very small probabilities , and @xcite uses the so - called _ push algorithm _  @xcite to concentrate computational effort on that part of the vector where most of the nonnegligible changes will take  place .    the outputs of these _ strongly local spectral methods _ obtain cheeger - like quality - of - approximation guarantees , and by design these procedures are extremely fast  the running time depends on the size of the output and is independent even of the number of nodes in the graph .",
    "thus , an advantage of this approach is that it opens up the possibility of performing more sophisticated eigenvector - based analytics on web - scale data matrices ; and these methods have already proven crucial in characterizing the clustering and community structure of social and information networks with up to millions of nodes  @xcite . at present , though , this approach has the disadvantage that it is very difficult to use : the exact statement of the theoretical results is extremely complicated , thereby limiting its interpretability ; it is extremely difficult to characterize and interpret for downstream applications what actually is being computed by these procedures , _",
    "i.e. _ , it is not clear what optimization problem these approximation algorithms are solving exactly ; and counterintuitive things like a seed node not being part of `` its own cluster '' can easily happen . at root",
    ", the reason for these difficulties is that the truncation and zeroing - out steps implicitly regularize  but they are done based on computational considerations , and it is not known what are the implicit statistical side - effects of these design decisions .    the precise relationship between these two approaches has not , to my knowledge , been characterized .",
    "informally , though , the truncating - to - zero provides a `` bias '' that is analogous to the early - stopping of iterative methods , such as those described in section  [ sxn : eigenvector ] , and that has strong structural similarities with thresholding and truncation methods , as commonly used in @xmath29-regularization methods and optimization more generally  @xcite .",
    "for example , the update step of the push algorithm , as used in  @xcite , is a form of stochastic gradient descent  @xcite , a method particularly well - suited for large - scale environments due to its connections with regularization and boosting  @xcite ; and the algorithm terminates after a small number of iterations when a truncated residual vector equals zero  @xcite , in a manner similar to other truncated gradient methods  @xcite .",
    "perhaps more immediately - relevant to database theory and practice as well as to implementing these ideas in large - scale statistical data analysis applications is simply to note that this operational and interactive approach to database algorithms is _ already _ being adopted in practice .",
    "for example , in addition to empirical work that uses these methods to characterize the clustering and community structure of large networks  @xcite , the body of work that uses diffusion - based primitives in database environments includes an algorithm to estimate pagerank on graph streams  @xcite , the approximation of pagerank on large - scale dynamically - evolving social networks  @xcite , and a mapreduce algorithm for the approximation of personalized pagerank vectors of all the nodes in a graph  @xcite .",
    "before concluding , i would like to share a few more general thoughts on approximation algorithm theory , in light of the above discussion . as a precursor",
    ", i should point out the obvious fact that the modern theory of np - completeness is an extremely useful theory .",
    "it is a theory , and so it is an imperfect guide to practice ; but it is a useful theory in the sense that it provides a qualitative notion of fast computation , a robust guide as to when algorithms will or will not perform well , etc . the theory achieved this by considering computation _ per se _ , as a one - step process that divorced the computation from the input and the output except insofar as the computation depended on relatively - simple complexity measures like the size of the input .",
    "thus , the success is due to the empirical fact that many natural problems of interest are solvable in low - degree polynomial time , that the tractability status of many of the `` hardest '' problems in np is in some sense equivalent , and that neither of these facts depends on the input data or the posedness of the problem .",
    "i think it is also fair to say that , at least in a very wide range of mmds applications , the modern theory of approximation algorithms is nowhere near as analogously useful .",
    "the bounds the theory provides are often very weak ; the theory often does nt provide constants which are of interest in practice ; the dependence of the bounds on various parameters is often not even qualitatively right ; and in general it does nt provide analogously qualitative insight as to when approximation algorithms will and will not be useful in practice for realistic noisy data .",
    "one can speculate on the reasons  technically , the combinatorial gadgets used to establish approximability and nonapproximability results might not be sufficiently robust to the noise properties of the input data ; many embedding methods , and thus their associated bounds , tend to emphasize the properties of `` far apart '' data points , while in most data applications `` nearby '' information is more reliable and more useful for downstream analysis ; the geometry associated with matrices and spectral graph theory is much more structured than the geometry associated with general metric spaces ; structural parameters like conductance and the isoperimetric constant are robust and meaningful and not brittle combinatorial constructions that encode pathologies ; and ignoring posedness questions and viewing the analysis of approximate computation as a one - step process might simply be too coarse .",
    "the approach i have described involves going `` beyond worst - case analysis '' to addressing questions that lie at the heart of the disconnect between what i have called the algorithmic perspective and the statistical perspective on large - scale data analysis . at the heart of this disconnect",
    "is the concept of regularization , a notion that is almost entirely absent from computer science , but which is central to nearly every application domain that applies algorithms to noisy data .",
    "both theoretical and empirical evidence demonstrates that approximate computation , in and of itself , can implicitly lead to statistical regularization , in the sense that approximate computation  either approximation algorithms in theoretical computer science or heuristic design decisions that practitioners must make in order to implement their algorithms in real systems  often implicitly leads to some sort of regularization .",
    "this suggests treating statistical modeling questions and computational considerations on a more equal footing , rather than viewing either one as very much secondary to the  other .",
    "the benefit of this perspective for database theory and the theory and practice of large - scale data analysis is that one can hope to achieve bicriteria of having algorithms that are scalable to very large - scale data sets and that also have well - understood inferential or predictive properties .",
    "of course , this is not a panacea  some problems are simply hard ; some data are simply too noisy ; and running an approximation algorithm may implicitly be making assumptions that are manifestly violated by the data .",
    "all that being said , understanding and exploiting in a more principled manner the statistical properties that are implicit in scalable worst - case algorithms should be of interest in many very practical mmds applications .",
    "r.  andersen , f.r.k .",
    "chung , and k.  lang . local graph partitioning using pagerank vectors . in _",
    "focs 06 : proceedings of the 47th annual ieee symposium on foundations of computer science _ , pages 475486 , 2006 .",
    "j.  leskovec , k.j .",
    "lang , a.  dasgupta , and m.w .",
    "statistical properties of community structure in large social and information networks . in _ www 08 : proceedings of the 17th international conference on world wide web _ , pages 695704 , 2008 .",
    "j.  leskovec , k.j .",
    "lang , and m.w .",
    "empirical comparison of algorithms for network community detection . in _ www 10 : proceedings of the 19th international conference on world wide web _ , pages 631640 , 2010 .",
    "m.  w. mahoney .",
    "algorithmic and statistical perspectives on large - scale data analysis . in u.",
    "naumann and o.  schenk , editors , _ combinatorial scientific computing _ , chapman & hall / crc computational science .",
    "crc press , 2012 .",
    "m.  w. mahoney and l.  orecchia . implementing regularization implicitly via approximate eigenvector computation . in _ proceedings of the 28th international conference on machine learning _ , pages 121128 , 2011 .",
    "m.  w. mahoney , l.  orecchia , and n.  k. vishnoi .",
    "a local spectral method for graphs : with applications to improving graph partitions and exploring data graphs locally . technical report .",
    "preprint : arxiv:0912.0681 ( 2009 ) .",
    "p.  o. perry and m.  w. mahoney .",
    "regularized laplacian estimation and fast eigenvector approximation . in _ annual advances in neural information processing systems 25 :",
    "proceedings of the 2011 conference _ , 2011 .",
    "spielman and s .- h .",
    "nearly - linear time algorithms for graph partitioning , graph sparsification , and solving linear systems . in _",
    "stoc 04 : proceedings of the 36th annual acm symposium on theory of computing _ , pages 8190 , 2004 ."
  ],
  "abstract_text": [
    "<S> database theory and database practice are typically the domain of computer scientists who adopt what may be termed an algorithmic perspective on their data . </S>",
    "<S> this perspective is very different than the more statistical perspective adopted by statisticians , scientific computers , machine learners , and other who work on what may be broadly termed statistical data analysis . in this article </S>",
    "<S> , i will address fundamental aspects of this algorithmic - statistical disconnect , with an eye to bridging the gap between these two very different approaches . </S>",
    "<S> a concept that lies at the heart of this disconnect is that of statistical regularization , a notion that has to do with how robust is the output of an algorithm to the noise properties of the input data . </S>",
    "<S> although it is nearly completely absent from computer science , which historically has taken the input data as given and modeled algorithms discretely , regularization in one form or another is central to nearly every application domain that applies algorithms to noisy data . by using several case studies , </S>",
    "<S> i will illustrate , both theoretically and empirically , the nonobvious fact that approximate computation , in and of itself , can implicitly lead to statistical regularization . </S>",
    "<S> this and other recent work suggests that , by exploiting in a more principled way the statistical properties implicit in worst - case algorithms , one can in many cases satisfy the bicriteria of having algorithms that are scalable to very large - scale databases and that also have good inferential or predictive  properties . </S>"
  ]
}