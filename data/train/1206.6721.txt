{
  "article_text": [
    "consider @xmath1 independent observations @xmath2 , where @xmath3 is a random response variable , and @xmath4 is a fixed @xmath5-dimensional vector of co - variables , @xmath6 . in a high - dimensional model ,",
    "the number of co - variables @xmath5 is much larger than the number of observations @xmath1 .",
    "there has been much literature on the linear model for this situation . in that case",
    ", one assumes that @xmath7 where @xmath8 is an unknown vector of coefficients , and @xmath9 are independent noise variables .",
    "the lasso estimator ( @xcite ) is @xmath10 the parameter @xmath11 is a regularization parameter , and @xmath12 is the @xmath0-norm of @xmath13 . for the case of orthogonal design ,",
    "that is , the case where the columns of the @xmath14 design matrix @xmath15 are orthogonal , the lasso estimator is the soft - thresholding estimator ( @xcite ) .",
    "we study in this paper the extension of the theoretical results for the lasso estimator , to the case of generalized linear models .    the theory for the lasso with least squares loss is well established .",
    "we refer to @xcite , @xcite , @xcite , @xcite , @xcite , @xcite .",
    "see also @xcite and the references therein .",
    "the main results concern oracle inequalities for the prediction error @xmath16 and variable selection properties of the lasso .",
    "oracle results say that the prediction error of the lasso estimator is up to log - factors as good as that of an oracle that uses the least squares `` estimator '' with only the co - variables in the unknown active set @xmath17 .",
    "variable selection results roughly state that with large probability , the estimated active set @xmath18 is with large probability equal to the true active set  @xmath19 .",
    "both results depend on appropriate conditions : for prediction one assumes restricted eigenvalue condition ( @xcite ; @xcite ; @xcite ) or compatibility conditions ( @xcite ) , and for variable selection , one assumes the neighborhood stability ( @xcite ) or equivalent irrepresentable condition ( @xcite ) .",
    "clearly , variable selection is a harder problem than prediction , so that one expects conditions for the former to be stronger than those for the latter . indeed , @xcite show that the irrepresentable condition implies the compatibility condition .",
    "concerning work on oracle inequalities for general loss , an earlier paper which uses @xmath0-regularization in this context is @xcite . here",
    ", the case of orthogonal design is considered ( thus , it has @xmath20 ) .",
    "the technique of proof is , however , very much along the lines of the later proofs for nonorthogonal design ( with possibly @xmath21 ) , as developed by @xcite and others .",
    "some remarks on the proof technique can be found in @xcite , highlighting that with an @xmath0-penalty one can derive oracle inequalities with rates faster than @xmath22 , despite the fact that the penalty - term @xmath23 itself is generally of larger order than @xmath24 .",
    "the case of quantile regression was studied in @xcite , again only for the case of orthonormal design . in @xcite , hinge loss with @xmath0-penalty is studied . here",
    "the design is not assumed to be orthogonal , and is in fact random .",
    "this paper does not use restricted eigenvalue or compatibility conditions , but rather a weighted eigenvalue condition .",
    "it shows that the @xmath0-penalty leads to estimators which are both adaptive to the `` smoothness '' or `` complexity '' of the underlying regression function , as well as to the `` margin behavior '' of the problem .",
    "the margin behavior expresses the amount of curvature of the theoretical risk near its minimum .",
    "the paper @xcite considers the density estimation problem . in @xcite , results are derived for generalized linear models with @xmath0-penalty and @xmath25 possibly larger than @xmath1 , assuming the compatibility condition .",
    "it covers the case of quadratic loss and of general lipschitz loss , and it allows for random design .",
    "similar results are in @xcite , although there the compatibility condition is replaced by one somewhat in the spirit conditions in @xcite . in @xcite , one can find further details concerning sparsity oracle inequalities for high - dimensional generalized linear models .",
    "there is a large body of literature extending the oracle results for the linear model to matrix versions .",
    "it is beyond the scope of this paper to review this work , and we only point to the generalization to robust loss , as given in @xcite .    within this volume , the paper @xcite gives a general account of oracle results for high - dimensional m - estimators .",
    "after our theo - rem  [ oracle-quasi-likelihood.theorem ] ,  we  briefly discuss its relation with @xcite .",
    "concerning variable selection , the fact that the irrepresentable condition is rather strong has led to considering modifications of the lasso , such as two step procedures , and the scad introduced by @xcite ; see , for example , @xcite for the case of quantile regression .",
    "our paper focuses only on the theoretical aspects .",
    "there is much literature on applications of the lasso in generalized linear models ; see @xcite , for example .",
    "the computational aspects are well - studied : see @xcite .",
    "the paper @xcite contains , apart from theory , software descriptions and a real data example for the case of huber loss . in @xcite , @xmath0-regularization with least absolute deviations loss is studied and compared numerically with the least squares lasso .",
    "we present new results for prediction and variable selection for the case of quasi - likelihood estimation .",
    "the findings for prediction are along the lines as those in @xcite , but this time completed with the compatibility condition .",
    "the paper details and extends the findings in @xcite .",
    "we also show that a weighted form of the irrepresentable condition implies consistent variable selection .",
    "we model the dependence of the distribution of @xmath26 on @xmath4 via a linear function @xmath27 , where @xmath28 is a vector of unknown coefficients .",
    "the problem is to estimate @xmath28 or the linear predictor vector @xmath29 , where @xmath30 .",
    "we study a high - dimensional situation , where the number of variables @xmath5 can be much larger than the sample size  @xmath1 .",
    "( for technical reasons , we assume that @xmath5 is at least  @xmath31 . )",
    "the vector @xmath28 is assumed to be sparse ; that is , its number of nonzero coefficients is assumed to be small .",
    "see section  [ sparsity.section ] for more details on sparsity .",
    "we consider two models .",
    "the first one is a generalized linear model , with a given inverse link function @xmath32 , that is , @xmath33 with @xmath8 a vector of unknown coefficients .",
    "the quasi-(log)likelihood function is @xmath34 where @xmath35 is a given variance function ; see also @xcite . together , quasi - likelihood and link function define quasi - likelihood loss , as follows :    [ quasi-likelihood.definition ] the quasi - likelihood loss function is @xmath36    in our second model , the dependence of the distribution of @xmath26 on @xmath4 may be described through quantiles or other aspects of the distribution . in particular",
    ", one can define this dependence via a loss function @xmath37 , and @xmath38 the generalized linear model assumes that @xmath39 for some @xmath8 .",
    "the robust case is the one where , for all @xmath40 , the loss function @xmath41 is lipschitz in @xmath42 , with lipschitz constant not depending on @xmath43 . without loss of generality",
    "one can then assume the lipschitz constant to be equal to one .",
    "this leads to the following definition :    [ robust.definition ] the loss function @xmath44 is _ robust _ if for all @xmath45 , @xmath46    quasi - likelihood loss is sometimes robust , but there are also many examples where it is not . moreover , there are many ( robust ) loss functions which do not correspond to minus quasi - likelihoods .",
    "see section  [ examples.section ] for some examples .    to handle the large @xmath5 situation",
    ", one needs a regularized estimation method .",
    "let us write a linear function with coefficients @xmath13 as @xmath47 in what follows , we sometimes , with some abuse of notation , let @xmath48 be the @xmath1-dimensional vector @xmath49 as well .",
    "the @xmath0-norm of a vector @xmath50 is @xmath51 we examine the @xmath0-penalized estimator @xmath52 of @xmath28 , defined as @xmath53 here , @xmath11 is a tuning parameter .",
    "large values correspond to more regularization , which means more shrinkage of the estimator @xmath52 .",
    "the expression @xmath54 is called the empirical risk ( at @xmath13 ) .",
    "for least squares loss ( i.e. , @xmath55 ) , the empirical risk is the usual sum of squares ( normalized by @xmath56 ) .",
    "the above estimator is then called the lasso estimator ( @xcite ) .",
    "we will study loss functions @xmath44 that are either minus quasi - likelihoods or robust ( or both ) .",
    "the normalized euclidean norm on @xmath57 is @xmath58 we will establish bounds for the `` prediction error '' @xmath59 , the @xmath0-error @xmath60 , and ( for the case of quasi - likelihood loss ) present sufficient conditions for variable selection using @xmath52 .",
    "we require throughout this paper , both for quasi - likelihood loss as well as for robust loss , that the map @xmath61 is convex for all @xmath45 .",
    "this assumption is important from a computational point of view .",
    "it also plays a crucial role in our theory , as it allows us to prove that the estimator @xmath52 is in an @xmath62-neighborhood of @xmath28 .",
    "this in turn will be invoked to establish sup - norm bounds for @xmath63 .",
    "the indices of the set of nonzero coefficients of @xmath28 is called the ( true ) active set .",
    "it is denoted by @xmath64 its cardinality @xmath65 is called the sparsity index of  @xmath28 .",
    "it is assumed that @xmath66 is relatively small , at least smaller than @xmath67 in order of magnitude ; see ( [ s0 ] ) , ( [ s02 ] ) , ( [ s03 ] ) , ( [ s04 ] ) and ( [ s05 ] ) .",
    "the vector @xmath28 is sparse if @xmath66 is small .",
    "more generally , one can call a vector @xmath28 sparse if it can in some sense be approximated by a vector with only a few nonzero entries . to avoid too many digressions , we will not elaborate on this issue , but only present a brief outline after the formulation of the main oracle result ; see remark  [ oracle-quasi-likelihood.remark ] .      as @xmath28 is unknown , its active set @xmath19 and its sparsity index @xmath66 are unknown as well",
    ". we will show in theorems  [ oracle-quasi-likelihood.theorem ] and  [ oracle-robust.theorem ] that the prediction error of the @xmath0-penalized estimator @xmath52 is , up to a @xmath68-term , the same as that of minimizer of the empirical risk without penalty , but with all coefficients not in @xmath19 restricted to be zero .",
    "the latter is not an estimator , as it depends on the unknown @xmath19 .",
    "it is often referred to as the oracle .",
    "we moreover show that a version of the irrepresentable condition , appropriate for quasi - likelihood loss , is sufficient for variable selection ; see theorem  [ selection-quasi-likelihood.theorem ] .",
    "all our results are stated in a nonasymptotic form , but to facilitate the interpretation , we also give asymptotic formulations .",
    "the next section provides some examples of quasi - likelihood and robust loss .",
    "section  [ compatibility.section ] gives the definition of the so - called _ compatibility constant _ , which will occur in the oracle results .",
    "section  [ oraclequasi-likelihood.section ] gives oracle inequalities for the prediction and @xmath0-error for quasi - likelihood loss , and section [ oracle-robust.section ] does the same for robust loss . in section  [ selection-quasi-likelihood.section ]",
    "we address the variable selection problem in the quasi - likelihood context .",
    "similar arguments can be used in the robust context , but this is omitted here .",
    "section  [ random-design.section ] briefly discusses the case of random design , and section  [ discussion ] concludes .",
    "the proofs are in the supplemental article @xcite .",
    "lemmas a.2 and a.7 there are based on a concentration inequality ( see @xcite ) and a contraction inequality ; see @xcite .",
    "these lemmas use only fourth moment assumptions , and are perhaps of interest in themselves .",
    "the least squares criterion has @xmath69 .",
    "it corresponds to a quasi - likelihood loss with variance function @xmath70 for all @xmath71 .",
    "the link function is then the identity , which is the canonical link function for this case .",
    "the loss function is convex , but not robust .      when the response @xmath26 is binary , say @xmath72 , @xmath73 , we have @xmath74 in logistic regression , one takes the quasi - likelihood with variance function @xmath75 , @xmath76 , and the canonical link function @xmath77 that is , @xmath78 hence , in this case , @xmath79 because @xmath80 , one sees that this leads to a robust loss function , that is , @xmath81 is lipschitz in @xmath42 for all @xmath45 .",
    "we acknowledge that logistic regression is not robust in the sense of having a bounded influence function ( but we will in fact assume in condition  [ cona1 ] that the covariables are bounded ) .",
    "as in all cases of quasi - likelihood with canonical link function , the loss also convex .",
    "consider binary response @xmath82 as in section  [ logistic.section ] , but now with more general inverse link function @xmath32 .",
    "@xmath83 if @xmath84 $ ] is a strictly increasing symmetric distribution function , then quasi - likelihood loss is convex .",
    "this is because the hazard @xmath85 ( @xmath86 being the derivate of @xmath32 ) is a decreasing function of  @xmath87 .",
    "when the hazard is uniformly bounded , quasi - likelihood loss is also robust .      if the dependence of the distribution of @xmath88 on @xmath4 is via its @xmath89-quantile ( @xmath90 ) , we take as loss function @xmath91 where @xmath92 this is clearly a robust loss function , but it does not correspond to a quasi - likelihood .",
    "let @xmath93 be an index set with cardinality  @xmath94 .",
    "we define for all @xmath50 , @xmath95    below , we present for constants @xmath96 the compatibility constant @xmath97 introduced in @xcite . for normalized design ( i.e. , @xmath98 for all  @xmath99 , where @xmath100 denotes the @xmath99th column of @xmath101 )",
    ", one can view @xmath102 as an @xmath0-version of the canonical correlation between the linear space spanned by the variables in @xmath103 on the one hand , and the linear space of the variables in @xmath104 on the other hand . instead of all linear combinations with normalized @xmath105-norm",
    ", we now consider all linear combinations with normalized @xmath0-norm of the coefficients . for a geometric interpretation",
    ", we refer to @xcite .",
    "the _ compatibility constant _ is @xmath106    the compatibility constant is closely related to ( and never smaller than ) the restricted eigenvalue as defined in @xcite , which is @xmath107    the calculation of the compatibility constant is a nonlinear eigenvalue problem [ see , e.g. , @xcite for computational aspects of nonlinear eigenvalues ] .",
    "lower bounds that hold with high probability follow , for example , if @xmath101 is an i.i.d .",
    "sample from a @xmath5-dimensional vector with nongenerate covariance matrix ; see section  [ random-design.section ] for some details . see also @xcite , and see @xcite for a discussion of the relation between restricted eigenvalues and compatibility .    for oracle results ,",
    "we need @xmath108 to be strictly positive for some @xmath109 ( depending on the tuning parameter @xmath110 ) . in this paper",
    ", we take @xmath111 for definiteness , and we require throughout that @xmath112 ( except when we consider sparse approximations of the truth ; see remark [ oracle-quasi-likelihood.remark ] ) . if @xmath113 , one sees that some conditions [ e.g. , condition ( [ s0 ] ) ] become impossible .",
    "as we will see , all bounds in this paper involve not so much the sparsity index @xmath66 itself , but rather the _ effective sparsity _",
    "@xmath114    as a simple numerical example , let us suppose @xmath115 , @xmath116 , @xmath117 and @xmath118 thus , the sparsity index is @xmath119 .",
    "one can easily verify that there is no @xmath50 with @xmath120 and @xmath121 .",
    "thus , the compatibility constant @xmath122 is strictly positive .",
    "in fact , @xmath123 is equal to the distance of @xmath124 to line that connects @xmath125 and @xmath126 , that is @xmath127 .",
    "the effective sparsity is @xmath128 .",
    "alternatively , when @xmath129 then @xmath130 .",
    "this is due to the sharper angle between @xmath131 and @xmath132 .",
    "to appreciate the results we will present for the general case , it may be useful to first reconsider the standard linear model and least squares loss .",
    "let @xmath133 and suppose @xmath134 let @xmath52 be the lasso estimator @xmath135 let @xmath136 denote the @xmath99th column of the design matrix  @xmath137 .",
    "if the errors @xmath138 are independent with mean zero and the design is normalized ( i.e. , for all @xmath99 ) one can prove that uniformly in @xmath99 , the `` correlations '' @xmath139 are small in absolute value , generally as small as @xmath140 .",
    "the regularization parameter @xmath110 is to be chosen in such a way that it `` overrules '' these correlations .",
    "indeed , this allows one to prove the following result [ see @xcite , theorem  6.1 ] by rather elementary means ( recall the notation @xmath141 ) :    [ leastsquares ] suppose that @xmath142 then @xmath143    this result says that if the effective sparsity@xmath144 is of the same order as the sparsity index @xmath145 ( i.e. , if the compatibility constant stays away from zero ) , then for a large class of error distributions the lasso estimator with @xmath146 is up to constants and a @xmath147-factor as good as as the oracle least squares `` estimator '' which knows the active set @xmath19 .",
    "the performance of @xmath52 is here measured in terms of its prediction error of an independent copy @xmath148 of @xmath149 is rather @xmath150 , where @xmath151 .",
    "we however do not include the additional variance @xmath152 in our definition .",
    "] @xmath153 .",
    "theorem  [ leastsquares ] moreover says that the @xmath0 error converges with rate @xmath154 .",
    "looking ahead at more general loss functions , ideas are based on quadratic approximations , which are generally only valid in a neighborhood of @xmath28 .",
    "this is why in our work , we will assume that @xmath154 is small , say @xmath155 , where @xmath156 is a sufficiently small constant . with @xmath157 , and a compatibility",
    "constant staying away from zero , it means we assume the sparsity index @xmath66 to be sufficiently smaller than @xmath158 .      as in the situation of the standard linear model and least squares loss , we will study the error @xmath159 and the @xmath0-error .",
    "for prediction , one will be interested in estimating the mean @xmath160 of the response variable @xmath149 .",
    "our conditions  [ cona3 ] and  [ cona4 ] below will ensure that @xmath32 has a bounded derivative on an appropriate domain .",
    "this means that bounds for @xmath161 immediately lead to similar bounds for @xmath162 . with some abuse of terminology",
    ", we refer to @xmath163 as the prediction error.=1    the theoretical properties of the @xmath0-penalized quasi - likelihood estimator @xmath52 depend on the tail - behavior of the error @xmath164 we will need at least finite second moments of the errors . for definiteness , we assume the errors have finite fourth moments . with higher order moments ,",
    "the confidence level in the oracle result of theorem  [ oracle-quasi-likelihood.theorem ] will be larger , and when the errors have sup - exponential tails , one can derive exponential probability inequalities for prediction error and @xmath0-error .",
    "[ cona ] there exist constants @xmath165 and @xmath166 such that @xmath167 and @xmath168    the next conditions ,  [ cona1][cona4 ] , allow us to use quadratic approximations in a neighborhood of @xmath28 .",
    "we assume throughout that the inverse link function @xmath32 is increasing and that its derivative @xmath169 exists .",
    "we further define @xmath170\\\\[-8pt ] b(\\mu , \\mu_0 ) & : = & \\int_{\\mu_0}^{\\mu } \\frac { u- \\mu_0}{v(u ) } \\ , du , \\quad   \\mu\\in\\mathcal{y } , \\nonumber\\end{aligned}\\ ] ] where @xmath171 is an arbitrary but fixed constant .",
    "we let @xmath172 that is , @xmath173 .",
    "note that @xmath174 is ( up to an additive constant ) the canonical link function .",
    "when @xmath175 , we get @xmath176 for all @xmath42 .",
    "the term @xmath177 in the quasi - likelihood @xmath178 containing the response @xmath43 is then linear in @xmath42 . in a sense",
    ", @xmath179 measures the departure from linearity of this term .",
    "we let @xmath180 the quantity @xmath181 is the `` regret '' for choosing the expectation @xmath182 instead of the `` true '' @xmath183 .",
    "[ cona1 ] there exists a constant @xmath184 such that @xmath185    we remark that condition  [ cona1 ] serves as normalization of the design , albeit not in terms of the @xmath186 norm but rather in supremum norm . as our results will be presented in nonasymptotic form , it is in principle possible to see the effect when , say , @xmath184 grows with @xmath5 and/or @xmath1 .",
    "[ cona2 ] there exists a constant @xmath187 such that @xmath188    [ cona3 ] with @xmath184 and @xmath187 given in conditions  [ cona1 ] and  [ cona2 ] respectively , there exists a positive constant @xmath189 such that for all @xmath190 , @xmath191    [ cona4 ] with @xmath184 and @xmath187 given in conditions [ cona1 ] and  [ cona2 ] respectively , there exists a constant  @xmath192 , such that for all @xmath193 , @xmath194    [ examples.remark ] there is an interplay between conditions  [ cona ] ,  [ cona1 ] and  [ cona2 ] .",
    "for example , for quadratic loss , we do not need  [ cona1 ] and  [ cona2 ] when the errors are ( sub)gaussian",
    ". conditions  [ cona1 ] and  [ cona2 ] are imposed so that we need conditions  [ cona3 ] and  [ cona4 ] only in the neighborhood @xmath195 . as for condition  [ cona3 ] ,",
    "when @xmath32 is the inverse of the canonical link function @xmath174 , it holds with @xmath196 , as @xmath179 is then the identity . for quadratic loss , and logistic loss for example ( which have canonical link function )",
    ", condition  [ cona4 ] holds as well .",
    "we actually will only need the lower bound for @xmath197 in this section , and the upper bound will come into play in section  [ selection-quasi-likelihood.section ] .    to organize the constants appearing in our results , let use the short hand notation",
    "@xmath198 thus , up to constants @xmath199 is the effective sparsity .",
    "as in the case of least squares loss , we assume the regularization parameter @xmath110 to be of order at least @xmath200 .",
    "the larger @xmath110 , the larger the confidence level of our bounds will be ( in theorem [ leastsquares ] this the probability of @xmath201 ) but then these bounds themselves are also larger .",
    "we introduce a variable @xmath202 to describe this effect , and define @xmath203 if we choose the tuning parameter @xmath110 at least as large as @xmath204 , the confidence level will be at least @xmath205 , where @xmath206 + 3 \\kappa^4 /(n \\sigma^4).\\ ] ] the variable @xmath207 is in principle arbitrary , but it is , however , not allowed to be arbitrarily large . as we can only apply the quadratic approximations in a neighborhood of @xmath28",
    "we will need to show that @xmath52 is with large probability in such a neighborhood .",
    "for that reason , we can not let the tuning parameter @xmath110 to be arbitrarily large ( as a large @xmath110 will give slow rates ) ; see condition ( [ lambda.equation ] ) in theorem [ oracle-quasi-likelihood.theorem ] below .",
    "a  reasonable choice for @xmath207 is for example @xmath208 , in which case @xmath209 .",
    "[ oracle-quasi-likelihood.theorem ]",
    "let @xmath210 be the @xmath0-penalized quasi - likelihood estimator .",
    "assume conditions [ cona ] and [ cona1][cona4 ] .",
    "suppose that @xmath211 take @xmath212 with probability at least @xmath205 , it holds that @xmath213 and @xmath214    [ wainwright ] our result in theorem  [ oracle-quasi-likelihood.theorem ] is comparable to corollary 3 in @xcite , albeit that we do not assume bounded responses or canonical link function , and our compatibility condition is weaker than their assumed restricted eigenvalue condition .",
    "on the other hand , we require ( [ s0 ] ) , and only give bounds for the @xmath0-error and prediction error , not for the @xmath105-error .",
    "[ constants ] we have presented the result in a nonasymptotic form , but did not try to optimize the constants .    [ interpretation.remark ] thus , up to the compatibility constant , and taking @xmath110 of order @xmath200 , the prediction error is of order @xmath215 .",
    "@xmath216 an oracle that knows @xmath19 and does empirical risk minimization without penalty but with the restriction that all coefficients not in @xmath19 are set to zero , has a prediction error of order @xmath217 .",
    "we see that for not knowing  @xmath19 , one pays a price of order @xmath218 .",
    "we moreover have @xmath219    [ oracle-quasi-likelihood.remark ] we have presented the above oracle inequality involving the sparsity of the true @xmath28 .",
    "if the truth is not sparse , or if actually the generalized linear model is misspecified , one may replace the truth by a sparse linear approximation of the truth , and the oracle inequality involves a trade - off between the approximation error on the one hand , and the sparsity and compatibility constant on the other .",
    "this trade - off is of the following form .",
    "let for an arbitrary index set @xmath220 , @xmath221 where @xmath222 is the average regret @xmath223 thus , @xmath224 is the best approximation of @xmath225 using only the variables in @xmath103 .",
    "then under some regularity conditions the prediction error of @xmath226 is with probability @xmath227 bounded by @xmath228 the `` const . ''",
    "depends on the constants occurring in the regularity conditions , the constant @xmath229 depends moreover on the choice of @xmath110 , and the confidence level @xmath89 depends on all these . for more details on this extension ,",
    "we refer to @xcite and the references therein .",
    "[ s0.remark ] condition ( [ s0 ] ) assumes that the sparsity index @xmath66 is sufficiently smaller than@xmath230 , a condition we already announced in section [ leastsquares.section ] .",
    "this assumption plays its part in all our results : it will also be important for variable selection and simplifies the derivation of results for the case of random design .",
    "in the case of least squares loss , the assumption can be avoided , even in some cases with random design .",
    "it should , however , be noted that a large @xmath66 means a slow rate . in particular ,",
    "when the sparsity is of larger order than @xmath230 , the bound for the prediction error is of larger order than @xmath200 , and this can not be improved up to the @xmath68-term .",
    "thus , then the bounds are actually quite large in order of magnitude . indeed , recall that the prediction error is @xmath231 , which is the squared distance between @xmath63 and @xmath232 .",
    "assumption  ( [ s0 ] ) allows us to conclude that @xmath233 , and hence , that @xmath234 for all @xmath235 .",
    "the latter was used because we only want to require conditions  [ cona3 ] and  [ cona4 ] for bounded values of the argument @xmath42 .",
    "when dealing with least squares loss , conditions  [ cona3 ] and  [ cona4 ] hold for all @xmath236 .",
    "this means that with least squares loss , assumption ( [ s0 ] ) can be dropped in theorem [ oracle-quasi-likelihood.theorem ] ; see theorem  [ leastsquares ] .",
    "[ sigma.remark ] the lower bound in ( [ lambda.equation ] ) for the tuning parameter @xmath110 depends on the noise level @xmath237 as well as other unknown constants . in practice ,",
    "one may for instance apply cross - validation .",
    "the noise level @xmath237 can also be treated as additional parameter which can be estimated along with @xmath28 .",
    "see @xcite for a discussion .",
    "in this section , we assume throughout that @xmath44 is robust loss ; see definition  [ robust.definition ] .",
    "we define for @xmath238 , @xmath239 and assume that @xmath240 exists .",
    "[ conb ] for @xmath184 and @xmath187 given in conditions  [ cona1 ] and  [ cona2 ] respectively , we have for some constant @xmath241 and for all @xmath235 , @xmath242    the least absolute deviations loss is @xmath243 .",
    "let @xmath244 be distribution function of @xmath26 given @xmath4 ( @xmath245 ) .",
    "then @xmath246 is the median of @xmath244 , and condition  [ conb ] requires that @xmath244 has a strictly positive density @xmath247 on @xmath248 for all @xmath235 .",
    "we now define @xmath249 .\\ ] ] fix some @xmath202 and define @xmath250 the following theorem is a reformulation of results  in @xcite , @xcite or @xcite .",
    "[ oracle-robust.theorem ] let @xmath52 be the @xmath0-penalized robust estimator .",
    "assume conditions  [ cona1 ] ,  [ cona2 ] and [ conb ] .",
    "suppose that @xmath251 take @xmath252 with probability at least @xmath205 , where @xmath253 $ ] , it holds that @xmath254 and @xmath255    [ oracle-robust.remark ] similar remarks can be made as for the @xmath0-penalized quasi - likelihood estimator .",
    "the new element in the result is that with robustness the tuning parameter @xmath110 does not depend on some noise level @xmath237 .",
    "note that the bounds for the @xmath0-error @xmath60 , given in theorems  [ oracle-quasi-likelihood.theorem ] and  [ oracle-robust.theorem ] , can be invoked to show that , with large probability , the @xmath0-regularized estimator will detect most of the nonzero coefficients @xmath256 which are large enough : for all @xmath257 , @xmath258 in other words , if a large proportion of the nonzero coefficients is sufficiently far above the noise level in absolute value , then there will also be many true positives . by this argument ,",
    "if all nonzero coefficients of @xmath28 are of larger order than @xmath259 , we will have @xmath260 , where @xmath261    this section will study the false positives .",
    "we show that for the case of quasi - likelihood loss , an irrepresentable condition similar to meinshausen and bhlmann ( @xcite ) and @xcite implies that there are no false positives , that is , that @xmath262 .",
    "such result can also be obtained for robust loss , but is omitted here .      again , as preparation , let us first consider the standard linear model and the least squares lasso estimator , @xmath263 let @xmath264 be the design matrix consisting of the variables in @xmath103 , and let @xmath265 in @xcite ( exercise 7.5 ) or @xcite , one can find the following result .    [",
    "teo7.1 ] suppose that @xmath266 where @xmath267 .",
    "assume moreover the irrepresentable condition @xmath268 then @xmath262 .",
    "we remark that an irrepresentable condition ( see also below in definition  [ irrepresentable.definition ] ) is always rather strong . however , for exact variable selection , an irrepresentable condition is essentially necessary , as shown in @xcite , @xcite , @xcite . by thresholding the estimated coefficients and refitting , or by applying the adaptive lasso , one can often improve on variable selection and yet maintain a good prediction and estimation error .",
    "the conditions for the latter are much less restrictive than the irrepresentable condition .",
    "we refer to @xcite for details .",
    "the results are based on he karush ",
    "kuhn  tucker ( or kkt- ) conditions ; see @xcite . in our context , they read as follows :    we have @xmath269 here @xmath270 , and moreover @xmath271    let @xmath272 where @xmath273 thus , @xmath274 is the weighted gram matrix @xmath275 we write @xmath276 , so that @xmath277 .",
    "let @xmath278 be the weighted design matrix consisting of the variables in @xmath103 , and @xmath279    [ irrepresentable.definition ] let @xmath280 be given .",
    "we say that the @xmath281-irrepresentable condition is met for the set @xmath103 if @xmath282    here is how the @xmath281-irrepresentable condition can be linked with variable selection .",
    "[ select.theorem ] let @xmath283 .",
    "suppose that @xmath284 where @xmath285 , and @xmath286 , @xmath287 .",
    "suppose moreover the @xmath281-irrepresentable condition is met for @xmath19 , with @xmath288",
    ". then @xmath262 .    in the proof of theorem  [ selection-quasi-likelihood.theorem ] below ,",
    "we show that equation ( [ kkt2 ] ) in theorem  [ select.theorem ] holds for some @xmath289 satisfying the conditions of this theorem .",
    "this allows us then to conclude that @xmath262 .    as one sees in the kkt conditions , the derivative at @xmath52 of the loss function occurs .",
    "we will need to compare this by the derivative at @xmath28 . to bring this to an end",
    "we need , in addition to conditions  [ cona3 ] and  [ cona4 ] , certain lipschitz conditions on @xmath290 and @xmath86 .",
    "[ cona5 ] for @xmath184 and @xmath187 , given in conditions  [ cona1 ] and [ cona2 ] respectively , we have for all @xmath291 , and some constant @xmath292 , @xmath293    [ cona6 ] for @xmath184 and @xmath187 given in conditions  [ cona1 ] and [ cona2 ] respectively , we have for all @xmath291 , and some constant @xmath294 , @xmath295    @xmath296 under the additional conditions  [ cona5 ] and  [ cona6 ] , one can improve the constants in theorem  [ oracle-quasi-likelihood.theorem ] .",
    "it is also clear that conditions  [ cona5 ] and  [ cona6 ] hold for least squares and logistic loss .    with these new constants ,",
    "we define @xmath297 we moreover let @xmath298 and @xmath299 fix some @xmath202 , and define @xmath300 and @xmath301 define @xmath302 + 9 \\kappa^4 /(n \\sigma^4).\\ ] ] thus , up to constants , @xmath303 and @xmath304 are the effective sparsity . moreover , for @xmath208 ( say ) , @xmath305 and @xmath209 .",
    "we arrive at the main result of this section .",
    "[ selection-quasi-likelihood.theorem ] let @xmath52 be the @xmath0-penalized quasi - likelihood estimator .",
    "assume conditions [ cona ] and [ cona1][cona6 ] .",
    "assume that ( [ s0 ] ) holds , that is , @xmath306 where @xmath307 is given by @xmath308 assume now that @xmath309 as well as @xmath310    assume furthermore the @xmath281-irrepresentable condition with @xmath311    with probability at least @xmath312 , it holds that .",
    "[ var.remark ] let us take @xmath313 .",
    "the constants @xmath314 , @xmath307 and @xmath315 are small , depending on the constants appearing in conditions  [ cona ] and  [ cona1][cona6 ] . fixing these",
    ", they can be kept away from zero , and hence also the @xmath281-irrepresentable condition is assumed for a value of @xmath281 that stays away from zero .",
    "conditions ( [ s03 ] ) , and ( [ s04 ] ) again require that the effective sparsity is sufficiently smaller that @xmath200 .",
    "formulated differently , the results of theorems  [ selection-quasi-likelihood.theorem ] and  [ oracle-quasi-likelihood.theorem ] imply that if the @xmath281-irrepresentable condition holds , and if @xmath316 for sufficiently small values of @xmath281 and @xmath174 ( depending only on the constants appearing in conditions [ cona ] and ) , then with an appropriate choice of @xmath317 the lasso estimator has with large probability prediction error @xmath318 , @xmath0-error @xmath319 and no false positives .",
    "consider quasi - likelihood loss .",
    "it is easy to see that under the conditions of theorem [ oracle-quasi-likelihood.theorem ] , one has with large probability @xmath320 this follows from @xmath321 , where as in section  [ selection-quasi-likelihood.section ] , @xmath322 , @xmath323 .",
    "let @xmath324 be some other @xmath325 positive semi - definite matrix .",
    "then @xmath326 where @xmath327 thus , under the conditions of theorem  [ oracle-quasi-likelihood.theorem ] , one has that with large probability @xmath328 one can verify that if @xmath329 is small enough , say for some @xmath330 sufficiently small @xmath331 then one may reformulate the compatibility condition replacing @xmath332 by @xmath333 , and the theory for prediction and @xmath0-error goes through essentially without new arguments .",
    "one can then also establish bounds for @xmath334 .",
    "similarly , one may reformulate the @xmath281-irrepresentable condition with @xmath274 replaced by @xmath324 , and obtain variable selection without needing new arguments . in the case where @xmath324 is the population version of @xmath274 , the latter built from an i.i.d .",
    "sample of covariables , one can show that with large probability @xmath335 is of order @xmath200 .",
    "in other words ( and modulo the compatibility constant ) , then condition ( [ s05 ] ) is another instance where it is required that the sparsity @xmath66 is not of larger order than@xmath336 .",
    "we refer to @xcite for more precise statements .",
    "the results of this paper show that the oracle and variable selection properties of the lasso for the linear model also hold for the generalized linear model .",
    "we prove this under the assumption that the is sparsity sufficiently smaller than @xmath230 .",
    "we note that the results rely heavily on the convexity of the loss function .",
    "this allows one to work with an unbounded parameter space .",
    "if the estimators are a priori restricted to lie in a given bounded set , one can extend the results to nonconvex loss [ see @xcite for the mixture model , and @xcite for the mixed effects model ] and one can moreover prove oracle results for the almost linear in @xmath66 regime of sparsity .",
    "research supported by snf 20pa21 - 120050 ."
  ],
  "abstract_text": [
    "<S> we consider the theory for the high - dimensional generalized linear model with the lasso . after a short review on theoretical results in literature , we present an extension of the oracle results to the case of quasi - likelihood loss . </S>",
    "<S> we prove bounds for the prediction error and @xmath0-error . </S>",
    "<S> the results are derived under fourth moment conditions on the error distribution . </S>",
    "<S> the case of robust loss is also given . </S>",
    "<S> we moreover show that under an irrepresentable condition , the @xmath0-penalized quasi - likelihood estimator has no false positives .    . </S>"
  ]
}