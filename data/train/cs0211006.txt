{
  "article_text": [
    "the support vector machine ( svm ) is known as one of state - of - the - art methods especially for pattern recognition @xcite .",
    "the original svm maximizes the margin which is defined by the minimum distance between samples and a separating hyperplane in a hilbert space @xmath0 .",
    "even when the dimensionality of @xmath0 is very large , it has been proved that the original svm has a bound for a generalization error which is independent of the dimensionality . in practice",
    ", however , the original svm sometimes gives a very small margin in the input space , because the metric of the feature space is usually quite different from that of the input space .",
    "such a situation is undesirable in particular when the input space is already a well - designed feature space by using some prior knowledge@xcite .",
    "this paper gives a learning algorithm to maximize the margin in the input space .",
    "one difficulty is getting an explicit form of the margin in the input space , because the classification boundary is curved and the vertical projection from a sample point to the boundary is not always unique .",
    "we solve this problem by linear approximation techniques .",
    "the derived algorithm basically consists of iterations of the alternating two stages as follows : one is to estimate the projection point and the other is to solve a quadratic programming to find optimal parameter values .",
    "such a dual structure appears in other frameworks , such as em algorithm and variational bayes .",
    "much more related work is the principal curve proposed by hastie et al@xcite .",
    "the principal curve finds a curve in a ` center ' of the points in the input space .",
    "the derived algorithm is not a gradient - descent type but newton - like ; hence we have to investigate its convergence property .",
    "it is shown that the derived algorithm does not always converges to the global optimum , but it converges to a local optimum under mild conditions . some interesting relations to the original svm",
    "are also shown : the original svm can be seen as a special case of the algorithm ; and the number of support vectors does not increase so much from the original svm .",
    "the algorithm is verified through simple simulations .",
    "we consider a binary classification problem .",
    "the purpose of learning is to construct a map from an @xmath1-dimensional input @xmath2 to a corresponding output @xmath3 by using a finite number of samples @xmath4 .",
    "let us consider a linear classifier , @xmath5 $ ] , where @xmath6 ; @xmath7 is a feature of an input @xmath8 in a hilbert space @xmath0 , @xmath9 is a weight parameter and @xmath10 is a bias parameter .",
    "those parameters @xmath11 and @xmath12 define a separating hyperplane in the feature space . as a feature function @xmath7 , we only consider a differentiable nonlinear map .",
    "a margin in the input space is defined by the minimum distance from sample points to the classification boundary in the input space . since the classification boundary forms a complex curved surface , the distance can not be obtained in an explicit form , and more significantly , a projection from a point to the boundary is not unique .    here , the metric in the input space is not necessary to be euclidean .",
    "some riemannian metric @xmath13 may be defined , which enables us to represent many kinds of prior knowledge .",
    "for example , the invariance of patterns@xcite can be implemented in this form .",
    "another example is that fisher information matrix is a natural metric , when the input space is a parameter space of some probability distribution@xcite .",
    "although the distance is theoretically preferable to be measured by the length of a geodesic in the riemannian space , it causes computational difficulty . in our formulation , since we only need a distance from a sample point to another point , we use a computationally feasible ( nonsymmetric ) distance from a sample point @xmath14 to another point @xmath8 in the quadratic norm , @xmath15 where @xmath16 .    for simplicity",
    ", we mainly consider the hard margin case in which sample points are separable by a hyperplane in the hilbert space .",
    "the soft margin case is discussed in the section [ sec : soft ] .",
    "let @xmath17 be the closest point on the boundary surface from a sample point @xmath14 , and @xmath18 .",
    "since @xmath19 is invariant under a scalar transformation of @xmath20 , we can assume all points are separated with satisfying @xmath21 if we assume at least one of them is an equality , the margin is given by @xmath22 .",
    "then we can find the optimal parameter by minimizing a quadratic objective function @xmath23 with the constraints ( [ eq : constraint ] ) and @xmath24 .    in order to solve the optimization problem",
    ", we start from a solution of the original svm and update the solution iteratively . by two kinds of linearization technique and a kernel trick which are described in the next section ,",
    "we obtain a discriminant function at the @xmath25-th iteration step in the form of @xmath26 where s.v .",
    "is a set of indices of support vectors , @xmath27 is a kernel function and @xmath28 is its derivative defined by @xmath29 .",
    "we have two groups of parameters here : one is of @xmath30 , @xmath31 and @xmath12 which are parameters of linear coefficients , and the other is of @xmath32 which is an estimate of the projection point @xmath17 and forms base functions . @xmath30 and @xmath12 are initialized by the corresponding parameters in the original svm and the other parameters are initialized by @xmath33 , @xmath34 .",
    "in this section , we overview the derivation of update rules of those parameters .",
    "the resultant algorithm is summarized in sec.[sec : overall ] .",
    "suppose an estimated projection point @xmath32 is given , we can get an approximate distance @xmath35 by a linear approximation@xcite .",
    "taking the taylor expansion of + @xmath36 around @xmath32 up to the first order , we obtain a constraint on @xmath19 , @xmath37 where @xmath38 . minimizing @xmath39 under this constraint",
    ", we have @xmath40 where @xmath41 .",
    "note that this approximate value is unique , and it is invariant under a scalar transformation of @xmath20 .",
    "moreover , the approximation is strictly correct when @xmath42 and @xmath43 .      using the approximate value of the distance",
    ", we have a nonlinear constraint , @xmath44    \\ge { \\|{{\\omega}\\cdot{{\\mbox{\\boldmath $ \\psi$}}}({{\\hat{{{\\mbox{\\boldmath $ x$}}}}}}_i)}\\|{_{g_i^{-1}}}\\over\\sqrt{{{\\omega}\\cdot{\\omega}}}}.\\ ] ] since the constraint is nonlinear for @xmath11 , we linearize it around an approximate solution @xmath45 which is the solution at a current step .",
    "this linearization not only simplifies the problem , but also enables us to derive a dual problem .",
    "let @xmath46 be the right hand side of ( [ eq : nlconst ] ) , the first order expansion is @xmath47 now let @xmath48 , then we have a linear constraint for @xmath11 , @xmath49}\\ge { { \\hat{g}}}_i- f_0 y_i,\\ ] ] where we used the fact @xmath50",
    ". suppose @xmath51 and @xmath52 , then @xmath53 and @xmath54 are given by @xmath55 by the above linearization , we can derive the dual problem in a similar way to the original svm , @xmath56\\cdot[y_j \\{\\phi({{\\hat{{{\\mbox{\\boldmath $ x$}}}}}}_j ) -   { { \\mbox{\\boldmath $ \\psi$}}}({{\\hat{{{\\mbox{\\boldmath $ x$}}}}}}_j){^{\\mathsf t}}{{\\hat{{{\\mbox{\\boldmath $ d$}}}}}}_j   \\}-{{\\hat{\\eta}}}_j ] , \\nonumber\\end{aligned}\\ ] ] which is maximized under constraints @xmath57 + and @xmath58 .",
    "the solution @xmath11 is given by @xmath59.\\ ] ] here we can see an apparent relation to the original svm , i.e. , by letting @xmath34 , @xmath60 , and @xmath61 , we have the exactly the same optimization problem as the original svm .      in order to avoid the calculation of mapping into high dimensional hilbert space",
    ", svm applies a kernel trick , by which an inner product is replaced by a symmetric positive definite kernel function ( mercer kernel ) that is easy to calculate@xcite . in our formulation",
    ", @xmath62 is replaced by a mercer kernel @xmath27 .",
    "we also have to calculate the inner product related to @xmath63 ( the derivative of @xmath64 ) .",
    "let us assume that the kernel function @xmath65 is differentiable .",
    "then , @xmath66 is replaced by a vector @xmath67 , and @xmath68 is replaced by a matrix @xmath69 .",
    "now we can derive the kernel version of the optimization problem . in ( [ eq : wt ] ) , @xmath70 has bases related to @xmath71 and @xmath72 , and the solution @xmath11 has bases @xmath73 additionally .",
    "although @xmath72 can have any kinds of bases , we restrict it in the following form to avoid increasing number of bases .",
    "@xmath74 then we have @xmath75 .",
    "now let @xmath76 then @xmath77 is given by @xmath78 , and @xmath53 by ( [ eq : h ] ) .",
    "further , let us define additional temporal variables that represent several terms in the objective function , @xmath79 then we have the objective function in a kernel form , @xmath80 which is maximized under constraints @xmath81    the new parameters can be determined from ( [ eq : wt ] ) by @xmath82 where @xmath83 .    as for the bias term @xmath12 , since the constraint ( [ eq : constraint3 ] ) should be satisfied in equality for @xmath84 from the kuhn - tucker condition",
    ", we have for any @xmath85 , @xmath86    from ( @xmath87 ) , we can estimate the number of support vectors .",
    "let @xmath88 be the indices of nonzero @xmath89 s at the @xmath25-th step , then the number of support vectors is bounded from upper by @xmath90 .",
    "since @xmath88 does not change much as long as the structure of classification boundary is similar , the number of support vectors is expected to be not so larger than the original svm .      to complete the algorithm",
    ", we have to consider the update of the approximate value of the projection point @xmath32 which is initialized by @xmath14 , otherwise the convergent solution is not precise what we want .",
    "if good approximates @xmath72 and @xmath91 of the solution are given , we can refine @xmath32 iteratively in the same way as in sec .",
    "[ sec : d ] : suppose @xmath92 , the projection point @xmath32 can be estimated by iterating the following steps for @xmath93 , @xmath94}}= { { \\mbox{\\boldmath $ x$}}}_i -     { { { \\hat{{\\mbox{\\boldmath",
    "$ q$}}}}}_i{^{[l]}}\\over\\|{{\\hat{{\\mbox{\\boldmath $ q$}}}}}_i{^{[l]}}\\|{_{g_i^{-1}}^2 } }     \\left[{{\\hat{p}}}_i{^{[l]}}- ( { { \\hat{{{\\mbox{\\boldmath $ x$}}}}}}_i{^{[l]}}{}-{{\\mbox{\\boldmath $ x$}}}_i){^{\\mathsf t}}{{\\hat{{\\mbox{\\boldmath $ q$}}}}}_i{^{[l]}}+ { { \\hat{f}}}_0\\right]\\ ] ] where @xmath95}$ ] is initialized by @xmath96 ; @xmath97}}$ ] and @xmath98}}$ ] are defined in a similar way as @xmath99 and @xmath100 , @xmath101}}&\\equiv & { { { \\hat{{\\omega}}}}\\cdot\\phi({{\\hat{{{\\mbox{\\boldmath $ x$}}}}}}_i{^{[l ] } } ) } \\nonumber\\\\   & = &    \\sum_j \\{{{\\hat{a}}}_j{\\mathrm{k}}({{\\hat{{{\\mbox{\\boldmath $ x$}}}}}}_j{^{\\mathrm{old}}},{{\\hat{{{\\mbox{\\boldmath $ x$}}}}}}_i{^{[l ] } } ) + { { \\hat{{{\\mbox{\\boldmath $ b$}}}}}}_j{^{\\mathsf t}}{\\mathbf{k}_x}({{\\hat{{{\\mbox{\\boldmath $ x$}}}}}}_j{^{\\mathrm{old}}},{{\\hat{{{\\mbox{\\boldmath $ x$}}}}}}_i{^{[l]}})\\ } , \\nonumber \\\\   { { \\hat{{\\mbox{\\boldmath $ q$}}}}}_i{^{[l]}}&\\equiv &    { { { \\hat{{\\omega}}}}\\cdot{{\\mbox{\\boldmath $ \\psi$}}}({{\\hat{{{\\mbox{\\boldmath $ x$}}}}}}_i{^{[l]}})}\\nonumber\\\\   & = & \\sum_j \\ { { { \\hat{a}}}_j    { \\mathbf{k}_x}({{\\hat{{{\\mbox{\\boldmath $ x$}}}}}}_i{^{[l ] } } , { { \\hat{{{\\mbox{\\boldmath $ x$}}}}}}_j{^{\\mathrm{old } } } ) +    { \\mathrm{k}_{xy}}({{\\hat{{{\\mbox{\\boldmath $ x$}}}}}}_i{^{[l]}},{{\\hat{{{\\mbox{\\boldmath $ x$}}}}}}_j{^{\\mathrm{old}}}){{\\hat{{{\\mbox{\\boldmath $ b$}}}}}}_j    \\}.\\nonumber\\end{aligned}\\ ] ]    note that locally maximum points and saddle points of the distance are also equilibrium states of ( [ eq : upmycdx ] ) .",
    "the following proposition guarantees such a point is not stable .",
    "a point @xmath102 is an equilibrium state of the iteration step ( [ eq : upmycdx ] ) , when and only when the point is a critical point of the distance from @xmath14 to the separating boundary , i.e. , a local minimum , a local maximum or a saddle point .",
    "the equilibrium state is not stable when the point is a local maximum or a saddle point .    _",
    "proof : _ it is straightforward to show that a point is an equillibrium state of the iteration step ( [ eq : upmycdx ] ) , only when the point is a critical point of the projection point @xmath39 . without loss of generality , we can assume the uniform metric case @xmath103 , because update rule ( [ eq : upmycdx ] ) is invariant of a metric transformation .",
    "we consider the behavior around a critical point @xmath17 .",
    "let @xmath104}}={{{{{\\mbox{\\boldmath $ x$}}}}^*}}_i+{{\\mbox{\\boldmath $ \\varepsilon$}}}$ ] , for a sufficiently small vector @xmath105 .",
    "one can show that @xmath104}}$ ] is mapped into the separating hypersurface @xmath106 for a small @xmath105 after one step iteration .",
    "therefore , we only consider the case @xmath104}}$ ] is on the hypersurface .",
    "since @xmath17 is a critical point of the distance , the tangent vector @xmath107 is collinear to the distant vector @xmath108 , i.e. , for some constant @xmath109 , it holds @xmath110 furthermore , if @xmath104}}$ ] is in a point of @xmath111 , @xmath107 is nearly orthogonal to @xmath105 , i.e. , @xmath112 by expanding ( [ eq : upmycdx ] ) around @xmath17 , we have a new estimation @xmath113}}$ ] by @xmath114}}\\simeq { { { { { \\mbox{\\boldmath $ x$}}}}^*}}_i   + { 1\\over\\lambda}\\nabla^2 f({{{{{\\mbox{\\boldmath $ x$}}}}^*}}_i){{\\mbox{\\boldmath $ \\varepsilon$}}}- { { { \\mbox{\\boldmath $ d$}}}_i{^{\\mathsf t}}\\nabla^2 f({{{{{\\mbox{\\boldmath $ x$}}}}^*}}_i){{\\mbox{\\boldmath $ \\varepsilon$}}}\\over\\lambda\\|{{\\mbox{\\boldmath $ d$}}}_i\\|}{{\\mbox{\\boldmath $ d$}}}_i,\\ ] ] where @xmath115 is a hessian matrix of @xmath116 .",
    "without loss of generality , we can take the coordinate of @xmath8 as follows : the first coordinate is the direction of @xmath19 , and the second to the @xmath1-th coordinates are taken orthogonally such that an @xmath117 submatrix of @xmath118 for those coordinates is diagonalized , i.e. , @xmath118 is in the form , @xmath119 under this coordinate system , since @xmath120 is of small order value , the first element calculated from the second and third term in ( [ eq : mycdx ] ) vanishes and we have @xmath121}}- { { { { { \\mbox{\\boldmath $ x$}}}}^*}}_i \\simeq { 1\\over\\lambda }   ( 0 , c_2 \\varepsilon_2,\\ldots , c_m\\varepsilon_m){^{\\mathsf t}}.\\ ] ] the iteration step is stable at @xmath17 only when @xmath122}}-{{{{{\\mbox{\\boldmath $ x$}}}}^*}}_i\\|\\le\\|\\forall{{\\mbox{\\boldmath $ \\varepsilon$}}}\\|$ ] , i.e. , t@xmath123 for all @xmath124 .",
    "@xmath125    the condition for 1-@xmath126 plane is shown in figure [ fig : stability ] .",
    "when the point is a local maximum or saddle , the hypersurface is in the unstable region .",
    "however , even in the case of local minimum , there exist an unstable region , when the hypersurface is stronglly curved",
    ". we can avoid the undesired behavior by slowing down .",
    "for example , first @xmath127 and @xmath109 are estimated from @xmath128 and @xmath115 values at the current estimate , and then if @xmath129 for all @xmath124 , the point is to be local minima , then the movement @xmath113}}-{{\\hat{{{\\mbox{\\boldmath $ x$}}}}}}_i{^{[l]}}$ ] to the axes in which @xmath130 should be shrinked by multiplying some factor @xmath131",
    ".    this computationally intensive treatment would be usually necessary only after the several steps , because it is considered that the unstablity for local minima occurs a small region relatively to the size of @xmath19 .",
    "the update of @xmath32 causes another problem : we assumed in section [ sec : qp ] that @xmath11 and @xmath72 have the same bases .",
    "however , @xmath72 has bases based on the old @xmath32 , while we need the new @xmath11 based on the new @xmath32 .",
    "to solve that problem , @xmath72 is projected into new bases , i.e. , from the old one @xmath132 to a new one , @xmath133 .",
    "although @xmath134 can have more bases other than s.v . , we restrict the bases to support vectors to preserve the sparsity of bases .",
    "there are several possibilities of the projection . in this paper",
    ", we use the one which minimizes the cost function @xmath135 where @xmath136 is a certain set of @xmath8 , and we use @xmath137 @xmath138 , @xmath96 , @xmath139 ; @xmath140 .    minimizing ( [ eq : e ] ) leads to a simple least square problem , which can be solved by linear equations .",
    "another possibility of the cost function is @xmath141 , which leads to another set of linear equations .",
    "now let us summarize the algorithm below .    ' '' ''",
    "initialization step : let the solution of the original svm be @xmath142 and @xmath143 ; let @xmath144 and @xmath145 .    for @xmath146 , repeat the following steps until convergence :    1 .",
    "update of @xmath32 : calculate @xmath147 by applying ( [ eq : upmycdx ] ) iteratively to @xmath148 .",
    "projection of hyperplane : calculate @xmath149 , @xmath150 and @xmath91 based on @xmath147 by a certain projection method from @xmath151 , @xmath152 and @xmath153 based on @xmath148 ( sec.[sec : proj ] ) .",
    "qp step : solve the qp problem ( [ eq : qp ] ) with respect to @xmath89 .",
    "parameter update : calculate @xmath154 , @xmath155 and @xmath156 by ( [ eq : newab ] ) and ( [ eq : newf ] ) .",
    "the discriminant function at the @xmath25-th step is given by ( [ eq : f ] ) .    ' '' ''    although algorithm 1 does not always converge to the global minimum , we can prove the following proposition concerning about the convergence of the algorithm .",
    "equilibrium points of algorithm 1 are critical points of the margin in the input space .",
    "the algorithm is stable , when the update rule of @xmath32 ( [ eq : upmycdx ] ) is stable for all @xmath157 ( see also proposition 1 ) .",
    "this proposition can be proved basically by proposition 1 and the fact that the linearization of qp is almost exact by a small perturbation of @xmath11 . as in the case of ( [ eq : upmycdx ] )",
    ", we can modify the algorithm by slowing down in ( [ eq : d ] ) and ( [ eq : upmycdx ] ) so that the equilibrium state is stable when and only when the margin is locally optimal .",
    "however , we do nt use it in the simulation because the case that the local minimum is unstable is expected to be rare .",
    "another problem of algorithm 1 is that each iteration step does not always increase the margin monotonically .",
    "although it is usually faster than gradient type algorithms , the algorithm sometimes does not improve the solution of the original svm at all . because the original svm can be seen as a special case of the algorithm",
    ", we can use some annealing technique , for example , updating temporal variables and parameters more gradually from their initial values .",
    "however , for simplicity , we use a crude method in the simulation as follows : repeat several steps of the algorithm ( 5 steps in the simulation ) and then choose the best solution which gives the largest estimated value of the margin .    as for the complexity of the algorithm ,",
    "we need @xmath158 space and @xmath159 time complexity to calculate temporal variables if the computation of a kernel function is @xmath160 , while the original svm requires @xmath161 space and @xmath162 time .",
    "those calculation can be pararellized easily .",
    "this complexity is not so different when @xmath1 is comparatively small .",
    "once the variables are calculated , the complexity for qp is just the same .",
    "therefore , as far as the calculation for temporal variables is comparative to the qp time , the proposed algorithm is comparative to the original svm .",
    "if the algorithm 1 is heavy because of the large @xmath1 , we can use a simplified algorithm as shown in the section [ sec : simple ] .    as for the iteration of qp which is carried out usually for a few steps ,",
    "since a current solution is an estimate of the solution , it may be able to reduce the complexity of the qp at the next iteration step .",
    "in this section , we give a simulation result for artificial data sets in order to verify the proposed algorithm and to examine the basic performance .",
    "20 training samples and 1000 test samples are randomly drawn from positive and negative distribution , each of which is a gaussian mixture of 3 components with uniformly distributed centers @xmath163 and fixed spherical variance @xmath164 .",
    "the kernel function used here is a spherical gaussian kernel with @xmath165 .",
    "the metric is taken to be euclidean ( i.e. , @xmath166 is the unit matrix ) .",
    "figure [ fig : svm ] and [ fig : alg1 ] show an example of results by the original svm ( initial condition ) and the proposed algorithm ( after 5 steps ) . in this case , the margin value increases from 0.040 to 0.096 .",
    "such a simulation is repeated for 100 sets of samples with different random numbers .",
    "the estimated margins in the input space for the original and proposed algorithm is shown in figure [ fig : margin ] ( log - log scale ) . by the crude algorithm described in the previous section",
    ", there are 4 cases among 100 runs that can not improve the margin of the original svm .",
    "the ratios of the margin are distributed from 1.00 ( no improvement ) to 27.9 .",
    "the misclassification errors for test samples is shown in figure [ fig : error ]",
    ". the ratios of error distributed between [ 0.40(best),1.37(worst ) ] .",
    "this results indicates that the margin in the input space is efficient to improve the generalization performance in average , but there are cases that can not reduce the generalization error even when the margin in the input space increases .    ) and crosses ( @xmath167 ) are positive and negative samples .",
    "squares ( @xmath125 ) represent estimates of the projection of the points by applying ( [ eq : upmycdx ] ) for 10 steps . ]    ]",
    "for noisy situation , the hard margin classifier often overfits samples .",
    "there are several possibitilities to incorporate the soft margin , here we give a simple one .",
    "the soft margin can be derived by introducing slack variables @xmath168 into the optimization problem . if we use a soft constraint in the form @xmath169}\\ge { { \\hat{g}}}_i - f_0 y_i - z_i,\\ ] ] and adding penalty for the slack variables , @xmath170    by this modification , only the constraint ( [ eq : constrainta ] ) for @xmath89",
    "is changed to @xmath171 which is the same constraint as the soft margin of the original svm . however , the geometrical meaning of ( [ eq : constraint5 ] ) in the space is not clear .",
    "it is a future work to introduce a natural soft constraint in the input space .",
    "although algorithm 1 achieves the precise solution , the computation costs is high for large dimensionality of inputs . in this section ,",
    "we give a simplified algorithm .",
    "if we do nt update @xmath32 , the first and the second steps of algorithm 1 is not necessary any more .",
    "this simplification makes algorithm 1 a little simpler because all @xmath172 terms vanish .",
    "however , let us consider further simplification .",
    "we have shown the relation to the original svm : the original svm can be derived @xmath61 and @xmath60 . since @xmath54 causes many temporal variables ,",
    "we only maintain @xmath53 .",
    "then all the terms related to @xmath150 s vanish .    consequently , the above simplifications lead to the algorithm much like the original svm .",
    "in fact , the existing code for the original svm can be used as follows :    for each step , first @xmath53 is calculated , @xmath173 then , by letting the @xmath174 element of kernel matrix be @xmath175 , the original svm for this kernel matrix gives the solution for each step of the simplified algorithm .",
    "we have proposed a new learning algorithm to find a kernel - based classifier that maximizes the margin in the input space .",
    "the derived algorithm consists of an alternating optimization between the foot of perpendicular and the linear coefficient parameters .",
    "such a dual structure appears in other frameworks , such as em algorithm , variational bayes , and principal curve .",
    "there are many issues to be studied about the algorithm , for example , analyzing the generalization performance theoretically and finding an efficient algorithm that reduces the complexity and converges more stably .",
    "it is also an interesting issue to extend our framework to other problems than classification , such as regression@xcite .    in this paper",
    ", we have assumed that the kernel function is given and fixed .",
    "recently , several techniques and criteria to choose a kernel function have been proposed extensively .",
    "we expect that those techniques and much other knowledge for the original svm can be incorporated in our framework . applying the algorithm to real world data",
    "is also important .",
    "12 s. akaho , curve fitting that minimizes the mean square of perpendicular distances from sample points , _ spie vision geometry ii _ ( also found in _ selected spie papers on cd - rom _ , 8 , 1999 ) , 237244 ( 1993 )                    p.y .",
    "simard , y.a .",
    "le cun , j.s .",
    "denker , b. victorri , transformation invariance in pattern recognition ",
    "tangent distance and tangent propagation , in _ neural networks : tricks of the trade _ , g. orr and k .-",
    "mller , eds . , springer - verlag , vol.1524 , pp.239274 ( 1998 )"
  ],
  "abstract_text": [
    "<S> we propose a novel criterion for support vector machine learning : maximizing the margin in the input space , not in the feature ( hilbert ) space . </S>",
    "<S> this criterion is a discriminative version of the principal curve proposed by hastie et al . </S>",
    "<S> the criterion is appropriate in particular when the input space is already a well - designed feature space with rather small dimensionality . </S>",
    "<S> the definition of the margin is generalized in order to represent prior knowledge . </S>",
    "<S> the derived algorithm consists of two alternating steps to estimate the dual parameters . </S>",
    "<S> firstly , the parameters are initialized by the original svm . </S>",
    "<S> then one set of parameters is updated by newton - like procedure , and the other set is updated by solving a quadratic programming problem . </S>",
    "<S> the algorithm converges in a few steps to a local optimum under mild conditions and it preserves the sparsity of support vectors . although the complexity to calculate temporal variables increases the complexity to solve the quadratic programming problem for each step does not change . </S>",
    "<S> it is also shown that the original svm can be seen as a special case . </S>",
    "<S> we further derive a simplified algorithm which enables us to use the existing code for the original svm . </S>"
  ]
}