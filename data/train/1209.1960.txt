{
  "article_text": [
    "clustering , the unsupervised classification of patterns into groups , is one of the most important tasks in exploratory data analysis @xcite . primary goals of clustering include gaining insight into data ( detecting anomalies , identifying salient features , etc . ) , classifying data , and compressing data .",
    "clustering has a long and rich history in a variety of scientific disciplines including anthropology , biology , medicine , psychology , statistics , mathematics , engineering , and computer science . as a result ,",
    "numerous clustering algorithms have been proposed since the early 1950s @xcite .",
    "clustering algorithms can be broadly classified into two groups : hierarchical and partitional @xcite .",
    "hierarchical algorithms recursively find nested clusters either in a top - down ( divisive ) or bottom - up ( agglomerative ) fashion . in contrast , partitional algorithms find all the clusters simultaneously as a partition of the data and do not impose a hierarchical structure .",
    "most hierarchical algorithms have quadratic or higher complexity in the number of data points @xcite and therefore are not suitable for large data sets , whereas partitional algorithms often have lower complexity .    given a data set @xmath0 in @xmath1 , i.e. , @xmath2 points ( vectors ) each with @xmath3 attributes ( components ) , hard partitional algorithms divide @xmath4 into @xmath5 exhaustive and mutually exclusive clusters @xmath6 @xmath7 @xmath8 for @xmath9 .",
    "these algorithms usually generate clusters by optimizing a criterion function .",
    "the most intuitive and frequently used criterion function is the sum of squared error ( ` sse ` ) given by :    @xmath10    where @xmath11 denotes the euclidean ( @xmath12 ) norm and @xmath13 is the centroid of cluster @xmath14 whose cardinality is @xmath15 .",
    "the optimization of is often referred to as the minimum ` sse ` clustering ( mssc ) problem .",
    "the number of ways in which a set of @xmath2 objects can be partitioned into @xmath5 non - empty groups is given by stirling numbers of the second kind :    @xmath16    which can be approximated by @xmath17 it can be seen that a complete enumeration of all possible clusterings to determine the global minimum of is clearly computationally prohibitive except for very small data sets @xcite .",
    "in fact , this non - convex optimization problem is proven to be np - hard even for @xmath18 @xcite or @xmath19 @xcite . consequently",
    ", various heuristics have been developed to provide approximate solutions to this problem @xcite . among these heuristics ,",
    "lloyd s algorithm @xcite , often referred to as the ( batch ) k - means algorithm , is the simplest and most commonly used one .",
    "this algorithm starts with @xmath5 arbitrary centers , typically chosen uniformly at random from the data points .",
    "each point is assigned to the nearest center and then each center is recalculated as the mean of all points assigned to it .",
    "these two steps are repeated until a predefined termination criterion is met .",
    "the k - means algorithm is undoubtedly the most widely used partitional clustering algorithm @xcite .",
    "its popularity can be attributed to several reasons .",
    "first , it is conceptually simple and easy to implement .",
    "virtually every data mining software includes an implementation of it .",
    "second , it is versatile , i.e. , almost every aspect of the algorithm ( initialization , distance function , termination criterion , etc . )",
    "can be modified .",
    "this is evidenced by hundreds of publications over the last fifty years that extend k - means in various ways .",
    "third , it has a time complexity that is linear in @xmath2 , @xmath3 , and @xmath5 ( in general , @xmath20 and @xmath21 ) .",
    "for this reason , it can be used to initialize more expensive clustering algorithms such as expectation maximization @xcite , dbscan @xcite , and spectral clustering @xcite .",
    "furthermore , numerous sequential @xcite and parallel @xcite acceleration techniques are available in the literature .",
    "fourth , it has a storage complexity that is linear in @xmath2 , @xmath3 , and @xmath5 .",
    "in addition , there exist disk - based variants that do not require all points to be stored in memory @xcite .",
    "fifth , it is guaranteed to converge @xcite at a quadratic rate @xcite . finally , it is invariant to data ordering , i.e. , random shufflings of the data points .    on the other hand , k - means has several significant disadvantages .",
    "first , it requires the number of clusters , @xmath5 , to be specified _",
    "a priori_. the value of this parameter can be determined automatically by means of various cluster validity measures @xcite .",
    "second , it can only detect compact , hyperspherical clusters that are well separated .",
    "this can be alleviated by using a more general distance function such as the mahalanobis distance , which permits the detection of hyperellipsoidal clusters @xcite .",
    "third , due its utilization of the squared euclidean distance , it is sensitive to noise and outlier points since even a few such points can significantly influence the means of their respective clusters .",
    "this can addressed by outlier pruning @xcite or using a more robust distance function such as city - block ( @xmath22 ) distance .",
    "fourth , due to its gradient descent nature , it often converges to a local minimum of the criterion function @xcite . for the same reason , it is highly sensitive to the selection of the initial centers .",
    "adverse effects of improper initialization include empty clusters , slower convergence , and a higher chance of getting stuck in bad local minima @xcite .",
    "fortunately , all of these drawbacks except the first one can be remedied by using an adaptive initialization method ( i m ) .    in this study",
    ", we investigate some of the most popular ims developed for the k - means algorithm .",
    "our motivation is three - fold .",
    "first , a large number of ims have been proposed in the literature and thus a systematic study that reviews and compares these methods is desirable .",
    "second , these ims can be used to initialize other partitional clustering algorithms such as fuzzy c - means and its variants and expectation maximization .",
    "third , most of these ims can be used independently of k - means as standalone clustering algorithms .",
    "this study differs from earlier studies of a similar nature @xcite in several respects :    a more comprehensive overview of the existing ims is provided ,    the experiments involve a larger set of methods and a significantly more diverse collection of data sets ,    in addition to clustering effectiveness , computational efficiency is used as a performance criterion , and    the experimental results are analyzed more thoroughly using non - parametric statistical tests .",
    "the rest of the paper is organized as follows .",
    "section [ sec_init_methods ] presents a survey of k - means ims .",
    "section [ sec_exp_setup ] describes the experimental setup .",
    "section [ sec_exp_results ] presents the experimental results , while section [ sec_conc ] gives the conclusions .",
    "in this section , we briefly review some of the commonly used ims with an emphasis on their time complexity ( with respect to @xmath2 ) . in each complexity class , methods are presented in chronologically ascending order .",
    "forgy s method @xcite assigns each point to one of the @xmath5 clusters uniformly at random .",
    "the centers are then given by the centroids of these initial clusters .",
    "this method has no theoretical basis , as such random clusters have no internal homogeneity @xcite .",
    "jancey s method @xcite assigns to each center a synthetic point randomly generated within the data space .",
    "unless the data set fills the space , some of these centers may be quite distant from any of the points @xcite , which might lead to the formation of empty clusters .",
    "macqueen @xcite proposed two different methods .",
    "the first one , which is the default option in the quick cluster procedure of ibm spss statistics @xcite , takes the first @xmath5 points in @xmath4 as the centers .",
    "an obvious drawback of this method is its sensitivity to data ordering .",
    "the second method chooses the centers randomly from the data points .",
    "the rationale behind this method is that random selection is likely to pick points from dense regions , i.e. , points that are good candidates to be centers .",
    "however , there is no mechanism to avoid choosing outliers or points that are too close to each other @xcite .",
    "multiple runs of this method is the standard way of initializing k - means @xcite .",
    "it should be noted that this second method is often mistakenly attributed to forgy @xcite .",
    "ball and hall s method @xcite takes the centroid of @xmath4 , i.e. , @xmath23 , as the first center .",
    "it then traverses the points in arbitrary order and takes a point as a center if it is at least @xmath24 units apart from the previously selected centers until @xmath5 centers are obtained .",
    "the purpose of the distance threshold @xmath24 is to ensure that the seed points are well separated .",
    "however , it is difficult to decide on an appropriate value for @xmath24 .",
    "in addition , the method is sensitive to data ordering .",
    "the simple cluster seeking method @xcite is identical to ball and hall s method with the exception that the first point in @xmath4 is taken as the first center .",
    "this method is used in the fastclus procedure of sas @xcite .",
    "spth s method @xcite is similar to forgy s method with the exception that the points are assigned to the clusters in a cyclical fashion , i.e. , the @xmath25-th ( @xmath26 ) point is assigned to the @xmath27-th cluster .",
    "in contrast to forgy s method , this method is sensitive to data ordering",
    ".    maximin method @xcite chooses the first center @xmath28 arbitrarily and the @xmath29-th ( @xmath30 ) center @xmath31 is chosen to be the point that has the greatest minimum - distance to the previously selected centers , i.e. , @xmath32 .",
    "this method was originally developed as a @xmath33-approximation to the @xmath5-center clustering problem points in a metric space , the goal of @xmath5-center clustering is to find @xmath5 representative points ( centers ) such that the maximum distance of a point to a center is minimized . ] .",
    "it should be noted that , motivated by a vector quantization application , katsavounidis _ et al .",
    "_ s variant @xcite takes the point with the greatest euclidean norm as the first center .",
    "al - daoud s density - based method @xcite first uniformly partitions the data space into @xmath34 disjoint hypercubes .",
    "it then randomly selects @xmath35 points from hypercube @xmath36 ( @xmath37 ) to obtain a total of @xmath5 centers ( @xmath38 is the number of points in hypercube @xmath36 ) .",
    "there are two main disadvantages associated with this method .",
    "first , it is difficult to decide on an appropriate value for @xmath34 .",
    "second , the method has a storage complexity of @xmath39 , where @xmath40 is the number of bits allocated to each attribute .",
    "bradley and fayyad s method @xcite starts by randomly partitioning the data set into @xmath41 subsets .",
    "these subsets are clustered using k - means initialized by macqueen s second method producing @xmath41 sets of intermediate centers each with @xmath5 points .",
    "these center sets are combined into a superset , which is then clustered by k - means @xmath41 times , each time initialized with a different center set .",
    "members of the center set that give the least ` sse ` are then taken as the final centers .",
    "pizzuti _ et al . _",
    "@xcite improved upon al - daoud s density - based method using a multiresolution grid approach .",
    "their method starts with @xmath42 hypercubes and iteratively splits these as the number of points they receive increases .",
    "once the splitting phase is completed , the centers are chosen from the densest hypercubes .    the k - means++ method @xcite interpolates between macqueen s second method and the maximin method .",
    "it chooses the first center randomly and the @xmath29-th ( @xmath43 ) center is chosen to be @xmath44 with a probability of @xmath45 , where @xmath46 denotes the minimum - distance from a point @xmath47 to the previously selected centers .",
    "this method yields an @xmath48 approximation to the mssc problem .",
    "the greedy k - means++ method probabilistically selects @xmath49 centers in each round and then greedily selects the center that most reduces the ` sse ` .",
    "this modification aims to avoid the unlikely event of choosing two centers that are close to each other .",
    "the pca - part method @xcite uses a divisive hierarchical approach based on pca ( principal component analysis ) @xcite . starting from an initial cluster that contains the entire data set , the method iteratively selects the cluster with the greatest `",
    "sse ` and divides it into two subclusters using a hyperplane that passes through the cluster centroid and is orthogonal to the principal eigenvector of the cluster covariance matrix .",
    "this procedure is repeated until @xmath5 clusters are obtained .",
    "the centers are then given by the centroids of these clusters .",
    "the var - part method @xcite is an approximation to pca - part , where the covariance matrix of the cluster to be split is assumed to be diagonal . in this case , the splitting hyperplane is orthogonal to the coordinate axis with the greatest variance .",
    "lu _ et al . _ s method @xcite uses a two - phase pyramidal approach .",
    "the attributes of each point are first encoded as integers using @xmath50-level quantization , where @xmath51 is a resolution parameter .",
    "these integer points are considered to be at level @xmath52 of the pyramid . in the bottom - up phase , starting from level @xmath52 , neighboring data points at level @xmath53 ( @xmath54 ) are averaged to obtain weighted points at level @xmath55 until at least @xmath56 points are obtained .",
    "data points at the highest level are refined using k - means initialized with the @xmath5 points with the largest weights . in the top - down phase ,",
    "starting from the highest level , centers at level @xmath57 are projected onto level @xmath53 and then used to initialize the @xmath53-th level clustering .",
    "the top - down phase terminates when level @xmath52 is reached .",
    "the centers at this level are then inverse quantized to obtain the final centers .",
    "the performance of this method degrades with increasing dimensionality @xcite .",
    "onoda _ et al . _ s method @xcite first calculates @xmath5 independent components ( ics ) @xcite of @xmath4 and then chooses the @xmath29-th ( @xmath58 ) center as the point that has the least cosine distance from the @xmath29-th ic .",
    "hartigan s method @xcite first sorts the points according to their distances to @xmath59 .",
    "the @xmath29-th ( @xmath58 ) center is then chosen to be the ( @xmath60)-th point .",
    "this method is an improvement over macqueen s first method in that it is invariant to data ordering and is more likely to produce seeds that are well separated .",
    "the computational cost of this method is dominated by the complexity of sorting , which is @xmath61 .",
    "al - daoud s variance - based method @xcite first sorts the points on the attribute with the greatest variance and then partitions them into @xmath5 groups along the same dimension .",
    "the centers are then chosen to be the points that correspond to the medians of these groups .",
    "note that this method disregards all attributes but one and therefore is likely to be effective only for data sets in which the variability is mostly on one dimension .",
    "redmond and heneghan s method @xcite first constructs a kd - tree of the data points to perform density estimation and then uses a modified maximin method to select @xmath5 centers from densely populated leaf buckets .",
    "the computational cost of this method is dominated by the complexity of kd - tree construction , which is @xmath61 .",
    "the robin ( robust initialization ) method @xcite uses a local outlier factor ( lof ) @xcite to avoid selecting outlier points as centers . in iteration",
    "@xmath29 ( @xmath62 ) , the method first sorts the data points in decreasing order of their minimum - distance to the previously selected centers .",
    "it then traverses the points in sorted order and selects the first point that has an lof value close to @xmath63 as the @xmath29-th center .",
    "the computational cost of this method is dominated by the complexity of sorting , which is @xmath61 .      astrahan s method @xcite uses two distance thresholds @xmath64 and @xmath65 .",
    "it first calculates the _ density _ of each point as the number of points within a distance of @xmath64 .",
    "the points are sorted in decreasing order by their densities and the highest density point is chosen as the first center .",
    "subsequent centers are chosen in order of decreasing density subject to the condition that each new center be at least at a distance of @xmath65 from the previously selected centers .",
    "this procedure is continued until no more centers can be chosen .",
    "finally , if more than @xmath5 centers are chosen , hierarchical clustering is used to group the centers until only @xmath5 of them remain .",
    "the main problem with this method is that it is very sensitive to the values of @xmath64 and @xmath65 .",
    "for example , if @xmath64 is too small there may be many isolated points with zero density whereas if it is too large a few centers will cover the entire data set @xcite .",
    "lance and williams @xcite suggested that the output of a hierarchical clustering algorithm can be used to initialize k - means . despite the fact that such algorithms often have quadratic or higher complexity ,",
    "this method is highly recommended in the statistics literature @xcite possibly due to the limited size of the data sets in this field .",
    "kaufman and rousseeuw s method @xcite takes @xmath59 as the first center and the @xmath29-th ( @xmath30 ) center is chosen to be the point that most reduces the ` sse ` .",
    "since pairwise distances between the data points need to be calculated in each iteration , the time complexity of this method is @xmath66 .",
    "cao _ et al . _",
    "@xcite formalized astrahan s density - based method within the framework of a neighborhood - based rough set model . in this model",
    ", the @xmath67-neighborhood of a point is defined as the set of points within @xmath67 distance from it according to a particular distance measure .",
    "based on this neighborhood model , the concepts of _ cohesion _ and _ coupling _ are defined .",
    "the former is a measure of the centrality of a point with respect to its neighborhood ; whereas the latter is a measure of separation between two neighborhoods .",
    "the method first sorts the data points in decreasing order of their _ cohesion _ and takes the point with the greatest _",
    "cohesion _ as the first center .",
    "it then traverses the points in sorted order and takes the first point that has a _ coupling _ of less than @xmath67 with the previously selected centers as the @xmath29-th ( @xmath30 ) center .",
    "the computational cost of this method is dominated by the complexity of the @xmath67-neighborhood calculations , which is @xmath66 .",
    "the binary - splitting method @xcite takes @xmath59 as the first center . in iteration @xmath68 ( @xmath69 ) , each of the existing @xmath70 centers is split into two new centers by subtracting and adding a fixed perturbation vector @xmath71 , i.e. , @xmath72 and @xmath73 ( @xmath74 ) .",
    "these @xmath75 new centers are then refined using k - means .",
    "there are two main disadvantages associated with this method .",
    "first , there is no guidance on the selection of a proper value for @xmath71 , which determines the direction of the split @xcite .",
    "second , the method is computationally demanding since after each iteration k - means has to be run for the entire data set .    the directed - search binary - splitting method @xcite is an improvement over the binary - splitting method in that it determines the value of @xmath71 using pca .",
    "however , it has even higher computational requirements due to the calculation of the principal eigenvector in each iteration .",
    "the global k - means method @xcite takes @xmath59 as the first center . in iteration @xmath29 ( @xmath76 ) it considers each of the @xmath2 points in turn as a candidate for the @xmath77-st center and runs k - means with @xmath78 centers on the entire data set .",
    "this method is computationally prohibitive for large data sets as it involves @xmath79 runs of k - means on the entire data set .",
    "it should be noted that the two splitting methods and the global k - means method are not initialization methods _ per se_. these methods can be considered as complete clustering methods that utilize k - means as a local search procedure .",
    "for this reason , to the best of our knowledge , none of the initialization studies to date included these methods in their comparisons .",
    "we should also mention ims based on metaheuristics such as simulated annealing @xcite and genetic algorithms @xcite .",
    "these algorithms start from a random initial configuration ( population ) and use k - means to evaluate their solutions in each iteration ( generation ) .",
    "there are two main disadvantages associated with these methods .",
    "first , they involve numerous parameters that are difficult to tune ( initial temperature , cooling schedule , population size , crossover / mutation probability , etc . ) @xcite .",
    "second , due to the large search space , they often require a large number of iterations , which renders them computationally prohibitive for all but the smallest data sets .",
    "interestingly , with the recent developments in combinatorial optimization algorithms , it is now feasible to obtain globally minimum ` sse ` clusterings for small data sets without resorting to metaheuristics @xcite .",
    "based on the descriptions given above , it can be seen that superlinear methods often have more elaborate designs when compared to linear ones .",
    "an interesting feature of the superlinear methods is that they are often deterministic , which can be considered as an advantage especially when dealing with large data sets . in contrast , linear methods are often non - deterministic and/or order - sensitive . as a result , it is common practice to perform multiple runs of such methods and take the output of the run that produces the least ` sse ` @xcite .",
    "a frequently cited advantage of the more elaborate methods is that they often lead to faster k - means convergence , i.e. , require fewer iterations , and as a result the time gained during the clustering phase can offset the time lost during the initialization phase @xcite .",
    "this may be true when a standard implementation of k - means is used",
    ". however , convergence speed may not be as important when a fast k - means variant is used as such methods often require significantly less time compared to a standard k - means implementation . in this study",
    ", we utilize a fast k - means variant based on triangle inequality @xcite and partial distance elimination @xcite techniques . as will be seen in ",
    "[ sec_exp_results ] , this fast and exact k - means implementation will diminish the computational efficiency differences among various ims . in other words , we will demonstrate that elaborate methods that lead to faster k - means convergence are not necessarily more efficient than simple methods with slower convergence .",
    "in order to obtain a comprehensive evaluation of various ims , we conducted two sets of experiments . the first experiment involved @xmath80 commonly used real data sets with sizes ranging from @xmath81 to @xmath82 points .",
    "most of these data sets were obtained from the uci machine learning repository @xcite ( see table [ tab_data_set ] . ) the second experiment involved a large number of synthetic data sets with varying clustering complexity .",
    "we used a recent algorithm proposed by maitra and melnykov @xcite to generate these data sets .",
    "this algorithm involves the calculation of the exact overlap ( @xmath83 ) between each cluster pair , measured in terms of their total probability of misclassification , and guided simulation of gaussian mixture components satisfying prespecified overlap characteristics .",
    "the algorithm was used with the following parameters : mean overlap ( @xmath84 ) , number of points ( @xmath85 ) , number of attributes ( @xmath86 ) , and number of classes ( @xmath87 ) .",
    "the parameter @xmath88 denotes the mean overlap between pairs of clusters .",
    "however , we observed that two synthetic data sets with the same @xmath88 can have considerably different clustering complexity .",
    "therefore , we quantified clustering complexity using the following indirect approach .",
    "for each data set , we executed the k - means algorithm initialized with the `` true '' centers given by the cluster generation algorithm and calculated the ` rand ` , ` vd ` , and ` vi ` measures ( see  [ sec_perf_crit ] ) upon convergence .",
    "the average of these measures , @xmath89 , was taken as a quantitative indicator of clustering complexity .",
    "note that each of these normalized measures takes values from the @xmath90 $ ] interval . for ` rand",
    "` larger values are better , whereas for ` vd ` and ` vi ` smaller values are better . therefore , we inverted the ` rand ` values by subtracting them from @xmath63 to make this measure compatible with the other two .",
    "finally , using the aforementioned complexity quantification scheme , we generated @xmath91 synthetic data sets from each of the following complexity classes : easy ( @xmath92 ) , moderate ( @xmath93 ) , and difficult ( @xmath94 ) . the total number of synthetic data sets was thus @xmath95 .",
    "figure [ fig_synthetic_data_sets ] shows sample data sets with @xmath96 clusters from each complexity class .",
    "lc data set complexity & i m ranking +   + easy & @xmath97 + moderate & @xmath98 + difficult & @xmath99 +   + easy & @xmath100 + moderate & + difficult & +   + easy & @xmath101 + moderate & + difficult & +   + easy & @xmath102 + moderate & + difficult & +   + easy & @xmath103 + moderate & @xmath104 + difficult & +   + easy & @xmath105 + moderate & @xmath106 + difficult & @xmath107 +    lc data set complexity & i m ranking +   + easy & @xmath108 + moderate & @xmath109 + difficult & @xmath110 +   + easy & @xmath111 + moderate & @xmath112 + difficult & @xmath113 +   + easy & @xmath111 + moderate & @xmath114 + difficult & +   + easy & @xmath111 + moderate & @xmath112 + difficult & @xmath115 +   + easy & @xmath111 + moderate & @xmath116 + difficult & +   + easy & @xmath117 + moderate & @xmath118 + difficult & @xmath119 +    lc data set complexity & i m ranking +   + easy & @xmath120 + moderate & same as easy + difficult & @xmath121 +   + easy & @xmath122 + moderate & @xmath123 + difficult & @xmath124 +   + easy & @xmath122 + moderate & @xmath125 + difficult & @xmath124 +   + easy & @xmath122 + moderate & @xmath123 + difficult & +   + easy & @xmath126 + moderate & @xmath123 + difficult & @xmath127 +   + easy & @xmath128 + moderate & @xmath123 + difficult & @xmath129 +"
  ],
  "abstract_text": [
    "<S> k - means is undoubtedly the most widely used partitional clustering algorithm . </S>",
    "<S> unfortunately , due to its gradient descent nature , this algorithm is highly sensitive to the initial placement of the cluster centers . </S>",
    "<S> numerous initialization methods have been proposed to address this problem . in this paper , we first present an overview of these methods with an emphasis on their computational efficiency . </S>",
    "<S> we then compare eight commonly used linear time complexity initialization methods on a large and diverse collection of data sets using various performance criteria . </S>",
    "<S> finally , we analyze the experimental results using non - parametric statistical tests and provide recommendations for practitioners . </S>",
    "<S> we demonstrate that popular initialization methods often perform poorly and that there are in fact strong alternatives to these methods . </S>"
  ]
}