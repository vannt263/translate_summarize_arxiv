{
  "article_text": [
    "recent years have seen a steady increase in the availability of large amounts of economic data .",
    "this raises the question of how best to exploit this information to refine benchmark techniques used by the private industry and the academic community . in this study",
    "we focus on the question of how to perform consistent variable selection and ( finite sample ) inference in classical time series regression models with a fixed number of candidate variables and a general error distribution .    to this end",
    ", we consider a technique recently introduced in the machine learning community belonging to the group of shrinkage methodologies ( that is , following the idea of shrinking to zero the coefficients of the irrelevant variables ) that has proven its worth and is becoming increasingly popular : the least absolute shrinkage and selection operator ( lasso ) , introduced by , and its refined version known as the adaptive lasso , proposed by .",
    "the main problem with the lasso is that it requires a condition denoted as the _ irrepresentable condition _ , which is essentially a necessary condition for exact recovery of the non - zero coefficients that is much too restrictive in many cases .",
    "indeed , irrepresentable conditions show that the lasso typically selects too many variables and that so - called false positives are unavoidable .",
    "proposed the adaptive lasso to alleviate this problem and to try to reduce the number of false positives .",
    "moreover , the adaptive lasso estimator also fulfills the oracle property in the sense introduced by .",
    "the interest in using lasso - type techniques in general applications as well , such as those in economics and finance , raises the question of how to extend the most advanced theoretical results derived for the iid cross - sectional setting to the time - series setting .",
    "recent papers investigating this question in settings requiring different assumptions and conditions on the number of active variables and on the error distribution include : , , , , , , and .",
    "two studies recently proposed are closely related to our work : and .",
    "each investigates the asymptotic properties of the adaptive lasso estimator in single - equation linear time series models : while focuses more on the conditions needed to perform consistent variable selection in stationary and nonstationary autoregressive models using the adaptive lasso with a fixed number of variables , extend the basic linear model investigated in the studies cited above to include exogenous variables , non - gaussian , conditionally heteroscedastic and possibly time - dependent errors , and a number of variables ( candidate and relevant ones ) that is allowed to increase as a function of the sample size .",
    "although some of their results are identical to those of the present study , conditions on the model and proofs are substantially ( if not completely ) different .",
    "moreover , in this paper we take the discussion a step further and contribute to the literature on the adaptive lasso along two main dimensions .",
    "first , we quantify the bias in finite sample that is incurred when making inference on the active variables in time series regressions by introducing an explicit formula .",
    "second , we show how we can make inference on the inactive variables in the regression . in particular",
    ", we introduce a very simple procedure to test the hypothesis that a given parameter is equal to zero , i.e.  that the corresponding variable does not belong to the active set .",
    "our theoretical results show that the adaptive lasso may combine efficient parameter estimation , variable selection , and valid finite sample inference in one step .    to the best of our knowledge ,",
    "these are new results .",
    "they extend the usefulness of the ( adaptive ) lasso beyond variable selection for performing statistical inference ( such as tests of hypotheses and the construction of confidence intervals ) in a broad spectrum of applications in all fields dealing with a large amount of iid and time series data",
    ".    some related research on the significance of the active variables in a lasso model has been recently proposed by .",
    "nevertheless , their proposed covariance statistic for testing the significance of predictor variables as they enter the active set , along the lasso solution path , differs considerably from the approach we propose in this study . as maintain in their discussion section at the end of the paper , the question of how to carry out a significance test for any predictor in the active set given a lasso model at some fixed value of @xmath0 ( i.e.  the tuning parameter of the lasso that controls the amount of shrinkage )",
    "was left for future research .",
    "finally , a similar approach for the lasso in the gaussian iid setting has been proposed by .",
    "using an extensive simulation study based on data generating processes with a different number of variables , error distributions , and number of observations at disposal , we investigate the relevance of the bias correction factor and the effectiveness of the introduced testing procedure .",
    "first , results show the importance in finite samples of the bias correction factor for the active variables : the empirical coverages are significantly improved , in particular for variables with small coefficients .",
    "second , although conservative in their construction , tests of the null hypothesis that coefficients of the inactive variables are equal to zero give accurate results , yielding reasonably small proportions of false rejections .",
    "third , if enough data is available , tests on the active variables with small coefficients also produce satisfactory power results .    finally , we apply the adaptive lasso to a classic problem in financial economics that has been investigated in the academic community for the last twenty years : the relation between interest rates and the state of the economy .",
    "in particular , we analyze which variables , from a group of macroeconomic and financial indicators , are relevant explaining the short - term interest rate in a simple taylor rule - type monetary policy model ( see , , page 202 ) .",
    "not surprisingly , we find that the only predictors belonging to the set of active variables identified by the adaptive lasso are the following three : one - lag past short rate values ( which take into account the well - known persistence of the short rate dynamics and act as a proxy for additional macroeconomic , monetary policy , or even financial variables ) , an inflation indicator , and the unemployment rate .",
    "this result adds a purely statistical foundation to the classic economic intuition driving the taylor rule , suggesting that the federal reserve system ( fed ) increases interest rates in times of high inflation , or when employment is above the full employment levels , and decreases interest rates in the opposite situations .",
    "the content of this paper can be summarized as follows : section 2 introduces the model we are going to consider .",
    "oracle properties of the adaptive lasso for time series regressions are discussed in section 3 . in section 4",
    ", we introduce the statistical testing procedure that can be used to make finite samples inference on both active and inactive variables .",
    "monte carlo simulation results are shown in section 5 , and the application on the prediction of the short - term interest rate is performed in section 6 . finally , section 7 concludes .",
    "all proofs of the theorems in the paper are provided in the appendix .",
    "consider the stationary linear regression model @xmath1 where @xmath2 , @xmath3 is a vector of covariates at time @xmath4 , @xmath5 is a vector of regressors at time @xmath6 assumed to predict @xmath7 , @xmath8 is a zero - mean error term , and @xmath9 is the unknown parameter of interest . and @xmath10 are introduced in theorem [ t1 ] below . ]",
    "important examples of linear regression models ( [ pr ] ) include : autoregressive models ( when @xmath11 and @xmath12 , for @xmath13 and @xmath14 ) ; iid linear regression models ( when @xmath15 and @xmath12 , for @xmath16 and @xmath14 ) ; and predictive regression models ( when @xmath15 and @xmath17 , for @xmath16 and @xmath18 ) .",
    "to simplify our notation , we just write @xmath19 , i.e. , we set @xmath20 , for @xmath16 , @xmath21 , for @xmath22 , and @xmath23 , for @xmath24 . a common way to estimate the unknown parameter @xmath25 relies on the least squares approach .",
    "more precisely , we can introduce the least squares estimator @xmath26 of @xmath25 defined as @xmath27 where @xmath28 and @xmath29 denote the sample size . furthermore , under some regularity conditions and using standard techniques , we can show that @xmath30 i.e , @xmath31 is a consistent estimator of @xmath25 with normal limit distribution and covariance matrix @xmath32 .",
    "let @xmath33 denote the set of the non - zero coefficients , and assume that @xmath34 .",
    "similarly , let @xmath35 .",
    "then , in general @xmath36 .",
    "thus , in spite of an efficient estimate of the unknown parameter , the least squares approach does not perform variable selection . in the iid context , introduces a lasso procedure which combines both efficient parameter estimation and variable selection in one step . to achieve this objective in time series regression models as well , we extend the lasso method to our setting .",
    "more precisely , we introduce the adaptive lasso estimator @xmath37 of @xmath25 defined as @xmath38 where @xmath0 is a tuning parameter and @xmath39 . to simplify the presentation of our results ,",
    "we consider only the weights @xmath39 instead of the more general penalizations @xmath40 proposed in , where @xmath41 and @xmath42 is a root-@xmath29 consistent estimator of @xmath43 , for instance the lasso estimate of @xmath25 .",
    "however , with some slight modifications we can extend the results in sections [ oracle ] and [ fin ] also to this more general framework .",
    "it is important to note that the penalization of the variables in ( [ alasso ] ) also depends on the least squares estimate @xmath31 . in particular , variables with least squares",
    "estimates close to zero are more penalized .",
    "this property represents a key condition for ensuring valid variable selection , as highlighted in in the iid context . in the next section ,",
    "we derive the asymptotic properties of the adaptive lasso for time series regression models .",
    "in the iid context , the adaptive lasso procedure introduced in possesses the so - called oracle properties .",
    "more precisely , the adaptive lasso simultaneously performs correct variable selection and provides an efficient estimate of the non - zero coefficients as if only the relevant variables had been included in the model . in the next theorem",
    ", we show that the adaptive lasso enjoys these properties in time series regression models as well . before presenting the main results ,",
    "first we introduce some notation .",
    "let @xmath44 denote the sub - vector of the non - zero coefficients of @xmath25 .",
    "similarly , let @xmath45 and @xmath46 denote the least squares and adaptive lasso estimates of @xmath47 .",
    "furthermore , let @xmath48 be the asymptotic covariance matrix of @xmath49 .",
    "finally , let @xmath50 .",
    "the oracle properties of the adaptive lasso are derived in the next theorem .",
    "[ t1 ] let @xmath2 .",
    "assume that @xmath51 and @xmath52 are stationary processes such that @xmath53 , where @xmath54 is a nonrandom matrix of full rank , and @xmath55 , for some covariance matrix @xmath56 .",
    "if ( i ) @xmath57 and ( ii ) @xmath58 , then :    * variable selection : @xmath59 . * limit distribution : @xmath60 where the bias term is given by @xmath61 @xmath62 is the sub - vector of @xmath63 for the non - zero coefficients , and @xmath64 , @xmath65 .",
    "the assumptions in theorem [ t1 ] are mild conditions that are also required for proving the consistency and deriving the limit distribution of the least squares estimator @xmath31 . statement ( i ) of theorem [ t1 ] establishes that the adaptive lasso performs correct variable selection also in time series settings , i.e. the adaptive lasso asymptotically identifies the sub - vector of the non - zero coefficients of @xmath25 .",
    "furthermore , in statement ( ii ) we derive the limit distribution .",
    "in particular , we note that the adaptive lasso has the same limit distribution of the least squares estimator .",
    "therefore , the adaptive lasso performs variable selection and efficient parameter estimation in one step .    the oracle properties ( i ) and ( ii ) discussed above are in line with the results shown in and , which are derived using substantially different arguments and proofs .",
    "moreover , moving beyond those studies , in statement ( ii ) we also provide an explicit formula for the bias term @xmath66 that is incurred when making inference on the active variables .",
    "this term is asymptotically negligible but provides important refinements for finite sample inference , as highlighted in section [ mc ] below .",
    "in the previous section , we derived the asymptotic properties of the adaptive lasso for time series regression models . in particular , we showed that the limit distribution of the estimators of the non - zero coefficients is normal , while those of the zero coefficients collapse to zero .",
    "this allows us to use these results to introduce inference on the non - zero coefficients .",
    "however , since a priori we do not know the non - zero coefficients @xmath47 , in this context the practical implementation of testing procedures remains unclear . to deepen our understanding of this issue ,",
    "suppose that the estimate of the first component of @xmath25 is different from zero , i.e. , @xmath67 .",
    "then , we have two cases : ( i ) @xmath68 or ( ii ) @xmath69 .",
    "if @xmath68 , then by theorem [ t1 ] the limit distribution of @xmath70 is normal . thus , we can construct gaussian confidence intervals for the parameter of interest . on the other hand ,",
    "if @xmath69 , then by theorem [ t1 ] we can only conclude that @xmath70 must collapse to zero asymptotically .",
    "since a priori we do not know whether ( i ) or ( ii ) is satisfied , it turns out that we also do not know how to make inference on the parameter @xmath71 .",
    "the aim of this section is to clarify how to introduce valid finite sample inference with the adaptive lasso . in particular , we show that the adaptive lasso may combine efficient parameter estimation , variable selection , and valid finite sample inference in one step . to achieve this objective ,",
    "we introduce some notation and terminology in line with . in particular",
    ", first we show that the limit distribution of the adaptive lasso is discontinuous in the tuning parameter .",
    "finally , we prove that with an appropriate selection of the critical values , adaptive lasso tests have correct asymptotic size , where the asymptotic size is the limit of the exact size of the test , as defined in ( [ asy ] ) below .    to this end",
    ", we slightly change our notation .",
    "let @xmath72 be the adaptive lasso estimate of @xmath25 defined in ( [ alasso ] ) , where the tuning parameter @xmath73 is fixed and does not depend on the sample size @xmath29 .",
    "furthermore , for @xmath74 , let @xmath75 denote the limit of @xmath76 , i.e. , @xmath77 as @xmath78 .",
    "note that for the non - zero coefficients , @xmath79 .",
    "then , in the next theorem we derive the limit distribution of @xmath80 .",
    "[ t2 ] let @xmath2 .",
    "assume that @xmath51 and @xmath52 are stationary processes such that @xmath53 , where @xmath54 is a nonrandom matrix of full rank , and @xmath55 , for some covariance matrix @xmath56 .",
    "let @xmath73 .",
    "then , @xmath81 where @xmath82 and @xmath83 .",
    "note that since for the non - zero coefficients @xmath79 , it turns out that in ( [ dist ] ) only the zero coefficients are penalized .",
    "furthermore , when @xmath84 , then for @xmath74 , @xmath85 , and therefore @xmath86 , that is the classic least squares case for the full regression . finally , when @xmath87 , the estimates of the zero coefficients collapse to zero .",
    "suppose that we want to test the null hypothesis @xmath88 versus the alternative @xmath89 , for some @xmath90 and @xmath91 . to this end , consider the adaptive lasso test statistic @xmath92 . in theorem [ t2",
    "] , we establish pointwise convergences that are discontinuous in @xmath93 and depend on the unknown values @xmath94 , @xmath74 .",
    "it turns out that because of the lack of uniformity , the limit distribution in ( [ dist ] ) can provide very poor approximations of the sampling distribution of the test statistic @xmath95 under the null hypothesis ( see e.g. , andrews and guggenberger ( 2010 ) for more details ) . to better evaluate the finite sample properties of @xmath95",
    ", it is necessary to study the asymptotic size of the test statistic .",
    "therefore , following , we introduce the exact size and asymptotic size of @xmath95 as @xmath96 where the parameter space @xmath97 is defined as @xmath98 and @xmath99 is the joint distribution of the stationary regression model ( [ pr ] ) such that @xmath100 , and @xmath101 , @xmath102 denote the critical value of the test , and @xmath103 is the significance level . as pointed out in , the definition of the asymptotic size incorporates uniformity over @xmath104 . therefore , the asymptotic size always ensures a valid approximation of the finite sample size of the test statistic .    using the results in theorem [ t2 ]",
    ", we can show that the adaptive lasso test implies correct asymptotic size . to this end , let @xmath105 denote the @xmath106 quantile of the limit distribution of the test statistic @xmath95 .",
    "for instance , when @xmath84 , then @xmath107 is simply the @xmath106 quantile of the random variable @xmath108 , where @xmath109 , and @xmath110 is the @xmath111-th diagonal term of @xmath32 . using the results in theorem [ t2 ] , we can easily verify that for all @xmath112 , @xmath113 i.e. , the @xmath106 quantile @xmath105 is maximized at @xmath84 . to better understand this point , in figure [ figure1 ] below we plot the @xmath114-quantiles of the distribution of the random variable @xmath115 , where @xmath116 minimizes the function @xmath117 defined in ( [ dist ] ) .",
    "@xmath118{quantiles.pdf } \\end{array}$ ]    given the illustrative goal of the figure , we consider the simple case where @xmath119 and @xmath120 . the horizontal solid line in figure [ figure1",
    "] represents the @xmath114-quantile of the distribution of @xmath115 with @xmath121 , i.e , @xmath122 and @xmath123 .",
    "the dashed line represents instead the @xmath114-quantiles of the random variable @xmath115 for different values of @xmath124 $ ] in equation ( [ dist ] ) .",
    "the figure shows that the quantiles are indeed maximized by @xmath123 .",
    "then , they decrease almost linearly as @xmath125 increases .",
    "finally , when @xmath126 the @xmath114-quantile is practically zero .    the result in ( [ key ] )",
    "represents the key condition for proving the validity of the finite sample inference with the adaptive lasso .",
    "indeed , consider the adaptive lasso test statistic @xmath95 with critical value @xmath127 .",
    "then , using ( [ key ] ) it follows trivially that @xmath128 , i.e. , the adaptive lasso test implies a correct asymptotic size .",
    "this result is summarized in the following corollary .",
    "[ inf ] let @xmath2 .",
    "assume that @xmath51 and @xmath52 are stationary processes such that @xmath53 , where @xmath54 is a nonrandom matrix of full rank , and @xmath55 , for some covariance matrix @xmath56 .",
    "let @xmath73 , and let @xmath72 be the adaptive lasso estimate of @xmath25 .",
    "consider the test statistic @xmath92 , with the critical value @xmath107 .",
    "then , the asymptotic size of the test of the null hypothesis @xmath129 versus the alternative @xmath130 satisfies @xmath131    using the result in corollary [ inf ] inference based on the adaptive lasso is straightforward . to test the null hypothesis @xmath132 at the significance level @xmath103 we can simply use the adaptive lasso test statistic @xmath92 with the normal critical value @xmath107 . as established in ( [ cors ] ) , this test has correct asymptotic size .",
    "this result shows that the adaptive lasso can combine efficient parameter estimation , variable selection , as well as valid finite sample inference in one step .",
    "in this section we use monte carlo simulations to study the accuracy of the inference based on the adaptive lasso .",
    "in particular , we consider five different settings . to satisfy the assumptions in theorem 3.1 and as it is customary in the literature , in the monte carlo experiments we select the tuning parameter @xmath133 $ ] according to the bayesian schwartz information criterion ( bic ) .",
    "[ [ setting-1 ] ] setting 1 : + + + + + + + + + +    @xmath134 and @xmath135 .",
    "we generate @xmath136 samples according to model ( [ pr ] ) with @xmath134 , @xmath137 , @xmath138 , and @xmath139 , for @xmath140 .",
    "we consider gaussian error terms @xmath135 .",
    "furthermore , for @xmath141 and @xmath142 , let @xmath143 and @xmath144 .",
    "the simulated sample sizes are @xmath145 and @xmath146 .    in a first exercise ,",
    "we study the accuracy of the inference for the active variables .",
    "more precisely , using the results in theorem [ t1 ] , we construct @xmath114-confidence intervals for the non - zero coefficients @xmath137 , and @xmath138 .",
    "the empirical coverages are summarized in table [ table1 ] , panel a. in the first part of table [ table1 ] , panel a , we apply the results in theorem [ t1 ] without the bias term @xmath66 .",
    "in contrast , in the bottom part we use instead the bias - corrected limit distribution .",
    "* monte carlo simulations : setting 1 *    panel a : empirical coverages for the active variables    @xmath147    . [ cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]     applying corollary [ inf ] we test the null hypothesis @xmath148 , @xmath149 , ( @xmath150 regressors and the first lagged short rate as predictors ) . as shown in table [ table3 ] , column 2 , the only variables that are significantly different from zero using the adaptive lasso testing procedure are the lagged short rate , the producer price index , and the unemployment rate .",
    "it is interesting to see that there are other variables with adaptive lasso estimates different from zero . without the use of the testing procedure introduced in this study",
    "we would not have been able to classify them as false positives .",
    "this result is not surprising .",
    "indeed , the predictors we find to be statistically significant and to belong to the active set of variables identified by the adaptive lasso procedure are those also commonly thought to be economically relevant in the taylor rule monetary policy model for the short rate . according to this rule",
    ", the central bank sets the nominal short - term interest rate , @xmath151 , based on the following equation    @xmath152    where @xmath153 denote inflation , @xmath154 is the output gap , and @xmath155 is a sequence of independent and normally distributed innovations with mean zero and variance @xmath156 .",
    "thus , our result adds a purely statistical foundation ( from the viewpoint of variable choice in the regression ) to this economically intuitive rule .",
    "moreover , the sign and ( partially ) the magnitude of the coefficients of the active variables are in line with the literature , that is a positive relation between inflation and the short rate , a negative relation between unemployment and the short rate , and a high persistence of the short rate dynamics .",
    "it is important to highlight that this result would not have been possible without the theory for testing null hypotheses of the type @xmath129 versus the alternative @xmath130 , for some @xmath90 , developed in this study .",
    "in fact we would have found more active variables using both the adaptive lasso and the classical full least squares estimates ( whose results are summarized in column 4 of table [ table3 ] ) , completely losing the economic intuition behind the taylor rule and rendering the interpretation of the results very difficult .",
    "we presented new theoretical and empirical results on the finite sample and asymptotic properties of the adaptive lasso in time series regression models .",
    "we extended previous results presented in the literature along two main lines : ( i ) computing analytically a bias correction term for doing finite sample inference on the active variables in the adaptive lasso , and ( ii ) introducing a simple , conservative , but effective testing procedure for the null hypothesis that a parameter is equal to zero in the adaptive lasso model with a fixed amount of shrinkage .    through extensive monte carlo simulations with a changing number of candidate variables , different error distributions , different sample sizes , and different correlation structures among the covariates",
    ", we showed the accuracy of the testing procedure in finite sample .",
    "moreover , testing our procedure in a more involved simulation experiment where we relaxed the assumption of iid errors , we also empirically confirmed the theoretical results and showed that the methodology is robust against this kind of deviation from the standard setting .",
    "this result is not surprising and confirms the recent findings and discussions in and .",
    "finally , we investigated the implications of the new testing procedure in an empirical application concerning the relation between the short - term interest rate dynamics and the ( macro)economy . to this end , we considered a taylor rule monetary policy model , where we let the adaptive lasso choose from a number of macroeconomic and financial predictors the relevant ones to put in the active set .",
    "we then tested using the new procedure to see whether all remaining active variables had a corresponding coefficient significantly different from zero .",
    "in contrast with the full least squares approach on all variables , the only variables with a coefficient different than zero identified by our testing procedure were an inflation indicator , the unemployment rate , and the one - lagged past short rate .",
    "we interpreted this result as a statistical confirmation of the taylor rule .",
    "our theoretical results are general and can be applied to a broad spectrum of iid and time series applications , in particular when the researcher has to do variable selection and inference among many candidate variables .",
    "classic examples are realized volatility modeling , excess returns or inflation prediction .",
    "moreover , in light of the theoretical results proved in this study , an alternative way of conducting finite sample inference can be envisaged . in the spirit of the recent works proposed by and , we plan to develop bootstrap simulation techniques that can be applied to the ( adaptive ) lasso to perform finite sample testing of the resulting parameters .",
    "finally , we plan to investigate whether the theory we introduced can be generalized to the case where the number of variables is increasing with the sample size and/or applications dealing with more variables than observations .",
    "this is left for future research .",
    "99    andrews , w.k . , and guggenberger , p. ( 2010 ) .",
    "asymptotic size and a problem with subsampling and with the @xmath157 out of @xmath29 bootstrap .",
    "_ econometric theory _ , * 26 * , 426 - 468 .",
    "ang , a. , beckaert , g. , and wei , w. ( 2008 ) .",
    "the term structure of real rates and expected inflation .",
    "_ journal of finance _ , * 64 * , 797 - 849 .",
    "ang , a. , and piazzesi , m. ( 2003 ) .",
    "a no - arbitrage vector autoregression of term structure dynamics with macroeconomic and latent variables .",
    "_ journal of monetary economics _ , * 50 * , 745 - 787 .",
    "audrino , f. , and knaus , s. ( 2012 ) . lassoing the har model : a model selection perspective on realized volatility dynamics .",
    "university of st .",
    "gallen , seps discussion paper series .",
    "bhlmann , p. , and van de geer , s. ( 2011 ) .",
    "_ statistics for high - dimensional data : methods , theory and applications_. springer series in statistics , springer , heidelberg .",
    "chatterjee , a. , and lahiri , s.n .",
    "bootstrapping lasso estimators",
    ". _ journal of the american statistical association _ , * 106 * , 608 - 625 .",
    "chatterjee , a. , and lahiri , s.n .",
    "( 2013 ) . rates of convergence of the adaptive lasso estimators to the oracle distribution and higher order refinements by the bootstrap . _",
    "annals of statistics _ , * 41 * , 1232 - 1259 .",
    "clarida , r. , gali , j. , and gertler , m. ( 2000 ) . monetary policy rules and macroeconomic stability :",
    "evidence and some theory . _ the quarterly journal of economics _ , * 115 * , 147 - 180 .",
    "dewachter , h. , and lyrio , m. ( 2006 ) .",
    "macro factors and term structure of interest rates .",
    "_ journal of money , credit and banking _ , * 38 * , 119 - 140",
    ".    fan , j. , and li , r. ( 2001 ) .",
    "variable selection via nonconcave penalized likelihood and its oracle properties .",
    "_ journal of the american statistical association _ , * 96 * , 1348 - 1360 .    fan , j. , and h. peng ( 2004 ) . on nonconcave penalized likelihood with diverging number of parameters .",
    "_ the annals of statistics _ , * 32 * , 928 - 961 .",
    "filipova , k. , audrino , f. , and de giorgi , e. ( 2013 ) .",
    "monetary policy regimes : implications for the yield curve and bond pricing . _ journal of financial economics _ , forthcoming .",
    "geyer , c. ( 1994 ) .",
    "on the asymptotics of constrained m - estimation .",
    "_ the annals of statistics _ , * 22 * , 1993 - 2010 .",
    "hsu , n. , hung , h. and chang , y. ( 2008 ) .",
    "subset selection for vector autoregressive processes using lasso .",
    "_ computational statistics & data analysis _ , * 52 * , 3645 - 3657 .",
    "knight , k. , and fu , w. ( 2000 ) .",
    "asymptotics for lasso - type estimators . _",
    "the annals of statistics _",
    ", * 28 * , 1356 - 1378 .",
    "kock , a.b .",
    "( 2012 ) . on the oracle property of the adaptive lasso in stationary and nonstationary autoregressions .",
    "creates research papers 2012 - 05 , aarhus university .",
    "kock , a.b . , and callot , l.a.f .",
    "oracle inequalities for high dimensional vector autoregressions .",
    "creates research paper 2012 - 12 , aarhus university .",
    "javanmard , a. and montanari , a. ( 2013 ) .",
    "hypothesis testing in high - dimensional regression under the gaussian random design model : asymptotic theory .",
    "arxiv e - prints .",
    "lehmann , e. , and romano , j. ( 2005 ) .",
    "_ testing statistical hypotheses_. springer verlag .",
    "lockhart , t. taylor , j. , tibshirani r.j . , and tibshirani r. ( 2013 ) . a significance test for the lasso .",
    "arxiv e - prints .",
    "medeiros , m.c . , and mendes , e.f .",
    "( 2012 ) . estimating high - dimensional time series models .",
    "creates research paper 2012 - 37 , aarhus university .",
    "moench , e. ( 2008 ) .",
    "forecasting the yield curve in a data - rich environment : a no - arbitrage factor - augmented var approach .",
    "_ journal of econometrics _",
    ", * 146 * , 26 - 43 .",
    "nardi , y. , and rinaldo , a. ( 2011 ) .",
    "autoregressive process modeling via the lasso procedure .",
    "_ journal of multivariate analysis _ , * 102 * , 528 - 549 .    park , h. , and sakaori , f. ( 2012 ) .",
    "lag weighted lasso for time series model . _ computational statistics _ , forthcoming .",
    "rudebusch , g. ( 2010 ) .",
    "macro - finance models of interest rates and the economy . _ the manchester school _ , * 7 * , 25 - 52 .    song , s. , and bickel , p.j .",
    "large vector autoregressions ( 2011 ) .",
    "arxiv e - prints .",
    "taylor , j.b .",
    "discretion versus policy rules in practice . _ carnegie - rochester conference series on public policy _ , * 39 * , 195 - 214 .",
    "tibshirani , r. ( 1996 ) .",
    "regression analysis and selection via the lasso .",
    "_ journal of the royal statistical society , series b _ , * 58 * , 267 - 288 .",
    "wang , h. , li , g. and tsai , c. ( 2007 ) .",
    "regression coefficient and autoregressive order shrinkage and selection via the lasso .",
    "_ journal of the royal statistical society , series b _ , * 69 * , 63 - 78 .",
    "zhao , p. and yu , b. ( 2006 ) . on model consistency of lasso .",
    "_ journal of machine learning research _ * 7 * , 2541 - 2563 .",
    "zou , h. ( 2006 ) .",
    "the adaptive lasso and its oracle properties .",
    "_ journal of the american statistical association _ ,",
    "* 101 * , 1418 - 1429 .",
    "* proof of theorem [ t1 ] : * first we derive the limit distribution of the adaptive lasso . in particular , we adopt the same argument as in the proof of theorem 2 in .",
    "finally , we use this result to prove ( i ) variable selection and to compute the bias term in ( ii ) limit distribution .    let @xmath158+\\lambda_n\\sum_{i=1}^{p } \\lambda_{n , i}\\left [ \\vert \\theta_i^{\\ast}+u_i/\\sqrt{n}\\vert -\\vert \\theta_i^{\\ast}\\vert\\right].\\ ] ] note that @xmath159 is minimized at @xmath160 .",
    "furthermore , we know that @xmath161 \\to_d -2u'w+u'cu,\\ ] ] where @xmath83",
    ". now , consider the limit of the second term @xmath162 $ ] .",
    "if @xmath163 , then @xmath164 , and consequently @xmath165\\to 0 $ ] .",
    "if @xmath166 , then @xmath167 , and furthermore @xmath168 , where @xmath169 .",
    "let @xmath170 denote the limit of @xmath171 .",
    "then , we can conclude that @xmath172 where @xmath173 and @xmath174 is the sub - matrix of @xmath56 for the non - zero coefficients .",
    "note that @xmath159 is convex , and the unique minimum of @xmath117 is @xmath175 .",
    "therefore , by it follows that @xmath176 where @xmath177 denote the adaptive lasso estimate of the zero coefficients @xmath178 of @xmath25 .    using this result , we can prove ( i ) variable selection .",
    "we adopt the same argument as in the proof of lemma 5 in .",
    "let @xmath179 with some abuse of notation , we write @xmath180 .",
    "we show that with probability tending to @xmath181 , for any @xmath182 satisfying @xmath183 and any constant @xmath54 , @xmath184 to this end , for @xmath185 consider @xmath186 where @xmath187 denote the @xmath188-component of the vector @xmath63 .",
    "note that @xmath189 , while the dominant term is @xmath190 , since ( i ) @xmath57 and ( ii ) @xmath58 .",
    "thus , the sign of @xmath191 determines the sign of @xmath192 .",
    "more precisely , we have @xmath193 this concludes the proof of ( i ) variable selection .    finally ,",
    "using these results , we can focus on ( ii ) limit distribution and also derive the bias term .",
    "note that for @xmath29 large enough , for @xmath194 we have @xmath195 furthermore , for @xmath29 large enough @xmath196 for @xmath185 .",
    "thus , we can rewrite the @xmath197 equations ( [ foc ] ) in matrix form @xmath198 where @xmath199 . now consider the term @xmath200 . a taylor expansion around @xmath25 yields @xmath201 again , since @xmath202 for @xmath185 and @xmath29 large enough , from ( [ taylor ] ) it turns out that @xmath203 therefore , by combining ( [ focm ] ) and ( [ taylorq ] ) we have @xmath204 i.e. , @xmath205 since @xmath58 , it turns out that for @xmath194 , @xmath206 .",
    "therefore , @xmath60 where the bias term is given by @xmath207 this concludes the proof .",
    "* proof of theorem [ t2 ] : * to prove theorem [ t2 ] , we use the same arguments as in the proof of theorem 2 in . more precisely , let @xmath158+\\lambda_n\\sum_{i=1}^{p } \\lambda_{n , i}\\left [ \\vert \\theta_i^{\\ast}+u_i/\\sqrt{n}\\vert -\\vert \\theta_i^{\\ast}\\vert\\right].\\ ] ] note that @xmath159 is minimized at @xmath208 .",
    "furthermore , @xmath209 & \\to_d & -2u'w+u'cu,\\\\ \\lambda_n\\sum_{i=1}^{p+r } \\lambda_{n , i}\\left [ \\vert \\theta_i^{\\ast}+u_i/\\sqrt{n}\\vert -\\vert \\theta_i^{\\ast}\\vert\\right ] & \\to & \\sum_{i=1}^{p}\\lambda_{0,i}\\vert u_i\\vert.\\end{aligned}\\ ] ] thus , @xmath210 as @xmath78 .",
    "since @xmath159 is convex and @xmath117 has a unique minimum , it follows from that @xmath211 this concludes the proof ."
  ],
  "abstract_text": [
    "<S> we derive new theoretical results on the properties of the adaptive least absolute shrinkage and selection operator ( adaptive lasso ) for time series regression models . </S>",
    "<S> in particular , we investigate the question of how to conduct finite sample inference on the parameters given an adaptive lasso model for some fixed value of the shrinkage parameter . </S>",
    "<S> central in this study is the test of the hypothesis that a given adaptive lasso parameter equals zero , which therefore tests for a false positive . to this end </S>",
    "<S> we construct a simple testing procedure and show , theoretically and empirically through extensive monte carlo simulations , that the adaptive lasso combines efficient parameter estimation , variable selection , and valid finite sample inference in one step . </S>",
    "<S> moreover , we analytically derive a bias correction factor that is able to significantly improve the empirical coverage of the test on the active variables . finally , we apply the introduced testing procedure to investigate the relation between the short rate dynamics and the economy , thereby providing a statistical foundation ( from a model choice perspective ) to the classic taylor rule monetary policy model . + </S>",
    "<S> * jel classifications * : c12 ; c22 ; e43 . </S>",
    "<S> + * keywords * : adaptive lasso ; time series ; oracle properties ; finite sample inference ; taylor rule monetary policy model . </S>"
  ]
}