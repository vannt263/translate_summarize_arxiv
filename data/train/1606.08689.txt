{
  "article_text": [
    "text documents coming in a sequence are common in real - world applications and can arise in various contexts . for example , consider web pages surfed by users in random walks along the hyperlinks , streams of click - through urls associated with a query in search engine , publications of an author in chronological order , threaded posts in online discussion forums , answers to a question in online knowledge - sharing communities , or e - mails in a common thread , to name a few .",
    "the co - occurrences of documents in a temporal sequence may reveal relatedness between them , such as their semantic and topical similarity .",
    "in addition , sequences of words within the documents introduce another rich and complex source of the data , which can be leveraged together with the document stream information to learn useful and insightful representations of both documents and words .    in this paper , we introduce algorithm that can simultaneously model documents from a stream and their residing natural language in one common lower - dimensional vector space .",
    "such algorithm goes beyond representations which consider each document as a separate bag - of - words composition , most notably probabilistic latent semantic analysis ( plsa ) @xcite and latent dirichlet allocation ( lda ) @xcite .",
    "we focus on learning continuous representations of documents in vector space jointly with distributed word representations using statistical neural language models @xcite , whose success was previously shown in a number of publications @xcite . more precisely , we propose hierarchical models where document vectors act as units in a context of document sequences , and also as global contexts of word sequences contained within them .",
    "the probability distribution of observing a word depends not only on some fixed number of surrounding words , but is also conditioned on the specific document",
    ". meanwhile , the probability distribution of a document depends on the surrounding documents in stream data .",
    "our work is most similar to @xcite as it also considers document vectors as a global context for words contained within , however the authors in @xcite do not model the document relationships which brings significant modeling strength to our models and opens a plethora of application areas for the neural language models . in our work ,",
    "the models are trained to predict words and documents in a sequence with maximum likelihood .",
    "we optimize the models using stochastic gradient learning , a flexible and powerful optimization framework suitable for the considered large - scale problems in an online setting where new samples arrive sequentially .",
    "vector representations of documents and words learned by our model are useful for various applications of critical importance to online businesses .",
    "for example , by means of simple nearest - neighbor searches in the joint vector space between document and word representations , we can address a number of important tasks : 1 ) given a query keyword , search for similar keywords to expand the query ( useful in the search product ) ; 2 ) given a keyword , search for relevant documents such as news stories ( useful in document retrieval ) ; 3 ) given a document , retrieve similar or related documents ( useful for news stream personalization and document recommendation ) ; and 4 ) automatically generate related words to tag or summarize a given document ( useful in native advertising or document retrieval ) .",
    "all these tasks are essential elements of a number of online applications , including online search , advertising , and personalized recommendation .",
    "in addition , as we show in the experimental section , learned document representations can be used to obtain state - of - the - art document classification results .",
    "the proposed approach represents a step towards automatic organization , semantic analysis , and summarization of documents observed in sequences .",
    "we summarize our main contributions below :    * we propose hierarchical neural language model which takes advantage of the context of document sequences and the context of each word within a document to learn their low - dimensional vector representations .",
    "the document contexts can act as an empirical prior which helps learn smoothed representations for documents .",
    "this is useful for learning representations of short documents with a few words , for which @xcite tends to learn poor document representations as separately learned document vectors may overfit to a few words within a specific document .",
    "conversely , the proposed model is not dependent on the document content as much .",
    "* the proposed approach can capture inherent connections between documents in the data stream .",
    "we tailor our models to analyze movie reviews where a user s preferences may be biased towards particular genres , as well as yahoo news articles for which we collect click - through logs of a large number of users and learn useful news article representations .",
    "the experimental results on retrieval and categorization tasks demonstrate effectiveness of the proposed model . *",
    "our learning framework is flexible and it is straightforward to add more layers in order to learn additional representations for related concepts .",
    "we propose extensions of the model and discuss how to learn explicit distributed representations of users on top of the basic framework .",
    "the extensions can be applied to personalized recommendation and social relationship mining .",
    "the related work is largely centered on the notion of neural language models @xcite , which improve generalization of the classic @xmath0-gram language models by using continuous variables to represent words in a vector space .",
    "this idea of distributed word representations has spurred many successful applications in natural language processing .",
    "more recently , the concept of distributed representations has been extended beyond pure language words to a number of applications , including modeling of phrases @xcite , sentences and paragraphs @xcite , relational entities @xcite , general text - based attributes @xcite , descriptive text of images @xcite , online search sessions @xcite , smartphone apps @xcite , and nodes in a network @xcite .",
    "neural language models take advantage of a word order , and state the same assumption as @xmath0-gram models that words closer in a sequence are statistically more dependent .",
    "typically , a neural language model learns the probability distribution of next word given a fixed number of preceding words which act as the context .",
    "more formally , given a word sequence @xmath1 in a training data , the objective of the model is to maximize log - likelihood of the sequence , @xmath2 where @xmath3 is the @xmath4 word in the sequence , and @xmath5 represents a sequence of successive preceding words @xmath6 that act as the context to the word @xmath3 .",
    "a typical model architecture to approximate probability distribution @xmath7 is to use a neural network @xcite .",
    "the neural network is trained by projecting the concatenation of vectors for context words @xmath6 into a latent representation with multiple non - linear hidden layers and the output soft - max layer comprising @xmath8 nodes , where @xmath8 is a size of the vocabulary , where the network attempts to predict @xmath3 with high probability . however , a neural network of large size is challenging to train , and the word vectors are computationally expensive to learn from large - scale data sets comprising millions of words , which are commonly found in practice .",
    "recent approaches with different versions of log - bilinear models @xcite or log - linear models @xcite attempt to modify the model architecture in order to reduce the computational complexity .",
    "the use of hierarchical soft - max @xcite or noise contrastive estimation @xcite can also help speed up the training complexity . in the following we review two recent neural language models",
    "@xcite which directly motivated our work , namely continuous bag - of - words ( cbow ) and skip - gram ( sg ) model .",
    "the continuous bag - of - words model is a simplified neural language model without any non - linear hidden layers .",
    "a log - linear classifier is used to predict a current word based on its preceding and succeeding words , where their representations are averaged as the input .",
    "more precisely , the objective of the cbow model is to maximize the log - likelihood , @xmath9 where @xmath10 is the context length , and @xmath11 is the sub - sequence @xmath12 excluding @xmath3 itself .",
    "the probability @xmath13 is defined using the softmax , @xmath14 where @xmath15 is the output vector representation of @xmath3 , and @xmath16 is averaged vector representation of the context , found as @xmath17 here @xmath18 is an input vector representation of word @xmath19 .",
    "it is worth noting that cbow takes only partial advantage of the word order , since any ordering of a set of contextual words will result in the same vector representation .      instead of predicting the current word based on the words before and after it",
    ", the skip - gram model tries to predict the surrounding words within a certain distance based on the current one .",
    "more formally , skip - gram defines the objective function as the exact counterpart to the continuous bag - of - words model , @xmath20 furthermore , the model simplifies the probability distribution , introducing an assumption that the contextual words @xmath11 are independent given current word @xmath3 , @xmath21 with @xmath22 defined as @xmath23 where @xmath18 and @xmath24 are the input and output vectors of @xmath19 . increasing the range of context @xmath10",
    "would generally improve the quality of learned word vectors , albeit at the expense of higher computation cost .",
    "skip - gram assumes that the surrounding words are equally important , and in this sense the word order is again not fully exploited , similarly to the cbow model . a wiser strategy to account for",
    "this issue would be to sample training word pairs in less from relatively distant words that appear in the context @xcite .",
    "in this section we present our algorithm for joint modeling of streaming documents and the words contained within , where we learn distributed representations for both the documents and the words in a shared , low - dimensional embedding space .",
    "the approach is inspired by recently proposed methods for learning vector representations of words which take advantage of a word order observed in a sentence @xcite . however , and unlike similar work presented by the authors of @xcite , we also exploit temporal neighborhood of the streaming documents . in particular , we model the probability of observing a particular document by taking into account its temporally - close documents , in addition to conditioning on the content of the document .      in the considered problem setting , we assume the training documents are given in a sequence .",
    "for example , if the documents are news articles , a document sequence can be a sequence of news articles sorted in a chronological order in which the user read them . more specifically",
    ", we assume that we are given a set @xmath25 of @xmath26 document sequences , with each sequence @xmath27 consisting of @xmath28 documents , @xmath29 . moreover , each document in a stream @xmath30 is a sequence of @xmath31 words , @xmath32 .",
    "we aim to simultaneously learn low - dimensional representations of streaming documents and language words in a common vector space , and represent each document and word as a continuous feature vector of dimensionality @xmath33 .",
    "if we assume that there are @xmath34 unique documents in the training corpus and @xmath8 unique words in the vocabulary , then during training we aim to learn @xmath35 model parameters .",
    "the context of a document sequence and the natural language context are modeled using the proposed hierarchical neural language model , where document vectors act not only as the units to predict their surrounding documents , but also as the global context of word sequences contained within them .",
    "the architecture consists of two embedded layers , shown in figure  [ fig : architecture ] .",
    "the upper layer learns the temporal context of a document sequence , based on the assumption that temporally closer documents in the document stream are statistically more dependent .",
    "we note that temporally does not need to refer to the time of creation of the document , but also to the time the document was read by the user , which is the definition we use in our experiments .",
    "the bottom layer models the contextual information of word sequences .",
    "lastly , we connect these two layers by adopting the idea of paragraph vectors @xcite , and consider each document token as a global context for all words within the document .    more formally , given set @xmath25 of sequences of documents , the objective of the hierarchical model is to maximize log - likelihood of the streaming data , @xmath36 where @xmath37 is the weight that specifies a trade - off between focusing on minimization of the log - likelihood of document sequence and of the log - likelihood of word sequences ( we set @xmath38 in the experiments ) , @xmath39 is the length of the training context for document sequences , and @xmath10 is the length of the training context for word sequences . in this particular architecture",
    ", we are using the cbow model in the lower , sentence layer , and the skip - gram model in the upper , document layer .",
    "however , we note that either of the two models can be used in any level of the hierarchy , and a specific choice depends on the modalities of the problem at hand .    given the architecture illustrated in figure [ fig : architecture ] , probability of observing one of the surrounding documents based on the current document @xmath40 is defined using the soft - max function as given below , @xmath41 where @xmath42 and @xmath43 are the input and output vector representations of document @xmath44 .",
    "furthermore , probability of observing a word depends not only on its surrounding words , but also on a specific document that the word belongs to .",
    "more precisely , probability @xmath45 is defined as @xmath46 where @xmath47 is the output vector representation of @xmath48 , and @xmath16 is the averaged vector representation of the context ( including the specific document @xmath30 ) , defined as follows , @xmath49",
    "lastly , we define probability of observing a document given the words contained within @xmath50 in a similar way , by reusing equations and and replacing the appropriate variables .      in the previous section we presented a typical architecture where we specified language models in each layer of the hierarchy",
    "however , in real - world applications , we could vary the language models for different purposes in a straightforward manner . for example",
    ", a news website would be interested in predicting on - the - fly which news article a user would read after a few clicks on some other news stories , in order to personalize the news feed .",
    "then , it could be more reasonable to use directed , feed - forward models which estimate @xmath51 , probability of the @xmath52 document in the sequence given its @xmath39 preceding documents . or ,",
    "equivalently , if we want to model which documents were read prior to the currently observed sequence , we can use feed - backward models which estimate @xmath53 , probability of the @xmath52 document given its @xmath39 succeeding documents .",
    "moreover , it is straightforward to extend the described model for problems that naturally have more than two layers",
    ". let us assume that we have a data set with streaming documents specific to a different set of users ( e.g. , for each document we also know which user generated or read the document ) .",
    "then , we may build more complex models to simultaneously learn distributed representations of users by adding additional user layer on top of the document layer . in this setup a user vector could serve as a global context of streaming documents pertaining to that specific user , much like a document vector serves as a global context to words pertaining to that specific document . more specifically , we would predict a document based on the surrounding documents and its content , while also conditioning on a specific user .",
    "this variant models @xmath54 , where @xmath55 is an indicator variable for a user .",
    "learning vector representations of users would open doors for further improvement of personalized services , where personalization would reduce to simple nearest - neighbor search in the joint embedding space .",
    "the model is optimized using stochastic gradient ascent , which is very suitable for large - scale problems .",
    "however , computation of gradient @xmath56 in is proportional to the vocabulary size @xmath8 , and of gradients @xmath57 and @xmath58 is proportional to the number of training documents @xmath34 .",
    "this is computationally expensive in practice , since both @xmath8 and @xmath34 could easily reach millions in our applications .    [",
    "cols=\"<,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     an efficient alternative that we use is hierarchical soft - max @xcite , which reduces the time complexity to @xmath59 in our case , where @xmath60 is the total number of words in the document sequence . instead of evaluating every distinct word or document during each gradient step in order to compute the sums in equations and , hierarchical",
    "softmax uses two binary trees , one with distinct documents as leaves and the other with distinct words as leaves . for each leaf node",
    ", there is a unique assigned path from the root which is encoded using binary digits . to construct a tree structure the huffman encoding",
    "is typically used , where more frequent documents ( or words ) in the data set have shorter codes to speed up the training .",
    "the internal tree nodes are represented as real - valued vectors , of the same dimensionality as word and document vectors .",
    "more precisely , hierarchical soft - max expresses the probability of observing the current document ( or word ) in the sequence as a product of probabilities of the binary decisions specified by the huffman code of the document as @xmath61 where @xmath62 is the @xmath63 bit in the code with respect to @xmath64 , which is the @xmath63 node in the specified tree path of @xmath65 .",
    "the probability of each binary decision is defined as follows , @xmath66 where @xmath67 is the sigmoid function , and @xmath68 is the vector representation of node @xmath64 . it can be verified that @xmath69 , and hence the property of probability distribution is preserved .",
    "we can compute @xmath50 in the same manner .",
    "furthermore , we similarly express @xmath70 , but with construction of a separate huffman tree pertaining to the words .",
    "in this section we describe experimental evaluation of the proposed approach , which we refer to as _ hierarchical document vector _ ( hdv ) model .",
    "first , we validated the learned representations on a public movie ratings data set , where the task was to classify movies into different genres .",
    "then , we used a large - scale data set of user click - through logs on a news stream collected on yahoo servers to showcase a wide application potential of the hdv algorithm . in all experiments we used cosine distance to measure the closeness of two vectors ( either document or word vectors ) in the common low - dimensional embedding space .",
    "in the first set of experiments we validated quality of the obtained distributed document representations on a classification task using a publicly available data set .",
    "we note that , although we had access to a proprietary data set discussed in section [ sect : exp2 ] which served as our initial motivation to develop the hdv model , obtaining a similar public data set proved to be much more difficult . to this end",
    ", we generated such data by combining a public movie ratings data set movielens 10 m , consisting of movie ratings for around @xmath71 movies generated by more than @xmath72 users , with a movie synopses data set from internet movie database ( imdb ) that was found online .",
    "each movie in the data set is tagged as belonging to one or more genres , such as  action \" ,  comedy \" , or  horror \" .",
    "then , following terminology from the earlier sections , we viewed movies as  documents \" and synopses as  document content \" .",
    "the document streams were obtained by taking for each user the movies that they rated @xmath73 and above ( on the scale from 1 to 5 ) , and ordering them in a sequence according to a timestamp of the rating .",
    "the described preprocessing resulted in @xmath74 document sequences comprising @xmath75 distinct movies , with an average synopsis length of @xmath76 words .",
    "let us discuss several explicit assumptions that we made while generating the movie data set .",
    "first , we retained only high - rated movies for each user in order to make the data less noisy , as the assumption is that the users are more likely to enjoy two movies that belonged to the same genre , than two movies coming from two different genres . thus , by removing low - rated movies we aim to keep only similar movies in a single user s sequence . as we show in the remainder of the section ,",
    "the experimental results indicate that the assumption holds true .",
    "in addition , we used the timestamp of a rating as a proxy for a time when the movie was actually watched .",
    "although this might not always hold in reality , the empirical results suggest that the assumption was reasonable for learning useful movie and word embeddings .",
    "we learned movie vector representations for the described data set using the following methods : 1 ) lda @xcite , which learns low - dimensional representations of documents ( i.e. , movies ) as a topic distribution over their synopses ; 2 ) paragraph vector ( paragraph2vec ) @xcite , where an entire movie synopsis is taken as a single paragraph ; 3 ) word2vec @xcite , where movie sequences are used as  sentences \" and movies are used as  words \" ; and 4 ) the proposed hdv method .",
    "we used publicly available implementations of online lda training @xcitevw/ , accessed march 2015 ] and word2vec , and used our implementations of paragraph2vec and hdv algorithms . note that lda and paragraph2vec take into account only the content of the documents ( i.e. , movie synopses ) ,",
    "word2vec only considers the movie sequences and does not consider the movie synopses and contained natural language in any way , while hdv combines the two approaches and jointly considers and models both the movie sequences and the content of movie synopses .",
    "dimensionality of the embedding space was set to @xmath77 for all low - dimensional embedding methods ( in the case of lda this amounts to using @xmath78 topics ) .",
    "the neighborhood of the neural language methods was set to @xmath79 for both document and word sequences , and the models were trained for @xmath79 iterations .",
    "once we obtained the document vectors using the abovementioned methods , we used linear support vector machine ( svm ) @xcite to predict a movie genre .",
    "note that we chose a linear classifier , instead of some more powerful non - linear one , in order to reduce the effect of variance of non - linear methods on the classification performance and help with the interpretation of the results .",
    "the classification results after 5-fold cross - validation are shown in table [ tbl : movie ] , where we report results on eight binary one - vs - rest classification tasks for eight most frequent movie genres in the data set .",
    "we can see that the neural language models on average obtained higher accuracy than lda , although lda achieved very competitive results on the last six tasks .",
    "it is interesting to observe that the word2vec algorithm obtained higher accuracy than paragraph2vec despite the fact that word2vec only considered sequences in which the movies were seen without using the movie synopses , and that , unlike word2vec , the paragraph2vec model was specifically designed for document representation .",
    "this result indicates that the users have strong genre preferences that exist in the movie sequences which was utilized by word2vec , validating our assumption discussed earlier .",
    "furthermore , we see that the proposed approach achieved higher accuracy than the competing state - of - the - art methods , obtaining on average @xmath80 better performance over the paragraph2vec and @xmath81 over the word2vec model . this can be explained by the fact that the hdv method successfully exploited both the document content and the relationships in a stream between them , resulting in improved performance .      in this section",
    "we evaluate the proposed algorithm a large - scale data set collected on yahoo servers . the data consists of nearly @xmath82 distinct news stories , viewed by a subset of company s users from march to june , 2014 .",
    "after pre - processing where the stopwords were removed , we trained the hdv model on more than @xmath83 million document sequences generated by users , containing a total of @xmath78 million words with a vocabulary size of @xmath84 . considering that the used data set is proprietary and that numerical results may carry business - sensitive information , we first illustrate a wide potential of the proposed method for numerous online applications using qualitative experiments , followed by the document classification task where we show relative performance improvements over the baseline method .      an important task in many online applications is finding similar or related words given an input word as a query",
    "this is useful in the setting of , for example , search retargeting @xcite , where advertisers bid on search keywords related to their product or service and may use the proposed model to expand the list of targeted keywords with additional relevant keywords .",
    "table [ tb : word - word ] shows example keywords from the vocabulary , together with their nearest word neighbors in the embedding space .",
    "clearly , meaningful semantic relationships and associations can be observed within the closest distance of the input keywords . for example , for the query word  batman \" , the model found that other superheroes such as  superman \" and  avengers \" are related , and also found keywords related to comics in general , such as  comics \" ,  marvel \" , or  sequel \" .",
    "given a query word , one may be interested in finding the most relevant documents , which is a typical task of an online search engine or news webservice perform .",
    "we consider the keywords used in the previous section , and find the titles of the closest document vectors in the common embedding space .",
    "as can be seen in table [ tb : word - document ] , the retrieved documents are semantically related to the input keyword .",
    "interestingly , in some cases it might seem that the document is irrelevant , as , for example , in the case of keyword  university \" and headlines  spring storm brings blizzard warning for cape cod \" and  no friday night lights at $ 60 million texas stadium \" .",
    "however , after closer inspection and a search for the headlines in a popular search engine , we can see that the snow storm from the first headline affected school operations and the article includes a comment by an affected student .",
    "similar search also confirmed that the second article discussed school facilities and an education fund .",
    "although the titles may be misleading , we can see that the both articles are of interest to users interested in the keyword  university \" , as our model correctly learned from the actual user sessions .",
    "we note that the proposed approach differs from the traditional information retrieval methods due to the fact that retrieved documents do not necessarily need to contain the query word , as examplified in table [ tb : word - document ] in a case of the keyword  boxing \" .",
    "as we can see , the hdv method found that the articles discussing ufc ( ultimate fighting championship ) and wsof ( world series of fighting ) events are related to the sport , despite the fact that neither of them specifically mentions the word  boxing \" .",
    "an important task in online news services is automatic document tagging , where , given a news article , we assign a list of relevant keywords ( called _ tags _ ) that explain the article content .",
    "tagging is very useful in improving the document retrieval systems , document summarization , document recommendation , contextual advertising ( tags can be used to match display ads shown alongside the article ) , and many other applications .",
    "our model is suitable for such tasks due to the fact that document and word vectors reside in the same feature space , which allows us to reduce complex task of document tagging to a trivial @xmath85-nearest - neighbor search in the joint embedding space .    using the trained model",
    ", we retrieved the nearest words given a news story as an input . in table",
    "[ tb : document - word ] we give the results , where titles of the example news stories are shown together with their lists of nearest words .",
    "we can see that the retrieved keywords often summarize and further explain the documents .",
    "for example , for the news article  this year s best buy isas \" , related to individual savings account ( isa ) , the keywords include  pensioners \" and  taxfree \" , while in the mortgage - related example (  uncle sam buying mortgages ? who knew ? \" ) keywords include several financial companies and advisors ( e.g. , nationstar , moelis , berkowitz ) .      in the document recommendation task , we search for the nearest news articles given a news story .",
    "the retrieved articles can be provided as a reading recommendations for users viewing the query news story .",
    "we give the examples in table  [ tb : document - document ] , where we can see that relevant and semantically related documents are located nearby in the latent vector space .",
    "for example , we can see that the nearest neighbors for article related to the 2014 ukraine crisis are other news stories discussing the political situation in ukraine , while for the article focusing on samsung galaxy s5 all nearest documents are related to the smartphone industry .",
    "lastly , we used the learned representations to label news documents with @xmath86 first - level topic tags from the company s internal interest taxonomy ( e.g. , taxonomy includes  home & garden \" and  science \" tags ) .",
    "we used linear svm to predict each topic separately , and the average improvement over lda after 5-fold cross - validation is given in table [ tbl : news_doc ] .",
    "we see that the proposed method outperformed the competition on this large - scale problem , strongly confirming the benefits of hdv for representation of streaming documents .",
    "we described a general unsupervised learning framework to model the latent structure of streaming documents , that learns low - dimensional vectors to represent documents and words in the same latent space .",
    "our model exploits the context of documents in streams and learns representations that can capture temporal co - occurrences of documents and statistical patterns of the words contained within .",
    "the method was verified on a movie classification task , outperforming the current state - of - the - art by a large margin .",
    "moreover , experiments on a large - scale data set comprising click - through logs on yahoo news demonstrated effectiveness and wide applicability of the proposed neural language approach ."
  ],
  "abstract_text": [
    "<S> we consider the problem of learning distributed representations for documents in data streams . </S>",
    "<S> the documents are represented as low - dimensional vectors and are jointly learned with distributed vector representations of word tokens using a hierarchical framework with two embedded neural language models . in particular , we exploit the context of documents in streams and use one of the language models to model the document sequences , and the other to model word sequences within them . </S>",
    "<S> the models learn continuous vector representations for both word tokens and documents such that semantically similar documents and words are close in a common vector space . </S>",
    "<S> we discuss extensions to our model , which can be applied to personalized recommendation and social relationship mining by adding further user layers to the hierarchy , thus learning user - specific vectors to represent individual preferences . </S>",
    "<S> we validated the learned representations on a public movie rating data set from movielens , as well as on a large - scale yahoo news data comprising three months of user activity logs collected on yahoo servers . </S>",
    "<S> the results indicate that the proposed model can learn useful representations of both documents and word tokens , outperforming the current state - of - the - art by a large margin .    </S>",
    "<S> [ language models ] [ text processing ] </S>"
  ]
}