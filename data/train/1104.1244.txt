{
  "article_text": [
    "in this paper we consider the trace regression problem .",
    "assume that we observe @xmath1 independent random pairs @xmath2 , @xmath3 . here",
    "@xmath4 are random matrices of dimension @xmath5 and known distribution @xmath6 , @xmath7 are random variables in @xmath8 which satisfy @xmath9 where @xmath10 is an unknown matrix , @xmath11 is the conditional expectation of @xmath7 given @xmath12 and @xmath13 denotes the trace of the matrix @xmath14 .",
    "we consider the problem of estimating of @xmath0 based on the observations @xmath15 .",
    "though the results of this paper are obtained for general @xmath16 , our main motivation is the high - dimensional case , which corresponds to @xmath17 , with low rank matrices @xmath0 .    setting @xmath18",
    "we can equivalently write our model in the form @xmath19 the noise variables @xmath20 are independent and have mean zero .    the problem of estimating low rank matrices recently generated a considerable number of works .",
    "the most popular methods are based on minimization of the empirical risk penalized by the nuclear norm with various modifications , see , for example , @xcite .    in this paper",
    "we propose a new estimator of @xmath0 . in our construction",
    "we combine the penalization by the rank with the use of the knowledge of the distribution @xmath21 .",
    "an important feature of our estimator is that in a number of interesting examples we can write it out explicitly .",
    "penalization by the rank was previously considered in @xcite for the multivariate response regression model .",
    "the criterion introduced by bunea , she and wegkamp in @xcite , the rank selection criterion ( rsc ) , minimizes the frobenius norm of the fit plus a regularization term proportional to the rank .",
    "the rank of the rsc estimator gives a consistent estimation of the number of the singular values of the signal @xmath22 above a certain noise level .",
    "here @xmath23 is the matrix of predictors . in @xcite",
    "the authors also establish oracle inequalities on the mean squared errors of rsc .",
    "the paper @xcite is mainly focused on the case of unknown variance of the noise .",
    "the author gives a minimal sublinear penalty for rsc and provides oracle inequalities on the mean squared risks .",
    "the idea to incorporate the knowledge of the distribution @xmath24 in the construction of the estimator was first introduced in @xcite but with a different penalization term , proportional to the nuclear norm . in @xcite",
    "the authors establish general sharp oracle inequalities for trace regression model and apply them to the noisy matrix completion problem .",
    "they also provide lower bounds .    in the present work we consider a more general model than the model of @xcite .",
    "it contains as particular cases a number of interesting problems such as matrix completion , multi - task learning , linear regression model , matrix response regression model .",
    "the analysis of our model requires different techniques and uses the matrix version of bernstein s inequality for the estimation of the stochastic term , similarly to @xcite .",
    "however , we use a different penalization term than in @xcite and the main scheme of our proof is quite different . in particular , we obtain a bound for the rank of our estimator in a very general setting ( theorem [ thm1 ] , ( i ) ) and estimations for the prediction error in expectation ( theorem [ thm2 ] )",
    ". such bounds are not available for nuclear norm penalization used in @xcite .",
    "note , however , that under very specific assumptions on @xmath12 , @xcite shows that the rank of @xmath0 can be reconstructed exactly , with high probability , when the dimension of the problem is smaller then the sample size .",
    "the paper is organized as follows . in section [ def ]",
    "we define the main objects of our study , in particular , our estimator .",
    "we also show how some well - known problems ( matrix completion , column masks,complete  subgaussian design ) are related to our model . in section [ general ] , we show that the rank of our estimator is bounded from above by the rank of the unknown matrix @xmath0 with a constant close to 1 . in the same section",
    "we prove general oracle inequalities for the prediction error both in probability and in expectation .",
    "then , in section [ matrix ] we apply these general results to the noisy matrix completion problem . in this case",
    "our estimator has a particularly simple form : it is obtained by hard thresholding of the singular values of a matrix constructed from the observations @xmath15 .",
    "moreover , up to a logarithmic factor , the rates attained by our estimator are optimal under the frobenius risk for a simple class of matrices @xmath25 defined as follows : for any @xmath26 the rank of @xmath0 is supposed not to be larger than a given @xmath27 and all the entries of @xmath0 are supposed to be bounded in absolute value by a constant @xmath28 .",
    "finally , in section [ matrix regression ] , we consider the matrix regression model and compare our bounds to those obtained in @xcite .",
    "for @xmath29 the schatten - q ( quasi-)norm of the matrix @xmath14 is defined by @xmath30 where @xmath31 are the singular values of @xmath14 ordered decreasingly .    for any matrices @xmath32 , we define the scalar product @xmath33 and the bilinear symmetric form @xmath34 we introduce the following assumption on the distribution of the matrix @xmath4 :    [ ass1 ] there exists a constant @xmath35 such that , for all matrices @xmath36 @xmath37    under assumption 1 the bilinear form defined by is a scalar product . this assumption is satisfied , often with equality , in several interesting examples such as matrix completion , column masks , `` complete '' subgaussian design .",
    "the trace regression model is quite a general model which contains as particular cases a number of interesting problems :    * * * assume that the design matrices @xmath12 are i.i.d uniformly distributed on the set @xmath38 where @xmath39 are the canonical basis vectors in @xmath40 .",
    "then , the problem of estimating @xmath0 coincides with the problem of matrix completion under uniform sampling at random ( usr ) .",
    "the latter problem was studied in @xcite in the non - noisy case ( @xmath41 ) and in @xcite in the noisy case . in a slightly different setting the problem of matrix completion",
    "was considered , for example , in @xcite .",
    "+ for such @xmath12 , we have the relation @xmath42 for all matrices @xmath43 . *",
    "* * assume that the design matrices @xmath12 are i.i.d .",
    "replications of a random matrix @xmath23 , which has only one nonzero column . if the distribution of @xmath23 is such that all the columns have the same probability to be non - zero and the non - zero column @xmath44 is such that @xmath45 is the identity matrix , then the assumption [ ass1 ] is satisfied with @xmath46 .",
    "* * * suppose that the design matrices @xmath12 are i.i.d .",
    "replications of a random matrix @xmath23 and the entries of @xmath23 are either i.i.d .",
    "standard gaussian or rademacher random variables . in both cases ,",
    "assumption [ ass1 ] is satisfied with @xmath47 . *",
    "* * the matrix regression model is given by @xmath48 where @xmath49 are @xmath50 vectors of response variables , @xmath51 are @xmath52 vectors of predictors , @xmath0 is an unknown @xmath53 matrix of regression coefficients and @xmath54 are random @xmath50 vectors of noise with independent entries and mean zero .",
    "+ we can equivalently write this model as a trace regression model .",
    "let @xmath55 , @xmath56 and @xmath57 , where @xmath58 are the @xmath59 vectors of the canonical basis of @xmath60 .",
    "then we can write as @xmath61 set @xmath62 and @xmath63 .",
    "then @xmath64 assumption [ ass1 ] , which is a condition of isometry in expectation , is used in the case of random @xmath12 . in the case of matrix regression with deterministic @xmath51",
    "we do not need it , see section [ matrix regression ] for more details .",
    "* * * let @xmath65 and @xmath66 denotes the set of diagonal matrices of size @xmath67 . if @xmath14 and @xmath68 then the trace regression model becomes the linear regression model with vector parameter .",
    "the main motivation of this paper is the matrix completion and matrix regression problems , which we treat in section [ matrix ] and section [ matrix regression ] .",
    "we define the following estimator of @xmath0 : @xmath69 where @xmath70 is a regularization parameter and @xmath71 is the rank of the matrix @xmath14 .    for matrix regression problem and deterministic @xmath12 ,",
    "our estimator coincides with the rsc estimator : @xmath72 this estimator , called the rsc estimator , can be computed efficiently using the procedure described in @xcite .    under assumption 1",
    ", the functional @xmath73 tends to @xmath74 when @xmath75 .",
    "so there exists a constant @xmath76 such that @xmath77 .",
    "as the mapping @xmath78 is lower semi - continuous , the functional @xmath79 is lower semi - continuous ; thus @xmath80 attains a minimum on the compact set @xmath81 and the minimum is a global minimum of @xmath80 on @xmath82 .",
    "suppose that assumption [ ass1 ] is satisfied with equality , i.e. , @xmath83 then our estimator has a particularly simple form : @xmath84 where @xmath85 the optimization problem may equivalently be written as @xmath86.\\ ] ] here , the inner minimization problem is to compute the restricted rank estimators @xmath87 that minimizes the norm @xmath88 over all matrices of rank @xmath89 .",
    "write the singular value decomposition ( svd ) of @xmath90 : @xmath91 where    * @xmath92 are the singular values of @xmath90 indexed in the decreasing order , * @xmath93 ( resp .",
    "@xmath94 ) are the left ( resp .",
    "right ) singular vectors of @xmath90 .",
    "following @xcite , one can write : @xmath95 using this , we easily see that @xmath96 has the form @xmath97 thus , the computation of @xmath96 reduces to hard thresholding of singular values in the svd of @xmath98",
    ".    * remark .",
    "* we can generalize the estimator given by , taking the minimum over a closed set of the matrices , instead of the set @xmath99 , such as a set of all diagonal matrices , for example .",
    "in the following theorem we bound the rank of our estimator in a very general setting . to the best of our knowledge , such estimates were not known .",
    "we also prove general oracle inequalities for the prediction errors in probability analogous to those obtained in ( * ? ? ?",
    "* theorem 2 ) for the nuclear norm penalization .    given @xmath1 observations @xmath100 and @xmath12",
    ", we define the random matrix @xmath101 the value @xmath102 determines the  the noise level  of our problem .",
    "+ let @xmath103 .",
    "[ thm1 ] let assumption [ ass1 ] be satisfied and @xmath104 . if @xmath105 , then    1 .",
    "[ rank ] @xmath106 2 .",
    "[ lbound ] @xmath107 3 .",
    "@xmath108    it follows from the definition of the estimator @xmath96 that , for all @xmath109 , one has @xmath110 note that @xmath111 and @xmath112 therefore we obtain @xmath113 due to the trace duality @xmath114 for @xmath115 and @xmath116 such that + @xmath117 , we have @xmath118 under assumption 1 , this yields @xmath119 which implies @xmath120 to prove ( i ) , we take @xmath121 in and we obtain : @xmath122 thus , @xmath123 to prove ( ii ) , we first consider the case @xmath124 . then implies @xmath125 therefore , for @xmath126",
    ", we have @xmath127 using ( i ) we obtain @xmath128    consider now the case , @xmath129 .",
    "using that @xmath130 for @xmath131 and @xmath132 , we get from that @xmath133 finally , the elementary inequality @xmath134 yields @xmath135 using and , we obtain ( ii ) .    to prove ( iii ) , we use to obtain @xmath136 from which we get @xmath137 and ( iii ) follows .    in the next theorem",
    "we obtain bounds for the prediction error in expectation .",
    "set @xmath138 , @xmath139 and @xmath140 .",
    "suppose that @xmath141 and let @xmath142 be the set of non - negative random variables @xmath143 bounded by @xmath27 .",
    "we set @xmath144    [ thm2 ] let assumption [ ass1 ] be satisfied .",
    "consider @xmath145 and a regularization parameter @xmath146 satisfying @xmath147 . then    1 .",
    "@xmath148 2 .",
    "@xmath149 + and 3 .",
    "@xmath150    to prove ( a ) we take the expectation of to obtain @xmath151 if @xmath152 , as @xmath153 we obtain @xmath154 which implies @xmath155 .",
    "if @xmath156 , @xmath157 and we get @xmath158 which proves part ( a ) of theorem [ thm2 ] . to prove ( b ) , and yield @xmath159 where @xmath160 is the indicator function of the event @xmath161",
    "taking the expectation we obtain @xmath162 note that cauchy - schwarz inequality and @xmath163 imply @xmath164 taking @xmath165 we find @xmath166 if @xmath167 , which implies @xmath168 , as @xmath153 we obtain @xmath169 this prove ( b ) in the case @xmath167 .",
    "if @xmath170 , from we get @xmath171 using that @xmath145 and the elementary inequality @xmath134 we find @xmath172 the cauchy - schwarz inequality and ( a ) imply @xmath173 using that @xmath174 when @xmath175 for @xmath176 we get ( b ) .",
    "we now prove part ( c ) . from",
    "we compute @xmath177 which implies @xmath178 taking the expectation we obtain @xmath179 as @xmath163 we compute @xmath180 the assumption on @xmath146 and imply ( c ) .",
    "this completes the proof of theorem [ thm2 ] .",
    "the next lemma gives an upper bound on @xmath181 in the case when @xmath182 concentrates exponentially around its mean .",
    "[ concentratio_bound ] assume that @xmath183 for some positive constants @xmath184 and @xmath185 .",
    "then @xmath186 for @xmath187 .",
    "write @xmath188        \\\\&+2\\be\\left ( \\delta\\right ) \\be\\left [ \\big ( \\delta-\\be \\left ( \\delta\\big ) \\right ) w\\right]\\\\        & \\leq 2\\left ( \\be\\left ( \\delta\\right ) \\right ) ^{2}\\max\\{\\be ( w),1\\}\\\\&+ \\be\\left [ \\big ( \\delta-\\be \\left ( \\delta\\right ) \\big ) ^{2}w\\right]\\\\&+         \\be\\left [ \\big ( \\delta-\\be \\left ( \\delta\\right ) \\big ) ^{2}w^{2}\\right ]",
    ".        \\end{split}\\ ] ] setting @xmath189 , we see that it is enough to estimate @xmath190 $ ] for @xmath191 . putting @xmath192 and using hlder s inequality",
    "we get @xmath193 where @xmath194 .",
    "we first estimate @xmath195 .",
    "inequality implies that @xmath196 the gamma - function satisfies the following bound : @xmath197 cf .",
    "proposition [ proposition1 ] . plugging it into",
    "we find @xmath198 if @xmath199 we get directly from . if @xmath200 , the bound @xmath201 implies that @xmath202 and thus @xmath203 then follows from and .",
    "in this section we present some consequences of the general oracles inequalities of theorems [ thm1 ] and [ thm2 ] for the model of usr matrix completion .",
    "assume that the design matrices @xmath12 are i.i.d uniformly distributed on the set @xmath204 defined in .",
    "this implies that @xmath42 for all matrices @xmath43 .",
    "then , we can write @xmath96 explicitly @xmath205    set @xmath206 . in the case of matrix completion",
    ", we can improve point ( i ) of theorem [ thm1 ] and give an estimation on the difference of the first @xmath207 singular values of @xmath96 and @xmath0 .",
    "we also get bounds on the prediction error measured in norms different from the frobenius norm , in particular in the spectral norm .",
    "[ thm3 ] let @xmath146 satisfy the inequality @xmath208 ( as in theorem [ thm1 ] )",
    ". then    1 .",
    "@xmath209 ; 2 .",
    "@xmath210 for @xmath211 ; 3 .",
    "@xmath212 ; 4 .   for @xmath213",
    ", one has @xmath214 where we set @xmath215 for @xmath216 .",
    "the proof is obtained by adapting the proof of ( * ? ? ?",
    "* theorem 8) to hard thresholding estimators . for completeness",
    ", we give the proof of ( iii ) and ( iv ) .",
    "let us start with the proof of ( iii ) .",
    "note that @xmath217 .",
    "let @xmath218 , by we have that @xmath219 .",
    "then @xmath220 and we get ( iii ) .    to prove ( iv ) we use the following interpolation inequality ( see ( * ? ?",
    "* lemma 11 ) ) : for @xmath221 let @xmath222 $ ] be such that @xmath223 then for all @xmath36we have    @xmath224    for @xmath225 take @xmath226 and @xmath227 . from theorem [ thm1 ] ( ii )",
    "we get that @xmath228 now , plugging ( iii ) of theorem [ thm3 ] and into , we obtain @xmath229 and ( iv ) follows .",
    "this completes the proof of theorem [ thm3 ] .    in view of theorems [ thm1 ] and [ thm2 ] , to specify the value of regularization parameter @xmath146",
    ", we need to estimate @xmath230 with high probability .",
    "we will use the bounds obtained in @xcite in the following two settings of particular interest :    * _ statistical learning setting_. there exists a constant @xmath231 such that + @xmath232 .",
    "then , we set @xmath233 @xmath234 * _ sub - exponential noise .",
    "_ we suppose that the pairs @xmath235 are iid and that there exist constants @xmath236 and @xmath237 such that @xmath238 let @xmath239 and @xmath240 .",
    "then , we set @xmath241 @xmath242 where @xmath243 is a large enough constant that depends only on @xmath244 .",
    "in both case we can estimate @xmath230 with high probability :    [ lemma1 ] for all @xmath245 , with probability at least @xmath246 in the case of statistical learning setting ( respectively , @xmath247 in the case of sub - exponential noise ) , one has @xmath248    as a corollary of lemma [ lemma1 ] we obtain the following bound for @xmath249    [ lemma2 ] let one of the set of conditions ( * a * ) or ( * b * ) be satisfied .",
    "assume @xmath250 , @xmath251 and @xmath143 is a non - negative random variable such that @xmath252 , then @xmath253    we will prove in the case of statistical learning setting .",
    "the proof in the case of sub - exponential noise is completely analogous .",
    "set @xmath254 note that lemma [ lemma1 ] implies that @xmath255 and @xmath256 we set @xmath257 , @xmath258 and @xmath259 . by hlder",
    "s inequality we get @xmath260 we first estimate @xmath261 . inequalities and imply that @xmath262 the gamma - function satisfies the following bound : @xmath263 we give a proof of this inequality in the appendix . plugging it into we compute @xmath264 observe that @xmath250 implies @xmath265 and we obtain @xmath266 if @xmath267 we get directly from . if @xmath268 , the bound @xmath252 implies that @xmath269 and we compute @xmath270 the function @xmath271 is a decreasing function of @xmath272 which is smaller then @xmath273 for @xmath251 .",
    "this implies @xmath274 plugging and into and using we get .",
    "this completes the proof of lemma [ lemma2 ] .",
    "the natural choice of @xmath275 in lemma [ lemma1 ] is of the order @xmath272 ( see the discussion in @xcite ) .",
    "then , in theorems [ thm1 ] and [ thm2 ] we can take @xmath276 , where the constant @xmath184 is large enough , to obtain the following corollary .",
    "[ cor1 ] let one of the set of conditions ( * a * ) or ( * b * ) be satisfied .",
    "assume @xmath250 , @xmath251 , @xmath145 and @xmath276 .",
    "then ,    1 .   with probability at least @xmath277 , one has @xmath278 and , in particular , @xmath279 2 .   with probability at least @xmath277 , one has @xmath280 3 .",
    "@xmath281 + and , in particular , @xmath282 4 .",
    "@xmath283 + and , in particular , @xmath284 5 .   with probability at least @xmath277",
    ", one has @xmath285 6 .   with probability at least @xmath277 ,",
    "one has @xmath286",
    "\\(i ) - ( iv ) are straightforward in view of theorems [ thm1 ] and [ thm2 ] .",
    "( v ) is a consequence of theorem [ thm3 ] ( iii ) .",
    "the proof of ( vi ) follows from ( i ) using the same argument as in @xcite corollary 2 .",
    "this corollary guarantees that the normalized frobenius error @xmath287 of the estimator @xmath96 is small whenever @xmath288 with a constant @xmath289 large enough .",
    "this quantifies the sample size @xmath1 necessary for successful matrix completion from noisy data .    comparing corollary [ cor1 ] with theorem 6 and theorem 7 of we",
    "see that , in the case of gaussian errors and for the statistical learning setting , the rate of convergence of our estimator is optimal , for the class of matrices @xmath25 defined as follows : for any @xmath26 the rank of @xmath0 is supposed not to be larger than a given @xmath27 and all the entries of @xmath0 are supposed to be bounded in absolute value by a constant @xmath28 .",
    "in this section we apply the general oracles inequalities of theorems [ thm1 ] and [ thm2 ] to the matrix regression model and compare our bounds to those obtained by bunea , she and wegkamp in @xcite . recall that matrix regression model is given by @xmath290 where @xmath49 are @xmath50 vectors of response variables , @xmath51 are @xmath52 vectors of predictors",
    ", @xmath0 is a unknown @xmath53 matrix of regression coefficients and @xmath54 are random @xmath50 vectors of noise with independent entries and mean zero .    as mentioned in the section [ def ] , we can equivalently write this model as a trace regression model . let @xmath291 and @xmath57 where @xmath58 are the @xmath59 vectors of the canonical basis of @xmath60",
    ". then we can write as @xmath61 set @xmath62 , @xmath63 and @xmath292 , then for deterministic predictors @xmath293 note that we use assumption [ ass1 ] in the proof of theorem [ thm1 ] to derive from . in the case of matrix regression with deterministic @xmath51",
    ", we do not need this assumption and proceed as follows .",
    "let @xmath294 denote the orthogonal projector on the linear span of the columns of matrix @xmath14 and let @xmath295 .",
    "note that @xmath296",
    ". then , one has @xmath297 .",
    "now , we use and the fact that @xmath298 hence , the trace duality yields where we set @xmath299 .",
    "thus , in the case of matrix regression with deterministic @xmath51 , we have proved that theorems [ thm1 ] and [ thm2 ] hold with @xmath299 even if assumption [ ass1 ] is not satisfied .    in order to get an upper bound on @xmath300 in the case of gaussian noise",
    "we will use the following result .",
    "[ lemma_bunea ] let @xmath301 and assume that @xmath302 are independent @xmath303 random variables.then @xmath304 and@xmath305    using and lemma [ concentratio_bound ] applied to @xmath306 we get the following bound on @xmath181 :    assume that @xmath302 are independent @xmath303 , then@xmath307    for @xmath308 , we have that @xmath309 .",
    "then , these two lemmas imply that in theorems [ thm1 ] and [ thm2 ] we can take @xmath310 to get the following corollary :    assume that @xmath302 are independent @xmath303 , @xmath308 , @xmath311 and @xmath312",
    ". then    1 .   with probability at least @xmath313 , one has @xmath314 and , in particular , @xmath315 2 .   with probability at least @xmath313 , one has @xmath316 3 .",
    "@xmath317 + and , in particular , @xmath318 4 .",
    "@xmath319 + and , in particular , @xmath320    the symbol @xmath321 means that the inequality holds up to multiplicative numerical constants .",
    "this corollary shows that our error bounds are comparable to those obtained in @xcite",
    ". points ( i ) and ( iii ) are new ; here we have inequalities with leading constant 1 . the results ( ii ) and",
    "( iv ) give the same bounds as in @xcite up to constants and to an additional exponentially small term in the analog of ( iv ) in @xcite .",
    "for completeness , we give here the proof of .",
    "we set @xmath323 using functional equation for @xmath324 we note that @xmath325 applying this equality @xmath1 times we get @xmath326 by stirling s formula , we have that @xmath327 plugging this into we obtain @xmath328 + x(\\log 2 - 1)+\\dfrac{1}{2}\\log(\\pi/2 ) .",
    "\\end{split}\\ ] ] note that @xmath329    defining @xmath330 we have @xmath331 observe that @xmath332 is infinitely differentiable on @xmath333 .",
    "moreover the series defining @xmath332 can be differentiated @xmath89 times to obtain @xmath334 .",
    "thus @xmath335 and @xmath336 the relation implies that @xmath337 . using for @xmath338 we get @xmath339 where @xmath340 is the euler s constant . together with @xmath341 and",
    "@xmath342 this implies that @xmath343 for any @xmath344 .",
    "this completes the proof of proposition [ proposition1 ] ."
  ],
  "abstract_text": [
    "<S> in this paper we consider the trace regression model . </S>",
    "<S> assume that we observe a small set of entries or linear combinations of entries of an unknown matrix @xmath0 corrupted by noise . </S>",
    "<S> we propose a new rank penalized estimator of @xmath0 . </S>",
    "<S> for this estimator we establish general oracle inequality for the prediction error both in probability and in expectation . </S>",
    "<S> we also prove upper bounds for the rank of our estimator . </S>",
    "<S> then , we apply our general results to the problems of matrix completion and matrix regression . in these cases </S>",
    "<S> our estimator has a particularly simple form : it is obtained by hard thresholding of the singular values of a matrix constructed from the observations . </S>"
  ]
}