{
  "article_text": [
    "mutual information @xcite is a quantity of central importance in information theory .",
    "it is a nonlinear correlation function @xcite between two random variables that measures how much information about one random variable is encoded in the other . in other words , it measures the reduction of the uncertainty of a random variable resulting from knowing the other one .",
    "since the shannon entropy quantifies the randomness of a random variable , mutual information is a difference between shannon entropies . more generally , given two stochastic time series the information per unit of time between them is quantified by the rate of mutual information , which is a difference between shannon entropy rates . whereas the shannon entropy rate of markovian time series can be expressed in terms of the stationary probability distribution @xcite",
    ", no general formula is known for non - markovian processes .",
    "recently , we have obtained an analytical upper bound on the rate of mutual information and calculated it numerically for a class of markov processes @xcite .",
    "this class is formed by bipartite networks where the full state of the systems is determined by two coarse - grained variables : one corresponding to an external markovian process and the other to an internal non - markovian process . in this paper",
    "we generalize the results obtained in @xcite by calculating an upper bound on the rate of mutual information for a more general class of markov processes , where both coarse - grained processes can be non - markovian .",
    "moreover , we develop a numerical method to estimate the shannon entropy rate of a continuous time coarse - grained non - markovian process by adapting an extant numerical method for discrete time @xcite .",
    "apart from the quite challenging mathematical problem of determining the rate of mutual information , there are physical motivations for our study .",
    "first , within stochastic thermodynamics @xcite , which is a framework for far from equilibrium systems , a central quantity is the thermodynamic entropy production . in a nonequilibrium steady state",
    ", it characterizes the rate at which heat is dissipated . on the other hand , the rate of mutual information is an information theoretic entropy rate that characterizes the correlations between the two coarse - grained processes .",
    "the study of the relation between both quantities for specific models should improve our understanding of the relation between thermodynamics and information .",
    "more specifically , a considerable amount of work on the role of information in the stochastic thermodynamics of feedback driven systems , for which a controller acts at periodic time intervals , has emerged recently @xcite . in such periodic steady states",
    "the rate of mutual information between system and controller is just the average mutual information due to each new measurement divided by the length of the period @xcite .",
    "the second law of thermodynamics bounding the maximum extractable work has then to be modified in order to include the mutual information between system and controller , linking directly thermodynamic and information theoretic entropy productions . on the other hand ,",
    "if a maxwell s demon is described as an autonomous system @xcite , calculating the rate of mutual information in such a genuine nonequilibrium steady states shows that in this case there is no such relation between the rate of mutual information and the thermodynamic entropy production @xcite .",
    "second , the study of the energetic costs of sensing in biochemical networks is a field emerging at this interface between thermodynamics and information theory@xcite .",
    "for example , an intriguing relation between the energy costs of dissipation , quantified by the thermodynamic entropy production , and the adaptation error has been found in a model for the _ e. coli _ sensory system @xcite . in these papers , the observables characterizing the quality of sensing are the adaptation error @xcite and the uncertainty in the external ligand concentration @xcite .",
    "alternatively , a natural quantity that should be discussed in this context with the same dimension of the thermodynamic entropy production is the rate of mutual information .",
    "hence , the study of the relation between these two quantities in biochemical sensory networks could contribute to an understanding of the thermodynamics of such systems @xcite .",
    "this paper is organized as follows . in sect .",
    "[ sec2 ] , we discuss a one spin system with a fluctuating magnetic field as a simple introductory example .",
    "we define the bipartite network and the quantities of interest in sect .",
    "[ sec3 ] . in sect .",
    "[ sec4 ] , we derive our first main result , which is the analytical upper bound on the rate of mutual information . our second main result , namely , the continuous time numerical method , is explained in sect .",
    "[ sec5 ] , where we also discuss the discrete time case . in sect .",
    "[ sec6 ] , we calculate the rate of mutual information explicitly for four - state systems considering cases where the rate of mutual information admits a simple interpretation .",
    "we conclude in sect .",
    "for a simple illustration let us start with the four - state model represented in fig .",
    "one spin is subjected to a time varying magnetic field while in contact with a thermal reservoir inducing flips .",
    "the magnetic field is controlled by some external device that randomly changes it .",
    "more precisely , the field is a poisson process with rate @xmath0 , fluctuating between the values @xmath1 and @xmath2 .",
    "the transition rates for the spin flip are denoted by @xmath3 ( from @xmath4 to @xmath5 ) , where @xmath6 represents the state of the magnetic field and @xmath7 the orientation of the spin .",
    "these transition rates are given by the local detailed balance assumption , i.e. , @xmath8 where we set boltzmann constant multiplied by temperature to @xmath9 .",
    ", controlled by an external device , correspond to a change in the magnetic field between the values @xmath1 ( full blue line ) and @xmath2 ( dashed red line ) .",
    "the right panel shows the corresponding time series . ]",
    "now consider the three time series shown in fig .",
    "the first time series @xmath10 represents a stochastic trajectory of the full four - state markov process .",
    "the time series @xmath11 is also markovian , because if we integrate out the spin variable we get a two - state markov process .",
    "the physical reason for the markov character of this process is that the magnetic field is controlled by an external device that does not care about the internal state ( the spin orientation ) .",
    "the spin time series @xmath12 is non - markovian and contains information about the magnetic field time series @xmath11 , i.e. , both are correlated .",
    "the rate of mutual information @xmath13 ( see definition below ) quantifies how much information about the time series @xmath11 is encoded in the time series @xmath12 . in other words , it gives a ( non - linear ) measure of how correlated both time series are , being zero in the case where they are independent and positive otherwise . within the present model ,",
    "both time series become independent only for @xmath14 . for this choice of parameters ,",
    "we obtain a two - state markov process for the spin by integrating out the magnetic field , i.e. , for @xmath14 the processes @xmath11 and @xmath12 become two independent markov processes .",
    "moreover , the model is in equilibrium , i.e. , detailed balance is fulfilled if and only if @xmath14 . the thermodynamic entropy production @xmath15 ( see definition below ) is a signature of nonequilibrium since it is zero when detailed balance is fulfilled and strictly positive for nonequilibrium stationary states . for this one - spin system , @xmath15 is the rate at which the system dissipates heat to the thermal reservoir .",
    "as cited in the introduction , for feedback driven systems the second law of thermodynamics has to be adapted in order to include the rate of mutual information between the system and the controller .",
    "for these systems it is possible to rectify fluctuations in order to extract work from a single heat bath , where the rate of the extracted work is bounded by the rate of mutual information . a complementary question , considering the model of fig .",
    "[ fig1 ] , is whether the rate of mutual information between @xmath16 and @xmath17 , which is non - zero only when the system is out of equilibrium , is bounded by the dissipation rate required to sustain the nonequilibrium stationary state . in @xcite",
    "we have shown that , in general , there is no such bound . in fig .",
    "[ fig3 ] , we compare the thermodynamic entropy production @xmath15 with the rate of mutual information @xmath13 for the one - spin system of fig .",
    "[ fig1 ] using the results derived further below .",
    "( [ sdef ] ) and the rate of mutual information @xmath13 for the model of fig .",
    "[ fig1 ] as a function of @xmath18 , where @xmath19 , @xmath20 and @xmath21 .",
    "the abbreviation ( disc . ) indicates the mutual information obtained with the extrapolation for @xmath22 in discrete time explained in sect . [",
    "5.1 ] and ( cont . ) is related to the continuous time numerical method explained sect . [ 5.2 ] .",
    "@xmath23 shows the analytical upper bound ( [ ubi ] ) . ]",
    "we now define the class of bipartite markov processes studied in this paper and the rate of mutual information , for discrete and continuous time .",
    "first we consider a discrete time markov process where the states are labeled by the pair of variables @xmath24 , where @xmath25 and @xmath26 .",
    "the markov chain is defined by the following transition probabilities , @xmath27 where @xmath28 is the time spacing .",
    "transitions where both variables change are not allowed , which means that the network of states is bipartite .",
    "we denote a discrete time series of the full markov process with @xmath29 jumps by @xmath30 , where @xmath31 .",
    "the shannon entropy rate of the markov chain ( [ defrates ] ) is defined by @xcite @xmath32\\ln p[\\{z_n\\}_0^n ] , \\label{entz}\\ ] ] where the sum is over all possible stochastic trajectories @xmath33 .",
    "since the full process is markovian , it is well known that this entropy rate can be expressed in terms of the stationary probability distribution @xmath34 in the form @xcite",
    "@xmath35    moreover , the shannon entropy rates of the coarse - grained processes @xmath36 and @xmath37 are defined as @xmath38\\ln p[\\{x_n\\}_0^n ] , \\label{entx}\\ ] ] @xmath39\\ln p[\\{y_n\\}_0^n ] .",
    "\\label{enty}\\ ] ] these two coarse - grained processes are in general non - markovian .",
    "more precisely , they are hidden markov processes @xcite .",
    "the quantity we wish to calculate is the rate of mutual information between the two coarse - grained variables , which is defined as @xmath40 therefore , in order to obtain the rate of mutual information we have to calculate the shannon entropy rates of the two coarse - grained variables @xmath41 and @xmath42 . using the definitions of the shannon entropy rates ( [ entz ] ) , ( [ entx ] ) , and ( [ enty ] ) , we can rewrite @xmath43 in the form @xmath44||p[\\{x_n\\}_0^n]p[\\{y_n\\}_0^n]),\\ ] ] where the kullback - leibler distance is defined as @xcite @xmath45||p[\\{x_n\\}_0^n]p[\\{y_n\\}_0^n])\\equiv\\sum_{\\{z_n\\}_0^n}p[\\{z_n\\}_0^n]\\ln\\frac{p[\\{z_n\\}_0^n]}{p[\\{x_n\\}_0^n]p[\\{y_n\\}_0^n]}.\\ ] ] with this formula",
    "it becomes explicit that the rate of mutual information measures how correlated the two processes are .    in @xcite",
    "we have studied the particular case where @xmath41 is an external process independent of the internal states , i.e. , @xmath46 for all @xmath26 . in this case , the external process is also markovian and @xmath47 becomes @xmath48 where @xmath49 for later convenience we also define @xmath50    in this paper we are mostly interested in the continuous time limit @xmath22 . for the calculation of an analytical upper bound on the continuous time rate of mutual information , it is useful to consider the discrete time markov chain and then take the limit @xmath51 . in this limit , the shannon entropy rates diverge as @xmath52 @xcite , however , the rate of mutual information is a well defined finite quantity : it is a difference between shannon entropy rates for which the term proportional to @xmath52 cancels .",
    "it is possible to calculate the rate of mutual information numerically for the discrete time case as a function of @xmath28 and then extrapolate to the limit @xmath22 . alternatively , we develop a more efficient numerical method to directly estimate the entropy rate of a continuous time series .",
    "we now define the shannon entropy rates and the rate of mutual information for the continuous time case .",
    "the continuous time markov process is defined by the transition rates ( transition probability per time ) @xmath53 the stochastic trajectory for a fixed time interval @xmath54 is written as @xmath55 ( in this case the time interval is fixed and the number of jumps @xmath29 is a random variable ) . similarly , the definition of the shannon entropy rate of the full markov process is @xmath56\\mathcal{p}[\\{z(t)\\}_0^t]\\ln\\mathcal{p}[\\{z(t)\\}_0^t ] , \\label{entropyzcont}\\ ] ] where @xmath57 $ ] is the probability density of the trajectory @xmath55 and the integral is over all possible stochastic trajectories .",
    "since the @xmath58 process is markovian the continuous time shannon entropy rate can also be written in terms of the stationary probability distribution , and it is given by @xcite @xmath59 since the transition rates can take any positive value , it is clear that this shannon entropy rate can be negative .",
    "this is a well known fact for continuous random variables @xcite .",
    "the shannon entropy rates of the @xmath41 and @xmath42 processes are defined in the same way , @xmath60\\mathcal{p}[\\{x(t)\\}_0^t]\\ln\\mathcal{p}[\\{x(t)\\}_0^t ] , \\label{entropyxcont}\\ ] ] @xmath61\\mathcal{p}[\\{y(t)\\}_0^t]\\ln\\mathcal{p}[\\{y(t)\\}_0^t ] .",
    "\\label{entropyycont}\\ ] ] moreover , the definition of the continuous time rate of mutual information is @xmath62 where the relation between @xmath13 and the discrete time rate of mutual information ( [ mutualdef ] ) is @xmath63 .",
    "even though the shannon entropy rates @xmath64 , @xmath65 , and @xmath66 may be negative , the rate of mutual information @xmath13 , the quantity of central interest in this paper , fulfills @xmath67 . in order to show this we write the rate of mutual information as a kullback - leibler distance , @xmath68||\\mathcal{p}[\\{x(t)\\}_0^t]\\mathcal{p}[\\{y(t)\\}_0^t]\\right)\\ge 0.\\ ] ]",
    "a central quantity in stochastic thermodynamics is the thermodynamic entropy production @xcite , which for the rates ( [ defrates2 ] ) reads @xmath69 analogously to the rate of mutual information , the thermodynamic entropy production can also be expressed as @xcite @xmath70||\\mathcal{p}[\\{\\tilde{z}(t)\\}_0^t]\\right)\\ge 0.\\ ] ] where @xmath71 denotes the time - reversed trajectory , i.e. , @xmath72 .",
    "depending on the physical interpretation of the transition rates , the entropy rate @xmath15 may characterize the dissipation associated with the full network of states , being zero only if detailed balance is fulfilled . as discussed above , for the one spin system of fig .",
    "[ fig1 ] it is proportional to the heat that flows from the system to the thermal reservoir . on the other hand , @xmath13 is the information theoretic entropy rate that quantifies the correlation between the @xmath41 and @xmath42 processes .",
    "no closed formula like equation ( [ sdef ] ) is known for the rate of mutual information . however , as we show next , it is still possible to calculate it numerically and to obtain an analytical upper bound .",
    "let us take the @xmath42 process in the discrete time case and in the stationary regime .",
    "the conditional shannon entropy is defined as @xmath73 where @xmath74 is a conditional probability .",
    "the knowledge of one extra random variable can only decrease the uncertainty about @xmath75 , which means that @xmath76 .",
    "therefore , as the @xmath42 process is stationary , we obtain that this conditional entropy is a decreasing function of @xmath29 , i.e. , @xmath77 moreover , in the limit @xmath78 , we have @xcite @xmath79 which means that the conditional entropy ( [ conddef ] ) bounds the shannon entropy rate @xmath80 from above . furthermore , it can be shown that @xmath80 is bounded from below by @xcite @xmath81 leading to @xmath82 where the bounds become tighter for increasing @xmath29 .    as we show in the appendix ,",
    "for any finite @xmath29 , @xmath83 and , analogously , @xmath84 from ( [ entz2 ] ) we obtain the following formula for the entropy rate @xmath85 , @xmath86 for convenience we define the average transition rates @xmath87 @xmath88    the @xmath29-th upper bound on the rate of mutual information is then @xmath89 from equations ( [ hyn ] ) , ( [ hxn ] ) , and ( [ hzn ] ) , it is given by @xmath90 taking the continuous time limit @xmath51 , the rate of mutual information is hence bounded from above by @xmath91 two remarks are important .",
    "first , it is interesting to note the formal similarity between this expression and the one for the thermodynamic entropy production ( [ sdef ] ) . substituting in the latter inside the logarithm the rate of a reversed transition by the respective average forwards rates ( [ avgi ] ) and ( [ avgalpha ] )",
    ", we get the former .",
    "second , to calculate the true rate of mutual information we would have to take the limit @xmath92 with fixed @xmath28",
    ". this would give an expression for the rate of mutual information that would be valid for any time spacing @xmath28 and should become the continuous time rate of mutual information by taking the limit @xmath51 afterwards",
    ".     obtained from ( [ bounds ] ) for the discrete time version of the model of fig .",
    "the parameters are @xmath19 , @xmath20 , @xmath21 , @xmath93 ( left panel ) , and @xmath94 ( right panel ) . in the limit @xmath51 , the upper bounds go to the value given by ( [ ubi ] ) and the lower bounds go to zero.,title=\"fig : \" ]    obtained from ( [ bounds ] ) for the discrete time version of the model of fig .",
    "the parameters are @xmath19 , @xmath20 , @xmath21 , @xmath93 ( left panel ) , and @xmath94 ( right panel ) . in the limit @xmath51 , the upper bounds go to the value given by ( [ ubi ] ) and the lower bounds go to zero.,title=\"fig : \" ]    a similar calculation for the lower bounds in equation ( [ bounds ] ) shows that in the continuous time limit they all go to zero .",
    "this is illustrated in fig .",
    "[ fig4 ] , where we plot upper and lower bounds obtained from ( [ bounds ] ) as a function of the time spacing @xmath28 for the discrete time version of the one spin model of fig .",
    "this discrete time version is defined by the transition probabilities given by ( [ defrates ] ) obtained from the transition rates represented in fig .",
    "[ fig1 ] .    finally , one limiting case for which the rate of mutual information saturates the upper bound is the following .",
    "we take the @xmath41 process to be markovian , i.e. , @xmath46 for all @xmath26 . from equation ( [ entx2 ] ) , it follows @xmath95 furthermore , if the @xmath41 transitions are much faster than the @xmath42 transitions ( @xmath96 ) , the @xmath42 process becomes approximately markovian , with transition rates @xmath97 @xcite .",
    "therefore , in this limit we expect @xmath98 the continuous time rate of mutual information @xmath13 obtained from ( [ hzn ] ) , ( [ hxmark ] ) and ( [ hymark ] ) is then precisely the upper bound ( [ ubi ] )",
    ". therefore , in the case where the @xmath41 process is markovian and much faster then the @xmath42 process , the rate of mutual information saturates the upper bound ( [ ubi ] ) . in sect .",
    "[ sec6 ] , we illustrate this fact explicitly for four - state models .",
    "for discrete time , the probability of a stochastic trajectory of the @xmath42 process can be written as @xmath99= \\sum_{x_{n}x_{n-1}\\ldots x_1 x_0 } p[y_n , x_n|y_{n-1},x_{n-1 } ] \\ldots p[y_1,x_1|y_{0},x_{0 } ] p(x_0,y_0 ) , \\label{masternon}\\ ] ] where @xmath100 denotes the initial probability distribution and @xmath101 $ ] is the conditional probability .",
    "explicitly , for @xmath102 and @xmath103 we have @xmath101=w^{\\alpha\\beta}_{ij}$ ] .",
    "let the random matrix @xmath104 be defined by @xmath105=p[z_n|z_{n-1 } ] .",
    "\\label{defty}\\ ] ] this is a @xmath106 matrix , where the variables @xmath107 make it random . using this matrix , equation ( [ masternon ] ) can be rewritten as @xmath108=   \\mathbf{v } \\boldsymbol{t}(y_n , y_{n-1})\\ldots \\boldsymbol{t}(y_1,y_{0 } ) \\mathbf{p}_{y_0 } \\label{prody}\\ ] ] where @xmath109 is a row vector with all @xmath110 components equal to one and @xmath111 is a column vector with components @xmath112 , with @xmath113 . the shannon entropy rate ( [ enty ] ) can then be written as @xmath114 moreover , in the large @xmath29 limit , where boundary terms become irrelevant , we can replace the product of matrices ( [ prody ] ) in equation ( [ prody2 ] ) with @xmath115 , where @xmath116 is any matrix norm @xcite .",
    "therefore , in order to estimate the entropy rate @xmath80 we generate a long time series @xmath117 with a numerical simulation and calculate @xmath118 such a numerical method to calculate the shannon entropy rate has been used in @xcite . the appropriate way to calculate this product , avoiding numerical precision problems for large @xmath29 , is to normalize the product every @xmath119 steps and repeat the procedure @xmath120 times , so that @xmath121 @xcite .",
    "more precisely , for @xmath122 we calculate the vector @xmath123 \\mathbf{u}_{m-1},\\ ] ] and the normalization factor @xmath124 where @xmath125 is the normalized vector @xmath126 and the initial vector @xmath127 is any random vector with an unitary norm . by calculating the normalization factors",
    "iteratively we obtain the shannon entropy rate with the formula @xmath128    the present method is based on the fact that the probability of an @xmath42 stochastic trajectory can be written as a product of random matrices ( [ prody ] ) . since this is true for any coarse - grained non - markovian variable we can also apply the same method to calculate @xmath129 .",
    "explicitly , if we define the @xmath130 random matrix @xmath131 , \\label{deftx}\\ ] ] then we can estimate the shannon entropy rate from the numerically generated time series @xmath132 from @xmath133 moreover , we can also apply the same procedure of normalizing the product after some steps and keep track of the normalization factor to calculate this product numerically .",
    "finally , with the shannon entropy rates ( [ eq41 ] ) and ( [ eq47 ] ) we obtain the rate of mutual information from ( [ entz2 ] ) and ( [ mutualdef ] ) .    in fig .",
    "[ fig4 ] , we show the numerically obtained rate of mutual information for two sets of the kinetic parameters of the discrete time version of the one spin system of fig .",
    "[ fig1 ] as a function of the time spacing @xmath28 . for small @xmath28",
    ", the rate of mutual information shows a linear behavior , which we can extrapolate in order to obtain the continuous time rate of mutual information @xmath13 .",
    "the result has been shown in fig .",
    "[ fig3 ] . a more efficient numerical method to obtain @xmath13 , which generalizes the above discussion to the continuous time case ,",
    "is introduced next .",
    "we consider the continuous time trajectory @xmath134 that stays in state @xmath135 during the waiting time @xmath136 .",
    "the number of jumps @xmath29 is a random functional of the trajectory and the time interval @xmath137 is fixed .",
    "the main difference , in relation to the discrete time case , is the presence of the exponentially distributed waiting times in the probability density of the continuous time trajectory , which is written as @xmath138=    \\exp(-\\lambda_{z_n}\\tau_n)\\left[\\prod_{n=1}^{n}w_{z_{n-1}z_{n}}\\exp(-\\lambda_{z_{n-1}}\\tau_{n-1})\\right ] p(z_0).\\ ] ] where @xmath139 is the initial probability distribution . for @xmath140 ,",
    "the escape rate is @xmath141 furthermore for @xmath142 the transition rates are @xmath143 .    as illustrated in fig .",
    "[ fig5 ] , the path @xmath134 has @xmath144 jumps for which the variable @xmath41 changes and @xmath145 jumps for which the variable @xmath42 changes . due to the bipartite form of the network of states , there are no jumps where both variables change , which implies @xmath146 .",
    "we denote the time intervals between jumps for the trajectory @xmath147 by @xmath148 , with @xmath149 .",
    "similarly , for the trajectory @xmath150 we have @xmath151 , with @xmath152 . in fig . [ fig5 ] , an example of a trajectory with @xmath153 jumps",
    "is shown .",
    "process jumps @xmath154 times and the @xmath41 and @xmath42 process each jumps @xmath155 times , i.e. , @xmath156 . ]",
    "the random matrix @xmath157 is defined by its elements @xmath158 , which are the transition rate @xmath159 if @xmath160 and @xmath161 otherwise .",
    "more precisely , we can define @xmath157 using its relation with the matrix @xmath104 , defined in ( [ defty ] ) , which is @xmath162 where @xmath163 is the @xmath106 identity matrix and @xmath164 is the kronecker delta .",
    "in addition , we define the matrix @xmath165 similarly to the discrete time case , for which equation ( [ prody ] ) holds , from the master equation , we obtain @xmath166= { } & \\mathbf{v } \\boldsymbol{\\mathcal{f}}_{y_{n_y}}(\\tau_{n_y}^{(y)})\\boldsymbol{\\mathcal{t}}(y_{n_y},y_{n_y-1})\\boldsymbol{\\mathcal{f}}_{y_{n_y-1}}(\\tau_{n_y-1}^{(y)})\\nonumber\\\\ & \\times\\ldots \\boldsymbol{\\mathcal{t}}(y_2,y_1)\\boldsymbol{\\mathcal{f}}_{y_1}(\\tau_1^{(y)})\\boldsymbol{\\mathcal{t}}(y_1,y_0)\\boldsymbol{\\mathcal{f}}_{y_0}(\\tau_0^{(y ) } ) \\mathbf{p}_{y_0}. \\label{prodconty}\\end{aligned}\\ ] ]    moreover , the same expression is valid for the probability density of the @xmath41 time - series , i.e. , @xmath167= { } & \\mathbf{v } \\boldsymbol{\\mathcal{f}}_{x_{n_x}}(\\tau_{n_x}^{(x)})\\boldsymbol{\\mathcal{t}}(x_{n_x},x_{n_x-1})\\boldsymbol{\\mathcal{f}}_{x_{n_x-1}}(\\tau_{n_x-1}^{(x)})\\nonumber\\\\   & \\times\\ldots \\boldsymbol{\\mathcal{t}}(x_2,x_1)\\boldsymbol{\\mathcal{f}}_{x_1}(\\tau_1^{(x)})\\boldsymbol{\\mathcal{t}}(x_1,x_0)\\boldsymbol{\\mathcal{f}}_{x_0}(\\tau_0^{(x ) } ) \\mathbf{p}_{x_0}. \\label{prodcontx}\\end{aligned}\\ ] ] the matrix @xmath168 is now defined as @xmath169 where @xmath170 is given by ( [ deftx ] ) and @xmath171 is the @xmath172 identity matrix .",
    "the matrix @xmath173 is defined as @xmath174    in order to calculate the shannon entropy rates a procedure similar to the discrete time case method can be used : we generate a long continuous time series , with the waiting times , @xmath175 , with @xmath176 jumps , and estimate the non - markovian shannon entropy rates through the expressions @xmath177 we are assuming that @xmath178 and @xmath179 are large , so that boundary terms can be neglected and we can use any matrix norm .",
    "these products are also numerically calculated by normalizing after a certain number of steps and keeping track of the normalization factors .",
    "the result obtained with the continuous time method for the one spin system of fig .",
    "[ fig1 ] can be seen in fig .",
    "this method is more direct because for discrete time we have to obtain the result as a function of @xmath28 and then extrapolate for @xmath51 .",
    "moreover , when the probabilities of not jumping in discrete time are large , the continuous time method is computationally cheaper .",
    "the continuous time method we presented above is not restricted to the bipartite networks we consider in this paper : it could be applied for other kinds of coarse - graining .",
    "the method only depends on the fact that we can write the probability density of a trajectory as a product of random matrices .",
    "we now illustrate the main results of this paper , namely , the analytical upper bound and the continuous time numerical method , by considering the general four - state network shown in fig .",
    "[ fig6 ] , for which the one spin system of fig .",
    "[ fig1 ] is a particular example .",
    "since @xmath180 , there are four @xmath157 and four @xmath181 matrices , each of which is a two by two matrix . for the sake of clarity , let us write these matrices explicitly . using the superscript @xmath182 for the @xmath157 matrices and @xmath183 for the @xmath181 matrices , they are given by : @xmath184 @xmath185 @xmath186 @xmath187 in the following we treat two simple cases for which the rate of mutual information acquires a simple form in some limit .",
    "here we consider @xmath188 .",
    "for this choice of rates a jump in the @xmath42 process can happen only after a jump in the @xmath41 process . in this sense , @xmath42 follows @xmath41 . calculating the stationary probability distribution , we obtain for the upper bound on the rate of mutual information ( [ ubi ] ) the expression @xmath189}\\left(\\ln\\frac{k_2 + 2\\gamma}{\\gamma}+\\ln\\frac{k_3 + 2\\gamma}{\\gamma}\\right ) , \\label{upperbound1}\\ ] ] where @xmath190 .",
    "if we further assume @xmath191 and @xmath192 , the rate of mutual information can be obtained with the following heuristic argument .",
    "a typical time series of the full process is an alternating sequence of long time intervals of size @xmath193 with short time intervals of size @xmath194 .",
    "if we know the @xmath41 time series , we can predict in which of the @xmath195 intervals of size @xmath194 the @xmath42 jumps will take place . since this information amounting to @xmath196 occurs at the rate @xmath0 of the @xmath41 jumps ,",
    "we obtain that for @xmath192 @xmath197 more generally , for @xmath198 , from the same kind of argument , we obtain @xmath199 this expression is in agreement with the upper bound ( [ upperbound1 ] ) in the limit @xmath200 .     compared to the upper bound @xmath23 ( [ upperbound1 ] ) as a function of @xmath201 for @xmath202 , @xmath188 , @xmath203 , and @xmath204 . ]",
    "moreover , we can also understand the rate of mutual information in the limit @xmath205 .",
    "this corresponds to the case where the @xmath41 process becomes markovian and much faster than the @xmath42 process , therefore , as discussed in sect .",
    "[ sec4 ] the rate of mutual information should saturate the upper bound . suppose that we know the @xmath42 time series . in the time interval between two @xmath42 jumps",
    "there are many @xmath41 jumps and we have no information about the @xmath41 state during this time interval .",
    "when a @xmath42 jump takes place , we know the state @xmath41 with absolute precision , i.e. , if the @xmath42 jump is @xmath206 ( @xmath207 ) then the @xmath41 state is @xmath208 ( @xmath9 ) .",
    "furthermore , since the @xmath41 jumps are fast compared to @xmath209 , the time interval between two @xmath42 jumps is long enough for the @xmath41 process to decorrelate , so that the information obtained with an @xmath42 jump is completely new .",
    "the complete knowledge of a binary random variable accounts for @xmath210 of mutual information .",
    "the average rate of @xmath42 transitions is given by @xmath211 , where @xmath212 and @xmath213 denote the stationary probabilities of the states @xmath214 and @xmath215 defined in fig .",
    "this leads to the expression @xmath216 valid for @xmath205 . as expected",
    ", this form is also in agreement with the upper bound ( [ upperbound1 ] ) in the respective limit .",
    "[ fig7 ] , where we compare the analytical upper bound with the numerical result , demonstrates that in the limits @xmath217 and @xmath205 the upper bound and the numerical result indeed tend to the same value .      as a second example",
    ", we consider a network in equilibrium for which the rate of mutual information is nevertheless non - zero . in fig .",
    "[ fig6 ] , we set @xmath218 , @xmath219 , and @xmath220 . for this choice of rates",
    "detailed balance is fulfilled because the product of the transition rates for the clockwise cycle equals the product of the transition rates for the counterclockwise cycle .",
    "moreover , in the stationary state all states are equally probable .",
    "the upper bound on the rate of mutual information ( [ ubi ] ) is independent of @xmath0 and given by @xmath221 where @xmath222 and @xmath223 . as we show in fig .",
    "[ fig8 ] , the rate of mutual information tends to the upper bound in the limit @xmath224 .",
    "this is again in agreement with the discussion at the end of sect .",
    "[ sec4 ] , since the @xmath41 process is markovian and , in the limit @xmath224 , much faster than the @xmath42 process .",
    "moreover , similarly to the way we obtained the result ( [ mutuheu1 ] ) for the previous model , the rate of mutual information can be easily explained in this limit .",
    "the difference in relation to the previous explanation is that when an @xmath42 jump occurs the mutual information about the @xmath41 state is @xmath225 .",
    "this happens because if a @xmath42 jump occurs , then the probability of @xmath41 being in state @xmath9 is @xmath226 and in state @xmath208 is @xmath227 .",
    "as the average rate of a @xmath42 jump is simply @xmath228 , we obtain @xmath229 which is equal to the upper bound ( [ upperbound2 ] ) .",
    "compared to the upper bound @xmath23 ( [ upperbound2 ] ) as a function of @xmath202 .",
    "the other parameters are @xmath230 and @xmath231 , thus enforcing equilibrium . ]    more generally , if the only restrictions are @xmath218 and @xmath232 , then from the same kind of argument we obtain @xmath233 where @xmath234 and @xmath235 .",
    "this more general expression accounts for the results ( [ mutuheu1 ] ) and ( [ mutuheu2 ] ) .",
    "in this paper we have addressed the problem of calculating the rate of mutual information between two coarse - grained processes that together fully specify a continuous time markov process . to this end , we have developed a numerical method to estimate the shannon entropy rate of hidden markov processes from a continuous time series , generalizing the numerical method used in the discrete time case @xcite .",
    "moreover , for the class of bipartite markov processes we considered in this paper , we have obtained an expression for an upper bound on the rate of mutual information in terms of the stationary probability distribution . while this expression has some formal similarity with the one for the rate of thermodynamic entropy production , it has become clear that these two rates , in general , are not related through a simple inequality .",
    "as applications of the theory developed here we have studied three four - state systems each of which can serve as illustrating , _ inter alia _ , the apparent independence of the rate of mutual information from the rate of thermodynamic entropy production .",
    "first , the one spin system with time - varying magnetic field is arguably the simplest case which shows that in an non - equilibrium steady state the rate of mutual information is not bounded by the dissipation rate .",
    "second , for a four state network for which some transition rates are zero , the rate of mutual information is still well defined whereas the thermodynamic entropy production is not since the latter requires that each backward transition is possible with a finite rate as well .",
    "third , a four state system in equilibrium with zero thermodynamic entropy production can still have non - zero rate of mutual information .",
    "moreover , in these four - state systems it is typically possible to find , and to understand in simple terms , a limiting case for the rates such that the analytical upper bound on the rate of mutual information becomes saturated .    on the mathematical side , finding a general expression for the rate of mutual information at least for the bipartite case on which we focused is most likely as hard a problem as finding one for the shannon entropy rate of a non - markovian process . for interesting physical perspectives , the rate of mutual information could become particularly relevant for the emerging theories of both autonomous information machines and cellular sensing systems .",
    "in both cases , one could suspect that even though there is no simple bound between the information - theoretic and the thermodynamic rate of entropy production in general , in more specific settings these two quantities might obey relations still to be uncovered . the algorithm described here to calculate",
    "the former will help in generating the necessary data for any specific model network efficiently .",
    "support by the esf through the network epsd is gratefully acknowledged .",
    "the first upper bound @xmath236 can be easily calculated by using the conditional probability @xmath237 where @xmath238 .",
    "we here performed the substitutions @xmath239 , @xmath240 , and @xmath241 . using this formula in ( [ conddef ] ) we obtain",
    "@xmath242    moreover , @xmath243 up to order @xmath28 is given by the above formula for any finite @xmath29 . in order to demonstrate this we first rewrite ( [ conddef ] ) as @xmath244 where @xmath245 denotes the probability of having a sequence for which @xmath246 .",
    "for @xmath247 , the expression of the conditional probability @xmath248 has at least one transition probability term of order @xmath28 .",
    "therefore , as @xmath248 is at least a term of order @xmath28 , it is convenient to further rewrite the above expression as @xmath249 where in the first line we summed over the variables @xmath250 .",
    "the three following relations are important for the subsequent derivation .",
    "first , for @xmath247 , @xmath251 moreover , @xmath252 where @xmath253 is an integer and @xmath254 is a constant independent of @xmath28 .",
    "finally , the conditional probability distribution fulfills @xmath255 where @xmath256 is an integer and @xmath257 is a constant independent of @xmath28 . with these three relations ,",
    "the term in the second line in equation ( [ ure ] ) becomes @xmath258 where we used @xmath259 .",
    "for the term in the third line in equation ( [ ure ] ) we need the relations , @xmath260 and @xmath261 which lead to @xmath262 inserting ( [ ure1 ] ) and ( [ ure2 ] ) in ( [ ure ] ) we obtain @xmath263 therefore , since the @xmath42 process is stationary , from ( [ hy1 ] ) , we obtain for any finite @xmath29 @xmath264 applying the same method to the @xmath41 process we get , @xmath265"
  ],
  "abstract_text": [
    "<S> the problem of calculating the rate of mutual information between two coarse - grained variables that together specify a continuous time markov process is addressed . as a main obstacle , </S>",
    "<S> the coarse - grained variables are in general non - markovian , therefore , an expression for their shannon entropy rates in terms of the stationary probability distribution is not known . </S>",
    "<S> a numerical method to estimate the shannon entropy rate of continuous time hidden - markov processes from a single time series is developed . with this method </S>",
    "<S> the rate of mutual information can be determined numerically . moreover , </S>",
    "<S> an analytical upper bound on the rate of mutual information is calculated for a class of markov processes for which the transition rates have a bipartite character . </S>",
    "<S> our general results are illustrated with explicit calculations for four - state networks .    </S>",
    "<S> # 1    [ [ section ] ] * # 1 + * + + + + +    # 1 i </S>"
  ]
}