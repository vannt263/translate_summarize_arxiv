{
  "article_text": [
    "in this paper we investigate the estimation of an unknown deterministic signal that is observed through a deterministic data matrix under additive noise @xcite .",
    "although the data matrix and the output vector are not exactly known , estimates for both of them as well as uncertainty bounds on the estimates are given @xcite .",
    "when the model parameters are not known exactly , a popular method to estimate the desired signal is to use the robust ls method @xcite , since the performances of the classical ls estimators degrade significantly when the perturbations on the data matrix and the output vector are relatively high @xcite .",
    "a prevalent approach to find robust solutions to such estimation problems is the robust ls method @xcite , in which the uncertainties in the data matrix and the output vector are incorporated into optimization framework via a minimax residual formulation .",
    "another well - known approach to compensate for errors in the data matrix and the output vector is the total least squares method ( tls ) @xcite , which may yield undesirable results since it employs a conservative approach due to data de - regularization .",
    "furthermore , the data matrix usually has a known special structure , such as toeplitz and hankel , in many linear regression problems @xcite and the performance of the estimators based on minimax approaches are shown to improve when such a prior knowledge on data matrix structure is integrated into the problem formulation @xcite .",
    "although the robust ls methods are able to minimize the ls error for the worst - case perturbations , they usually provide unsatisfactory results on the average @xcite due to their conservative nature . in order to counterbalance this conservative nature of the robust ls methods",
    "@xcite , we propose a novel robust ls approach that minimizes a worst case `` regret '' that is defined as the difference between the squared residual error and the smallest attainable squared residual error with an ls estimator . by this regret formulation ,",
    "we seek a linear estimator whose performance is as close as possible to that of the optimal estimator for all possible perturbations on the data matrix and the output vector .",
    "our main goal in proposing the minimax regret formulation is to provide a trade - off between the robust ls methods tuned to the worst possible data parameters ( under the uncertainty bounds ) and the optimal ls estimator tuned to the underlying unknown model parameters .",
    "furthermore , after studying the data estimation problems in the presence of bounded data uncertainties , we extend the regret formulation to the regularized ls problem , where the regret is defined as the difference between the cost of the regularized ls algorithm @xcite , and the smallest attainable cost with a linear regularized ls estimator .",
    "furthermore , we extend our discussions to scenarios involving both structured and unstructured data . under these frameworks ,",
    "we provide the solutions for the proposed regret based minimax ls and the regret based minimax regularized ls approaches in semi - definite programming ( sdp ) forms .",
    "we emphasize that sdp problems can be efficiently solved even for real - time applications @xcite .",
    "minimax regret approaches have been presented in signal processing literature to alleviate the pessimistic nature of the worst case optimization methods @xcite . however , we emphasize that the methods proposed in this paper extensively differ from @xcite .",
    "note that the optimization frameworks investigated here are different than @xcite , where the regret terms are directly adjoined in the cost functions .",
    "although a similar regret notion is used in @xcite , the cost function as well as the constraints on uncertainties in the data matrix and the output vector are substantially different in this paper . moreover , unlike this paper , in @xcite , the problem is described for the channel equalization scenario , where the authors rely on the statistical assumptions . furthermore , we note that the uncertainty is in the statistics of the transmitted signal in @xcite . in @xcite and @xcite ,",
    "the uncertainty is in the transmitted signal and the channel parameters , respectively . unlike these relevant works , in this paper ,",
    "the uncertainty is both on the data matrix and the output vector .",
    "furthermore , the solutions to the ls problems presented in this paper can not be obtained from @xcite , since the cost functions are different in our optimization formulations .",
    "while in @xcite , the authors have considered a similar framework , the results of this paper builds upon them and provide a complete solution to the regret based robust ls estimation methods unlike @xcite .",
    "we emphasize that perturbation bounds on the data matrix and the output vector heavily depend on the estimation algorithms employed to obtain them .",
    "since our methods are formulated for given perturbation bounds , different estimation algorithms can be readily incorporated into our framework with the corresponding perturbation bounds @xcite .    in this paper , we first present a novel robust ls approach in which we seek to find the transmitted signal by minimizing the worst case regret , i.e. , the worst case difference between the residual error of the ls estimator and the residual error with the optimal ls estimator . in this sense ,",
    "our aim is to introduce a trade off between the performance of the robust ls methods and the tuned ls estimator ( ls estimator that is tuned to the unknown data matrix and the output vector ) .",
    "we next propose a minimax regret method for the regularized ls problem .",
    "finally , we introduce a structured robust ls method in which the data matrix has a special structure such as toeplitz and hankel .",
    "we demonstrate that the proposed robust methods can be cast as sdp problems . in our simulations , we observe that these approaches provide better performance compared to the robust methods that are optimized with respect to the worst - case residual error @xcite , the tuned ls and the tuned regularized ls estimators ( tuned to the estimates of the data matrix and the output vector ) , respectively .",
    "our main contributions in this paper are as follows . _",
    "i ) _ we introduce a novel and efficient robust ls estimation method in which we find the transmitted signal by minimizing the worst - case regret , i.e. , the worst - case difference between the residual error of the ls estimator and the residual error of the optimal ls estimator tuned to the underlying model . in this sense , we present a robust estimation method that achieves a tradeoff between the robust ls estimation methods and the direct ls estimation method tuned to the estimates of the data matrix and output vector . _",
    "ii ) _ we next propose a minimax regret formulation for the regularized ls estimation problem . _ iii ) _ we then introduce a structured robust ls estimation method in which the data matrix is known to have a special structure such as toeplitz or hankel . _",
    "iv ) _ we demonstrate that the robust estimation methods we propose can be cast as sdp problems , hence our methods can be efficiently applied for real - time @xcite . _",
    "iv ) _ in our simulations , we observe that our approaches provide better performance compared to the robust methods that are optimized with respect to the worst - case residual error @xcite , and the conventional methods that directly solve the estimation problem using the perturbed data .",
    "the organization of the paper is as follows .",
    "an overview to the problem is provided in section [ sec : system ] . in section [ sec : urls ] , we first introduce the ls estimation method based on our regret formulation , and then present the regularized ls estimation approach in section [ sec : urrls ] .",
    "we then consider the structured ls approach in section [ sec : srls ] and provide the explicit sdp formulations for all problems .",
    "the numerical examples are demonstrated in section [ sec : numer ] .",
    "finally , the paper concludes with certain remarks in section [ sec : conc ] .",
    "in this paper , all vectors are column vectors and represented by boldface lowercase letters .",
    "matrices are represented by boldface uppercase letters . for a matrix @xmath0",
    ", @xmath1 is the conjugate transpose , @xmath2 is the spectral norm , @xmath3 is the pseudo - inverse , @xmath4 represents a positive definite matrix and @xmath5 represents a positive semi - definite matrix . for a square matrix @xmath0 , @xmath6 is the trace .",
    "naturally , for a vector @xmath7 , @xmath8 is the @xmath9-norm . here",
    ", @xmath10 denotes a vector or matrix with all zero elements and the dimensions can be understood from the context .",
    "similarly , @xmath11 represents the appropriate sized identity matrix .",
    "the operator @xmath12 is the vectorization operator , i.e. , it stacks the columns of a matrix of dimension @xmath13 into a @xmath14 column vector . finally , the operator @xmath15 is the kronecker product @xcite .",
    "we investigate the problem of estimating an unknown deterministic vector @xmath16 which is observed through a deterministic data matrix .",
    "however , instead of the actual data matrix and the output vector , their estimates @xmath17 and @xmath18 and uncertainty bounds on these estimates are provided . in this sense , our aim is to find a solution to the following data estimation problem @xmath19 such that @xmath20 for deterministic perturbations @xmath21 , @xmath22 .",
    "although these perturbations are unknown , a bound on each perturbation is provided , i.e. , @xmath23 where @xmath24 . in this sense , we refrain from any assumptions on the data matrix and the output vector , yet consider that the estimates @xmath0 and @xmath25 are at least accurate to `` some degree '' but their actual values under these uncertainties are completely unknown to the estimator .    even in the presence of these uncertainties ,",
    "the symbol vector @xmath7 can be naively estimated by simply substituting the estimates @xmath0 and @xmath25 into the ls estimator @xcite .",
    "for the ls estimator we have @xmath26 where @xmath3 is the pseudo - inverse of @xmath0 @xcite .",
    "however , this approach yields unsatisfactory results , when the errors in the estimates of the data matrix and the output vector are relatively high @xcite . a common approach to find a robust solution is to employ a worst - case residual minimization @xcite @xmath27 where @xmath7 is chosen to minimize the worst - case residual error in the uncertainty region .",
    "however , since the solution is found with respect to the worst possible data matrix and output vector in the uncertainty regions , it may be highly conservative @xcite .    here",
    ", we propose a novel ls estimation approach that provides a tradeoff between performance and robustness in order to mitigate the conservative nature of the worst - case residual minimization approach as well as to preserve robustness @xcite .",
    "the regret for not using the optimal ls estimator is defined as the difference between the residual error with an estimate of the input vector and the residual error with the optimal ls estimator , i.e. , @xmath28 by making such a regret definition , we force our estimator not to construct the symbol vector according to the worst possible scenario considering that it may be too conservative .",
    "instead , we define the regret of any estimator by the difference in the estimation performances of that estimator and the `` smartest '' estimator knowing both data matrix and output vector in hindsight , so that we achieve a tradeoff between robustness and estimation performance .",
    "we emphasize that the regret defined in is completely different than the regret formulation introduced in @xcite . in",
    ", the uncertainty is on the data matrix where the desired data vector @xmath7 is completely unknown , unlike @xcite .",
    "we emphasize that we use the residual error @xmath29 instead of the estimation error @xmath30 since the estimation error directly depends on the vector @xmath7 and can not be used in the regret formulation since @xmath7 is assumed to be unknown in the presence of data uncertainties .",
    "moreover , in our formulation , the estimate @xmath31 is not constrained to be linear unlike @xcite since our regret formulation is well - defined without any limitations on the estimated @xmath31 .    in the next sections , the proposed approaches to the robust ls estimation problems are provided .",
    "we first introduce the regret based unstructured ls estimation method .",
    "we next present the unstructured regularized ls estimation approach in which the worst - case regret is optimized .",
    "finally , we investigate the structured ls estimation approach .",
    "in this section , we provide a novel robust unstructured ls estimator based on a certain minimax criterion . we consider the most generic estimation problem @xmath32 where @xmath33 is defined as in .",
    "now considering the second term in , we define @xmath34 , @xmath35 , where @xmath36 is a full rank matrix , and denote the estimation performance of the optimal ls estimator for some given @xmath36 and @xmath37 by @xmath38 since we consider an unconstrained minimization over @xmath39 , we have @xcite @xmath40 as the optimal data vector minimizing the residual error . then we have @xmath41 where the third line follows from @xmath42 @xcite and @xmath43 is the projection matrix of the space perpendicular to the range space of @xmath36 .",
    "if we use the taylor series expansion based on wirtinger calculus @xcite for @xmath44 around @xmath45 and @xmath46 , then @xmath47 \\right ) \\right\\ } + o\\left ( { \\left|\\left|\\left[{\\delta{{\\mbox{$\\mathbf{h}$}}}}\\hspace{0.1 in } { \\delta{{\\mbox{$\\mathbf{y}$}}}}\\right]\\right|\\right|}^2 \\right).\\ ] ] note that the first order taylor approximation is introduced in order to obtain a tractable solution .",
    "clearly , the effect of using this approximation vanishes as @xmath48\\right|\\right|}$ ] decreases and for distortions with larger @xmath48\\right|\\right|}$ ] , one can easily use higher order approximations instead .",
    "however , we observe through our simulations that even for relatively large perturbations , a satisfactory performance is obtained using this approximation .",
    "we now introduce the following lemma in order to obtain the first order taylor approximation in in a closed form .",
    "[ lem1 ] let @xmath49 be a full rank matrix and @xmath50 , where @xmath51 and @xmath52 .",
    "then defining @xmath53 , where @xmath54 , we have @xmath55 and @xmath56 where @xmath57    since @xmath36 is full rank and @xmath58 , the pseudo - inverse of @xmath36 is found by @xcite @xmath59 hence , we have @xcite @xmath60 and @xmath61 where the last line of the equality follows since @xmath62 is a symmetric matrix according to the definition of the pseudo - inverse operation .",
    "this concludes the proof of lemma 1 .",
    "@xmath63    now turning our attention back to , we denote @xmath64 and @xmath65 where we emphasize that the closed form definitions of @xmath66 and @xmath67 can be obtained from lemma 1 .",
    "we then approximate and obtain the first order taylor approximation as follows @xmath68^h \\left[{\\delta{{\\mbox{$\\mathbf{h}$}}}}\\;\\ ; { \\delta{{\\mbox{$\\mathbf{y}$}}}}\\right ] \\right ) \\right\\ } { \\nonumber}\\\\                & = \\kappa + 2\\operatorname{re}\\left\\ { \\left({\\mathrm{vec}}({{\\mbox{$\\mathbf{d}$}}})^h { \\mathrm{vec}}({\\delta{{\\mbox{$\\mathbf{h}$ } } } } ) + { { \\mbox{$\\mathbf{b}$}}}^h { \\delta{{\\mbox{$\\mathbf{y}$}}}}\\right ) \\right\\ } { \\nonumber}\\\\                & = \\kappa + { { \\mbox{$\\mathbf{d}$}}}^h { \\delta{\\mbox{$\\mathbf{h}$}}}+ { \\delta{\\mbox{$\\mathbf{h}$}}}^h { { \\mbox{$\\mathbf{d}$}}}+ { { \\mbox{$\\mathbf{b}$}}}^h { \\delta{{\\mbox{$\\mathbf{y}$}}}}+ { \\delta{{\\mbox{$\\mathbf{y}$}}}}^h { { \\mbox{$\\mathbf{b}$}}},\\end{aligned}\\ ] ] where @xmath69 , @xmath70 , and @xmath71 .",
    "hence we can approximate the regret in as follows @xmath72    in the following theorem , we illustrate how the optimization ( or equivalently estimation ) problem in can be put in an sdp form .",
    "[ thm1 ] let @xmath17 and @xmath73 be the estimates of the data matrix and the output vector , respectively , both having deterministic additive perturbations @xmath74 and @xmath75 , respectively , i.e. , @xmath49 and @xmath50 , where @xmath36 is the full rank data matrix , @xmath37 is the output vector , and @xmath58 .",
    "then the problem @xmath76 where @xmath33 is defined as in , is equivalent to solving the following sdp problem @xmath77 where @xmath78 is the @xmath79 matrix defined as @xmath80 .",
    "the proof of theorem 1 is provided in [ app : pot1 ] .    in the proof of theorem 1 , we use proposition 1 that relies on the lossless _ s_-procedure . however , _",
    "s_-procedure is lossless with two constraints when the corresponding two quadratic ( hermitian ) forms on the complex linear space @xcite .",
    "however , classical _",
    "s_-procedure for quadratic forms is , in general , lossy with two constraints in the real case @xcite .",
    "hence , theorem 1 can not be extended for real linear space .",
    "now we can consider two important corollaries of theorem 1 .",
    "first , a special case of theorem 1 in which the uncertainty is only in the data matrix .",
    "we emphasize that the perturbation errors only in the data matrix are also common in a wide range of real life applications @xcite . here",
    ", we can define the regret as follows @xmath81 and similar to the previous case , we calculate the optimal estimation performance under a given uncertainty bound @xmath82 hence we approximate the regret in as follows @xmath83    [ cor1 ] let @xmath17 and @xmath73 be the estimates of the data matrix and the output vector , respectively , where @xmath58 .",
    "suppose there is a bounded uncertainty on the full rank data matrix @xmath36 , i.e. , @xmath49 , @xmath84 .",
    "then the problem @xmath85 where @xmath86 is defined as in , is equivalent to solving the following sdp problem @xmath87    the proof of corollary 1 can be explicitly derived from the proof of theorem 1 by simply setting @xmath88 and @xmath89 , hence is omitted .",
    "@xmath63    second , we consider another special case of theorem 1 in which the uncertainty is only in the output vector .",
    "we emphasize that similar to the previous case , this one is also a common case in a wide range of real - life applications @xcite , and studied under a similar framework in @xcite . here , we can define the regret as follows @xmath90 and similar to the previous case , we calculate the optimal also performance under a given uncertainty bound @xmath91 hence we approximate the regret in as follows @xmath92    [ cor2 ] let @xmath17 and @xmath73 be the estimates of the data matrix and the output vector , respectively , where @xmath58 .",
    "suppose there is a bounded uncertainty on the output vector @xmath37 , i.e. , @xmath50 , @xmath93 .",
    "then the problem @xmath94 where @xmath95 is defined as in , is equivalent to solving the following sdp problem @xmath96    the proof of corollary 2 can be explicitly derived from the proof of theorem 1 by simply setting @xmath97 and @xmath98 , hence is omitted .",
    "corollaries 1 and 2 follows from the proof of theorem 1 , which relies on the lossless _ s_-procedure . under the frameworks presented in the corollaries 1 and 2",
    ", one can safely extend the same conclusions for the real case also , since _",
    "s_-procedure is lossless for quadratic forms with one constraint both in complex and real spaces @xcite .",
    "in this section , we introduce a worst - case regret optimization approach to solve the regularized ls estimation problem in @xcite . the regret for not using the optimal regularized ls estimator",
    "is defined by @xmath99 where @xmath100 is the regularization parameter .",
    "we emphasize that there are different approaches to choose @xmath101 , however , for the focus of this paper , we assume that it is already set before the optimization so that these methods can be readily incorporated in our framework . hence , we solve the regularized ls estimation problem for an arbitrary @xmath100 and note that we have already covered the @xmath102 case in section [ sec : urls ] .",
    "similar to the previous case , we denote the estimation error of the optimal ls estimator for some estimated data matrix @xmath0 and output vector @xmath25 by @xmath103 where @xmath104 . considering the first order taylor series expansion based on wirtinger calculus @xcite for @xmath105 around @xmath45 and @xmath46 @xmath106 \\right ) \\right\\ } , { \\nonumber}\\\\              & = \\kappa + { { \\mbox{$\\mathbf{d}$}}}^h { \\delta{\\mbox{$\\mathbf{h}$}}}+ { \\delta{\\mbox{$\\mathbf{h}$}}}^h { { \\mbox{$\\mathbf{d}$}}}+ { { \\mbox{$\\mathbf{b}$}}}^h { \\delta{{\\mbox{$\\mathbf{y}$}}}}+ { \\delta{{\\mbox{$\\mathbf{y}$}}}}^h { { \\mbox{$\\mathbf{b}$ } } } , { \\nonumber}\\end{aligned}\\ ] ] where @xmath107 , @xmath71 , @xmath108 and @xmath109 where the last line follows since @xmath110 is symmetric .",
    "hence we can approximate the regret in as follows @xmath111 similar to . in the following theorem",
    ", we illustrate how the optimization problem in can be put in an sdp form .",
    "[ thm2 ] let @xmath17 and @xmath73 be the estimates of the data matrix and the output vector , respectively , both having deterministic additive perturbations @xmath74 and @xmath75 , respectively , i.e. , @xmath49 and @xmath50 , where @xmath36 is the full rank data matrix , @xmath37 is the output vector , and @xmath58 .",
    "then the problem @xmath112 where @xmath33 is defined as in , is equivalent to solving the following sdp problem @xmath113    the proof of theorem 2 follows similar lines to the proof of theorem 1 , hence is omitted here .",
    "@xmath63    under the framework introduced in this section , one can straightforwardly obtain the corollaries similar to corollaries 1 and 2 by considering cases in which the uncertainty is either only on the data matrix or only on the output vector , i.e. , @xmath88 and @xmath97 cases , respectively .",
    "the derivations follow similar lines to corollaries 1 , 2 and theorem 2 , hence is omitted .",
    "however , similar results can be readily derived from the result in theorem 2 with suitable changes in the sdp formulations .",
    "there are various communication systems where the data matrix and the perturbation on it have a special structure such as toeplitz , hankel , or vandermonde @xcite . incorporating this prior knowledge into the estimation framework",
    "could improve the performance of the regret based minimax ls estimation approach @xcite .",
    "hence , in this section , we investigate a special case of the problem in , where the associated perturbations for the data matrix @xmath0 and the output vector @xmath25 have special structures .",
    "the structure on the perturbations is defined as follows @xmath114 and @xmath115 where @xmath116 , @xmath117 , and @xmath118 are known but @xmath119 , @xmath120 , are unknown .",
    "however , the bounds on the norm of @xmath121^h$ ] and @xmath122^h$ ] are provided as @xmath123 and @xmath124 , where @xmath125 .",
    "we emphasize that this formulation can represent a wide range of constraints on the structure of perturbations of the data matrix and the output vector such as toeplitz and hankel @xcite .",
    "our aim is to solve the following optimization problem @xmath126 where @xmath127    after following similar lines to section [ sec : urls ] , and introducing the first order taylor approximation to @xmath128 around @xmath129 and @xmath130 , we obtain @xmath131 \\right ) \\right\\},\\ ] ] where @xmath132 and @xmath133 .",
    "we next introduce the following lemma to calculate the first order taylor approximation in in a closed form .",
    "[ lem2 ] let @xmath49 be a full rank matrix and @xmath50 , where @xmath51 , @xmath52 , @xmath134 and @xmath135 are defined as in and , respectively .",
    "then denoting @xmath136 , where @xmath54 , we have @xmath137^h,\\ ] ] and @xmath138^h,\\ ] ] where @xmath57 .    note that the derivative of @xmath44 is taken with respect to @xmath139 $ ] , hence we can use the chain rule to calculate the derivatives by using the results we have obtained in lemma 1 .",
    "first , we consider the derivative of @xmath44 with respect to @xmath140 , @xmath141 , i.e. , @xmath142 where the last line follows from the cyclic property of the trace operator .",
    "similarly , we next consider the derivative of @xmath44 with respect to @xmath143 , @xmath141 , i.e. , @xmath144 this concludes the proof of lemma 2 .",
    "@xmath63    now turning our attention back to , we denote @xmath145 and @xmath146 where we emphasize that the closed form definitions of @xmath147 and @xmath67 can be obtained from lemma 2 .",
    "we then approximate and obtain the first order taylor approximation as follows @xmath148 therefore , we can approximate the regret in as follows @xmath149    in the following theorem , we illustrate how the optimization problem in can be put in an sdp form .",
    "[ thm3 ] let @xmath150 , @xmath151 , @xmath152 , @xmath153 , where @xmath36 is the full rank data matrix defined as in , @xmath37 is the output vector defined as in , with the corresponding estimates @xmath0 and @xmath25 , respectively .",
    "then the problem @xmath154 where @xmath33 is defined as in , is equivalent to solving the following sdp problem @xmath155 where @xmath156 $ ] and @xmath157 $ ] .",
    "the proof of theorem 2 follows similar lines to the proof of theorem 1 , hence is omitted here .",
    "@xmath63    under the framework introduced in this section , one can straightforwardly obtain the corollaries similar to corollaries 1 and 2 by considering cases in which the uncertainty is either only on the data matrix or only on the output vector , i.e. , @xmath158 and @xmath159 cases , respectively .",
    "the derivations follow similar lines to corollaries 1 , 2 and theorem 3 , hence is omitted .",
    "however , similar results can be readily derived from the result in theorem 3 with suitable changes in the sdp formulations .",
    "the proofs of theorem 2 and theorem 3 follow from the results of theorem 1 , which relies on the lossless _ s_-procedure . however , _",
    "s_-procedure is lossless with two constraints when the corresponding two quadratic ( hermitian ) forms on the complex linear space @xcite .",
    "however , classical _",
    "s_-procedure for quadratic forms is , in general , lossy with two constraints in the real case @xcite .",
    "hence , theorem 2 and theorem 3 can not be extended for real linear space . on the other hand , under the frameworks described in remark 3 and remark 4",
    ", one can safely extend the same conclusions for the real case also , since _",
    "s_-procedure is lossless for quadratic forms with one constraint both in complex and real spaces @xcite .",
    "we provide numerical examples in different scenarios in order to illustrate the merits of the proposed algorithms . in the first set of the experiments , we randomly generate a data matrix of size @xmath160 , and an output vector of size @xmath161 , which are normalized to have unit norms .",
    "then , we generate @xmath162 random perturbations @xmath134 , @xmath135 , where @xmath163 , @xmath164 , @xmath165 , @xmath166 , and @xmath167 . here",
    ", we label the algorithm in theorem 1 as `` rgrt - ls '' , the robust ls algorithm of @xcite as `` rbst - ls '' , the total ls algorithm @xcite as `` tls '' , and finally the ls algorithm tuned to the estimates of the data matrix and the output vector as `` ls '' , where we directly use @xmath168 .",
    "trials when @xmath169 , @xmath165 , and @xmath166.,title=\"fig:\",scaledwidth=75.0% ] +    for each algorithm and for each random perturbation , we find the corresponding @xmath31 and calculate the error @xmath170 . after we calculate the errors for each algorithm and for all random perturbations , we plot the corresponding sorted errors in ascending order in fig . 1 for 1000 perturbations .",
    "since the rbst - ls algorithm optimizes the worst - case residual error with respect to worst possible disturbance , it usually yields the smaller worst - case residual error among all algorithms for these simulations . on the other hand ,",
    "since the ls algorithm directly uses the estimates , it usually yields the smaller residual error when the perturbations on the data matrix and the output vector are significantly small .",
    "these results can be observed in fig .",
    "1 , where in one extreme , the largest residual errors are observed as @xmath171 for the tls estimator , @xmath172 for the ls estimator , @xmath173 for the rbst - ls estimator , and @xmath174 for the rgrt - ls estimator . in the other extreme ,",
    "i.e. , when there is almost no perturbation , the smallest estimation errors are observed as @xmath175 for the ls estimator , @xmath176 for the tls estimator , @xmath177 for the rbst - ls estimator , and @xmath178 for the rgrt - ls estimator .",
    "while the ls estimator can be preferable when there is relatively smaller perturbations and the rbst - ls estimator can be preferable when there is significantly higher perturbations , the introduced algorithm provides a tradeoff between these algorithms and achieve a significantly smaller average error performance .",
    "the average residual error of the rgrt - ls estimator is observed as @xmath179 , whereas this value is @xmath180 for the ls estimator , @xmath181 for the rbst - ls estimator , and @xmath182 for the tls estimator .",
    "hence , the rgrt - ls estimator is not only robust but also efficient in terms of the average error performance compared to its well - known alternatives .",
    "trials when @xmath183 $ ] , @xmath165 , and @xmath166.,title=\"fig:\",scaledwidth=75.0% ] +    for the second experiment , we generate @xmath162 random perturbations @xmath134 , @xmath135 , where @xmath163 , @xmath164 , @xmath165 , @xmath166 for different perturbation bounds and compute the averaged error over @xmath162 trials for the rgrt - ls , the ls , the rbst - ls , and the tls algorithms . in fig .",
    "2 , we present the averaged residual errors for these algorithms for different values of perturbation bounds , i.e. , @xmath184 $ ] .",
    "we observe that the proposed rgrt - ls algorithm has the best average residual error performance over different perturbation bounds compared to the ls , the rbst - ls and the tls algorithms .",
    "trials when @xmath185 , @xmath165 , and @xmath166.,title=\"fig:\",scaledwidth=75.0% ] +    as can be observed from fig .",
    "2 , as the perturbation bounds increase , the performances of the ls and the tls estimators significantly deteriorate , whereas the performance of the rbst - ls estimator almost does not change .",
    "the residual error of the rgrt - ls estimator , on the other hand , slightly increases as the perturbation bounds increase , yet the robustness of this estimator can be observed in fig .",
    "2 . furthermore , the rgrt - ls estimator significantly outperforms its competitors in terms of the average error performance by introducing a transition between the best - case performance of the ls estimator and the worst - case performance of the rbst - ls estimator .    in the next experiment",
    ", we examine a system identification problem @xcite , which can be formulated as @xmath186 , where @xmath187 is the observed noisy toeplitz matrix and @xmath188 is the observed noisy output vector . here , the convolution matrix @xmath0 ( which is toeplitz ) constructed from @xmath189 which is selected as a random sequence of @xmath190 s . for a randomly generated filter @xmath189 of length @xmath191",
    ", we generate @xmath162 random structured perturbations for @xmath192 and @xmath193 , where @xmath194 , and plotted the sorted estimation errors in ascending order in fig .",
    "trials when @xmath195 , @xmath196 , @xmath197 , and @xmath198.,title=\"fig:\",scaledwidth=75.0% ] +    the average residual errors , on the other hand , are observed as @xmath199 for the structured regret ls estimator  str - rgrt - ls \" of remark 4 , @xmath200 for the structured robust ls algorithm `` str - rbst - ls '' , @xmath201 for the tls estimator , @xmath202 for the ls estimator , and @xmath203 for the structured least squares bounded data uncertainties estimator , labeled as `` sls - bdu '' and presented in @xcite .",
    "therefore , we observe that the str - rgrt - ls algorithm yields the smallest average residual error among its competitors . in addition , we observe that the str - rgrt - ls estimator has a smaller residual error in most of the trials compared to its well - known alternatives , owing to its novel regret formulation .    finally , in fig .",
    "4 , we provide errors sorted in ascending order for the algorithm in theorem 2 as `` rgrt - reg - ls '' , for the robust regularized ls algorithm in @xcite as `` rbst - reg - ls '' and finally for the regularized ls algorithm as `` reg - ls '' @xcite , where the experiment setup is the same as in the first experiment except the perturbation bounds are set to @xmath204 and the regularization parameter is chosen as @xmath205 . in fig .",
    "4 , we observe that the robustness and the performance tradeoff ( between the rbst - reg - ls and the reg - ls algorithms ) of the introduced rgrt - reg - ls algorithm .",
    "when there is small perturbations on the data matrix and the output vector , i.e. , in the best - case scenario , the residual error of the reg - ls estimator is @xmath206 , whereas it is @xmath207 for the rgrt - reg - ls estimator and @xmath208 for the rbst - reg - ls estimator . as can be observed from fig .",
    "4 , for higher perturbations , the performance of the reg - ls estimator significantly deteriorates , whereas the rgrt - reg - ls and rbst - reg - ls algorithms provide a robust performance . on the other hand",
    ", the rgrt - reg - ls estimator significantly outperforms the rbst - reg - ls estimator in terms of the average error performance and achieves even a more desirable error performance compared to the reg - ls estimator .",
    "the average residual errors are calculated as @xmath209 for the rgrt - reg - ls estimator , @xmath210 for the reg - ls estimator , and @xmath211 for the rbst - reg - ls estimator .",
    "in this paper , we introduce a robust approach to ls estimation problems under bounded data uncertainties based on a novel regret formulation .",
    "we study the robust ls estimation problems in the presence of unstructured and structured perturbations under residual and regularized residual error criteria . in all cases ,",
    "the data vectors that minimize the worst - case regrets are found by solving certain sdp problems . in our simulations",
    ", we observed that the proposed estimation methods provide an efficient tradeoff between the performance and robustness , better than the best available alternatives in different signal processing applications .",
    "before we introduce the proof of theorem 1 , we need the following proposition that follows proposition 2 of @xcite .",
    "[ prop1 ] given matrices @xmath212 , @xmath213 , @xmath214 , @xmath215 , @xmath216 , where @xmath216 is a hermitian matrix , i.e. , @xmath217 , @xmath218 @xmath219 , if and only if there exist @xmath220 such that @xmath221    following similar lines to @xcite , we first note that @xmath218 @xmath219 , if and only if for every @xmath222 we have @xmath223 where the last line follows from the cauchy - schwartz inequality by choosing @xmath224 and @xmath225 furthermore , from the cauchy - schwartz inequality , can be written as @xmath226 @xmath227 .",
    "note that the constraint @xmath228 is equivalent to @xmath229 and similarly , @xmath230 is equivalent to @xmath231 hence , after some algebra we obtain as follows @xmath232 @xmath233 such that @xmath234 and @xmath235 then applying _",
    "s_-procedure @xcite , we have @xmath236 note that due to the structures of @xmath237 and @xmath238 , the regularity conditions can be easily verified . since @xmath239 are hermitian matrices , i.e. , @xmath240 , @xmath241 , by theorem 1.1 in @xcite , is satisfied if and only if @xmath242 such that @xmath243 that is @xmath244 this concludes the proof of proposition 1 .",
    "@xmath63    now we consider the minimax problem defined in , and reformulate it as follows @xmath245 subject to @xmath246 where @xmath247 and @xmath248 . by applying the schur complement to the constraints in",
    ", we can compactly denote in the matrix form as follows @xmath249 @xmath250 . rearranging terms in",
    ", we obtain @xmath251 @xmath250 , where we used @xmath252 , @xmath253 , and @xmath80 . by applying proposition 1 to",
    ", it follows that is equivalent to @xmath254 hence the desired result .",
    "therefore , this concludes the proof of theorem 1 .",
    "@xmath63          b.  zhang , s.  makram - ebeid , r.  prevost , g.  pizaine , fast solver for some computational imaging problems : a regularized weighted least - squares approach , digital signal processing 27  ( 0 ) ( 2014 ) 107  118 .",
    "r.  caballero - guila , a.  hermoso - carazo , j.  linares - prez , least - squares linear estimators using measurements transmitted by different sensors with packet dropouts , digital signal processing 22  ( 6 ) ( 2012 ) 1118  1125 .",
    "e.  e. kuruoglu , p.  j.  w. rayner , w.  j. fitzgerald , impulsive noise elimination using polynomial iteratively reweighted least squares , in : ieee digital signal processing workshop proceedings , 1996 , pp .",
    "347350 .",
    "n.  kalantarova , m.  a. donmez , s.  s. kozat , competitive least squares problem with bounded data uncertainties , in : ieee international conference on acoustics , speech and signal processing ( icassp ) , 2012 , pp .",
    "38413844 .",
    "a.  h. sayed , s.  chandrasekaran , estimation in the presence of multiple sources and levels of uncertainties with applications , in : proc .",
    "asimolar conference on signals , systems , and computers , vol .  2 , 1998 ,",
    ". 18111815 ."
  ],
  "abstract_text": [
    "<S> we study the problem of estimating an unknown deterministic signal that is observed through an unknown deterministic data matrix under additive noise . </S>",
    "<S> in particular , we present a minimax optimization framework to the least squares problems , where the estimator has imperfect data matrix and output vector information . we define the performance of an estimator relative to the performance of the optimal least squares ( ls ) estimator tuned to the underlying unknown data matrix and output vector , which is defined as the regret of the estimator . </S>",
    "<S> we then introduce an efficient robust ls estimation approach that minimizes this regret for the worst possible data matrix and output vector , where we refrain from any structural assumptions on the data . </S>",
    "<S> we demonstrate that minimizing this worst - case regret can be cast as a semi - definite programming ( sdp ) problem . </S>",
    "<S> we then consider the regularized and structured ls problems and present novel robust estimation methods by demonstrating that these problems can also be cast as sdp problems . </S>",
    "<S> we illustrate the merits of the proposed algorithms with respect to the well - known alternatives in the literature through our simulations .    </S>",
    "<S> data estimation , least squares , robust , minimax , regret . </S>"
  ]
}