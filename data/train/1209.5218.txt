{
  "article_text": [
    "according to the implementation of a differential equation , most approaches to continuous - time optimization can be classified as either a dynamical system @xcite,@xcite,@xcite or a neural network @xcite,@xcite,@xcite,@xcite .",
    "the dynamical system approach relies on the numerical integration of differential equations on a digital computer . unlike discrete optimazation methods",
    ", the step sizes of dynamical system approaches can be controlled automatically in the integration process and can sometimes be made larger than usual .",
    "this advantage suggests that the dynamical system approach can in fact be comparable with currently available conventional discrete optimal methods and facilitate faster convergence @xcite,@xcite .",
    "the application of a higher - order numerical integration process also enables us to avoid the zigzagging phenomenon , which is often encountered in typical linear extrapolation methods @xcite . on the other hand ,",
    "the neural network approach emphasizes implementation by analog circuits , very large scale integration , and optical technologies @xcite .",
    "the major breakthrough of this approach is attributed to the seminal work of hopfield , who introduced an artificial neural network to solve the traveling salesman problem ( tsp ) @xcite . by employing analog hardware ,",
    "the neural network approach offers low computational complexity and is suitable for parallel implementation .    for continuous - time equality - constrained optimization",
    ", existing methods can be classified into three categories @xcite : feasible point method ( or primal method ) , augmented function method ( or penalty function method ) , and the lagrangian multiplier method .",
    "determining whether one method outperforms the others is difficult because each method possesses distinct advantages and disadvantages .",
    "readers can refer to @xcite,@xcite,@xcite,@xcite and the references therein for details .",
    "the feasible point method directly solves the original problem by searching through the feasible region for the optimal solution .",
    "each point in the process is feasible , and the value of the objective function constantly decreases .",
    "compared with the two other methods , the feasible point method offers three significant advantages that highlight its usefulness as a general procedure that is applicable to almost all nonlinear programming problems @xcite : i ) the terminating point is feasible if the process is terminated before the solution is reached ; ii ) the limit point of the convergent sequence of solutions must be at least a local constrained minimum ; and iii ) the approach is applicable to general nonlinear programming problems because it does not rely on special problem structures such as convexity .    in this paper ,",
    "a continuous - time feasible point approach is proposed for equality - constrained optimization .",
    "first , the equality constraint is transformed into a continuous - time dynamical system with solutions that always satisfy the equality constraint .",
    "then , the singularity is explained in detail and a new projection matrix is proposed to avoid singularity .",
    "an update ( or say a controller ) is subsequently designed to decrease the objective function along the solutions of the transformed system .",
    "the invariance principle is applied to analyze the behavior of the solution .",
    "we also propose a modified approach for addressing cases in which solutions do not satisfy the equality constraint .",
    "finally , the proposed optimization approach is applied to two examples to demonstrate its effectiveness .",
    "local convergence results do not assume convexity in the optimization problem to be solved .",
    "compared with global optimization methods , local optimization methods are still necessary .",
    "first , they often server as a basic component for some global optimizations , such as the branch and bound method @xcite . on the other hand",
    ", they can require less computation for online optimization .",
    "compared with the discrete optimal methods offered by matlab , at least two illustrative examples show that the proposed approach avoids convergence to a singular point and facilitates faster convergence through numerical integration on a digital computer . in view of these , the contributions of this paper are clear and listed as follows .",
    "\\i ) a new projection matrix is proposed to remove a standard regularity assumption that is often associated with feasible point methods , namely that the gradients of constraints are linearly independent , see ( * ? ? ?",
    "* , equ.(4 ) ) , ( * ? ? ?",
    "* , equ.(2.3 ) ) , ( * ? ? ?",
    "* , assumption 1 ) .",
    "compared with a commonly - used modified projection matrix , the proposed projection matrix has better precision .",
    "moreover , its recursive form can be implemented more easily .",
    "\\ii ) based on the proposed matrix , a continuous - time , equality - constrained optimization method is developed to avoid convergence to a singular point .",
    "the invariance principle is applied to analyze the behavior of the solution .",
    "\\iii ) the modified version of the proposed optimization is further developed to address cases in which solutions do not satisfy the equality constraint .",
    "this ensures its robustness against uncertainties caused by numerical error or realization by analog hardware .",
    "we use the following notation .",
    "@xmath0 is euclidean space of dimension @xmath1 .",
    "@xmath2 denotes the euclidean vector norm or induced matrix norm .",
    "@xmath3 is the identity matrix with dimension @xmath4 @xmath5 denotes a zero vector or a zero matrix with dimension @xmath6 direct product @xmath7 and @xmath8  operation are defined in _",
    "appendix a_. the function @xmath9   _ { \\times}:\\mathbb{r } ^{3}$ ] @xmath10 @xmath11with matrix @xmath12 is defined in _ appendix b_.",
    "suppose @xmath13 @xmath14 the gradient of the function @xmath15 is given by @xmath16 @xmath17^{t}\\in\\mathbb{r } ^{n}\\ $ ] and the matrix of second partial derivatives of @xmath18 known as hessian is given by @xmath19 @xmath20 @xmath21 @xmath10 @xmath22 and @xmath23   _ { ij}.$ ]",
    "the class of equality - constrained optimization problems considered here is defined as follows:@xmath24 where @xmath25 @xmath26 is the objective function and @xmath27 @xmath28 @xmath29 @xmath30^{t}\\in\\mathbb{r } ^{m},$ ] @xmath31 are the equality constraints .",
    "they are both twice continuously differentiable .",
    "denote by @xmath32{cccc}\\nabla c_{1}\\left (   x\\right )   & \\nabla c_{2}\\left (   x\\right )   & \\cdots & \\nabla c_{m}\\left (   x\\right ) \\end{array } \\right ]   \\in\\mathbb{r } ^{n\\times m}.$ ] to avoid a trivial case , suppose the constraint ( or feasible set ) @xmath33 .",
    "* definition 1 * @xcite . for the problem ( [ optimazation ] ) ,",
    "a vector @xmath34 is a global minimum if @xmath35 @xmath36 a vector @xmath34 is a local ( strict local ) minimum if there is a neighborhood @xmath37 of @xmath38 such that @xmath39 for @xmath40    * definition 2 * @xcite .",
    "a vector @xmath34 is said to be a regular point if the gradient vectors @xmath41 are linearly independent .",
    "otherwise , it is called a singular point .",
    "this paper aims to propose an approach to continuous - time , equality - constrained optimization to identify the local minima based on a feedback control perspective .    * remark 1 .",
    "* inequality - constrained optimizations can be transformed into equality - constrained optimizations by introducing new variables .",
    "for example , the inequality constraint @xmath42 can be replaced with an equality constraint @xmath43 also , the inequality constraint @xmath44 can be replaced with an equality constraint @xmath45 here , we only focus on equality - constrained optimization .",
    "optimization problems are often solved by using numerical iterative methods . for an equality - constrained optimization problem ,",
    "the major difficulty lies in ensuring that each iteration satisfies the constraint and can further move toward the minimum . to address this difficulty , a transformation of the equality constraint is proposed , which is formulated as an assumption .    *",
    "assumption 1*. * * for a given @xmath46 there exists a function @xmath47 such that@xmath48 with solutions that satisfy @xmath49 where @xmath50 @xmath51 @xmath52 @xmath53 .    from a feedback control perspective",
    ", the update @xmath54 can be considered as a control input .",
    "the objective function @xmath55 can be considered a lyapunov - like function , although @xmath56 is not required to be a lyapunov function .",
    "based on _ assumption 1 _ , the objective of this paper can be restated as : to design a control input @xmath54 to decrease @xmath56 along the solutions of ( [ dynamical system ] ) until @xmath57 has achieved a local minimum . in the following , we will omit the variable @xmath58 except when necessary .",
    "* remark 2*. the proposition of _ assumption 1 _ is motivated by the property of attitude kinematics @xcite : @xmath59 , where @xmath60^{t}\\in\\mathbb{r } ^{4},$ ] @xmath61 and @xmath62 @xmath63_{\\times}^{t}]^{t}\\in\\mathbb{r } ^{4\\times3}.$ ] the function @xmath64   _ { \\times}:\\mathbb{r } ^{3}$ ] @xmath10 @xmath65 is defined in _",
    "appendix b. _ all solutions of the attitude kinematics satisfy the constraint @xmath66 driven by any @xmath67 .",
    "the explanation is given as follows .",
    "it is easy to check that @xmath68since @xmath69_{\\times}q=0 $ ] for@xmath70 therefore , the solution always satisfies the constraint @xmath71 if @xmath72 @xmath73 another representation of attitude kinematics is @xmath74   _ { \\times}r \\label{rotation}\\ ] ] where @xmath75 is a rotation matrix satisfying the constraint @xmath76 . for ( [ rotation ] ) ,",
    "we have@xmath77   _ { \\times}+\\left [   w\\right ]   _ { \\times}^{t}\\right )   r=0_{3\\times3}.\\end{aligned}\\ ] ] that is why the evolution of @xmath78 always lies on the constraint @xmath79    * remark 3*. the best choice of @xmath80 is to satisfy @xmath81 however , it is difficult to achieve .",
    "for example , if @xmath82 , @xmath83 @xmath84^{t}\\in\\mathbb{r } ^{2},$ ] then @xmath85 .",
    "since the two sets @xmath86 and @xmath87 are not connected , the solution of ( [ dynamical system ] ) starting from either set can not access the other .",
    "although @xmath88 , we still expect the global minimum @xmath89 that is why we often require that the initial value @xmath90 be close to the global minimum @xmath91 besides this , it is also expected that the function @xmath80 is chosen to make the set @xmath92  as large as possible so that the probability of @xmath93 is higher .",
    "if @xmath94 @xmath95 , then the function @xmath80 can be chosen to satisfy @xmath96",
    "* theorem 1*. suppose that @xmath98 and @xmath99 where @xmath100 is with full column rank , and the space spanned by the columns of @xmath100 is the null space of @xmath101 then @xmath96 @xmath97    _ proof . _ since @xmath102 the remaining task is to prove @xmath103 @xmath104  namely for any @xmath105 there exists a control input @xmath106 that can transfer any initial state @xmath107 to @xmath108 since @xmath109 there exist @xmath110 such that @xmath111 and @xmath112 by the definition of @xmath113 design a control input @xmath114{c}\\frac{1}{\\bar{t}}\\left (   \\bar{u}-u_{0}\\right )   , \\\\ 0 , \\end{array}\\begin{array } [ c]{c}0\\leq",
    "t\\leq\\bar{t}\\\\ t>\\bar{t}. \\end{array } \\right .   .\\ ] ] with the control input above , we have @xmath115 when @xmath116 .",
    "then @xmath117 @xmath118 hence @xmath119 @xmath97 consequently , @xmath120 @xmath97 @xmath121    from the proof of _ theorem 1 _ , the choice of @xmath80 becomes a controllability problem .",
    "however , it is difficult to obtain a controllability condition of a general nonlinear system .",
    "correspondingly , it is difficult to choose @xmath80 for a general nonlinear function @xmath122 to satisfy @xmath123 motivated by the linear case above , we aim to design a function @xmath124whose range is the null space of @xmath125 for any fixed @xmath126 this idea can be formulated as @xmath127 , where @xmath128",
    "the function @xmath129 is the projection matrix , which orthogonally projects a vector onto the null space of @xmath130 .",
    "one well - known projection matrix is given as follows @xcite,@xcite,@xcite:@xmath131 we can easily verify that @xmath132 this projection matrix requires that @xmath133 should have full column rank , i.e. , every @xmath134 is a regular point .",
    "however , the assumption does not hold in cases where @xmath135 is singular .",
    "this condition is the major motivation of this paper .",
    "for example , consider an equality constraint as@xmath136 where @xmath137{cc}x_{1 } & x_{2}\\end{array } \\right ]   ^{t}\\in\\mathbb{r } ^{2}.$ ] the feasible set is either @xmath138 or @xmath139 as shown in fig.1 , the point @xmath140{cc}-2 & 0 \\end{array } \\right ]   ^{t}$ ] has a unique feasible direction and the point @xmath141{cc}0 & 0 \\end{array } \\right ]   ^{t}$ ] also has a unique feasible direction . whereas , the point @xmath142{cc}-1 & 1 \\end{array } \\right ]   ^{t}$ ] has two feasible directions .",
    "this causes the singular phenomena .",
    "the singularity often occurs at the intersection of the feasible sets , where exist non - unique feasible directions .",
    "mathematically , @xmath143 is singular .",
    "concretely , the gradient vector of @xmath122 is@xmath144{c}2x_{1}+2\\\\ -2x_{2}+2 \\end{array } \\right ]   .\\ ] ] at the points @xmath145 and @xmath146 the gradient vector of @xmath147 is@xmath148{c}-2\\\\ 2 \\end{array } \\right ]   , \\nabla c\\left (   x_{p_{2}}\\right )   = \\left [ \\begin{array } [ c]{c}2\\\\ 2 \\end{array } \\right]\\ ] ] and by ( [ projectionmatrix ] ) , the projection matrices are further @xmath149{cc}0 & 1\\\\ 1 & 0 \\end{array } \\right ]   , f\\left (   x_{p_{2}}\\right )   = \\left [ \\begin{array } [ c]{cc}0 & -1\\\\ -1 & 0 \\end{array } \\right]\\ ] ] respectively . whereas",
    ", at the point @xmath150 the gradient vector of @xmath151 is@xmath152{c}0\\\\ 0 \\end{array } \\right ]   .\\ ] ] for such a case , @xmath153 does not exist .    to avoid singularity ,",
    "a commonly - used modified projection matrix is given as follows@xmath154 where @xmath155 is a small positive scale .",
    "we have @xmath156 no matter how small @xmath157 is . on the other hand , to obtain @xmath80 by ( [ modifiedprojectionmatrix ] ) , a very small @xmath157 will cause ill - conditioning problem especially for a low - precision processor .",
    "for example , consider the following gradient vectors:@xmath158{cccc}1 & 1 & 1 & 1 \\end{array } \\right ] \\nonumber\\\\ \\nabla c_{2 }   &   = \\left [ \\begin{array } [ c]{cccc}2 & 1 & 1 & 1 \\end{array } \\right ] \\nonumber\\\\ \\nabla c_{3 }   &   = \\left [ \\begin{array } [ c]{cccc}3 & 2 & 2 & 2 \\end{array } \\right ]   .",
    "\\label{gradient}\\ ] ] taking @xmath159 as the precision error , we employ ( [ modifiedprojectionmatrix ] ) with different @xmath160 to obtain the projection matrix @xmath129 .",
    "as shown in fig.2 , the error varies with different @xmath161 .",
    "the best precision error can be achieved only at @xmath162 with a precision error around @xmath163 . reducing @xmath157",
    "further will increase the numerical error .",
    "the best cure is to remove the linearly dependent vector directly from @xmath164 .",
    "for example , in @xmath165{ccc}\\nabla c_{1}\\left (   x\\right )   & \\nabla c_{2}\\left (   x\\right )   & \\nabla c_{3}\\left (   x\\right ) \\end{array } \\right ]   \\in\\mathbb{r } ^{n\\times3}$ ] , if @xmath166 can be represented by a linear combination of @xmath167and @xmath168 then @xmath169 is singular .",
    "the best cure is to remove @xmath170 from @xmath133 , resulting in@xmath171{cc}\\nabla c_{1}\\left (   x\\right )   & \\nabla c_{2}\\left (   x\\right ) \\end{array } \\right ]   \\in\\mathbb{r } ^{n\\times2}.\\ ] ] with it , the projection matrix becomes@xmath172 it is easy to see that @xmath173 for a linear time - invariant matrix @xmath174 namely independent of @xmath57 , we can avoid singularity by removing dependent terms out of @xmath133 before computing a projection matrix .",
    "however , this idea does not work for a general @xmath175 depending on @xmath176 therefore , the best cure  can not be implemented continuously , which further can not be realized by analog hardware .",
    "for such a purpose , we will propose a new projection matrix .",
    "for a special case @xmath177 such a @xmath80 is designed in _",
    "theorem 2_. consequently , a method is proposed to construct a projection matrix for a general case @xmath178 . before the design",
    ", we have the following preliminary results .",
    "* lemma 1*. let @xmath179 where @xmath180 and @xmath181{c}1\\\\ 0 \\end{array } \\right .",
    "\\begin{array } [ c]{c}x=0,x\\in\\mathbb{r}\\\\ x\\neq0,x\\in\\mathbb{r}\\end{array } .$ ] then @xmath182    _ proof .",
    "_ see _ appendix c_. @xmath121    * theorem 2*. suppose that @xmath183 and the function @xmath80 is designed to be@xmath184 then _ assumption 1 _ is satisfied with @xmath185 and @xmath186    _ proof . _ since @xmath187and@xmath188 , the function @xmath189 is defined as in ( [ fx ] ) so that @xmath190 by _ lemma 1 . _ therefore , _ assumption 1 _ is satisfied with @xmath191 further by _ lemma 1 _ , @xmath192 @xmath121    * theorem 3*. suppose that @xmath178 and the function @xmath80 is in a recursive form as follows:@xmath193 @xmath194 then _ assumption 1 _ is satisfied with @xmath195 and @xmath185 and @xmath186    _ proof .",
    "_ see _ appendix d_. @xmath121    * remark 4*. in ( [ modrecursiveform ] ) , if @xmath196 then @xmath197 namely@xmath198 this is the normal way to construct a projection matrix . on the other hand ,",
    "if @xmath199 can be represented by a linear combination of @xmath200 then @xmath201as @xmath202 in this case , @xmath203 consequently , the projection matrix will reduce to the previous one @xmath204 , that is equivalent to removing the term @xmath205 this is consistent with the best way .",
    "* remark 5*. in practice , the impulse function @xmath206 is approximated by some continuous functions such as @xmath207 , where @xmath208 is a large positive scale .",
    "let us revisit the example for the gradient vectors ( [ gradient ] ) .",
    "taking @xmath159 as the error again , we employ ( [ modrecursiveform ] ) with @xmath209 to obtain the projection matrix @xmath129 with @xmath210 this demonstrates the advantage of our proposed projection matrix over ( [ modifiedprojectionmatrix ] ) .",
    "furthermore , compared with ( [ projectionmatrix ] ) or ( [ modifiedprojectionmatrix ] ) , the explicit recursive form of the proposed projection matrix is also easier for the designer to implement .",
    "in this section , by using lyapunov s method , the update ( or say controller ) @xmath54 is designed to result in @xmath211 .",
    "however , the objective function @xmath55 is not required to be positive definite .",
    "we base our analysis upon the lasalle invariance theorem @xcite .      taking the time derivative of @xmath55 along the solutions of ( [ dynamical system ] )",
    "results in@xmath212 where @xmath213 in order to get @xmath214 a direct way of designing @xmath54 is proposed as follows@xmath215 where @xmath216 and @xmath217 @xmath218 @xmath219 .",
    "then ( [ dv ] ) becomes@xmath220 substituting ( [ controller ] ) into the continuous - time dynamical system ( [ dynamical system ] ) results in@xmath221 with solutions which always satisfy the constraint @xmath222 the closed - loop system corresponding to the continuous - time dynamical system ( [ dynamical system ] ) and the controller ( [ controller ] ) is depicted in fig.3 .      unlike a lyapunov function",
    ", the objective function @xmath55 is not required to be positive definite . as a consequence ,",
    "the conclusions for lyapunov functions are not applicable . instead",
    ", the invariance principle is applied to analyze the behavior of the solution of ( [ dynamics1 ] ) .    * theorem 4*. under _ assumption 1 _ , given @xmath107 , if the set @xmath223",
    "@xmath224 @xmath225 is bounded , then the solution of ( [ dynamics1 ] ) starting at @xmath90 approaches @xmath226 , where @xmath227 @xmath224 @xmath228 if in addition @xmath229 then there must exist a @xmath230 @xmath224 @xmath231 @xmath29 @xmath232 @xmath233^{t}$ ] @xmath234 such that @xmath235and @xmath236 namely @xmath237 is a karush  kuhn  tucker ( kkt ) point .",
    "furthermore , if @xmath238 , for all @xmath239 then @xmath237 is a strict local minimum , where @xmath240    _ proof_. the proof is composed of three propositions : _ proposition 1 _ is to show that @xmath223 is compact and positively invariant with respect to ( [ dynamics1 ] ) ; _ proposition 2 _ is to show that the solution of ( [ dynamics1 ] ) starting at @xmath90 approaches @xmath226 ; _ proposition 3 _ is to show that @xmath226 is a kkt point , further a strict local minimum .",
    "the three propositions are  proven in _",
    "appendix e_. @xmath121    * corollary 1*. suppose that @xmath80 is chosen as ( [ fx ] ) for @xmath178 and the set @xmath223",
    "@xmath224 @xmath225 is bounded for given @xmath107 .",
    "then the solution of ( [ dynamics1 ] ) starting at @xmath90 approaches @xmath226 , where @xmath227 @xmath224 @xmath241 where@xmath242 is a kkt point .",
    "in addition , if @xmath243 , for all @xmath244 then @xmath237 is a strict local minimum , where @xmath245    _ proof . _",
    "since @xmath246 by _ theorem 3 _ , _ _ the remainder of the proof is the same as that of _ theorem 4_. * * @xmath121    * corollary 2*. consider the following equality - constrained optimization problem@xmath247 if ( i ) @xmath55 is convex and twice continuously differentiable , ( ii ) @xmath248 with rank@xmath249 ( iii ) @xmath250 is bounded , then the solution of ( [ dynamics1 ] ) with @xmath251 starting at any @xmath107 approaches @xmath38 .    _ proof . _",
    "the solution of ( [ dynamics1 ] ) starting at @xmath90 approaches @xmath252 since rank@xmath249 we have @xmath253 since the equality constrained optimization problem ( [ convex ] ) is convex , a kkt point @xmath237  is a global minimum @xmath38 of the problem ( [ convex ] ) .",
    "the remainder of proof is the same as that of _ theorem 4_. * * @xmath121    * remark 6*. if @xmath223 is not a bounded set , then @xmath227 defined in _ theorem 4 _ may be empty . therefore , the boundedness of the set @xmath223 is necessary . for example , @xmath254 s.t .",
    "the set @xmath256 is unbounded .",
    "according to _ theorem 1 _ , we have @xmath257 @xmath258^{t}.$ ] in this case , @xmath259and then the set @xmath227 is empty .",
    "although the proposed approach ensures that the solutions satisfy the constraint , this approach may fail if @xmath260 or if numerical algorithms are used to compute the solutions . moreover , if the impulse function @xmath261 is approximated , then the constraints will also be violated . with these results ,",
    "the following modified closed - loop dynamical system is proposed to amend this situation .",
    "similar to @xcite , we introduce the term @xmath262 into ( [ dynamics1 ] ) , resulting in@xmath263 where @xmath264 .",
    "define @xmath265 then @xmath266 where @xmath267 is utilized .",
    "if the impulse function @xmath261 is approximated , then @xmath268 and can be ignored in practice .",
    "therefore , the solutions of ( [ modifieddynamics1 ] ) will tend to the feasible set @xmath269 if @xmath133 is of full column rank .",
    "once @xmath270 the modified dynamical system ( [ modifieddynamics1 ] ) degenerates to ( [ dynamics1 ] ) .",
    "the self - correcting feature enables the step size to be automatically controlled in the numerical integration process or to tolerate uncertainties when the differential equation is realized by using analog hardware .",
    "* remark 7*. the matrix @xmath271 plays a role in coordinating the convergence rate of all states by minimizing the condition number of the matrix functions like @xmath272 .",
    "moreover , it also plays a role in avoiding instability in the numerical solution of differential equations by normalizing the lipschitz condition of functions like @xmath273 concrete examples are given in the following section .",
    "for a given lyapunov function , the crucial step in any procedure for estimating the attraction domain is determining the optimal estimate .",
    "consider the system of differential  equations:@xmath274 where @xmath275 is the state vector , @xmath276 is a hurwitz matrix , and @xmath277 is a vector function .",
    "let @xmath278 be a given quadratic lyapunov function for the origin of ( [ marginsys ] ) , i.e. , @xmath279 is a positive - definite matrix such that @xmath280 .",
    "then the largest ellipsoidal estimate of the attraction domain of the origin can be computed via the following equality - constrained optimization problem @xcite:@xmath281   = 0.\\nonumber\\ ] ]    since @xmath282 is bounded , the subset @xmath283 = 0\\}\\ ] ] is bounded no matter what @xmath15 is .    for simplicity , consider ( [ marginsys ] ) with @xmath83 @xmath84^{t}\\in\\mathbb{r } ^{2},$ ] @xmath284 @xmath285 and @xmath286 @xmath84^{t},$ ] where @xmath287 then the optimization problem is formulated as@xmath288 since @xmath289 the problem is further formulated as @xmath290 then @xmath291^{t}\\\\ \\nabla c\\left (   x\\right )    &   = \\left [ \\begin{array } [ c]{c}d_{2}-0.1d_{1}^{2}-0.2d_{1}d_{3}\\\\ d_{2}-0.1d_{1}^{2}+d_{3}\\end{array } \\right ] \\\\",
    "d_{1 }   &   = x_{1}+1,d_{2}=x_{2}+1,d_{3}=x_{1}+x_{2}+2.\\end{aligned}\\ ] ] in this example , we adopt the modified dynamics ( [ modifieddynamics1 ] ) , where @xmath129 is chosen as ( [ fx ] ) with@xmath292  and the parameters@xmath293are chosen as@xmath294 we solve the differential equation ( [ modifieddynamics1 ] ) by using the matlab function ode45  with variable - step .",
    "compared with the matlab optimal constrained nonlinear multivariate function fmincon , we derive the comparisons in table 1 .",
    "@xmath295{ccccc}\\hline\\hline { \\small method } & { \\small initial point } & { \\small solution } & { \\small optimal value } & { \\small cpu time ( sec.)}\\\\\\hline { \\small matlab fmincon } & { \\small [ -3 1]}$^{\\text{{\\scriptsize t}}}$ & { \\small [ -1 -1]}$^{\\text{{\\scriptsize t}}}$ & { \\small 2.0000 } & { \\small not available}\\\\ { \\small new method } & { \\small [ -3 1]}$^{\\text{{\\scriptsize t}}}$ & { \\small [ 0.2062 -0.8546]}$^{\\text{{\\scriptsize t}}}$ & { \\small 0.7729 } & { \\small 0.125}\\\\\\hline { \\small matlab fmincon } & { \\small [ 2 -4]}$^{\\text{{\\scriptsize t}}}$ & { \\small [ -1 -1]}$^{\\text{{\\scriptsize t}}}$ & { \\small 2.0000 } & { \\small not available}\\\\ { \\small new method } & { \\small [ 2 -4]}$^{\\text{{\\scriptsize t}}}$ & { \\small [ 0.2062 -0.8545]}$^{\\text{{\\scriptsize t}}}$ & { \\small 0.7726 } & { \\small 0.0940}\\\\\\hline { \\small matlab fmincon } & { \\small [ 1 -4]}$^{\\text{{\\scriptsize t}}}$ & { \\small [ 0.2143 -0.8533]}$^{\\text{{\\scriptsize t}}}$ & { \\small 0.7740 } & { \\small 0.2030}\\\\ { \\small new method } & { \\small [ 1 -4]}$^{\\text{{\\scriptsize t}}}$ & { \\small [ 0.2056 -0.8550]}$^{\\text{{\\scriptsize t}}}$ & { \\small 0.7733 } & { \\small 0.1100}\\\\\\hline\\hline",
    "\\end{tabular } .}\\ ] ]    the point @xmath296 @xmath297^{t}$ ] is a singular point , at which @xmath298 @xmath299^{t}.$ ] as shown in table 1 , under initial points @xmath300 @xmath258^{t}\\in\\mathcal{f}$ ] and @xmath301 @xmath302^{t}\\in\\mathcal{f},$ ] the matlab function fails to find the minimum and stops at the singular point , whereas the proposed approach still finds the minimum .",
    "under initial point @xmath303 @xmath302^{t}\\notin\\mathcal{f},$ ] the proposed approach can still find the minimum , similar to the matlab function . under a different initial value ,",
    "the evolutions of ( [ modifieddynamics1 ] ) are shown in fig.4 .",
    "as shown , once close to the singular point @xmath304 @xmath297^{t}$ ] , the solutions of ( [ modifieddynamics1 ] ) change direction and then move to the minimum @xmath305^{t}$ ] .",
    "compared with the discrete optimal methods offered by matlab , these results show that the proposed approach avoids convergence to a singular point .",
    "moreover , the proposed approach is comparable with currently available conventional discrete optimal methods and facilitates even faster convergence .",
    "the latter conclusion is consistent with that proposed in @xcite,@xcite",
    ".          for simplicity , assume that images are taken by two identical pin - hole cameras with focal length equal to one .",
    "the two cameras are specified by the camera centers @xmath306 and attached orthogonal camera frames @xmath307 and @xmath308 , respectively .",
    "denote @xmath309 to be the translation from the first camera to the second and @xmath75 to be the rotation matrix from the basis vectors @xmath310 to @xmath311 , expressed with respect to the basis @xmath312 then , it is well known in the computer vision literature @xcite that two corresponding image points are represented as follows:@xmath313 where @xmath314 represent the positions of the @xmath161th point expressed in the two camera frames @xmath307 to @xmath315 respectively ; @xmath316 represent the third element of vectors @xmath317 respectively .",
    "they have the relationship @xmath318 @xmath319 these corresponding image points satisfy the socalled epipolar constraint @xcite:@xmath320 where @xmath321   _ { \\times}r\\ $ ] is known as the _ essential matrix_.    by using the direct product @xmath7 and the @xmath322  operation , the equations in ( [ essential ] ) are equivalent to@xmath323 where@xmath324{c}m_{2,1}^{t}\\otimes m_{1,1}^{t}\\\\ \\vdots\\\\ m_{2,n}^{t}\\otimes",
    "m_{1,n}^{t}\\end{array } \\right ]   \\in\\mathbb{r } ^{n\\times9},\\nonumber\\\\ \\varphi &   = \\text{vec}\\left (   \\left [   t\\right ]   _ { \\times}r\\right )   .",
    "\\label{a}\\ ] ] in practice , these image points @xmath325 and @xmath326 are subject to noise , @xmath327",
    ". therefore , @xmath328 and @xmath78 are often solved by the following optimization problem@xmath329 where @xmath330 vec@xmath331^{t}\\in\\mathbb{r } ^{12}$ ] .",
    "this is an equality - constrained optimization considered here . in the following",
    ", the proposed approach is applied to the optimization problem ( [ essential matrix optimal ] ) . by _",
    "theorem 2 _ , the projection matrix for the constraint @xmath332 is@xmath333 since @xmath334 has to be satisfied exactly or approximately , then @xmath335 so , the projection matrix for the constraint is @xmath336 then the constraint is transformed into @xmath337 where@xmath338 . by ( [ rotation ] ) , the constraint @xmath339is transformed into @xmath340   _ { \\times}r,\\ ] ] where @xmath341 furthermore , the equation above is rewritten as @xmath342 then the continuous - time dynamical system , whose solutions always satisfy the equality constraints @xmath343 @xmath224 @xmath344and @xmath345 @xmath224 @xmath346 , is expressed as ( [ dynamical system ] ) with@xmath347{cc}i_{3}-tt^{t}\\left/   \\left\\vert t\\right\\vert ^{2}\\right .   & 0_{3\\times3}\\\\ 0_{9\\times3 } & \\left (   r^{t}\\otimes i_{3}\\right )   h \\end{array } \\right ]   \\in\\mathbb{r } ^{12\\times6},\\nonumber\\\\ u   &   = \\left [ \\begin{array } [ c]{c}u_{1}\\\\ u_{2}\\end{array } \\right ]   \\in\\mathbb{r } ^{6}. \\label{f}\\ ] ] if the initial value @xmath348 and @xmath349 then all solutions of ( [ dynamical system ] ) satisfy the equality constraints . since @xmath350{cc}\\left (   r^{t}\\otimes i_{3}\\right )",
    "h & i_{3}\\otimes\\left [   t\\right ]   _ { \\times}\\end{array } ] ^{t}a^{t}a\\varphi,$ ] the time derivative of @xmath55 along the solutions of ( [ dynamical system ] ) is @xmath351 where @xmath352{c}\\left (   i_{3}-tt^{t}\\left/   \\left\\vert t\\right\\vert ^{2}\\right .",
    "\\right ) ^{t}h^{t}\\left (   r^{t}\\otimes i_{3}\\right )   ^{t}\\\\ h^{t}\\left (   r^{t}\\otimes i_{3}\\right )   ^{t}\\left (   i_{3}\\otimes\\left [ t\\right ]   _ { \\times}\\right )   ^{t}\\end{array } \\right ]   \\in\\mathbb{r } ^{6\\times9}.\\ ] ]    the simplest way of choosing @xmath271 is @xmath353 . in this case , the eigenvalues of the matrix @xmath354 are often ill - conditioned , namely @xmath355 convergence rates of the components of @xmath356 depend on the eigenvalues of @xmath357 as a consequence , some components of @xmath358 converge fast , while the other may converge slowly .",
    "this leads to poor asymptotic performance of the closed - loop system .",
    "it is expected that each component of @xmath358 can converge at the same speed as far as possible .",
    "suppose that there exists a @xmath359 such that @xmath360 then @xmath361 by _ theorem 4 _ , @xmath362will approach the set @xmath363 each element of which is a global minimum since @xmath364 in the set .",
    "moreover , each component of @xmath358 converges at a similar speed .",
    "however , it is difficult to obtain such a @xmath359 , since the number of degrees of freedom of @xmath365 is less than the number of elements of @xmath366 .",
    "a modified way is to make @xmath367 a natural choice is proposed as follows@xmath368 where @xmath369 @xmath370 denotes the moore penrose inverse of @xmath371 .",
    "the matrix @xmath372 is to make @xmath271 positive definite , where @xmath373 is a small positive real . from the procedure above",
    ", @xmath374 needs to be computed every time .",
    "this however will cost much time .",
    "a time - saving way is to update @xmath271 at a reasonable interval .",
    "then ( [ dynamics1 ] ) becomes@xmath375 where @xmath80 is defined in ( [ f ] ) . the differential equation can be solved by runge - kutta methods , etc .",
    "the solutions of ( [ dynamics2 ] ) satisfy the constraints , where @xmath330 vec@xmath376^{t}.$ ] moreover , the dynamic system will reach some final resting state eventually .",
    "suppose that there exist 6 points in the field of view , whose positions are expressed in the first camera frame as follows : @xmath377 @xmath378 @xmath258^{t},$ ]",
    "@xmath379 @xmath380 @xmath258^{t},$ ] @xmath381 @xmath382 @xmath258^{t},$ ] @xmath383 @xmath382 @xmath258^{t},$ ] @xmath384 @xmath378 @xmath258^{t},$ ] @xmath385 @xmath386 @xmath258^{t}.$ ] compared with the first camera frame , the second camera frame has translated and rotated with@xmath387{c}1\\\\ 1\\\\ -1 \\end{array } \\right ]   , \\bar{r}=\\left [ \\begin{array } [ c]{ccc}0.9900 & -0.0894 & 0.1088\\\\ 0.0993 & 0.9910 & -0.0894\\\\ -0.0998 & 0.0993 & 0.9900 \\end{array } \\right ]   .\\ ] ]    the image points are generated by ( [ image points ] ) . using the generated image points , we obtain @xmath388 by ( [ a ] ) . setting the initial value as follows @xmath389 @xmath380 @xmath258^{t},$ ] @xmath390",
    "@xmath391 @xmath392 we solve the differential equation ( [ modifieddynamics1 ] ) by using matlab function ode45  with variable - step .",
    "compared with matlab optimal constrained nonlinear multivariate function fmincon , we have the following comparisons:@xmath393{ccc}\\hline\\hline { \\small method } & $ \\left\\vert r^{\\ast t}\\bar{r}-i_{3}\\right\\vert $ & { \\small cpu time ( sec.)}\\\\\\hline { \\small matlab fmincon } & { \\small 1.2469e-004 } & { \\small 0.2500}\\\\ { \\small new approach } & { \\small 1.8784e-005 } & { \\small 0.1400}\\\\\\hline\\hline \\end{tabular } .}\\ ] ] as shown in table 2 , the proposed approach requires less time to achieve a higher accuracy .",
    "given that @xmath394 , the solution is a global minimum .",
    "the evolution of each element of @xmath57 is shown in fig.5 .",
    "the state eventually reaches a rest state at a similar speed . with different initial values ,",
    "several other simulations are also implemented .",
    "based on the results , the proposed algorithm has met the expectations .",
    "an approach to continuous - time , equality - constrained optimization based on a new projection matrix is proposed for the determination of local minima . with the transformation of the equality constraint into a continuous - time dynamical system ,",
    "the class of equality - constrained optimization is formulated as a control problem .",
    "the resultant approach is more general than the existing control theoretic approaches .",
    "thus , the proposed approach serves as a potential bridge between the optimization and control theories . compared with other standard discrete - time methods , the proposed approach avoids convergence to a singular point and facilitates faster convergence through numerical integration on a digital computer .",
    "_ a. kronecker product and vec _",
    "the symbol vec@xmath395 is the column vector obtained by stacking the second column of @xmath396 under the first , and then the third , and so on . with @xmath397   \\in\\mathbb{r } ^{n\\times m}$ ] , the _ kronecker product _",
    "@xmath398 is the matrix@xmath399{ccc}x_{11}y & \\cdots & x_{1m}y\\\\ \\vdots & \\ddots & \\vdots\\\\ x_{n1}y & \\cdots & x_{nm}y \\end{array } \\right ]   .\\ ] ] in fact , we have the following relationships vec@xmath400vec@xmath401 @xcite .",
    "the cross product of two vectors @xmath402 and @xmath403 is denoted by @xmath404   _ { \\times}y,$ ] where the symbol @xmath64   _ { \\times}:$ ] @xmath405 @xmath10 @xmath65 is defined as @xcite:@xmath406   _ { \\times}\\triangleq\\left [ \\begin{array } [ c]{ccc}0 & -x_{3 } & x_{2}\\\\ x_{3 } & 0 & -x_{1}\\\\ -x_{2 } & x_{1 } & 0 \\end{array } \\right ]   \\in\\mathbb{r } ^{3\\times3}.\\ ] ] by the definition of @xmath407   _ { \\times},$ ] we have@xmath408   _ { \\times}x=0_{3\\times1},$ ] @xmath409and@xmath410   _ { \\times}\\right )    &   = hx,\\\\ h   &   = \\left [ \\begin{array } [ c]{ccccccccc}0 & 0 & 0 & 0 & 0 & 1 & 0 & -1 & 0\\\\ 0 & 0 & -1 & 0 & 0 & 0 & 1 & 0 & 0\\\\ 0 & 1 & 0 & -1 & 0 & 0 & 0 & 0 & 0 \\end{array } \\right ]   ^{t}.\\end{aligned}\\ ] ]      since @xmath411 if @xmath412 and @xmath413 if @xmath414 we have @xmath415 , @xmath416 according to this , we have the following relationship@xmath417 this implies that @xmath418 @xmath419 , namely @xmath420 . on the other hand ,",
    "any @xmath421 is rewritten as @xmath422 where @xmath423 is utilized .",
    "hence @xmath424 consequently , @xmath182      denote@xmath425 first , by _ theorem 2 _ , _ _ it is easy to see that the conclusions are satisfied with @xmath426 .",
    "assume @xmath427 @xmath224 @xmath428 and then prove that @xmath429 @xmath224 @xmath430 holds .",
    "if so , then we can conclude this proof .",
    "by @xmath431 @xmath224 @xmath432 we have@xmath433 by _ lemma 1 _ , we have@xmath434 namely , @xmath435 where @xmath436      \\(i ) proof of _ proposition 1 . _ in the space @xmath437 the set @xmath223 is compact iff it is bounded and closed by theorem 8.2 in @xcite .",
    "hence , the remainder of work is to prove that @xmath223 is closed .",
    "suppose , to the contrary , @xmath438is not closed .",
    "then there exists a sequence @xmath439 with @xmath440 whereas , @xmath441 and @xmath442 which imply @xmath443the contradiction implies that @xmath223 is closed .",
    "hence , the set @xmath223 is compact . by ( [ dv1 ] ) , @xmath444 with respect to ( [ dynamics1 ] ) , @xmath445 . by _ assumption 1 _ ,",
    "all solutions of ( [ dynamics1 ] ) satisfy @xmath222 therefore , @xmath223 is positively invariant with respect to ( [ dynamics1 ] ) .",
    "\\(ii ) proof of _ proposition 2 . _ since _ _",
    "@xmath223 is compact and positively invariant with respect to ( [ dynamics1 ] ) , by _ theorem 4.4 _ ( invariance principle ) _ _ in @xcite , the solution of ( [ dynamics1 ] ) starting at @xmath90 approaches @xmath446 namely @xmath447 in addition , since ( [ dynamics1 ] ) becomes@xmath448 in @xmath227 , the solution approaches a constant vector @xmath252    \\(iii ) proof of _ proposition 3 . _ since @xmath449 and @xmath226 satisfy the following two equalities @xmath450 there exists a @xmath54 such that @xmath451 for any @xmath452 as a consequence , for any @xmath453 @xmath454 there must exist @xmath455 @xmath456 such that @xmath457 .",
    "otherwise @xmath458,@xmath459 therefore , @xmath226 is a kkt point @xcite .",
    "furthermore , by theorem 12.6 in @xcite , @xmath237 is a strict local minimum if @xmath460 , for all @xmath461"
  ],
  "abstract_text": [
    "<S> in equality - constrained optimization , a standard regularity assumption is often associated with feasible point methods , namely the gradients of constraints are linearly independent . in practice , the regularity assumption may be violated . </S>",
    "<S> to avoid such a singularity , we propose a new projection matrix , based on which a feasible point method for the continuous - time , equality - constrained optimization problem is developed . </S>",
    "<S> first , the equality constraint is transformed into a continuous - time dynamical system with solutions that always satisfy the equality constraint . </S>",
    "<S> then , the singularity is explained in detail and a new projection matrix is proposed to avoid singularity . an update ( or say a controller ) </S>",
    "<S> is subsequently designed to decrease the objective function along the solutions of the transformed system . </S>",
    "<S> the invariance principle is applied to analyze the behavior of the solution . </S>",
    "<S> we also propose a modified approach for addressing cases in which solutions do not satisfy the equality constraint . </S>",
    "<S> finally , the proposed optimization approaches are applied to two examples to demonstrate its effectiveness .    </S>",
    "<S> optimization , equality constraints , continuous - time dynamical systems , singularity </S>"
  ]
}