{
  "article_text": [
    "we develop quasi - newton and limited memory quasi - newton algorithms for functions defined on a grassmannian @xmath0 as well as a product of grassmannians @xmath1 , with bfgs and l - bfgs updates .",
    "these are algorithms along the lines of the class of algorithms studied by edelman , arias , and smith in @xcite and more recently , the monograph of absil , mahony , and sepulchre in @xcite .",
    "they are algorithms that respect the riemannian metric structure of the manifolds under consideration , and not mere applications of the usual bfgs and l - bfgs algorithms for functions on euclidean space .",
    "the actual computations of our bfgs and l - bfgs algorithms on grassmannians , like the algorithms in @xcite , require nothing more than standard numerical linear algebra routines and can therefore take advantage of the many high quality softwares developed for matrix computations @xcite .",
    "in other words , manifold operations such as movement along geodesics and parallel transport of tangent vectors and linear operators do not require actual numerical solutions of the differential equations defining these operations ; instead they are characterized as matrix operations on local and global coordinate representations ( as matrices ) of points on the grassmannians or points on an appropriate vector bundle .",
    "a departure and improvement from existing algorithms for manifold optimization @xcite is that we undertake a _ local coordinates _ approach .",
    "this allows our computational costs to be reduced to the order of the _ intrinsic dimension _ of the manifold as opposed to the dimension of ambient euclidean space . for a grassmannian embedded in the euclidean space of @xmath2 matrices , i.e.@xmath3 , computations in local coordinates have @xmath4 unit cost whereas computations in global coordinates , like the ones in @xcite , have @xmath5 unit cost",
    "this difference becomes more pronounced when we deal with products of grassmannians @xmath6for @xmath7 and @xmath8 , we have a computational unit cost of @xmath9 and @xmath10 flops between the local and global coordinates versions the bfgs algorithms .",
    "more importantly , we will show that our bfgs update in local coordinates on a product of grassmannians ( and grassmannian in particular ) shares the same well - known optimality property of its euclidean counterpart , namely , it is the best possible update of the current hessian approximation that satisfies the secant equations and preserves symmetric positive definiteness ( cf .  theorem  [ thm : opt ] ) . for completeness and as an alternative",
    ", we also provide the global coordinate version of our bfgs and l - bfgs algorithms analogous to the algorithms described in @xcite .",
    "however the aforementioned optimality is not possible for bfgs in global coordinates .",
    "while we have limited our discussions to bfgs and l - bfgs updates , it is straightforward to substitute these updates with other quasi - newton updates ( e.g.  dfp or more general broyden class updates ) by applying the same principles in this paper .      in part to illustrate the efficiency of these algorithms , this paper also addresses the following two related problems about the multilinear approximations of tensors and symmetric tensors , which are also important problems in their own right with various applications in analytical chemistry @xcite , bioinformatics @xcite , computer vision @xcite , machine learning @xcite , neuroscience @xcite , quantum chemistry @xcite , signal processing @xcite , etc .",
    "see also the very comprehensive bibliography of the recent survey @xcite . in data",
    "analytic applications , the multilinear approximation of general tensors is the basis behind the _ tucker model _ @xcite while the multilinear approximation of symmetric tensors is used in _ independent components analysis _ @xcite and _ principal cumulant components analysis _ @xcite .",
    "the algorithms above provide a natural method to solve these problems that exploits their unique structures .",
    "the first problem is that of finding a best multilinear rank-@xmath11 approximation to a tensor , i.e.  approximating a given tensor @xmath12 by another tensor @xmath13 of lower multilinear rank @xcite , @xmath14 for concreteness we will assume that the norm in question is the frobenius or hilbert - schmidt norm @xmath15 . in notations that we will soon define , we seek matrices @xmath16 with orthonormal columns and a tensor @xmath17 such that@xmath18    the second problem is that of finding a best multilinear rank-@xmath19 approximation to a symmetric tensor @xmath20 . in other words ,",
    "we seek a matrix @xmath21 whose columns are mutually orthonormal , and a symmetric tensor @xmath22 such that a multilinear transformation of @xmath23 by @xmath21 approximates @xmath24 in the sense of minimizing a sum - of - squares loss . using the same notation as in , the problem is @xmath25 this problem is significant because many important tensors that arise in applications are symmetric tensors .",
    "we will often refer to the first problem as the _ general case _ and the second problem as the _",
    "symmetric case_. most discussions are presented for the case of @xmath26-tensors for notational simplicity but key expressions are given for tensors of arbitrary order to facilitate structural analysis of the problem and algorithmic implementation .",
    "the matlab codes of all algorithms in this paper are available for download at @xcite .",
    "all of our implementations will handle @xmath26-tensors and in addition , our implementation of the bfgs with scaled identity as initial hessian approximation will handle tensors of arbitrary order .",
    "in fact the reader will find an example of a tensor of order-@xmath27 in section  [ qng : sec : comex ] , included to show that our algorithms indeed work on high - order tensors .",
    "our approach is summarized as follows .",
    "observe that due to the unitary invariance of the sum - of - squares norm @xmath15 , the orthonormal matrices @xmath28 in and the orthonormal matrix @xmath21 in are only determined up to an action of @xmath29 and an action of @xmath30 , respectively .",
    "we exploit this to our advantage by reducing the problems to an optimization problem on a product of grassmannians and a grassmannian , respectively . specifically , we reduce , a minimization problem over a product of three stiefel manifolds and a euclidean space @xmath31 , to a maximization problem over a product of three grassmannians @xmath32 ; and likewise we reduce from @xmath33 to a maximization problem over @xmath0",
    ". this reduction of to product of grassmannians has been exploited in @xcite .",
    "the algorithms in @xcite involve the hessian , either explicitly or implicitly via its approximation on a tangent .",
    "whichever the case , the reliance on hessian in these methods results in them quickly becoming infeasible as the size of the problem increases . with this in mind , we consider the quasi - newton and limited memory quasi - newton approaches described in the first paragraph of this section .",
    "an important case not addressed in @xcite is the multilinear approximation of symmetric tensors .",
    "note that the general and symmetric cases are related but different , not unlike the way the singular value problem differs from the symmetric eigenvalue problem for matrices .",
    "the problem for general tensors is linear in the entries of @xmath28 ( quadratic upon taking norm - squared ) whereas the problem for symmetric tensors is cubic in the entries of @xmath21 ( sextic upon taking norm - squared ) . to the best of our knowledge ,",
    "all existing solvers for are unsatisfactory because they rely on algorithms for .",
    "a typical heuristic is as follows : find _ three _ orthonormal matrices @xmath34 and a nonsymmetric @xmath35 that approximates  @xmath24,@xmath36 then artificially set @xmath37 by either averaging or choosing the last iterate and then symmetrize @xmath38 .",
    "this of course is not ideal .",
    "furthermore , using the framework developed for the general tensor approximation to solve the symmetric tensor approximation problem will be computationally much more expensive . in particular , to optimize @xmath39 without taking the symmetry into account",
    "incurs a @xmath40-fold increase in computational cost relative to @xmath41 .",
    "the algorithm proposed in this paper solves directly .",
    "it finds a _ single _",
    "@xmath42 and a _ symmetric _ @xmath22 with@xmath43 the symmetric case can often be more important than the general case , not surprising since symmetric tensors are common in practice , arising as higher order derivatives of smooth multivariate real - valued functions , higher order moments and cumulants of a vector - valued random variable , etc .",
    "like the grassmann - newton algorithms in @xcite and the trust - region approach in @xcite , the quasi - newton algorithms proposed in this article have guaranteed convergence to stationary points and represent an improvement over gauss - seidel type coordinate - cycling heuristics like alternating least squares ( als ) , higher - order orthogonal iteration ( hooi ) , or higher - order singular value decomposition ( hosvd ) @xcite . and as far as accuracy is concerned , our algorithms perform as well as the algorithms in @xcite and outperform gauss - seidel type strategies in many cases . as far as robustness and speed are concerned , our algorithms work on much larger problems and perform vastly faster than grassmann - newton algorithm .",
    "asymptotically the memory storage requirements of our algorithms are of the same order - of - magnitude as gauss - seidel type strategies . for large problems ,",
    "our grassmann l - bfgs algorithm outperforms even gauss - seidel strategies ( in this case hooi ) , which is not unexpected since it has the advantage of requiring only a small number of prior iterates",
    ".    we will give the reader a rough idea of the performance of our algorithms .",
    "using matlab on a laptop computer , we attempted to find a solution to an accuracy within machine precision , i.e.  @xmath44 . for general @xmath26-tensors of size @xmath45 ,",
    "our grassmann l - bfgs algorithm took less than @xmath46 minutes while for general @xmath47-tensors of size @xmath48 , it took about @xmath49 minutes . for symmetric @xmath26-tensors of size",
    "@xmath45 , our grassmann bfgs algorithm took about @xmath50 minutes while for symmetric @xmath47-tensors of size @xmath48 , it took less than @xmath51 minutes . in all cases ,",
    "we seek a rank-@xmath52 or rank-@xmath53 approximation .",
    "for a general order-@xmath27 tensor of dimensions @xmath54 , a rank-@xmath55 approximation took about @xmath56 minutes to reach the same accuracy as above .",
    "more extensive numerical experiments are reported in section  [ qng : sec : comex ] .",
    "the reader is welcomed to try our algorithms , which have been made publicly available at @xcite .",
    "the structure of the article is as follows . in sections [ sec : tensors ] and",
    "[ sec : mrank ] , we present a more careful discussion of tensors , symmetric tensors , multilinear rank , and their corresponding multilinear approximation problems . in section  [ qng : sec : optrm ] we will discuss how quasi - newton methods in euclidean space may be extended to riemannian manifolds and , in particular , grassmannians .",
    "section  [ qng : sec : geodpartran ] contains a discussion on geodesic curves and transport of vectors on grassmannians . in section  [ qng :",
    "sec : qn ] we present the modifications on quasi - newton methods with bfgs updates in order for them to be well - defined on grassmannians .",
    "also , the reader will find proof of the optimality properties of bfgs updates on products of grassmannians .",
    "section  [ qng : sec : lbfgs ] gives the corresponding modifications for limited memory bfgs updates .",
    "section  [ qng : sec : brapp ] states the corresponding expressions for the tensor approximation problem , which are defined on a product of grassmannians .",
    "the symmetric case is detailed in section  [ qng : sec : symmcase ] .",
    "section [ sec : ex ] contains a few examples with numerical calculations illustrating the presented concepts .",
    "the implementation and the experimental results are found in section  [ qng : sec : comex ] .",
    "related work and the conclusions are discussed in section [ sec : relwork ] and [ qng : sec : conclusion ] respectively .",
    "tensors will be denoted by calligraphic letters , e.g.  @xmath57 , @xmath58 , @xmath23 .",
    "matrices will be denoted in upper case letters , e.g.  @xmath59 , @xmath60 , @xmath61 .",
    "we will also use upper case letters @xmath59 , @xmath62 to denote iterates or elements of a grassmannian since we represent them as ( equivalence classes of ) matrices with orthonormal columns .",
    "vectors and iterates in vector form are denoted with lower case letters , e.g.  @xmath63 , @xmath64 , @xmath65 , @xmath66 , where the subscript is the iteration index . to denote scalars we use lower case greek letters , e.g.  @xmath67 , @xmath68 , and @xmath69 , @xmath70 .",
    "we will use the usual symbol @xmath71 to denote the _",
    "outer product of tensors _ and a large boldfaced version @xmath72 to denote the _ kronecker product of operators_. for example , if @xmath73 and @xmath74 are matrices , then @xmath75 will be a @xmath47-tensor in @xmath76 whereas @xmath77 will be a matrix in @xmath78 .",
    "in the former case , we regard @xmath79 and @xmath80 as @xmath51-tensors while in the latter case , we regard them as matrix representations of linear operators .",
    "the contracted products used in this paper are defined in appendix [ app:1 ] .    for @xmath81",
    ", we will let @xmath82 denote the _ stiefel manifold _ of @xmath83 matrices with orthonormal columns .",
    "the special case @xmath84 , i.e.  the _ orthogonal group _ , will be denoted @xmath85 . for @xmath86 , @xmath30 acts on @xmath87 via right multiplication .",
    "the set of orbit classes @xmath88  is a manifold called the _ grassmann manifold _ or _ grassmannian _ ( we adopt the latter name throughout this article ) and will be denoted @xmath0 .    in this paper , we will only cover a minimal number of notions and notations required to describe our algorithm .",
    "further mathematical details concerning tensors , tensor ranks , tensor approximations , as well as the counterpart for symmetric tensors may be found in @xcite .",
    "specifically we will use the notational and analytical framework for tensor manipulations introduced in ( * ? ? ?",
    "* section  2 ) and assume that these concepts are familiar to the reader .",
    "let @xmath89 be real vector spaces of dimensions @xmath90 respectively and let @xmath91 be an element of the tensor product @xmath92 , i.e.  @xmath91 is a _",
    "tensor _ of order @xmath40 @xcite .",
    "up to a choice of bases on @xmath89 , one may represent a tensor @xmath91 as a @xmath40-dimensional _ hypermatrix _",
    "@xmath93\\in\\mathbb{r}^{n_{1}\\times\\dots\\times n_{k}}$ ] .",
    "similarly , let @xmath94 be a real vector space of dimension @xmath95 and @xmath96 be a _",
    "symmetric tensor _ of order @xmath40 @xcite .",
    "up to a choice of basis on @xmath94 , @xmath97 may be represented as a @xmath40-dimensional hypermatrix @xmath98\\in\\mathbb{r}^{n\\times\\dots\\times n}$ ] whose entries are invariant under any permutation of indices , i.e.@xmath99 we will write @xmath100 for the subspace of @xmath101 satisfying . henceforth , we will assume that there are some predetermined bases and will not distinguish between a tensor @xmath102 and its hypermatrix representation @xmath103 and likewise for a symmetric tensor @xmath96 and its hypermatrix representation @xmath104 .",
    "furthermore we will sometimes present our discussions for the case @xmath105 for notational simplicity .",
    "we often call an order-@xmath40 tensor simply as a @xmath40-tensor and an order-@xmath40 symmetric tensor as a symmetric @xmath40-tensor",
    ".    as we have mentioned in section  [ qng : sec : intro ] , symmetric tensors are common in applications , largely because of the two examples below .",
    "the use of higher - order statistics in signal processing and neuroscience , most notably the technique of independent component analysis , symmetric tensors often play a central role .",
    "the reader is referred to @xcite for further discussion of symmetric tensors .",
    "let @xmath106 and @xmath107 be an open subset . if @xmath108 , then for @xmath109 , the @xmath40th derivative of @xmath110 at @xmath111 is a symmetric tensor of order @xmath40,@xmath112   _ { i_{1}+\\dots+i_{n}=k}\\in\\mathsf{s}^{k}(\\mathbb{r}^{n}).\\ ] ] for @xmath113 , the vector @xmath114 and the matrix @xmath115 are the gradient and hessian of @xmath110 at @xmath116 , respectively .",
    "[ eg : cum]let @xmath117 be random variables with respect to the same probability distribution @xmath118 .",
    "the moments and cumulants of the random vector @xmath119 are symmetric tensors of order @xmath40 defined by@xmath120_{i_{1},\\dots , i_{k}=1}^{n}\\\\ &   = \\left [   \\idotsint x_{i_{1}}x_{i_{2}}\\cdots x_{i_{k}}~d\\mu(x_{i_{1}})\\cdots d\\mu(x_{i_{k}})\\right ]   _ { i_{1},\\dots , i_{k}=1}^{n}\\ ] ] and@xmath121   _ { i_{1},\\dots , i_{k}=1}^{n}\\ ] ] respectively . the sum above is taken over all possible partitions @xmath122 .",
    "it is not hard to show that both @xmath123 and @xmath124 . for @xmath125 , the quantities @xmath126 for @xmath127 have well - known names",
    ": they are the expectation , variance , skewness , and kurtosis of the random variable @xmath59 , respectively .",
    "matrices can act on other matrices through two independent multiplication operations : left - multiplication and right - multiplication . if @xmath128 and @xmath129 , @xmath130 , then the matrix  @xmath131 may be transformed into the matrix @xmath73 , by @xmath132 matrices act on order-@xmath26 tensors via _ three _ different multiplication operations . as in the matrix case ,",
    "these can be combined into a single formula .",
    "if @xmath17 and @xmath133 , @xmath134 , @xmath135 , then the @xmath26-tensor  @xmath23 may be transformed into the @xmath26-tensor @xmath12 via @xmath136 we call this operation the _ trilinear multiplication _ of  @xmath57 by matrices @xmath59 , @xmath60 and @xmath61 , which we write succinctly as @xmath137 this is nothing more than the trilinear equivalent of , which in this notation has the form @xmath138 informally , amounts to multiplying the @xmath26-tensor @xmath57 on its three ` sides ' or modes by the matrices @xmath59 , @xmath60 , and @xmath61 respectively .    an alternative but equivalent way of writing",
    "is as follows .",
    "define the _",
    "outer product _ of vectors @xmath139 , @xmath140 , @xmath141 by @xmath142\\in\\mathbb{r}^{l\\times m\\times n},\\ ] ] and call a tensor of the form @xmath143 a _ decomposable tensor _ or , if non - zero , a",
    "_ _ rank-__@xmath144 _ tensor_. one may also view as a _ trilinear combination _ ( as opposed to a linear combination ) of the decomposable tensors given by@xmath145 the vectors @xmath146 , @xmath147 , @xmath148 are , of course , the column vectors of the respective matrices @xmath16 above . in this article , we find it more natural to present our algorithms in the form and therefore we refrain from using .    more abstractly , given linear transformations of real vector spaces @xmath149 ,",
    "@xmath150 , @xmath151 , the functoriality of tensor product @xcite ( denoted by the usual notation @xmath152 below ) implies that one has an induced linear transformation between the tensor product of the respective vector spaces@xmath153 the trilinear matrix multiplication above is a coordinatized version of this abstract transformation . in the special case where @xmath154 , @xmath155 , @xmath156 and @xmath157 , @xmath158 , @xmath159 are ( invertible ) change - of - basis transformations ,",
    "the operation in describes the manner a ( contravariant ) @xmath26-tensor transforms under change - of - coordinates of @xmath160 , @xmath94 and @xmath161 .",
    "associated with the trilinear matrix multiplication is the following notion of tensor rank that generalizes the row - rank and column - rank of a matrix .",
    "let @xmath12 . for fixed values of @xmath162 and @xmath163",
    ", consider the column vector , written in a matlab - like notation , @xmath164 .",
    "likewise we may consider @xmath165 for fixed values of @xmath166 , and @xmath167 for fixed values of @xmath168 .",
    "define @xmath169 note that @xmath170 may also be viewed as @xmath171 .",
    "then @xmath172 is simply the rank of  @xmath57 regarded as an @xmath173 matrix , see the discussion on tensor matricization in @xcite with similar interpretations for @xmath174 and @xmath175 .",
    "the _ multilinear rank _ of @xmath57 is the @xmath26-tuple @xmath176 and we write @xmath177 we need to ` store ' all three numbers as @xmath178 in general  a clear departure from the case of matrices where row - rank and column - rank are always equal .",
    "note that a rank-@xmath144 tensor must necessarily have multilinear rank @xmath179 , i.e.@xmath180 if @xmath181 are non - zero vectors .    for symmetric tensors",
    ", one would be interested in transformation that preserves the symmetry . for a symmetric matrix @xmath182",
    ", this would be@xmath183 matrices act on symmetric order-@xmath26 tensors @xmath20 via the symmetric version of   @xmath184 we call this operation the _ symmetric trilinear multiplication _ of  @xmath24 by matrix @xmath59 , which in the notation above , is written as @xmath185 this is nothing more than the cubic equivalent of , which in this notation becomes @xmath186 informally , amounts to multiplying the @xmath26-tensor @xmath57 on its three ` sides ' or modes by the same matrix @xmath59 . in the multilinear combination form , this is@xmath187 where the vectors @xmath188 are the column vectors of the matrix @xmath59 above .    more abstractly , given a linear transformation of real vector spaces @xmath189 , the functoriality of symmetric tensor product @xcite implies that one has an induced linear transformation between the tensor product of the respective vector spaces@xmath190 the symmetric trilinear matrix multiplication above is a coordinatized version of this abstract transformation . in the special case where @xmath155 and @xmath191 is an ( invertible ) change - of - basis transformation ,",
    "the operation in describes the manner a symmetric @xmath26-tensor transforms under change - of - coordinates of @xmath94 .    for a symmetric tensor @xmath20",
    ", we must have@xmath192 by its symmetry .",
    "see lemma  [ lem : symrank ] for a short proof .",
    "the _ symmetric multilinear rank _ of @xmath24 is the common value , denoted @xmath193 .",
    "when referring to a symmetric tensor in this article , rank would always mean symmetric multilinear rank ; e.g.  a rank-@xmath194 symmetric tensor @xmath24 would be one with @xmath195 .",
    "we note that there is a different notion of tensor rank and symmetric tensor rank , defined as the number of terms in a minimal decomposition of a tensor ( resp .",
    "symmetric tensor ) into rank-@xmath144 tensors ( resp .",
    "rank-@xmath144 symmetric tensors ) .",
    "associated with this notion of rank are low - rank approximation problems for tensors and symmetric tensors analogous to the ones discussed in this paper .",
    "unfortunately , these are ill - posed problems that may not even have a solution @xcite . as such",
    ", we will not discuss this other notion of tensor rank .",
    "it is implicitly assumed that whenever we discuss tensor rank or symmetric tensor rank , it is with the multilinear rank or symmetric multilinear rank defined above in mind .",
    "let @xmath12 be a given third order tensor and consider the problem @xmath196 under this rank constraint , we can write @xmath58 in factorized form @xmath197 where @xmath17 and @xmath198 , @xmath134 , @xmath135 , are full rank matrices . in other words , one would like to solve the following _ best multilinear rank approximation problem _",
    ", @xmath199 this is the optimization problem underlying the _ tucker model _",
    "@xcite that originated in psychometrics but has become increasingly popular in other areas of data analysis .",
    "in fact , there is no loss of generality if we assume @xmath200 , @xmath201 and @xmath202 .",
    "this is verified as follows , for any full column - rank matrices @xmath203 , @xmath204 and @xmath205 we can compute their qr - factorizations @xmath206 and multiply the right triangular matrices into the core tensor , i.e.@xmath207 with the orthonormal constraints on @xmath59 , @xmath60 and @xmath61 the tensor approximation problem can be viewed as an optimization problem on a product of stiefel manifolds . using the identity @xmath208 we can rewrite the tensor approximation problem as a maximization problem with the objective function @xmath209 in which the small core tensor @xmath23 is no longer present .",
    "see references @xcite for the elimination of @xmath23 .",
    "the objective function @xmath210 is invariant under orthogonal transformation of the variable matrices @xmath16 from the right .",
    "specifically , for any orthogonal matrices @xmath211 , @xmath212 and @xmath213 it holds that @xmath214 .",
    "this homogeneity property implies that @xmath210 is in fact defined on a product of three grassmannians @xmath215 .",
    "let @xmath20 be a given symmetric @xmath26-tensor and consider the problem @xmath216 under this rank constraint , we can write @xmath217 in factorized form @xmath218 where @xmath22 and @xmath219 is a full - rank matrix . in other words , one would like to solve the following _ best symmetric multilinear rank approximation problem _",
    ", @xmath220 as with the general case , there is no loss of generality if we assume @xmath200 . with the orthonormal constraints on @xmath59",
    ", the tensor approximation problem can be viewed as an optimization problem on a single stiefel manifold ( as opposed to a product of stiefel manifolds in ) . using the identity @xmath221 we may again rewrite the tensor approximation problem as a maximization problem with the objective function @xmath222 in which the core - tensor @xmath23 is no longer present . as with the general case",
    ", the objective function @xmath223 also has an invariance property , namely @xmath224 for any orthogonal @xmath225 . as before",
    ", this homogeneity property implies that @xmath223 is well - defined on a single grassmannian @xmath0 .",
    "these multilinear approximation problems may be viewed as ` dimension reduction ' or ` rank reduction ' for tensors and symmetric tensors respectively .",
    "in general , a matrix requires @xmath226 storage and an order-@xmath40 tensor requires @xmath227 storage . while it is sometimes important to perform dimension reduction to a matrix , a dimension reduction is almost always necessary if one wants to work effectively with a tensor of higher order .",
    "a dimension reduction of a matrix @xmath228 of the form @xmath229 , where @xmath230 and diagonal @xmath231 reduces dimension from @xmath226 to @xmath232 . a dimension reduction of a tensor @xmath233 of the form @xmath234 where @xmath235 and @xmath236 , reduces dimension from @xmath227 to @xmath237 .",
    "if @xmath19 is significantly smaller than @xmath95 , e.g.  @xmath238 , then a dimension reduction in the higher order case could reduce problem size by orders of magnitude .",
    "in this section we discuss the necessary modifications for generalizing an optimization algorithm from euclidean space to riemannian manifolds .",
    "specifically we consider the quasi - newton methods with bfgs and _ limited memory _",
    "bfgs ( l - bfgs ) updates .",
    "first we state the expressions in euclidean space and then we point out what needs to be modified .",
    "the convergence properties of quasi - newton methods defined on manifolds were established by gabay  @xcite .",
    "numerical treatment of algorithms on the grassmannian are given in @xcite .",
    "a recent book on optimization on manifolds is  @xcite . in this and the next three sections , i.e.  sections  [ qng : sec : optrm ] through [ qng : sec : lbfgs ] , we will discuss our algorithms in the context of minimization problems , as is conventional",
    ". it will of course be trivial to modify them for maximization problem .",
    "indeed the tensor approximation problems discussed in sections [ qng : sec : brapp ] through [ qng : sec : comex ] will all be solved as maximization problems on grassmannians or product of grassmannians .",
    "assume that we want to minimize a nonlinear real valued function @xmath239 where @xmath240 . as is well - known , in quasi - newton methods , one solves@xmath241 to obtain the direction of descent @xmath242 from the current iterate @xmath65 and the gradient @xmath243 at @xmath65 .",
    "unlike newton method , which uses the exact hessian for @xmath244 , in @xmath244 is only an approximation of the hessian at @xmath65 .",
    "after computing the ( search ) direction @xmath242 one obtains the next iterate as @xmath245 in which the step length @xmath70 is usually given by a line search method satisfying the wolfe or the goldstein conditions @xcite . instead of recomputing the hessian at each new iterate @xmath246 , it is updated from the previous approximation .",
    "the bfgs update has the following form , @xmath247 where @xmath248 quasi - newton methods with bfgs updates are considered to be the most computationally efficient algorithms for minimization of general nonlinear functions .",
    "this efficiency is obtained by computing a new hessian approximation as a rank-@xmath51 modification of the previous hessian .",
    "the convergence of quasi - newton methods is super - linear in a vicinity of a local minimum . in most cases the quadratic convergence of the newton method",
    "is outperformed by quasi - newton methods since each iteration is computationally much cheaper than a newton iteration .",
    "a thorough study of quasi - newton methods may be found in @xcite .",
    "the reader is reminded that quasi - newton methods do not necessarily converge to local minima but only to stationary points .",
    "nevertheless , using the hessians that we derived in sections  [ sec : genderiv ] and [ sec : genderivsym ] ( for the general and symmetric cases respectively ) , the nature of these stationary points can often be determined .",
    "we will give a very brief overview of riemannian geometry tailored specifically to our needs in this paper .",
    "first we sketch the modifications that are needed in order for an optimization algorithm to be well - defined when the objective function is defined on a manifold . for details and proof , the reader should refer to standard literature on differential and riemannian geometry @xcite .",
    "informally a manifold , denoted with @xmath249 , is an object locally homeomorphic to @xmath250 .",
    "we will regard @xmath249 as a submanifold of some high - dimensional ambient euclidean space @xmath251 .",
    "our objective function @xmath252 will be assumed to have ( at least ) continuous second order partial derivatives .",
    "we will write @xmath239 , @xmath253 , and @xmath254 for the value of the function , the gradient , and the hessian at @xmath255 . these will be reviewed in greater detail in the following .",
    "equations  are the basis of any algorithmic implementation involving bfgs or l - bfgs updates",
    ". the key operations are ( 1 ) computation of the gradient , ( 2 ) computation of the hessian or its approximation , ( 3 ) subtraction of iterates , e.g.  to get @xmath256 or @xmath246 and ( 4 ) subtraction of gradients .",
    "each of these points needs to be modified in order for these operations to be well - defined on manifolds .",
    "[ [ computation - of - the - gradient ] ] computation of the gradient + + + + + + + + + + + + + + + + + + + + + + + + + + +    the gradient @xmath257 at @xmath255 of a real - valued function defined on a manifold , @xmath252 , @xmath258 , is a vector in the tangent space @xmath259 of the manifold at the given point @xmath63 .",
    "we write @xmath260 .    to facilitate computations , we will often embed our manifold @xmath249 in some ambient euclidean space @xmath251 and in turn endow @xmath249 with a system of global coordinates @xmath261 where @xmath262 is usually larger than @xmath263 , the intrinsic dimension of @xmath249 .",
    "the function @xmath110 then inherits an expression in terms of @xmath264 , say , @xmath265 .",
    "we would like to caution our readers that computing @xmath266 on @xmath249 is not a matter of simply taking partial derivatives of @xmath267 with respect to @xmath264 . an easy way to observe",
    "this is that @xmath266 is a @xmath268-tuple when expressed in local coordinates whereas @xmath269 is an @xmath262-tuple .",
    "[ [ computation - of - the - hessian - or - its - approximation ] ] computation of the hessian or its approximation + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the hessian @xmath270 at @xmath255 of a function @xmath252 is a linear transformation of the tangent space @xmath271 to itself , i.e.@xmath272 as in the case of gradient , when @xmath110 is expressed in terms of global coordinates , differentiating the expression twice will in general not give the correct hessian .    [",
    "[ updating - the - current - iterate ] ] updating the current iterate + + + + + + + + + + + + + + + + + + + + + + + + + + + +    given an iterate @xmath273 , a step length @xmath70 and a search direction @xmath274 the update @xmath245 will in general not be a point on the manifold .",
    "the corresponding operation on a manifold is to move along the geodesic curve of the manifold given by the direction @xmath242 .",
    "geodesics on manifolds correspond to straight lines in euclidean spaces .",
    "the operation @xmath275 is undefined in general when the points on the right hand side belong to a general manifold .",
    "[ [ updating - vectors - and - operators ] ] updating vectors and operators + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the quantities @xmath256 and @xmath66 are in fact tangent vectors and the hessians ( or hessian approximations ) are linear operators all defined at a specific point of the manifold .",
    "a given hessian @xmath254 is only defined at a point @xmath255 and correspondingly only acts on vectors in the tangent space @xmath271 . in the right hand side of the bfgs",
    "update all quantities need to be defined at the same point @xmath65 in order to have well - defined operations between the terms .",
    "in addition the resulting sum will define an operator at a new point @xmath246 .",
    "the notion of parallel transporting vectors along geodesic curves resolves all of these issues .",
    "the operations to the hessian are similar and involves parallel transport operation back and forth between two different points .",
    "in this paper , the riemannian manifold @xmath249 of most interest to us is @xmath0 , the grassmannian or grassmannian of @xmath19-planes in @xmath250 .",
    "our discussion will proceed with @xmath276 .",
    "representing points on grassmannians as ( equivalence classes of ) matrices allows us to take advantage of matrix arithmetic and matrix algorithms , as well as the readily available libraries of highly optimized and robust matrix computational softwares developed over the last five decades @xcite .",
    "a major observation of @xcite is the realization that common differential geometric operations on points of grassmann and stiefel manifolds can all be represented in terms of matrix operations . for our purposes ,",
    "the two most important operations are ( 1 ) the determination of a geodesic at a point along a tangent vector and ( 2 ) the parallel transport of a tangent vector along a geodesic . on a product of grassmannians , these operations may likewise be represented in terms of matrix operations @xcite",
    ". we will give explicit expressions for geodesic curves on grassmannians and two different ways of parallel transporting tangent vectors .",
    "first we will review some preliminary materials from @xcite .",
    "a point @xmath277 on the grassmannian @xmath0 is an equivalence class of orthonormal matrices whose columns form an orthonormal basis for an @xmath19-dimensional subspace of @xmath250 .",
    "explicitly , we write @xmath278 where @xmath279 , i.e.  @xmath59 is an @xmath2 matrix and @xmath280 .",
    "the set @xmath281 is also a manifold , often called the stiefel manifold .",
    "when @xmath282 , @xmath283 is the orthogonal group .",
    "it is easy to see that the dimensions of these manifolds are@xmath284 in order to use standard linear algebra in our computations , we will not be able to work with a whole equivalence class of matrices .",
    "so by a point on a grassmannian , we will always mean some @xmath285 that represents the equivalence class .",
    "the functions @xmath110 that we optimize in this paper will take orthonormal matrices in @xmath87 as arguments but will always be well - defined on grassmannians : given two representatives @xmath286 , we will have @xmath287 , i.e.@xmath288 abusing notations slightly , we will sometimes write @xmath289 .    the tangent space @xmath290 , where @xmath291 , is an affine vector space with elements in @xmath292",
    ". it can be shown that any element @xmath293 satisfies @xmath294 the projection on the tangent space is @xmath295 where @xmath296 is an orthogonal complement of @xmath59 , i.e.  the square matrix @xmath297 $ ] is an @xmath298 orthogonal matrix . since by definition @xmath299 , any tangent vector can also be written as @xmath300 where @xmath301 is an @xmath302 matrix .",
    "this shows that the columns of @xmath296 may be interpreted as a basis for @xmath290 .",
    "we say that @xmath303 is a _ global coordinate representation _ and @xmath301 is a _ local coordinate representation _ of the same tangent .",
    "note that the number of degrees of freedom in @xmath301 equals the dimension of the tangent space @xmath290 , which is @xmath4 .",
    "it follows that for a given tangent in global coordinates @xmath303 , its local coordinate representation is given by @xmath304 .",
    "observe that to a given local representation @xmath301 of a tangent there is an associated basis matrix @xmath296 .",
    "tangent vectors are also embedded in @xmath305 since in global coordinates they are given by @xmath2 matrices .",
    "we will define algorithms using both global coordinates as well as intrinsic local coordinates .",
    "when using global coordinates , the grassmannian @xmath0 is ( isometrically ) embedded in the euclidean space @xmath292 and a product of grassmannians in a corresponding product of euclidean spaces .",
    "the use of plcker coordinates to represent points on grassmannian is not useful for our purpose .",
    "let @xmath291 and @xmath303 be a tangent vector at @xmath59 , i.e.  @xmath293 .",
    "the geodesic path from @xmath59 in the direction @xmath303 is given by @xmath306\\begin{bmatrix } \\cos\\sigma t\\\\ \\sin\\sigma t \\end{bmatrix } v^{\\mathsf{t } } , \\label{eq : geodesic}\\ ] ] where @xmath307 is the thin svd and we identify @xmath308 .",
    "observe that omitting the last @xmath94 in will give the same path on the manifold but with a different representation .",
    "this information is useful because some algorithms require a consistency in the matrix representations along a path but other algorithms do not .",
    "for example , in a newton - grassmann algorithm we may omit the second @xmath94 @xcite but in quasi - newton - grassmann algorithms @xmath94 is necessary .",
    "let @xmath59 be a point on a grassmannian and consider the geodesic given by the tangent vector @xmath293 .",
    "the matrix expression for the parallel transport of an arbitrary tangent vector @xmath309 is given by @xmath310\\begin{bmatrix } -\\sin\\sigma t\\\\ \\cos\\sigma t \\end{bmatrix } u^{\\mathsf{t}}+(i - uu^{\\mathsf{t}})\\right )   \\delta_{2}\\equiv t_{x,\\delta } ( t)\\delta_{2 } , \\label{eq : parvectran}\\ ] ] where @xmath311 is the thin svd and we define @xmath312 to be the _ _ parallel transport matrix _ _ and @xmath303 and just write @xmath313 when there is no risk for confusion .",
    "] from the point @xmath59 in the direction @xmath303 . if @xmath314 expression can be simplified .",
    "let @xmath291 , @xmath293 , and @xmath296 be an orthogonal complement of @xmath59 so that @xmath315 $ ] is orthogonal . recall that we may write @xmath300 , where we view @xmath296 as a basis for @xmath290 and @xmath301 as a local coordinate representation of the tangent vector  @xmath303 .",
    "assuming that @xmath316 is the geodesic curve given in , the parallel transport of the corresponding basis @xmath317 for @xmath318 is given by @xmath319 where @xmath312 is the transport matrix defined in .",
    "it is straightforward to show that the matrix @xmath320 $ ] is orthogonal for all @xmath69 , i.e.@xmath321 using we can write the parallel transport of a tangent vector @xmath322 as @xmath323 equation shows that the local coordinate representation of the tangent vector is constant at all points of the geodesic path @xmath316 when the basis for @xmath318 is given by @xmath317 . the global coordinate representation , on the other hand , varies with @xmath69 .",
    "this is an important observation since explicit parallel transport of tangents and hessians ( cf .",
    "section  [ sec : bfgs - opt ] ) can be avoided if the algorithm is implemented using local coordinates . the computational complexity for these two operations are @xmath324 and @xmath325 respectively . the cost saved in avoiding parallel transports of tangents and hessians",
    "is paid instead in the parallel transport of the basis @xmath296 .",
    "this matrix is computed in the first iteration at a cost of at most @xmath326 operations , and in each of the consecutive iterations , it is parallel transported at a cost of @xmath327 operations .",
    "there are also differences in memory requirements : in global coordinates tangents are stored as @xmath2 matrices and hessians as @xmath328 matrices , whereas in local coordinates tangents and hessians are stored as @xmath329 and @xmath330 matrices respectively .",
    "local coordinate implementation also requires the additional storage of @xmath296 as an @xmath331 matrix . in most cases ,",
    "the local coordinate implementation provides greater computational and memory savings , as we observed in our numerical experiments .    by introducing the thin svd of @xmath332 ,",
    "we can also write as @xmath333 this follows from the identities @xmath334 which are obtained from @xmath335",
    ". using this , we will derive a general property of inner products for our later use .",
    "[ thm : vectranprod ] let @xmath291 and @xmath336 .",
    "define the transport matrix in the direction @xmath303 @xmath337 where @xmath307 is the thin svd .",
    "then @xmath338 where @xmath339 and @xmath340 are parallel transported tangents .",
    "the proof is a direct consequence of the levi - civita connection used in the definition of the parallel transport of tangents .",
    "or we can use the canonical inner product on the grassmannian @xmath341 . then , inserting the parallel transported tangents we obtain @xmath342 the proof is concluded by observing that the second and third terms after the last equality are zero because @xmath343 and @xmath344 .",
    "[ [ remark ] ] remark + + + + + +    we would like to point out that it is the tangents that are parallel transported . in global coordinates tangents are represented by @xmath345 matrices and their parallel transport is given by .",
    "on the other hand , in local coordinates , tangents are represented by @xmath302 matrices and this representation does not change when the basis for the tangent space is parallel transported according to . in other words , in local coordinates , parallel transported tangents",
    "are represented by the _ same matrix _ at every point along a geodesic .",
    "this is to be contrasted with the global coordinate representation of points on the manifold @xmath0 , which are @xmath2 matrices that differ from point to point on a geodesic .",
    "in this section we will present the necessary modifications in order for bfgs updates to be well - defined on a grassmannian .",
    "we will write @xmath346 instead of @xmath239 since the argument to the function is a point on a grassmannian and represented by a matrix @xmath347_{i , j=1}^{n , r}\\in\\mathbb{r}^{n\\times r}$ ] . similarly the quantities @xmath256 and @xmath66 from equations and will be written as matrices @xmath348 and @xmath349 , respectively .",
    "we describe here the expressions of various quantities required for defining bfgs updates in global coordinates .",
    "the corresponding expressions in local coordinates are in the next section .",
    "[ [ gradient ] ] gradient + + + + + + + +    the grassmann gradient of the objective function @xmath346 is given by @xmath350   _ { i , j=1}^{n , r},\\ ] ] where @xmath351 is the projection on the tangent space @xmath290 .",
    "[ [ computing - s_k ] ] computing @xmath348 + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we will now modify the operations in equation , i.e.@xmath352 so that it is valid on a grassmannian .",
    "let @xmath353 be given by @xmath354 where the geodesic path originating from @xmath62 is defined by the tangent ( or search direction ) @xmath355 .",
    "the step size is given by @xmath70 .",
    "we will later assume that @xmath356 and with the tangent @xmath355 , corresponding to @xmath242 , we conclude that @xmath357 where @xmath358 is the transport matrix defined in .",
    "[ [ computing - y_k ] ] computing @xmath349 + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    similarly , we will translate @xmath359 from equation . computing the grassmann gradient at @xmath353 we get @xmath360 .",
    "parallel transporting @xmath361 along the direction @xmath303 and subtracting the two gradients as in equation we get @xmath362 where we again use the transport matrix . recall that @xmath349 corresponds to  @xmath66 .",
    "the expressions for @xmath266 , @xmath348 and @xmath349 are given in matrix form , i.e.  they have the same dimensions as the variable matrix @xmath59 .",
    "it is straightforward to obtain the corresponding vectorized expressions .",
    "for example , with @xmath363 , the vector form of the grassmann gradient is given by @xmath364 where @xmath365 is the ordinary column - wise vectorization of a matrix .",
    "for simplicity we switch to this presentation when working with the hessian .    [ [ updating - the - hessian - approximation ] ] updating the hessian ( approximation ) + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    identify the tangents ( matrices ) @xmath366 with vectors in @xmath305 and assume that the grassmann hessian @xmath367 at the iterate @xmath62 is given .",
    "then @xmath368 is the transported hessian defined at iterate @xmath353 . as previously @xmath358 is the transport matrix from @xmath62 to @xmath353 given in and @xmath369 is the transport matrix from @xmath353 to @xmath62 along the same geodesic path .",
    "informally we can describe the operations in as follows .",
    "tangent vectors from @xmath370 are transported with @xmath369 to @xmath371 on which @xmath244 is defined .",
    "the hessian @xmath244 transforms the transported vectors on @xmath371 and the result is then forwarded with @xmath358 to @xmath370 .",
    "since all vectors and matrices are now defined at @xmath353 , the bfgs update is computed using equation in which we replace @xmath244 with @xmath372 and use @xmath373 and @xmath374 from equations and respectively .      using local coordinates",
    "we obtain several simplifications .",
    "first given the current iterate @xmath59 we need the orthogonal complement @xmath296 .",
    "when it is obvious we will omit the iteration subscript @xmath40 .",
    "[ [ grassmann - gradient ] ] grassmann gradient + + + + + + + + + + + + + + + + + +    in local coordinates the grassmann gradient is given  by @xmath375 where we have used the global coordinate representation for the grassmann gradient .",
    "we denote quantities in local coordinates with a hat to distinguish them from those in global coordinates .",
    "[ [ parallel - transporting - the - basis - x_perp ] ] parallel transporting the basis @xmath296 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    it is necessary to parallel transport the basis matrix @xmath296 from the current iterate @xmath62 to the next iterate @xmath353 .",
    "only in this basis will the local coordinates of parallel transported tangents be constant .",
    "the parallel transport of the basis matrix is given by equation .",
    "[ [ computing - widehats_k - and - widehaty_k ] ] computing @xmath376 and @xmath377 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    according to the discussion in section [ qng : sec : glcoord ] , in the transported tangent basis @xmath317 , the local coordinate representation of any tangent is constant . specifically this is true for @xmath376 and @xmath377 .",
    "the two quantities are obtained with the same expressions as in the euclidean space .",
    "[ [ updating - the - hessian - approximation-1 ] ] updating the hessian ( approximation ) + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    since explicit parallel transport is not required in local coordinates , the hessian remains constant as well .",
    "the local coordinate representations for @xmath244 in the basis @xmath317 for points on the geodesic path @xmath316 are the same .",
    "this statement is proven in theorem [ thm : indlo ] .",
    "the effect of using local coordinates on the grassmannian is only in the geodesic transport of the current point @xmath62 and its orthogonal complement @xmath378 .",
    "the transported orthogonal complement @xmath379 is used to compute the grassmann gradient @xmath380 in local coordinates at the new iterate @xmath354 .",
    "assuming tangents are in local coordinates at @xmath62 in the basis @xmath378 and tangents at @xmath353 are given in the basis @xmath379 , the bfgs update is given by , i.e.  exactly the same update as in the euclidean space .",
    "this is a major advantage compared with the global coordinate update of @xmath244 . in global coordinates @xmath244",
    "is multiplied by matrices from the left and from the right .",
    "this is relatively expensive since the bfgs update itself is just a rank-@xmath51 update , see equation .",
    "it is not difficult to see that if the gradient @xmath381 is written as an @xmath2 matrix , then the second derivative will take the form of a 4-tensor @xmath382 .",
    "the bfgs update can be written in a different form using the tensor structure of the hessian .",
    "the action of this operator will map matrices to matrices .",
    "assuming @xmath373 and @xmath244 is a matricized form of @xmath383 , the matrix - vector contraction @xmath384 can be written as @xmath385 .",
    "obviously the result of the first operation is a vector whereas the result of the second operation is a matrix , and of course @xmath386 .    furthermore keeping the tangents , e.g.  @xmath348 or @xmath349 , in matrix form",
    ", the parallel transport of the hessian in equation can be written as a multilinear product between @xmath382 and the two transport matrices @xmath358 and @xmath369 , both in @xmath387 , along the first and third modes , @xmath388    finally , noting that the outer product between vectors corresponds to tensor products between matrices the bfgs update becomes are defined in appendix [ app:1 ] . ]",
    "@xmath389 where the matrices @xmath390 are given by and respectively .    in local coordinates the update is even simpler since we do not have to parallel transport the hessian operator , @xmath391 where @xmath392 and @xmath393 .",
    "the _ hat _ indicates that the corresponding variables are in local coordinates .",
    "assume now that the objective function @xmath110 is defined on a product of three for notational simplicity ; generalization of these discussions to arbitrary @xmath40 is straightforward . ] grassmannians , i.e.@xmath394 and is twice continuously differentiable .",
    "we write @xmath395 where @xmath396 , @xmath397 and @xmath398 .",
    "the hessian of the objective function will have a ` block tensor ' structure but the blocks will not have conforming dimensions .",
    "the action of a ( approximate ) hessian operator on tangents @xmath399 , @xmath400 and @xmath401 may be written symbolically as @xmath402 the blocks of the hessian are @xmath47-tensors and elements of the tangent spaces are matrices .",
    "the result of the operationis a triplet where each element is in the corresponding tangent space .",
    "for example @xmath403 is an @xmath404 tensor which acts on the tangent matrix @xmath405 of size @xmath406 with the result @xmath407 . off diagonal example",
    "may look as follows , @xmath408 is an @xmath409 tensor which acts on the tangent matrix @xmath410 of size @xmath2 with the result @xmath411 .",
    "the equality in the last step follows from the fact that the @xmath412 tensor @xmath413 is a permutation of the @xmath414 tensor @xmath408 .",
    "this is expected since for twice continuously differentiable functions @xmath415 .",
    "but in our case they have different ` shapes ' . the three tangent spaces",
    "@xmath290 , @xmath416 and @xmath417 are interconnected through the hessian of @xmath395 in the sense that every block in is a linear operator mapping matrices from one tangent space to another tangent space .",
    "for example @xmath418 and @xmath419 .",
    "the corresponding bfgs in the product manifold case has basically the same form as equations and where the action of the hessian on @xmath348 , which will be a triplet with an element on each tangent space , is replaced with formulas as in .",
    "also the tensor / outer product needs to be modified in the obvious way , i.e.  if @xmath420 and @xmath421 then we let @xmath422 where the results are conveniently stored in a ` block matrix ' whose blocks are tensors of different dimensions ( possibly nonconforming ) .",
    "the bfgs update in quasi - newton methods is optimal because it is the solution to @xmath423 where @xmath256 and @xmath66 are given by and respectively @xcite .",
    "for the euclidean case it is immaterial whether @xmath424 is considered as an abstract operator or explicitly represented as a matrix . the final conclusion with respect",
    "to optimality is the same  it amounts to a rank-@xmath51 change of @xmath244 .",
    "the situation is different when considering the corresponding optimality problem on grassmannians . in particular , a given hessian ( or approximate hessian ) matrix @xmath244 considered in a global coordinate representation and defined at @xmath425 has the following form when parallel transported along a geodesic , @xmath426 this is the same expression as equation .",
    "while the hessian operator should not change by a parallel transport to a new point on the manifold , its representation evidently changes .",
    "this has important numerical and computational ramifications .",
    "in fact , the global coordinate representation of the hessian at the previous point is usually very different from the global coordinate representation of the transported hessian at the current point .",
    "assume now the hessian matrix ( or its approximation ) is given in local coordinates @xmath427 at @xmath425 and let @xmath428 be the associated basis matrix for the tangent space .",
    "representation of the parallel transported hessian will not change if the associated basis matrix @xmath429 is transported according to .",
    "the updated hessian at the current point is a rank-@xmath51 modification of the hessian from the previous point given by the bfgs update .",
    "the optimality of bfgs update on euclidean spaces is with respect to a change in successive hessian matrices ; we will prove that in the correct tangent space basis and in local coordinates , the bfgs update is also optimal on grassmannians .",
    "we now give a self contained proof for this statement .",
    "first we will state the optimality results for the euclidean case .",
    "the proofs of theorem  [ thm : rank1update ] , lemma  [ lem : spdexist ] , and theorem  [ thm : eucl - bfgs - opt ] are based on @xcite .",
    "we will then use these to deduce the corresponding optimality result on a product of grassmannians in theorem  [ thm : opt ] .",
    "[ thm : rank1update]let @xmath430 , @xmath431 , @xmath432 . the solution to@xmath433 is given by@xmath434    note that while the set @xmath435 is non - compact ( closed but unbounded ) , for a fixed @xmath80 , the function @xmath436 , @xmath437 is coercive and",
    "therefore a minimizer @xmath438 is attained .",
    "this demonstrates existence .",
    "the minimizer is also unique since @xmath439 is convex while @xmath110 is strictly convex .",
    "we claim that @xmath440 : observe that @xmath441 and so @xmath442 ; for any @xmath443 , @xmath444    [ lem : spdexist]let @xmath431 , @xmath432 . then the set @xmath445 contains a symmetric positive definite matrix iff @xmath446 and @xmath447 for some @xmath448 and @xmath449 .",
    "if such @xmath450 and @xmath451 exist , then @xmath452 and so @xmath453 is a symmetric positive definite matrix in @xmath439 . on the other hand ,",
    "if @xmath443 is symmetric positive definite , its cholesky factorization @xmath454 yields an @xmath449 .",
    "if we let @xmath447 , then @xmath455 , as required .",
    "[ thm : eucl - bfgs - opt ] let @xmath431 , @xmath432 .",
    "let @xmath449 and @xmath456 .",
    "there is a symmetric positive definite matrix @xmath457 iff @xmath458 . in this case , the bfgs update @xmath459 is one where@xmath460    in order for the update to exist it is necessary that there exists @xmath461 and @xmath462 such that @xmath463 and @xmath464 . hence@xmath465 as required .",
    "if @xmath450 is known , then the nearest matrix to @xmath451 that takes @xmath450 to @xmath64 would be the update given in theorem  [ thm : rank1update ] , i.e.@xmath466 hence we need to find the vector @xmath450 . by lemma  [ lem : spdexist],@xmath467 and so @xmath468 for some @xmath469 .",
    "now it remains to find the scalar @xmath67 . plugging into and using @xmath456",
    ", we get@xmath470 if @xmath458 , this defines an update in @xmath439 that is symmetric positive definite .",
    "it is straightforward to verify that @xmath459 yields the bfgs update@xmath471    [ thm : indlo ] let @xmath291 and @xmath296 be the orthogonal complement to @xmath59 , i.e.  @xmath297 $ ] is orthogonal .",
    "let @xmath293 and @xmath472 be a geodesic with @xmath473 the corresponding transport matrix , defined according to equations and .",
    "identify @xmath290 with @xmath474 and consider a linear operator in local coordinates @xmath475 .",
    "consider the corresponding linear operator in global coordinates @xmath476 , in which tangents in @xmath290 are embedded .",
    "the relation between the two operators is given by @xmath477 furthermore , the parallel transported operator @xmath478 has the same representation for all @xmath69 along the geodesic @xmath316 , i.e.  @xmath479 .",
    "let @xmath480 be a tangent vector with corresponding global coordinate matrix representation @xmath481",
    ". obviously @xmath482 .",
    "we may write @xmath483",
    ". set @xmath484 and it follows that @xmath485 the corresponding operation in global coordinates are @xmath486 and it follows that @xmath487 , which proves .    for any tangent @xmath488 it holds that @xmath489 , where @xmath490 is a projection onto @xmath290 , and consequently @xmath491 .",
    "thus the operations in global coordinates also satisfy @xmath492 this proves equation .    for the third part we have @xmath493 and @xmath494 with @xmath495 .",
    "we want to prove that @xmath479 for all @xmath69 .",
    "the operator @xmath496 is defined in the following sense : a tangent @xmath497 is parallel transported with @xmath498 to @xmath499 along @xmath316 , the operator transformations is performed in @xmath290 , thus @xmath500 and the result is forwarded to @xmath318 , i.e.  @xmath340 .",
    "the parallel transported operator in global coordinates takes the form @xmath501 then , in the basis @xmath317 , the local coordinate representation of the operator is @xmath502 substituting into , we obtain @xmath503 recall that @xmath504 and thus @xmath505 .",
    "similarly one can show that @xmath506 and we get @xmath507 for all @xmath69 .    a different proof of essentially the same statement may be found in @xcite .",
    "[ lem : tind - prod - grass ] let @xmath508 , @xmath509 with corresponding tangent spaces @xmath510 .",
    "let @xmath511 be given such that @xmath512 $ ] is orthogonal . on each grassmannian @xmath513 ,",
    "let @xmath514 be a geodesic and @xmath515 be its orthogonal complement corresponding to the tangent @xmath516 .",
    "then a local coordinate representation of the linear operator @xmath517 is independent of @xmath69 when parallel transported along the geodesics @xmath514 and in the tangent basis @xmath515 .",
    "first we observe that the operator @xmath478 must necessarily have the structure @xmath518 where each @xmath519 , @xmath520 is such that @xmath521 . now , applying a similar procedure as in theorem  [ thm : indlo ] on each block @xmath519 proves that local coordinate representation of @xmath519 and thus @xmath478 is independent of @xmath69 along the geodesics @xmath514 in the tangent space basis @xmath515 .",
    "now we will give an explicit expression for the general bfgs update in tensor form and in local coordinates .",
    "we omit the hat and iteration index below for clarity . for a function defined on a product of @xmath40 grassmannians @xmath522",
    ", we write @xmath523 and @xmath524 , @xmath525 where @xmath526 and @xmath527 for @xmath528 .",
    "the hessian or its approximation has the symbolic form @xmath529 where each block is a @xmath47-tensor .",
    "the bfgs update takes the form , @xmath530 where the @xmath531 is given by a formula similar to with the result being a @xmath40-tuple , the tensor product between @xmath40-tuples of tangents is an obvious generalization of , and of course @xmath532 .",
    "finally , we have all the ingredients required to prove the optimality of the bfgs update on a product of grassmannians .",
    "[ optimality of bfgs update on product of grassmannians][thm : opt]consider a function @xmath523 in the variables @xmath508 , @xmath509 that we want to minimize .",
    "let @xmath514 be geodesic defined by @xmath516 with the corresponding tangent space basis matrices @xmath515 .",
    "in these basis for the tangent spaces , the bfgs updates in on the product grassmannians have the same optimality properties as a function with variables in a euclidean space , i.e.  it is the least change update of the current hessian approximation that satisfies the secant equations .",
    "first we observe that the grassmann hessian of @xmath523 ( or its approximation ) is a linear operator @xmath533 and according to lemma  [ lem : tind - prod - grass ] its local coordinate representation is constant along the geodesics @xmath514 . given this ,",
    "the bfgs optimality result on product grassmannians is a consequence from theorem  [ thm : eucl - bfgs - opt]the optimality of bfgs in euclidean space .",
    "[ [ remark-1 ] ] remark + + + + + +    an important difference on ( product ) grassmannians is that we need to keep track of the basis for the tangent spaces@xmath515 from equation .",
    "only then will the local coordinate representation of an operator be independent of @xmath69 when transported along geodesics .",
    "note that theorem  [ thm : opt ] is a coordinate dependent result .",
    "if we regard hessians as abstract operators , there will no longer be any difference between the global and the local scenario .",
    "but the corresponding optimality as the least amount of change in successive hessians can not be obtained in global coordinate representation and is thus not true if the hessians are regarded as abstract operators .",
    "movement along geodesics and parallel transport of tangents are the most straightforward and natural generalizations to the key operations from euclidean spaces to manifolds . there are also methods for dealing with the manifold structure in optimization algorithms based on different principles .",
    "for example , instead of moving along geodesics from one point to another on the manifold one could use the notion of _ retractions _ , which is a smooth mapping from the tangent bundle of the manifold onto the manifold .",
    "another example is the notion of _ vector transport _ that generalizes the parallel translation / transport of tangents used in this paper .",
    "all these notions are defined and described in @xcite .",
    "it is not clear how the use of the more general vector transport would effect the convergence properties of the resulting bfgs methods .",
    "we give a brief summary of the limited memory quasi - newton method with l - bfgs updates on euclidean spaces @xcite that we need later for our grassmann variant .",
    "see also the discussion in simon07 . in euclidean space",
    "the bfgs update can be represented in the following compact form @xmath534 where @xmath535   $ ] , @xmath536   $ ] , @xmath537   $ ] and @xmath538 are obtained using equations and .",
    "observe that in this section @xmath348 and @xmath349 are not the same as in and respectively .",
    "the limited memory version of the algorithm is obtained when replacing the initial hessian @xmath539 by a sparse matrix , usually this is a suitably scaled identity matrix @xmath540 , and only keep the @xmath541 most resent @xmath542 and @xmath543 in the update . since @xmath544 the amount of storage and computations in each iteration is only a small fraction compared to the regular bfgs . according to @xcite satisfactory results are often achieved with @xmath545 , even for large problems .",
    "our experiments confirm this heuristic .",
    "thus for the limited memory bfgs we have @xmath546 where now @xmath547   , \\quad y_{k}=\\left [   y_{k - m},\\dots , y_{k-1}\\right ]   , \\quad d_{k}=\\operatorname*{diag}\\left [   s_{k - m}^{\\mathsf{t}}y_{k - m},\\dots , s_{k-1}^{\\mathsf{t}}y_{k-1}\\right]\\ ] ] and @xmath548      analyzing the l - bfgs update above with the intent of modifying it to be applicable on grassmannians , we observe the following :    1 .   the columns in the matrices @xmath348 and @xmath349 represent tangents , and as such , they are defined on a specific point of the manifold . in each iteration",
    "we need to parallel transport these vectors to the next tangent space . assuming @xmath256 and @xmath66 are vectorized forms of and the transport amounts to computing @xmath549 and @xmath550 where @xmath358 is the grassmann transport matrix .",
    "2 .   the matrices @xmath551 and @xmath552 contain inner products between tangents .",
    "fortunately , the inner products are invariant with respect to parallel transporting . given vectors @xmath553 and a transport matrix @xmath191 from @xmath371 to @xmath370 , i.e.  @xmath554",
    ", we have that @xmath555 .",
    "this is a direct result from theorem  [ thm : vectranprod ] , showing that there is no need for modifying @xmath551 or @xmath552 .",
    "because of this property one may wonder whether the transport matrix @xmath191 is orthogonal , but this is not the case , @xmath556 .",
    "3 .   recalling the relation from equation between local and global coordinate representation of an operator",
    ", we conclude that the global representation is necessarily a singular matrix , simply because the local coordinate representation of the operator is a smaller matrix .",
    "the same is true for the hessian using global coordinates .",
    "but by construction , the l - bfgs update @xmath244 in is positive definite and thus nonsingular .",
    "this causes no problem since @xmath371 is an invariant subspace of @xmath244 , i.e.  if @xmath557 then @xmath558 , see lemma  [ lem : invss ] .",
    "similarly for the solution of the ( quasi-)newton equations since @xmath559 and @xmath560 , then obviously @xmath561 .",
    "this is valid for @xmath244 from both and .",
    "[ lem : invss ] the tangent space @xmath371 is an invariant subspace of the operator obtained by the l - bfgs update .",
    "this is straightforward .",
    "simply observe that for a vector @xmath562 we have that @xmath563 is a linear combination of vectors , and all of them belong to @xmath371 .",
    "l - bfgs algorithms are intended for large scale problems where the storage of the full hessian may not be possible . with this in mind",
    "we realize that the computation and storage of the orthogonal complement @xmath296 , which is used in local coordinate implementations , may not be practical . for large and sparse problems",
    "it is more economical to do the parallel transports explicitly than to update a basis for the tangent space .",
    "the computational time is reasonable since only @xmath564 vectors are parallel transported each step and @xmath541 is usually very small compared to the dimensions of the hessian .",
    "in this section we apply the algorithms developed in the last three sections to the tensor approximation problem described earlier .",
    "recall from section  [ sec : approx - max ] that the best multilinear rank-@xmath11 approximation of a general tensor is equivalent to the maximization of @xmath565 where @xmath12 and @xmath198 , @xmath134 , @xmath135 .",
    "recall also that @xmath16 may be regarded as elements of @xmath566 , @xmath567 , and @xmath0 respectively and @xmath568 may be regarded as a function defined on a product of the three grassmannians .",
    "the grassmann gradient of @xmath568 will consist of three parts .",
    "setting @xmath569 , one can show that in global coordinates the gradient is the triplet @xmath570 , where @xmath571 and @xmath572 , @xmath573 and @xmath574 , see equation .",
    "for derivation of these formulassee @xcite .    to obtain the corresponding expressions in local coordinates we observe that a projection matrix can also be written as @xmath575 .",
    "then for tangent vectors @xmath576 , we have @xmath577 which gives the local coordinates of @xmath578 as @xmath579 .",
    "the practical implication of these manipulations is that in local coordinates we simply replace the projection matrices @xmath580 with @xmath581 .",
    "we get @xmath582 , where @xmath583 note that the expressions of the gradient in global and local coordinates are different . in order to distinguish between them",
    "we put a hat on the gradient , i.e.  @xmath584 , when it is expressed in local coordinates .      in the general case",
    "we will have an order-@xmath40 tensor @xmath585 and the objective function takes the form @xmath586 the low rank approximation problem becomes @xmath587 the same procedure used to derive the gradients for the order-3 case can be used for the general case .",
    "the results are obvious modifications of what we have for 3-tensors .",
    "first we introduce matrices @xmath588 , @xmath509 such that each @xmath589 $ ] forms an orthogonal matrix and we define the tensors @xmath590 the grassmann gradient of the objective function in local coordinates is given by the @xmath40-tuple @xmath591 each @xmath592 is an @xmath593 matrix representing a tangent in @xmath510 . to obtain the corresponding global coordinate representation , simply replace each @xmath588 with the projection @xmath594",
    ".    we will also give the expression of the hessian since we may wish to initialize our approximate hessian with the exact hessian .",
    "furthermore , in our numerical experiments in section  [ qng : sec : comex ] , the expression for the hessian will be useful for checking whether our algorithms have indeed arrived at a local maximum . in order to express the hessian",
    ", we will need to introduce the additional variables @xmath595 where each term is a multilinear tensor - matrix product involving the tensor @xmath57 and a subset of the matrices in @xmath596 .",
    "the subscripts @xmath597 and @xmath598 in @xmath599 indicate that @xmath588 and @xmath600 are multiplied in the @xmath597th and @xmath598th mode of @xmath57 , respectively .",
    "all other modes are multiplied with the corresponding @xmath601 , @xmath602 and @xmath603 .",
    "for example we have @xmath604 together with @xmath605 , introduced earlier , one can express the complete grassmann hessian of the objective function @xmath606 .",
    "the derivation of the hessian is somewhat tricky .",
    "the interested reader should refer to @xcite for details . in this paper we only state the final result in a form that can be directly implemented in a solver .",
    "the diagonal blocks of the hessian are sylvester operators and have the form @xmath607 the off - diagonal block operators are @xmath608 where @xmath609 , @xmath610 and @xmath611 see appendix [ app:1 ] for definition of the contracted products @xmath612 .",
    "recall from section  [ sec : tensors ] that an order-@xmath40 tensor @xmath613 is called _ symmetric _ if @xmath614 where @xmath615 , the set of all permutations with @xmath40 integers . for example , a third order cubical tensor @xmath616 is symmetric iff @xmath617 for all @xmath618 .",
    "the definition given above is equivalent to the usual definition given in , say @xcite ; see @xcite for a proof of this simple equivalence .",
    "recall also that the set of all order-@xmath40 dimension-@xmath95 symmetric tensors is denoted @xmath100 .",
    "this is a subspace of @xmath101 and@xmath619    [ lem : symrank]if @xmath620 and @xmath621 , then@xmath622 in other words , the multilinear rank of a symmetric tensor is always of the form @xmath623 for some @xmath19 .",
    "we will write @xmath193 for this common value .",
    "furthermore , we have a multilinear decomposition of the following form @xmath624 where @xmath625 and @xmath626 .",
    "the ranks @xmath627 being equal follows from observing that the matricizations @xmath628 of @xmath24 are , due to symmetry , all equal .",
    "the factorization is a consequence of the higher order singular value decomposition ( hosvd )  @xcite .    in application where noise is an inevitable factor",
    ", we would like to study instead the approximation problem @xmath629 instead of the exact decomposition in .",
    "more precisely , we want to solve@xmath630    similar analysis as in the general case shows that the minimization problem can be reformulated as a maximization of @xmath631 , with the constraint @xmath200 .",
    "the objective function becomes @xmath632 where now @xmath633 .",
    "observe that the symmetric tensor approximation problem is defined on one grassmannian only , regardless of the order of the tensor .",
    "these problems require much less storage and computations compared to a general problem of the same dimensions .",
    "applications involving symmetric tensors are found in signal processing , independent component analysis , and the analysis of multivariate cumulants in statistics @xcite .",
    "we refer interested readers to @xcite for discussion of a different notion of rank for symmetric tensors .",
    "the same procedure for deriving the gradient for the general case can be used to obtain the gradient for the symmetric case . in particular",
    "it involves the very same terms as the nonsymmetric gradient with obvious modifications .",
    "it is straightforward to show that , due to symmetry of @xmath24 , @xmath634 we will use the first expression without loss of generality . in which case",
    ", the grassmann gradient in global coordinates becomes @xmath635 where @xmath351 ; and in local coordinate it is @xmath636 where @xmath296 is the orthogonal complement of @xmath59 . compare these with equations  for the general case .",
    "as for the general case discussed in @xcite , we may identify the second order terms in the taylor expansion of @xmath637 .",
    "there are 15 second order terms and all have the form @xmath638 for some linear operator @xmath639 .",
    "two specific examples are @xmath640 where @xmath641 , @xmath642 and @xmath643 .",
    "the subscripts @xmath144 and @xmath51 indicate that the projection matrix @xmath490 is multiplied with @xmath24 in the first and second mode respectively .",
    "not surprisingly , analysis of these terms reveals equality among the second order terms due to the symmetry of @xmath24 .",
    "gathering like terms and summing up the expressions , we see that the hessian is a sum of three different terms , @xmath644 so the action of the hessian on a tangent is simply@xmath645 observe that the second term in arises from the fact that the objective function is defined on a grassmannian , see @xcite for details .      with the analysis and expressions for symmetric @xmath26-tensors at hand , generalization to symmetric @xmath40-tensors",
    "is straightforward .",
    "we will only state the final results and in local coordinates .",
    "assume we have an order-@xmath40 symmetric tensor @xmath104 .",
    "the corresponding symmetric low rank tensor approximation problem is written as @xmath646 using the tensor products @xmath647 where @xmath296 is such that @xmath315 $ ] forms an orthogonal matrix , the grassmann gradient becomes @xmath648 observe that the symmetric case involves the very same tensor products @xmath649 as in the general case ( given in section  [ sec : genderiv ] ) but due to the symmetry of the problem all terms are equal .",
    "we also introduce tensor - matrix multilinear products @xmath599 similar to those in equation .",
    "two specific examples are @xmath650 in general @xmath599 , where @xmath651 , @xmath652 and @xmath653 , is a multilinear product of two @xmath296 s that are multiplied in the @xmath597th and @xmath598th mode of @xmath24 .",
    "all other modes are multiplied with @xmath59 .",
    "the second order terms of the taylor expansion of @xmath223 contain the following diagonal block operators @xmath654 again , due to symmetry all these are identical and summing them up we get @xmath655 the off - diagonal block operators have the form @xmath656 where @xmath651 , @xmath652 and @xmath657 similarly , due to symmetry all of them are identical .",
    "we have @xmath658 the complete grassmann hessian operator is simply @xmath659      the second order terms are described using the canonical inner product on grassmannians and contracted tensor products . next we will derive the expression of the hessian as a matrix acting on the vector @xmath660 .",
    "the terms in involve only matrix operations and vectorizing the second argument in the inner product yields @xmath661 the vectorization of the terms from and involve the 4-tensors @xmath662 and is done using the tensor matricization described in @xcite .",
    "we get @xmath663 in @xmath664 we map indices of the first and third mode to row - indices and indices of the second and fourth mode to column - indices obtaining the matrix @xmath665 . in this way the contractions in the matrix - vector product coincide with the tensor - matrix contractions . similarly for @xmath666 .",
    "the matrix form of the hessian becomes @xmath667 to obtain the hessian in local coordinates we replace @xmath490 with @xmath296 in the computations of the factors involved and thereafter perform the same matricization procedure .",
    "we will now give two small explicit examples to illustrate the computations involved in the algorithms for tensor approximation described before .",
    "in this example we will compute the gradient of the objective function , both in global and in local coordinates .",
    "let the @xmath668 tensor @xmath57 be given by @xmath669 let the current point of the product manifold be given by @xmath670 where @xmath671 are the corresponding projection matrices onto the three tangent spaces .",
    "the expression for the grassmann gradient at the current iterate is given by .",
    "the intermediate quantities , cf .  equation , needed in the calculations of the grassmann gradient are@xmath672 and the grassmann gradient in global coordinates is given by @xmath673 to compute the grassmann gradient in local coordinates we need a basis for the tangent spaces . for the current iterate we choose @xmath674 as the corresponding basis matrices for the tangent spaces at @xmath59 , @xmath60 and @xmath61 .",
    "obviously @xmath315 $ ] , @xmath675 $ ] and @xmath676 $ ] are orthogonal and @xmath677 . replacing the projection matrices @xmath490 , @xmath678 and @xmath679 by the orthogonal complements @xmath296 , @xmath680 and @xmath681 in , we obtain @xmath682 , and thus the local coordinate representation of the grassmann gradient is given by @xmath683 recall that we use a hat to distinguish local coordinate representation from global coordinate representation . the local coordinate representation is depending on the choice of basis matrices for the tangent spaces .",
    "a different choice of @xmath296 , @xmath680 and @xmath681 would yield a different representation of @xmath584 .",
    "[ sec : exp2]next we will illustrate the parallel transport of tangent vectors along geodesics on a product of grassmannians . let the tensor @xmath57 , the current iterate , and the corresponding gradient be the same as in the previous example . introduce tangent vectors @xmath684 clearly we have @xmath685 .",
    "the tangent @xmath303 will determine the geodesic path from the current point and in turn the transport of the grassmann gradient ( see figure  [ qng : man ] ) .",
    "we may also verify that @xmath686 is indeed a tangent of the product grassmannian at the current iterate .",
    "the thin or compact svds , written @xmath687 of the tangents are@xmath688    the transport matrix , cf .  equation , in the direction @xmath578 at @xmath59 with a step size @xmath689 is given by @xmath690 similarly , it is straightforward to calculate @xmath691 parallel transporting one tangent we get @xmath692 . for all tangents in @xmath303 and @xmath686",
    "we get @xmath693    the above are calculations in global coordinates . in local",
    "coordinates we parallel transport the basis matrices @xmath296 , @xmath680 and @xmath681 so that the local coordinate representation of a tangent is the same as in the previous point .",
    "the computations are given by equation and in this example we get @xmath694 i.e.  the second and third columns of each transport matrix due to the specific choice of @xmath296 , @xmath680 and @xmath681 .    taking a step of size @xmath689 from @xmath59 , @xmath60 and @xmath61 along the specified geodesic we arrive  at @xmath695 the value of the objective function at the starting point is @xmath696 and at the new point is @xmath697 , an increment as expected .",
    "figure  [ qng : man ] illustrates the procedures involved in the algorithms on the grassmannian @xmath698 , which we may regard as the @xmath51-sphere @xmath699 ( unit sphere in @xmath700 ) .",
    "for the best rank-@xmath144 tensor approximation of a @xmath668 tensor , the optimization takes place on a product of three spheres @xmath701 , one for each vector that needs to be determined .",
    "the procedure starts at a point @xmath702 and a direction of ascent , therefore ` ascent ' as opposed to ` descent ' . ]",
    ", the tangent @xmath703 , is obtained through some method .",
    "next we perform a movement of the point @xmath702 along the geodesic defined by @xmath704 .",
    "geodesics on spheres are just great circles . at the new point",
    "@xmath705 we repeat the procedure , i.e.  determine a new direction of ascent @xmath322 and take a geodesic step in this direction .    , which is simply the sphere @xmath699.,scaledwidth=80.0% ]",
    "all algorithms described here and the object oriented grassmann classes required for them are available for download as two matlab packages @xcite and @xcite .",
    "we encourage our readers to try them out .",
    "we will now test the actual performance of our algorithms with a few large numerical examples .",
    "all algorithms in a given test are started with the same initial points on a grassmannian , represented as truncated singular matrices from the hosvd and a number of additional _ higher order orthogonal iterations_hooi iterations @xcite , which are introduced to make the initial hessian of @xmath568 negative definite .",
    "the number of initial hooi iterations ranges between @xmath50 and @xmath706 depending on the size of the problem .",
    "the bfgs algorithm is either started with ( possibly a modification of ) the exact hessian or a scaled identity matrix according to @xcite .",
    "the l - bfgs algorithm is always started with a scaled identity matrix but one can modify the number of columns @xmath541 in the matrices representing the hessian approximation , see equation .",
    "this number is between @xmath50 and @xmath707 .",
    "although we use the hosvd to initialize our algorithms , any other reasonable initialization procedure would work as long as the initial hessian approximate is negative definite .",
    "the quasi - newton methods can be used as stand - alone algorithms for solving the tensor approximation problem as well as other problems defined on grassmannians .    in the following figures ,",
    "the @xmath64-axis measures the norm of the relative gradient , i.e.  @xmath708 , and the @xmath63-axis shows iterations .",
    "this ratio is also used as our stopping condition , which typically requires that @xmath709 , the machine precision of our computer . at a true local maximizer",
    "the gradient of the objective function is zero and its hessian is negative definite . in the various figures we present convergence results for four principally different algorithms .",
    "these are ( 1 ) quasi - newton - grassmann with bfgs , ( 2 ) quasi - newton - grassmann with l - bfgs , ( 3 ) newton - grassmann , denoted with ng and ( 4 ) hooi which is an alternating least squares approach .",
    "in addition , the tags for bfgs methods may be accompanied by i or h indicating whether the initial hessian was a scaled identity matrix or the exact hessian , respectively .",
    "we run all our numerical experiments in matlab on a macbook with a @xmath710-ghz intel core 2 duo processor and @xmath47  gb of physical memory .    figure  [ qng : fig:1 ] shows convergence results for two tests with tensors generated with @xmath711-distributed values .",
    "tensor is approximated by a rank-@xmath52 tensor .",
    "bfgs initiated with the exact hessian ; in l - bfgs @xmath712 .",
    "_ right : _",
    "a @xmath713 tensor is approximated by a rank-@xmath714 tensor . in this case",
    "the initial hessian is a scaled identity and @xmath715.,title=\"fig:\",scaledwidth=49.0% ]    tensor is approximated by a rank-@xmath52 tensor .",
    "bfgs initiated with the exact hessian ; in l - bfgs @xmath712 .",
    "_ a @xmath713 tensor is approximated by a rank-@xmath714 tensor . in this case",
    "the initial hessian is a scaled identity and @xmath715.,title=\"fig:\",scaledwidth=49.0% ]    in the left plot a @xmath716 tensor is approximated with a rank-@xmath52 tensor .",
    "one can observe superlinear convergence in the bfgs method .",
    "the right plot shows convergence results of a @xmath713 tensor approximated with a rank-@xmath714 tensor .",
    "both bfgs and l - bfgs methods exhibit rapid convergence in the vicinity of a stationary point .",
    "figure  [ qng : fig:2 ] ( _ left _ ) shows convergence for an even larger @xmath45 tensor approximated by a tensor of rank-@xmath717 using l - bfgs with @xmath718 .",
    "tensor approximated by a rank-@xmath717 tensor .",
    "_ right : _ effect of varying @xmath541 in l - bfgs .",
    "a @xmath719 tensor approximated by a rank-@xmath720 tensor with @xmath721.,title=\"fig:\",scaledwidth=49.0% ]   tensor approximated by a rank-@xmath717 tensor . _",
    "right : _ effect of varying @xmath541 in l - bfgs . a @xmath719 tensor approximated by a rank-@xmath720 tensor with @xmath721.,title=\"fig:\",scaledwidth=49.0% ]    in the right plot",
    "we approximate a @xmath719 tensor by a rank-@xmath720 tensor where we vary over a range of values of @xmath541 in the l - bfgs algorithm , namely , @xmath721 .",
    "@xmath712 gives ( in general ) slightly poorer performance , otherwise the different runs can not be distinguished .",
    "in other words , our grassmann l - bfgs algorithm can in practice work as well as our grassmann bfgs algorithm , just as one would expect ( from the numerical experiments performed ) in the euclidean case .    figure  [ qng : fig:3 ] shows convergence plots for two symmetric tensor approximation problems .",
    "symmetric tensor is approximated by a rank-@xmath50 symmetric tensor ; @xmath715 .",
    "_ right : _ here we have a @xmath722 symmetric tensor approximated by a rank-@xmath723 symmetric tensor ; @xmath715.,title=\"fig:\",scaledwidth=49.0% ]    symmetric tensor is approximated by a rank-@xmath50 symmetric tensor ; @xmath715 . _",
    "right : _ here we have a @xmath722 symmetric tensor approximated by a rank-@xmath723 symmetric tensor ; @xmath715.,title=\"fig:\",scaledwidth=49.0% ]    in the left plot we approximate a symmetric @xmath719 tensor by a rank-@xmath50 symmetric tensor . we observe that bfgs initialized with the exact hessian ( bfgs : h tag ) converges much more rapidly , almost as fast as the newton - grassmann method , than when initialized with a scaled identity matrix ( bfgs : i tag ) . in the right plot we give convergence results for a @xmath713 symmetric tensor approximated by a rank-@xmath723 symmetric tensor . in both cases @xmath715 .    in figure  [ qng : fig:4 ]",
    "we show the performance of a local coordinate implementation of the bfgs algorithm on problems with @xmath47-tensors .",
    "the first plot shows convergence results for a @xmath48 tensor approximated by a rank-@xmath53 tensor .",
    "the second convergence plot is for a symmetric @xmath47-tensor with the same dimensions approximated by a symmetric rank-@xmath50 tensor .",
    "tensor is approximated by a rank-@xmath53 tensor .",
    "_ right : _ here we have a @xmath724 symmetric tensor approximated by a rank-@xmath50 symmetric tensor.,title=\"fig:\",scaledwidth=49.0% ]   tensor is approximated by a rank-@xmath53 tensor . _",
    "right : _ here we have a @xmath724 symmetric tensor approximated by a rank-@xmath50 symmetric tensor.,title=\"fig:\",scaledwidth=49.0% ]    again the h and i tags indicate whether the exact hessian or a scaled identity is used for initialization .",
    "we end this section with two unusual examples to illustrate the extent of our algorithms applicability : a high order tensor and an objective function that includes tensors of different orders .",
    "the left plot in figure  [ qng : fig:5 ] is a high - order example : it shows the convergence of bfgs verses hooi when approximating an order-@xmath27 tensor with dimensions @xmath725 with a rank-@xmath726 tensor . the right plot in figure  [ qng : fig:5 ]",
    "has an unusual objective function that involves an order-@xmath51 , an order-@xmath26 , and an order-@xmath47 tensor , @xmath727 where @xmath728 is a @xmath729 symmetric matrix , @xmath730 is a @xmath731 symmetric @xmath26-tensor , and @xmath732 is a @xmath733 symmetric @xmath47-tensor .",
    "such objective functions have appeared in _",
    "independent component analysis with soft whitening _ @xcite and in _ principal cumulants components analysis _",
    "@xcite where @xmath734 measure the multivariate variance , kurtosis , skewness respectively ( cf .  example  [ eg : cum ] ) .",
    "tensor of order-10 is approximated with a rank-@xmath726 .",
    "_ right : _ a ` simultaneous ' rank-@xmath50 approximation of a weighted sum of tensors of orders @xmath51 , @xmath26 , and @xmath47.,title=\"fig:\",scaledwidth=49.0% ]   tensor of order-10 is approximated with a rank-@xmath726 . _",
    "right : _ a ` simultaneous ' rank-@xmath50 approximation of a weighted sum of tensors of orders @xmath51 , @xmath26 , and @xmath47.,title=\"fig:\",scaledwidth=49.0% ]    in both examples we observe a fast rate of convergence at the vicinity of a local minimizer for the bfgs algorithm .",
    "it is evident from the convergence plots here that bfgs and l - bfgs have faster rate of convergence compared with hooi .",
    "the newton - grassmann algorithm takes few iterations but is computationally more expensive , specifically for larger problems .",
    "our implementation of the different algorithms in matlab give shortest runtime for the bfgs and l - bfgs methods .",
    "the time for one iteration of bfgs , l - bfgs and hooi is of the same magnitude for smaller problems . in larger problems ,",
    "the l - bfgs performs much faster than all other methods .",
    "our algorithms use the basic arithmetic and data types in the tensortoolbox @xcite for convenience .",
    "we use our own object - oriented routines for operations on grassmannians and product of grassmannians , e.g.  geodesic movements and parallel transports @xcite .",
    "we note that there are several different ways to implement bfgs updates @xcite ; for simplicity reasons , we have chosen to update the inverse of the hessian approximation",
    ". a possibly better alternative will be to update the cholesky factors of the approximate hessians so that one may monitor the approximate hessians for indefiniteness during the iterations @xcite .",
    "the grassmann quasi - newton methods presented in this report all fit within the procedural framework given in algorithm  [ qng : alg : frame ] .",
    "given tensor @xmath57 and starting points @xmath735 and an initial hessian @xmath736 * 1 * compute the grassmann gradient . *",
    "2 * parallel transport the hessian approximation to the new point . * 3 * update the hessian or its compact representation . * 4 * solve the quasi - newton equations to obtain @xmath737 . * 5 * move the points @xmath738 along the geodesic curve given by @xmath303 .",
    "[ [ general - case ] ] general case + + + + + + + + + + + +    in analyzing computational complexity , we will assume for simplicity that @xmath57 is a general @xmath739 @xmath26-tensor being approximated with a rank-@xmath740 @xmath26-tensor",
    ". a problem of these dimensions will give rise to a @xmath741 hessian matrix in global coordinates and a @xmath742 hessian matrix in local coordinates .",
    "table [ qng : tab : compcmpl ] gives approximately the amount of computations required in each step of algorithm  [ qng : alg : frame ] .",
    "recall that in l - bfgs @xmath541 is a small number , see section  [ qng : sec : lbfgs ] .",
    "[ c]c|c|c|c & bfgs - gc & bfgs - lc & l - bfgs + * 1 * & @xmath743 & @xmath744 & @xmath743 + * 2 * & @xmath745 &  & @xmath746 + * 3 * & @xmath747 & @xmath748 &  + * 4 * & @xmath749 & @xmath750 & @xmath751 +    we have omitted terms of lower asymptotic complexity as well as the cost of point * 5 * since that is negligible compared with the costs of points * 1**4*. for example , the geodesic movement of @xmath62 requires the thin svd @xmath752 which takes @xmath753 flops ( floating point operations ) @xcite . on the other hand ,",
    "given the step length @xmath69 and @xmath754 in , the actual computation of @xmath316 amounts to only @xmath755 flops .",
    "[ [ symmetric - case ] ] symmetric case + + + + + + + + + + + + + +    the symmetric tensor approximation problem involves the determination of one @xmath2 matrix , resulting in an @xmath756 hessian in global coordinates and an @xmath757 hessian in local coordinates .",
    "therefore the complexity of the symmetric problem differs only by a constant factor from that of the general case .",
    "[ [ curse - of - dimensionality ] ] curse of dimensionality + + + + + + + + + + + + + + + + + + + + + + +    the approximation problem will suffer from the _ curse of dimensionality _ when the order of a tensor increases .",
    "in general , an @xmath758 order-@xmath40 tensor requires the storage of @xmath759 entries in memory .",
    "the additional memory requirement , mainly for storing the hessian , is of order @xmath760 for the bfgs methods and @xmath761 for the l - bfgs method , respectively . in the current approach",
    "we assume that the tensor is explicitly given .",
    "our proposed algorithms are applicable as long as the given tensor fits in memory .",
    "there have been various proposals to deal with the curse of dimensionality using tensors @xcite . for cases where the tensor is represented in compact or functional forms our methods can take direct advantage of these simply by computing the necessary gradients ( and hessians ) using the specific representations .",
    "in fact this was considered in @xcite for symmetric tensor approximations .",
    "[ [ convergence ] ] convergence + + + + + + + + + + +    there is empirical evidence suggesting that als based algorithms have fast convergence rate for specific tensors .",
    "this was also pointed out in @xcite .",
    "these are tensors that have inherently low multilinear rank and the approximating tensor has the correct low ranks , or tensors that have fast decay in its multilinear singular values @xcite , or a substantial gap in the multilinear singular values at the site of truncation , e.g. the source tensor is given by a low rank tensor with noise added . on the other hand",
    "not all tensors have gaps or fast decaying multilinear singular vales .",
    "this is specifically true for sparse tensors .",
    "it is still desirable to obtain low rank approximations for these more difficult \" tensors .",
    "and on these tensors als performs very poorly , but methods using first and second order derivatives of the objective function , including the methods presented in this paper perform good . among the methods that are currently available , quasi - newton methods presented in this paper have the best computational efficiency .",
    "there are several different approaches to solve the tensor approximation problem . in this section",
    "we will briefly describe them and point out the main differences with our work .",
    "the algorithms most closely related to the quasi - newton methods are given in @xcite .",
    "all three references address the best low rank tensor approximation based on the grassmannian structure of the problem and use explicit computation of the hessian .",
    "the obtained newton equations are solved either fully @xcite or approximately @xcite . in the latter case",
    "the authors used a truncated conjugate gradient approach to approximately solve the newton equations .",
    "the iterates are updated using the more general notion of retractions instead of taking a step along the geodesic on the manifold .",
    "in addition a trust region scheme is incorporated making the procedure more stable with respect to occasional indefinite hessians .",
    "the computation of the hessian is a limiting factor in these algorithms .",
    "this is the case even when the hessian is not formed explicitly but used implicitly via its action on a tangent . in our experiments , on moderate - sized problems , e.g.  @xmath26-tensors of dimensions around @xmath762 , the bfgs methods noticeably outperformed hessian - based methods ; and for dimensions around @xmath763 , we were unable to get any methods relying on hessians to work despite our best efforts .",
    "there is a different line of algorithms for related tensor approximation problems based on als and mutltigrid accelerated als @xcite . in our experience ,",
    "the convergence of als - type methods depend on the decay of the multilinear singular values of the given tensor .",
    "the exact dependence is unclear but the relation seems to be that the faster the decay , the faster the convergence of als . in this regard",
    "the class of functions and operators considered in @xcite appears to possess these favorable properties .",
    "yet a third approach to obtain low multilinear rank tensor approximations are the cross methods in @xcite .",
    "the novelty of such methods is that they discard some given information and retain only a fraction of the original tensor , and as such it is markedly different from our approach , which uses all given information to achieve maximal accuracy .",
    "in addition , there is an assumption on the tensor that there exist approximations within pre - specified bounds and of specific low ranks while we make no such assumptions .",
    "in this paper we studied quasi - newton algorithms adapted to optimization problems on riemannian manifolds .",
    "more specifically , we proposed algorithms with bfgs and l - bfgs updates on a product of grassmannians that ( 1 ) respect the riemannian metric structure and ( 2 ) require only standard matrix operations in their implementations .",
    "two different algorithmic implementations are presented : one based on local / intrinsic coordinates while the other one uses global / embedded coordinates .",
    "in particular , our use of local coordinates is a novelty not previously explored in other manifold optimization @xcite .",
    "we proved the optimality of our grassmannian bfgs updates in local coordinates , showing that the well - known bfgs optimality @xcite extends to grassmannian and products of grassmannians .",
    "we also applied these algorithms to the problem of determining a best multilinear rank approximation of a tensor and the analogous ( but very different ) problem for a symmetric tensor .",
    "while a newton version of this was proposed in @xcite , here we make substantial improvements with respect to the grassmann - newton algorithm in terms of speed and robustness .",
    "furthermore , we presented specialized algorithms that take into account the symmetry in the multilinear approximation of symmetric tensors and related problems .",
    "in addition to the numerical experiments in this paper , we have made our codes freely available for download @xcite so that the reader may verify the speed , accuracy , and robustness of these algorithms for himself .",
    "in this section we define the contracted tensor product notation used throughout this paper . for given third order tensors @xmath57 and @xmath58",
    "we define the following contracted products : @xmath764 when contracting several indices , with the corresponding indices of the two arguments being the same , we write @xmath765 the subscript ` 1 ' in @xmath766 and subscripts ` 1,2 ' in @xmath767 indicate that the contraction is over the first index and both the first and second indices respectively .",
    "if instead the contraction is to be performed on different indices , we write @xmath768 the subscripts indicating the indices to be contracted are separated by a semicolon .",
    "it is also convenient to introduce a notation when contraction is performed in all but one or a few indices .",
    "for example the products in and may also be written @xmath769        e.  anderson , z.  bai , c.  h.  bischof , j.  w.  demmel , j.  j.  dongarra , j.  j.  du croz , a.  greenbaum , s.  j.  hammarling , a.  mckenney , s.  ostrouchov , and d.  c.  sorensen , _ lapack users guide _ , 3rd ed . , siam , philadelphia , pa , 1999 .",
    "l.  de  lathauwer , _ tucker compression , parallel factor analysis and block term decompositions : new results _ , european meeting on challenges in modern massive data sets ( emmds 09 ) , copenhagen , denmark , 2009 .",
    "l.  de  lathauwer , l.  hoegaerts , and j.  vandewalle , _ a grassmann - rayleigh quotient iteration for dimensionality reduction in ica _ , proc .",
    "independent component analysis and blind signal separation ( ica 04 ) , 5 ( 2004 ) , pp .",
    "335342 .",
    "l.  de  lathauwer and j.  vandewalle , _ dimensionality reduction in higher - order signal processing and rank-@xmath771 reduction in multilinear algebra _ , linear algebra appl . , 391 ( 2004 ) , pp .",
    " , _ dimensionality reduction in ica and rank-@xmath772 reduction in multilinear algebra _ , proc .",
    "conf .  independent component analysis and blind signal separation ( ica 04 ) , 5 ( 2004 ) , pp .",
    "295302 .",
    "m.  ishteva , l.  de  lathauwer , p .- a .",
    "absil , and s.  van  huffel , _ dimensionality reduction for higher - order tensors : algorithms and applications _ , int",
    ".  j.  pure appl .",
    ", 42 ( 2008 ) , no .  3 , pp .",
    " , _ best low multilinear rank approximation of higher - order tensors , based on the riemannian trust - region scheme _ , tech",
    ", 09 - 142 , esat - sista , katholieke universiteit leuven , leuven , belgium , 2009 .",
    "r.  lehoucq , d.  sorensen , and c.  yang , _",
    "arpack users guide _ ,",
    "siam , philadelphia , pa , 1998 .",
    "lim and j.  morton , _ cumulant component analysis : a simultaneous generalization of pca and ica _ , computational algebraic statistics , theories and applications ( casta 08 ) , kyoto university , kyoto , japan , december 1011 2008 .",
    "l.  omberg , g.  h.  golub , and o.  alter , _ a tensor higher - order singular value decomposition for integrative analysis of dna microarray data from different studies _ , proc .",
    "sci . , 104 ( 2007 ) , no .",
    "47 , pp .",
    "1837118376 . i.  v.  oseledets , _ compact matrix form of the @xmath268-dimensional tensor decomposition _ ,",
    "tech .  rep .",
    ", institute of numerical mathematics , russian academy of science , moscow , russia , 2009 .",
    "i.  v.  oseledets , d.  v.  savostianov , and e.  e.  tyrtyshnikov , _ tucker dimensionality reduction of three - dimensional arrays in linear time _",
    ", siam j.  matrix anal .",
    "appl . , 30 ( 2008 ) , no .  3 , pp ."
  ],
  "abstract_text": [
    "<S> in this paper we proposed quasi - newton and limited memory quasi - newton methods for objective functions defined on grassmannians or a product of grassmannians . </S>",
    "<S> specifically we defined bfgs and l - bfgs updates in local and global coordinates on grassmannians or a product of these . </S>",
    "<S> we proved that , when local coordinates are used , our bfgs updates on grassmannians share the same optimality property as the usual bfgs updates on euclidean spaces . when applied to the best multilinear rank approximation problem for general and symmetric tensors , our approach yields fast , robust , and accurate algorithms that exploit the special grassmannian structure of the respective problems , and which work on tensors of large dimensions and arbitrarily high order . </S>",
    "<S> extensive numerical experiments are included to substantiate our claims .    </S>",
    "<S> grassmann manifold , grassmannian , product of grassmannians , grassmann quasi - newton , grassmann bfgs , grassmann l - bfgs , multilinear rank , symmetric multilinear rank , tensor , symmetric tensor , approximations    65f99 , 65k10 , 15a69 , 14m15 , 90c53 , 90c30 , 53a45 </S>"
  ]
}