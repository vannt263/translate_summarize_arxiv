{
  "article_text": [
    "biological neurons and synapses can provide the blueprint for inference and learning machines that are potentially thousandfold more energy efficient than mainstream computers .",
    "however , the breadth of application and scale of present - day neuromorphic hardware remains limited , mainly due to a lack of general and efficient inference and learning algorithms compliant with the spatial and temporal constraints of the brain .",
    "+ machine learning and deep learning are well poised for solving a broad set of applications using neuromorphic hardware , thanks to their general - purpose , modular , and fault - tolerant nature @xcite .",
    "one outstanding question is whether the learning phase in deep neural networks can be efficiently carried out in neuromorphic hardware as well .",
    "performing learning online can confer continuous adaptability in dynamic or less controlled environments ( where no prior , representative dataset exists ) and more fine - grained context awareness to behaving cognitive agents , while reducing the energetic footprint of learning .",
    "however , deep learning usually relies on the immediate availability of network - wide information stored with high - precision memory .",
    "the non - locality of deep learning on a brain - like substrate is due to the gradient descent back - propagation ( bp ) rule @xcite , which requires the transpose of the feedforward matrices in the feedback path . in digital computers ,",
    "the access to this information funnels through the von neumann bottleneck , which dictates the fundamental limits of the computing substrate .",
    "distributing computations along multiple cores ( such as in gpus ) is a popular solution to mitigate this problem , but even there the scalability of bp is often limited by its memory - intensive operations @xcite . + here , we build on the recent advances in approximate forms of the gradient bp rule @xcite for training spiking neurons of the type used in neuromorphic hardware to perform supervised learning .",
    "these approximations solve the non - locality problem by replacing bp weights with random ones , leading to remarkably little loss in classification performance on benchmark tasks @xcite .",
    "although a general theoretical understanding of random bp ( rbp ) is still lacking , extended simulations and analyses of linear networks show that , during learning , the network adjusts its feed - forward weights to learn an approximation of the pseudo - inverse of the ( random ) feedback weights , which is at least equally good in communicating gradients .",
    "+ we describe how an event - driven implementation of rbp can be tightly embedded with the neuron dynamics , and lay out the foundations of neuromorphic deep learning machines .",
    "our results show that mnist classification performances of the on - line trained spiking network are nearly identical to those of equivalent artificial neural networks running on digital computers .",
    "finally , we show that erbp is robust in fixed width simulations with limited neural and synaptic state precision .",
    ".[tab : erbp ] bold indicates online trained spiking network [ cols= \" > , < , < , < , < , < \" , ]      +    event - driven rbp ( erbp ) is a presynpatic spike - driven rule modulated by top - down errors and gated by the membrane voltage of the postsynaptic neuron .",
    "the idea behind this additional modulation factor is similar in spirit to previous biologically plausible models of three - factor plasticity rules @xcite , and was argued to subserve supervised , unsupervised and reinforcement learning , an idea that was also reported in @xcite .",
    "the erbp learning rule can be summarized as follows : @xmath0\\,\\theta(v[t ] ) \\mathrm{s^{pre}}(t)\\ ] ] where @xmath1 is the pre - synaptic spike train , and a @xmath2 is the derivative of the spiking neuron s activation function evaluated at the post - synaptic membrane potential @xmath3 $ ] . for the final output ( prediction )",
    "layer , @xmath4 is equal to the classification error @xmath5 , similarly to the delta rule . for hidden layers ,",
    "the @xmath4 is equal to the error multiplied by a random vector @xmath6 , _ i.e. _ @xmath7 . all @xmath8 are fixed during the learning . + for neurons , we find that a boxcar function in place of @xmath2 provides very good results , while being more amenable to hardware implementation than computing the exact derivative of the activation function .",
    "this choice is motivated by the fact that the activation function of leaky i&f neurons with absolute refractory period can be approximated by a threshold unit with saturation .",
    "using a boxcar function with boundaries @xmath9 and @xmath10 , the erbp update consist of additions and comparisons only , and can be captured using the following operations for each neuron :    @xmath11 ,    this rule is reminiscent of membrane voltage - based rules , where spike - driven plasticity is induced only when membrane voltage is inside an eligibility window @xcite .",
    "furthermore , erbp does not require `` causal '' updates of , so reverse look - up tables or recently studied approximate forms of are not necessary @xcite .",
    "the learning rule assumes that the @xmath12 modulation is available at the pre - synaptic synapses . in the following section",
    ", we show how erbp can be realized in a neural network of integrate & fire ( i&f ) neurons .",
    "the network used for erbp consists of one or two feed - forward layers ( fig .",
    "[ fig : erbp ] ) , although results can be generalized to any feedforward connectivity between any number of layers .",
    "the top layer , labeled @xmath13 , is the prediction .",
    "the feedback from the error population is fed back directly to the hidden layers  neurons .",
    "this type of feedback is also called `` skipped rbp''@xcite .",
    "this direct feedback simplifies the delivery of the error modulation to the hidden neurons and improves the performance of the learning on standard benchmarks @xcite .",
    "+ the proposed network is composed of three types of neurons : + 1 ) * error - coding neurons * are non - leaky i&f neurons following the linear dynamics : @xmath14 where @xmath15 and @xmath16 are spike trains from prediction neurons and labels ( teaching signal ) , respectively and @xmath17 is a fixed common bias .",
    "the notation for time dependencies of @xmath18 and @xmath19 are omitted to reduce clutter .",
    "the firing rate of the error - coding neurons is proportional to a linear rectification of the inputs .",
    "when the prediction neurons classify correctly , the two terms on the rhs cancel and the neuron fires at a constant rate @xmath20 . when an error occurs , the firing rate deviates linearly from @xmath21 down to the rectification point .",
    "+ 2 ) * hidden neurons * follow current - based leaky i&f dynamics : @xmath22 where @xmath23 and @xmath24 are the spike trains of the data neurons and the hidden neurons , respectively , @xmath25 are current - based synapse dynamics , @xmath26 a poisson process of rate @xmath27hz , and @xmath28 is a stochastic bernouilli process with probability @xmath29 .",
    "the latter type of stochasticity was previously shown to be beneficial for regularization and decorrelation of spike trains , while being easy to implement in neuromorphic hardware @xcite . in this work ,",
    "we consider feed - forward networks , _",
    "i.e _ the weight matrix @xmath30 is restricted to be upper diagonal .",
    "each neuron is equipped with a separate `` dendritic '' compartment @xmath31 following similar subthreshold dynamics as the membrane potential and where @xmath32 is the spike train of the error - coding neurons and @xmath33 is a fixed random matrix such that for every hidden neuron @xmath34 , @xmath35 .",
    "this condition ensures that the spontaneous firing rate of the error - coding neurons does not bias the learning .",
    "the synaptic weights dynamics follow a dendrite - modulated and neural state - gated rule : @xmath36 3 ) * prediction neurons * , synapses and synaptic weight updates follow the same dynamics as the hidden neurons except for the dendritic compartment : @xmath37 where the extra bias term @xmath38 counteracts the effect of the spontaneous firing rate of the error - coding neurons .",
    "we trained fully connected feed - forward networks mnist hand - written digits , separated in three groups , training , validation , and testing ( 50000 , 10000 , 10000 samples respectively ) . during a training epoch ,",
    "each of the training diits were presented in sequence during @xmath39 .",
    "the pixel intensities of the samples were transformed into spike trains online using `` sigmoid neurons '' , _",
    "i.e. _ a spike response model @xcite with an exponential activation ( hazard ) function and a refractory period matching those of the hidden and prediction neurons .",
    "we tested erbp using two configurations : one with additive noise ( @xmath40 , @xmath41 , labeled erbp ) , and one with multiplicative noise implemented as blank - out noise on the connections ( blank - out probability @xmath42 and @xmath43 , labeled perbp ) .",
    "stochasticity was beneficial in reducing synchronization across hidden and prediction layers .",
    "furthermore , to prevent the network from learning ( spurious ) transitions between digits , the synaptic weights did not update in the first @xmath44 window of each digit presentation .",
    "+ we tested erbp training on two different implementations : 1 ) spiking neural network based on the auryn simulator @xcite , and 2 ) hardware compatible simulator with quantized neural states and weights .",
    "results are compared against gpu implementations of rbp and standard bp performed in theano @xcite using an equivalent , non - spiking neural network .",
    "we focus on permutation - invariant ( pi ) tasks to emphasize that the network was unstructured ( _ e.g. _ no convolutions or poolings ) .",
    "( * left * ) mnist classification error on a fully connected 784 - 200 - 200 - 10 network .",
    "the spiking network converged after 60 epochs .",
    "( * right * ) classification error and number of synaptic operations using first spikes upon stimulus onset in the 784 - 200 - 10 network .",
    "horizontal line is 2.85%.,title=\"fig : \" ] ( * left * ) mnist classification error on a fully connected 784 - 200 - 200 - 10 network .",
    "the spiking network converged after 60 epochs .",
    "( * right * ) classification error and number of synaptic operations using first spikes upon stimulus onset in the 784 - 200 - 10 network .",
    "horizontal line is 2.85%.,title=\"fig : \" ]      we tested erbp in networks consisting of one and two hidden layers ( tab .",
    "[ tab : erbp ] ) , although erbp can generalize to deeper networks as well . rather than optimizing for absolute classification performance",
    ", we compare to equivalent artificial ( non - spiking ) neural networks trained with rbp and standard bp , with free parameters fine - tuned to achieve high accuracy on the classification tasks . on most network configurations erbp achieved performances equivalent to those achieved with rbp in artificial networks .",
    "erbp equipped with probabilistic connections ( perbp ) performed better overall , and more so for the deeper , 784 - 200 - 200 - 10 , network .",
    "this is because , as learning progresses , a significant portion of the neurons tend to fire near their maximum rate and synchronize their spiking activity across layers as a result of large synaptic weights ( and thus pre - synaptic inputs ) .",
    "synchronized spike activity is not well captured by a simple firing rate model , which is inherently assumed by the erbp formulation .",
    "however , probabilistic connections that randomly blank - out pre - synaptic spikes , effectively introduce irregularity in the pre - synaptic spike - trains to improve learning .",
    "additive noise , on the other hand , has relatively small effect when the magnitude of the pre - synaptic input is large .",
    "overall , the learned classification accuracy is comparable with those obtained with offline training of spiking neural networks ( _ e.g. _ gpus ) using exact standard backpropagation @xcite .",
    "the response of the 784 - 200 - 10 network after stimulus onset is about one synaptic time constant . using",
    "the first spike after @xmath45 from the stimulus onset for classification leads to less than 4.5% error ( fig .",
    "[ fig : edp ] ) , and improves steadily as the number output layer spikes increase .",
    "classification using the first spike incurred 53407 synaptic operations ( averaged over 10000 test samples ) , which can potentially result in sub - micro joule energy per classification in dedicated neuromorphic hardware @xcite .",
    "the low latency and accurate response may seem at odds with the inherent firing rate code underlying the network computations .",
    "however , a code based on the time of the first - spike is consistent with a firing rate code , since a neuron with a high firing rate is expected to fire first @xcite .",
    "in addition , the onset of the stimulus provokes a burst of synchronized activity , which further favors the rapid onset of the response .",
    "these results suggest that despite the underlying firing rate code , erbp takes advantage of the spiking dynamics , comparably to spiking networks trained exclusively for single - spike classification @xcite      the spiking neural network erbp requires fewer epochs to reach the peak classification performance compared to the artificial neural network running rbp .",
    "the artificial neural network was trained using minibatches of @xmath46 data samples .",
    "minibatches improve learning speed in conventional hardware thanks to vectorization libraries or leveraging gpus , and lead to smoother convergence .",
    "however , the weight updates are averaged over the minibatch , resulting in @xmath47 times fewer weight updates per epoch compared to the spiking network per epoch where @xmath47 is effectively equal to one standard artificial neural networks can be trained using @xmath48 , but learning becomes much slower on standard platforms because the operations can not be vectorized across data samples .",
    "such faster learning of the spiking network is a fortunate by - product of on - line gradient descent , given that potentials applications of neuromorphic hardware often involve real - time streaming data .",
    "the effectiveness of stochastic gradient descent degrades when the precision of the synaptic weights using a fixed point representation is smaller than 16 bits @xcite .",
    "this is because quantization determines the smallest learning rate and bounds the range of the synaptic weights , thereby preventing averaging the variability across dataset iterations .",
    "the tight integration of memory with computing circuits as pursued in neuromorphic chip design is challenging due to space constraints and memory leakage .",
    "for this reason , full precision ( or even 16 bit ) computer simulations of spiking networks may be unrepresentative of performance that can be attained in dedicated neuromorphic chip due to quantization of neural states and parameters , and synaptic weights .",
    "+ extended simulations suggest that the random bp performances at 10 bits precision is indistinguishable from unquantized weights @xcite , but whether this is the case for online learning without using minibatches was not yet tested . here , we hypothesize that 8 bit synaptic weight is a good trade - off between learnability and hardware footprint .",
    "to demonstrate robustness to such constraints , we simulate quantized versions of the erbp network using low precision fixed point representations ( 8 bits per synaptic weight and 16 bits for neural states ) .",
    "consistent with existing findings , our simulations of erbp in a quantized 784 - 100 - 10 network show that erbp still performs reasonably well under these conditions ( fig .",
    "[ fig : qedp ] ) . while many weights aggregate at the boundaries , a majority of them remain between the boundaries .",
    "( * left * ) mnist classification error using a fully connected 784 - 100 - 10 network with quantized synaptic weights and neural states ( 8 bits and 16 bits respectively ) ( * right * ) synaptic weights of the quantized network after training.,title=\"fig : \" ] ( * left * ) mnist classification error using a fully connected 784 - 100 - 10 network with quantized synaptic weights and neural states ( 8 bits and 16 bits respectively ) ( * right * ) synaptic weights of the quantized network after training.,title=\"fig : \" ]",
    "our results demonstrates that general purpose deep learning using streaming spike - event data in neuromorphic platforms at artificial neural network proficiencies is realizable .",
    "erbp can be implemented in a very compact fashion , while using local synaptic plasticity rules modulated by global error on a neuron - to - neuron basis .",
    "although our experiments targeted digital neuromorphic implementations , membrane - voltage based learning rules implemented in mixed - signal neuromorphic hardware @xcite are compatible with erbp provided that synaptic weight updates can be modulated by an external signal on a neuron - to - neuron basis .",
    "thus , the recent advances in neuromorphic engineering and emerging nanotechnologies combined with erbp can become key to ultra low - power processing in space and power constrained platforms . in tandem with developments in deep learning and recurrent neural networks",
    ", we envision that such rbp techniques will enable the embedded learning of pattern recognition , attention , working memory and action selection mechanisms which promise transformative hardware architectures for embedded computing .",
    "this work was partly supported by the intel corporation and by the national science foundation under grant 1640081 , and the nanoelectronics research corporation ( nerc ) , a wholly - owned subsidiary of the semiconductor research corporation ( src ) , through extremely energy efficient collective electronics ( excel ) , an src - nri nanoelectronics research initiative under research task i d 2698.003 .",
    "x.  zhu , m.  awatramani , d.  rover , and j.  zambreno , `` onac : optimal number of active cores detector for energy efficient gpu computing , '' in _ computer design ( iccd ) , 2016 ieee 34th international conference on_.1em plus 0.5em minus 0.4emieee , 2016 , pp .",
    "512519 .",
    "p.  u. diehl _",
    "et  al . _ ,",
    "`` fast - classifying , high - accuracy spiking deep networks through weight and threshold balancing , '' in _ international joint conference on neural networks ( ijcnn),_.1em plus 0.5em minus 0.4emieee , 2015 , pp .",
    "j.  park _ et  al .",
    "_ , `` a 65k - neuron 73-mevents / s 22-pj / event asynchronous micro - pipelined integrate - and - fire array transceiver , '' in _ biomedical circuits and systems conference ( biocas)_.1em plus 0.5em minus 0.4emieee , oct . 2014 ."
  ],
  "abstract_text": [
    "<S> an ongoing challenge in neuromorphic computing is to devise general and computationally efficient models of inference and learning which are compatible with the spatial and temporal constraints of the brain . </S>",
    "<S> the gradient descent backpropagation rule is a powerful algorithm that is ubiquitous in deep learning , but it relies on the immediate availability of network - wide information stored with high - precision memory . </S>",
    "<S> however , recent work shows that exact backpropagated weights are not essential for learning deep representations . </S>",
    "<S> random backpropagation replaces feedback weights with random ones and encourages the network to adjust its feed - forward weights to learn pseudo - inverses of the ( random ) feedback weights . here </S>",
    "<S> , we demonstrate an event - driven random backpropagation ( erbp ) rule that uses an error - modulated synaptic plasticity for learning deep representations in neuromorphic computing hardware . </S>",
    "<S> the rule is very suitable for implementation in neuromorphic hardware using a two - compartment leaky integrate & fire neuron and a membrane - voltage modulated , spike - driven plasticity rule . </S>",
    "<S> our results show that using erbp , deep representations are rapidly learned without using backpropagated gradients , achieving nearly identical classification accuracies compared to artificial neural network simulations on gpus , while being robust to neural and synaptic state quantizations during learning . </S>"
  ]
}