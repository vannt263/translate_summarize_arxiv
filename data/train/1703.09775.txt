{
  "article_text": [
    "onset detection and instrument recognition in polyphonic music are two of the most important sub - tasks in automatic music transcription ( amt ) , and represent processing challenges on their own in the music information retrieval ( mir ) community .",
    "on one side , onset detection can be roughly defined as the automatic process of locating each single note onset in a musical piece , and where the notion of note onset , perhaps better defined as a phenomenal accent @xcite , refers to discrete temporal events in an audio stream where there is a marked change in any of the perceived psychoacoustical properties of sound , i.e. , loudness , timbre , and pitch .",
    "its applications are multifold .",
    "onset detection is a frontend to beat induction algorithms @xcite , empowers segmentation for rhythmic analysis ( tempo identification and meter identification ) and event manipulation both online and offline @xcite , and provides a basis for automatically collating event databases for compositional and information retrieval applications @xcite . throughout the present document ,",
    "we adopt a signal processing approach , striving to detect magnitude changes , harmonic changes and pitch leaps .",
    "onset detection task can be roughly divided into three different blocks : 1 ) sound representation ; 2 ) computation of a detection function , and 3 ) extraction of note events .",
    "we will put a special focus on the first operation of investigating the most appropriate sound representation for onset extraction . a widely used approach",
    "to onset detection in the frequency domain is the spectral flux @xcite , where sudden changes of the sound spectrum are detected by differentiating the signal s short - time successive spectra .",
    "detection of onsets in a monophonic signal is not a difficult problem , especially if onsets are prominent , which is the case of most  decay \" instruments .",
    "for example , onsets in a monophonic piano signal could be calculated with high accuracy by simply locating peaks in the amplitude envelope of the input signal . however , in complex polyphonic mixtures of music , there are simultaneously occurring events with different combinations of playing techniques . because the amplitude envelope of an entire signal reveals little of what is going on in individual frequency regions of the signal , where note onsets and offsets may occur , resulting in masking effects and blurred note transitions , polyphonic music makes it hard to detect individual onsets . as a consequence",
    ", detection functions were proposed that analyze the signal in a band - wise fashion to extract transients occurring in certain frequency regions of the signal @xcite .    on the other side ,",
    "instrument recognition is defined as the automatic process of identifying an instrument given a sound input produced by it .",
    "the goals of automatic recognition of instruments are multiple : first , to provide labels for monophonic recordings , for `` sound samples '' inside sample libraries , or for new patches created with a given synthesizer , and to provide indexes for locating the main instruments that are included in a musical mixture .",
    "the majority of research on the automatic recognition of musical instruments until now has been made on isolated notes or on excerpts from solo performances , depending on the application .",
    "a comprehensive review of proposed approaches on instrument recognition can be found in @xcite .",
    "common methods combine tools from audio signal processing and machine learning .",
    "a successful combination was mel - frequency cepstral coefficients ( mfccs ) as a sound representation and support vector machine ( svm ) for the classification , as in @xcite who used 16 mfccs on 0.2 second sound segments to label 8 solo instruments playing musical scores from well - known composers .",
    "other promising applications of svm that are related to music classification but are not specific to music instrument labelling can be found in @xcite .",
    "sophisticated gaussian - mixture and hidden - markov models @xcite have also been developed on such feature vectors to optimize audio classification . as other noticeable research works on instrument recognition , @xcite developed a system to recognize solo instruments in accompanied sonatas and concertos , using the relative magnitudes of harmonics ( normalized to the overall magnitude of the harmonics ) as the feature for classification , and showed that the feature is robust against background accompaniment .",
    "@xcite developed the software instogram for instrument recognition , based on the temporal trajectory of instrument existence probabilities for every possible @xmath1 .    over the past few years",
    "much research has been devoted to finding effective representations of sound to address the many challenging problems in mir @xcite .",
    "common sound representations include fourier transform , mfccs and wavelets , which have all been extensively used in mir tasks .",
    "extending these representations , @xcite developed a mathematical operator called deep scattering transform , consisting in a cascade of wavelet decompositions and modulus operators .",
    "on the contrary to common wavelet transforms @xcite , scattering transform was developed around the notion of invariability , crucial to define robust instrument - specific templates used in supervised classification .",
    "all current application studies have revealed a great potential of scattering representation to serve automatic retrieval / classification tasks of unidentified numerical objects , including musical instrument classification @xcite , note characterization @xcite , genre recognition @xcite , face recognition @xcite , environmental sounds @xcite , texture classification @xcite and medical signal analysis @xcite .",
    "our paper aims to build upon these achievements , and to further strengthen these experimental validations through the two amt subtasks of onset detection and instrument recognition .",
    "we now present the organization of this paper . in section [ theorsr ] ,",
    "we present briefly the theoretical background of scattering transforms , tracing its evolution through well - know sound representation methods . in section",
    "[ methods ] , we develop methods and computational details for the two tasks of onset detection and instrument recognition . in section",
    "[ results ] , the performances of scattering method are evaluated comparatively to other methods , and discussed to highlight the potential usefulness of scattering for some amt applications .",
    "the short - time fourier transform of a time series @xmath2 , used to build classical spectrogram , is defined by    @xmath3    where @xmath4 is a time window of size t. spectrograms compute then locally time - shift invariant descriptors over durations limited by a window .",
    "however , @xcite showed that high - frequency spectrogram coefficients are not stable to variability due to time - warping deformations , which often occur in audio signals .",
    "mel - frequency spectral coefficients ( mfscs ) are obtained by averaging the spectrogram @xmath5 over mel - frequency intervals , which can be written as    @xmath6    where each @xmath7 covers a mel - frequency interval indexed by j. mffcs are cosine transforms of mfscs , and are efficient local descriptors at time scales up to t ( @xmath8 ) .",
    "log - frequency scales have actually been widely used in various audio processing applications , because it makes signals stable to deformation by low - pass filtering it , hence removing the more variable high - frequency content . to capture longer - range structures , these mfccs are either aggregated in segments @xcite that cover longer time intervals , or are complemented with other features such as delta - mfccs @xcite .",
    "the continuous wavelet transformation ( cwt ) was introduced in order to overcome the limited time - frequency localization of the fft for nonstationary signals and was found to be suitable in a lot of applications @xcite . unlike the fft , the continuous wavelet transformation has a variable time - frequency resolution grid with a high frequency resolution and a low time resolution in low - frequency area and a high temporal / low frequency resolution on the other frequency side . in that respect",
    "it is similar to the human ear which exhibits similar time - frequency resolution characteristics @xcite .",
    "mathematically , wavelet analysis seeks to address the defect of the fourier transform by decomposing the time series into local , time - dilated , and time - translated wavelet components using time - frequency atoms or wavelets @xmath9 : @xmath10    @xmath11 is the basic wavelet function that satisfies certain very general condition , a is the scale and b is the time shift .",
    "@xmath12 is then the  energy \" of f(t ) of scale a at time b.      scattering representation was introduced by @xcite .",
    "it is interesting to underline the motivations which originate this method , conceived as a wavelet - based extension of mfscs .",
    "we already mentioned that high - frequencies are more sensitive to deformation than low - frequencies , which makes the fourier - based spectrogram particularly non - adapted to take into account small deformations of a signal .",
    "the logarithmic averaging used in a mel - scale removes this instability , providing to the mfscs a non - variable representation of a signal from one observation to another .",
    "however , this averaging also induces a loss of high - frequency information , especially over time intervals larger than t , which is why mel - frequency spectrograms are limited to such short time intervals",
    ". based on these observations , scattering aims to provide a stable transform , yet without any loss of information .    in mathematical terms",
    ", we saw that mfscs coefficients are defined as the spectrogram averaged along a mel - frequency scale .",
    "the averaging resulting from the integration of the mfscs ( eq . [ mfscexp ] ) is actually equivalent ( by the parseval s theorem ) to convolute them with a low - pass filter @xmath13 , as follows @xmath14 .",
    "this averaging naturally losses high - frequency information .",
    "the scattering transform then aims to recover the information lost by averaging , observing that equation @xmath15 can be written as the low - frequency component of the wavelet transform of @xmath16 , i.e. : @xmath17    where j and p delimit filter supports on which each wavelet @xmath9 is dilated in a specific way ( see @xcite for details ) . since the wavelet transform is invertible , the information lost by the convolution with @xmath18 is recovered by the wavelet coefficients @xmath19 .",
    "averaging @xmath19 by @xmath18 again entails a loss of high frequencies , which can be recovered by a new wavelet transform . iterating this process leads to establish the scattering transform s , stable to deformation , @xmath20    the calculation of eq .",
    "[ eqcaswav ] , as illustrated in figure [ diagblocscatt ] , can be viewed as a cascade of a wavelet modulus propagator u , defined as u x = @xmath21 .",
    "that is why , the scattering representation finds its main application in characterizing high - frequency acoustic features . while providing a multiscale representation of x , the scattering transform",
    "consists of a highly non linear transform , as opposed to the underlying discrete wavelet transform .",
    "furthermore , one can also separate a filter factor from an excitation factor using a logarithm and a discrete cosine transform , an operation similar to mfccs .",
    "such coefficients are referred to as cosine log scattering coefficients ( clscs ) .",
    "still like in mfccs , only the low - frequency of the discrete cosine transform coefficients are kept .",
    "the scattering feature vector for each time position k is then defined as @xmath22 , where only the two first order coefficients are exploited , and @xmath23 represents the normalized second order scattering coefficients , so as to remove the dependency of the amplitude of second order coefficients upon that of the first order coefficients .",
    "the feature dimensionality here is 386 .    in practice , in the present contribution ,",
    "a complex wavelet is used , consisting of the analytic part ( restriction to positive frequencies ) of a battle - lemarie cubic spline wavelet @xcite .",
    "the window @xmath24 is the cubic spline scaling function associated to this wavelet .",
    "for all scattering computation , we use the scatnet matlab software , available at http://www.di.ens.fr / data / software / scatnet/.",
    "the aim of onset detection function ( odf ) is to highlight onsets in the signal so as to provide a clear onset trace .",
    "we use a simple energy - based odf built upon the spectral flux . the spectral flux @xcite describes the temporal variation of the magnitude spectrogram by computing the difference between two consecutive short - time spectra .",
    "this odf gives a measure of the non - stationarity of the signal in each frame of the spectral transform by calculating the deviation of each frequency bins energy from a prediction made using the previous frames .",
    "in mathematical terms , we have    @xmath25    with @xmath26 being the half - wave rectifier function .",
    "the rectification has the effect of counting only those frequencies where there is an increase in energy , and is intended to emphasize onsets rather than offsets @xcite .",
    "we have chosen to omit other common methods such as phase deviation @xcite , high frequency content @xcite or rectified complex domain @xcite , since they only exhibited negligible enhancements on our test experiments .",
    "the shape of the odf bears a great importance . in an ideal case ,",
    "at those time instants where phenomenal accent occur the odf would display well - localized narrow peaks whose magnitudes are proportional to the sound intensity change . a simple peak - picking above a fixed threshold",
    "would be enough to find onset locations . in practice , the odf tends to be much noisier over the range of real world signals for a number of reasons , such as the occurrence of various onset types , a low signal - to - noise ratio and loudness variations . to take into account these variations ,",
    "most odfs are post - processed with a dynamic thresholding @xmath27 , and in our case the corresponding activation probabilities , which can be computed ( adapted from @xcite ) as    @xmath28    with @xmath29 an offset coefficient .",
    "this threshold @xmath30 is applied to the onset function , leading to a thresholded observation , whose non - zero values indicate peaks in the stft , which can be simply picked with a maxima search . the peak - processing stage selects onset candidate peaks above the adaptive threshold @xmath30 and discards those being too small in a 25 ms range around a larger peak .      to evaluate comparatively the performances of scattering representation on the task of onset detetion ,",
    "the different sound representations presented in section [ theorsr ] ( i.e. fft , mfcc , wavelets ) are also applied to this task with the same procedure exposed in sections [ defodf ] and [ defpp ] . also , tu put into perspective our results , two amt algorithms are used considered : tolonen model @xcite and the halca algorithm @xcite .",
    "halca is a state - of - the - art algorithm for the task of onset detection , evaluated by the mirex community with an average transcription score of 62 % @xcite , and will serve as a performance benchmark for all results .",
    "evaluation of onset detection is naturally performed using a note - oriented approach @xcite .",
    "we then define correct note events based on tolerance errors around onset estimations . a correct note event is then assumed to be correct if its onset is within a 40 ms range of a ground - truth onset .",
    "evaluation metrics are defined by following equations [ metr1]-[metr4 ] @xcite , resulting in the note - based onset - offset recall ( tpr ) , fall - out ( fpr ) , precision ( ppv ) and f - measure ( the harmonic mean of precision and recall ) :    @xmath31}}{\\sum_{n=1}^n { \\text{tp}[n ] + \\text{fn}[n]}}\\ ] ]    @xmath32}}{\\sum_{n=1}^n { \\text{fp}[n ] + \\text{tn}[n]}}\\ ] ]    @xmath33}}{\\sum_{n=1}^n { \\text{tp}[n ] + \\text{fp}[n]}}\\ ] ]    @xmath34    where n is the total number of notes , and tp , fp and fn scores stand for the well - known true positive , false positive and false negative detections .",
    "the tpr and fpr metrics ( eq . [ metr1]-[metr2 ] ) can then be included in roc ( receiver operating characteristic ) curves , which is a graphical plot illustrating the performance of a binary classifier system as its discrimination threshold is varied @xcite .",
    "for each value , indexed by the entire k , of the scaling factor @xmath35 present in the threshold @xmath27 ( eq . [ defthres ] ) , 20 different musical sequences are tested and the average of their respective metrics is used as the coordinate @xmath36 associated to this value . along a roc curve , we can define the so - called operation point , defined as the threshold position giving the best detection score , i.e. the highest tpr and lowest fpr .",
    "based on the @xmath37 and @xmath38 scores at this point , numerical performances can be attributed for each method through the metric @xmath39 defined as    @xmath40        based on the structural risk minimization inductive principle @xcite , a svm is machine learning technique using a systematic approach to find a linear function with the lowest complexity . for linearly non - separable data , svms can ( non - linearly ) map the input to a high dimensional feature space where a linear hyperplane can be found .",
    "this mapping is done by means of a so - called kernel function .",
    "the classification performances of feature vectors are evaluated with an svm classifier computed with a gaussian kernel .      for instrument recognition",
    ", we formatted our scattering coefficients by log - compressing their coefficients in order to reduce intra - instrument variation , getting the already defined clscs ( see sec .",
    "[ scatttheo ] ) , and by removing redundancy between coefficients with a principal component analysis pre - processing , shifting down their dimension from 346 to 50 , comparable to the mfcc order .",
    "clscs will be evaluated comparatively to the delta - mfccs coefficients in the task of instrument recognition .",
    "delta - mfccs @xcite are defined as the difference between mfcc coefficients of two consecutive audio frames and thus cover a time interval of twice the size .",
    "these complement the ordinary mfccs , providing information on the temporal audio dynamics over longer time intervals .",
    "each audio track of our evaluation datasets ( being either note samples or musical excerpts ) is decomposed in frames of duration t which are represented using delta - mfccs or clscs .",
    "our instrument recognition has been made regardless of the pitch of sound samples .",
    "a multi - class svm is implemented over the audio frames with a 1 vs 1 approach which trains an svm to discriminate each pair of classes . to classify a whole track , each frame is classified using the svm and the class with the largest number of frames in the track is selected ( a method called maximum voting ) .",
    "the gaussian kernel parameter and the svm slack variable c are optimized with a cross - validation on a subset of the training set . for the classification part ,",
    "the dataset was randomly split into training ( 70% ) and test data ( 30% ) .",
    "the training data were used to train the classifier , which was then tested with a five - fold cross - validation with the unseen test data to give success rates for each class , expressed as a confusion matrix .",
    "final results are calculated in the form of the error rates of wrongly classified tracks in the test set .",
    "table [ table_detailsdatasound_scatt ] provides full details on the sound material used for evaluation of the tasks of onset detection and instrument recognition in this paper , and table [ table_detailsdatasound_datasetformat ] details how the different datasets are defined . before detailing specific datasets for the two tasks of onset detection and instrument recognition",
    ", we detail their common characteristics . for each instrument and pitch ,",
    "18 sound templates were extracted , from 3 different instrument models .",
    "sources are detailed in table [ table_detailsdatasound_scatt ] .",
    "the continuous musical excerpts are 23.8s - long sequences randomly selected from different musical pieces ( resulting either from midi scores or real audio recordings ) .",
    "we remove from the generated sequences the truncated notes .      for evaluation of the onset detection task , we only used instruments @xmath41 and @xmath42 , namely the _ marovany _ zither and the classical piano .",
    "a first dataset @xmath43 was built automatically by combining midi scores and pre - recorded real instrument samples per pitch .",
    "such a generative process allows generating a large set of labelled training data with a minimal amount of human labour , allowing the inclusion of an important variety of different instruments and scores .",
    "in addition to that , it uniformizes the different test sequences , removing variability between data due to recording and production conditions . when synthesizing a sequence from a midi score and real note templates , an instrument model is first selected .",
    "then , for each note location , the template is scaled to the correct amplitude given by the midi file for each note event .",
    "as our templates encompass a certain variety in the playing dynamic , we can match as well as possible the modifications of timbre induced by this dynamic .",
    "to do so , our templates are first ranged by amplitudes , and selected accordingly to their position on the midi amplitude scale .",
    "our template - based midi sequences were also degraded using the audio degradation toolbox @xcite , to approximate audio recording conditions .",
    "one may criticize that they are less realistic than real recordings , but actually this process is not that far different from other midi - driven dataset generation such as the disklavier technology ( used e.g. in @xcite ) , as musical parameters are  midi - discretized \" all the same .",
    "although for the piano , scores are actual midi files selected from a specialized webpage hosting midi files freely available under creative commons licenses , for the _ marovany _ repertoire , scores were obtained with an original multi - sensor retrieval system @xcite .",
    "although quite invasive as needing a complex experimental set - up during recording sessions , such a system allows for fast and very reliable polyphonic transcriptions .    also , three other datasounds were derived from the dataset @xmath43 , each of which emphasizing a specific acoustic or musical feature @xmath44 controlled by a numerical parameter @xmath45 .",
    "we now detail these three other sound datasets :    @xmath46 : :    with the snr feature this acoustic feature is normalized on an unitary    range , and then directly modified by the factor    @xmath47 , which is itself controlled by the    parameter snr of the audio degradation toolbox @xcite ; @xmath48 : :    with the sparsity degree feature this musical feature refers to the    number of simultaneous and successive active notes in a 10-s time    interval .",
    "such a feature can then be controlled by the factor    @xmath49 in the construction of our synthetic    sequences , by simply forcing the number of notes to be lower than this    factor in 10-s intervals ; @xmath50 : :    with the intermodulation feature this feature is characterized by    strong modulations taking the form of peaks and valleys in the    temporal envelop .",
    "it depends both on the intrinsic timbre properties    of an instrument , and the playing mode of a musician .",
    "this feature    generally makes onset detection trickier , as the strong modulations    induced may result in fake transients with soft attacks .",
    "we quantified    the intermodulation strength @xmath51 of a given    note sample with the amplitude modulation acoustic descriptor ( see    @xcite ) , and selected a template during the generation of a musical    sequence accordingly to the desired @xmath51 value .",
    "this feature can be very prominent in repertoires of plucked string    instruments , and especially in the _ marovany _ repertoire .",
    "eventually , to put into perspective our results on the dataset @xmath43 with a more realistic dataset , we also used the sound dataset @xmath52 which gather the same musical pieces as in @xmath43 , although now they are extracted from real world recordings .",
    "our evaluation of the instrument recognition task processing will also use two different sound datasets , so as to extend our results to different scenarios of applications . a first one labelled @xmath53 using isolated note samples of 7 different plucked - string instruments and one piano , in order to perform note wise classification .",
    "our instrument sound samples were all extracted from the rwc database @xcite , as detailed in table [ table_detailsdatasound_scatt ] . during the frame - wise segmentation , windows consisting of silence signal were detected thanks to a heuristic approach based on power thresholding then discarded .",
    "note that there is a very important trade - off in endorsing this isolated - notes strategy : we gain simplicity and tractability , but we lose contextual and time - dependent cues that can be exploited as relevant features for classifying musical sounds in complex mixtures .",
    "so in a second sound dataset labelled @xmath54 , we compiled excerpts of real recordings available commercially .",
    "figures [ roccurves_onsetdetection_si ] and [ roccurves_onsetdetection_rw ] show roc curves for onset detection of the different tested algorithms , for the sound datasets detailed in section [ table_detailsdatasound_datasetformat ] .",
    "considering first the synthetic database , the clscs achieve results close to those obtained with state - of - the - art algorithms .",
    "numerically , for the synthesized sequences of dataset @xmath55 , we obtain the following scattering scores ( @xmath37=0.74 ; @xmath38=0.08 ; @xmath39= 0.27 for the piano and @xmath37=0.69 ; @xmath38=0.09 ; @xmath39= 0.32 for the _ marovany _ ) and halca scores ( @xmath37=0.76 ; @xmath38=0.15 ; @xmath39= 0.28 for the piano and @xmath37=0.68 ; @xmath38=0.1 ; @xmath39= 0.34 for the _ marovany _ ) .",
    "the lower optimal threshold obtained for scattering operators supports the fact that this representation flattens out the values around onset peaks , and allows for a more efficient selection of peaks representative of onsets .    for the real recording database @xmath56",
    ", we obtain the following scattering scores ( @xmath37=0.7 ; @xmath38=0.11 ; @xmath39= 0.32 for the piano and @xmath37=0.57 ; @xmath38=0.09 ; @xmath39= 0.44 for the _ marovany _ ) and halca scores ( @xmath37=0.68 ; @xmath38=0.15 ; @xmath39= 0.35 for the piano and @xmath37=0.63 ; @xmath38=0.16 ; @xmath39= 0.4 for the _ marovany _ ) .",
    "the larger percentage of spurious notes is likely a result of a weaker signal - to - noise ratio and the playing dynamics , as the system often misses quietly played notes , which are masked by other louder notes or chords occurring shortly before or after the missed onset . in the task of onset detection",
    ", scattering transform appears to deliver a better information to facilitate the rudimentary processing of peak picking .",
    "most contribution comes from the second order scattering coefficients , which has been shown @xcite to capture the high - frequency amplitude modulations that are note attacks .",
    "a more in - depth analysis is now necessary to really identify error sources .",
    "then , as a second step in our evaluation , we tested each onset detection algorithm , calibrated on their operating point threshold , to different acoustic perturbations , namely uncorrelated noise ( typically due to ambient noise and recording quality ) , correlated noise ( which can be identified to a sparsity degree in music ) and intermodulation .",
    "figures [ roccurves_onsetdetection_sensitivityall_piano]-[roccurves_onsetdetection_sensitivityall_maro ] show evolution of these performances against our three acoustical features .",
    "for all of these acoustic features of influence , scattering systematically ranks among the top performances .",
    "first , we can see that the scattering - based detection is very robust to noise , as musical features do not interfere with additive noise , which is flattened out across the different interference bands .",
    "concerning note sparsity in music , scattering achieves also good results , showing that it can better discriminate the energy respective to each note in a given analysis frame , and also detect a transient embedded in the decay phase of a note previously played . for what concerns envelop modulation , which exhibits `` valleys '' in the signal waveform that can be confounded with an attack transient , scattering appears to provide superior results than other methods .",
    "an explicative comment would be that scattering discriminates more efficiently the information on envelop filtering and transient interferences than other methods .",
    "intermodulation - related transients are smoothed out in the clscs representation , as a very strong high - frequency energy content must be present to reach the highest scattering level .",
    "it appears that onset detection of intermodulated notes can be done with a much more tolerant threshold descriptor than a parametric one evaluating the variations of the energy envelop .    as a more general comment",
    ", the particular strength of scattering towards these features may be explained by the more complex and rich band - decompositions operated on high frequencies .",
    "indeed , @xcite stated that , for onset detection , it is advantageous that the detection system divides the frequency range into fewer sub - bands as done by the human auditory system .",
    "band - wise filtering has then been applied by many authors for this task @xcite .",
    "table [ resultrecoinstru ] gives a global overview of our results on instrument recognition .",
    "tables [ confmat_isonotes ] and [ confmat_contiseq ] detail the confusion matrices of the scattering transform respective to the isolated complete note and the continuous sequence datasets . based on our results on instrument recognition , clscs were shown to achieve significantly higher accuracy than other classical representations ( e.g. mfccs or delta - mfccs ) .",
    "as explicative comments about the general better results obtained with the scattering transform , we put forward its ability to recover lost non - stationary signal structures and characterize them in a richer representation ( i.e. providing complementary co - occurrence information which refines mfcc descriptors ) , and open the possibility to capture more sophisticated auditory phenomena such as transients , amplitude and frequency modulations , time - varying filters and chord structure @xcite .",
    "in particular , numerous psychoacoustic studies have shown that the onset provides an important cue for timbre perception and thus musical instrument identification , particularly in the case of isolated tones @xcite . as we saw",
    "that note onsets are well captured in the second - order scattering coefficients , this may explain its higher performances then mfccs .",
    "when considering continuous excerpts and longer analysis segments , the error decreases as larger - scale musical information is encoded .",
    "indeed , continuous excerpts of music includes complementary musical features of the timbre , such as rhythm and playing techniques , from which scattering transform seem to benefit the most for the task of instrument recognition .",
    "such an observation has already been made in past studies @xcite . here",
    "is pointed out a major difficulty of audio representations for classification , i.e its multiplicity of information at different time scales : pitch and timbre at the scale of milliseconds , the rhythm of speech and music at the scale of seconds , and the music progression over minutes and hours . facing this difficulty , deep scattering has been precisely developed as a stable and invariant signal representation over time scales larger than 25 ms @xcite .",
    "in this paper , we tested the use of the deep scattering transform on the two tasks of onset detection and instrument recognition , which are of first important for automatic music transcription @xcite . taken altogether our results based on various simulation experiments , scattering operators seem to possess certain advantages in accomplishing these tasks , relatively to other classical methods .",
    "our two tasks of investigation benefited positively from the richer information ( due to recovered high - frequency information ) and the invariance property ( due to successive averaging of high - frequency information ) in the scattering transform .",
    "this tendency confirms the potential of scattering representations , announced theoretically by @xcite , and already well validated by other studies in different classification / retrieval tasks ( see introduction ) .",
    "eventually , larger database could be used to confirm these promising performances .",
    "also , one noticeable drawback of the method is its high time - consuming algorithm , which would need more efficient implementation for further applications .      at the risk of omitting some relevant names",
    ", the authors would like to especially thank march chemillier ( cams - ehess ) for recordings of the _ marovany _ and laurent quartier ( lam - upmc ) for technical supports .",
    "bauge , c. , lagrange , m. , anden , j. , and mallat , s. ( * 2013 * ) .",
    "`` representing environmental sounds using the separable scattering transform . '' in _ ieee international conference on acoustics , speech and signal processing_. pp",
    ". 86678671 .    bay , m. , ehmann , a.f . , and downie , j.s .",
    "( * 2009 * )",
    ". `` evaluation of multiple-@xmath57 estimation and tracking systems . '' in _",
    "10th international society of music information retrieval conference , kobe , japan_. pp .",
    "315320 .",
    "benetos , e. , cherla , s. , and weyde , t. ( * 2013 * ) . `` an efficient shift - invariant model for polyphonic music transcription . '' in _ 6th int .",
    "workshop on machine learning and music , prague , czech republic_.              chang , k.y . ,",
    "lin , c.f . , chen , c.s . , and hung , y.p .",
    "( * 2012 * ) . `` applying scattering operators for face recognition : a comparative study . '' in _ 21st international conference on pattern recognition_. pp .",
    "29852988 .",
    "chen , x. and ramadge , p. ( * 2013",
    ". `` music genre classification using multiscale scattering and sparse representations . '' in _",
    "47th annual conference on information sciences and systems ( ciss)_. pp .",
    "chudacek , v. , anden , j. , mallat , s. , abry , p. , and doret , m. ( * 2014 * ) .",
    "`` scattering transform for intrapartum fetal heart rate variability fractal analysis : a case - control study . ''",
    "ieee transactions on biomedical engineering , * 61 * , 11001108 .",
    "duxbury , c. , bello , j.p . , davies , m. , and sandler , m.b .",
    "( * 2003 * ) .",
    "`` complex domain onset detection for musical signals . '' in _ proceedings of the 6th international conference on digital audio effects ( dafx)_.    eggink , j. and brown , g.j .",
    "( * 2004 * ) . `` instrument recognition in accompanied sonatas and concertos . '' in _ ieee international conference on acoustics , speech , and signal processing_. vol .  4 , pp .",
    "217220 .",
    "essid , s. , leveau , p. , richard , g. , daudet , l. , and david , b. ( * 2005 * ) .",
    "`` on the usefulness of differentiated transient / steady - state processing in machine recognition of musical instruments . '' in _",
    "aes 118th convention , barcelona , spain_.    fonseca , n. and ferreira , a. ( * 2009 * ) .",
    "`` measuring music transcription results based on a hybrid decay / sustain evaluation . '' in _",
    "7th triennial conference of european society for the cognitive sciences of music , jyvaskyla , finland_. pp .",
    "119124 .",
    "fuentes , b. , badeau , r. , and richard , g. ( * 2012 * ) . `` blind harmonic adaptive decomposition applied to supervised source separation . '' in _",
    "20th european signal processing conference , bucharest , romania_. pp . 26542658 .",
    "goto , m. , hashiguchi , h. , nishimura , t. , and oka , r. ( * 2003 * ) .",
    "`` rwc music database : popular , classical , and jazz music databases . '' in _ 3rd international conference on music information retrieval , baltimore , md . _",
    ". 287288 .",
    "kitahara , t. , goto , m. , komatani , k. , ogata , t. , and okuno , h. ( * 2006 * ) .",
    "`` instrogram : a new musical instrument recognition technique without using onset detection nor f0 estimation . '' in _ ieee int . conf .",
    "audio , speech , signal process .",
    "_ vol .  5 , pp . 229232 .",
    "mauch , m. and ewert , s. ( * 2013 * ) . `` the audio degradation toolbox and its application to robustness evaluation . '' in _ 14th international society for music information retrieval conference , curitiba , pr , brazil_. pp . 8388 .",
    "mcadams , s. and bigand , e. , eds .",
    "( * 1993 * ) . _",
    "thinking in sound : the cognitive psychology of human audition _ ( oxford university press , oxford , uk ) , chap .",
    "recognition of auditory sound sources and events , pp .",
    "146195 .",
    "tzanetakis , g. , essl , g. , and cook , p. ( * 2001",
    "`` audio analysis using the discrete wavelet transform . '' in _ proc .",
    "acoustics and music : theory 2001 and applications ( amta 2001 ) skiathos , greece_."
  ],
  "abstract_text": [
    "<S> automatic music transcription ( amt ) is one of the oldest and most well - studied problems in the field of music information retrieval . within this challenging research field , </S>",
    "<S> onset detection and instrument recognition take important places in transcription systems , as they respectively help to determine exact onset times of notes and to recognize the corresponding instrument sources . </S>",
    "<S> the aim of this study is to explore the usefulness of multiscale scattering operators for these two tasks on plucked string instrument and piano music . after resuming the theoretical background and illustrating the key features of this sound representation method </S>",
    "<S> , we evaluate its performances comparatively to other classical sound representations . using both midi - driven datasets with real instrument samples and real musical pieces , </S>",
    "<S> scattering is proved to outperform other sound representations for these amt subtasks , putting forward its richer sound representation and invariance properties .    * </S>",
    "<S> deep scattering transform applied to note onset detection and instrument recognition *    cazau dorian@xmath0 , guillaume revillon@xmath0 and olivier adam@xmath0 +     + </S>"
  ]
}