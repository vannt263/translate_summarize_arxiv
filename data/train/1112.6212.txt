{
  "article_text": [
    "adaptive network consists of a collection of agents that are interconnected to each other and solve distributed estimation and inference problems in a collaborative manner .",
    "two useful strategies that enable adaptation and learning over such networks in real - time are the incremental strategy @xcite and the diffusion strategy @xcite .",
    "incremental strategies rely on the use of a hamiltonian cycle , i.e. , a cyclic path that covers all nodes in the network , which is generally difficult to enforce since determining a hamiltonian cycle is an np - hard problem .",
    "in addition , cyclic trajectories are not robust to node or link failure . in comparison ,",
    "diffusion strategies are scalable , robust , and able to match well the performance of incremental networks . in adaptive diffusion implementations",
    ", information is processed locally at the nodes and then diffused in real - time across the network .",
    "diffusion strategies were originally proposed in @xcite and further extended and studied in @xcite .",
    "they have been applied to model self - organized and complex behavior encountered in biological networks , such as fish schooling @xcite , bird flight formations @xcite , and bee swarming @xcite .",
    "diffusion strategies have also been applied to online learning of gaussian mixture models @xcite and to general distributed optimization problems @xcite .",
    "there have also been several useful works in the literature on distributed consensus - type strategies , with application to multi - agent formations and distributed processing @xcite .",
    "the main difference between these works and the diffusion approach of @xcite is the latter s emphasis on the role of adaptation and learning over networks .    in the original diffusion least - mean - squares ( lms ) strategy @xcite , the weight estimates that are exchanged among the nodes can be subject to quantization errors and additive noise over the communication links . studying the degradation in mean - square performance that results from these particular perturbations",
    "can be pursued , for both incremental and diffusion strategies , by extending the mean - square analysis already presented in @xcite , in the same manner that the tracking analysis of conventional stand - alone adaptive filters was obtained from the counterpart results in the stationary case ( as explained in ( * ? ? ?",
    "* ch . 21 ) ) .",
    "useful results along these lines , which study the effect of link noise during the exchange of the weight estimates , already appear for the traditional diffusion algorithm in the works @xcite and for consensus - based algorithms in @xcite . in this paper ,",
    "our objective is to go beyond these earlier studies by taking into account additional effects , and by considering a more general algorithmic structure .",
    "the reason for this level of generality is because the analytical results will help reveal which noise sources influence the network performance more seriously , in what manner , and at what stage of the adaptation process .",
    "the results will suggest important remedies and mechanisms to adapt the combination weights in real - time .",
    "some of these insights are hard to get if one focuses solely on noise during the exchange of the weight estimates .",
    "the analysis will further show that noise during the exchange of the regression data plays a more critical role than other sources of imperfection : this particular noise alters the learning dynamics and modes of the network , and biases the weight estimates .",
    "noises related to the exchange of other pieces of information do not alter the dynamics of the network but contribute to the deterioration of the network performance .",
    "to arrive at these results , in this paper , we first consider a generalized analysis that applies to a broad class of diffusion adaptation strategies ( see  further ahead ; this class includes the original diffusion strategies and as two special cases ) .",
    "the analysis allows us to account for various sources of information noise over the communication links .",
    "we allow for noisy exchanges during _ each _ of the three processing steps of the adaptive diffusion algorithm ( the two combination steps and and the adaptation step ) . in this way , we are able to examine how the three sets of combination coefficients @xmath0 in  influence the propagation of the noise signals through the network dynamics .",
    "our results further reveal how the network mean - square - error performance is dependent on these combination weights . following this line of reasoning",
    ", the analysis leads to algorithms and further ahead for choosing the combination coefficients to improve the steady - state network performance .",
    "it should be noted that several combination rules , such as the metropolis rule @xcite and the maximum degree rule @xcite , were proposed previously in the literature  especially in the context of consensus - based iterations @xcite .",
    "these schemes , however , usually suffer performance degradation in the presence of noisy information exchange since they ignore the network noise profile @xcite .",
    "when the noise variance differs across the nodes , it becomes necessary to design combination rules that are aware of this variation as outlined further ahead in section vi - b .",
    "moreover , in a mobile network @xcite where nodes are on the move and where neighborhoods evolve over time , it is even more critical to employ adaptive combination strategies that are able to track the variations in the noise profile in order to cope with such dynamic environments .",
    "this issue is taken up in section vi - c .",
    "we use lowercase letters to denote vectors , uppercase letters for matrices , plain letters for deterministic variables , and boldface letters for random variables .",
    "we also use @xmath1 to denote conjugate transposition , @xmath2 for the trace of its matrix argument , @xmath3 for the spectral radius of its matrix argument , @xmath4 for the kronecker product , and @xmath5 for a vector formed by stacking the columns of its matrix argument .",
    "we further use @xmath6 to denote a ( block ) diagonal matrix formed from its arguments , and @xmath7 to denote a column vector formed by stacking its arguments on top of each other .",
    "all vectors in our treatment are column vectors , with the exception of the regression vectors , @xmath8 , and the associated noise signals , @xmath9 , which are taken to be row vectors for convenience of presentation .",
    "we consider a connected network consisting of @xmath10 nodes .",
    "each node @xmath11 collects scalar measurements @xmath12 and @xmath13 regression data vectors @xmath8 over successive time instants @xmath14 . note that we use parenthesis to refer to the time - dependence of scalar variables , as in @xmath12 , and subscripts to refer to the time - dependence of vector variables , as in @xmath8 .",
    "the measurements across all nodes are assumed to be related to an unknown @xmath15 vector @xmath16 via a linear regression model of the form @xcite : @xmath17 where @xmath18 denotes the measurement or model noise with zero mean and variance @xmath19 .",
    "the vector @xmath16 in denotes the parameter of interest , such as the parameters of some underlying physical phenomenon , the taps of a communication channel , or the location of food sources or predators .",
    "such data models are also useful in studies on hybrid combinations of adaptive filters @xcite .",
    "the nodes in the network would like to estimate @xmath16 by solving the following minimization problem : @xmath20 in previous works @xcite , we introduced and studied several distributed strategies of the diffusion type that allow nodes to cooperate with each other in order to solve problems of the form in an adaptive manner .",
    "these diffusion strategies endow networks with adaptation and learning abilities , and enable information to diffuse through the network in real - time .",
    "we review the adaptive diffusion strategies below .      in @xcite ,",
    "two classes of diffusion algorithms were proposed .",
    "one class is the so - called combine - then - adapt ( cta ) strategy : @xmath21\\\\ \\end{aligned}\\right.\\end{aligned}\\ ] ] and the second class is the so - called adapt - then - combine ( atc ) strategy : @xmath22\\\\ { \\bs{w}}_{k , i}&=\\sum_{l\\in{\\mc{n}}_k}a_{2,lk}{\\bs{\\psi}}_{l , i}\\\\ \\end{aligned}\\right.\\end{aligned}\\ ] ] where the @xmath23 are small positive step - size parameters and the @xmath0 are nonnegative entries of the @xmath24 matrices @xmath25 , respectively .",
    "the coefficients @xmath0 are zero whenever node @xmath26 is not connected to node @xmath11 , i.e. , @xmath27 , where @xmath28 denotes the neighborhood of node @xmath11 .",
    "the two strategies and can be integrated into one broad class of diffusion adaptation @xcite : @xmath29\\\\ \\label{eqn : idealdiffusionpostdiff } { \\bs{w}}_{k , i}&=\\sum_{l\\in{\\mc{n}}_k}a_{2,lk}{\\bs{\\psi}}_{l , i}\\end{aligned}\\ ] ] several diffusion strategies can be obtained as special cases of  through proper selection of the coefficients @xmath0 .",
    "for example , to recover the cta strategy , we set @xmath30 , and to recover the atc strategy , we set @xmath31 , where @xmath32 denotes the @xmath24 identity matrix . in the general diffusion strategy  , each node @xmath11 evaluates its estimate @xmath33 at time @xmath34 by relying solely on the data collected from its neighbors through steps and and on its local measurements through step .",
    "the matrices @xmath35 , @xmath36 , and @xmath37 are required to be left or right - stochastic , i.e. , @xmath38 where @xmath39 denotes the @xmath40 vector whose entries are all one .",
    "this means that each node performs a convex combination of the estimates received from its neighbors at every iteration @xmath34 .    the mean - square performance and convergence properties of the diffusion algorithm ",
    "have already been studied in detail in @xcite . for the benefit of the analysis in the subsequent sections , we present below in the recursion describing the evolution of the weight error vectors across the network",
    "to do so , we introduce the error vectors : @xmath41 and substitute the linear model into the adaptation step to find that @xmath42 where the @xmath43 matrix @xmath44 and the @xmath15 vector @xmath45 are defined as : @xmath46 we further collect the various quantities across all nodes in the network into the following block vectors and matrices : @xmath47 then , from , , and , the recursion for the network error vector @xmath48 is given by @xmath49 where @xmath50      each of the steps in  involves the sharing of information between node @xmath11 and its neighbors .",
    "for example , in the first step , all neighbors of node @xmath11 send their estimates @xmath51 to node @xmath11 .",
    "this transmission is generally subject to additive noise and possibly quantization errors .",
    "likewise , steps and involve the sharing of other pieces of information with node @xmath11 .",
    "these exchange steps can all be subject to perturbations ( such as additive noise and quantization errors )",
    ". one of the objectives of this work is to analyze the _ aggregate _ effect of these perturbations on general diffusion strategies of the type  and to propose choices for the combination weights in order to enhance the mean - square performance of the network in the presence of these disturbances .     to node @xmath11 . ]",
    "so let us examine what happens when information is exchanged over links with additive noise .",
    "we model the data received by node @xmath11 from its neighbor @xmath26 as @xmath52 where @xmath53 and @xmath54 are @xmath15 noise signals , @xmath9 is a @xmath13 noise signal , and @xmath55 is a scalar noise signal ( see fig . [",
    "fig : noise ] ) . observe further that in  , we are including several sources of information exchange noise . in comparison ,",
    "references @xcite only considered the noise source @xmath53 in and one set of combination coefficients @xmath56 ; the other coefficients were set to @xmath57 for @xmath58 and @xmath59 . in other words , these references only considered and the following traditional cta strategy without exchange of the data @xmath60  compare with ; note that the second step in only uses @xmath61 : @xmath62\\\\ \\end{aligned}\\right.\\end{aligned}\\ ] ] the analysis that follows examines the aggregate effect of all four noise sources appearing in  , in addition to the three sets of combination coefficients appearing in . we introduce the following assumption on the statistical properties of the measurement data and noise signals .",
    "[ asm : all ]    1 .",
    "the regression data @xmath8 are temporally white and spatially independent random variables with zero mean and covariance matrix @xmath63 .",
    "2 .   the noise signals @xmath64 , @xmath53 , @xmath55 , @xmath9 , and @xmath54 are temporally white and spatially independent random variables with zero mean and ( co)variances @xmath19 , @xmath65 , @xmath66 , @xmath67 , and @xmath68 , respectively .",
    "in addition , @xmath65 , @xmath66 , @xmath67 , and @xmath68 are all zero if @xmath27 or @xmath69 .",
    "3 .   the regression data @xmath70 , the model noise signals @xmath71 , and the link noise signals @xmath72 , @xmath73 , @xmath74 , and @xmath75 are mutually - independent random variables for all @xmath76 and @xmath77 .",
    "using the perturbed data  , the diffusion algorithm  becomes @xmath78\\\\ \\label{eqn : noisydiffusionpostdiffold } { \\bs{w}}_{k , i}&\\!=\\!\\sum_{l\\in{\\mc{n}}_k}a_{2,lk}{\\bs{\\psi}}_{lk , i}\\end{aligned}\\ ] ] where we continue to use the symbols @xmath79 to avoid an explosion of notation . from and ,",
    "expressions  can be rewritten as @xmath80\\\\ \\label{eqn : noisydiffusionpostdiff } { \\bs{w}}_{k , i}&\\!=\\!\\sum_{l\\in{\\mc{n}}_k}a_{2,lk}{\\bs{\\psi}}_{l , i}\\!+\\!{\\bs{v}}_{k , i}^{(\\psi)}\\end{aligned}\\ ] ] where we are introducing the symbols @xmath81 and @xmath82 to denote the aggregate @xmath15 zero - mean noise signals defined over the neighborhood of node @xmath11 : @xmath83 with covariance matrices @xmath84 it is worth noting that @xmath85 and @xmath86 depend on the combination coefficients @xmath56 and @xmath87 , respectively .",
    "this property will be taken into account when optimizing over @xmath56 and @xmath87 in a later section .",
    "we further introduce the following scalar zero - mean noise signal : @xmath88 for @xmath89 , whose variance is @xmath90 to unify the notation , we define @xmath91 .",
    "then , from , , and , it is easy to verify that the noisy data @xmath92 are related via @xmath93 for @xmath94 .",
    "continuing with the adaptation step and substituting , we get @xmath95\\end{aligned}\\ ] ] then , we can derive the following error recursion for node @xmath11 ( compare with ):",
    "@xmath96 where the @xmath43 matrix @xmath97 and the @xmath98 vector @xmath99 are defined as ( compare with and ): @xmath100 we further introduce the block vectors and matrices : @xmath101 and the corresponding covariance matrices for @xmath102 and @xmath103 : @xmath104 then , from , , and , we arrive at the following recursion for the network weight error vector in the presence of noisy information exchange : @xmath105\\!-\\!{\\bs{v}}_i^{(\\psi)}\\nonumber\\\\ { } & = { \\mc{a}}_2^{\\t}\\left[(i_{nm}\\!-\\!{\\mc{m}}{\\bs{\\mc{r}}}_i')({\\mc{a}}_1^{\\t}{\\wt{\\bs{w}}}_{i-1}\\!-\\!{\\bs{v}}_{i-1}^{(w ) } ) \\!-\\!{\\mc{m}}{\\bs{z}}_i\\right]\\!-\\!{\\bs{v}}_i^{(\\psi)}\\end{aligned}\\ ] ] that is , @xmath106 compared to the previous error recursion , the noise terms in consist of three parts :    * @xmath107 is contributed by the noise introduced at the information - exchange step _ before _ adaptation .",
    "* @xmath108 is contributed by the noise introduced at the adaptation step .",
    "* @xmath103 is contributed by the noise introduced at the information - exchange step _ after _ adaptation .",
    "given the weight error recursion , we are now ready to study the mean convergence condition for the diffusion strategy  in the presence of disturbances during information exchange under assumption [ asm : all ] . taking expectations of both sides of ,",
    "we get @xmath109 where @xmath110 from , , , and , it can be verified that @xmath111 whereas , from and assumption [ asm : all ] , we get @xmath112\\nonumber\\\\ { } & = -\\left(\\sum_{l\\in{\\mc{n}}_k}c_{lk}r_{v , lk}^{(u)}\\right)w^o\\end{aligned}\\ ] ] let us define an @xmath113 matrix @xmath114 that collects all covariance matrices @xmath115 , @xmath116 , weighted by the corresponding combination coefficients @xmath117 , such that its @xmath118th @xmath43 submatrix is @xmath119 . note that @xmath114 itself is _ not _ a covariance matrix because @xmath120 for all @xmath11 .",
    "then , from and , we arrive at @xmath121 therefore , using and , expression becomes @xmath122 with a driving term due to the presence of @xmath123 .",
    "this driving term would disappear from if there were no noise during the exchange of the regression data . to guarantee convergence of , the coefficient matrix @xmath124 must be stable , i.e. , @xmath125 . since @xmath126 and @xmath127 are right - stochastic matrices , it can be shown that the matrix @xmath124 is stable whenever @xmath128 itself is stable ( see appendix [ app : meanconvergence ] ) .",
    "this fact leads to an upper bound on the step - sizes @xmath23 to guarantee the convergence of @xmath129 to a steady - state value , namely , we must have @xmath130 for @xmath131 , where @xmath132 denotes the largest eigenvalue of its matrix argument .",
    "note that the neighborhood covariance matrix @xmath133 in is related to the combination weights @xmath117 .",
    "if we further assume that @xmath37 is doubly - stochastic , i.e. , @xmath134 then , by jensen s inequality @xcite , @xmath135 since ( i ) @xmath132 coincides with the induced @xmath136-norm for any positive semi - definite hermitian matrix ; ( ii ) matrix norms are convex functions of their arguments @xcite ; and ( iii ) by , @xmath117 are convex combination coefficients .",
    "thus , we obtain a sufficient condition for the convergence of in lieu of : @xmath137 } } \\ ] ] for @xmath131 , where the upper bound for the step - size @xmath138 becomes independent of the combination weights @xmath117 .",
    "this bound can be determined solely from knowledge of the covariances of the regression data and the associated noise signals that are accessible to node @xmath11 .",
    "it is worth noting that for traditional diffusion algorithms where information is perfectly exchanged , condition reduces to @xmath139}\\end{aligned}\\ ] ] for @xmath131 . comparing with",
    ", we see that the link noise @xmath9 over regression data reduces the dynamic range of the step - sizes for mean stability . now ,",
    "under , and taking the limit of as @xmath140 , we find that the mean error vector will converge to a steady - state value @xmath141 : @xmath142",
    "it is well - known that studying the mean - square convergence of a single adaptive filter is a challenging task , since adaptive filters are nonlinear , time - variant , and stochastic systems .",
    "when a network of adaptive nodes is considered , the complexity of the analysis is compounded because the nodes now influence each other s behavior . in order to make the performance analysis more tractable , we rely on the energy conservation approach @xcite , which was used successfully in @xcite to study the mean - square performance of diffusion strategies under perfect information exchange conditions .",
    "that argument allows us to derive expressions for the mean - square - deviation ( msd ) and the excess - mean - square - error ( emse ) of the network by analyzing how energy ( measured in terms of error variances ) flows through the nodes .    from recursion and under assumption",
    "[ asm : all ] , we can obtain the following weighted variance relation for the global error vector @xmath48 : @xmath143\\}\\\\ { } & \\quad+\\e\\|{\\mc{a}}_2^{\\t}(i_{nm}\\!-\\!{\\mc{m}}{\\bs{\\mc{r}}}_i'){\\bs{v}}_{i-1}^{(w)}\\|_{\\sigma}^2+\\e\\|{\\bs{v}}_i^{(\\psi)}\\|_{\\sigma}^2 \\end{aligned } } \\ ] ] where @xmath144 is an arbitrary @xmath113 positive semi - definite hermitian matrix that we are free to choose .",
    "moreover , the notation @xmath145 stands for the quadratic term @xmath146 .",
    "the weighting matrix @xmath147 in can be expressed as @xmath148 where @xmath124 is given by and @xmath149 denotes a term on the order of @xmath150 . evaluating the term @xmath149 requires knowledge of higher - order statistics of the regression data and link noises , which are not available under current assumptions .",
    "however , this term becomes negligible if we introduce a small step - size assumption .",
    "[ asm : smallstepsize ] the step - sizes are sufficiently small , i.e. , @xmath151 , such that terms depending on higher - order powers of the step - sizes can be ignored .",
    "hence , in the sequel we use the approximation : @xmath152 observe that on the right - hand side ( rhs ) of relation , only the first and third terms relate to the error vector @xmath153 . by assumption",
    "[ asm : all ] , the error vector @xmath153 is independent of @xmath154 and @xmath155 .",
    "thus , from , the third term on rhs of can be expressed as @xmath156\\cdot\\e{\\wt{\\bs{w}}}_{i-1}\\}\\nonumber\\\\ { } & = -2\\,{\\mathfrak{re}}(z^*{\\mc{m}}{\\mc{a}}_2\\sigma{\\mc{a}}_2^{\\t}{\\mc{a}}_1^{\\t}\\cdot\\e{\\wt{\\bs{w}}}_{i-1})+o({\\mc{m}}^2)\\end{aligned}\\ ] ] since we already showed in the previous section that @xmath157 converges to a fixed bias @xmath141 , quantity will converge to a fixed value as well when @xmath140 . moreover , under assumption [ asm : all ] , the second , fourth , and fifth terms on rhs of relation are all fixed values .",
    "therefore , the convergence of relation depends on the behavior of the first term @xmath158 .",
    "although the weighting matrix @xmath147 of @xmath153 is different from the weighting matrix @xmath144 of @xmath159 , it turns out that the entries of these two matrices are approximately related by a linear equation shown ahead in .",
    "introduce the vector notation @xcite : @xmath160 then , by using the identity @xmath161 , it can be verified from that @xmath162 where the @xmath163 matrix @xmath164 is given by @xmath165 to guarantee mean - square convergence of the algorithm , the step - sizes should be sufficiently small and selected to ensure that the matrix @xmath164 is stable @xcite , i.e. , @xmath166 , which is equivalent to the earlier condition @xmath125 . although more specific conditions for mean - square stability can be determined without assumption [ asm : smallstepsize ] @xcite , it is sufficient for our purposes here to conclude that the diffusion strategy  is stable in the mean and mean - square senses if the step - sizes @xmath23 satisfy or and are sufficiently small .",
    "the conclusion so far is that sufficiently small step - sizes ensure convergence of the diffusion strategy  in the mean and mean - square senses , even in the presence of exchange noises over the communication links .",
    "let us now determine expressions for the error variances in steady - state .",
    "we start from the weighted variance relation . in view of",
    ", it shows that the error variance @xmath167 depends on the mean error @xmath129 .",
    "we already determined the value of @xmath168 in .",
    "we continue to use the vector notation and proceed to evaluate all the terms , except the first one , on rhs of in the following . for the _ second _ term",
    ", it can be expressed as @xmath169^*\\sigma\\end{aligned}\\ ] ] where we used the identity @xmath170^*\\sigma$ ] for any hermitian matrix @xmath171 , and @xmath172 denotes the autocorrelation matrix of @xmath173 .",
    "it is shown in appendix [ app : rz ] that @xmath172 is given by @xmath174 where @xmath175 is defined in , @xmath123 is in , and @xmath176 are two @xmath113 positive semi - definite block diagonal matrices : @xmath177\\end{aligned}\\ ] ] from expression and assumption [ asm : smallstepsize ] , the _ third _ term on rhs of is given by @xmath178\\sigma\\}\\nonumber\\\\ { } & = -\\!\\left[\\vec\\left({\\mc{a}}_2^{\\t}{\\mc{a}}_1^{\\t}(\\e{\\wt{\\bs{w}}}_{i-1})z^*{\\mc{m}}{\\mc{a}}_2 + { \\mc{a}}_2^{\\t}{\\mc{m}}z(\\e{\\wt{\\bs{w}}}_{i-1})^*{\\mc{a}}_1{\\mc{a}}_2\\right)\\right]^*\\sigma\\end{aligned}\\ ] ] likewise , the _ fourth _ term on rhs of is approximated by @xmath179\\right)\\right]^*\\sigma\\nonumber\\\\ { } & \\approx\\left[\\vec({\\mc{a}}_2^{\\t}{\\mc{r}}_v^{(w)}{\\mc{a}}_2)\\right]^*\\sigma\\end{aligned}\\ ] ] where we are now ignoring terms on the order of @xmath180 and @xmath150 .",
    "the _ fifth _",
    "term on rhs of is given by @xmath181^*\\sigma\\end{aligned}\\ ] ] let us introduce @xmath182 at steady - state , as @xmath140 , by and  , the weighted variance relation becomes @xmath183^*\\sigma\\end{aligned}\\ ] ] where we are using the compact notation @xmath184 to refer to @xmath185  doing so allows us to represent @xmath147 by the more compact relation @xmath186 on rhs of ; we shall be using the weighting matrix @xmath144 and its vector representation @xmath187 interchangeably for ease of notation ( likewise , for @xmath147 and @xmath188 ) .",
    "the steady - state weighted variance relation can be rewritten as @xmath189^*\\sigma\\end{aligned}\\ ] ] where the term @xmath190 is contributed by the model noise @xmath191 while the remaining terms @xmath192 are contributed by the link noises @xmath193 .",
    "recall that we are free to choose @xmath144 and , hence , @xmath187 .",
    "let @xmath194 , where @xmath195 is another arbitrary positive semi - definite hermitian matrix .",
    "then , we arrive at the following theorem .",
    "[ lemma : steadystatevariancerelation ] under assumptions [ asm : all ] and [ asm : smallstepsize ] , for any positive semi - definite hermitian matrix @xmath195 , the steady - state weighted error variance relation of the diffusion strategy  is approximately given by @xmath196^*(i_{n^2m^2}-{\\mc{f}})^{-1}\\vec(\\omega ) \\end{aligned } } \\ ] ] where @xmath197 is given in , @xmath198 in , @xmath199 in , and @xmath164 in .",
    "each subvector of @xmath200 corresponds to the estimation error at a particular node , say , @xmath201 for node @xmath11 .",
    "the network msd is defined as @xcite : @xmath202 since we are free to choose @xmath195 , we select it as @xmath203 . then , expression gives @xmath204^*(i_{n^2m^2}-{\\mc{f}})^{-1}\\vec(i_{nm } )",
    "\\end{aligned } } \\ ] ] similarly , if we instead select @xmath205 , where @xmath206 then expression would allow us to evaluate the network emse as : @xmath207^*(i_{n^2m^2}-{\\mc{f}})^{-1}\\vec({\\mc{r}}_u ) \\end{aligned } } \\ ] ] where , under assumption [ asm : all ] , the network emse is given by @xmath208      we showed in the earlier sections that the link noise over regression data biases the weight estimators . in this section",
    "we examine how the results simplify when there is no sharing of regression data among the nodes .",
    "[ asm : nodatasharing ] nodes do not share regression data within neighborhoods , i.e. , assume @xmath209 .    by assumptions [ asm : smallstepsize ] and [ asm : nodatasharing ] , matrices @xmath210 in , , and become @xmath211 where @xmath212 is given in .",
    "then , the network msd and emse expressions and simplify to : @xmath213^ * ( i_{n^2m^2}-{\\mc{f}})^{-1}\\vec(i_{nm } ) \\end{aligned } } \\ ] ] and @xmath214^ * ( i_{n^2m^2}-{\\mc{f}})^{-1}\\vec({\\mc{r}}_u ) \\end{aligned } } \\ ] ]      recalling that @xmath198 and @xmath164 are related to the combination matrices @xmath215 , or , equivalently , @xmath216 , results and express the network msd and emse in terms of @xmath216 .",
    "however , it is generally difficult to use these expressions to optimize over @xmath216 to reduce the impact of link noise . instead , by substituting into and using the fact that @xmath164 is stable",
    ", we can arrive at another useful expression for the network msd : @xmath217^ * \\sum_{j=0}^{\\infty}{\\mc{f}}^j\\vec(i_{nm})\\nonumber\\\\ { } & = \\frac{1}{n}\\!\\left[\\vec({\\mc{a}}_2^{\\t}{\\mc{m}}{\\mc{s}}{\\mc{m}}{\\mc{a}}_2\\!+\\!{\\mc{r}}_v)\\right]^ * \\sum_{j=0}^{\\infty}({\\mc{b}}^{\\t}\\otimes{\\mc{b}}^*)^j\\vec(i_{nm})\\nonumber\\\\ { } & = \\frac{1}{n}\\!\\left[\\vec({\\mc{a}}_2^{\\t}{\\mc{m}}{\\mc{s}}{\\mc{m}}{\\mc{a}}_2\\!+\\!{\\mc{r}}_v)\\right]^ * \\sum_{j=0}^{\\infty}\\vec({\\mc{b}}^{*j}{\\mc{b}}^j)\\end{aligned}\\ ] ] that is , @xmath218 } \\ ] ] where @xmath124 is given in .",
    "similarly , the network emse can be expressed as @xmath219 } \\ ] ] expressions and reveal in an interesting way how the noise sources originating from any particular node end up influencing the overall network performance .",
    "let us denote @xmath220 the error recursion can be rewritten as @xmath221 where @xmath222 then , @xmath223 under assumption [ asm : nodatasharing ] , @xmath224 in and can be simplified as @xmath225 where @xmath226 are given in and . by assumption [ asm : all ] , @xmath224 are temporally independent for different @xmath34 and @xmath227 where @xmath124 is given by . as @xmath140 ,",
    "the first term on rhs of becomes @xmath228\\right\\}\\nonumber\\\\ { } & \\stackrel{(a)}{\\approx}\\lim_{i\\rightarrow\\infty}\\tr\\left[\\left(\\e{\\bm{\\phi}}_{0,i}\\right ) ( \\e{\\wt{\\bs{w}}}_{-1}{\\wt{\\bs{w}}}_{-1}^*)\\left(\\e{\\bm{\\phi}}_{0,i}\\right)^*\\right]\\nonumber\\\\ { } & = \\lim_{i\\rightarrow\\infty}\\tr\\left[{\\mc{b}}^{i+1}(\\e\\,{\\wt{\\bs{w}}}_{-1}{\\wt{\\bs{w}}}_{-1}^*){\\mc{b}}^{(i+1)*}\\right]\\nonumber\\\\ { } & \\stackrel{(b)}{=}0\\end{aligned}\\ ] ] where ( a ) is obtained by approximating the expectation of the product by the product of expectations and ( b ) is due to the stability of @xmath124 .",
    "therefore , the steady - state value of gives @xmath229\\nonumber\\\\ { } & \\stackrel{(a)}{\\approx}\\lim_{i\\rightarrow\\infty}\\sum_{m=0}^{i}\\tr\\left[{\\mc{b}}^{i - m}({\\mc{a}}_2^{\\t}{\\mc{m}}{\\mc{s } } { \\mc{m}}{\\mc{a}}_2+{\\mc{r}}_v){\\mc{b}}^{(i - m)*}\\right]\\nonumber\\\\ { } & \\stackrel{(b)}{=}\\lim_{i\\rightarrow\\infty}\\sum_{j=0}^{i}\\tr\\left[{\\mc{b}}^{j}({\\mc{a}}_2^{\\t}{\\mc{m}}{\\mc{s } } { \\mc{m}}{\\mc{a}}_2+{\\mc{r}}_v){\\mc{b}}^{j*}\\right]\\nonumber\\\\ { } & = \\sum_{j=0}^{\\infty}\\tr\\left[{\\mc{b}}^{j}({\\mc{a}}_2^{\\t}{\\mc{m}}{\\mc{s}}{\\mc{m}}{\\mc{a}}_2+{\\mc{r}}_v){\\mc{b}}^{*j}\\right]\\end{aligned}\\ ] ] where , by and , ( a ) is due to @xmath230 and ( b ) is simply a change of variable : @xmath231 . since the @xmath232th term of the summation in or is contributed by the term @xmath233 , which consists of all the noise sources at time @xmath234 , expression shows how various sources of noises are involved and how they contribute to the network msd .",
    "before we optimize the combination matrices @xmath216 , we first specialize the msd expression and the emse expression for the atc and cta algorithms . for the atc algorithm , we set @xmath31 and @xmath235 , and for the cta algorithm , we set @xmath236 and @xmath30 .",
    "let us denote @xmath237 then , we get @xmath238\\!\\!\\\\ \\label{eqn : noisyatcemse } \\!\\!{\\textrm{emse}}_{\\textrm{atc}}&\\!\\approx\\!\\frac{1}{n}\\!\\sum_{j=0}^{\\infty}{\\mathrm{tr}}\\!\\left[{\\mc{b}}_{\\textrm{atc}}^j ( { \\mc{a}}^{\\t}{\\mc{m}}{\\mc{s}}{\\mc{m}}{\\mc{a}}\\!+\\!{\\mc{r}}_v^{(\\psi ) } ) { \\mc{b}}_{\\textrm{atc}}^{*j}{\\mc{r}}_u\\right]\\!\\!\\end{aligned}\\ ] ] and @xmath239\\\\ \\label{eqn : noisyctaemse } { \\textrm{emse}}_{\\textrm{cta}}&\\approx\\frac{1}{n}\\sum_{j=0}^{\\infty}{\\mathrm{tr}}\\left[{\\mc{b}}_{\\textrm{cta}}^j ( { \\mc{m}}{\\mc{s}}{\\mc{m}}+{\\mc{r}}_v^{(w)}){\\mc{b}}_{\\textrm{cta}}^{*j}{\\mc{r}}_u\\right]\\end{aligned}\\ ] ]      minimizing the msd expression or the emse expression for the atc algorithm over left - stochastic matrices @xmath240 is generally nontrivial .",
    "we pursue an approximate solution that relies on optimizing an upper bound and performs well in practice .",
    "let us use @xmath241 to denote the nuclear norm ( also known as the trace norm , or the ky fan @xmath242-norm ) of matrix @xmath243 @xcite , which is defined as the sum of the singular values of @xmath243 .",
    "therefore , @xmath244 for any @xmath243 and @xmath245 when @xmath243 is hermitian and positive semi - definite .",
    "let us also denote @xmath246 as the block maximum norm of matrix @xmath243 ( see appendix [ app : meanconvergence ] ) .",
    "then , @xmath247 & = \\big\\|{\\mc{b}}_{\\textrm{atc}}^j({\\mc{a}}^{\\t}{\\mc{m}}{\\mc{s}}{\\mc{m}}{\\mc{a } } + { \\mc{r}}_v^{(\\psi)}){\\mc{b}}_{\\textrm{atc}}^{*j}\\big\\|_*\\nonumber\\\\ { } & \\le\\|{\\mc{b}}_{\\textrm{atc}}^j\\|_*\\cdot\\|{\\mc{a}}^{\\t}{\\mc{m}}{\\mc{s } } { \\mc{m}}{\\mc{a}}+{\\mc{r}}_v^{(\\psi)}\\|_*\\cdot\\|{\\mc{b}}_{\\textrm{atc}}^{*j}\\|_*\\nonumber\\\\ { } & \\le c^2\\cdot\\|{\\mc{b}}_{\\textrm{atc}}^j\\|_{b,\\infty}^2\\cdot { \\mathrm{tr}}({\\mc{a}}^{\\t}{\\mc{m}}{\\mc{s}}{\\mc{m}}{\\mc{a}}+{\\mc{r}}_v^{(\\psi)})\\nonumber\\\\ { } & \\le c^2\\cdot\\|{\\mc{b}}_{\\textrm{atc}}\\|_{b,\\infty}^{2j}\\cdot { \\mathrm{tr}}({\\mc{a}}^{\\t}{\\mc{m}}{\\mc{s}}{\\mc{m}}{\\mc{a}}+{\\mc{r}}_v^{(\\psi)})\\nonumber\\\\ { } & \\le c^2\\cdot(\\|{\\mc{a}}\\|_{b,\\infty}\\cdot\\|i_{nm}-{\\mc{m}}{\\mc{r}}_u\\|_{b,\\infty})^{2j } { \\mathrm{tr}}({\\mc{a}}^{\\t}{\\mc{m}}{\\mc{s}}{\\mc{m}}{\\mc{a}}+{\\mc{r}}_v^{(\\psi)})\\nonumber\\\\ { } & = c^2\\cdot\\rho(i_{nm}-{\\mc{m}}{\\mc{r}}_u)^{2j}\\cdot{\\mathrm{tr } } ( { \\mc{a}}^{\\t}{\\mc{m}}{\\mc{s}}{\\mc{m}}{\\mc{a}}+{\\mc{r}}_v^{(\\psi)})\\end{aligned}\\ ] ] where @xmath248 is some positive scalar such that @xmath249 because @xmath241 and @xmath246 are submultiplicative norms and all such norms are equivalent @xcite . in the last step of we used lemmas [ lemma : rghtstocmat ] and [ lemma : blockdiagonal ] from appendix [ app : meanconvergence ] .",
    "thus , we can upper bound the network msd by @xmath250 ^ 2}\\end{aligned}\\ ] ] where the combination matrix @xmath251 appears only in the numerator .",
    "the result motivates us to consider instead the problem of minimizing the upper bound , namely , @xmath252 using and , the cost function in can be expressed as @xmath253\\end{aligned}\\ ] ] problem can therefore be decoupled into @xmath10 separate optimization problems of the form : @xmath254\\\\ \\st & \\;\\ ; \\sum_{l\\in{\\mc{n}}_k}a_{lk}=1,\\;\\;a_{lk}\\ge0,\\;\\;a_{lk}=0\\;\\mbox{if}\\;l\\notin{\\mc{n}}_k   \\\\ \\end{aligned } } \\ ] ] for @xmath255 . with each node @xmath94 , we associate the following nonnegative _ variance product _ measure : @xmath256 this measure incorporates information about the link noise covariances @xmath257 .",
    "the solution of is then given by @xmath258 we refer to this combination rule as the relative variance combination rule ; it is an extension of the rule devised in @xcite to the case of noisy information exchanges . in particular , the definition of the scalars @xmath259 in is different and now depends on both subscripts @xmath26 and @xmath11 .    minimizing the emse expression for the atc algorithm over left - stochastic matrices",
    "@xmath240 can be pursued in a similar manner by noting that @xmath260\\le c^2[\\rho(i_{nm}\\!-\\!{\\mc{m}}{\\mc{r}}_u)]^{2j}\\ , { \\mathrm{tr}}({\\mc{a}}^{\\t}{\\mc{m}}{\\mc{s}}{\\mc{m}}{\\mc{a}}\\!+\\!{\\mc{r}}_v^{(\\psi)})\\,{\\mathrm{tr}}({\\mc{r}}_u)\\end{aligned}\\ ] ] thus , minimizing the upper bound of the network emse leads to the same solution . using the same argument",
    ", we can also show that the same result minimizes the upper bound of the network msd or emse for the cta algorithm .      to apply the relative variance combination rule",
    ", each node @xmath11 needs to know the variance products , @xmath259 , of their neighbors , which in general are not available since they require knowledge of the quantities @xmath261 .",
    "therefore , we now propose an adaptive combination rule by using data that are available to the individual nodes . for the atc algorithm , we first note from and that @xmath262 for @xmath89 .",
    "since the algorithm converges in the mean and mean - square senses under assumption [ asm : smallstepsize ] , all the estimates @xmath263 tend close to @xmath16 as @xmath140 .",
    "this allows us to estimate @xmath264 for node @xmath11 by using instantaneous realizations of @xmath265 , where we replace @xmath51 by @xmath266 .",
    "similarly , for node @xmath11 itself , we can use realizations of @xmath267 to estimate @xmath268 . to unify the notation",
    ", we define @xmath269 .",
    "let @xmath270 denote an estimator for @xmath264 that is computed by node @xmath11 at time @xmath34 .",
    "then , one way to evaluate @xmath270 is through the recursion : @xmath271 for @xmath272 , where @xmath273 is a forgetting factor that is usually close to one . in this way",
    ", we arrive at the adaptive combination rule : @xmath274^{-1}}{\\sum_{m\\in{\\mc{n}}_k}[{\\widehat{\\bm{\\gamma}}}_{mk}^{2}(i)]^{-1 } } , & { \\textrm{if $ l\\in{\\mc{n}}_k$ } } \\\\ 0 , & { \\textrm{otherwise } } \\\\ \\end{cases } } \\ ] ]",
    "the diffusion strategy  is adaptive in nature .",
    "one of the main benefits of adaptation ( by using constant step - sizes ) is that it endows networks with tracking abilities when the underlying weight vector @xmath16 varies with time . in this section",
    "we analyze how well an adaptive network is able to track variations in @xmath16 . to do so",
    ", we adopt a random - walk model for @xmath16 that is commonly used in the literature to describe the non - stationarity of the weight vector @xcite .",
    "[ asm : randomwalk ] the weight vector @xmath16 changes according to the model : @xmath275 where @xmath276 has a constant mean @xmath16 for all @xmath34 , @xmath277 is an i.i.d .",
    "random sequence with zero mean and covariance matrix @xmath278 ; the sequence @xmath277 is independent of the initial conditions @xmath279 and of all regression data and noise signals across the network for all time instants .",
    "we now define the error vector at node @xmath11 as @xmath280 so that the global error recursion ( [ eqn : noisyerrorrecursion1 ] ) for the network is replaced by @xmath281 where the @xmath282 vector @xmath283 is defined as @xmath284      by assumptions [ asm : all ] and [ asm : randomwalk ] , it can be verified that the condition for mean convergence continues to be @xmath285 , where @xmath124 is defined in .",
    "in addition , it can also be verified that the error recursion converges in the mean sense to the same non - zero bias vector @xmath141 as in . from and under assumption [ asm : smallstepsize ] , we can derive the weighted variance relation : @xmath286\\}\\nonumber\\\\ { } & \\qquad+{\\mathbb{e}}\\|{\\mc{a}}_2^{\\t}(i_{nm}-{\\mc{m}}{\\bs{\\mc{r}}}_i'){\\bs{v}}_{i-1}^{(w)}\\|_{\\sigma}^2\\nonumber\\\\ { } & \\qquad+{\\mathbb{e}}\\|{\\mc{a}}_2^{\\t}{\\mc{m}}{\\bs{z}}_i\\|_{\\sigma}^2+{\\mathbb{e}}\\|{\\bs{v}}_i^{(\\psi)}\\|_{\\sigma}^2\\end{aligned}\\ ] ] where @xmath164 is given in .",
    "if the step - sizes are sufficiently small , then we can assume that the network continues to be mean - square stable .",
    "the steady - state performance is affected by the non - stationarity of @xmath16 . from assumption",
    "[ asm : smallstepsize ] , at steady - state , expression becomes @xmath287^*(i_{n^2m^2}\\!-\\!{\\mc{f}})^{-1}{\\mathrm{vec}}(\\omega)\\end{aligned}\\ ] ] where @xmath197 is given in , @xmath288 in , @xmath199 in , @xmath164 in , and @xmath289 is the covariance matrix of @xmath283 : @xmath290 by , , and , we get @xmath291 then , following the same argument that led to , we find that the network msd is now given by : @xmath292^*(i_{n^2m^2}-{\\mc{f}})^{-1}\\vec(i_{nm } ) \\end{aligned } } \\ ] ] similarly , the network emse is given by : @xmath293^*(i_{n^2m^2}-{\\mc{f}})^{-1}\\vec({\\mc{r}}_u ) \\end{aligned } } \\ ] ] where @xmath212 is defined in .",
    "observe that the main difference relative to and is the addition of the term @xmath289 .",
    "therefore , all the results that were derived in the earlier section , such as and , continue to hold by adding @xmath289 .",
    "in particular , if assumptions [ asm : smallstepsize ] and [ asm : nodatasharing ] are adopted , expressions and can be approximated as @xmath294^*(i_{n^2m^2}-{\\mc{f}})^{-1}\\vec(i_{nm } ) \\end{aligned } } \\ ] ] and @xmath295^*(i_{n^2m^2}-{\\mc{f}})^{-1}\\vec({\\mc{r}}_u ) \\end{aligned } } \\ ] ] where @xmath198 is now given in .",
    "we simulate two scenarios : noisy information exchanges and non - stationary environments . we consider a connected network with @xmath296 nodes .",
    "the network topology is shown in fig .",
    "[ fig : topology ] .",
    "the unknown complex parameter @xmath16 of length @xmath297 is randomly generated ; its value is @xmath298 $ ] .",
    "we adopt uniform step - sizes , @xmath299 , and uniformly white gaussian regression data with covariance matrices @xmath300 , where @xmath301 are shown in fig . [",
    "fig : regressor ] .",
    "the variances of the model noises , @xmath302 , are randomly generated and shown in fig .",
    "[ fig : modelnoise ] .",
    "we also use white gaussian link noise signals such that @xmath303 , @xmath304 , and @xmath305 .",
    "all link noise variances , @xmath306 , are randomly generated and illustrated in fig .",
    "[ fig : linknoise ] from top to bottom .",
    "we assign the link number by the following procedure .",
    "we denote the link from node @xmath26 to node @xmath11 as @xmath307 , where @xmath308 .",
    "then , we collect the links @xmath309 in an ascending order of @xmath26 in the list @xmath310 ( which is a set with _ ordered _ elements ) for each node @xmath11 .",
    "for example , for node @xmath311 in fig .",
    "[ fig : topology ] , it has @xmath312 links ; the ordered links are then collected in @xmath313 .",
    "we concatenate @xmath314 in an ascending order of @xmath11 to get the overall list @xmath315 .",
    "eventually , the @xmath316th link in the network is given by the @xmath316th element in the list @xmath317 .",
    "nodes.,height=172 ]    we examine the simplified cta and atc algorithms in and , namely , no sharing of data among nodes ( i.e. , @xmath209 ) , under various combination rules : ( i ) the relative variance rule in , ( ii ) the metropolis rule in @xcite : @xmath318 where @xmath319 denotes the degree of node @xmath11 ( including the node itself ) , ( iii ) the uniform weighting rule : @xmath320 and ( iv ) the adaptive rule in with @xmath321 .",
    "we plot the network msd and emse learning curves for atc algorithms in figs .",
    "[ fig : msd_atc ] and [ fig : emse_atc ] by averaging over 50 experiments . for cta algorithms , we plot their network msd and emse learning curves in figs . [",
    "fig : msd_cta ] and [ fig : emse_cta ] also by averaging over 50 experiments .",
    "moreover , we also plot their theoretical results and in the same figures . from fig .",
    "[ fig : sim2 ] we see that the relative variance rule makes diffusion algorithms achieve the lowest msd and emse levels at steady - state , compared to the metropolis and uniform rules as well as the algorithm from @xcite ( which also requires knowledge of the noise variances ) .",
    "in addition , the adaptive rule attains msd and emse levels that are only slightly larger than those of the relative variance rule , although , as expected , it converges slower due to the additional learning step .          the value for each entry of the complex parameter @xmath322 is assumed to be changing over time along a circular trajectory in the complex plane , as shown in fig .",
    "[ fig : track ] .",
    "the dynamic model for @xmath323 is expressed as @xmath324 , where @xmath325 , @xmath326 , and @xmath327 .",
    "the covariance matrices @xmath328 are randomly generated such that @xmath329 when @xmath330 , but their traces are normalized to be one , i.e. , @xmath331 , for all nodes",
    ". the variances for the model noises , @xmath332 , are also randomly generated .",
    "we examine two different scenarios : the low noise - level case where the average noise variance across the network is @xmath333 db and the noise variances are shown in fig .",
    "[ fig : hsnr ] ; and the high noise - level case where the average variance is @xmath334 db and the variances are shown in fig .",
    "[ fig : lsnr ] .",
    "we simulate 3000 iterations and average over 20 experiments in figs .",
    "[ fig : track_hsnr ] and [ fig : track_lsnr ] for each case .",
    "the step - size is 0.01 and uniform across the network . for simplicity",
    ", we adopt the simplified atc algorithm where @xmath209 , and only use the uniform weighting rule . the tracking behavior of the network , denoted as @xmath335 , is obtained by averaging over all the estimates , @xmath336 , across the network . figs .",
    "[ fig : track_hsnr ] and [ fig : track_lsnr ] depict the complex plane ; the horizontal axis is the real axis and the vertical axis is the imaginary axis .",
    "therefore , for every time @xmath34 , each entry of @xmath323 or @xmath337 represents a point in the plane .",
    "when @xmath34 is increasing , @xmath338 moves along the red trajectory ( in @xmath339 ) , @xmath340 along the blue trajectory ( in @xmath341 ) , @xmath342 along the green trajectory ( in @xmath343 ) , and @xmath344 along the magenta trajectory ( in @xmath345 ) . from fig .",
    "[ fig : track ] , it can be seen that diffusion algorithms exhibit the tracking ability in both high and low noise - level environments .",
    "in this work we investigated the performance of diffusion algorithms under several sources of noise during information exchange and under non - stationary environments .",
    "we first showed that , on one hand , the link noise over the regression data biases the estimators and deteriorates the conditions for mean and mean - square convergence . on the other hand",
    ", diffusion strategies can still stabilize the mean and mean - square convergence of the network with noisy information exchange .",
    "we derived analytical expressions for the network msd and emse and used these expressions to motivate the choice of combination weights that help ameliorate the effect of information - exchange noise and improve network performance .",
    "we also extended the results to the non - stationary scenario where the unknown parameter @xmath16 is changing over time .",
    "simulation results illustrate the theoretical findings and how well they match with theory .",
    "following @xcite , we first define the block maximum norm of a vector .",
    "[ def : vecblkmaxnorm ] given a vector @xmath347 consisting of @xmath10 blocks @xmath348 , the block maximum norm is the real function @xmath349 , defined as @xmath350 where @xmath351 denotes the standard @xmath136-norm on @xmath352 .    similarly , we define the matrix norm that is induced by the block maximum norm as follows :    [ def : blkmaxnorm ] given a block matrix @xmath353 with block size @xmath43 , then @xmath354 denotes the induced block maximum ( matrix ) norm on @xmath355 .",
    "[ lemma : blkunitaryinvariant ] the block maximum matrix norm is block unitary invariant , i.e. , given a block diagonal unitary matrix @xmath356 consisting of @xmath10 unitary blocks @xmath357 , where @xmath358 , for any matrix @xmath353 , then @xmath359 where @xmath360 denotes the block maximum matrix norm on @xmath355 with block size @xmath43 .",
    "[ lemma : rghtstocmat ] let @xmath361 be a right - stochastic matrix . then , for block size @xmath43 , @xmath362    from definition [ def : blkmaxnorm ] , we get @xmath363_{lk}x_k\\|_2}{\\max_{k}\\|x_k\\|_2}\\nonumber\\\\ { } & \\le\\max_{x\\in{\\mathbb{c}}^{mn}\\backslash\\{0\\ } } \\frac{\\max_{l}\\sum_{k=1}^{n}[a]_{lk}\\|x_k\\|_2}{\\max_{k}\\|x_k\\|_2}\\nonumber\\\\ { } & \\le\\max_{x\\in{\\mathbb{c}}^{mn}\\backslash\\{0\\ } } \\frac{\\max_{l}(\\sum_{k=1}^{n}[a]_{lk})\\cdot\\max_k\\|x_k\\|_2}{\\max_{k}\\|x_k\\|_2}\\nonumber\\\\ { } & \\le\\max_{x\\in{\\mathbb{c}}^{mn}\\backslash\\{0\\}}\\frac{\\max_{l}1\\cdot\\max_{k}\\|x_k\\|_2}{\\max_{k}\\|x_k\\|_2}\\nonumber\\\\ { } & = 1\\end{aligned}\\ ] ] where @xmath364 consists of @xmath10 blocks @xmath348 , and @xmath365_{lk}$ ] denotes the @xmath366th entry of @xmath240 . on the other hand , for any induced matrix norm ,",
    "say , the block maximum norm , it is always lower bounded by the spectral radius of the matrix @xcite : @xmath367 combining and completes the proof .",
    "[ lemma : blockdiagonal ] let @xmath368 be a block diagonal hermitian matrix with block size @xmath43 .",
    "then the block maximum norm of the matrix @xmath251 is equal to its spectral radius , i.e. , @xmath369    denote the @xmath11th @xmath43 submatrix on the diagonal of @xmath251 by @xmath370 .",
    "let @xmath371 be the eigen - decomposition of @xmath370 , where @xmath372 is unitary and @xmath373 is diagonal .",
    "define the block unitary matrix @xmath374 and the diagonal matrix @xmath375 .",
    "then , @xmath376 . by lemma [ lemma : blkunitaryinvariant ] ,",
    "the block maximum norm of @xmath251 with block size @xmath43 is @xmath377 where we used the fact that the induced @xmath136-norm is identical to the spectral radius for hermitian matrices @xcite . on the other hand ,",
    "any matrix norm is lower bounded by the spectral radius @xcite , i.e. , @xmath378 combining and completes the proof .",
    "now we show that the matrix @xmath379 is stable if @xmath128 is stable . for any induced matrix norm , say , the block maximum norm with block size @xmath43",
    ", we have @xcite @xmath380 where , from and , @xmath126 and @xmath127",
    "satisfy lemma [ lemma : rghtstocmat ] . by and",
    ", it is straightforward to see that @xmath128 is block diagonal with block size @xmath43 .",
    "then , by lemma [ lemma : blockdiagonal ] , expression can be further expressed as @xmath381 which completes the proof . in appendix",
    "i of @xcite and the matrix @xmath180 in lemma 2 of @xcite are block diagonal , the @xmath382 norm used in these references should simply be replaced by the @xmath360 norm used here and as already done in @xcite . ]",
    "let us denote the @xmath366th submatrix of @xmath383 by @xmath384 . by assumptions [ asm : all ] and expression ,",
    "@xmath385 can be evaluated as @xmath386 where , by expressions and , @xmath387 when @xmath388 , expression reduces to @xmath389 when @xmath390 , expression becomes @xmath391 where @xmath392 denotes the kronecker delta function .",
    "evaluating the last term on rhs of requires knowledge of the excess kurtosis of @xmath393 , which is generally not available . in order to proceed ,",
    "we invoke a separation principle to approximate it as @xmath394 substituting into leads to @xmath395\\end{aligned}\\ ] ] from and , we get @xmath396\\end{aligned}\\ ] ] substituting into , we obtain @xmath397\\end{aligned}\\ ] ] from  and  , we arrive at expression .",
    "l.  li and j.  a. chambers , `` distributed adaptive estimation based on the apa algorithm over diffusion netowrks with changing topology , '' in _ proc .",
    "ieee workshop stat . signal process .",
    "( ssp ) _ , cardiff , uk , aug .",
    "2009 , pp . 757760 .",
    "n.  takahashi and i.  yamada , `` link probability control for probabilistic diffusion least - mean squares over resource - constrained networks , '' in _ proc .",
    "speech , signal process .",
    "( icassp ) _ , dallas , tx , mar .",
    "2010 , pp . 35183521 .",
    "z.  towfic , j.  chen , and a.  h. sayed , `` collaborative learning of mixture models using diffusion adaptation , '' in _ proc .",
    "workshop machine learn . signal process .",
    "( mlsp ) _ , beijing , china , sept .",
    "2011 , pp .",
    "j.  chen and a.  h. sayed , `` diffusion adaptation strategies for distributed optimization and learning over networks , '' to appear in _ ieee trans .",
    "signal process .",
    "[ also available online at http://arxiv.org/abs/1111.0034 as _ arxiv:1111.0034v2 [ math.oc]_ , oct . 2011 . ]",
    "r.  abdolee and b.  champagne , `` diffusion lms algorithms for sensor networks over non - ideal inter - sensor wireless channels , '' in _ proc .",
    "sensor systems ( dcoss ) _ , barcelona , spain , june 2011 , pp .",
    "16 .",
    "l.  xiao , s.  boyd , and s.  lall , `` a scheme for robust distributed sensor fusion based on average consensus , '' in _ proc .",
    "acm / ieee int .",
    "sensor networks ( ipsn ) _ , los angeles , ca , apr .",
    "2005 , pp . 6370 .",
    "d.  mandic , p.  vayanos , c.  boukis , b.  jelfs , s.  l. goh , t.  gautama , and t.  rutkowski , `` collaborative adaptive learning using hybrid filters , '' in _ proc .",
    "speech , signal process .",
    "( icassp ) _ , honolulu , hi , apr .",
    "2007 , pp . 921924 .",
    "tu and a.  h. sayed , `` optimal combination rules for adaptation and learning over netowrks , '' in _ proc .",
    "workshop comput .",
    "advances multi - sensor adapt . process .",
    "( camsap ) _ , san juan , puerto rico , dec .",
    "2011 , pp . 317320 ."
  ],
  "abstract_text": [
    "<S> adaptive networks rely on in - network and collaborative processing among distributed agents to deliver enhanced performance in estimation and inference tasks . </S>",
    "<S> information is exchanged among the nodes , usually over noisy links . </S>",
    "<S> the combination weights that are used by the nodes to fuse information from their neighbors play a critical role in influencing the adaptation and tracking abilities of the network . </S>",
    "<S> this paper first investigates the mean - square performance of general adaptive diffusion algorithms in the presence of various sources of imperfect information exchanges , quantization errors , and model non - stationarities . among other results </S>",
    "<S> , the analysis reveals that link noise over the regression data modifies the dynamics of the network evolution in a distinct way , and leads to biased estimates in steady - state . </S>",
    "<S> the analysis also reveals how the network mean - square performance is dependent on the combination weights . </S>",
    "<S> we use these observations to show how the combination weights can be optimized and adapted . </S>",
    "<S> simulation results illustrate the theoretical findings and match well with theory .    </S>",
    "<S> diffusion adaptation , adaptive networks , imperfect information exchange , tracking behavior , diffusion lms , combination weights , energy conservation . </S>"
  ]
}