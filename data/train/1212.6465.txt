{
  "article_text": [
    "outstanding performance of low - density parity - check ( ldpc ) codes and iterative , message - passing ( mp ) decoding algorithms  @xcite has attracted considerable attention over the past decade and these techniques are being deployed in a growing number of practical applications . at high signal - to - noise ratio ( snr ) , however , ldpc codes and mp decoders may be subject to the error floor phenomenon , which manifests itself as an abrupt change in the slope of the error - rate curve .",
    "since many important applications , such as data storage and high - speed digital communication , often require extremely low error rates , the study of error floors in ldpc codes remains of considerable practical , as well as theoretical , interest .",
    "the error floor phenomenon is commonly attributed to the existence of certain error - prone substructures ( epss ) in a tanner graph representation of the code . in the binary erasure channel ( bec )",
    ", it has been shown that substructures known as _ stopping sets _ determine the error - rate performance and the observed error floor  @xcite . however , for general memoryless binary - input output - symmetric ( mbios ) channels such as the binary symmetric channel ( bsc ) and the additive white gaussian noise channel ( awgnc ) , the epss that dominate the error floor performance have not yet been fully characterized , although some classes of epss have been identified and studied , such as _ near - codewords _",
    "@xcite , _ trapping sets _",
    "@xcite , _ absorbing sets _",
    "@xcite , and _ pseudocodewords _  @xcite",
    ".    one common way to improve the error floor performance of ldpc codes has been to redesign the codes to have tanner graphs with large girth and without problematic epss which usually consist of small number of variable nodes  @xcite .",
    "however , for ldpc codes that have been standardized , approaches are needed that do not modify the codes . in the literature , many modifications to the iterative mp decoding algorithms have been proposed in order to improve high snr performance , such as averaged decoders @xcite , reordered decoders @xcite , and decoders with post processing @xcite . in  @xcite , the authors noticed that the emergence of errors in epss is heuristically related to a sudden magnitude change in the values of certain variable nodes ( vns ) .",
    "hence , it was proposed to average the messages in a belief - propagation ( bp ) decoder over several iterations to avoid such sudden changes and therefore slow down the convergence rate for variable nodes in a trapping set and decrease the frequency of trapping set errors .",
    "another heuristic approach is to process messages based on the order of node reliabilities computed at each iteration  @xcite , and it was suggested that the scheduled decoders are able to resolve some standard trapping set errors @xcite .",
    "although these general approaches are capable of improving the average error rate performance to some extent , the resulting decoders still fail on small epss and their effect on the error floor is not significant .    to further improve the error floor behavior , decoders that make use of the prior knowledge of some small size epss",
    "have been designed to reduce the decoding failures due to such epss . in  @xcite and @xcite ,",
    "the authors proposed a post - processing decoder that matches the configuration of unsatisfied check nodes ( cns ) to trapping sets in a precomputed list after conventional mp decoding has failed .",
    "the size and completeness of the trapping set list directly affect the performance gain of such decoders , but to obtain a complete list of small trapping sets of a given ldpc code is generally quite computationally complex .",
    "a symbol - selecting post - processing technique was also developed in  @xcite .",
    "it saturates the channel messages on a set of selected variable nodes at each stage after the conventional mp algorithms fails . in @xcite , han and",
    "ryan proposed a bi - mode erasure decoder that combines several problematic check nodes into a generalized constraint processor , to which a corresponding maximum a posteriori ( map ) algorithm , such as the bcjr algorithm , is then applied .",
    "another post - processing approach that utilizes the graph - theoretic structure of absorbing sets , proposed in @xcite , adjusts the appropriate messages in the iterative mp decoding once the decoder enters and remains in the absorbing set of interest .",
    "all the above approaches either change the message update rules of mp decoders or require extra processing steps after conventional mp decoding fails , both of which increase the decoding complexity relative to the original iterative mp algorithms .",
    "moreover , the post - processing approaches that require prior knowledge of the set of epss causing the error floor are only effective when applied to ldpc codes whose epss have been carefully studied .    in fixed - point implementation of iterative mp decoding",
    ", efforts have also been made to improve the error - rate performance in the waterfall region and/or error - floor region by optimizing parameters of uniform quantization @xcite . in @xcite ,",
    "zhao _ et al .",
    "_ studied the effect of message clipping and uniform quantization on the performance of the min - sum decoder in the waterfall region and heuristically optimized the number of quantization bits and the quantization step size for selected ldpc codes . in @xcite ,",
    "a dual - mode adaptive uniform quantization scheme was proposed to better approximate the log - tanh function used in sum - product algorithm ( spa ) decoding . specifically , for magnitudes less than 1 , all quantization bits were used to represent the fractional part ; for magnitudes greater than or equal to 1 , all bits were dedicated to the representation of the integer part . in @xcite , zhang _ et al .",
    "_ proposed a conceptually similar idea to increase precision in the quantization of the log - tanh function .",
    "uniform quantization was applied to messages generated by both variable nodes and check nodes , but the quantization step sizes used in the two cases were separately optimized .",
    "we note , however , that none of these modified quantization schemes were primarily intended to significantly increase the saturation level , or range , of quantized messages , and in their reported simulation results , error floors can still be clearly observed .",
    "it has been observed that the high error floors associated with certain epss of some ldpc codes are closely related to the saturation level imposed on messages passed in the spa decoder .",
    "( see , for example , @xcite and references cited therein . ) in this work , we investigate the cause of error floors in binary ldpc codes from the perspective of the mp decoder implementation , with special attention to limitations that decrease the dynamic range of messages passed during decoding . we show that , under certain idealized assumptions , the epss which are commonly associated with high error floors of some ldpc codes will not trap iterative mp decoders and cause high error floors if message magnitudes and the number of iterations are not limited . based upon an analysis of the growth rate of messages outside an eps in an idealized scenario , we propose a novel quasi - uniform quantization method that captures the essence of messages in different ranges of reliability .",
    "the proposed quantization method has an extremely large saturation level which prevents iterative mp decoders from being trapped by an eps .",
    "this property , to the best of our knowledge , distinguishes it from other quantization techniques for iterative mp decoding that have appeared in the literature . with the new quantization method , it is possible to have a fixed point implementation of iterative mp decoders that achieves low error floors without an additional post - processing stage or a modification of either the decoding update rules or the graphical code representation upon which the iterative mp decoder operates .",
    "we present simulation results for min - sum decoding , spa decoding , and some of their variants , that demonstrate a significant reduction in the error floors of four representative ldpc codes , with no increase in decoding complexity .",
    "the remainder of the paper is organized as follows .",
    "section  [ sec : nd ] gives some notation and definitions used throughout the paper . in section  [ sec : ef ] , we analytically investigate the impact that message quantization can have on mp decoder performance and the error floor phenomenon . in section  [ sec : quandec ] , we propose an enhanced quantization method intended to overcome the limitations of traditional quantization rules . in section  [ sec : nr ]",
    ", we incorporate the new quantizer into spa and min - sum decoding and , through computer simulation of several ldpc codes known for their high error floors , demonstrate the significant improvement in error - rate performance that this new quantization approach can afford .",
    "section  [ sec : con ] concludes the paper .",
    "the study of the phenomenon of error floors began shortly after ldpc codes were rediscovered about a decade ago .",
    "it has been shown that the epss known as _ stopping sets _ cause the error floor in the binary erasure channel ( bec ) , and such epss have a clear combinatorial description .",
    "enumeration of these structures makes it possible to accurately estimate the error floor  @xcite .",
    "however , for other mbios channels such as the binary symmetric channel ( bsc ) and the additive white gaussian noise channel ( awgnc ) , it is more difficult to establish the relationship between epss and error floors . in  @xcite , it was first pointed out that the _ near - codewords _ caused error floors in simulations of margulis and ramanujan - margulis ldpc codes on the awgnc .",
    "the term _ trapping set _ proposed by richardson  @xcite",
    "is operationally defined as a subset of variable nodes ( vns ) that is susceptible to errors under a certain iterative mp decoder over an mbios channel .",
    "hence , this concept depends on both the channel and the decoding algorithm . in @xcite ,",
    "the error floor is associated with some combinatorial substructures within the tanner graph , named _ absorbing sets _ , which are defined independently of the channel .",
    "the absorbing sets correspond to a particular type of near codewords or trapping sets that are stable under bit - flipping operations .",
    "all these epss have been believed to be the cause of error floors , and for some ldpc codes , techniques such as importance sampling used to estimate the error floor are based on the probability of decoding failures on such epss @xcite . in this section",
    ", we will show that under certain idealized assumptions about the computation trees of variable nodes within a given eps , as well as the correctness of variable node messages outside the eps , conventional iterative decoders that accurately represent messages will eventually correct errors supported by the eps .    to facilitate our discussion ,",
    "we define a substructure called an _ absolute trapping set _ from a purely graph - theoretic perspective , independent of the channel and the decoder .",
    "let @xmath1 denote the tanner graph of a binary ldpc code with vns @xmath2 , cns @xmath3 , and edge set @xmath4 .",
    "[ def_eps ] a _ stopping set _ of size @xmath5 is a configuration of @xmath5 variable nodes such that the induced subgraph has no check nodes of degree - one .",
    "an @xmath6 _ trapping set _ is a configuration of @xmath5 variable nodes , for which the induced subgraph is connected and has @xmath7 odd - degree check nodes .",
    "if the induced subgraph of an @xmath6 trapping set does not contain a stopping set , it is called an _ absolute trapping set_.    in the literature , all trapping sets of interest that contribute to the error floor of an ldpc code are of size smaller than the minimum stopping set size of the code , since otherwise the stopping sets would be the dominant contributor to the error floor  @xcite .",
    "note that the requirement that an absolute trapping set contain no stopping set also implies that it must have at lease one degree - one check node . as we will discuss later in this section ,",
    "these degree - one check nodes are essential because they are able to pass correct extrinsic messages into the trapping set . to the best of our knowledge , almost all trapping sets of interest in the literature are absolute trapping sets .",
    "for example , both of the well - known ( 5,3 ) trapping sets in the tanner code of length 155 , the notorious ( 12,4 ) trapping sets in the ( 2640,1320 ) margulis code , and the ( 5,5 ) trapping set in some codes of variable - degree five are all absolute trapping sets . unless otherwise indicated , all trapping sets referred to in this paper are absolute trapping sets , as well .    in analogy to the definition of _ computation tree _ in @xcite",
    ", we define a _ k - iteration computation tree _ as follows .",
    "[ def_ct ] a _ k - iteration computation tree _",
    "@xmath8 for an iterative decoder in the tanner graph @xmath9 is a tree graph constructed by choosing variable node @xmath10 as its root and then recursively adding edges and leaf nodes to the tree that participate in the iterative message - passing decoding during @xmath11 iterations . to each vertex that is created in @xmath8 , we associate the corresponding node update function in @xmath9 .",
    "let @xmath12 be the induced subgraph of an @xmath13 trapping set contained in @xmath9 , with vn set @xmath14 and cn set @xmath15 .",
    "let set @xmath16 be the set of degree - one cns in the subgraph @xmath12 , and let set @xmath17 be the set of neighboring vns of cns in @xmath18 .",
    "we refer to a message on an edge adjacent to vn @xmath19 as a _",
    "message if its sign reflects the correct value of @xmath19 , and as an _ incorrect _ message , otherwise .",
    "let @xmath20 be the set of all descendants of the vertex @xmath21 in a given computation tree .",
    "[ def_separation ] given a tanner graph @xmath9 and an induced subgraph @xmath12 of a trapping set , a variable node @xmath22 is said to be _ @xmath11-separated _ if , for at least one of its neighboring degree - one check node @xmath23 , no variable node @xmath24 belongs to @xmath25 .",
    "if every @xmath22 is @xmath11-separated , the induced subgraph @xmath12 is said to satisfy the _",
    "@xmath11-separation assumption_.    in fig .",
    "[ subfig_ts44 ] , we show the graph of a @xmath26 trapping set and some of its neighboring nodes . the set of vns in the trapping set is @xmath27 , represented as solid black circles .",
    "the set of cns in the trapping set is @xmath28 , @xmath29 . in this trapping set",
    ", every vn has a neighboring degree - one cn , i.e. , @xmath30 , and @xmath31 .",
    "for example , the 3-iteration computation tree of vn @xmath32 is shown in fig .",
    "[ subfig_comptree ] .",
    "it can be verified from this computation tree that @xmath32 is 2-separated but not 3-separated , because @xmath33 is a descendant of @xmath34 in @xmath35 , but not in @xmath36 .",
    "it is worth noting that whether or not a trapping set satisfies the @xmath11-separation assumption depends on the tanner graph outside the trapping set , not the trapping set itself .",
    "we want to point out that the @xmath11-separation assumption is much weaker than the isolation assumption in @xcite .",
    "the separation assumption here only applies to the vns that have neighboring degree - one cns in the induced subgraph @xmath12 , and these neighboring degree - one cns do not have any vns from the trapping set as their descendants in the corresponding @xmath11-iteration computation tree . with the separation assumption ,",
    "the descendants of @xmath23 are separated from all the nodes in the trapping set , meaning that the incorrect messages passed in the trapping set do not affect the extrinsic messages sent towards @xmath37 in the computation tree .",
    "to get further insight into the connection between trapping sets and decoding failures of iterative mp decoders , we first consider a simple iterative mp decoder , the min - sum ( ms ) decoder , which can be viewed as a simple approximation of the sum - product algorithm .",
    "we now briefly recall the vn and cn update rules of min - sum decoding .",
    "a vn @xmath38 receives an input message @xmath39 from the channel , typically the log - likelihood ratio ( llr ) of the corresponding channel output , defined as follows @xmath40 where @xmath41 is the code bit and @xmath42 is the corresponding received symbol .",
    "denote by @xmath43 and @xmath44 the messages sent from @xmath38 to @xmath45 and from @xmath45 to @xmath38 , respectively , and denote by @xmath46 the set of neighboring nodes of vn @xmath47 ( or cn @xmath48 ) .",
    "then , the message sent from @xmath38 to @xmath45 in min - sum decoding is given by @xmath49 and the message from cn @xmath50 to vn @xmath51 is computed as @xmath52\\cdot \\min\\limits_{i'\\in n(j)\\setminus i } |l_{i'\\rightarrow j}|.\\ ] ] in the initialization step , we set @xmath53 . it can been seen from and that the min - sum decoding algorithm is insensitive to linear scaling , meaning that linearly scaling all input messages from the channel would not affect the decoding performance .    for the ms decoder",
    ", we can show that a trapping set does not cause decoding failure if its induced subgraph in the tanner graph satisfies certain criteria .",
    "[ thm_ms ] let @xmath9 be the tanner graph of a variable - regular ldpc code that contains a subgraph @xmath12 induced by a trapping set .",
    "assume that the channel is either a bsc or an awgnc , and that the messages from the channel to all vns outside @xmath12 are correct .",
    "if @xmath12 satisfies the @xmath11-separation assumption for sufficiently large @xmath11 , then the corresponding min - sum decoder will successfully correct all erroneous vns in @xmath12 .",
    "see appendix  [ appendix_thm_ms ] .    in general ,",
    "the error - rate performance of ms decoding is not as good as that of spa decoding .",
    "however , there are several quite simple but effective ways to adjust the cn update rule of ms decoding to get comparable performance to spa decoding .",
    "one method is _ attenuated - min - sum _ ( ams ) decoding  @xcite , where the magnitudes of messages are attenuated at cns .",
    "the corresponding cn update rule of ams is as follows @xmath54\\cdot \\alpha \\cdot \\min\\limits_{i'\\in n(j)\\setminus i } |l_{i'\\rightarrow j}|,\\ ] ] where @xmath55 is the attenuation factor , which can be a fixed constant or adaptively adjusted .",
    "another way to improve the error - rate performance of ms decoding is _ offset - min - sum _ ( oms ) decoding , which applies an offset to reduce the magnitudes of cn output messages .",
    "the resulting cn update equation is @xmath56\\cdot \\max\\{\\min\\limits_{i'\\in n(j)\\setminus i } |l_{i'\\rightarrow j}|-\\beta,~0\\},\\ ] ] where @xmath57 is the offset which , like the attenuation factor , can be a fixed constant or adaptively adjusted . in some implementations , for additional simplicity , the attenuation factor or offset is set to be the same fixed constant for all cns and all iterations  @xcite .",
    "theorem  [ thm_ms ] can be extended to both ams and oms decoding , where we assume that , in each iteration , all cns use the same attenuation factor @xmath58 in ams or the same offset @xmath59 in oms .",
    "[ cor_mms ] let @xmath9 be the tanner graph of a variable - regular ldpc code that contains a subgraph @xmath12 induced by a trapping set .",
    "assume that the channel is either a bsc or an awgnc , and that the messages from the channel to all vns outside @xmath12 are correct .",
    "if @xmath12 satisfies the @xmath11-separation assumption for sufficiently large @xmath11 , then the both ams and oms decoder will successfully correct all erroneous vns in @xmath12 .",
    "see appendix  [ appendix_cor_mms ] .    as shown in appendix  [ appendix_cor_mms",
    "] , the extension to ams decoding follows easily from theorem  [ thm_ms ] . on the other hand ,",
    "the proof of the extension to the oms decoder makes use of ideas introduced in the analysis of spa decoding in the next subsection .      in this subsection",
    ", we further extend theorem  [ thm_ms ] to sum - product algorithm decoding .",
    "the optimality criterion in the design of the spa decoder is symbol - wise maximum a _ posteriori _ probability ( map ) , and it is an optimal symbol - wise decoder on tanner graphs without cycles .",
    "in spa decoding , vn nodes take log - likelihood ratios of received information from the channel as initial input messages .",
    "the vn update rule is the same as that of ms decoding described in , which involves the summation of all incoming extrinsic messages . in the cn update rule of spa decoding ,",
    "the message sent from cn @xmath50 to vn @xmath51 is computed as @xmath60    in practical implementations of the spa , the following equivalent cn update rule is often used @xmath61\\cdot \\phi^{-1}\\left(\\sum\\limits_{i'\\in n(j)\\setminus i } \\phi(|l_{i'\\rightarrow j}|)\\right)\\ ] ] where @xmath62=\\log\\left ( ( e^x+1)/(e^x-1 ) \\right)$ ] , @xmath63 , and @xmath64 . in some fixed - point implementations , in order to have better approximation , different look - up tables could be used to compute @xmath65 and @xmath66  @xcite .",
    "we note that the hyperbolic tangent function , @xmath67 , has numerical saturation problems when computed with finite precision .",
    "for example , in 64-bit floating - point ( in ieee 754 standard format @xcite ) computer implementation , it can be shown that @xmath68 would be rounded to 1 when @xmath69 , meaning that @xmath70 for @xmath69 @xcite . in order to avoid such problems that can arise from limited precision , thresholds on the magnitudes of messages",
    "must be applied in simulation studies  @xcite .    in order to maintain the performance advantage of spa decoding over ms decoding ,",
    "the quantization method has to preserve the self - inverse property of the @xmath65 function and to accurately compute the cn update function in .",
    "however , it is difficult to have a good approximation of the @xmath65 function with limited resolution , because this requires both fine precision and large range .",
    "efforts have been made to design quantization methods that work effectively with the @xmath65 function .",
    "for example , a variable - precision quantization scheme proposed in @xcite uses larger quantization step size for magnitudes greater than 1 , and smaller step size for magnitudes less than 1 .",
    "an adaptive uniform quantization method proposed in @xcite uses different quantization step sizes for the outputs of the @xmath65 and the @xmath66 function in .",
    "if the output of the @xmath65 function is quantized with finite precision @xmath71 , inputs greater than @xmath72 can not be distinguished , and @xmath72 is quite small even for extremely fine precision , e.g. , @xmath73 and @xmath74 .",
    "hence , the largest supported magnitude during decoding depends on the finest precision of quantization .",
    "this means that increasing the quantization range without improving the precision is not beneficial .    in order to avoid dealing with the @xmath65 function , a variety of other cn update rules , most of which are approximations to the spa ,",
    "have been proposed .",
    "some of these approximation are based on the following equivalent version of the spa cn update rule represented by or , @xmath75 where @xmath76 is the pairwise `` box - plus '' operator defined as @xmath77 with @xmath78    the proof of equivalence between and can be found in @xcite .",
    "we call such an implementation _ box - plus _ spa decoding",
    ". the formulation above does not have the precision problem that and have , and , in fact , in 64-bit double - precision floating - point implementation , the maximum magnitude of a message that can be supported is approximately @xmath79 , which is the largest double - precision value supported by the ieee 754 standard .",
    "moreover , unlike the @xmath65 function , the function @xmath80 can be well quantized or approximated with piecewise linear functions @xcite .",
    "if the term @xmath81 is omitted when using to calculate the cn output in box - plus decoding , the result is the same as that produced by the ms algorithm using  .",
    "therefore , box - plus spa decoding can be viewed as ms decoding with a correction factor .",
    "it is known that the magnitude of @xmath81 is bounded above by @xmath82 ( see , for example , @xcite ) .",
    "in fact , as shown in  @xcite,@xcite , given the same inputs , a message produced by a cn in spa decoding has the same sign as the corresponding message in ms decoding , with equal or smaller magnitude . because of their relevance to the proof of theorem  [ thm_spa ] below , we summarize these observations relating the cn updates produced by the spa and ms decoders in lemma  [ lemma_ms_vs_spa ] .",
    "[ lemma_ms_vs_spa ] let @xmath83 denote the message from cn @xmath50 to vn @xmath51 as computed in ( [ eq_ms_cn ] ) , and let @xmath84 denote the message from cn @xmath50 to vn @xmath51 as computed in ( [ eq_spa_cn ] ) , ( [ eq_spa_cnphi ] ) , and ( [ eq_spa_cnbox ] ) .",
    "then @xmath85 and @xmath86 the correction term @xmath81 in ( [ eq_s ] ) satisfies @xmath87 , and @xmath88 when @xmath89 .",
    "see appendix  [ appendix_lemma_ms_vs_spa ] .",
    "finally , we note that if the correction term @xmath81 is replaced with a fixed constant , the resulting cn update rule corresponds to that of the oms decoder in ( [ eq_oms_cn ] ) .",
    "as we discussed earlier , no matter how one designs the fixed - point implementation of the original spa using the @xmath65 function , or even with the floating - point implementation , the function @xmath90 is unbounded .",
    "even if we saturate both the input and the output of the @xmath65 function , the value of @xmath90 is still unbounded and linear in @xmath91 .",
    "therefore , the cn output of a practical implementation of or can significantly differ from the true computed value .",
    "however , since box - plus spa decoding can be considered as min - sun decoding with a correction factor , the implementation error mainly comes from the computation and quantization of the correction factor , which is a small bounded value , as shown in lemma  [ lemma_ms_vs_spa ] .",
    "now , we can extend theorem  [ thm_ms ] to spa decoding .",
    "[ thm_spa ] let @xmath9 be the tanner graph of a variable - regular ldpc code that contains a subgraph @xmath12 induced by a trapping set .",
    "assume that the channel is either a bsc or an awgnc , and that the messages from the channel to all vns outside @xmath12 are correct .",
    "if @xmath12 satisfies the @xmath11-separation assumption for sufficiently large @xmath11 , then the spa decoder will successfully correct all erroneous vns in @xmath12 .",
    "see appendix  [ appendix_thm_spa ] .",
    "[ rm3 ] as will be shown in the simulation results , linear scaling of the input llrs to the spa decoder will indeed affect the decoding performance , because the correction factor @xmath81 is not linear in either @xmath91 or @xmath92 .    for most ldpc codes ,",
    "the trapping sets typically satisfy the @xmath11-separation assumption only for small values of @xmath11 .",
    "nevertheless , as described more fully in section  [ sec : nr ] , in our 64-bit double - precision floating - point computer simulations of ms decoding and box - plus spa decoding applied to several ldpc codes traditionally associated with high error floors , we have not observed , in tens of billions of channel realizations of both the bsc and the awgnc , any decoding failure in which the error patterns correspond to the support of a small trapping set .",
    "moreover , when we force every vn in a trapping set to be in error and all other vns to be correct , the floating - point decoders can successfully decode , whereas a decoder implementation that limits the magnitude of messages may not be able to resolve the errors in the trapping set and would then fail to decode to the correct codeword .",
    "we emphasize that the analytical and numerical results in this paper are mainly for variable - regular ldpc codes .",
    "extension of this analysis to variable - irregular ldpc codes does not appear to be straightforward .",
    "as mentioned above , several empirical studies have shown that the range and the precision of quantized messages in iterative ldpc decoders can influence the observed error floor . moreover , analytical models used to study the dynamical evolution of messages show that message magnitudes can exhibit exponential growth behavior as a function of the number of decoder iterations . likewise , the proofs of the theorems and corollaries in section  [ sec : ef ] suggest that iterative decoder performance can be improved by allowing for the exponential growth of message magnitudes .",
    "these results serve as the motivation for a new quantization method that we refer to as @xmath93-_bit quasi - uniform _ quantization , which we now describe .",
    "consider first the uniform quantizer with quantization step @xmath94 .",
    "for any real number @xmath91 , it is defined by @xmath95 the outputs of the uniform quantizer are of the form @xmath96 .",
    "the quantization intervals can be visualized by expressing the quantization rule as @xmath97    now , let @xmath98 , where @xmath99 is an integer value @xmath100 .",
    "the @xmath99-bit uniform quantizer combines the uniform quantization intervals corresponding to the output values @xmath101 into a single semi - infinite interval whose elements are quantized to @xmath102 and , similarly , combines the intervals corresponding to the output values @xmath103 into a single semi - infinite interval whose elements are quantized to @xmath104 .",
    "denoting the @xmath99-bit quantizer with step @xmath94 by @xmath105 , we have    @xmath106    the number of intervals is @xmath107 , and the quantizer output levels @xmath108 , can be denoted by the signed @xmath99-bit binary representation of @xmath109 , that is , @xmath110 $ ] , where the last @xmath111 bits are the binary representation of @xmath112 , and @xmath113 is the sign bit with value 0 ( resp .",
    "1 ) when @xmath109 is positive ( resp .",
    "negative ) .",
    "note that the output level 0 has two such binary representations ; one of them can be selected using any preferred convention .",
    "one approach to expanding the range of quantized messages is to increase the step size @xmath94 , without changing the resolution @xmath99 .",
    "this approach , however sacrifices the precision of the quantization .",
    "alternatively , one could maintain the value of @xmath94 and increase @xmath99 to resolve larger magnitudes .",
    "this would increase implementation complexity when incorporated into the decoding hardware .    in the context of our application , the @xmath93-_bit quasi - uniform _",
    "quantizer represents a compromise between these conflicting objectives of retaining fine precision , allowing large dynamic range , and controlling implementation complexity as messages grow exponentially in the number of decoder iterations .",
    "the definition of the quantizer involves another parameter @xmath114 , which we refer to as the _ growth rate _ parameter . roughly speaking ,",
    "the underlying idea behind the quantizer is as follows . for input values in the interval @xmath115,we use @xmath99-bit uniform quantization with step size @xmath94 .",
    "the intervals corresponding to quantized values @xmath116 are exactly like those of the @xmath99-bit uniform quantizer . for values",
    "@xmath102and @xmath104 , the semi - infinite intervals are shortened to have length @xmath117 for input values with magnitude larger than @xmath118 , the quantizer outputs can take an additional @xmath119 values of the form @xmath120 , with corresponding intervals that increase exponentially in length with growth rate @xmath121 .",
    "more precisely , the @xmath93-_bit quasi - uniform _",
    "quantizer , denoted by @xmath122 is defined as follows .",
    "@xmath123    a    from definition  , we see that the quantization levels can be represented with only @xmath124 bits .",
    "the levels @xmath108 are represented by @xmath125 $ ] , where @xmath110 $ ] is the signed binary representation of the integer @xmath109 and the final _ indicator bit _ , @xmath126 is set to zero , i.e. , @xmath127 , to reflect the fact that the @xmath99-bit uniform quantizer has been applied .",
    "the @xmath128 quantized levels @xmath129 are denoted by @xmath130 $ ] , where @xmath131 $ ] is the signed binary representation of @xmath132 , and the indicator bit @xmath133 is set to 1 , i.e. , @xmath134 , to indicate that non - uniform quantization has been used .",
    "similarly , we denote the @xmath128 quantized levels @xmath135 by @xmath136 $ ] , where @xmath131 $ ] is the signed binary representation of @xmath137 , and the indicator bit @xmath133 is again set to 1 , i.e. , @xmath134 .",
    "it is sometimes convenient to represent these quantization levels in the form @xmath138 , where @xmath92 is the decimal integer representation of the signed binary @xmath99-tuple @xmath110 $ ] or @xmath131 $ ] , and @xmath7 is the indicator bit @xmath126 or @xmath133 .",
    "table  [ tab_31bit ] shows an example of ( 3 + 1)-bit quasi - uniform quantization with @xmath139 , @xmath140 , and @xmath141 . here",
    "the operation of the quantizer is shown only for non - negative real inputs .",
    "the operation on negative reals can be obtained by odd symmetry .",
    "the first bit is the sign bit , and the last bit is the indicator bit .",
    "the quantizer behaves just like the 3-bit uniform quantizer in the interval @xmath143 .",
    "when @xmath144 , the quantizer uses intervals of exponentially increasing length , with input @xmath91 quantized to the smallest value in the interval in which @xmath91 falls .",
    "for example , all values within the quantization interval @xmath145 are quantized to 27 .",
    "the decimal values are used in the vn and cn update computations , and then the corresponding quantized binary messages are passed between vns and cns .    .4-bit quasi - uniform quantization with @xmath139 , @xmath141 , and @xmath146 . [ cols=\"^,^,^ \" , ]     [ tab_4bit ]    we can further extend the idea of ( @xmath99 + 1)-bit quasi - uniform quantization , as follows .",
    "the @xmath93-bit quasi - uniform quantizer uses @xmath124 bits in total to represent @xmath147 different output magnitudes , or @xmath148 quantization intervals if signs are taken into account . as described in and illustrated in table  [ tab_31bit ] , @xmath119 output magnitudes , including 0 , are allocated to the uniform quantization domain and the remaining @xmath119 magnitudes correspond to exponentially growing quantization interval lengths .",
    "the generalized ( symmetric ) @xmath93-bit quasi - uniform quantizer represents the same number of magnitudes , but it can assign any number , say @xmath149 , to the uniform quantization range and the remaining @xmath150 magnitudes to the exponential quantization range . with a quantization rule similar to , the quantized values of the general @xmath124-bit quasi - uniform quantization are @xmath96 for @xmath151 ; @xmath152 for @xmath153 , and @xmath154 for @xmath155 .",
    "table  [ tab_4bit ] shows an example of a general 4-bit quasi - uniform quantization with @xmath146 , @xmath139 , and @xmath141 .",
    "the uniform quantization range in this example is from @xmath156 to 4 with uniform step size 1 , and the exponential range is above 4 or below @xmath156 with exponential step sizes @xmath157 for @xmath158 .",
    "the motivation for the proposed quasi - uniform quantization method was the analysis of message - passing decoder behavior on trapping sets that satisfy the @xmath11-separation assumption for large @xmath11 .",
    "although this property is generally not satisfied by trapping sets in practical ldpc codes , the simulation results in the next section demonstrate that , for a variety of ldpc codes that were examined , this quantization approach can substantially lower error floors when used with standard ms - based and spa - based decoders .",
    "in this section we compare the error - rate performance obtained with the proposed quasi - uniform quantization method to that obtained using uniform quantization .",
    "we consider four know lpdc codes covering a range of rates and lengths : a rate-@xmath159 , ( 640,192 ) quasi - cyclic ( qc ) ldpc code @xcite ; the rate-@xmath160 , ( 2640,1320 ) margulis ldpc code @xcite ; the rate-@xmath161 , ( 1280 , 1024 ) ar4ja ldpc code @xcite ; and mackay s ( 4095,3358 ) regular ldpc code ( the 4095.737.3.101 code in @xcite ) with rate approximately 0.82 .",
    "results are shown for various combinations of the bsc and awgn channels using the ms , oms , ams , spa , and approximated - spa decoders .",
    "all of the frame error rate ( fer ) curves are based on monte carlo simulations that generated at least 200 error frames for each plotted error rate , and the maximum number of decoding iterations was set to 200 , unless otherwise indicated .",
    "we first present some empirical data in support of the contention that some benefit may come from allowing message magnitudes to grow during iterative decoding .",
    "[ fig_pdf ] shows the empirical probability density functions ( pdf ) of the message magnitudes observed during decoding simulations for two ldpc codes .",
    "[ subfig_pdf640 ] shows the pdf for the ms decoder applied to the ( 640,192 ) qc - ldpc code on the bsc with @xmath162 , where the magnitude of all input llrs is scaled to 1 .",
    "[ subfig_pdf2640 ] shows the pdf of the spa decoder applied to the margulis code on the awgnc with @xmath163 db .",
    "the data used to create these figures were obtained using floating - point decoder implementations and more than 10 million channel output symbols .",
    "the messages passed on all edges during all decoding iterations were collected to generate the pdfs . in the simulations , the iterative mp decoders stopped when a codeword was found or when the maximum number of iterations ( 200 ) was reached .",
    "the figures confirm that a substantial fraction of messages had `` large '' magnitudes .",
    "moreover , upon further examination of the simulation data , we found that such `` strong '' messages , in general , helped to successfully decode the received symbols , as suggested by the idealized theoretical analysis in section [ sec : ef ] .     and @xmath141 .",
    "]     and @xmath141 . ]",
    "[ fig_bsc640ms ] and [ fig_awgn640ms ] show simulation results for the ( 640,192 ) qc - ldpc code using various types of quantized ms decoders and floating - point ms decoders , extending some of the results presented in @xcite . for the bsc",
    ", we scaled the magnitudes of decoder input messages from the channel to 1 since , for linear decoders such as gallager - b and ms , the scaling of channel input messages does not affect the decoding performance .",
    "the uniform quantization step size @xmath94 is set to 1 or 0.5 .",
    "so , for example , when @xmath139 , the 3-bit uniform quantizer produces values  @xmath164 , and the ( 3 + 1)-bit quasi - uniform quantizer with @xmath141 yields the values described in table  [ tab_31bit ] . in the simulation , the parameter @xmath121 was heuristically chosen by testing different values .",
    "when @xmath99 is large , a small @xmath121 proved to be enough to represent a large range of message magnitudes .    in fig .",
    "[ fig_bsc640ms ] , we see that the slope of the error floor resulting from uniform quantization with either step size , @xmath139 or @xmath165 , is similar to that of the gallager - b decoder error floor . this is because , when most messages have the same magnitude , ms decoding essentially degenerates to gallager - b decoding , which relies solely upon the signs of messages .",
    "comparing uniform quantizers with the same number of bits but different step sizes , we see that smaller step size produces better performance in the waterfall region but a higher error floor .",
    "this observation can be explained by the saturation level of these quantizers .",
    "for example ,",
    "3-bit and 4-bit uniform quantizers with step size @xmath139 saturate at magnitudes 3 and 7 , respectively , whereas with step size @xmath165 , they saturate at magnitudes 1.5 and 3.5 , respectively .",
    "the stronger messages , i.e. , the messages with larger magnitudes , can be helpful or harmful to the decoding process , depending on whether they are correct or not .",
    "the correct ones can help overcome the incorrectly received bits , but the incorrect ones tend to negatively influence the recovery of correctly received bits .",
    "in the error - floor region , when channel conditions are good , very few bits are received incorrectly , and as suggested by the proofs of theorems [ thm_ms ] and [ thm_spa ] , large saturation levels allow messages corresponding to correct bits to grow sufficiently to overcome the `` incorrect '' messages in trapping sets .",
    "this behavior is evident in fig .",
    "[ fig_bsc640ms ] , where the error floors produced by the different uniform quantizers monotonically decrease as the saturation levels increase .    on the other hand , in the waterfall region where many bits are received incorrectly ,",
    "reducing the saturation level may limit the propagation of strong incorrect messages .",
    "moreover , in this specific case , quantization with the smaller step size @xmath165 may be expected to improve performance relative to that achieved with the larger step size @xmath139 or with a floating - point ms decoder implementation .",
    "the reasoning is that , since the magnitudes of input llrs to the ms decoder from the bsc are scaled to 1 , the low saturation level and the possible appearance of non - integral saturated messages may reduce the possibility of the messages at a vn summing to zero . because having vns summing to zero could result in oscillatory behavior in the decoder and failure to decode correctly",
    ", this could explain why in fig .",
    "[ fig_bsc640ms ] the ms decoder using ( 3 + 1)-bit quasi - uniform quantization and step size @xmath165 yields better performance than the floating - point decoder .",
    "[ fig_awgn640ms ] shows the performance of ms decoding of the ( 640,192 ) qc - ldpc code on the awgnc channel . here",
    "the @xmath166-bit quasi - uniform quantizer yields substantial reduction of the error floor in comparison not only to 8-bit uniform quantization but also to the floating - point results .",
    "this is consistent with , and more impressive than , the results shown in @xcite for the margulis code , where @xmath167-bit quasi - uniform quantization surpassed 6-bit uniform quantization and paralleled floating - point results .",
    "heuristic reasoning along the lines used above suggests that codes with higher variable - node degree would benefit even more from the quasi - uniform quantization .",
    "however , it is important to point out that the gains can be code - dependent , so further performance studies are needed to confirm this .    , @xmath168 , and attenuation factor @xmath169 . ]    , @xmath168 , and attenuation factor @xmath169 . ]",
    "quasi - uniform quantization can be directly applied to modified ms decoders , such as ams and oms , with the possibility of significant reduction in the error floor .",
    "this was illustrated in @xcite for the ( 640,192 ) qc - ldpc code with ams decoding on the bsc and with oms decoding on the awgnc . in case of ams decoding",
    ", @xmath166-bit quasi - uniform quantization dramatically reduced the error floor relative to 4-bit uniform quantization , achieving the performance of the unsaturated ams decoder . for oms decoding with @xmath170-bit quantization , the comparisons to 5-bit uniform quantization and unsaturated decoding were analogous .",
    "here we consider the performance of ams and oms decoding on longer codes with higher rates , specifically , the rate-0.8 , ( 4095,3358 ) regular code and the rate-0.8 , ( 1280 , 1024 ) , irregular ar4ja code .",
    "[ fig_awgnar4jaoms ] compares the quasi - uniform quantization method with uniform quantization in oms decoding .",
    "the performance of the floating - point oms decoder is also shown .",
    "with uniform quantization ranging from 5 bits to 9 bits , we can see that 8 bits suffice to closely approach the error - rate performance of floating - point oms , whereas the ( 4 + 1)-bit quasi - uniform quantization actually surpasses floating - point decoder .",
    "[ fig_awgn4095ams ] shows a similar comparison for ams decoding of mackay s ( 4095,3358 ) ldpc code .",
    "the attenuation factor @xmath58 was set to the value 0.7 , which was found empirically to give the best error floor performance among integer multiples of 0.1 in the range [ 0.5 , 0.9 ] .",
    "after normalization by this factor in every cn update , we found that the quantized value lost precision due to the coarse step size @xmath165 .",
    "as a consequence , the floating - point ams decoder had better performance than any of the quantized decoders , most noticeably in the waterfall region .",
    "uniform quantization with 7 or more bits appears to eventually achieve floating point performance at fer below @xmath171 , as does @xmath167-bit quasi - uniform quantization .",
    "we now consider the application of quasi - uniform quantization to spa decoding . in our simulations of quantized spa decoding , the input llrs and the messages passed between cns and vns are quantized values . for convenience , the cn updates are carried out with floating - point arithmetic using the box - plus update rule in ;",
    "the resulting message is then quantized appropriately .     and @xmath168 .",
    "]     and @xmath168 . ]    in  @xcite , we illustrated the performance of quasi - uniform quantization with spa decoding of the ( 640,192 ) qc - ldpc code on the bsc .",
    "we saw that with llr magnitudes scaled to 2 , the ( 6 + 1)-bit quasi - uniform quantizer with step size @xmath172 and @xmath173 performs significantly better than 7-bit uniform quantization with the same step size .",
    "its performance is comparable to that of the floating - point spa decoder , which is superior to floating - point spa decoding with exact llr magnitudes @xmath174 when the channel error probability @xmath175 is small .",
    "here we consider the same code and channel , with step size again set to @xmath172 , but with quantization value scale factor reduced to @xmath168 .",
    "with llr magnitudes scaled to 2 , we simulated 6-bit through 10-bit uniform quantization , ( 5 + 1)-bit quasi - uniform quantization , and floating - point spa decoding with llr magnitudes scaled to 2 as well as with exact llr magnitudes .",
    "the simulation results , shown in fig .  [ fig_bsc640spa ] , indicate that the @xmath167-bit quasi - uniform quantizer provides the best performance for @xmath176 .",
    "comparing to the results in  @xcite , the performance of the @xmath167-bit quantizer with @xmath168 is only slightly worse than that of the @xmath177-bit quantizer with @xmath173 .",
    "we note that the selection of the input llr magnitude , here set to 2 , is heuristic and code - dependent .",
    "the value 2 was found empirically to give much better performance than , for example , the value 1 , but does not necessarily represent the optimal llr magnitude scaling .",
    "results for spa decoding of the ( 640,192 ) qc - ldpc code on the awgn channel were also presented in  @xcite .",
    "the @xmath177-bit quasi - uniform quantizer with @xmath172 and @xmath173 was found to significantly improve upon 7-bit uniform quantization and match the performance of the floating - point box - plus spa decoder .    in  @xcite , we found similar relative performance for the margulis code on the awgnc .",
    "the @xmath177-bit quasi - uniform quantizer outperformed 7-bit uniform quantization , with step size parameters @xmath172 and @xmath178 , and its performance equaled that of the `` approximated box - plus spa '' decoder .",
    "the latter made use of a two - piece linear approximation for @xmath179 , taken from  @xcite , in computing the correction factor @xmath81 for box - plus spa decoding in , namely , @xmath180 the approximated decoder ran about five times faster than the floating - point spa decoder , with a performance penalty of less than 0.02  db in the waterfall region .    in fig .",
    "[ fig_awgn2640spa ] we show further results for the margulis code on the awgnc . the plot shows the fer results for @xmath167-bit quasi - uniform quantization , as well as 6- , 7- , and 8-bit uniform quantizers , with quantization parameters set to @xmath172 and @xmath168 .",
    "we also evaluated the dual quantization spa decoding proposed by zhang _",
    "@xcite , where the @xmath65 function is quantized into a mapping table , denoted as @xmath181 . following the notation in @xcite , we considered dual quantization with parameters q4.2/1.5 , q5.2/1.6 , and q6.2/1.7 for 6-bit , 7-bit , and 8-bit quantizers , respectively .",
    "the q@xmath182 quantizer uses uniform quantization to represent a signed fixed - point number with @xmath109 bits to the left of the radix point for the integer part and @xmath183 bits to the right of the radix point for the fractional part .",
    "for example , a q4.2 quantizer has uniform quantization step size of 0.25 and a range @xmath184 $ ] .",
    "hence , all the quantization methods compared here have the same uniform step size of @xmath172 when quantizing the input llrs .",
    "we know that the saturation level @xmath185 is limited by the quantization step size , because it is desirable to have @xmath186 for all @xmath91 satisfying @xmath187 .",
    "in other words , in the dual quantization scheme , the saturation level has to match the resolution of the quantizer ; otherwise the error - rate performance in both the waterfall region and the error - floor region will be significantly degraded .",
    "based on error - rate simulations using a range of saturation levels for dual quantization methods , we chose the saturation level for @xmath187 to be 5.5 , 7 , and 8 for the 6-bit , 7-bit , and 8-bit dual quantizers , respectively .",
    "as the figure reveals , the @xmath167-bit quasi - uniform quantizer yields the best fer performance in the error - floor region .     and @xmath168 . ]     and @xmath168 .",
    "]    we also evaluated the performance of quasi - uniform quantization in the context of decoding an irregular ldpc code , namely the rate-@xmath161 , @xmath188 ar4ja code .",
    "this protograph - based code has variable node degrees ranging from 1 to 6 .",
    "[ fig_awgn1024spa ] shows the fer obtained with approximated - spa decoding and ( 5 + 1)-bit quasi - uniform quantization , with @xmath165 and @xmath168 .",
    "also shown are the results obtained with the floating - point decoder , as well as those produced by 8-bit uniform quantization with step size @xmath189 .",
    "the @xmath167-bit scheme was superior to both of these alternatives .",
    "the figure also includes two curves taken from @xcite , corresponding to an 8-bit quantized spa decoder with modified vn update rules that were designed specifically for this code , as well as a `` fully - optimized '' 8-bit decoder with more sophisticated vn / cn update rules .",
    "the @xmath167-bit quasi - uniform quantizer s performance surpassed that of the former , but it could not match that of the fully - optimized 8-bit decoder .      figs .",
    "[ fig_awgn640ms ]  [ fig_awgn1024spa ] show that ( @xmath99 + 1)-bit quasi - uniform quantization can provide attractive error - floor performance , sometimes even better than the double - precision floating - point box - plus spa decoder . in generating these results , we observed from the simulation data that the floating - point spa generally requires more iterations to decode a codeword than the quasi - uniform quantized spa , especially in the high snr region .",
    "since the maximum number of iterations was set to 200 in our simulations , the faster convergence of the quasi - uniform quantized spa allowed it to outperform the floating - point spa scheme .",
    "the convergence properties of the quasi - uniform quantized spa decoder appear to derive from its use of non - uniform , exponentially growing step sizes . from the theoretical analysis discussed in section  [ sec : ef ] , we know that the exponential growth rate of correct messages is larger than that of incorrect messages",
    ". we might expect that , with a properly designed quasi - uniform quantizer , the correct messages can reach the higher magnitude level earlier than the incorrect messages , and therefore incorrect messages are more likely to be quantized to lower magnitude levels .",
    "hence , the correct messages can `` overcome '' the incorrect messages more rapidly , allowing the decoder to converge to a codeword after fewer iterations .    in fig .",
    "[ fig_awgn4095spa ] , we explore the effect of limiting the number of iterations in approximated - spa decoding of mackay s rate-0.82 , ( 4095,3358 ) ldpc code . with the maximum number of iterations set to 200 , we show the results for 6-bit and 10-bit uniform quantizers , the @xmath167-bit quasi - uniform quantizer , and the floating - point decoder .",
    "we also compare the performance of @xmath167-bit quasi - uniform quantization and the floating - point decoder when the maximum number is raised to 10k and even further to 100k .    with a limit of 200 iterations ,",
    "this code manifested a high error floor with floating - point spa decoding .",
    "the error floor was lower when the number of iterations could go as high as 10k , and dropped even further when up to 100k iterations were allowed .",
    "however , even in the latter case , the fer was only slightly lower than that found with the quasi - uniform quantizer with no more than 200 iterations .",
    "the performance of the quasi - uniform quantizer continued to improve in raising the limit to 10k and then to 100k .",
    "these results seem to be consistent with the intuition suggested by the theoretical analysis .",
    "trapping sets and other error - prone substructures are known to influence the error - rate performance of ldpc codes with iterative message - passing decoding . in this paper , we have shown that the use of uniform quantization in iterative mp decoding can be a significant factor contributing to the error floor phenomenon in ldpc code performance .",
    "an analysis of iterative mp decoding in an idealized setting suggests that decoder message saturation plays a key role in the occurrence of errors in small trapping sets , leading to observed error floor behaviors . to address this problem",
    ", we proposed a novel quasi - uniform quantization method that effectively extends the dynamic range of the quantizer . without modifying the cn and vn update rules or adding extra stages to standard iterative decoding algorithms , the use of this quantizer was shown to significantly lower the error floors of several well - studied ldpc codes when used with various iterative mp decoding algorithms on the bsc and awgnc .",
    "simulation results confirmed that this new quantization method can significantly reduce the error floors of these codes with essentially no increase in decoding complexity .",
    "assume vn @xmath190 is @xmath11-separated and the corresponding computation tree is @xmath191 .",
    "let @xmath192 be the neighboring degree - one cn of @xmath193 in @xmath12 . from the separation assumption and the assumed correctness of channel messages for vns outside @xmath12 , all descendants of @xmath194 in @xmath191 receive correct initial messages from the bsc .",
    "like the llrs of the bsc outputs , all the initial messages in the decoder , @xmath39 , @xmath195 , have the same magnitude .",
    "denote the subtree starting with cn @xmath194 as @xmath196 . with the vn",
    "/ cn update rules of the ms decoder , we analyze the messages sent from the descendants of @xmath194 in @xmath196 .",
    "first , according to the cn update rule described in , all messages received by a vn from its children cns in @xmath196 must have the same sign as the message received from the channel by this vn , because all the messages passed in @xmath196 are correct .",
    "therefore , the outgoing message from any vn @xmath38 to its parent cn @xmath45 in @xmath196 satisfies the following equality @xmath197    moreover , since the ldpc code considered is variable - regular and all the channel messages from the bsc have the same magnitude , all incoming messages received by a vn from its children cns in @xmath196 must have the same magnitude as well .",
    "therefore , all the messages sent from vns in the same level of the computation tree @xmath196 have the same magnitude .",
    "let @xmath198 be the magnitude of the messages sent by the vns whose shortest path to a leaf vn contains @xmath199 cns in @xmath196 ; in particular , @xmath200 is the magnitude of messages sent by leaf vns , as well as the magnitude of channel inputs .",
    "the discussion above implies that @xmath201 where @xmath202 is the variable node degree .",
    "hence , it can be seen that the magnitudes of messages sent towards the root cn @xmath194 of the computation tree @xmath196 grow exponentially , with @xmath203 as the base , in every upper vn level .",
    "therefore , for @xmath204 , the magnitude of the message sent in the @xmath199-th iteration from @xmath194 to its parent node @xmath193 , the @xmath11-separated root vn of @xmath191 , is greater than @xmath205 .",
    "now , let us look at the subtree of @xmath191 that has as its root a child cn @xmath206 of the root @xmath193 . denote this subtree by @xmath207 .",
    "we assume that the message @xmath208 received by @xmath193 from @xmath209 after @xmath199 iterations has a different sign than the message received from @xmath192 ; otherwise , @xmath193 would already have been corrected . now consider any subtree of @xmath207 that has as its root a vn @xmath210 and contains @xmath211 levels of vns .",
    "we denote such a tree by @xmath212 . if @xmath213 , the subtree @xmath212 must include at least one cn from the set @xmath18 . to see this , recall that the induced subgraph of the trapping set is connected . since there are @xmath5 vns in the trapping set , it follows that any two vns in the trapping set can be connected by a path of length less than @xmath214 .",
    "therefore , for @xmath213 , @xmath212 actually includes all the cns and vns in the induced subgraph of the trapping set , in particular a cn from @xmath18 . of course ,",
    "for most trapping sets , @xmath212 can include a cn from @xmath18 with @xmath211 much smaller than @xmath214 .",
    "now , consider @xmath212 as a `` super - node '' with @xmath215 children vns . since @xmath212 includes a cn from @xmath18",
    ", at least one of these children vns has the property that all of its descendants receive correct messages from the channel .",
    "this means that at least one of the incorrect messages going into the super - node would be canceled out by one or more such correct messages .",
    "so if the output message , @xmath216 , of such a super - node is incorrect , its magnitude satisfies @xmath217 is the largest magnitude of all incoming incorrect messages , and the second term @xmath218 is an upper bound on the sum of the channel input llrs to all of the vns in the @xmath211-level subtree .",
    "note that the leaf vns of @xmath212 are not necessarily the leaf vns of @xmath191 .",
    "thus , we can upper bound the magnitude of the incorrect message sent from @xmath209 to @xmath193 after @xmath199 iterations by @xmath219^{\\lceil l / t \\rceil } \\end{array}\\ ] ] where @xmath220 is the smallest integer greater than or equal to @xmath91 . the upper bound in",
    "is extremely loose , and for most small - size trapping sets , the upper bound is generally less than @xmath221 .",
    "therefore , by taking the logarithms of @xmath198 in and @xmath222 in , respectively , we have @xmath223 and @xmath224\\\\ & < & \\log\\left(|l_0|+|\\bar l_{ch}|\\right ) + \\log\\left[(d_v-1)^{t}-1\\right ] + l\\cdot\\frac{1}{t}\\cdot\\log\\left[(d_v-1)^{t}-1\\right].\\nonumber\\end{aligned}\\ ] ] note that the first term in and the first two terms in are constants and independent of the number of iterations @xmath199 .",
    "since @xmath225 $ ] , if @xmath199 is large enough and there is no limitation imposed on the magnitude of messages , it is easy to see from and that @xmath198 would be greater than @xmath222 multiplied by any constant .",
    "this means that the correct messages coming from outside of the trapping set to vns in @xmath226 through their neighboring cns in @xmath18 will eventually have greater magnitude than the sum of incorrect messages from other neighboring cns , i.e. , @xmath227 .",
    "hence , all the erroneous vns in @xmath226 will be corrected . since , by definition ,",
    "an absolute trapping set does not contain a stopping set , the remaining erroneous vns must form a smaller absolute trapping set .",
    "therefore , we can use the same argument to show that as the number of iterations continues to grow , the correct messages would eventually be large enough to correct all erroneous vns .",
    "now , we show that the proof technique above can be extended to the awgnc . define @xmath228 and @xmath229 to be the minimum and maximum magnitudes , respectively , of the input llrs from the awgnc . in this setting ,",
    "the bounds on @xmath230 and @xmath231 corresponding to those in and take the form @xmath232 and @xmath233   + l\\cdot\\frac{1}{t}\\cdot\\log\\left[(d_v-1)^{t}-1\\right ] .",
    "% \\begin{array}{rl } % \\log|l_l'|<&\\log\\left(|l_{\\max}| + |\\bar l_{ch}| \\right ) + \\log\\left[(d_v-1)^{t}-1\\right]\\\\ % & + l\\cdot\\frac{1}{t}\\cdot\\log\\left[(d_v-1)^{t}-1\\right ] .",
    "% \\end{array } \\vspace{-0.1in}\\ ] ] since the quantities @xmath234 and @xmath235 are constant and do not change as @xmath199 increases , we can conclude , as we did for the bsc , that the correct messages from outside the trapping set will eventually have greater magnitude than the incorrect messages from within the trapping set .",
    "therefore , all of the vns will eventually be correctly decoded .",
    "we first consider ams decoding . referring to the proof of theorem  [ thm_ms ] for the bsc case",
    ", we can replace the quantity @xmath236 in and by @xmath237 , where @xmath58 is the attenuation factor . in practice",
    ", we would always choose @xmath58 such that @xmath237 is greater than 1 ; otherwise , the error - correction performance of the ams decoder would be inferior to that of the ms decoder .",
    "similar reasoning to that used in the proof of the theorem then leads to the desired conclusion . for the awgnc case",
    ", we make the corresponding changes in and , and argue similarly .    for the oms decoder",
    ", the proof follows from the proof of theorem  [ thm_spa ] in appendix  [ appendix_thm_spa ] .",
    "there , we simply replace the quantity @xmath238 by the offset @xmath59 .",
    "the first statement regarding the relationship between the sign and magnitude of the cn messages @xmath84 and @xmath83 is proved in @xcite,@xcite . for completeness , we include here an elementary alternative proof",
    ".    first note that if @xmath239 , then @xmath240 .",
    "now , if @xmath91 and @xmath92 are nonzero and have the same sign ( i.e. , @xmath241 ) , then @xmath242 and hence @xmath243 .",
    "hence , we can see from that the first statement is true if the inequality @xmath244 holds for any positive real values @xmath91 and @xmath92",
    ". without loss of generality , if we assume @xmath245 , then the following inequalities are equivalent @xmath246 since @xmath247 and @xmath248 , the final inequality holds .",
    "hence , the first statement is proved .    to prove the second statement , note that    @xmath249    therefore , @xmath250 .",
    "when @xmath251 , a similar line of reasoning shows that @xmath252 and @xmath253 .",
    "from lemma  [ lemma_ms_vs_spa ] , we know that a cn message in spa decoding has the same sign as the corresponding cn message in ms decoding . moreover , the magnitude of the former is less than or equal to that of the latter . to compute the output for a cn of degree @xmath254",
    ", the box - plus spa uses the pairwise box - plus operation at most @xmath255 times .",
    "hence , the difference between output messages of the spa and the ms algorithm is upper bounded by @xmath256 .    by applying an approach similar to that used in the proof of theorem  [ thm_ms ]",
    ", we can lower bound the magnitude of messages @xmath257 in spa decoding as follows @xmath258    since all input messages to the decoder from the bsc have the same magnitude , if we scale the magnitudes of all initial messages such that @xmath259 then the magnitudes of messages sent towards @xmath194 in the computation tree @xmath191 grow exponentially in the number of iterations , with base @xmath203 . hence , using the same reasoning as in the proof of theorem  [ thm_ms ] , it can be shown that , if @xmath11 is large enough and there is no limit on the magnitudes of messages , the correct messages from outside the trapping set eventually overcome the incorrect messages passed within the trapping set , thereby correcting all erroneous vns in the trapping set .",
    "the extension to the awgnc case is analogous to that used in theorem  [ thm_ms ] .",
    "let @xmath200 denote the minimum magnitude of all input llrs from the awgnc , and linearly scale the magnitudes of all the input messages such that the inequality is satisfied .",
    "then , reasoning as in the proof of the bsc case above , we can show that the magnitudes of correct messages outside the trapping set still grow exponentially with @xmath203 as the base , and eventually they correct all erroneous vns in the trapping set .",
    "the authors would like to thank yang han and william ryan for providing the parity check matrix of the ( 640,192 ) qc ldpc code , brian butler for helpful discussions , and the anonymous reviewers for their numerous and detailed suggestions that helped to improve this paper .    1 r. g. gallager , `` low - density parity - check codes , '' _ ire trans .",
    "inform . theory _",
    ", vol . 8 , pp .",
    "2128 , jan . 1962 .",
    "d. j. mackay and r. m. neal , `` near shannon - limit performance of low - density parity check codes , '' _ electron .",
    "33 , pp . 457458 , mar .",
    "1997 . c.  di , d.  proietti , e.  telatar , t.  richardson , and r.  urbanke , `` finite length analysis of low - density parity - check codes on the binary erasure channel , '' _ ieee trans .",
    "inf . theory _ ,",
    "48 , no . 6 , pp .",
    "15701579 , jun . 2002 .",
    "d. mackay and m. postol , `` weakness of margulis and ramanujan - margulis low - density parity check codes , '' _ electron .",
    "notes theor . comp .",
    "_ , vol . 74 , 2003 .",
    "t.  richardson , `` error - floors of ldpc codes , '' in _ proc .",
    "41st annual allerton conf .",
    "communication , control , and computing _ , monticello , il , oct . 13 , 2003 ,",
    ". 14261435 .",
    "l.  dolecek , z.  zhang , v.  anantharam , m.  wainwright , and b.  nikolic , `` analysis of absorbing sets and fully absorbing sets of array - based ldpc codes , '' _ ieee trans .",
    "inform . theory _",
    "1 , pp . 181201 , jan .",
    "p. o. vontobel and r. koetter , `` graph - cover decoding and finite - length analysis of message - passing iterative decoding of ldpc codes , '' corr , arxiv.org/abs/cs.it/0512078 .",
    "d. divsalar and c. jones , `` protograph based low error floor ldpc coded modulation , '' in _ proc .",
    "ieee military commun .",
    "_ , vol . 1 , atlantic city , nj , oct .",
    "2005 , pp . 378385 .",
    "j. lu and j. m. f. moura , `` structured ldpc codes for high - density recording : large girth and low error floor , '' _ ieee trans .",
    "magnetics _ , vol .",
    "42 , pp . 208213 , feb .",
    "s.  k.  chilappagari , s.  sankaranarayanan , and b.  vasic , `` error floors of ldpc codes on the binary symmetric channel , '' in _ proc .",
    "_ , istanbul , turkey , jun .",
    "2006 , pp . 10891094 .",
    "s. laendner and o. milenkovic , `` algorithmic and combinatorial analysis of trapping sets in structured ldpc codes , '' in _ proc .",
    "wireless networks , commun . , mobile comp .",
    "_ , maui , hi , jun .",
    "2005 , pp . 630635 .",
    "v. savin , `` iterative ldpc decoding using neighborhood reliabilities , '' in _ proc .",
    "ieee ieee int",
    "inform . theory ( isit )",
    "_ , nice , france , jun . 2007 , pp . 221225 .",
    "a. casado , m. griot , and r.  wesel , `` informed dynamic scheduling for belief - propagation decoding of ldpc codes , '' in _ proc .",
    "_ , glasgow , uk , jun .",
    "2007 , pp . 932937 .",
    "e. cavus and b. daneshrad , `` a performance improvement and error floor avoidance technique for belief propagation decoding of ldpc codes , '' in _ proc .",
    ". pers . , indoor and mobile radio comm .",
    "_ , berlin , germany , sept .",
    "2005 , pp . 23862390 .",
    "g.  kyung and c.  wang , `` exhaustive search for small fully absorbing sets and the corresponding low error - floor decoder , '' in _ proc .",
    "inform . theory ( isit )",
    "_ , austin , tx , jul",
    ". 2010 , pp . 739743 .",
    "n. varnica , m.  p.  c.  fossorier , and a.  kavcic , `` augmented belief propagation decoding of low - density parity - check codes , '' _ ieee trans .",
    "13081317 , jul .",
    "y. han and w. e. ryan , `` low - floor decoders for ldpc codes , '' _ ieee trans .",
    "57 , no . 6 , pp",
    ". 16631673 , jun . 2009 .",
    "z. zhang , l. dolecek , b.  nikolic , v.  anantharam , and m.  wainwright , `` lowering ldpc error floors by postprocessing , '' in _ proc .",
    "ieee glob .",
    "_ , new orleans , la , nov .- dec .",
    "2008 , pp . 16 .",
    "j. zhao , f. zarkeshvari , and a. banihashemi , `` on implementation of min - sum algorithm and its modifications for decoding ldpc codes , '' _ ieee trans .",
    "4 , pp . 549554 , apr . 2005 .",
    "t. zhang , z. wang , and k. parhi , `` on finite precision implementation of ldpc codes decoder , '' in _ proc .",
    "ieee iscas _ ,",
    "sydney , australia , may 2001 , pp .",
    "z. zhang , l. dolecek , b.  nikoli , v.  anatharam , and m.  wainwright , `` design of ldpc decoders for improved low error rate performance : quantization and algorithm choices , '' _ ieee trans .",
    "wireless commun .",
    "_ , vol . 8 , no . 11 , pp .",
    "32583268 , nov .",
    "z. zhang , `` design of ldpc decoders for improved low error rate performance , '' ph.d . dissertation ,",
    "of california at berkeley , 2009 .",
    "b. butler and p. siegel , `` error floor approximation for ldpc codes in the awgn channel , '' in _ proc .",
    "49th annual allerton conf .",
    "communication , control , and computing _ , monticello , il , sep .",
    "2011 , pp .",
    "l. dolecek , z. zhang , m.  wainwright , and v.  anatharam .",
    "`` evaluation of the low frame error rate performance of ldpc codes using importance sampling , '' in _ proc .",
    "ieee inform .",
    "theory workshop ( itw ) _ , lake tahoe , ca , sep . 2007 , pp .",
    "b. frey , r. koetter , and a.  vardy , `` signal - space characterization of iterative decoding , '' _ ieee trans .",
    "inform . theory _",
    "766 - 781 , feb . 2001 .",
    "s. k. planjery , d. declercq , s.  k.  chilappagari , and b.  vasic , `` multilevel decoders surpassing belief propagation on the binary symmetric channel , '' in _ proc .",
    "theory ( isit ) _ , austin , tx , jul .",
    "2010 , pp . 769773 .",
    "j. chen , a. dholakia , e. eleftheriou , m. fossorier , and x.  hu , `` reduced - complexity decoding of ldpc codes , '' _ ieee trans .",
    "communications _ , vol .",
    "53 , no . 8 , pp . 12881299 , aug .",
    "2005 . _ ieee standard for floating - point arithmetic _ , ieee standard 754 - 2008 , aug . 29 , 2008 . b. butler and p. siegel , `` numerical problems of belief propagation decoders and solutions , '' in _ proc .",
    "ieee glob . telecom .",
    "_ , anaheim , ca , dec .",
    "2012 , pp . 32013207 .",
    "x. hu , e. eleftheriou , d.  arnold , and a.  dholakia , `` efficient implementations of the sum - product algorithm for decoding ldpc codes , '' in _ proc .",
    "ieee global telecommun .",
    "2 , san antonio , tx , nov .",
    "2001 , pp . 10361036e .",
    "g. richter , g. schmidt , m. bossert , and e.  costa , `` optimization of a reduced - complexity decoding algorithm for ldpc codes by density evolution , '' in _ proc .",
    "1 , seoul , korea , may 2005 , pp . 642646 . j. chen and m. fossorier , `` near optimum universal belief propagation based decoding of low - density parity check codes , '' _ ieee trans .",
    "communications _ , vol .",
    "3 , pp . 406414 , mar .",
    "ryan and s. lin , _ channel codes : classical and modern_. cambridge , u.k . : cambridge univ . press , 2009 .",
    "x. zhang and p. h. siegel , `` efficient algorithms to find all small error - prone substructures in ldpc codes , '' in _ proc .",
    "ieee glob . telecom .",
    "_ , houston , tx , dec .",
    "2011 , pp . 16 .",
    "d. j. c. mackay , _ encyclopedia of sparse graph codes_. [ online ] .",
    "available : http://www.inference.phy.cam.ac.uk/mackay/codes/data.html j. hamkins , `` performance of low - density parity - check coded modulation , '' _ ipn progress report 42 - 184 _ , feb .",
    "[ online ] .",
    "available : http://ipnpr.jpl.nasa.gov/progress_report/42-184/184d.pdf x. zhang and p. siegel , `` quantized min - sum decoders with low error floor for ldpc codes , '' in _ proc .",
    "theory ( isit ) _ , cambridge , ma , july 25 , 2012 , pp .",
    "x. zhang and p. siegel , ``",
    "will the real error floor please stand up ? '' in _ proc .",
    "conf . signal process .",
    "( spcom ) _ , bangalore , india , july 2225 , 2012 , pp ."
  ],
  "abstract_text": [
    "<S> the error floor phenomenon observed with ldpc codes and their graph - based , iterative , message - passing ( mp ) decoders is commonly attributed to the existence of error - prone substructures  variously referred to as near codewords , trapping sets , absorbing sets , or pseudocodewords  in a tanner graph representation of the code . </S>",
    "<S> many approaches have been proposed to lower the error floor by designing new ldpc codes with fewer such substructures or by modifying the decoding algorithm . using a theoretical analysis of iterative mp decoding in an idealized trapping set scenario , </S>",
    "<S> we show that a contributor to the error floors observed in the literature may be the imprecise implementation of decoding algorithms and , in particular , the message quantization rules used . </S>",
    "<S> we then propose a new quantization method  </S>",
    "<S> ( @xmath0-bit quasi - uniform quantization  that efficiently increases the dynamic range of messages , thereby overcoming a limitation of conventional quantization schemes . </S>",
    "<S> finally , we use the quasi - uniform quantizer to decode several ldpc codes that suffer from high error floors with traditional fixed - point decoder implementations . </S>",
    "<S> the performance simulation results provide evidence that the proposed quantization scheme can , for a wide variety of codes , significantly lower error floors with minimal increase in decoder complexity .    low - density parity - check ( ldpc ) codes , iterative message - passing decoding , sum - product algorithm , message quantization , error floors , trapping sets . </S>"
  ]
}