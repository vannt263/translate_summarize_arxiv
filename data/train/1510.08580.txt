{
  "article_text": [
    "distributed computation and estimation recently have received much research attention , e.g. , consensus problems @xcite , distributed estimation @xcite , sensor localization @xcite , and distributed control @xcite . in particular , distributed optimization problems have been extensively investigated in @xcite-@xcite , among which the distributed subgradient or gradient algorithms @xcite-@xcite belong to the primal domain methods while @xcite-@xcite belong to the primal - dual domain methods .",
    "the paper considers a distributed constrained optimization problem , where @xmath1 agents connected in a network collectively minimize the sum of local objective functions @xmath2 subject to a global constraint @xmath3 , where @xmath4 is a convex set and @xmath5 is a convex function in @xmath4 . besides",
    ", @xmath5 and @xmath4 are the local data known to agent @xmath6 and can not be shared with other agents .",
    "this problem is equivalent to a convex optimization problem with single linear coupling constraint and a convex set constraint .",
    "the main contribution of the paper is to propose a distributed primal - dual algorithm with constant step size to solve the constrained optimization problem over the multi - agent network .",
    "the algorithm is derived on the basis of the gradient algorithm for finding saddle points of an augmented lagrange function @xcite . in an iteration",
    "each agent updates its estimate only using the local data and the information derived from the neighboring agents . with appropriately chosen constant step size",
    ", the estimates derived at all agents are shown to reach a consensus at an optimal solution .",
    "besides , it is found that the value of the cost function at the time - averaged estimate converges with rate @xmath0 to the optimal value for the unconstrained problem .",
    "a general constrained convex optimization problem is studied in @xcite , where the constraint sets are assumed to be compact .",
    "the problem in the random case is investigated by @xcite for non - smooth objective functions , meanwhile , the convex sets are assumed to be compact and the global constraint set is required to have a nonempty interior . here",
    ", we study the problem in the deterministic case for smooth objective functions , while imposing weaker assumptions on the convex sets .",
    "when there are no constraints , the problem of the paper becomes the one discussed in @xcite .",
    "the estimates produced by the distributed gradient descent ( dgd ) algorithm with constant step size @xcite converge to a neighborhood of the optimal solution .",
    "in contrast to this , our algorithm gives the accurate estimate . to solve the distributed optimization problems , some continuous - time distributed algorithms",
    "are proposed in @xcite , while here the discrete - time distributed algorithm is investigated .",
    "the estimates generated by the fast distributed gradient algorithms @xcite and by extra @xcite converge to an optimal solution , but in @xcite each cost function is assumed to be convex with gradients being bounded and lipschitz continuous , while extra @xcite only deals with unconstrained problems . though it is shown by @xcite that extra @xcite is also a saddle point method , the augmented lagrange function used in @xcite is different from ours .",
    "besides , the convergence rate @xmath0 derived here for the unconstrained case is a new result .",
    "the primal - dual algorithm proposed in the paper can be seen as an extension of extra @xcite to constrained problems .",
    "the rest of the paper is organized as follows : in section 2 , some preliminary information about graph theory and convex analysis is provided and the problem is formulated . in section 3 , a distributed primal - dual algorithm is proposed for solving the problem , while its convergence is proved in section 4 .",
    "two numerical examples are demonstrated in section 5 , and some concluding remarks are given in section 6 .",
    "we first provide some information about graph theory , convex functions , and convex sets .",
    "then we formulate the distributed constrained optimization problem to be investigated .      consider a network of @xmath1 agents .",
    "the communication relationship among the @xmath1 agents is described by an undirected graph @xmath7 , where @xmath8 is the node set with node @xmath6 representing agent @xmath6 ; @xmath9 is the undirected edge set , and the unordered pair of nodes @xmath10 if and only if agent @xmath6 and agent @xmath11 can exchange information with each other ; @xmath12\\in   \\mathbb{r}^{n\\times n } $ ] is the adjacency matrix of @xmath13 , where @xmath14 if @xmath15 , and @xmath16 , otherwise .",
    "denote by @xmath17 the neighboring agents of agent @xmath6 .",
    "the laplacian matrix of graph @xmath18 is defined as @xmath19 , where @xmath20 . for a given pair @xmath21 , if there exists a sequence of distinct nodes @xmath22 such that @xmath23 @xmath24 , @xmath25 , @xmath26 , then @xmath27 is called the undirected path between @xmath28 and @xmath11 .",
    "we say that @xmath13 is connected if there exists an undirected path between any @xmath29    the following lemma presents some properties of the laplacian matrix @xmath30 corresponding to an undirected graph @xmath31 .",
    "[ lem1 ] @xcite the laplacian matrix @xmath32 of an undirected graph @xmath33 has the following properties :    \\i ) @xmath34 is symmetric and positive semi - definite ;    \\ii ) @xmath35 has a simple zero eigenvalue with corresponding eigenvector equal to @xmath36 , and all other eigenvalues are positive if and only if the graph @xmath33 is connected , where @xmath36 denotes the vector of compatible dimension with all entries equal to 1 .      for a given function @xmath37,$ ]",
    "denote its domain as @xmath38 let @xmath39 be a convex function , and let @xmath40 . for a smooth ( differentiable ) function @xmath39 ,",
    "denote by @xmath41 the gradient of the function @xmath39 at point @xmath42 .",
    "then @xmath43 where @xmath44 denotes the inner product of vectors @xmath42 and @xmath45    for a nonempty convex set @xmath46 and a point @xmath47 , we call the point in @xmath48 that is closest to @xmath42 the projection of @xmath42 on @xmath48 and denote it by @xmath49 .",
    "if @xmath46 is closed , then @xmath49 contains only one element for any @xmath50    consider a convex closed set @xmath46 and a point @xmath51 . define the normal cone to @xmath48 at @xmath42 as @xmath52 .",
    "it is shown in ( * ? ? ?",
    "* lemma 2.38 ) that the following equation holds for any @xmath51 : @xmath53    a set @xmath54 is affine if it contains the lines that pass through any pairs of points @xmath55 with @xmath56 . let @xmath46 be a nonempty convex set .",
    "we say that @xmath47 is a relative interior point of @xmath57 if @xmath58 and there exists an open sphere @xmath59 centered at @xmath42 such that @xmath60 where @xmath61 is the intersection of all affine sets containing @xmath48 .",
    "a point @xmath47 is called the interior point of @xmath62 if @xmath58 and there exists an open sphere @xmath59 centered at @xmath42 such that @xmath63 a pair of vectors @xmath64 and @xmath65 is called a saddle point of the function @xmath66 in @xmath67 if @xmath68 these definitions can be found in @xcite .",
    "consider a network of @xmath1 agents that collectively solve the following constrained optimization problem @xmath69 where @xmath70 is a closed convex set , representing the local constraint set of agent @xmath6 , and @xmath71 is a smooth convex function in @xmath72 , representing the local objective function of agent @xmath6 .",
    "assume that @xmath73 and @xmath4 are privately known to agent @xmath6 .",
    "we assume that there exists at least one finite solution @xmath74 to the problem",
    ". for the problem , denote by @xmath75 the optimal value , and by @xmath76 the optimal solution set .",
    "we use an undirected graph @xmath7 to describe the communication among agents .",
    "let @xmath34 denote the laplacian matrix of the undirected graph @xmath77 .",
    "let us introduce the following conditions for the problem .",
    "a1 : :    @xmath78 has at least one relative interior point .",
    "a2 : :    the undirected graph @xmath77 is connected .",
    "a3 : :    for any @xmath79 , @xmath80    is locally lipschitz continuous on @xmath4 .",
    "we first give an equivalent form of the problem . then define a distributed primal - dual algorithm with constant step size to solve the formulated problem .",
    "[ lem2 ] ( * ? ? ?",
    "* lemma 3 ) if a2 holds , then the problem is equivalent to the following optimization problem @xmath81 where @xmath82 , @xmath83 denotes the cartesian product , @xmath84 denotes the kronecker product , @xmath85 denotes the identity matrix of size @xmath86 , and @xmath87 denotes the vector of compatible dimension with all entries equal to 0 .    [ r1 ]",
    "lemma [ lem2 ] implies that solving the problem is equivalent to solving the problem when the underlying graph is undirected and connected .",
    "if @xmath88 is a solution to the problem , i.e. , @xmath89 , then @xmath90 for some @xmath91 by @xmath92 and @xmath93 .",
    "thus , @xmath94 , and hence @xmath95 is an optimal solution to the problem .",
    "define the lagrange function @xmath96 , where @xmath97 is the lagrange multiplier vector .",
    "then the original problem can be rewritten as @xmath98 , while the dual problem is defined as follows @xmath99    [ lem3 ] assume a1 and a2 hold",
    ". then @xmath100 has at least one saddle point in @xmath101 .",
    "a pair @xmath102 is the primal - dual solution to the problems and if and only if @xmath103 is a saddle point of @xmath100 in @xmath101 .",
    "* proof * : since @xmath104 are continuous and the problem has at least one finite solution , @xmath105 is finite .",
    "moreover , a1 implies that there exists a relative interior @xmath106 of set @xmath48 such that @xmath107 .",
    "then by ( * ? ? ?",
    "* proposition 5.3.3 ) we know that the primal and dual optimal values are equal , i.e. , @xmath108and there exists at least one dual optimal solution .",
    "so , by we conclude that @xmath100 has at least one saddle point in @xmath101 .",
    "since the minimax equality holds , by ( * ? ? ?",
    "* proposition 3.4.1 ) we know that @xmath109 is the primal optimal solution and @xmath110 is the dual optimal solution if and only if @xmath111 is a saddle point of @xmath112 on @xmath113 .",
    "this completes the proof .",
    "@xmath114      denote by @xmath115 the estimate for the optimal solution to the problem given by agent @xmath6 at time @xmath116 , and by @xmath117 the corresponding lagrange multiplier .",
    "they are updated as follows : @xmath118    set @xmath119 @xmath120 and @xmath121 then can be written in the compact form as follows : @xmath122    note that the algorithm actually is the gradient algorithm for finding saddle points of the augmented lagrange function @xmath123 in @xmath101 . by lemma [ lem3 ]",
    "we see that if the algorithm converges to a saddle point of the augmented lagrange function , then it solves the original problem .",
    "convergence properties of the primal - dual method have been studied extensively , see , for example , @xcite . in general",
    ", only a subsequence of the sequence @xmath124 converges to a saddle point of the augmented lagrange function . to obtain the convergence of the whole sequence @xmath124 ,",
    "it is often to assume that the augmented lagrangian function is strictly convex - concave .",
    "however , for the problem studied in the paper , the augmented lagrange function is neither strongly convex in @xmath125 nor strongly concave in @xmath126 .",
    "thus , the standard analysis of gradient methods for finding saddle points is not applicable here .",
    "instead , we apply the lyapunov function method to analyze convergence .",
    "convergence results for the proposed primal - dual algorithm are presented in section 4.1 with the proof given in sections 4.2 and 4.3 .      by a2 from lemma [ lem1 ]",
    "we know that all eigenvalues of @xmath34 are nonnegative real numbers , and zero is a simple eigenvalue .",
    "let us write the eigenvalues of @xmath34 in the non - decreasing order as @xmath127 .",
    "set @xmath128 let @xmath129 be a saddle point of @xmath130 .",
    "define @xmath131 construct a candidate lyapunov function as follows @xmath132    the following theorem shows that the local estimates derived at all agents asymptotically reach a consensus at an optimal solution to the problem .",
    "[ thm1 ] assume a1-a3 hold .",
    "let @xmath133 and @xmath134 be produced by with initial values @xmath135 .",
    "let @xmath103 be a saddle point of @xmath100 in @xmath101 .",
    "assume , in addition , that the constant step size @xmath136 satisfies @xmath137 , where @xmath138 is the local lipschitz constant of @xmath139 in the compact set @xmath140 with @xmath141 defined by @xmath142 where @xmath143 denotes the smallest eigenvalue of a symmetric matrix , and @xmath144 then    \\(i ) @xmath145 monotonously decreases and converges ,    \\(ii ) @xmath146 ,    \\(iii ) @xmath147 for some @xmath148    the problem considered in @xcite is in the same form as the problem , but the local constraint is a hyper - box or hyper - sphere , which is a special case of a1 . unlike the discrete - time algorithm , the continuous - time distributed algorithm is proposed in @xcite .",
    "though the estimates given by all agents converge to the same optimal solution , some intermediate sequence might be unbounded , which makes the algorithm difficult to be implemented .",
    "denote by @xmath149 the time - averaged estimate . in what follows ,",
    "the convergence rate of the algorithm for the case where @xmath150 is shown .",
    "[ thm2 ] assume @xmath151 , a2 , and a3 hold .",
    "let @xmath133 and @xmath134 be produced by the algorithm with initial values @xmath135 .",
    "let @xmath152 be a saddle point of @xmath153 in @xmath101 . if @xmath137 , where @xmath138 is the local lipschitz constant of @xmath139 in the compact set @xmath140 with @xmath154 defined by , then    & ( _ m ) |x_k= , [ con ] + &   ( |x_k ) f^ * + ( x_0 -x^*^2 + _ 0 ^ 2- x_k+1 - x^*^2 + & - _ k+1 ^ 2 ) + ( v(x_0 , _ 1)- v(x_k+1 , _ k+2 ) ) , [ sum0 ] + &  ( |x_k ) f^*- , [ sum00 ]    where @xmath155 .",
    "since @xmath156 by theorem [ thm1](ii ) , @xmath157 is uniformly bounded in @xmath116 by a constant .",
    "then by we see that @xmath158 converges to @xmath87 with rate @xmath159 since theorem [ thm1](i ) implies that @xmath160 by the value of the cost function @xmath161 at @xmath162 converges to the optimal value with rate @xmath159    note that @xmath163 then from theorems [ thm1 ] and [ thm2 ] we see that for small enough @xmath164 depending on the distance between the initial value and the optimal solution , and on the structure of the cost functions in the neighborhood of the optimal solution , the estimates given by all agents finally reach a consensus at an optimal solution .",
    "if @xmath80 is globally lipschitz continuous in set @xmath4 with constant @xmath165 for any @xmath79 , then the results given in theorems [ thm1 ] and [ thm2 ] hold as well for any @xmath136 satisfying @xmath166 but independent of the initial values .",
    "prior to proving theorem [ thm1 ] , we give a lemma that will be used in the proof .",
    "* theorem 2.1.5 ) [ lemma5 ] if @xmath167 is a convex function whose gradient is globally lipschitz continuous with constant @xmath165 , then @xmath168    * proof of theorem [ thm1 ] : * note that @xmath169 we now estimate the last two terms on the right hand side of .    since @xmath152 is a saddle point of @xmath130 , by lemma [ lem3 ]",
    "we see that @xmath109 is an optimal solution to the problem , and hence @xmath170 since @xmath171 is symmetric , by we derive @xmath172 thus , @xmath173    from we derive @xmath174 , and hence by @xmath175 then by multiplying both sides of with @xmath176 , from the rule of the kronecker product @xmath177 we obtain @xmath178    set @xmath179 then by we derive @xmath180 moving the last term at the right - hand side of to the left and subtracting @xmath181 from both sides of we derive the following recursion @xmath182 or in the alternative form @xmath183 then by we derive @xmath184    by the definition of the saddle point we have @xmath185 therefore , @xmath109 minimizes @xmath186 over @xmath48 .",
    "since @xmath187 is convex in @xmath188 for each @xmath189 , by noticing @xmath190 from the optimal condition ( * ? ? ?",
    "* proposition 1.1.8 ) we derive @xmath191    from it follows that @xmath192 , and hence @xmath193 by .",
    "then by the definition of normal cone we obtain @xmath194    then by combining we derive @xmath195 this incorporating with yields @xmath196    since @xmath34 is symmetric , there exists an orthogonal matrix @xmath197 such that @xmath198 and hence @xmath199 then by we know that all possible distinct eigenvalues of @xmath200 are @xmath201 and @xmath202 .",
    "if @xmath203 then @xmath204 , and hence @xmath205 therefore , for any @xmath136 with @xmath206 the matrix @xmath200 is positive semi - definite , and hence by we derive @xmath207    let the constant @xmath136 be such that @xmath137 . in what follows ,",
    "we show that @xmath145 monotonously decreases and @xmath208 by induction .",
    "we first show that @xmath209 and @xmath210 . by the definition of the local lipschitz constant @xmath138",
    ", we know that @xmath211 is lipschitz continuous on the compact set @xmath140 with lipschitz constant @xmath138 . since @xmath212 by the definition of @xmath213 , from lemma [ lemma5 ] we see @xmath214 this incorporating with @xmath215 leads to @xmath216 then from here by we have @xmath217    by we know that all possible distinct eigenvalues of @xmath218 are @xmath219 .",
    "since @xmath220 we have @xmath221 .",
    "thus , @xmath222 is positive definite . as a result , @xmath210 .",
    "then @xmath223 , and hence @xmath209 .",
    "assume that @xmath224 and @xmath225 for @xmath226 .",
    "since holds and @xmath227 , similar to the case @xmath228 , we can show that @xmath229 and @xmath230 .    in summary , by the mathematical induction we conclude that @xmath208 , and @xmath145 monotonously decreases .",
    "since @xmath231 , by the same procedure for deriving we obtain @xmath232 then from here by we derive @xmath233 thus , we conclude that @xmath145 converges since it is nonnegative . summing up both sides of from @xmath234 to @xmath235 we derive @xmath236 then by letting @xmath237 we have @xmath238    consequently , we derive @xmath239 by since @xmath240 is positive definite , and @xmath241 by . by convergence of @xmath145",
    "we conclude that @xmath242 and @xmath243 contain convergent subsequences @xmath244 and @xmath245 to some limits @xmath246 and @xmath247 , respectively .",
    "since @xmath248 and @xmath249 , by noticing that @xmath250 is continuous in @xmath42 and @xmath251 , from we derive @xmath252    then from by we see @xmath253 and hence by the definition of normal cone we conclude @xmath254 since @xmath255 is convex in @xmath188 for each @xmath256 , by we have @xmath257 from we see @xmath258 , and hence by definition we know @xmath259 is a saddle point of @xmath130 in @xmath101 .",
    "thus , by lemma [ lem3 ] we see that @xmath246 is an optimal solution to the problem .",
    "since @xmath260 converges , by setting @xmath261 from @xmath262 and @xmath263 we conclude that @xmath264 converges to zero .",
    "therefore , @xmath265 thus , by remark [ r1 ] we conclude that @xmath147 for some @xmath148 @xmath114      * proof : * ( i ) summing up both sides of from @xmath234 to @xmath235 leads to @xmath266 and hence holds .",
    "\\(ii ) when @xmath267 the equation turns to @xmath268 by we see @xmath269 then by noticing that @xmath240 is positive definite we derive @xmath270    by we have @xmath271 noticing @xmath187 is convex in @xmath188 for any @xmath189 , by we have @xmath272 since @xmath273 is bounded and @xmath109 is an optimal solution to the problem , by we see @xmath274 since @xmath34 is positive semi - definite by lemma [ lem1 ] , from it follows that @xmath275 thus , from here by we derive @xmath276 and hence @xmath277 summing up both sides of this inequality from @xmath234 to @xmath235 for @xmath278 we obtain @xmath279 from here by the convexity of @xmath280 we derive @xmath281    we now give an upper bound for @xmath282 . by",
    "we have @xmath283 thus , @xmath284 , and hence @xmath285 by substituting into we derive @xmath286 then from here by we obtain .",
    "\\(iii ) by we derive @xmath287 , and hence for any dual solution @xmath110 @xmath288 then by we derive .",
    "in this section , we give two numerical examples to demonstrate the obtained theoretic results .",
    "* example 5.1 .",
    "* this example shows that the primal - dual algorithm with constant step size can produce the accurate estimates for the constrained optimization problem , where gradients of the cost functions are only locally lipschitz continuous , and the agents are equipped with different constraint sets . besides , some of the constraint sets are not compact .",
    "consider an undirected network of three agents with edge set @xmath289 .",
    "objective functions for the agents are as follows : @xmath290 while the constraint sets for agents are @xmath291 and @xmath292 denote the optimal solution by @xmath293 , which is at the boundary of the global constraint set .",
    "let @xmath133 and @xmath134 be produced by the algorithm with initial values @xmath294 , and @xmath295 .",
    "denote by @xmath296 and @xmath297 the estimates for @xmath298 and @xmath299 by agent @xmath6 at time @xmath300 respectively .",
    "note that the primal - dual solution pair @xmath111 satisfies and .",
    "define the residual of the optimal condition as @xmath301 the local estimates of all agents and 2-norm of the residual @xmath302 are shown in figure [ a3 ] . from the figure",
    "it is seen that the estimates for all agents converge to the same optimal solution .",
    "* example 5.2 . *",
    "consider a randomly generated undirected network of @xmath303 agents , where each agent has an average degree 4 .",
    "each agent @xmath79 is assigned with a huber loss function @xmath304 with @xmath305 for any @xmath306 @xmath307 is generated according to the uniform distribution over the interval @xmath308 $ ] .",
    "the optimal solution of @xmath309 is denoted by @xmath95 .",
    "we compare the primal - dual algorithm with the existing ones by this example .",
    "set @xmath310 denote by @xmath311 the estimate for @xmath95 given by agent @xmath6 at time @xmath116 with the initial values @xmath312 .",
    "the simulation is for the case where the communication topology is shown in figure [ figure a1 ] , and the entries of the adjacency matrix @xmath313 are metropolis wights @xcite . with this @xmath314",
    "we carry out the simulations for the primal - dual algorithm with @xmath315 , for the dgd algorithm @xcite , for extra @xcite with constant step size @xmath136 , and for the distributed nesterov gradient ( d - ng ) algorithm in @xcite .",
    "the dgd algorithm runs separately for three cases : constant step size @xmath136 , diminishing step sizes @xmath316 , and @xmath317 . the d - ng algorithm , i.e. , equations ( 2)-(4 ) in @xcite , is run with @xmath318 and @xmath319    denote by @xmath320 the normalized relative error , where @xmath321 .",
    "the numerical results are shown in figure [ figure a2 ] , where the horizontal axis denotes the number of iterations @xmath116 and the vertical axis denotes @xmath322 . from the figure",
    "it is seen that the dgd algorithms with decreasing step sizes converge to the optimal solution but the rate of convergence are the slowest in comparisons with other methods .",
    "it is also seen that dgd with constant step size quickly approaches to the neighborhood of the optimal solution .",
    "the estimates generated by d - ng @xcite , by the algorithm , and by extra @xcite all converge to the optimal solution .",
    "besides , the algorithm brings a satisfactory convergence rate for the unconstrained problem as well .",
    "in the paper , a distributed primal - dual algorithm is proposed for multiple agents in a network to minimize the sum of individual cost functions subject to a global constraint , which is the intersection of the local constraints .",
    "the proposed algorithm with constant step size makes the estimates of all agents converge to the same optimal solution and achieve the convergence rate @xmath0 when there is no constraint .",
    "the effectiveness and the priority of the proposed algorithm have been demonstrated by two numerical examples .",
    "s. kar , j. m. f. moura , and k. ramanan ,  distributed parameter estimation in sensor networks : nonlinear observation models and imperfect communication , \" _ ieee trans .",
    "inf . theory _ , vol .",
    "58 , no . 6 , pp . 35753605 , 2012 .",
    "u. a. khan , s. kar , and j. m. f. moura ,  distributed sensor localization in random environments using minimal number of anchor nodes , \" _ ieee trans .",
    "singnal processing _ , vol .",
    "5 , pp . 20002016 , 2009 ."
  ],
  "abstract_text": [
    "<S> the paper studies a distributed constrained optimization problem , where multiple agents connected in a network collectively minimize the sum of individual objective functions subject to a global constraint being an intersection of the local constraint sets assigned to the agents . based on the augmented lagrange method , a distributed primal - dual algorithm with a projection operation included is proposed to solve the problem . </S>",
    "<S> it is shown that with appropriately chosen constant step size , the local estimates derived at all agents asymptotically reach a consensus at an optimal solution . </S>",
    "<S> in addition , the value of the cost function at the time - averaged estimate converges with rate @xmath0 to the optimal value for the unconstrained problem . by these properties </S>",
    "<S> the proposed primal - dual algorithm is distinguished from the existing algorithms for distributed constrained optimization . </S>",
    "<S> the theoretical analysis is justified by numerical simulations .    distributed constrained optimization , primal - dual algorithm , augmented lagrange method , multi - agent network . </S>"
  ]
}