{
  "article_text": [
    "social media has become an important tool for public engagement .",
    "businesses are interested in engaging with potential buyers on platforms such as facebook or their company news pages ; bloggers are publishers are interested in increasing their follower and reader base by writing captivating articles that prompt high levels of user feedback ( in the form of likes , comments , etc . ) .",
    "such groups will inevitably need to understand the types of content that is likely to elicit the most user engagement as well as any underlying patterns in the data such as temporal trends .",
    "specifically , we focus on predicting the number of feedbacks ( i.e. , comments ) that a blog post is expected to receive . given a set of blog documents that appeared in the past , for which the number and time stamp received",
    ", the task is to predict how many feedbacks recently published blog - entries will receive in the next @xmath2 hours .    a first challenge in answering this question is how to effectively pre - process the data .",
    "despite offering a rich source of information , such data sets are usually noisy , high - dimensional , with many correlated features . in this paper , we focus on two unsupervised learning approaches for pre - processing : the traditional method of principal component analysis ( pca ) , and a more recently - developed method from deep - learning , the sparse autoencoder .",
    "these pre - processing methods are generally used to reduce dimensionality , eliminate correlations among variables ( since there may be a number of irrelevant features in the data set ) , decrease the computation time , and extract good features for subsequent analyses .",
    "pca linearly transforms the original inputs into new uncorrelated features .",
    "the sparse autoencoder is trained to reconstruct its own inputs through a non - recurrent neural network , and it creates as output new features from non - linear combinations of the old ones .",
    "we compare the effects of these two methods on two prediction models of linear regression and regression trees in the feedback prediction task .    the rest of the paper proceeds as follows .",
    "section @xmath3 includes a short review of related literature .",
    "section @xmath4 describes the data set used .",
    "section @xmath5 discusses the unsupervised feature learning methods of pca and sparse autoencoder .",
    "section @xmath6 gives comparative results from different models for the unprocessed data , and the pre - processed data using pca and sparse autoencoder .",
    "section @xmath7 concludes and opens future research directions .",
    "section @xmath8 acknowledges help .",
    "al . @xmath9 $ ] compare pca , kernel pca ( kpca ) , and independent component analysis ( ica ) as applied to support vector machine ( svm ) for feature extraction to three data sets ( sunspot data , satan fe data set a , and five real futures contracts ) .",
    "svm by feature extraction using pca , kpca or ica can achieve better generalization performance than that without feature extraction , and that kpca and ica perform better than pca on all the studied data sets , with the best performance in kpca .",
    "the reason lies in the fact that kpca and ica can explore higher order information of the original inputs than pca .",
    "based on these results , we expect the ( sparse ) autoencoder to perform better than pca as it is also a non - linear combination of features .",
    "buza @xmath10 $ ] uses the same data set as we do and compares a variety of models to predict the number of future feedbacks for a blog .",
    "they consider two performance metrics : area under curve explained ( auc ) , and the number of blog pages that were predicted to have the largest number of feedbacks out of the top ten blog pages that had the highest number of feedbacks in reality .",
    "buza @xmath10 $ ] examines various models : a multilayer perceptron model , rbf - networks , regression trees ( rep - tree , m5p - tree ) , nearest neighbor models , multivariate linear regression and bagging , however do not pre - process the data .",
    "he finds that m5p trees and rep trees seem to work very well both in terms of accuracy and runtime , averaging @xmath11 feedbacks and @xmath12 for the examined models .",
    "the neural network model is competitive to the regression trees .",
    "the best models are the linear model ( auc @xmath13 and @xmath14 hits ) and neural network ( auc @xmath15 , @xmath16 hits ) .",
    "bagging does not statistically improve the performance of mlps and rbf - network both in terms of comments and auc ( improved the absolute performance , but not taking into account standard deviations ) .",
    "we use the same data set as in buza @xmath10 $ ] , but approach the problem differently .",
    "specifically , we use unsupervised feature learning techniques to pre - process the data before running prediction models .",
    "this data set is made available from the uci machine learning repository , and comprises a total of @xmath17 crawled blog pages .",
    "the prediction task is to predict the number of comments for a blog post in the upcoming @xmath18 hours .",
    "the processed data has a total of @xmath19 features ( without the target variable , i.e. , number of feedbacks).$ ] . ]",
    "the blog documents ( instances ) are transformed into vectors to be inputted to the machine learning algorithm .",
    "this collection corresponds approximately @xmath7 gb of plain html document ( i.e. , without images ) .",
    "the following features are extracted from each document :    * basic features : number of links and feedbacks in the previous @xmath20 hours ; number of links and feedbacks in the previous @xmath21 hours ; how the number of links and feedbacks increased / decreased since to the publication date of the blog ; number of links and feedbacks in the first @xmath20 hours after the publication of the document .",
    "* textual features : the most discriminative bag - of - words features . * weekday features : binary indicator features that describe on which day of the week the main text of the document was published and for which day of the week the prediction has to be calculated . *",
    "parent features : a document dp is treated as a patent of document d , if d is a reply to dp , i.e. , there is a trackback link on dp that points to d ; parent features are the number of parents , minimum , maximum and average number of feedbacks that the parents received .",
    "prior to implementing any unsupervised feature learning method , we first eliminate four variables which contain only zeros , resulting in @xmath22 predictor variables ( i.e. , features ) and one outcome variable . out of these variables , @xmath23 are continuous , and the remainder are binary .",
    "we center and scale all non - binary variables to have zero mean and unit standard deviation .",
    "this step prevents any one variable from dominating the variance of the data set simply due to taking values in a larger range",
    ". we also center and scale the outcome variable for testing later .",
    "histograms of the number of feedbacks received for both train and test sets are included in figures [ hist_train ] and [ hist_test ] .",
    "we observe that most values of the centered and scaled response variable are concentrated near zero for both training and test sets .",
    "this prevents any one point from potentially skewing the estimation results later on .",
    "pca is a powerful technique for extracting structure from high - dimensional data sets .",
    "we implement pca in order to capture the intrinsic variability in the data and reduce the dimensionality of the data set . an important decision in pca analysis",
    "is to determine the optimal number of principal components to use , since the data will be projected onto this new basis of principal components before any subsequent analysis . to determine the optimal number of components , we implement two gap - style tests as in @xmath24 $ ] . in both tests ,",
    "the gap formula has the same form    @xmath25 - log(w_k),\\ ] ]    where @xmath26 is the logarithm of some objective function corresponding to the first @xmath27 principal components and @xmath28 $ ] is calculated in the similar way with the expectation taken over uniform samples from the smallest subspace containing the original data .      in the first approach",
    ", we define @xmath29 to be the reconstruction error in using the first @xmath27 principal components to approximate the data .",
    "specifically , let @xmath30 , @xmath31 be the projection of the @xmath32 data point onto the rank @xmath27 principal component approximation .",
    "then @xmath29 is defined by @xmath33    the steps for this gap - style test are as follows .    * after calculating the @xmath22 principal components , for each @xmath34 , we determine @xmath29 as above and take their log values to obtain @xmath22 values , @xmath35 s . *",
    "we generate @xmath36 samples uniformly from the smallest subspace containing the original data and find the principal components for this newly generated matrix .",
    "as above , for each @xmath34 , we calculate @xmath29 and take their log values .",
    "so for each @xmath27 , we obtain @xmath36 values ; we take their average to get an estimate for @xmath37 $ ] and calculate their standard deviation .",
    "we obtain @xmath22 such values , and call them @xmath38 $ ] for @xmath34 . *",
    "calculate the adjusted standard error according to the formula @xmath39",
    "= \\sqrt{1 + \\frac{1}{b } } \\times sd[k].\\ ] ]    we then choose the optimal @xmath27 to be the smallest value of @xmath27 such that @xmath40 \\geq gap[k + 1 ] - se[k + 1].\\ ] ]    figure [ gap_re ] shows a plot of the gap statistic versus the number of principal components for ( a ) the first fifty principal components , and ( b ) the total number of principal components .",
    "the optimal number of principal components that minimizes the gap is @xmath41 .      in this approach , we define @xmath29 to be the variation explained by the first @xmath27 principal components .",
    "specifically , @xmath29 is the ratio of the sum of the first @xmath27 principal components variances and the sum of the total variance of all principal components .",
    "mathematically speaking , @xmath42    where @xmath43 is the @xmath32 principal component s variance .",
    "the steps in this gap test are the same as those for the reconstruction error approach with the exception of @xmath29 , which we have described above .",
    "we then choose the optimal @xmath27 to be the smallest value of @xmath27 such that @xmath40 \\leq gap[k + 1 ] - se[k + 1].\\ ] ]    we note the difference in direction of the two conditions in defining the optimal @xmath27 between the two gap - style tests .",
    "this is as expected because for the gap test using reconstruction error , we want to minimize @xmath29 while for the gap test using explained variation we want to maximize the corresponding @xmath29 .",
    "however different , this gap - style test gives the same optimal number of components as the other one which is @xmath41 .",
    "figure [ gap_ev ] shows the corresponding plot of the gap statistic versus the number of principal components for ( a ) the first fifty principal components , and ( b ) the total number of principal components .      the autoencoder is based on the concept of sparse coding proposed in a seminal paper by olshausen et al .",
    "@xmath44 $ ] . in this paper",
    ", we implement a @xmath4-layer autoencoder ( figure [ autoencoder_diagram ] ) in order to learn a compressed representation ( encoding ) of the features .",
    "each ` neuron ' ( circle ) represents a computational unit that takes as input @xmath45 , @xmath46 , ...",
    "@xmath47 ( and a  @xmath48 \" intercept term , called a bias unit ) , and outputs    @xmath49    where @xmath50 is called the transfer ( activation ) function .",
    "we choose @xmath51 to be the hyperbolic tangent ( tanh function ) .",
    "the tanh function was chosen instead of the sigmoid function since its output range , [ -1,1 ] , more closely approximates the range of our predictor variable than the sigmoid function ( range is [ 0,1 ] ) .",
    "the tanh activation function is given below :",
    "@xmath52    the leftmost layer of the network is called the input layer , and the rightmost layer the output layer .",
    "the middle layer of nodes is called the hidden layer since its values are not observed in the training set .",
    "-layer autoencoder architecture .",
    "]    the autoencoder tries to learn a function @xmath53 .",
    "in other words , it is trying to learn an approximation to the identity function , so as to output @xmath54 that is similar to @xmath55 .",
    "the sparse autoencoder is the autoencoder with the sparsity constraint added to the objective function . in other words ,",
    "the objective function of the sparse autoencoder is given by the reconstruction error with regularization :    @xmath56    where @xmath57 is the number of training samples , @xmath58 is the number of layers ( @xmath4 in our case ) , and @xmath59 is the number of units in layers @xmath60 . the first term , @xmath61 is an average sum - of - squares error term .",
    "the second term is a regularization ( @xmath62 ) term that tends to decrease the magnitude of the weights , and helps prevent overfitting .",
    "the weight decay parameter @xmath63 controls the relative importance of the terms .",
    "the sparsity parameter @xmath64 controls how sparse the autoencoder is .",
    "this neural network is then trained using a back - propagation algorithm , where the objective function is minimized using batch gradient descent .",
    "although the identity function seems like a trivial function to be trying to learn , by placing constraints on the network , such as the weight decay , the sparsity parameter , and the number of hidden units , it is possible to discover interesting structure about the data .",
    "when we use a few hidden units , the network is forced to learn a compressed representation of the input .",
    "if there is some structure in the data , for example , if some of the input features are correlated , the algorithm will be able to discover some of those correlations , so that the ( sparse ) autoencoder often ends up learning a low - dimensional representation similar to pca .",
    "in order to determine the optimal values of the hyper - parameters and the number of hidden units , we split the data into training , validation and test sets and perform a grid search over the parameter space of the number of units ( @xmath65 $ ] ) and the weight decay @xmath63 ( @xmath66 $ ] ) while using the default value for @xmath64 of @xmath67 .",
    "the reason for not cross - validating over @xmath64 and for using small sets of possible values for @xmath63 and the number of hidden units is the high computational cost . for each value of @xmath63",
    ", we use @xmath6-fold cross - validation on the training data to select the optimal number of hidden units .",
    "we then calculate the root mean squared error ( rmse ) on the validation data and choose the value for the weight decay corresponding to the smallest rmse .",
    "this way , we obtain the optimal weight decay and the optimal number of units in the hidden layer .",
    "the optimal weight decay was found to be @xmath68 .",
    "for this @xmath63 , we plot the rmse over the number of units in the hidden layer in figure [ rmse_val ] and obtain @xmath6 as the optimal number of units .        with the determined optimal values of the weight decay , the sparsity parameter , and the number of hidden units",
    ", we can also determine the estimated weight matrices and biases .",
    "we then fit the training and test data through these weights and biases to obtain the processed data for subsequent analysis .",
    "we use two models to predict the number of feedbacks : linear regression ( a linear model ) and regression tree ( a non - linear model ) .",
    "we compare the test rmse achieved using the output from pca and sparse autoencoder for each model . as a baseline",
    ", we use the centered and scaled data as input to the models . for pca , we use the projected data onto the @xmath41 components , and for the sparse autoencoder , we use the optimal number of @xmath6 units in the hidden layer , as found by cross - validation .",
    "results are summarized in table @xmath69 .",
    "table @xmath69 : rmse on test set of different models and methods .    [ cols=\"<,^,^\",options=\"header \" , ]     for linear regression",
    ", pca achieves an @xmath70 improvement compared to the baseline model while the sparse autoencoder achieves a @xmath0 improvement . for the regression tree model",
    ", pca achieves a @xmath1 improvement compared to the baseline model .",
    "the sparse autoencoder , however , performs worse than the baseline model ( an increase in rmse from @xmath71 to @xmath72 ) .",
    "unsupervised feature learning in the pre - processing step hence generally improves the prediction accuracy . taking into account the small range of the outcome variable after scaling and centering , the improvements are certainly significant .",
    "for the linear regression model the sparse autoencoder outperforms pca .",
    "this is likely because the sparse autoencoder solves many of the drawbacks of pca : pca only allows linear combinations of the features , restricting the output to orthogonal vectors in feature space that minimize the reconstruction error ; pca also assumes points are multivariate gaussian , which is most likely not true in many applications , including ours .",
    "the sparse autoencoder is able to learn much more complex , non - linear representations of the data and thus achieves much better accuracy .",
    "an interesting pattern can be observed in the interactions between the linearity and non - linearity of the models and the feature learning methods .",
    "the non - linear feature selection method ( sparse autoencoder ) achieves significant improvement in rmse for the linear model ( linear regression ) , while the linear feature selection method ( pca ) performs best for the non - linear model ( regression tree ) . combining the non - linear regression tree model with the non - linear sparse autoencoder",
    ", however , leads to worse results than the baseline .",
    "this is likely because regression trees have a top - down construction , splitting at every step on the variable that best divides the data .",
    "the sparse autoencoder returns non - linear combinations of features , making it difficult for regression trees to anticipate .",
    "in addition , each tree samples a subset of variables to split on while sparse autoencoders are known to be sensitive to parameter selection , and hence are not optimized to predict on subsets of the predictor variables , leading to unstable results and poorer performance .",
    "we show empirically that using unsupervised feature learning to pre - process the data can improve the feedback prediction accuracy significantly .",
    "these results should be of interest to businesses and publishers in pre - screening or editing their social - media posts prior to publicizing so as to estimate the level of engagement they would be expected to achieve . for instance , an automatic editor may flag posts with low predicted user engagement and draw attention to the writer that revisions may be needed .",
    "two directions of future work are immediately obvious .",
    "first , we can extend the work by trying other unsupervised feature learning methods such as ica and kernel pca in order to better understand how these methods ( linear versus non linear ) interact with the model type and to what extent our observations can be generalized .",
    "second , we would be interested in investigating whether results can be further improved by using different transfer functions and additional hidden layers in the sparse autoencoder ( i.e. , stacked sparse autoencoder ) in order to better capture the time - series aspects of the data set",
    ". we may also want to compare the feature learning methods on other models such as svm , boosting , random forests , etc . + * acknowledgements * : we thank robert tibshirani for helpful comments ."
  ],
  "abstract_text": [
    "<S> in this paper , we investigate the effectiveness of unsupervised feature learning techniques in predicting user engagement on social media . specifically , we compare two methods to predict the number of feedbacks ( i.e. , comments ) that a blog post is likely to receive . </S>",
    "<S> we compare principal component analysis ( pca ) and sparse autoencoder to a baseline method where the data are only centered and scaled , on each of two models : linear regression and regression tree . </S>",
    "<S> we find that unsupervised learning techniques significantly improve the prediction accuracy on both models . for the linear regression model , sparse autoencoder achieves the best result , with </S>",
    "<S> an improvement in the root mean squared error ( rmse ) on the test set of @xmath0 over the baseline method . for the regression tree model , pca achieves the best result , with an improvement in rmse of @xmath1 over the baseline . </S>"
  ]
}