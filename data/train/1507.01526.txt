{
  "article_text": [
    "long short - term memory ( lstm ) networks are recurrent neural networks equipped with a special gating mechanism that controls access to memory cells  @xcite . since the gates can prevent the rest of the network from modifying the contents of the memory cells for multiple time steps , lstm networks preserve signals and propagate errors for much longer than ordinary recurrent neural networks . by independently reading , writing and erasing content from the memory cells , the gates can also learn to attend to specific parts of the input signals and ignore other parts .",
    "these properties allow lstm networks to process data with complex and separated interdependencies and to excel in a range of sequence learning domains such as speech recognition @xcite , offline hand - writing recognition @xcite , machine translation @xcite and image - to - caption generation @xcite .",
    "even for non - sequential data , the recent success of deep networks has shown that long chains of sequential computation are key to finding and exploiting complex patterns .",
    "deep networks suffer from exactly the same problems as recurrent networks applied to long sequences : namely that information from past computations rapidly attenuates as it progresses through the chain  the _ vanishing gradient problem _ @xcite  and that each layer can not dynamically select or ignore its inputs .",
    "it therefore seems attractive to generalise the advantages of lstm to deep computation .",
    "we extend lstm cells to deep networks within a unified architecture .",
    "we introduce grid lstm , a network that is arranged in a grid of one or more dimensions .",
    "the network has lstm cells along any or all of the dimensions of the grid .",
    "the depth dimension is treated like the other dimensions and also uses lstm cells to communicate directly from one layer to the next .",
    "since the number @xmath0 of dimensions in the grid can easily be 2 or more , we propose a novel , robust way for modulating the _",
    "n_-way communication across the lstm cells .",
    "_ n_-dimensional grid lstm ( _ n_-lstm for short ) can naturally be applied as feed - forward networks as well as recurrent ones .",
    "one - dimensional grid lstm corresponds to a feed - forward network that uses lstm cells in place of transfer functions such as @xmath1 and relu @xcite .",
    "these networks are related to highway networks @xcite where a gated transfer function is used to successfully train feed - forward networks with up to 900 layers of depth .",
    "grid lstm with two dimensions is analogous to the _ stacked lstm _ , but it adds cells along the depth dimension too .",
    "grid lstm with three or more dimensions is analogous to _ multidimensional lstm _",
    "@xcite , but differs from it not just by having the cells along the depth dimension , but also by using the proposed mechanism for modulating the _",
    "n_-way interaction that is not prone to the instability present in multidimesional lstm .",
    "we study some of the learning properties of grid lstm in various algorithmic tasks .",
    "we compare the performance of two - dimensional grid lstm to stacked lstm on computing the addition of two 15-digit integers without curriculum learning and on memorizing sequences of numbers @xcite .",
    "we find that in these settings having cells along the depth dimension is more effective than not having them ; similarly , tying the weights across the layers is also more effective than untying the weights , despite the reduced number of parameters .",
    "we also apply grid lstm to two empirical tasks .",
    "the architecture achieves 1.47 bits - per - character in the 100 m characters wikipedia dataset @xcite outperforming other neural networks .",
    "secondly , we use grid lstm to define a novel neural translation model that _ re - encodes _ the source sentence based on the target words generated up to that point .",
    "the network outperforms the reference phrase - based cdec system @xcite on the iwslt btec chinese - to - ensligh translation task .",
    "the appendix contains additional results for grid lstm on learning parity functions and classifying mnist images .",
    "the outline of the paper is as follows . in sect .  2 we describe standard lstm networks that comprise the background .",
    "in sect .",
    "3 we define the grid lstm architecture . in sect .",
    "4 we consider the six experiments and we conclude in sect .  5 .",
    "and @xmath2 dimensions .",
    "the dashed lines indicate identity transformations .",
    "the standard lstm block does not have a memory vector in the vertical dimension ; by contrast , the 2d grid lstm block has the memory vector @xmath3 applied along the vertical dimension . ]",
    "we begin by describing the standard lstm recurrent neural network and the derived _ stacked _ and _ multidimensional _ lstm networks ; some aspects of the networks motivate the grid lstm .",
    "the lstm network processes a sequence of input and target pairs @xmath4 . for each pair",
    "@xmath5 the lstm network takes the new input @xmath6 and produces an estimate for the target @xmath7 given all the previous inputs @xmath8 .",
    "the past inputs @xmath9 determine the state of the network that comprises a _ hidden _",
    "vector @xmath10 and a _ memory _ vector @xmath11 .",
    "the computation at each step is defined as follows @xcite : @xmath12 where @xmath13 is the logistic sigmoid function , @xmath14 in @xmath15 are the recurrent weight matrices of the network and @xmath16 is the concatenation of the new input @xmath6 , transformed by a projection matrix @xmath17 , and the previous hidden vector @xmath18 : @xmath19 the computation outputs new hidden and memory vectors @xmath20 and @xmath21 that comprise the next state of the network .",
    "the estimate for the target is then computed in terms of the hidden vector @xmath20 .",
    "we use the functional @xmath22 as shorthand for eq .  1 as follows :",
    "@xmath23 where @xmath24 concatenates the four weight matrices @xmath25 .",
    "one aspect of lstm networks is the role of the gates @xmath26 and @xmath27 .",
    "the forget gate @xmath28 can delete parts of the previous memory vector @xmath29 whereas the gate @xmath27 can write new content to the new memory @xmath30 modulated by the input gate @xmath31 .",
    "the output gate controls what is then read from the new memory @xmath30 onto the hidden vector @xmath32 .",
    "the mechanism has two important learning properties .",
    "each memory vector is obtained by a _ linear _",
    "transformation of the previous memory vector and the gates ; this ensures that the forward signals from one step to the other are not repeatedly squashed by a non - linearity such as @xmath1 and that the backward error signals do not decay sharply at each step , an issue known as the vanishing gradient problem @xcite .",
    "the mechanism also acts as a memory and implicit attention system , whereby the signal from some input @xmath6 can be written to the memory vector and attended to in parts across multiple steps by being retrieved one part at a time .",
    "a model that is closely related to the standard lstm network is stacked lstm @xcite .",
    "stacked lstm adds capacity by stacking lstm layers on top of each other . the output hidden vector @xmath32 in eq .",
    "1 from the lstm below is taken as the input to the lstm above in place of @xmath33 .",
    "the stacked lstm is depicted in fig .  2 .",
    "note that although the lstm cells are present along the sequential computation of each lstm network , they are not present in the vertical computation from one layer to the next .",
    "another related model is multidimensional lstm @xcite . here",
    "the inputs are not arranged in a sequence , but in a @xmath0-dimensional grid , such as the two - dimensional grid of pixels in an image . at each input @xmath34 in the array the network receives @xmath0 hidden vectors @xmath35 and @xmath0 memory vectors @xmath36 and computes a hidden vector @xmath18 and a memory vector @xmath37 that are passed as the next state for each of the @xmath0 dimensions .",
    "the network concatenates the transformed input @xmath38 and the @xmath0 hidden vectors @xmath35 into a vector @xmath39 and as in eq .",
    "1 computes @xmath40 and @xmath27 , as well as @xmath0 forget gates @xmath41 .",
    "these gates are then used to compute the memory vector as follows : @xmath42 as the number of paths in a grid grows combinatorially with the size of each dimension and the total number of dimensions @xmath0 , the values in @xmath37 can grow at the same rate due to the unconstrained summation in eq .",
    "this can cause instability for large grids , and adding cells along the depth dimension increases @xmath0 and exacerbates the problem .",
    "this motivates the simple alternate way of computing the output memory vectors in the grid lstm .",
    "grid lstm deploys cells along any or all of the dimensions including the depth of the network . in the context of predicting a sequence ,",
    "the grid lstm has cells along two dimensions , the temporal one of the sequence itself and the vertical one along the depth . to modulate the interaction of the cells in the two dimensions ,",
    "the grid lstm proposes a simple mechanism where the values in the cells can not grow combinatorially as in eq .  4 .",
    "in this section we describe the multidimensional _ blocks _ and the way in which they are combined to form a grid lstm .              as in multidimensional lstm ,",
    "n_-dimensional block in a grid lstm receives as input @xmath0 hidden vectors @xmath35 and @xmath0 memory vectors @xmath36 . unlike the multidimensional case , the block outputs @xmath0 hidden vectors @xmath43 and @xmath0 memory vectors @xmath44 that are all distinct .",
    "the computation is simple and proceeds as follows .",
    "the model first concatenates the input hidden vectors from the _",
    "n _ dimensions : @xmath45 then the block computes @xmath0 transforms @xmath22 , one for each dimension , obtaining the desired output hidden and memory vectors : @xmath46 each transform has distinct weight matrices @xmath47 in @xmath48 and applies the standard lstm mechanism across the respective dimension .",
    "note how the vector @xmath39 that contains all the input hidden vectors is shared across the transforms , whereas the input memory vectors affect the @xmath0-way interaction but are not directly combined . _",
    "n_-dimensional blocks can naturally be arranged in a _",
    "n_-dimensional grid forming a grid lstm . as for a block ,",
    "the grid has @xmath0 sides with _ incoming _ hidden and memory vectors and @xmath0 sides with _ outgoing _ hidden and memory vectors .",
    "note that a block does not receive a separate data representation .",
    "a data point is projected into the network via a pair of input hidden and memory vectors along one of the sides of the grid .      in a _",
    "n_-dimensional block the transforms for all dimensions are computed in parallel .",
    "but it can be useful for a dimension to know the outputs of the transforms from the other dimensions , especially if the outgoing vectors from that dimension will be used to estimate the target .",
    "for instance , to prioritize the first dimension of the network , the block first computes the @xmath49 transforms for the other dimensions obtaining the output hidden vectors @xmath50 .",
    "then the block concatenates these output hidden vectors and the input hidden vector @xmath51 for the first dimension into a new vector @xmath52 as follows : @xmath53 the vector is then used in the final transform to obtain the prioritized output hidden and memory vectors @xmath54 and @xmath55 .      in grid lstm networks that have only a few blocks along a given dimension in the grid",
    ", it can be useful to just have regular connections along that dimension without the use of cells .",
    "this can be naturally accomplished inside the block by using for that dimension in eq .",
    "6 a simple transformation with a nonlinear activation function instead of the transform @xmath22 . given a weight matrix @xmath56 , for the first dimension this looks as follows : @xmath57 where @xmath58 is a standard nonlinear transfer function or simply the identity .",
    "this allows us to see how , modulo the differences in the mechanism inside the blocks , grid lstm networks generalize the models in sect .  2 .",
    "a 2d grid lstm applied to temporal sequences with cells in the temporal dimension but not in the vertical depth dimension , corresponds to the stacked lstm .",
    "likewise , the 3d grid lstm without cells along the depth corresponds to multidimensional lstm , stacked with one or more layers .      if we picture a _",
    "n_-dimensional block as in fig .  1",
    ", we see that _ n _ of the sides of the block have input vectors associated with them and the other _",
    "n _ sides have output vectors .",
    "as the blocks are arranged in a grid , this separation extends to the grid as a whole ; each side of the grid has either input or output vectors associated with it . in certain tasks that have inputs of different types",
    ", a model can exploit this separation by projecting each type of input on a different side of the grid .",
    "the mechanism inside the blocks ensures that the hidden and memory vectors from the different sides will interact closely without being conflated .",
    "this is the case in the neural translation model introduced in sect .  4 where source words and target words are projected on two different sides of a grid lstm .",
    "sharing of weight matrices can be specified along any dimension in a grid lstm and it can be useful to induce invariance in the computation along that dimension . as in the translation and image models ,",
    "if multiple sides of a grid need to share weights , capacity can be added to the model by introducing into the grid a new dimension without sharing of weights .",
    "if the weights are shared along all dimensions including the depth , we refer to the model as a _ tied @xmath0-lstm_.",
    "we first experiment with @xmath59-lstm networks on learning to sum two 15-digit integers .",
    "the problem formulation is similar to that in @xcite , where each number is given to the network one digit at a time and the result is also predicted one digit at a time .",
    "the input numbers are separated by delimiter symbols and an end - of - result symbol is predicted by the network ; these symbols as well as input and target padding are indicated by @xmath60 .",
    "an example is as follows : @xmath61 contrary to the work in @xcite that uses from 4 to 9 digits for the input integers , we fix the number of digits to 15 , we do not use curriculum learning strategies and we do not put digits from the partially predicted output back into the network , forcing the network to remember its partial predictions and making the task more challenging . the predicted output numbers have either 15 or 16 digits ."
  ],
  "abstract_text": [
    "<S> this paper introduces _ grid long short - term memory _ , a network of lstm cells arranged in a multidimensional grid that can be applied to vectors , sequences or higher dimensional data such as images . </S>",
    "<S> the network differs from existing deep lstm architectures in that the cells are connected between network layers as well as along the spatiotemporal dimensions of the data . </S>",
    "<S> the network provides a unified way of using lstm for both deep and sequential computation . </S>",
    "<S> we apply the model to algorithmic tasks such as 15-digit integer addition and sequence memorization , where it is able to significantly outperform the standard lstm . </S>",
    "<S> we then give results for two empirical tasks . </S>",
    "<S> we find that 2d grid lstm achieves 1.47 bits per character on the wikipedia character prediction benchmark , which is state - of - the - art among neural approaches . </S>",
    "<S> in addition , we use the grid lstm to define a novel two - dimensional translation model , the _ reencoder _ , and show that it outperforms a phrase - based reference system on a chinese - to - english translation task . </S>"
  ]
}