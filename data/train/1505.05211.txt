{
  "article_text": [
    "the massive quantities of data being generated every day , and the ease of collaborative data analysis and data science have led to severe issues in management and retrieval of datasets .",
    "we motivate our work with two concrete example scenarios .    *  [ intermediate result datasets ] for most organizations dealing with large volumes of diverse datasets ,",
    "a common scenario is that many datasets are repeatedly analyzed in slightly different ways , with the intermediate results stored for future use .",
    "often , we find that the intermediate results are the same across many pipelines ( e.g. , a _ pagerank _",
    "computation on the web graph is often part of a multi - step workflow ) .",
    "often times , the datasets being analyzed might be slightly different ( e.g. , results of simple transformations or cleaning operations , or small updates ) , but are still stored in their entirety .",
    "there is currently no way of reducing the amount of stored data in such a scenario : there is massive redundancy and duplication ( this was corroborated by our discussions with a large software company ) , and often the computation required to recompute a given version from another one is small enough to not merit storing a new version . *  [ data science dataset versions ] in our conversations with a computational biology group , we found that every time a data scientist wishes to work on a dataset , they make a private copy , perform modifications via cleansing , normalization , adding new fields or rows , and then store these modified versions back to a folder shared across the entire group .",
    "once again there is massive redundancy and duplication across these copies , and there is a need to minimize these storage costs while keeping these versions easily retrievable .    in such scenarios and many others ,",
    "it is essential to keep track of versions of datasets and be able to recreate them on demand ; and at the same time , it is essential to minimize the storage costs by reducing redundancy and duplication . the ability to manage a large number of datasets , their versions , and derived datasets , is a key foundational piece of a system we are building for facilitating collaborative data science , called datahub  @xcite .",
    "datahubenables users to keep track of datasets and their versions , represented in the form of a directed _ version graph _ that encodes derivation relationships , and to retrieve one or more of the versions for analysis .    in this paper , we focus on the problem of trading off storage costs and recreation costs in a principled fashion . specifically , the problem we address in this paper is : given a collection of datasets as well as ( possibly ) a directed version graph connecting them , minimize the overall storage for storing the datasets and the recreation costs for retrieving them .",
    "the two goals conflict with each other  minimizing storage cost typically leads to increased recreation costs and vice versa .",
    "we illustrate this trade - off via an example",
    ".     indicates a storage cost of @xmath0 and a recreation cost of @xmath1 ; ( ii , iii , iv ) three possible storage graphs ]    [ fig : version_graph ]     indicates a storage cost of @xmath0 and a recreation cost of @xmath1 ; ( ii , iii , iv ) three possible storage graphs , title=\"fig : \" ] ( i )     indicates a storage cost of @xmath0 and a recreation cost of @xmath1 ; ( ii , iii , iv ) three possible storage graphs , title=\"fig : \" ] ( ii ) [ fig : extreme1 ]     indicates a storage cost of @xmath0 and a recreation cost of @xmath1 ; ( ii , iii , iv ) three possible storage graphs , title=\"fig : \" ] ( iii ) [ fig : extreme2 ]     indicates a storage cost of @xmath0 and a recreation cost of @xmath1 ; ( ii , iii , iv ) three possible storage graphs , title=\"fig : \" ] ( iv ) [ fig : storage_graph ]    figure  [ fig : version_graph](i ) displays a version graph , indicating the derivation relationships among 5 versions .",
    "let @xmath2 be the original dataset .",
    "say there are two teams collaborating on this dataset : team 1 modifies @xmath2 to derive @xmath3 , while team 2 modifies @xmath2 to derive @xmath4 . then , @xmath3 and @xmath4 are merged and give @xmath5 . as presented in figure  [ fig : version_graph ] ,",
    "@xmath2 is associated with @xmath6 , indicating that @xmath2 s storage cost and recreation cost are both @xmath7 when stored in its entirety ( we note that these two are typically measured in different units  see the second challenge below ) ; the edge @xmath8 is annotated with @xmath9 , where @xmath10 is the storage cost for @xmath4 when stored as the modification from @xmath2 ( we call this the _ delta _ of @xmath4 from @xmath2 ) and @xmath11 is the recreation cost for @xmath4 given @xmath2 , i.e , the time taken to recreate @xmath4 given that @xmath2 has already been recreated .",
    "one naive solution to store these datasets would be to store all of them in their entirety ( figure [ fig : version_graph ] ( ii ) ) . in this case , each version can be retrieved directly but the total storage cost is rather large , i.e. , @xmath12 . at the other extreme , only one version is stored in its entirety while other versions are stored as modifications or deltas to that version , as shown in figure  [ fig : version_graph ] ( iii ) .",
    "the total storage cost here is much smaller ( @xmath13 ) , but the recreation cost is large for @xmath14 and @xmath5 .",
    "for instance , the path @xmath15 needs to be accessed in order to retrieve @xmath5 and the recreation cost is @xmath16 .",
    "figure [ fig : version_graph ] ( iv ) shows an intermediate solution that trades off increased storage for reduced recreation costs for some version .",
    "here we store versions @xmath2 and @xmath4 in their entirety and store modifications to other versions .",
    "this solution also exhibits higher storage cost than solution ( ii ) but lower than ( iii ) , and still results in significantly reduced retrieval costs for versions @xmath4 and @xmath5 over ( ii ) .    despite the fundamental nature of the storage - retrieval problem",
    ", there is surprisingly little prior work on formally analyzing this trade - off and on designing techniques for identifying effective storage solutions for a given collection of datasets .",
    "version control systems ( vcs ) like git , svn , or mercurial , despite their popularity , use fairly simple algorithms underneath , and are known to have significant limitations when managing large datasets  @xcite .",
    "much of the prior work in literature focuses on a linear chain of versions , or on minimizing the storage cost while ignoring the recreation cost ( we discuss the related work in more detail in section  [ sec : related ] ) . in this paper",
    ", we initiate a formal study of the problem of deciding how to jointly store a collection of dataset versions , provided along with a version or derivation graph . aside from being able to handle the scale , both in terms of dataset sizes and the number of versions ,",
    "there are several other considerations that make this problem challenging .    * different application scenarios and constraints",
    "lead to many variations on the basic theme of balancing storage and recreation cost ( see table  [ table : prob ] ) .",
    "the variations arise both out of different ways to reconcile the conflicting optimization goals , as well as because of the variations in how the differences between versions are stored and how versions are reconstructed .",
    "for example , some mechanisms for constructing differences between versions lead to symmetric differences ( either version can be recreated from the other version )  we call this the _ undirected _ case . the scenario with asymmetric , one - way differences",
    "is referred to as _ directed _ case .",
    "* similarly , the relationship between storage and recreation costs leads to significant variations across different settings . in some cases the recreation cost is proportional to the storage cost ( e.g. , if the system bottleneck lies in the i / o cost or network communication ) , but that may not be true when the system bottleneck is cpu computation .",
    "this is especially true for sophisticated differencing mechanisms where a compact derivation procedure might be known to generate one dataset from another . *",
    "another critical issue is that computing deltas for all pairs of versions is typically not feasible .",
    "relying purely on the version graph may not be sufficient and significant redundancies across datasets may be missed . *",
    "further , in many cases , we may have information about relative _ access frequencies _ indicating the relative likelihood of retrieving different datasets . several baseline algorithms for solving this problem can not be easily adapted to incorporate such access frequencies .",
    "we note that the problem described thus far is inherently `` online '' in that new datasets and versions are typically being created continuously and are being added to the system . in this paper",
    ", we focus on the static , off - line version of this problem and focus on formally and completely understanding that version .",
    "we plan to address the online version of the problem in the future .",
    "the key contributions of this work are as follows .",
    "* we formally define and analyze the dataset versioning problem and consider several variations of the problem that trade off storage cost and recreation cost in different manners , under different assumptions about the differencing mechanisms and recreation costs ( section  [ sec : proboverview ] ) .",
    "table  [ table : prob ] summarizes the problems and our results .",
    "we show that most of the variations of this problem are np - hard ( section  [ sec : complexity ] ) .",
    "* we provide two light - weight heuristics : one , when there is a constraint on average recreation cost , and one when there is a constraint on maximum recreation cost ; we also show how we can adapt a prior solution for balancing minimum - spanning trees and shortest path trees for undirected graphs ( section  [ sec : algorithms ] ) .",
    "* we have built a prototype system where we implement the proposed algorithms .",
    "we present an extensive experimental evaluation of these algorithms over several synthetic and real - world workloads demonstrating the effectiveness of our algorithms at handling large problem sizes ( section [ sec : experiments ] ) .    [",
    "cols=\"<,<,<,<,<,<\",options=\"header \" , ]     [ table : ilp ]",
    "perhaps the most closely related prior work is source code version systems like git , mercurial , svn , and others , that are widely used for managing source code repositories . despite their popularity",
    ", these systems largely use fairly simple algorithms underneath that are optimized to work with modest - sized source code files and their on - disk structures are optimized to work with line - based diffs .",
    "these systems are known to have significant limitations when handling large files and large numbers of versions  @xcite . as a result , a variety of extensions like git - annex  @xcite , git - bigfiles  @xcite , etc .",
    ", have been developed to make them work reasonably well with large files .",
    "there is much prior work in the temporal databases literature  @xcite on managing a linear chain of versions , and retrieving a version as of a specific time point ( called _ snapshot _ queries )  @xcite .",
    "@xcite proposed an archiving technique where all versions of the data are merged into one hierarchy .",
    "an element appearing in multiple versions is stored only once along with a timestamp .",
    "this technique of storing versions is in contrast with techniques where retrieval of certain versions may require undoing the changes ( unrolling the deltas ) .",
    "the hierarchical data and the resulting archive is represented in xml format which enables use of xml tools such as an xml compressor for compressing the archive .",
    "it was not , however , a full - fledged version control system representing an arbitrarily graph of versions ; rather it focused on algorithms for compactly encoding a linear chain of versions .",
    "snapshot queries have recently also been studied in the context of array databases  @xcite and graph databases  @xcite . seering et al .",
    "@xcite considered the problem of storing an arbitrary tree of versions in the context of scientific databases ; their proposed techniques are based on finding a minimum spanning tree ( as we discussed earlier , that solution represents one extreme in the spectrum of solutions that needs to be considered ) .",
    "they also proposed several heuristics for choosing which versions to materialize given the distribution of access frequencies to historical versions .",
    "several databases support `` time travel '' features ( e.g. , oracle flashback , postgres  @xcite ) .",
    "however , those do not allow for branching trees of versions .",
    "@xcite articulates a similar vision to our overall datahubvision ; however , they do not propose formalisms or algorithms to solve the underlying data management challenges .",
    "in addition , the schema of tables encoded with flashback can not change .",
    "there is also much prior work on compactly encoding differences between two files or strings in order to reduce communication or storage costs .",
    "in addition to standard utilities like unix diff , many sophisticated techniques have been proposed for computing differences or edit sequences between two files ( e.g. , xdelta  @xcite , vdelta  @xcite , vcdiff  @xcite , zdelta  @xcite ) . that work is largely orthogonal and complementary to our work .",
    "many prior efforts have looked at the problem of minimizing the total storage cost for storing a collection of related files ( i.e. , problem 1 ) .",
    "these works do not typically consider the recreation cost or the tradeoffs between the two .",
    "quinlan et al .",
    "@xcite propose an archival `` deduplication '' storage system that identifies duplicate blocks across files and only stores them once for reducing storage requirements .",
    "zhu et al .",
    "@xcite present several optimizations on the basic theme .",
    "douglis et al .",
    "@xcite present several techniques to identify pairs of files that could be efficiently stored using delta compression even if there is no explicit derivation information known about the two files ; similar techniques could be used to better identify which entries of the matrices @xmath17 and @xmath18 to reveal in our scenario .",
    "ouyang et al .",
    "@xcite studied the problem of compressing a large collection of related files by performing a sequence of pairwise delta compressions .",
    "they proposed a suite of text clustering techniques to prune the graph of all pairwise delta encodings and find the optimal branching ( i.e. , mca ) that minimizes the total weight .",
    "burns and long  @xcite present a technique for in - place re - construction of delta - compressed files using a graph - theoretic approach . that work could be incorporated into our overall framework to reduce the memory requirements during reconstruction .",
    "similar dictionary - based reference encoding techniques have been used by  @xcite to efficiently represent a target web page in terms of additions / modifications to a small number of reference web pages .",
    "kulkarni et al .",
    "@xcite present a more general technique that combines several different techniques to identify similar blocks among a collection files , and use delta compression to reduce the total storage cost ( ignoring the recreation costs ) .",
    "we refer the reader to a recent survey  @xcite for a more comprehensive coverage of this line of work .",
    "large datasets and collaborative and iterative analysis are becoming a norm in many application domains ; however we lack the data management infrastructure to efficiently manage such datasets , their versions over time , and derived data products . given the high overlap and duplication among the datasets , it is attractive to consider using delta compression to store the datasets in a compact manner , where some datasets or versions are stored as modifications from other datasets ; such delta compression however leads to higher latencies while retrieving specific datasets . in this paper",
    ", we studied the trade - off between the storage and recreation costs in a principled manner , by formulating several optimization problems that trade off these two in different ways and showing that most variations are np - hard .",
    "we also presented several efficient algorithms that are effective at exploring this trade - off , and we presented an extensive experimental evaluation using a prototype version management system that we have built .",
    "there are many interesting and rich avenues for future work that we are planning to pursue .",
    "in particular , we plan to develop online algorithms for making the optimization decisions as new datasets or versions are being created , and also adaptive algorithms that reevaluate the optimization decisions based on changing workload information .",
    "we also plan to explore the challenges in extending our work to a distributed and decentralized setting .",
    "git uses delta compression to reduce the amount of storage required to store a large number of files ( objects ) that contain duplicated information .",
    "however , git s algorithm for doing so is not clearly described anywhere .",
    "an old discussion with linus has a sketch of the algorithm  @xcite .",
    "however there have been several changes to the heuristics used that do nt appear to be documented anywhere .",
    "here we focus on `` repack '' , where the decisions are made for a large group of objects .",
    "however , the same algorithm appears to be used for normal commits as well .",
    "most of the algorithm code is in file : ` builtin / pack - objects.c `        note the name hash is not a true hash ; the ` pack_name_hash ( ) ` function ( ` pack - objects.h ` ) simply creates a number from the last 16 non - white space characters , with the last characters counting the most ( so all files with the same suffix , e.g. , ` .c ` , will sort together ) .",
    "* step 2 : * the next key function is ` ll_find_deltas ( ) ` , which goes over the files in the sorted order .",
    "it maintains a list of @xmath19 objects ( @xmath19 = window size , default 10 ) at all times . for the next object ,",
    "say @xmath20 , it finds the delta between @xmath20 and each of the objects , say @xmath21 , in the window ; it chooses the the object with the minimum value of : ` delta(b , o ) / ( max_depth - depth of b ) ` where ` max_depth ` is a parameter ( default 50 ) , and depth of b refers to the length of delta chain between a root and b.    the original algorithm appears to have only used ` delta(b , o ) ` to make the decision , but the `` depth bias '' ( denominator ) was added at a later point to prefer slightly larger deltas with smaller delta chains .",
    "the key lines for the above part :    * line 1812 ( check each object in the window ) : + .... ret = try_delta(n , m , max_depth , & mem_usage ) ; .... * lines 1617 - 1618 ( depth bias ) : + .... max_size = ( uint64_t)max_size * ( max_depth -       src->depth ) / ( max_depth - ref_depth + 1 ) ; .... * line 1678 ( compute delta and compare size ) : + .... delta_buf = create_delta(src->index , trg->data ,       trg_size , & delta_size , max_size ) ; ....    ` create_delta ( ) ` returns non - null only if the new delta being tried is smaller than the current delta ( modulo depth bias ) , specifically , only if the size of the new delta is less than ` max_size ` argument .",
    "note : lines 1682 - 1688 appear redundant given the depth bias calculations .",
    "* step 3 . *",
    "originally the window was just the last @xmath19 objects before the object @xmath20 under consideration .",
    "however , the current algorithm shuffles the objects in the window based on the choices made .",
    "specifically , let @xmath22 be the current objects in the window .",
    "let the object chosen to delta against for @xmath20 be @xmath23 .",
    "then @xmath23 would be moved to the end of the list , so the new list would be : @xmath24 $ ] .",
    "then when we move to the new object after @xmath20 ( say @xmath25 ) , we slide the window and so the new window then would be : @xmath26 $ ] .",
    "small detail : the list is actually maintained as a circular buffer so the list does nt have to be physically `` shifted '' ( moving @xmath23 to the end does involve a shift though ) .",
    "relevant code here is lines 1854 - 1861 .",
    "finally we note that git never considers / computes / stores a delta between two objects of different types , and it does the above in a multi - threaded fashion , by partitioning the work among a given number of threads .",
    "each of the threads operates independently of the others ."
  ],
  "abstract_text": [
    "<S> the relative ease of collaborative data science and analysis has led to a proliferation of many thousands or millions of _ versions _ of the same datasets in many scientific and commercial domains , acquired or constructed at various stages of data analysis across many users , and often over long periods of time . managing , storing , and recreating these dataset versions is a non - trivial task . </S>",
    "<S> the fundamental challenge here is the _ storage - recreation trade - off _ : the more storage we use , the faster it is to recreate or retrieve versions , while the less storage we use , the slower it is to recreate or retrieve versions . despite the fundamental nature of this problem , </S>",
    "<S> there has been a surprisingly little amount of work on it . in this paper </S>",
    "<S> , we study this trade - off in a principled manner : we formulate six problems under various settings , trading off these quantities in various ways , demonstrate that most of the problems are intractable , and propose a suite of inexpensive heuristics drawing from techniques in delay - constrained scheduling , and spanning tree literature , to solve these problems . </S>",
    "<S> we have built a prototype version management system , that aims to serve as a foundation to our datahubsystem for facilitating collaborative data science  @xcite . </S>",
    "<S> we demonstrate , via extensive experiments , that our proposed heuristics provide efficient solutions in practical dataset versioning scenarios . </S>"
  ]
}