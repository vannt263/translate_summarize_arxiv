{
  "article_text": [
    "in spite of substantial effort to improve the efficiency of markov chain monte carlo ( mcmc ) methods , spatial correlations remain a major impediment .",
    "these correlations can severely restrict the possible configurations of a system by imposing complicated relationships between variables .",
    "it is well known that judicious elimination of variables by renormalization can reduce long range correlations ( see @xcite ) .",
    "the remaining variables are distributed according to the marginal distribution , @xmath0 where @xmath1 is the full distribution . given the values of the @xmath2 variables and the marginal distribution @xmath3 the @xmath4 variables are distributed according to the conditional distribution @xmath5 for systems exhibiting critical phenomena , the path through the space of distributions taken by marginal distributions under repeated renormalization can yield essential information about critical indices and the location of critical points ( see @xcite ) . more generally , because these marginal distributions exhibit shorter correlation lengths and weaker local correlations , they are useful in the acceleration of markov chain monte carlo methods . as explained in the next section , parallel marginalization takes advantage of the shorter correlation lengths present in marginal distributions of the target density .",
    "the use of monte carlo updates on lower dimensional spaces is not a new concept .",
    "in fact this is a necessary procedure in high dimensions .",
    "one simply constructs a chain with steps that preserve the conditional probability density of the full measure .",
    "this is usually accomplished by perturbing a few components of the chain while holding all other components of the chain constant . in other words",
    "the chain takes steps of the form @xmath6 where @xmath7 and the move preserves @xmath8 there have been many important attempts to use proposals in more general sets of projected coordinates .",
    "the multi - grid monte carlo method presented in @xcite is one such method .",
    "these techniques do not incorporate marginal densities .    in @xcite ,",
    "brandt and ron propose a multi - grid method which approximates successive marginal distributions of the ising model and then uses these approximations to generate large scale movements of the markov chain sampling the full joint distribution of all variables .",
    "their method , while demonstrating the efficacy of incorporating information from successive marginal distributions , suffers from two limitations .",
    "first , the method used to approximate the marginal distributions is specific to a small class of problems .",
    "for example , it can not be easily generalized to systems in continuous spaces . second , information from the approximate marginal distributions is adopted by the markov chain in a way which does not preserve the target distribution of all variables .",
    "the design of a generally applicable method which approximates the marginal distributions was addressed in @xcite by chorin , and in @xcite by stinis .",
    "both authors approximate the renormalized hamiltonian of the system given by the formula , @xmath9 thus @xmath10 is the marginal distribution of the @xmath2 variables .",
    "chorin determines the coefficients in an expansion of @xmath11 by first expanding the derivatives @xmath12 , which can be expressed as conditional expectations with respect to the full distribution .",
    "stinis shows that a maximum likelihood approximation to the renormalized hamiltonian can be found by minimizing the error in the expectations of the basis functions in an expansion of @xmath11 . for applications of related ideas to mcmc simulations",
    "see @xcite and @xcite .",
    "two parallel marginalization algorithms are developed in the next section along with propositions that guarantee that the resulting markov chains satisfy the detailed balance condition . in the final section",
    "the conditional path sampling problem is described and numerical results are presented for the bridge sampling and smoothing / filtering problems . a brief introduction to parallel marginalization can be found in @xcite .",
    "in this section , it is assumed that appropriate approximate marginal distributions are available .",
    "how to find these marginal distributions depends on the application and will be discussed here only in the context of the examples presented in this paper .",
    "a new markov chain monte carlo method is introduced which uses approximate marginal distributions of the target distribution to accelerate sampling .",
    "auxiliary markov chains that sample approximate marginal distributions are evolved simultaneously with the markov chain that samples the distribution of interest . by swapping their configurations , these auxiliary chains pass information between themselves and with the chain sampling the original distribution .",
    "assume that the system of interest has a probability density , @xmath13 , where @xmath14 lies in some space @xmath15 suppose further that , by the metropolis - hastings or any other method ( see @xcite ) , one can construct a markov chain , @xmath16 , which has @xmath17 as its stationary measure .",
    "that is , for two points @xmath18 @xmath19 where @xmath20 is the probability density of a move to @xmath21 given that @xmath22 . here",
    ", @xmath23 is the algorithmic step .    in order to take advantage of the shorter spatial correlations exhibited by marginal distributions of @xmath17 , a collection of lower dimensional markov chains which approximately sample marginal distributions of @xmath17 is considered .",
    "suppose the random variable @xmath24 has @xmath25 components .",
    "divide these into two subsets , @xmath26 where @xmath27 has @xmath28 components and @xmath29 has @xmath30 components .",
    "recall that the @xmath27 variables are distributed according to the marginal density , @xmath31 and that given the value of the @xmath27 variables , the @xmath29 variables are distributed according to the conditional density , @xmath32 label the domain of the @xmath27 variables @xmath33 .",
    "suppose further that an approximation to the marginal distribution of the @xmath27 variables , @xmath34 is available .",
    "the sense in which @xmath35 approximates @xmath36 is intentionally left vague . in applications of parallel marginalization",
    "the accuracy of the approximation manifests itself through an acceptance rate .",
    "now let @xmath37 be independent of the @xmath24 random variables and drawn from @xmath38 .",
    "notice that @xmath39 represents the same physical variables as @xmath27 though its probability density is not the exact marginal density .",
    "continue in this way to remove variables from the system by decomposing @xmath40 into proper subsets as @xmath41 and defining @xmath42 to be independent of the @xmath43 random variables and drawn from an approximation @xmath44 to @xmath45 .",
    "clearly each @xmath46 represents fewer physical variables than @xmath47 .",
    "just as one can construct a markov chain @xmath48 to sample @xmath24 , one can also construct markov chains @xmath49 to sample @xmath50 .",
    "in other words , for each @xmath51 choose a transition probability density @xmath52 such that @xmath53 for all @xmath54    the chains @xmath55 can be arranged in parallel to yield a larger markov chain , @xmath56 the probability density of a move to @xmath57 given that @xmath58 for @xmath59 is given by @xmath60 since @xmath61 the stationary distribution of @xmath62 is @xmath63    the next step in the construction is to allow interactions between the chains @xmath51 and to thereby pass information from the rapidly equilibrating chains on the lower dimensional spaces ( large @xmath64 ) down to the chain on the original space ( @xmath65 ) .",
    "this is accomplished by swap moves . in a swap move between levels @xmath64 and @xmath66 , a subset , @xmath67 , of the @xmath68 variables is exchanged with the @xmath69 variables .",
    "the remaining @xmath70 variables are resampled from the conditional distribution @xmath71 . for the full chain",
    ", this swap takes the form of a move from @xmath58 to @xmath57 where @xmath72 and @xmath73 the @xmath74 variables are drawn from @xmath71 and the ellipses represent components of @xmath62 that remain unchanged in the transition .",
    "if these swaps are undertaken unconditionally , the resulting chain may equilibrate rapidly , but will not , in general , preserve the product distribution @xmath75 . to remedy this the swap acceptance probability @xmath76 is introduced . recall that @xmath77 is the function resulting from the integration of @xmath50 over the @xmath70 variables as in equation .",
    "given that @xmath58 , the probability density of @xmath57 , after the proposal and either acceptance with probability @xmath78 or rejection with probability @xmath79 , of a swap move , is given by @xmath80 for @xmath81 .",
    "@xmath82 is the dirac delta function .",
    "we have the following proposition .",
    "the transition probabilities @xmath83 satisfy the detailed balance condition for the measure @xmath84 i.e. @xmath85 where @xmath86    fix @xmath81 such that @xmath87 .",
    "@xmath88    when @xmath87 @xmath89 and @xmath90 are both zero unless @xmath91 for all @xmath92 except @xmath64 and @xmath66 and @xmath93 .",
    "therefore it is enough to check that the function @xmath94 is symmetric in @xmath95 and @xmath96 when @xmath93 .",
    "plugging in the definition of @xmath97 @xmath98 rearranging terms gives , @xmath99 recall from , that @xmath100 therefore , since @xmath101 @xmath102 the final formula is clearly symmetric in @xmath95 and @xmath96 .",
    "the detailed balance condition stipulates that the probability of observing a transition @xmath103 is equal to that of observing a transition @xmath104 and guarantees that the resulting markov chain preserves the distribution @xmath75 .",
    "if the chain is also harris recurrent then averages over a trajectory of @xmath62 will converge to averages over @xmath75 .",
    "in fact , chains generated by swaps as described above can not be recurrent and must be combined with another transition rule to generate a convergent markov chain . since @xmath105 if @xmath62 is harris",
    "recurrent with invariant distribution @xmath75 , averages over @xmath17 can be calculated by taking averages over the trajectories of the first @xmath25 components of @xmath62 .",
    "notice that the formula for @xmath78 requires the evaluation of @xmath77 at the points @xmath106 while the approximation of @xmath77 by functions on @xmath107 is in general a very difficult problem , its evaluation at a single point is often not terribly demanding .",
    "in fact , in many cases , including the examples in chapter 3 , the @xmath108 variables can be chosen so that the remaining @xmath109 variables are conditionally independent given @xmath110    despite this mitigating factor , the requirement that @xmath77 be evaluated before acceptance of any swap is inconvenient . fortunately , and somewhat surprisingly , this requirement is not necessary .",
    "in fact , standard strategies for approximating the point values of the marginals yield markov chains that also preserve the target measure . thus even a poor estimate of the ratio appearing in can give rise to a method that is exact in the sense that the resulting markov chain will asymptotically sample the target measure .    before moving on to the description of the resulting markov chain monte carlo algorithms",
    "consider briefly the general problem of evaluating marginal densities .",
    "let @xmath111 and @xmath112 be the densities of two equivalent measures with marginal densities , @xmath113 and @xmath114 respectively . for any integrable function @xmath115 @xmath116 & = \\int \\gamma(x , y )",
    "p_2(x , y ) p_1(y\\vert x ) dy\\\\ & = \\frac{\\overline{p}_2(x)}{\\overline{p}_1(x ) } \\int \\gamma(x , y ) p_2(y\\vert x ) p_1(x , y ) dy\\\\ & = \\frac{\\overline{p}_2(x)}{\\overline{p}_1(x ) } \\mathbf{e}_{p_2}\\left [ \\gamma\\left(x , y\\right ) p_1\\left(x , y\\right )       \\vert \\left\\{x = x\\right\\}\\right]\\end{aligned}\\ ] ]    thus given @xmath117 the value of @xmath118 at @xmath2 can be obtained through the formula , @xmath119 }      { \\mathbf{e}_{p_1}\\left [ \\gamma\\left(x , y\\right ) p_2\\left(x , y\\right )       \\vert \\left\\{x = x\\right\\}\\right]}\\ ] ] of course , the usual importance sampling concerns apply here .",
    "in particular , the approximation of the conditional expectations in will be much easier when @xmath120 lives in a lower dimensional space .",
    "similar approximations can be inserted into our acceptance probabilities @xmath78 in place of the ratio @xmath121 for example , if @xmath122 is a reference density approximating @xmath123 then the choice @xmath124 yields @xmath125 where the @xmath126 are samples from @xmath127 thus if @xmath128 are samples from @xmath129 then @xmath130{a.s . }",
    "\\frac{\\mathbf{e}_{p_l}\\left[\\frac{\\pi_l\\left(x_{l+1},\\widetilde{x}_l\\right ) } { p_l\\left(\\widetilde{x}_l\\vert x_{l+1}\\right)}\\ \\vert \\left\\{\\widehat{x}_l = x_{l+1}\\right\\}\\right ] } { \\mathbf{e}_{p_l}\\left[\\frac{\\pi_l\\left(\\hat{x}_l,\\widetilde{x}_l\\right ) } { p_l\\left(\\widetilde{x}_l\\vert \\hat{x}_l\\right)}\\ \\vert \\left\\{\\widehat{x}_l=\\hat{x}_l\\right\\}\\right ] } = \\frac{\\overline{\\pi}_l(x_{l+1})}{\\overline{\\pi}_l(\\hat{x}_l)}\\ ] ] in the numerical examples presented here , @xmath131 is a gaussian approximation of @xmath132 .",
    "how @xmath133 is chosen depends on the problem at hand ( see numerical examples below ) . in general @xmath131 should be easily evaluated and independently sampled , and it should `` cover '' @xmath134 in the sense that regions where @xmath134 is not negligible should be contained in regions where @xmath131 is not negligible . in the case mentioned above that the @xmath108 variables can be chosen so that the remaining @xmath109 variables are conditionally independent given @xmath108 the conditional density @xmath132 can be written as a product of many low dimensional densities . as mentioned above ,",
    "the problem of finding a reference density for importance sampling is much simpler in low dimensional spaces .    the following algorithm results from replacing @xmath78 in with approximation of the form .",
    "assume that the current position of the chain is @xmath58 where @xmath135 algorithm [ pm1 ] will result in either @xmath136 or @xmath57 where @xmath137 and @xmath74 is approximately drawn from @xmath138    [ pm1 ] the chain moves from @xmath62 to @xmath139 as follows :    1 .",
    "let @xmath128 for @xmath140 be independent random variables sampled from @xmath141 ( recall that the swap is between @xmath142 and @xmath143 which are both in @xmath107 ) .",
    "2 .   evaluate the weights @xmath144 the choice of @xmath133 made above affects the variance of these weights , and therefore the variance of the acceptance probability below .",
    "3 .   draw the random index @xmath145 according to the probabilities @xmath146   = \\frac{w_u^j}{\\sum_{m=1}^{m } w_u^m}.\\ ] ] set @xmath147 notice that @xmath148 is an approximate sample from @xmath149 4 .",
    "let @xmath150 and draw @xmath126 for @xmath151 independently from @xmath152 notice that the @xmath128 variables depend on @xmath153 while the @xmath126 variables depend on @xmath142 .",
    "define the weights @xmath154 6 .   set @xmath155 with probability @xmath156 and @xmath157 with probability @xmath158 .    the transition probability density for the above swap move from @xmath2 to @xmath4 for @xmath81is given by @xmath159\\   \\prod \\delta_{\\left\\{y_j = x_j\\right\\}}\\\\ + \\mathbf{p}\\left[\\left\\{\\text{swap is accepted}\\right\\}\\cap   \\left\\{\\widetilde{y}'=\\tilde{y}_l\\right\\ } \\right]\\\\ \\times \\delta_{\\left\\{\\left(\\hat{y}_l , y_{l+1}\\right ) = \\left(x_{l+1},\\hat{x}_l\\right)\\right\\ } } \\prod_{j\\notin\\left\\{i , i+1\\right\\}}\\delta_{\\left\\{y_j = x_j\\right\\}},\\end{gathered}\\ ] ] where @xmath82 is again the dirac delta function .",
    "notice that to find the probability density @xmath160 $ ] one must integrate over the possible values of the @xmath128 and @xmath126 variables . since @xmath50 appears in the integrand",
    "it is not possible , in general , to evaluate the integral .",
    "however , as indicated in the proof of the next proposition , it is not necessary to evaluate this density to show that the method converges .    while the preceding swap move corresponds to a method for approximating the ratio @xmath161 appearing in formula for @xmath97 it also has similarities with the multiple - try metropolis method , presented in @xcite , that uses multiple suggestion samples to improve acceptance rates of standard mcmc methods . in fact the proof of the following proposition is motivated by the proof of the detailed balance condition for the multiple try method .",
    "the transition probabilities @xmath162 satisfy the detailed balance condition for the measure @xmath163    for @xmath81 such that @xmath87 , @xmath164\\\\ \\times \\delta_{\\left\\{\\left(\\hat{y}_l , y_{l+1}\\right ) = \\left(x_{l+1},\\hat{x}_l\\right)\\right\\ } } \\prod_{j\\notin\\left\\{i , i+1\\right\\}}\\delta_{\\left\\{y_j = x_j\\right\\}},\\end{gathered}\\ ] ] as in the previous proof it can be assumed that @xmath91 for all @xmath92 except @xmath64 and @xmath66 and @xmath93 . since in this case",
    "@xmath165 for all @xmath166 it remains to show that if @xmath93 then @xmath167\\end{gathered}\\ ] ] is symmetric in @xmath95 and @xmath96 .",
    "define a random index @xmath145 by the relation @xmath168 then , since the @xmath128 are @xmath169 , @xmath170 = & \\\\ & \\hspace{-70pt } \\sum_{j=1}^{m}\\mathbf{p}\\left[\\left\\{\\text{swap is accepted}\\right\\}\\cap   \\left\\{\\tilde{y}_l = u^j \\right\\ } \\cap \\left\\{j = j\\right\\ } \\right]\\\\   & \\hspace{-70pt } = m\\ \\mathbf{p}\\left[\\left\\{\\text{swap is accepted}\\right\\}\\cap   \\left\\{\\tilde{y}_l = u^1 \\right\\ } \\cap \\left\\{j=1\\right\\ } \\right]\\end{aligned}\\ ] ] thus , @xmath171   \\end{aligned}\\ ] ] writing out the density on the right of this relation gives , @xmath172 replacing @xmath173 by @xmath74 and rearranging gives , @xmath174 since @xmath101 @xmath175 therefore , after replacing @xmath142 by @xmath176 @xmath177 which is symmetric in @xmath95 and @xmath96 .",
    "for small values of @xmath178 in , calculation of the swap acceptance probabilities is very cheap .",
    "however , higher values of @xmath178 may improve the acceptance rates .",
    "for example , if the @xmath50 for @xmath179 are exact marginals of @xmath180 then @xmath181 while @xmath182 in practice one has to balance the speed of evaluating @xmath183 for small @xmath178 with the possible higher acceptance rates for @xmath178 large .    in analogy again with the multiple - try method , the above algorithm can be generalized to include correlated samples @xmath128 and @xmath126 .",
    "this generalization is useful because it allows reference densities that can not be independently sampled . again",
    "consider a transition from @xmath58 where @xmath72 to either @xmath136 or @xmath57 where @xmath73    first choose some reference transition densities @xmath184 that sample a variable @xmath128 given the previous @xmath185 samples and the value of the @xmath142 variables .",
    "let @xmath186 for example , one might choose the @xmath187 to be markov transition kernels associated with some markov chain monte carlo method with stationary measure @xmath188 also let @xmath189 be any function satisfying the relation @xmath190    [ pm2 ] we move the chain from @xmath62 to @xmath139 as follows :    1 .   for @xmath140 sample @xmath128 from @xmath191 notice the conditioning on the value @xmath192 .",
    "2 .   define the weights @xmath193 notice the reversal in the ordering of the @xmath128 and the conditioning on @xmath194 .",
    "3 .   choose the random index @xmath145 according to the probabilities @xmath195   = \\frac{w_u^j}{\\sum_{m=1}^{m } w_u^m}.\\ ] ] set @xmath196 4 .",
    "let @xmath197 and for @xmath198 let @xmath199 . for @xmath200 sample @xmath126 from @xmath201 notice the conditioning on the value @xmath194 .",
    "define the weights @xmath202 6 .   set @xmath155 with probability @xmath156 and @xmath157 with probability @xmath158 .",
    "the transition probability density for the above swap move from @xmath2 to @xmath4 for @xmath81is again given by @xmath159\\   \\prod \\delta_{\\left\\{y_j = x_j\\right\\}}\\\\ + \\mathbf{p}\\left[\\left\\{\\text{swap is accepted}\\right\\}\\cap   \\left\\{\\widetilde{y}'=\\tilde{y}_l\\right\\ } \\right]\\\\ \\times   \\delta_{\\left\\{\\left(\\hat{y}_l , y_{l+1}\\right ) = \\left(x_{l+1},\\hat{x}_l\\right)\\right\\ } } \\prod_{j\\notin\\left\\{i , i+1\\right\\}}\\delta_{\\left\\{y_j = x_j\\right\\}}\\end{gathered}\\ ] ] where and @xmath82 is again the dirac delta function .",
    "again , the density @xmath160 $ ] can not and need not be evaluated .",
    "algorithm [ pm1 ] can be derived from algorithm [ pm2 ] by setting @xmath203 and @xmath204 notice also that if @xmath205 where , for each @xmath206 @xmath207 is a conditional density satisfying @xmath208 then @xmath209 =   \\int \\pi_l(u^j , x_{l+1 } ) q^j\\left((u_1,\\dots , u_{j-1})\\vert \\hat{x}_l , x_{l+1}\\right)\\prod_{i=1}^j du^i   =   \\overline{\\pi}_l(x_{l+1}).\\ ] ] thus , if the @xmath210 generate a sequence that satisfies a law of large numbers , then @xmath211 the same holds for the @xmath212 so that @xmath213 more general choices of @xmath214 lead to @xmath215 which converge to correpondingly more general acceptance probabilities than @xmath216    of course , expression points the way to even more general algorithms .",
    "algorithms [ pm1 ] and [ pm2 ] correspond to choices of @xmath217 in that make the conditional expectation on the bottom of equal to one .",
    "other choices of @xmath217 may improve the variance of the resulting weights .",
    "the transition probabilities @xmath162 satisfy the detailed balance condition for the measure @xmath163    fix @xmath81 such that @xmath87 .",
    "for @xmath81 such that @xmath87 , @xmath164\\\\ \\times \\delta_{\\left\\{\\left(\\hat{y}_l , y_{l+1}\\right ) = \\left(x_{l+1},\\hat{x}_l\\right)\\right\\ } } \\prod_{j\\notin\\left\\{i , i+1\\right\\}}\\delta_{\\left\\{y_j = x_j\\right\\}},\\end{gathered}\\ ] ] as in the previous two proofs it can be assumed that @xmath91 for all @xmath92 except @xmath64 and @xmath66 and @xmath93 . since in this case",
    "@xmath165 for all @xmath166 it remains to show that if @xmath93 then @xmath218\\end{gathered}\\ ] ] is symmetric in @xmath95 and @xmath96 .",
    "summing over disjoint events , @xmath219 = \\\\ \\sum_{j=1}^{m}\\mathbf{p}\\left[\\left\\{\\text{swap is accepted}\\right\\}\\cap   \\left\\{\\tilde{y}_l = u^j \\right\\ } \\cap \\left\\{j = j\\right\\ } \\right]\\end{gathered}\\ ] ] thus @xmath220 will be symmetric if for each @xmath92 the function @xmath221\\end{gathered}\\ ] ] is symmetric .",
    "@xmath222 recall the definition of the weights and the fact that @xmath223 @xmath224 and @xmath199 for @xmath198 @xmath225 thus , @xmath226 definition implies that for all @xmath92 , @xmath227 thus , @xmath228 which can be rewritten , @xmath229 plugging @xmath230 and @xmath231 into this expression yields , @xmath232 by the symmetry property of @xmath233 this expression is symmetric in @xmath95 and @xmath96 .",
    "clearly a markov chain that evolves only by swap moves can not sample all configurations , ie .",
    "the chain generated by @xmath234 is not @xmath235-irreducible for any non trivial measure @xmath236 these swap moves must therefore be used in conjunction with a transition rule that can reach any region of space .",
    "more precisely , let @xmath237 from expression be harris recurrent with stationary distribution @xmath75 ( see @xcite ) .",
    "the the transition rule for parallel marginalization is @xmath238 where @xmath239 and @xmath240 is the probability that a swap move occurs .",
    "@xmath241 dictates that , with probability @xmath242 , the chain attempts a swap move between levels @xmath243 and @xmath244 where @xmath243 is a random variable chosen uniformly from @xmath245 .",
    "next , the chain evolves according to @xmath237 . with probability @xmath246 the chain moves only according to @xmath237 and does not attempt a swap .",
    "the next result guarantees the invariance of @xmath75 under evolution by @xmath241 .",
    "it is not difficult to verify that the chain generated by @xmath241 has invariant measure @xmath75 and is harris recurrent if the chain generated by @xmath237 has these properties .",
    "thus by combining standard mcmc steps on each component , governed by the transition probability @xmath237 , with swap steps between the components governed by @xmath234 , an mcmc method results that not only uses information from rapidly equilibrating lower dimensional chains , but is also convergent .",
    "in this section i consider applications of parallel marginalization to two conditional path sampling problems for a one dimensional stochastic differential equation , @xmath247 where @xmath248 and @xmath249 are real valued functions of @xmath250 one must first approximate @xmath251 by a discrete process for which the path density is readily available .",
    "let @xmath252 be a mesh on which one wishes to calculate path averages .",
    "one such approximate process is given by the linearly implicit euler scheme ( a balanced implicit method , see @xcite ) , @xmath253 here @xmath254 is an approximation to @xmath255 at time @xmath256 the reader should note that the rate of convergence of the above scheme to the solution of would not be effected by the insertion in of a non - negative constant in front of the @xmath257 term .",
    "the choice of @xmath258 made here seemed to improve the stability of the resulting scheme for large values of @xmath259 the @xmath260 are independent gaussian random variables with mean 0 and variance 1 , and @xmath261 @xmath262 is assumed to be a power of 2 .",
    "the choice of this scheme over the euler scheme ( see @xcite ) is due to its favorable stability properties as explained later .",
    "it is henceforth assumed that @xmath263 instead of @xmath251 is the process of interest .",
    "the first of the conditional sampling problems discussed here is the bridge sampling problem in which one generates samples of transition paths between two states .",
    "this problem arises , for example , in financial volatility estimation where , given a sequence of observations , @xmath264 with @xmath265 the goal is to estimate the diffusion term @xmath249 ( assumed here to be constant ) appearing in the stochastic differential equation .",
    "since in general one can not easily evaluate the transition probability between times @xmath266 and @xmath267 ( and thus the likelihood of the observations ) it is necessary to generate samples between the observations , @xmath268 where @xmath269 ( assumed to be an integer ) and @xmath270 denotes the value of the process at time @xmath271 .",
    "it is then easy to evaluate the likelihood of a path @xmath272 given a particular value of the volatility , @xmath273    the filtering / smoothing problem is similar to the financial volatility example of the previous paragraph except that now it is assumed that the observations are noisy functions of the underlying process .",
    "for example , one may wish to sample possible trajectories taken by a rocket given somewhat unreliable gps observations of its position .",
    "if the conditional density of the observations given the position of the rocket is known , it is possible to generate conditional samples of the trajectories .      in the bridge path sampling problem one seeks to approximate conditional expectations of the form @xmath274\\ ] ] where @xmath275 is a real valued function , and @xmath251 is solution to .    without the condition @xmath276 above , generating an approximate sample @xmath277 path is a relatively straitforward endeavor .",
    "one simply generates a sample of @xmath278 , then evolves with this initial condition .",
    "however , the presence of information about @xmath279 complicates the task . in general ,",
    "some sampling method which requires only knowlege of a function proportional to conditional density of @xmath280 must be applied .",
    "the approximate path density associated with discretization is @xmath281 where @xmath282 ^ 2 } { 2\\sigma^2\\left(x\\right)\\triangle}\\ ] ]    at this point the parallel marginalization sampling procedure is applied to the density @xmath17 .",
    "however , as indicated above , a prerequisite for the use of parallel marginalization is the ability to estimate marginal densities . in some important problems homogeneities in the underlying system yield simplifications in the calculation of these densities by the methods in @xcite .",
    "these calculations can be carried out before implementation of parallel marginalization , or they can be integrated into the sampling procedure .    in some cases , computer generation of the @xmath283 can be completely avoided .",
    "the examples presented here are two such cases .",
    "let @xmath284 ( recall @xmath262 is a power of 2 ) .",
    "decompose @xmath285 as @xmath286 where @xmath287 and @xmath288 in the notation of the previous sections , @xmath289 where @xmath290 and @xmath291 in words , the hat and tilde variables represent alternating time slices of the path . for all @xmath64 fix @xmath292 and @xmath293 .",
    "we choose the approximate marginal densities @xmath294 where for each @xmath64 , @xmath295 is defined by successive coarsenings of .",
    "that is , @xmath296 since @xmath50 will be sampled using a metropolis - hastings method with @xmath297 and @xmath298 fixed , knowlege of the normalization constants @xmath299 is unnecessary .",
    "notice from that , conditioned on the values of @xmath300 and @xmath301 , the variance of @xmath254 is of order @xmath302 .",
    "thus any perturbation of @xmath254 which leaves @xmath303 fixed for @xmath304 and which is compatible with joint distribution must be of the order @xmath305 .",
    "this suggests that distributions defined by coarser discretizations of will allow larger perturbations , and consequently will be easier to sample .",
    "however , it is important to choose a discretization that remains stable for large values of @xmath302 .",
    "for example , while the linearly implicit euler method performs well in the experiments below , similar tests using the euler method were less successful due to limitations on the largest allowable values of @xmath302 .    in this numerical example bridge paths",
    "are sampled between time 0 and time 10 for a diffusion in a double well potential @xmath306 the left and right end points are chosen as @xmath307 .",
    "@xmath308 @xmath309 is the @xmath310 level of the parallel marginalization markov chain at algorithmic time @xmath23 .",
    "there are 10 chains ( @xmath311 ) .",
    "the observed swap acceptance rates are reported in table .",
    "notice that the swap rates are highest at the lower levels but seems to stabilize at the higher levels .",
    "@rrrrrrrrrrrrr 1clevels & 1c0/1&1c1/2 & 1c2/3&1c3/4 & 1c4/5&1c5/6 & 1c6/7&1c7/8 & 1c8/9 1c & 1c0.86&1c0.83 & 1c0.75&1c0.69 & 1c0.54&1c0.45 & 1c0.30&1c0.22 & 1c0.26    swaps between levels @xmath64 and @xmath312    [ swaprates1 ]    let @xmath313 denote the midpoint of the path defined by @xmath314 ( i.e. an approximate sample of the path at time 5 ) . in figure [ fig1 ] the autocorrelation of @xmath315 @xmath316\\ ] ] is compared to that of a standard metropolis - hastings rule using 1 dimensional gaussian random walk proposals . in the figure ,",
    "the time scale of the autocorrelation for the metropolis - hastings method has been scaled by a factor of 1/10 to more than account for the extra computational time required per iteration of parallel marginalization .",
    "the relaxation time of the parallel chain is clearly reduced .     for metropolis - hastings method with 1-d gaussian random walk proposals ( solid ) and parallel marginalization ( dotted ) .",
    "the x - axis runs from 0 to 10000 iterations of the metropolis - hastings method and from 0 to 1000 iterations of parallel marginalization .",
    "this rescaling more than compensates for the extra work for parallel marginalization per iteration .",
    ", width=384 ]    in these numerical examples , parallel marginalization is applied with a slight simplification as detailed in the following algorithm .",
    "the chain moves from @xmath62 to @xmath139 as follows :    1 .",
    "generate m independent gaussian random paths @xmath317 with independent components @xmath318 of mean 0 and variance @xmath319 .",
    "2 .   for each @xmath92 and @xmath320 let @xmath321 3 .",
    "define the weights @xmath322 where @xmath133 is defined by the choice in step 1 as @xmath323 4 .",
    "choose @xmath145 according to the probabilities @xmath146   = \\frac{w_u^j}{\\sum_{k=1}^{m } w_u^k}.\\ ] ] set @xmath196 5 .",
    "set @xmath197 and for @xmath151 set @xmath324 6 .",
    "define the weights @xmath325 7 .   set @xmath326 with probability @xmath327 and @xmath157 with probability @xmath158 .",
    "this simplification reduces by half the number of gaussian random variables needed to evaluate the acceptance probability but may not be appropriate in all settings . for this problem , the choice of @xmath178 in , the number of samples @xmath328 and @xmath329 , seems to have little effect on the swap acceptance rates . in the numerical experiment @xmath330 for swaps between levels @xmath64 and @xmath66 .",
    "the results of the metropols - hastings and parallel marginalization methods applied to the above bridge sampling problem after a run time of 10 minutes on a standard workstation are presented in figures [ bridgesample ] and [ parbridge ] . apparently the sample generated by parallel marginalization is a reasonable bridgepath while the metropolis - hastings method has clearly not converged .              in the non - linear smoothing and filtering problem",
    "one seeks to approximate conditional expectations of the form @xmath331\\ ] ] where the real valued processes @xmath251 and @xmath332 are given by the system @xmath333 @xmath275 , @xmath248 , @xmath249 , and @xmath334 are real valued functions of @xmath335 .",
    "the @xmath336 are real valued independent random variable drawn from the density @xmath337 and are independent of the brownian motion @xmath338 @xmath339 and @xmath340 the process @xmath251 is a hidden signal and the @xmath332 are noisy observations .",
    "the idea of computing the above conditional expectation by conditional path sampling has been suggested in @xcite .",
    "popular alternatives include particle filters ( see @xcite ) and ensemble kalman filters ( see @xcite ) .    again ,",
    "begin by discretizing the system .",
    "assume that @xmath269 is an integer and let @xmath261 the linearly implicit euler scheme gives @xmath341 where @xmath270 represents the discrete time approximation to @xmath342 for @xmath343 the @xmath260 are independent gaussian random variables with mean 0 and variance 1 .",
    "the @xmath260 are independent of the @xmath344 .",
    "@xmath262 is again assumed to be a power of 2 .",
    "the approximate path measure for this problem is @xmath345 the approximate marginals are chosen as @xmath346 where @xmath347 , @xmath295 and @xmath285 are as defined in the previous section .    in this example , samples of the smoothed path are generated between time time 0 and time 10 for the same diffusion in a double well potential .",
    "the densities @xmath337 and @xmath348 are chosen as @xmath349 the function @xmath334 in is the identity function .",
    "the observation times are @xmath350 with @xmath351 for @xmath352 and @xmath353 for @xmath354 .",
    "@xmath308 there are 8 chains ( @xmath355 ) .",
    "the observed swap acceptance rates are reported in table .",
    "notice that the swap rates are again highest at the lower levels but , for this problem , become unreasonably small at the highest level .",
    "@rrrrrrrrrrrrr 1clevels & 1c0/1&1c1/2 & 1c2/3&1c3/4 & 1c4/5&1c5/6 & 1c6/7 1c & 1c0.86&1c0.83 & 1c0.74&1c0.65 & 1c0.46&1c0.23 & 1c0.04    swaps between levels @xmath64 and @xmath312    [ swaprates2 ]    again , @xmath313 denotes the midpoint of the path defined by @xmath314 ( i.e. an approximate sample of the path at time 5 ) . in figure [ fig2 ] the autocorrelation of @xmath315",
    "is compared to that of a standard metropolis - hastings rule .",
    "the figure has been adjusted as in the previous example .",
    "the relaxation time of the parallel chain is again clearly reduced .     for metropolis - hastings method with 1-d gaussian random walk proposals ( solid ) and parallel marginalization ( dotted ) .",
    "the x - axis runs from 0 to 10000 iterations of the metropolis - hastings method and from 0 to 1000 iterations of parallel marginalization .",
    "this rescaling more than compensates for the extra work for parallel marginalization per iteration .",
    ", width=384 ]    the algorithm is modified as in the previous example . for this problem ,",
    "acceptable swap rates require a higher choice of @xmath178 in than needed in the bridge sampling problem . in this numerical experiment @xmath356 for swaps between levels @xmath64 and @xmath66 .",
    "the results of the metropols - hastings and parallel marginalization methods applied to the smoothing problem above after a run time of 10 minutes on a standard workstation are presented in figure [ pathsmooth ] and [ parsmooth ] .",
    "apparently the sample generated by parallel marginalization is a reasonable bridgepath while the metropolis - hastings method has clearly not converged .",
    "a markov chain monte carlo method has been proposed and applied to two conditional path sampling problems for stochastic differential equations .",
    "numerical results indicate that this method , parallel marginalization , can have a dramatically reduced equilibration time when compared to standard mcmc methods .",
    "note that parallel marginalization should not be viewed as a stand alone method .",
    "other acceleration techniques such as hybrid monte carlo can and should be implemented at each level within the parallel marginalization framework .",
    "as the smoothing problem indicates , the acceptance probabilities at coarser levels can become small .",
    "the remedy for this is the development of more accurate approximate marginal distributions by , for example , the methods in @xcite and @xcite .",
    "i would like to thank prof .",
    "a. chorin for his guidance during this research , which was carried out while i was a ph.d .",
    "student at u. c. berkeley .",
    "i would also like to thank prof .",
    "o. hald , dr .",
    "p. okunev , dr .",
    "p. stinis , and dr .",
    "xuemin tu , for their very helpful comments .",
    "this work was supported by the director , office of science , office of advanced scientific computing research , of the u. s. department of energy under contract no .",
    "de - ac03 - 76sf00098 and national science foundation grant dms0410110 ."
  ],
  "abstract_text": [
    "<S> monte carlo sampling methods often suffer from long correlation times . </S>",
    "<S> consequently , these methods must be run for many steps to generate an independent sample . in this paper </S>",
    "<S> a method is proposed to overcome this difficulty . </S>",
    "<S> the method utilizes information from rapidly equilibrating coarse markov chains that sample marginal distributions of the full system . </S>",
    "<S> this is accomplished through exchanges between the full chain and the auxiliary coarse chains . </S>",
    "<S> results of numerical tests on the bridge sampling and filtering / smoothing problems for a stochastic differential equation are presented . </S>"
  ]
}