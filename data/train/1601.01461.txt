{
  "article_text": [
    "in many real - life applications such as audio processing or medical image analysis , one encounters the situation when given observations ( most likely noisy ) have been generated by several sources @xmath1 that one wishes to reconstruct separately . in this case",
    ", the reconstruction problem can be understood as an inverse problem of unmixing type , where the solution @xmath2 consists of several ( two or more ) components of different nature , which have to be identified and separated .    in mathematical terms , an unmixing problem can be stated as the solution of an equation @xmath3 where @xmath4 and @xmath5 but @xmath6 for @xmath7 in the sense that @xmath8 for all @xmath9 and @xmath10 @xmath11 in general , we are interested to acquire the minimal amount of information on @xmath12 so that we can selectively reconstruct with the best accuracy one of the components @xmath13 , but not necessarily also the other components @xmath14 for @xmath15 . in this settings , we further assume that @xmath16 can not be specifically tuned to recover @xmath13 but should be suited to gain universal information to recover @xmath13 by a specifically tuned decoder .    a concrete example of this setting is the _ noise folding phenomenon _ arising in compressed sensing , related to noise in the signal that is eventually amplified by the measurement procedure . in this setting , it is reasonable to consider a model problem of the type @xmath17 where @xmath18 is the random gaussian noise with variance @xmath19 on the original signal @xmath20 and @xmath21 is the linear measurement matrix .",
    "several recent works ( see , for instance , @xcite and the references therein ) illustrate how the measurement process actually causes the noise folding phenomenon . to be more specific , one can show that ( [ model_problem ] ) is equivalent to solving @xmath22 where @xmath23 is composed by i.i.d .",
    "gaussian entries with distribution @xmath24 , and the variance @xmath25 is related to the variance of the original signal by @xmath26 .",
    "this implies that the variance of the noise on the original signal is amplified by a factor of @xmath27 .    under the assumption that @xmath16 satisfies the so - called restricted isometry property , it is known from the work on the dantzig selector in @xcite that one can reconstruct @xmath28 from measurements @xmath29 as in ( [ model_problem_eq ] ) such that @xmath30 where @xmath31 denotes the number of nonzero elements of the solution @xmath32 the estimate ( [ recon_rate ] )",
    "is considered ( folklore ) nearly - optimal in the sense that no other method can really improve the asymptotic error @xmath33 therefore , the noise folding phenomenon may in practice significantly reduce the potential advantages of compressed sensing in terms of the trade - off between robustness and efficient compression ( given by the factor @xmath27 here ) compared to other more traditional subsampling methods @xcite .",
    "in @xcite the authors present a two - step numerical method which allows not only to recover the large entries of the original signal @xmath12 accurately , but also has enhanced properties in terms of support identification over simple @xmath0-minimization based algorithms . in particular , because of the lack of separation between noise and reconstructed signal components , the latter ones can easily fail to recover the support when the support is not given a priori . however , the computational cost of the second phase of the procedure presented in @xcite , being a non - smooth and non - convex optimization problem , is too demanding to be performed on problems with realistic dimensionalities .",
    "it was also shown that other methods based on a different penalization of the signal and noise can lead to higher support detection rate .",
    "the follow up work @xcite , which addresses the noise folding scenario by means of multi - penalty regularization , provides the first numerical evidence of the superior performance of multi - penalty regularization compared to its single parameter counterparts for problem ( [ model_problem ] ) .",
    "in particular , the authors consider the functional @xmath34 here @xmath35 may all be considered as regularization parameters of the problem .",
    "the parameter @xmath36 ensures the @xmath37coercivity of @xmath38 also with respect to the component @xmath18 . in the infinite dimensional setting the authors presented a numerical approach to the minimization of ( [ mp ] ) for @xmath39 , based on simple iterative thresholding steps , and analyzed its convergence .",
    "the results presented in this paper are very much inspired not only by the above - mentioned works in the signal processing and compressed sensing fields , but also by theoretical developments in sparsity - based regularization ( see @xcite and references therein ) and multi - penalty regularization ( @xcite , just to mention a few ) .",
    "while the latter two directions are considered separately in most of the literature , there have also been some efforts to understand regularization and convergence behavior for multiple parameters and functionals , especially for image analysis @xcite .",
    "however , to the best of our knowledge , the present paper is the first one providing a theoretical analysis of the multi - penalty regularization with a non - smooth sparsity promoting regularization term , and an explicit comparison with the single - parameter counterpart .",
    "in section 2 we concisely recall the pertinent features and concepts of multi - penalty and single - penalty regularization .",
    "we further show that @xmath0-regularization can be considered as the limiting case of the multi - penalty one , and thus the theory of @xmath0-regularization can be applied to multi - penalty setting . in section 3",
    "we recall and discuss conditions for exact support recovery in the single - parameter case .",
    "the main contributions of the paper are presented in sections 4 and 5 , where we extend and generalize the results from the previous sections to the multi - penalty setting . in section 5",
    "we also open the discussion on the set of admissible parameters for the exact support recovery for unmixing problem in single - parameter as well as multi - penalty cases .",
    "in particular , we study the sensitivity of the multi - penalty scheme with respect to the parameter choice .",
    "the theoretical findings and discussion are illustrated and supported by extensive numerical validation tests presented in section 6 .",
    "finally , in section 7 we compare the performance of the multi - penalty regularization and its single - parameter counterpart for compressive sensing problems .",
    "we first provide a short reminder and collect some definitions of the standard notation used in this paper .",
    "the true solution @xmath28 of the unmixing problem   is called @xmath31-sparse if it has at most @xmath31 non - zero entries , i.e. , @xmath40 , where @xmath41 denotes the support of @xmath28 .",
    "we propose to solve the unmixing problem   with @xmath31-sparse true solution using multi - penalty tikhonov regularization of the form @xmath42 the solution of which we will denote by @xmath43 .",
    "we note that we can , formally , interpret standard @xmath0-regularization as the limiting case @xmath44 , setting @xmath45 obviously , the pair of minimizers of @xmath46 will always be equal to @xmath47 , where @xmath48 minimizes @xmath49 .",
    "let @xmath50 be fixed .",
    "we say that a set @xmath51 is a _ set of exact support recovery for the unmixing problem with operator @xmath16 _ , if there exists @xmath52 , such that @xmath53 whenever the given data @xmath29 has the form @xmath54 with @xmath55 .    the parameters @xmath52 for which this property holds are called _",
    "admissible for @xmath56_.    specifically , we will study for @xmath57 the sets @xmath58 and the corresponding class @xmath59 the set @xmath60 is a set of exact support recovery , if there exists some regularization parameter @xmath52 , such that we can apply multi - penalty regularization with parameters @xmath61 and @xmath62 ( or single - parameter @xmath0-regularization with parameter @xmath61 in case @xmath44 ) to the unmixing problem   and obtain a result with the correct support , provided that the @xmath63-norm of the noise is smaller than @xmath64 and the non - zero coefficients of @xmath28 are larger than @xmath65 .",
    "typical examples of real - life signals that can be modeled by signals from the set @xmath60 can be found in asteroseismology , see for instance @xcite .",
    "we note that this class of signals is very similar to the one studied in  @xcite .",
    "the main difference is that we focus on the case where the noise @xmath18 is bounded only componentwise ( that is , with respect to the @xmath63-norm ) , whereas  @xcite deals with noise that has a bounded @xmath66-norm for some @xmath67 .",
    "additionally , we allow the noise also to mix with the signal @xmath28 to be identified in the sense that the supports of @xmath18 and @xmath28 may have a non - empty intersection .",
    "in contrast , the signal and the noise are assumed to be strictly separated in  @xcite .    throughout the paper",
    "we will several times refer to the sign function @xmath68 , which we always interpret as being the set valued function @xmath69 if @xmath70 , @xmath71 if @xmath72 , and @xmath73 $ ] if @xmath74 , applied componentwise to the entries of the vector @xmath48 .",
    "we use the notation @xmath75 to denote the restriction of the operator @xmath16 to the span of the support of @xmath28 .",
    "additionally , we denote by @xmath76 the complement of @xmath77 , and by @xmath78 the restriction of @xmath16 to the span of @xmath79 .",
    "we note that the adjoints @xmath80 and @xmath81 are simply the compositions of the adjoint @xmath82 of @xmath16 with the projections onto the spans of @xmath77 and @xmath79 , respectively .    as a first result",
    ", we show that the solution of the multi - penalty problem   simultaneously solves a related single - penalty problem .",
    "[ le : single ] the pair @xmath43 solves  ( [ eq : multi ] ) if and only if @xmath83 and @xmath84 solves the optimization problem @xmath85 with @xmath86 and @xmath87    we can solve the optimization problem in   in two steps , first with respect to @xmath18 and then with respect to @xmath12 . assuming that @xmath12 is fixed , the optimality condition for @xmath18 in   reads @xmath88 that is , for fixed @xmath12 , the optimum in   with respect to @xmath18 is obtained at @xmath89 inserting this into the tikhonov functional , we obtain the optimization problem @xmath90 using  , we can write @xmath91 thus the optimization problem for @xmath12 simplifies to @xmath92 now note that @xmath93 inserting this equality in  , we obtain the optimization problem @xmath94 which is the same as  .    as a consequence of lemma  [ le : single ]",
    ", we can apply the theory of @xmath0-regularization also to the multi - penalty setting we consider here .",
    "in particular , this yields , for fixed @xmath95 , estimates of the form @xmath96 provided that @xmath28 satisfies a source condition of the form @xmath97 with @xmath98 for every @xmath99 , and the restriction of the mapping @xmath100 to the span of the support of @xmath28 is injective ( see  @xcite ) .",
    "additionally , it is easy to show that these conditions hold for @xmath100 provided that they hold for @xmath16 and @xmath62 is sufficiently large .",
    "the main focus of this paper is the question whether multi - penalty regularization allows for the exact recovery of the support of the true solution @xmath28 and how it compares to single - penalty regularization . because , as we have seen in lemma  [ le : single ] , multi - penalty regularization can be rewritten as single - parameter regularization for the regularized operator @xmath100 and right hand side @xmath101",
    ", we will first discuss recovery conditions in the single - parameter setting .    in order to find conditions for exact support recovery , we first recall the necessary and sufficient optimality condition for @xmath0-regularization :    [ le : opt ] the vector @xmath48 minimizes @xmath102 if and only if @xmath103    using this result , we obtain a condition that guarantees exact support recovery for the single - penalty method :    [ le : support ] we have @xmath104 , if and only if there exists @xmath105 such that @xmath106    this immediately follows from lemma  [ le : opt ] by testing the optimality conditions on the vector @xmath48 given by @xmath107 for @xmath108 and @xmath109 else .",
    "our main result concerning support recovery for single - parameter regularization is the following :    [ pr : cond_single ] assume that @xmath75 is injective and that @xmath110 then the set @xmath60 defined in   is a set of exact support recovery for the unmixing problem whenever @xmath111 moreover , every parameter @xmath52 satisfying @xmath112 is admissible on @xmath113 .",
    "first we note that the injectivity of @xmath75 implies that the mapping @xmath114 is invertible .",
    "thus the condition   actually makes sense .",
    "moreover , the inequality   is necessary and sufficient for the existence of @xmath61 satisfying  .",
    "now let @xmath115 and assume that @xmath61 satisfies  .",
    "we denote @xmath116 and define @xmath117 because @xmath118 , it follows from the second inequality in   that @xmath119 and therefore @xmath120 thus @xmath121 actually satisfies the equation @xmath122 and thus @xmath123 which is the first condition in lemma  [ le : support ] .",
    "it remains to show that @xmath124 however , @xmath125 and thus @xmath126 now the first inequality in   implies that this term is smaller than @xmath61 .",
    "thus @xmath121 satisfies the conditions of lemma  [ le : support ] , and thus @xmath104 .    in the case where @xmath127 is the identity operator ,",
    "the conditions above reduce to the conditions that @xmath128 and @xmath129 .",
    "since @xmath0-regularization in this setting reduces to soft thresholding , these conditions are very natural and are actually both sufficient and necessary : since the noise may componentwise reach the value of @xmath64 , it is necessary to choose a regularization parameter of at least @xmath64 in order to remove it .",
    "however , on the support @xmath77 of the signal , the smallest values of the noisy signal value are at least of size @xmath130 .",
    "thus they are retained as long as the regularization parameter does not exceed this value .    for more complicated operators @xmath16 , the situation is similar , i.e. , a too small regularization parameter @xmath61 is not able to remove all the noise , while a too large one destroys part of the signal as well .",
    "the exact bounds for the admissible regularization parameters , however , are much more complicated .",
    "we now consider the setting of multi - penalty regularization for the solution of the unmixing problem . applying lemma  [ le : single ]",
    ", we can treat multi - penalty regularization with the same methods as single - penalty regularization . to that end",
    ", we introduce the regularized operator @xmath131 in particular , we have with the notation of lemma  [ le : support ] that @xmath132 and @xmath133 .    as a first result",
    ", we obtain the following analogon to lemma  [ le : support ] :    [ le : support_multi ] we have @xmath134 , if and only if there exists @xmath105 such that @xmath135    applying lemma  [ le : opt ] to the single - penalty problem  , we obtain the conditions @xmath136 now the claim follows from the equalities @xmath137    since the proof of proposition  [ pr : cond_single ] only depends on the optimality conditions and the representation of the data as @xmath138 , we immediately obtain a generalization of proposition  [ pr : cond_single ] to the multi - penalty setting .",
    "[ pr : cond_multi ] assume that @xmath139 is such that @xmath140 then the set @xmath60 is a set of exact support recovery for the unmixing problem in the multi - penalty setting whenever @xmath141 moreover , all the pairs of parameter @xmath142 satisfying   and @xmath143 are admissible on @xmath60 .    the proof is analogous to the proof of proposition  [ pr : cond_single ] .",
    "we note that the condition @xmath144 implies the analogous inequality for @xmath145 provided that @xmath62 is sufficiently large .",
    "similarly , if @xmath61 satisfies the conditions in proposition  [ pr : cond_single ] that guarantee admissibility on @xmath113 , the pair @xmath142 will satisfy the conditions for admissibility in proposition  [ pr : cond_multi ] provided that @xmath62 is sufficiently large . the converse , however , need not be true : if the pair @xmath142 is admissible for exact support recovery on @xmath146 with multi - penalty regularization , it need not be true that the single parameter @xmath61 is admissble for the single - penalty setting as well .",
    "examples where this actually happens can be found in section  [ se : valid ] ( see in particular table  [ tb : cond ] ) .",
    "as a consequence of propositions  [ pr : cond_single ] and  [ pr : cond_multi ] , we obtain that the condition @xmath147 is sufficient for @xmath148 to be a set of exact support recovery for the unmixing problem , provided that the ratio @xmath149 is sufficiently large ; the condition for the single - parameter case can be extracted from   by setting @xmath44 , in which case @xmath145 reduces to @xmath16 .",
    "now define the signal - to - noise ratio of a pair @xmath150 as @xmath151 that is , @xmath152 is the ratio of the smallest significant value of the signal @xmath12 , and the largest value of the noise @xmath18 .",
    "denote moreover @xmath153 then the inequality   implies that multi - penalty regularization with parameter @xmath62 allows for the recovery of the support of @xmath31-sparse vectors @xmath12 from data @xmath154 provided the signal - to - noise ratio of the pair @xmath150 satisfies @xmath155 whenever the signal - to - noise ratio is larger than @xmath156 , we can recover the support of the vector @xmath12 with _ some _ regularization parameter @xmath61 .",
    "there are , however , upper and lower limits for the admissible parameters @xmath61 , given by inequality  . in order to visualize them ,",
    "we consider instead the ratio @xmath157 defining @xmath158 and @xmath159 we then obtain the condition @xmath160 for exact support recovery .",
    "if the ratio @xmath161 is smaller than @xmath162 , then it can happen that some of the noise @xmath18 is not filtered out by the regularization method .",
    "on the other hand , if @xmath161 is larger than @xmath163 , then some parts of the signal @xmath28 might actually be lost because of the regularization .",
    "we note that the function @xmath164 is piecewise linear and concave , and @xmath165 .",
    "thus the region of admissible parameters defined by   is a convex and unbounded polyhedron .",
    "moreover , we have that @xmath166 .",
    "additionally , we note that the behaviour of the function @xmath167 near infinity is determined by the term @xmath168 if this value is small , then the slope of the function @xmath169 for large values of @xmath170 is large , and thus the set of admissible parameter grows fast with increasing signal - to - noise ratio .",
    "if , on the other hand , @xmath171 is large , then the set of admissible parameters is relatively small even for large signal - to - noise ratio . thus @xmath171 can be reasonably interpreted as the sensitivity of multi - parameter regularization with respect to parameter choice .",
    "the larger @xmath171 is , the more precise the parameter @xmath61 has to be chosen in order to guarantee exact support recovery .",
    "the main motive behind the study and application of multi - penalty regularization is the problem that @xmath0-regularization is often not capable to identify the support of signal correctly ( see @xcite and references therein ) . including the additional @xmath172-regularization term , however",
    ", might lead to an improved performance in terms of support recovery , because we can expect that the @xmath172-term takes care of all the small noise components .    in order to verify this observation ,",
    "a series of numerical experiments was performed , in which we illustrate for which parameters and gaussian matrices the conditions for support recovery derived in the previous section were satisfied .",
    "in addition , we studied whether the inclusion of the @xmath172-term indeed increases the performance .    in a first set of experiments , we have generated a set of 20 gaussian random matrices of different sizes and have tested for each three - dimensional subspace spanned by the basis elements whether the condition   is satisfied , first for the single - penalty case , and then for the multi - penalty case with different values of @xmath62 .",
    "the results for matrices of dimensions 30 times 60 and 40 times 80 , respectively , are summarized in table  [ tb : cond ] and figure  [ fi : cond ] .",
    "as to be expected from the bad numerical performance of @xmath0-regularization in terms of support recovery , the inequality   fails in a relatively large number of cases , especially when the discrepancy between the dimension @xmath173 of the vectors to be recovered and the number of measurements @xmath174 is quite large .",
    "for instance , in the case @xmath175 and @xmath176 , the condition most of the time failed for more than half of the three - dimensional subspaces .",
    "in contrast , the corresponding condition   for multi - parameter regularization fails in the same situation only for about an eighth of the subspaces if @xmath177 , and in even fewer cases for @xmath178 .    for other combinations of dimensionality of the problem and number of measurements ,",
    "the situation is similar . introducing the additional @xmath172-penalty term",
    "always allows for the exact support reconstruction on a larger number of subspaces than single - penalty regularization .",
    "additionally , the results indicate that the number of recoverable subspaces increases with decreasing @xmath62 .",
    ".[tb : cond ] percentage of 3-sparse subspaces for which the condition   failed .",
    "the condition was tested on samples of 20 gaussian random matrices of dimensions 30 times 60 ( upper table ) and 40 times 80 ( lower table ) .",
    "other combinations of dimensionality and number of measurements showed qualitatively similar results . [ cols=\">,^,^,^,^ \" , ]",
    "the figure reports the results of two different decoding procedures of the same problem , where the circles represent the noisy signal and the crosses represent the original signal .",
    "_ upper figure : _ results with single - penalty regularization .",
    "_ lower figure : _ results with multy - penalty regularization .",
    "note that multi - penalty regularization allows for a better reconstruction of the support of the true signal @xmath28 . , title=\"fig:\",scaledwidth=95.0% ]   the figure reports the results of two different decoding procedures of the same problem , where the circles represent the noisy signal and the crosses represent the original signal .",
    "_ upper figure : _ results with single - penalty regularization . _",
    "lower figure : _ results with multy - penalty regularization .",
    "note that multi - penalty regularization allows for a better reconstruction of the support of the true signal @xmath28 .",
    ", title=\"fig:\",scaledwidth=95.0% ] +"
  ],
  "abstract_text": [
    "<S> inspired by several real - life applications in audio processing and medical image analysis , where the quantity of interest is generated by several sources to be accurately modeled and separated , as well as by recent advances in regularization theory and optimization , we study the conditions on optimal support recovery in inverse problems of unmixing type by means of multi - penalty regularization .    </S>",
    "<S> we consider and analyze a regularization functional composed of a data - fidelity term , where signal and noise are additively mixed , a non - smooth , convex , sparsity promoting term , and a quadratic penalty term to model the noise . </S>",
    "<S> we prove not only that the well - established theory for sparse recovery in the single parameter case can be translated to the multi - penalty settings , but we also demonstrate the enhanced properties of multi - penalty regularization in terms of support identification compared to sole @xmath0-minimization . we additionally confirm and support the theoretical results by extensive numerical simulations , which give a statistics of robustness of the multi - penalty regularization scheme with respect to the single - parameter counterpart . </S>",
    "<S> eventually , we confirm a significant improvement in performance compared to standard @xmath0-regularization for compressive sensing problems considered in our experiments . </S>"
  ]
}