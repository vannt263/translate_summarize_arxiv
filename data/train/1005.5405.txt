{
  "article_text": [
    "non - equilibrium gas flows are met in several different physical situations ranging from the re - entry of spacecraft in upper planetary atmospheres to fluid - structure interaction in small - scale devices @xcite .",
    "the correct description of nonequilibrium effects requires replacing the traditional hydrodynamic equations with the boltzmann equation which , in the absence of assigned external force fields , reads @xmath1 in eq .",
    "( [ eq : be ] ) , the distribution function @xmath2 is the atomic number density at the single atom phase space point @xmath3 at time @xmath4 .",
    "the symbols @xmath5 and @xmath6 denote atom position and velocity , respectively . the left hand side of eq",
    ".   represents the rate of change of @xmath7 due to the indipendent motion of gas atoms .",
    "effects of collisions are accounted for by the source term @xmath8 which is a non - linear functional of @xmath7 whose precise structure depends on the assumed atomic interaction forces . obtaining numerical solutions of eq .",
    "( [ eq : be ] ) for realistic flow conditions is a challenging task because it has the form of a non - linear integro - differential equation in which the unknown function , @xmath7 , depends on seven variables .",
    "numerical methods used to solve eq .   can be roughly divided into three groups :    * particle methods * semi - regular methods * regular methods    methods in group ( a ) originate from the direct simulation monte carlo ( dsmc ) scheme proposed by g.a .",
    "bird @xcite .",
    "they are by far the most popular and widely used simulation methods in rarefied gas dynamics .",
    "the distribution function is represented by a number of mathematical particles which move in the computational domain and collide according to stochastic rules derived from boltzmann equation .",
    "macroscopic flow properties are usually obtained by time averaging particle properties .",
    "if the averaging time is long enough , then accurate flow simulations can be obtained by a relatively small number of particles .",
    "the method can be easily extended to deal with mixtures of chemically reacting polyatomic species @xcite and to dense fluids @xcite .",
    "although dsmc ( in its traditional implementation ) is to be recommended in simulating most of rarefied gas flows , it is not well suited to the simulation of low mach number or unsteady flows .",
    "attempts have been made to extend dsmc in order to improve its capability to capture the small deviations from the equilibrium condition met in low mach number flows @xcite .",
    "however , in simulating high frequency unsteady flows , typical of microfluidics application to mems , the possibility of time averaging is lost or reduced .",
    "acceptable accuracy can then be achieved by increasing the number of simulation particles or superposing several flow snapshots obtained from statistically independent simulations of the same flow ; in both cases the computing effort is considerably increased .",
    "+ methods in groups ( b ) and ( c ) adopt similar strategies in discretizing the distribution function on a regular grid in the phase space and in using finite difference schemes to approximate the streaming term . however , they differ in the way the collision integral is evaluated . in semi - regular methods @xmath8",
    "is computed by monte carlo or quasi monte carlo quadrature methods @xcite whereas deterministic integration schemes are used in regular methods @xcite .",
    "whatever method is chosen to compute the collision term , the adoption of a grid in the phase space considerably limits the applicability of methods ( b ) and ( c ) to problems where particular symmetries reduce the number of spatial and velocity variables . as a matter of fact , a spatially three - dimensional problem would require a memory demanding six - dimensional phase space grid .",
    "extensions to polyatomic gases are possible @xcite but the necessity to store additional variables associated with internal degrees of freedom further limits the applications to multi - dimensional flows .",
    "therefore , until now the direct solution of the boltzmann equation by semi - regular or regular methods has not been considered a viable alternative to dsmc for simulating realistic flows , not even for low speed and/or unsteady flows .",
    "the availability of low cost graphics processing units ( gpus ) has changed the situation .",
    "although gpus have been originally developed for graphics applications , they have been increasingly used to do general purpose scientific and engineering computing @xcite .",
    "mapping efficiently an algorithm on the simd - like architecture of the gpus , however , is a difficult task which often requires the algorithm to be revised or even redesigned to both balance the hardware structure benefits and meet the implementation requirements .",
    "for instance , preliminary tests , performed within the framework of the research work described here , have shown that the standard form of dsmc is not efficiently ported on gpu s because of their simd - like architecture .",
    "on the other hand , we have shown in ref .",
    "@xcite that a regular method of solution of the bgkw kinetic model equation is ideally suited for gpus .",
    "the main aim of the present paper is to translate efficiently a semi - regular method of solution of the full non - linear boltzmann equation into a parallel code to be executed on a gpu .",
    "the efficiency of the algorithm is assessed by solving the classical two - dimensional driven cavity flow .",
    "it is shown that it is possible to cut down the computing time of the sequential code of two order of magnitudes .",
    "this paper is organized as follows .",
    "sections 2 and 3 are devoted to a concise description of the mathematical model and the adopted numerical method . in section 4",
    "the key aspects of the gpu hardware architecture and @xmath0 programming model are briefly described and implementation details are provided .",
    "sections 5 is devoted to the description of the test problem and the discussion of the results .",
    "concluding remarks are presented in section 6 .",
    "the hard - sphere model is a good approximation for simple fluids , that is fluids whose properties are largely determined by harshly repulsive short range forces .",
    "the hard - sphere boltzmann collision integral reads    @xmath9    in eq .",
    ", @xmath10 is the hard sphere diameter , @xmath11 is the relative velocity between two colliding atoms and @xmath12 , @xmath13 .",
    "here and in the remainder of the paper , integration extends over the whole velocity space .",
    "similarly , the solid angle integration is over the surface of the unit sphere , whose points are associated with the unit vector @xmath14 .",
    "the pre - collisional velocities , @xmath15 , are obtained from the post - collision velocities , @xmath16 , and the unit vector on the sphere , @xmath14 , by the relationships @xmath17 in view of the applications to the study of low mach flows , refs .",
    "@xcite will be followed to rewrite eqs .",
    "( [ eq : be ] ) and ( [ collision_integral ] ) in terms of the deviational part of the distribution function , @xmath18 , defined as @xmath19 where @xmath20 is a parameter that measures the deviation from equilibrium conditions and @xmath21 is the maxwellian at equilibrium with uniform and constant density @xmath22 and temperature @xmath23 , i.e. , @xmath24 the physical rationale behind this formulation is a proper rescaling of the ( small ) deviation from equilibrium to reduce the variance in the monte carlo evaluation of the collision integral and thus to capture arbitrarily small deviations from equilibrium with a computational cost which is independent of the magnitude of the deviation . by substituting eq .   into eq .  , we obtain @xmath25 where , by using the property @xmath26 , the collision integral takes the form @xmath27    for later reference , we here report the expressions of dimensionless perturbed density , velocity , temperature and stress tensor @xmath28 where @xmath29 . at the boundaries ,",
    "maxwell s completely diffuse boundary condition is assumed .",
    "accordingly , the distribution function of atoms emerging from walls is given by the following expression    @xmath30    in eq .",
    "( [ eq : bcmax ] ) , @xmath31 is the inward normal and @xmath32 is the normalized wall maxwellian distribution function    @xmath33}\\ ] ]    where @xmath34 the wall velocity and @xmath35 the wall temperature . the wall density @xmath36 is determined by imposing zero net mass flux at any boundary point @xmath37 where @xmath38 .",
    "it is worth noticing that when the perturbation is sufficiently small , i.e. , @xmath39 , eq .",
    "( [ eq : devboltz ] ) reduces to the linearized boltzmann equation and eqs .",
    "( [ densita])-([sforzi ] ) to the linearized expression of the macroscopic quantities .",
    "the formulation in terms of the deviational part of the distribution function , however , is not restricted to a vanishing perturbation but it is valid in the non - linear case as well .",
    "the method of solution adopted to solve eq .   is a semi - regular method in which a finite difference discretization is used to evaluate the free streaming term on the left hand side while the collision integral on the right hand side is computed by a monte carlo technique .",
    "the three - dimensional physical space is divided into @xmath40 parallelepipedal cells .",
    "likewise , the three - dimensional velocity space is replaced by a parallelepipedal box divided into @xmath41 cells .",
    "the size and position of the `` velocity box '' in the velocity space have to be properly chosen , in order to contain the significant part of @xmath42 at any spatial position .",
    "the distribution function is assumed to be constant within each cell of the phase space .",
    "hence , @xmath42 is represented by the array @xmath43 ; @xmath44 and @xmath45 are the values of the spatial coordinates and velocity components in the center of the phase space cell corresponding to the indexes @xmath46 and @xmath47 . + the algorithm that advances @xmath48 to @xmath49 is constructed by time - splitting the evolution operator into a free streaming step , in which the right hand side of eq .",
    "( [ eq : devboltz ] ) is neglected , and a purely collisional step , in which spatial motion is frozen and only the effect of the collision operator is taken into account . more precisely , the distribution function @xmath50 is advanced to @xmath51 by computing an intermediate value , @xmath52 , from the free streaming equation @xmath53 when solving eq .",
    ", boundary conditions have to be taken into account .",
    "( [ eq : freestreaming ] ) is discretized by a simple first order explicit upwind conservative scheme . for later reference , we here report the difference scheme in the two dimensional case with @xmath54 and @xmath55    @xmath56    in eq .",
    "( [ eq : streaming2d ] ) , @xmath57 and @xmath58 are the courant numbers in the @xmath59 and @xmath60 directions , respectively . + after completing the free streaming step , @xmath51 is obtained by solving the homogeneous relaxation equation @xmath61 where @xmath62 is given by eq .  .",
    "in order to be solved , eq .",
    "( [ eq : homrel ] ) is first integrated over the cell of the velocity space @xmath63 @xmath64 where @xmath65 represents the deviation of the number of particles with position @xmath66 in the velocity cell centered around the velocity node @xmath67 with respect to its mean value at equilibrium , i.e. , @xmath68 with @xmath69 the volume of the velocity cell @xmath63 .",
    "the integral in eq .",
    "( [ eq : intsucj ] ) is then transformed into an integral extended to the whole velocity domain @xmath70 @xmath71 where @xmath72 is the characteristic function of the cell @xmath63 @xmath73 making use of some fundamental properties of the collision integral @xcite , eq .",
    "( [ integ ] ) can be written in the following form @xmath74 \\left [ h({\\boldsymbol{v } } ) + h({\\boldsymbol{v}}_{1 } ) + \\epsilon h({\\boldsymbol{v } } ) h({\\boldsymbol{v}}_{1 } ) \\right ]     eight - fold integral in eq .",
    "( [ eq : coll8dim ] ) is calculated by a monte carlo integration method , since a regular quadrature formula would be too demanding in term of computing time .",
    "the advantage of writing the rate of change of @xmath75 in the above form is that the gaussian distribution function @xmath76 may be considered a probability density function from which the velocity points are drawn to estimate the collision integral with lower variance .",
    "the monte carlo estimate of the integral on the right hand side of eq .",
    "gives @xmath77 \\\\ \\left [ h({\\boldsymbol{v}}_{l } ) + h({\\boldsymbol{v}}_{1l } ) + \\epsilon \\",
    ", h({\\boldsymbol{v}}_{l})\\,h({\\boldsymbol{v}}_{1l } ) \\right ] |{\\boldsymbol{\\hat k}}\\cdot { \\boldsymbol{v}}_{r}|\\end{gathered}\\ ] ] where @xmath78 is the number of velocity samples @xcite .",
    "it is worth noticing that the same set of collisions can be used to evaluate the collision integral at different space locations .",
    "once the collision integral have been evaluated , the solution is advanced from the @xmath79-th time level to the next according to the explicit scheme @xmath80 in eq .",
    ", @xmath81 is given by eq .",
    "with @xmath42 the deviational part of the distribution function at the end of the streaming step , that is @xmath82 .",
    "although memory demanding , the method outlined above produces accurate approximations of the solution which do not require time averaging to provide smooth macroscopic fields .",
    "a drawback of the technique is that , due to the discretization in the velocity space , momentum and energy are not exactly conserved .",
    "the numerical error is usually small but tends to accumulate during the time evolution of the distribution function .",
    "the correction procedure proposed in ref .",
    "@xcite has been adopted to overcome this difficulty . at each time",
    "step the full distribution function is corrected in the following way @xmath83\\ ] ] where the constants @xmath84 and @xmath85 are determined from the conditions @xmath86 being @xmath87 .",
    "the correction procedure given by eq .   involves the full distribution function and not only its deviational part in order the linear system ( [ eq : correzione ] ) to be well conditioned .",
    "@xmath88gpu is built around a fully programmable processors array organized into a number of multiprocessors with a simd - like architecture , i.e. at any given clock cycle , each core of the multiprocessor executes the same instruction but operates on different data .",
    "@xmath0 is the high level programming language specifically created for developing applications on this platform @xcite . +",
    "a @xmath0 program is organized into a serial program which runs on the host cpu and one or more kernels which define the computation to be performed in parallel by a massive number of threads .",
    "threads are organized into a three - level hierarchy . at the highest level ,",
    "all threads form a grid ; they all execute the same kernel function .",
    "each grid consists of many different blocks which contain the same number of threads .",
    "a single multiprocessor can manage a number of blocks concurrently up to the resource limits .",
    "blocks are independent , meaning that a kernel must execute correctly no matter the order in which blocks are run .",
    "a multiprocessor executes a group of threads beloging to the active block , called warp .",
    "all threads of a warp execute the same instruction but operate on different data .",
    "if a kernel contains a branch and threads of the same warp follow different paths , then the different paths are executed sequentially ( warp divergence ) and the total run time is the sum of all the branches .",
    "divergence and re - convergence are managed in hardware but may have a serious impact on performances .",
    "when the instruction has been executed , the multiprocessor moves to another warp . in this manner",
    "the execution of threads is interleaved rather than simultaneous .",
    "+ each multiprocessor has a number of registers which are dynamically partitioned among the threads running on it .",
    "registers are memory spaces that are readable and writable only by the thread to which they are assigned . threads of a single block are allowed to synchronize with each other and are available to share data through a high - speed shared memory .",
    "threads from different blocks in the same grid may coordinate only via operations in a slower global memory space which is readable and writable by all threads in a kernel as well as by the host . shared memory",
    "can be accessed by threads within a block as quickly as accessing registers . on the contrary , i / o operations involving global memory are particularly expensive , unless access is coalesced @xcite . because of the interleaved warp execution , memory access latency is partially hidden , i.e. , threads which have read their data can be performing computations while other warps running on the same multiprocessor are waiting for their data to come in from global memory .",
    "note , however , that gpu global memory is still ten time faster than the main memory of recent cpus . + code optimization is a delicate task . in general , applications which require many arithmetic operations between memory read / write , and which minimize the number of out - of - order memory access , tend to perform better .",
    "number of blocks and number of threads per block have to be chosen carefully .",
    "there should be at least as many blocks as multiprocessors in the device . running only one block per multiprocessor",
    "can force the multiprocessor to idle during thread synchronization and device memory reads . by increasing the number of blocks , on the other hand , the amount of available shared memory for each block diminishes . allocating more threads per block is better for efficient time slicing , but the more threads per block , the fewer registers are available per thread .",
    "the code to numerically solve eq .",
    "( [ eq : devboltz ] ) is organized into a host program , which deals with all memory management and other setup tasks , and three kernels running on the gpu which perform the streaming and the collision steps . in the following , we report and discuss the pseudo - codes of each kernel .",
    "because of their different impact on the code performance , we distinguish the slow global memory reads , @xmath89 , and writes , @xmath90 , from the fast reads , @xmath91 , and writes , @xmath92 , from local registers and shared memory .",
    "algorithm reports the pseudocode of the two dimensional streaming kernel .",
    "the one - dimensional case has been discussed in ref .",
    "@xcite whereas the extension to three - dimensional geometries is straightforward .",
    "moreover , for clarity of presentation , the pseudo - code of the streaming kernel refers to one cell of the velocity space with @xmath54 and @xmath55 .",
    "the other cases can be handled analogously .",
    "as shown by eq .",
    "( [ eq : streaming2d ] ) , for each cell of the velocity domain , the streaming step involves the distribution function evaluated at different space locations . similarly to the one dimensional case , the key performance enhancing strategy is to allow threads to cooperate in the shared memory . in order to fit into the device s resources , blocks are composed by a two dimensional grid of threads with dimension @xmath93 having each thread associated with one cell of the physical space .",
    "when a block become active , each thread loads one element of the distribution function from global memory , stores it into shared memory ( line 5 ) , updates its value according to eqs .",
    "( [ eq : streaming2d ] ) ( line 21 ) and then saves it back to the global memory ( line 22 ) .",
    "this procedure is repeated sequentially @xmath94 times . to ensure non - overlapping access ,",
    "threads are synchronized at the onset of writing to the global memory ( lines 20 ) . in order to obtain a coalesced access to the global memory , values of the discretized distribution function of cells which are adjacent in the physical space",
    "are stored in contiguous memory locations . however , not all the threads in a block can read data in a coalescent manner .",
    "in fact , in order to update @xmath95 the values of the distribution function of two upwind neighboring nodes , often referred to as `` halo '' nodes @xcite , are required .",
    "the halos on one physical direction can be read in with coalesced access ( line 13 - 19 ) while the others have to be read in with non - coalesced access ( line 6 - 12 ) . threads which update boundary points perform calculations which are slightly different to account for the incoming maxwellian flux from the boundaries of the domain ( lines 8 and 15 ) .",
    "+ the relaxation step is organized into two kernels whose pseudo - codes are listed in algorithms ( [ alg : sampling ] ) and ( [ alg : collision ] ) .",
    "the first kernel computes the sequence of @xmath96 collisions used in the monte carlo evaluation of the collision integral .",
    "the second kernel updates the discretized distribution function , executes the correction procedure and computes the macroscopic quantities of interest as well .",
    "+ algorithm   reports the pseudo - code of the sampling kernel . here",
    ", there are as many threads as the number of the collision samples , @xmath78 .",
    "firstly , each thread generates the pre - collisional velocities @xmath6 and @xmath97 by sampling the maxwellian distribution function with the box - muller algorithm ( line 1 - 2 ) and the unit vector @xmath14 by sampling the uniform distribution on the unit sphere ( line 3 ) .",
    "afterwards , the post - collisional velocities are evaluated ( line 4 - 6 ) and the index of the velocity cells to which they belong are calculated and stored in the vectors @xmath98 , @xmath99 , @xmath100 and @xmath101 defined in the global memory ( lines 7 - 10 ) .",
    "finally , for each velocity cell , the values to be added and subtracted to these velocity cells are calculated ( lines 11 - 14 ) and stored ( lines 15 - 18 ) in the vectors @xmath85 , @xmath102 , @xmath103 and @xmath104 defined in the global memory .",
    "it is important to note that in order to maximize the performance all the accesses to the global memory are done in a coalesced manner @xcite .",
    "+ to update the discretized distribution function in a cell of the physical space according to eq .",
    "( [ collision_step ] ) , no information from nearby space cells is required .",
    "this naturally fits for the gpu , where one may define as many threads as the number of cells in the physical space .",
    "moreover , by having one thread for each cell of the physical space , potentially dangerous conflicts between threads are avoided and the accesses to the global memory may be coalesced .",
    "firstly , each thread updates the discretized distribution function according to eq .",
    ", then executes the correction procedure to enforce the conservation of momentum and energy , eq .  , and finally compute the macroscopic quantities of interest .",
    "algorithm shows the pseudo - code of the relaxation kernel .",
    "the main part of the algorithm is in between the lines 7 and 21 where the collision integral is evaluated according to the monte carlo method , eq .  .",
    "lines from 1 to 6 and from 22 to 30 evaluate the required moments of the distribution function before and after the collision step , respectively .",
    "these moments are used in eq .   to obtain the constants @xmath105 and @xmath85 .",
    "the last loop over the velocity space ( lines 32 - 39 ) corrects the distribution function according to eq .   and",
    "compute the macroscopic quantities of interest . +",
    "the computations that are shown below , have been performed on a commercially available gpu geforce gtx 260 produced by @xmath88 using @xmath0 version 2.0 .",
    "the gtx 260 gpu model consists of 24 streaming multiprocessors with 8 streaming processors ( sp ) each for a total of 192 units .",
    "each sp is clocked at 1.242 ghz and performs up to 3 floating point operation ( flop ) per clock cycle , yielding a peak theoretical performance of 715.4 gflops ( @xmath106 ) .",
    "each group of sp shares one 16 kb of fast per - block shared memory while the gpu has 896 mb of device memory with a memory bandwidth of 111.9 gb / s .",
    "the graphic processing unit has been hosted by a personal computer equipped with 4 gb of main memory and an @xmath107 core duo quad q9300 cpu , running at 2.5 ghz .",
    "the host machine has also been used to run the sequential version of the program to obtain the speed - up data .",
    "the host code has been compiled using the gcc / g++ compiler with optimization option `` -03 '' .",
    "the driven cavity flow is a classical benchmark problem . in spite of its simple geometry , in fact",
    ", it contains most of the features of more complicated problems described by kinetic equations @xcite . in the following ,",
    "we restrict to the spatially two - dimensional case .",
    "we thus consider a monatomic rarefied gas contained in a square enclosure with length @xmath108 , i.e. , @xmath109\\times[0,l]$ ] .",
    "all the walls are kept at uniform and constant temperature @xmath110 .",
    "initially , the gas is supposed to be in uniform equilibrium with density @xmath111 and temperature @xmath110 .",
    "the flow is driven by the translation of the lid of the cavity with constant velocity @xmath112 .",
    "we describe the dynamics of the gas by eq .   and assume that atoms which strike the walls are re - emitted according to the maxwell s scattering kernel with complete accomodation , eq .  .",
    "+ as characteristic length we choose @xmath113 , with @xmath114 the viscosity of the hard sphere gas @xcite . likewise the characteristic time is given by @xmath115 , where @xmath116 .",
    "the cavity flow problem has been solved for three values of the rarefaction parameter @xmath117 , being @xmath118 proportional to the reciprocal of the knudsen number .",
    "since the proposed method of solution is particularly effective in capturing small deviations from equilibrium , we set the lid velocity to @xmath119 .",
    "the computations described in below , hence , refer to very low mach number driven cavity flows .",
    "the square cavity has been divided into @xmath120 uniform square cells , with @xmath121 .",
    "likewise the velocity space has been divided into @xmath122 with @xmath123 .",
    "since the deviation form equilibrium is supposed to be small , a cubic velocity space has been constructed by distributing the velocity nodes along each velocity component in the interval @xmath124 $ ] . in order to achieve a faster convergence of the solutions in the velocity space , the lengths of the cells",
    "are uniformly stretched with a progression ratio @xmath125 , being the smaller cells located at the origin of the velocity space .",
    "more precisely , it has been chosen @xmath126 and @xmath127 for @xmath128 and @xmath129 whereas @xmath130 and @xmath131 for @xmath132 the number of collisions used in the monte carlo evaluation of the collision integral have been varied with the rarefaction parameter . in particular , it has been set @xmath133 for @xmath128 , @xmath134 for @xmath129 and @xmath135 for @xmath132 .",
    "finally , the time step has been determined by setting the maximum courant number to @xmath136 .      in this section ,",
    "we first carry out a convergence analysis of the method in the physical space and then we investigate the parallel performances of the code . + in order to establish the convergence rate we compute two global flowfield properties , namely the mean dimensionless shear stress on the moving wall , @xmath137 , and the dimensionless flow rate of the main vortex , @xmath138 .",
    "the two quantities are defined as @xmath139 the absolute relative error in the long - term mean values of @xmath137 and @xmath138 are shown in figs  [ fig : e_d_g]a and [ fig : e_d_g]b , respectively , versus the spatial grid size , @xmath140 , and for @xmath128 ( circles ) , @xmath129 ( squares ) and @xmath132 ( triangles ) .",
    "the exact values of @xmath137 and @xmath138 , which are referred to as @xmath141 and @xmath142 , have been extrapolated from the linear fit of the results when @xmath143 .",
    "the linear behaviour of the absolute relative errors demonstrates that the results are in the asymptotic range of convergence and the method is first order accurate @xcite .",
    "the finest physical grid size provides predictions which are accurate only within few percent .",
    "more precisely , the largest error in @xmath137 is of the order of @xmath144 and is attained at @xmath132 whereas the one in @xmath138 is @xmath145 at @xmath128 .",
    "the error is mainly due to the physical and velocity discretizations . as a matter of fact the statistical error associated with the finite sample size used in the monte carlo evaluation of the collision integral",
    "does not affect the results significantly .",
    "the standard deviation of @xmath137 and @xmath138 with respect to their averaged values in stationary conditions , in fact , is negligible small .",
    "for instance , the standard deviation of @xmath137 at @xmath129 from its long - term mean value is less than 0.05% .",
    "the grid resolution in the physical and the velocity domains are the more accurate discretization compatible with the gpu global memory constraint , i.e. , @xmath146 , @xmath126 for @xmath128 and @xmath129 whereas @xmath147 , @xmath130 for @xmath132 .",
    "therefore , in order to improve the accuracy of the numerical solutions , we have adopted a nonuniform grid in the physical space .",
    "the lengths of the physical cells are uniformly stretched with progression ratios that depend on the rarefaction parameter , @xmath148 , @xmath149 for @xmath132 and @xmath150 , @xmath151 otherwise .",
    "the smaller cells are located close to the upper corners of the cavity where severe gradients are anticipated .",
    "all the results which follow have been obtained with these discretizations .",
    "table [ tab : deg ] compares the predictions of the long - term mean values of @xmath137 and @xmath138 with the extrapolated exact values , @xmath152 and @xmath153 , and the results reported in ref .",
    "@xcite where the linearized bgkw equation has been solved with a discrete velocity method .",
    "the accuracy of the numerical solution of the non - linear boltzmann equation can be estimated to be within @xmath154% .",
    "the agreement with the bgkw results is good , which is not unexpected .",
    "since the velocity of the lid is small , in fact , the gas is in a weakly nonequilibrium state and the solution of the non - linear boltzmann equation approaches the solution of the linearized bgkw equation .",
    "figures [ fig : profili]a and [ fig : profili]b show the profiles of the dimensionless horizontal component of the velocity , @xmath155 , along the vertical line crossing the center of the cavity and the dimensionless vertical component of the velocity , @xmath156 , along the horizontal line crossing the center of the main vortex which forms in the cavity , respectively .",
    "lines are the solutions of the non - linear boltzmann equation , whereas symbols are the results reported in ref .",
    "the results refer to two different values of the rarefaction parameters , @xmath128 ( dashed lines and squares ) and @xmath132 ( solid lines and circles ) .",
    "the agreement is very good .",
    "+ although the convergence analysis has been performed by referring to quantities evaluated in stationary conditions , it is important to point out that the proposed method of solution provides accurate results in the unsteady regime as well . as an example , figure [ fig : dragvstime ] shows the evolution of @xmath137 during the simulation for @xmath132 .",
    "such a result would be difficult to obtain with a particle method where computationally expensive ensemble averanging are needed to provide smooth macroscopic fields .",
    "the performance of the gpu implementation is compared against the single - threaded version running on the cpu by computing the speed - up factor @xmath157 , where @xmath158 and @xmath159 are the times used by the cpu and gpu , respectively .",
    "times are measured after initial setup , and do not include the time required to transfer data between the disjoint cpu and gpu memory spaces .",
    "+ we analyse separately the streaming and the collision step , the latter comprising both the sampling and the collision kernel .",
    "figure [ fig : speedup ] reports the obtained speed - up data as a function of the number of spatial grid points @xmath160 for @xmath129 .",
    "the speed - up grows rapidly with @xmath160 and than levels up at about 450 if @xmath160 approximately exceeds @xmath161 .",
    "this behavior is the result of the parallel set up of the collision step in @xmath160 independent threads one for each cell of the physical space .",
    "as discussed below , the collision step absorbs most of the computational resources and its execution strongly affects the overall performances . as shown by the speed - up curve , the gpu power is not fully exploited till the number of concurrent threads reaches a threshold . beyond , the speed - up saturates and the computing time approximately behaves as a linear function of @xmath160 .",
    "this behaviour is similar to the one reported in refs .",
    "figure  [ fig : times ] shows the relative time which is spent on the streaming step ( dark bar ) and on the collision step ( light bar ) as well as the total execution time in seconds ( numbers over the bars ) for @xmath129 . as expected , the collision step is more time consuming than the streaming step which takes at most 36% of overall computing time . + a strongly simplified evaluation of ideal performances of the streaming and collision step can be carried out as follows .",
    "a single application of the upwind scheme requires the execution of 11 floating point operations and 2.3 accesses to the global memory .",
    "the gpu delivers 715.4 gflops but the transfer rate to / from the main memory is limited to 111.9 gb / s . since in the case of the streaming step",
    "the ratio of number of floating point operations to the number of bytes accessed is low ( @xmath162 ) , it is reasonable to obtain the number of floating point operation per second from the transfer rate alone .",
    "hence , the ideal number of gflops can be obtained by assuming that 11 floating point operations will be executed in the time required to transfer 9.2 bytes from the main memory .",
    "accordingly , this simple argument yields an ideal performance of the streaming step of 133 gflops .",
    "a similar performance analysis can be applied to the collision step which encompasses both the sampling and collision kernel . in order to update the distribution function and compute the macroscopic quantities of interest , the number of flops that the gpu executes at any given time step and for each cell in the physical space is the sum of two contributions .",
    "the first is proportional to the number of velocity cells , @xmath163 , and the second one is proportional to the number of collisions used to evaluate the collision integral , @xmath78 .",
    "the resulting number of flops for each time step are of the order of @xmath164 .",
    "likewise the number of bytes accesses to the global memory per time step is of the order of @xmath165 arguments similar to those above lead to estimate an ideal performace of the collision step of about 174.7 gflops .",
    "+ timing the execution of the separate kernels and counting the number of associated floating point operations provides the real performance .",
    "the results are reported in fig .",
    "[ fig : gflopdricav ] where gflops are shown as a function of the number of grid points , @xmath160 .",
    "solid line with circles , dashed line with squares and dot - dashed line with triangles are the measured performances of the streaming step , the collision step and the overall code . respectively .",
    "it is possible to note that the performance of the streaming step grows with @xmath160 and quickly levels at about @xmath166 gflops , approximately one fourth of the estimated ideal performance .",
    "the difference can be justified by observing that the real @xmath0 implementation of the finite difference scheme is not free from thread divergence and ancillary tasks whose effects can be evaluated with difficulty @xcite .",
    "the collision step performance closely patterns the speed - up behavior , that is it rapidly grows in the range @xmath167 and then levels up at about @xmath168 gflop / s , reasonably close to the theoretical estimate .",
    "the collision kernel performs better than the streaming kernel due to its higher flop to memory operation ratio which , in turn , allows a more efficient use of gpu computing power .",
    "the absence of thread divergence is also a feature which positively affects performances .",
    "in this paper we have assessed the possibility of exploiting the computational power of modern gpus to simulate nonequilibrium rarefied gas flows .",
    "the full nonlinear boltzmann equation has been solved by means of a semi - regular method which combines a finite difference discretization of the free - streaming term with a monte carlo evaluation of the collision integral .",
    "this method of solution is ideally suited for the simd - like architecture provided by the commercially available gpus .",
    "the two dimensional driven cavity flow has been used as a benchmark problem .",
    "the results lead to concluding that the porting of the sequential code onto gpus allows a reduction of the computing time of two orders of magnitude , being the observed speed - up as high as 400 .",
    "although the test problem examined here has clearly shown that the size of physical memory is the main obstacle toward the application to complex two or three - dimensional flows , the numerical method described above can be reformulated as a less memory demanding particle scheme in many ways .",
    "hence , the present work and results are a first step toward the construction of a more flexible and efficient method for the numerical solution of kinetic equations .",
    "support received from * fondazione cariplo * within the framework of project _",
    "`` fenomeni dissipativi e di rottura in micro e nano sistemi elettromeccanici '' _ , and * galileo programme * of universit italo - francese within the framework of project monument ( modellizzazione numerica in mems e nanotecnologie ) is gratefully acknowledged .",
    "the authors wish to thank professor dimitris valougeorgis for providing his numerical results .",
    "50 c. cercignani , the boltzmann equation and its applications , springer - verlag , new york , 1988 .",
    "g. a. bird , molecular gas dynamics and the direct simulation of gas flows , oxford university press , 1994 .",
    "a. frezzotti , l. gibelli , s. lorenzani , `` mean field kinetic theory description of evaporation of a fluid into vacuum '' , phys .",
    "fluids 17 , 012102 - 12 ( 2005 ) .",
    "t. m. m. homolle , n. g. hadjiconstantinou , `` a low - variance deviational simulation monte carlo for the boltzmann equation '' , j. comput .",
    "( 2007 ) 2341 - 2358",
    ". w. wagner , `` deviational particle monte carlo for the boltzmann equation '' , monte carlo methods and applications 14 ( 2008 ) 191 - 268 .",
    "a. frezzotti , `` numerical study of the strong evaporation of a binary mixture '' , fluid dynamics research 8 ( 1991 ) 175 - 187 .",
    "f. tcheremissine , `` direct numerical solution of the boltzmann equation '' , aip conf .",
    "( 2005 ) 677 - 685 .",
    "v. v. aristov , direct methods for solving the boltzmann equation and study of nonequilibrium flows , springer - verlag , new york , 2001 .",
    "a. frezzotti , `` a numerical investigation of the steady evaporation of a polyatomic gas '' , eur .",
    "b : fluids 26 ( 1 ) ( 2007 ) 93 - 104 .",
    "p. matinsen , j. blaschke , r. knnemeyer , r. jordan , `` accelerating monte carlo simulations with an @xmath88 graphics processor '' , comput .",
    "180 ( 10 ) ( 2009 ) 1983 - 1989 .",
    "m. januszewski , m. kostur , `` accelerating numerical solution of stochastic differential equations with cuda '' , comput .",
    "181 ( 1 ) ( 2010 ) 183 - 188 .",
    "a. frezzotti , g. p. ghiroldi , l. gibelli , `` solving model kinetic equations on gpus '' , arxiv:0903.4044v1 l. l. baker , n. g. hadjiconstantinou , `` variance - reduced monte carlo solutions of the boltzmann equation for low - speed gas flows : a discontinuous galerkin formulation '' , int .",
    "fluids 58 ( 2008 ) 381 - 402 v.v .",
    "aristov and f.g .",
    "tcheremissine , u.s.s.r .",
    "comput . math .",
    "( 1980 ) 208 - 225 .",
    "nvidia corporation , `` nvidia cuda programming guide '' , jun .",
    "version 2.0 .",
    "p. micikevivius , `` 3d finite difference computation on gpus using cuda '' , acm international conference proceeding series , 383 ( 2009 ) 79 - 84 .",
    "s. varoutis , d. valougeorgis , f. sharipov , `` application of the integro - moment method to steady - state two - dimensional rarefied gas flows subject to boundary induced discontinuities '' , j. comput .",
    "( 2008 ) 6272 - 6287 .",
    "s. chapman , t. g. cowling , the mathematical theory of non - uniform gases , cambridge university press , 1990 .",
    "m. d. salas , `` some observations on grid convergence '' , comp .",
    "& fluids 35 ( 2006 ) 688 - 692 .",
    "e. elsen , p. legresley , e. darve , `` large calculation of the flow over a hypersonic vehicle using a gpu '' , j. comp .",
    "( 2008 ) 10148 - 10161 .",
    "j. a. anderson , c. d. lorenz , a. travesset , `` general purpose molecular dynamics simulations fully implemented on graphics processing units '' , j. comp .",
    "( 2008 ) 5342 - 5359 .",
    "[ fig : e_d_g ] : absolute relative error on ( a ) drag coefficient and ( b ) mean flow rate for @xmath128 ( circles ) , @xmath129 ( squares ) and @xmath132 ( triangles ) versus the size @xmath169 of the physical grid . lines are the least - mean square fit of the results .",
    "@xmath126 , @xmath127 , @xmath170 for @xmath128 ; @xmath126 , @xmath127 , @xmath171 for @xmath129 ; @xmath130 , @xmath131 , @xmath172 for @xmath132 .",
    "[ fig : profili ] : profiles of the dimensionless ( a ) horizontal mean velocity along the vertical line crossing the center of the cavity , and ( b ) vertical mean velocity component along the horizontal line crossing the center of the main vortex .",
    "solid and dashed lines : numerical solutions obtained by solving eq .   with the semi - regular method for @xmath132 and @xmath128 , respectively .",
    "circles and squares : numerical solutions reported in ref .",
    "@xcite for @xmath132 and @xmath128 , respectively .",
    "@xmath146 , @xmath150 , @xmath151 , @xmath126 , @xmath127 , @xmath170 for @xmath128 ; @xmath147 , @xmath148 , @xmath149 , @xmath130 , @xmath131 , @xmath172 for @xmath132 .",
    "[ fig : times ] : relative time spent on the streaming step ( dark bar ) and on collision step ( light bar ) .",
    "the numbers above the bars refer to the total execution time expressed in seconds .",
    "@xmath129 , @xmath126 , @xmath171 .",
    "[ fig : gflopdricav ] : gflops versus the number of cells in the physical space , @xmath160 . solid line with circles : streaming step ; dashed line with squares : collision step ; dot and dashed line with triangles : overall code .",
    "@xmath129 , @xmath126 , @xmath171 .",
    "@xmath175 , courant number along @xmath59 direction @xmath176 , courant number along @xmath60 direction @xmath177 , number of threads in @xmath59 direction @xmath178 , number of threads in @xmath60 direction @xmath179 , thread index in @xmath59 direction within the block @xmath180 , thread index in @xmath60 direction within the block @xmath181 , matrix @xmath182 in the shared memory @xmath183",
    "@xmath184 @xmath185 @xmath186 @xmath187 @xmath186 @xmath188 syncthreads @xmath189 @xmath190 @xmath191 @xmath192    @xmath193 , global thread index in the grid @xmath194boxmulller",
    "@xmath195boxmulller @xmath196unitsphere @xmath197 @xmath198 @xmath199 cells@xmath200 cells@xmath201 cells@xmath202 cells@xmath203 @xmath204 @xmath205 @xmath206 @xmath207    @xmath208 , global thread index in the grid @xmath209",
    "@xmath210 @xmath211 @xmath212 @xmath213 @xmath214 @xmath215 @xmath216 @xmath217 @xmath218 @xmath219 @xmath220 @xmath221 @xmath222 @xmath223 @xmath224 @xmath225 @xmath226 @xmath227 @xmath228 @xmath229 @xmath230 // others moments of the distribution function @xmath230 = linearsolver(@xmath231 ) @xmath226 @xmath232 @xmath233 @xmath230 //",
    "compute macroscopic quantities @xmath230"
  ],
  "abstract_text": [
    "<S> we show how to accelerate the direct solution of the boltzmann equation using graphics processing units ( gpus ) . in order to fully exploit the computational power of the gpu , </S>",
    "<S> we choose a method of solution which combines a finite difference discretization of the free - streaming term with a monte carlo evaluation of the collision integral . </S>",
    "<S> the efficiency of the code is demonstrated by solving the two - dimensional driven cavity flow . </S>",
    "<S> computational results show that it is possible to cut down the computing time of the sequential code of two order of magnitudes . </S>",
    "<S> this makes the proposed method of solution a viable alternative to particle simulations for studying unsteady low mach number flows .    ,    ,    boltzmann equation , semi - regular methods , gas microflows , parallel computing , graphics processing units , @xmath0 programming model 02.70.bf , 47.45.ab , 51.10.+y </S>"
  ]
}