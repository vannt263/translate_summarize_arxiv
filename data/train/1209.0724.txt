{
  "article_text": [
    "the problem of probability transformation dates back to von neumann @xcite in 1951 , who first considered the problem of simulating an unbiased coin by using a biased coin with unknown probability .",
    "he observed that when one focuses on a pair of coin tosses , the events ht and th have the same probability ( h is for ` head ' and t is for ` tail ' ) ; hence , ht produces the output symbol @xmath3 and th produces the output symbol @xmath4 .",
    "the other two possible events , namely , hh and tt , are ignored , namely , they do not produce any output symbols . more efficient algorithms for simulating an unbiased coin from a biased coin were proposed by hoeffding and simons @xcite , elias @xcite , stout and warren @xcite and peres @xcite . in 1976 ,",
    "knuth and yao @xcite presented a simple procedure for generating sequences with arbitrary probability distributions from an unbiased coin ( the probability of h and t is @xmath5 ) .",
    "they showed that the expected number of coin tosses is upper - bounded by the entropy of the target distribution plus two .",
    "han and hoshi @xcite and abrahams @xcite generalized their approach and demonstrated how to generate an arbitrary probability distribution using a general @xmath6-sided biased coin .",
    "all these works have been focusing on the  simulation \" side of probability transformation , and their goal is to minimize the expected number of coin tosses for generating a certain number of target distributions .",
    "-splitters for any @xmath7 and generates probability @xmath5.,height=153 ]    there are a few works that considered the problem of probability transformation from a synthetic perspective , namely , designing a physical system for  synthesizing \" target distributions , by connecting certain probabilistic elements .",
    "such probabilistic elements can be electrical ones based on internal thermal noise or molecular ones based on inherent randomness in chemical reactions . in this scenario ,",
    "the size of the construction becomes a central issue . in 1962 , gill @xcite@xcite discussed the problem of generating rational probabilities using a sequential state machine .",
    "later , sheng @xcite considered applying threshold logic elements as a discrete probability transformer .",
    "recently , wilhelm and bruck @xcite proposed a procedure for synthesizing stochastic switching circuits to realize desired discrete probabilities .",
    "more properties and constructions of stochastic switching circuits were studied by zhou , loh and bruck @xcite ; qian et .",
    "@xcite studied combinational logic for transforming a set of given probabilities into target probabilities . motivated by stochastic computing based on chemical reaction networks@xcite , in this paper we study stochastic flow networks .",
    "a stochastic flow network is a directed graph with incoming edges ( inputs ) and outgoing edges ( outputs ) , tokens enter through the input edges , travel stochastically in the network and can exit the network through the output edges .",
    "each node in the network is a splitter , namely , a token can enter a node through an incoming edge and exit on one of the output edges according to a predefined probability distribution .",
    "we address a fundamental synthesis question : given a finite set of possible splitters and an arbitrary rational probability distribution , design an _ optimal - sized _ stochastic flow network , such that every token that enters the input edge will exit the outputs with the prescribed probability distribution .",
    "stochastic flow networks can be easily implemented by chemical reaction networks , where each splitter corresponds to two types of molecules , and incoming tokens ( another type of molecules ) can react with both , hence react with one of them with a certain probability .",
    "compared to the synthetic stochastic systems described above , stochastic flow networks demonstrate strong powers in expressing an arbitrary rational target distribution .",
    "[ fig_example1 ] depicts von neumann s algorithm in the language a stochastic flow network that consists of three @xmath7-splitters for any @xmath7 and generates probability @xmath5 . here",
    ", a @xmath7-splitter indicates a splitter with two outgoing edges with probabilities @xmath7 and @xmath8 . in this construction",
    ", we have two outputs @xmath9 ( corresponding to the labels @xmath3 and @xmath4 , respectively ) . for each incoming token",
    ", it has the same probability @xmath10 to reach either output @xmath3 or output @xmath4 directly , and it has probability @xmath11 to come back to the starting point .",
    "eventually , the probability for the token to reach each of the outputs is @xmath5 . in general , the outputs of a stochastic flow network have labels denoted by @xmath12 .",
    "a token will reach an output @xmath13 @xmath14 with probability @xmath15 , and we call @xmath15 the probability of @xmath13 and call @xmath16 the output probability distribution of the network , where @xmath17 .    in this paper we assume , without loss of generality , that the probability of each splitter is @xmath5 ( @xmath5-splitters can be implemented using three @xmath7-splitters for any @xmath7 ) .",
    "our goal is to realize the target probabilities or distributions by constructing a network of minimum size .",
    "in addition , we study the expected latency , namely the expected number of splitters a token need to pass before reaching the output ( or we call it the expected operating time ) .    the main contributions of the paper are    1 .   _",
    "general optimal construction : _ for any desired rational probability , an _ optimal - sized _ construction of stochastic flow network is provided .",
    "2 .   _ the power of feedback : _ we show that with feedback ( loops ) , stochastic flow networks can generate significantly more probabilities than those without feedback .",
    "constructions with well - bounded expected latency : _ we give two constructions whose expected latencies are well - bounded by constants . as a price",
    ", they use a few more splitters than the optimal - sized one .",
    "constructions for arbitrary rational distributions : _ we generalize our constructions so that they can realize an arbitrary rational probability distribution @xmath16 .    the remainder of this paper is organized as follows . in section [ section_preliminary ] we introduce some preliminaries including knuth and yao s scheme and a few mathematical tools for calculating the distribution of a given stochastic flow network .",
    "section [ section_part1 ] introduces an optimal - sized construction of stochastic flow networks for synthesizing an arbitrary rational probability , and it demonstrates that feedback significantly enhances the expressibility of stochastic flow networks .",
    "section [ section_properties ] analyzes the expected latency of the optimal - sized construction .",
    "section [ section_upg ] gives two constructions whose expected latencies are upper bounded by constants .",
    "section [ section_distributions ] presents the generalizations of our results to arbitrary rational probability distributions .",
    "the concluding remarks and the comparison of different stochastic systems are given in section [ section_conclusion ] .",
    "in this section , we introduce some preliminaries , including knuth and yao s scheme for simulating an arbitrary distribution from a biased coin , and how using absorbing markov chains or mason rule to calculate the output distribution of a given stochastic flow network .      in 1976 ,",
    "knuth and yao proposed a simple procedure for simulating an arbitrary distribution from an unbiased coin ( the probability of h and t is @xmath5 ) @xcite .",
    "they introduced a concept called generating tree for representing the algorithm @xcite .",
    "the leaves of the tree are marked by the output symbols , and the path from the root node to the leaves indicates the sequences of bits generated by the unbiased coin .",
    "starting from the root node , the scheme selects edges to follow based on the coin tosses until it reaches one of the leaves .",
    "then it outputs the symbol marked on that leaf .",
    "distribution.,height=172 ]    in general , we assume that the target distribution is @xmath18 . since all the leaves of the tree have probabilities of the form @xmath19 ( if the depth of the leaf is @xmath20 ) , we split each probability @xmath21 into atoms of this form .",
    "specifically , let the binary expansion of the probability @xmath21 be @xmath22 where @xmath23 or @xmath3 .",
    "then for each probability @xmath21 , we get a group of atoms @xmath24 . for these atoms ,",
    "we allot them to leaves with label @xmath25 on the tree .",
    "hence , the probability of generating @xmath25 is @xmath21 .",
    "we can see that the depths of all the atoms satisfy the kraft inequality @xcite , i.e. , @xmath26 so we can always construct such a tree with all the atoms allotted . knuth and",
    "yao showed that the expected number of fair bits required by the procedure ( i.e. the expected depth of the tree ) to generate a random variable @xmath27 with distribution @xmath28 lies between @xmath29 and @xmath30 where @xmath29 is the entropy of the target distribution .",
    "[ fig_example2 ] depicts a generating tree that generates a distribution @xmath31 , where the atoms for @xmath32 are @xmath33 , and the atoms for @xmath34 are @xmath35 .",
    "we see that the construction of generating trees is , in some sense , a special case of stochastic flow networks that without cycles .",
    "if we consider each node in the generating tree as a splitter , then each token that enters the tree from the root node will reach the outputs with the target distribution . while knuth and yao s scheme aims to minimize the expected depth of the tree ( or in our framework ,",
    "we call it the expected latency of the network ) , our goal is to optimize the size of the construction , i.e. , the number of nodes in the network .",
    "let s consider a stochastic flow network with @xmath2 splitters and @xmath36 outputs , in which each splitter is associated with a state number in @xmath37 and each output is associated with a state number in @xmath38 .",
    "when a token reaches splitter @xmath39 with @xmath40 , we say that the current state of this network is @xmath39 .",
    "when it reaches output @xmath20 with @xmath41 , we say that the current state of this network is @xmath42 .",
    "note that the current state of the network only depends on the last state , and when the token reach one output it will stay there forever .",
    "so we can describe token flows in this network using an absorbing markov chain . if the current state of the network is @xmath39 , then the probability of reaching state @xmath43 at the next instant of time is given by @xmath44 . here , @xmath45",
    "( @xmath46 ) if and only if state @xmath39 and state @xmath43 is connected by an edge @xmath47 ( @xmath48 )",
    ".    clearly , the network with @xmath2 splitters and @xmath36 outputs with different labels can be described by an absorbing markov chain , where the first @xmath2 states are transient states and the last @xmath36 states are absorbing states .",
    "and we have @xmath49    the transition matrix of this markov chain is given by @xmath50 where @xmath51 is an @xmath52 matrix , @xmath53 is an @xmath54 matrix , @xmath3 is an @xmath55 zeros matrix and @xmath56 is an @xmath57 identity matrix .",
    "let @xmath58 be the probability for an absorbing markov chain reaching the state @xmath59 if it starts in the transient state @xmath39 .",
    "then @xmath60 is an @xmath54 matrix , and @xmath61    assume this markov chain starts from state @xmath4 and let @xmath62 be the probability for it reaching the absorbing state @xmath59 . then @xmath63 is the distribution of the network @xmath64b= e_1(i - q)^{-1}r.\\ ] ]     distribution.,height=153 ]    given a stochastic flow network , we can use the formula above to calculate its probability distribution . for example , the transition matrix of the network in fig .",
    "[ fig_example3 ] is    @xmath65    from which we can obtain the probability distribution @xmath66      mason s gain rule is a method used in control theory to find the transfer function of a given control system . it can be applied to any signal flow graph .",
    "generally , we describe it as follows ( see more details about mason s rule in @xcite ) :    let @xmath67 denote the transfer function of a signal flow graph .",
    "define the following notations :    1 .",
    "@xmath68 determinant of the graph .",
    "@xmath69 number of forward paths , with @xmath70 , @xmath71 denoting the forward path gains .",
    "@xmath72 determinant of the graph that remains after deleting the @xmath20th forward path @xmath70 .    to calculate the determinant of a graph @xmath73",
    ", we list all the loops in the graph and their gains denoted by @xmath74 , all pairs of non - touching loops @xmath75 , all pairwise non - touching loops @xmath76 , and so forth . then @xmath77    the transfer function is @xmath78 called mason s rule .",
    "let s treat a stochastic flow network as a control system with input @xmath79 .",
    "applying mason s rule to this system , we can get the probability that one token reaches output @xmath20 with @xmath41 . also having the network in fig .",
    "[ fig_example3 ] as an example : in this network , we want to calculate the probability for a token to reach output @xmath4 ( for short , we call it as the probability of @xmath4 ) .",
    "since there is only one loop with gain @xmath80 and only one forward path with forward gain @xmath81 , we can obtain that the probability of @xmath4 is @xmath82 which accords with the result of absorbing markov chains .",
    "in fact , it can be proved that the mason s rule and the matrix form based on absorbing markov chains are equivalent .",
    "in this section we present an optimal - sized construction of stochastic flow networks .",
    "it consists of splitters with probability 1/2 and computes an arbitrary rational probability .",
    "we demonstrate that feedback ( loops ) in stochastic flow networks significantly enhance their expressibility . to see that ,",
    "let s first study stochastic flow networks without loops , and then those with loops .      here , we want to study the expressive power of loop - free networks .",
    "we say that there are no loops in a network if no tokens can pass any position in the network more than once . for loop - free networks",
    ", we have the following theorem :    for a loop - free network with @xmath2 @xmath5-splitters , any probability @xmath83 with integer @xmath84 can be realized , and only probabilities @xmath83 with integer @xmath84 can be realized .",
    "[ theorem_loopfree ]    \\a ) in order to prove that all probability @xmath83 with integer @xmath84 can be realized , we only need to provide the constructions of the networks .     for an integer @xmath84 .,width=153 ]    1 .",
    "construct a tree , as shown in fig .",
    "[ fig_tree1 ] . in this tree structure",
    ", each token will reach @xmath85 with probability @xmath86 , and reach @xmath87 with probability @xmath88 .",
    "2 .   let @xmath89 , where @xmath90 or @xmath4 .",
    "for each @xmath43 with @xmath91 , @xmath92 , we connect @xmath93 to output @xmath3 ; otherwise , we connect @xmath93 to output @xmath4 .",
    "then we connect @xmath87 to output @xmath4 .",
    "eventually , the probability for a token to reach output @xmath3 is @xmath94    using the procedure above , we can construct a network such that its probability is @xmath83 . actually , it is a special case of knuth and yao s construction @xcite .",
    "\\b ) now , we prove that only probability @xmath83 with integer @xmath84 can be realized .",
    "if this is true , then @xmath83 with odd @xmath95 can not be realized with less than @xmath2 splitters .",
    "it means that in the construction above , the network size @xmath2 is optimal .    according to mason s rule , for a network without loops , the probability for a token reaching one output is @xmath96 where @xmath97 is the path gain of a forward path from the root to the output .",
    "given @xmath2 splitters , the length of each forward path should be at most @xmath2 .",
    "otherwise , there must be a loop along this forward path ( have to pass the same splitter for at least two times ) .",
    "for each @xmath20 , @xmath97 can be written as @xmath98 for some @xmath99 . as a result",
    ", we can get that @xmath100 can be written as @xmath83 for some @xmath95 .",
    "we showed that stochastic flow networks without loops can only realize binary probabilities . here , we show that feedback ( loops ) plays an important rule in enhancing their expressibility .",
    "for example , with feedback , we can realize probability @xmath32 with only two splitters , as shown in fig .",
    "[ fig_example3 ] .",
    "but without loops , it is impossible ( or requires an infinite number of splitters ) to realize @xmath32 .",
    "more generally , for any desired rational probability @xmath0 with integers @xmath101 , we have the following theorem :    for a network with @xmath2 @xmath5-splitters , any rational probability @xmath0 with integers @xmath101 can be realized , and only rational probabilities @xmath0 with integers @xmath101 can be realized .",
    "[ theorem_gereral1 ]    \\a ) we prove that all rational probability @xmath0 with integers @xmath101 can be realized .",
    "when @xmath102 , the problem becomes trivial due to the result of theorem [ theorem_loopfree ] . in the following proof , without loss of generality ( w.l.o.g ) , we only consider the case in which @xmath103 for some @xmath2 .",
    "we first show that all probability distributions @xmath104 with integers @xmath105 s.t .",
    "@xmath106 can be realized with @xmath2 splitters .",
    "now let s construct the network iteratively .",
    "when @xmath107 , by enumerating all the possible connections , we can verify that all the following probability distributions can be realized : @xmath108 @xmath109 so all the probability distributions @xmath110 with integers @xmath105 s.t .",
    "@xmath111 can be realized .",
    "assume that all the probability distribution @xmath112 with integers @xmath105 s.t .",
    "@xmath113 can be realized by a network with @xmath20 splitters , then we show that any desired probability distribution @xmath114 s.t .",
    "@xmath115 can be realized with one more splitter . since @xmath115 , at least one of @xmath105 is even .",
    "w.l.o.g , we let @xmath95 be even . then there are two cases to consider : either both @xmath116 and @xmath117 are even , or both @xmath116 and @xmath117 are odd .",
    "when both @xmath116 and @xmath117 are even , the problem is trivial since the desired probability distribution can be written as @xmath118 , which can be realized by a network with @xmath20 splitters .",
    "when both @xmath116 and @xmath117 are odd , w.l.o.g , we assume that @xmath119 . in this case , we construct a network to realize probability distribution @xmath120 with @xmath20 splitters . by connecting the last output with probability @xmath121 to an additional splitter , we can get a new distribution @xmath122 .",
    "if we consider the second and the third output as a single output , then we can get a new network in fig .",
    "[ fig_loop3 ] , whose probability distribution is @xmath114 .    hence , for any probability distribution @xmath104 with @xmath123 , we can always construct a network with @xmath2 splitters to realize it .",
    "now , in order to realize probability @xmath0 with @xmath103 for some @xmath2 , we can construct a network with probability distribution @xmath124 with @xmath2 splitters and connect the last output ( output @xmath125 ) to the starting point of the network , as shown in fig .",
    "[ fig_loop6 ] . using the method of absorbing markov chains",
    ", we can obtain that the probability for a token to reach output @xmath3 is @xmath0 .",
    "a simple understanding for this result is that : ( 1 ) the ratio of the probabilities for a token to reach the first output and the second output is @xmath126 that equals @xmath127 ( 2 ) the sum of these two probabilities is @xmath4 , since the tokens will finally reach one of the two outputs .",
    "\\b ) now we prove that with @xmath2 splitters , only rational probability @xmath0 with integers @xmath101 can be realized . for any flow network with @xmath2 splitters",
    ", it can be described as an absorbing markov chain with @xmath2 transient states and @xmath125 absorbing states , whose transition matrix @xmath100 can be written as @xmath128 where each row consists of two @xmath5 entries and @xmath2 zeros .",
    "let @xmath129 then the probability distribution of the network can be written as @xmath130    in order to prove the result in the theorem , we only need to prove that @xmath131 can be written as @xmath132 with @xmath133 , where @xmath134 is an integer matrix ( all the entries in @xmath134 are integers ) .",
    "let @xmath135 , we know that @xmath136 is invertible if and only @xmath137 . in this case , we have @xmath138 where @xmath139 is defined as the determinant of the square matrix of order @xmath140 obtained from @xmath136 by removing the @xmath141 row and the @xmath142 column multiplied by @xmath143 .    since each entry of @xmath136",
    "is chosen from @xmath144 , @xmath139 can be written as @xmath145 for some integer @xmath146 and @xmath147 can be written as @xmath148 for some integer @xmath149 . according to lemma [ lemma_appendix1 ] in the appendix , we have @xmath150 , which leads us to @xmath151 ( note that @xmath137 ) .    then , we have that @xmath152    since each entry of @xmath53 is also in @xmath144 , we know that @xmath153 is an integer matrix .    as a result @xmath154 where each entry of @xmath134 is an integer .",
    "so all the probabilities in the final distribution are of the form @xmath0 .",
    "this completes the proof .",
    "based on the method in the theorem above , we can realize any arbitrary rational probability with an optimal - sized network .",
    "the construction has two steps :    1 .",
    "construct a network with output distribution @xmath124 iteratively using at most @xmath2 splitters .",
    "2 .   connect the last output to the starting point , such that the distribution of the resulting network is @xmath155 .    when @xmath102 for some @xmath2 , the construction above is exactly the generating tree construction in the knuth and yao s scheme as described in section [ section_preliminary ] .",
    "now , assume we want to realize probability @xmath156",
    ". we can first generate a probability distribution @xmath157 , which can be realized by adding one splitter to a network with probability distribution @xmath158 ... recursively , we can have the following probability distributions : @xmath159 @xmath160    as a result , we get a network to generate probability distribution @xmath157 , as shown in fig .",
    "[ fig_loop5 ] , where only @xmath161 splitters are used . connecting the last output to the starting point results in the network in fig .",
    "[ fig_loop7 ] with probability @xmath156 .",
    "comparing the results in theorem [ theorem_gereral1 ] with those in theorem [ theorem_loopfree ] , we see that introducing loops into networks can strongly enhance their expressibility .",
    "besides of network size , anther important issue of a stochastic flow network is the expected operating time , or we call it expected latency , defined as the expected number of splitters a token need to pass before reaching one of the outputs . for the optimal - sized construction proposed in the above section , we have the following results about its expected latency .",
    "let s prove this by induction . when @xmath165 or @xmath107 , it is easy to see that this conclusion is true .",
    "assume when @xmath166 , this conclusion is true , we want to show that the conclusion still holds for @xmath167 .",
    "note that in the optimal - sized construction , a network with size @xmath168 can be constructed by adding two more splitters to a network with size @xmath20 .",
    "let @xmath169 denote the latency of the network with size @xmath20 , then @xmath170=e[t_k]+p_1+p_2,\\ ] ] where @xmath171 is the probability for a token to reach the first additional splitter and @xmath172 is the probability for a token to reach the second additional splitter .",
    "assume the distribution of the network with size @xmath20 is @xmath173 , then @xmath174 so the conclusion is true for @xmath167 . by induction , we know that it holds for all @xmath175 .    secondly , we prove that if the expected latency of the network with distribution @xmath173 is @xmath176 , then by connecting its last output to its starting point , we can get a network such that its expected latency is @xmath177 .",
    "this conclusion can be obtained immediately from @xmath178                                    w. qian , m. d. riedel , h. zhou , and j. bruck ,  transforming probabilities with combinational logic \" , _ ieee trans . on computer - aided design of integrated circuits and systems _ , vol .",
    "1279 - 1292 , 2011 .",
    "given @xmath51 an @xmath52 matrix with each entry in @xmath144 , such that sum of each row is at most @xmath4 , then we have @xmath182 , where @xmath56 is an identity matrix and @xmath183 is the determinant of a matrix .",
    "[ lemma_appendix1 ]    before proving this lemma , we can see that for any given matrix @xmath51 , it has the following properties : for any @xmath184 such that @xmath185 , switching the @xmath141 row with the @xmath142 row then switching the @xmath141 column with the @xmath142 column , the determinant of @xmath135 keeps unchanged . and more , each entry of @xmath51 is still from @xmath144 and sum of each row of @xmath51 is at most @xmath4 .",
    "now , we call the transform above as equivalent transform of @xmath51",
    ".      assume the result of the lemma hold for @xmath188 matrix , we want to prove that this result also holds for @xmath52 matrix .",
    "now , given a @xmath52 matrix @xmath51 , according to the definition in the lemma , we know that the sum of all the entries in @xmath51 is at most @xmath2 .",
    "as a result , there exists a column such that the sum of the entries in the column is at most @xmath4 . using equivalent transform",
    ", we have that      now , for the @xmath189 column of @xmath190 , let s continue using the equivalent transform to move all the non - zero entries to the beginning of this column .",
    "the possible non - zero entry set of the @xmath189 column of @xmath190 is @xmath191    the first three cases , the result in the lemma can be easily proved . in the following proof ,",
    "we only consider the other cases ( let @xmath192 denote the non - zero entry set for the @xmath189 column of @xmath190 ) :            let @xmath199 , since both @xmath134 and @xmath60 has at most one non - zero entry @xmath5 , we know that each entry of @xmath200 is from @xmath144 , and the sum of all the entries is at most one .",
    "according to our assumption , we know that @xmath201 as a result , we have @xmath202                              where @xmath220 and @xmath221 finally , we can get that @xmath222+\\frac{1}{2 } \\det[i-\\left (                           \\begin{array}{c }                           b+f\\\\                           c+e\\\\                           d\\\\                           \\end{array }                         \\right)]\\end{aligned}\\ ] ]              it is easy to prove that the case for @xmath229 or @xmath230 is true . in the following proof ,",
    "we only show the case for @xmath231 briefly .",
    "w.l.o.g , we assume @xmath232 . without considering the order of the leaves , we have only two binary - tree structures , as shown in fig .",
    "[ fig_distribution4 ] .      in both of the structures , for any pair of leaves @xmath233 and @xmath234 , if @xmath233 s sibling is @xmath234 s ancestor then @xmath235",
    "otherwise , we can switch the position of @xmath233 and @xmath234 to reduce @xmath236 .",
    "so if the tree structure ( a ) in fig .",
    "[ fig_distribution4 ] is the optimal one , we have @xmath237 or @xmath238 . now , we will show that if the tree structure ( b ) in fig .",
    "[ fig_distribution4 ] is the optimal one , we also have @xmath237 or @xmath238 .",
    "so we can minimize @xmath248 instead of minimizing @xmath246 . fixing @xmath249",
    ", we can see that @xmath250 increases as @xmath95 increases when @xmath251 ; @xmath252 also increases as @xmath95 increases .",
    "so fixing @xmath249 , @xmath248 is minimized if and only if @xmath95 is minimized , which will cause @xmath237 or @xmath253 .",
    "based on the discussion above , we know that in the optimal tree , @xmath241 and @xmath242 must be siblings .",
    "let s replace @xmath241 , @xmath242 and their parent node using a leaf with weight @xmath254 .",
    "then we can get an optimal tree for distribution @xmath255 , whose @xmath246 value is @xmath256 . assume the optimal @xmath246 value for distribution @xmath257 is @xmath258 , then @xmath259    let s consider a tree constructed by huffman procedure for @xmath257 , whose @xmath246 value is @xmath260 . we want to show that this tree is optimal . according to the procedure , we know that @xmath241 and @xmath242 are also siblings . by combing @xmath241 and @xmath242 to a leaf with @xmath254 , we can get a new tree .",
    "this new tree can be constructed by applying huffman procedure to distribution @xmath255 . due to our assumption for @xmath230 ,",
    "it is optimal , as a result the following result is true , @xmath261"
  ],
  "abstract_text": [
    "<S> a stochastic flow network is a directed graph with incoming edges ( inputs ) and outgoing edges ( outputs ) , tokens enter through the input edges , travel stochastically in the network , and can exit the network through the output edges . </S>",
    "<S> each node in the network is a splitter , namely , a token can enter a node through an incoming edge and exit on one of the output edges according to a predefined probability distribution . </S>",
    "<S> stochastic flow networks can be easily implemented by dna - based chemical reactions , with promising applications in molecular computing and stochastic computing . in this paper </S>",
    "<S> , we address a fundamental synthesis question : given a finite set of possible splitters and an arbitrary rational probability distribution , design a stochastic flow network , such that every token that enters the input edge will exit the outputs with the prescribed probability distribution .    </S>",
    "<S> the problem of probability transformation dates back to von neumann s 1951 work and was followed , among others , by knuth and yao in 1976 . </S>",
    "<S> most existing works have been focusing on the  simulation \" of target distributions . in this paper , we design _ optimal - sized _ stochastic flow networks for  synthesizing \" target distributions . </S>",
    "<S> it shows that when each splitter has two outgoing edges and is unbiased , an arbitrary rational probability @xmath0 with @xmath1 can be realized by a stochastic flow network of size @xmath2 that is optimal . </S>",
    "<S> compared to the other stochastic systems , feedback ( cycles in networks ) strongly improves the expressibility of stochastic flow networks .    </S>",
    "<S> stochastic flow network , random - walk graph , probability synthesis . </S>"
  ]
}