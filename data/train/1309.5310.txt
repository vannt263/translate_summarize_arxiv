{
  "article_text": [
    "consider the classical linear forward model @xmath3 , which relates a parameter vector @xmath4 to an observation vector @xmath5 through a linear transformation ( henceforth referred to as a _ dictionary _ ) @xmath6 .",
    "this forward model , despite its apparent simplicity , provides a reasonable mathematical approximation of reality in a surprisingly large number of application areas and scientific disciplines @xcite .",
    "while the operational significance of this linear ( forward ) model varies from one application to another , the fundamental purpose of it in all applications stays the same : _ given knowledge of @xmath7 and @xmath8 , make an inference about @xmath9 .",
    "_ however , before one attempts to solve an inference problem using the linear model , it is important to understand the conditions under which doing so is even feasible .",
    "for instance , inferring anything about @xmath9 will be a moot point if the nullspace of @xmath8 were to contain @xmath9 .",
    "thus , a large part of the literature on linear models is devoted to characterizing conditions on @xmath8 and @xmath9 that facilitate reliable inference .",
    "classical literature on inference using linear models proceeds under the assumption that the number of observations @xmath10 equals or exceeds the number of parameters @xmath11 . in this",
    "setting , conditions such as @xmath8 being full column rank or @xmath12 being well conditioned  both of which can be explicitly verified  are common in the inference literature @xcite .",
    "in contrast , there has recently been a growing interest to study inference under linear models when @xmath10 is much smaller than @xmath11 .",
    "this setting is the hallmark of high - dimensional statistics @xcite , arises frequently in many application areas @xcite , and forms the cornerstone of the philosophy behind compressed sensing @xcite .",
    "it of course follows from simple linear algebra that inferring about every possible @xmath9 from @xmath13 is impossible in this setting ; instead , the high - dimensional inference literature commonly operates under the assumption that @xmath9 has only a few nonzero parameters  typically on the order of @xmath10and characterizes corresponding conditions on @xmath8 for reliable inference .",
    "some notable conditions in this regard include the spark @xcite , the restricted isometry property @xcite , the irrepresentable condition @xcite ( and its variant , the incoherence condition @xcite ) , the restricted eigenvalue assumption @xcite , and the nullspace property @xcite . while these and other conditions in the literature differ from each other in one way or the other , they all share one simple fact : _ requiring that @xmath8 satisfies one of these conditions implies that one or more column submatrices ( subdictionaries ) of @xmath8 must be full column rank and/or well conditioned . _ unfortunately , explicitly verifying that @xmath8 satisfies one of these properties is computationally daunting ( np - hard in some cases @xcite ) , while indirect means of verifying these conditions provide rather pessimistic bounds on the dimensions of subdictionaries of @xmath8 that are well conditioned @xcite .    in a recent series of influential papers , several researchers have managed to circumvent the pessimistic bounds associated with verifiable conditions on @xmath8 for high - dimensional inference by resorting to an _ average - case analysis _ @xcite .",
    "representative work by tropp @xcite , for instance , shows that _ most _ subdictionaries of @xmath8 having unit @xmath14-norm columns are guaranteed to be well conditioned when the number of columns in the subdictionary is proportional to @xmath15provided that correlations between the columns of @xmath8 do not exceed a certain threshold , a condition readily verifiable in polynomial time .",
    "in particular , these results imply that if @xmath8 is a unit norm tight frame @xcite , corresponding to @xmath16 , then it can be _ explicitly verified _ that most subdictionaries of @xmath8 of dimension @xmath17 are well conditioned .",
    "if there exist some @xmath18 such that @xmath19 for all @xmath20 . ] the biggest advantage of such _ average - case analysis _ results for the conditioning of subdictionaries of @xmath8 lies in their ability to facilitate tighter verifiable conditions for inference under the linear model using an arbitrary ( random or deterministic ) dictionary @xmath8 .",
    "several works in this regard have been able to leverage the results of @xcite to provide tighter verifiable conditions for average - case sparse recovery @xcite",
    "( i.e. , obtaining @xmath9 from @xmath21 _  la _ compressed sensing @xcite ) , average - case model selection @xcite ( i.e. , estimating locations of the nonzero entries of @xmath9 from @xmath22 ) , and average - case linear regression @xcite ( i.e. , estimating @xmath23 from @xmath24 ) .",
    "our focus in this paper is on inference under the linear model in the `` @xmath10 smaller than @xmath11 '' setting , in the case when @xmath9 not only has a few nonzero parameters , but also its nonzero parameters exhibit a certain _ block _ ( or _ group _ ) structure .",
    "specifically , we have @xmath25^*$ ] with @xmath26 for @xmath27 , @xmath28 , and only @xmath29 of the @xmath30 s are nonzero ( sub)vectors . such setups are often referred to as _ block sparse _ ( or _ group sparse _ ) and arise in various contexts in a number of inference problems @xcite . the most fundamental challenge for inference in this block - sparse setting",
    "then becomes specifying conditions under which one or more _ block subdictionaries _ of @xmath8 are full column rank and/or well conditioned .",
    "a number of researchers have made substantial progress in this regard recently , reporting conditions on @xmath8 in the block setting that mirror many of the ones reported in @xcite for the classical setup ; see , e.g. , @xcite .",
    "however , just like in the classical setup , verifying that @xmath8 satisfies one of these properties in the block setting ends up being either computationally intractable or results in rather pessimistic bounds on the dimensions of block subdictionaries of @xmath8 that are well conditioned .",
    "in contrast to these works , and in much the same way @xcite reasoned in the classical case , we are interested in overcoming the pessimistic bounds associated with verifiable conditions on @xmath8 for high - dimensional inference in the block - sparse setting by resorting to an average - case analysis .",
    "our first main contribution in this regard is a generalization of @xcite that establishes that _ most block subdictionaries _ of @xmath8 having unit @xmath14-norm columns are guaranteed to be well conditioned with the number of _ blocks _ in the subdictionary proportional to @xmath31 provided that @xmath8 satisfies a polynomial - time verifiable condition that we term the _ block incoherence condition_. in particular , these results also imply that if @xmath8 is a unit norm tight frame then it can be explicitly verified that most _ block subdictionaries _ of @xmath8 of dimension @xmath17 are well conditioned .    while our ability to guarantee that most block subdictionaries of a dictionary that satisfies the block incoherence condition are well conditioned makes us optimistic about the use of such dictionaries in inference problems , there remains an analytical gap in going from conditioning of block subdictionaries to performance of inference tasks .",
    "our second main contribution in this regard is the application of results concerning the conditioning of block subdictionaries to provide tighter verifiable conditions for average - case block - sparse recovery ( i.e. , obtaining @xmath9 from @xmath21 with @xmath9 being block sparse ) and average - case block - sparse regression ( i.e. , estimating @xmath23 from @xmath32 with @xmath9 being block sparse ) .",
    "last , but not least , we carry out a series of numerical experiments to highlight an aspect of inference under the linear model that is rarely discussed in the related literature : _ the spectral norm of the dictionary @xmath33 influences the inference performance much more than any of its other measures_. specifically , our results show that performances of block - sparse recovery and regression are inversely proportional to @xmath34 and tend to be independent of correlations between the columns of @xmath8 for the most part  an outcome that also hints at the possible ( orderwise ) tightness of our results concerning the conditioning of block subdictionaries .",
    "the following notation will be used throughout the rest of this paper .",
    "we use uppercase and lowercase roman / greek letters for matrices and vectors / scalars , respectively .",
    "given a vector @xmath35 , we use @xmath36 and @xmath37 to denote the usual @xmath38 norm and conjugate transpose of @xmath35 , respectively .",
    "we define the _ scalar _ sign operator for @xmath39 as @xmath40 , while we use @xmath41 for a vector @xmath35 to denote entry - wise sign operation .",
    "in addition , we define the _ vector _ sign operator for a vector @xmath35 as @xmath42 , which returns the unit - norm vector pointing in the direction of @xmath35 .",
    "given two vectors @xmath43 and @xmath35 , we define the inner product between them as @xmath44 . given a matrix @xmath45 ,",
    "we use @xmath46 and @xmath47 to denote the spectral norm ( @xmath48 ) and the adjoint operator of @xmath45 , respectively . in addition , assuming @xmath45 has unit @xmath14-norm columns and using @xmath49 to denote the @xmath50 column of @xmath45 , the _ coherence _ of @xmath45 is defined as @xmath51 . given a set @xmath52 ,",
    "we use @xmath53 ( resp .",
    "@xmath54 ) to denote the submatrix ( resp .",
    "subvector ) obtained by retaining the columns of @xmath45 ( resp .",
    "entries of @xmath35 ) corresponding to the indices in @xmath52 .",
    "given a random variable @xmath55 , we use @xmath56 $ ] to denote @xmath57\\big)^{1/q}$ ] .",
    "finally , @xmath58 denotes the identity operator and @xmath59 denotes a kronecker product .",
    "the rest of this paper is organized as follows .",
    "section  [ sec : result ] presents the main result of this paper concerning the conditioning of block subdictionaries .",
    "section  [ sec : grouprecovery ] leverages the result of section  [ sec : result ] and presents an average - case analysis of convex optimization - based block - sparse recovery from noiseless measurements , along with some discussion and numerical experiments .",
    "section  [ sec : grouplasso ] makes use of the result of section  [ sec : result ] to present an average - case analysis of block - sparse regression and the associated numerical experiments .",
    "finally , some concluding remarks are provided in section  [ sec : conc ] . for the sake of clarity of exposition",
    ", we relegate the proofs of most of the lemmas and theorems to several appendices .",
    "in this section , we state and discuss the main result of this paper concerning the conditioning of block subdictionaries of the @xmath60 dictionary @xmath8 . here , and",
    "in the following , it is assumed that @xmath8 has a block structure that comprises @xmath61 blocks of dimensions @xmath62 each ; in particular , we can write without loss of generality that @xmath63 $ ] , where each block @xmath64 $ ] is an @xmath65 matrix .",
    "we also assume throughout this paper that the columns of @xmath8 are normalized : @xmath66 for all @xmath67 , @xmath68 .",
    "the problem we are interested in addressing in this section is the following .",
    "let @xmath69 with @xmath70 and define an @xmath71 block subdictionary @xmath72 $ ] .",
    "then what are the conditions on @xmath8 that will guarantee that the singular values of @xmath73 concentrate around unity ?",
    "since addressing this question for an _ arbitrary _ subset @xmath52 is known to lead to either nonverifiable conditions or pessimistic bounds on @xmath74 ( cf .",
    "section  [ ssec : contributions ] ) , our focus here is on a subset @xmath52 that is drawn uniformly at random from all @xmath75 possible @xmath74-subsets of @xmath76 .",
    "our main result concerning the conditioning of random block subdictionaries relies on a condition that we term the _ block incoherence condition _ ( bic ) .",
    "define the intra - block coherence of the dictionary @xmath8 as @xmath77 and the inter - block coherence of the dictionary @xmath8 as . ]",
    "@xmath78 we say that @xmath8 satisfies the _ block incoherence condition _ ( bic ) with parameters @xmath79 if @xmath80 and @xmath81 for some positive numerical constants @xmath82 and @xmath83 .",
    "note that @xmath84 measures the deviation of individual blocks @xmath85 from being orthonormal and is identically equal to zero for the case of orthonormal blocks .",
    "in contrast , @xmath86 measures the similarity between different blocks and can not be zero in the @xmath10 smaller than @xmath11 setting .",
    "informally , the bic dictates that individual blocks of @xmath8 do not diverge from being orthonormal in an unbounded fashion and the dissimilarity between different blocks scales as @xmath87 .",
    "the most desirable aspect of the bic is that it can be verified in polynomial time .",
    "we are now ready to state our first result .",
    "[ thm : rand_cond ] suppose that the @xmath88 dictionary @xmath89 $ ] satisfies the bic with parameters @xmath79 .",
    "let @xmath52 be a @xmath74-subset drawn uniformly at random from all @xmath90 possible @xmath74-subsets of @xmath76 .",
    "then , as long as @xmath91 for some positive numerical constant @xmath92 that depends only on @xmath93 , the singular values of the block subdictionary @xmath72 $ ] satisfy @xmath94 $ ] , @xmath95 , with probability with respect to the random choice of the subset @xmath52 of at least @xmath96 .    the interval @xmath97 $ ] in theorem  [ thm : rand_cond ] is somewhat arbitrary . in general",
    ", it can be replaced with @xmath98 $ ] for any @xmath99 , resulting in the probability of success either increasing ( @xmath100 ) or decreasing ( @xmath101 ) .    in words ,",
    "theorem  [ thm : rand_cond ] states that if a dictionary satisfies the bic then most of its block subdictionaries of dimensions @xmath71 act as isometries on @xmath102 for @xmath103 . in order to better understand the bound @xmath103 , notice that @xmath104 for the case of a normalized dictionary @xcite , implying @xmath105 .",
    "more importantly , the equality @xmath106 is achievable by dictionaries with orthogonal rows ( also referred to as tight frames @xcite ) , implying theorem  [ thm : rand_cond ] allows optimal scaling of the dimensions of well - conditioned block subdictionaries .",
    "perhaps the most surprising aspect of this theorem , which sets it apart from other works on inference under linear models in block settings @xcite , is the assertion it makes about the effects of different measures of @xmath8 on the conditioning of random block subdictionaries .",
    "roughly , theorem  [ thm : rand_cond ] suggests that as soon as the bic is satisfied , both @xmath84 and @xmath86 stop playing a role in determining the order - wise dimensions of the subdictionaries that are well conditioned ; rather , it is the spectral norm of the dictionary @xmath33 that plays a primary role in this regard .",
    "such an assertion of course needs to be carefully examined , given that theorem  [ thm : rand_cond ] is only concerned with sufficient conditions .",
    "nevertheless , carefully planned numerical experiments carried out in the context of block - sparse recovery ( cf .",
    "section  [ sec : grouprecovery ] ) and block - sparse regression ( cf .",
    "section  [ sec : grouplasso ] ) are consistent with this assertion .",
    "the proof of theorem  [ thm : rand_cond ] leverages the analytical tools employed by tropp in @xcite for conditioning of canonical ( i.e. , non - block ) random subdictionaries , coupled with a _ poissonization _ argument that is now standard in the literature ( see , e.g , @xcite ) . to proceed , we define @xmath107 independent and identically distributed ( i.i.d . ) bernoulli random variables @xmath108 with parameter @xmath109 ( i.e. , @xmath110 ) and a random set @xmath111 .",
    "next , we define a random block subdictionary @xmath112 $ ] and use @xmath113 and @xmath114 to denote the hollow gram matrix of @xmath8 and the hollow gram matrix of @xmath115 , respectively . finally , define @xmath116 to be a random diagonal matrix , @xmath117 to be a block masking matrix , and notice from definition of the spectral norm that @xmath118 . using this notation",
    ", we can show that the @xmath119 norm of the random variable @xmath120 for @xmath121 is controlled by @xmath84 , @xmath86 , and @xmath33 .",
    "[ lem : cond_block ] for @xmath122 and @xmath123 , the @xmath119 norm of the random variable @xmath124 can be bounded as @xmath125    the proof of lemma  [ lem : cond_block ] , which is fundamental to the proof of theorem  [ thm : rand_cond ] and comprises novel generalizations of some of the results in @xcite to the block setting of this paper , is provided in appendix  [ sec : mainproof ] .",
    "we are now ready to provide a proof of theorem  [ thm : rand_cond ] .",
    "define @xmath126 and notice that @xmath127 $ ] , @xmath95 , if and only if @xmath128 . instead of studying @xmath129 directly , however , we first study the related random variable @xmath130 , where @xmath115 is the random subdictionary defined in relation to lemma  [ lem : cond_block ] .",
    "it then follows from the markov inequality and lemma  [ lem : cond_block ] that @xmath131)^q\\nonumber\\\\ & \\le 2^q \\left(48\\mu_b\\log{p}+17\\sqrt{\\delta \\log{p}(1+\\mu_i)}\\|\\phi\\|_2 + 2\\delta\\|\\phi\\|_2 ^ 2 + 3\\mu_i\\right)^q ,   \\label{eq : zprime_prf}\\end{aligned}\\ ] ] where @xmath132 .",
    "next , our goal is to show that for all @xmath133 , @xmath134 using the poissonization argument from  @xcite . toward this end",
    ", we explicitly write @xmath135 and note that @xmath136 where @xmath137 is a subset drawn uniformly at random from all @xmath138 possible @xmath139-subsets of @xmath76 .",
    "we now make two observations .",
    "first , @xmath140 is a binomial random variable with parameters @xmath141 and therefore @xmath142 due to @xmath74 being the median of @xmath140 .",
    "second , since each ( random ) submatrix @xmath143 for a given value of @xmath144 is a submatrix of some ( random ) @xmath145 and the spectral norm of a matrix is lower bounded by that of its submatrices , we have that @xmath146 is a nondecreasing function of @xmath147",
    ". therefore we can write @xmath148 where the last equality follows since @xmath149 and @xmath52 have the same probability distribution . by combining ( [ eq : zprime_prf ] ) and ( [ eq : poisscp_prf ] )",
    ", we therefore obtain @xmath150 finally , the expression inside parentheses in the above equation can be bounded by @xmath151 for small - enough constants @xmath152 , and @xmath83 , resulting in @xmath153 .      among existing works in the literature focusing on the conditioning of random ( non - block ) subdictionaries @xcite , @xcite and @xcite are the ones with the most general and strongest results . specifically , @xcite deal with the case of the dictionary @xmath8 being a concatenation of two orthonormal bases , while @xcite studies the case of @xmath8 being a disjoint union of orthonormal bases .",
    "the results in @xcite and @xcite are related to each other in the sense that @xcite extends @xcite to the case when the subdictionaries of @xmath8 are not necessarily selected uniformly at random .",
    "the proof technique employed in this paper for conditioning of random block subdictionaries is inspired by @xcite and is rather tight in the sense that in the case of @xmath154 , @xmath155 , and for a unit - norm dictionary @xmath8 , lemma  [ lem : cond_block ] reduces to ( * ? ? ? * corollary  5.2 ) . while we believe our result can be extended to the case when the random block subdictionaries of @xmath8 are selected with a more `` structured randomness '' by leveraging the insights offered by @xcite",
    ", we leave this for future work .",
    "it is also instructive to note that while theorem  [ thm : rand_cond ] is the most general incarnation of results concerning conditioning of random block subdictionaries , it is rather straightforward to specialize this result for conditioning of random block subdictionaries of _ structured _ dictionaries .",
    "next , we specialize theorem  [ thm : rand_cond ] to one such structure that corresponds to @xmath8 being a kronecker product of an arbitrary unit - norm dictionary and a dictionary with orthonormal columns . such _ kronecker - structured _ dictionaries arise in many contexts  @xcite and have a special connection to the literature on multiple measurement vectors ( mmv ) @xcite and multivariate linear regression @xcite problems . the following section therefore will also help understand our work in the context of these two research areas .",
    "consider an arbitrary , unit - norm @xmath156 dictionary @xmath157 and an @xmath158 dictionary @xmath159 with orthonormal columns ( i.e. , @xmath160 ) , where @xmath161 , @xmath162 , and @xmath163 .",
    "then a corollary of theorem  [ thm : rand_cond ] is that conditioning of random block subdictionaries of the kronecker - structured dictionary @xmath164 is simply a function of the coherence , @xmath165 , and spectral norm , @xmath166 , of @xmath157 , where @xmath167 denotes the @xmath50 column of @xmath157 . formally , this corollary has the following statement .",
    "[ cor : kron_rand_cond ] suppose that the @xmath88 dictionary @xmath164 with @xmath168 and @xmath169 for a positive numerical constant @xmath83 .",
    "let @xmath52 be a @xmath74-subset drawn uniformly at random from all @xmath90 possible @xmath74-subsets of @xmath76 . then",
    ", as long as @xmath170 for some positive numerical constant @xmath171 , the singular values of the block subdictionary @xmath172 $ ] satisfy @xmath173 $ ] , @xmath95 , with probability at least @xmath96 . here",
    ", the probability is with respect to the random choice of the subset @xmath52 .",
    "corollary  [ cor : kron_rand_cond ] is a simple consequence of properties of kronecker product . in terms of the spectral norm of @xmath8",
    ", we have @xmath174 . in terms of the intra- and inter - block coherences",
    ", we note that @xmath175 which trivially leads to @xmath176 note that corollary  [ cor : kron_rand_cond ] is not the tightest possible result for kronecker - structured dictionaries since theorem  [ thm : rand_cond ] does not exploit any dictionary structure .",
    "in particular , one can obtain a variant of corollary  [ cor : kron_rand_cond ] in which the @xmath177 terms are replaced with the @xmath178 terms by explicitly accounting for the kronecker structure in the proof of lemma  [ lem : cond_block ] .",
    "we conclude this section by connecting corollary  [ cor : kron_rand_cond ] to the mmv / multivariate linear regression problem , which will help clarify the similarities and differences between our work and mmv - related works .",
    "the inference problems studied under the mmv setting are essentially special cases of inference in the block - sparse setting studied here . in the mmv setting",
    ", it is assumed there are @xmath179 parameter vectors , @xmath180 , collected as @xmath179 columns of an @xmath181 matrix @xmath182 . in addition , each @xmath183 is observed using the same @xmath184 dictionary @xmath45 , @xmath185 , and the observation vectors @xmath186 are collected as @xmath179 columns of an @xmath65 matrix @xmath187 . a typical assumption in this mmv setting states that the @xmath179 different parameter vectors share locations of their @xmath188 nonzero entries , resulting in @xmath182 having no more than @xmath74 nonzero _",
    "rows_. it is however easy to see that if we define @xmath189 and @xmath190 then @xmath191 , where the vector @xmath192 exhibits block sparsity . in other words ,",
    "inference in the mmv setting requires understanding the conditioning of block subdictionaries of @xmath193 . in this case , we already know from corollary  [ cor : kron_rand_cond ] that the conditioning of random block subdictionaries of @xmath193 is simply a function of the coherence and spectral norm of @xmath45 .",
    "interestingly , while there exists a significant body of literature in the mmv setting @xcite , most of these works do not provide near - optimal , verifiable conditions for guaranteeing success of mmv - based inference problems .",
    "the most notable exception to this is the recent work @xcite , which studies the problem of noiseless recovery in the mmv setting .",
    "nonetheless , our block - sparsity results ( including the forthcoming noiseless recovery results ) are much more general than the ones in @xcite because of the mmv setting being just a special case of corollary  [ cor : kron_rand_cond ] in the block - sparse setting .",
    "we now shift our focus to the applicability of theorem  [ thm : rand_cond ] in the context of inference problems .",
    "we first begin with the problem of recovery of @xmath9 from @xmath21 when the signal @xmath9 is block sparse .",
    "block sparsity is one of the most popular structures used in sparse signal recovery problems .",
    "it is also intrinsically linked with the multiple measurement vectors ( mmv ) problem described in section  [ sec : mmv ] , as there is an equivalent block - sparse formulation for each mmv problem .",
    "block sparsity arises in many applications , including union - of - subspaces models  @xcite , multiband communications  @xcite , array processing  @xcite , and multi - view medical imaging  @xcite .",
    "because of the relevance of block sparsity in these and other applications , significant efforts have been made toward development of block - sparse signal recovery methods / algorithms and matching guarantees on the number of measurements required for successful recovery @xcite .",
    "however , the results reported in some of these works are only applicable in the case of randomized dictionary constructions  @xcite , while those reported in other works rely on dictionary conditions that either can not be explicitly verified in polynomial time @xcite or result in a suboptimal scaling of the number of measurements due to their focus on the worst - case performance @xcite .    to the best of our knowledge ,",
    "the only work that does not have the aforementioned limitations is @xcite . nonetheless , the focus in @xcite is only on the restrictive mmv problem , rather than the general block - sparse signal recovery problem .",
    "in addition , the analytical guarantees provided in @xcite rely on the nonzero entries of @xmath9 following either gaussian or spherical distributions . in contrast , we make use of the main result of section  [ sec : result ] in the following to state a result for average - case recovery of block - sparse signals that suffers from none of these and earlier limitations .",
    "our result depends primarily on the spectral norm of @xmath8 , while it has a mild dependence on the intra- and inter - block coherence through the bic ; all three of these quantities can be explicitly computed in polynomial time .",
    "it further requires only weak assumptions on the distribution of the nonzero entries of @xmath9 .",
    "equally important , the forthcoming result does not suffer from the so - called `` square - root bottleneck '' @xcite ; specifically , it allows near - optimal scaling of the sparsity level @xmath194 as a function of the number of measurement @xmath10 for dictionaries @xmath8 with small spectral norms ( e.g. , tight frames ) .",
    "our exposition throughout the rest of this section will be based upon the following formulation .",
    "we are interested in recovering a block - sparse signal @xmath4 from noiseless measurements @xmath195 , where the dictionary @xmath8 denotes an @xmath88 observation matrix with @xmath196 and @xmath197 denotes the observation vector .",
    "we assume @xmath9 comprises a total of @xmath107 blocks , each of size @xmath179 ( yielding @xmath28 ) , and represent it without loss of generality as @xmath25^*$ ] with each block @xmath26 . in order to make this problem well posed , we require that @xmath9 is @xmath74-block sparse with @xmath198 .",
    "finally , we impose a mild statistical prior on @xmath9 , as described below .    1 .   the _ block support _ of @xmath9 , @xmath199 , has a uniform distribution over all @xmath74-subsets of @xmath76 , 2 .",
    "entries in @xmath9 have zero median ( i.e. , the nonzero entries are equally likely to be positive and negative ) : @xmath200 , and 3 .",
    "nonzero blocks of the block - sparse signal @xmath9 have statistically independent `` directions . ''",
    "specifically , we require @xmath201 , where @xmath202 with @xmath203 denoting the unit sphere in @xmath204 .",
    "note that m2 and m3 are trivially satisfied in the case of the nonzero blocks of @xmath9 drawn independently from either gaussian or spherical distributions .",
    "however , it is easy to convince oneself that many other distributions  including those that are not absolutely continuous  will satisfy these two conditions . conditions m1m3 provide a probabilistic characterization of block - sparse @xmath9 that is inspired by tropp @xcite and cands and plan @xcite in which a related characterization of non block - sparse @xmath9 helped them overcome some analytical hurdles in relation to performance specifications of sparse recovery and regression problems , respectively .      in this section ,",
    "we are interested in understanding the average - case performance of the following mixed - norm convex optimization program for recovery of block - sparse signals satisfying m1m3 : @xmath205 where the @xmath206 norm of a vector @xmath207 containing @xmath107 blocks of @xmath179 entries each is defined as @xmath208 . while has been utilized in the past for recovery of block sparse signals ( see , e.g. , @xcite )",
    ", an average - case analysis result along the following lines is novel .",
    "the following theorem is proven in appendix  [ sec : l12proof ] .",
    "suppose that @xmath4 is @xmath74-block sparse and it is drawn according to the statistical model m1 , m2 , and m3 .",
    "further , assume that @xmath9 is observed according to the linear model @xmath195 , where the @xmath209 matrix @xmath8 satisfies the bic with some parameters @xmath93 . then , as long as @xmath210 for some positive numerical constant @xmath211 , the minimization ( [ eq : l21min ] ) results in @xmath212 with probability at least @xmath213 .",
    "[ thm : l12 ]    interestingly , theorem  [ thm : l12 ] specialized to the case of non - block sparse signals ( by setting @xmath214 and @xmath215 ) gives us an average - case analysis result for recovery of sparse signals that has never been explicitly stated in prior works .",
    "the optimization program in this case reduces to the standard _",
    "basis pursuit _",
    "program @xcite : @xmath216 the bic reduces to a bound on the coherence of @xmath8 , and theorem  [ thm : l12 ] reduces to the following corollary .. while it is possible to leverage the results in @xcite for obtaining corollary  [ thm : l1 ] , rather than obtaining corollary  [ thm : l1 ] from theorem  [ thm : l12 ] in this paper , such a result does not explicitly exist in prior literature to the best of our knowledge . ]    suppose @xmath4 is @xmath74-sparse , its support ( i.e. , locations of its nonzero entries ) is a @xmath74-subset drawn uniformly at random from all @xmath217 possible @xmath74-subsets of @xmath218 , its nonzero entries are drawn from a multivariate distribution with zero median ( i.e. , the nonzero entries are equally likely to be positive and negative ) , and the signs of the nonzero entries are independent .",
    "then , as long as @xmath219 and @xmath220 for some positive numerical constants @xmath221 and @xmath222 , the minimization ( [ eq : l1 ] ) successfully recovers @xmath9 from @xmath195 with probability at least @xmath213.[thm : l1 ]    we now elaborate on the similarities and differences between our ( average - case ) guarantees for recovery of block - sparse ( theorem  [ thm : l12 ] ) and non - block sparse ( corollary  [ thm : l1 ] ) signals . in terms of similarities ,",
    "both results allow for the same scaling of the _ total number of nonzero entries _ in @xmath9 : @xmath223 in the case of block - sparse signals and @xmath224 in the case of sparse signals .",
    "however , while corollary  [ thm : l1 ] requires that the inner product of any two columns in @xmath8 be @xmath225 , theorem  [ thm : l12 ] allows for less restrictive inner products of columns _ within _ blocks as long as @xmath226 .",
    "stated differently , explicit exploitation of the block structure in enables us to shift the @xmath227 scaling requirement from @xmath228 in corollary  [ thm : l1 ] to @xmath86 in theorem  [ thm : l12 ] .",
    "similarly , while corollary  [ thm : l1 ] requires that the signs of the nonzero entries in @xmath9 be independent , theorem  [ thm : l12 ] allows for correlations among the signs of entries _ within _ nonzero blocks . with the caveat that theorem  [ thm : l12 ] and corollary  [ thm : l1 ] only specify sufficient conditions , these two results seem to suggest that explicitly accounting for block structures in sparse signals allows one to expand the classes of sparse signals @xmath9 _ and _ dictionaries @xmath8 under which successful ( average - case ) recovery can be guaranteed .    next , we comment on the tightness of the scaling on the number of nonzero entries in theorem  [ thm : l12 ] and corollary  [ thm : l1 ] .",
    "assuming appropriate conditions on statistical properties of @xmath9 and ( intra-/inter - block ) coherence of @xmath8 are satisfied , both results allow for the number of nonzero entries to scale like @xmath229 for dictionaries @xmath8 that are `` approximately '' tight frames @xcite : @xmath230 .",
    "this suggests a near - optimal nature of both results ( modulo perhaps @xmath231 factors ) as one can not expect better than linear scaling of the number of nonzero entries as a function of the number of observations . in particular , existing literature on frame theory @xcite can be leveraged to specialize these results for oft - used classes of random dictionaries ( e.g. , gaussian , random partial fourier ) and to establish that in such cases the scaling of our guarantee matches that obtained using nonverifiable conditions such as the restricted isometry property @xcite .",
    "additionally , we note that when corollary  [ thm : l1 ] is specialized to the case of ( approximately ) tight frames we obtain average - case guarantees somewhat similar to the ones reported in ( * ? ? ?",
    "* theorem 14 ) .",
    "the main difference between the two results is the role that the coherence @xmath228 plays in the guarantees . in (",
    "* theorem 14 ) , the maximum allowable sparsity @xmath74 is required to be inversely proportional to @xmath232 .",
    "in contrast , we assert that the maximum allowable sparsity scaling is not fundamentally determined by the coherence .",
    "numerical experiments reported in the following section suggest that this is indeed the case .    despite the order - wise tightness of theorem  [ thm : l12 ] for tight frames , it is important to note that the bound on @xmath74 in that theorem is only a sufficient condition . in particular , we do not have a converse that shows the impossibility of recovering @xmath9 from @xmath7 when this bound is violated . to the best of our knowledge , however , no such converses exist even in the non block - sparse setting for arbitrary dictionaries and/or non - asymptotic analysis ( cf .",
    "@xcite ) .",
    "one of the fundamental takeaways of this section is that the spectral norm of the dictionary , rather than the ( intra-/inter - block ) coherence of the dictionary , determines the maximum allowable sparsity in ( block)-sparse signal recovery problems . in order to experimentally verify this insight",
    ", we performed a set of block - sparse signal recovery experiments with carefully designed dictionaries having varying spectral norms and coherence values . throughout our experiments",
    ", we set the signal length to @xmath233 , the block size and the number of blocks to @xmath234 and @xmath235 , respectively , and the number of observations to @xmath236 ( computed from the bound in  @xcite for @xmath237 nonzero blocks ) . in order to design our dictionaries , we first used matlab s random number generator to obtain 2000 matrices with unit - norm columns .",
    "next , we manipulated the singular values of each of these matrices to increase their spectral norms by a set of integer multipliers @xmath238 . finally , for each of the latexmath:[$2000\\cdot    obtain our dictionaries and recorded their spectral norms @xmath33 , coherences @xmath228 , inter - block coherences @xmath240 , and intra - block coherences @xmath241 .",
    "we evaluate the block - sparse signal recovery performance of each resulting dictionary @xmath8 using monte carlo trials , corresponding to the generation of 1000 block - sparse signals with @xmath74 nonzero blocks .",
    "each signal has block support selected uniformly at random according to m1 and nonzero entries drawn independently from the standard gaussian distribution @xmath242 .",
    "we then obtain the observations @xmath195 using the dictionary @xmath8 under study for each one of these signals and perform recovery using the minimization ( [ eq : l21min ] ) .",
    "we define successful recovery to be the case when the block support of @xmath243 matches the block support of @xmath9 and the submatrix of @xmath8 with columns corresponding to the block support of @xmath9 has full rank .",
    "figure  [ fig : same_mu ] shows the performances of dictionaries @xmath8 of increasing spectral norms ( @xmath244 ) , where we choose the dictionary ( among the 2000 available options ) whose coherence value is closest to 0.2 .",
    "the spectral norms , coherences , inter - block coherences , and intra - block coherences for these chosen dictionaries are collected in table  [ table : same_mu ] .",
    "the performance is shown as a function of the number of nonzero blocks @xmath74 in the signal .",
    "the figure shows a consistent improvement in the values of @xmath74 for which successful recovery is achieved as the spectral norm of the dictionary decays , even though @xmath228 does not significantly change among the dictionaries .     with varying spectral norms and roughly equal coherences in block - sparse signal recovery as a function of the number of nonzero blocks @xmath74 ;",
    "@xmath245 denotes the value of the spectral norm multiplier used to generate the dictionary.,width=288 ]    .spectral norms and coherences for the dictionaries used in the experiments of figure  [ fig : same_mu ] . [ cols=\"<,^,^,^,^\",options=\"header \" , ]",
    "in this section , we leverage theorem  [ thm : rand_cond ] to obtain average - case results for linear regression of block - sparse vectors , defined as estimating @xmath23 from @xmath246 when @xmath9 has a block - sparse structure .",
    "in particular , we focus on two popular convex optimization - based methods , the lasso  @xcite and the group lasso  @xcite , for characterizing results for linear regression of block - sparse vectors .",
    "our focus on these two methods is due to their widespread adoption by the signal processing and statistics communities . in the signal processing literature ,",
    "these methods are typically used for efficient sparse approximations of arbitrary signals in overcomplete dictionaries . in the statistics literature , they are mostly used for efficient variable selection and reliable regression under the linear model assumption .",
    "nonetheless , ample empirical evidence in both fields suggests that an appropriately regularized group lasso can outperform the lasso whenever there is a natural grouping of the dictionary atoms / regression variables in terms of their contributions to the observations  @xcite . in this section ,",
    "we analytically characterize the linear regression performances of both the lasso and the group lasso for block - sparse vectors , which helps us highlight one of the ways in which the group lasso might outperform the lasso for regression problems .",
    "note that analytical characterization of the group lasso using @xmath247 regularization for the `` underdetermined '' setting , in which one can have far more regression variables than observations ( @xmath196 ) , has received attention recently in the statistics literature @xcite . however , prior analytical work on the performance of the group lasso either studies an asymptotic regime  @xcite , focuses on random design matrices ( i.e. , dictionaries )  @xcite , and/or relies on conditions that are either computationally prohibitive to verify  @xcite or that do not allow for near - optimal scaling of the number of observations with the number of active blocks of regression variables @xmath74  @xcite .",
    "in contrast , our analysis for the regression performance of the group lasso using @xmath247 regularization in the underdetermined case for block - sparse vectors circumvents these shortcomings of existing works by adopting a probabilistic model , described by the conditions m1m3 in section  [ ssec : stat_model ] , for the blocks of regression coefficients in @xmath9 . to the best of our knowledge , the result stated in the sequel concerning the linear regression performances of the group lasso regularization as `` group lasso '' throughout the rest of this paper for brevity . ] for block - sparse vectors is the first one for block linear regression that is non - asymptotic in nature and applicable to arbitrary design matrices through verifiable conditions , while still allowing for near - optimal scaling of the number of observations with the number of blocks of nonzero regression coefficients .",
    "our proof techniques are natural extensions of the ones used in @xcite for the non - block setting and rely on theorem  [ thm : rand_cond ] for many of the key steps .",
    "this section concerns regression in the `` underdetermined '' setting for the case when the observations @xmath197 can be approximately explained by a linear combination of a small number of blocks ( @xmath248 ) of regression variables ( predictors ) .",
    "mathematically , we have that @xmath249 , where @xmath8 denotes the design matrix ( dictionary ) containing one regression variable per column , @xmath250^*$ ] denotes the @xmath74-block sparse vector of regression coefficients corresponding to these variables ( i.e. , @xmath198 ) , and @xmath251 denotes the modeling error . here , we assume without loss of generality that @xmath8 has unit - norm columns , while we assume the modeling error @xmath252 to be an i.i.d . gaussian vector with variance @xmath253 .",
    "finally , in keeping with the earlier discussion , we impose a mild statistical prior on the vector of regression coefficients @xmath9 that is given by the conditions m1 , m2 , and m3 in section  [ ssec : stat_model ] . the fundamental goal in here then is to obtain an estimate @xmath243 from the observations @xmath7 such that @xmath254 is as close to @xmath255 as possible , where the closeness is measured in terms of the @xmath14 regression error , @xmath256 .",
    "in this section , we are interested in understanding the average - case regression performance of two methods in the block - sparse setting .",
    "the first one of these methods is the lasso @xcite , which ignores any grouping of the regression variables and estimates the vector of regression coefficients as @xmath257 where @xmath258 is a tuning parameter . in terms of a baseline result for the lasso , we can extend the probabilitic model of cands and plan @xcite for non - block linear regression to the block setting and state the following theorem that follows from theorem  [ thm : rand_cond ] in this paper and the proof of ( * ? ? ?",
    "* theorem  1.2 ) .",
    "suppose that the vector of regression coefficients @xmath4 is @xmath74-block sparse and that the observation vector can be modeled as @xmath259 with the modeling error @xmath252 being i.i.d .",
    "gaussian with variance @xmath253 .",
    "further , assume that @xmath9 is drawn according to the statistical model m1 and m2 with the signs of its nonzero entries being i.i.d .",
    ", and the @xmath88 matrix @xmath8 satisfies ( @xmath0 ) @xmath260 and ( @xmath1 ) the bic with some parameters @xmath261 . then",
    ", as long as @xmath262 for some positive numerical constant @xmath263 , the lasso estimate @xmath243 in computed with @xmath264 obeys @xmath265 with probability at least @xmath266 , where @xmath267 is a constant independent of the problem parameters .",
    "[ thm : lasso_ind.signs ]    the proof of this theorem is omitted here because it is a straightforward extension .",
    "while this theorem suggests that the lasso solution in the block setting enjoys many of the optimality properties of the lasso solution in the non - block setting ( see , e.g. , the discussion in @xcite ) , it fails to extend to the case when the independence assumption on the signs of the nonzero regression coefficients is replaced by the less restrictive condition m3 . in particular",
    ", one expects that allowing for arbitrary correlations within the blocks of regression coefficients will limit the usefulness of the lasso for linear regression in the presence of large blocks .",
    "while such an insight can be difficult to confirm in the case of arbitrary design matrices and average - case analysis , we provide an extension of theorem  [ thm : lasso_ind.signs ] in the following that highlights the challenges for the lasso in the case of regression of block - sparse vectors with arbitrarily correlated blocks .",
    "the proof of this theorem is given in appendix  [ app : thm : lasso ] .",
    "suppose that the vector of regression coefficients @xmath4 is @xmath74-block sparse and it is drawn according to the statistical model m1 , m2 , and m3 .",
    "further , assume that the observation vector can be modeled as @xmath259 , where the @xmath88 matrix @xmath8 satisfies @xmath268 and @xmath269 for some positive numerical constants @xmath270 , @xmath271 , and the modeling error @xmath252 is i.i.d .",
    "gaussian with variance @xmath253 .",
    "then , as long as @xmath272 for some positive numerical constant @xmath273 , the lasso estimate @xmath243 in computed with @xmath264 obeys @xmath274 with probability at least @xmath275 , where @xmath276 is a constant independent of the problem parameters .",
    "[ thm : lasso ]    it can be seen from theorems  [ thm : lasso_ind.signs ] and [ thm : lasso ] that while both the theorems guarantee same scaling of the regression error , the scalings of the maximum number of allowable nonzero blocks and the block coherence in theorem  [ thm : lasso ] match the ones in theorem  [ thm : lasso_ind.signs ] only for the case of @xmath277 ; otherwise , theorem  [ thm : lasso ] with correlated blocks results in less - desirable scalings of @xmath74 and @xmath240 .",
    "it can be seen from the proof of theorem  [ thm : lasso ] in appendix  [ app : thm : lasso ] that this dependence upon @xmath179the size of the blocks  is a direct consequence of allowing for arbitrary correlations within blocks .",
    "a natural question to ask then is whether it is possible to return to the scalings of theorem  [ thm : lasso_ind.signs ] _ without _ sacrificing intra - block correlations .",
    "the answer to this question is in the affirmative as long as one explicitly accounts for the block structure of the regression problem .    specifically , the group lasso explicitly accounts for the grouping of the regression variables in its formulation and estimates the vector of regression coefficients as @xmath278 where @xmath258 is once again a tuning parameter .",
    "the following theorem shows that the group lasso can achieve the same scaling results as the lasso for block - sparse vectors ( cf .",
    "theorem  [ thm : lasso_ind.signs ] ) , while allowing for arbitrary correlations among the regression coefficients within blocks .",
    "the proof of this theorem is given in appendix  [ sec : grouplassoproof ] .",
    "suppose that the vector of regression coefficients @xmath4 is @xmath74-block sparse and it is drawn according to the statistical model m1 , m2 , and m3 .",
    "further , assume that the observation vector can be modeled as @xmath259 , where the @xmath88 matrix @xmath8 satisfies the bic with some parameters @xmath93 , and the modeling error @xmath252 is i.i.d .",
    "gaussian with variance @xmath253 .",
    "then , as long as @xmath279 for some positive numerical constant @xmath280 , the group lasso estimate @xmath243 in computed with @xmath281 obeys @xmath282 with probability at least @xmath275 , where @xmath283 is a constant independent of the problem parameters .",
    "[ thm : grouplasso ]    note that if one has @xmath154 then block sparsity reduces to the canonical sparsity , the group lasso reduces to the lasso , the block coherence @xmath240 reduces to the coherence @xmath228 , and theorem  [ thm : grouplasso ] essentially reduces to ( * ? ? ?",
    "* theorem  1.2 ) .    with the caveat that both theorems  [ thm : lasso ] and [ thm : grouplasso ] are concerned with sufficient conditions for average - case regression performance",
    ", we now comment on the strengths and weaknesses of these two results . assuming appropriate conditions are satisfied for the two theorems , we have that both the lasso and the group lasso result in the same scaling of the regression error , @xmath284 , in the presence of intra - block correlations .",
    "this scaling of the regression error is indeed the best that any method can achieve , modulo the logarithmic factor , since we are assuming that the observations are described by a total of @xmath285 regression variables . unlike the lasso",
    ", however , the group lasso also allows for a more favorable scaling of the maximum number of regression variables contributing to the observations , @xmath286 , even when arbitrary intra - block correlations are permitted .",
    "in fact , similar to the discussion in section  [ sec : grouprecovery ] , it is easy to conclude that this scaling of the number of nonzero regression coefficients is near - optimal since it leads to a linear relationship ( modulo logarithmic factors ) between the number of observations @xmath10 and the number of active regression variables @xmath194 for the case of design matrices that are approximately tight frames : @xmath230 .",
    "the other main difference between theorems  [ thm : lasso ] and [ thm : grouplasso ] is the role that the inter - block coherence @xmath240 plays in guarantees for the lasso and the group lasso .",
    "specifically , theorem  [ thm : lasso ] requires the inter - block coherence to be smaller , @xmath287 , than theorem  [ thm : grouplasso ] for the lasso to yield near - optimal regression error in the case of intra - block correlations .",
    "this discussion suggests that reliable linear regression of block - sparse vectors can be carried out using the group lasso for a larger class of regression vectors and design matrices than the lasso .",
    "we plan to provide a more rigorous mathematical understanding of these and other subtle but important differences between the lasso and the group lasso in future works",
    ".    while the block - sparse structures of theorems  [ thm : rand_cond][thm : grouplasso ] can be found in many applications , there exist other applications in which canonical block sparsity does not adequately capture the sparsity structure of @xmath9 .",
    "consider , for instance , wavelet expansions of piecewise smooth signals .",
    "nonzero wavelet coefficients in these cases appear for chains of wavelets at multiple scales and overlapping offsets , as captured through parent  children relationships in wavelet trees @xcite .",
    "more general structured - sparsity models ( also known as model - based sparsity and union - of - subspaces models ) are often used in the literature to express these kinds of sparsity structures by allowing some supports and disallowing others  @xcite . the support of structured - sparse signals in many instances can be expressed in terms of unions of groups of indices @xmath288 where each group @xmath289 contains indices of @xmath74 coefficients that become active simultaneously in structured - sparse signals , and the groups @xmath290 s may or may not be disjoint . therefore , despite the non - block sparse nature of @xmath9 in this setting , one can reorganize the columns of the dictionary @xmath8 and the entries of @xmath9 as @xmath291 $ ] and @xmath292 $ ] , respectively , so that @xmath293 . ( in the case of overlapping @xmath290 s , this does require minor corrections to @xmath294 to ensure each nonzero entry in @xmath9 appears only once in @xmath294 ; see , e.g. , @xcite . ) doing so converts the structured - sparse problem involving @xmath9 into a block - sparse problem involving @xmath294 and results of theorems  [ thm : rand_cond][thm : grouplasso ] can still be utilized in this case as long as the number of sets @xmath295 is not prohibitively large .",
    "one of the most important implications of this section is that , similar to the case of recovery of block - sparse signals , the number of maximum allowable active regression variables in regression of block - sparse vectors is fundamentally a function of the spectral norm of the design matrix , provided the inter- and intra - block coherence of the design matrix are not too large .",
    "however , such a claim needs to be carefully investigated since our results are only concerned with sufficient conditions on design matrices . to this end",
    ", we construct numerical experiments that help us evaluate the regression performance of the group lasso for a range of design matrices with varying spectral norms , coherences , inter - block coherences , and intra - block coherences . in order to generate these design matrices",
    ", we reuse the experimental setup described in section  [ ssec : recovery_num_exp ] ( corresponding to @xmath236 , @xmath234 , and @xmath235 ) .     with varying spectral norms and extremal coherence values ( cf .",
    "table  [ table : extreme_mu ] ) in regression of block - sparse vectors as a function of the number of nonzero regression blocks @xmath74 ; @xmath296 denotes the value of the spectral norm multiplier used .",
    "solid lines correspond to matrices with minimum coherence , while dashed lines correspond to matrices with maximum coherence.,width=288 ]    for the sake of brevity , we focus only on the performance of the group lasso for regression of block - sparse vectors .",
    "this performance is evaluated for different design matrices using monte carlo trials , corresponding to generation of 1000 block - sparse @xmath9 with @xmath74 nonzero blocks .",
    "each vector of regression coefficients has block support selected uniformly at random according to m1 in section  [ ssec : stat_model ] and nonzero entries drawn independently from the gaussian distribution .",
    "we then obtain the observations @xmath297 using the design matrix ( dictionary ) @xmath8 under study for each one of the block - sparse @xmath9 , where the variance @xmath253 of the modeling error @xmath252 is selected such that @xmath298 .",
    "finally , we carry out linear regression using the group lasso by setting @xmath299 and we then record the regression error @xmath300 .",
    "figure  [ fig : reg_extreme_mu ] shows the regression performance of the group lasso for design matrices @xmath8 with increasing spectral norms ( @xmath301 ) , where we once again choose matrices with the largest and smallest coherence values for each @xmath296 ( among the 2000 available options ) .",
    "the spectral norms , coherences , inter - block coherences , and intra - block coherences for these chosen design matrices are still given by table  [ table : extreme_mu ] in section  [ ssec : recovery_num_exp ] .",
    "similar to the case of block - sparse recovery , we observe that significant changes in the values of the ( intra-/inter - block ) coherences do not significantly affect the regression performance .",
    "this behavior is clearly in agreement with our expectation from theorem  [ thm : grouplasso ] that the role of ( intra-/inter - block ) coherences in regression is limited to the bic and is decoupled from the scaling of the number of nonzero blocks @xmath74 ( equivalently , number of nonzero regression coefficients @xmath194 ) .",
    "in addition , we observe that as the spectral norm of the matrix decreases , the range of values of @xmath74 for which the regression error exhibits a linear trend also increases .",
    "this is again consistent with the statement of theorem  [ thm : grouplasso ] .",
    "in this paper , we have provided conditions under which most block subdictionaries of a dictionary are well conditioned .",
    "in contrast to prior works , these conditions are explicitly computable in polynomial time , they lead to near - optimal scaling of the dimensions of the well - conditioned subdictionaries for dictionaries that are approximately tight frames , and they suggest that the spectral norm plays a far important role than the ( inter-/intra - block ) coherences of the dictionary in determining the order - wise dimensions of the well - conditioned subdictionaries . in addition , we have utilized this result to investigate the average - case performances of convex optimization - based methods for block - sparse recovery and regression of block - sparse vectors .",
    "our average - case performance results significantly improve upon the existing results for both block - sparse recovery and regression of block - sparse vectors .",
    "finally , numerical experiments conducted in the context of block - sparse recovery and regression problems validate the insight offered by our results in relation to the effects of spectral norm and ( inter-/intra - block ) coherences of the dictionary on inference problems in block - sparse settings .",
    "we gratefully acknowledge many helpful comments provided by dr .",
    "lorne applebaum in relation to a preliminary draft of this paper .",
    "we are also thankful to the anonymous reviewers for their many valuable remarks that helped improve the quality of this paper .",
    "the proof of this lemma relies on many lemmas and tools , some of which are generalizations of the corresponding results in  @xcite to the block setting of this paper . to begin",
    ", we denote the matrix @xmath302 in block - partitioned fashion : @xmath303 = \\left[\\begin{array}{cccc } g_{1,1}&g_{1,2}&\\ldots&g_{1,r } \\\\ g_{2,1}&g_{2,2}&\\ldots&g_{2,r } \\\\ \\vdots&\\vdots&\\ddots&\\vdots \\\\ g_{r,1}&g_{r,2}&\\ldots&g_{r , r } \\end{array } \\right],\\ ] ] where @xmath304 for @xmath305 .",
    "we then split @xmath306 , where @xmath307 contains the diagonal blocks @xmath308 , and @xmath309 contains only the non - diagonal blocks .",
    "we next define the following `` norms '' for block matrices :    * when we block only columns of a matrix @xmath295 , we define @xmath310 , and * when we block both columns and rows of a matrix @xmath295 , we define @xmath311 .",
    "finally , we make use of some standard inequalities in the following , including :    * cauchy - schwarz inequality : @xmath312 * hlder s inequality : @xmath313 , @xmath314 and @xmath315 . * jensen s inequality for a convex function @xmath316 : @xmath317 * scalar khintchine inequality : let @xmath318 be a finite sequence of complex numbers and @xmath319 be a rademacher ( uniformly random @xmath320 binary , i.i.d . ) sequence . for each @xmath321",
    ", we have @xmath322 where @xmath323 . * noncommutative khintchine inequality  @xcite : let @xmath324 be a finite sequence of matrices of the same dimensions and @xmath319 be a rademacher sequence . for each @xmath325 , @xmath326 where @xmath327 denotes the schatten @xmath328-norm for a matrix @xmath295 ( equal to the @xmath38-norm of the vector @xmath329 , which contains singular values of the matrix @xmath295 ) and @xmath330 .",
    "we need the following five lemmas in our proof of lemma  [ lem : cond_block ] .",
    "the first two lemmas here are used to prove the later ones .",
    "let @xmath331 $ ] be a block matrix and @xmath332 be its block diagonalization , i.e. , a block - diagonal matrix @xmath333 containing the matrices @xmath334 in its diagonal , with all other elements being equal to zero .",
    "then , we have @xmath335 [ lem : blockdiag ]    for a vector @xmath336 of appropriate length , we evaluate the ratio @xmath337 .",
    "we partition @xmath338^*$ ] into its pieces @xmath339 matching the number of columns of the blocks @xmath340 , @xmath341 .",
    "then , we have @xmath342 thus , the spectral norm obeys @xmath343    the next lemma is a generalization of the lemma in ( * ? ? ?",
    "* sec .  2 ) to our block setting .",
    "let @xmath331 $ ] be a block matrix where each block @xmath340 has @xmath179 columns with @xmath344 and let @xmath319 be a rademacher sequence .",
    "for any @xmath345 , we have @xmath346 [ lem : rudelson ]    we start by bounding the spectral norm by the schatten @xmath328-norm : @xmath347 now we use the noncommutative khintchine inequality ( noting that the two terms in the inequality s @xmath348 are equal in this case ) to get @xmath349 we can bound the schatten @xmath328-norm by the spectral norm by paying a multiplicative penalty of @xmath350 , where @xmath11 is the maximum rank of the matrix sum . by the hypothesis @xmath345 , this penalty does not exceed @xmath351 , resulting in @xmath352 finally , we note that the sum term is a quadratic form that can be expressed in terms of @xmath73 and its block diagonalization , as follows : @xmath353 where the last step used lemma  [ lem : blockdiag ] . now replace @xmath354 to complete the proof .    the next lemma is a generalization of ( * ? ? ?",
    "* proposition 2.1 ) to our block setting .",
    "let @xmath309 be a hermitian matrix with zero blocks on the diagonal .",
    "then @xmath355 , where @xmath356 with @xmath357 denoting an independent realization of the random matrix @xmath358 .",
    "[ lem : decouple ]    we establish the result for @xmath359 for simplicity and without loss of generality .",
    "denote by @xmath360 the masking of the matrix @xmath309 that preserves only the subblock @xmath361 and makes other entries of @xmath309 zero .",
    "then , we have @xmath362 let @xmath363 be i.i.d .",
    "bernoulli random variables with parameter @xmath364 .",
    "we then use jensen s inequality on this new ( multivariate ) random variable @xmath365 . specifically , we define @xmath366 , and note that @xmath367 for all @xmath368 , where @xmath369 denotes expectation over the random variable @xmath370 ( in contrast to the notation @xmath371 , where @xmath328 is a constant ) .",
    "we also define the function @xmath372 then , by applying jensen s inequality to @xmath373 , we obtain @xmath374\\zeta_i\\zeta_j ( \\hhat_{i , j}+\\hhat_{j , i})\\right\\|_2.\\ ] ] there is a 0 - 1 vector @xmath375 for which the expression exceeds its expectation over @xmath370 . letting @xmath376 , we get @xmath377 where @xmath378 is an independent realization of the sequence @xmath379 .",
    "the equality in is a combination of the following facts : ( @xmath0 ) @xmath380 and , therefore , defining @xmath381 and @xmath382 , we have @xmath383 ; ( @xmath1 ) we can reorder the columns and rows of @xmath45 and @xmath182 to have those corresponding to the set @xmath384 first , followed by those corresponding to the set @xmath385 later , giving us matrices of the form @xmath386 respectively , where @xmath387 ; ( @xmath2 ) permuting the columns and rows of a matrix does not affect its spectral norm ; ( @xmath388 ) the hermitian dilation map of a matrix @xmath295 , @xmath389 preserves the spectral norm of @xmath295 : @xmath390 ; and ( @xmath35 ) removal of all - zero rows and columns from a matrix preserves its spectral norm . combining these five facts together , we have @xmath391 finally , since the norm of a submatrix does not exceed the norm of the matrix , we re - introduce the missing blocks to complete the argument : @xmath392    the next lemma is adapted to our problem setup of block matrices from  ( * ? ? ?",
    "* theorem  3.1 ) , ( * ? ? ? * proposition  12 ) .",
    "let @xmath393 $ ] be a matrix with @xmath107 column blocks , and suppose @xmath394",
    ". then @xmath395 [ lem : rv ]    we denote @xmath396 and note that @xmath397 next , we replace @xmath398 by @xmath399 , with @xmath400 denoting an independent copy of the sequence @xmath379 .",
    "we then take the expectation out of the norm by applying jensen s inequality to get @xmath401 we now symmetrize the distribution by introducing a rademacher sequence @xmath319 , noticing",
    "that the expectation does not change due to the symmetry of the random variables @xmath402 : @xmath403 we apply the triangle inequality to separate @xmath404 and @xmath405 , and by noticing that they have the same distribution , we obtain @xmath406 writing @xmath407 , we see that @xmath408 where we have split the expectation on the random variables @xmath379 and @xmath319 .",
    "now we use lemma  [ lem : rudelson ] on the term in parentheses to get @xmath409 using the cauchy - schwarz inequality , we get @xmath410 this inequality takes the form @xmath411 .",
    "we bound @xmath412 by the largest solution of this quadratic form : @xmath413 thereby proving the lemma .",
    "the last lemma that we need for our proof is a generalization of ( * ? ? ?",
    "* proposition 13 ) to our block setting .",
    "let @xmath414 = \\left[\\begin{array}{cccc } m_{1,1}&m_{1,2}&\\ldots&m_{1.r } \\\\",
    "m_{2,1}&m_{2,2}&\\ldots&m_{2.r } \\\\ \\vdots&\\vdots&\\ddots&\\vdots \\\\ m_{r,1}&m_{r,2}&\\ldots&m_{r.r } \\end{array } \\right]\\ ] ] be a block matrix , where each block @xmath415 has size @xmath416 .",
    "assume @xmath417 .",
    "then , we have @xmath418 [ lem : bnorm ]    we begin by seeing that @xmath419 ^ 2 = { { \\mathbb{e}}}_{q/2 } \\left(\\max_{1 \\le j \\le r } \\sum_{i=1}^r \\zeta_i \\|m_{i , j}\\|_2 ^ 2\\right)\\ ] ] in the sequel , we abbreviate @xmath420 and @xmath421 .",
    "we continue by using the same technique as in the proof of lemma  [ lem : rv ] : we split a term for the mean value of the sequence @xmath379 , then replace the term by @xmath422  an independent copy of the sequence , then exploit symmetrization by introducing a rademacher sequence @xmath319 , and then finish by merging the two terms due to their identical distributions : @xmath423 now we bound the maximum by the sum and separate the expectations on the two sequences : @xmath424 for the inner term , we can use the scalar khintchine inequality to obtain @xmath425 we continue by bounding the outer sum by the maximum term times the number of terms : @xmath426 since @xmath427 , it holds that @xmath428 , which implies that @xmath429 .",
    "we now use hlder s inequality inside the sum term @xmath430 with @xmath431 , @xmath359 : @xmath432 now we recall that @xmath420 and @xmath421 , to get @xmath433 and notice that @xmath412 has appeared on the right hand side . by following the same argument that ends the proof of lemma  [ lem",
    ": rv ] , we complete the proof .",
    "we now have all the required results to prove lemma  [ lem : cond_block ] . split @xmath302 into its diagonal blocks @xmath307 ( containing @xmath434 , @xmath435 ) and off - diagonal blocks @xmath309 ( containing @xmath436 , @xmath437 ) and apply lemma  [ lem : decouple ] : @xmath438 to estimate the first term , we apply lemma  [ lem : rv ] twice ; once for @xmath55 , and once for @xmath439 : @xmath440 by applying lemma  [ lem : bnorm ] on the first term , we obtain @xmath441 + 3\\sqrt{\\frac{\\delta q}{2 } } { { \\mathbb{e}_q}}\\|hr'\\|_{b,1 } + \\delta \\|h\\|_2.\\end{aligned}\\ ] ]",
    "since @xmath55 and @xmath439 have the same distribution , we can collect terms to get @xmath442 next , in order to bound @xmath443 , we use the notation @xmath444 $ ] ; we then have @xmath445 now we use the facts @xmath446 , @xmath447 and , using lemma  [ lem : blockdiag ] , @xmath448 to complete the proof of the lemma : @xmath449",
    "in this appendix , we will prove that the minimization ( [ eq : l21min ] ) successfully recovers a @xmath74-block sparse @xmath9 from @xmath450 with high probability .",
    "mathematically , this is equivalent to showing that @xmath451 for all @xmath452 such that @xmath453 . in the following",
    ", we will argue that this is true as long as there exists a vector @xmath454 such that ( @xmath0 ) @xmath455 , where @xmath52 denotes the block support of @xmath9 , @xmath456 denotes the adjoint of @xmath457 , and @xmath458 denotes the block - wise extension of @xmath459 to the blocks in @xmath52 , and ( @xmath1 ) @xmath460 for all @xmath461 .",
    "note that these two conditions on the vector @xmath462 imply that ( @xmath2 ) @xmath463 for all @xmath464 .    to prove the sufficiency of conditions ( @xmath0 ) and ( @xmath1 ) above , we follow the same ideas as in  @xcite .",
    "to begin , we need the following lemma ; its proof is a simple exercise using hlder s inequality .    consider two block vectors @xmath336 and @xmath192 such that the blocks of @xmath336 have nonidentical @xmath14-norms and the blocks of @xmath192 are nonzero .",
    "then @xmath465 , where @xmath466 .",
    "[ lem : holder2 ]    we can write @xmath467 where the vectors @xmath468 $ ] and @xmath469 $ ] are defined as ones that contain the @xmath14-norms of the blocks of @xmath336 and @xmath192 , respectively . using the conditions of  (",
    "* lemma 6 ) , we have @xmath470 thereby proving the lemma .",
    "note that if we remove the requirements on @xmath336 and @xmath192 , it can be shown that @xmath471 ; that is , the conditions on @xmath336 and @xmath192 remove the possibility of equality .",
    "in addition to this lemma , we will also need to use the vector bernstein inequality from  @xcite .",
    "[ lem : app : bernstein ] let @xmath472 be a finite sequence of independent random vectors .",
    "suppose that @xmath473 and @xmath474 almost surely , and put @xmath475 .",
    "then for all @xmath476 , @xmath477    we are ready now to formally prove theorem  [ thm : l12 ] .",
    "we begin by writing @xmath478 next , we assume that conditions ( @xmath0 ) and ( @xmath1 ) ( which together imply condition ( @xmath2 ) ) are true in our case and consider any @xmath479 such that @xmath480 . then since @xmath481 , we have @xmath482 where @xmath483 denotes the support of a different solution @xmath294 as described earlier .",
    "we now consider two cases .",
    "if not all norms @xmath484 are identical over @xmath485 , then we apply lemma  [ lem : holder2 ] to obtain @xmath486 where the last inequality is due to condition ( @xmath2 ) .",
    "if all the norms @xmath484 are identical over @xmath485 , note that since @xmath487 and since theorem  [ thm : rand_cond ] guarantees that @xmath457 has linearly independent columns with high probability ( noting that we will come back to theorem  [ thm : rand_cond ] later ) , then there must exist a block index @xmath488 such that @xmath489 . from condition ( @xmath1 ) , we know that for such a @xmath490 we have @xmath491 , meaning that @xmath492 for all @xmath493 .",
    "we then leverage ( [ eq : blockcs ] ) to obtain @xmath494 in order to complete the proof of the theorem , the only thing that remains to be shown now is that conditions ( @xmath0 ) and ( @xmath1 ) hold in our case .    to simplify conditions ( @xmath0 ) and ( @xmath1 ) , we can define the vector @xmath495 , where @xmath496 denotes the moore ",
    "penrose inverse of a matrix .",
    "note that such an @xmath462 trivially satisfies condition ( @xmath0 ) .",
    "condition ( @xmath1 ) then reduces to @xmath497 it remains to prove ( [ eq : l12cond ] ) .",
    "denote the vector @xmath498 for each @xmath499 .",
    "further denote @xmath500 we then simply need to show that with large probability @xmath501 .",
    "define the matrix @xmath502 for @xmath503 .",
    "further , denote by @xmath504 the submatrix of @xmath505 containing the block of rows that corresponds to @xmath506 .",
    "we can then write @xmath507 , where @xmath508 is the adjoint of @xmath504 .",
    "the sum terms have norms bounded by @xmath509 we now use lemma  [ lem : app : bernstein ] by setting @xmath510 and @xmath511 to obtain @xmath512 for @xmath513 ( as @xmath514 ) . a union bound then gives us @xmath515 , where @xmath516 .",
    "we can also see from the statement of theorem  [ thm : rand_cond ] concerning the conditioning of random block subdictionaries that @xmath517 with probability at least @xmath96 .",
    "thus , conditioning on the probability events @xmath518 and @xmath519 ( which also accounts for the use of theorem  [ thm : rand_cond ] earlier in the proof ) , and replacing @xmath520 , the probability that @xmath521 is at most @xmath522 . by seeing that @xmath523 ,",
    "the bic allows us to set @xmath524 so that this probability of failure is upper bounded by @xmath525 for sufficiently small values of @xmath83 .",
    "the proof of the theorem now follows by taking a final union bound over the events @xmath526 and @xmath521 .",
    "similar to the proof in  @xcite for linear regression in the non - block setting , the proof of this theorem relies on three conditions involving the design matrix @xmath8 , the vector of regression coefficients @xmath9 , and the modeling error @xmath252 .",
    "we once again use @xmath52 to denote the block support of the @xmath74-block sparse @xmath9 and use @xmath457 to denote the matrix containing columns of the blocks indexed by @xmath52 , i.e. , an @xmath71 submatrix of @xmath8 .",
    "additionally , note that throughout the paper @xmath456 denotes the adjoint of @xmath457 rather than a column submatrix of @xmath527 .",
    "finally , we assume in this appendix that @xmath528 without loss of generality .",
    "we then have from the analysis for the lasso in @xcite that the following three conditions are sufficient for the theorem statement to hold :    * _ invertibility condition _ :",
    "the submatrix @xmath529 is invertible and obeys @xmath519 . *",
    "_ orthogonality condition _ : the vector @xmath252 obeys @xmath530 . * _ complementary size condition _ :",
    "the following inequality holds : @xmath531    in order to prove this theorem , therefore , we need only evaluate the probability with which each condition holds under the assumed statistical model , after which a simple union bound will get us the desired result . in this regard , we already have from theorem  [ thm : rand_cond ] that the invertibility condition fails to hold with probability at most @xmath532 .",
    "it is also easy to see that the orthogonality condition in our case is the same as that in  @xcite , where it is shown that the condition fails to hold with probability at most @xmath533 .",
    "therefore , we focus only on understanding the probability of failure for the complementary size condition in the following .",
    "we begin by posing two separate statements that imply the condition , following  @xcite : @xmath534 first , we consider the inequality ( [ eq : csc1 ] ) .",
    "denote by @xmath535 the @xmath536 column of the @xmath50 block @xmath537 , and write @xmath538 for each @xmath539 . additionally , denote @xmath540 we simply need to show that with large probability @xmath541 . for brevity , we write @xmath542 ; we can then write @xmath543 , where @xmath544 denotes the block of the column @xmath545 corresponding to the entry @xmath546 .",
    "we bound the magnitude of the sum terms as @xmath547 a union bound then gives us @xmath548 , where @xmath549 .",
    "we can see that under the invertibility condition , @xmath550 thus , conditioned on a bound @xmath551 and the invertibility condition , and replacing @xmath552 , the probability of the inequality ( [ eq : csc1 ] ) failing to hold is at most @xmath553 .    in order to establish",
    ", we use the following result that is a simple consequence of the chernoff bound on the tail probability of the gaussian distribution @xcite and the union bound ( see , e.g. , ( * ? ? ?",
    "* lemma 3.3 ) ) .",
    "let @xmath554 be a fixed collection of vectors in @xmath555 and set @xmath556 .",
    "we then have @xmath557 for any @xmath558 .",
    "[ lem : randbound ]    we denote @xmath559 for @xmath503 , @xmath560 , where @xmath535 represents the @xmath536 column of the @xmath50 block @xmath537",
    ". then we can write @xmath561 to use lemma  [ lem : randbound ] in this case , we assume that the invertibility condition holds and search for a bound on @xmath562 : @xmath563 thus , we have that conditioned on the bound ( [ eq : gamma ] ) and the invertibility condition , ( [ eq : csc2_1 ] ) holds except with probability at most @xmath564 .    to finalize , we define @xmath565 and define the event @xmath566 then we have that the probability @xmath157 of the complementary size condition not being met is upper bounded by @xmath567 we set @xmath568 with small enough @xmath569 so that each of the first two terms of the right hand side is upper bounded by @xmath570 . to get the probability of the bound ( [ eq : gamma ] )",
    "not being valid , we appeal to lemma  [ lem : bnorm ] together with the markov inequality and a poissonization argument ( see ( [ eq : poissonization ] ) and ( [ eq : poisscp_prf ] ) for an example ) to obtain @xmath571 where @xmath572 .",
    "we replace the values of @xmath573 and @xmath328 selected above as well as the bounds on @xmath74 , @xmath84 , and @xmath86 from the theorem to obtain @xmath574 by picking the constants @xmath575 small enough so that the base of the exponential term on the right hand side is less than @xmath364 , we get @xmath576 .",
    "thus , the complementary size condition holds with probability at least @xmath577 .    by combining the three conditions ( noting that the third condition already accounts for the first )",
    ", we have that theorem  [ thm : lasso ] holds with probability at least @xmath578 .",
    "the proof of this theorem mirrors the steps taken for the proof of theorem  [ thm : lasso ] along with some necessary modifications . to begin",
    ", we need the following lemma concerning the behavior of the group lasso .",
    "since @xmath243 minimizes the objective function over @xmath9 , then 0 must be a subgradient of the objective function at @xmath243 .",
    "the subgradients of the group lasso objective function are of the form@xcite @xmath580 where @xmath581 is given by @xmath582 if @xmath583 and @xmath584 otherwise .",
    "hence , since 0 is a subgradient at @xmath243 , there exists @xmath585^*$ ] such that @xmath586 the conclusion follows from the fact that @xmath587 .      * _ invertibility condition : _ the submatrix @xmath529 is invertible and obeys @xmath588 * _ orthogonality condition : _ the vector @xmath252 satisfies the following inequality : @xmath589 . * _ complementary size condition : _ the following inequality holds : @xmath590    we assume that these three conditions hold . since @xmath243 minimizes the group lasso objective function , we must have @xmath591 define @xmath592 , and note that @xmath593 plugging this identity with @xmath594 into the above inequality and rearranging the terms gives @xmath595 next , break up @xmath243 into @xmath596 and @xmath597 and rewrite the above equation as @xmath598 for each @xmath599 , we have @xmath600 where the first inequality is due to the projection of @xmath601 on @xmath602 having magnitude at most @xmath603 , and the second inequality is due to @xmath604 for all @xmath605 .",
    "thus , we can write @xmath606 . merging this inequality with  ( [ eq : xh ] ) gives us @xmath607 the orthogonality condition and lemma  [ lem : holder2 ] also imply @xmath608 merging this result with ( [ eq : xh2 ] ) results in @xmath609 where @xmath610 .",
    "we aim to bound each of the terms on the right hand side independently .",
    "for the first term , we have @xmath611 denote the two terms on the right hand side as @xmath612 and @xmath613 , respectively .",
    "for @xmath612 we use lemma  [ lem : holder2 ] to obtain @xmath614",
    "now we bound these two terms .",
    "for the first term , we get @xmath615 due to the invertibility condition . using the orthogonality condition",
    ", we also get @xmath616 for the second term @xmath617 , we use lemma  [ lem : gle ] and the orthogonality condition to get @xmath618 combining , we finally get @xmath619 . for @xmath613 , we have from lemma  [ lem : holder2 ] that @xmath620 because of the complementary size condition .",
    "using now these bounds on @xmath621 , we have @xmath622 plugging this into ( [ eq : xh3 ] ) gives @xmath623 which suffices to prove the theorem , modulo the three conditions .",
    "to finish the proof of the theorem , we now must evaluate the probability of each condition failing to hold under the assumed statistical model .",
    "the invertibility condition in this regard simply follows from theorem  [ thm : rand_cond ] , which means that it fails to hold with probability at most @xmath525 .",
    "next , note that @xmath624 is implied by @xmath625 , which matches the orthogonality condition in the proof of theorem  [ thm : lasso ] ( cf .",
    "appendix  [ app : thm : lasso ] ) .",
    "therefore , the orthogonality condition fails to holds with probability at most @xmath533 in the case of the group lasso .",
    "therefore , we only need to evaluate the complementary size condition .    in order to study the complementary size condition , we partition it into two statements : @xmath626 in order to evaluate ,",
    "we compare it to ( [ eq : l12cond ] ) and note that the only difference between the two expressions is a change from @xmath151 to @xmath627 . given that both theorem  [ thm : l12 ] and this theorem operate under the same statistical model , it is therefore straightforward to argue from the analysis of ( [ eq : l12cond ] ) that ( [ eq : csc3 ] ) holds except with probability at most @xmath628 , where @xmath573 is defined as any positive scalar that satisfies @xmath629 . the second condition ( [ eq : csc4 ] ) is implied by the inequality @xmath630 which is shown to hold with probability at most @xmath564 in ( [ eq : csc2_1 ] ) ( cf",
    ".  appendix  [ app : thm : lasso ] ) . to conclude",
    ", we once again define @xmath565 and define the event @xmath566 then we have that the probability @xmath157 of the complementary size condition not being met is upper bounded by @xmath631 we set @xmath632 for small enough @xmath633 so that each of the first two terms of the right hand side is upper bounded by @xmath634 . in order to bound the last term",
    ", we appeal to lemma  [ lem : bnorm ] together with the markov inequality and a poissonization argument ( see ( [ eq : poissonization ] ) and ( [ eq : poisscp_prf ] ) for an example ) to obtain @xmath635 then replacing @xmath632 and @xmath121 as well as the bounds on @xmath74 , @xmath84 and @xmath86 from the theorem and the bic in ( [ eq : bigpoisson ] ) , we obtain @xmath636 by picking the constants @xmath637 small enough so that the base of the exponential term on the right hand side is less than @xmath364 , we get @xmath638 . thus , the complementary size condition fails to hold with probability at most @xmath639 .    by combining the failures of the three conditions ( and noting that the third condition already accounts for the first one ) , we have that theorem  [ thm : grouplasso ] holds with probability at least @xmath578 .",
    "m.  f. duarte , w.  u. bajwa , and r.  calderbank , `` the performance of group lasso for linear regression of grouped variables , '' duke university , dept .",
    "computer science , durham , nc , technical report tr-2010 - 10 , feb . 2011 .",
    "w.  u. bajwa , m.  f. duarte , and r.  calderbank , `` average case analysis of high - dimensional block - sparse recovery and regression for arbitrary designs , '' in _ proc .",
    "17th intl .",
    "conf . artificial intelligence and statistics",
    "( aistats14 ) _ , reykjavik , iceland , apr .",
    "2014 , pp .",
    "5767 .",
    "d.  l. donoho , `` high - dimensional data analysis : the curses and blessings of dimensionality , '' in _ proc .",
    "math challenges of the 21st century _ ,",
    "los angeles , ca , aug . 2000 . [ online ] .",
    "available : http://www-stat.stanford.edu/~donoho/lectures/ams2000/curses.pdf    i.  daubechies , m.  defrise , and c.  de mol , `` an iterative thresholding algorithm for linear inverse problems with a sparsity constraint . ''",
    "_ communications on pure and applied mathematics _ , vol .",
    "57 , no .",
    "11 , pp . 14131457 , 2004 .",
    "m.  j. wainwright , `` sharp thresholds for high - dimensional and noisy sparsity recovery using @xmath641-constrained quadratic programming ( lasso ) , '' _ ieee trans .",
    "inform . theory _",
    "55 , no .  5 , pp .",
    "21832202 , may 2009 .",
    "w.  u. bajwa and a.  pezeshki , `` finite frames for sparse signal processing , '' in _ finite frames _ , p.  casazza and g.  kutyniok , eds.1em plus 0.5em minus 0.4emcambridge , ma : birkhuser boston , 2012 , ch .",
    "303335 .",
    "e.  j. cands , j.  romberg , and t.  tao , `` robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information , '' _ ieee trans .",
    "inform . theory _",
    "52 , no .  2 ,",
    "489509 , feb .",
    "2006 .",
    "s.  gurevich and r.  hadani , `` the statistical restricted isometry property and the wigner semicircle distribution of incoherent dictionaries , '' mar .",
    "2009 , unpublished manuscript .",
    "[ online ] .",
    "available : http://arxiv.org/abs/0903.3627                    s.  cotter , b.  rao , e.  kjersti , and k.  kreutz - delgado , `` sparse solutions to linear inverse problems with multiple measurement vectors , '' _ ieee trans . signal processing _",
    "53 , no .  7 , pp .",
    "24772488 , july 2005 .",
    "r.  gribonval , h.  rauhut , k.  schnass , and p.  vandergheynst , `` atoms of all channels , unite ! average case analysis of multi - channel sparse recovery using greedy algorithms , '' _ j. fourier anal .",
    "_ , vol .",
    "14 , no .  5 , pp . 655687 , 2008 .",
    "h.  liu and j.  zhang , `` estimation consistency of the group lasso and its applications , '' in _ proc .",
    "artificiall intelligence and statistics ( aistats ) _ , clearwater beach , fl , apr .",
    "2009 , pp .",
    "376383 .",
    "m.  stojnic , f.  parvaresh , and b.  hassibi , `` on the reconstruction of block - sparse signals with an optimal number of measurements , '' _ ieee trans .",
    "signal processing _ , vol .",
    "57 , no .  8 , pp .",
    "30753085 , aug .",
    "2009 .",
    "j.  fang and h.  li , `` recovery of block - sparse representations from noisy observations via orthogonal matching pursuit , '' sep .",
    "2011 , unpublished manuscript .",
    "[ online ] .",
    "available : http://arxiv.org/abs/1109.5430    z.  ben - haim and y.  c. eldar , `` near - oracle performance of greedy block - sparse estimation techniques from noisy measurements , '' _",
    "ieee j. select .",
    "top . signal processing _ , vol .  5 , no .  5 , pp .",
    "10321047 , sep .",
    "j.  m. kim , o.  k. lee , and j.  c. ye , `` compressive music : revisiting the link between compressive sensing and array signal processing , '' _ ieee trans .",
    "inform . theory _",
    "58 , no .  1 , pp . 278301 , jan .",
    "2012 .",
    "m.  f. duarte , m.  b. wakin , d.  baron , s.  sarvotham , and r.  g. baraniuk , `` measurement bounds for sparse signal ensembles via graphical models , '' _ ieee trans .",
    "info . theory _",
    "59 , no .  7 , pp .",
    "42804289 , july 2013 .",
    "g.  obozinski , b.  taskar , and m.  i. jordan , `` joint covariate selection and joint subspace selection for multiple classification problems , '' _ statistics and computing _ , vol .",
    "20 , no .  2 ,",
    "pp . 231252 , 2010 .",
    "d.  malioutov , m.  cetin , and a.  s. willsky , `` a sparse signal reconstruction perspective for source localization with sensor arrays , '' _ ieee trans .",
    "signal proc .",
    "_ , vol .",
    "53 , no .  8 , pp . 30103022 , aug .",
    "i.  f. gorodnitsky and b.  d. rao , `` sparse signal reconstruction from limited data using focuss : a re - weighted minimum norm algorithm , '' _ ieee trans .",
    "signal proc .",
    "_ , vol .  45 , no .  3 , pp . 600616 , mar",
    "1997 .",
    "n.  rao , b.  recht , and r.  nowak , `` universal measurement bounds for structured sparse signal recovery , '' in _ proc .",
    "conf . artificial intelligence and statistics ( aistats )",
    "_ , la palma , spain , apr .",
    "2012 , pp . 942950 ."
  ],
  "abstract_text": [
    "<S> the linear model , in which a set of observations is assumed to be given by a linear combination of columns of a matrix ( often termed a dictionary ) , has long been the mainstay of the statistics and signal processing literature . </S>",
    "<S> one particular challenge for inference under linear models is understanding the conditions on the dictionary under which reliable inference is possible . </S>",
    "<S> this challenge has attracted renewed attention in recent years since many modern inference problems ( e.g. , high - dimensional statistics , compressed sensing ) deal with the `` underdetermined '' setting , in which the number of observations is much smaller than the number of columns in the dictionary . </S>",
    "<S> this paper makes several contributions for this setting when the set of observations is given by a linear combination of a small number of _ groups of columns _ of the dictionary , termed the `` block - sparse '' case . </S>",
    "<S> first , it specifies conditions on the dictionary under which most block submatrices of the dictionary ( often termed block subdictionaries ) are well conditioned . </S>",
    "<S> this result is fundamentally different from prior work on block - sparse inference because ( @xmath0 ) it provides conditions that can be explicitly computed in polynomial time , ( @xmath1 ) the given conditions translate into near - optimal scaling of the number of columns of the block subdictionaries as a function of the number of observations for a large class of dictionaries , and ( @xmath2 ) it suggests that the spectral norm , rather than the column / block coherences , of the dictionary fundamentally limits the scaling of dimensions of the well - conditioned block subdictionaries . </S>",
    "<S> second , in order to help understand the significance of this result in the context of block - sparse inference , this paper investigates the problems of block - sparse recovery and block - sparse regression in underdetermined settings . in both of these problems , </S>",
    "<S> this paper utilizes its result concerning conditioning of block subdictionaries and establishes that near - optimal block - sparse recovery and block - sparse regression is possible for a large class of dictionaries as long as the dictionary satisfies easily computable conditions and the coefficients describing the linear combination of groups of columns can be modeled through a mild statistical prior . </S>",
    "<S> third , the paper reports extensive numerical experiments that highlight the effects of different measures of the dictionary in block - sparse inference problems .    </S>",
    "<S> block - sparse inference , block - sparse recovery , block - sparse regression , compressed sensing , group lasso , group sparsity , high - dimensional statistics , multiple measurement vectors , random block subdictionaries </S>"
  ]
}