{
  "article_text": [
    "text classification ( tc ) is to assign new unlabeled natural language documents to predefined thematic categories  @xcite .",
    "many classification algorithms have been proposed for tc , e.g. , @xmath3-nearest neighbors  @xcite , centroid - based classifier  @xcite , and support vector machines ( svms )  @xcite .    generally , text feature space is often sparse and high - dimensional .",
    "for instance , the dimensionality of a moderate - sized text corpus can reach up to tens or hundreds of thousands .",
    "the high dimensionality of feature space will cause the `` curse of dimensionality '' , increase the training time , and affect the accuracy of classifiers  @xcite .",
    "therefore , feature selection techniques are proposed to reduce the dimensionality under the premise of guaranteeing the performance of classifiers .",
    "existing feature selection methods are based on statistical theory and information theory , such as @xmath1 , ig , mi , and ece .",
    "the theoretical basis of the four methods is sound , but the performances of these methods on tc tasks are different .",
    "both @xmath1 and ig often achieved better accuracy than mi and document frequency ( df )  @xcite .",
    "however , other authors suspected the performance of ig on skewed text corpora  @xcite .",
    "besides the classical methods , many improved methods have been proposed .",
    "for example , yang et al .",
    "@xcite considered the terms whose relative term frequency was larger than a predefined threshold @xmath4 , and then modified the ig formula to select features .",
    "forman  @xcite proposed the bi - normal separation ( bns ) method , which used the standard normal distribution s inverse cumulative probability function to construct feature selection function .",
    "uguz  @xcite proposed a two - stage feature selection method for tc by combining ig , principal component analysis and genetic algorithm .",
    "more and more methods have been generated , such as , mr2pso  @xcite , and improved tfidf method  @xcite .",
    "it is worth noting that @xmath0-test has been used for gene expression and genotype data  @xcite .",
    "however , the variable in gene expression or genotype data is different from that in text data , i.e. , the term frequency .",
    "thus we try to validate the role of @xmath0-test in text feature selection .    from document frequency perspective ,",
    "the above methods almost use df sufficiently . however , no efficient method is proposed from term frequency perspective .",
    "it inspires our motivation of this paper .",
    "our paper makes the following contributions :    \\(1 ) using central limit theorem ( clt ) , we prove that the frequency distribution of a term within a specific category or within the entire collection will be approximately normally distributed .",
    "\\(2 ) we model the diversity of the frequency of a term between the specific category and the entire corpus with @xmath0-test .",
    "it means that if the distribution of one term within the specific category is obviously different with that within the entire corpus , the term can be considered to be feature .",
    "\\(3 ) we verify our new approach on two common text corpora with three well - established classifiers .",
    "the experiments show that our approach is comparable to or even slightly better than the state - of - the - art @xmath1 and ece in terms of both macro-@xmath2 and micro-@xmath2 , and it outperforms ig and mi methods significantly on unbalanced text corpus .",
    "many feature selection approaches have been proposed in tc tasks , but we only give detailed analysis on four methods because they have been widely used and achieved better performance , the formulae can be found in refs  @xcite .",
    "they are : chi - square statistic ( @xmath1 ) , information gain ( ig ) , mutual information ( mi ) , and expected cross - entropy ( ece ) .",
    "@xmath1 was proposed by pearson early in 1900  @xcite .",
    "the @xmath1 statistic is used to measure the lack of independence between @xmath5 and @xmath6 , and can be regards as the @xmath1 distribution with one degree of freedom . in real - world corpus ,",
    "@xmath1 statistic is based , however , on several assumptions that do not hold for most textual analysis  @xcite .",
    "for instance , if term @xmath7 occurs in 50% documents of a specific category @xmath6 and term @xmath8 occurs in 49% documents , but the frequency of @xmath8 is much higher than that of @xmath7 .",
    "experts often think term @xmath8 should have more discriminating power than @xmath7 in the specific category @xmath6 .",
    "@xmath1 , however , will be prone to select term @xmath7 as feature , rather than @xmath8 .",
    "the problem is that @xmath1 is not reliable for low - frequency terms  @xcite .",
    "the weakness of mi is that the score is strongly influenced by the marginal probabilities of terms , because rare terms will have a higher score than common terms .",
    "therefore , the scores are not comparable across terms of widely differing frequency  @xcite . besides , mi gives longer documents higher weights in the estimation of the feature scores .",
    "ig was firstly used as attribute selection measure in decision tree  @xcite .",
    "this measure is from entropy in information theory , which studies the value or `` information content '' of messages .",
    "ig is defined as the difference between the original information requirement ( i.e. , based on just the proportion of classes ) and the new requirement ( i.e. , obtained after partitioning on term @xmath5 ) .",
    "ig is also called average mutual information .",
    "the weakness of ig method is that it prefers to select terms distributed in many categories , but these terms have less discriminating power in tc tasks .",
    "differing from ig , expected cross - entropy ( ece )  @xcite only considers the terms occurred in a document and ignores the absent terms .    as we know",
    ", if a term ( except stop words ) occurs frequently within a specific category , the term should be considered as a feature or discriminator of the category .",
    "for example , ",
    "computer \" occurs frequently in the it category .",
    "however , the above methods are all based on document frequency , and * ignore * the term frequency . in next section , we will propose a new approach based on term frequency , and it can capture the information of high - frequency terms .",
    "the @xmath0-test , namely the student @xmath0-test , is often used to assess whether the means of two classes are statistically different from each other by calculating a ratio between the difference of two class means and the variability of the two classes  @xcite . in this section , we explain why the averaged term frequency within a single category or in the whole corpus is approximately normal using lindeberg - levy central limit theorems , and then how the @xmath0-test is constructed based on the averaged term frequencies .",
    "let us consider the term frequency in text corpus consisting of @xmath9 documents . given a vocabulary v , the term frequency ( @xmath10 ) of a term @xmath11 in the @xmath12th document @xmath13",
    "can be considered as a random variable , which subjects to some unknown distribution , e.g. , multinomial model  @xcite . in the multinomial model",
    ", a document is an ordered sequence of word events drawn from the same vocabulary v , and the probability of each word event in a document is independent of the word s context and position in the document .",
    "therefore , each document @xmath14 is drawn from a multinomial distribution of words with as many independent trials  @xcite .",
    "that is , the occurrence of one term in each document is dominated by a multinomial function .",
    "then ,    \\(1 ) let @xmath15 be a random sample of size @xmath16 , where @xmath16 is the number of documents in the collection , and @xmath17 is the term frequency of @xmath5 in @xmath12th document .",
    "that is , a sequence of independent and identically distributed random variables with expected values @xmath18 and variances @xmath19 , where @xmath20 is the distributed probability of term @xmath5 in the collection .",
    "each sample belongs to one of @xmath21 classes @xmath22 .",
    "\\(2 ) let @xmath23 be the sample average of these random variables in terms of @xmath5 .",
    "\\(3 ) let @xmath24 be the sample average of term @xmath5 in category @xmath25 , where @xmath26 is an indicator to discriminate whether document @xmath14 belongs to @xmath25 , and @xmath27 is the total samples in class @xmath3 .    according to lindeberg - levy central limit theorems ( lv clt )  @xcite",
    ", @xmath28 is approximately normal with mean @xmath29 and variance @xmath30 , denoted as @xmath31 ; and @xmath32 is approximately normal with mean @xmath29 and variance @xmath33 , denoted as @xmath34 .",
    "then we know that @xmath35 is also approximately normal distributed with mean @xmath36 and variance @xmath37 .",
    "the variance ( var ) is induced as follows : @xmath38    besides , we define the pooled within - class deviation as follows : @xmath39    according to the definition of the @xmath0-test  @xcite , we construct the following formula : @xmath40 where @xmath41 is standard deviation , and @xmath42 .    the eq .",
    "[ equ : ttest ] is used to measure whether the means of the two normal distributions ( i.e. , @xmath32 and @xmath28 ) have the statistically significant difference",
    ". the bigger the value of @xmath43 is , the larger the difference of the means is .",
    "for some threshold @xmath44 , if the @xmath45 , it implies that the averaged frequency of term @xmath5 in the specific category @xmath25 has the same or similar mean with that in the entire corpus ; otherwise , it implies the averaged frequency of term @xmath5 in the specific category @xmath25 is significantly different from that in the entire corpus , and the term has more discriminating power for the specific category @xmath25 .",
    "compared with the average of term frequency in the entire corpus , the term @xmath5 occurred many or few times in @xmath25 can be considered as the feature of category @xmath25 .",
    "we combine the category - specific scores of a term into two alternate ways : @xmath46    @xmath47",
    "_ reuters-21578 _  : the reuters corpus is a widely used benchmark collection  @xcite . according to the modapte split",
    ", we get a collection of 52 categories ( 9100 documents ) after removing unlabeled documents and documents with more than one class label .",
    "reuters-21578 is a very skewed data set . altogether",
    "319 stop words , punctuation and numbers areremoved .",
    "all letters are converted into lowercase , and the word stemming is applied .    _ 20newsgroup _  :",
    "the newsgroup is also a widely used benchmark  @xcite , and consists of 19,905 documents , which are uniformly distributed in twenty categories .",
    "we randomly divide it into training and test sets by 2:1 , and only keep `` subject '' , `` keyword '' and `` content '' .",
    "the stop words list has 823 words , and we filter words containing non - characters .",
    "all letters are converted into lowercase and word stemming is applied .",
    "each document is represented by a vector in the term space , and term weighting is calculated by standard @xmath48  @xcite , and then the vector is normalized to have one unit length .      in our experiments ,",
    "we choose three well - established classifiers for the comparison purpose .",
    "they are : support vector machines ( svms )  @xcite , weighted knn classifier ( @xmath3nn )  @xcite , and classic centroid - based classifier ( cc )  @xcite .",
    "the svms implementation we use is libsvm  @xcite with linear kernels . for @xmath3nn",
    ", we set @xmath49  @xcite .",
    "the similarity measure we use is the cosine function .",
    "we measure the effectiveness of classifiers in terms of @xmath2 widely used for tc . for multi - class task ,",
    "@xmath2 is estimated in two ways , i.e. , the macro - averaged @xmath2 ( macro-@xmath2 ) and the micro - averaged @xmath2 ( micro-@xmath2 ) , as the following : @xmath50 @xmath51 where @xmath52 is the @xmath2 value of the predicted @xmath53th class , and @xmath54 and @xmath55 are the precision and recall values across all classes , respectively .",
    "in general , macro-@xmath2 gives the same weight to all categories .",
    "in contrast , micro-@xmath2 gives the same weight to each instance , which can be dominated by the performance of common or majority categories .",
    "firstly , we show one case study of @xmath0-test in real - world corpus . tables  [ tab : keywordsexample ] lists the scores of seven different feature selection functions for the selected four terms in category `` acq '' from the real - life corpus , i.e. , reuters-21578 . based on the literal meaning , the first two terms ,",
    "i.e. ,  acquir \" and  stake \" , are closely related to the content of category `` acq '' , while the last two terms , i.e. ,  payout \" and  dividend \" , belong to other category . however , according to the @xmath1 , ece , and tf methods , we wrongly select  acquir \" and  dividend \" as the features of category `` acq '' , whereas @xmath0-test , ig and mi select the features correctly .",
    ".the feature values of four terms in  acq \" .",
    "[ cols=\"^,^,^,^,^\",options=\"header \" , ]     [ tab : keywordsexample ]    then , we show the performance of @xmath0-test on two corpora with three classifiers . for reuters-21578 , the number of feature space is all , 17000 , 15000 , 13000 , 11000 , 10000 , 8000 , 6000 , 4000 , and 2000 , respectively , accounting to ten groups of data sets . on 20 newsgroup corpus ,",
    "the original feature space reaches up to 210 thousand and we only select less terms as features to save training time .",
    "the dimensionality of feature space is all , 2000 , 1500 , 1000 , 500 , and 200 , respectively , accounting to six groups of data sets .    for @xmath1 ,",
    "mi , and @xmath0-test methods , we tested the two alternative combinations , i.e. , @xmath56 and @xmath57 ways .",
    "we observed that the averaged way was always better than the maximized way for multi - classes problem .",
    "thus we only report the best results of three methods .",
    "the macro-@xmath2 and micro-@xmath2 of five methods with @xmath3nn on imbalanced reuters-21578 are shown in fig .",
    "[ fig : maknnre ] , fig .",
    "[ fig : miknnre ] , respectively .",
    "it is clear that @xmath0-test , @xmath1 , and ece achieve evidently better performance than mi and ig in terms of macro-@xmath2 .",
    "however , the diversity among the three methods is small . as shown in fig .",
    "[ fig : maknnre ] , when the number of feature space is larger than 13000 , @xmath1 , and ece is a little better than @xmath0-test ; however , when the number of features falls in [ 8000 , 13000 ] , @xmath0-test performs the best macro-@xmath2 .",
    "nn on reuters-21578 in terms of macro-@xmath2.,scaledwidth=50.0% ]",
    "nn on reuters-21578 in terms of micro-@xmath2.,scaledwidth=50.0% ]    the micro-@xmath2 of five methods increases as the number of features decreases , as shown in fig .",
    "[ fig : miknnre ] .",
    "it demonstrates that @xmath3nn often obtains better performance with less features .",
    "our @xmath0-test method performs consistently the best in distinct feature dimensionality , and the highest micro-@xmath2 of @xmath0-test is 89.8% when the number of features is 4000 , which improves up to 4.2% than @xmath1 .",
    "ig achieves the worst performance in the all experiments on skewed corpus with @xmath3nn .    as shown in fig .",
    "[ fig : maknnre ] and fig .",
    "[ fig : miknnre ] , for unbalanced multi - class tasks , we find ig is inferior to mi in terms of both macro-@xmath2 and micro-@xmath2 , whereas ig is superior to mi for binary classification tasks according to the comparative experiments of yang et al  @xcite .",
    "the conflict shows that feature selection methods depends on the practical classification problem .",
    "nn on 20 newsgroup in terms of micro-@xmath2.,scaledwidth=50.0% ]    because macro-@xmath2 on balanced corpus is close to micro-@xmath2 , we only show the results of micro-@xmath2 on 20 newsgroup . as shown in fig .",
    "[ fig : miknnnews ] , the micro-@xmath2 of both @xmath1 and ig are slightly better than our @xmath0-test method , and the four methods are obviously better than mi .",
    "especially , the performance of ig is comparable to @xmath1 , and ece on balanced corpus .",
    "[ fig : masvmre ] and fig .",
    "[ fig : misvmre ] depict the macro-@xmath2 and micro-@xmath2 of different methods on the reuters-21578 corpus using svms .",
    "the @xmath0-test , @xmath1 , and ece methods perform similar performances , which are better than ig and mi methods .",
    "meanwhile , the macro-@xmath2 scores of three methods increase as the number of features reduces .",
    "it is worth noting that mi does better than other methods when the number of features is in [ 15,000 , 24,411 ] , and then mi falls dramatically .     of different methods on reuters-21578 using svms.,scaledwidth=50.0% ]     of different methods on reuters-21578 using svms.,scaledwidth=50.0% ]    the performance of these methods in terms of micro-@xmath2 on reuters-21578 corpus",
    "is shown in fig .",
    "[ fig : misvmre ] .",
    "the micro-@xmath2 points of different feature selection methods show a tendency to increase as the number of the features decreases .",
    "however , these methods show consistent performance in micro-@xmath2 , and the @xmath0-test method is still the best among these methods .     of different methods on 20 newsgroup using svms.,scaledwidth=50.0% ]    fig .",
    "[ fig : misvmnews ] depicts the micro-@xmath2 of different methods on the 20 newsgroups using svm .",
    "the trends of the curves are similar to those in fig .",
    "[ fig : miknnnews ] .",
    "the @xmath0-test , @xmath1 , ig , and ece achieve similar performances , which are better than mi .",
    "our @xmath0-test is slightly better than others .      for centroid - based classifier",
    ", the macro-@xmath2 of five methods is shown in fig .",
    "[ fig : maccre ] . we can observe that @xmath1 , ece , and @xmath0-test do better than mi and ig methods , and @xmath1 is slightly better than ece and @xmath0-test . the same conclusion can be done in terms of micro-@xmath2 , as shown in fig .",
    "[ fig : miccre ] .     of five methods on reuters-21578 using centroid - based classifier.,scaledwidth=50.0% ]     of five methods on reuters-21578 using centroid - based classifier.,scaledwidth=50.0%",
    "]    meanwhile , our @xmath0-test is slightly better than @xmath1 , ece , and ig methods on 20 newsgroup corpus .",
    "the four methods outperform the mi method significantly .     of five methods on 20 newsgroup using centroid - based classifier.,scaledwidth=50.0% ]",
    "in this paper , we proposed a new feature selection method based on term frequency and @xmath0-test .",
    "then we compare our approach with the state - of - the - art methods on two corpora using three classifiers in terms of macro-@xmath2 and micro-@xmath2 .",
    "extensive experiments have indicated that our new approach offers comparable performance with @xmath1 , and ece , even slightly better than them . in future work",
    ", we will verify our method on more text collections .",
    "a.  unler , a.  murat , and r.  b. chinnam .",
    "mr2pso : a maximum relevance minimum redundancy feature selection method based on swarm intelligence for support vector machine classification . , 2011 , 181(20):4625 - 4641 ."
  ],
  "abstract_text": [
    "<S> much work has been done on feature selection . </S>",
    "<S> existing methods are based on document frequency , such as chi - square statistic , information gain etc . </S>",
    "<S> however , these methods have two shortcomings : one is that they are not reliable for low - frequency terms , and the other is that they only count whether one term occurs in a document and ignore the term frequency . </S>",
    "<S> actually , high - frequency terms within a specific category are often regards as discriminators .    </S>",
    "<S> this paper focuses on how to construct the feature selection function based on term frequency , and proposes a new approach based on @xmath0-test , which is used to measure the diversity of the distributions of a term between the specific category and the entire corpus . </S>",
    "<S> extensive comparative experiments on two text corpora using three classifiers show that our new approach is comparable to or or slightly better than the state - of - the - art feature selection methods ( i.e. , @xmath1 , and ig ) in terms of macro-@xmath2 and micro-@xmath2 .    </S>",
    "<S> = 10000 = 10000 </S>"
  ]
}