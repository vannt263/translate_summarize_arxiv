{
  "article_text": [
    "this paper is devoted to the statistical study of certain piecewise deterministic markov processes ( pdmp ) modeling the activity of a biological neural network .",
    "more precisely , we are interested in estimating the the underlying jump rate of the process , _ i.e. _  the spiking rate function of each single neuron .    piecewise deterministic markov processes ( pdmp s ) have been introduced by davis ( @xcite and @xcite ) as a family of cdlg markov processes following a deterministic drift with random jumps .",
    "pdmp s are widely used in probabilistic modeling of _",
    "e.g. _  biological or chemical phenomena ( see _ e.g. _ @xcite or @xcite , see @xcite for an overview ) . in the present paper ,",
    "we study the particular case of pdmp s which are systems of interacting neurons . building a model for",
    "the activity of a neural network that can fit biological considerations is crucial in order to understand the mechanics of the brain .",
    "many papers in the literature use hawkes processes in order to describe the spatio - temporal dependencies which are typical for huge systems of interacting neurons , see @xcite , @xcite and @xcite for example .",
    "our model can be interpreted as hawkes process with memory of variable length ( see @xcite ) ; it is close to the model presented in @xcite .",
    "it is of crucial interest for modern neuro - mathematics to be able to statistically identify the basic parameters defining the dynamics of a model for neural networks .",
    "the most relevant mechanisms to study are the way the neurons are connected to each other and the way that a neuron deals with the information it receives . in @xcite and in @xcite",
    ", the authors build an estimator for the interaction graph , in discrete or in continuous time . in the present work",
    ", we assume that we observe a subsystem of neurons which are all interconnected and behaving in a similar way .",
    "we then focus on the estimation of the firing rate of a neuron within this system .",
    "this rate depends on the membrane potential of the neuron , influenced by the activity of the other neurons .",
    "more precisely , we consider a process @xmath8 where @xmath5 is the number of neurons in the network and where each variable @xmath9 represents the membrane potential of neuron @xmath10 for @xmath11 each membrane potential @xmath12 takes values in a compact interval @xmath13 , $ ] where @xmath14 is interpreted as resting potential ( corresponding to @xmath15 in real neurons ) and where @xmath16 ( see _ e.g. _ @xcite ) .",
    "this process has the following dynamic .",
    "a deterministic drift attracts the membrane potential of each neuron to an equilibrium potential @xmath17 with an exponential speed of parameter @xmath18 moreover , a neuron with membrane potential @xmath19  fires \" ( _ i.e. _ , jumps ) with intensity @xmath20 where @xmath21 is a given intensity function .",
    "when a neuron fires , its membrane potential is reset to @xmath3 interpreted as resting potential , while the membrane potentials of the other neurons are increased by @xmath22 until they reach the maximal potential height @xmath23    the goal of this paper is to explore the statistical complexity of the model described above in a non - parametric setting .",
    "we aim at giving precise statistical characteristics ( such as optimal rates of convergence , estimation procedures ) such that we are able to compare systems of interacting neurons to benchmark non - parametric models like density estimation or nonlinear regression .",
    "more precisely , given the continuous observation of the system of interacting neurons over a time interval @xmath24 $ ] ( with asymptotics being taken as @xmath25 ) , we infer on the different parameters of the model which are : the equilibrium potential @xmath26 the speed of attraction @xmath27 and the spiking rate function @xmath28 . since in a continuous time setting , the coefficients @xmath29 and @xmath30 are known ( they can be identified by any observation of the continuous trajectory of a neuron s potential between two successive jumps ) , the _ typical _ problem is the estimation of the unknown spiking rate @xmath31",
    "therefore we restrict our attention to the estimation of the unknown spiking rate @xmath32 we measure smoothness of the spiking rate by considering hlder classes of possible shapes for the spiking rate and suppose that the spiking rate has smoothness of order @xmath33 in a hlder sense . to estimate the jump rate @xmath28 in a position @xmath34 we propose a nadaraya - watson type kernel estimator which is roughly speaking of the form @xmath35 $ } } { \\mbox { occupation time of $ b_h(a)$ during $ [ 0 , t ] $ } } , \\ ] ] where @xmath36 is a neighborhood of size @xmath37 of the position @xmath38 where we estimate the jump rate function @xmath39 a rigorous definition of the estimator is given in terms of the jump measure and an occupation time measure of the process @xmath40 the convergence of the estimator is implied by the fact that the compensator of the jump measure is the occupation time measure integrated against the jump rate function @xmath41 together with uniform ergodicity of the process . assuming that the jump rate function @xmath28 has smoothness of order @xmath33 in a hlder sense",
    ", we obtain the classical rate of convergence of order @xmath42 for the point - wise @xmath43error of the estimator .",
    "this rate is shown to be optimal .",
    "we also state two important probabilistic tools that are needed in order to obtain the statistical results .",
    "the first one is the uniform positive harris recurrence of process .",
    "the second one is the existence of a regular density function of the invariant measure of a single neuron .    in the literature ,",
    "non - parametric estimation for pdmp s has already been studied , see for example @xcite and , more particularly concerning the estimation of the jump rate , @xcite . on the contrary to these studies , the framework of the present work is more difficult for two reasons .",
    "the first reason is the fact that our process is multidimensional , presenting real interactions between the neurons .",
    "of course , estimation problems for multidimensional pdmp s have already been studied . however , in all cases we are aware of , a so - called  many - to - one formula \" ( see @xcite , see also @xcite ) allows to express the occupation time measure of the whole system in terms of a single `` typical '' particle .",
    "this is not the case in the present paper  and it is for this reason that we have to work under the relatively strong condition of uniform ergodicity which is implied by compact state space  a condition which is biologically meaningful .",
    "the second , more important , reason is the fact that the transition kernel associated to jumps is degenerate .",
    "this is why the construction of our estimator is different from other constructions in previous studies .",
    "the degeneracy of the transition kernel also leads to real difficulties in the study of the regularity of the invariant density of a single neuron , see @xcite and the discussions therein .    in section [ sec : results ] , we describe more precisely our model and state our main results .",
    "we first provide two probabilistic results necessary to prove the convergence of the estimator : firstly , the positive harris recurrence of the process @xmath44 in theorem [ theo : harrisok ] and secondly the properties of the invariant measure in theorem [ theo : invmeasure ] .",
    "the speed of convergence of our estimator is established in theorem [ theo : main ] .",
    "finally , theorem [ theo : lowerbound ] states that our speed of convergence is optimal for the point - wise @xmath45error , uniformly in @xmath39 the key tool to prove this optimality is to study the asymptotic properties of the likelihood process for a small perturbation of the function @xmath28 close to @xmath46    the proofs of theorems [ theo : harrisok],[theo : main ] and [ theo : lowerbound ] are respectively given in sections [ sec : harris ] , [ sec : proofmain ] and [ sec : optimal ] .",
    "we refer the reader to @xcite for a proof of theorem [ theo : invmeasure ] .",
    "let @xmath47 be fixed and @xmath48 be a family of _ i.i.d .",
    "_  poisson random measures on @xmath49 having intensity measure @xmath50 we study the markov process @xmath51 taking values in @xmath52^n$ ] and solving , for @xmath53 , for @xmath54 , @xmath55 in the above equation , @xmath56 is a positive number , @xmath57 is the equilibrium potential value such that @xmath58 moreover , we will always assume that @xmath59 finally , the functions @xmath60 \\to [ 0 , k ]   $ ] and @xmath61 satisfy ( at least ) the following assumption .    [ ass:1 ]   + 1 .",
    "@xmath60 \\to [ 0 , \\frac{1}{n } ]   $ ] is non - increasing and smooth , @xmath62 for all @xmath63 and @xmath64 for all @xmath65 + 2 .",
    "@xmath66 @xmath28 is non - decreasing , @xmath67 and there exists @xmath68 non - decreasing , such that @xmath69 for all @xmath70 .",
    "all membrane potentials take values in @xmath71 , $ ] where @xmath72 is the maximal height of the membrane potential of a single neuron .",
    "@xmath14 is interpreted as resting potential ( corresponding to @xmath15 in real neurons ) and @xmath16 ( see _ e.g. _ @xcite ) . in",
    ", @xmath27 gives the speed of attraction of the potential value of each single neuron to an equilibrium value @xmath73 the function @xmath74 denotes the increment of membrane potential received by a neuron when an other neuron fires . for neurons with membrane potential away from the bound @xmath75 this increment is equal to @xmath4 however , for neurons with membrane potential close to @xmath75 this increment may bring their membrane potential above the bound @xmath23 this is why we impose this dynamic close to the bound @xmath23    in what follows , we are interested in the estimation of the intensity function @xmath41 assuming that the parameters @xmath76 and @xmath74 are known and that the function @xmath28 belongs to a certain hlder class of functions .",
    "the parameters of this class of functions are also supposed to be known .",
    "the assumption @xmath77 comes from biological considerations and expresses the fact that a neuron , once it has fired , has a refractory period during which it is not likely to fire .",
    "the generator of the process @xmath44 is given for any smooth test function @xmath78^n \\to \\r $ ] and @xmath79^n$ ] by @xmath80 - \\lambda \\sum_i \\left ( \\frac{\\partial \\varphi}{\\partial x_i } ( x ) \\left [   x_i -m   \\right ]   \\right ) , \\ ] ] where @xmath81    the existence of a process @xmath44 with such dynamics is ensured by an acceptance / rejection procedure that allows to construct solutions to ( [ eq : dyn ] ) explicitly .",
    "more precisely , since each neuron spikes at maximal intensity @xmath82 we can work conditionally on the realization of a poisson process @xmath83 with intensity @xmath84 we construct the process @xmath44 considering the jump times @xmath85 of @xmath83 as candidates for the jump times of @xmath44 and accepting them with probability @xmath86 it is then possible to construct a solution to step by step , following the deterministic drift between the jump times of @xmath87 and jumping according to this acceptance / rejection procedure .",
    "we refer the reader to theorem 9.1 in chapter iv of @xcite for a proof of the existence of the process @xmath88    we denote by @xmath89 the probability measure under which the solution @xmath90 of starts from @xmath91^n .",
    "$ ] moreover , @xmath92^n } \\nu   ( dx ) p_x $ ] denotes the probability measure under which the process starts from @xmath93 figure [ fig .",
    "1 ] is an example of trajectory for @xmath94 neurons , choosing @xmath95 and @xmath96        the aim of this work is to estimate the unknown firing rate function @xmath28 based on an observation of @xmath97 continuously in time .",
    "notice that for all @xmath98 @xmath99 reaches the value @xmath14 only through jumps .",
    "therefore , the following definition gives the successive spike times of the @xmath100th neuron , @xmath101 we put @xmath102 and introduce the jump measures @xmath103 by our assumptions , @xmath104 is compensated by @xmath105 and therefore the compensator @xmath106 of @xmath107 is given by @xmath108 is the total occupation time measure of the process @xmath40    we will also write @xmath109 for the successive jump times of the process @xmath110 _ _",
    "i.e.__@xmath111    for some kernel function @xmath112 such that @xmath113 we define the kernel estimator for the unknown function @xmath28 at a point @xmath38 with bandwidth @xmath37 , based on observation of @xmath44 up to time @xmath114 by @xmath115 for @xmath37 small , @xmath116 is a natural estimator for @xmath117 indeed , this expression as a ratio follows the intuitive idea to count the number of jumps that occurred with a position close to @xmath38 and to divide by the occupation time of a neighborhood of @xmath34 which is natural to estimate an intensity function depending on the position @xmath46 more precisely , by the martingale convergence theorem , the numerator @xmath118 should behave , for @xmath119 large , as @xmath120 but by the ergodic theorem , @xmath121 as @xmath122 where @xmath123 is the stationary measure of each neuron",
    "@xmath124 finally , if the invariant measure @xmath125 is sufficiently regular , then @xmath126 as @xmath127    we restrict our study to fixed hlder classes of rate functions @xmath39 for that sake , we introduce the notation @xmath128 for @xmath129 and @xmath130 we consider the following hlder class for arbitrary constants @xmath131 and a function @xmath132 as in assumption [ ass:1 ] .",
    "@xmath133 , \\ ; \\\\",
    "f(x ) \\geq f_{min}(x ) \\mbox { for all $ x\\in [ 0 , k]$ , }   \\ ; \\ | f^{(k ) } ( x ) - f^{(k ) } ( y ) | \\le l |x- y |^\\alpha   \\mbox { for all } x , y \\in [ 0 , k]\\ } .\\end{gathered}\\ ] ]      in this section , we collect important probabilistic results .",
    "we first establish that the process @xmath134 is recurrent in the sense of harris .",
    "[ theo : harrisok ] grant assumption [ ass:1 ] .",
    "then the process @xmath44 is positive harris recurrent having unique invariant probability measure @xmath135 _ i.e. _  for all @xmath136^n ) , $ ] @xmath137 for all @xmath138^n   .$ ] moreover , there exist constants @xmath139 and @xmath140 which do only depend on the class @xmath141 but not on @xmath41 such that @xmath142    it is well - known that the behavior of a kernel estimator such as the one introduced in depends heavily on the regularity properties of the invariant probability measure of the system .",
    "our system is however very degenerate .",
    "firstly , it is a piecewise deterministic markov process ( pdmp ) in dimension @xmath143 with interactions between particles .",
    "hence , no brownian noise is present to smoothen things .",
    "moreover , the transition kernels associated to the jumps of system are highly degenerate ( recall ) . the transition kernel @xmath144 with @xmath145 puts one particle ( the one which is just spiking ) to the level @xmath146 as a consequence , the above transition does not create density  and it even destroys smoothness due to the reset to @xmath14 of the spiking neuron . finally , the only way that `` smoothness '' is generated by the process is the smoothness which is present in the `` noise of the jump times '' ( which are basically of exponential density ) .",
    "for this reason , we have to stay away from the point @xmath147 where the drift of the flow vanishes .",
    "moreover , the reset - to-@xmath14 of the spiking particles implies that we are not able to say anything about the behavior of the invariant density of a single particle in @xmath148 ( actually , near to @xmath14 ) neither .",
    "finally , we also have to stay strictly below the upper bound of the state space @xmath23 that is why we introduce the following open set @xmath149 given by @xmath150 : \\frac{\\lfloor   \\beta \\rfloor}{n } < w < k-    \\frac{\\lfloor   \\beta \\rfloor}{n } , |w - m| > d \\ } , \\ ] ] where @xmath151 is the smoothness of the fixed class @xmath152 that we consider and where @xmath153 is fixed such that @xmath154 notice that @xmath155 also depends on @xmath156 and @xmath5 which are supposed to be known .",
    "we are able to obtain a control of the invariant measure only on this set @xmath157 the dependence in @xmath151 is due to the fact that the regularity of @xmath28 is transmitted to the invariant measure by the means of successive integration by parts ( see @xcite for more details ) .",
    "we quote the following theorem from @xcite .",
    "[ theo : invmeasure ] ( theorem 5 of @xcite )    suppose that @xmath158 let @xmath159 be the invariant measure of a single neuron , _",
    "@xmath160 then @xmath161 possesses a bounded continuous lebesgue density @xmath162 on @xmath155 for any @xmath163 such that @xmath164 which is bounded on @xmath165 uniformly in @xmath158 moreover , @xmath166 and @xmath167 where the constant @xmath168 depends on @xmath169 and on the smoothness class @xmath170 but on nothing else .",
    "we can now state the main theorem of our paper which describes the quality of our estimator in the minimax theory .",
    "we assume that @xmath57 and @xmath27 are known and that @xmath28 is the only parameter of interest of our model .",
    "we shall always write @xmath171 and @xmath172 in order to emphasize the dependence on the unknown @xmath39 fix some @xmath173 and some suitable point @xmath174 for any possible rate of convergence @xmath175 increasing to @xmath176 and for any process of @xmath177measurable estimators @xmath178 we shall consider point - wise square risks of the type @xmath179 , \\ ] ] where @xmath180 is roughly the event ensuring that sufficiently many observations have been made near @xmath34 during the time interval @xmath181.$ ] we are able to choose @xmath182 small enough such that @xmath183 see proposition [ prop : imlb ] below .",
    "recall that the kernel @xmath112 is chosen to be of compact support .",
    "let us write @xmath184 for the diameter of the support of @xmath185 therefore @xmath186 if @xmath187 for any fixed @xmath188 write @xmath189 here , @xmath190    [ theo : main ] let @xmath191 and choose @xmath192 such that @xmath193 for all @xmath194 and @xmath195 then there exists @xmath196 such that the following holds for any @xmath197 and for any @xmath198 + ( i ) for the kernel estimate with bandwidth @xmath199 for all @xmath200,$ ] @xmath201   < \\infty .\\ ] ]    \\(ii ) moreover , for @xmath202 for every @xmath203 and @xmath204 @xmath205 weakly under @xmath206 where @xmath207    the next theorem shows that the rate of convergence achieved by the kernel estimate @xmath208 is indeed optimal .",
    "[ theo : lowerbound ] let @xmath204 and @xmath209 $ ] be any starting point .",
    "then we have @xmath210 > 0 , \\ ] ] where the infimum is taken over the class of all possible estimators @xmath211 of @xmath212    the proofs of theorems [ theo : main ] and [ theo : lowerbound ] are given in sections [ sec : proofmain ] and [ sec : optimal ] .      in this subsection , we present some results on simulations , for different jump rates @xmath39 the other parameters are fixed : @xmath213 and @xmath214 the dynamics of the system are the same when @xmath215 and @xmath28 have the same ratio . in other words , variations of @xmath215 and @xmath28 keeping the same ratio between the two parameters lead to the same law for the process rescaled in time .",
    "this is why we fix @xmath216 and propose different choices for @xmath39 the kernel @xmath112 used here is a truncated gaussian kernel with standard deviation 1 .",
    "we present for each choice of a jump rate function @xmath28 the associated estimated function @xmath217 and the observed distribution of @xmath44 or more precisely of @xmath218 figures 2 , 3 and 4 correspond respectively to the following definitions of @xmath219 and @xmath220    for figures [ fig .",
    "2 ] , [ fig . 3 ] and [ fig .",
    "4 ] , we fixed the length of the time interval for observations respectively to @xmath221 @xmath222 and @xmath223 this allows us to obtain a similar number of jump for each simulation , respectively equal to @xmath224 @xmath225 and @xmath226 these simulations are realized with the software r.    the optimal bandwidth @xmath227 depends on the regularity of @xmath28 given by the parameter @xmath228 therefore , we propose a data - driven bandwidth chosen according to a cross - validation procedure . for that sake",
    ", we define the sequence @xmath229 by @xmath230 for all @xmath231 for each @xmath232 $ ] and each sample @xmath233 for @xmath234 we define the random variable @xmath235 by @xmath236 @xmath235 can be seen as an estimator of the invariant measure @xmath237 of the discrete markov chain .",
    "we propose an adaptive estimation procedure at least for this simulation part .",
    "we use a smoothed cross - validation ( scv ) to choose the bandwidth ( see for example the paper of hall , marron and park @xcite ) , following ideas which were first published by bowmann @xcite and rudemo @xcite . as the bandwidth is mainly important for the estimation of the invariant probability @xmath238 , we use a cross validation procedure for this estimation .",
    "more precisely , we use a first part of the trajectory to estimate @xmath239 and then another part of the trajectory to minimize the cross validation @xmath240 in @xmath241 in order to be closer to the stationary regime , we chose the two parts of the trajectory far from the starting time .",
    "moreover we chose two parts of the trajectory sufficiently distant from each other .",
    "this is why we consider @xmath242 and @xmath243 such that @xmath244    we use the method of the least squares cross validation and minimize @xmath245 ( where we have approximated the integral term by a riemann approximation ) , giving rise to a minimizer @xmath246 we then calculate the estimator @xmath217 the long of the trajectory . in the next figure , we use this method to find the reconstructed @xmath247 with an adaptive choice of @xmath241    ]    ]    ]    as expected , we can see that the less observations we have , the worse is our estimator . note that close to @xmath14 the observed density of @xmath44 explodes .",
    "this was also expectable due to the reset to @xmath14 of the jumping neurons .",
    "moreover , the simulations show a lack of regularity of the observed density close to @xmath248 which is consistent with our results , but this does not seem to affect the quality of the estimator .",
    "in this section , we give the proof of theorem [ theo : harrisok ] and show that the process @xmath249 is positive recurrent in the sense of harris .",
    "we follow a classical approach and prove the existence of regeneration times .",
    "this is done in the next subsection and follows ideas given in duarte and ost @xcite .",
    "the main idea of proving a regeneration property of the process is to find some uniform `` noise '' for the whole process on some `` good subsets '' of the state space . since the transition kernel associated to the jumps of our process is not creating any density ( and actually destroys it for the spiking neurons which are reset to @xmath14 ) , the only source of noise is given by the random times of spiking .",
    "these random times are then transported through the deterministic flow @xmath250 which is given for any starting configuration @xmath251^n$ ] by @xmath252    the key idea of what follows  which is entirely borrowed from @xcite  is the following .",
    "write @xmath253 for the sequence giving the index of the spiking neuron at time @xmath254 _ i.e. _ @xmath255 if and only if @xmath256 for some @xmath257 it is clear that in order to produce an absolute continuous law with respect to the lebesgue measure on @xmath71^n , $ ] we need at least @xmath5 jumps of the process . on any event of the type @xmath258 it is possible to write the position of the process at time @xmath259 as a concatenation of the deterministic flows given by @xmath260 proving absolute continuity amounts to prove that the determinant of the jacobian of the map @xmath261 does not vanish . for general sequences of @xmath262",
    "this will not be true ( think _ e.g. _  of the sequence @xmath263 ) .",
    "the main idea is however to consider the sequence @xmath264 and to use the _ regeneration property of spiking _ , _",
    "i.e. _  the fact that the neuron @xmath265 spiking at time @xmath266 is reset to zero at time @xmath267 in this case , for all later times , its position does not depend on @xmath268 any more .",
    "in other words , the jacobian of @xmath269 is a diagonal matrix , and all we have to do is to control that all diagonal elements do not vanish .",
    "the second idea is to linearize the flow , _",
    "i.e. _  to consider the flow during very short time durations , and to use that , just after spiking , each diagonal element is basically of the form @xmath270 the important fact here is that the absolute value of the drift term of the deterministic flow of one neuron is strictly positive when starting from the initial value @xmath146    in the following , this idea is made rigorous .",
    "our proof follows the approach given in section 4 of @xcite .",
    "we fix @xmath271 and put @xmath272 and @xmath273 which is the event that all @xmath5 neurons have spiked in the fixed order given by their numbers , _",
    "i.e. _  neuron @xmath274 spikes first , then neuron @xmath275 then @xmath276 and so on .",
    "we introduce @xmath277 which would be the position of neurons after @xmath5 spikes and on the event @xmath278 if @xmath279 ( here , we suppose w.l.o.g . that @xmath280 )",
    ".    now we fix any initial configuration @xmath251^n $ ] and introduce the sequence of configurations @xmath281 given by @xmath282 @xmath283 and @xmath284 notice that @xmath285 if @xmath286 notice also that @xmath287    we cite the following lemma from @xcite .    [ lem : positionbresiliens ] if @xmath288 then on the event @xmath289 we have for all @xmath290 + ( i ) @xmath291 if @xmath292 + ( ii ) @xmath293 if @xmath294 + ( iii ) @xmath295 if @xmath296 and @xmath297    here , @xmath298 and @xmath299 moreover , the remainder functions are of order @xmath300 and all partial derivatives are of order either @xmath301 or @xmath302 uniformly in @xmath303    our model is slightly different from the model in @xcite : instead of an attraction to the empirical mean of the system , we have an attraction to a fixed equilibrium value @xmath2 this leads to our definition of @xmath304 which is slightly different from the one used in @xcite .",
    "put @xmath305 then we have on @xmath306 @xmath307 where @xmath308    we put as in @xcite @xmath309 where @xmath310 hence @xmath311 models how the @xmath5 successive jump times @xmath312 are mapped , through the deterministic flow , into a final position at time @xmath313 on the event @xmath314 in order to control how the law of the @xmath1 successive jump times @xmath315 is transported through this flow , we calculate the partial derivatives of @xmath316 with respect to @xmath317 one sees immediately that @xmath318 whence    for each @xmath319 the determinant of the jacobian of the map @xmath320 is given by @xmath321 which is different from zero for @xmath322 and @xmath301 small enough , for all @xmath323    as in proposition 4.1 of @xcite , we now have two important conclusions from the above discussion .",
    "there exists @xmath324 and @xmath325 such that for @xmath326 @xmath327 where @xmath328 is a probability measure and @xmath3290 , 1 [ .$ ]    the lower bound is a local doeblin condition , and its proof is given in proposition 4.1 of @xcite .",
    "we call @xmath330 a regeneration set : if the process visits this regeneration set , then after a time @xmath331 there is a probability @xmath332 that the law of the process is independent from its initial position @xmath333    to be able to make use of the local doeblin condition , we have to be sure that the process actually does visit the regeneration set @xmath334 this is granted by the following result .    [",
    "prop : control ] there exist @xmath271 and @xmath335 such that @xmath336^n }   p_{t^ * } ( v , b_{\\delta^ * } ( u^ * ) ) \\geq \\eta_2 , \\ ] ] for @xmath305    by ( [ eq:14 ] ) , there exists @xmath322 such that for all @xmath337^n , $ ] we have that @xmath338 on @xmath339 when @xmath340 hence @xmath341    recalling ( [ eq : flow ] ) , we then obtain @xmath342 where @xmath343 where the sequence @xmath344 is given as in ( [ eq : vk ] ) .",
    "since by assumption @xmath345^n , $ ] it is immediate to see that @xmath346 for a constant @xmath347 for all @xmath348 for all @xmath349 and for all @xmath350 moreover , @xmath351 and since @xmath28 is non decreasing , satisfying @xmath352 and @xmath353 this implies that @xmath354 on @xmath355 similar arguments show that all consecutive terms are strictly lower bounded uniformly in @xmath203 as well . as a consequence , @xmath356 which concludes the proof .",
    "[ rq : regenunif ] in the proof of proposition 4.1 of @xcite , the authors have no need to obtain ( [ eq : regen ] ) uniformly in @xmath158 however , it is easy to see that we can rewrite their proof using the bounds for @xmath357 appearing in the proof of proposition [ prop : control ] above . as a consequence , we obtain @xmath358 for some @xmath359    once we dispose of the uniform local doeblin condition and of the control given in proposition [ prop : control ] , it is classical , using regeneration arguments , to show that the process is recurrent in the sense of harris .      using the regeneration procedure",
    ", we can prove that the process @xmath44 is positive harris recurrent .",
    "we denote by @xmath360 the total variation distance , _",
    "@xmath361^n ) } \\left| \\nu_1 ( b ) - \\nu_2 ( b ) \\right| , $ ] for any two probability measures @xmath362 on @xmath363^n , { \\mathcal b } ( [ 0 , k ] ^n ) ) .$ ]    we first show that the process is indeed harris . for that sake , define the sequence of stopping times @xmath364 @xmath365 and for all @xmath366 @xmath367    let @xmath368 be a sequence of _ i.i.d . _  uniform random variables on @xmath369,$ ] which are independent of the process @xmath40 then",
    ", working conditionally on the realization of @xmath370 we define the sequence @xmath371 and the sequence @xmath372 of regeneration times as follows .",
    "@xmath373 and for all @xmath366 @xmath374 where @xmath375 is given in .",
    "[ rem : iidregen ] ( [ eq : regenunif ] ) allows us to construct the process @xmath134 on a bigger probability space in such a way that for all @xmath376 and @xmath377 is independent from @xmath378 this construction is known as nummelin splitting , we refer the interested reader to chapter 6 of lcherbach ( 2013 ) @xcite .",
    "[ lem : ftregen ] for all @xmath79^n , e_x ( r_1 ) < \\infty$ ] and @xmath379    the proof of this lemma is postponed to the next subsection where we prove a stronger result .",
    "now the following result implies that our process is actually positive harris recurrent .",
    "[ prop : hr ] @xmath44 is harris recurrent with invariant probability measure @xmath380 which is given by @xmath381    fix @xmath382^n)$ ] and define the process @xmath383 by @xmath384 assume that @xmath385 then , according to the definition of harris recurrence , it is enough to show that for all @xmath386    we denote by @xmath387 , @xmath388 and @xmath389 the counting processes respectively associated with the sequences of stopping times @xmath390 and @xmath391    @xmath392    for all @xmath114 we have @xmath393 and @xmath394    when @xmath114 goes to @xmath395 we obtain , using lemma [ lem : ftregen ] to deal with the first and the last terms ,    @xmath396    the decomposition between even and odd regeneration times is used here to be able to apply the strong law of large numbers , based on remark [ rem : iidregen ] . in this way we obtain that @xmath397 we can use the same decomposition to obtain that @xmath398 putting all together we have @xmath399 and we can conclude the proof using lemma [ lem : ftregen ] once again .      we now show how to couple two processes @xmath44 and @xmath400 following the same dynamics ( [ eq : dyn ] ) using proposition [ prop : control ] and the lower bound ( [ eq : regenunif ] ) of remark [ rq : regenunif ] .",
    "this coupling will give us a control of the distance in total variation between @xmath89 and @xmath401 where @xmath19 and @xmath402 are the respective starting points of processes @xmath44 and @xmath403    the coupling procedure consists in using the same realization of uniform random variables @xmath404 for both processes , relying on ( [ eq : regenunif ] ) , when both processes @xmath44 and @xmath400 are in the regeneration set @xmath330 at the same time .",
    "more precisely , we let evolve @xmath44 and @xmath400 independently up to the first time that they are both in the set @xmath334 we introduce the sequence of stopping times @xmath405 and @xmath406    applying proposition [ prop : control ] to two independent processes @xmath44 and @xmath407 we obtain    @xmath408^n   }   p^{\\otimes 2}_{t^ * } ( ( v_1 , v_2 ) , b_{\\delta^ * } ( u^ * ) ^2 ) \\geq \\eta_2 ^ 2.\\ ] ]    as a consequence , @xmath409 almost surely for all @xmath410 and @xmath411 _ i.e. _  @xmath412 and @xmath413 possess exponential moments @xmath414 < \\infty\\ ] ] uniformly in the starting configuration @xmath415 for all @xmath416    we are now able to couple the processes @xmath44 and @xmath403 we work conditionally on the realization of a sequence of _ i.i.d .",
    "_   uniform random variables @xmath404 and define the coupling time @xmath417 by @xmath418 using the regenerative construction described in the previous subsection based on ( [ eq : regenunif ] ) , it is evident that @xmath44 and @xmath400 can be constructed jointly in such a way that @xmath419 and such that @xmath420 for all @xmath421 since @xmath422 is constructed by sampling within the sequence @xmath423 at an independent geometrical time , it is immediate to see that there exists @xmath424 such that @xmath425^n } e_{(v^1,v^2)}(\\kappa^{\\tau } ) < + \\infty.\\ ] ]    notice that the regeneration time @xmath426 can be compared to @xmath422 and that @xmath427 as a consequence , implies a proof of lemma [ lem : ftregen ] .    since the two processes @xmath44 and @xmath400 follow the same trajectory after time @xmath428 we obtain the following classical upper bound on the total variation distance .",
    "@xmath429    now putting @xmath430^n } e_{(x , y ) } ( \\kappa^{\\tau}),$ ] the integration of ( [ eq : couplingspeed ] ) with respect to the invariant measure @xmath431 implies that @xmath432^n } \\left\\vert p_t(x,\\cdot ) - \\pi \\right\\vert_{tv } \\leq c \\kappa^{-t}.\\ ] ] this finishes the proof of theorem [ theo : harrisok ] .",
    "@xmath433      we start with some simple preliminary estimates . recall that @xmath434 denotes the jump measure of the system , with compensator @xmath435 let @xmath436 be the jump chain .",
    "then the following holds .",
    "[ prop:3 ] @xmath437 is harris recurrent with invariant measure given by @xmath438 for any @xmath439 measurable and bounded .",
    "let @xmath440 be a bounded test function .",
    "we have to prove that @xmath441 as @xmath442 @xmath443almost surely , for any fixed starting point @xmath79^n.$ ] but @xmath444 and , putting @xmath445 @xmath446 by the law of large numbers , @xmath447 and this convergence holds almost surely .",
    "moreover , @xmath448 where @xmath449.$ ] then @xmath450 is in @xmath451 the set of all locally square integrable purely discontinuous martingales , with predictable quadratic covariation process @xmath452 where @xmath453 almost surely , as @xmath454 by the martingale convergence theorem , see _",
    "e.g. _  jacod - shiryaev ( 2003 ) @xcite , chapter viii , corollary 3.24 , @xmath455 converges in law to a normal distribution . as a consequence , @xmath456 almost surely .",
    "we now treat the second term in . by the ergodic theorem for integrable additive functionals , @xmath457 and this finishes the proof .",
    "* exchangeability of the invariant measure * we denote by @xmath458 the @xmath100th coordinate map .    for all @xmath459 @xmath460    fix an initial configuration @xmath461^n$ ] consisting of @xmath5 particles which are all in the same position . let @xmath462 be a bounded test function and introduce @xmath463 _ i.e.",
    "_  @xmath464 depends only on the first coordinate . by the ergodic theorem , @xmath465 @xmath443almost surely .    now , introduce the system @xmath466 given by @xmath467 for all @xmath468 and @xmath469 @xmath470 since the generator of @xmath44 is invariant under permutations , @xmath471 in particular , @xmath472 on the other hand , @xmath473 and this finishes the proof .",
    "we are now going to study the support properties of the invariant measure of a single neuron .",
    "for that sake define for all @xmath200 , \\ ; b(x):=\\lambda ( x - m)$ ] and recall that @xmath474 denotes the solution of @xmath475 given by @xmath476 moreover , for @xmath477 @xmath478 finally , let @xmath479 where @xmath480 and where @xmath481 was defined in before .",
    "we will use the change of variable , for a fixed value of @xmath482 @xmath483 and denote by @xmath484 the inverse function of @xmath485    these definitions permit to obtain an expression of @xmath125 .",
    "for all @xmath486 we have @xmath487 here the notation @xmath488 denotes either @xmath489y , m[$ ] if @xmath490 or @xmath489m , y[$ ] if @xmath491    we have , by proposition [ prop:3 ] , @xmath492 we use that @xmath493 then we obtain @xmath494 now , let @xmath495 be a smooth test function having compact support in @xmath496 using , we obtain @xmath497 then , with the change of variable announced in , we can rewrite in the following way : @xmath498      [ prop : suptrans ] for all @xmath4990,k[,$ ] all @xmath500 we have @xmath501    fix @xmath4990,k[$ ] and let @xmath502 and @xmath503 be such that @xmath504    we define the time @xmath505 such that @xmath506 and consider , for a fixed @xmath507 the following events :    @xmath508 and @xmath509 the idea of the proof is that the event @xmath510 leads the neuron 1 to a position close to @xmath402 after a time @xmath511    at time @xmath512 the neuron 1 jumps so that its position is reset to @xmath3 the time @xmath505 is defined such that at time @xmath513 the position of neuron 1 is close to @xmath514 then in an interval of time short enough for the deterministic drift to be insignificant , we impose that the other neurons jump @xmath265 times so that at time @xmath515 the position of neuron 1 is indeed close to @xmath516    in other words we can use similar arguments to the ones used in the proof of lemma [ lem : positionbresiliens ] to obtain that , for all @xmath200^n,$ ] if @xmath517 then on the event @xmath518 we have @xmath519 and we can choose @xmath520 such that @xmath521    now we have to prove that @xmath522^n } p_x ( a_y \\cap s_y ) > 0 , \\ ] ] which can be done as in the proof of proposition [ prop : control ] .    finally , integrating this result against the measure @xmath523 gives us the conclusion of the proof .",
    "we can now obtain as corollary of the following proposition .",
    "[ prop : imlb ] we have that @xmath524 and for all @xmath200^n,$]and for all @xmath525 @xmath526    recalling the construction of @xmath123 in ( [ eq : pi1dens ] ) , we have @xmath527 to obtain a lower bound uniform in @xmath28 of this expression we use again the bounds of the class of function @xmath528 @xmath529 doing this , we will also need an upper bound for @xmath530 this is possible due to the term @xmath531 since @xmath402 is such that @xmath532 the flow starting from @xmath402 can reach @xmath38 in a finite time , even if we consider the worst cases where @xmath533 or @xmath23    thanks to proposition [ prop : suptrans ] , we have @xmath534 implying that the integration of @xmath535 against the measure @xmath536 is not 0 . finally , due to the definition of @xmath537",
    "we have no problem to obtain this lower bound uniformly in @xmath538 and this finishes the proof of .",
    "is obtained easily from thanks to the ergodic theorem : we have    @xmath539 ( recall that @xmath540 ) , which concludes the proof .",
    "we now study the speed of convergence of our estimator .",
    "first we have the following classical kernel approximation :    [ prop : kernapprox ] for any hlder function @xmath464 of order @xmath541 satisfying @xmath542 for some constant @xmath543 and for a kernel @xmath112 as in theorem [ theo : main ] , we have : @xmath544 where we recall that @xmath184 is the diameter of the support of @xmath112 and where @xmath545 denotes the @xmath546-norm of @xmath547    using the property @xmath548 and the change of variable @xmath549 we obtain @xmath550 then , a taylor - lagrange expansion of the function @xmath464 gives us @xmath551 for some @xmath552a , a+xh [ \\cup ] a + xh , a [ .$ ] by the assumptions of theorem [ theo : main ] , @xmath193 for all @xmath553 then condition ( [ eq : holderg ] ) allows to conclude .",
    "fix @xmath554 and define , for all @xmath555 the centered jump measure .",
    "[ prop : ernum ] under the conditions of theorem [ theo : main ] , there exists a constant @xmath556 depending only on @xmath557 and @xmath185 such that for all @xmath558 for all @xmath200 $ ] and for a bandwidth of the form @xmath559 for some @xmath560 @xmath561 } \\int_\\r q_h ( y - a ) \\tilde   \\mu ( ds , dy )   \\right)^2 \\right ] \\leq \\frac{c_1}{ht}.\\ ] ]    we start working under the invariant regime in the first part of the proof , _",
    "i.e. _  we will work under @xmath562 in a second time we will use theorem [ theo : harrisok ] to obtain the result for any starting point @xmath200^n.$ ]    we use the properties of the compensator @xmath563 and its explicit expression to write @xmath564}\\int_\\r q_h ( y - a ) \\tilde \\mu(ds , dy )   \\right)^2 \\right ]    = \\frac{1}{(nt)^2 } e_{\\pi } \\left [ \\int_0^t \\int_\\r \\left ( q_h ( y - a ) \\right)^2 \\hat \\mu(ds , dy ) \\right ] \\\\    = \\frac{1}{(nt)^2 } e_{\\pi } \\left [ \\int_0^t \\int_\\r \\left ( q_h ( y - a ) \\right)^2 f(y ) \\eta(ds , dy )   \\right].\\end{gathered}\\ ] ]    now , since we are in the invariant regime , we can use the density of the invariant measure of a single particle ( recall theorem [ theo : invmeasure ] ) to obtain @xmath565 } \\int_\\r q_h ( y - a ) \\tilde \\mu(ds , dy )   \\right)^2 \\right ] = \\frac{1}{nt }   \\int_\\r \\left ( q_h ( y - a ) \\right)^2 f(y ) \\pi_1 ( y)dy.\\ ] ] our aim is to obtain a control of @xmath566 independently of @xmath241 to do this we use the change of variable @xmath567 and write @xmath565 } \\int_\\r q_h ( y - a ) \\tilde \\mu(ds , dy )   \\right)^2 \\right ] \\\\ = \\frac{1}{nht }   \\int_\\r q^2 ( x ) f(a+xh ) \\pi_1 ( a+xh)dx .\\ ] ] this yields @xmath568}\\int_\\r q_h ( y - a ) \\tilde \\mu(ds , dy )   \\right)^2 \\right ] \\leq \\frac{f}{nht } \\| q \\|_{l^2 } ^2 \\sup _ { x \\in s_{d/2 , k } } \\pi_1 ( x).\\ ] ] this result holds in stationary regime , but thanks to the exponential speed of convergence of theorem [ theo : harrisok ] , we can obtain it for any starting point @xmath200^n$ ] as we are going to show now . for that sake",
    "we fix the bandwidth @xmath37 in function of @xmath114 so that this speed of convergence depends only on @xmath6 for the moment , we will assume that @xmath37 is of the form @xmath569 for some constant @xmath5700,1[.$ ] as in the beginning of the proof , we can write @xmath571}\\int_\\r q_h ( y - a ) \\tilde   \\mu(ds , dy )   \\right)^2 \\right ] = \\frac{1}{(nt)^2 } e_x \\left [ \\int_0^t \\int_\\r \\left ( q_h ( y - a ) \\right)^2 f(y ) \\eta(ds , dy )   \\right].\\ ] ] now , we have the following decomposition @xmath572}\\int_\\r q_h ( y - a ) \\tilde   \\mu(ds , dy )   \\right)^2 \\right ] \\\\ = \\frac{1}{(n t)^2 } e_x \\left [ \\sum_{i=1}^n \\int_0^t \\left ( q_h ( x_s^i - a ) \\right)^2 f(x_s^i ) ds   \\right ] - \\frac{1}{(n t)^2 } e_{\\pi } \\left [ \\sum_{i=1}^n \\int_0^t \\left ( q_h ( x_s^i - a ) \\right)^2 f(x_s^i ) ds   \\right ]",
    "\\\\   + \\frac{1}{(n t)^2 } e_{\\pi } \\left [ \\sum_{i=1}^n \\int_0^t \\left ( q_h ( x_s^i - a ) \\right)^2 f(x_s^i ) ds   \\right].\\end{gathered}\\ ] ] the last term is controlled by .",
    "we will deal with the difference in the second line using theorem [ theo : harrisok ] as follows : for all @xmath5730,1-\\alpha[,$ ] we have @xmath574 - \\frac{1}{(n t)^2 } e_{\\pi } \\left [ \\sum_{i=1}^n \\int_0^t \\left ( q_h ( x_s^i - a ) \\right)^2 f(x_s^i ) ds   \\right ] \\\\",
    "=   \\frac{1}{(n t)^2 } e_x \\left [ \\sum_{i=1}^n \\int_0^{t^p } \\left ( q_h ( x_s^i - a ) \\right)^2 f(x_s^i ) ds   \\right ] - \\frac{1}{(n t)^2 } e_{\\pi } \\left [ \\sum_{i=1}^n \\int_0^{t^p } \\left ( q_h ( x_s^i - a ) \\right)^2 f(x_s^i ) ds   \\right ]",
    "\\\\ + \\frac{1}{(n t)^2 } \\sum_{i=1}^n \\int_{t^p}^t \\big ( e_x \\left [ \\left ( q_h ( x_s^i - a ) \\right)^2 f(x_s^i ) \\right ] -   e_{\\pi } \\left [ \\left ( q_h ( x_s^i - a ) \\right)^2 f(x_s^i ) \\right ] \\big ) ds.\\end{gathered}\\ ] ] to conclude , we use the upper bounds @xmath575 and @xmath576 for @xmath112 and @xmath28 to control the second line and we use theorem [ theo : harrisok ] to control the last term . as a consequence , @xmath577 - \\frac{1}{(n t)^2 } e_{\\pi } \\left [ \\sum_{i=1}^n \\int_0^t \\left ( q_h ( x_s^i - a ) \\right)^2 f(x_s^i ) ds   \\right ] \\right| \\\\ \\leq \\frac{f \\parallel q \\parallel_\\infty^2}{nh^2 t^2 } \\left ( 2 t^p + c \\int_{t^p}^t \\kappa^{-s } ds \\right ) = \\frac{f \\parallel q \\parallel_\\infty^2}{nh^2 t^2 }",
    "\\left ( 2 t^p + c \\frac{\\kappa^{-t^p}-\\kappa^{-t}}{\\ln ( \\kappa ) } \\right ) = \\frac{1}{ht } { \\mathcal o } \\left ( \\frac{t^p}{ht } \\right ) .\\end{gathered}\\ ] ] now recall that @xmath578 by ( [ eq : htmalpha ] ) and that @xmath5730,1-\\alpha[.$ ] thus @xmath579 - \\frac{1}{(n t)^2 } e_{\\pi } \\left [ \\sum_{i=1}^n \\int_0^t \\left ( q_h ( x_s^i - a ) \\right)^2 f(x_s^i ) ds   \\right ] = o \\left ( \\frac{1}{ht } \\right),\\ ] ] which allows to conclude .",
    "proposition [ prop : ernum ] will help us to control the numerator of our estimator .",
    "we want to establish the same kind of result for the denominator and this leads to the following proposition :    [ prop : erden ] for all @xmath538 define @xmath580 under the conditions of theorem [ theo : main ] , there exists a constant @xmath581 depending only on @xmath582 and @xmath185 such that for all @xmath558 for all @xmath200 $ ] and for a bandwidth of the form @xmath559 for some @xmath560 @xmath583 \\leq \\frac{c_2}{t } h^{2 \\left ( 1\\wedge \\beta \\right ) -1}.\\ ] ]    as in the preceding proof we start by working in the stationary regime , _",
    "i.e. _  under @xmath584 @xmath585 \\\\ \\leq \\frac{2}{(nt)^2 } e_{\\pi } \\left [ \\int_0^t   \\int_\\r \\big| \\tilde q_{h , f}(x ) \\big| \\eta(ds ,",
    "dx ) \\big| e_{\\pi } \\left ( \\int_s^t   \\int_\\r \\tilde q_{h , f}(y ) \\eta(du , dy ) \\big| { \\mathcal f}_s \\right ) \\big| \\right].\\end{gathered}\\ ] ] we deal with the conditional expectation using the markov property and write @xmath586 now going back to the definition of @xmath587 we can use theorem [ theo : harrisok ] and write @xmath588 due to the assumption ( [ eq : hspace ] ) on the hlder space containing @xmath39 ( recall that @xmath184 is the diameter of the support of @xmath547 ) the integrability of the function @xmath589allows to deduce from this that @xmath590 for some constant @xmath591 taking this result into account in ( [ eq : errorstartden ] ) , we obtain @xmath592 \\leq \\frac{2{\\tilde c}}{nht^2 }   ( f\\vee l)(rh)^{1\\wedge \\beta } \\parallel q \\parallel_\\infty e_{\\pi } \\left[\\int_0^t \\int_\\r \\big|",
    "\\tilde q_{h , f}(x ) \\big| \\eta(ds , dx ) \\right].\\ ] ] the end of the proof is similar to the one of proposition [ prop : ernum ] : the fact that we are in the invariant regime allows to use the density of the invariant measure of a single particle and its control given by theorem [ theo : invmeasure ] .",
    "then we use the same change of variable @xmath567 to obtain @xmath592 \\leq \\frac{4 { \\tilde c } } { ht } ( f\\vee l)^2(rh)^{2 \\left ( 1\\wedge \\beta \\right ) } \\parallel q \\parallel_\\infty   \\parallel q \\parallel_{l^1 } \\sup _ { x \\in s_{d/2 , k } } \\pi_1 ( x).\\ ] ] this result is established under the invariant regime , but we are able to extend it to any starting point @xmath200^n,$ ] using the same trick as the one in the proof of proposition [ prop : ernum ] .",
    "this finishes the proof .      introducing @xmath594 we have @xmath595}\\int_\\r q_h ( y - a ) \\mu ( ds , dy ) -f(a)d^{t , h } \\\\   = \\frac{1}{nt } \\int_{[0 , t ] } \\int_\\r q_h ( y - a ) \\tilde \\mu ( ds , dy ) + \\frac{1}{nt } \\int_0^t \\int_\\r \\frac1h q\\left",
    "( \\frac { y - a}{h } \\right ) \\left ( f(y ) - f(a ) \\right ) \\eta ( ds , dy ) .\\end{gathered}\\ ] ] with the definition of @xmath596 in ( [ eq : defqtilde ] ) , we have the following decomposition : @xmath597}\\int_\\r q_h ( y - a ) \\tilde \\mu(ds , dy ) + \\frac{1}{nt } \\int_0^t \\int_\\r \\tilde q_{h , f}(y )   \\eta ( ds , dy ) + \\pi_1 \\big ( q_h ( \\cdot - a ) \\left ( f(\\cdot )",
    "-   f(a ) \\right ) \\big )    .\\end{gathered}\\ ] ] the first two terms of the previous sum are controlled respectively by propositions [ prop : ernum ] and [ prop : erden ] .",
    "we deal with the third term using proposition [ prop : kernapprox ] as follows :    @xmath598    both functions @xmath125 and @xmath599 are hlder of order @xmath151 ( recall theorem [ theo : invmeasure ] ) and we can apply proposition [ prop : kernapprox ] to each of the last two terms , using the upper bound @xmath576 for @xmath600    putting all together in ( [ eq : erdecomp ] ) , we have @xmath601 with constants @xmath602 and @xmath603 depending only on @xmath604 and @xmath547 as in the proof of proposition [ prop : erden ] , we will fix the dependence in @xmath114 of @xmath37 putting @xmath605 and choosing @xmath5700,1[$ ] to obtain an optimal speed of convergence .",
    "this leads to the choice @xmath606 and @xmath607 which gives us @xmath608 to finish the proof of theorem [ theo : main ] , we have to work conditionally on the event @xmath609 , for @xmath610 on which we have @xmath611 @xmath612 = \\frac{1}{p_x \\left ( a_{t , r } \\right ) }   e_x \\left [ \\left ( \\hat f_{t , h_t } ( a ) - f ( a ) \\right)^2 1_{a_{t , r } } \\right ] \\\\   \\leq \\frac{1}{r^2 p_x \\left ( a_{t , r } \\right ) } \\parallel d^{t , h_t } \\left ( \\hat f_{t , h_t } ( a ) - f ( a ) \\right ) \\parallel_{l^2(p_x^f)}^2 \\leq \\frac{c(\\beta , f , l , f_{min } , q)^2 t^{-\\frac{2 \\beta}{2 \\beta + 1}}}{r^2 p_x \\left ( a_{t , r } \\right ) } , \\end{gathered}\\ ] ] and the conclusion follows thanks to ( [ eq : imdlb ] ) . @xmath613",
    "the proof relies on the martingale convergence theorem given in corollary 3.24 of @xcite chapter viii .",
    "we use the following decomposition @xmath615 where @xmath616}\\int_\\r q \\left(\\frac{y - a}{h } \\right ) \\tilde \\mu(ds , dy).\\ ] ] we define for all @xmath617 @xmath618 } \\int_\\r q \\left(\\frac{y - a}{h } \\right ) \\tilde \\mu(du , dy)\\ ] ] and show that the assumption 3.23 of @xcite chapter viii is satisfied for this sequence of processes .",
    "therefore , we have to study , for all @xmath619 and all @xmath620 the limit of @xmath621    as @xmath114 goes to @xmath622 since @xmath112 is bounded and @xmath623 there exists @xmath624 such that for all @xmath625 @xmath626 consequently , the above limit is @xmath14 and assumption 3.23 of @xcite chapter viii is indeed satisfied .",
    "moreover , @xmath627 since our process is positive harris recurrent , by the ergodic theorem , we have the following proposition .",
    "@xmath628 converges in @xmath89-probability as @xmath114 goes to @xmath629 to @xmath630    since our process is positive harris recurrent , @xmath28 being continuous and @xmath112 with compact support , we have @xmath631 = 0.\\ ] ] then the result is obtained by continuity of @xmath125 and @xmath28 on @xmath632    consequently , corollary 3.24 of @xcite chapter viii with @xmath633 gives us the weak convergence of @xmath634 to @xmath635    we deal with the second term of ( [ eq : decomptcl ] ) as in the previous subsection and obtain @xmath636 therefore , when @xmath114 goes to @xmath637 ( [ eq : decomptcl ] ) gives us the following weak convergence : @xmath638 since @xmath639    finally , we deal with the additive functional @xmath640 using the ergodic theorem .",
    "recall that @xmath641 thanks to , @xmath642 and the ergodic theorem gives us the almost sure convergence to @xmath643 ( since @xmath644 ) , which allows us to conclude .",
    "the proof of theorem [ theo : lowerbound ] follows closely the proof of theorem 8 of hoffmann and olivier ( 2015 ) @xcite , going back to similar ideas developed in @xcite .",
    "let @xmath645 and fix any test rate function @xmath646 for some fixed @xmath647 0 , f \\wedge l [ .$ ] then , as in @xcite , we define a perturbation @xmath648 of @xmath649 by @xmath650 where @xmath651 is a positive constant , @xmath652 is a positive kernel function of compact support included in @xmath653 $ ] such that @xmath654 @xmath655 for all @xmath19 and @xmath656    notice that the first @xmath657 derivatives of @xmath658 are of order @xmath659 therefore the factor @xmath660 implies that @xmath661 if we choose @xmath662 sufficiently small .",
    "an important point in the above choice of @xmath663 is that @xmath664 since @xmath665    in the following , we shall write shortly @xmath666 and @xmath667 for the associated probability measures in restriction to @xmath668 the following lower bound is by now classical . for any fixed constant @xmath669 using markov s inequality and denoting by @xmath670 the likelihood ratio of @xmath671 with respect to @xmath672 on @xmath673 @xmath674   \\\\ & & \\geq t^ { \\frac{2\\beta}{1 + 2 \\beta } }   \\left [ \\frac12 \\e_0 [ | \\hat f_t ( a ) - f_0 ( a ) |^2 ]   + \\frac12 \\e_t [ | \\hat f_t ( a ) - f_t ( a ) |^2 ] \\right ] \\\\ & & \\geq \\frac{c^2 } { 2 } \\left [ \\p_0 \\left ( t^ { \\frac{\\beta}{1 + 2 \\beta } } | \\hat f_t ( a ) - f_0 ( a ) | \\geq c \\right ) +   \\p_t \\left ( t^ { \\frac{\\beta}{1 + 2 \\beta } } | \\hat f_t ( a ) - f_t ( a ) | \\geq c \\right ) \\right ] \\\\ & & = \\frac{c^2 } { 2 } \\left [ \\p_0 \\left ( t^ { \\frac{\\beta}{1 + 2 \\beta } } | \\hat f_t ( a ) - f_0 ( a ) | \\geq c \\right ) +   \\e_0 \\left ( l_t^{f_t / f_0 } 1_{\\ { t^ { \\frac{\\beta}{1 + 2 \\beta } } | \\hat f_t ( a ) - f_t ( a ) | \\geq c   \\}}\\right ) \\right ] .   \\ ] ] now , @xmath675",
    "\\geq   t^ { \\frac{\\beta}{1 + 2 \\beta } } | f_0(a )   - f_t ( a ) | \\geq b , \\ ] ] which is due to . as a consequence ,",
    "if we choose @xmath676 then @xmath677 in particular , @xmath678 we conclude that @xmath674\\\\ & & \\geq \\frac { b^2}{8 }   \\e_0 \\left [ 1_{\\ { t^ { \\frac{\\beta}{1 + 2 \\beta } } | \\hat f_t ( a ) - f_0 ( a ) | \\geq   \\frac{b}{2 }   \\ } }   + l_t^ { f_t / f_0 } 1_{\\ { t^ { \\frac{\\beta}{1 + 2 \\beta } } | \\hat f_t ( a ) - f_0 ( a ) | <   \\frac{b}{2 }   \\ } } \\right ]   \\\\ & & \\geq   \\frac { b^2}{8 }   e^{- s } \\p_0 ( l_t^{f_t/ f_0 } \\geq e^{-s } ) , \\end{aligned}\\ ] ] for any @xmath679 therefore , in order to achieve the proof of theorem [ theo : lowerbound ] , it suffices to show that @xmath680 < \\infty .\\ ] ]    indeed , we can deduce from the following statements : @xmath681    recall that by construction , @xmath682 moreover , since the support of @xmath683 is included in @xmath684,$ ] @xmath685 implies @xmath686.$ ] now , theorem 3.5 of lcherbach ( 2002 ) @xcite , applied to the particular case without branching , shows that @xmath687 and @xmath688 are equivalent on @xmath689 with density @xmath690 we now proceed exactly as in @xcite , proof of lemma 11 .",
    "the @xmath691 martingale part within is given by @xmath692 where @xmath693 is the @xmath694compensator of @xmath695 its angle bracket is @xmath696 since @xmath697 by definition of @xmath698 ( recall ) .",
    "all other terms in are treated exactly as in @xcite .",
    "therefore , it only remains to show that @xmath699 we apply once more theorem [ theo : harrisok ] and rewrite @xmath700 where @xmath701 for @xmath702 and @xmath703 denotes the lebesgue density of @xmath704 which exists on @xmath705 by choice of @xmath706 for @xmath114 sufficiently large . using the change of variables",
    "@xmath707 we obtain @xmath708 which implies finally by theorem [ theo : invmeasure ] .",
    "we thank an anonymous referee for helpful comments and suggestions . this research has been conducted as part of the project labex mme - dii ( anr11-lbx-0023 - 01 ) , as part of the agence nationale de la recherche piece 12-js01 - 0006 - 01 and as part of the activities of fapesp research , dissemination and innovation center for neuromathematics ( grant 2013/07699 - 0 , s.  paulo research foundation ) ."
  ],
  "abstract_text": [
    "<S> we consider a model of interacting neurons where the membrane potentials of the neurons are described by a multidimensional piecewise deterministic markov process ( pdmp ) with values in @xmath0 where @xmath1 is the number of neurons in the network . </S>",
    "<S> a deterministic drift attracts each neuron s membrane potential to an equilibrium potential @xmath2 when a neuron jumps , its membrane potential is reset to a resting potential , here @xmath3 while the other neurons receive an additional amount of potential @xmath4 we are interested in the estimation of the jump ( or spiking ) rate of a single neuron based on an observation of the membrane potentials of the @xmath5 neurons up to time @xmath6 we study a nadaraya - watson type kernel estimator for the jump rate and establish its rate of convergence in @xmath7 this rate of convergence is shown to be optimal for a given hlder class of jump rate functions . </S>",
    "<S> we also obtain a central limit theorem for the error of estimation . </S>",
    "<S> the main probabilistic tools are the uniform ergodicity of the process and a fine study of the invariant measure of a single neuron . </S>"
  ]
}