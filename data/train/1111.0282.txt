{
  "article_text": [
    "the fundamental statistical question arising from the opera timing experiment has the following form : the observations ( neutrino arrival times ) can be seen as independent samples @xmath0 from a family @xmath1 of probability distributions shifted by a common unknown offset , and the task is to estimate this offset .",
    "unfortunately , the @xmath1 are not known exactly ; rather some proxy measurement @xmath2 is available , and the statistics must take this into account .",
    "there may also be small errors in the measurement of the @xmath0 that are not independent .    in the specific case of the opera experiment ,",
    "@xmath2 , the @xmath3 proton waveform , is the output of the waveform digitizer connected to the bct ( beam current transformer ) for the @xmath3 proton pulse ( or `` spill '' ) .",
    "following @xcite , we write @xmath4 for the ( unknown ) difference @xmath5 in  @xcite . ] between the actual time - of - flight and that expected ( distance divided by @xmath6 ) .",
    "let @xmath7 denote the expected time delay between the proton waveforms and neutrino detection events in the case that neutrinos move at the speed of light , where @xmath8 and @xmath9 incorporate all known neutrino and proton measurement delays respectively .",
    "then we may define @xmath1 by saying that detection events for the interval @xmath10 correspond to a poisson process with rate @xmath11 .",
    "the notation is set so that @xmath2 and @xmath1 have the same time origin ( and would be proportional under idealized conditions ) , but @xmath1 is defined in terms of detection events .",
    "this gives us the notational freedom to hypothesize any relationship between @xmath2 and @xmath1 . in this paper",
    "we shall indicate how one may cope with certain unknown differences between them .",
    "if the chance of a neutrino giving rise to a detection event is independent of its position in the @xmath12s pulse , then @xmath1 can be thought of as the neutrino waveform .",
    "this is a fairly mild assumption , but even so requires justification since , for example , it could be imagined that for some reason the neutrinos towards the end of the @xmath12s pulse tend to be of slightly higher or lower energy than those at the start , and that the detector has different sensitivities to neutrinos of different energies .    in the idealized case",
    "that the bct measures the proton current exactly , that each proton has the same chance of ultimately leading to a neutrino detection , and that the relevant clocks are perfectly synchronized , then @xmath1 and @xmath2 are equal . however , they are clearly not exactly equal for a number of reasons ; see , for example , the discussion of oscillations on page 125 of  @xcite . since only @xmath2 can be measured , but naive statistical analysis requires knowledge of @xmath1 , a problem called `` model uncertainty '' arises . to obtain reliable statistical results",
    ", it is necessary to quantify the extent of this uncertainty ( the difference between the @xmath2 and the @xmath1 ) , or at least to argue explicitly that it is negligible ; it is _ also _ necessary to quantify how sensitive the output of the statistical estimation procedure is to changes in the `` input '' @xmath1 .",
    "more explicitly , given @xmath1 , the task of estimating @xmath13 is a simple closed mathematical problem",
    ". we can think of the functions @xmath1 as leading to a one - parameter family ( parameterized by @xmath13 ) of models of when we expect neutrinos to be detected , and then standard techniques such as maximum likelihood estimation will work well .",
    "but in practice what we actually know is @xmath2 , so we do nt know which model @xmath1 to use .",
    "as far as possible we d like to model the model uncertainty statistically and just make a bigger model , but some types of uncertainty may be hard to model this way , for example because they might be non - independent for different @xmath14 .    in the arxiv preprint  @xcite posted by the opera collaboration it is implicitly assumed that any failure of @xmath1 to be proportional to @xmath2 is accounted for in the systematic errors .",
    "however , it is possible that discrepancies between @xmath2 and @xmath1 could reinforce each other in such a way as to create a much greater discrepancy in the final estimate of @xmath13 .",
    "similarly , the estimate of @xmath13 could be particularly sensitive to small discrepancies in particular parts of the waveform ; a qualitative discussion of a potential effect of this type is given in  @xcite .",
    "therefore we suggest that the statistical analysis should be conducted so as to be as robust as possible against such discrepancies of @xmath1 from @xmath2 , and we give examples of how this might be done for certain classes of discrepancies .    one strategy adopted in  @xcite for ( it appears )",
    "precisely the purpose of reducing the effect of noise in the measured current @xmath2 involves summing the @xmath2 to make an aggregate proton waveform @xmath15 .",
    "[ for simplicity of notation , and because it is not relevant to the discussion here , we omit the distinction between the first and second extractions ; when we speak of summing all proton waveforms , this can be understood to mean all waveforms within the first ( or second ) extraction .",
    "] we believe that this process is potentially flawed , for a reason given below , and if not flawed , then it appears not to be making best use of the information available .",
    "if the @xmath2 are treated separately for each @xmath14 , then it is likely that a more accurate result can be obtained , which would , for example , enable one to probe the energy dependence of @xmath13 with greater precision .",
    "whether this is in fact possible depends on details of the waveforms @xmath2 , which do not seem to be publicly available at the time of writing , as well as on properties of the discrepancies between the @xmath1 and the @xmath2 .",
    "in section 7 of @xcite and section 8.1 of  @xcite , it is stated that the aggregate distribution ( pdf ) @xmath15 is constructed by adding @xmath2 over @xmath14 corresponding to neutrino detections , then normalizing the result .",
    "this procedure is statistically flawed for reasons given below , and if it had been used could very easily explain the anomalous result @xmath16 .",
    "we raised this issue with one of the authors of  @xcite ( dario autiero ) , who informed us in a private communication  @xcite that the @xmath2 were in fact first normalized before being added together .",
    "this avoids the above - mentioned flaw , but we believe that it may still be useful to describe this effect , because the references @xcite are not clear ( or if anything , clear the wrong way ! ) on this important point .    treating ( for the moment )",
    "@xmath2 and @xmath1 as interchangeable , the problem with adding up the @xmath2 is that it may be that @xmath17 depends on @xmath14 , i.e. , that some pulses contain more protons that others .",
    "only those @xmath2 leading to a ( single , we assume ) neutrino detection event are selected .",
    "conditioned on @xmath2 giving rise to a single neutrino event , the time of this event ( or rather , of the proton causing the event ) is simply a random variable with density given by the normalized version @xmath18 of @xmath2 .",
    "hence the statistically proper thing to do would be to first normalize each @xmath2 for each @xmath14 corresponding to a neutrino event , and then add up the resulting @xmath18 , and then normalize again ( i.e. , divide by the total number of detections ) to get the aggregate distribution @xmath15 .",
    "summing the @xmath2 and then normalizing corresponds to taking a weighted average of the correct distributions @xmath18 , with each weighted by @xmath19 .",
    "this is known as ` size - biasing ' .",
    "if there is a correlation between @xmath19 and the shape of the waveform @xmath2 ( which seems very plausible given that the individual @xmath2 seem to differ significantly ) , then the size - biased average @xmath20 may differ from @xmath15 .",
    "it is quite possible that these averages may have a very similar shape , but with a small time offset .",
    "this could lead to an error in the estimate of @xmath13 that would not be picked up by the tests described in  @xcite , such as the monte carlo simulation , assuming these tests also used @xmath20 rather than @xmath15 or the individual @xmath2 .",
    "adding up the @xmath2 before normalizing would make sense if _ all _ @xmath2 were included , not just those corresponding to neutrino detection events ( or , at least , the subset of the @xmath2 to be added together is chosen without regard to whether or not a neutrino was detected ) .",
    "this would construct a different unbiased estimator of the aggregate distribution .",
    "it may be counterintuitive that simply ignoring waveforms that did not lead to a detection can introduce an error .",
    "however , taking this point of view , selecting events leading to a detection introduces size - biasing , which must then be reversed by normalizing . we believe that summing all @xmath2 is unlikely to be the best way to proceed statistically ,",
    "since it throws away information about which neutrino events occurred , so from now on we assume that @xmath14 indexes only those pulses for which a neutrino was detected .",
    "from now on we assume that all individual waveforms @xmath2 and @xmath1 are normalized . we ignore the known time offset @xmath21 , assuming that the data @xmath0 now represent detection times from which @xmath21 has been subtracted .",
    "later , we shall adopt the following model : independent observations @xmath22 are drawn from @xmath23 .",
    "the measured arrival times @xmath24 are then given by @xmath25 , where @xmath13 is the parameter we wish to measure and the @xmath26 represent small noise in the form of `` jitter '' .",
    "the actual observed values are @xmath27 , as in  ( * ? ? ?",
    "* section 7 ) , where @xmath28 , but we use @xmath29 when we want to think of these observations as random variables .    for the moment , assume that @xmath30 and @xmath31 .",
    "it is then a purely mathematical problem to estimate @xmath13 , since we have ( temporarily ) removed all sources of errors in this description .",
    "a good method to use here is maximum likelihood estimation : if @xmath32 is a candidate value for the unknown shift , then writing @xmath33 for the collection @xmath34 , the log likelihood is @xmath35 and the maximum likelihood estimator ( mle ) of @xmath13 is @xmath36 , the value of @xmath32 that maximizes @xmath37 .",
    "intuitively , most of the statistical benefit of a good estimator for @xmath13 comes from the parts of the input waveforms , @xmath2 , that are varying the fastest .",
    "we can quantify this by taking expectations over @xmath0 to look at typical values of @xmath37 : the expectation is @xmath38    differentiating with respect to @xmath32 and substituting @xmath39 for @xmath40 in the integral , we see that @xmath41 which means that the mle will be asymptotically unbiased . @xmath42",
    "( the fisher information ) is a measure of how responsive the likelihood is to changes in @xmath32 , and so quantifies the information available .",
    "it can be identified with the expected second derivative of the graphs in ( * ? ? ?",
    "8) at the true value @xmath13 ( which may be slightly different from the second derivative at the maximum of these graphs ) .",
    "the mle asymptotically achieves this fisher information , which has an expression independent of the unknown @xmath13 : @xmath43 ( more precisely , assuming the integral converges , then if the distribution or family of distributions is fixed and the number of samples tends to infinity , then the standard deviation of @xmath44 will scale with @xmath45 . )    as expected , the more wiggly the waveforms are , the better , especially if the wiggle is where @xmath2 is small . in practice",
    ", there are limits to how much use can be made of the regions where @xmath2 is small , due to small uncertainties in the value of @xmath2 , and to the limited number of samples available ; the latter effect means that the asymptotic regime is not necessarily attained .",
    "in contrast , treating the samples as independent samples from a single aggregate distribution @xmath46 , our estimator is the maximum of the log likelihood in   with @xmath47 in place of @xmath48 .",
    "the information available by this method is @xmath49 , and it is easy to check that @xmath50 , with equality if and only if all the @xmath2 are equal .",
    "thus averaging the @xmath2 will result in a less sensitive ( higher variance ) estimator .",
    "the interpretation of this is that , as might be expected , the procedure described in  @xcite of averaging together all the proton distributions will wash away information that is present in the individual distributions .",
    "the amount of information lost will depend on how different the individual distributions are , but comparing the graphs in figures 4 and 9 of  @xcite it seems that these individual distributions are indeed very different , and much more wiggly than the aggregate distribution .",
    "this suggests that an estimator of @xmath13 based on individual distributions could be considerably more accurate than the existing estimator .",
    "( even if it is not possible to effectively take advantage of the fine structure of the @xmath51 , it should be possible to benefit from the 5-peak structure much more strongly present in figure 4 than in figure 9 . )",
    "the above argument assumed that @xmath52 and @xmath30 ( i.e. , @xmath53 ) : to be correct it should have been written in terms of @xmath54 and @xmath1 , not @xmath0 and @xmath2 . since @xmath26 and @xmath1",
    "are unknown , we may be tempted to forget about these differences , but unfortunately the estimator @xmath36 so obtained might be quite susceptible to small discrepancies between @xmath2 and @xmath1 , or to jitter @xmath26 .",
    "for example , for a particular @xmath14 , we might have @xmath55 for @xmath39 near to @xmath56 , which would cause it to be impossible to estimate @xmath13 near its true value , regardless of any amount of other data .    according to a remark in a private communication  @xcite ,",
    "this kind of effect ( specifically `` small white noise ( electronics ) on the baseline '' ) in each @xmath2 is the reason why individual @xmath2 were not used in  @xcite .",
    "however , we believe that it should be possible to construct estimators that are robust against such effects , and therefore enable us to tap into the large amount of extra information available from the individual @xmath2 .",
    "this is the subject of the next section .",
    "note that averaging the @xmath2 does not completely eliminate noise ; it merely reduces its variance . when using statistics to extract a very delicate result ( order of @xmath57ns accuracy ) from a wide spread ( approx @xmath58s ) , the small residual discrepancy between @xmath59 and @xmath60 may yet be important . in other words ,",
    "whatever method is chosen ( averaging @xmath2 or not ) , it is still necessary to have a robust statistical appraisal of the effects of possible discrepancies between @xmath2 and @xmath1 .",
    "it seems then that a proper statement of a statistical analysis of this type should always be preceded by a caveat of the form `` assuming that @xmath2 and @xmath1 can differ in the following sorts of ways , then ... '' .",
    "this is the limit of what can be said without further experimental data , since if we have no bound at all on how different @xmath2 and @xmath1 can be , then obviously no useful information can be extracted from any statistics .",
    "in accordance with the above prescription , we first decide on the class of discrepancies between the actual distribution of @xmath61 and its idealized version that we shall work with .",
    "then we choose an estimator , and finally we measure the bias and efficiency of the estimator by finding lower bounds on the probabilities of confidence intervals centred on the estimator .",
    "we shall assume that @xmath62 where @xmath63 corresponding to a total variation distance of at most @xmath64 .",
    "we write @xmath65 whenever this holds , and @xmath66 to indicate that it holds for every @xmath14 .",
    "we assume that @xmath67 consists of independent samples from the probability densities @xmath68 , and that @xmath69 where @xmath70 for each @xmath14 .",
    "we think of @xmath71 as representing `` noise '' in the measurement @xmath51 of @xmath72 , and the `` jitter '' @xmath26 as including errors in the measurement of the arrival times , and so on . there is some redundancy here , since certain types of difference between the distributions @xmath51 and @xmath72 can also be seen as jitter .",
    "note that these discrepancies are worse than statistical ( independent random ) or systematic ( random , but constant over @xmath14 ) error , since they are assumed to be chosen non - randomly  `` maliciously ''  in such a way as to change the estimator from the true value by as much as possible .",
    "the motivation for this is that we may not be sure which errors are independent , and we wish to avoid having to describe all possible ways in which they might be correlated .",
    "for example , we should like our estimator to be robust against small timing errors that are somehow correlated with the time offset of the proton within the pulse , or timing errors that result from very slight unknown changes in distance and hence @xmath73 over the months and years .",
    "if there were no jitter , then the 200mhz signal present in each @xmath2 ( @xcite ) would be extremely useful in pinning down a good estimate of @xmath13 .",
    "but to be on the safe side we presume that there is jitter of the order of at least 2.5ns , which means that the 200mhz information is illusory , and in fact we must be careful not to try to make use of it , since + /-",
    "2.5ns jitter could cause disproportionately large changes in the likelihood by means of aligning , or anti - aligning , the peaks of the 200mhz signal in @xmath2 with the data .",
    "this argument illustrates the kind of effect we need to be robust against .",
    "if we knew the distribution of @xmath33 , then we could define any estimator @xmath36 , and establish statistical bounds on @xmath74 ( i.e. , a confidence interval ) simply by repeatedly generating samples from the ( vector valued ) distribution @xmath33 and evaluating the difference @xmath74 .",
    "( since @xmath13 is unknown , in general one has to consider all possible values of @xmath13 , but as we shall see , this is not necessary here . ) of course , we do not know this distribution exactly .",
    "our assumptions imply that we can write @xmath25 , where the @xmath75 are independent with distributions @xmath1 satisfying @xmath76 .",
    "note that we make no independence assumptions concerning the @xmath26 or @xmath0 . if we can show that @xmath77 then the estimator @xmath36 can be said to be within @xmath78 of @xmath13 with a confidence of @xmath79 , whatever the unknown @xmath1 and @xmath26 are .",
    "for this to be useful we need two properties : ( i ) that we havent given away too much , i.e. , that values of @xmath78 and @xmath79 satisfying ( [ conf ] ) give reasonably small error bounds , and ( ii ) there is an effective way to show that ( [ conf ] ) is satisfied for given @xmath78 and @xmath79 , since , for example , simply exhausting over the space @xmath80^n$ ] of all possible jitters is computationally impractical .",
    "fortunately , it is possible to satisfy both requirements simultaneously .",
    "the estimators we shall consider here can be thought of as modified maximum likelihood estimators : we choose a modified log likelihood , @xmath81 , then , analogously to ( [ ll ] ) , define @xmath36 to be the value of @xmath32 that maximizes the modified likelihood : @xmath82    although the method we describe is mathematically rigorous for any choice of @xmath81 , the results ( width of the final interval ) will vary depending on the choice of @xmath81 , which must be adapted to the real data @xmath2 and assumptions made in place of and .",
    "we shall consider @xmath81 defined by @xmath83 where @xmath84 is a gaussian with width @xmath85 , and @xmath86 and @xmath85 are suitably chosen .",
    "( convolving @xmath2 with @xmath87 before taking the logarithm also appears to work reasonably well , though convolving after taking the logarithm , as here , is slightly easier to reason about . )    before turning to the verification that such an @xmath81 gives good results ( narrow confidence intervals ) , we briefly motivate this choice for @xmath81 .    setting a minimum probability level , @xmath86 , will greatly reduce sensitivity to noise where @xmath2 is small , and hence @xmath88 is large in magnitude .",
    "this change will not affect the asymptotic bias in the idealized case ( when @xmath89 and @xmath30 ) .",
    "indeed , in place of ( [ bias ] ) we have the modified bias formula @xmath90 when @xmath91 , it is easy to check that @xmath92 , so putting a floor on the value of @xmath2 does not in itself significantly bias the estimator . it does increase its variance ; this is part of a trade - off between variance of the estimator as applicable in ideal conditions , and sensitivity to jitter and noise .",
    "the other modification in ( [ modll ] ) is convolution with the gaussian @xmath93 ; for the opera data this will remove the 200mhz signal if @xmath94ns , and can be expected to prevent jitter from disproportionately affecting the likelihood if @xmath95 .",
    "there is an effect on the bias , but it is only of order @xmath96 as @xmath97 since @xmath87 is symmetric under @xmath98 .",
    "the unknown parameter @xmath13 appears in , but can easily be eliminated .",
    "indeed , the estimator we use is translation invariant , in that @xmath99 ( the minus sign here comes from the following the sign convention for @xmath13 in  @xcite . )",
    "the difference @xmath100 that we are interested in can thus be written as @xmath101 , where @xmath102 .",
    "let @xmath103^n : |m({\\mathbf{n}},{\\mathbf{j}})|{\\leqslant}w\\bigr ) .",
    "\\end{split}\\ ] ] in the second last line , the infimum is over all allowed distributions of @xmath104 ; the final expression follows since we make no assumptions on @xmath105 other than that @xmath106 for each @xmath14 , so the worst value @xmath107 of @xmath105 can be chosen after seeing @xmath108 . as discussed above ,",
    "@xmath109 is a confidence interval for @xmath13 with confidence level @xmath110 .",
    "unfortunately , the space of possible values of the @xmath72 ( depending on the noise ) and the space @xmath80^n$ ] of jitter values are high - dimensional .",
    "because the position of the maximum of @xmath111 can theoretically change a lot given a small change in one of the summands @xmath112 , optimizing over these spaces is likely to be computationally intractable . to cope with this ,",
    "we replace @xmath110 with a possibly slightly weaker ( though in practice very similar ) estimate based on derivatives .    to simplify the exposition slightly",
    ", we assume _ a priori _ that the maximum of @xmath111 will occur in a limited range @xmath113 $ ] .",
    "( in practice , the probability of this assumption failing can be estimated by techniques similar to those described below . )",
    "we use the observation that , in this case , if the derivative @xmath114 is positive on @xmath115 $ ] and negative on @xmath116 $ ] , then the maximum over @xmath113 $ ] lies in @xmath117 $ ] .",
    "the point is that we can express the derivative of @xmath118 as a sum of individual derivatives , and bound them separately .",
    "thus we set @xmath119\\ ; d_\\mathrm{min}(\\delta,{\\mathbf{n}})>0\\ ; \\mathrm{and}\\\\        & \\hskip1.85cm\\forall\\delta\\in[w , w_\\mathrm{max}]\\ ; d_\\mathrm{max}(\\delta,{\\mathbf{n}})<0\\bigr ) , \\end{split}\\ ] ] where the infimum is over all allowed distributions of @xmath108 , i.e. , over all possible values of the noise .",
    "this ensures that whatever the unknown noise and jitter , with probability at least @xmath120 the function @xmath121 will be increasing on @xmath115 $ ] and decreasing on @xmath116 $ ] , and so will have a maximum in @xmath117 $ ] that is global over @xmath113 $ ] .",
    "this implies that @xmath122 , assuming that @xmath123 does not have a rival maximum further away than @xmath124 .",
    "the use of @xmath124 is a technicality , and can be removed with a slightly refined version of this argument , but we retain it for simplicity .",
    "( alternatively , it happens that for our approximate reconstruction of the opera data , even choosing @xmath124 as large as @xmath125ns has only a negligible effect on our final confidence bound ( @xmath126 ; see below ) .",
    "but the interval @xmath127 $ ] is wide enough to encompass all plausible values of @xmath13 . )",
    "the advantage of calculating a version of @xmath128 in terms of the derivative of @xmath118 as in ( [ conf4 ] ) , is that the optimization over @xmath107 becomes tractable : it decomposes into separate optimizations over each @xmath129 .",
    "for example , @xmath130    there does not seem to be a simple way to handle the final infimum in , but we may replace it with a sum of more easily calculated terms while only modestly weakening the confidence bound . from now on we discretize time , by rounding all @xmath0 to multiples of some small constant , say @xmath131ns .",
    "the effect of any errors this introduces is bounded by increasing @xmath132 by @xmath133ns .",
    "all derivatives then become simply differences of consecutive values . for @xmath134",
    "$ ] let @xmath135 be the `` failure '' event that @xmath136 , and for @xmath137 $ ] let @xmath135 be the event that @xmath138",
    ". then @xmath120 as defined above is the infimum of the probability that no @xmath135 holds .",
    "hence @xmath139 the final supremum is easy to calculate .",
    "suppose , for example , that @xmath140 .",
    "then @xmath135 is the event that @xmath141 , where @xmath142 is a function of @xmath143 that is easy to calculate from the known data .",
    "the assumption @xmath76 implies that @xmath75 can be coupled with a variable @xmath144 having the known density @xmath2 so that @xmath145 . if we choose the distribution of @xmath75 by removing the `` most helpful '' @xmath64-fraction of the distribution of @xmath144 , i.e. , the part where @xmath146 is smallest , and replacing it by probability @xmath64 of having the least helpful value , then the distribution of this individual summand @xmath147 is `` worse than '' ( stochastically dominates ) any other possible distribution where @xmath76 .",
    "since @xmath148 is a sum of independent terms , the worst case overall is given by combining these individual worst case distributions .",
    "( note that the worst case will be different for different values of @xmath32 . )",
    "it remains to estimate the probability that this sum of @xmath149 independent variables has the right sign .",
    "this can be done efficiently by the method described in appendix 1 , and a short example program implementing this can be found associated with this preprint .",
    "this could also be done by monte carlo simulation , though obviously it would be quite slow to estimate very low tail probabilities this way .",
    "a slight modification will improve the confidence bound in the case that there is a small bias in the estimator . instead of summing over @xmath150\\cup[w , w_\\mathrm{max}]$ ]",
    ", we sum over @xmath151\\cup[w+b , w_\\mathrm{max}]$ ] , on the assumption that the modified estimator is @xmath152 where @xmath153 is determined once and for all by optimizing the resulting confidence estimate : @xmath154\\cup \\\\ [ w+b , w_\\mathrm{max } ] } } \\sup_{{\\mathbf{n}}\\approx{\\mathbf{p } } } { \\mathbb{p}}(f_\\delta)\\right).\\ ] ]",
    "in order to properly evaluate the performance of the @xmath36 as defined by ( [ est ] ) , we would need to know the 16111 proton waveforms , @xmath2 .",
    "these do not appear to be publicly available at the time of writing , so instead we use the @xmath2 given by ( * ? ? ?",
    "4 ) and , on the assumption that it is typical , set all @xmath51 equal to this one .",
    "this graph , whose axes are not displayed , was interpreted on the assumption that the vertical axis is a linear scale with an offset .",
    "the zero point was assumed to be at the mean of the left and right tail values , which are assumed to be noise .",
    "after that , a plausible 200mhz signal was added .",
    "in addition to the @xmath2 , we would need to know what assumptions can be made about the various sources of error .",
    "again , these are not described in  @xcite , so we use the error model described in section  [ ss_error ] with @xmath155(ns ) and @xmath156 , @xmath157 or @xmath158 . then the estimator parameters , @xmath85 and @xmath159 , were chosen to optimize the confidence for a 30ns error ( @xmath160 ) .    in practice ,",
    "the amount of smoothing chosen was always large enough ( @xmath161 ) to filter out almost all of any 200mhz signal .",
    "we might hope that the majority of the statistical noise would also be eliminated by such smoothing , leaving @xmath162 to account for any remaining `` malicious '' noise .",
    ".tail confidence values @xmath163 , showing the confidence of the estimator being within 30ns of the true value under various different robustness scenarios .",
    "@xmath164 is linked to @xmath159 by @xmath165 .",
    "the last column shows the equivalent number of standard deviations for a normal distribution : @xmath166 is equal to the probability that a standard normal is less than @xmath167 in magnitude .",
    "the last row shows the tail confidence figure using the methods of @xcite based on a 6.9ns statistical error . [ cols=\">,>,>,>,>,>\",options=\"header \" , ]     the results shown in table  [ t1 ] can be compared with the statistical component of the error as reported in @xcite .",
    "we note that @xmath168 ( stat . ) from @xcite means that the confidence that the estimator from @xcite is within 30ns of the true value would be @xmath169 .",
    "if we assume that @xmath170 , @xmath162 is sufficiently pessimistic , then the corresponding tail confidence using the methods of this paper would be @xmath171 . alternatively , using these methods , this same level of significance is reached for the same confidence bound of 30ns using only approximately @xmath172 ( in fact 74% ) of the current requirement of @xmath28 observations .",
    "if @xmath173 is realistic , then 9800 observations ( 61% ) would be sufficient for the same confidence level .",
    "these comparisons obviously rely on the assumption that we are using a typical @xmath51 , and the assumption that our jitter and noise values are appropriate , though we would expect these methods to be useful to at least some extent in any case .",
    "note that we are comparing a method with provable robustness against ( the given type of ) jitter and noise with one where the effects of these are not quantified .",
    "if the method of @xcite were adjusted to allow for these effects , the difference might be greater , although it is plausible that there is little noise effect using the method of @xcite , since that is the benefit of adding up different @xmath51 .",
    "we have described what may be a more effective and more robust way of analysing the opera neutrino time - of - flight data , and in general data from similar experimental set - ups . to do this",
    "we have presented some statistical techniques together with an analysis of their effectiveness .",
    "it is only possible to properly gauge the effectiveness of these techniques when applied to the opera data with access to the full experimental data , which does not seem to be publicly available at the present time .",
    "it is possible that there are types of differences between the proton waveforms , @xmath2 , and the distribution of neutrino detection events other than the noise and jitter we have considered here . for example , in section 8.1 of  @xcite it is mentioned that there are spurious oscillations with periods @xmath174ns and @xmath175ns in some of the @xmath2 .",
    "these are not mentioned in the more recent opera paper  @xcite , but if they are still present amongst the 16111 opera samples , then a new evaluation of estimator effectiveness ought to be carried out , and possibly a new kind of estimator would be required .",
    "it may be possible to use techniques similar to those of this paper to do this , though it is not possible to say for sure without seeing a more detailed exposition of the nature of these oscillations . in  @xcite it",
    "is explained that the spurious oscillations are filtered out with an 8mhz low - pass filter ; the necessary evaluation of the effect that this may have on the estimator is not described there .",
    "it may be thought that the monte carlo techniques of  @xcite and  @xcite would be sufficient to catch a poor or biased estimator . however , these monte carlo simulations appear to be based on samples from the aggregate ( summed ) waveform distribution .",
    "this is not correct in principle and will fail to catch some classes of problems .",
    "in addition , such monte carlo methods lack the ability to simulate worst - case situations , as we have sought to do in this paper in the case of noise and jitter .",
    "we outline here the ( fairly standard ) method we used to bound @xmath176 , where @xmath177 is a sum of independent variables @xmath178 with @xmath179 .",
    "the method is based on cramr s exponential tilting trick plus the berry  esseen theorem . for notational convenience",
    "we take the @xmath180 to have the same distribution @xmath181 ; the method also works for different distributions by using a single tilting parameter @xmath182 chosen to achieve @xmath183 below .",
    "let @xmath184 denote the probability density function of @xmath181 . by assumption @xmath185 .",
    "it follows ( unless @xmath181 is never positive ) that there is some @xmath186 such that @xmath187 , i.e. , such that @xmath188 .",
    "let @xmath189 denote the distribution with density function @xmath190 where @xmath191 is a normalizing constant .",
    "then @xmath192=0 $ ] .",
    "let @xmath177 denote the sum of @xmath149 independent variables with distribution @xmath181 , and @xmath193 the sum of @xmath149 independent variables with distribution @xmath189 .",
    "a standard calculation shows that their densities are related by @xmath194 it follows that @xmath195    let @xmath196 then ( integrating by parts ) we can rewrite the formula above as @xmath197 of course , we do not know @xmath198 in a practical way , but by the berry  esseen theorem ( with a modern form of the constant from @xcite ) , we have @xmath199 where @xmath200 is the probability that a normal distribution with the same mean ( 0 here ) and variance as @xmath193 exceeds @xmath143 .",
    "the worst case is given by taking @xmath201 and @xmath202 for @xmath203 such that @xmath204 , and @xmath205 for larger @xmath143 .",
    "writing @xmath206 , where @xmath207 is the variance of @xmath193 , and substituting @xmath208 in the integral , this gives @xmath209 where @xmath210 denotes the distribution function of a standard gaussian random variable .",
    "r x[glossary ] @xmath2&the @xmath211 distribution output by the waveform digitizer , with unspecified time origin , normalized from section  [ sec3 ] onwards . + @xmath1&the normalized @xmath211 detection - event distribution , where the origin of time is that of @xmath2 , offset by the time - of - flight of neutrinos from cern to opera , together with all known measurement delays . ( see introduction for details . ) + @xmath13&the ( unknown ) early arrival time of the neutrinos at opera , i.e. , @xmath5 .",
    "+ @xmath36&estimator for @xmath13 .",
    "+ @xmath149&the number of neutrino detection events . taken to be 16111 .",
    "+ @xmath212&the measured observation times of neutrino detection events , in the same timeframe as that of @xmath1 .",
    "+ @xmath0&arrival times in the statistical model : the @xmath212 are the actual measured values of the @xmath0 . + @xmath33&the collection @xmath213 .",
    "+ @xmath108&a sequence of independent random variables @xmath214 with densities @xmath72",
    ". + @xmath215&a function , derived from @xmath51 , that will play the role of @xmath216 in a modified - log - likelihood estimator .",
    "+ @xmath32&in section  [ estchoice ] , @xmath32 is a candidate value for the unknown @xmath13 . in section  [ anasig ] , @xmath13 has been subtracted out and @xmath217 is the true value .",
    "+ @xmath26&small ( actual , unknown ) `` jitter '' defined by @xmath25 .",
    "+ @xmath105&the collection @xmath218 .",
    "+ @xmath129&candidate @xmath26 when optimizing over all possible jitter .",
    "+ @xmath107&the collection @xmath219 .",
    "+ @xmath64&maximum noise considered ( section  [ ss_error ] ) .",
    "+ @xmath132&maximum jitter considered : @xmath106 ( section  [ ss_error ] ) .",
    "+ @xmath86&floor parameter used in defining modified mle ( section  [ estchoice ] ) .",
    "+ @xmath85&smoothing parameter used in defining modified mle ( section  [ estchoice ] ) .",
    "+ @xmath220&(@xmath221 ) successively weaker , but more tractable / better , versions of the confidence bound : @xmath222 .",
    "+ @xmath223&@xmath224 where @xmath225 .        the minos collaboration , measurement of neutrino velocity with the minos detectors and numi neutrino beam , _ phys .",
    "* 76 * ( 2007 ) , 072005 [ 6 pages ] .",
    "g. brunetti , neutrino velocity measurement with the opera experiment in the cngs beams , phd thesis , in joint supervision from universit claude bernard lyon- i and universit di bologna , 2011 ."
  ],
  "abstract_text": [
    "<S> time - of - flight measurements such as the opera and minos experiments rely crucially on statistical analysis ( as well as many other ingredients ) for their conclusions . </S>",
    "<S> the nature of these experiments leads to a simple class of statistical models for the results ; however , which model in the class is appropriate is not known exactly , as this depends on information obtained experimentally , which is subject to noise and other errors . to obtain robust conclusions , this problem , known as `` model uncertainty </S>",
    "<S> , '' needs to be addressed , with quantitative bounds on the effect such uncertainty may have on the final result .    </S>",
    "<S> the opera ( and minos ) analysis appears to take steps to mitigate the effects of model uncertainty , though without quantifying any remaining effect . </S>",
    "<S> we describe one of the strategies used ( averaging individual probability distributions ) , and point out a potential source of error if this is not carried out correctly . </S>",
    "<S> we then argue that the correct version of this strategy is not the most effective , and suggest possible alternatives . </S>",
    "<S> these alternatives may give more accurate statistical results using the same data , allowing , for example , more accurate determination of the dependence of the anomalous time shift on energy . which strategies work and how well can only be evaluated with access to the full data .    whether or not the anomalous result from opera turns out to be confirmed , we believe that techniques such as those presented here may be appropriate for the analysis of other timing experiments of this type . </S>"
  ]
}