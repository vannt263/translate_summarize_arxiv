{
  "article_text": [
    "in this paper , we consider structured convex optimization problem of the form @xmath3 where @xmath4 $ ] and @xmath5 $ ] are given extended real - valued closed proper convex functions ( not necessarily differentiable ) , and @xmath6 is a given real matrix of size @xmath7-by-@xmath8 . here",
    "the functions @xmath9 and @xmath10 are assumed to be extended real - valued and thus hard constraints on the variables can be hidden in the objective function .",
    "problems of this kind arise from numerous areas and applications , for example , signal and image reconstruction , compressive sensing and machine learning , to name a few , see , e.g. , @xcite and references therein . in such applications ,",
    "the objective function usually consists of a data fitting term and a regularization term , which commonly have different structures and analytic properties , such as separability and lipschitz continuity , and so on .",
    "very often , the functions @xmath9 and @xmath10 are relatively simple in the sense that their respective characteristics can be explored to design practical and efficient algorithms for solving ( p@xmath11 ) with fairly large scale data . in this paper , the structures that we assume for @xmath9 and @xmath10 are related to the proximity operator of closed proper convex functions , which is defined below .",
    "let @xmath12 $ ] be an extended real - valued closed proper convex function , and @xmath13 .",
    "the proximity operator of @xmath14 is defined as @xmath15    throughout this paper , we make the following assumption on @xmath9 and @xmath10 .    [ as : f - g - simple ] assume that the proximity operators of @xmath9 and @xmath10 can be evaluated efficiently .",
    "we now present other formulations of ( p@xmath11 ) . to sufficiently explore the problem structures , it is not uncommon to introduce an auxiliary variable and decouple the composition of @xmath10 and @xmath6 . as such , ( p@xmath11 )",
    "is usually reformulated as a linear equality constrained problem with separable objective function , that is @xmath16 this formulation allows one to take advantage of the structures of @xmath9 and @xmath10 ( and possibly @xmath6 ) individually and adequately . to present the dual problem of ( p@xmath11 ) or ( p@xmath17 ) ,",
    "we need the notion of conjugate of a closed proper convex function @xmath18 $ ] , which is defined as @xmath19 the well - known moreau s decomposition ( see , e.g. , @xcite ) links the proximity operator of a closed proper convex function @xmath14 and that of its conjugate @xmath20 as follows @xmath21 it follows from and assumption [ as : f - g - simple ] that the proximity operators of @xmath22 and @xmath23 are also easy to evaluate .",
    "the lagrangian dual problem of ( p@xmath11 ) or ( p@xmath17 ) can be represented as @xmath24 where @xmath25 is the dual variable .",
    "similarly , we can rewrite ( d@xmath11 ) , by introducing a new variable @xmath26 , as @xmath27 the primal - dual formulation of ( p@xmath11 ) or ( d@xmath11 ) is given by @xmath28    it is apparent that any algorithm that solves ( p@xmath11 ) ( resp . , ( d@xmath11 ) ) also solves ( p@xmath17 ) ( resp .",
    ", ( d@xmath17 ) ) , and vice versa . if an algorithm solves ( pd ) , then ( p@xmath11 ) and ( d@xmath11 ) are solved simultaneously .",
    "all the algorithms discussed in this paper solve ( pd ) , and thus ( p@xmath11 ) and ( d@xmath11 ) .",
    "we emphasize that , throughout this paper , @xmath29 and @xmath25 denote , respectively , the essential primal and dual variables , while @xmath30 and @xmath26 are auxiliary variables for the primal and the dual problems , respectively . in the following ,",
    "we first review briefly some popular methods that are closely related to this work and then summarize our motivation and contributions .",
    "the augmented lagrangian method ( abbreviated as alm , also known as the method of multipliers ) , pioneered by hestense @xcite and powell @xcite , is among the most influential approaches for solving constrained optimization problems , especially when the constraints contain linear equalities .",
    "we take the dual problem ( d@xmath17 ) as an example and explain the main idea of the alm . the augmented lagrangian function associated with ( d@xmath17 )",
    "is given by @xmath31 where @xmath32 is the lagrangian multiplier ( and also the primal variable in ( p@xmath11 ) or ( p@xmath17 ) ) , @xmath33 is a penalty parameter , and @xmath34 is defined as @xmath35 given @xmath36 , the alm iterates as    [ alm ] @xmath37    the most important feature of the alm is that it solves a constrained optimization problem via solving a sequence of unconstrained ones .",
    "note that in our setting the functions @xmath9 and @xmath10 have structures .",
    "it is thus rather unwise to ignore the separability of the objective function and apply a joint minimization with respect to @xmath38 as in , because in this case it can be very difficult to fully explore the structures of @xmath9 and @xmath10 due to the mixing of variables .",
    "in contrast , the alternating direction method of multipliers ( abbreviated as adm ) , pioneered by glowinski and marrocco @xcite and gabay and mercier @xcite , is a practical variant of the alm .",
    "it applies alternating minimization with respect to @xmath25 and @xmath26 in in a gauss - seidel fashion , with the other variable fixed . after each sweep of alternating minimization ,",
    "the multiplier @xmath29 is updated just as in the alm .",
    "specifically , given @xmath39 , the adm iterates as    [ adm - yvx ] @xmath40    compared to the alm , an obvious advantage of the adm is that it solves simpler subproblems in each round . since the minimizations for @xmath25 and @xmath26 are now separated , the structures of @xmath9 and @xmath10 can be utilized individually .",
    "interested readers are referred to the recent tutorial paper @xcite for more details on the alm and the adm , including convergence analysis with nonexpansive mappings .    to make adm efficient",
    ", it is very important to have low per - iteration cost and fast iterations .",
    "note that , by the definition of @xmath41 in , the @xmath26-subproblem is already a proximal minimization step and is thus simple enough under assumption [ as : f - g - simple ] . in comparison ,",
    "it is very likely that the @xmath25-subproblem is not easy in the sense that it calls for an iterative solver , even though the proximity operator of @xmath10 is easily obtainable .",
    "it is easy to observe that the components of @xmath25 in the quadratic term @xmath42 are mixed - all - together due to the presence of the linear operator @xmath6 . to avoid solving it iteratively",
    ", the @xmath25-subproblem needs to be treated wisely .",
    "the most widely used technique to modify so that the resulting subproblem can be solved easily by utilizing the structure of @xmath10 is to linearize with respect to @xmath25 the quadratic term @xmath43 at @xmath44 and meanwhile adding a proximal term @xmath45 for some @xmath46 .",
    "this proximal - linearization technique has been used extensively in the context of structured convex optimization , see , e.g. , @xcite . in this paper",
    ", we refer the algorithm resulting from adm and this proximal - linearization technique applied to one of the adm subproblems as linearized adm or ladm for short .",
    "another approach closely related to this work is the classical proximal point method ( abbreviated as ppm , @xcite ) .",
    "let @xmath47 be a set - valued maximal monotone operator from @xmath48 to its power set .",
    "the ppm is an approach to solving the maximal monotone operator inclusion problem , i.e. , find @xmath49 such that @xmath50 .",
    "the ppm has proven to be an extremely powerful algorithmic tool and contains many well known algorithms as special cases , including the aforementioned alm and adm , see @xcite .",
    "see also @xcite for inexact , relaxed and accelerated variants of the ppm .",
    "the primary ppm for minimizing a differentiable function @xmath51 can be interpreted as an implicit one - step discretization method for the ordinary differential equations ( abbreviated as odes ) @xmath52 , where @xmath53 is differentiable , @xmath54 denotes its derivative , and @xmath55 is the gradient of @xmath56 .",
    "suppose that @xmath56 is closed proper convex and its minimum value is attained , then every solution trajectory @xmath57 of this differential system converges to a minimizer of @xmath56 as @xmath58 .",
    "similar conclusion can be drawn for the maximal monotone operator inclusion problem by considering the evolution differential inclusion problem @xmath59 almost everywhere on @xmath60 , provided that the operator @xmath61 satisfies certain conditions @xcite .",
    "the ppm is a single - step method , meaning that each new iterate depends only on the current point . to accelerate speed of convergence ,",
    "multi - step methods have been proposed in the literature , which can usually be viewed as certain discretizations of second - order odes of the form @xmath62 where @xmath63 is a friction parameter .",
    "relevant studies in the context of optimization can be traced back to @xcite .",
    "it was shown in @xcite that if @xmath56 is convex and its minimum value is attained then each solution trajectory @xmath64 of converges to a minimizer of @xmath56 . in theory",
    "the convergence of the solution trajectories of to a stationary point of @xmath56 can be faster than those of the first - order odes , while in practice the second order term @xmath65 can be exploited to design faster algorithms @xcite . motivated by the properties of , an implicit discretization method was proposed in @xcite .",
    "specifically , given @xmath66 and @xmath67 , the next point @xmath68 is determined via @xmath69 which results to an iterative algorithm of the form @xmath70 where @xmath71 , @xmath72 and @xmath73 is the identity operator .",
    "note that is no more than a proximal point step taken at the extrapolated point @xmath74 , rather than @xmath67 itself as in the classical ppm .",
    "the iterative scheme is a two - step method and is usually referred as inertial ppm , because describes in the two dimensional case the motion of a heavy ball on the graph of @xmath56 under its own inertial , together with friction and gravity forces .",
    "convergence properties of were studied in @xcite under some assumptions on the parameters @xmath75 and @xmath76 .",
    "subsequently , this inertial technique was extended to solve the maximal monotone operator inclusion problem in @xcite .",
    "recently , there are increasing interests in studying inertial type algorithms , e.g. , inertial forward - backward splitting methods @xcite , inertial douglas - rachford operator splitting method @xcite and inertial adm @xcite . lately , we proposed in @xcite a general inertial ppm for mixed variational inequality , where the weighting matrix is allowed to be positive semidefinite .",
    "global point - convergence and certain convergence rate results are also given there .",
    "in this paper , we study inertial versions of chambolle - pock s primal - dual algorithm @xcite and variants . in the recent work @xcite , we proposed a general inertial ppm under the setting of mixed variational inequality ( abbreviated as mvi ) . there ,",
    "an inertial ladm was proposed , where the two adm subproblems must be linearized simultaneously in order to guarantee the positive definiteness of a weighting matrix when the resulting algorithm is viewed as a general ppm .",
    "note that the main aim of applying the proximal - linearization technique is to solve all subproblems efficiently via utilizing the proximity operators .",
    "it is apparent that the @xmath26-subproblem is already a proximal minimization step , which can make full use of the proximity operator of @xmath9 under assumption [ as : f - g - simple ] and the moreau s decomposition .",
    "thus , an approximation of by proximal - linearization is unnecessary in any sense .",
    "it is thus desirable to consider inertial ladm with only one of the adm subproblems linearized .    in this paper , we first further clarify , based on previous observations in @xcite , the connection between cpas and ladm , where only one of the adm subproblems is modified by proximal - linearization .",
    "in particular , we show that cpas generate exactly the same sequence of points with ladm applied to either the primal problem or its lagrangian dual , as long as the initial points for the ladm are properly chosen . by focusing on cyclically equivalent forms of the algorithms , we can relax the dependence on initial points for ladm .",
    "then , by utilizing the fact that cpas are applications of a general ppm , we are able to propose inertial cpas , whose global point - convergence , nonasymptotic @xmath0 and asymptotic @xmath1 rates can be guaranteed . since cpas are equivalent to ladms , the proposed algorithms are also inertial ladms .      the paper is organized as follows . in section [ sc : cpa = ladm ] , we study the equivalence of cpas and different applications of ladm . in section [ sc : vi - ladm ] , we explain cpas as applications of a general ppm to the mvi formulation of the kkt system .",
    "this explanation allows us to study cpas within the setting of ppm .",
    "inertial cpas are also proposed in this section , and convergence results including global point - convergence , nonasymptotic @xmath0 and asymptotic @xmath1 convergence rates are given . in section [ sc : numerical ] , we demonstrate the performance of inertial cpas via experimental results on compressive image reconstruction based on total variation minimization . finally , we give some concluding remarks in section [ sc : concluding ] .",
    "our notation is rather standard , as used above in this section . the standard inner product and @xmath77 norm are denoted by @xmath78 and @xmath79 , respectively .",
    "the superscript ",
    "@xmath80 \" denotes the matrix / vector transpose operator . for any positive semidefinite matrix @xmath81 of size @xmath8-by-@xmath8 and vectors",
    "@xmath82 , we let @xmath83 and @xmath84 . the spectral radius of a square matrix @xmath81 is denoted by @xmath85 .",
    "the identity matrix of order @xmath7 is denoted by @xmath86 . with a little abuse of notation , the columnwise adhesion of two columns vectors @xmath30 and @xmath26 , i.e. , @xmath87 ,",
    "is often denoted by @xmath88 whenever it does not incure any confusion .",
    "other notation will be introduced as the paper progresses .",
    "in this section , we first present two versions of cpas , which can be viewed as applying the original cpa proposed in @xcite to the primal problem ( p@xmath11 ) and the dual problem ( d@xmath11 ) , respectively .",
    "each version of the cpas can appear in two different forms , depending on which variable is updated first .",
    "we then study the equivalence of cpas and different applications of ladm .",
    "our results are based on the previous observations given in @xcite .",
    "recall that the proximity operator of a closed proper convex function is defined in . the original cpa proposed by chambolle & pock in @xcite to solve ( p@xmath11 ) and its other formulations",
    "is summarized below in algorithm [ alg : yxx ] .",
    "[ alg : yxx ] given @xmath89 , @xmath90 and @xmath91 . set @xmath92 . for @xmath93 , iterate as    [ yxx ] @xmath94    algorithm [ alg : yxx ] will be referred as cp-@xmath95 subsequently , because it updates the dual variable @xmath25 first , followed by the primal variable @xmath29 , and finally an extrapolation step in @xmath29 to obtain @xmath96 .",
    "we note that in the original work @xcite the extrapolation step takes the form @xmath97 , where @xmath98 $ ] is a parameter . in this paper",
    ", we only focus on the case @xmath99 , which is exclusively used in practice .",
    "it was shown in @xcite that , under certain assumptions , the sequence @xmath100 generated by converges to a solution of ( pd ) for general closed proper convex functions @xmath9 and @xmath10 .",
    "in particular , an ergodic sublinear convergence result was obtained .",
    "accelerations of cp-@xmath95 were also considered there for problems with strong convexity .    by moving to after and reindexing the points , we obtain a cyclically equivalent form of cp-@xmath95 , which is summarized below in algorithm [ alg : xxy ] and will be referred as cp-@xmath101 subsequently .",
    "note that , compared to cp-@xmath95 , @xmath102 is no longer needed to launch the algorithm .",
    "[ alg : xxy ] given @xmath89 , @xmath90 and @xmath91 .",
    "for @xmath93 , iterate as    [ xxy ] @xmath103    by comparing ( d@xmath11 ) with ( p@xmath11 ) , it is easy to observe that @xmath22 , @xmath23 , @xmath104 and @xmath25 in ( d@xmath11 ) play , respectively , the roles of @xmath10 , @xmath9 , @xmath6 and @xmath29 in ( p@xmath11 ) .",
    "thus , by exchanging of variables , functions and parameters in cp-@xmath95 as follows @xmath105 and using the fact that @xmath106 for any closed proper convex function @xmath14 , see , e.g. , @xcite , we obtain another cpa , which is stated below and will be referred as cp-@xmath107 later .",
    "[ alg : xyy ] given @xmath89 , @xmath90 and @xmath91 .",
    "set @xmath108 . for @xmath93 , iterate as    [ xyy ] @xmath109    if cp-@xmath95 is viewed as applying the original cpa in @xcite to the primal problem ( p@xmath11 ) , then cp-@xmath107 can be considered as applying the original cpa to the dual problem ( d@xmath11 ) . similarly , by moving to after and reindexing the points",
    ", we obtain a cyclically equivalent algorithm , which will be referred as cp-@xmath110 and is given below in algorithm [ alg : yyx ] .",
    "it is alike that , compared to cp-@xmath107 , @xmath111 is no longer needed to start the algorithm .",
    "[ alg : yyx ] given @xmath89 , @xmath91 and @xmath90 .",
    "for @xmath93 , iterate as    [ yyx ] @xmath112    compared with cp-@xmath95 given in , it is easy to see that cp-@xmath107 given in just exchanged the updating order of the primal and the dual variables . after updating both the primal and the dual variables , cp-@xmath95 and cp-@xmath107 apply an extrapolation step to the latest updated variable . in comparison , cp-@xmath101 and cp-@xmath110 given respectively in and apply an extrapolation step immediately after one of the variables is updated , followed by updating the other variable . in any case , the extrapolation step must be applied to the latest updated variable .",
    "we emphasize that , although cp-@xmath95 and cp-@xmath101 are cyclically equivalent , it is more convenient to analyze cp-@xmath101 in the setting of ppm .",
    "this is because the iteration of cp-@xmath101 is from @xmath113 to @xmath114 . by using the notion of proximity operator",
    ", we can express @xmath115 and @xmath116 explicitly in terms of @xmath117 and @xmath118 , which is convenient for analysis . in comparison",
    ", cp-@xmath95 does not have this feature .",
    "similar remarks are applicable to cp-@xmath107 and cp-@xmath110 .",
    "therefore , we only concentrate on cp-@xmath101 and cp-@xmath110 for convergence analysis . in the following ,",
    "we prove that cp-@xmath95/cp-@xmath101 and cp-@xmath107/cp-@xmath110 are equivalent to applying ladm to the dual problem ( d@xmath17 ) and the primal problem ( p@xmath17 ) , respectively .",
    "let @xmath119 and @xmath34 be defined in and , respectively .",
    "to solve ( d@xmath17 ) by the adm , the following subproblem needs to be solved repeatedly : @xmath120 to avoid solving it iteratively and construct an algorithm with cheap per - iteration cost , ladm linearizes the quadratic term @xmath121 at @xmath44 and meanwhile adds a proximal term @xmath45 for some @xmath122 . as such , @xmath116 is obtained as the solution of the following approximation problem ( constant terms are omitted ) @xmath123 where @xmath46 is a proximal parameter . by using the proximity operator defined in , we can summarize the resulting ladm below in algorithm [ alg : ladm - d ] , which will be referred as ladmd-@xmath124 for apparent reason .",
    "[ alg : ladm - d ] given @xmath125 , @xmath90 , @xmath91 and @xmath126 .",
    "the ladm applied to the dual problem ( d@xmath17 ) iterates , for @xmath93 , as    [ ladm - d ] @xmath127    the next theorem establishes the equivalence of cp-@xmath95 and ladmd-@xmath124 . in @xcite , cp-@xmath95 was explained as a preconditioned adm .",
    "[ thm : yxx = ladm - d ] let @xmath125 , @xmath90 , @xmath91 and @xmath126 be given .",
    "suppose that @xmath128",
    ". then , cp-@xmath95 and ladmd-@xmath124 given in and , respectively , are equivalent in the sense that both algorithms generate exactly the same sequence @xmath129 .",
    "we will show that the sequence @xmath129 generated by satisfies .",
    "let @xmath93 .",
    "from the moreau s decomposition , @xmath130 given in can be rewritten as @xmath131 it is easy to see from that @xmath115 given in satisfies @xmath132 which is exactly . from and",
    "the assumption that @xmath128 , we obtain @xmath133 note that @xmath92 in cp-@xmath95 .",
    "it is thus clear from and that @xmath134 for all @xmath93 . by direct calculation",
    ", we have @xmath135 therefore , @xmath116 given in reduces to @xmath136 , which is .    by moving to after and reindexing the points",
    ", we obtain a cyclically equivalent form of ladmd-@xmath124 , which is given below and will be referred as ladmd-@xmath137 .",
    "[ alg : ladm - d - vxy ] given @xmath125 , @xmath90 and @xmath91 .",
    "the ladm applied to the dual problem ( d@xmath17 ) iterates , for @xmath93 , as    [ ladm - d - vxy ] @xmath138    note that , since ladmd-@xmath137 updates @xmath26 first , it can be launched with @xmath139 but without initialization of @xmath26 .",
    "similarly , @xmath102 is not needed to start cp-@xmath101 .",
    "the equivalence of cp-@xmath101 and ladmd-@xmath137 is stated in theorem [ thm : xxy = ladm - d - vxy ] , whose proof is analogous to that of theorem [ thm : yxx = ladm - d ] and is thus omitted .",
    "in contrast to the equivalence of cp-@xmath95 and ladmd-@xmath124 , that of cp-@xmath101 and ladmd-@xmath137 does not require the condition @xmath140 anymore .",
    "[ thm : xxy = ladm - d - vxy ] let @xmath125 , @xmath90 and @xmath91 be given .",
    "then , cp-@xmath101 and ladmd-@xmath137 given , respectively , in and are equivalent in the sense that both algorithms generate exactly the same sequence @xmath129 .",
    "now we apply ladm to the primal problem ( p@xmath17 ) . the augmented lagrangian function associated with ( p@xmath17 )",
    "is given by @xmath141 where @xmath142 is the lagrangian multiplier ( and also the dual variable in ( d@xmath11 ) or ( d@xmath17 ) ) , @xmath46 is a penalty parameter , and @xmath143 is defined as @xmath144 given @xmath145 , the adm for solving ( p@xmath17 ) iterates as    [ adm - xuy ] @xmath146    similarly , due to the presence of linear operator @xmath6 in @xmath147 , the solution of calls for an inner loop in general . to avoid solving it iteratively ,",
    "we linearize at each iteration the quadratic term @xmath148 at @xmath149 , add a proximal term and approximate it by ( again , constant terms are omitted ) @xmath150 where @xmath151 is a proximal parameter .",
    "the resulting ladm is given in algorithm [ alg : ladm - p ] and will be referred as ladmp-@xmath152 .",
    "[ alg : ladm - p ] given @xmath125 , @xmath90 , @xmath153 and @xmath91 .",
    "the ladm applied to the primal problem ( p@xmath17 ) iterates , for @xmath93 , as    [ ladm - p ] @xmath154    the equivalence of cp-@xmath107 and ladmp-@xmath152 can be established completely in analogous as in theorem [ thm : yxx = ladm - d ] .",
    "see also @xcite .",
    "similarly , to guarantee that both schemes generate exactly the same sequence of points , a condition @xmath155 must be imposed on the initial points , which was not stated in the literature .",
    "[ thm : xyy = ladm - p ] let @xmath125 , @xmath90 , @xmath153 and @xmath91 be given .",
    "suppose that @xmath155 .",
    "then , cp-@xmath107 and ladmp-@xmath152 given in and , respectively , are equivalent in the sense that both algorithms generate exactly the same sequence @xmath129 .    similarly , by moving to after and reindexing the points",
    ", we obtain a cyclically equivalent algorithm that does not need @xmath156 in initialization .",
    "the algorithm , which will be referred as ladmp-@xmath157 , and its equivalence to cp-@xmath110 are summarized in algorithm [ alg : ladm - p - uyx ] and theorem [ thm : yyx = ladm - p - uyx ] , respectively .",
    "[ alg : ladm - p - uyx ] given @xmath125 , @xmath90 and @xmath91 .",
    "the ladm applied to the primal problem ( p@xmath17 ) iterates , for @xmath93 , as    [ ladm - p - uyx ] @xmath158    [ thm : yyx = ladm - p - uyx ] let @xmath125 , @xmath90 and @xmath91 be given .",
    "then , cp-@xmath110 and ladmp-@xmath157 given in and , respectively , are equivalent in the sense that both algorithms generate exactly the same sequence @xmath129 .",
    "in this section , we first show that cpas are equivalent to applying a general ppm to the mvi formulation of the kkt system of ( pd ) .",
    "we then propose inertial cpas .",
    "again , since cpas are equivalent to ladms , the proposed inertial cpas can also be called inertial ladms . in the following , we mainly focus on cp-@xmath110 given in and discussions for other cpas are alike",
    ".      under certain regularity assumptions , see , e.g. , @xcite , solving the primal - dual pair ( p@xmath11 ) and ( d@xmath11 ) is equivalent to finding @xmath159 such that the following kkt conditions are satisfied :    [ kkt - p2 ] @xmath160    in the rest of this paper , we use the notation @xmath161 , @xmath162 since the coefficient matrix defining @xmath163 is skew - symmetric , @xmath163 is thus monotone . using these notation ,",
    "the kkt system can be equivalently represented as a mvi problem : find @xmath164 such that @xmath165 we make the following assumption on the problem ( pd ) .    [ as : kkt - nonempty ] assume that the set of solutions of , denoted by @xmath166 , is nonempty .",
    "using analysis similar to that in ( * ? ? ?",
    "* lemma 2.2 ) , we can show that cp-@xmath110 is a general ppm applied to the mvi formulation . though the proof is simple , we give it for completeness .",
    "[ lem : ladm - mvi - k+1 ] for given @xmath167 , the new iterate @xmath168 generated by cp-@xmath110 given in satisfies @xmath169 where @xmath170 is given by @xmath171    recall that the proximity operator is defined as the solution of an optimization problem in .",
    "the optimality conditions of and read @xmath172 which can be equivalently represented as @xmath173 by the notation defined in , it is clear that the addition of the above two inequalities yields , with @xmath170 defined in .    for cp-@xmath101",
    "given in , similar result holds .",
    "specifically , the new iterate @xmath168 generated by cp-@xmath101 from a given @xmath167 satisfies with the weighting matrix @xmath170 given by @xmath174    throughout this paper , we make the following assumption on the parameters @xmath175 and @xmath176 .    [ as : tau - sigma ] the parameters @xmath175 and @xmath176 satisfy the conditions @xmath177 and @xmath178 .",
    "it is apparent that @xmath170 defined in or is symmetric and positive definite under assumption [ as : tau - sigma ] .",
    "thus , cp-@xmath110 and cp-@xmath101 can be viewed as a general ppm with a symmetric and positive definite weighting matrix @xmath170 . with this explanation",
    ", the convergence results of cpa can be established very conveniently under the ppm framework .",
    "here we present the convergence results and omit the proof .",
    "interested readers can refer to , e.g. , @xcite , for similar convergence results and different analytic techniques .",
    "[ thm : convergence - ladm ] assume that @xmath175 and @xmath176 satisfy assumption [ as : tau - sigma ] .",
    "let @xmath179 be generated by cp-@xmath110 given in or cp-@xmath101 given in from any starting point @xmath180 .",
    "the following results hold .    1 .",
    "the sequence @xmath179 converges to a solution of , i.e. , there exists @xmath181 such that @xmath182 , where @xmath183 and @xmath184 are , respectively , solutions of ( p@xmath11 ) and ( d@xmath11 ) .",
    "2 .   for any fixed integer @xmath185 ,",
    "define @xmath186 .",
    "then , it holds that @xmath187 3 .",
    "after @xmath185 iterations , we have @xmath188 moreover , it holds as @xmath189 that @xmath190 .",
    "since cpas are applications of a general ppm , we can study the corresponding inertial algorithms by following the analysis in @xcite . in this section ,",
    "we propose inertial versions of cp-@xmath110 and cp-@xmath101 and present their convergence results .",
    "the inertial versions of cp-@xmath110 and cp-@xmath101 are summarized below in algorithms [ alg : iyyx ] and [ alg : ixxy ] , respectively .",
    "[ alg : iyyx ] let @xmath191 and a sequence of nonnegative parameters @xmath192 be given .",
    "starting at any initial point @xmath193 , the algorithm iterates , for @xmath93 , as    [ iyyx ] @xmath194    [ alg : ixxy ] let @xmath191 and a sequence of nonnegative parameters @xmath192 be given .",
    "starting at any initial point @xmath193 , the algorithm iterates , for @xmath93 , as    [ ixxy ] @xmath195    we will refer to algorithms [ alg : iyyx ] and [ alg : ixxy ] as icp-@xmath110 and icp-@xmath101 , respectively . recall that we use the notation @xmath196 , @xmath197 and @xmath163 defined in .",
    "we further define @xmath198 according to theorem [ thm : convergence - ladm ] , the new point @xmath68 generated by icp-@xmath110 or icp-@xmath101 conforms to @xmath199 where @xmath170 is given by for icp-@xmath110 and for icp-@xmath101 .",
    "the global point - convergence , nonasymptotic @xmath0 and asymptotic @xmath1 convergence rate results of icp-@xmath110 and icp-@xmath101 , or equivalently - with @xmath170 given by for icp-@xmath110 and for icp-@xmath101 , follow directly from ( * ? ? ?",
    "* theorem 2.2 ) .",
    "[ theorem1 ] suppose that @xmath175 and @xmath176 satisfy assumption [ as : tau - sigma ] , and , for all @xmath93 , it holds that @xmath200 for some @xmath201 .",
    "let @xmath202 conform to or , or equivalently , - with @xmath170 given by for icp-@xmath110 and for icp-@xmath101 .",
    "then , the following results hold .    1 .",
    "the sequence @xmath202 converges to a member in @xmath166 as @xmath189 ; 2 .   for any @xmath203 and positive integer @xmath2",
    ", it holds that @xmath204 furthermore , it holds as @xmath189 that @xmath205    it is easy to see from and the definition of @xmath170 in or that if @xmath206 then @xmath68 is a solution of .",
    "thus , the @xmath0 and @xmath1 results given in and , respectively , can be viewed as convergence rate results of icp-@xmath110 and icp-@xmath101 .",
    "it is also worth to point out that the global convergence result given in theorem [ theorem1 ] is point - convergence , which is stronger than convergence in function values for the accelerated methods in @xcite , which in fact can also be viewed as inertial type methods .",
    "our stronger convergence result is obtained at the cost of more restrictive conditions on the inertial extrapolation parameters @xmath192 .",
    "in this section , we present numerical results to compare the performance of cpas and the proposed inertial cpas . in particular",
    ", we mainly concentrate on cp-@xmath110 and its inertial variant icp-@xmath110 ( the reasons will be explained below in section [ sc : numerical - reason ] ) .",
    "all algorithms were implemented in matlab , and the experiments were performed with microsoft windows 8 and matlab v7.13 ( r2011b ) , running on a 64-bit lenovo laptop with an intel core i7 - 3667u cpu at 2.00 ghz and 8 gb of memory . here , we only concentrate on a total variation ( tv ) based image reconstruction problem and compare icp-@xmath110 with cp-@xmath110 .",
    "the performance of cpas or ladmms relative to other state - of - the - art algorithms is well illustrated in the literature , see , e.g. , @xcite for various imaging problems , @xcite for @xmath207-norm minimization in compressive sensing , @xcite for nuclear norm minimization in low - rank matrix completion , and @xcite for danzig selector .      in our experiments , we tested the problem of reconstructing an image from a number of its linear measurements , as in the setting of compressive sensing .",
    "the reconstruction is realized via total variation minimization . in variational image processing , tv minimizations",
    "have been widely used ever since the pioneering work @xcite and have empirically shown to give favorable reconstruction results .",
    "it is well known that the edges of images can be well preserved if one minimizes the tv .",
    "another very important reason of the popularity of tv minimizations for image restoration is the availability of very fast numerical algorithms , e.g. , @xcite . in the compressive sensing setting , exact reconstruction guarantee of piecewise constant images from their incomplete frequencies and via tv minimization was first obtained in @xcite .",
    "lately , it was shown in @xcite that an image can be accurately recovered to within its best @xmath208-term approximation of its gradient from approximately @xmath209 nonadaptive linear measurements , where @xmath210 denotes the number of pixels of the underlying image . in particular",
    ", the reconstruction is exact if the gradient of the image is precisely sparse , i.e. , the image is piecewise constant .",
    "this is even true for high dimensional signals , see @xcite .    in the following , we let @xmath211 be the first - order global forward finite difference matrices ( with certain boundary conditions assumed ) in the horizontal and the vertical directions , respectively .",
    "let @xmath212 , @xmath213 , be the corresponding first - order local forward finite difference operator at the @xmath214th pixel , i.e. , each @xmath215 is a two - row matrix formed by stacking the @xmath214th rows of @xmath216 and @xmath217 .",
    "let @xmath218 be an original @xmath8-by-@xmath8 image , whose columns are stacked in an left - upper and right - lower order to form a vector of length @xmath219 .",
    "our discussions can be applied to rectangle images , and here we concentrate on square images only for simplicity .",
    "given a set of linear measurements @xmath220 , where @xmath221 is a linear operator .",
    "the theory developed in @xcite guarantees that one can reconstruct @xmath222 from @xmath223 and @xmath224 to within certain high accuracy , as long as @xmath223 satisfies certain technical conditions . specifically , to reconstruct @xmath222 from @xmath223 and @xmath224",
    ", one seeks an image that fits the observation data and meanwhile has the minimum tv , i.e. , a solution of the following tv minimization problem @xmath225 here @xmath226 denotes the indicator function of a set @xmath227 , i.e. , @xmath226 is equal to @xmath228 if @xmath229 and @xmath230 otherwise . for @xmath231 , @xmath232 , we define @xmath233 note that @xmath234 and @xmath235 denote the same set of variables .",
    "let @xmath236 $ ] and @xmath237 be , respectively , defined as    [ def - gf - tvcs ] @xmath238    then , can be rewritten as @xmath239 , which is clearly in the form of ( p@xmath11 ) .",
    "let @xmath240 be the adjoint operator of @xmath223 and @xmath241 be the identity operator .",
    "in our experiments , the linear operator @xmath223 satisfies @xmath242 .",
    "therefore , the proximity operator of @xmath9 is given by @xmath243 note that the proximity operator of an indicator function reduces to the orthogonal projection onto the underlying set .",
    "the proximity parameter is omitted because it is irrelevant in this case . on the other hand , with the convention @xmath244 , the proximity operator of ",
    "@xmath79 \" is given by @xmath245 furthermore , it is easy to observe from that @xmath10 is separable with respect to @xmath246 and thus the proximity operator of @xmath10 can also be expressed explicitly .",
    "therefore , the functions @xmath9 and @xmath10 defined in satisfy assumption [ as : f - g - simple ] . as a result , cpas and",
    "the proposed inertial cpas are easy to implement .      in our experiments ,",
    "the linear operator @xmath223 is set to be randomized partial walsh - hadamard transform matrix , whose rows are randomly chosen and columns randomly permuted . therefore , it holds that @xmath242 . specifically , the walsh - hadamard transform matrix of order @xmath247 is defined recursively as @xmath248 , h_{2 ^ 1 } = \\left [              \\begin{array}{cc }                1 & 1 \\\\                1 & -1 \\\\",
    "\\end{array }            \\right ] , \\ldots , h_{2^j } = \\left [              \\begin{array}{cc }                h_{2^{j-1 } } & h_{2^{j-1 } } \\\\                h_{2^{j-1 } } & -h_{2^{j-1 } } \\\\",
    "\\end{array }            \\right].\\ ] ] it can be shown that @xmath249 . in our experiments ,",
    "the linear operator @xmath223 contains random selected rows from @xmath250 , where @xmath251 is a normalization factor .",
    "it is worth to point out that for some special linear operators , e.g. , @xmath223 is a partial fourier matrix or a partial discrete cosine transform , ( and its denoising variants when the observation data contains noise ) can be solved by the classical adm framework without proximal - linearizing any of the subproblems , as long as the constraints are wisely treated and the finite difference operations are assumed to satisfy appropriate boundary conditions . in these cases ,",
    "the @xmath29-subproblem can usually be solved by fast transforms , see , e.g. , @xcite . in our setting , the matrices @xmath252 and @xmath253 can not be diagonalized simultaneously , no matter what boundary conditions are assumed for @xmath6 . therefore , when solving the problem by the classical adm , the @xmath29-subproblem is not easily solvable , no matter how the constraints @xmath254 are adapted ( e.g. , penalization or relaxation ) . in contrast ,",
    "when solving the problem by cpas , no linear system needs to be solved and the algorithms are easily implementable as long as the proximity operators of the underlying functions can be efficiently evaluated .",
    "we tested 12 images , most of which are obtained from the usc - sipi image database .",
    "the image sizes are @xmath255-by-@xmath255 , @xmath256-by-@xmath256 and @xmath257-by-@xmath257 , each of which contains 4 images . the tested images , together with their names in the database , are given in figure [ fig1-images ] .",
    "the parameters common to cpas and their corresponding inertial cpas are @xmath176 and @xmath175 , for which we used the same set of values . in our experiments ,",
    "periodic boundary conditions are assumed for the finite difference operations .",
    "it is easy to show that @xmath258 .",
    "the parameters @xmath176 and @xmath175 were set to be @xmath259 and @xmath260 uniformly for all tests , which may be suboptimal but perform favorably for imaging problems with appropriately scaled data .",
    "in particular , this setting satisfies the convergence requirement of all algorithms .",
    "the extrapolation parameter @xmath261 for inertial cpas was set to be @xmath262 and held constant .",
    "this value of @xmath261 is determined based on experiments .",
    "how to select @xmath261 adaptively to achieve faster convergence remains a research issue .",
    "here our main goal is to illustrate the effect of the extrapolation steps .",
    "we will take icp-@xmath110 as an example and present some experimental results to compare its performance with different constant values of @xmath261 . in our experiments , we initialized @xmath263 and @xmath264 for all algorithms .",
    "it is clear from that if @xmath265 and @xmath266 then a solution is already obtained .",
    "thus , we terminated cpas by @xmath267 where @xmath268 is a tolerance parameter , and @xmath269 . for inertial cpas ,",
    "the same can be said , except that @xmath113 needs to be replaced by @xmath270 .",
    "thus , we terminated inertial cpas by @xmath271 the quantities in and can be viewed as optimality residues ( in a relative sense ) .",
    "the tolerance parameter @xmath272 will be specified below .",
    "the quality of recovered images is evaluated by signal - to - noise ratio ( snr ) , which is defined as @xmath273 here @xmath222 and @xmath29 represent the original and the recovered images , and @xmath274 denotes the mean intensity of @xmath222 .",
    "note that the constraint @xmath254 is always preserved at each iteration and for all algorithms .",
    "therefore , we only report the objective function value @xmath275 , denoted by @xmath276 , but not the data fidelity @xmath277 .",
    "recall that cp-@xmath107 ( resp .",
    "cp-@xmath95 ) and cp-@xmath110 ( resp .",
    "cp-@xmath101 ) are cyclically equivalent .",
    "therefore , they generate exactly the same sequence of points as long as the initial points are properly chosen . as a result",
    ", we only need to concentrate on cp-@xmath110 and cp-@xmath101 , and compare with their corresponding inertial variants .",
    "it is interesting that we have observed from our extensive experimental results on total variation based image reconstruction problems that cp-@xmath110 and cp-@xmath101 perform almost identically as long as the same set of parameters ( @xmath175 and @xmath176 ) and initial points @xmath139 are used . in particular , cp-@xmath110 and cp-@xmath101 generate two sequences of points with very close optimality residues , objective function values ( i.e. , @xmath276 ) and snrs ( defined in ) .",
    "after very few iterations , these quantities usually differ little .",
    "note that , instructed by , we meassured the optimality residue by @xmath278 in our experiments , where @xmath170 is given by and , respectively , for cp-@xmath110 and cp-@xmath101 .",
    "the similar performance of cp-@xmath110 and cp-@xmath101 seems to be reasonable by directly comparing the iteration formulas in and .",
    "another plausible explanation of this phenomenon is as follows .",
    "recall that both cp-@xmath110 and cp-@xmath101 are applications of a general ppm , i.e. , they satisfy with the weighting matrix given by and , respectively . by comparing the two matrices",
    ", we see that they are different only in the signs of @xmath6 and @xmath279 . as a result",
    ", they have exactly the same spectrum , which performs as the essential magnitudes of proximity .    for the respective inertial algorithms ,",
    "i.e. , icp-@xmath110 and icp-@xmath101 , we observed the similar performance and the same remarks given above apply .    for illustrative purpose",
    ", here we present some computational results on the cameraman image with four levels of measurements , i.e. , @xmath280 .",
    "we used the parameters specified in section [ sc : num - parameters ] , run each algorithm for 1000 iterations and recorded the optimality residues , the objective function values and the snrs of the generated sequences .",
    "these quantities are denoted by @xmath281 , @xmath282 and @xmath283 , where @xmath284 for cp-@xmath110 and icp-@xmath110 , and @xmath285 for cp-@xmath101 and icp-@xmath101 .",
    "note that , for inertial cpas , the optimality residue is defined as @xmath286 with @xmath170 given by and for icp-@xmath110 and icp-@xmath101 , respectively .",
    "this definition is justified by .",
    "the comparison results of cp-@xmath110 and cp-@xmath101 ( resp . icp-@xmath110 and icp-@xmath101 ) on these quantities are given in the first ( resp .",
    "the second ) row of figure [ fig2-cpyyx - cpxxy ] , where we presented the differences ( in either absolute or relative sense ) of these quantities .",
    "it is easy to observe that the results given in figure [ fig2-cpyyx - cpxxy ] basically justify our remarks given above in this subsection .",
    "therefore , in our experiments we only compare cp-@xmath110 and its inertial version icp-@xmath110 .",
    "experimental results for cp-@xmath101 and icp-@xmath101 are not presented since they are similar to those of cp-@xmath110 and icp-@xmath110 , respectively .",
    "recall that the image size is denoted by @xmath287 , and the number of measurements is denoted by @xmath288 .",
    "for each image , we tested four levels of measurements , that is @xmath280 . to implement the algorithms , a computation of the form ",
    "@xmath289 \" must be carried out at each iteration . in implementation , this is completed by using the moreau s decomposition , i.e. , compute an intermediate variable @xmath30 first as  @xmath290 \" and then recover @xmath25 via  @xmath291 \" . here",
    "the quantity @xmath292 can be viewed as primal residue for the equivalent constrained formulation ( p@xmath17 ) . in the experimental results , besides snr and objective function value , we also present this feasibility residue , measured by infinity norm @xmath293 , and the number of iterations required by the algorithms ( denoted , respectively , by it1 and it2 for cp-@xmath110 and icp-@xmath110 ) to meet the condition or .",
    "we do not present the cpu time results for comparison because the per - iteration cost of the algorithms is roughly identical and the consumed cpu time is basically proportional to the respective number of iterations .",
    "detailed experimental results for @xmath294 and @xmath295 are given in tables [ table - tol=1e-2]-[table - tol=1e-4 ] , respectively .",
    "note that in tables [ table - tol=1e-2]-[table - tol=1e-4 ] the results for @xmath276 and @xmath296 are given in scientific notation , where the first number denotes the significant digit and the second denotes the power .     [ 1pt ] @xmath297 & & & & & & & & & & & + [ 2pt ]    [ table - tol=1e-2 ]     [ 1pt ] @xmath297 & & & & & & & & & & & + [ 2pt ]    [ table - tol=1e-3 ]     [ 1pt ] @xmath297 & & & & & & & & & & & + [ 2pt ]    [ table - tol=1e-4 ]    it can be seen from tables [ table - tol=1e-2]-[table - tol=1e-4 ] that , to obtain solutions satisfying the aforementioned conditions , icp-@xmath110 is generally faster than cp-@xmath110 .",
    "specifically , within our setting the numbers of iterations consumed by icp-@xmath110 range , roughly , from @xmath298@xmath299 of those consumed by cp-@xmath110 . in most cases ,",
    "icp-@xmath110 obtained recovery results with slightly better final objective function values and feasibility residues .",
    "the quality of recovered images is also slightly better in terms of snr . by comparing results between different tables",
    ", we see that high accuracy solutions in optimization point of view generally imply better image quality measured by snr .",
    "this could imply that solving the problem to a certain high accuracy is in some sense necessary for better recovery , though the improvement of image quality could be small when the solution is already very accurate .",
    "it can also be observed from the results that both algorithms converge very fast at the beginning stage and slow down afterwards . in particular , to improve the solution quality by one more digit of accuracy ( measured by optimality residue defined in - ) , the number of iterations could be multiplied by a few times , which is probably a common feature of first - order optimization algorithms .",
    "the fact is that in most cases one does not need to solve imaging problems to extremely high accuracy , because the recovered results hardly have any difference detectable by human eyes when they are already accurate enough . in words",
    ", the inertial technique accelerates the original algorithm to some extent without increasing the total computational cost .    to better visualize the performance improvement of icp-@xmath110 over cp-@xmath110 , we reorganized the results given in tables [ table - tol=1e-2]-[table - tol=1e-4 ] and presented them in figure [ fig - m3 ] .",
    "for each measurement level @xmath297 and image size @xmath8 , we accumulated the number of iterations for different images and took an average .",
    "the results for @xmath300 and @xmath295 are given in figure [ fig - m3 ] . by comparing the three plots in figure [ fig - m3 ]",
    ", we see that the number of iterations increased from a few dozens to around one thousand when the accuracy tolerance @xmath272 was decreased from @xmath301 to @xmath295 . from the results we can also observe that , on average , both algorithms perform stably in the sense that the consumed number of iterations do not vary much for different image sizes .",
    "we also examined the performance of icp-@xmath110 with different constant strategies for the inertial extrapolation stepsize @xmath261 .",
    "in particular , for @xmath302 we tested @xmath303 .",
    "the results are given in figure [ fig - alp4 ] .",
    "it can be seen from the results that , for the four tested @xmath75 values , larger ones generally give better performance .",
    "recall that , according to our analysis , icp-@xmath110 is guaranteed to converge under the condition @xmath304 for all @xmath2 .",
    "indeed , we have observed that icp-@xmath110 either slows down or performs instable for large values of @xmath75 , say , larger than @xmath305 , especially when the number of measurements is relatively small .",
    "this is the main reason that we set @xmath261 a constant value that is near @xmath305 but not larger .",
    "similar discussions for compressive principal component pursuit problems can be found in @xcite .",
    "in this paper , based on the observations given in @xcite , we showed that cpas and ladms generate exactly the same sequence of points if the initial points for ladms are properly chosen .",
    "the dependence on initial points for ladm can be relaxed by focusing on cyclically equivalent forms of the algorithms . by using the fact that cpas are applications of a general ppm to the kkt system",
    ", we were able to propose inertial cpas for solving structured convex optimization problem . under certain conditions ,",
    "the global point - convergence , nonasymptotic @xmath0 and asymptotic @xmath1 rate of convergence of the proposed inertial cpas are guaranteed .",
    "these convergence rate results are previously not known for inertial type methods .",
    "our preliminary implementation of the algorithms and extensive experimental results on tv based image reconstruction problems have shown that inertial cpas are generally faster than the corresponding original cpas .",
    "though in a sense the acceleration is not very significant , the inertial extrapolation step does not introduce any additional yet unnegligible computational cost either .",
    "the extrapolation steplength @xmath261 was set to be constant in our experiments , which was determined based on experimental results . how to select @xmath261 adaptively such that the overall performance is stable and more efficient deserves further investigation .",
    "moreover , the requirement that @xmath192 is nondecreasing seems not reasonable either .",
    "interesting topics for future research may be to relax the conditions on @xmath192 , to improve the convergence rate results and to propose modified inertial type algorithms so that the extrapolation stepsize @xmath261 can be significantly enlarged .",
    "felipe alvarez and hedy attouch .",
    "an inertial proximal method for maximal monotone operators via discretization of a nonlinear oscillator with damping .",
    ", 9(1 - 2):311 , 2001 .",
    "wellposedness in optimization and related topics ( gargnano , 1999 ) .",
    "r.  glowinski and a.  marrocco .",
    "sur lapproximation , par lments finis dordre un , et la rsolution , par pnalisation - dualit , dune classe de problmes de dirichlet non linaires .",
    ", 9(r-2):4176 , 1975 ."
  ],
  "abstract_text": [
    "<S> the primal - dual algorithm recently proposed by chambolle & pock ( abbreviated as cpa ) for structured convex optimization is very efficient and popular . </S>",
    "<S> it was shown by chambolle & pock in @xcite and also by shefi & teboulle in @xcite that cpa and variants are closely related to preconditioned versions of the popular alternating direction method of multipliers ( abbreviated as adm ) . in this paper </S>",
    "<S> , we further clarify this connection and show that cpas generate exactly the same sequence of points with the so - called linearized adm ( abbreviated as ladm ) applied to either the primal problem or its lagrangian dual , depending on different updating orders of the primal and the dual variables in cpas , as long as the initial points for the ladm are properly chosen . </S>",
    "<S> the dependence on initial points for ladm can be relaxed by focusing on cyclically equivalent forms of the algorithms . </S>",
    "<S> furthermore , by utilizing the fact that cpas are applications of a general weighted proximal point method to the mixed variational inequality formulation of the kkt system , where the weighting matrix is positive definite under a parameter condition , we are able to propose and analyze inertial variants of cpas . under certain conditions , global point - convergence , nonasymptotic @xmath0 and asymptotic @xmath1 convergence rate of the proposed inertial cpas can be guaranteed , where @xmath2 denotes the iteration index . finally , we demonstrate the profits gained by introducing the inertial extrapolation step via experimental results on compressive image reconstruction based on total variation minimization </S>",
    "<S> .    structured convex optimization , primal - dual algorithm , inertial primal - dual algorithm , linearized alternating direction method of multipliers , proximal point method , total variation , image reconstruction .    65k05 , 65k10 , 65j22 , 90c25 </S>"
  ]
}