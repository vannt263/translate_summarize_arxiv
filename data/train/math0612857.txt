{
  "article_text": [
    "consider the problem of estimating a @xmath0-vector of parameters @xmath3 from the linear model @xmath4 where @xmath5 is an @xmath6-vector of responses , @xmath7 is an @xmath8 random design matrix with i.i.d .",
    "@xmath9 , @xmath10 is a @xmath0-vector of parameters , and @xmath11 is an @xmath6-vector of i.i.d . random errors . when dimension @xmath0 is high , it is often assumed that only a small number of predictors among @xmath12 contribute to the response , which amounts to assuming ideally that the parameter vector @xmath3 is sparse . with sparsity ,",
    "variable selection can improve estimation accuracy by effectively identifying the subset of important predictors , and also enhance model interpretability with parsimonious representation .",
    "sparsity comes frequently with high dimensional data , which is a growing feature in many areas of contemporary statistics .",
    "the problems arise frequently in genomics such as gene expression and proteomics studies , biomedical imaging , functional mri , tomography , tumor classifications , signal processing , image analysis , and finance , where the number of variables or parameters @xmath0 can be much larger than sample size @xmath6 .",
    "for instance , one may wish to classify tumors using microarray gene expression or proteomics data ; one may wish to associate protein concentrations with expression of genes or predict certain clinical prognosis ( e.g. , injury scores or survival time ) using gene expression data .",
    "for this kind of problems , the dimensionality can be much larger than the sample size , which calls for new or extended statistical methodologies and theories .",
    "see , e.g. , donoho ( 2000 ) and fan and li ( 2006 ) for overviews of statistical challenges with high dimensionality .    back to the problem in ( [ 011 ] ) , it is challenging to find tens of important variables out of thousands of predictors , with number of observations usually in tens or hundreds .",
    "this is similar to finding a couple of needles in a huge haystack .",
    "a new idea in candes and tao ( 2007 ) is the notion of uniform uncertainty principle ( uup ) on deterministic design matrices .",
    "they proposed the dantzig selector , which is the solution to an @xmath13-regularization problem , and showed that under uup , this minimum @xmath13 estimator achieves the ideal risk , i.e. , the risk of the oracle estimator with the true model known ahead of time , up to a logarithmic factor @xmath2 .",
    "appealing features of the dantzig selector include : 1 ) it is easy to implement because the convex optimization the dantzig selector solves can easily be recast as a linear program ; and 2 ) it has the oracle property in the sense of donoho and johnstone ( 1994 ) .    despite their remarkable achievement",
    ", we still have four concerns when the dantzig selector is applied to high or ultra - high dimensional problems .",
    "first , a potential hurdle is the computational cost for large or huge scale problems such as implementing linear programs in dimension tens or hundreds of thousands .",
    "second , the factor @xmath2 can become large and may not be negligible when dimension @xmath0 grows rapidly with sample size @xmath6 .",
    "third , as dimensionality grows , their uup condition may be hard to satisfy , which will be illustrated later using a simulated example .",
    "finally , there is no guarantee the dantzig selector picks up the right model though it has the oracle property .",
    "these four concerns inspire our work .",
    "dimension reduction or feature selection is an effective strategy to deal with high dimensionality .",
    "with dimensionality reduced from high to low , computational burden can be reduced drastically . meanwhile",
    ", accurate estimation can be obtained by using some well - developed lower dimensional method .",
    "motivated by this along with those concerns on the dantzig selector , we have the following main goal in our paper :    * reduce dimensionality @xmath0 from a large or huge scale ( say , @xmath14 for some @xmath15 ) to a relatively large scale @xmath16 ( e.g. , @xmath17 ) by a fast and efficient method .",
    "we achieve this by introducing the concept of sure screening and proposing a sure screening method based on a correlation learning which filters out the features that have weak correlation with the response .",
    "such a correlation screening is called sure independence screening ( sis ) . here and below , by sure screening we mean a property that all the important variables survive after variable screening with probability tending to one .",
    "this dramatically narrows down the search for important predictors .",
    "in particular , applying the dantzig selector to the much smaller submodel relaxes our first concern on the computational cost .",
    "in fact , this not only speeds up the dantzig selector , but also reduces the logarithmic factor in mimicking the ideal risk from @xmath2 to @xmath18 , which is smaller than @xmath19 and hence relaxes our second concern above .",
    "it also addresses he third concern since the uup condition is easier to satisfy .",
    "oracle properties in a stronger sense , say , mimicking the oracle in not only selecting the right model , but also estimating the parameters efficiently , give a positive answer to our third and fourth concerns above .",
    "theories on oracle properties in this sense have been developed in the literature .",
    "fan and li ( 2001 ) lay down groundwork on variable selection problems in the finite parameter setting .",
    "they discussed a family of variable selection methods that adopt a penalized likelihood approach , which includes well - established methods such as the aic and bic , as well as more recent methods like the bridge regression in frank and friedman ( 1993 ) , lasso in tibshirani ( 1996 ) , and scad in fan ( 1997 ) and antoniadis and fan ( 2001 ) , and established oracle properties for nonconcave penalized likelihood estimators .",
    "later on , fan and peng ( 2004 ) extend the results to the setting of @xmath20 and show that the oracle properties continue to hold .",
    "an effective algorithm for optimizing penalized likelihood , local quadratic approximation ( lqa ) , was proposed in fan and li ( 2001 ) and well studied in hunter and li ( 2005 ) .",
    "zou ( 2006 ) introduces an adaptive lasso in a finite parameter setting and shows that lasso does not have oracle properties as conjectured in fan and li ( 2001 ) , whereas the adaptive lasso does .",
    "zou and li ( 2008 ) propose a local linear approximation algorithm that recasts the computation of non - concave penalized likelihood problems into a sequence of penalized @xmath1-likelihood problems .",
    "they also proposed and studied the one - step sparse estimators for nonconcave penalized likelihood methods .",
    "there is a huge literature on the problem of variable selection . to name a few in addition to those mentioned above , fan and li ( 2002 ) study variable selection for cox s proportional hazards model and frailty model ; efron , hastie , johnstone and tibshirani ( 2004 ) propose lars ; hunter and li ( 2005 ) propose a new class of algorithms , mm algorithms , for variable selection ; meinshausen and bhlmann ( 2006 ) look at the problem of variable selection with the lasso for high dimensional graphs , and zhao and yu ( 2006 ) give an almost necessary and sufficient condition on model selection consistency of lasso .",
    "meier , van de geer and bhlmann ( 2008 ) proposed a fast implementation for group lasso .",
    "more recent studies include huang , horowitz and ma ( 2008 ) , paul _ et al . _",
    "( 2007 ) , zhang ( 2007 ) , and zhang and huang ( 2008 ) , which signficantly advances the theory and methods of the penalized least - squares approaches .",
    "it is worth to mention that in variable selection , there is a weaker concept than consistency , called persistency , introduced by greenshtein and ritov ( 2004 ) .",
    "motivation of this concept lies in the fact that in machine learning such as tumor classifications , the primary interest centers on the misclassification errors or more generally expected losses , not the accuracy of estimated parameters .",
    "greenshtein and ritov ( 2004 ) study the persistency of lasso - type procedures in high dimensional linear predictor selection , and greenshtein ( 2006 ) extends the results to more general loss functions .",
    "meinshausen ( 2007 ) considers a case with finite nonsparsity and shows that under quadratic loss , lasso is persistent , but the rate of persistency is slower than that of a relaxed lasso .      to gain some insight on challenges of high dimensionality in variable selection , let us look at a situation where all the predictors @xmath12 are standardized and the distribution of @xmath21 is spherically symmetric , where @xmath22 and @xmath23 . clearly , the transformed predictor vector @xmath24 has covariance matrix @xmath25 .",
    "our way of study in this paper is to separate the impacts of the covariance matrix @xmath26 and the distribution of @xmath24 , which gives us a better understanding on difficulties of high dimensionality in variable selection .",
    "the real difficulty when dimension @xmath0 is larger than sample size @xmath6 comes from four facts .",
    "first , the design matrix @xmath27 is rectangular , having more columns than rows . in this case",
    ", the matrix @xmath28 is huge and singular .",
    "the maximum spurious correlation between a covariate and the response can be large ( see , e.g. , figure 1 ) because of the dimensionality and the fact that an unimportant predictor can be highly correlated with the response variable due to the presence of important predictors associated with the predictor .",
    "these make variable selection difficult .",
    "second , the population covariance matrix @xmath26 may become ill - conditioned as @xmath6 grows , which adds difficulty to variable selection .",
    "third , the minimum nonzero absolute coefficient @xmath29 may decay with @xmath6 and get close to the noise level , say , the order @xmath30 .",
    "fourth , the distribution of @xmath24 may have heavy tails .",
    "therefore , in general , it is challenging to estimate the sparse parameter vector @xmath3 accurately when @xmath31 .",
    "figures / hdcorr1.eps",
    "+    when dimension @xmath0 is large , some of the intuition might not be accurate .",
    "this is exemplified by the data piling problems in high dimensional space observed in hall , marron and neeman ( 2005 ) . a challenge with high",
    "dimensionality is that important predictors can be highly correlated with some unimportant ones , which usually increases with dimensionality .",
    "the maximum spurious correlation also grows with dimensionality .",
    "we illustrate this using a simple example .",
    "suppose the predictors @xmath32 are independent and follow the standard normal distribution .",
    "then , the design matrix is an @xmath33 random matrix , each entry an independent realization from @xmath34 .",
    "the maximum absolute sample correlation coefficient among predictors can be very large .",
    "this is indeed against our intuition , as the predictors are independent . to show this , we simulated 500 data sets with @xmath35 and @xmath36 and @xmath37 , respectively .",
    "figure 1 shows the distributions of the maximum absolute sample correlation .",
    "the multiple canonical correlation between two groups of predictors ( e.g. , 2 in one group and 3 in another ) can even be much larger , as there are already @xmath38 choices of the two groups in our example .",
    "hence , sure screening when @xmath0 is large is very challenging .",
    "the paper is organized as follows . in the next section",
    "we propose a sure screening method sure independence screening ( sis ) and discuss its rationale as well as its connection with other methods of dimensionality reduction . in section 3",
    "we review several known techniques for model selection in the reduced feature space and present two simulations and one real data example to study the performance of sis based model selection methods . in section 4",
    "we discuss some extensions of sis and in particular , an iterative sis is proposed and illustrated by three simulated examples .",
    "section 5 is devoted to the asymptotic analysis of sis , an iteratively thresholded ridge regression screener as well as two sis based model selection methods .",
    "some concluding remarks are given in section 6 .",
    "technical details are provided in the appendix .",
    "by sure screening we mean a property that all the important variables survive after applying a variable screening procedure with probability tending to one . a dimensionality reduction method is desirable if it has the sure screening property .",
    "below we introduce a simple sure screening method using componentwise regression or equivalently a correlation learning . throughout the paper",
    "we center each input variable so that the observed mean is zero , and scale each predictor so that the sample standard deviation is one .",
    "let @xmath39 be the true sparse model with nonsparsity size @xmath40 .",
    "the other @xmath41 variables can also be correlated with the response variable via linkage to the predictors contained in the model .",
    "let @xmath42 be a @xmath43-vector obtained by the componentwise regression , that is , @xmath44 where the @xmath8 data matrix @xmath27 is first standardized columnwise as mentioned before .",
    "hence , @xmath45 is really a vector of marginal correlations of predictors with the response variable , rescaled by the standard deviation of the response .    for",
    "any given @xmath46 , we sort the @xmath0 componentwise magnitudes of the vector @xmath45 in a decreasing order and define a submodel @xmath47 $ largest of all}\\right\\},\\ ] ] where @xmath48 $ ] denotes the integer part of @xmath49 .",
    "this is a straightforward way to shrink the full model @xmath50 down to a submodel @xmath51 with size @xmath52<n$ ] .",
    "such a correlation learning ranks the importance of features according to their marginal correlation with the response variable and filters out those that have weak marginal correlations with the response variable .",
    "we call this correlation screening method sure independence screening ( sis ) , since each feature is used independently as a predictor to decide how useful it is for predicting the response variable .",
    "this concept is broader than the correlation screening and is applicable to generalized linear models , classification problems under various loss functions , and nonparametric learning under sparse additive models .",
    "the computational cost of correlation learning or sis is that of multiplying a @xmath53 matrix with an @xmath6-vector plus getting the largest @xmath16 components of a @xmath0-vector , so sis has computational complexity @xmath54 .",
    "it is worth to mention that sis uses only the order of componentwise magnitudes of @xmath45 , so it is indeed invariant under scaling .",
    "thus the idea of sis is identical to selecting predictors using their correlations with the response . to implement sis , we note that linear models with more than @xmath6 parameters are not identifiable with only @xmath6 data points . hence",
    ", we may choose @xmath55 $ ] to be conservative , for instance , @xmath56 or @xmath57 depending on the order of sample size @xmath6 .",
    "although sis is proposed to reduce dimensionality @xmath0 from high to below sample size @xmath6 , nothing can stop us applying it with final model size @xmath58 , say , @xmath59 .",
    "it is obvious that larger @xmath16 means larger probability to include the true model @xmath60 in the final model @xmath61 .",
    "sis is a hard - thresholding - type method . for orthogonal design matrices ,",
    "it is well understood .",
    "but for general design matrices , there is no theoretical support for it , though this kind of idea is frequently used in applications .",
    "it is important to identify the conditions under which the sure screening property holds for sis , i.e. , @xmath62 for some given @xmath63 .",
    "this question as well as how the sequence @xmath64 should be chosen will be answered by theorem 1 in section 5 .",
    "we would like to point out that the simple thresholding algorithm ( see , e.g. , baron _",
    "et al . _ , 2005 and gribonval _ et al . _ , 2007 ) that is used in sparse approximation or compressed sensing is a one step greedy algorithm and related to sis .",
    "in particular , our asymptotic analysis in section 5 helps to understand the performance of the simple thresholding algorithm .      to better understand the rationale of the correlation learning",
    ", we now introduce an iteratively thresholded ridge regression screener ( itrrs ) , which is an extension of the dimensionality reduction method sis .",
    "but for practical implementation , only the correlation learning is needed .",
    "itrrs also provides a very nice technical tool for our understanding of the sure screening property of the correlation screening and other methods .    when there are more predictors than observations , it is well known that the least squares estimator @xmath65 is noisy , where @xmath66 denotes the moore - penrose generalized inverse of @xmath67 .",
    "we therefore consider the ridge regression , namely , linear regression with @xmath68-regularization to reduce the variance .",
    "let @xmath69 be a @xmath70-vector obtained by the ridge regression , that is , @xmath71 where @xmath72 is a regularization parameter .",
    "it is obvious that @xmath73 and the scaled ridge regression estimator tends to the componentwise regression estimator : @xmath74 in view of ( [ 118 ] ) , to make @xmath75 less noisy we should choose large regularization parameter @xmath76 to reduce the variance in the estimation .",
    "note that the ranking of the absolute components of @xmath75 is the same as that of @xmath77 . in light of ( [ 096 ] ) the componentwise regression estimator is a specific case of the ridge regression with regularization parameter @xmath78 , namely , it makes the resulting estimator as less noisy as possible .    for",
    "any given @xmath79 , we sort the @xmath0 componentwise magnitudes of the vector @xmath75 in a descending order and define a submodel @xmath80 $ largest of all}\\right\\}.\\ ] ] this procedure reduces the model size by a factor of @xmath81 .",
    "the idea of itrrs to be introduced below is to perform dimensionality reduction as above successively until the number of remaining variables drops to below sample size @xmath6 .",
    "it will be shown in theorem 2 in section 5 that under some regularity conditions and when the tuning parameters @xmath82 and @xmath83 are chosen appropriately , with overwhelming probability the submodel @xmath84 will contain the true model @xmath85 and its size is an order @xmath86 for some @xmath87 lower than the original one @xmath70 .",
    "this property stimulates us to propose itrrs as follows :    * first , carry out the procedure in ( [ 039 ] ) to the full model @xmath88 and get a submodel @xmath84 with size @xmath89 $ ] ; * then , apply a similar procedure to the model @xmath84 and again obtain a submodel @xmath90 with size @xmath91 $ ] , and so on ; * finally , get a submodel @xmath92 with size @xmath93<n$ ] , where @xmath94\\geq n$ ] .",
    "we would like to point out that the above procedure is different from the threshholded ridge regression , as the submodels and estimated parameters change over the course of iterations .",
    "the only exception is the case that @xmath78 , in which the rank of variables do not vary with iterations .",
    "now we are ready to see that the correlation learning introduced in section 2.1 is a specific case of itrrs since the componentwise regression is a specific case of the ridge regression with an infinite regularization parameter .",
    "the itrrs provides a very nice technical tool for understanding how fast the dimension @xmath0 can grow compared with sample size @xmath6 and how the final model size @xmath16 can be chosen while the sure screening property still holds for the correlation learning . the question of whether itrrs has the sure screening property as well as how the tuning parameters @xmath95 and @xmath83 should be chosen will be answered by theorem 3 in section 5 .",
    "the number of steps in itrrs depends on the choice of @xmath96 .",
    "we will see in theorem 3 that @xmath83 can not be chosen too small which means that there should not be too many iteration steps in itrrs .",
    "this is due to the cumulation of the probability errors of missing some important variables over the iterations .",
    "in particular , the backward stepwise deletion regression which deletes one variable each time in itrrs until the number of remaining variables drops to below sample size might not work in general as it requires @xmath97 iterations .",
    "when @xmath0 is of exponential order , even though the probability of mistakenly deleting some important predictors in each step of deletion is exponentially small , the cumulative error in exponential order of operations may not be negligible .      as pointed out",
    "before , sis uses the marginal information of correlation to perform dimensionality reduction .",
    "the idea of using marginal information to deal with high dimensionality has also appeared independently in huang , horowitz and ma ( 2008 ) who proposed to use marginal bridge estimators to select variables for sparse high dimensional regression models .",
    "we now look at sis in the context of classification , in which the idea of independent screening appears natural and has been widely used .",
    "the problem of classification can be regarded as a specific case of the regression problem with response variable taking discrete values such as @xmath98 .",
    "for high dimensional problems like tumor classification using gene expression or proteomics data , it is not wise to classify the data using the full feature space due to the noise accumulation and interpretability",
    ". this is well demonstrated both theoretically and numerically in fan and fan ( 2008 ) .",
    "in addition , many of the features come into play through linkage to the important ones ( see , e.g. , figure 1 ) .",
    "therefore feature selection is important for high dimensional classification . how to effectively select important features and how many of them to include are two tricky questions to answer .",
    "various feature selection procedures have been proposed in the literature to improve the classification power in presence of high dimensionality .",
    "for example , tibshirani _ et al . _",
    "( 2002 ) introduce the nearest shrunken centroids method , and fan and fan ( 2008 ) propose the features annealed independence rules ( fair ) procedure .",
    "theoretical justification for these methods are given in fan and fan ( 2008 ) .",
    "sis can readily be used to reduce the feature space .",
    "now suppose we have @xmath99 samples from class @xmath100 and @xmath101 samples from class @xmath102 .",
    "then the componentwise regression estimator ( [ 120 ] ) becomes @xmath103 written more explicitly , the @xmath104-th component of the @xmath0-vector @xmath45 is @xmath105 by recalling that each covariate in ( [ a01 ] ) has been normalized marginally , where @xmath106 is the sample average of the @xmath104-th feature with class label `` 1 '' and @xmath107 is the sample average of the @xmath104-th feature with class label `` @xmath102 '' .",
    "when @xmath108 , @xmath109 is simply a version of the two - sample @xmath110-statistic except for a scaling constant . in this case , feature selection using sis is the same as that using the two - sample @xmath110-statistics .",
    "see fan and fan ( 2008 ) for a theoretical study of sure screening property in this context .",
    "two - sample @xmath110-statistics are commonly used in feature selection for high dimensional classification problems such as in the significance analysis of gene selection in microarray data analysis ( see , e.g. , storey and tibshirani , 2003 ; fan and ren , 2006 ) as well as in the nearest shrunken centroids method of tibshirani _ et al . _",
    "therefore sis is an insightful and natural extension of this widely used technique .",
    "although not directly applicable , the sure screening property of sis in theorem 1 after some adaptation gives theoretical justification for the nearest shrunken centroids method .",
    "see fan and fan ( 2008 ) for a sure screening property .    by using sis",
    "we can single out the important features and thus reduce significantly the feature space to a much lower dimensional one . from this point on , many methods such as the linear discrimination ( ld ) rule or the naive bayes ( nb ) rule can be applied to conduct the classification in the reduced feature space .",
    "this idea will be illustrated on a leukemia data set in section 3.3.3 .",
    "as shown later in theorem 1 in section 5 , with the correlation learning , we can shrink the full model @xmath50 straightforward and accurately down to a submodel @xmath111 with size @xmath52=o(n)$ ] .",
    "thus the original problem of estimating the sparse @xmath70-vector @xmath3 in ( [ 011 ] ) reduces to estimating a sparse @xmath112-vector @xmath113 based on the now much smaller submodel @xmath114 , namely , @xmath115 where @xmath116 denotes an @xmath117 submatrix of @xmath27 obtained by extracting its columns corresponding to the indices in @xmath114 .",
    "apparently sis can speed up variable selection dramatically when the original dimension @xmath0 is ultra high .",
    "now we briefly review several well - developed moderate dimensional techniques that can be applied to estimate the @xmath112-vector @xmath3 in ( [ 001 ] ) at the scale of @xmath16 that is comparable with @xmath6 .",
    "those methods include scad in fan and li ( 2001 ) and fan and peng ( 2004 ) , adaptive lasso in zou ( 2006 ) , the dantzig selector in candes and tao ( 2007 ) , among others .",
    "penalization is commonly used in variable selection .",
    "fan and li ( 2001 , 2006 ) give a comprehensive overview of feature selection and a unified framework based on penalized likelihood approach to the problem of variable selection .",
    "they consider the penalized least squares ( pls ) @xmath118 where @xmath119 and @xmath120 is a penalty function indexed by a regularization parameter @xmath121 .",
    "variation of the regularization parameters across the predictors allows us to incorporate some prior information .",
    "for example , we may want to keep certain important predictors in the model and choose not to penalize their coefficients .",
    "the regularization parameters @xmath121 can be chosen , for instance , by cross - validation ( see , e.g. , breiman , 1996 and tibshirani , 1996 ) . a unified and effective algorithm for optimizing penalized likelihood , called local quadratic approximation ( lqa ) , was proposed in fan and li ( 2001 ) and well studied in hunter and li ( 2005 ) . in particular ,",
    "lqa can be employed to minimize the above pls . in our implementation",
    ", we choose @xmath122 and select @xmath82 by bic .    .",
    "right panel : @xmath123 for penalized @xmath1 ( thin solid ) , scad with @xmath124 ( dashed ) and @xmath125 ( dotted ) and adaptive lasso ( thick solid ) with @xmath126.,title=\"fig:\",height=216 ] .",
    "right panel : @xmath123 for penalized @xmath1 ( thin solid ) , scad with @xmath124 ( dashed ) and @xmath125 ( dotted ) and adaptive lasso ( thick solid ) with @xmath126.,title=\"fig:\",height=225 ] +    an alternative and effective algorithm to minimize the penalized least - squares problem ( [ 111 ] ) is the local linear approximation ( lla ) proposed by zou and li ( 2008 ) . with the local linear approximation",
    ", the problem ( [ 111 ] ) can be cast as a sequence of penalized @xmath1 regression problems so that the lars ( efron , _ et al .",
    "_ , 2004 ) or other algorithms can be employed .",
    "more explicitly , given the estimate @xmath127 at the @xmath128-th iteration , instead of minimizing ( [ 111 ] ) , one minimizes @xmath129 which after adding the constant term @xmath130 is a local linear approximation to @xmath131 in ( [ 111 ] ) , where @xmath132 ) can be regarded as a family of weighted penalized @xmath1-problem and the function @xmath133 dictates the amount of penalty at each location . the emphasis on non - concave penalty functions by fan and li ( 2001 ) is to ensure that penalty decreases to zero as @xmath134 gets large .",
    "this reduces unnecessary biases of the penalized likelihood estimator , leading to the oracle property in fan and li ( 2001 ) .",
    "figure 2 depicts how the scad function is approximated locally by a linear or quadratic function and the derivative functions @xmath123 for some commonly used penalty functions . when the initial value @xmath135 , the first step estimator is indeed lasso so the implementation of scad can be regarded as an iteratively reweighted penalized @xmath1-estimator with lasso as an initial estimator .",
    "see section 6 for further discussion of the choice of initial values @xmath136 .",
    "the pls ( [ 111 ] ) depends on the choice of penalty function @xmath120 .",
    "commonly used penalty functions include the @xmath137-penalty , @xmath138 , nonnegative garrote in breiman ( 1995 ) , and smoothly clipped absolute deviation ( scad ) penalty , in fan ( 1997 ) and a minimax concave penality ( mcp ) in zhang ( 2007 ) ( see below for definition ) . in particular , the @xmath13-penalized least squares is called lasso in tibshirani ( 1996 ) . in seminal papers , donoho and huo ( 2001 ) and donoho and elad ( 2003 ) show that penalized @xmath139-solution can be found by penalized @xmath13-method when the problem is sparse enough , which implies that the best subset regression can be found by using the penalized @xmath13-regression .",
    "antoniadis and fan ( 2001 ) propose the pls for wavelets denoising with irregular designs .",
    "fan and li ( 2001 ) advocate penalty functions with three properties : sparsity , unbiasedness , and continuity .",
    "more details on characterization of these three properties can be found in fan and li ( 2001 ) and antoniadis and fan ( 2001 ) . for penalty functions",
    ", they showed that singularity at the origin is a necessary condition to generate sparsity and nonconvexity is required to reduce the estimation bias .",
    "it is well known that @xmath137-penalty with @xmath140 does not satisfy the continuity condition , @xmath137-penalty with @xmath141 does not satisfy the sparsity condition , and @xmath13-penalty ( lasso ) possesses the sparsity and continuity , but generates estimation bias , as demonstrated in fan and li ( 2001 ) , zou ( 2006 ) , and meinshausen ( 2007 ) .    fan ( 1997 ) proposes a continuously differentiable penalty function called the smoothly clipped absolute deviation ( scad ) penalty , which is defined by @xmath142 fan and li ( 2001 ) suggest using @xmath143 .",
    "this function has similar feature to the penalty function @xmath144 advocated in nikolova ( 2000 ) .",
    "the mcp in zhang ( 2007 ) translates the flat part of the derivative of the scad to the origin and is given by @xmath145 which minimizes the maximum of the concavity .",
    "the scad penalty and mcp satisfy the above three conditions simultaneously .",
    "we will show in theorem 5 in section 5 that sis followed by the scad enjoys the oracle properties .",
    "the lasso in tibshirani ( 1996 ) has been widely used due to its convexity .",
    "it however generates estimation bias .",
    "this problem was pointed out in fan and li ( 2001 ) and formally shown in zou ( 2006 ) even in a finite parameter setting . to overcome this bias problem , zou ( 2006 ) proposes an adaptive lasso and meinshausen ( 2007 ) proposes a relaxed lasso .",
    "the idea in zou ( 2006 ) is to use an adaptively weighted @xmath13 penalty in the pls ( [ 111 ] ) .",
    "specifically , he introduced the following penalization term @xmath146 where @xmath147 is a regularization parameter and @xmath148 is a known weight vector .",
    "he further suggested using the weight vector @xmath149 , where @xmath150 , the power is understood componentwise , and @xmath151 is a root-@xmath6 consistent estimator . in view of ( [ jf1 ] ) ,",
    "the adaptive lasso is really the implementation of pls ( [ 111 ] ) with @xmath152 using lla .",
    "its connections with the family of non - concave penalized least - squares is apparently from ( [ jf1 ] ) and figure 2 .",
    "the case of @xmath153 is closely related to the nonnegative garrote in breiman ( 1995 ) .",
    "zou ( 2006 ) also showed that the adaptive lasso can be solved by the lars algorithm , which was proposed in efron , hastie , johnstone and tibshirani ( 2004 ) .",
    "using the same finite parameter setup as that in knight and fu ( 2000 ) , zou ( 2006 ) establishes that the adaptive lasso has the oracle properties as long as the tuning parameter is chosen in a way such that @xmath154 and @xmath155 as @xmath156 .",
    "the dantzig selector was proposed in candes and tao ( 2007 ) to recover a sparse high dimensional parameter vector in the linear model .",
    "adapted to the setting in ( [ 001 ] ) , it is the solution @xmath157 to the following @xmath13-regularization problem @xmath158 where @xmath159 is a tuning parameter , @xmath160 is an @xmath6-vector of the residuals , and @xmath161 and @xmath162 denote the @xmath13 and @xmath163 norms , respectively . they pointed out that the above convex optimization problem can easily be recast as a linear program : @xmath164 where the optimization variables are @xmath165 and @xmath166 , and @xmath167 is a @xmath112-vector of ones",
    ".    we will show in theorem 4 in section 5 that an application of sis followed by the dantzig selector can achieve the ideal risk up to a factor of @xmath168 with @xmath169 , rather than the original @xmath2 .",
    "in particular , if dimension @xmath0 is growing exponentially fast , i.e. , @xmath170 for some @xmath15 , then a direct application of the dantzig selector results in a loss of a factor @xmath171 which could be too large to be acceptable . on the other hand , with the dimensionality first reduced by sis",
    "the loss is now merely of a factor @xmath168 , which is less than @xmath19 .",
    "[ htb ]    figures / sis.eps +      for the problem of ultra - high dimensional variable selection , we propose first to apply a sure screening method such as sis to reduce dimensionality from @xmath70 to a relatively large scale @xmath112 , say , below sample size @xmath6 .",
    "then we use a lower dimensional model selection method such as the scad , dantzig selector , lasso , or adaptive lasso .",
    "we call sis followed by the scad and dantzig selector sis - scad and sis - ds , respectively for short in the paper . in some situations",
    ", we may want to further reduce the model size down to @xmath172 using a method such as the dantzig selector along with the hard thresholding or the lasso with a suitable tuning , and finally choose a model with a more refined method such as the scad or adaptive lasso .",
    "in the paper these two methods will be referred to as sis - ds - scad and sis - ds - adalasso , respectively for simplicity .",
    "figure 3 shows a schematic diagram of these approaches .",
    "the idea of sis makes it feasible to do model selection with ultra high dimensionality and speeds up variable selection drastically .",
    "it also makes the model selection problem efficient and modular .",
    "sis can be used in conjunction with any model selection technique including the bayesian methods ( see , e.g. , george and mcculloch , 1997 ) and lasso .",
    "we did not include sis - lasso for numerical studies due to the approximate equivalence between dantzig selector and lasso ( bickel , ritov and tsybakov , 2007 ; meinshausen , rocha and yu , 2007 ) .",
    "to study the performance of sis based model selection methods proposed above , we now present two simulations and one real data example .      for the first simulation , we used the linear model ( [ 011 ] ) with i.i.d .",
    "standard gaussian predictors and gaussian noise with standard deviation @xmath173 .",
    "we considered two such models with @xmath174 and @xmath175 , respectively .",
    "the sizes @xmath176 of the true models , i.e. , the numbers of nonzero coefficients , were chosen to be 8 and 18 , respectively , and the nonzero components of the @xmath0-vectors @xmath3 were randomly chosen as follows .",
    "we set @xmath177 and @xmath178 , respectively , and picked nonzero coefficients of the form @xmath179 for each model , where @xmath180 was drawn from a bernoulli distribution with parameter @xmath181 and @xmath182 was drawn from the standard gaussian distribution .",
    "in particular , the @xmath68-norms @xmath183 of the two simulated models are 6.795 and 8.908 , respectively . for each model",
    "we simulated 200 data sets . even with i.i.d .",
    "standard gaussian predictors , the above settings are nontrivial since there is nonnegligible sample correlation among the predictors , which reflects the difficulty of high dimensional variable selection . as an evidence , we report in figure 4 the distributions of the maximum absolute sample correlation when @xmath184 and @xmath185 and @xmath186 , respectively .",
    "it reveals significant sample correlation among the predictors .",
    "the multiple canonical correlation between two groups of predictors can be much larger .",
    "figures / hdcorr2.eps +    to estimate the sparse @xmath0-vectors @xmath3 , we employed six methods : the dantzig selector ( ds ) using a primal - dual algorithm , lasso using the lars algorithm , sis - scad , sis - ds , sis - ds - scad , and sis - ds - adalasso ( see figure 3 ) . for sis - scad and sis - ds ,",
    "we chose @xmath187 $ ] and for the last two methods , we chose @xmath188 and @xmath189 $ ] and in the middle step the dantzig selector was used to further reduce the model size from @xmath16 to @xmath190 by choosing variables with the @xmath190 largest componentwise magnitudes of the estimated @xmath16-vector ( see figure 3 ) .",
    "figures / rank01.eps +    table 1 : results of simulation i +    [ c]cccccccccccc & + & + @xmath70 & * ds & & * lasso & & * sis - scad & & * sis - ds & & * sis - ds - scad & & * sis - ds - adalasso + @xmath191 & @xmath192 & & 62.5 & & 15 & & 37 & & 27 & & 34 + & 1.381 & & 0.895 & & 0.374 & & 0.795 & & 0.614 & & 1.269 + 20000 &  & &  & & 37 & & 119 & & 60.5 & & 99 + &  & &  & & 0.288 & & 0.732 & & 0.372 & & 1.014 + * * * * * *    the simulation results are summarized in figure 5 and table 1 .",
    "figure 5 , produced based on 500 simulations , depicts the distribution of the minimum number of selected variables , i.e. , the selected model size , that is required to include all variables in the true model by using sis .",
    "it shows clearly that in both settings it is safe to shrink the full model down to a submodel of size @xmath193 $ ] with sis , which is consistent with the sure screening property of sis shown in theorem 1 in section 5 .",
    "for example , for the case of @xmath194 and @xmath36 , reducing the model size to 50 includes the variables in the true model with high probability , and for the case of @xmath195 and @xmath196 , it is safe to reduce the dimension to about 500 .",
    "for each of the above six methods , we report in table 1 the median of the selected model sizes and median of the estimation errors @xmath197 in @xmath68-norm .",
    "four entries of table 1 are missing due to limited computing power and software used . in comparison",
    ", sis reduces the computational burden significantly .    from table 1",
    "we see that the dantzig selector gives nonsparse solutions and the lasso using the cross - validation for selecting its tuning parameter produces large models",
    ". this can be due to the fact that the biases in lasso require a small bandwidth in cross - validation , whereas a small bandwidth results in lack of sparsistency , using the terminology of ravikumar _ et al . _",
    "this has also been observed and demonstrated in the work by lam and fan ( 2007 ) in the context of estimating sparse covariance or precision matrices .",
    "we should point out here that a variation of the dantzig selector , the gauss - dantzig selector in candes and tao ( 2007 ) , should yield much smaller models , but for simplicity we did not include it in our simulation . among all methods ,",
    "sis - scad performs the best and generates much smaller and more accurate models .",
    "it is clear to see that scad gives more accurate estimates than the adaptive lasso in view of the estimation errors .",
    "also , sis followed by the dantzig selector improves the estimation accuracy over using the dantzig selector alone , which is in line with our theoretical result .",
    "figures / rank02.eps +    table 2 : results of simulation ii +    [ c]cccccccccccc & + & + @xmath70 & * ds & & * lasso & & * sis - scad & & * sis - ds & & * sis - ds - scad & & * sis - ds - adalasso + @xmath191 & @xmath192 & & 91 & & 21 & & 56 & & 27 & & 52 + ( @xmath198 ) & 1.256 & & 1.257 & & 0.331 & & 0.727 & & 0.476 & & 1.204 + & @xmath192 & & 74 & & 18 & & 56 & & 31.5 & & 51 + ( @xmath199 ) & 1.465 & & 1.257 & & 0.458 & & 1.014 & & 0.787 & & 1.824 + 20000 &  & &  & & 36 & & 119 & & 54 & & 86 + &  & &  & & 0.367 & & 0.986 & & 0.743 & & 1.762 + * * * * * *      for the second simulation , we used similar models to those in simulation i except that the predictors are now correlated with each other .",
    "we considered three models with @xmath200 , @xmath201 , and @xmath202 , respectively , where @xmath176 denotes the size of the true model , i.e. , the number of nonzero coefficients .",
    "the three @xmath0-vectors @xmath3 were generated in the same way as in simulation i. we set @xmath203 , @xmath204 , and @xmath205 , respectively .",
    "in particular , the @xmath68-norms @xmath183 of the three simulated models are 3.304 , 6.795 , and 7.257 , respectively . to introduce correlation between predictors , we first used a matlab function ` sprandsym ` to randomly generate an @xmath206 symmetric positive definite matrix @xmath207 with condition number @xmath208 , and drew samples of @xmath209 predictors @xmath210 from @xmath211 .",
    "then we took @xmath212 and defined the remaining predictors as @xmath213 , @xmath214 and @xmath215 , @xmath216 with @xmath217 , @xmath218 , and @xmath218 , respectively . for each model",
    "we simulated 200 data sets .",
    "we applied the same six methods as those in simulation i to estimate the sparse @xmath70-vectors @xmath3 . for sis - scad and sis - ds ,",
    "we chose @xmath219 $ ] , @xmath220 $ ] , and @xmath221 $ ] , respectively , and for the last two methods , we chose @xmath188 and @xmath222 $ ] , @xmath220 $ ] , and @xmath193 $ ] , respectively .",
    "the simulation results are similarly summarized in figure 6 ( based on 500 simulations ) and table 2 .",
    "similar conclusions as those from simulation i can be drawn . as in simulation",
    "i , we did not include the gauss - dantzig selector for simplicity .",
    "it is interesting to observe that in the first setting here , the lasso gives large models and its estimation errors are noticeable compare to the norm of the true coefficient vector @xmath3 .",
    "we also applied sis to select features for the classification of a leukemia data set .",
    "the leukemia data from high - density affymetrix oligonucleotide arrays were previously analyzed in golub _",
    "( 1999 ) and are available at ` http://www.broad.mit.edu/cgi-bin/cancer/datasets.cgi ` .",
    "there are 7129 genes and 72 samples from two classes : 47 in class all ( acute lymphocytic leukemia ) and 25 in class aml ( acute mylogenous leukemia ) . among those 72 samples , 38",
    "( 27 in class all and 11 in class aml ) of them were set as the training sample and the remaining 34 ( 20 in class all and 14 in class aml ) of them were set to be the test sample .",
    "we used two methods sis - scad - ld and sis - scad - nb that will be introduced below to carry out the classification . for each method , we first applied sis to select @xmath223 $ ] genes with @xmath224 the training sample size chosen above and then used the scad to get a family of models indexed by the regularization parameter @xmath225 . here",
    ", we should point out that our classification results are not very sensitive to the choice of @xmath112 as long as it is not too small .",
    "there are certainly many ways to tune the regularization parameter @xmath76 . for simplicity , we chose a @xmath76 that produces a model with size equal to the optimal number of features determined by the features annealed independence rules ( fair ) procedure in fan and fan ( 2008 ) .",
    "16 genes were picked up by their approach .",
    "now we selected 16 genes and got a linear model with size 16 by using sis - scad .",
    "finally , the sis - scad - ld method directly used the above linear discrimination rule to do classification , and the sis - scad - nb method applied the naive bayes ( nb ) rule to the resulted 16-dimensional feature space .",
    "the classification results of the sis - scad - ld , sis - scad - nb , and nearest shrunken centroids method in tibshirani _",
    "( 2002 ) are shown in table 3 .",
    "the results of the nearest shrunken centroids method were extracted from tibshirani _ et al . _",
    "the sis - scad - ld and sis - scad - nb both chose 16 genes and made 1 test error with training errors 0 and 4 , respectively , while the nearest shrunken centroids method picked up 21 genes and made 1 training error and 2 test errors .",
    "table 3 : classification errors on the leukemia data set +    [ cols=\"^,^,^,^\",options=\"header \" , ]",
    "like modeling building in linear regression , there are many variations in the implementation of correlation learning .",
    "this section discusses some extensions of sis to enhance its methodological power .",
    "in particular , an iterative sis ( isis ) is proposed to overcome some weak points of sis .",
    "the methodological power of isis is illustrated by three simulated examples .",
    "the key idea of sis is to apply a single componentwise regression .",
    "three potential issues , however , might arise with this approach .",
    "first , some unimportant predictors that are highly correlated with the important predictors can have higher priority to be selected by sis than other important predictors that are relatively weakly related to the response .",
    "second , an important predictor that is marginally uncorrelated but jointly correlated with the response can not be picked by sis and thus will not enter the estimated model .",
    "third , the issue of collinearity between predictors adds difficulty to the problem of variable selection .",
    "these three issues will be addressed in the extensions of sis below , which allow us to use more fully the joint information of the covariates rather than just the marginal information in variable selection .",
    "it will be shown that when the model assumptions are satisfied , which excludes basically the three aforementioned problems , sis can accurately reduce the dimensionality from ultra high to a moderate scale , say , below sample size .",
    "but when those assumptions fail , it could happen that sis would miss some important predictors . to overcome this problem",
    ", we propose below an isis to enhance the methodological power .",
    "it is an iterative applications of the sis approach to variable selection .",
    "the essence is to iteratively apply a large - scale variable screening followed by a moderate - scale careful variable selection .",
    "the isis works as follows . in the first step ,",
    "we select a subset of @xmath226 variables @xmath227 using an sis based model selection method such as the sis - scad or sis - lasso . these variables were selected , using scad or lasso , based on the joint information of @xmath193 $ ] variables that survive after the correlation learning .",
    "then we have an @xmath6-vector of the residuals from regressing the response @xmath228 over @xmath229 . in the next step ,",
    "we treat those residuals as the new responses and apply the same method as in the previous step to the remaining @xmath230 variables , which results in a subset of @xmath231 variables @xmath232 .",
    "we remark that fitting the residuals from the previous step on @xmath233 can significantly weaken the priority of those unimportant variables that are highly correlated with the response through their associations with @xmath229 , since the residuals are uncorrelated with those selected variables in @xmath234 .",
    "this helps solving the first issue .",
    "it also makes those important predictors that are missed in the previous step possible to survive , which addresses the second issue above .",
    "in fact , after variables in @xmath234 entering into the model , those that are marginally weakly correlated with @xmath228 purely due to the presence of variables in @xmath234 should now be correlated with the residuals .",
    "we can keep on doing this until we get @xmath235 disjoint subsets @xmath236 whose union @xmath237 has a size @xmath16 , which is less than @xmath6 . in practical implementation",
    ", we can choose , for example , the largest @xmath238 such that @xmath239 . from the selected features in @xmath240 , we can choose the features using a moderate scale method such as scad , lasso or dantzig .",
    "for the problem of ultra - high dimensional variable selection , we now have the isis based model selection methods which are extensions of sis based model selection methods . applying a moderate dimensional method such as the scad",
    ", dantzig selector , lasso , or adaptive lasso to @xmath240 will produce a model that is very close to the true sparse model @xmath85 .",
    "the idea of isis is somewhat related to the boosting algorithm ( freund and schapire , 1997 ) .",
    "in particular , if the sis is used to select only one variable at each iteration , i.e. , @xmath241 , the isis is equivalent to a form of matching pursuit or a greedy algorithm for variable selection ( barron , _ et al . _ , 2008 ) .",
    "grouping the input variables is often used in various problems .",
    "for instance , we can divide the pool of @xmath70 variables into disjoint groups each with 5 variables .",
    "the idea of variable screening via sis can be applied to select a small number of groups . in this way",
    "there is less chance of missing the important variables by taking advantage of the joint information among the predictors .",
    "therefore a more reliable model can be constructed .",
    "a notorious difficulty of variable selection lies in the collinearity between the covariates .",
    "effective ways to rule out those unimportant variables that are highly correlated with the important ones are being sought after .",
    "a good idea is to transform the input variables .",
    "two possible ways stand out in this regard .",
    "one is subject related transformation and the other is statistical transformation .    subject related transformation is a useful tool . in some cases , a simple linear transformation of the input variables can help weaken correlation among the covariates .",
    "for example , in somatotype studies the common sense tells us that predictors such as the weights @xmath242 and @xmath243 at 2 , 9 and 18 years are positively correlated .",
    "we could directly use @xmath244 and @xmath243 as the input variables in a linear regression model , but a better way of model selection in this case is to use less correlated predictors such as @xmath245 , which is a linear transformation of @xmath246 that specifies the changes of the weights instead of the weights themselves .",
    "another important example is the financial time series such as the prices of the stocks or interest rates .",
    "differencing can significantly weaken the correlation among those variables .",
    "methods of statistical transformation include an application of a clustering algorithm such as the hierarchical clustering or @xmath128-mean algorithm using the correlation metrics to first group variables into highly correlated groups and then apply the sparse principal components analysis ( pca ) to construct weakly correlated predictors .",
    "now those weakly correlated predictors from each group can be regarded as the new covariates and an sis based model selection method can be employed to select them .",
    "the statistical techniques we introduced above can help identify the important features and thus improve the effectiveness of the vanilla sis based model selection strategy .",
    "introduction of nonlinear terms and transformation of variables can also be used to reduced the modeling biases of linear model .",
    "( 2007 ) introduced sparse additive models ( spam ) to deal with nonlinear feature selection .      to study",
    "the performance of the isis proposed above , we now present three simulated examples .",
    "the aim is to examine the extent to which isis can improve sis in the situation where the conditions of sis fail .",
    "we evaluate the methods by counting the frequencies that the selected models include all the variables in the true model , namely the ability of correctly screening unimportant variables .      for the first simulated example",
    ", we used a linear model @xmath247 where @xmath12 are @xmath0 predictors and @xmath248 is a noise that is independent of the predictors . in the simulation ,",
    "a sample of @xmath249 with size @xmath6 was drawn from a multivariate normal distribution @xmath250 whose covariance matrix @xmath251 has entries @xmath252 , @xmath253 and @xmath254 , @xmath255",
    ". we considered 20 such models characterized by @xmath256 with @xmath257 , @xmath191 , @xmath258 , @xmath259 , @xmath260 , and @xmath261 , @xmath262 , @xmath263 , @xmath264 , respectively , and for each model we simulated 200 data sets .    for each model , we applied sis and the isis to select @xmath6 variables and tested their accuracy of including the true model @xmath265 . for the isis , the sis - scad with @xmath266 $ ] was used at each step and we kept on collecting variables in those disjoint @xmath267 s until we got @xmath6 variables ( if there were more variables than needed in the final step , we only included those with the largest absolute coefficients ) . in table 4 , we report the percentages of sis , lasso and isis that include the true model .",
    "all of these three methods select @xmath56-variables , in order to make fair comparisons .",
    "it is clear that the collinearity ( large value of @xmath268 ) and high - dimensionality deteriorate the performance of sis and lasso , and lasso outperforms sis somewhat .",
    "however , when the sample size is 50 or more , the difference in performance is very small , but sis has much less computational cost . on the other hand , isis improves dramatically the performance of this simple sis and lasso .",
    "indeed , in this simulation , isis always picks all true variables .",
    "it can even have much less computational cost than lasso when lasso is used in the implementation of isis .",
    "table 4 : results of simulated example i : accuracy of sis , lasso and isis + in including the true model @xmath265 +    [ c]c|cccccc@xmath0 & @xmath6 & & @xmath261 & @xmath269 & @xmath270 & @xmath271 + & & sis & .755 & .855 & .690 & .670 + & 20 & lasso & .970 & .990 & .985 & .870 + 100 & & isis & 1 & 1 & 1 & 1 + & & sis & 1 & 1 & 1 & 1 + & 50 & lasso & 1 & 1 & 1 & 1 + & & isis & 1 & 1 & 1 & 1 + & & sis & .205 & .255 & .145 & .085 + & 20 & lasso & .340 & .555 & .556 & .220 + & & isis & 1 & 1 & 1 & 1 +    & & sis & .990 & .960 & .870 & .860 + 1000 & 50 & lasso & 1 & 1 & 1 & 1 + & & isis & 1 & 1 & 1 & 1 + & & sis & 1 & .995 & .97 & .97 + & 70 & lasso & 1 & 1 & 1 & 1 + & & isis & 1 & 1 & 1 & 1 +      for the second simulated example , we used the same setup as in example i except that @xmath268 was fixed to be 0.5 for simplicity .",
    "in addition , we added a fourth variable @xmath272 to the model and the linear model is now @xmath273 where @xmath274 and has correlation @xmath275 with all the other @xmath276 variables .",
    "the way @xmath272 was introduced is to make it uncorrelated with the response @xmath228 .",
    "therefore , the sis can not pick up the true model except by chance .",
    "again we simulated 200 data sets for each model . in table 5 , we report the percentages of sis , lasso and isis that include the true model of four variables . in this simulation example , sis performs somewhat better than lasso in variable screening , and isis outperforms significantly the simple sis and lasso . in this simulation",
    "it always picks all true variables .",
    "this demonstrates that isis can effectively handle the second problem mentioned at the beginning of section 4.1 .",
    "table 5 : results of simulated example ii : accuracy of sis , lasso and isis + in including the true model @xmath277 +    [ c]cccccc@xmath0 & @xmath270 & & @xmath258 & @xmath278 & @xmath279 + & & sis & .025 & .490 & .740 + 100 & & lasso & .000 & .360 & .915 + & & isis & 1 & 1 & 1 + & & sis & .000 & .000 & .000 + 1000 & & lasso & .000 & .000 & .000 + & & isis & 1 & 1 & 1 +      for the third simulated example , we used the same setup as in example ii except that we added a fifth variable @xmath280 to the model and the linear model is now @xmath281 where @xmath282 and is uncorrelated with all the other @xmath276 variables . again",
    "@xmath272 is uncorrelated with the response @xmath228 .",
    "the way @xmath280 was introduced is to make it have a very small correlation with the response and in fact the variable @xmath280 has the same proportion of contribution to the response as the noise @xmath283 does . for this particular example",
    ", @xmath280 has weaker marginal correlation with @xmath228 than @xmath284 and hence has a lower priority to be selected by sis .    for each model we simulated 200 data sets .",
    "in table 6 , we report the accuracy in percentage of sis , lasso and isis in including the true model .",
    "it is clear to see that the isis can improve significantly over the simple sis and lasso and always picks all true variables .",
    "this shows again that the isis is able to pick up two difficult variables @xmath272 and @xmath280 , which addresses simultaneously the second and third problem at the beginning of section 4 .",
    "table 6 : results of simulated example iii : accuracy of sis , lasso and isis + in including the true model @xmath285 +    [ c]cccccc@xmath0 & @xmath270 & & @xmath258 & @xmath278 & @xmath279 + & & sis & .000 & .285 & .645 + 100 & & lasso & .000 & .310 & .890 + & & isis & 1 & 1 & 1 + & & sis & .000 & .000 & .000 + 1000 & & lasso & .000 & .000 & .000 + & & isis & 1 & 1 & 1 +      now let us go back to the two simulation studies presented in section 3.3 .",
    "for each of them , we applied the technique of isis with scad and @xmath266 $ ] to select @xmath286 $ ] variables .",
    "after that , we estimated the @xmath287-vector @xmath3 by using scad .",
    "this method is referred to as isis - scad .",
    "we report in table 7 the median of the selected model sizes and median of the estimation errors @xmath197 in @xmath68-norm .",
    "we can see clearly that isis improves over the simple sis .",
    "the improvements are more drastic for simulation ii in which covariates are more correlated and the variable selections are more challenging .",
    "table 7 : simulations i and ii in section 3.3 revisited : medians of the selected + model sizes ( upper entry ) and the estimation errors ( lower entry ) +    [ c]ccccc & * simulation i & & + @xmath0 & * isis - scad & & & * isis - scad + 1000 & 13 & & @xmath288 & 11 + & 0.329 & & & 0.223 + & & & @xmath289 & 13.5 + & & & & 0.366 + 20000 & 31 & & & 27 + & 0.246 & & & 0.315 + * * *",
    "we introduce an asymptotic framework below and present the sure screening property for both sis and itrrs as well as the consistency of the sis based model selection methods sis - ds and sis - scad .",
    "recall from ( [ 011 ] ) that @xmath290 . throughout the paper",
    "we let @xmath291 be the true sparse model with nonsparsity size @xmath40 and define @xmath292 where @xmath293 and @xmath23 .",
    "clearly , the @xmath6 rows of the transformed design matrix @xmath294 are i.i.d .",
    "copies of @xmath24 which now has covariance matrix @xmath295 . for simplicity ,",
    "all the predictors @xmath296 are assumed to be standardized to have mean 0 and standard deviation 1 .",
    "note that the design matrix @xmath27 can be factored into @xmath297 . below we will make assumptions on @xmath294 and @xmath26 separately .",
    "we denote by @xmath298 and @xmath299 the largest and smallest eigenvalues of a matrix , respectively . for @xmath294 ,",
    "we are concerned with a concentration property of its extreme singular values as follows : + * concentration property : * the random matrix @xmath294 is said to have the concentration property if there exist some @xmath300 and @xmath301 such that the following deviation inequality @xmath302 holds for any @xmath303 submatrix @xmath304 of @xmath294 with @xmath305",
    ". we will call it property c for short .",
    "property c amounts to a distributional constraint on @xmath24 .",
    "intuitively , it means that with large probability the @xmath6 nonzero singular values of the @xmath303 matrix @xmath304 are of the same order , which is reasonable since @xmath306 will approach @xmath307 as @xmath308 : the larger the @xmath309 , the closer to @xmath307 .",
    "it relies on the random matrix theory ( rmt ) to derive the deviation inequality in ( [ 086 ] ) .",
    "in particular , property c holds when @xmath310 has a @xmath43-variate gaussian distribution ( see appendix a.7 ) .",
    "we conjecture that it should be shared by a wide class of spherically symmetric distributions . for studies on the extreme eigenvalues and limiting spectral distributions , see , e.g. , silverstein ( 1985 ) , bai and yin ( 1993 ) , bai ( 1999 ) , johnstone ( 2001 ) , and ledoux ( 2001 , 2005 ) .",
    "some of the assumptions below are purely technical and only serve to provide theoretical understanding of the newly proposed methodology .",
    "we have no intent to make our assumptions the weakest possible .    * condition 1 . * @xmath311 and @xmath312 for some @xmath313 , where @xmath314 is given by condition 3 .",
    "* condition 2 .",
    "* @xmath24 has a spherically symmetric distribution and property c. also , @xmath315 for some @xmath316 .",
    "* condition 3 .",
    "* @xmath317 and for some @xmath318 and @xmath319 , @xmath320    as seen later , @xmath314 controls the rate of probability error in recovering the true sparse model .",
    "although @xmath321 is assumed here to be bounded away from zero , our asymptotic study applies as well to the case where @xmath322 tends to zero as @xmath156 . in particular , when the variables in @xmath323 are uncorrelated , @xmath324",
    "this condition rules out the situation in which an important variable is marginally uncorrelated with @xmath228 , but jointly correlated with @xmath228 .",
    "* condition 4 .",
    "* there exist some @xmath325 and @xmath326 such that @xmath327 this condition rules out the case of strong collinearity .",
    "the largest eigenvalue of the population covariance matrix @xmath26 is allowed to diverge as @xmath6 grows . when there are many predictors , it is often the case that their covariance matrix is block diagonal or nearly block diagonal under a suitable permutation of the variables",
    ". therefore @xmath328 usually does not grow too fast with @xmath6 .",
    "in addition , condition 4 holds for the covariance matrix of a stationary time series ( see bickel and levina , 2004 , 2008 ) .",
    "see also grenander and szeg ( 1984 ) for more details on the characterization of extreme eigenvalues of the covariance matrix of a stationary process in terms of its spectral density",
    ".      analyzing the @xmath0-vector @xmath45 in ( [ 120 ] ) when @xmath311 is essentially difficult .",
    "the approach we took is to first study the specific case with @xmath329 and then relate the general case to the specific case .",
    "_ ( accuracy of sis ) .",
    "_ under conditions 14 , if @xmath330 then there exists some @xmath331 such that when @xmath332 with @xmath333 , we have for some @xmath334 , @xmath335    we should point out here that @xmath336 $ ] is implied by our assumptions as demonstrated in the technical proof .",
    "the above theorem shows that sis has the sure screening property and can reduce from exponentially growing dimension @xmath70 down to a relatively large scale @xmath52= o(n^{1-\\theta})<n$ ] for some @xmath337 , where the reduced model @xmath338 still contains all the variables in the true model with an overwhelming probability .",
    "in particular , we can choose the submodel size @xmath16 to be @xmath56 or @xmath339 for sis if conditions 1 - 4 are satisfied .",
    "another interpretation of theorem 1 is that it requires the model size @xmath340 = n^{\\theta^*}$ ] with @xmath341 in order to have the sure screening property .",
    "the weaker the signal , the larger the @xmath314 and hence the larger the required model size .",
    "similarly , the more severe the collinearity , the larger the @xmath342 and the larger the required model size . in this sense ,",
    "the restriction that @xmath343 is not needed , but @xmath344 is needed since we can not detect signals that of smaller order than root-@xmath6 consistent . in the former case , there is no guarantee that @xmath345 can be taken to be smaller than one .",
    "the proof of theorem 1 depends on the iterative application of the following theorem , which demonstrates the accuracy of each step of itrrs .",
    "we first describe the result of the first step of itrrs .",
    "it shows that as long as the ridge parameter @xmath82 is large enough and the percentage of remaining variables @xmath83 is large enough , the sure screening property is ensured with overwhelming probability .    _",
    "( asymptotic sure screening ) .",
    "_ under conditions 14 , if @xmath330 , @xmath346 , and @xmath347 as @xmath156 , then we have for some @xmath334 , @xmath348    the above theorem reveals that when the tuning parameters are chosen appropriately , with an overwhelming probability the submodel @xmath84 will contain the true model @xmath85 and its size is an order @xmath86 ( for some @xmath87 ) lower than the original one .",
    "this property stimulated us to propose itrrs .    _",
    "( accuracy of itrrs ) .",
    "_ let the assumptions of theorem 2 be satisfied . if @xmath349 as @xmath156 for some @xmath331 , then successive applications of the procedure in ( [ 039 ] ) for @xmath128 times results in a submodel @xmath350 with size @xmath93 < n$ ] such that for some @xmath334 , @xmath351    theorem 3 follows from iterative application of theorem 2 @xmath128 times , where @xmath128 is the first integer such that @xmath352 < n$ ] .",
    "this implies that @xmath353 .",
    "therefore , the accumulated error probability , from the union bound , is still of exponentially small with a possibility of a different constant @xmath354 .",
    "itrrs has now been shown to possess the sure screening property . as mentioned before",
    ", sis is a specific case of itrrs with an infinite regularization parameter and hence enjoys also the sure screening property .",
    "note that the number of steps in itrrs depends on the choice of @xmath96 .",
    "in particular , @xmath83 can not be too small , or equivalently , the number of iteration steps in itrrs can not be too large , due to the accumulation of the probability errors of missing some important variables over the iterations . in particular , the stepwise deletion method which deletes one variable each time in itrrs might not work since it requires @xmath97 steps of iterations , which may exceed the error bound in theorem 2 .      to study the property of the dantzig selector , candes and tao ( 2007 ) introduce the notion of uniform uncertainty principle ( uup ) on deterministic design matrices which essentially states that the design matrix obeys a  restricted isometry hypothesis .",
    "\" specifically , let @xmath207 be an @xmath117 deterministic design matrix and for any subset @xmath355 . denote by @xmath356 the @xmath357 submatrix of @xmath207 obtained by extracting its columns corresponding to the indices in @xmath358 . for any positive integer @xmath359 , the @xmath360-restricted isometry constant @xmath361 of @xmath207",
    "is defined to be the smallest quantity such that @xmath362 holds for all subsets @xmath358 with @xmath363 and @xmath364 . for any pair of positive integers",
    "@xmath365 with @xmath366 , the @xmath365-restricted orthogonality constant @xmath367 of @xmath207 is defined to be the smallest quantity such that @xmath368 holds for all disjoint subsets @xmath369 of cardinalities @xmath363 and",
    "@xmath370 , @xmath364 , and @xmath371 .",
    "the following theorem is obtained by the sure screening property of sis in theorem 1 along with theorem 1.1 in candes and tao ( 2007 ) , where @xmath372 for some @xmath316 . to avoid the selection bias in the prescreening step ,",
    "we can split the sample into two halves : the first half is used to screen variables and the second half is used to construct the dantzig estimator .",
    "the same technique applies to scad , but we avoid this step of detail for simplicity of presentation .    _",
    "( consistency of sis - ds ) .",
    "_ assume with large probability , @xmath373 and choose @xmath374 in ( [ 002 ] ) . then with large probability , we have @xmath375 where @xmath376 and @xmath209 is the number of nozero components of @xmath3 .",
    "this theorem shows that sis - ds , i.e. , sis followed by the dantzig selector , can now achieve the ideal risk up to a factor of @xmath168 with @xmath169 , rather than the original @xmath2 .",
    "now let us look at sis - scad , that is , sis followed by the scad . for simplicity , a common regularization parameter @xmath76",
    "is used for the scad penalty function .",
    "let @xmath377 be a minimizer of the scad - pls in ( [ 111 ] ) . the following theorem is obtained by the sure screening property of sis in theorem 1 along with theorems 1 and 2 in fan and peng ( 2004 ) .    _",
    "( oracle properties of sis - scad ) .",
    "_ if @xmath378 and the assumptions of theorem 2 in fan and peng ( 2004 ) be satisfied , then , with probability tending to one , the scad - pls estimator @xmath379 satisfies : ( i ) @xmath380 for any @xmath381 ; ( ii ) the components of @xmath382 in @xmath85 perform as well as if the true model @xmath85 were known .",
    "the sis - scad has been shown to enjoy the oracle properties .",
    "this paper studies the problem of high dimensional variable selection for the linear model .",
    "the concept of sure screening is introduced and a sure screening method based on correlation learning that we call the sure independence screening ( sis ) is proposed .",
    "the sis has been shown to be capable of reducing from exponentially growing dimensionality to below sample size accurately .",
    "it speeds up variable selection dramatically and can also improve the estimation accuracy when dimensionality is ultra high .",
    "sis combined with well - developed variable selection techniques including the scad , dantzig selector , lasso , and adaptive lasso provides a powerful tool for high dimensional variable selection .",
    "the tuning parameter @xmath16 can be taken as @xmath266 $ ] or @xmath383 , depending on which model selector is used in the second stage . for non - concave penalized least - squares ( [ jf1 ] ) , when one directly applies the lla algorithm to the original problem with @xmath384 , one needs initial values that are not readily available .",
    "sis provides a method that makes this feasible by screening many variables and furnishing the corresponding coefficients with zero . the initial value in ( [ jf1 ] )",
    "can be taken as the ols estimate if @xmath385 $ ] and zero [ corresponding to @xmath386 when @xmath383 , which is lasso .",
    "some extensions of sis have also been discussed . in particular , an iterative sis ( isis )",
    "is proposed to enhance the finite sample performance of sis , particularly in the situations where the technical conditions fail .",
    "this raises a challenging question : to what extent does isis relax the conditions for sis to have the sure screening property ?",
    "an iteratively thresholded ridge regression screener ( itrrs ) has been introduced to better understand the rationale of sis and serves as a technical device for proving the sure screening property . as a by - product , it is demonstrated that the stepwise deletion method may have no sure screening property when the dimensionality is of an exponential order .",
    "this raises another interesting question if the sure screening property holds for a greedy algorithm such as the stepwise addition or matching pursuit and how large the selected model has to be if it does .",
    "the paper leaves open the problem of extending the sis and isis introduced for the linear models to the family of generalized linear models ( glm ) and other general loss functions such as the hinge loss and the loss associated with the support vector machine ( svm ) .",
    "questions including how to define associated residuals to extend isis and whether the sure screening property continues to hold naturally arise .",
    "the paper focuses only on random designs which commonly appear in statistical problems , whereas for many problems in fields such as image analysis and signal processing the design matrices are often deterministic .",
    "it remains open how to impose a set of conditions that ensure the sure screening property .",
    "it also remains open if the sure screening property can be extended to the sparse additive model in nonparametric learning as studied by ravikumar _",
    "these questions are beyond the scope of the current paper and are interesting topics for future research .",
    "hereafter we use both @xmath354 and @xmath387 to denote generic positive constants for notational convenience .      motivated by the results in theorems 2 and 3 , the idea is to successively apply dimensionality reduction in a way described in ( [ 122 ] ) below .",
    "to enhance the readability , we split the whole proof into two mains steps and multiple substeps .    *",
    "step 1.*let @xmath79 .",
    "similarly to ( [ 039 ] ) , we define a submodel @xmath388 $ largest of all}\\right\\}.\\ ] ] we aim to show that if @xmath389 in such a way that @xmath390 as @xmath156 , we have for some @xmath334 , @xmath391    the main idea is to relate the general case to the specific case with @xmath329 , which is separately studied in sections a.4a.6 below . a key ingredient is the representation ( [ 089 ] ) below of the @xmath392 random matrix @xmath67 . throughout ,",
    "let @xmath393 and @xmath394 be a unit vector in @xmath395 with the @xmath396-th entry 1 and 0 elsewhere , @xmath397 .    since @xmath398",
    ", it follows from ( [ 088 ] ) that @xmath399 where @xmath400 are @xmath6 eigenvalues of @xmath401 , @xmath402 , and @xmath403 is uniformly distributed on the orthogonal group @xmath404 . by ( [ 011 ] ) and ( [ 120 ] ) , we have @xmath405 we will study the above two random vectors @xmath406 and @xmath407 separately .      _",
    "step 1.1.1 .",
    "bounding @xmath409 from above_.it is obvious that @xmath410 ^ 2i_n\\ ] ] and @xmath411 . these and ( [ 089 ] ) lead to @xmath412 ^ 2 { \\mbox{\\boldmath",
    "$ \\beta$}}\\t{\\mathbf{\\sigma}}^{1/2}\\widetilde{{\\mbox{\\bf u}}}\\t\\widetilde{{\\mbox{\\bf u}}}{\\mathbf{\\sigma}}^{1/2}{\\mbox{\\boldmath $ \\beta$}}.\\end{aligned}\\ ] ]    let @xmath413 such that @xmath414 .",
    "then , it follows from lemma 1 that @xmath415 where we use the symbol @xmath416 to denote being identical in distribution for brevity . by condition 3 , @xmath417 , and",
    "thus by lemma 4 , we have for some @xmath334 , @xmath418 since @xmath419 and @xmath420 by conditions 2 and 4 , ( [ 093 ] ) and ( [ 094 ] ) along with bonferroni s inequality yield @xmath421    _ step 1.1.2 . bounding @xmath422 ,",
    "@xmath423 , from below_.this needs a delicate analysis .",
    "now fix an arbitrary @xmath424 . by ( [ 089 ] ) , we have @xmath425 note that @xmath426 , @xmath427 . by condition 3",
    ", there exists some @xmath333 such that @xmath428 thus , there exists @xmath413 such that @xmath429 and @xmath430 since @xmath431 is independent of @xmath432 by lemma 1 and the uniform distribution on the orthogonal group @xmath404 is invariant under itself , it follows that @xmath433 where @xmath434 .",
    "we will examine the above two terms @xmath435 and @xmath436 separately .",
    "clearly , @xmath437 and thus by condition 2 , lemma 4 , and bonferroni s inequality , we have for some @xmath333 and @xmath334 , @xmath438 this along with ( [ 098 ] ) gives for some @xmath333 , @xmath439    similarly to step 1.1.1 , it can be shown that @xmath440 since @xmath431 is independent of @xmath432 by lemma 1 , the argument in the proof of lemma 5 applies to show that the distribution of @xmath441 is invariant under the orthogonal group @xmath442 .",
    "then , it follows that @xmath443 , where @xmath444 , independent of @xmath445 .",
    "thus , we have @xmath446 in view of ( [ 101 ] ) , ( [ 102 ] ) , and @xmath447 , applying the argument in the proof of lemma 5 gives for some @xmath333 , @xmath448 where @xmath449 is a @xmath34-distributed random variable .",
    "let @xmath450 .",
    "then , by the classical gaussian tail bound , we have @xmath451 which along with ( [",
    "103])and bonferroni s inequality shows that @xmath452 therefore , by bonferroni s inequality , combining ( [ 099 ] ) , ( [ 104 ] ) , and ( [ 105 ] ) together gives for some @xmath333 , @xmath453      _ step 1.2.1 .",
    "bounding @xmath455 from above_.clearly , we have @xmath456 then , it follows that @xmath457 from condition 2 , we know that @xmath458 are i.i.d . @xmath459-distributed random variables .",
    "thus , by ( [ 004 ] ) in lemma 3 , there exist some @xmath333 and @xmath334 such that @xmath460 which along with ( [ 106 ] ) , conditions 2 and 4 , and bonferroni s inequality yields @xmath461    _ step 1.2.2 . bounding @xmath462 from above_.given that @xmath463 , @xmath464 .",
    "hence , @xmath465 with @xmath466 let @xmath467 be the event @xmath468 for some @xmath333 . then , using the same argument as that in step 1.1.1 , we can easily show that for some @xmath334 , @xmath469 on the event @xmath467 , we have @xmath470 where @xmath449 is a @xmath34-distributed random variable .",
    "thus , it follows from ( [ 108 ] ) and ( [ 058 ] ) that @xmath471      _ step 1.3_.finally , we combine the results obtained in steps 1.1 and 1.2 together . by bonferroni s inequality , it follows from ( [ 041 ] ) , ( [ 095 ] ) , ( [ 155 ] ) , ( [ 107 ] ) , and ( [ 109 ] ) that for some constants @xmath475 , @xmath476 this shows that with overwhelming probability @xmath477 , the magnitudes of @xmath478 , @xmath424 , are uniformly at least of order @xmath479 and more importantly , for some @xmath333 , @xmath480 where @xmath481 denotes the number of elements in a set .      *",
    "step 2.*fix an arbitrary @xmath483 and choose a shrinking factor @xmath482 of the form @xmath484 , for some integer @xmath485 .",
    "we successively perform dimensionality reduction until the number of remaining variables drops to below sample size @xmath6 :    * first , carry out the procedure in ( [ 122 ] ) to the full model @xmath486 and get a submodel @xmath487 with size @xmath89 $ ] ; * then , apply a similar procedure to the model @xmath487 and again obtain a submodel @xmath488 with size @xmath489 $ ] , and so on ; * finally , get a submodel @xmath490 with size @xmath93=[\\delta ^rn]<n$ ] , where @xmath94=[\\delta ^{r-1}n]>n$ ]",
    ".      now fix an arbitrary @xmath493 and pick some @xmath494 very close to 1 such that @xmath495 .",
    "we choose a sequence of integers @xmath485 in a way such that @xmath496 where @xmath497 .",
    "then , applying the above scheme of dimensionality reduction results in a submodel @xmath498 , where @xmath499 satisfies @xmath500    before going further , let us make two important observations .",
    "first , for any principal submatrix @xmath501 of @xmath26 corresponding to a subset of variables , condition 4 ensures that @xmath502 second , by definition , property c in ( [ 086 ] ) holds for any @xmath303 submatrix @xmath304 of @xmath294 with @xmath305 , where @xmath503 is some constant .",
    "thus , the probability bound in ( [ 123 ] ) is uniform over dimension @xmath504 $ ] .",
    "therefore , for some @xmath334 , by ( [ 125 ] ) and ( [ 123 ] ) we have in each step @xmath505 of the above dimensionality reduction , @xmath506 which along with bonferroni s inequality gives @xmath507    it follows from ( [ 125 ] ) that @xmath508 , which is of order @xmath509 by condition 1 .",
    "thus , a suitable increase of the constant @xmath334 in ( [ 127 ] ) yields @xmath510 finally , in view of ( [ 126 ] ) , the above probability bound holds for any @xmath332 , with @xmath331 and @xmath333 .",
    "this completes the proof .",
    "one observes that ( [ 039 ] ) uses only the order of componentwise magnitudes of @xmath75 , so it is invariant under scaling .",
    "therefore , in view of ( [ 096 ] ) we see from step 1 of the proof of theorem 1 that theorem 2 holds for sufficiently large regularization parameter @xmath76 .    it remains to specify a lower bound on @xmath76 .",
    "now we rewrite the @xmath0-vector @xmath511 as @xmath512{\\mbox{\\boldmath $ \\omega$}}.\\ ] ] let @xmath513{\\mbox{\\boldmath $ \\omega$}}$ ] .",
    "it follows easily from @xmath514 that @xmath515 and thus @xmath516 ^ 2\\left\\|{\\mbox{\\boldmath $ \\omega$}}\\right\\|^2\\\\ & \\leq\\left[\\lambda_{\\text{max}}(\\lambda ^{-1 } { \\mbox{\\bf x}}\\t{\\mbox{\\bf x}})\\right]^2\\left\\|{\\mbox{\\boldmath $ \\omega$}}\\right\\|^2\\\\ & \\leq \\lambda",
    "^{-2}p ^2\\left[\\lambda_{\\text{max}}(p ^{-1}{\\mbox{\\bf z}}{\\mbox{\\bf z}}\\t)\\right]^2 \\left[\\lambda_{\\text{max}}({\\mathbf{\\sigma}})\\right]^2\\left\\|{\\mbox{\\boldmath $ \\omega$}}\\right\\|^2,\\end{aligned}\\ ] ] which along with ( [ 119 ] ) , conditions 2 and 4 , and bonferroni s inequality shows that @xmath517 again , by bonferroni s inequality and ( [ 119 ] ) , any @xmath76 satisfying @xmath518 can be used . note that @xmath519 by assumption . so",
    "in particular , we can choose any @xmath76 satisfying @xmath520 as @xmath156 .        throughout sections a.4a.6 below",
    ", we assume that @xmath311 and the distribution of @xmath24 is continuous and spherically symmetric , that is , invariant under the orthogonal group @xmath404 . for brevity , we use @xmath521 to denote the probability law or distribution of the random variable indicated . let @xmath522 be the centered sphere with radius @xmath523 in @xmath287-dimensional euclidean space @xmath524 .",
    "in particular , @xmath525 is referred to as the unit sphere in @xmath524 .",
    "[ [ the - distribution - of - mboxbf - s - leftmboxbf - ztmboxbf - zrightmboxbf - ztmboxbf - z ] ] the distribution of @xmath393 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    it is a classical fact that the orthogonal group @xmath404 is compact and admits a probability measure that is invariant under the action of itself , say , @xmath526 this invariant distribution is referred to as the uniform distribution on the orthogonal group @xmath404 .",
    "we often encounter projection matrices in multivariate statistical analysis .",
    "in fact , the set of all @xmath392 projection matrices of rank @xmath6 can equivalently be regarded as the grassmann manifold @xmath527 of all @xmath6-dimensional subspaces of the euclidean space @xmath395 ; throughout , we do not distinguish them and write @xmath528    it is well known that the grassmann manifold @xmath527 is compact and there is a natural @xmath404-action on it , say , @xmath529 clearly , this group action is transitive , i.e. for any @xmath530 , there exists some @xmath531 such that @xmath532 .",
    "moreover , @xmath527 admits a probability measure that is invariant under the @xmath404-action defined above . this invariant distribution is referred to as the uniform distribution on the grassmann manifold @xmath527 . for more on group action and invariant measures on special manifolds , see eaton ( 1989 ) and chikuse ( 2003 ) .",
    "the uniform distribution on the grassmann manifold is not easy to deal with directly .",
    "a useful fact is that the uniform distribution on @xmath527 is the image measure of the uniform distribution on @xmath404 under the mapping @xmath533    by the assumption that @xmath24 has a continuous distribution , we can easily see that with probability one , the @xmath534 matrix @xmath294 has full rank @xmath6 .",
    "let @xmath535 be its @xmath6 singular values .",
    "then , @xmath294 admits a singular value decomposition @xmath536 where @xmath537 , @xmath538 , and @xmath539 is an @xmath534 diagonal matrix whose diagonal elements are @xmath535 , respectively .",
    "thus , @xmath540 and its moore - penrose generalized inverse is @xmath541 where @xmath542 .",
    "therefore , we have the following decomposition , @xmath543    from ( [ 087 ] ) , we know that @xmath544 , and thus @xmath545 by the assumption that @xmath546 is invariant under the orthogonal group @xmath404 , the distribution of @xmath294 is also invariant under @xmath404 , i.e. , @xmath547 thus , conditional on @xmath548 and @xmath549 , the conditional distribution of @xmath550 is invariant under @xmath404 , which entails that @xmath551 where @xmath432 is uniformly distributed on the orthogonal group @xmath404 . in particular , we see that @xmath552 is independent of @xmath553 . therefore , these facts along with ( [ 015 ] ) yield the following lemma .",
    "@xmath554 and @xmath549 is independent of @xmath555 , where @xmath432 is uniformly distributed on the orthogonal group @xmath404 and @xmath556 are @xmath6 eigenvalues of @xmath557 .",
    "moreover , @xmath558 is uniformly distributed on the grassmann manifold @xmath527 .",
    "[ [ deviation - inequality - on - leftlanglemboxbf - smboxbf - e_1mboxbf - e_1rightrangle ] ] deviation inequality on @xmath559 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    @xmath560 , where @xmath561 and @xmath562 are two independent @xmath563-distributed random variables with degrees of freedom @xmath6 and @xmath564 , respectively , that is , @xmath559 has a beta distribution with parameters @xmath565 and @xmath566 .",
    "lemma 1 gives @xmath567 , where @xmath403 is uniformly distributed on @xmath404 .",
    "clearly , @xmath568 is a random vector on the unit sphere @xmath569 .",
    "it can be shown that @xmath570 is uniformly distributed on the unit sphere @xmath571 .",
    "\\(i ) recall that the moment generating function of a @xmath459-distributed random variable @xmath582 is @xmath583 thus , for any @xmath576 and @xmath584 , by chebyshev s inequality ( see , e.g. van der vaart and wellner , 1996 ) we have @xmath585 where @xmath586 . setting the derivative @xmath587 to zero",
    "gives @xmath588 , where @xmath589 attains the maximum @xmath590/2 $ ] , @xmath576 .",
    "therefore , we have @xmath591 this proves ( [ 004 ] ) .        from lemma 3 , we know",
    "that @xmath601 where @xmath582 is @xmath561-distributed and @xmath602 is @xmath603-distributed .",
    "note that @xmath604 and @xmath605 are increasing in @xmath283 and have the same range @xmath606 . for any @xmath334",
    ", it follows from the proof of lemma 3 that there exist @xmath607 and @xmath608 with @xmath609 , such that @xmath610 and @xmath611 .",
    "now define @xmath612 let @xmath613 and @xmath614 .",
    "then , it can easily be shown that @xmath615 it follows from ( [ 004 ] ) and ( [ 005 ] ) and the choice of @xmath607 and @xmath608 above that @xmath616 therefore , by @xmath617 and bonferroni s inequality , the results follow from ( [ 010 ] ) and ( [ 016 ] ) .    [ [ deviation - inequality - on - leftlanglemboxbf - smboxbf - e_1mboxbf - e_2rightrangle ] ] deviation inequality on @xmath618 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    let @xmath619 . then , given that the first coordinate @xmath620 , the random vector @xmath621 is uniformly distributed on the sphere @xmath622 . moreover",
    ", for any @xmath334 , there exists some @xmath503 such that @xmath623 where @xmath449 is an independent @xmath34-distributed random variable .    in view of ( [ 015 ] ) , it follows that @xmath624 where @xmath625 . for any @xmath626 ,",
    "let @xmath627 .",
    "thus , by lemma 1 , we have @xmath628 this shows that given @xmath620 , the conditional distribution of @xmath621 is invariant under the orthogonal group @xmath442 .",
    "therefore , given @xmath620 , the random vector @xmath621 is uniformly distributed on the sphere @xmath622 .",
    "let @xmath629 be i.i.d .",
    "@xmath34-distributed random variables , independent of @xmath630 .",
    "conditioning on @xmath630 , we have @xmath631 let @xmath334 be a constant . from the proof of lemma 4 , we know that there exists some @xmath632 such that @xmath633 it follows from ( [ 005 ] ) that there exists some @xmath634 such that @xmath635 since @xmath311 .",
    "let @xmath636 .",
    "then , by @xmath637 and bonferroni s inequality , ( [ 020 ] ) follows immediately from ( [ 022])([019 ] ) .      in this section",
    ", we check property c in ( [ 086 ] ) for gaussian distributions .",
    "assume @xmath310 has a @xmath70-variate gaussian distribution .",
    "then , the @xmath534 design matrix @xmath638 and @xmath639 i.e. , all the entries of @xmath294 are i.i.d . @xmath34 random variables , where the symbol @xmath640 denotes the kronecker product of two matrices . we will invoke results in the random matrix theory on extreme eigenvalues of random matrices in gaussian ensemble .    before proceeding ,",
    "let us make two simple observations .",
    "first , in studying singular values of @xmath294 , the role of @xmath6 and @xmath70 is symmetric .",
    "second , when @xmath311 , by letting @xmath641 , independent of @xmath294 , and @xmath642 then the extreme singular values of @xmath294 are sandwiched by those of @xmath304 .",
    "therefore , a combination of lemmas 6 and 7 below immediately implies property c in ( [ 086 ] ) .",
    "let @xmath617 and @xmath643 .",
    "then , there exists some @xmath334 such that for any eigenvalue @xmath82 of @xmath401 and any @xmath644 , @xmath645 moreover , for each @xmath82 , the same inequality holds for a median of @xmath646 instead of the mean .",
    "the first result follows directly from geman ( 1980 ) : @xmath650 for the smallest eigenvalue , it is well known that ( see , e.g. , silverstein , 1985 or bai , 1999 ) @xmath651 this and fatou s lemma entails the second result .",
    "bickel , p. j. and levina , e. ( 2004 ) . some theory for fisher s linear discriminant function ,  naive bayes \" , and some alternatives when there are many more variables than observations .",
    "_ bernoulli _ * 10 * , 9891010 .                                    fan , j. and li , r. ( 2006 ) . statistical challenges with high dimensionality : feature selection in knowledge discovery .",
    "_ proceedings of the international congress of mathematicians _ ( m. sanz - sole , j. soria , j.l .",
    "varona , j. verdera , eds . ) vol .",
    "iii , 595622 .",
    "golub , t. r. , slonim , d. k. , tamayo , p. , huard , c. , gaasenbeek , m. , mesirov , j. p. , coller , h. , loh , m. l. , downing , j. r. , caligiuri , m. a. , bloomfield , c. d. and lander , e. s. ( 1999 ) .",
    "molecular classification of cancer : class discovery and class prediction by expression monitoring .",
    "_ science _ * 286 * , 531537 ."
  ],
  "abstract_text": [
    "<S> variable selection plays an important role in high dimensional statistical modeling which nowadays appears in many areas and is key to various scientific discoveries . for problems of large scale or dimensionality @xmath0 , estimation accuracy and computational cost are two top concerns . in a recent paper , candes and tao ( 2007 ) </S>",
    "<S> propose the dantzig selector using @xmath1 regularization and show that it achieves the ideal risk up to a logarithmic factor @xmath2 . </S>",
    "<S> their innovative procedure and remarkable result are challenged when the dimensionality is ultra high as the factor @xmath2 can be large and their uniform uncertainty principle can fail .    </S>",
    "<S> motivated by these concerns , we introduce the concept of sure screening and propose a sure screening method based on a correlation learning , called the sure independence screening ( sis ) , to reduce dimensionality from high to a moderate scale that is below sample size . in a fairly general asymptotic framework , the correlation learning is shown to have the sure screening property for even exponentially growing dimensionality . as a methodological extension , an iterative sis ( isis ) </S>",
    "<S> is also proposed to enhance its finite sample performance . with dimension reduced accurately from high to below sample size , variable selection can be improved on both speed and accuracy , and can then be accomplished by a well - developed method such as the scad , dantzig selector , lasso , or adaptive lasso . </S>",
    "<S> the connections of these penalized least - squares methods are also elucidated .    </S>",
    "<S> _ short title _ : sure independence screening + _ ams 2000 subject classifications _ : primary 62j99 ; secondary 62f12 + _ keywords _ : variable selection , dimensionality reduction , sis , sure screening , oracle estimator , scad , dantzig selector , lasso , adaptive lasso </S>"
  ]
}