{
  "article_text": [
    "we consider the classical problem of multiclass classification , where given an instance @xmath4 , the goal is to predict the most likely label @xmath5 , according to some unknown probability distribution .",
    "a common general approach to multiclass learning is to reduce a multiclass problem to a set of binary classification problems  @xcite .",
    "this approach is composable with any binary learning algorithm , including online algorithms , bayesian algorithms , and even humans .",
    "a key technique for analyzing reductions is _ regret analysis _ , which bounds the regret of the resulting multiclass classifier in terms of the average classification regret on the induced binary problems . here",
    "( formally defined in section  [ notation ] ) is the difference between the incurred loss and the smallest achievable loss on the problem , i.e. , excess loss due to suboptimal prediction .",
    "the most commonly applied reduction is one - against - all , which creates a binary classification problem for each of the @xmath0 classes .",
    "the classifier for class @xmath6 is trained to predict whether the label is @xmath6 or not ; predictions are then done by evaluating each binary classifier and randomizing over those that predict `` yes , '' or over all labels if all answers are `` no '' .",
    "this simple reduction is _ inconsistent _ , in the sense that given optimal ( zero - regret ) binary classifiers , the reduction may not yield an optimal multiclass classifier in the presence of noise .",
    "optimizing squared loss of the binary predictions instead of the @xmath7 loss makes the approach consistent , but the resulting multiclass regret scales as @xmath8 in the worst case , where @xmath9 is the average squared loss regret on the induced problems .",
    "the probing reduction  @xcite upper bounds @xmath9 by the average binary classification regret .",
    "this composition gives a consistent reduction to binary classification , but it has a square root dependence on the binary regret ( which is undesirable as regrets are between 0 and 1 ) .    the probabilistic error - correcting output code approach ( pecoc )  @xcite reduces @xmath0-class classification to learning @xmath10 regressors on the interval @xmath11 $ ] , creating @xmath10 binary examples per multiclass example at both training and test time , with a test time computation of @xmath12 .",
    "the resulting multiclass regret is bounded by @xmath13 , removing the dependence on the number of classes @xmath0 .",
    "when only a constant number of labels have non - zero probability given features , the computation can be reduced to @xmath14 per example  @xcite .",
    "this state of the problem raises several questions :    1 .",
    "is there a consistent reduction from multiclass to binary classification that does not have a square root dependence on @xmath9  @xcite ?",
    "for example , an average binary regret of just @xmath15 may imply a pecoc multiclass regret of @xmath16 .",
    "2 .   is there a consistent reduction that requires just @xmath1 computation , matching the information theoretic lower bound ?",
    "+ the well - known @xmath1 tree reduction distinguishes between the labels using a balanced binary tree , with each non - leaf node predicting `` is the correct multiclass label to the left or not ? ''  @xcite . as shown in section  [ tree ] , this method is inconsistent .",
    "3 .   can the above be achieved with a reduction that only performs pairwise comparisons between classes ?",
    "+ one fear associated with the pecoc approach is that it creates binary problems of the form `` what is the probability that the label is in a given random subset of labels ? , '' which may be hard to solve .",
    "although this fear is addressed by regret analysis ( as the latter operates only on avoidable , excess loss ) , and is overstated in some cases  @xcite , it is still of some concern , especially with larger values of @xmath0 .",
    "the error - correcting tournament family presented here answers all of these questions in the affirmative .",
    "it provides an exponentially faster in @xmath0 method for multiclass prediction with the resulting multiclass regret bounded by @xmath17 , where @xmath9 is the average binary regret ; and every binary classifier logically compares two distinct class labels .",
    "the result is based on a basic observation that if a non - leaf node fails to predict its binary label , which may be unavoidable due to noise in the distribution , nodes between this node and the root should have no preference for class label prediction . utilizing this observation",
    ", we construct a reduction , called the _ filter tree _ , which uses a @xmath1 computation per multiclass example at both training and test time , and",
    "whose multiclass regret is bounded by @xmath18 times the average binary regret .",
    "the decision process of a filter tree , viewed bottom up , can be viewed as a single - elimination tournament on a set of @xmath0 players . using multiple independent single - elimination tournaments is of no use as it does not affect the _ average _ regret of an adversary controlling the binary classifiers .",
    "somewhat surprisingly , it is possible to have @xmath18 complete single - elimination tournaments between @xmath0 players in @xmath1 rounds , with no player playing twice in the same round . an _ error - correcting tournament _ , first pairs labels in such simultaneous single - elimination tournaments , followed by a final carefully weighted single - elimination tournament that decides among the @xmath18 winners of the first phase . as for the filter tree ,",
    "test time evaluation can start at the root and proceed to a multiclass label with @xmath1 computation .",
    "this construction is also useful for the problem of robust search , yielding the first algorithm which allows the adversary to err a constant fraction of the time in the `` full lie '' setting  @xcite , where a comparator can missort any comparison .",
    "previous work either applied to the `` half lie '' case where a comparator can fail to sort but can not actively missort  @xcite , or to a `` full lie '' setting where an adversary has a fixed known bound on the number of lies  @xcite or a fixed budget on the fraction of errors so far  @xcite .",
    "indeed , it might even appear impossible to have an algorithm robust to a constant fraction of full lie errors since an error can always be reserved for the last comparison .",
    "repeating the last comparison @xmath1 times defeats this strategy .",
    "the result here is also useful for the actual problem of tournament construction in games with real players .",
    "our analysis does not assume that errors are _ i.i.d .",
    "_  @xcite , or have known noise distributions  @xcite or known outcome distributions given player skills  @xcite . consequently , the tournaments we construct are robust against severe bias such as a biased referee or some forms of bribery and collusion .",
    "furthermore , the tournaments we construct are shallow , requiring fewer rounds than @xmath19-elimination bracket tournaments , which do not satisfy the guarantee provided here . in an @xmath19-_elimination bracket tournament _ , bracket @xmath6 is a single - elimination tournament on all players except the winners of brackets @xmath20 .",
    "after the bracket winners are determined , the player winning the last bracket @xmath19 plays the winner of bracket @xmath21 repeatedly until one player has suffered @xmath19 losses ( they start with @xmath21 and @xmath22 losses respectively ) .",
    "the winner moves on to pair against the winner of bracket @xmath22 , and the process continues until only one player remains .",
    "this method does not scale well to large @xmath19 , as the final elimination phase takes @xmath23 rounds . even for @xmath24 and @xmath25 ,",
    "our constructions have smaller maximum depth than bracketed @xmath26-elimination .",
    "to see that the bracketed @xmath19-elimination tournament does not satisfy our goal , note that the second - best player could defeat the first player in the first single elimination tournament , and then once more in the final elimination phase to win , implying that an adversary need control only two matches .",
    "[ [ paper - overview ] ] paper overview + + + + + + + + + + + + + +    we begin by defining the basic concepts and introducing some of the notation in section  [ notation ] .",
    "section  [ tree ] shows that the simple divide - and - conquer tree approach is inconsistent , motivating the filter tree algorithm described in section  [ s : algorithm ] ( which applies to more general cost - sensitive multiclass problems ) .",
    "section  [ s : analysis ] proves that the algorithm has the best possible computational dependence , and gives two upper bounds on the regret of the returned ( cost - sensitive ) multiclass classifier .",
    "subsection  [ s : experiments ] presents some experimental evidence that the filter tree is indeed a practical approach for multiclass classification .",
    "section  [ sec : multi - elimination ] presents the error - correcting tournament family parametrized by an integer @xmath27 , which controls the tradeoff between maximizing robustness ( @xmath19 large ) and minimizing depth ( @xmath19 small ) .",
    "setting @xmath28 gives the filter tree , while @xmath29 gives a ( multiclass to binary ) regret ratio of @xmath30 with @xmath1 depth .",
    "setting @xmath31 gives regret ratio of @xmath32 with depth @xmath10 .",
    "the results here provide a nearly free generalization of earlier work  @xcite in the robust search setting , to a more powerful adversary that can missort as well as fail to sort .",
    "section  [ sec : lb ] gives an algorithm independent lower bound of 2 on the regret ratio for large @xmath0 .",
    "when the number of calls to a binary classifier is independent ( or nearly independent ) of the label predicted , we strengthen this lower bound to @xmath26 for large @xmath0 .",
    "let @xmath33 be the underlying distribution over @xmath34 , where @xmath35 is some observable feature space and @xmath36 is the label space .",
    "the _ error rate _ of a classifier @xmath37 on @xmath33 is given by @xmath38.\\ ] ] the _ multiclass regret _ of @xmath39 on @xmath33 is defined as @xmath40 the algorithms here extend to the _ cost - sensitive _ case , where the underlying distribution @xmath33 is over @xmath41^{k}$ ] .",
    "the _ expected cost _ of a classifier @xmath42 on @xmath33 is @xmath43.\\ ] ] here @xmath44^{k}$ ] gives the cost of each of the @xmath0 choices for @xmath45 . as in the multiclass case ,",
    "the _ cost - sensitive regret _ of @xmath39 on @xmath33 is defined as @xmath46",
    "one standard approach for reducing multiclass learning to binary learning is to split the set of labels in half , learn a binary classifier to distinguish between the two subsets , and repeat recursively until each subset contains one label .",
    "multiclass predictions are made by following a chain of classifications from the root down to the leaves .",
    "this tree reduction transforms @xmath33 into a distribution @xmath47 over binary labeled examples by drawing a multiclass example @xmath48 from @xmath33 and a random non - leaf node @xmath6 , and outputting instance @xmath49 with label 1 if @xmath50 is in the left subtree of node @xmath6 , and 0 otherwise .",
    "a binary classifier @xmath39 for this induced problem gives a multiclass classifier @xmath51 , via a chain of binary predictions starting from the root .",
    "the following theorem gives an example of a multiclass problem such that even if we have an optimal classifier for the induced binary problem at each node , the tree reduction does not yield an optimal multiclass predictor .    for all @xmath52 , for all binary trees over the labels , there exists a multiclass distribution @xmath33 such that @xmath53 for any @xmath54 .",
    "find a node with one subset corresponding to two labels and the other subset corresponding to a single label .",
    "( if the tree is perfectly balanced , simply let @xmath33 assign probability 0 to one of the labels . )",
    "since we can freely rename labels without changing the underlying problem , let the first two labels be @xmath55 and @xmath56 , and the third label be @xmath26 .",
    "fix any @xmath57 . choose @xmath33 with the property that labels @xmath55 and @xmath56 each have a @xmath58 chance of",
    "being drawn given @xmath45 , and label @xmath26 is drawn with the remaining probability of @xmath59 . under this distribution ,",
    "the fraction of examples for which label @xmath55 or @xmath56 is correct is @xmath60 , so any minimum error rate binary predictor must choose either label @xmath55 or label @xmath56 .",
    "each of these choices has an error rate of @xmath61 .",
    "the optimal multiclass predictor chooses label @xmath26 and suffers an error rate of @xmath62 , implying that the regret of the tree classifier based on the optimal binary classifier is @xmath63 , which is strictly greater than 0 as @xmath64 .",
    "filter tree .",
    "each node predicts whether the left or the right input label is more likely , conditioned on a given @xmath65 .",
    "the root node predicts the best label for @xmath45 . ]",
    "the filter tree algorithm , illustrated by figure  [ fig : filter - tree ] , is equivalent to a single - elimination tournament on the set of labels , structured as a binary tree @xmath66 over the labels . in the first round ,",
    "the labels are paired according to the lowest level of the tree , and a classifier is trained for each pair to predict which of the two labels is more likely .",
    "( the labels that do nt have a pair in a given round , win that round for free . )",
    "the winning labels from the first round are in turn paired in the second round , and a classifier is trained to predict whether the winner of one pair is more likely than the winner of the other .",
    "the process of training classifiers to predict the best of a pair of winners from the previous round is repeated until the root classifier is trained .",
    "the key trick in the training stage ( algorithm  [ alg : filter ] ) is to form the right training set at each interior node .",
    "we use @xmath67 to denote the subtree of @xmath66 rooted at node @xmath68 , and @xmath69 to denote the set of leaves in the tree @xmath66 . a training example for node @xmath68",
    "is formed conditioned on the predictions of classifiers in the round before it .",
    "thus the learned classifiers from the first level of the tree are used to `` filter '' the distribution over examples reaching the second level of the tree .    given @xmath45 and classifiers at each node ,",
    "every edge in @xmath66 is identified with a unique label .",
    "the optimal decision at any non - leaf node is to choose the input edge ( label ) that is more likely according to the true conditional probability .",
    "this can be done by using the outputs of classifiers in the round before it as a filter during the training process : for each observation , we set the label to 0 if the left parent s output matches the multiclass label , 1 if the right parent s output matches , and reject the example otherwise .",
    "define @xmath70 if label @xmath50 is in the left subtree of node @xmath71 ; otherwise @xmath72 .",
    "+ set @xmath73 add @xmath74 to @xmath75 let @xmath76 @xmath77    set @xmath73 [ line3 ] let @xmath78 and @xmath79 be the two classes input to @xmath68 @xmath80-.18 in [ line5 ] let @xmath76 @xmath81    algorithm  [ alg : cs - filter - tree ] extends this idea to the cost - sensitive multiclass case where each choice has a different associated cost , as defined in section  [ notation ] .",
    "the algorithm relies upon an _ importance weighted _ binary learning algorithm ` learn ` , which takes examples of the form @xmath82 , where @xmath4 is a feature vector used for prediction , @xmath83 is a binary label , and @xmath84 is the importance any classifier pays if it does nt predict @xmath50 on @xmath45 . the importance weighted problem can be further reduced to binary classification using the costing reduction  @xcite , which alters the underlying distribution using rejection sampling on the importances .",
    "this is the reduction we use here .",
    "the testing algorithm is the same for both multiclass and cost - sensitive variants , and is very simple : given a test example @xmath4 , we output the label @xmath50 such that every classifier on the path from the root to @xmath50 prefers @xmath50 .",
    "before analyzing the regret of the algorithm , we note its computational characteristics .      since the algorithm is a reduction , we count the computational complexity of the reduction itself , assuming that oracle calls take unit time .",
    "algorithm  [ alg : filter ] requires @xmath1 computation per multiclass example , by searching for the correct leaf in @xmath1 time , then filtering back toward the root .",
    "this matches the information theoretic lower bound since simply reading one of @xmath0 labels requires @xmath85 bits .",
    "algorithm  [ alg : cs - filter - tree ] requires @xmath10 computation per cost - sensitive example , because there are @xmath86 nodes , each requiring constant computation per example .",
    "since any method must read the @xmath0 costs , this bound is tight .",
    "testing requires @xmath1 computation per example to descend a binary tree .",
    "any method must write out @xmath87 bits to specify its prediction .",
    "algorithm  [ alg : cs - filter - tree ] transforms each cost - sensitive multiclass example ( line  [ line3 ] ) into importance weighted binary labeled examples ( line  [ line5 ] ) , one for every non - leaf node @xmath68 in the tree .",
    "this process implicitly transforms the underlying distribution @xmath33 over cost - sensitive multiclass examples into a distribution @xmath88 over importance weighted binary examples at each @xmath68 .",
    "we can further reduce from importance weighted binary classification to binary classification using the costing reduction  @xcite , which alters each @xmath88 using rejection sampling on the importance weights .",
    "this composition further transforms @xmath88 into a distribution @xmath89 over binary examples .",
    "let @xmath90 be a classifier for the binary classification problem induced at node @xmath68 .",
    "the relevant quantity is the _ average binary regret _ ,",
    "@xmath91 where @xmath92 , and @xmath93 is the importance weight formed in line  [ line5 ] of algorithm  [ alg : cs - filter - tree ] ( the difference in cost between the two labels that node @xmath68 chooses between on @xmath45 ) .",
    "this quantity , which is just the average _ importance weighted _ binary regret of @xmath90 on @xmath88 , is induced by the reduction ( algorithm  [ alg : cs - filter - tree ] ) .",
    "the core theorem below relates @xmath94 to the regret of the resulting cost - sensitive classifier @xmath51 on @xmath33 .",
    "again , given a test example @xmath4 , the classifier @xmath51 returns the unique label @xmath50 such that every @xmath90 on the path from the root to @xmath50 prefers @xmath50 .",
    "this type of analysis is similar to boosting : at each round @xmath68 , the booster creates an input distribution @xmath88 and calls a weak learning algorithm to obtain a classifier @xmath90 , which has some error rate on @xmath88 .",
    "the distribution @xmath88 depends on the classifiers returned by the oracle in previous rounds .",
    "the accuracy of the final classifier on the original distribution @xmath33 is analyzed in terms of these error rates .",
    "[ main ] for all binary classifiers @xmath39 and all cost - sensitive multiclass distributions @xmath33",
    ", @xmath95 where @xmath92 , and @xmath93 is the importance weight formed in line  [ line5 ] of algorithm  [ alg : cs - filter - tree ] ( the difference in cost between the two labels that node @xmath68 chooses between on @xmath45 ) .    before proving the theorem , we state the corollary for multiclass classification .",
    "[ cor : multi ] for all binary classifiers @xmath39 and multiclass distributions @xmath33 , @xmath96 where @xmath97 is the depth of the tree @xmath66 .",
    "since all importance weights are either 0 or 1 , we do nt need to apply costing in the multiclass case .",
    "the proof of the corollary given the theorem is simple since for any @xmath48 , the induced @xmath98 has at most one node per level with induced importance weight 1 ; all other importance weights are 0",
    ". therefore , @xmath99 .",
    "theorem  [ bound ] provides an alternative bound for cost - sensitive classification .",
    "it is the first known bound giving a worst - case dependence of less than @xmath0 .    for all binary classifiers @xmath39 and all cost - sensitive @xmath0-class distributions @xmath33 , @xmath100 where @xmath51 and @xmath101 are as defined above .",
    "[ bound ]    a simple example in section  [ example ] shows that this bound is essentially tight .",
    "the proof of theorem  [ main ] uses the following folk theorem from @xcite .",
    "[ th : translate]_(translation theorem @xcite ) _ for any importance - weighted distribution @xmath102 , there exists a constant @xmath103 $ ] such that for any classifier @xmath39 , @xmath104 =   \\langle c \\rangle { \\mathbf{e}}_{(x , y , c)\\sim { p}'}[{\\mathbf{1}}(f(x)\\neq y)],\\ ] ] where @xmath105 .    thus choosing @xmath39 to minimize the error rate under @xmath106 is equivalent to choosing @xmath39 to minimize the expected cost under @xmath102 .",
    "the costing  @xcite reduction uses rejection sampling according to the weights to draw examples from @xmath106 given examples drawn from @xmath102 .",
    "the remainder of this section proves theorems  [ main ] and [ bound ] .",
    "* proof of theorem  [ main ] : *   it is sufficient to prove the claim for any @xmath4 because that implies that the result holds for all expectations over @xmath45 .    conditioned on the value of @xmath45 ,",
    "each label @xmath50 has a distribution over costs with an expected value of @xmath107 $ ] .",
    "the zero regret cost - sensitive classifier predicts according to @xmath108 $ ] .",
    "suppose that @xmath51 predicts @xmath109 on @xmath45 , inducing cost - sensitive regret @xmath110 - \\min_y { \\mathbf{e}}_{\\vec{c } \\sim d\\mid x } [ c_y].\\ ] ] first , we show that the sum over the binary problems of the importance weighted regret is at least @xmath111 , using induction starting at the leaves .",
    "the induction hypothesis is that the sum of the regrets of importance - weighted binary classifiers in any subtree bounds the regret of the subtree output .    for node @xmath68",
    ", each importance weighted binary decision between class @xmath78 and class @xmath79 has an importance weighted regret which is either @xmath112 or @xmath113 |    = |{\\mathbf{e}}_{\\vec{c } \\sim d|x } [ c_a ] - { \\mathbf{e}}_{\\vec{c } \\sim d|x } [ c_b ] | , $ ] depending on whether the prediction is correct or not .",
    "assume without loss of generality that the predictor outputs class @xmath79 .",
    "the regret of the subtree @xmath67 rooted at @xmath68 is given by @xmath114 - \\min_{y \\in { l}(t_n ) } { \\mathbf{e}}_{\\vec{c } \\sim d|x } [ c_y].\\ ] ]    as a base case , the inductive hypothesis is trivially satisfied for trees with one label .",
    "inductively , assume that @xmath115 and @xmath116 for the left subtree @xmath117 of @xmath68 ( providing @xmath78 ) and the right subtree @xmath118 ( providing @xmath79 ) .",
    "there are two possibilities .",
    "either the minimizer comes from the leaves of @xmath117 or the leaves of @xmath118 .",
    "the second possibility is easy since we have @xmath119 - \\min_{y \\in { l}(r ) } { \\mathbf{e}}_{\\vec{c } \\sim d|x } [ c_y ]     = r_r \\leq \\sum_{n ' \\in r } r_{n ' } \\leq \\sum_{n ' \\in { t_n } } r_{n ' } , \\end{aligned}\\ ] ] proving the induction .",
    "for the first possibility , @xmath119 - \\min_{y \\in { l}(l ) } { \\mathbf{e}}_{\\vec{c } \\sim d|x } [ c_y ]   \\\\ & = { \\mathbf{e}}_{\\vec{c } \\sim d|x } [ c_b ] - { \\mathbf{e}}_{\\vec{c } \\sim d|x } [ c_a ] +   { \\mathbf{e}}_{\\vec{c } \\sim d|x } [ c_a ]    - \\min_{y \\in { l}(l ) } { \\mathbf{e}}_{\\vec{c } \\sim d|x } [ c_y ]   \\\\ & = { \\mathbf{e}}_{\\vec{c } \\sim d|x } [ c_b ] - { \\mathbf{e}}_{\\vec{c } \\sim d|x } [ c_a ] + r_l \\\\ & \\leq r_n + \\sum_{n ' \\in l } r_{n ' }   \\leq \\sum_{n ' \\in { t_n } } r_{n'},\\end{aligned}\\ ] ] which completes the induction .",
    "the inductive hypothesis for the root is that @xmath120 .    using the folk theorem from  @xcite ( theorem  [ th : translate ] in this paper ) , each @xmath121 is bounded by @xmath122 plugging this in and using definition  ( [ abr ] ) , we get the theorem .",
    "the proof of theorem  [ bound ] makes use of the following lemma .",
    "consider a filter tree @xmath66 on @xmath0 labels , evaluated on a cost - sensitive multiclass example with cost vector @xmath123^k$ ] .",
    "let @xmath124 be the sum of importances over all nodes in @xmath66 , and @xmath125 be the sum of importances over the nodes where the class with the larger cost was selected for the next round .",
    "let @xmath126 denote the cost of the winner chosen by @xmath66 .    for any @xmath123^k$ ] ,",
    "[ claim1 ]    the inequality follows by induction , the result being immediate when @xmath128 .",
    "assume that the claim holds for the two subtrees , @xmath117 and @xmath118 , providing their respective inputs @xmath129 and @xmath9 to the root of @xmath66 , and @xmath66 outputs @xmath9 without loss of generality . using the inductive hypotheses for @xmath117 and @xmath118 , we get @xmath130 .    if @xmath131 , we have @xmath132 , and @xmath133 as desired .",
    "if @xmath134 , we have @xmath135 and @xmath136 , completing the proof .",
    "* proof of theorem  [ bound ] : *   fix @xmath137^k$ ] and take the expectation over the draw of @xmath98 from @xmath33 as the last step .",
    "consider a filter tree @xmath66 evaluated on @xmath98 using a given binary classifier @xmath39 .",
    "as before , let @xmath124 be the sum of importances over all nodes in @xmath66 , and @xmath125 be the sum of importances over the nodes where @xmath39 made a mistake . recall that the regret of @xmath66 on @xmath98 , denoted in the proof by @xmath138 , is the difference between the cost of the tree s output and the smallest cost @xmath139 .",
    "the importance - weighted binary regret of @xmath39 on @xmath98 is simply @xmath140 .",
    "since the expected importance is upper bounded by 1 , @xmath140 also bounds the binary regret of @xmath39 .",
    "the inequality we need to prove is @xmath141 .",
    "the proof is by induction on @xmath0 , the result being trivial if @xmath128 .",
    "assume that the assertion holds for the two subtrees , @xmath117 and @xmath118 , providing their respective inputs @xmath129 and @xmath9 to the root of @xmath66 .",
    "( the number of classes in @xmath117 and @xmath118 can be taken to be even , by splitting the odd class into two classes with the same cost as the split class , which has no effect on the quantities in the theorem statement . )",
    "let the best cost @xmath139 be in the left subtree @xmath117 .",
    "suppose first ( * case 1 * ) that @xmath66 chooses @xmath9 and @xmath142 .",
    "let @xmath143 .",
    "we have @xmath144 and @xmath145 .",
    "the left hand side of the inequality is thus @xmath146 the first inequality follows from lemma [ claim1 ] .",
    "the second and fourth follow from @xmath147 .",
    "the third follows from @xmath148 .",
    "the last follows from @xmath149 for @xmath150 .",
    "the proofs for the remaining three cases ( @xmath151 , @xmath152 , and @xmath153 ) use the same machinery as the proof above .",
    "* case  2 * :   @xmath66 outputs @xmath129 , and @xmath154 . in this case",
    "the left hand side can be rewritten as @xmath156 the first inequality follows from the lemma , the second from @xmath157 , the third from @xmath148 , the fourth from @xmath158 , and the fifth because @xmath135 .",
    "* case  3 * :   @xmath66 outputs @xmath129 , and @xmath159 .",
    "we have @xmath155 .",
    "the left hand side can be written as @xmath160 the first inequality follows from the inductive hypothesis and the lemma , the second from @xmath161 and @xmath162 , and the third from @xmath163 and @xmath164 .",
    "* case  4 * :   @xmath66 outputs @xmath9 , and @xmath159 .",
    "let @xmath165 .",
    "we have @xmath166 .",
    "the left hand side can be written as @xmath167 the first inequality follows from the inductive hypothesis and the lemma , the second from @xmath148 , and the third from @xmath168 .",
    "the last three terms are upper bounded by @xmath169 and thus can be ignored , yielding @xmath170 , which completes the proof .",
    "taking the expectation over @xmath98 completes the proof .",
    "the following simple example shows that the theorem is essentially tight .",
    "let @xmath0 be a power of two , and let every label have cost 0 if it is is even , and 1 otherwise .",
    "the tree structure is a complete binary tree of depth @xmath18 with the nodes being paired in the order of their labels .",
    "suppose that all pairwise classifications are correct , except that class @xmath0 wins all its @xmath18 games leading to cost - sensitive multiclass regret 1 .",
    "we have @xmath171 , @xmath172 , and @xmath173 , leading to the regret ratio @xmath174 almost matching the theorem s bound of @xmath175 on the ratio .",
    "there is a variant of the filter tree algorithm , which has a significant difference in performance in practice .",
    "every classification at any node @xmath68 is essentially between two labels computed at test time , implying that we could simply learn one classifier for every pair of labels that could reach @xmath68 at test time .",
    "( note that a given pair of labels can be compared only at a single node , namely their least common ancestor in the tree . )",
    "the conditioning process and the tree structure gives us a better analysis than is achievable with the all - pairs approach  @xcite .",
    "this variant uses more computation and requires more data but often maximizes performance when the form of the classifier is constrained .",
    "we compared the performance of filter tree and its all - pairs variant described above to the performance of all - pairs and the tree reduction , on a number of publicly available multiclass datasets  @xcite .",
    "some datasets came with a standard training / test split : ` isolet ` ( isolated letter speech recognition ) , ` optdigits ` ( optical handwritten digit recognition ) , ` pendigits ` ( pen - based handwritten digit recognition ) , ` satimage ` , and ` soybean ` . for all other datasets , we reported the average result over 10 random splits , with @xmath176 of the dataset used for training and @xmath177 for testing .",
    "( the splits were the same for all methods . )",
    "error rates ( in % ) of ` tree ` versus ` filter - tree ` ( top ) and all - pairs versus all - pairs filter tree ( top ) on several different datasets with a decision tree or logistic regression classifier.,title=\"fig : \" ]   error rates ( in % ) of ` tree ` versus ` filter - tree ` ( top ) and all - pairs versus all - pairs filter tree ( top ) on several different datasets with a decision tree or logistic regression classifier.,title=\"fig : \" ]    if computation is constrained and we can afford only @xmath1 computation per multiclass prediction , the filter tree dominates the tree reduction , as shown in figure  [ fig : comparison ]",
    ".    if computation is relatively unconstrained , all - pairs and the all - pairs filter tree are reasonable choices .",
    "the comparison in figure  [ fig : comparison ] shows that there the all - pairs filter tree yields similar prediction performance while using only @xmath10 computation instead of @xmath12 .",
    "test error rates using decision trees ( j48 ) and logistic regression as binary classifier learners are reported in table  [ t1 ] , using weka s implementation with default parameters  @xcite .",
    "the lowest error rate in each row is shown in bold , although in some cases the difference is insignificant .",
    "in this section , we extend filter trees to @xmath19-elimination tournaments , also called @xmath178-error - correcting tournaments . as this section builds on sections  [ s : algorithm ] and [ s : analysis ] , understanding them",
    "is required before reading this section . for simplicity , we work with only the multiclass case .",
    "an extension for cost - sensitive multiclass problems is possible using the importance weighting techniques of the previous section .      an _ @xmath19-elimination tournament _ operates in two phases .",
    "the first phase consists of @xmath19 single - elimination tournaments over the @xmath0 labels where a label is paired against another label at most once per round .",
    "consequently , only one of these single elimination tournaments has a simple binary tree structure ; see , for example , figure  [ fig : me_phase1 ] for an @xmath25 elimination tournament on @xmath24 labels .",
    "there is substantial freedom in how the pairings of the first phase are done ; our bounds depend on the depth of any mechanism which pairs labels in @xmath19 distinct single elimination tournaments .",
    "one such explicit mechanism is given in  @xcite .",
    "note that once an example has lost @xmath19 times , it is eliminated and no longer influences training at the nodes closer to the root .    the second phase is a final elimination phase , where we select the winner from the @xmath19 winners of the first phase .",
    "it consists of a redundant single - elimination tournament , where the degree of redundancy increases as the root is approached . to quantify the redundancy ,",
    "let every subtree @xmath179 have a _",
    "@xmath180 equal to the number of leaves under the subtree .",
    "first phase winners at the leaves of the final elimination tournament have charge @xmath55 . for any non - leaf node comparing the outputs of subtrees @xmath181 and @xmath182 , the importance weight of a binary example created at the node is set to either @xmath183 or @xmath184 , depending on whether the label comes from @xmath182 or @xmath181 . in tournament applications ,",
    "an importance weight can be expressed by playing games repeatedly where the winner of @xmath181 must beat the winner of @xmath182 @xmath184 times to advance , and vice versa .",
    "when the two labels compared are the same , the importance weight is set to @xmath112 , indicating there is no preference in the pairing amongst the two choices .",
    "an example of a @xmath26-elimination tournament on @xmath24 players .",
    "there are @xmath25 distinct single elimination tournaments in first phase  one in black , one in blue , and one in red .",
    "after that , a final elimination phase occurs over the three winners of the first phase .",
    "the final elimination tournament has an extra weighting on the nodes , detailed in the text . ]",
    "a key concept throughout this section is the _ importance depth _ , defined as the worst - case length ( number of games ) of the overall tournament , where importance - weighted matches in the final elimination phase are played as repeated games . in theorem",
    "[ thm : importance - depth ] we prove a bound on the importance depth .",
    "the computational bound per example is essentially just the importance depth .",
    "[ thm : structural - depth ] _",
    "( structural depth bound ) _ for any @xmath19-elimination tournament , the training and test computation is @xmath185 per example .",
    "the proof is by simplification of the importance depth bound ( theorem [ thm : importance - depth ] ) , which bounds the sum of importance weights at all nodes in the tournament .    to see that the importance depth controls the computation , first note that the importance depth bounds the tournament depth since all importance weights are at least 1 . at training time , any one example is used at most once per tournament level starting at the leaves . at testing time , an unlabeled example can have its label determined by traversing the structure from root to leaf .",
    "our regret theorem is the analogue of corollary  [ cor : multi ] for error - correcting tournaments , and the notation is as defined there . as in the previous section",
    ", the reduction transforms a multiclass distribution @xmath33 into an induced distribution @xmath101 over binary labeled examples . as before",
    ", @xmath51 denotes the multiclass classifier induced by a given binary classifier @xmath39 and tournament structure @xmath66 .",
    "it is useful to have the notation @xmath186 for the smallest power of @xmath56 larger than or equal to @xmath19 .",
    "[ multi - theorem ] _ ( main theorem ) _ for all distributions @xmath33 over @xmath0-class examples , all binary classifiers @xmath39 , all @xmath19-elimination tournaments @xmath66 , the ratio of @xmath187 to @xmath188 is upper bounded by @xmath189    the first case shows that a regret ratio of @xmath26 is achievable for very large @xmath19 .",
    "the second case is the best bound for cases of common interest .",
    "for @xmath29 it gives a ratio of @xmath30 .",
    "the proof holds for each input @xmath45 , and hence in expectation over @xmath45 .",
    "fix @xmath45 , and let @xmath190 for @xmath191 .",
    "we can define the regret of any label @xmath50 as @xmath192 , where @xmath193 .",
    "the regret of a node @xmath68 comparing labels @xmath78 and @xmath79 from subtrees @xmath181 and @xmath182 , and outputting @xmath78 , is @xmath194 where we use the predicate @xmath195 .",
    "thus @xmath121 is 0 if @xmath68 outputs the more likely label . if @xmath68 is in a first phase tournament , @xmath196 .",
    "finally , the regret of a subtree @xmath66 is defined as @xmath197 .    the first part of the proof is by induction on the tree structure @xmath198 of the final phase .",
    "the invariant for a subtree @xmath179 of @xmath198 won by label @xmath78 is @xmath199 where @xmath200 is the winner of a first phase single - elimination tournament @xmath201 .    when @xmath179 is a leaf @xmath200 of @xmath198 , we have @xmath202 , where the inequality is from corollary  [ cor : multi ] noting that the depth of @xmath201 times the average regret over the nodes in @xmath201 is @xmath203 .",
    "assume inductively that the hypothesis holds at node @xmath68 comparing labels @xmath78 and @xmath79 from subtrees @xmath181 and @xmath182 , and outputting @xmath78 : @xmath204 and @xmath205 we have @xmath206 by the inductive hypothesis .",
    "now , there are two cases : either @xmath207 , in which case @xmath208 and @xmath209 , as desired",
    ". or @xmath210 , in which case @xmath211 and thus @xmath212 finishing the induction .    finally , letting @xmath50 be the prediction of @xmath51 on @xmath45 , @xmath213 where @xmath97 is the maximum importance depth . applying the importance depth theorem  ( theorem",
    "[ thm : importance - depth ] ) and algebra completes the proof .",
    "the depth bound follows from the following three lemmas .",
    "[ lem : fpdb ] _",
    "( first phase depth bound ) _ the importance depth of the first phase tournament is bounded by the minimum of @xmath214    the depth of the first phase is bounded by the classical problem of robust minimum finding with low depth .",
    "the first three cases hold because any such construction upper bounds the depth of an error - correcting tournament , and one such construction has these bounds  @xcite .    for the fourth case",
    ", we construct the depth bound by analyzing a continuous relaxation of the problem .",
    "the relaxation allows the number of labels remaining in each single elimination tournament of the first phase to be broken into fractions .",
    "relative to this version , the actual problem has two important discretizations :    1 .",
    "when a single - elimination tournament has only a single label remaining , it enters the next single elimination tournament .",
    "this can have the effect of _ decreasing _ the depth compared to the continuous relaxation .",
    "2 .   when a single - elimination tournament has an odd number of labels remaining , the odd label does not play that round .",
    "thus the number of players does not quite halve , potentially _ increasing _ the depth compared to the continuous relaxation .    in the continuous version , tournament @xmath6 on round",
    "@xmath97 has @xmath215 labels , where the first tournament corresponds to @xmath216 .",
    "consequently , the number of labels remaining in any of the tournaments is @xmath217 we can get an estimate of the depth by finding the value of @xmath97 such that this number is 1 .    this value of @xmath97 can be found using the chernoff bound .",
    "the probability that a coin with bias @xmath218 has @xmath21 or fewer heads in @xmath97 coin flips is bounded by @xmath219 , and the probability that this occurs in @xmath0 attempts is bounded by @xmath0 times that . setting this value to @xmath55",
    ", we get @xmath220 solving the equation for @xmath97 , gives @xmath221 .",
    "this last formula was verified computationally for @xmath222 and @xmath223 by discretizing @xmath0 into factors of @xmath56 and running a simple program to keep track of the number of labels in each tournament at each level . for @xmath224",
    ", we used a pessimistic value of @xmath225 in the above formula to compute the bound , and compared it to the output of the program for @xmath226 .",
    "[ lem : spdb ] _",
    "( second phase depth bound ) _ in any @xmath19-elimination tournament , the second phase has importance depth at most @xmath227 rounds for @xmath228 .",
    "when two labels are compared in round @xmath229 , the importance weight of their comparison is at most @xmath230 .",
    "thus we have @xmath231 .",
    "putting everything together gives the importance depth theorem .",
    "[ thm : importance - depth ] _",
    "( importance depth bound ) _ for all @xmath19-elimination tournaments , the importance depth is upper bounded by @xmath232    we simply add the depths of the first and second phases from lemmas  [ lem : fpdb ] and [ lem : spdb ] . for the last case , we bound @xmath233 and eliminate subtractions in lemma [ lem : spdb ] .",
    "all of our lower bounds hold for a somewhat more powerful adversary which is more natural in a game playing tournament setting .",
    "in particular , we disallow reductions which use importance weighting on examples , or equivalently , all importance weights are set to @xmath55 .",
    "note that we can modify our upper bound to obey this constraint by transforming final elimination comparisons with importance weight @xmath6 into @xmath234 repeated comparisons and use the majority vote .",
    "this modified construction has an importance depth which is at most @xmath19 larger implying the ratio of the adversary and the reduction s regret increases by at most @xmath55 .",
    "the first lower bound says that for any reduction algorithm @xmath182 , there exists an adversary @xmath181 with the average per - round regret @xmath9 such that @xmath181 can make @xmath182 incur regret @xmath235 even if @xmath182 knows @xmath9 in advance .",
    "thus an adversary who corrupts half of all outcomes can force a maximally bad outcome . in the bounds below",
    ", @xmath236 denotes the multiclass classifier induced by a reduction @xmath182 using a binary classifier @xmath39 .    for any deterministic reduction @xmath182 from @xmath237 classification to binary classification",
    ", there exists a choice of @xmath33 and @xmath39 such that @xmath238 .",
    "the adversary @xmath181 picks any two labels @xmath6 and @xmath239 .",
    "all comparisons involving @xmath6 but not @xmath239 , are decided in favor of @xmath6 .",
    "similarly for @xmath239 .",
    "the outcome of comparing @xmath6 and @xmath239 is determined by the parity of the number of comparisons between @xmath6 and @xmath239 in some fixed serialization of the algorithm .",
    "if the parity is odd , @xmath6 wins ; otherwise , @xmath239 wins .",
    "the outcomes of all other comparisons are picked arbitrarily .",
    "suppose that the algorithm halts after some number of queries @xmath240 between @xmath6 and @xmath239 .",
    "if neither @xmath6 nor @xmath239 wins , the adversary can simply assign probability @xmath218 to @xmath6 and @xmath239 .",
    "the adversary pays nothing while the algorithm suffers loss 1 , yielding a regret ratio of @xmath241 .",
    "assume without loss of generality that @xmath6 wins .",
    "the depth of the tournament is either @xmath240 or at least @xmath242 , because each label can appear at most once in any round .",
    "if the depth is @xmath240 , then since @xmath237 , some label is not involved in any query , and the adversary can set the probability of that label to @xmath55 resulting in @xmath243 .    otherwise , @xmath181 can set the probability of label @xmath239 to be @xmath55 while all others have probability @xmath112 .",
    "the total regret of @xmath181 is at most @xmath244 , while the regret of the winning label is @xmath55 .",
    "multiplying by the depth bound @xmath242 , gives a regret ratio of at least @xmath56 .",
    "note that the number of rounds in the above bound can depend on @xmath181 .",
    "next , we show that for any algorithm @xmath182 taking the same number of rounds for any adversary , there exists an adversary @xmath181 with a regret of roughly one third , such that @xmath181 can make @xmath182 incur the maximal loss , even if @xmath182 knows the power of the adversary .    for any deterministic reduction @xmath182 to binary classification with number of rounds independent of the query outcomes",
    ", there exists a choice of @xmath33 and @xmath39 such that @xmath245 .",
    "let @xmath182 take @xmath246 rounds to determine the winner , for any set of query outcomes .",
    "we will design an adversary @xmath181 with incurs regret @xmath247 , such that @xmath181 can make @xmath182 incur the maximal loss of 1 , even if @xmath182 knows @xmath9 .    the adversary s query answering strategy is to answer consistently with label @xmath55 winning for the first @xmath248 rounds , breaking ties arbitrarily .",
    "the total number of queries that @xmath182 can ask during this stage is at most @xmath249 since each label can play at most once in every round , and each query occupies two labels .",
    "thus the total amount of regret at this point is at most @xmath249 , and there must exist a label @xmath6 other than label @xmath0 with at most @xmath9 losses . in the remaining @xmath250 rounds",
    ", @xmath181 answers consistently with label @xmath6 and all other skills being 0 .",
    "now if @xmath182 selects label @xmath55 , @xmath181 can set @xmath251 with @xmath252 average regret from the first stage .",
    "if @xmath182 selects label @xmath6 instead , @xmath181 can choose that @xmath253 . since the number of queries between labels @xmath6 and @xmath0 in the second stage is at most @xmath9 , the adversary can incurs average regret at most @xmath252 . if @xmath182 chooses any other label to be the winner , the regret ratio is unbounded .",
    "00    m. adler , p. gemmell , m. harchol - balter , r. karp , and c. kenyon .",
    "selection in the presence of noise : the design of playoff systems , soda 1994 .",
    "e. allwein , r. schapire , and y. singer .",
    "reducing multiclass to binary : a unifying approach for margin classifiers , _ journal of machine learning research _ , 1 : 113141 , 2000 .    j. aslam and a. dhagat . searching in the presence of linearly bounded errors , stoc 1991 .    c. blake and c. merz , uci repository of machine learning databases , university of california , irvine .",
    "r. borgstrom , s. rao kosaraju .",
    "comparison - base search in the presence of errors , stoc 1993 .",
    "p. denejko , k. diks , a. pelc , and m. piotrow .",
    "reliable minimum finding comparator networks , _ fundamenta informaticae _ , 42 : 235249 , 2000 .",
    "t.  dietterich and g.  bakiri .",
    "solving multiclass learning problems via error - correcting output codes , _ journal of artificial intelligence research _ , 2 : 263286 , 1995 .",
    "u. feige , d. peleg , p. raghavan , and e. upfal .",
    "computing with unreliable information , _ symposium on theory of computing _",
    ", 128137 , 1990 .",
    "d.  foster and d.  hsu , ` http://hunch.net/?p=468 ` .",
    "_ applied regression analysis , linear models , and related methods _ , sage publications , 1997 .",
    "v. guruswami and a. sahai .",
    "multiclass learning , boosting , and error correcting codes , colt 1999 .",
    "t. hastie and r. tibshirani .",
    "classification by pairwise coupling , nips 1997 .",
    "r. herbrich , t. minka , and t. graepel .",
    "trueskill(tm ) : a bayesian skill rating system , nips 2007 .",
    "d. hsu , j. langford , s. kakade , and t. zhang , multi - label prediction via compressed sensing , arxiv:0902.1284v1 , 2009 .",
    "j.  langford and a.  beygelzimer .",
    "sensitive error correcting output codes , colt 2005 .",
    "j.  langford and b.  zadrozny , estimating class membership probabilities using classifier learners , aistat 2005 .",
    "b. ravikumar , k. ganesan , and k. b. lakshmanan . on selecting the largest element in spite of erroneous information , lecture notes in computer science , 247 : 8899 , 1987 .",
    "b. williamson , personal communication .",
    "i. witten and e. frank .",
    "data mining : practical machine learning tools with java implementations , 2000 : ` http://www.cs.waikato.ac.nz/ml/weka/ ` .",
    "a. c. yao and f. f. yao . on fault - tolerant networks for sorting .",
    "_ siam journal of computing _ , 14(1 ) : 120128 , 1985 .",
    "b. zadrozny , j. langford , and n. abe .",
    "cost - sensitive learning by cost - proportionate example weighting , icdm 2003 .",
    "[ cols= \" < , > , > , > , > , > , > , > , > , > , > \" , ]"
  ],
  "abstract_text": [
    "<S> text of abstract we present a family of pairwise tournaments reducing @xmath0-class classification to binary classification . these reductions are provably robust against a constant fraction of binary errors , simultaneously matching the best possible computation @xmath1 and regret @xmath2 .    </S>",
    "<S> the construction also works for robustly selecting the best of @xmath0-choices by tournament . </S>",
    "<S> we strengthen previous results by defeating a more powerful adversary than previously addressed while providing a new form of analysis . in this </S>",
    "<S> setting , the error correcting tournament has depth @xmath1 while using @xmath3 comparators , both optimal up to a small constant </S>",
    "<S> .    reductions , multiclass classification , cost - sensitive learning , tournaments , robust search </S>"
  ]
}