{
  "article_text": [
    "many recent studies have been done on the construction , behavior , and performance of reservoir computers .",
    "two popular reservoir structures are the echo state network ( esn ) and the single nonlinear node with delay line  @xcite .",
    "jaeger provided a mathematical description and analysis of echo states , in which case the reservoir is assumed to have a finite number of randomly interconnected internal nodes  @xcite . while much work has been done to construct , test , and analyze single - node reservoir schemes  @xcite ,",
    "there remains a lack of understanding and an absence of a rigorous mathematical analysis of the underlying dynamics of such a system .",
    "we seek to provide some mathematical insight into the dynamics of the single - node reservoir computer .",
    "this analysis is done in two parts .",
    "section [ results1 ] describes how the variation in outputs ( in the sense of the @xmath0 norm ) is bounded above by a constant multiple of the variation of time - series input vectors . to this end",
    ", we draw on basic mathematical tools to prove something of a lipschitz continuity condition on the entire system :    suppose @xmath1 is @xmath2-lipschitz with @xmath3 and define @xmath4 by @xmath5 for an input spike train @xmath6 then    @xmath7    for all inputs @xmath8    section [ classification_section ] provides a translation of some well known classification metrics  @xcite to the context of the single - node with delay line reservoir structure .",
    "that section also contains a brief review and analysis of the importance of selecting an injective nonlinear sigmoid function for the single nonlinear input node .",
    "in particular , we show that classification mishaps are possible if the injectivity condition is disregarded , or if the input data are not properly pre - conditioned .",
    "let @xmath9 denote discrete time and let @xmath10 .",
    "an _ input _ is a vector @xmath11 where the @xmath12th entry is defined as @xmath13 since we only consider reservoirs with a single nonlinear node and delay line , the term _ reservoir _ will be reserved for that structure .",
    "[ resdef ] let @xmath14 be non - linear . a _ reservoir of length _",
    "@xmath15 , _ nonlinearity _ @xmath16 , _ input gain _",
    "@xmath17 _ and feedback gain _",
    "@xmath18 denoted @xmath19 is a set of @xmath15 functions @xmath20 where @xmath21 defined by the relations @xmath22 we will have occasion to use a modified version of definition [ resdef ] in which @xmath23 instead of @xmath24 in .",
    "call @xmath25 the _ state of the reservoir at time _",
    "@xmath26 a vector / set of _ weights _ is @xmath27 where @xmath28 is the weight of the  node \" @xmath29 given a set of weights , the _ output of the reservoir at time @xmath30 _ is defined as the sum of weighted node states @xmath31 suppose we are working with a given data set of ( masked ) inputs , call it @xmath32 .",
    "put @xmath33 .",
    "the respective reservoir outputs will be considered over the interval of discrete time @xmath34.$ ] in order to compare two inputs @xmath35 and @xmath36 it is convenient to simply extend all input vectors of dimension less than @xmath37 . to this end , if @xmath38 and if @xmath39 identify @xmath35 with @xmath40 from this point forward we will write @xmath41 as the set of all inputs after this elongation process is performed .",
    "writing @xmath42 for the reservoir output at time @xmath30 with respect to an input @xmath43 a discrete time interval @xmath34 $ ] yields an output vector @xmath44 so it is reasonable to consider the @xmath0 norm of @xmath35 or @xmath45 denoted @xmath46 , or just @xmath47 where the dimension of @xmath35 is clear from the context .",
    "[ cols=\"<,<,<,<\",options=\"header \" , ]      when preparing a reservoir to perform a task , it is necessary to choose a set of weights .",
    "we assume the existence of a data set consisting of a collection of inputs . one way or another , these data are used to determine ( train ) appropriate weights .",
    "to train a set of weights , one needs a set of inputs and a corresponding set of target outputs @xmath48 to be compared to the actual machine outputs .",
    "that is , one finds @xmath49 such that the variation between @xmath50 and @xmath51 is minimized .",
    "there is a variety of methods to train weights that is used in the literature including least squares , ridge regression , minimization of mean square error , and minimization of normalized mean square error  @xcite .",
    "ridge regression is commonly used to accomodate systems with several parameters to avoid overfitting weights to training data .",
    "the goal is to ensure that the system does not categorize two distinct members of the same input class as  different . \"",
    "one of the most popular benchmark tasks for testing a reservoir computer is narma , wherein the normalized root mean square error ( nrmse ) is minimized  @xcite .",
    "the authors of @xcite developed an algorithm for computing what are called dantzig selectors , which were defined by candes and tao in @xcite .",
    "let @xmath52 and @xmath53 let @xmath54 be a diagonal matrix where @xmath55 is the @xmath0 norm of the @xmath56th column of @xmath57 the dantzig selector , denoted @xmath58 , solves the following optimization problem : @xmath59 this concept can be used to train reservoir node weights by storing the states of the reservoir over time as the columns of @xmath60 ; that is , @xmath61 .",
    "to ensure that an input @xmath62 output system is reasonably accurate , it is desirable to have a growth condition that places an upper bound on the distortion of distances between two outputs with respect to the distance between their corresponding inputs . the following is a classical definition that describes such a condition .",
    "[ lipschitz_definition ] suppose @xmath60 and @xmath63 are metric spaces and @xmath64 a mapping @xmath65 is _",
    "@xmath2-lipschitz _ if @xmath66 for all @xmath67 the function @xmath16 is _ lipschitz _ if it is @xmath2-lipschitz for some @xmath64    if @xmath14 is differentiable , then @xmath68 is bounded if and only if @xmath16 is lipschitz .",
    "in fact , if @xmath1 is any lipschitz function , then @xmath16 is absolutely continuous , and is differentiable except on a set @xmath69 of lebesgue measure @xmath70 definition [ lipschitz_definition ] says that @xmath16 can only increase distances by a bounded amount .    in this section",
    "we assume that there is a specific task at hand , and that target outputs have been defined and used to train a set of weights .",
    "it is also assumed that for the set of inputs @xmath41 , each @xmath71 has been standardized to have dimension @xmath37 as described in subsection [ notation_subsection ] .",
    "in other words , we only consider inputs over the discrete time interval @xmath34\\cap \\mathbb{n}$ ] .    for the purpose of classification in a reservoir system ,",
    "it is desirable that the distance between two outputs is controlled by the distance between their respective inputs .",
    "the lipschitz condition clearly describes this property .",
    "the main result in this section is that the function @xmath72 is lipschitz , where the lipschitz constant depends on @xmath73 and @xmath74 this result does not appear to be optimal , however , since the lipschitz constant is directly proportional to the dimension of the input vectors and the number of nodes in the reservoir .",
    "recall definition [ resdef ] :    [ 1 ] for a system with @xmath75 nodes and input @xmath43 @xmath76    the following couple of modest observations will be useful for proving theorem [ main_result ] .",
    "[ 2 ] if @xmath77 then @xmath78 and @xmath79 .",
    "[ 3 ] if @xmath80\\cap \\mathbb{z}$ ] then @xmath81 .    since @xmath82 definition [ 1 ] and fact [ 2 ]",
    "yield @xmath83    it would be convenient to have a lipschitz continuity condition on the function @xmath84 where @xmath30 is fixed .",
    "this idea seems naive , however , since we do not expect the difference of reservoir outputs @xmath85 to depend entirely on the difference in one entry of the inputs @xmath86 that is , we do not know how to capture the behavior of the reservoir simply by looking at the input difference at a particular time step .",
    "an approach to predicting classification accuracy involves the concept of  separation \" of inputs and reservoir states , which will be discussed in section [ classification_section ] .",
    "[ main_result ] suppose @xmath87 is a reservoir and @xmath1 is @xmath2-lipschitz with @xmath88 then for two inputs @xmath35 and @xmath89    @xmath90    the bars @xmath91 indicate the length of the vector as a time series , whereas the bars @xmath92 are used for all other lengths .    for all @xmath93 @xmath94    if @xmath95 there is nothing to prove .",
    "we will deal with special time intervals and achieve the above estimation on each interval .",
    "a.   @xmath96 : + + @xmath97 + @xmath98 + @xmath99 + @xmath100 + in this case , @xmath101 it follows from , , and the schwarz inequality that @xmath102 + write @xmath103 .",
    "note that the function @xmath104 is linear and hence injective .",
    "therefore equation gives @xmath105 so that @xmath106 b.   @xmath107 + note that for @xmath108 we have @xmath109 so the node states can be written as follows : + @xmath110 + @xmath111 + @xmath112 @xmath113 + fix @xmath30 and write @xmath114 for some @xmath115 then + @xmath116 + so @xmath117 is bounded above by @xmath118| \\\\ & \\hspace{1 cm } + \\sum_{i_0",
    "< i \\leq n } |w_i| \\cdot |f(\\beta u(t - i ) ) - f(\\beta v(t - i))| \\end{split}\\ ] ] also @xmath119| \\\\ & = l|\\alpha[f(\\beta u(t - n - i-1))-f(\\beta v(t - n - i-1 ) ) ] + \\beta[u(t - i)-v(t - i)]| \\\\ & \\leq l\\left [ \\alpha|f(\\beta u(t - n - i-1))-f(\\beta v(t - n - i-1))| + \\beta|u(t - i)-v(t - i)| \\right ] \\\\ &",
    "[ \\alpha l \\beta |u(t - n - i-1)-v(t - n - i-1)| + \\beta|u(t - i)-v(t - i)| ] \\\\ & \\leq \\alpha l^2 \\beta |u(t - n - i-1)-v(t - n - i-1)| + l\\beta|u(t - i)-v(t - i)| .",
    "\\end{split}\\ ] ] thus the square of the quantity is bounded above by @xmath120 ^ 2 \\\\ & \\leq 2(l\\beta)^2 [ ~~ ( \\alpha l)^2 \\left(\\sum_{0\\leq i\\leq i_0 } |w_i| \\cdot |u(t - n - i-1)-v(t - n - i-1)| \\right)^2 \\\\ & \\hspace{1 cm } + \\left(\\sum_{i_0 < i \\leq n } |w_i| \\cdot |u(t - i)-v(t - i)|\\right)^2 ] \\\\ & \\leq 2(l\\beta)^2 [ ~~ ( \\alpha l)^2 \\sum_{0 \\leq",
    "i \\leq i_0 } |w_i|^2 |u(t - n - i-1)-v(t - n - i-1)|^2 \\\\ &",
    "\\hspace{1 cm } + \\sum_{i_0 < i \\leq",
    "n } |w_i|^2 \\cdot |u(t - i)-v(t - i)|^2 ~~ ] \\end{split}\\ ] ] it now follows that @xmath121 note that we have used the schwarz inequality and the convexity of the function @xmath122 to obtain . c.   @xmath123 + + @xmath124 + @xmath125 + @xmath126 + @xmath127 +",
    "let @xmath128 by the same argument used to obtain , we see that the quantity @xmath117 is bounded above by @xmath129 \\\\ &",
    "+ \\sum_{i_0 < i \\leq n } |w_i| \\cdot |f(\\alpha f(\\beta u(t - n - i-1 ) ) + \\beta u(t - i ) ) - f(\\alpha f(\\beta v(t - n - i-1 ) ) + \\beta v(t - i))| .",
    "\\end{split}\\ ] ] by the same argument as in , the first summand in has the property @xmath130 . \\end{split}\\ ] ] the second summand of is simply the first summand of , so similarly @xmath131 so that the quantity is bounded above by @xmath132 let @xmath133 for each pair @xmath134 write @xmath135 .",
    "then the square of becomes @xmath136 ^ 2 \\\\ & \\leq 2 c_1 ^ 2 [ ~~ 2\\left[\\alpha^2 l^2 \\sum_{0\\leq i\\leq i_0 } |w_i| \\cdot |u(z_2^i(t))-v(z_2^i(t))| \\right]^2 + 2 \\left[\\alpha l \\sum_i |w_i| \\cdot |u(z_1^i(t))-v(z_1^i(t))| \\right]^2 \\\\ & \\hspace{1.5 cm } + \\left [ \\sum_{i_0 < i\\leq n } |w_i| \\cdot |u(z_0^i(t))-v(z_0^i(t))| \\right]^2 ~~ ] \\\\ & = 2 c_1 ^ 2 [ ~~ 2 ( \\alpha l)^4 \\left [ \\sum_{0\\leq i\\leq i_0 } |w_i| \\cdot |u(z_2^i(t))-v(z_2^i(t))| \\right]^2 + 2(\\alpha l)^2 \\left [ \\sum_i |w_i| \\cdot |u(z_1^i(t))-v(z_1^i(t))| \\right]^2 \\\\ & \\hspace{1.5 cm } + \\left [ \\sum_{i_0 < i\\leq n } |w_i| \\cdot |u(z_0^i(t))-v(z_0^i(t))| \\right]^2 ~~ ] \\\\ & \\leq 2 c_1 ^ 2 ( n+1)^2 [ ~~ 2(\\alpha l)^4 \\sum_i |w_i|^2 \\cdot |u(z_2^i(t))-v(z_2^i(t))|^2 + 2(\\alpha l)^2 \\sum_i |w_i|^2 \\cdot |u(z_1^i(t))-v(z_1^i(t))|^2 \\\\ & \\hspace{2.5 cm } + \\sum_i |w_i|^2 \\cdot |u(z_0^i(t))-v(z_0^i(t))|^2 ~~ ] \\end{split}\\ ] ] by @xmath137 |w|^2 \\|u - v\\|_{\\ell^2(\\mathbb{r}^m)}^2.\\ ] ] d.   in general , on the interval @xmath138 $ ] for @xmath139 we can iterate this construction to see that for @xmath140 + @xmath141 \\right ) & , ~ t\\geq jn+i+2 \\\\ f \\left ( ( \\alpha f)^{j-2 } \\left [ \\alpha f(\\beta u(z_{j-1}^i(t))+\\beta u(z_{j-2}^i ) \\right ] \\right ) & , ~ t < jn+i+2 \\end{array } \\right.\\ ] ] + where @xmath142 is the @xmath143th iterate of the function @xmath144 note that when the convexity property is used iteratively , it follows the pattern @xmath145 so the highest power of @xmath146 is used twice , while the other powers of @xmath146 descend to @xmath147 + iteration of the above argument using the schwarz inequality and convexity of @xmath122 shows that for @xmath148,$ ] @xmath149 \\|u - v\\|^2 \\\\ & \\leq ( l\\beta)^2(n+1 ) |w|^2 \\left [ 1 + \\sum_{k=1}^{\\infty } 2^k ( \\alpha l)^{2(k-1 ) } \\right ] \\|u - v\\|^2 \\\\ & = ( l\\beta)^2(n+1 ) |w|^2 \\left(1+\\frac{2}{1 - 2\\alpha^2 l^2 } \\right)\\|u - v\\|_{\\ell^2(\\mathbb{r}^m)}^2 .",
    "\\end{split}\\ ] ] + let @xmath150 finally , the desired result is attainable since @xmath151    while aesthetically pleasing , the upper bound given by theorem [ main_result ] is quite crude",
    ". moreover , this upper bound considers the entire time interval @xmath152,$ ] but gives no comparison of the two instantaneous reservoir states at time @xmath153 $ ] with respect to the inputs @xmath8 it would be ideal to have an upper bound that applies at any time @xmath26    is there a constant @xmath154 such that @xmath155 for all @xmath30 ?",
    "there are several methods used to determine reservoir quality of esns and liquid state machines . in @xcite",
    "gibbons provided an overview of the separation method ( see @xcite,@xcite ) in great generality .",
    "our goal is to reduce that overview to the case of the single non - linear node with delay line .",
    "a fundamental difference between separation for esns and separation for the delay line structure is that with an esn , there are multiple input nodes .",
    "so if @xmath56 is the number of input nodes of an esn , then at a given time @xmath30 an entire sequence of @xmath56 data points can be fed into the reservoir simultaneously to yield an instantaneous reservoir state @xmath156 . for the delay line structure , however , a single input node does not provide such latitude . if one wishes to feed in a sequence of data ( i.e. an input ) of length @xmath56 , then @xmath56 time steps are required instead of just 1 .",
    "let @xmath41 be a set of inputs as in subsection [ notation_subsection ] .",
    "we assume that at time @xmath93 the set of all reservoir states @xmath157 is a disjoint union of @xmath158 prescribed classes @xmath159 .",
    "it is desirable for reservoir states within the same class to be close to each other , and for states of different classes to be far from each other . to that end , it is useful employ several averaging processes that provide insight into the separation within and between classes .      for each @xmath160 the _ class average ( center of mass ) _ of @xmath161 is the average of all the members of @xmath161 : @xmath162 the _ inter - class distance _ of @xmath163 is the average of all distances between centers of mass in @xmath163 : @xmath164    we need a measure of spread within classes in order to define separation ; it is obtained in the following way . for each class @xmath161 , find the average distance from the elements of @xmath161 to the class average @xmath165 then , compute the average of this quantity over all classes .      to perform good classification , the class averages should be separated by an amount substantial enough to differentiate them . moreover",
    ", the members of a given class should not stray too far from their class average , or else they could be misclassified .",
    "therefore the following definition provides a tool for measuring the effectiveness of a single - node reservoir classification system .",
    "definition [ separation ] first appeared in @xcite , in which goodman obtained results yielding large positive correlations between separation and classification accuracy of the output function .",
    "this was done for temporal pattern classification problems in the context of randomly generated reservoirs  @xcite .",
    "so at time step @xmath30 , if @xmath168 is large while @xmath169 is relatively small , one expects high classification accuracy .",
    "it is shown in @xcite that a hallmark of good classification for random reservoirs is the existence of a constant @xmath170 such that @xmath171 such that @xmath172 is  large . \" for sake of simplicity , let us say that should hold whenever @xmath173      if the chosen nonlinear function @xmath16 is injective ( which is fairly typical considering the common usage of @xmath174 ) , then an inequality like is not outside the realm of possibility , since the function @xmath175 is also injective .",
    "suppose @xmath178 and assume @xmath179 put @xmath180 . by definition [ resdef ] , @xmath181 and since @xmath182 we have @xmath183 then @xmath184 since @xmath185 so it follows from that @xmath186 by it is clear that @xmath187 since @xmath188 which is a contradiction .",
    "suppose that we graph @xmath35 as a function of time @xmath93 and that we desire a single - node reservoir @xmath87 whose state is not extremely sensitive to vertical translations .",
    "intuitively , it seems reasonable to use a nonlinear function @xmath16 that is periodic .",
    "the following fact shows that periodic functions @xmath16 can , in some sense , yield periodic reservoir states .",
    "the case @xmath95 is trivial . for the base case ,",
    "let @xmath193 and note that @xmath194 clearly @xmath195 for all @xmath196 so that @xmath197 now assume @xmath198 then @xmath199 so it remains to show that @xmath200 for @xmath201 since @xmath202 @xmath203    in the case @xmath204 , fact [ periodic_fact ] demonstrates the importance of pre - conditioning reservoir inputs @xmath35 so that @xmath205 $ ] for all @xmath206 and for all @xmath71 .",
    "depending on the task , it may or may not be desirable to put @xmath207 and @xmath208 into the same class , since the period @xmath189 can be made arbitrarily large .",
    "so choosing @xmath16 to be periodic can yield the undesirable pair of separation properties @xmath209 and @xmath210 which is a blatant violation of condition . since @xmath204 is injective on @xmath211,$ ] such difficulties",
    "can easily be avoided by normalizing each @xmath71 , which gives @xmath212    it is not clear whether any or all of the aforementioned quality metrics are useful for the single - node with delay line reservoir model . since the reservoir states rapidly change over time while receiving a signal , it is quite difficult to produce a lower bound on the difference in reservoir states in terms of the difference in inputs such as ."
  ],
  "abstract_text": [
    "<S> for a reservoir computer composed of a single nonlinear node and delay line , we show that after a finite period of discrete time , the distance between two reservoir outputs is bounded above by a constant multiple of the distance between their respective inputs . </S>",
    "<S> we also translate familiar separation properties from the context of echo state networks to that of the single nonlinear node structure . </S>"
  ]
}