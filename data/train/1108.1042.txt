{
  "article_text": [
    "global optimization problems are considered where the computation of objective function values , using the standard computer arithmetic , is problematic because of either underflows or overflows .",
    "a perspective means for solving such problems is the arithmetic of infinity @xcite .",
    "besides fundamentally new problems of minimization of functions whose computation involves infinite or infinitesimal values , the arithmetic of infinity can be also very helpful for the cases where the computation of objective function values is challenging because of the involvement of numbers differing in many orders of magnitude .",
    "for example , in some problems of statistical inference @xcite , the values of operands , involved in the computation of objective functions , differ by more than a factor of @xmath0 .",
    "the arithmetic of infinity can be applied to the optimization of challenging objective functions in two ways .",
    "first , the optimization algorithm can be implemented in the arithmetic of infinity .",
    "second , the arithmetic of infinity can be applied to scale the objective function values to be suitable for processing by a conventionally implemented optimization algorithm .",
    "the second case is simpler to apply , since the arithmetic of infinity should be applied only to the scaling of function values .",
    "if both implementation versions of the algorithm perform identically with respect to the generation of sequences of points where the objective function values are computed , the algorithm is called strongly homogeneous . in the present paper",
    ", we show that both implementation versions - of the p - algorithm and of the one - step bayesian algorithm - are strongly homogeneous .    to be more precise , let us consider two objective functions @xmath1 and @xmath2 , @xmath3 differing only in scales of function values , i.e. @xmath4 where @xmath5 and @xmath6 are constants that can assume not only finite but also infinite and infinitesimal values expressed by numerals introduced in @xcite . in its turn",
    ", @xmath1 is defined by using the traditional finite arithmetic .",
    "the sequences of points generated by an algorithm , when applied to these functions , are denoted by @xmath7 and @xmath8 respectively .",
    "the algorithm that generates the identical sequences @xmath9 is called strongly homogeneous .",
    "a weaker property of algorithms is considered in @xcite , where the algorithms that generate the identical sequences for the functions @xmath1 and @xmath10 are called homogeneous .",
    "since the proper scaling of function values by translation alone is not always possible , in the present paper we consider invariance of the optimization results with respect to a more general ( affine ) transformation of the objective function values .",
    "let us consider the minimization problem @xmath11 where the multimodality of the objective function @xmath1 is expected .",
    "although the properties of the feasible region are not essential in a further analysis , for the sake of explicitness , @xmath12 is assumed to be a hyper - rectangle . for the arguments justifying the construction of global optimization algorithms using statistical models of objective functions , we refer to @xcite .",
    "global optimization algorithms based on statistical models implement the ideas of the theory of rational decision making under uncertainty @xcite .",
    "the p - algorithm is constructed in @xcite stating the rationality axioms in the situation of selection of a point of current computation of the value of @xmath1 ; it follows from the axioms that a point should be selected where the probability to improve the current best value is maximal .    to implement the p - algorithm ,",
    "gaussian stochastic functions are used mainly because of their computational advantages ; however such type of statistical models is justified axiomatically and by the results of a psychometric experiment @xcite .",
    "application for a statistical model of a non - gaussian stochastic function would imply at least serious implementation difficulties .",
    "let @xmath13 be the gaussian stochastic function with mean value @xmath14 , variance @xmath15 , and correlation function @xmath16 .",
    "the choice of the correlation function normally is based on the supposed properties of the aimed objective functions , and the properties of the corresponding stochastic function , e.g. frequently used correlation functions are @xmath17 , @xmath18 .",
    "the parameters @xmath14 and @xmath15 should be estimated using a sample of the objective function values .",
    "let @xmath19 be the function values computed during the previous @xmath20 minimization steps . by the p - algorithm @xcite the next function value",
    "is computed at the point of maximum probability to overpass the aspiration level @xmath21 : @xmath22 since @xmath13 is the gaussian stochastic function , the maximization in ( [ p ] ) can be reduced to the maximization of @xmath23 where @xmath24 and @xmath25 denote the conditional mean and conditional variance of @xmath13 with respect to @xmath26 .",
    "the explicit formulae of @xmath24 and @xmath25 are presented below since they will be needed in a further analysis @xmath27",
    "to evaluate the influence of data scaling on the whole optimization process , two objective functions are considered : @xmath1 and @xmath28 , where @xmath5 and @xmath6 are constants .",
    "let us assume that the first @xmath20 function values were computed for both functions at the same points @xmath29 .",
    "the next points of computation of the values of @xmath30 and @xmath31 are denoted by @xmath32 and @xmath33 .",
    "we are interested in the strong homogeneity of the p - algorithm , i.e. in the equality @xmath34 .",
    "the parameters of the stochastic function , estimated using the same method but different function values , normally are different .",
    "the estimates of @xmath14 and @xmath15 , obtained using the data @xmath35 and @xmath36 , are denoted as @xmath37 and @xmath38 , respectively .",
    "it is assumed that @xmath39 and @xmath40 ; as shown below , this natural assumption is satisfied for the two most frequently used estimators .    obviously",
    ", the unbiased estimates of @xmath14 and of @xmath15 , @xmath41 , and @xmath42 , satisfy the assumptions made . although those estimates are well justified only for independent observations , they sometimes ( especially when only a small number ( @xmath43 ) of observations is available ) are used also for rough estimation of the parameters @xmath14 and of @xmath15 despite the correlation between the @xmath44 .",
    "the maximum likelihood estimates also satisfy the assumptions : @xmath45 where @xmath46 , and @xmath47 is the @xmath20 dimensional unit vector .",
    "it is easy to show that the maximum likelihood estimates implied by ( [ likl ] ) are equal to @xmath48 it follows from ( [ likl1 ] ) and ( [ likl2 ] ) that @xmath39 , and @xmath40 correspondingly .",
    "the aspiration levels are defined depending on the scales of function values : @xmath49 , @xmath50 .",
    "the p - algorithm , based on the gaussian model with estimated parameters , is strongly homogeneous .    according to the definition of @xmath51 the following equalities are valid @xmath52    taking into account the relation between @xmath53 and @xmath54 and the corresponding relations between the estimates of @xmath14 and @xmath55 , equalities ( [ v ] )",
    "can be extended as follows @xmath56 the equality between @xmath33 and @xmath32 means that the sequence of points generated by the p - algorithm is invariant with respect to the scaling of the objective function values . the strong homogeneity of the p - algorithm is proven .",
    "as shown in @xcite , the p - algorithm and the radial basis function algorithm are equivalent under very general assumptions .",
    "therefore the statement on the strong homogeneity of the p - algorithm is also valid for the radial basis function algorithm .",
    "statistical models of objective functions are also used to construct bayesian algorithms @xcite .",
    "let a gaussian stochastic function @xmath13 be chosen for the statistical model as in section  [ sec : p ] .",
    "an implementable version of the bayesian algorithm is the so called one - step bayesian algorithm defined as follows : @xmath57    the one - step bayesian algorithm , based on the gaussian model with estimated parameters , is strongly homogeneous .",
    "the value of the objective function is computed by the one - step bayesian algorithm at the point of maximum average improvement ( [ b ] ) .",
    "the formula of conditional mean in ( [ b ] ) can be rewritten as follows @xmath58 where @xmath59 denotes the gaussian probability density with the mean value @xmath14 and variance @xmath15 . for simplicity ,",
    "we use in this formula and hereinafter the traditional symbol @xmath60 . obviously ,",
    "when one starts to work in the framework of the infinite arithmetic @xcite , it should be substituted by an appropriate infinite number that has been defined a priori by the chosen statistical model .",
    "integration by parts in ( [ b1 ] ) results in the following formula @xmath61 where @xmath62 is the laplace integral : @xmath63 . from the formulae ( [ cond ] ) , ( [ v1 ] ) ,",
    "the equalities @xmath64 follow implying the invariance of the sequence @xmath65 generated by the one - step bayesian algorithm with respect to the scaling of values of the objective function .",
    "the strong homogeneity of the one - step bayesian algorithm is proven .",
    "although the invariance of the whole optimization process with respect to affine scaling of objective function values seems very natural , not all global optimization algorithms are strongly homogeneous .",
    "for example , the rather popular algorithm direct @xcite is not strongly homogeneous .",
    "we are not going to investigate in detail the properties of direct related to the scaling of objective function values . instead an example is presented contradicting the necessary conditions of strong homogeneity .",
    "for the sake of simplicity let us consider the one - dimension version of direct .",
    "let the feasible region ( interval ) be partitioned into subintervals @xmath66 $ ] , @xmath67 .",
    "the objective function values computed at the points @xmath68 are supposed positive , @xmath69 ; denote @xmath70 .",
    "a @xmath71-th subinterval is said to be potentially optimal if there exists a constant @xmath72 such that @xmath73 where @xmath74 , and @xmath75 is a constant defining the requested relative improvement , @xmath76 .",
    "all potentially optimal subintervals are subdivided at the current iteration .",
    "let us consider the iteration where the potentially optimal @xmath71-th subinterval is not the longest one .",
    "then @xmath77 for all @xmath78 where @xmath79 .",
    "otherwise there exists a constant @xmath80 such that @xmath81    the values @xmath82 and @xmath83 corresponding to the minimum in ( [ l2 ] ) are denoted as @xmath84 and @xmath85 correspondingly , i.e. @xmath86    let the values of the function @xmath87 be computed at the points @xmath78 , and assume that the following inequality @xmath88 is valid , where @xmath89    for the data related to @xmath90 the following inequality holds : @xmath91 and a constant @xmath80 satisfying the inequalities @xmath92 can not exist .",
    "therefore the @xmath71-th subinterval for the function @xmath90 is not potentially optimal because necessary conditions ( analogous to ( [ l2 ] ) and ( [ l3 ] ) for the function @xmath1 ) are not satisfied .",
    "to demonstrate the strong homogeneity of the p - algorithm an example of one dimensional optimization is considered . for a statistical model the stationary gaussian stochastic",
    "function with correlation function @xmath93 is chosen .",
    "let the values of the first objective function ( say @xmath1 ) computed at the points ( 0 , 0.2 , 0.5 , 0.9 , 1 ) be equal to ( -0.8 , -0.9 , -0.65 , -0.85 , -0.55 ) , and the values of the second objective function ( say @xmath90 ) be equal to ( 0 , -0.4 , 0.6 , -0.2 , 0.99 ) .",
    "the graphs of the conditional mean and conditional standard deviation for both sets of data are presented in figure  [ fig:1 ] . in the section of figure  [ fig:1 ] showing the conditional means , the horizontal lines are drawn at the levels @xmath94 and @xmath95 correspondingly .        in spite of the obvious difference in the data , the functions expressing the probability of improvement for both cases coincide . therefore , their maximizers which define the next points of function evaluations also coincide .",
    "this coincidence is implied by the strong homogeneity of the p - algorithm and the following relation : @xmath96 , where the values of @xmath97 up to five decimal digits are equal to @xmath98 .",
    "both the p - algorithm and the one - step bayesian algorithm are strongly homogeneous .",
    "the optimization results by these algorithms are invariant with respect to affine scaling of values of the objective function .",
    "the implementations of these algorithms using the conventional computer arithmetic combined with the scaling of function values , using the arithmetic of infinity , are applicable to the objective functions with either infinite or infinitesimal values .",
    "the optimization results , obtained in this way , would be identical with the results obtained applying the implementations of the algorithms in the arithmetic of infinity .",
    "the valuable remarks of two unknown referees facilitated a significant improvement of the presentation of results .",
    "jones d.r .",
    "( 1993 ) lipschitzian optimization without the lipschitz constant , journal of optimization theory and applications , vol.79 ( 1 ) , 157181 .",
    "mockus j. ( 1972 ) on bayesian methods of search for extremum , avtomatika i vychislitelnaja tekhnika , no.3 , 53 - 62 , ( in russian ) .",
    "mockus j. ( 1988 ) bayesian approach to global optimization , kluwer academic publishers , dodrecht .",
    "sergeyev ya.d .",
    "( 2008 ) a new applied approach for executing computations with infinite and infinitesimal quantities , informatica , vol.19(4 ) , 567 - 596 .",
    "sergeyev ya.d .",
    "( 2009 ) numerical computations and mathematical modelling with infinite and infinitesimal numbers , journal of applied mathematics and computing , vol.29 , 177 - 195 .",
    "sergeyev ya.d .",
    "( 2010 ) lagrange lecture : methodology of numerical computations with infinities and infinitesimals , rendiconti del seminario matematico delluniversit e del politecnico di torino , vol.68(2 ) , 95113 .",
    "strongin r. , sergeyev ya.d .",
    "( 2000 ) global optimization with non - convex constraints , kluwer academic publishers , dodrecht .",
    "trn a. , ilinskas a. ( 1989 ) global optimization , lecture notes in computer science , vol.350 , 1 - 255 .",
    "ilinskas a. ( 1982 ) axiomatic approach to statistical models and their use in multimodal optimization theory , mathematical programming , vol.22 , 104 - 116 .",
    "ilinskas a. ( 1985 ) axiomatic characterization of a global optimization algorithm and investigation of its search strategies , operations research letters , vol.4 , 35 - 39 .",
    "ilinskas a. ( 2010 ) on similarities between two models of global optimization : statistical models and radial basis functions , journal of global optimization , vol.48 , 173 - 182 .",
    "ilinskas a. ( 2011 ) small sample estimation of parameters for wiener process with noise , communications in statistics - theory and methods , vol .",
    "40(16 ) , 3020 - 3028 . ilinskas a , . ilinskas j. ( 2010 ) interval arithmetic based optimization in nonlinear regression , informatica , vol.21(1 ) , 149 - 158 ."
  ],
  "abstract_text": [
    "<S> the implementation of global optimization algorithms , using the arithmetic of infinity , is considered . </S>",
    "<S> a relatively simple version of implementation is proposed for the algorithms that possess the introduced property of strong homogeneity . </S>",
    "<S> it is shown that the p - algorithm and the one - step bayesian algorithm are strongly homogeneous .    </S>",
    "<S> arithmetic of infinity , global optimization , statistical models </S>"
  ]
}