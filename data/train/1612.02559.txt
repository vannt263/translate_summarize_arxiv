{
  "article_text": [
    "convolutional neural networks  ( cnns ) , trained on large scale data , have significantly advanced the state - of - the - art on traditional vision problems such as object recognition  @xcite and object detection  @xcite .",
    "success of these networks is mainly due to their high selectivity for semantically meaningful visual concepts , , objects and object parts  @xcite .",
    "in addition to ensuring good performance on the problem of interest , this property of cnns also allows for _ transfer _ of knowledge to several other vision tasks  @xcite .",
    "the object recognition network of  @xcite , , has been successfully used for object detection  @xcite , scene classification  @xcite , texture classification  @xcite and domain adaptation  @xcite , using various transfer mechanisms .    0.5ex",
    "cnn - based transfer is generally achieved either by _ finetuning _ a pre - trained network , such as in  @xcite , on a new image dataset or by designing a new image representation on such a dataset based on the activations of the pre - trained network layers  @xcite .",
    "recent proposals of transfer have shown highly competitive performance on different predictive tasks with a modest amount of new data ( as few as 50 images per class ) .",
    "the effectiveness of transfer - based methods , however , has not yet been tested under more severe constrains such as in a _",
    "few shot _ or a _ one shot _ learning scenario . in these problems",
    ", the number of examples available for learning may be as few as one per class .",
    "fine - tuning a pre - trained cnn with millions of parameters to such inadequate datasets is clearly not a viable option .",
    "a one - shot classifier trained on cnn activations will also be prone to over - fitting due to the high dimensionality of the feature space . the only way to solve the problem of limited data is to _ augment _ the training corpus by generating more examples for the given classes .",
    "0.5ex while augmentation techniques can be as simple as flipping , rotating , adding noise , or extracting random crops from images @xcite , _ task - specific _ , or _",
    "augmentation strategies @xcite have the potential to generate more realistic synthetic samples .",
    "this is a particularly important issue , since performance of cnns heavily relies on sufficient coverage of the variability that we expect in unseen testing data . in scene recognition , , we desire sufficient variability in the constellation and transient states of scene categories ( @xcite ) , whereas in object recognition , we desire variability in the specific incarnations of certain objects , lighting conditions , pose , or depth , just to name a few . unfortunately , this variability is often dataset - specific and can cause substantial bias in recognition results @xcite .    0.5ex",
    "an important observation in the context of our work is that augmentation is typically performed on an image , or video level .",
    "while this is not a problem with simple techniques , such as flipping or cropping , it can become computationally expensive if more elaborate augmentation techniques are used .",
    "we argue that , in specific problem settings , augmentation might as well be performed in _ feature space _ , especially in situations where features are input to subsequent learning steps .",
    "this is common , , in recognition tasks , where the softmax output of trained cnns is often not used directly , but activations at earlier layers are input to an external discriminant classifier .    0.5ex",
    "* contribution .",
    "* we propose an approach to augment the training set with _ feature descriptors _ instead of images .",
    "specifically , we advocate an augmentation technique that learns to synthesize features , guided by desired values for a set of object attributes , such as depth or pose .",
    "an illustration of this concept is shown in fig .",
    "[ fig : intro ] .",
    "we first train a fast rcnn  @xcite detector to identify objects in 2d images .",
    "this is followed by training a neural network regressor which predicts the 3d attributes of a detected object , namely its depth from the camera plane and pose .",
    "an encoder - decoder network is then trained which , for a detected object at a certain depth and pose , will `` hallucinate '' the changes in its rcnn features for a set of desired depths / poses . using this architecture , for a new image , we are able to augment existing feature descriptors by an auxiliary set of features that correspond to the object changing its 3d position .",
    "since our framework relies on object attributes to guide augmentation , we refer to it as _ attribute - guided augmentation ( aga)_.    0.5ex * organization .",
    "[ section : relatedwork ]  reviews prior work . sec .",
    "[ section : architecture ] introduces the proposed encoder - decoder architecture for attribute - guided augmentation .",
    "[ section : experiments ] studies the building blocks of this approach in detail and demonstrates that aga in feature space improves one - shot unseen object recognition and object - based scene recognition performance .",
    "[ section : discussion ] concludes the paper with a discussion and an outlook on potential future directions .",
    "our review of related work primarily focuses on _ data augmentation _ strategies .",
    "while many techniques have been proposed in the context of training deep neural networks to avoid overfitting and to increase variability in the data , other ( sometimes closely related ) techniques have previously appeared in the context of one - shot and transfer learning .",
    "we can roughly group existing techniques into ( 1 ) _ generic _ , computationally cheap approaches and ( 2 ) task - specific , or guided approaches that are typically more computationally involved .",
    "0.5ex as a representative of the first group , krizhevsky @xcite leverage a set of label - preserving transformations , such as patch extraction + reflections , and pca - based intensity transformations , to increase training sample size .",
    "similar techniques are used by zeiler and fergus @xcite . in @xcite ,",
    "chatfield and zisserman demonstrate that the augmentation techniques of @xcite are not only beneficial for training deep architectures , but shallow learning approaches equally benefit from such _ simple _ and _ generic _ schemes .    0.5ex in the second category of guided - augmentation techniques , many approaches have recently been proposed . in @xcite , ,",
    "charalambous and bharath employ guided - augmentation in the context of gait recognition .",
    "the authors suggest to simulate synthetic gait video data ( obtained from avatars ) with respect to various confounding factors ( such as clothing , hair , etc . ) to extend the training corpus .",
    "similar in spirit , rogez and schmid @xcite propose an image - based synthesis engine for augmenting existing 2d human pose data by photorealistic images with greater pose variability .",
    "this is done by leveraging 3d motion capture ( mocap ) data . in @xcite ,",
    "peng also use 3d data , in the form of synthetic cad models , to render synthetic images of objects ( with varying pose , texture , background ) that are then used to train cnns for object detection .",
    "it is shown that synthetic data is beneficial , especially in situations where few ( or no ) training instances are available , but 3d cad models are .",
    "su @xcite follow a similar pipeline of rendering images from 3d models for viewpoint estimation , however , with substantially more synthetic data obtained , , by deforming existing 3d models before rendering .",
    "another ( data - driven ) guided augmentation technique is introduced by hauberg @xcite .",
    "the authors propose to _ learn _ class - specific transformations from external training data , instead of manually specifying transformations as in @xcite .",
    "the learned transformations are then applied to the samples of each class . specifically , diffeomorphisms are learned from data and encouraging results are demonstrated in the context of digit recognition on mnist .",
    "notably , this strategy is conceptually similar to earlier work by miller @xcite on one - shot learning , where the authors synthesize additional data for digit images via an iterative process , called _",
    "congealing_. during that process , external images of a given category are aligned by optimizing over a class of geometric transforms ( , affine transforms ) .",
    "these transformations are then applied to single instances of the new classes to increase data for one - shot learning .    0.5ex",
    "marginally related to our work , we remark that alternative approaches to implicitly learn spatial transformations have been proposed .",
    "for instance , jaderberg @xcite introduce _ spatial transformer",
    "_ modules that can be injected into existing deep architectures to implicitly capture spatial transformations inherent in the data , thereby improving invariance to this class of transformations .",
    "0.5ex while _ all _ previously discussed methods essentially propose _ image - level _ augmentation to train cnns , our approach is different in that we perform augmentation in _ feature space_. along these lines , the approach of kwitt @xcite is conceptually similar to our work . in detail , the authors suggest to learn how features change as a function of the strength of certain transient attributes ( such as sunny , cloudy , or foggy ) in a scene - recognition context .",
    "these models are then transferred to previously unseen data for one - shot recognition .",
    "however , different to our approach , the learned models are simple linear regressors and learning is done in a _ scene - class specific _ manner .",
    "in contrast , we learn deep non - linear models in a _ class - agnostic _ manner which enables straightforward application to object recognition , without the requirement of a direct relation of new classes to classes in the external training data .",
    "* notation . * to describe our architecture , we let @xmath0 denote our feature space , @xmath1 denotes a feature descriptor ( , a representation of an object ) and @xmath2 denotes a set of attributes that are available for objects in the external training corpus .",
    "further , we let @xmath3 denote the value of an attribute @xmath4 , associated with @xmath5 . we assume ( 1 ) that this attribute can be predicted by an attribute regressor @xmath6 and ( 2 ) that it is possible that its range can be divided into @xmath7 intervals @xmath8 $ ] , where @xmath9 denote the lower and upper bounds of the @xmath10-th interval .",
    "the set of desired object attribute values is @xmath11 .    0.5ex * objective . * on a conceptual level , we aim for a synthesis function @xmath12 which , given a desired attribute value @xmath13 for some object attribute @xmath14 , transforms object features @xmath15 such that the attribute value changes in a controlled manner to a desired target value @xmath13 .",
    "more formally , we aim to learn @xmath16 since , the formulation in eq .",
    "is overly generic , we constrain the problem to the case where we learn different @xmath17 for a selection of intervals @xmath8 $ ] within the range of attribute @xmath14 and a selection of @xmath18 desired object attribute values @xmath19 . in our illustration of fig .",
    "[ fig : intro ] , , we have one interval @xmath20=[1,2]$ ] and one attribute ( depth ) with target value 3[m ] .",
    "while learning separate synthesis functions simplifies the problem , it requires a good a - priori _ attribute predictor _ , since , otherwise , we could not decide which @xmath17 to use . during testing , we ( 1 ) predict the object s attribute value from its original feature @xmath5 , , @xmath21 , and then ( 2 ) synthesize additional features as @xmath22 for @xmath23 . in case @xmath24",
    "$ ] , @xmath17 is not used .",
    "next , we discuss each component of this approach in detail .",
    "an essential part of our architecture is the attribute regressor @xmath6 for a given attribute @xmath14 .",
    "this regressor takes as input a feature @xmath5 and predicts its strength or value , , @xmath25 .",
    "while @xmath26 could , in principle , be implemented by a variety of approaches , such as support vector regression @xcite or gaussian processes @xcite , we use a two - layer neural network instead , to accomplish this task .",
    "this is not an arbitrary choice , as it will later enable us to easily re - use this building block in the learning stage of the synthesis function(s ) @xmath17 .",
    "the architecture of the attribute regressor is shown in fig .",
    "[ fig : cor ] , consisting of two linear layers , interleaved by batch normalization @xcite and rectified linear units ( relu ) @xcite .",
    "while this architecture is admittedly simple , adding more layers did not lead to significantly better results in our experiments .",
    "nevertheless , the design of this component is problem - specific and could easily be replaced by more complex variants , depending on the characteristics of the attributes that need to be predicted .    0.5ex * learning . *",
    "the attribute regressor can easily be trained from a collection of @xmath27 training tuples @xmath28 for each attribute .",
    "as the task of the attribute regressor is to predict in which interval the original feature @xmath5 resides , we do not need to organize the training data into intervals in this step .      to implement @xmath12 , we design an encoder - decoder architecture , reminiscent of a conventional autoencoder @xcite .",
    "our objective , however , is not to encode and then reconstruct the input , but to produce an output that resembles a feature descriptor of an object at a desired attribute value .    in other words ,",
    "the _ encoder _ essentially learns to extract the essence of features ; the _ decoder _ then takes the encoding and decodes it to the desired result . in general",
    ", we can formulate the optimization problem as @xmath29 where the minimization is over a suitable class of functions @xmath30 .",
    "notably , when implementing @xmath12 as an encoder - decoder network with an appended ( pre - trained ) attribute predictor ( see fig .",
    "[ fig : edn ] ) and loss @xmath31 , we have little control over the decoding results in the sense that we can not guarantee that the _ identity _ of the input is preserved .",
    "this means that features from a particular object class might map to features that are no longer recognizable as this class , as the encoder - decoder will _ only _ learn to `` fool '' the attribute predictor @xmath26 .",
    "for that reason , we add a _ regularizer _ to the objective of eq .  , , we require the decoding result to be close , , in the @xmath32 norm , to the input .",
    "this changes the optimization problem of eq .   to @xmath33",
    "interpreted differently , this resembles the loss of an autoencoder network with an added _ target attribute mismatch _ penalty . the encoder - decoder network that implements the function class @xmath30 to learn @xmath12",
    "is shown in fig .",
    "[ fig : edn ] .",
    "the core building block is a combination of a linear layer , batch normalization , elu @xcite , followed by dropout @xcite .",
    "after the final linear layer , we add one relu layer to enforce @xmath34 .",
    "0.5ex * learning .",
    "* training the encoder - decoder network of fig .",
    "[ fig : edn ] requires an a - priori trained attribute regressor @xmath26 for each given attribute @xmath4 . during training",
    ", this attribute regressor is appended to the network and its _ weights are frozen_. hence , only the encoder - decoder weights are updated .",
    "to train one @xmath17 for each interval @xmath8 $ ] of the object attribute range and a desired object attribute value @xmath19 , we partition the training data from the external corpus into subsets @xmath35 , such that @xmath36 $ ] .",
    "one @xmath17 is learned from @xmath35 for each desired object attribute value @xmath19 . as training",
    "is in feature space @xmath0 , we have no convolutional layers and consequently training is computationally cheap . for testing",
    ", the attribute regressor is removed and only the trained encoder - decoder network ( implementing @xmath17 ) is used to synthesize features .",
    "consequently , given @xmath37 attributes , @xmath7 intervals per attribute and @xmath38 target values for an object attribute , we obtain @xmath39 synthesis functions .",
    "we first discuss the generation of adequate training data for the encoder - decoder network , then evaluate every component of our architecture separately and eventually demonstrate its utility on ( 1 ) one - shot object recognition in a transfer learning setting and ( 2 ) one - shot scene recognition .",
    "0.5ex * dataset .",
    "* we use the sun rgb - d dataset from song @xcite .",
    "this dataset contains 10335 rgb images with depth maps , as well as detailed annotations for more than 1000 objects in the form of 2d and 3d bounding boxes . in our setup , we use object depth and pose as our attributes , , @xmath40 . for each ground - truth 3d",
    "bounding box , we extract the depth value at its centroid and obtain pose information as the rotation of the 3d bounding box along the @xmath41-axis . in all experiments ,",
    "we use the first 5335 images as our _ external database _ , , the database for which we assume availability of attribute annotations .",
    "the remaining 5000 images are used for testing ; more details are given in the specific experiments .",
    "0.5ex * training data . *",
    "notably , in sun rgb - d , the number of instances of each object class are not evenly distributed , simply because this dataset was not specifically designed for object recognition tasks .",
    "consequently , images are also not object - centric , meaning that there is substantial variation in the location of objects , as well as the depth and pose at which they occur .",
    "this makes it difficult to extract a sufficient and balanced number of feature descriptors per object class , if we would _ only _ use the ground - truth bounding boxes to extract training data .",
    "we circumvent this problem by leveraging the fast rcnn detector of @xcite with object proposals generated by selective search @xcite . in detail , we finetune the imagenet model from @xcite to sun rgb - d , using the same 19 objects as in @xcite .",
    "we then run the detector on all images from our training split and keep the proposals with detection scores @xmath42 and a sufficient overlap ( measured by the iou @xmath430.5 ) with the 2d ground - truth bounding boxes .",
    "this is a simple augmentation technique to increase the amount of available training data .",
    "the associated rcnn activations ( at the ` fc7 ` layer ) are then used as our features @xmath5 . each proposal that remains after overlap and score thresholding",
    "is annotated by the attribute information of the corresponding ground - truth bounding box in 3d .",
    "as this strategy generates a larger number of descriptors ( compared to the number of ground - truth bounding boxes ) , we can evenly balance the training data in the sense that we can select an equal number of detections per object class for training ( 1 ) the attribute regressor and ( 2 ) the encoder - decoder network .",
    "training data generation is illustrated in fig .",
    "[ fig : trainingdata ] on four example images .",
    "0.5ex * implementation . * the attribute regressor and the encoder - decoder network are implemented in ` torch ` .",
    "all models are trained using ` adam ` @xcite . for the attribute regressor",
    ", we train for 30 epochs with a batch size of 300 and a learning rate of @xmath44 .",
    "the encoder - decoder network is also trained for 30 epochs with the same learning rate , but with a batch size of 128 . the dropout probability during training",
    "is set to @xmath45 .",
    "no dropout is used for testing . for our classification experiments",
    ", we use a linear c - svm , as implemented in ` liblinear ` @xcite . on a linux system , running ubuntu 16.04 , with 128 gb of memory and one nvidia titan x , training one model ( , one @xmath17 ) takes @xmath46 seconds .",
    "the relatively low demand on computational resources highlights the advantage of aga in feature space , as no convolutional layers need to be trained .",
    "all trained models are publicly available at anonymousurl .      while our strategy , aga , to data augmentation is _ agnostic _ to the object classes , in both the training and testing dataset , it is interesting to compare attribute prediction performance to the case where we train _ object - specific _ regressors . in other words , we compare object - agnostic training to training one regressor @xmath47 for each object class in @xmath48 .",
    "this allows us to quantify the potential loss in prediction performance in the object - agnostic setting .",
    "table  [ table : maecor ] lists the median - absolute - error ( mae ) of depth ( in [ m ] ) and pose ( in [ deg ] ) prediction per object .",
    "we train on instances of 19 object classes ( @xmath48 ) in our training split of sun rgb - d and test on instances of the same object classes , but extracted from the testing split .",
    "as we can see , training in an object - specific manner leads to a lower mae for most objects , both for depth and pose .",
    "this is not surprising , since the training data is more specialized to each particular object , which essentially amounts to solving simpler sub - problems .",
    "however , in many cases , especially for depth , the object - agnostic regressor performs on par , except for object classes with fewer training samples ( , lamp , door , etc . ) .",
    "we also remark that , in general , pose estimation from 2d data is a substantially harder problem than depth estimation ( which works remarkably well , even on a per - pixel level , @xcite ) .",
    "nevertheless , our recognition experiments ( in secs . [",
    "subsection : one - shot ] and [ section : exp_scenes ] ) show that even with mediocre performance of the pose predictor ( due to symmetry issues , etc . ) , augmentation along this dimension is still beneficial .",
    "we assess the performance of the feature regressor(s ) @xmath17 , , the part of our architecture from fig .  [ fig : edn ] that is used to generate synthetic features . in all experiments ,",
    "we use an overlapping sliding window to bin the range of each attribute @xmath4 into @xmath7 intervals @xmath8 $ ] . in case of ` depth ` , we set @xmath49 = [ 0,1]$ ] and shift each interval by @xmath50 meter ; in case of ` pose ` , we set @xmath49 = [ 0^\\circ,45^\\circ]$ ] and shift by @xmath51 .",
    "we generate as many intervals as needed to cover the full range of the attribute values in the training data .",
    "the bin - width / step - size were set to ensure a roughly equal number of features in each bin .",
    "for augmentation , we choose @xmath52 as target attribute values for ` depth ` and @xmath53 for ` pose ` .",
    "this results in @xmath54 target values for ` depth ` and @xmath55 for ` pose ` .",
    "0.5ex we use two separate evaluation metrics to assess the performance of @xmath17 . _",
    "first _ , we are interested in _ how well _ the feature regressor can generate features that correspond to the desired attribute target values . to accomplish this , we run each synthetic feature @xmath56 through the attribute predictor and assess the mae , , @xmath57 , over all attribute targets @xmath13",
    ". table  [ table : ednnonseen ] lists the average mae , per object , for ( 1 ) features from object classes that were _ seen _ in the training data and ( 2 ) features from objects that we have never seen before .",
    "as wee can see from table [ table : ednnonseen ] , mae s for seen and unseen objects are similar , indicating that the encoder - decoder has learned to synthesize features , such that @xmath58 .",
    "0.5ex _ second _ , we are interested in _ how much _ synthesized features _",
    "differ _ from original features .",
    "while we can not evaluate this directly ( as we do not have data from one particular object instance at multiple depths and poses ) , we can assess how `` close '' synthesized features are to the original features .",
    "the intuition here is that closeness in feature space is indicative of object - identity preserving synthesis . in principle",
    ", we could simply evaluate @xmath59 , however , the @xmath32 norm is hard to interpret .",
    "instead , we compute the pearson correlation coefficient @xmath60 between each original feature and its synthesized variants , , @xmath61 . as @xmath60 ranges from @xmath62 $ ] , high values indicate a strong linear relationship to the original features .",
    "results are reported in table  [ table : ednnonseen ] .",
    "similar to our previous results for mae , we observe that @xmath60 , when averaged over all objects , is slightly lower for objects that did not appear in the training data . this decrease in correlation , however , is relatively small .",
    "0.5ex in summary , we conclude that these results warrant the use of @xmath17 on feature descriptors from object classes that have _ not _ appeared in the training corpus .",
    "this enables us to test @xmath17 in transfer learning setups , as we will see in the following one - shot experiments of secs .",
    "[ subsection : one - shot ] and [ section : exp_scenes ] .",
    "first , we demonstrate the utility of our approach on the problem of one - shot object recognition in a transfer learning setup .",
    "specifically , we aim to learn attribute - guided augmenters @xmath17 from instances of object classes that are available in an external , annotated database ( in our case , sun rgb - d ) .",
    "we denote this collection of object classes as our _ source classes _ @xmath48 .",
    "given one instance from a collection of completely different object classes , denoted as the _ target classes _ @xmath63 , we aim to train a discriminant classifier @xmath64 on @xmath63 , , @xmath65 .",
    "hence , in this setting , @xmath66 .",
    "note that no attribute annotations for instances of object classes in @xmath63 are available .",
    "this can be considered a variant of transfer learning , since we transfer knowledge from object classes in @xmath48 to instances of object classes in @xmath63 , _ without _ any prior knowledge about @xmath63 .    0.5ex * setup .",
    "* we evaluate one - shot object recognition performance on three collections of previously unseen object classes in the following setup : first , we randomly select two sets of 10 object classes and ensure that each object class has at least 100 samples in the testing split of sun rgb - d .",
    "we further ensure that no object class is in @xmath48 .",
    "this guarantees ( 1 ) that we have never seen the image , nor ( 2 ) the object class during training . since",
    ", sun rgb - d does not have object - centric images , we use the ground - truth bounding boxes to obtain the actual object crops .",
    "this allows us to tease out the benefit of augmentation without having to deal with confounding factors such as background noise .",
    "the two sets of object classes are denoted @xmath67 = \\{picture , whiteboard , fridge , counter , books , stove , cabinet , printer , computer , ottoman } ] and @xmath68 = \\{mug , telephone , bowl , bottle , scanner , microwave , coffee table , recycle bin , cart , bench } ] .",
    "we additionally compile a third set of target classes @xmath69 and remark that @xmath70 .",
    "consequently , we have two 10-class problems and one 20-class problem . for each object image in @xmath71 , we then collect rcnn ` fc7 ` features .",
    "_ baseline _ , we `` train '' a linear c - svm ( with @xmath72-normalized features ) using only the single instances of each object class in @xmath71 ( svm cost fixed to 1 ) .",
    "exactly the same parameter settings of the svm are then used to train on the single instances + features synthesized by aga .",
    "we repeat the selection of one - shot instances 500 times and report the average recognition accuracy .    0.5ex _ remark . _",
    "the design of this experiment is similar to ( * ? ? ?",
    "* section 4.3 . ) , with the exceptions that we ( 1 ) _ do not _ detect objects , ( 2 ) augmentation is performed in feature space and ( 3 ) no object - specific information is available .",
    "the latter is important , since @xcite assumes the existence of 3d cad models for objects in @xmath71 from which synthetic images can be rendered . in our case",
    ", augmentation does not require any a - priori information about the objects classes .",
    ".[table : oneshot ] _ recognition accuracy _",
    "( averaged over 500 trials ) for three one - shot object recognition problems .",
    "the number in parentheses indicates the number of classes .",
    "a  indicates that the result is statistically different ( at 5% significance ) from the _ baseline_. + * * d * * indicates adding ` depth`-aug .",
    "features to the one - shot instances ; + * * p * * indicates addition of ` pose`-aug . features and + * * d * * , * p * denotes adding a combination of ` depth`-/`pose`-aug . features . [ cols=\">,^,^,^,^ \" , ]     0.5ex * results .",
    "* table  [ table : oneshot ] lists the classification accuracy for the different sets of one - shot training data .",
    "_ first _ , using original one - shot instances augmented by ` depth`-guided features ( + * * d * * ) ; _ second _ , using original features + ` pose`-guided features ( + * * d * * ) and _ third _ , a combination of both ( + * * d * * , * p * ) ; in general , we observe that adding aga synthesized features improves recognition accuracy over the _ baseline _ in all cases . for ` depth`-augmented features , gains range from 3 - 5 percentage points , for ` pose`-augmented features , gains range from 2 - 4 percentage points on average .",
    "we attribute this effect to the difficulty in predicting object pose from 2d data , as can be seen from table  [ table : maecor ] .",
    "nevertheless , in both augmentation settings , the gains are statistically significant ( the _ baseline _ ) , as evaluated by a wilcoxn rank sum test for equal medians @xcite at @xmath73 significance ( indicated by  in table  [ table : oneshot ] ) . adding both ` depth`- _ and _ ` pose`-augmented features to the original one - shot features achieves the greatest improvement in recognition accuracy , ranging from 4 - 6 percentage points .",
    "this indicates that information from depth and pose is complementary and allows for better coverage of the feature space .",
    "notably , we also experimented with the metric - learning approach of fink  @xcite which only lead to negligible gains over the _ baseline _ ( , 33.85% on @xmath67 ) .      * motivation .",
    "* we can also use aga for a different type of transfer , namely the transfer from object detection networks to one - shot scene recognition .",
    "although , object detection is a challenging task in itself , significant progress is made , every year , in competitions such as the imagenet challenge .",
    "extending the gains in object detection to other related problems , such as scene recognition , is therefore quite appealing . a system that uses an accurate object detector such as an rcnn  @xcite to perform scene recognition , could generate comprehensive annotations for an image in one forward pass",
    "an object detector that supports one shot - scene recognition could do so with the least amount of additional data .",
    "it must be noted that such systems are different from object recognition based methods such as  @xcite , where explicit detection of objects is not necessary .",
    "they apply filters from object recognition cnns to several regions of images and extract features from all of them , whether or not an object is found .",
    "the data available to them is therefore enough to learn complex descriptors such as fisher vectors ( fvs ) .",
    "a detector , on the other hand , may produce very few features from an image , based on the number of objects found .",
    "aga is tailor - made for such scenarios where features from an rcnn - detected object can be augmented .    0.5ex",
    "* setup . * to evaluate aga in this setting , we select a 25-class subset of mit indoor  @xcite , which may contain objects that the rcnn is trained for .",
    "the reason for this choice is our reliance on a detection cnn , which has a vocabulary of 19 objects from sun rgb - d . at present",
    ", this is the largest such dataset that provides objects and their 3d attributes .",
    "the system can be extended easily to accommodate more scene classes if a larger rgb - d object dataset becomes available .",
    "as the rcnn produces very few detections per scene image , the best approach , without augmentation , is to perform pooling of rcnn features from proposals into a fixed - size representation .",
    "we used max - pooling as our _ baseline_. upon augmentation , using predicted depth/ pose , an image has enough rcnn features to compute a gmm - based fv .",
    "for this , we use the experimental settings in  @xcite .",
    "the fvs are denoted as ` aug .",
    "fv(+d ) ` and ` aug .",
    "fv(+p ) ` , based on the attribute used to guide the augmentation .",
    "one - shot classification is performed using a linear c - svm with a fixed parameter .",
    "0.5ex * results . * table  [ table : oneshot_scenes ] lists the averaged one - shot recognition accuracy over multiple iterations .",
    "benefits of the proposed aga are clear from the results , as both aug .",
    "fvs perform better than the max - pooling baseline by 0.5 - 1% points .",
    "training on a combination ( concatenated vector ) of the augmented fvs and max - pooling , denoted as ` aga  cl-1 ` , ` aga  cl-2 ` and ` aga  cl-3 ` further improves by about 1 - 2% points . finally , we combined our augmented fvs with the state - of - the - art semantic fv of  @xcite and places cnn features  @xcite for one - shot classification . both combinations , denoted `",
    "aga  sem - fv ` and ` aga  places ` , improved by a non - trivial margin ( @xmath741% points ) .",
    "we presented an approach toward attribute - guided augmentation in feature space .",
    "experiments show that object attributes , such as pose / depth , are beneficial in the context of one - shot recognition , , an extreme case of limited training data .",
    "notably , even in case of mediocre performance of the attribute regressor ( , on pose ) , results indicate that synthesized features can still supply useful information to the classification process .",
    "while we do use bounding boxes to extract object crops from sun rgb - d in our object - recognition experiments , this is only done to clearly tease out the effect of augmentation . in principle , as our encoder - decoder is trained in an _ object - agnostic _ manner , no external knowledge about classes is required .",
    "0.5ex as sun rgb - d exhibits high variability in the range of both attributes , augmentation along these dimensions can indeed help classifier training",
    ". however , when variability is limited , , under controlled acquisition settings , the gains may be less apparent . in this case ,",
    "augmentation with respect to other object attributes might be required .",
    "0.5ex two aspects are specifically interesting for future work .",
    "_ first _ , replacing the attribute regressor for pose with a specifically tailored component will potentially improve learning of the synthesis function(s ) @xmath17 and consequently lead to more realistic synthetic samples .",
    "_ second _ , we conjecture that , as additional data with more annotated object classes and attributes becomes available ( , @xcite ) , the encoder - decoder can leverage more diverse samples and thus model feature changes with respect to the attribute values more accurately ."
  ],
  "abstract_text": [
    "<S> we consider the problem of data augmentation , i.e. , generating artificial samples to extend a given corpus of training data . </S>",
    "<S> specifically , we propose attributed - guided augmentation ( aga ) which learns a mapping that allows to synthesize data such that an attribute of a synthesized sample is at a desired value or strength . </S>",
    "<S> this is particularly interesting in situations where little data with no attribute annotation is available for learning , but we have access to a large external corpus of heavily annotated samples . while prior works primarily augment in the space of images , we propose to perform augmentation in feature space instead . </S>",
    "<S> we implement our approach as a deep encoder - decoder architecture that learns the synthesis function in an end - to - end manner . </S>",
    "<S> we demonstrate the utility of our approach on the problems of ( 1 ) one - shot object recognition in a transfer - learning setting where we have no prior knowledge of the new classes , as well as ( 2 ) object - based one - shot scene recognition . as external data , we leverage 3d depth and pose information from the sun rgb - d dataset . </S>",
    "<S> our experiments show that attribute - guided augmentation of high - level cnn features considerably improves one - shot recognition performance on both problems . </S>"
  ]
}