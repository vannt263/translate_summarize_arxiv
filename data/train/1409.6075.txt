{
  "article_text": [
    "[ [ section ] ]    a good overview of the state of the art can be found in @xcite .",
    "that paper introduces the general problem item was designed to solve , and very briefly surveys the various methods available in the literature that can solve this problem .",
    "[ [ section-1 ] ]    consider the usual regression situation : we have data @xmath0 for @xmath1 . here",
    "@xmath2 and @xmath3 are the regressor and response variables .",
    "suppose also that @xmath4 and @xmath5 is a probability vector . in shorthand ,",
    "@xmath6 and @xmath7 .    [",
    "[ section-2 ] ]    a model is some function @xmath8 whose output is a prediction for @xmath9 . in most common situations ,",
    "the model @xmath10 must be deduced from the data @xmath11 and @xmath12 , this process is referred to as fitting .",
    "then the model would be used to make predictions for @xmath13 with @xmath14 , i.e. predict @xmath15 for the data points where we only know @xmath16 , this is referred to as projection or running .",
    "item is one such model , but several others will also be discussed .",
    "[ [ section-3 ] ]    in addition to the requirements above , a two more factors are added from an engineering standpoint :    * a large amount of data is available ( at least 100,000 observations ) * the data has more than a few dimensions ( @xmath17 has at least 5 dimensions )    for one dimensional data , various solutions are available , but generally , people will simply look at the results manually and make any adjustments that are needed . beyond a few dimensions , this no longer works efficiently .",
    "it uses too much labor , and the projections on to the individual dimensions become less and less tractable . building up a model from a set of @xmath18 regressors",
    "requires roughly @xmath19 operations , as for each regressor added , all other regressors need to be checked . with a moderate number of regressors ( dozens to hundreds",
    ") this is too time consuming for people , but could be done by a computer .",
    "the primary goal of item is to perform this multidimensional analysis and fitting automatically . since human guidance",
    "is limited , it is important for the model to be able to resist overfitting while simultaneously using the data efficiently to produce a model as accurate as possible .",
    "the dataset must be large , because it will generally require a lot of data to support good predictions across many dimensions .",
    "[ [ section-4 ] ]    before proceeding further , this entire paper is devoted to producing good models .",
    "it is worth taking a brief detour to discuss what is meant by a good model . for this purpose , it is helpful to use a car analogy . for a person buying a car , there are several statistics that might be important .    1 .",
    "acceleration 2 .",
    "top speed 3 .",
    "fuel efficiency 4 .",
    "material efficiency 5 .",
    "automated construction    [ [ section-5 ] ]    each of these factors helps drive the appeal ( and cost ) of a given car .",
    "for instance , a car could be made out of carbon fiber and titanium , thus getting better speed and efficiency , but by using these extremely expensive materials , the cost may be prohibitive .",
    "similarly , a car could be constructed with extremely advanced manufacturing techniques , but if it can not be mass produced , then again the car would be too expensive for typical use . among these five characteristics , often improving",
    "one will make others worse .",
    "[ [ section-6 ] ]    this tradeoff will define a sort of efficient frontier , a 5 dimensional surface enclosing some volume .",
    "each point on this frontier represents some tradeoff among the 5 qualities , but there is no point reachable with this technology that is better in all 5 categories .",
    "one definition of the quality of automotive technology is to simply say that better technology allows for a larger frontier , one that completely encloses the frontier for an inferior set of technology .",
    "a model has a similar set of factors that together help to determine its quality .    1 .   fitting computational efficiency 2 .",
    "running computational efficiency 3 .",
    "data efficiency 4 .",
    "parameter efficiency 5",
    ".   automated construction    [ [ section-7 ] ]    a good model would be computationally inexpensive to fit and to run . just like in the case of a car , these are nearly the same requirement .",
    "a car with good acceleration will typically have a high top speed , similarly a model that is efficient to fit will typically ( though not always ) also be efficient to run .",
    "these two factors will be grouped together as computational efficiency .",
    "[ [ section-8 ] ]    data efficiency and parameter efficiency are similar concepts as well , so they will be grouped together as information theoretic efficiency .",
    "data efficiency means essentially that statistically significant features of the data will be included in the model .",
    "we require that a feature will be picked up by the model without requiring excessive amounts of data , it will be added as soon as enough data is present for it to be significant .",
    "the concept of parameter efficiency means that the model will not pick up extra features beyond those indicated by the data .",
    "essentially , this is a requirement that the model use as few parameters as possible , while simultaneously fitting the data as well as possible .",
    "obviously there is some trade off between these two features , but together they define a part of the efficient frontier for models .",
    "we would like the model to be on or near this frontier , and for this frontier to itself pass close to the true distribution of the data .",
    "[ [ section-9 ] ]    the last requirement is simply that the model be constructed automatically , with limited human involvement .",
    "the whole purpose of this exercise is to produce a good model without expending a large amount of human effort to do so .",
    "[ [ section-10 ] ]    it is not possible for a model to excel in all five of these areas .",
    "some models are better in one , or another , and for most distributions of data there is a definite limit to how high the information theoretic efficiency of a model can be , just as a car s fuel efficiency runs into thermodynamic limits .",
    "the goal of the item system is to construct models that are very good ( though by no means ideal ) in all 5 of these areas .",
    "[ [ section-11 ] ]    for the purposes of this paper , a residential mortgage model will be considered . in the united states , a residential mortgage is a loan made by a lender ( typically a bank ) to a borrower ( typically an individual ) for the purpose of buying residential real estate , typically a house , but sometimes a condo or other living space .",
    "when making such a loan , it is important for the lender to be able to project the probable future behavior of the borrower . in order to do that",
    ", the lender can examine a large dataset of historical loan behavior , and attempt to build a model based on this historical record .",
    "[ [ section-12 ] ]    one such dataset is the freddie mac loan level dataset @xcite , which will be used to fit this example model .",
    "this data set has approximately 600 million observations , so it is large enough to show an example of a real model .",
    "other similar data sets are also available , most of which contain one to several billion observations . since this is real data , projections based on it have actual economic value .",
    "for instance , projections of future behavior determine the interest rate offered to the borrower , and help the lender determine how to hedge efficiently .",
    "[ [ section-13 ] ]    the dataset is a collection of loan - month observations . by this , it is meant that each loan in the dataset has one observation per month . this observation is little more than the knowledge of whether or not the borrower made his payment that month .",
    "these observations will be considered independently , ignoring the fact that some of them are related to each other through the underlying loan .",
    "the logic here is that if the model is accurate and the dataset is comprehensive , then the loan - months will be independent conditional on the data presented . another way of saying this is that the model residuals will be independent .",
    "this is an assumption that can be relaxed , but that is outside the scope of this paper .",
    "[ [ section-14 ] ]    each of these loan - months has several regressors , including fico , several flags related to the type of mortgage , loan age , debt - to - income ratios , refinance incentive , and others .",
    "each loan - month also has a status .",
    "this status is a combination of the number of payments the borrower is in arrears , as well as some additional information about potential foreclosure proceedings and similar items . for the purposes of this paper ,",
    "only three statuses will be considered .",
    "* status `` c '' : the borrower is `` current '' , and is not behind on his payments * status `` p '' : the borrower has repaid the entire loan , the loan no longer exists * status `` 3 '' : the borrower is one payment behind schedule ( `` 30 days delinquent '' )    [ [ section-15 ] ]    now consider the status that the loan will have next month . if the loan - month is historical data , this value may be known , otherwise it it necessary to project it .",
    "the purpose of the example model is to predict next month s status for any loan - month in which we know this month s status . in this way",
    ", a markov chain can be built up , progressively projecting the status further and further into the future .",
    "[ [ section-16 ] ]    in each month , the borrower transitions from his current delinquency state to a new state depending on the number of payments made . for a borrower who is current ( status `` c '' )",
    ", he can make 1 payment ( remaining `` c '' ) , 0 payments ( to become `` 3 '' ) , or all the payments ( to become `` p '' ) .",
    "similarly , if a borrower is already in status `` 3 '' , he could become more delinquent by missing additional payments and so on .",
    "a separate model can be fit for each loan status , modeling the transitions available to loan - months in that status , so for the purpose of this paper it is enough to consider only the status `` c '' .    [",
    "[ section-17 ] ]    therefore , there are three transitions available , in shorthand written thus :    * @xmath20 going from current to current * @xmath21 going from current to prepaid * @xmath22 going from current to 30 days delinquent    [ [ section-18 ] ]    the transition @xmath20 is more common than the others by several orders of magnitude , but most of the economic behavior of these loans is related to the @xmath21 and @xmath22 transitions .",
    "[ [ section-19 ] ]    long before large datasets were available , mortgages were modeled using pool level approaches , which simply averaged a group of loans together and then applied the model .",
    "some of these models are still around , however they are quickly falling out of favor .",
    "the essential issue with the pool level approach is that it is subject to numerous problems related to this averaging .",
    "for instance , there is a world of difference between two pools with 700 fico , one where every loan has exactly a 700 and another where half the loans have 600 and the other half have 800 . in many cases ,",
    "the pool behavior is driven by just a handful of loans , and the information about this handful of loans is destroyed by the averaging process .",
    ".example mortgage data [ cols=\"^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     [ [ section-147 ] ]    as can be seen from the table above , the cost per row of these optimizations is very moderate .",
    "it requires roughly 1700 clock cycles to do a row of curve fitting , and roughly 10,000 to do a row of coefficient optimization . in this case , the coefficient optimization was fitting only flags , that cost would be expected to rise somewhat as more curves are introduced .",
    "the curve fitting cost is far more important , as more than 90% of the fitting time is consumed by curve fitting rather than coefficient fitting .",
    "[ [ section-148 ] ]    the code used for these examples is not fully optimized . in particular",
    ", it uses correctly rounded @xmath23 and @xmath24 functions . replacing the correctly rounded exponential function with one with a relative error of approximately @xmath25 more than",
    "triples the performance of the computation at no material cost to accuracy .",
    "additional optimizations are certainly possible .",
    "the cycles / row computation assumes 100% utilization of all four cores in the cpu of the test machine , but in practice the utilization rate is somewhat lower ( about 70% ) , primarily due to the small size of the dataset preventing a more efficient division .",
    "additional tuning would increase that value to nearly 100% , reducing the cost still further .",
    "for the curve fitting , it should be possible to process a single observation in less than 200 clock cycles on a typical modern cpu in java .",
    "straight c or c++ code would perform similarly , though it might be possible to do better with carefully hand tuned assembly code that takes full advantage of the simd units within newer cpus .",
    "[ [ section-149 ] ]    similarly , the coefficient optimization is essentially the same operation that would be used to project future behavior .",
    "even with no additional code improvements , this cost is in line with the resource budget set out at the beginning of the paper .",
    "careful code optimization should keep the calculations under budget even in a full scale model using numerous regressors and interaction terms .      with the example above ( using 1 million data points ) ,",
    "already the results are greatly improved by the curves that were drawn .",
    "notice that all of these fits were computed entirely without human intervention other than for a human to define the regressors , and give the model the list of regressors it may use . a typical mortgage model built within industry takes at least a man - month of data analysis , fiting and validation .",
    "the item model automates most of these tasks , accomplishing in minutes what a human would typically do in a few days in a more traditional setting .",
    "[ [ section-150 ] ]            [ [ section-151 ] ]    in these charts , the curve labled before is from the model with all flags fit , but before any of the curves have been drawn .",
    "so this model does not include the effect of age or incentive .",
    "the curve labeled after is from the model with the curves added .",
    "the charts show clearly that even a few curves have dramatically improved the fitting of these regressors .",
    "[ [ section-152 ] ]    note that the age curve shown is strongly influenced by the fact that the loans under consideration are all originated near the same time .",
    "therefore , the spike seen around age 36 months is strongly influenced by the prevailing interest rates at that time . with a larger dataset ,",
    "this correlation between age and time would smooth out , and a more classic age curve would be recovered . however , for the purposes of this demonstration , it is enough to show that item can construct a model that represents the data .",
    "it is not necessary to show that the data set chosen is a truly fair representation of the mortgage universe .",
    "[ [ section-153 ] ]    the seasonality curve shows a good example where again annealing might be helpful .",
    "[ [ section-154 ] ]        [ [ section-155 ] ]    it can be seen in this example that the seasonality effect was not captured .",
    "ideally , that spike near the end should be captured , but give all the smaller spikes throughout the year , the optimizer would be likely to hit a very unsatisfying local minimum rather than finding the actual behavior related to december and january . also , in this example , the low season for prepayment spans the year end , so it is not localized when looking at the seasonality in this way .",
    "the model might still draw curves here that would help , but it would take a large amount of data and would take at least two curves to really improve this significantly .",
    "this is an example where a small amount of randomness could help dramatically , by allowing the optimizer to find the spike at the end .",
    "a full and complete examination of all curves drawn by this model is beyond the scope of this paper .",
    "however , for illustrative purposes , the curve related to incentive for the 5 million point dataset is shown below .",
    "[ [ section-156 ] ]        [ [ section-157 ] ]    as can be seen from figure [ fig : incentresponse ] , the response of the model to incentive is an extremely smooth curve that rapidly saturates in both the positive and negative directions .",
    "this curve has been represented here as a multiple . assuming the @xmath26 probability is not too large , then with a given value of incentive , that probability is multiplied by the value of this curve . only relative multiples matter here , so it does nt matter if the curve passes through 1 or not .",
    "this shows that a loan with strong negative incentive is about one third as likely to prepay as a loan with strong positive incentive .",
    "this fits with intuition , and in fact is a close approximation of the historical data seen in figure [ fig : incentbeforeafter ] .",
    "notice that in figure [ fig : incentbeforeafter ] the highest vs. lowest prepayment multiple is approximately 7 to 1 , but that graph is confounded by numerous other factors ( occupancy type , age , etc ... ) , whereas the response above is purely for incentive acting on its own .",
    "[ [ section-158 ] ]    the primary advantage of the item model is that it expresses all responses to input variables in terms of these extremely smooth curves , and thus tends not to over fit .",
    "this can be seen in the response to age , which has 5 different curves fit to it for the @xmath26 transition .",
    "[ [ section-159 ] ]        [ [ section-160 ] ]    even here , the curves combine to make a single very smooth curve .",
    "this response is largely driven by the correlation between age and interest rates , due to the limited fitting sample .",
    "arguably , this is a worst case scenario .",
    "as the sample size increases , and more variation in the loan - months is pulled in , this age curve will flatten out and look smoother .",
    "however , even as it is , the age curve makes intuitive sense .",
    "loans tend to not prepay for the first few years of life , then they enter a period of fast prepays before finally tailing off into old age .",
    "this curve captures that phenomenon surprisingly well already .",
    "[ [ section-161 ] ]    the previous sections explored some basic fitting routines and results .",
    "this section will concentrate on a single large fit ( roughly 15 million data points ) drawn randomly from the dataset .",
    "the loans in this dataset are selected by hasing the loan i d from the dataset using sha-256 , then reducing it mod 50 and taking any loan with a reduction congruent to 0 .",
    "this selection procedure should ensure a well distributed random sample .",
    "[ [ section-162 ] ]    in this exercise , several experimental fitting choices were made in an attempt to improve the fit .    1 .",
    "the ordering of the data points was randomized 2 .",
    "the starting curve centrality parameter was set to the regressor value of a random selection from the sample 3 .",
    "the starting curve slope parameter was randomized in sign and magnitude 4 .",
    "annealing was run at the conclusion of the initial round of fitting    [ [ section-163 ] ]    the addition of these random choices would be expected to make the model less likely to get caught up in local minima , and indeed the quality of the fit is qualitatively better than those seen before .",
    "one primary reason for this is that the optimizer finds a saddle point at zero beta , which means that it has a very difficult time reversing the sign of beta during the optimization .",
    "always starting the beta as a positive number would therefore tend to bias the selection strongly towards results that would naturally have a positive beta , reducing the fit quality . using a random starting point and",
    "random sign of beta helps this problem greatly .",
    "[ [ section-164 ] ]    one unexpected side - effect is that gaussian curves become significantly more common than logistic curves .",
    "the likely reason for this is that since the beta sign can not be easily flipped , a logistic that starts off with the wrong sign will be unable to improve the results .",
    "however , a gaussian can overcome the incorrect beta sign by simply moving the centrality parameter to be very high or very low . in the future ,",
    "perhaps an adjustment to attempt both signs on every iteration would reduce this bias towards gaussians .",
    "[ [ section-165 ] ]    surprisingly , annealing was unable to improve the quality of this fit .",
    "the issue appears to be the high degree of colinearity in the regressors . with so much colinearity ,",
    "dropping all the curves from a given regressor does not provide a very clean slate , and it may be found that only curves that interlock with existing curves in a very particular way are very effective . presumably , annealing would be more effective if it was used earlier in the process , as here it was used only at the end .",
    "in addition , in a less contrived example where more regressors are considered , it is expected that annealing will prove more useful .",
    "[ [ section-166 ] ]    the tables below show the in sample validation against several key variables . notice that the qualitative fit is not much better , but the regressors ( especially age ) that are strongly correlated with time make a lot more sense due to the better sampling that reduces the contemporality of the selected loans .",
    "[ [ section-167 ] ]        [ [ section-168 ] ]        [ [ section-169 ] ]        [ [ section-170 ] ]    the one curve that stands out is ltv .",
    "though the model was allowed to draw curves on ltv , it did not do so , yet the ltv results fit very well .",
    "primarily , this is due to colinearity between ltv and other regressors , primarily fico .",
    "if more up - to - date ltv values were available , this regressor woudl be far more useful .",
    "below , the model curves for these regressors are shown .",
    "here the model transition probabilities are charted directly for some sensible choice of the regressors not being charted .",
    "the effect of the excessive use of gaussians is immediately obvious , as the curves start to do unusual things once they leave the domain where the data lies . for loans that could be extreme outliers , this could matter .",
    "a solution as simple as capping / flooring each regressor at the 1-percentile level would suffice to eliminate any danger here .",
    "[ [ section-171 ] ]        [ [ section-172 ] ]        [ [ section-173 ] ]        [ [ section-174 ] ]        [ [ section-175 ] ]    notice in particular how fico begins to bend back after roughly 800 .",
    "there is simply no appreciable level of defaults at that fico level , and very little data of any form .",
    "improvements to the fitting routines might eliminate the usage of gaussians there , and greatly reduce the propensity for this sort of issue .",
    "a similar story holds for incentive .",
    "[ [ section-176 ] ]    again , the important result here is not that these curves are dramatically better than other curves show by other mortgage researchers .",
    "the advantage of this approach is that the curves are generated with no human intervention . using this as the starting point",
    ", it is easy to correct minor issues ( e.g. cap and floor some regressors ) and thus very quickly arrive at a very high quality model .",
    "the item model was designed to automate repetitive tasks and allow for an efficient model to be built automatically . in the examples above ,",
    "all fits were performed automatically , with no manual intervention beyond the initial definition of the regressors . in a more typical mortgage model",
    ", a modeler would typically spend months analyzing data , selecting regressors , and examining model residuals .",
    "each iteration of this process would require several days , and many dead ends would be explored , largely due to inefficient regressor and parameter choices .",
    "generally , another process would be required to remove insignificant parameters , which would then be followed up again by further rounds of fitting and analysis .",
    "[ [ section-177 ] ]    the item model automates this entire cycle .",
    "it runs thousands of iterations of fit , extend , analyze residuals , and then fit again , all within the space of minutes rather than days .",
    "the model does not add insignificant parameters , so there are none to be removed , though some annealing may be needed .",
    "similarly , item does not choose suboptimal curves or regressors , so it explores fewer dead ends .",
    "since the model is additive in logit space , projecting it onto any individual regressor gives a fair representation of its behavior , greatly easing the job of validation .",
    "[ [ section-178 ] ]    the workflowing using item is to simply select the dataset , define the regressors , allow item to fit a model , and then go straight to final model validation .",
    "[ [ section-179 ] ]    attached to this submission is a java reference implementation of the item model core .",
    "this reference implementation provides only the core modeling functionality . in order to run this model",
    ", the practitioner will need to define enumerations ( see edu.columbia.tjw.item.base.standardcurvetype as an example ) describing the statuses available to the modeled phenomenon , and also describing the regressors available .",
    "in addition , the practitioner will need to provide a class to produce grids of data implementing edu.columbia.tjw.item.itemfittinggrid , and related interfaces .",
    "[ [ section-180 ] ]    once these interfaces are implemented , the model may be fit using the itemfitter .",
    "the resulting model is suitable for projection , but no general projection code is provided in this reference implementation .",
    "similarly , at this time , the annealing code is not provided , but could be made available if there is interest .",
    "[ [ section-181 ] ]    this reference implementation is made available as a maven package :    * groupid : edu.columbia.tjw * artifactid : item * version : 1.0    [ [ section-182 ] ]    if there is interest , this archive could be published to the maven central repository .",
    "anticipated future work includes some results for annealing passes and the effect of noise introduction during fitting .",
    "in addition , the model could be applied to a larger set of regressors to show a more fully featured mortgage model . applying the model to large regressor sets",
    "can reveal previously unknown behavior which may be worth research in its own right .",
    "the hybrid simulation approach could also be investigated more fully .    9    hastie and tibshirani _ generalized additive models _ statistical science , 1986 , vol . 1 , no 3 , 297 - 318    goffe , ferrier , rogers _ global optimizatoin of statistical functions with simulated annealing _    s. czepiel _ maximum likelihood estimation of logistic regression models : theory and implementation",
    "_    b. krishnapuram et .",
    "sparse multinomial logistic regression : fast algorithms and generalization bounds _ ieee transactions on pattern analysis and machine intelligence , vol .",
    "27 , no . 6 , june 2005    r. tibshirani _ regression shrinkage and selection via the lasso _ journal of the royal statistical society .",
    "series b volume 58 , issue 1 1996    b. ripley _ selecting amongst large classes of models _",
    "ripley / nelder80.pdf    freddie mac _ single family loan - level dataset _",
    "jp morgan _ the jpmorgan prepayment model _",
    "http://www.investinginbonds.com/assets/files/jp_morgan_prepayment_model.pdf    _ hedonic regression _",
    "_ discrete space _",
    "[ [ section-183 ] ]        the first thing to note about modeling in general , is that it is closely related to the concept of relative entropy , i.e. entropy against a party that has access to some models or data .",
    "in fact , a model is little more than a compression function , with all the limitations that entails .",
    "for instance , suppose the datasets @xmath27 and @xmath28 are available . if one wished to transmit this data to another person , it might be more efficient to first fit a model @xmath29 , and then transmit @xmath27 , @xmath10 , and @xmath30 .",
    "ideally , the entropy of the model and the residual together would be smaller than the entropy of the response @xmath15 . we know , however , that any compression algorithm that makes at least one dataset shorter must make at least one dataset longer .",
    "so it is known apriori that there are some datasets for which this model will give such bad predictions that it actually increases the entropy .",
    "therefore , there can be no perfect model , as any model could be defeated by carefully chosen data .",
    "the relative entropy of a data set is simply its log likelihood ( up to some constant factors ) , so a model that improves the log likelihood fo the dataset enough , would be worthwhile , and one that does nt is not .",
    "various information criteria ( e.g. aic , bic ) are based on a calculation of the total entropy contained in the parameter set @xmath31 and the residuals @xmath30 , and comparing that value to what it would be for an alternate parameters set @xmath32 and @xmath33 .",
    "the best model is then the one with the lowest aic , it has extracted as much information as possible from the dataset without passing the point where additional model complexity is counter productive .      with this in mind , suppose that in the typical fashion , the phenomenon actually follows some well defined but unknown distribution , with some associated noise term .",
    "assume that the noise is pure noise , in that no model can improve its entropy , taking into consideration the entropy of the model itself .        here",
    ", @xmath16 is some observable data , but @xmath35 is unobservable .",
    "this situation might be simplified by absorbing the unobservable information into the noise term @xmath30 .",
    "the noise terms may no longer be i.i.d . , but they should at least be mean zero .        n.b . that the noise term is no longer pure noise , it is possible that a model could reduce it if there is correlation between @xmath35 and @xmath16 .",
    "this will result in a lower log likelihood , but will also result in a misattribution of some behavior away from @xmath35 towards @xmath16 .",
    "however , being aware of the dangers , suppose the model assumes that @xmath15 follows some parameterized distribution @xmath37 with some unknown parameters @xmath31 .",
    "the parameter ( in practice , perhaps many parameters ) @xmath31 is unknown , but it could be estimated , perhaps with maximum likelihood estimation .",
    "this would recover an estimator for @xmath31 , namely @xmath39 .",
    "now , to state that a model is information theoretically efficient , we will in this paper mean the combination of three requirements , expressed in terms of the above quantities and also the sample size @xmath18 .        briefly ,",
    "requirements ( 2 ) and ( 3 ) above are the standard requirements for an efficient estimator .",
    "the first condition states simply that the model @xmath37 will converge to the actual function defining the phenomenon , given enough data , i.e. the model is not misspecified . for parameterized models ,",
    "this is almost never true . only in extremely rare circumstances",
    "would it be the case that the actual physical phenomenon being modeled converges to exactly the functional form chosen for the model .",
    "as the dataset becomes large , the majority of the residual error in the model would be due to this misspecification .            where here , the varable @xmath43 is determining how many variables are included in the model . in other words",
    ", @xmath44 is now a family of models with an unbounded number of parameters . in this case ,",
    "information theoretic efficiency is expanded to include now four separate items .",
    "it is understood that all the following equations hold only as @xmath45 .        where again , the first item is ideal but unlikely in practice .",
    "similarly , the second item is an idealization , but can be taken to mean just that the model is parsimonious with parameters , getting meaningful improvement with each parameter added .",
    "the last two items are as before , measures of unbiased and asymptotically efficient estimation .",
    "a maximum likelihood estimation is ( under most conditions ) guaranteed to be both asymptotically efficient and unbiased , so the last two conditions will then be satisfied .",
    "any parameterized model is also virtually guaranteed to not converge to exactly the actual phenomenon under study .",
    "what remains then is to consider the distance between the model and the phenomenon for any given number of parameters , and when information theoretic efficiency is discussed , this is typically what will be meant . for this notion of closeness",
    ", a typical metric would be kullback - leibler divergence though any equivalent metric would do .      since no model can be perfect due to the compression arguments above",
    ", there is no true solution to this problem and no ideal model .",
    "it is therefore necessary to from here proceed with heuristics for which there is some theoretical basis to believe that they may in many cases produce good models .",
    "one common assumption is that @xmath49 is smooth and continuous with respect to @xmath16 , depending on the model family , other assumptions may be needed as well ."
  ],
  "abstract_text": [
    "<S> this document discusses the information theoretically efficient model ( item ) , a computerized system to generate an information theoretically efficient multinomial logistic regression from a general dataset . </S>",
    "<S> more specifically , this model is designed to succeed even where the logit transform of the dependent variable is not necessarily linear in the independent variables . </S>",
    "<S> this research shows that for large datasets , the resulting models can be produced on modern computers in a tractable amount of time . </S>",
    "<S> these models are also resistant to overfitting , and as such they tend to produce interpretable models with only a limited number of features , all of which are designed to be well behaved . </S>"
  ]
}