{
  "article_text": [
    "system identification is concerned with automatic dynamic model building from measured data . under this unifying umbrella , this field spans a rather broad spectrum of topics , considering different model classes ( linear , hybrid , nonlinear , continuous and discrete time ) as well as a variety of methodologies and algorithms , bringing together in a nontrivial way concepts from classical statistics , machine learning and dynamical systems .",
    "the demand for reliable automatic tools for data based modeling of dynamical systems has attracted a considerable interest in the automatic control as well as in the statistics and econometrics communities since the @xmath0s and has been mainly developed following the parametric maximum likelihood ( ml)/prediction error ( pe ) framework , whose widespread use is to be attributed mainly to its attractive asymptotic statistical properties @xcite . even if we restrict to linear , time - invariant , finite `` order '' dynamical systems ( i.e. systems described by linear differential or difference equations with constant coefficients ) , where parametric methods are by now well developed and understood ( see @xcite ) , it is fair to say that modeling can not still be considered a `` completely automated '' task .",
    "for instance , in advanced process control applications @xcite , modeling still is , by far , the most time consuming and costly step @xcite . as such , the demand for fast and reliable automated procedures for system identification makes this exciting field still a very active and lively one .",
    "the system identification community , inspired by work in statistics @xcite , machine learning @xcite and signal processing @xcite , has recently developed and adapted methods based on regularization to jointly perform model selection and estimation in a computationally efficient and statistically robust manner @xcite .",
    "the main task of regularization is to control model complexity to face the so - called _ bias / variance dilemma _ @xcite .",
    "different regularization strategies have been employed which can be classified in two main classes : regularization induced by smoothness priors ( aka tikhonov regularization , see @xcite for early references in the field of dynamical systems ) and regularization for selection .",
    "this latter is usually achieved by convex relaxation of the @xmath1 quasi - norm ( such as @xmath2 norm and variations thereof such as sum - of - norms , nuclear norm etc . ) or other nonconvex sparsity inducing penalties which can be conveniently derived in a bayesian framework , aka sparse bayesian learning ( sbl ) @xcite .    in this paper",
    "we shall be concerned with regularization induced by smoothness priors ; the structure of the chosen prior will bring in features usually encountered in sbl / automatic relevance determination @xcite and multiple kernel learning @xcite .",
    "this makes the algorithms and results in this paper of a rather general interest . in particular we shall address the impulse response estimation problem for single input , single output ( siso ) systems described by a convolution equation of the form @xmath3 where @xmath4 is the _ output _ signal , @xmath5 , is the measurable input signal",
    ", @xmath6 is the ( unknown ) _ impulse response _ and @xmath7 is a zero mean white noise signal with unknown variance @xmath8 .",
    "as discussed in @xcite , the very same framework studied in this paper can be easily adapted to identification of multi input single output ( miso ) systems ( see also @xcite ) , maintaining the key features which allow the application of the class of algorithms discussed herein .",
    "we shall work in a bayesian framework , thus modeling the unknown impulse response @xmath9 ( possibly an infinite dimensional object ) as a gaussian process @xcite with a suitable ( prior ) covariance @xmath10 @xcite ( also known as _ kernel _ ) .",
    "the chosen covariance is usually described by some unknown hyperparameters @xmath11 which give the prior enough flexibility to encode a sufficiently wide class of impulse responses .",
    "the number of hyperparameters is typically small as compared to the number of data as well as to the `` dimension '' of @xmath9 , which as mentioned above can be infinite dimensional .",
    "these hyperparameters can be estimated from data in a variety of ways ; empirical evidence as well as some theoretical results @xcite support the use of the so - called _ marginal likelihood _",
    "( i.e. the data likelihood as a function of the unknown hyperparameters , having marginalized the unknown impulse responses from the joint density of data and unknowns ) for hyperparameter estimation .",
    "this boils down to a challenging optimization problem with the following features :    * it is nonconvex ; * it requires to handle a large number of data @xmath12 ( also several thousands ) even when the number of unknowns ( hyperparameters ) is not too large ( some tenths in most cases ) ; * the hessian matrix is , in some cases , quite costly to compute ; * the computation of the objective function and its gradient requires the factorization of matrices which can be extremely ill - conditioned .",
    "thus , stable and effective algorithms should be designed carefully taking into account the features of the problem . in particular , the simple structure of the constraints , which usually reduce to non - negativity or box , can be exploited by suitable projection methods . + in this paper",
    "we propose a scaled gradient projection method for marginal likelihood optimization , whose basic ingredients are the variable stepsize and scaling matrix , which are computed with a negligible computational cost at each gradient projection iteration . the stepsize parameter is chosen according to the barzilai ",
    "borwein rules , while the scaling matrix is based on a gradient decomposition technique . in spite of the theoretical convergence rate estimate , which in general classifies the classical gradient projection method as linearly convergent , it has been shown in the recent literature that the combination of these choices makes it a very practical , effective and robust numerical tool for several signal and image restoration problems @xcite .",
    "+ in this paper we show that , with a suitable choice of the scaling matrix and a careful implementation , the scaled gradient projection method applied to the impulse response estimation problem outperforms some second order state - of - the - art methods , leading to a significant reduction of the computational time .",
    "+ the plan of the paper is the following . in section",
    "[ sec : mod ] we describe the system identification problem in the framework of the bayesian approach , deriving the corresponding optimization problem , whose main features are described in section [ sec : problem ] .",
    "the proposed optimization method is presented in section [ sec : opt ] , focusing on steplength and scaling matrix selection . in particular , in section [ sec : scaling ] , we consider the split gradient strategy , which is a state - of - the - art approach for defining the scaling matrix in presence of non - negativity constraints , and we extend it to the more general case of box constraints . some important implementation issues are discussed in section [ sec : implementation ] .",
    "finally , the results of an extensive numerical experience are presented in section [ sec : num ] , showing the effectiveness of the proposed approach on the system identification problem , also with respect to other recent solvers .",
    "our conclusions are offered in section [ sec : conc ] .      in the following the symbol",
    "@xmath13 indicates the matrix trace and @xmath14 the matrix determinant . we shall deal with real random vectors",
    "whose ( possibly conditional ) measure , will always be absolutely continuous w.r.t . the lebesgue measure and",
    "will thus admit a density @xmath15 .",
    "we shall denote with @xmath16 the density of @xmath17 ( always w.r.t . the lebesgue measure ) , @xmath18 the conditional density of @xmath17 given @xmath19 .",
    "densities may depend upon some parameters ( say @xmath20 ) , in which case we shall use subscripts such as @xmath21 or @xmath22 .",
    "we shall consider the following problem : given a finite data record @xmath23 from system , find an estimator of the impulse response @xmath9 .",
    "this is clearly an ill - posed inverse problem since the unknown @xmath9 is an infinite dimensional object . as customary in the literature on inverse problems @xcite",
    "this can be tackled using tikhonov regularization .",
    "equivalently the ( infinite dimensional ) unknown @xmath9 can be modeled as a gaussian process @xcite .",
    "we shall follow this second route since it provides a natural way to introduce estimators of the _ regularization ( hyper)parameters _ through the marginal likelihood .",
    "we refer the reader to @xcite and references therein for some recent work in support of this approach .    in order to avoid theoretical issues related to dealing with infinite dimensional unknowns , chiefly the complication of introducing probability densities for infinite dimensional objects , the unknown impulse response @xmath24 is truncated to a _",
    "finite dimensional _ , yet arbitrarily long vector .",
    "this approximation is always possible ( within any arbitrary accuracy ) since the impulse response of a finite dimensional linear systems @xmath25 decays exponentially fast as a function of the index @xmath26 .",
    "in addition , since only @xmath27 data points are available , no information could ever be obtained from data on the `` tail '' of the impulse response for @xmath28 .",
    "thus the model can be rewritten as @xmath29 where @xmath30 and @xmath31 is the vector whose components are the system impulse response coefficients .",
    "+ note that , depending on the `` true '' underlying system , @xmath32 can be arbitrarily large , so that estimating @xmath33 in the model is still an ill - conditioned inverse problem . we stress that this truncation is inessential ; by resorting to _ reproducing kernel hilbert space _",
    "theory one can deal with the original infinite dimensional problem , see @xcite .",
    "equation can be represented in matrix form as @xmath34 where @xmath35 , @xmath36 and @xmath37 .",
    "since the noise affecting the data is white and gaussian distributed with zero mean and variance @xmath8 , then @xmath38 conditioned on @xmath33 is gaussian , @xmath39 , and thus has conditional density @xmath40 we further model @xmath33 as a gaussian random vector , independent of @xmath41 , i.e. @xmath42 where @xmath10 is the prior covariance parametrized by the hyperparameter vector @xmath43 .",
    "typical examples of prior covariance @xmath10 will be given in section [ sec : kernels ] ; suffices here to say that the number of hyper parameters @xmath44 is typically `` small '' w.r.t . to the number of data points ( from a few units to a few tens ) . for convenience of notation",
    "we shall define @xmath45 . from it",
    "follows that @xmath38 is the linear combination of independent gaussian random vectors and , therefore , the _ marginal likelihood _ @xmath46 , i.e. the marginal of @xmath38 obtained integrating @xmath47 w.r.t .",
    "@xmath33 , is still a multivariate normal with mean @xmath48 and covariance matrix @xmath49 using bayes theorem we can compute the posterior density of @xmath33 given @xmath38 @xmath50 which still depends on the unknown hyperparameters @xmath20 .",
    "there are typically two approaches to deal with the unknown hyperparameters @xmath20 .",
    "the first is the so called _ full bayes approach _ : a prior distribution ( possibly uninformative ) for the hyperparameters is postulated which allows to integrate them out .",
    "the second , which we consider in this paper , is the so - called _ empirical bayes approach _",
    "@xcite : a point estimate @xmath51 of the hyperparameters @xmath20 is found and then the posterior is computed with @xmath20 fixed to its point estimate @xmath52 . in this paper @xmath52",
    "is obtained following the maximum likelihood ( ml ) approach : @xmath53 where @xmath54 is some suitable subset of @xmath55 and @xmath56 after a solution @xmath57 of problem has been found , the maximum a posteriori ( map ) estimate @xmath58 of @xmath33 , which is equal to the posterior mean for symmetric densities , can be computed : @xmath59 where in the last equality the fact that @xmath60 is symmetric has been used .",
    "+ unless strong prior knowledge is available , the _ a priori _",
    "mean @xmath61 is set to zero , thus estimating the impulse response coefficients @xmath33 requires going through the following steps :    1 .",
    "solve the nonconvex , constrained optimization problem where @xmath62 and @xmath63 is defined in 2 .",
    "compute the corresponding impulse response coefficients setting @xmath64 in : @xmath65      several kernel matrices have been introduced in the recent years to model impulse responses of dynamical systems .",
    "perhaps the major breakthrough has been the observation that the kernel has to capture structural properties of dynamical systems @xcite , such as the fact that for linear systems described by difference / differential equations , the impulse response is a linear combination of exponentially decaying functions @xcite . in order to do so",
    ", the seminal paper @xcite has introduced the family of _ stable - spline _ kernels ; the most used kernels in this family are the _ stable - spline kernel of order 1 _ , called also tuned / correlated ( tc ) kernel @xcite :    [ nonlinearkernels ] @xmath66 and the _ stable - spline kernel of order 2 _ : @xmath67 where @xmath68 , @xmath69 , @xmath70 . soon after @xcite several other papers appeared where different families of kernels have been introduced @xcite , among which the diagonal / correlated ( dc ) kernel @xmath71    where @xmath72 , @xmath69 , @xmath70 , @xmath73 . as discussed in @xcite , and further elaborated upon in @xcite",
    ", these kernels alone may not well represent impulse responses obtained by linear combination of exponentially decaying functions when the decay rates vary widely ; see e.g. example 2.1 in @xcite .",
    "for this reason the paper @xcite introduces a family of _ multiple kernels _ , which take the form @xmath74 where @xmath75 are given fixed symmetric and positive semidefinite matrices and the coefficients @xmath76 ( @xmath77 ) play the role of scale factors . + here , as in @xcite , the `` alphabet '' of kernels @xmath78",
    "is chosen from one of the kernels over a suitable grid of hyperarameters @xmath79 .",
    "all the kernel choices listed above correspond to an optimization problem  with box - type constraints .",
    "+    [ [ remark ] ] remark + + + + + +    in our approach , @xmath8 is treated as an optimization variable as suggested in @xcite . as an alternative , the noise variance @xmath8 can be estimated from the data using a high order ( and thus low bias ) arx model ( linear regressions ) as suggested in @xcite ; some care needs to be taken to avoid overfitting . in this case only @xmath11 , which corresponds to the first @xmath44 components of @xmath20 , would have to be optimized using the marginal likelihood .",
    "in this section we describe some properties of the optimization problem . we first need to introduce some notation , defining the objective function as @xmath80 with @xmath81 the @xmath82th component of the gradient of @xmath83 and @xmath84 can be expressed as @xmath85 where @xmath86 moreover , the element @xmath87 of the hessian matrix @xmath88 , for @xmath89 , is given by @xmath90 , where @xmath91 when @xmath10 is the multiple kernel , then @xmath83 and @xmath84 are convex and concave , respectively ( see @xcite ) . in this case , since @xmath92 , @xmath77 , is positive semidefinite , the gradient of the objective function has the following interesting property @xmath93 when @xmath94 .",
    "the first inequality is straightforward since @xmath95 is positive definite and @xmath96 is positive semidefinite for all @xmath97 , while the second one is a direct consequence of lemma ii.1 in @xcite .",
    "moreover , the objective function satisfies @xmath98 for all @xmath99 , where @xmath100 , so that its level sets are bounded , see ( * ? ? ? * b ) .",
    "+ observe that and are , in general , not true when the kernel @xmath10 nonlinearly depends on its parameter @xmath11 , as in ; in this case it is not even ensured that @xmath83 and @xmath84 are convex and concave , respectively .",
    "in this section we describe the optimization method we propose to solve .",
    "we focus on first order methods based on gradient projection , which are particularly suited when the constraints are simple .",
    "the main objection in the use of first order methods is that their convergence rate is , in general , linear .",
    "however , introducing some clever choices to define the descent direction , they are able to compute a medium accuracy solution with a small number of iterations .",
    "+ such acceleration strategies are implemented in the scaled gradient projection ( sgp ) method @xcite , which applies to any problem of the form @xmath101 where @xmath102 is a closed convex set , and employs a double scaling of the negative gradient direction through a positive scalar parameter @xmath103 and a positive definite matrix @xmath104 , both iteration dependent .",
    "the general scheme of sgp is summarized in algorithm [ gpm ] . to motivate the introduction of the scaling matrix",
    ", one can think , for example , to the newton s method , which actually scales the gradient direction with the inverse hessian , while other practical choices for @xmath103 and @xmath104 are described in the following sections . + in order to define a descent direction at step 3 , i.e. a vector @xmath105 such that @xmath106 , the projection at step 2 is computed with respect to the norm induced by the inverse of the scaling matrix @xmath104 , i.e. it is defined as @xmath107 thus , even if any positive definite matrix is allowed , the most practical choice for @xmath104 consists in a diagonal matrix with positive diagonal entries .",
    "once defined the descent direction at step 3 , an armijo backtracking loop computes the steplength @xmath108 to guarantee the sufficient decrease of the objective function @xcite , i.e. @xmath109    choose the starting point @xmath110 , set the parameters @xmath111 , @xmath112 , @xmath113 and fix a positive integer @xmath114 . + for @xmath115 do the following steps :    choose the parameter @xmath116 $ ] and the diagonal scaling matrix @xmath104 such that @xmath117 , @xmath118 ;    projection : @xmath119 ;    descent direction : @xmath120 ;    set @xmath121 ;    backtracking loop :    if @xmath122 then + go to step 6 ;    else + set @xmath123 and go to step 5 .",
    "endif    set @xmath124 .    end    the armijo condition is crucial for the proof of the following general convergence result , which can be found in ( * ? ? ?",
    "* theorem 2.1 ) ( see also ( * ? ? ? * theorem 4.2 ) ) .",
    "[ teorema ] let @xmath125 be the sequence generated by applying the sgp algorithm to problem .",
    "then , every accumulation point @xmath126 of the sequence @xmath125 is a constrained stationary point , that is @xmath127    we remark that all the iterates generated by sgp belong to the set @xmath128 .",
    "when @xmath129 is defined as in and @xmath10 has the form , we recall that holds : this implies that @xmath130 is bounded and , thus , the sequence @xmath131 admits at least one limit point .",
    "+ observe that theorem [ teorema ] holds without convexity assumptions and for any bounded choice of the stepsize @xmath103 and scaling matrix @xmath104 .",
    "this freedom of choice can be exploited to significantly improve the practical performances of sgp . in the following",
    "we describe the main strategies for the selection of these parameters .",
    "once a scaling matrix @xmath104 has been defined , a well performing choice of the stepsize parameter is the variant of the barzilai ",
    "borwein rules proposed in @xcite .",
    "the rationale behind this idea consists in computing the stepsize @xmath103 so that the matrix @xmath132 approximates in a quasi ",
    "newton sense the inverse hessian of the objective function . in practice",
    ", @xmath103 is computed as the solution of one of the following minimization problems : @xmath133 where @xmath134 and @xmath135 . the solutions of the minimum problems in are given by @xmath136 and , from the computational point of view , they can be computed in @xmath137 operations , where @xmath15 is the number of variables . actually , the scalar products @xmath138 and @xmath139 may be negative , leading to negative values in formula .",
    "if this occurs , we set @xmath140 and @xmath141 respectively : this choice is based on the observation that the @xmath142-th iterate lies in a region where the objective function might have a negative curvature ( if @xmath143 and @xmath144 is convex , both the scalar products are non - negative ) .",
    "thus , taking a long step along the negative gradient could help to go away from a nonoptimal stationary point .",
    "+ it is well known by the recent literature that the best performances are achieved by adaptively alternating the two rules , with a thresholding to keep the stepsize within the prefixed interval @xmath145 $ ] ( see step 1 in algorithm [ gpm ] ) . in our implementation",
    "we adopt the alternation strategy detailed below : +    if @xmath146 then + @xmath147 ; + else + @xmath148 ; + endif + if @xmath149 then + @xmath150 ; + else + @xmath151 ; + endif + if @xmath152 then + @xmath153 ;  @xmath154 ; + else + @xmath155 ;  @xmath156 ; + endif     + where @xmath157 is a prefixed non - negative integer and @xmath158 . the alternating rule described above has been proposed for unconstrained , strictly convex quadratic problems in @xcite , where the authors investigate the related theoretical properties and numerically show that this alternation of the two bb rules allows to better capture the spectral properties of the hessian matrix .",
    "successively , an adaptation of the alternating rule in @xcite has been proposed in @xcite and employed also in several applications of sgp to different convex , nonlinear , constrained problems @xcite . in this paper",
    "we adopt the same rule also for the nonlinear , nonconvex , constrained problem described in section [ sec : problem ] .      unlike the stepsize selection rules , the scaling matrix choice is strictly related to the specific structure of problem and it depends on both the objective function and the constraints . in particular , the constraints of problem are lower bounds when @xmath10 is the multiple kernel or box constraints when the kernels are selected .",
    "+ in this section we review the split gradient idea described in @xcite for lower bound constraints and we extend such approach to general box constraints . to introduce the split gradient idea ,",
    "we consider first the non - negatively constrained problem @xmath159 whose first order optimality conditions are given by @xmath160 where the equality and inequalities are componentwise .",
    "if the gradient of @xmath161 admits a decomposition like the following one @xmath162 then equality writes also as the fixed point equation @xmath163 .",
    "this formulation is related to the corresponding fixed point method @xmath164 whose convergence properties are not well studied , but which has the capability to preserve positivity when the initial point is positive and @xmath165 whenever @xmath99 .",
    "several methods in signal and image processing ( e.g. lucy - richardson / expectation minimization @xcite , iterative space reconstruction algorithm @xcite ) and statistical learning ( lee - seung algorithm for non - negative matrix factorization @xcite ) actually have exactly this multiplicative form ( see also @xcite ) .",
    "+ with a simple algebra the multiplicative method results in @xmath166 which corresponds to a scaled gradient iteration .",
    "these considerations suggest to define the scaling matrix for sgp as @xmath167 more in general , for lower bound constraints @xmath168 , @xmath169 , the following scaling matrix @xmath170 can be motivated using similar arguments as above . +",
    "this choice of the scaling matrix , combined with a suitable choice of the stepsize @xmath103 , leads the sgp method to very good performances on ill - posed / ill - conditioned inverse problems approached by the bayesian paradigm as convex , non - negatively constrained optimization problems @xcite .",
    "+ we propose to use the scaling also on problem with the multiple kernel , even if the objective function is nonconvex . in this case , recalling , the gradient of the objective function has the natural decomposition with @xmath171      in this section we consider a box constrained problem @xmath172 where @xmath173 ( @xmath174 , @xmath175 means that @xmath176 is unbounded below or above respectively ) , and we propose a scaling strategy also for this case . driven by the considerations made in the previous section , the generalization to box constraints consists in finding a positive diagonal scaling matrix @xmath104 such that @xmath177 is feasible , i.e. @xmath178 then , to design an appropriate scaling , we should consider the sign of the gradient at the current iterate to devise which constraints could be violated taking a step along the negative gradient direction . to this end , we define the following sets of indices @xmath179 to identify which variables are bounded below and/or above and which are unbounded . then , we define the following vector @xmath180 based on a gradient decomposition of the form @xmath181 indeed , @xmath182 implies @xmath183 and , as a consequence , @xmath184 . on the other side , @xmath185 if and only if @xmath186 , which yields @xmath187 . + finally , the diagonal entries of the scaling matrix are defined as @xmath188 for an objective function of the form @xmath189 a possible general rule to define @xmath190 and @xmath191 in can be devised in the following way .",
    "when @xmath192 , then @xmath193 and we define @xmath194 for some @xmath195 .",
    "similarly , when @xmath196 , then @xmath197 and we set @xmath198 it is easy to verify that definitions and lead to a gradient decomposition with the property .",
    "moreover , this choice of the scaling matrix reduces to in presence of lower bounds only .",
    "+ we adopt the scaling strategy associated to the decomposition  for problem when the kernel is given by .",
    "each sgp iteration requires the objective function and gradient  at the current point @xmath199 , which is the more relevant computational burden of the whole algorithm .",
    "if the armijo condition is not satisfied with @xmath200 , more function evaluations are needed .",
    "+ thus , the practical performances of the algorithm also relies on the implementation of the gradient and objective function computation . on the other side we should take into account the severe ill - conditioning possibly affecting the matrices @xmath201 and @xmath202 . in our implementation",
    "we implicitly assume that @xmath203 , which is quite realistic , and we devise an algorithm for the computation of @xmath204 and @xmath205 with complexity @xmath206 which is detailed below .",
    "+ we consider the approach proposed in @xcite for objective function and gradient evaluations , which is based on the cholesky factorization of @xmath207 , at a cost of @xmath206 .",
    "then , the cholesky factorization of the matrix @xmath208 is also computed .",
    "finally , the objective function is evaluated with the formula @xmath209 the cholesky factors @xmath210 and @xmath211 can be reused for the computation of @xmath212 and , then , of the gradient as follows . omitting for simplicity the dependency of @xmath201 and @xmath202 from @xmath213 and @xmath214 and applying the sherman - morrison - woodbury formula we obtain @xmath215",
    "finally , by observing that @xmath216 it follows that @xmath217 taking into account of , if we set @xmath218 then can be computed as    [ ucompute ] @xmath219 for @xmath97 and @xmath220    on the other side , recalling , we have    [ vcompute ] @xmath221 @xmath222    the main difference between our approach for gradient computation and the analogous one described in ( * ? ? ?",
    "* section 5 ) is that we avoid to explicitly compute the matrix @xmath223 , which is very ill - conditioned .",
    "+ the previous formulae for gradient computation clearly hold when @xmath201 does not reduce to the zero matrix : since the latter case can occur at some iteration @xmath26 , for sake of completeness we report the whole procedure in algorithm [ algo : fobj_gradobj ] .    preprocessing :",
    "compute @xmath224 , @xmath225 and @xmath226 .",
    "+ for any @xmath227 , @xmath228 do the following steps :    compute @xmath201 .",
    "if @xmath229 then    compute the cholesky factorization @xmath207 ;    compute @xmath230 ;    compute the cholesky factorization @xmath231 , @xmath232 and @xmath233 as in ;    compute @xmath204 by formula ;    compute @xmath234 and @xmath235 by means of and for @xmath236 .",
    "else    compute @xmath237 ;    compute @xmath238 and @xmath239 for @xmath97 ; @xmath240 ; @xmath241 ;    endif    compute @xmath242 .",
    "end    [ [ remark-1 ] ] remark + + + + + +    the computation of the hessian matrix can also be performed with a complexity of @xmath243 , without need of further factorizations but with at least @xmath44 additional matrix - matrix products of size @xmath244 , as detailed in the following .",
    "developing the formulae for the entries of the hessian matrix given in section [ sec : problem ] , for @xmath245 , we can set @xmath246 , where @xmath247 with @xmath248 defined as in .",
    "moreover we have @xmath249 where @xmath250 . as concerns the hessian of @xmath251 , exploiting the matrix trace properties , for @xmath245",
    "we have @xmath252 , where @xmath253 with @xmath233 defined as in , and @xmath254 with @xmath255 , @xmath232 defined as in and @xmath256 .",
    "observe that @xmath257 requires the explicit computation of the matrices @xmath258 , @xmath97 , with a complexity of @xmath259 .",
    "for the multiple kernel , where @xmath44 typically is of order of tenths , the hessian computation is a quite expensive task .",
    "it is worth stressing that the computation of the matrix product @xmath258 is not needed for , since the well known formula @xmath260 , where @xmath261 indicates the vectorization of a matrix by stacking its elements columnwise , can be applied .",
    "we consider the test sets described in ( * ? ? ?",
    "* section v.a ) , containing 1000 simulated data records @xmath262 :    * d1 : @xmath263 , output snr = 10 ; * d2 : @xmath263 , output snr = 1 ; * d3 : @xmath264 , output snr = 10 ; * d4 : @xmath264 , output snr = 1 .    the estimated model order is set to @xmath265 for all simulations .",
    "+ we consider two sets of test problems . in the first one , we choose @xmath266 as the multiple kernel , where the ` basis ' matrices @xmath78 are chosen as follows :    * : @xmath267 where @xmath268 is the dc kernel defined in and @xmath269 are points of the grid @xmath270 so that @xmath271 ; * : @xmath272 , @xmath273 where @xmath274 is defined in and @xmath275 , @xmath276 , @xmath277 where @xmath278 is defined in and @xmath279 . in this case",
    "we have @xmath280 .",
    "the matrices @xmath78 , @xmath97 are extremely ill - conditioned : indeed , the average condition number is about @xmath281 . as concerns",
    "the choice of @xmath44 , we performed several tests also with finer grids and we observed similar behaviours of the algorithms with no significant improvements in the quality of the estimated impulse response coefficients .",
    "+ in the second set of problems , we consider the following cases :    * @xmath266 is the dc kernel with @xmath282 , where @xmath69 , @xmath283 , @xmath284 ; * @xmath266 is the tc kernel with @xmath285 , where @xmath69 , @xmath286 ; * @xmath266 is the ss kernel with @xmath285 , where @xmath69 , @xmath286 .",
    "we choose the lower bounds on the ` @xmath287 ' variable according to @xcite , with the aim to impose a reasonable upper bound to the condition number of @xmath266 .",
    "+ the quality of the estimated models @xmath288 is evaluated by the coefficient @xmath289 where @xmath290 are the true impulse response coefficients and @xmath291 the estimated ones computed by formula .",
    "the sgp parameters have been set as follows : @xmath292 , @xmath293 , @xmath294 , @xmath295 , @xmath296 , @xmath297 , @xmath298 , @xmath299 .",
    "the initial point @xmath300 is the vector of all ones for the multiple kernels , while we set @xmath301 for the dc kernel and @xmath302 for the tc and @xmath303 kernels .",
    "the initial stepsize @xmath304 is set to 1 .",
    "+ since sgp is a projection method , it can occur that some of the iterates lay on the boundary of the feasible set .",
    "this may create some trouble , since for @xmath305 the matrix @xmath95 in may become singular . for these reasons ,",
    "we constrain the @xmath306-variable to be greater or equal to some positive constant .",
    "then , we actually consider a problem of the form where @xmath307 , @xmath308 , with @xmath309 , @xmath310 , and @xmath311 , @xmath312 , @xmath97 for the multiple kernels dc - m and tcss , @xmath313 , @xmath314 , @xmath315 , @xmath316 , @xmath317 , @xmath318 for the kernel dc and @xmath313 , @xmath319 , @xmath316 , @xmath317 for the kernels tc and ss .",
    "+ we experimentally observed that the constraint on @xmath8 is never active at the solution of with @xmath320 : we experienced also smaller values , down to @xmath321 , but we did not observe significant differences in the results and in the algorithms performance . as an alternative , this lower",
    "bound can be safely set to a fraction ( say between one tenth to one hundredth ) of a preliminary estimate of the noise variance which can be obtained , for instance , as discussed in @xcite .",
    "+ we include also the following stopping criterion for the iterates : @xmath322 with @xmath323 .",
    "indeed , we experienced different values of @xmath324 , ranging from @xmath325 to @xmath326 and we observed that no significant improvements in accuracy are obtained with smaller tolerance values . a maximum number of 5000 iterations is also imposed .      in order to show the significant influence of the scaling strategy in the convergence behaviour of gradient methods , we compare algorithm [ gpm ] with the scaling proposed in section [ sec : scaling ] ( sgp ) with the same algorithm without scaling ( gp , @xmath327 ) on some instances of the whole test sets described above . both algorithms adopt the same adaptive alternation of the barzilai ",
    "borwein rules described in section [ subsec : bb ] and have all the other parameters set as described in section [ subsec : sgpparams ] . + as further benchmark , we consider also the affine scaling ciclic barzilai ",
    "borwein method ( as - cbb ) proposed in @xcite , which consists in a diagonally scaled gradient method whose iteration is given by @xmath328 where @xmath329 with the convention @xmath330 .",
    "in particular , @xmath331 , @xmath332 for some fixed cycle length parameter @xmath333 and @xmath108 is computed by a nonmonotone armijo - type backtracking procedure .",
    "when @xmath144 is twice continuously differentiable , any limit point of the sequence generated by as - cbb is a stationary point ( * ? ? ?",
    "* theorem 4.1 ) ; moreover , the authors also shows the local r - linear convergence to non degenerate local minimum satisfying the second order optimality conditions ( * ? ? ?",
    "* theorem 7.1 ) . in our experiments , we set @xmath334 and the nonmonotone armijo parameter ( @xmath114 in formula ( 2.2 ) in @xcite ) equal to @xmath335 .",
    "+ the plots in figure [ fig : sgp - gp ] are obtained by : a ) running the matlab function ` fmincon ` to get a reference value @xmath336 for a minimum of @xmath144 ; b ) running each algorithm and computing the relative difference between @xmath336 and the current estimate @xmath204 at each iterate . + a significantly faster decrease of the objective function value is observed for sgp , with respect to the number of function evaluations , together with a smoother and faster improvement of the estimated impulse response , measured by means of the fit parameter defined in . in practice , after the very first sgp iterations , a good estimate of the impulse response is obtained . +",
    "the comparison between sgp , gp and as - cbb gives information about the relative behaviour of scaled gradient methods ( sgp , as - cbb ) with respect to a non scaled one ( gp ) and also about the importance of the scaling matrix choice ( sgp versus as - cbb ) , which , as observed before , leads to very different performances .",
    "[ cols=\"^,^,^ \" , ]     from tables [ tab : linear ] and [ tab : nonlinear ] and figure [ fig : performance - profile ] we can observe what follows :    * in general , all the considered methods provide solutions with comparable accuracy , measured in terms of the fit parameter .",
    "some differences in accuracy could be due to the fact that problem is nonconvex and , then , different algorithms can be attracted by different stationary points ; however , the overall results are satisfactory ; * in presence of simple constraints , a first order method as sgp , equipped with a suitable combination of a scaling matrix and a steplength parameter , is competitive with more sophisticated and highly optimized second order methods , as the ones implemented in the ` fmincon ` matlab function ; * the high flexibility of sgp allows to overcome some limits of state - of - the - art schemes as the mm approach and be applied also when the objective function is not a difference of convex functions .",
    "in this paper we have considered linear system identification in the bayesian framework .",
    "a key step is the estimation of the hyperparameters describing the bayesian prior , which leads to the nonconvex , nonlinear , bound constrained optimization problem .",
    "our aim was to analyze problem from a numerical point of view , proposing also an especially tailored version of sgp for its solution and presenting the results of an extensive numerical experimentation comparing several state - of - the - art algorithms .",
    "our analysis , together with the experimental results , aims to give new insights about the numerical issues related to the considered application and also about gradient projection methods and related scaling techniques .",
    "the numerical results , depicted in figure [ fig : performance - profile ] , show that the proposed method obtains the overall best performances in terms of time .",
    "+ the main contributions of the paper are summarized below . from the optimization point of view :    * we proposed a new split gradient approach for bound constrained optimization ; * the numerical experience shows that scaling techniques are useful to improve the performances of the gradient projection method also on nonconvex problems .",
    "the improvements obtained with the proposed approach are observed with respect to the nonscaled version of the same method and also with respect to a gradient method based on a different scaling technique ; * the combination of the proposed scaling technique with a suitable steplength selection rule makes sgp competitive with second - order methods , as the ones implemented in the ` fmincon ` matlab function .",
    "* we provide an @xmath206 algorithm to evaluate the objective function and gradient of problem and an @xmath337 algorithm for hessian computation ; * we also provide an extensive numerical experimentation with the matlab ` fmincon ` function , devising the most convenient algorithm options .    as concluding remarks",
    ", we point out that one of the main strength of the proposed approach is the capability to provide a good estimate of the impulse response coefficients after very few iterations , without need of the second order information , which , especially in the multiple kernel case , is quite costly to compute .",
    "we believe that the good performances of sgp rely on the fact that the proposed scaling technique takes into account of the problem structure , that is both the objective function and the constraints . on the other side , this is also the main difficulty to the generalization of sgp : indeed , the scaling technique is especially tailored for box constraints and the extension to more general constraints is not straightforward . this issue will be addressed in our future work , which will consider also a wider range of problems arising in machine learning and system identification where sparse bayesian learning ideas can be applied , e.g. the identification of multi - input , multi - output systems , where also automatic variable selection needs to be performed , or the basis selection problem in the context of machine learning .",
    ", _ iterative image reconstruction : a point of view _ , in mathematical methods in biomedical imaging and intensity - modulated radiation therapy ( imrt ) , y.  censor , m.  jiang , and a.  k. louis , eds .",
    ", birkhauser - verlag , pisa , italy , 2008 , pp ."
  ],
  "abstract_text": [
    "<S> a crucial task in system identification problems is the selection of the most appropriate model class , and is classically addressed resorting to cross - validation or using order selection criteria based on asymptotic arguments . as recently suggested in the literature , this can be addressed in a bayesian framework , where model complexity is regulated by few hyperparameters , which can be estimated via marginal likelihood maximization . </S>",
    "<S> it is thus of primary importance to design effective optimization methods to solve the corresponding optimization problem . </S>",
    "<S> if the unknown impulse response is modeled as a gaussian process with a suitable kernel , the maximization of the marginal likelihood leads to a challenging nonconvex optimization problem , which requires a stable and effective solution strategy . </S>",
    "<S> + in this paper we address this problem by means of a scaled gradient projection algorithm , in which the scaling matrix and the steplength parameter play a crucial role to provide a meaningful solution in a computational time comparable with second order methods . in particular , we propose both a generalization of the split gradient approach to design the scaling matrix in the presence of box constraints , and an effective implementation of the gradient and objective function . + the extensive numerical experiments carried out on several test problems show that our method is very effective in providing in few tenths of a second solutions of the problems with accuracy comparable with state - of - the - art approaches . </S>",
    "<S> moreover , the flexibility of the proposed strategy makes it easily adaptable to a wider range of problems arising in different areas of machine learning , signal processing and system identification .    system identification , optimization methods , regularization , empirical bayes method , marginal likelihood maximization    65k05 , 90c30 , 90c90 , 93b30 </S>"
  ]
}