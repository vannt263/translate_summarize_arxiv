{
  "article_text": [
    "despite decades of research , video understanding problems are still very much at the frontier of computer vision capabilities .",
    "the reasons for this are many , and include the challenges of collecting , labeling and processing video data , which is typically much larger yet less abundant than images .",
    "it is also at least partially due to the inherent ambiguity of actions in videos which often defy attempts to attach dichotomic labels to video sequences  @xcite .    rather than attempting to assign videos with single",
    "_ action labels _",
    "( in the same way that 2d images are assigned object classes in , say , the imagenet collection  @xcite ) a growing number of efforts focus on other representations for the semantics of videos .",
    "one popular approach assigns videos with natural language text annotations which describe the events taking place in the video  @xcite .",
    "systems are then designed to automatically predict these annotations .",
    "others attach video sequences with numeric values indicating what parts of the video are more interesting or important  @xcite .",
    "machine vision is then expected to determine the relative importance of each part of the video and summarize videos by keeping only their most important parts .",
    "importantly , although impressive progress was made on these and other video understanding problems , this progress was often made disjointedly : separate specialized systems tailored in order to obtain state of the art performance on different video understanding problems .",
    "still lacking is a _ unified _ general approach to solving these different tasks .",
    "our approach is inspired by recent 2d dense correspondence estimation methods ( e.g. ,  @xcite ) .",
    "these methods were successfully shown to solve a variety of image understanding problems by transferring per - pixel semantics from reference images to query images .",
    "this general approach was effectively applied to a variety of tasks , including single view depth estimation , semantic segmentation and more .",
    "we take an analogous approach , applying similar techniques to 1d video sequences rather than 2d images .    specifically , image based methods combine local , per - pixel appearance similarity with global , spatial smoothness . we instead combine local , per - frame appearance similarity with global semantics smoothness , or _ temporal coherence_. this is exemplified in fig .",
    "[ fig : teaser ] which shows how temporal coherence improves the text captions assigned to a video sequence .",
    "our contributions are as follows : * ( a ) * we describe a novel method for matching test video clips to reference clips .",
    "references are assumed to be associated with semantics representing the task at hand ; hence , by this matching we transfer semantics from reference to test videos .",
    "this process seeks to match clips which share similar appearances while maintaining semantic coherency between the assigned reference clips . *",
    "( b ) * we discuss two techniques for maintaining temporal coherency : the first uses unsupervised learning for this purpose whereas the second is supervised . *",
    "( c ) * finally , we show that our method is general by presenting state of the art results on two recent and challenging video understanding tasks , previously addressed separately : video caption generation on the lsmdc16 benchmark  @xcite and video summarization on the summe benchmark  @xcite .",
    "importantly , we plan to publicly release our code and models .",
    "* dense correspondences for 2d image understanding .",
    "* our work is inspired by methods developed for estimating per - pixel ( dense ) correspondences between two images of similar , though not necessarily the same , scenes .",
    "one of the earliest methods to show potential applications for this capability was  @xcite .",
    "it was later followed by the sift flow of  @xcite , which provided a robust means for computing and using dense correspondences .",
    "briefly , these methods assume that reference images are associated with a _ semantic channel _ containing , e.g. , per pixel depths  @xcite , semantic segmentation labels  @xcite and even character labels for text images  @xcite .",
    "robust dense correspondence methods are then applied in order to match the pixels in a novel , test image to these references . following this ,",
    "the values of the semantic channel can be transferred to the query .",
    "we refer to a recent survey of these methods and their applications in  @xcite .",
    "surprisingly , despite their potential , we are unaware of previous attempts to apply this approach to videos",
    ".    * video annotation . *",
    "significant progress was made in the relatively short time since work on video annotation / caption generation began .",
    "early methods such as  @xcite attempted to cluster captions and videos and applied this for video retrieval .",
    "others  @xcite generated sentence representations by first identifying semantic video content ( e.g. , verb , noun , etc . ) using classifiers tailored for particular objects and events and then generating template based sentences . unfortunately , this approach does not scale well , as it requires substantial efforts to provide suitable training data for the classifiers , as well as limits the possible sentences that the model can produce .    more recently , and following the success of image annotation systems based on deep networks such as  @xcite ,",
    "some began applying similar techniques to videos  @xcite . whereas image based methods used convolutional neural networks ( cnn ) for this purpose , application to video required addressing temporal data , which led to the use of long recurrent neural networks ( rnn ) , particularly short - term memory networks ( lstm )  @xcite .",
    "our own method uses cnn and lstm models , but in fundamentally different ways , as we later explain in sec .",
    "[ sec : tessellation ] .",
    "* video summarization .",
    "* this task involves selecting the subset of a query video s frames which represents its most important content .",
    "this is not a new problem and relevant existing work is substantial .",
    "early methods developed for this purpose relied on unsupervised , manually specified cues for determining which parts of a video are important and should be retained .",
    "a few such examples include  @xcite .",
    "more recently , the focus shifted towards supervised methods  @xcite , which assume that training videos provide also manually specified labels indicating the importance of different video scenes .",
    "these include method which use multiple man - tailored decisions to choose video portions for the summary  @xcite and methods relying on the determinatal point process ( dpp ) to increase the diversity of selected video subsets  @xcite .",
    "contrary to video description tasks , lstm based methods were only considered for summarization very recently in  @xcite .",
    "their use of lstm is also very different from ours .",
    "our approach assumes that test videos are partitioned into clips .",
    "it then matches each test clip with a reference ( _ training _ ) clip .",
    "matching is performed with two goals in mind : first , at the clip level , we select reference clips with visually similar appearances to the input .",
    "second , at the video level , we select a sequence of clips which best preserves semantic coherency .",
    "that is , taken in sequence , the order of selected , reference semantics should adhere to the order in which they appeared in the training videos .    following this step , the semantics associated with selected reference clips",
    "can be transferred to test clips , thereby allowing us to reason about the test video using information from our reference .",
    "this approach is general , as it allows for different types of semantics to be stored and transferred from reference , training videos to the test videos .",
    "this can include , in particular , textual annotations , action labels , manual annotations of interesting frames and more .",
    "thus , different semantics represent different video understanding problems which our method can be used to solve .",
    "we assume that training and test videos are partitioned into a sequence of clips .",
    "a clip @xmath0 consists of a few consecutive frames @xmath1 where @xmath2 is the number of frames in the clip .",
    "our tessellation approach is agnostic to the particular method chosen to represent these clips . of course , the more robust and discriminative the representation , the better we expect our results to be .",
    "we therefore chose the following two step process , based on the recent state of the art video representations of  @xcite .",
    "* step 1 : representing a single frame . * given a frame @xmath3 we use an off the shelf cnn to encode its appearance .",
    "we found the vgg-19 cnn to be well suited for this purpose .",
    "this network was recently proposed in  @xcite and used to obtain state of the art results on the imagenet , large scale image recognition benchmark ( ilsvrc )  @xcite . in their work ,",
    "@xcite used the last layer of this network to predict imagenet class labels , represented as one - hot encodings .",
    "we instead treat this network as a feature transform function @xmath4 which for image ( frame ) @xmath5 returns the @xmath6 response vector from the penultimate layer of the network .    to provide robustness to local translations , we extract these features by oversampling : @xmath5 is cropped ten times at different offsets around the center of the frame .",
    "these cropped frames are normalized by subtracting the mean value of each color channel and then fed to the network .",
    "finally , the ten @xmath6 response vectors returned by the network are pooled into a single vector by element - wise averaging .",
    "principle component analysis ( pca ) is further used to reduce the dimensionality of these features to @xmath7 giving us the final , per frame representation @xmath8 .",
    "* step 2 : representing multiple frames . *",
    "once frames are encoded , we pool them to obtain a representation for the entire clip . pooling is performed by recurrent neural network fisher vector ( rnn - fv ) encoding  @xcite .",
    "specifically , we use their rnn , trained to predict the feature encoding of a future frame , @xmath9 , given the encodings for its @xmath10 preceding frames , @xmath11 .",
    "this rnn was trained on the training set from the large scale movie description challenge  @xcite , containing roughly 100k videos .",
    "we apply the rnn - fv to the representations produced for all the frames in the clip .",
    "the gradient of the last layer of this rnn is then taken as a 100,500d representation for the entire sequence of frames in @xmath0 .",
    "we again use pca for dimensionality reduction , this time mapping the features produced by the rnn - fv to 2,000d dimensions , resulting in our pooled representation @xmath12 .",
    "we refer to  @xcite for more details about this process .",
    "as previously mentioned , the nature of the semantics associated with a video depends on the task at hand .",
    "we seek a single , effective means of representing these semantics , regardless of the particular task our method is applied to solve .",
    "we tested several representations for video semantics and chose the recent fisher vector of a hybrid gaussian - laplacian mixture model ( fv - hglmm )  @xcite , as it provided the best results in our tests .",
    "briefly , we assume a textual semantic representation , @xmath13 for a clip @xmath0 , where @xmath13 is a string containing natural language words .",
    "we use word2vec  @xcite to map the sequence of words in @xmath13 to a vector of numbers , @xmath14 , where @xmath15 is the number of words in @xmath13 and can be different for different clips .",
    "fv - hglmm then maps this sequence of numbers to a vector @xmath16 of fixed dimensionality , @xmath17 .",
    "fv - hglmm is based on the well - known fisher vectors ( fv )  @xcite .",
    "the standard gaussiam mixture models ( gmm ) typically used to produce fv representations are replaced here with a hybrid gaussian - laplacian mixture model which was shown in  @xcite to be effective for image annotation .",
    "more details on this representation can be found in that paper .",
    "clip representations and their associated semantics are all mapped to the joint svs .",
    "we aim to map the appearance of each clip and its assigned semantics to two neighboring points in the svs . by doing so , given an appearance representation for a query clip , we can search for potential semantic assignments for this clip in our reference set using standard euclidean distance .",
    "this property will later become important in sec .",
    "[ sec : distribution ] .    in practice ,",
    "all clip appearance representations @xmath12 and their associated semantic representations @xmath18 are jointly mapped to the svs using regularized canonical correlation analysis ( cca )  @xcite where the cca mapping is trained using the ground truth semantics provided with each benchmark .",
    "this produces representations @xmath19 and @xmath20 for the appearance and semantics , respectively , for each clip .",
    "[ cols=\"^,^ \" , ]",
    "we present a general tool for video understanding , designed to transfer different types of semantic information from reference , training videos to novel test videos .",
    "two alternatives are proposed : unsupervised tessellation which uses dynamic programming to apply temporal , semantic coherency , and supervised tessellation which employs lstm to predict future semantics .",
    "we show these methods , coupled with a recent video representation technique , to provide state of the art results on two very different video analysis domains : video annotation and video summarization .",
    "an obvious next step for this approach would be to apply it to other video understanding tasks , primarily action recognition .",
    "existing action recognition benchmarks  @xcite provide action labels rather than clip semantics and it remains to be seen how well these can be transferred and used to predict action labels .",
    "d.  l. chen and w.  b. dolan .",
    "collecting highly parallel data for paraphrase evaluation . in _ proc .",
    "annual meeting of the association for computational linguistics : human language technologies _ , pages 190200 .",
    "association for computational linguistics , 2011 .",
    "m.  denkowski and a.  lavie .",
    "meteor universal : language specific translation evaluation for any target language .",
    "in _ in proceedings of the ninth workshop on statistical machine translation_. citeseer , 2014 .",
    "j.  donahue , l.  anne  hendricks , s.  guadarrama , m.  rohrbach , s.  venugopalan , k.  saenko , and t.  darrell .",
    "long - term recurrent convolutional networks for visual recognition and description . in _ proc .",
    "vision pattern recognition _ , pages 26252634 , 2015 .",
    "s.  guadarrama , n.  krishnamoorthy , g.  malkarnenkar , s.  venugopalan , r.  mooney , t.  darrell , and k.  saenko . : recognizing and describing arbitrary activities using semantic hierarchies and zero - shot recognition . in _ proc .",
    "comput . vision _ ,",
    "pages 27122719 , 2013 .",
    "b.  klein , g.  lev , g.  sadeh , and l.  wolf .",
    "associating neural word embeddings with deep image representations using fisher vectors . in _ proc .",
    "vision pattern recognition _ , pages 44374446 , 2015 .",
    "n.  krishnamoorthy , g.  malkarnenkar , r.  j. mooney , k.  saenko , and s.  guadarrama .",
    "generating natural - language video descriptions using text - mined knowledge . in _",
    "aaai conf . on artificial intelligence _ ,",
    "volume  1 , page  2 , 2013 .",
    "lin and f.  j. och . automatic evaluation of machine translation quality using longest common subsequence and skip - bigram statistics . in _ proc .",
    "annual meeting on association for computational linguistics _",
    ", page 605 .",
    "association for computational linguistics , 2004 .",
    "p.  over , g.  awad , m.  michel , j.  fiscus , g.  sanders , b.  shaw , a.  f. smeaton , and g.  qunot .",
    "2012an overview of the goals , tasks , data , evaluation mechanisms and metrics . in _ proceedings of trecvid _ , 2012 .",
    "k.  papineni , s.  roukos , t.  ward , and w .- j .",
    "zhu . : a method for automatic evaluation of machine translation . in _ proc .",
    "annual meeting on association for computational linguistics _ , pages 311318 .",
    "association for computational linguistics , 2002 .",
    "a.  rohrbach , a.  torabi , t.  maharaj , m.  rohrbach , c.  pal , a.  courville , and b.  schiele .",
    "the large scale movie description and understanding challenge ( lsmdc 2016 ) , howpublished = available :  http://tinyurl.com/zabh4et , month = september , year = 2016 ."
  ],
  "abstract_text": [
    "<S> we present a general approach to video understanding , inspired by semantic transfer techniques successfully used for 2d image understanding . </S>",
    "<S> our method considers a video to be a 1d sequence of clips , each one associated with its own semantics . </S>",
    "<S> the nature of these semantics  natural language captions or other labels  depends on the task at hand . </S>",
    "<S> a test video is processed by forming correspondences between its clips and the clips of reference videos with known semantics , following which , reference semantics can be transferred to the test video . </S>",
    "<S> we describe two matching methods , both designed to ensure that ( a ) reference clips appear similar to test clips and ( b ) , taken together , the semantics of selected reference clips is consistent and maintains temporal coherence . </S>",
    "<S> we use our method for video captioning on the lsmdc16 benchmark and video summarization on the summe benchmark . in both cases , our method not only surpasses state of the art results , but importantly , it is the only method we know of that was successfully applied to both video understanding tasks . </S>"
  ]
}