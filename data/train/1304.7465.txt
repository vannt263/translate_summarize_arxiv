{
  "article_text": [
    "clustering , the unsupervised classification of patterns into groups , is one of the most important tasks in exploratory data analysis @xcite . primary goals of clustering include gaining insight into data ( detecting anomalies , identifying salient features , etc . ) , classifying data , and compressing data .",
    "clustering has a long and rich history in a variety of scientific disciplines including anthropology , biology , medicine , psychology , statistics , mathematics , engineering , and computer science . as a result ,",
    "numerous clustering algorithms have been proposed since the early 1950s @xcite .",
    "clustering algorithms can be broadly classified into two groups : hierarchical and partitional @xcite .",
    "hierarchical algorithms recursively find nested clusters either in a top - down ( divisive ) or bottom - up ( agglomerative ) fashion . in contrast , partitional algorithms find all the clusters simultaneously as a partition of the data and do not impose a hierarchical structure .",
    "most hierarchical algorithms have quadratic or higher complexity in the number of data points @xcite and therefore are not suitable for large data sets , whereas partitional algorithms often have lower complexity .    given a data set @xmath0 in @xmath1 , i.e. , @xmath2 points ( vectors ) each with @xmath3 attributes ( components ) , hard partitional algorithms divide @xmath4 into @xmath5 exhaustive and mutually exclusive clusters @xmath6 @xmath7 @xmath8 for @xmath9 .",
    "these algorithms usually generate clusters by optimizing a criterion function .",
    "the most intuitive and frequently used criterion function is the sum of squared error ( sse ) given by : @xmath10 where @xmath11 denotes the euclidean ( @xmath12 ) norm and @xmath13 is the centroid of cluster @xmath14 whose cardinality is @xmath15 .",
    "the optimization of is often referred to as the minimum sse clustering ( mssc ) problem .",
    "the number of ways in which a set of @xmath2 objects can be partitioned into @xmath5 non - empty groups is given by stirling numbers of the second kind : @xmath16 which can be approximated by @xmath17 it can be seen that a complete enumeration of all possible clusterings to determine the global minimum of is clearly computationally prohibitive except for very small data sets @xcite . in fact , this non - convex optimization problem is proven to be np - hard even for @xmath18 @xcite or @xmath19 @xcite .",
    "consequently , various heuristics have been developed to provide approximate solutions to this problem @xcite . among these heuristics ,",
    "lloyd s algorithm @xcite , often referred to as the ( batch ) k - means algorithm , is the simplest and most commonly used one .",
    "this algorithm starts with @xmath5 arbitrary centers , typically chosen uniformly at random from the data points .",
    "each point is assigned to the nearest center and then each center is recalculated as the mean of all points assigned to it .",
    "these two steps are repeated until a predefined termination criterion is met .",
    "the k - means algorithm is undoubtedly the most widely used partitional clustering algorithm @xcite .",
    "its popularity can be attributed to several reasons .",
    "first , it is conceptually simple and easy to implement .",
    "virtually every data mining software includes an implementation of it .",
    "second , it is versatile , i.e. , almost every aspect of the algorithm ( initialization , distance function , termination criterion , etc . ) can be modified .",
    "this is evidenced by hundreds of publications over the last fifty years that extend k - means in various ways .",
    "third , it has a time complexity that is linear in @xmath2 , @xmath3 , and @xmath5 ( in general , @xmath20 and @xmath21 ) .",
    "for this reason , it can be used to initialize more expensive clustering algorithms such as expectation maximization @xcite , dbscan @xcite , and spectral clustering @xcite . furthermore , numerous sequential @xcite and parallel @xcite acceleration techniques are available in the literature .",
    "fourth , it has a storage complexity that is linear in @xmath2 , @xmath3 , and @xmath5 .",
    "in addition , there exist disk - based variants that do not require all points to be stored in memory @xcite .",
    "fifth , it is guaranteed to converge @xcite at a quadratic rate @xcite . finally , it is invariant to data ordering , i.e. , random shufflings of the data points .",
    "on the other hand , k - means has several significant disadvantages .",
    "first , it requires the number of clusters , @xmath5 , to be specified in advance .",
    "the value of this parameter can be determined automatically by means of various internal / relative cluster validity measures @xcite .",
    "second , it can only detect compact , hyperspherical clusters that are well separated .",
    "this can be alleviated by using a more general distance function such as the mahalanobis distance , which permits the detection of hyperellipsoidal clusters @xcite .",
    "third , due its utilization of the squared euclidean distance , it is sensitive to noise and outlier points since even a few such points can significantly influence the means of their respective clusters .",
    "this can be addressed by outlier pruning @xcite or using a more robust distance function such as city - block ( @xmath22 ) distance .",
    "fourth , due to its gradient descent nature , it often converges to a local minimum of the criterion function @xcite .",
    "for the same reason , it is highly sensitive to the selection of the initial centers @xcite .",
    "adverse effects of improper initialization include empty clusters , slower convergence , and a higher chance of getting stuck in bad local minima @xcite .",
    "fortunately , except for the first two , these drawbacks can be remedied by using an adaptive initialization method ( i m ) .    a large number of ims have been proposed in the literature @xcite . unfortunately , many of these have superlinear complexity in @xmath2 @xcite , which makes them impractical for large data sets ( note that k - means itself has linear complexity ) .",
    "in contrast , linear ims are often random and/or order - sensitive @xcite , which renders their results unrepeatable .",
    "su and dy proposed two divisive hierarchical initialization methods named var - part and pca - part that are not only linear , but also deterministic and order - invariant @xcite . in this study , we propose a simple modification to these methods that improves their performance significantly .",
    "the rest of the paper is organized as follows .",
    "section [ sec_init_methods ] presents a brief overview of some of the most popular linear , order - invariant k - means ims and the proposed modification to var - part and pca - part .",
    "section [ sec_exp ] presents the experimental results , while section [ sec_disc ] analyzes these results .",
    "finally , section [ sec_conc ] gives the conclusions .",
    "forgy s method @xcite assigns each point to one of the @xmath5 clusters uniformly at random .",
    "the centers are then given by the centroids of these initial clusters .",
    "this method has no theoretical basis , as such random clusters have no internal homogeneity @xcite .",
    "macqueen @xcite proposed two different methods .",
    "the first one , which is the default option in the quick cluster procedure of ibm spss statistics @xcite , takes the first @xmath5 points in @xmath4 as the centers .",
    "an obvious drawback of this method is its sensitivity to data ordering .",
    "the second method chooses the centers randomly from the data points .",
    "the rationale behind this method is that random selection is likely to pick points from dense regions , i.e. , points that are good candidates to be centers .",
    "however , there is no mechanism to avoid choosing outliers or points that are too close to each other @xcite .",
    "multiple runs of this method is the standard way of initializing k - means @xcite .",
    "it should be noted that this second method is often mistakenly attributed to forgy @xcite .",
    "the maximin method @xcite chooses the first center @xmath23 arbitrarily and the @xmath24-th ( @xmath25 ) center @xmath26 is chosen to be the point that has the greatest minimum - distance to the previously selected centers , i.e. , @xmath27 .",
    "this method was originally developed as a @xmath28-approximation to the @xmath5-center clustering problem points in a metric space , the goal of @xmath5-center clustering is to find @xmath5 representative points ( centers ) such that the maximum distance of a point to a center is minimized .",
    "given a minimization problem , a _",
    "algorithm is one that finds a solution whose cost is at most twice the cost of the optimal solution . ] .",
    "the k - means++ method @xcite interpolates between macqueen s second method and the maximin method .",
    "it chooses the first center randomly and the @xmath24-th ( @xmath29 ) center is chosen to be @xmath30 with a probability of @xmath31 , where @xmath32 denotes the minimum - distance from a point @xmath33 to the previously selected centers .",
    "this method yields an @xmath34 approximation to the mssc problem .",
    "the pca - part method @xcite uses a divisive hierarchical approach based on pca ( principal component analysis ) @xcite . starting from an initial cluster that contains the entire data set , the method iteratively selects the cluster with the greatest sse and divides it into two subclusters using a hyperplane that passes through the cluster centroid and is orthogonal to the principal eigenvector of the cluster covariance matrix",
    "this procedure is repeated until @xmath5 clusters are obtained .",
    "the centers are then given by the centroids of these clusters .",
    "the var - part method @xcite is an approximation to pca - part , where the covariance matrix of the cluster to be split is assumed to be diagonal . in this case , the splitting hyperplane is orthogonal to the coordinate axis with the greatest variance .",
    "figure [ fig_varpart_ruspini ] illustrates the var - part procedure on a toy data set with four natural clusters @xcite . in iteration @xmath35 , the initial cluster that contains the entire data set",
    "is split into two subclusters along the y axis using a line ( one - dimensional hyperplane ) that passes through the mean point ( @xmath36 ) . between the resulting two clusters ,",
    "the one above the line has a greater sse . in iteration @xmath28 , this cluster is therefore split along the x axis at the mean point ( @xmath37 ) . in the final iteration , the cluster with the greatest sse , i.e. , the bottom cluster , is split along the x axis at the mean point ( @xmath38 ) . in figure",
    "[ part_d ] , the centroids of the final four clusters are denoted by stars .",
    "+      su and dy @xcite demonstrated that , besides being computationally efficient , var - part and pca - part perform very well on a variety of data sets . recall that in each iteration these methods select the cluster with the greatest sse and then project the @xmath3-dimensional points in this cluster on a partitioning axis .",
    "the difference between the two methods is the choice of this axis . in var - part ,",
    "the partitioning axis is the coordinate axis with the greatest variance , whereas in pca - part it is the major axis . after the projection operation ,",
    "both methods use the _ mean _ point on the partitioning axis as a ` threshold ' to divide the points between two clusters .",
    "in other words , each point is assigned to one of the two subclusters depending on which side of the _ mean _ point its projection falls to .",
    "it should be noted that the choice of this threshold is primarily motivated by computational convenience .",
    "here , we propose a better alternative based on discriminant analysis .    the projections of the points on the partitioning axis can be viewed as a discrete probability distribution , which can be conveniently represented by a histogram .",
    "the problem of dividing a histogram into two partitions is a well studied one in the field of image processing .",
    "a plethora of histogram partitioning , a.k.a .",
    "thresholding , methods has been proposed in the literature with the early ones dating back to the 1960s @xcite . among these",
    ", otsu s method @xcite has become the method of choice as confirmed by numerous comparative studies @xcite .",
    "given an image represented by @xmath39 gray levels @xmath40 , a thresholding method partitions the image pixels into two classes @xmath41 and @xmath42 @xmath43 ( object and background , or vice versa ) at gray level @xmath44 .",
    "in other words , pixels with gray levels less than or equal to the threshold @xmath44 are assigned to @xmath45 , whereas the remaining pixels are assigned to @xmath46 .",
    "let @xmath47 be the number of pixels with gray level @xmath24 .",
    "the total number of pixels in the image is then given by @xmath48 .",
    "the normalized gray level histogram of the image can be regarded as a probability mass function : @xmath49    let @xmath50 and @xmath51 denote the probabilities of @xmath45 and @xmath46 , respectively .",
    "the means of the respective classes are then given by : @xmath52 where @xmath53 and @xmath54 denote the first moment of the histogram up to gray level @xmath44 and mean gray level of the image , respectively .",
    "otsu s method adopts between - class variance , i.e. , @xmath55 ^ 2 $ ] , from the discriminant analysis literature as its objective function and determines the optimal threshold @xmath56 as the gray level that maximizes @xmath57 , i.e. , @xmath58 . between - class variance can be viewed as a measure of class separability or histogram bimodality .",
    "it can be efficiently calculated using : @xmath59 ^ 2 } } { { p_0(t ) p_1(t)}}\\ ] ]    it should be noted that the efficiency of otsu s method can be attributed to the fact that it operates on histogrammed pixel gray values , which are non - negative integers .",
    "var - part and pca - part , on the other hand , operate on the projections of the points on the partitioning axis , which are often fractional .",
    "this problem can be circumvented by linearly scaling the projection values to the limits of the histogram , i.e. , @xmath60 and @xmath61 .",
    "let @xmath62 be the projection of a point @xmath63 on the partitioning axis .",
    "@xmath62 can be mapped to histogram bin @xmath64 given by : @xmath65 where @xmath66 is the floor function which returns the largest integer less than or equal to @xmath67 .",
    "the computational complexities of histogram construction and otsu s method are @xmath68 ( @xmath69 : number of points in the cluster ) and @xmath70 , respectively .",
    "@xmath39 is constant in our experiments and therefore the proposed modification does not alter the linear time complexity of var - part and pca - part .",
    "figure [ fig_histo ] shows a histogram where using the mean point as a threshold leads to poor results .",
    "this histogram is constructed during the first iteration of pca - part from the projections of the points in the shuttle data set ( see table [ tab_data_set ] ) .",
    "as marked on the figure , the mean point of this histogram is @xmath71 , whereas otsu s method gives a threshold of @xmath72 .",
    "the sse of the initial cluster is @xmath73 .",
    "when the mean point of the histogram is used a threshold , the resulting two subclusters have sse s of @xmath74 and @xmath75 .",
    "this means that splitting the initial cluster with a hyperplane orthogonal to the principal eigenvector of the cluster covariance matrix at the mean point results in approximately @xmath76% reduction in the sse .",
    "on the other hand , when otsu s threshold is used , the subclusters have sse s of @xmath77 and @xmath78 , which translates to about @xmath79% reduction in the sse . in the next section",
    ", we will demonstrate that using otsu s threshold instead of the mean point often leads to significantly better initial clusterings on a variety of data sets .",
    "comparison of mean point and otsu s thresholds ]",
    "the experiments were performed on @xmath80 commonly used data sets from the uci machine learning repository @xcite .",
    "table [ tab_data_set ] gives the data set descriptions . for each data set , the number of clusters ( @xmath5 ) was set equal to the number of classes ( @xmath81 ) , as commonly seen in the related literature @xcite .    in clustering applications , normalization is a common preprocessing step that is necessary to prevent attributes with large ranges from dominating the distance calculations and also to avoid numerical instabilities in the computations .",
    "two commonly used normalization schemes are linear scaling to unit range ( min - max normalization ) and linear scaling to unit variance ( z - score normalization ) .",
    "several studies revealed that the former scheme is preferable to the latter since the latter is likely to eliminate valuable between - cluster variation @xcite . as a result",
    ", we used min - max normalization to map the attributes of each data set to the @xmath82 $ ] interval ."
  ],
  "abstract_text": [
    "<S> k - means is undoubtedly the most widely used partitional clustering algorithm . </S>",
    "<S> unfortunately , due to its gradient descent nature , this algorithm is highly sensitive to the initial placement of the cluster centers . </S>",
    "<S> numerous initialization methods have been proposed to address this problem . </S>",
    "<S> many of these methods , however , have superlinear complexity in the number of data points , making them impractical for large data sets . on the other hand , </S>",
    "<S> linear methods are often random and/or order - sensitive , which renders their results unrepeatable . </S>",
    "<S> recently , su and dy proposed two highly successful hierarchical initialization methods named var - part and pca - part that are not only linear , but also deterministic ( non - random ) and order - invariant . in this paper , we propose a discriminant analysis based approach that addresses a common deficiency of these two methods . </S>",
    "<S> experiments on a large and diverse collection of data sets from the uci machine learning repository demonstrate that var - part and pca - part are highly competitive with one of the best random initialization methods to date , i.e. , k - means++ , and that the proposed approach significantly improves the performance of both hierarchical methods . </S>"
  ]
}