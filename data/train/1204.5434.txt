{
  "article_text": [
    "computational physics has long been a consumer of advanced computational resources and a driver for the development of cutting - edge hardware .",
    "rapid strides in development continue today , and recent installations of publicly hosted research supercomputers have begun breaking the petaflop barrier .",
    "however , since single - core cpus hit the power wall over the last decade , exploiting modern resources in the physical sciences , which tend to deliver problems that are almost uniquely both bandwidth and compute limited , requires increasing levels of software sophistication with multiple layers of carefully designed parallelism .",
    "a recent flurry of activity has taken place in the use of graphics processing units ( gpus ) for general - purpose computing .",
    "these devices are inherently massively parallel , with modern gpus containing hundreds of light - weight cores .",
    "single gpus have recently passed the teraflop barrier in total throughput for single - precision arithmetic .",
    "the field that began with manually packaging non - graphical computations into the language of graphical operations has matured with the release of general programming frameworks such as nvidia s cuda and opencl .",
    "the development of these general programming frameworks , combined with the recent inclusion of hardware double - precision operations , has made targeting gpus an increasingly attractive endeavor for computational scientists , both as low - heat , low - expense desktop - size cluster replacements , and as high - performance co - processors in distributed cluster architectures .",
    "this activity is reflected in the recent uptick of publications reporting implementations and performance metrics of gpu simulation codes in fields as diverse as electronic structure theory@xcite , computational fluid dynamics@xcite , molecular dynamics@xcite , and electromagnetic simulations@xcite .",
    "polymer statistical field theory has proven to be a powerful theoretical construct for both analytical and numerical computations on a wide range of equilibrium polymer systems@xcite .",
    "field theories of homogeneous systems have been combined with powerful analytical techniques , such as the renormalization group , to elucidate important and fundamental phenomena such as the excluded volume effect@xcite .",
    "models of inhomogeneous systems , such as phase separated polymer alloys and block copolymers , have largely succumbed to numerical self - consistent field theory ( scft ) , which is a computer simulation methodology based on a mean - field approximation to the underlying field theory model@xcite . at the frontier of the field",
    "are `` field - theoretic simulations '' ( fts ) , which attempt to stochastically sample the unapproximated ( and complex valued ) field theory , thereby enabling the study of fluctuation phenomena in both homogeneous and inhomogeneous polymer systems@xcite .    in this article , we present details of a gpu implementation of the polymer field theory framework , including beyond - mean - field simulations , and report favorable runtimes compared to the same code running both in serial and in parallel on cpus .",
    "we have taken care to report benchmark timings together with precise models of the hardware involved .",
    "there are limited reports of gpu implementations of other polymer field theory simulation methods in the literature .",
    "gao and coworkers@xcite recently reported a self - consistent field theory implementation specifically for semi - flexible block copolymers in two dimensions , with an observed @xmath3 speedup over their cpu code .",
    "wright and wickham demonstrated a peak @xmath0 speedup on gpus for a simple landau - brazovskii type free - energy model of diffusive polymer dynamics@xcite .",
    "the analytic and real - valued free - energy model used in the latter avoids the expensive propagator calculations that are the hallmark of scft and the more involved fts simulations . to our knowledge",
    ", there has been no prior report in the literature on a gpu implementation of a field - theoretic polymer simulation .",
    "we note a recent critical review@xcite of reported gpu code speedups by lee _",
    "while reports of gpu codes running high - throughput tasks @xmath4 to @xmath5 faster than their cpu counterparts are not uncommon , lee _ et al . _",
    "attribute such favorable timings to comparisons between carefully optimized gpu code and less well - optimized cpu code . for a wide range of commonly used core algorithms , they report comparisons of carefully optimized gpu and cpu code , with the latter exploiting all of the avenues for on - chip vectorization , effective use of large on - chip caches , and appropriate reorganizations of memory access patterns .",
    "they find that cpu code can typically match the runtimes of gpu code to within an order of magnitude , if particular attention is spent optimizing both codes .",
    "thus , we have taken care to ensure that our cpu code , which shares much of the code base with the gpu implementation , is aggressively optimized .",
    "in particular , we do exploit on - chip vectorization in our cpu code , through the use of hand - coded streaming simd extensions 3 ( sse3 ) instructions for arithmetic on contiguous vectors of complex numbers , and through shared - memory parallelism with openmp for exploitation of multi - core architectures .",
    "we demonstrate in sec .",
    "[ sec : results ] that with such careful optimizations , our runtimes on both cpu and gpu are easily dominated by the fourier transformation steps that are handled by high - performance fast fourier transform ( fft ) libraries .",
    "this dominance is not entirely surprising , since multidimensional ffts are not trivially parallelizable and are the only component of our calculation that does not scale linearly with system size .",
    "a comprehensive discussion of the theoretical foundations of our approach can be found elsewhere@xcite . here",
    "we summarize the most important expressions that underpin our numerical scheme .",
    "we begin by specifying a particle - based model , with the relevant degrees of freedom corresponding to positions of statistical segments of the polymer chains .",
    "interactions between statistical segments are characterized by short - ranged intra - molecular interactions ( e.g. , gaussian stretching , backbone stiffness ,  ) , and short- or long - ranged intermolecular interactions ( e.g. , excluded - volume interaction , flory - like contact potentials , electrostatic interactions ,  ) .",
    "we limit the remainder of our discussion , though not necessarily our simulation code , to fully flexible gaussian chains that interact only through contact potentials . in order to decouple interactions between statistical segments ,",
    "we introduce auxiliary fluctuating fields through a hubbard - stratonovich transformation and integrate out the then independent particle degrees of freedom .",
    "the resulting canonical partition function is typically of the form @xmath6\\right ) , \\label{eq : canpartfn}\\ ] ] where @xmath7 is some field - independent constant , @xmath8 are the set of auxiliary `` chemical potential '' fields introduced to decouple interactions , @xmath9 denotes a functional integral over such a field , and @xmath10 is a complex - valued energy functional that contains model - dependent interactions .",
    "other statistical ensembles are readily derived .",
    "the energy functional , @xmath10 , usually contains both explicit and implicit dependences on the fields .",
    "for example , the edwards model of homopolymers in an implicit solvent@xcite , which has a single chemical potential field , may be written as @xmath11 = \\frac{b}{2}\\int d\\vec{x } \\left[\\omega\\left(\\vec{x}\\right)\\right]^2 - c\\tilde{v}\\ln q\\left[i\\omega\\right ] , \\label{eq : ham_edwards}\\ ] ] where @xmath12 is a measure of the excluded volume interaction , @xmath13 is the number of polymer chains per unit volume , @xmath14 is the system volume in units of the free - polymer radius of gyration , @xmath15 , and @xmath16 is the normalized partition function discussed below .",
    "similarly , the well - studied case of an incompressible melt of diblock copolymer consisting of connected blocks of incompatible `` a '' and `` b '' segments can be written in terms of `` pressure '' and `` exchange '' fields as@xcite @xmath17 = c \\int d\\vec{x}\\left[\\frac{\\omega_-^2}{\\chi_{ab}n } - i\\omega_+\\right ] - c\\tilde{v } \\ln q\\left[\\omega_a , \\omega_b\\right ] , \\label{eq : ham_db}\\ ] ] where @xmath18 is the flory contact interaction between statistical segments of `` a '' and `` b '' , and @xmath19 is the total number of statistical segments in the copolymer chain . the chemical potential fields @xmath20 and @xmath21 are the fields felt by `` a '' and `` b '' segments respectively , and they are related to the pressure and exchange fields ( @xmath22 and @xmath23 ) through the mappings @xmath24 , @xmath25 .",
    "both of these model hamiltonian functionals contain implicit non - linear and non - local dependencies on the chemical potential fields through the normalized partition function , @xmath16 , which is derived from the statistical ensemble of a single polymer chain interacting with the specified chemical potential field(s ) .",
    "all details of polymer architecture are embedded within the @xmath16 functional . while the present discussion is limited to fully flexible and monodisperse linear homopolymer or diblock copolymer chains , the method , and indeed our simulation code ,",
    "is easily extended to more complex architectures such as symmetric and fully asymmetric multiblock copolymers , branched and star polymer chains , and grafted copolymers , by simply substituting the appropriate form of @xmath16 and introducing fields to decouple any extra interactions .",
    "the intensive part of our simulation method is the determination of the normalized partition function , @xmath26 $ ] , for a fully specified set of chemical potential fields .",
    "we write @xmath16 in terms of a feynman - kac - like propagator , @xmath27 , as @xmath28 = \\frac{1}{\\tilde{v}}\\int d\\vec{x}\\ , q\\left(\\vec{x},s=1;\\left[\\left\\{\\omega\\right\\}\\right]\\right ) , \\label{eq : q}\\ ] ] where @xmath29 is the polymer backbone contour variable , which has been normalized by @xmath19 , the length of the polymer , so that the chain contour length is @xmath30 .",
    "@xmath27 ( and , for asymmetric chains that are not invariant to reversal of the contour direction , its conjugate @xmath31 ) is determined from a modified diffusion equation @xmath32\\right ) = \\left[\\nabla^2   - \\omega\\left(\\vec{x } ; s\\right)\\right]q\\left(\\vec{x},s;\\left[\\omega\\right]\\right ) .",
    "\\label{eq : diffusion}\\ ] ] the @xmath29 dependence of the chemical potential field , @xmath33 , is a feature of copolymers that arises from different species segments responding to different fields .",
    "the most common initial condition for eq .",
    "[ eq : diffusion ] is @xmath34 ; @xmath35 .    with the expression for @xmath16 formalized , we turn our attention to the functional integrals over fields in the canonical partition function , eq .  [ eq : canpartfn ] . for many systems that are dense and",
    "far from phase transitions , the functional integrals are dominated by saddle - point field configurations @xmath36\\right).\\ ] ] the self - consistent - field ( scft ) approach then reduces to finding the stationary , saddle - point field configurations , @xmath37 , for which the negative of the hamiltonian functional , @xmath38 , is real and large .",
    "a necessary condition for such a field configuration is for the hamiltonian to be stationary in the sense that its functional derivative with respect to any of the fields is zero : @xmath39 for example , in the case of the incompressible diblock copolymer melt ( eq .  [ eq : ham_db ] ) , the mean - field equations are @xmath40 where @xmath41 is the spatially averaged segment density , and @xmath42 and @xmath43 are spatially resolved segment densities of `` a '' and `` b '' components , which emerge from the functional derivatives of the normalized partition function , @xmath16 , with respect to pressure and exchange fields : @xmath44 and @xmath45 is the fraction of the copolymer chain that is composed of `` a '' segments .",
    "the first mean - field equation enforces local incompressibility of the melt , while the second encourages phase separation of the components  which must be balanced against the loss of chain conformational and translational entropy .    due to the complex nature of the field theory ,",
    "care must be taken to ensure that the stationary field configurations satisfying the mean - field equations have the correct saddle - point character , in that they correspond to local maxima for `` pressure - like '' fields that enter the hamiltonian as @xmath46 , and to local minima for other fields .    in order to search for the mean - field configuration in an efficient and stable way",
    ", we use a dynamical relaxation scheme with appropriate wick rotations applied to pressure - like fields . to move beyond the mean - field approximation ( scft ) and affect full fts simulations , we stochastically sample field configurations around the saddle - point using the complex langevin ( cl ) scheme@xcite .",
    "this scheme ameliorates the sign problem deriving from the complex `` probability distribution '' ( @xmath47 ) that leads to a critical loss of efficiency in monte carlo schemes .",
    "the numerical aspects of these methods will be detailed in the following section .",
    "with a model , and therefore a field - based hamiltonian , defined , field - theoretic simulations require a two - step iterative approach .",
    "the outer loop involves updating field configurations , either in search of a saddle - point configuration ( dynamical relaxation ) , or through stochastic exploration of the functional integrals ( complex langevin ) .",
    "these types of simulation differ only in that the latter contains a stochastic driving term in the equation of motion of the fields : @xmath48 where it is understood that here the fields , @xmath33 , are not wick rotated , and where the final term is omitted for mean - field scft calculations .",
    "the stochastic driving , @xmath49 , is a real , gaussian noise that is decorrelated in both space and time , and is defined by its first and second moments , which are selected to reproduce the correct time - averaged distribution using the fluctuation - dissipation theorem : @xmath50 and @xmath51 .    in order to solve for the time evolution of the chemical potential fields with discrete time stepping of the equation of motion ( eq .  [ eq : eom ] ) ,",
    "we employ recently developed stable and accurate integration schemes .",
    "for scft calculations , the most important numerical consideration is a wide stability window for large time steps .",
    "stable numerical schemes admit large time steps and therefore the rapid advancement of the fields to a saddle - point configuration , while details of the trajectory taken are less important . for this task",
    ", we have found the sis method introduced by ceniceros and fredrickson@xcite to be very efficient . in this approach , the linearized response of the density fields appearing in @xmath52",
    "are treated semi - implicitly to confer large gains in stability over standard forward - euler propagation .",
    "cl simulations , on the other hand , require _ accurate _ discretization schemes for time stepping in order to compute reliable time - averaged equilibrium quantities . achieving accurate time propagation for simulations in which fluctuations are large ( @xmath53 ) is not trivial .",
    "for this task , the recently introduced exponential time differencing ( etd ) algorithm@xcite , which incorporates the linearized force into an integrating factor , has proven to be accurate and efficient .",
    "still , with even the most efficient algorithms , the emphasis on time - step accuracy , and the requirement to collect stochastic averages of operators , necessitates that cl calculations in the strong fluctuation limit usually require one to two orders of magnitude more runtime than scft calculations , making fluctuating field - theoretic simulations an ideal candidate for acceleration with gpus .    to further formalize the discrete field - update algorithms , we discuss in detail only the example of a first - order semi - implicit scheme@xcite , which reduces to sis in the limit of zero noise .",
    "the final discretized time - stepping expressions in reciprocal space are : @xmath54}{\\delta\\hat{\\omega}^{t,\\vec{k } } }   + l\\left(\\frac{\\delta",
    "h\\left[\\left\\{\\hat{\\omega}^{t+\\delta t}\\right\\}\\right]}{\\delta \\hat{\\omega}^{t+\\delta t,\\vec{k}}}\\right)\\right]\\nonumber\\\\ & + & \\lambda\\delta t   l\\left(\\frac{\\delta",
    "h\\left[\\left\\{\\hat{\\omega}^{t}\\right\\}\\right]}{\\delta \\hat{\\omega}^{t,\\vec{k}}}\\right ) + \\mathcal{f}\\left(\\bar{\\eta}^{\\vec{x},t}\\right ) ,    \\label{eq : sis}\\end{aligned}\\ ] ] where @xmath55 implies taking the linear part of the force which , in the weak inhomogeneity limit , is typically of the form @xmath56 , where @xmath57 is a debye function that depends on the polymer architecture .",
    "@xmath58 is the discrete fourier transform , and @xmath59 is the _",
    "discretized _ gaussian noise , which has variance @xmath60 . treating the linear part of the force semi - implicitly leads to an effective wavevector - dependent time step : @xmath61}{\\delta \\hat{\\omega}^{\\vec{k},t } } + \\hat{\\bar{\\eta}}^{\\vec{k},t}.\\ ] ] any explicitly linear terms in the force expression are handled fully implicitly in the time - stepping algorithm , which further modifies the latter expression .",
    "we note here that field updates are entirely local in reciprocal space , even accounting for the linearization terms , so that the time propagation is insensitive to the choice of simulation boundary conditions for conditions compatible with plane wave and fourier bases .",
    "we also note that the field update equations are order-@xmath62 ( linear scaling with grid dimension , @xmath62 ) , local , and trivially parallelizable over the fourier spatial modes .    within the inner loop of a field - theoretic simulation , the modified diffusion equation ( eq .  [ eq : diffusion ] ) for the chain propagators , @xmath27 ,",
    "must be solved for the static field configurations so that the normalized partition functions and spatially resolved segment densities are available .",
    "for this task , we have found pseudo - spectral@xcite methods based on operator splitting to be the most efficient schemes .",
    "specifically , we use an extension to the original pseudo - spectral approach that cancels the lowest order error using richardson extrapolation , leading to a scheme that is globally fourth - order accurate in the step size of the discretized contour variable , @xmath63@xcite .",
    "the discrete solution for a single contour step may be written    @xmath64\\right ) & = & -\\frac{1}{3}\\left[e^{\\left(-\\omega\\left(\\vec{x}\\right)\\delta s/2\\right ) }   e^{\\left(\\nabla^2 \\delta s\\right ) }   e^{\\left(-\\omega\\left(\\vec{x}\\right)\\delta s/2\\right ) } \\right .",
    "\\nonumber \\\\      & - & \\left",
    ". 4 e^{\\left(-\\omega\\left(\\vec{x}\\right)\\delta s/4\\right ) }   e^{\\left(\\nabla^2 \\delta s/2\\right ) }   e^{\\left(-\\omega\\left(\\vec{x}\\right)\\delta s/2\\right ) } e^{\\left(\\nabla^2\\delta s/2\\right ) } e^{\\left(-\\omega\\left(\\vec{x}\\right)\\delta s/4\\right)}\\right ]   q\\left(\\vec{x},s;\\left[\\omega\\right]\\right ) + \\mathcal{o}\\left(\\delta s^5\\right ) ,    \\label{eq : rsos}\\end{aligned}\\ ] ]    where the application of the diffusion operator , @xmath65 , occurs in fourier space by forward and reverse fast fourier transform ( fft ) operations on the propagator .",
    "since the chemical potential field acts locally in real space , and the diffusion operator acts locally in fourier space , both operators are diagonal in the relevant representation , which makes evaluation of the exponentiated operators a numerically simple task . as we will show elsewhere@xcite",
    ", this scheme offers the best performance of our available methods , as measured by the wall - clock time taken to reduce the contour discretization error to an arbitrary pre - specified level@xcite . since the modified diffusion equation is semi - local ,",
    "boundary conditions must be specified .",
    "pseudo - spectral schemes can be easily developed for dirichlet , neumann and periodic boundary conditions by using discrete sine , cosine or fourier transforms respectively .",
    "a chebyshev - based pseudo - spectral approach is available for more general types of boundary conditions@xcite .",
    "our polymer field - theory code was written in c++ with strict object - oriented design principles and type templating .",
    "data is private and strictly encapsulated within each class , while an exposed public interface facilitates manipulation .",
    "this strategy allows a strong separation between the internal representation of any data structures and the operations that can be conducted on that data , which guarantees that the code for running on gpus , with its inaccessible memory spaces , or another parallel architecture , requires relatively few changes .",
    "inheritance is employed throughout the code to expose a common interface for objects of classes derived from a common parent , e.g. , the chain propagators for homopolymers , diblock copolymers , asymmetric multiblock polymers and other architectures are used identically after their creation .",
    "this feature provides a flexible framework for tackling a large body of field - theory problems .",
    "our class hierarchy can be broadly divided into low - level and high - level classes . within the low - level partition",
    "are classes that represent device - dependent functionality , such as an ` fftvector ` container for manipulating all functions of a single spatial variable that are subject to periodic boundary conditions . through inheritance ,",
    "these classes are targeted to run on a variety of platforms with highly optimized code for gpus , and for cpus exploiting openmp ( shared memory ) and/or mpi ( distributed memory ) parallelism .",
    "the low - level classes also include overloaded arithmetic operators to give a simple programming model for manipulating fundamental objects within the high - level classes .",
    "however , for optimal efficiency , we avoid binary operators ( ` * ` , ` + ` ,  ) , which necessitate creation of temporary objects .",
    "such a requirement imposes a high operational overhead when temporary objects can be tens or hundreds of megabytes in size , and we therefore take care to restructure all steps of our algorithms using only compound assignment operations ( ` * = ` , ` + = ` ,  ) and minimal object copies .",
    "the high - level partition of our code includes classes that define the components of various simulation types , including solving for the chain propagator , computing the normalized partition function and segment density , and stepping the discretized equation of motion with various `` plug - in '' field updater algorithms . since the code implementing rules of arithmetic",
    "are contained entirely within the low - level partition , the code in the high - level partition is ignorant of the details of the simulation platform .",
    "simulations running on a gpu , serial cpu , or parallel cpus have identical high - level code , as do simulations employing single- or double - precision floating - point arithmetic .",
    "developing code within the high - level partition can lead to the introduction of inefficient practices .",
    "we avoid this problem with careful code design and profiling , ensuring that our code is fft dominated on all platforms , and providing for a fair comparison between the runtimes for simulations running on gpu or cpu . for example , to achieve maximum throughput in solving the discretized diffusion equation , all exponentiated field operators used in the pseudo - spectral operator - splitting scheme ( eq .  [ eq : rsos ] ) are pre - computed when the propagator initial conditions are set .",
    "furthermore , specifically for the diffusion operator , pre - computation may occur just once per simulation provided the set of plane waves does not change ( i.e. , in the absence of variable - cell methods , which we do not consider in the present work ) .",
    "similarly , the linear and linearized force coefficients for the field time stepping ( eq .",
    "[ eq : sis ] ) are computed only once per simulation .",
    "our simulation code is parallelised over plane waves or real - space collocation grid points . for implementing the fts framework ,",
    "three distinct parallel patterns appear : fourier transformations , direct vector operations , and reduction operations .",
    "fourier transformations , which are required to solve the modified diffusion equation pseudo - spectrally , and to make sis and etd field updates local , are handled in cpu - targetted code by the freely available ` fftw ` library@xcite , which has serial and both simd and mimd parallel execution capabilities .",
    "we follow established conventions for data distribution with ` fftw ` .",
    "we compiled the ` fftw ` library with sse extensions , and thoroughly tested the performance of the library with respect to various compiler optimizations for each target platform . for simulations running in parallel ,",
    "three different strategies are available : shared - memory parallelism with openmp@xcite , distributed - memory parallelism with explicit message passing using mpi , and a combination of the two . with a dual - parallel execution model , it is important to be sure that cpu cores do not become oversubscribed by the launch of more threads than there are available cores .",
    "direct vector operations , used for example to apply exponentiated operators in the pseudo - spectral scheme , or to time - step fields once `` forces '' are fully determined , are trivially parallelisable , requiring no thread or process cooperation .",
    "reduction operations are used throughout our code to perform spatial integrations ( e.g. , in evaluating the normalized partition function , eq .",
    "[ eq : q ] , or operators such as the energy functional , eqns .",
    "[ eq : ham_edwards ] and [ eq : ham_db ] ) , or for evaluating any type of norm of a field or function .",
    "we implement reduction operations within mpi using ` mpi_allreduce ` operations , and within openmp using private variables for thread - level block reductions , followed by atomic updates of a shared result variable .",
    "we use version 12 of the intel c++ compiler wrapped with openmpi for compiling our cpu - target code .",
    "gpu architectures are optimized for high throughput computing rather than peak single - thread performance .",
    "they consist of a collection of symmetric multiprocessing units , each of which has a number of thread processors with distinct arithmetic logic units .",
    "for example , nvidia t20 series has @xmath66 multiprocessors with @xmath67 cores per processor , for a total of @xmath68 cores .",
    "gpu cores are relatively lightweight with simple instruction sets , and the hardware lacks many performance aids present in modern cpus , such as multiple pipelines and large on - chip caches .",
    "however , thread support is implemented in hardware , so that thread creation , destruction , switching and synchronization are low - overhead operations .    in principle",
    ", code can be written to target simultaneous gpu and cpu execution with load sharing . however , in most cases this would inevitably require explicit data transfer through the pci - express bus , which imposes a high penalty for simulation codes handling large data sets . for field theoretic simulations , we have yet to find any performance benefit in such load sharing .    while modern gpus have a large store of shared device - level dram ( @xmath69@xmath70 gb on nvidia tesla t20 series ) , memory accesses are relatively slow with high latency .",
    "this dictates a powerful strategy for achieving high performance : the gpu should be overloaded with threads .",
    "threads that are delayed due to memory latency are switched out by a scheduler in favor of those that are ready to run .",
    "the switching overhead is negligible , so in this way the memory latency can be `` hidden '' by overloading the gpu .",
    "threads are launched in groups , called blocks , of size determined by code implementation .",
    "efficient thread communication occurs only _ within _ blocks by individual threads loading and storing of data in a small reserve of on - chip shared memory .",
    "the shared memory is usually very limited , typically less than @xmath2 kb , but accesses are very fast .",
    "hence , in addition to being used for thread cooperation , the shared memory space can be used as a temporary store of data fetched from global device memory , to avoid subsequent repeat reads , and an important aspect of code optimization is the careful use of this limited resource .",
    "the blocking of threads imposes an important constraint for translating algorithms into gpu code : the order of block execution is undefined due to dynamic thread scheduling , so that cooperation between threads in different blocks is difficult and inefficient . for best performance and robust code ,",
    "the computation should not depend on the order in which blocks are executed .    at any given snapshot in time",
    ", threads will be running in multiples of the warp size ( @xmath67 ) , regardless of the block size .",
    "threads that are executing the same instructions concurrently will benefit with improved performance , as will threads that read neighboring memory addresses in device memory . hence , minimizing thread `` divergence '' , in terms of executed instructions and memory access patterns , yields optimal computational throughput .",
    "the general programming frameworks for gpus that have recently been developed , cuda and opencl , provide a small set of extensions to existing programming languages .",
    "host ( cpu ) code is compiled and executed as before , but extensions are included for orchestrating the launch of gpu `` kernels '' from the host code",
    ". with the release of these frameworks , gpu programming is significantly simplified , and the development of efficient code can be achieved by noting the small set of considerations listed above .",
    "given the design and optimization constraints outlined in the previous section , two crucial design principles emerge .",
    "efficient gpu use requires many threads to hide memory latency , each of which may be created and destroyed for the sole purpose of executing a single arithmetic operation on a single element of data .",
    "furthermore , data transfers between host system memory and gpu device memory should be infrequent .",
    "for these reasons , our field - theoretic code is fundamentally designed to permanently store all quantities on the gpu device memory .",
    "transfers of data to host system memory only occur for the infrequent output of instantaneous operator values and density - field snapshots .",
    "in addition , where possible , each thread during the execution of gpu kernels handles only a single plane - wave coefficient or real - space collocation sample of a function .    for the parallel patterns discussed in sec .  [",
    "sec : cpu ] , we have written efficient cuda kernels for parallelising the operations over individual spatial samples . the exception is the fourier transformations , which are handled by an efficient library , ` cufft ` , included with the cuda software development kit .",
    "thread computations within this library are by far the dominant part of our simulation runtime .",
    "complex langevin calculations require generation of the gaussian noise data for every field update ( see eq .  [",
    "eq : sis ] ) .",
    "the normally distributed noise can be generated with thread - wise parallelism on the gpu using the cuda `` curand '' library .",
    "the net result on runtimes is an overall @xmath71 increase in performance for cl simulations , compared with running the simulation on the gpu but generating the gaussian noise in serial on the cpu .    considering the",
    "vastly increased , but still limited , global memory size of @xmath69@xmath70 gb of dram available on modern gpus , a brief analysis of the memory costs of an fts calculation is in order . taking the example of a diblock copolymer melt that contains only a single distinct forward - chain propagator , we assume @xmath72 contour steps ( @xmath63 ) are required to achieve well - converged results for eq .",
    "[ eq : rsos ] .",
    "@xcite assuming all spatially dependent functions are resolved with @xmath73 plane waves in each of three spatial dimensions , corresponding to a large calculation , a single function of space represented with @xmath73-bit double - precision complex numbers requires @xmath67 mb of storage .",
    "thus , the chain propagator requires @xmath74 gb of data . upon introduction of the conjugate propagator , which is the same size",
    ", we already exceed the storage available on the largest gpus . each field and exponentiated operator , of which there are relatively few , also requires @xmath67 mb . to save memory , we do not store @xmath27 and @xmath31 separately , but rather the quantity @xmath75 . this quantity is computed and stored on - the - fly during the diffusion computation , so that the individual forward and conjugate propagators are never required to be stored individually .",
    "provided the propagators are only used to calculate the partition function and the density field , this quantity is all that is required , which allows for a @xmath76% reduction in memory cost .",
    "( note that other properties , such as the osmotic pressure or stress tensor , are more complicated functionals of the chain propagators , and this memory - saving measure can not be employed when such quantities are required ) . with this choice",
    ", almost all present simulations of interest can fit comfortably into @xmath70 gb of global storage .",
    "however , for the rare case that the gpu device memory is exhausted , we have implemented the option to move the quantity @xmath75 into host system memory as it is being computed .",
    "this saves @xmath77% of the memory cost of a typical calculation , at the expense of some performance loss .",
    "for our performance testing , we compare code runtimes on late - model cpus and gpus deployed in the ` knot ` cluster at the california nanosystems institute at the university of california , santa barbara .",
    "the cpus used in our tests were intel^^ six - core xeon^^ e5650 processors clocked at @xmath78ghz .",
    "each node of the cluster has two such six - core cpus installed on the motherboard , for a total shared - memory parallel capacity of 12 cores .",
    "115 nodes are connected in parallel using fast infiniband interconnects ( mt26438 from mellanox technologies ) for a total parallel capacity of 1380 cores . for the tests conducted purely with mpi - level parallelism , mpi processes can execute concurrently either on cores within the same cpu / node , or on cpu cores located in different nodes . in either case",
    "the parallel cooperation proceeds through explicit message passing , regardless of whether the cooperating cores have access to a common shared memory pool . for",
    "mpi computations involving more than 12 processes , the simulation inevitably must execute on multiple nodes with message passing through the infiniband interconnects , but with few processes we find that it is advantageous to run on a single node .",
    "this indicates that competition between the mpi processes for memory bandwidth provides less of a overhead than message passing between nodes .",
    "in contrast , with openmp parallelism execution threads may _ only _ cooperate through a shared memory pool ( typically @xmath79 gigabytes on ` knot ` ) and such simulations are therefore limited to @xmath80 cores the ` knot ` cluster . finally , for mixed ` openmp+mpi `",
    "parallelism , we exploit shared - memory cooperation within a node and explicit message passing for cooperation between nodes . in this case , each node handles two mpi process ( one for each multi - core cpu ) , which are responsible for orchestrating branching and merging of multiple openmp threads ( up to the maximum of @xmath80 per node ) .",
    "the gpus used in our tests were nvidia tesla c2050 boards based on the fermi architecture with 3 gb global dram .",
    "we turn off the error - correcting code ( ecc ) feature on the global memory storage of the gpu , leading to faster memory transactions .",
    "we observe an overall 10% performance boost with ecc turned off . unless otherwise stated , all of our timings are for simulations using double - precision arithmetic .",
    "[ fig : lamtimingsscft ] shows timing data for self - consistent field calculations of the lamellar ( lam ) phase of an incompressible melt of diblock copolymer . for this case",
    ", the calculations were run either on a _ single cpu core _",
    "( serial execution , albeit with some on - core vectorization in the form of sse instructions ) or a single gpu ( threaded parallel execution ) .",
    "panel ( a ) shows absolute timing data with respect to the total number of plane waves used to represent the field for 1d , 2d and 3d simulations .",
    "the gpu timings are clearly significantly shorter than simulations running on a single cpu core for all but the smallest of calculations , which suffer performance penalties due to not being able to overload the gpu with sufficient threads to hide memory latency .",
    "the gpu simulations require @xmath81sec .  for a single field update , even for more than one million plane waves .",
    "we also find that the gpu and cpu performance does not depend significantly on the dimensionality of space .",
    "[ fig : lamtimingsscft](b ) shows the relative speedup of the gpu code over the serially executed cpu code , defined as @xmath82 .",
    "the performance gap widens with increasing simulation size , and the peak speedup we observe is greater than @xmath0 .",
    "our largest calculation was a 3d simulation with @xmath73 plane waves per dimension , for a total of @xmath83 spatial degrees of freedom .",
    "larger simulations are possible , but they become increasingly challenging from the perspective of obtaining accurate timing statistics for single - core serial cpu references due to the long runtimes involved .        to demonstrate that the favorable gpu scaling we observed in the previous tests does not derive from a fortuitous benefit of phases such as lam with translational invariance in two of the spatial dimensions , we reran benchmarking tests on the gyroid phase of the incompressible diblock copolymer melt@xcite .",
    "this phase , shown in fig .",
    "[ fig : gyrmorph ] , has no unrestricted translational invariance and can not be represented in fewer than three spatial dimensions .",
    "the results shown in fig .",
    "[ fig : gyrtimingsscft ] expose a similar @xmath84 peak speedup over a single cpu core as found for simulations in the lam phase . in this case",
    ", we also compare to parallel cpu simulations conducted both with shared - memory ( openmp ) parallelism , and with distributed - memory ( mpi ) parallelism .",
    "we note that when all mpi processes are launched on a single shared - memory node when possible , the performance of our mpi implementation is comparable to that of our openmp implementation , indicating a low overhead of explicit message passing in this limit .",
    "presumably a result of the high communications overhead of the transposing steps taken in distributed - memory implementations of multi - dimensional ffts , we find an mpi performance that saturates at @xmath85 speedup even for 64 cpus .",
    "the result is that a single gpu outperforms even one hundred cpus cooperating via mpi ( not shown ) for our fft - dominated method .    ,",
    "and minority block fraction is @xmath86 .",
    "the simulation domain is of size @xmath87rg in each dimension .",
    "timings for this simulation are shown in fig .",
    "[ fig : gyrtimingsscft ] . ]        as discussed in sec .",
    "[ sec : cuda ] , for the rare simulations that exceed the gpu global shared memory capacity we have implemented the option to stage the quantity @xmath75 on - the - fly into host memory following each contour step in solving the discretized modified diffusion equation , eq .  [",
    "eq : rsos ] .",
    "subsequently , the density fields are calculated in serial on a single cpu core through integrations over the contour variable ( e.g. , eq .",
    "[ eq : dbdensity ] ) . with this option ,",
    "the memory allocation on the gpu is limited to a small number of individual fields and operators , while the much larger @xmath29-dependent propagators are never stored locally .",
    "while this option typically saves more than @xmath88% of the memory , depending on the underlying model , it also incurs some performance penalty both due to loss of parallelism on performing the contour integrals to evaluate the density , and from the cost of transferring gigabytes of data to host memory during the diffusion contour stepping .",
    "[ fig : rhocpu ] shows the performance loss from this memory - saving option , which amounts to approximately @xmath76% loss of the peak acceleration obtained from running entirely on a gpu .    .",
    "comparison of full simulation conducted on the gpu to a calculation in which the density is calculated using the cpu .",
    "the latter option significantly saves gpu device memory with some performance loss due to data transfer overhead and loss of parallelism . ]",
    "due to the reliance of our simulation code on type templating , switching arithmetic from double - precision ( 64-bit floating point numbers ) to single - precision ( 32-bit floats ) is a simple task . before the launch of the fermi architecture",
    ", nvidia gpus did not have hardware support for double - precision arithmetic , and even the latest hardware suffers a flop throughput reduction of a factor of two when performing double - precision arithmetic .",
    "hence , the ability to run calculations in single precision is desirable both to support older hardware , and to further improve performance . while single - precision arithmetic carries an associated increase in numerical error , which may accumulate during the solution of our non - linear differential equations , the advanced stable algorithms that we use sufficiently suppresses serious error accumulation and consequent destabilization .",
    "the utility of single - precision arithmetic in scft simulations is obvious for rapidly evolving to the saddle point solution of the mean - field equations .",
    "further refinement with double - precision processing can be performed if desired once the field iteration has converged in single precision .",
    "the runtime speedup for lam and gyr phases using single - precision arithmetic on a tesla c2050 gpu are shown in fig .  [ fig : sprec ] , with the reference runtime being the double - precision timings for serial cpu execution .",
    "we note that the cpu - only execution is not significantly accelerated when running with single - precision arithmetic , despite the use of sse3 instructions .     and [ fig : gyrtimingsscft ] respectively . ]",
    "[ fig : gyroidcl ] shows a comparison of serial , mpi and gpu runs of a complex langevin simulation of the cubic gyroid phase for relatively strong field fluctuations ( determined@xcite for the model of interest by the chain - density parameter @xmath89 ) . for this case",
    ", we plot the dynamical trace of the real part of the hamiltonian operator .",
    "once equilibration of the cl such trajectories has been achieved , time averages correspond to weighted averages over field configurations , and therefore to expectation values of the averaged operator . note that while the average of the hamiltonian operator , @xmath90 , is loosely linked to the helmholtz free energy@xcite , these quantities are not equal because @xmath90 does not include all entropic contributions from field fluctuations .",
    "however , this quantity remains a useful measure of equilibration of the cl trajectory .",
    "achieving fast equilibration of cl trajectories for large 3d simulations is a major challenge , and one that is only partially resolved by improved algorithms such as etd .",
    "[ fig : gyroidcl ] demonstrates that the @xmath91@xmath0 performance gain provided by running on gpus has a significant impact on the our ability to quickly move through the warmup stage of our simulations , with warmup achieved more than two times faster than @xmath2 mpi processes running on @xmath2 cpu cores .",
    "single - precision arithmetic can be used to further accelerate the warmup process by an additional factor of two . in the example",
    "shown , the operator averages in single- and double - precision match to with statistical error , though even if this were not the case , the equilibriated simulation could be continued accurately by switching to double precision after the warmup stage .",
    "note that the simulation cell used in this example is the smallest possible for the cubic gyroid phase , and fully capturing the effects of spatially decorrelated fluctuations may require larger simulation cells , which can require days to reach equilibrium with traditional mpi codes .     plotted against wall - clock runtime for the simulation of a single cubic unit cell of the gyroid phase of an incompressible diblock copolymer melt .",
    "the simulation parameters are the same as for the mean - field calculations of the same phase shown in fig .",
    "[ fig : gyrtimingsscft ] . for this example",
    ", we choose to represent fields with a grid of @xmath92 plane waves .",
    "the relative strength of fluctuations is determined by @xmath13 , which we set to @xmath76 for this simulation .",
    "operator values were saved after every @xmath72 field updates .",
    "single - precision ( sp ) gpu calculations progress approximately two times faster than double precision ( dp ) equivalents . ]    finally , we analyze the flat profile of serial single - core cpu and gpu runtimes shown in fig .  [",
    "fig : gpuprof](a ) and ( b ) respectively for the gyroid calculations presented in fig .",
    "[ fig : gyrtimingsscft ] .",
    "the profile data demonstrates that the gpu runtime is dominated by fast fourier transformation operations , with approximately @xmath93 of runtime consumed by this single activity .",
    "the remaining largest consumers of runtime are element - by - element products of field data for complex - complex multiplies ( @xmath94 ) or complex - real multiplies ( @xmath95 )",
    ". these three operations collectively account for @xmath96 of runtime on all simulation sizes .",
    "the latter two , involving multiplies within cuda kernels , are strongly bandwidth limited because multiple read / write operations are required for each arithmetic operation .",
    "we note that the design strategy of restricting all data to the gpu device memory , and not utilizing the host storage , results in less than @xmath97 of runtime being spent on cpu code and memory copies . we find similar profile data for our cpu - executed code , with the exception that fft costs reduce to only @xmath98 for the smallest of system sizes .    .",
    "the activities are : fast fourier transformations ( `` fft '' ) ; complex - complex vector multiplies , used in field operator application ( `` @xmath94 '' ) ; complex - real vector multiplies , used in application of the laplacian operator in reciprocal space ( `` @xmath95 '' ) ; the sum of two scaled vectors , used in richardson extrapolation of the operator - split diffusion equation ( `` @xmath99 '' ) ; and summation of two complex vectors , used in contour integration to obtain species density fields and in evolving the field equation of motion ( `` @xmath100 '' ) .",
    "the remaining runtime is consumed by memory copies and miscellaneous cpu - code execution . ]",
    "we have demonstrated that field - theoretic simulations running on a recent model graphics processing unit can progress up to sixty times faster than those running in serial on a single core of a recent model cpu . this enhanced performance was observed in simulations conducted in the mean - field ( scft ) limiting case , as well as in complex langevin simulations of the fully fluctuating field theory . due to the intensive communications overhead associated with fast fourier transforms , the same code running on cpus in parallel , either with shared or distributed memory strategies , saturates with approximately ten times speedup over a serial calculation .",
    "hence , a single gpu currently provides the highest throughput capacity for difficult field - theory problems , even when compared to the largest cpu - based clusters .",
    "the performance gains that we observe make fully fluctuating , large - scale 3d simulations of complex phases much more tractable .",
    "since gpu technology is evolving rapidly , and compute throughput is increasing significantly with each generation , we anticipate that future hardware developments or enhancements to the cuda numerical libraries will provide further significant `` drop - in '' gains in performance with little or no extra code development .",
    "while our peak gpu performance is very favorable , we note that small calculations do not benefit from running on a gpu . in such cases ,",
    "it is not possible to provide sufficient threads to hide memory latency .",
    "we find the break - even point to be somewhere between @xmath101 and @xmath102 total plane waves , which is a smaller basis size than is required for almost any problem of interest involving large simulation cells .",
    "strategies for further improvements in performance could include sharing the computation load between cpu and gpu , identifying strategies for exploitation of asynchronous gpu kernels , and adding additional levels of parallelism so that calculations can run simultaneously on multiple gpus .",
    "however , due to slow memory transfers and high communications overhead , it is unclear whether these strategies would provide significant further gains in performance .",
    "ktd was supported by the national science foundation solar program ( che-1035292 ) and ghf received support from nsf award dmr-0904499 .",
    "this work was partially supported by the mrsec program of the national science foundation under award no .",
    "dmr 1121053 and made use of the california nanosystems institute computing facility with resources provided by nsf award cns-0960316 .",
    "we thank paul weakliem of the cnsi computing facility for hardware support and fruitful discussions .",
    "the specific number of contour samples required will depend on the roughness of the field .",
    "scft calculations with a high @xmath103 parameter , and cl calculations in general , usually require increased contour resolution ."
  ],
  "abstract_text": [
    "<S> we report the first cuda^^ graphics - processing - unit ( gpu ) implementation of the polymer field - theoretic simulation framework for determining fully fluctuating expectation values of equilibrium properties for periodic and select aperiodic polymer systems . </S>",
    "<S> our implementation is suitable both for self - consistent field theory ( mean - field ) solutions of the field equations , and for fully fluctuating simulations using the complex langevin approach . </S>",
    "<S> running on nvidia^^ tesla t20 series gpus , we find double - precision speedups of up to @xmath0 compared to single - core serial calculations on a recent reference cpu , while single - precision calculations proceed up to @xmath1 faster than those on the single cpu core . </S>",
    "<S> due to intensive communications overhead , an mpi implementation running on @xmath2 cpu cores remains two times slower than a single gpu . </S>"
  ]
}