{
  "article_text": [
    "ever since the introduction of the popular television show _ deal or no deal _ , many authors have analyzed aspects of the game .",
    "_ deal or no deal _ provides an ` experiment ' with large stakes and a relatively simple probabilistic structure . using q - learning ( watkins , 1989 , whittle , 1982 , putterman , 1984 , polson and sorensen , 2011 )",
    "we address the question of optimal strategy .",
    "our solution technique provides a dynamic maximum expected utility solution ( ramsey , 1926 , de finetti , 1937 , von neumann and mortgensen , 1944 ) .",
    "q - learning is a popular reinforcement learning technique for calculating continuation values for sequential decision making .    in _ deal or no deal _",
    "contestants are presented with a simple decision of whether to take a banker s offer or to continue in the game .",
    "hence the value realized from deciding to decline the banker s offer - ` no deal ' - has a continuation value , a stylized fact of sequential decision making .",
    "previous empirical evidence has suggested that past outcomes affect future choices and that contestants have time - varying risk aversion .",
    "one key feature of continuation values is that they can appear to lead to the same effect ",
    "namely current actions appearing to exhibit time - varying risk aversion ",
    "when in fact it is no more than an optimal action to exercise the continuation value of the game .",
    "to illustrate these effects , we consider a simple scenario with logarithmic utility .",
    "this also illustrates a theoretical _ rule of thumb _ in _ deal or no deal _ : one should generally continue as long as there are two big prizes left .",
    "we analyze data from two players , frank and suzanne , from the european version of the game show ( post et al , 2008 ) .",
    "tables 1 and 2 provide the contestants choices .",
    "for example , in round seven , after several unlucky picks , frank opened the briefcase with the last remaining large prize (  @xmath0 ) and saw his expected prize tumble from  @xmath1 to  @xmath2 .",
    "the banker offered him  @xmath3 but frank rejected the offer and continued to play .",
    "he finally ended up with a briefcase with only  @xmath4 . in round",
    "nine , he even rejected a certain  @xmath5 in favor of a @xmath6 gamble of  @xmath4 or  @xmath7  clearly exhibiting risk seeking behavior .",
    "in contrast , suzanne was a `` lucky '' player . in round nine",
    "see faced a @xmath6 gamble of  @xmath8 or  @xmath9 ( two of the three largest prizes in the german edition ) . while she was hesitant in the earlier rounds , she rejected the banker s offer of  @xmath10  the expected payoff  and finally won the  @xmath9 prize .",
    "our analysis will track their choices and infer bounds on their levels of risk aversion at each stage .",
    "we provide a separate analysis of the last stage of the game , as both players exhibit risk seeking behavior here . other authors claim declining risk aversion of individuals after earlier expectations",
    "have been shattered by unfavorable outcomes .",
    "our approach shows that the continuation value is in fact high and therefore risk aversion of contestants need not decline in the aforementioned case .",
    "the rest of the paper is outlined as follows . in section 2",
    "we discuss the q - learning technique and how it is applied to the _ deal or no deal _ game , including log and power utility examples .",
    "section 3 solves for optimal strategy using q - learning .",
    "given an optimal strategy and the contestants empirical choices , we can then infer bounds on their risk aversion .",
    "we also analyze their terminal risk - seeking choices . finally , section 4 concludes .",
    "in the game _ deal or no deal _ , players are presented with a choice of briefcases which hold distinct monetary prize values .",
    "initially , players are asked to select one case , which will be referred to as ` their case ' and remain unopened . then",
    ", they proceed by choosing , and thus eliminating a predetermined number of cases each round . the values of these eliminated cases are shown to the player . at the end of each set of case eliminations , an entity referred to as ` the banker ' then presents the player with a monetary offer in exchange for their case . at this point ,",
    "the player is given two options : either take the banker s offer ( i.e. deal ) or continue eliminating cases ( i.e. no deal ) .",
    "the game continues until an offer is accepted or there is only one case remaining .    from this setup",
    ", we can decompose the game into a set of states .",
    "let the set @xmath11 contain all the possible prize values in the initial suitcases . let the set @xmath12 consist of all possible combinations of prize values from the given set of possible prize values @xmath11 .",
    "let , @xmath13 be one of the sets of possible remaining suitcase prize values . at each state @xmath14",
    ", the player is given a set of possible actions @xmath15 , defined as follows : @xmath16 after performing an action @xmath17 , the player then experiences a transition from state to state .",
    "we define the payoff to the player as a state - action map onto the space of real numbers : @xmath18 .",
    "the goal is simply to maximize the discounted resulting value of this mapping .",
    "q - learning allows us to maximize utility over a set of possible decisions , providing a map of the optimal path of actions .      in this subsection",
    ", we describe the basic ingredients of q - learning .",
    "first , we let the agent s immediate utility , taking the ` deal ' , be denoted by @xmath19 .",
    "the bellman principle of optimality states that the optimal solution path @xmath20 is the solution to the bellman equation defined by a value function @xmath21 that satisfies @xmath22 here @xmath23 is the transition matrix throughout states given action @xmath24 . in _ deal or no deal _ , actions can not affect the state s evolution , so we write @xmath25 . rather than directly computing the value function ,",
    "we instead calculate the matrix of @xmath26-values .",
    "they are defined as the total expected utility gained by choosing a current action @xmath27 and following the optimal path thereafter .",
    "an optimal policy must satisfy bellman s principle of optimality : _ that an optimal path has the property that whatever the initial conditions and control variables ( choices ) over some initial period , the control ( or decision variables ) chosen over the remaining period must be optimal for the remaining problem , with the state resulting from the early decisions taken to be the initial condition . _    to solve for this , we need a transition matrix for probabilities , a utility function and a banker s valuation function . to fix notation ,",
    "let @xmath13 denote the current state of the system and @xmath17 an action .",
    "define the @xmath26-value , @xmath28 , at time @xmath29 by the value of using action @xmath27 today and then proceeding optimally in the future .",
    "the bellman equation for @xmath26-values becomes : @xmath30 where @xmath31 is the set of all possible next period states given the current action @xmath27 , namely @xmath32 .",
    "the value function and optimal action are then simply given by : @xmath33 in our discrete setting , we can directly find the @xmath26-values without resorting to the simple stochastic approximation algorithms given in watkins ( 1989 ) and watkins and dayan ( 1992 ) .",
    "we now define the ingredients to solve the problem :    transition matrix .",
    ": :    since there is equal probability that a player chooses any of the    existing prize values , we define the transition probabilities as    follows :    @xmath34    for example , when you have three prizes left , with @xmath14 the    current state    @xmath35    where the transition matrix is uniform to the next state .",
    "+    there is no continuation value for taking the deal . if action    @xmath36 is chosen , then the value realized by the player is    either the utility of the offer presented by the banker or the utility    of the value of the player s case ( if no other cases remain ) .",
    "banker s function @xmath37 .",
    ": :    there are a number of different choices for modeling the banker s    function . in the live tv show",
    "one only sees the current banker offer    and of course , to address the issue of optimal policy and the    continuation value we need to know what they would offer in future    states of the world .",
    "one popular choice is expected value : let    @xmath38 be a possible prize , then    @xmath39    where @xmath14 is the set of the remaining prize values .    another choice is from the on - line version of the game where the    website ( www.nbc.com/dealornodeal ) uses the following criteria : let    _ big _ and _ small _ denote the biggest and smallest prizes left on the    board , respectively .",
    "then , the banker offer is given by    +    * with 3 prizes left :    @xmath40    * with 2 prizes left :    @xmath41 .",
    "+    this is not the case in the tv show as the banker has some discretion    on the offer .",
    "empirically , it almost strictly holds that :    @xmath42  the expected value of the remaining    prizes .",
    ": :    the utility of the next state depends on the contestant s value for    money and the bidding function @xmath37 of the banker . for    example , in the case of crra power utility    @xmath43    with log - utility @xmath44 a special case .",
    "a    more flexible choice is the exponential - power utility function    @xmath45    where @xmath46 is current wealth . for the purpose of our    analysis , we focus on the natural logarithm and crra cases .      to show that the continuation value can be large we consider an example where there are three prizes left including two large ones , @xmath47 .",
    "let us take an example where the contestant is risk averse with log - utility : @xmath48 .",
    "this is equivalent to the well - known kelly ( 1956 ) criterion .",
    "the contestant would be indifferent to a coin toss that doubled or halved their wealth .    for a base case analysis ,",
    "suppose that the banker s offers are determined by the expected value of the prizes left in the set @xmath14 .",
    "with log - utility this will look like a good deal in a one - shot version of the game . for this example",
    ", the utility of the offer is @xmath49 taking the deal leads to a utility @xmath50",
    ".    however , we have to compare this to the continuation problem ( ` no deal ' ) .",
    "the set of future possible states is @xmath51 where @xmath52 as the banker offers the expected value , if the contestant picks ` no deal ' we will have offers of @xmath53 , @xmath54 , and @xmath55 , respectively .",
    "this gives the following @xmath26-value calculation : @xmath56 with immediate utility given by @xmath57 .",
    "therefore , as @xmath58 the optimal action for the player at time @xmath29 is @xmath59 , ` deal ' .",
    "we see that the continuation value was not large enough to overcome the generous ( expected value ) offer by the banker .",
    "this analysis also illustrates the rule - of - thumb that most players should proceed as long as there are two large prizes left as the continuation value is high .",
    "the magnitude of the continuation value is related to the banker s bidding function .",
    "if we now use the banker s bidding function provided by the on - line game , defined by : @xmath60 hence , we can evaluate the banker s function at each of the @xmath61 next period states : @xmath62 using these , let us consider the optimal action with 2 prize values left for the player .",
    "to do so , we calculate the following @xmath26-values : @xmath63 since @xmath64 , the future optimal policy is ` no deal ' under @xmath65 . @xmath66 since @xmath67 ,",
    "the future optimal policy is ` deal ' under @xmath68 . @xmath69 since @xmath70 , the future optimal policy is ` deal ' under @xmath71 .",
    "now , solving for @xmath26-values at the previous step gives @xmath72 with a monetary equivalent of @xmath73 .",
    "this is the continuation value ( or ` no deal ' decision value ) at the current time period @xmath29 .",
    "we can compare this with the ` deal ' decision value : @xmath74 the banker then offers the contestant a monetary value of @xmath75 .",
    "now , comparing the @xmath26-values of the decisions gives the optimal action of @xmath76 , ` no deal ' as @xmath77 we get this result because the continuation value is large .",
    "essentially we are considering the difference between @xmath78 compared to @xmath79 , or a @xmath80% premium .    to extend this analysis to include higher levels of risk aversion we consider can power utility : @xmath81 where @xmath82 is a local measure of risk aversion , first introduced by de finetti ( 1952 ) .",
    "we note that as @xmath83 tends to 1 , the utility function above tends toward the aforementioned logarithmic utility function .",
    "therefore , the logarithmic utility function is just a special case of the more general power utility function .    with the same three prize values remaining :",
    "@xmath84 and an the expected value criterion for the banker s function , we get utilities : @xmath85 therefore , the utility of ` deal ' is @xmath86 .    as before",
    ", we consider the continuation problem ( ` no deal ' ) of the set @xmath51 with expected value banker offers leading to the values @xmath53 , @xmath54 , and @xmath55 , respectively .",
    "performing the @xmath26-value calculation gives : @xmath87 with immediate utility @xmath88 .",
    "hence , as the inequality @xmath89 holds for all levels of risk aversion @xmath90 , the optimal action is @xmath59 , ` deal ' .",
    "once again , we can see that the continuation value was not large enough to overcome the generous ( expected value ) offer by the banker .",
    "now , using the banker s bidding function from the website , we once again have : @xmath60 from the previous section , we found that @xmath91 , @xmath92 , and @xmath93 .",
    "we now consider the optimal action with 2 prize values left for the player .",
    "to do so , we calculate the following @xmath26-values : @xmath94 since @xmath64 for all levels of risk aversion @xmath90 , the future optimal policy is ` no deal ' under @xmath65 .",
    "@xmath95 here , for @xmath96 , we find that @xmath97 ( so the optimal future policy is ` no deal ' under @xmath68 ) . on the other hand , for @xmath98 , we find that @xmath67 ( so the future optimal policy is ` deal ' under @xmath68 ) .",
    "@xmath99 here , for @xmath100 , we find that @xmath101 ( so the optimal future policy is ` no deal ' under @xmath71 ) . on the other hand , for @xmath102 , we find that @xmath70 ( so the future optimal policy is ` deal ' under @xmath71 )",
    ".    solving for the @xmath26-values at the previous step gives @xmath103 from this , we can calculate the following monetary equivalents : @xmath104 these are the continuation values ( or ` no deal ' decision values ) at the current time period .",
    "we can compare these with the ` deal ' decision value : @xmath105 hence , the banker offers the contestant a monetary value of @xmath75 .",
    "now , comparing the @xmath26-values of the decisions :    * @xmath106 if @xmath107 with optimal action @xmath108 , ` no deal ' . * @xmath109 if @xmath110 with optimal action @xmath111 , ` deal ' .",
    "therefore , we can see that for most risk aversion levels ( except for really high values ) , the optimal action will be to choose ` no deal ' .",
    "this is because the continuation value is high relative to the value of the banker s offer . once again , for common risk aversion levels , we can approximately reduce the q - learning results to a simple rule of thumb .",
    "that is , continue as long as there are two large prizes left .",
    "a number of authors have argued that backwards induction appears most relevant in the early rounds of the game and that there is no difference with the myopic rule as the banker s offers ( near expected value ) lead risk averse players to proceed .",
    "our approach has shown that there is a significant continuation value at the end of the game .",
    "take for example , a set of three prizes containing two large ones .",
    "risk averse people will naively choose the action ` deal ' , when if they incorporated the continuation value , they would choose ` no deal ' . of course , this is sensitive to the banker s offer function .",
    "if risk aversion is very high as manifested by the contestant s utility function , then the continuation value may not always be high .",
    "post et al ( 2008 ) proposes that path - dependence factors heavily into the choices of contestants , and in fact the choices can be explained by varying levels of risk aversion as the game progresses favorably or unfavorably for the contestant .",
    "we find that this does not have to necessarily be the case since choosing to turn down a banker s offer does not have to imply decreasing risk aversion , but only a higher continuation value present by removing the myopia restriction / assumption on the contestants .    as well , as the game progresses , we generally see that we can place increasingly restrictive upper bounds on the risk aversion coefficient @xmath83 .",
    "this can be done whenever we observe a contestant choose the action ` no deal ' .",
    "however , since we can not place a lower bound on this parameter @xmath83 until we observe a contestant choose the action ` deal ' , we can not necessarily infer that the risk aversion level of a player is decreasing .",
    "this is because at the point that the action ` deal ' is taken , the game immediately ends .",
    "therefore , we can not observe a future action choice which requires a lower @xmath83 parameter .",
    "we now use the examples of the real german contestant susanne , as well as the dutch contestant frank ( both from post ) to illustrate our concept .",
    "we start by analyzing susanne s four remaining prizes @xmath112 at in round 7 .",
    "we let @xmath113 and work backwards from the final stage , time @xmath114 , where two prizes remain . hence",
    ", we evaluate all possible outcomes from the current state at time @xmath29 in order to determine @xmath26-values for this current state .",
    "first , we consider the expected banker s offers for all of the possible outcomes at @xmath115 . since for susanne",
    ", we can only observe the banker s offer for a single outcome , we use the observed percent of expected value of the given offer as a multiplier on the expected value of all other potential outcomes . for time @xmath115 , this is @xmath116 . using this , we calculate the banker s function for all other possible outcomes : @xmath117 therefore , we get the following @xmath26-values for the decisions @xmath118 : namely @xmath119 for any @xmath120 , by plugging into the power utility function .    similarly , we can work out the @xmath26-values for the decisions @xmath121 . for simplicity of notation",
    "we define @xmath122 .",
    "@xmath123 comparing these @xmath26-values at @xmath115 , we can determine the optimal actions at each of the possible states .",
    "that is , choose @xmath124 , @xmath125 since @xmath126 now , we consider the expected banker s offers at each of the possible states at time @xmath127 . here",
    "we use an expected value multiplier of @xmath128 : @xmath129 using these , we get the following @xmath26-values for the decisions @xmath130 for @xmath131 : @xmath132 using the optimal actions at @xmath115 , we can calculate the continuation @xmath26-values at @xmath127 , that is where @xmath133 : @xmath134 thus , we can see that : @xmath135 since we observe that suzanne , when faced with @xmath136 , chose action @xmath137 , we must have that her @xmath138 .    finally , we can now consider the banker s offer at the current state at time @xmath29 . here",
    ", we see that the expected value multiplier is @xmath139 : @xmath140 therefore , we get the following @xmath26-value for the decision @xmath141 : @xmath142 using the optimal actions at @xmath127 , we calculate the continuation @xmath26-values at @xmath29 , with @xmath143 and determine @xmath144&\\text{if } \\gamma<0.22077\\\\ \\frac{1}{4}\\left[q_{t+1}(1,1)+q_{t+1}(2,0)+q_{t+1}(3,1)+q_{t+1}(4,1)\\right]&\\text{if } 0.22077<\\gamma<0.22617\\\\ \\frac{1}{4}\\left[q_{t+1}(1,0)+q_{t+1}(2,0)+q_{t+1}(3,1)+q_{t+1}(4,1)\\right]&\\text{if } 0.22617<\\gamma<1.50645\\\\ \\frac{1}{4}\\left[q_{t+1}(1,0)+q_{t+1}(2,0)+q_{t+1}(3,0)+q_{t+1}(4,1)\\right]&\\text{if } 1.50645<\\gamma<1.54085 \\end{array } \\right.\\ ] ] where @xmath145 .    using our optimal choice of @xmath146 for a given value of @xmath83 we have :    * if @xmath147 , then @xmath148 for @xmath149 . * if @xmath150 , then @xmath148 for @xmath151 . * if @xmath152 , then @xmath148 for @xmath153 . *",
    "if @xmath154 , then @xmath148 for @xmath155 .",
    "therefore , for @xmath138 the optimal decision at time @xmath29 is @xmath143 , ` no deal ' , since the continuation value is the larger .",
    "this is consistent with the choice that susanne made .",
    "now , we can plot the evolving monetary values of each action @xmath27 at each point in time . in figure [ susanne ]",
    ", the solid line represents the value of the banker s offer ( @xmath36 ) , and the four dotted lines represent the continuation value ( @xmath156 ) for a range of ascending values of the risk aversion parameter @xmath83 . at each point in time , the contestant will choose the action with the larger value .",
    "therefore , we see that susanne will prefer action @xmath157 at time @xmath127 if her risk aversion parameter is @xmath138",
    ". however , at time @xmath115 , all values of @xmath158 will induce the optimal action @xmath159 since the value of the banker s offer is larger than all possible continuation values . naturally , at time @xmath160 , where the contestant holds only one case , there is complete certainty about the prize value , therefore ( for completeness ) the banker s offer has converged to the value of that prize .    also ,",
    "it is important to note that although we have only considered the final stages of susanne s game , it is unlikely that an earlier stage has placed a more restrictive upper bound on the risk aversion parameter . since the banker s offer as a percentage of expected value is increasing over time , choosing the action ` deal ' early on in the game becomes very unattractive . only a contestant with a very large risk aversion level",
    "will choose ` deal ' early and forgo the almost certain increasing percentage of expected value implicit in the banker s offer function .",
    "thus , we can see that given a constant risk aversion level of @xmath138 , it is possible to rationally observe the actions of the real player , susanne , except for the final time period @xmath115 .",
    "seemingly , only a completely risk - neutral or even risk - seeking player would be willing to turn down an expected value offer in exchange for a fair gamble on a set of prizes as per jensen s inequality .",
    "however , we have observed susanne turning down an offer of @xmath10 for a @xmath6 gamble on the set of prizes @xmath161 . what could cause her to do so while allowing us to not deviate from the constant risk aversion level hypothesis ?",
    "consider frank s four remaining prizes @xmath162 in round @xmath163 .",
    "as he plays through his remaining rounds , frank is presented with the following remaining prizes at each period : @xmath164 at each one of these periods , we observe the following banker s offers and implied @xmath165 values : @xmath166 interestingly , the banker s offer percentage is very high with respect to expected value .",
    "in fact , the final two offers are significantly above expected value .    at time",
    "period @xmath115 we can see that frank turns down the offer of @xmath5 for the 50/50 gamble between @xmath4 and @xmath7 .",
    "short of frank being risk averse and acting completely irrationally , there are two possible explanations for this observed action .",
    "either he truly has a risk aversion level characterized by @xmath167 ( i.e. he is risk seeking ) , or he has a large `` enjoyment '' benefit from playing the game ( i.e. his @xmath168 is quite large ) .",
    "it is possible that if he has a large enough value of @xmath168 , that his choice of ` no deal ' at time @xmath115 is not risk seeking , but actually risk averse !        as an additional point , looking at his decision at time @xmath127 , we can see that even though frank turns down a banker offer larger than the expected value , this choice may _ not _ have been risk seeking even if he has an `` enjoyment '' benefit of @xmath169 ! to see this , consider the continuation value : if frank expects that the banker s expected value offer percentage @xmath165 is increasing in each round , he may get an even larger banker s expected value offer percentage @xmath165 in the next round . therefore ,",
    "even if the @xmath165 value in one round is larger than @xmath170 , an observed action of ` no deal ' does not preclude risk aversion if an even larger @xmath165 value is expected in the next period .",
    "plainly , this is caused by a large continuation value .",
    "similarly as we did for susanne , figure [ frank ] shows the evolving monetary values of each action @xmath27 at every point in time .",
    "one interesting feature of suzanne s and frank s choices are their risk seeking behaviour at the terminal decision .",
    "clearly , there is no continuation value left in the game .",
    "how irrational are these decisions ?",
    "they are gaining enjoyment from other sources such as : enthusiasm from playing the game , audience encouragement , and even excitement from being on tv .",
    "if we assume that the marginal enjoyment benefit is positive at each further stage of the game since a player would likely prefer to play the game longer than shorter .",
    "if we let the marginal benefit to susanne to play the final round of the game to be the value @xmath168 , we then have the following condition to turn down the banker s final offer : @xmath171\\ ] ] @xmath172 therefore , we can see that even at the highest possible risk aversion level of @xmath173 , susanne only needs an `` enjoyment '' benefit of  @xmath174 in order to justify her ` no deal ' choice .",
    "the tv game show _ deal or no deal _ is well suited for analyzing risky choices .",
    "the stakes are high and the outcomes for the contestants can range from a multi - mullion payday to empty - handed .",
    "the game involves only binary decisions and there are no subjective probabilities as the odds are well - defined ahead of time .",
    "we show , however , that the sequential nature of the game induces a significant continuation value that has been missed by a number of researchers .",
    "choices can appear irrational and risky when in fact they are just an optimal strategy with reasonable ( constant ) parameters for risk aversion in a simple crra model of utility and choice .",
    "our approach uses @xmath26-learning , a popular technique in the reinforcement learning literature , to solve for the optimal strategy .",
    "we then show that choices provide bounds on risk aversion parameters conditional on optimal play .",
    "we analyze the frank and suzanne data - sets from post et al ( 2008 ) which tests the thaler and johnson ( 1990 ) hypothesis that many choices are affected by past outcomes . while post et al ( 2008 )",
    "conclude that many of the choices are the effect of time - varying risk aversion , our empirical findings are more in line with bombardini and trebbi ( 2007 ) who propose a dynamic expected utility model for the italian version of the game and conclude that contestants have constant levels of risk aversion but players are extremely heterogeneous in their beliefs .",
    "we note that the idea of time - varying risk aversion has been captured in the economics and finance literature in a number of ways .",
    "a common approach is via habit formation as shown by campbell and cochrane ( 1999 ) and brandt and wang ( 2011 ) .",
    "nevertheless , this seems unrealistic in such a short lived game .",
    "other interesting utility functions  where q - learning can easily be applied  are prospect theory which predicts possible time inconsistencies in gambling attitudes as discussed by barberis ( 2011 ) or recursive utility methods as proposed in kreps and porteus ( 1978 ) .",
    "we emphasize here that the neo - classical constant relative risk aversion model provides a realistic model for the contestants attitudes studied here except for their final risk seeking choices .",
    "banks , d. , f. petralia and s. wang ( 2011 ) . adversarial risk analysis : borel games .",
    "_ applied stochastic models _",
    ", 27(2 ) , 72 - 86 ."
  ],
  "abstract_text": [
    "<S> we derive an optimal strategy in the popular _ deal or no deal _ game show . </S>",
    "<S> q - learning quantifies the continuation value inherent in sequential decision making and we use this to analyze contestants risky choices . </S>",
    "<S> given their choices and optimal strategy , we invert to find implied bounds on their levels of risk aversion . in risky decision making , previous empirical evidence has suggested that past outcomes affect future choices and that contestants have time - varying risk aversion . </S>",
    "<S> we demonstrate that the strategies of two players ( suzanne and frank ) from the european version of the game are consistent with constant risk aversion levels except for their last risk - seeking choice . </S>"
  ]
}