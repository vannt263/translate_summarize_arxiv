{
  "article_text": [
    "the most fundamental parameter regarding a communication channel is unquestionably its capacity @xmath0 , a concept introduced by shannon @xcite that marks the highest rate at which information can be transmitted reliably over the channel .",
    "unfortunately , shannon s methods that established capacity as an achievable limit were non - constructive in nature , and the field of coding theory came into being with the agenda of turning shannon s promise into practical reality .",
    "progress in coding theory was very rapid initially , with the first two decades producing some of the most innovative ideas in that field , but no truly practical capacity - achieving coding scheme emerged in this early period .",
    "a satisfactory solution of the coding problem had to await the invention of turbo codes @xcite in 1990s .",
    "today , there are several classes of capacity - achieving codes , among them a refined version of gallager s ldpc codes from 1960s @xcite .",
    "( the fact that ldpc codes could approach capacity with feasible complexity was not realized until after their rediscovery in the mid-1990s . ) a story of coding theory from its inception until the attainment of the major goal of the field can be found in the excellent survey article @xcite .",
    "a recent addition to the class of capacity - achieving coding techniques is polar coding @xcite .",
    "polar coding was originally conceived as a method of boosting the channel cutoff rate @xmath1 , a parameter that appears in two main roles in coding theory .",
    "first , in the context of random coding and maximum likelihood ( ml ) decoding , @xmath1 governs the pairwise error probability @xmath2 , which leads to the union bound @xmath3 on the probability of ml decoding error @xmath4 for a randomly selected code with @xmath5 codewords .",
    "second , in the context of sequential decoding , @xmath1 emerges as the `` computational cutoff rate '' beyond which sequential decoding  a decoding algorithm for convolutional codes  becomes computationally infeasible .",
    "while @xmath1 has a fundamental character in its role as an error exponent as in , its significance as the cutoff rate of sequential decoding is a fragile one .",
    "it has long been known that the cutoff rate of sequential decoding can be boosted by designing variants of sequential decoding that rely on various channel combining and splitting schemes to create correlated subchannels on which multiple sequential decoders are employed to achieve a sum cutoff rate that goes beyond the sum of the cutoff rates of the original memoryless channels used in the construction .",
    "an early scheme of this type is due to pinsker @xcite , who used a concatenated coding scheme with an inner block code and an outer sequential decoder to get arbitrarily high reliability at constant complexity per bit at any rate @xmath6 ; however , this scheme was not practical .",
    "massey @xcite subsequently described a scheme to boost the cutoff by splitting a nonbinary erasure channel into correlated binary erasure channels .",
    "we will discuss both of these schemes in detail , developing the insights that motivated the formulation of polar coding as a practical scheme for boosting the cutoff rate to its ultimate limit , the channel capacity @xmath0 .",
    "the account of polar coding given here is not intended to be the shortest or the most direct introduction to the subject .",
    "rather , the goal is to give a historical account , highlighting the ideas that were essential in the course of developing polar codes , but have fallen aside as these codes took their final form . on a personal note , my acquaintance with sequential decoding began in 1980s during my doctoral work @xcite which was about sequential decoding for multiple access channels . early on in this period ,",
    "i became aware of the `` anomalous '' behavior of the cutoff rate , as exemplified in the papers by pinsker and massey cited above , and the resolution of the paradox surrounding the boosting of the cutoff rate has been a central theme of my research over the years .",
    "polar coding is the end result of such efforts .",
    "the rest of this paper is organized as follows .",
    "we discuss the role of @xmath1 in the context of ml decoding of block codes in section  [ sect : blockcoding ] and its role in the context of sequential decoding of tree codes in section  [ sect : sd ] . in section  [",
    "sect : boost ] , we discuss the two methods by pinsker and massey mentioned above for boosting the cutoff rate of sequential decoding . in section  [",
    "sect : sca ] , we examine the successive - cancellation architecture as an alternative method for boosting the cutoff rate , and in section  [ sect : polar ] introduce polar coding as a special instance of that architecture .",
    "the paper concludes in section  [ sect : conclusion ] with a summary .    throughout the paper , we use the notation @xmath7 to denote a discrete memoryless channel @xmath8 with input alphabet @xmath9 , output alphabet @xmath10 , and channel transition probabilities @xmath11 ( the conditional probability that @xmath12 is received given that @xmath13 is transmitted ) .",
    "we use the notation @xmath14 to denote a vector @xmath15 and @xmath16 to denote a subvector @xmath17 .",
    "all logarithms in the paper are to the base two .",
    "the goal of this section is to discuss the significance of the parameter @xmath1 in the context of block coding and ml decoding . throughout this section ,",
    "let @xmath18 be a fixed but arbitrary memoryless channel .",
    "we will suppress in our notation the dependence of channel parameters on @xmath8 with the exception of some definitions that are referenced later in the paper .      consider a communication system using block coding at the transmitter and ml decoding at the receiver . specifically , suppose that the system employs a @xmath19 block code , _",
    "i.e. _ , a code of length @xmath20 and rate @xmath21 , for transmitting one of @xmath22 messages by @xmath20 uses of @xmath8 .",
    "we denote such a code as a list of codewords @xmath23 , where each codeword is an element of @xmath24 .",
    "each use of this system comprises selecting , at the transmitter , a message @xmath25 at random from the uniform distribution , encoding it into the codeword @xmath26 , and sending @xmath26 over @xmath8 . at the receiver , a channel output @xmath27",
    "is observed with probability @xmath28 and @xmath27 is mapped to a decision @xmath29 by the ml rule @xmath30 the performance of the system is measured by the probability of ml decoding error , @xmath31    determining @xmath32 for a specific code @xmath33 is a well - known intractable problem .",
    "shannon s _ random - coding _",
    "method @xcite circumvents this difficulty by considering an ensemble of codes , in which each individual code @xmath23 is regarded as a sample with a probability assignment @xmath34 where @xmath35 is a channel input distribution .",
    "we will refer to this ensemble as a `` @xmath36 ensemble '' .",
    "we will use the upper - case notation @xmath37 to denote the random codeword for message @xmath38 , viewing @xmath26 as a realization of @xmath37 .",
    "the product - form nature of the probability assignment signifies that the array of @xmath39 symbols @xmath40 , constituting the code , are sampled in i.i.d . manner .",
    "the probability of error averaged over the @xmath36 ensemble is given by @xmath41 where the sum is over the set of all @xmath19 codes .",
    "classical results in information theory provide bounds of the form @xmath42 where the function @xmath43 is a _ random - coding exponent _ , whose exact form depends on the specific bounding method used . for an early reference on such random - coding bounds",
    ", we refer to fano ( * ? ? ?",
    "* chapter  9 ) , who also gives a historical account of the subject . here",
    ", we use a version of the random - coding bound due to gallager @xcite , ( * ? ? ?",
    "* theorem 5.6.5 ) ) , in which the exponent is given by @xmath44,\\ ] ] @xmath45^{1+\\rho}.\\ ] ] it is shown in @xcite that , for any fixed @xmath35 , @xmath46 for all @xmath47 , where @xmath48 is the channel capacity with input distribution @xmath35 , defined as @xmath49 this establishes that reliable communication is possible for all rates @xmath47 , with the probability of ml decoding error approaching zero exponentially in @xmath20 .",
    "noting that the channel capacity is given by @xmath50 the channel coding theorem follows as a corollary .    in a converse result @xcite",
    ", gallager also shows that @xmath43 is the best possible exponent of its type in the sense that @xmath51 for any fixed @xmath52 .",
    "this converse shows that @xmath43 is a channel parameter of fundamental significance in ml decoding of block codes .    for the best random - coding exponent of the type for a given @xmath21",
    ", one may maximize @xmath43 over @xmath35 , and obtain the optimal exponent as @xmath53    we conclude this part with an example . fig .",
    "[ fig : r0c ] gives a sketch of @xmath54 for a binary symmetric channel ( bsc ) , _ i.e. _ , a channel @xmath18 with @xmath55 and @xmath56 for some fixed @xmath57 .",
    "in this example , the @xmath35 that achieves the maximum in happens to be the uniform distribution , @xmath58 @xcite , as one might expect due to symmetry .",
    "the figure shows that @xmath54 is a convex function , starting at a maximum value @xmath59 at @xmath60 and decreasing to 0 at @xmath61 .",
    "the exponent has a straight - line portion for a range of rates @xmath62 , where the slope is @xmath63 .",
    "the parameters @xmath1 and @xmath64 are called , respectively , the _ cutoff rate _ and the _",
    "critical rate_. the union bound coincides with the random - coding bound over the straight - line portion of the latter , becomes suboptimal in the range @xmath65 ( shown as a dashed line in the figure ) , and useless for @xmath66 .",
    "these characteristics of @xmath54 and its relation to @xmath1 are general properties that hold for all channels . in the rest of this section ,",
    "we focus on @xmath1 and discuss it from various other aspects to gain a better understanding of this ubiquitous parameter .",
    "in general , the union bound is defined as @xmath67},\\end{aligned}\\ ] ] where @xmath68 is the channel cutoff rate with input distribution @xmath35 , defined as @xmath69 ^ 2.\\ ] ] the union bound may be obtained from the random - coding bound by setting @xmath70 in , instead of maximizing over @xmath71 and noticing that @xmath72 equals @xmath73 . the union bound and the random - coding bound coincide over a range of rates @xmath74 , where @xmath75 is the critical rate at input distribution @xmath35 .",
    "the tightest form of the union bound is obtained by using an input distribution @xmath35 that maximizes @xmath72 , in which case we obtain the usual form of the union bound as given by with @xmath76    the role of @xmath1 in connection with random - coding bounds can be understood by looking at the pairwise error probabilities under ml decoding . to discuss this , consider a specific code @xmath23 and fix two distinct messages @xmath77 , @xmath78 .",
    "let @xmath79 denote the probability of pairwise error , namely , the probability that the erroneous message @xmath80 appears to an ml decoder at least as likely as the correct message @xmath38 ; more precisely , @xmath81 where @xmath82 is a pairwise error event defined as @xmath83 although @xmath79 is difficult to compute for specific codes , its ensemble average , @xmath84 is bounded as @xmath85 we provide a proof of inequality in the appendix to show that this well - known and basic result about the cutoff rate can be proved easily from first principles .",
    "the union bound now follows from by noting that an ml error occurs only if some pairwise error occurs : @xmath86}.\\end{aligned}\\ ] ]    this completes our discussion of the significance of @xmath1 as an error exponent in random - coding . in summary",
    ", @xmath1 governs the random - coding exponent at low rates and is fundamental in that sense .    for future reference , we note here that , when @xmath8 is a binary - input channel with @xmath87 , the cutoff rate expression simplifies to @xmath88 where @xmath89 and @xmath90 is the channel _ bhattacharyya parameter _ , defined as @xmath91      consider a coding system identical to the one described in the preceding subsection , except now suppose that the decoder is a _ guessing _ decoder .",
    "given the channel output @xmath27 , a guessing decoder is allowed to produce a sequence of guesses @xmath92 at the correct message @xmath38 until a helpful `` genie '' tells the decoder when to stop .",
    "more specifically , after the first guess @xmath93 is submitted , the genie tells the decoder if @xmath94 ; if so , decoding is completed ; otherwise , the second guess @xmath95 is submitted ; and , so on .",
    "the operation continues until the decoder produces the correct message .",
    "we assume that the decoder never repeats an earlier guess , so the task is completed after at most @xmath96 guesses .",
    "an appropriate `` score '' for such a guessing decoder is the _ guesswork _ , which we define as the number of _ incorrect _ guesses @xmath97 until completion of the decoding task .",
    "the guesswork @xmath97 is a random variable taking values in the range @xmath98 to @xmath99 .",
    "it should be clear that the optimal strategy for minimizing the average guesswork is to use the ml order : namely , to set the first guess @xmath93 equal to a most likely message given @xmath27 ( as in ) , the second guess @xmath95 equal to a second most likely message given @xmath27 , _ etc_. we call a guessing decoder of this type an _ ml - type _ guessing decoder .",
    "let @xmath100 $ ] denote the average guesswork for an ml - type guessing decoder for a specific code @xmath33 .",
    "we observe that an incorrect message @xmath101 precedes the correct message @xmath38 in the ml guessing order only if a channel output @xmath27 is received such that @xmath102 ; thus , @xmath80 contributes to the guesswork only if a pairwise error event takes place between the correct message @xmath38 and the incorrect message @xmath80 .",
    "hence , we have @xmath103 = \\sum_{m}\\frac1 m",
    "\\sum_{m'\\neq m } p_{m , m'}(\\cc)\\ ] ] where @xmath79 is the pairwise error probability under ml decoding as defined in .",
    "we observe that the right side of is the same as the union bound on the probability of ml decoding error for code @xmath33 . as in the union bound , rather than trying to compute the guesswork for a specific code , we consider the ensemble average over all codes in a @xmath36 code ensemble , and obtain @xmath104 \\\\",
    "\\sum_{m}\\frac1 m \\sum_{m'\\neq m } \\overline{p}_{m , m'}(n , q),\\end{aligned}\\ ] ] which in turn simplifies by to @xmath105}.\\ ] ] the bound on the guesswork is minimized if we use an ensemble @xmath106 for which @xmath107 achieves the maximum of @xmath72 over all @xmath35 ; in that case , the bound becomes @xmath108}.\\ ] ]    in @xcite , the following converse result was provided for _ any _ code @xmath33 of rate @xmath21 and block length @xmath20 : @xmath109 \\ge \\max\\{0,2^{n(r - r_0-o(n))}-1\\},\\ ] ] where @xmath110 is a quantity that goes to 0 as @xmath20 becomes large .",
    "viewed together , and state that @xmath1 is a rate threshold that separates two very distinct regimes of operation for an ml - type guessing decoder on channel @xmath8 : for @xmath111 , the average guesswork is exponentially large in @xmath20 regardless of how the code is chosen ; for @xmath112 , it is possible to keep the average guesswork close to 0 by an appropriate choice of the code . in this sense , @xmath1 acts as a",
    "computational cutoff rate _ ,",
    "beyond which guessing decoders become computationally infeasible .",
    "although a guessing decoder with a genie is an artificial construct , it provides a valid computational model for studying the computational complexity of the sequential decoding algorithm , as we will see in sect .",
    "[ sect : sd ] .",
    "the interpretation of @xmath1 as a computational cutoff rate in guessing will carry over directly to sequential decoding .",
    "the random - coding results show that a code picked at random is likely to be a very good code with an ml decoder error probability exponentially small in code block length .",
    "unfortunately , randomly - chosen codes do not solve the coding problem , because such codes lack structure , which makes them hard to encode and decode .",
    "for a practically acceptable balance between performance and complexity , there are two broad classes of techniques .",
    "one is the algebraic - coding approach that eliminates random elements from code construction entirely ; this approach has produced many codes with low - complexity encoding and decoding algorithms , but so far none that is capacity - achieving with a practical decoding algorithm .",
    "the second is the probabilistic - coding approach that retains a certain amount of randomness while imposing a significant degree of structure on the code so that low - complexity encoding and decoding are possible .",
    "a tree code with sequential decoding is an example of this second approach .",
    "a tree code is a code in which the codewords conform to a tree structure .",
    "a convolutional code is a tree code in which the codewords are closed under vector addition .",
    "these codes were introduced by elias @xcite with the motivation to reduce the complexity of ml decoding by imposing a tree structure on block codes . in the discussion",
    "below , we will be considering tree codes with infinite length and infinite memory in order to avoid distracting details ; although , in practice , one would use a finite - length finite - memory convolutional code .",
    "the encoding operation for tree codes can be described with the aid of fig .",
    "[ fig : treecode ] , which shows the first four levels of a tree code with rate @xmath113 .",
    "initially , the encoder is situated at the root of the tree and the codeword string is empty . during each unit of time ,",
    "one new data bit enters the encoder and causes it to move one level deeper into the tree , taking the upper branch if the input bit is 0 , the lower one otherwise . as the encoder moves from one level to the next , it puts out the two - bit label on the traversed branch as the current segment of the codeword .",
    "an example of such encoding is shown in the figure , where in response to the input string 0101 the encoder produces 00111000 .    the decoding task for a tree code",
    "may be regarded as a search for the correct ( transmitted ) path through the code tree , given a noisy observation of that path .",
    "elias @xcite gave a random - coding argument showing that tree codes are capacity - achieving .",
    "( in fact , he proved this result also for time - varying convolutional codes . ) thus , having a tree structure in a code comes with no penalty in terms of capacity , but makes it possible to implement ml decoding at reduced complexity thanks to various search heuristics that exploit this structure .",
    "consider implementing ml decoding in an infinite tree code .",
    "clearly , one can not wait until the end of transmissions , decoding has to start with a partial ( and noisy ) observation of the transmitted path .",
    "accordingly , it is reasonable to look for a decoder that has , at each time instant , a working hypothesis with respect to the transmitted path but is permitted to go back and change that hypothesis as new observations arrive from the channel .",
    "there is no final decision in this framework ; all hypotheses are tentative .",
    "an algorithm of this type , called _ sequential decoding _ , was introduced by wozencraft @xcite , @xcite , and remained an important research area for more than a decade .",
    "a version of sequential decoding due to fano @xcite was used in the pioneer 9 deep - space mission in the late 1960s @xcite . following this brief period of popularity , sequential",
    "decoding was eclipsed by other methods and never recovered .",
    "( for a perspective on the rise and fall of sequential decoding , we refer to @xcite . )",
    "the main drawback of sequential decoding , which partly explains its decline , is the variability of computation .",
    "sequential decoding is an ml algorithm , capable of producing error - free output given enough time , but this performance comes at the expense of using a backtracking search .",
    "the time lost in backtracking increases with the severity of noise in the channel and the rate of the code . from the very beginning , it was recognized @xcite , @xcite that the computational complexity of sequential decoding is characterized by the existence of a computational cutoff rate , denoted @xmath114 ( or @xmath115 ) , that separates two radically different regimes of operation in terms of complexity : at rates @xmath116 the average number of decoding operations per bit remains bounded by a constant , while for @xmath117 , the decoding latency grows arbitrarily large . later work on sequential decoding",
    "established that @xmath114 coincides with the channel parameter @xmath1 . for a proof of the achievability part , @xmath118 , and bibliographic references",
    ", we refer to ( * ? ? ?",
    "* theorem 6.9.2 ) ; for the converse , @xmath119 , we refer to @xcite , @xcite .",
    "an argument that explains why `` @xmath120 '' can be given by a simplified complexity model introduced by jacobs and berlekamp @xcite that abstracts out the essential features of sequential decoding while leaving out irrelevant details .",
    "in this simplified model one fixes an arbitrary level @xmath20 in the code tree , as in fig .",
    "[ fig : treecodesearch ] , and watches decoder actions only at this level . the decoder visits level @xmath20 a number of times over the span of decoding , paying various numbers of visits to various nodes .",
    "a sequential decoder restricted to its operations at level @xmath20 may be seen as a type of guessing decoder in the sense of sect .",
    "[ subsect : guessing ] operating on the block code of length @xmath20 obtained by truncating the tree code at level @xmath20 . unlike the guessing decoder for a block code ,",
    "a sequential decoder does not need a genie to find out whether its current guess is correct ; an incorrect turn by the sequential decoder is sooner or later detected with probability one with the aid of a _ metric _ , _ i.e. _ , a likelihood measure that tends to decrease as soon as the decoder deviates from the correct path . to follow the guessing decoder analogy further ,",
    "let @xmath121 be the number of distinct nodes visited at level @xmath20 by the sequential decoder before its first visit to the correct node at that level . in light of the results given earlier for the guessing decoder ,",
    "it should not be surprising that @xmath122 $ ] shows two types of behavior depending on the rate : for @xmath111 , @xmath122 $ ] grows exponentially with @xmath20 ; for @xmath123 , @xmath122 $ ] remains bounded by a constant independent of @xmath20 .",
    "thus , it is natural that @xmath120 .    to summarize ,",
    "this section has explained why @xmath1 appears as a cutoff rate in sequential decoding by linking sequential decoding to guessing . while @xmath1 has a firm meaning in its role as part of the random - coding exponent",
    ", it is a fragile parameter as the cutoff rate in sequential decoding . by devising variants of sequential decoding ,",
    "it is possible to break the @xmath1 barrier , as examples in the next section will demonstrate .",
    "in this section , we discuss two methods for boosting the cutoff rate in sequential decoding .",
    "the first method , due to pinsker @xcite , was introduced in the context of a theoretical analysis of the tradeoff between complexity and performance in coding .",
    "the second method , due to massey @xcite , had more immediate practical goals and was introduced in the context of the design of a coding and modulation scheme for an optical channel .",
    "we present these schemes in reverse chronological order since massey s scheme is simpler and contains the prototypical idea for boosting the cutoff rate .",
    "a paper by massey @xcite revealed a truly interesting aspect of the cutoff rate by showing that it could be boosted by simply `` splitting '' a given channel .",
    "the simplest channel where massey s idea can be employed is a quaternary erasure channel ( qec ) with erasure probability @xmath124 , as shown in fig .",
    "[ fig : massey1](a ) .",
    "the capacity and cutoff rate of this channel are given by @xmath125 and @xmath126 .",
    "consider relabeling the inputs of the qec with a pair of bits as in fig .",
    "[ fig : massey1](b ) .",
    "this turns the qec into a vector channel with input @xmath127 and output @xmath128 and transition probabilities @xmath129 following such relabeling , we can split the qec into two binary erasure channels ( becs ) , as shown in fig .  [",
    "fig : massey1](c ) .",
    "the resulting becs are fully correlated in the sense that an erasure occurs in one if and only if an erasure occurs in the other .",
    "one way to employ coding on the original qec is to split it as above into two becs and employ coding on each bec independently , ignoring the correlation between them . in that case , the achievable sum capacity is given by @xmath130 , which is the same as the capacity of the original qec . even more surprisingly , the achievable sum cutoff rate after splitting is @xmath131 , which is strictly larger than @xmath132 for any @xmath133 .",
    "the capacity and cutoff rates for the two coding alternatives are sketched in fig .",
    "[ fig : massey3 ] , showing that substantial gains in the cutoff rate are obtained by splitting .",
    "the above example demonstrates in very simple terms that just by splitting a composite channel into its constituent subchannels one may be able obtain a net gain in the cutoff rate without sacrificing capacity .",
    "unfortunately , it is not clear how to generalize massey s idea to other channels .",
    "for example , if the channel has a binary input alphabet , it can not be split .",
    "even if the original channel is amenable to splitting , ignoring the correlations among the subchannels created by splitting may be costly in terms of capacity .",
    "so , massey s scheme remains an interesting isolated instance of cutoff - rate boosting by channel splitting .",
    "its main value lies in its simplicity and the suggestion that building correlated subchannels may be the key to achieving cutoff rate gains . in closing",
    ", we refer to @xcite for an alternative discussion of massey s scheme from the viewpoint of multi - access channels .",
    "pinsker was perhaps the first to draw attention to the flaky nature of the cutoff rate and suggest a general method to turn that into an advantage in terms of complexity of decoding .",
    "pinsker s scheme , shown in fig .",
    "[ fig : pinsker ] , combines sequential decoding with elias product - coding method @xcite .",
    "the main idea in pinsker s scheme is to have an inner block code clean up the channels seen by a bank of outer sequential decoders , boosting the cutoff rate seen by each sequential decoder to near 1 bit . in turn , the sequential decoders boost the reliability to arbitrarily high levels at low complexity . stated roughly , pinsker showed that his scheme can operate arbitrarily close to capacity while providing arbitrarily low probability of error at constant average complexity per decoded bit .",
    "the details are as follows .    following pinsker s exposition",
    ", we will assume that the channel @xmath8 in the system is a bsc with crossover probability @xmath134 , in which case the capacity is given by @xmath135 the user data consists of @xmath136 independent bit - streams , denoted @xmath137 .",
    "each stream is encoded by a separate convolutional encoder ( ce ) , with all ces operating at a common rate @xmath138 .",
    "each block of @xmath136 bits coming out of the ces is encoded by an inner block code , which operates at rate @xmath139 and is assumed to be a linear code .",
    "thus , the overall transmission rate is @xmath140 .",
    "the codewords of the inner block code are sent over @xmath8 by @xmath141 uses of that channel as shown in the figure .",
    "the received sequence is first passed through an ml decoder for the inner block code , then each bit obtained at the output of the ml decoder is fed into a separate sequential decoder ( sd ) , with the @xmath142th sd generating an estimate @xmath143 of @xmath144 , @xmath145 .",
    "the sds operate in parallel and independently ( without exchanging any information ) .",
    "an error is said to occur if @xmath146 for some @xmath145 .",
    "the probability of ml decoding error for the inner code , @xmath147 , is independent of the transmitted codeword since the code is _ linear _ and the channel is a bsc .",
    "arguments hold for any binary - input channel that is symmetric . ] . each frame error in the inner code causes a burst of bit errors that spread across the @xmath136 parallel bit - channels , but do not affect more than one bit in each channel thanks to interleaving of bits by the product code .",
    "thus , each bit - channel is a memoryless bsc with a certain crossover probability , @xmath148 , that equals the bit - error rate @xmath149 on the @xmath142th coordinate of the inner block code .",
    "so , the the cutoff rate `` seen '' by the @xmath142th ce - sd pair is @xmath150 which is obtained from with @xmath58 .",
    "these cutoff rates are uniformly good in the sense that @xmath151 since @xmath152 , as in any block code .",
    "it follows that the aggregate cutoff rate of the outer bit - channels is @xmath153 , which corresponds to a normalized cutoff rate of better than @xmath154 bits per channel use .",
    "now , consider fixing @xmath155 just below capacity @xmath156 and selecting @xmath141 large enough to ensure that @xmath157 .",
    "then , the normalized cutoff rate satisfies @xmath158 .",
    "this is the sense in which pinsker s scheme boosts the cutoff rate to near capacity .",
    "although pinsker s scheme shows that arbitrarily reliable communication at any rate below capacity is possible within constant complexity per bit , the `` constant '' entails the ml decoding complexity of an inner block code operating near capacity and providing near error - free communications .",
    "so , pinsker s idea does not solve the coding problem in any practical sense .",
    "however , it points in the right direction , suggesting that channel combining and splitting are the key to boosting the cutoff rate .      in this part",
    ", we discuss the two examples above in a more abstract way in order to identify the essential features that are behind the boosting of the cutoff rate .",
    "consider the system shown in fig .",
    "[ fig : derivedchannel ] that presents a framework general enough to accommodate both pinsker s method and massey s method as special instances .",
    "the system consists of a _ mapper _ @xmath159 and a _ demapper _ @xmath160 that implement , respectively , the combining and splitting operations for a given memoryless channel @xmath18 .",
    "the mapper and the demapper can be any functions of the form @xmath161 and @xmath162 , where the alphabets @xmath163 and @xmath164 , as well as the dimensions @xmath165 and @xmath20 are design parameters .",
    "the mapper acts as a pre - processor to create a derived channel @xmath166 from vectors of length @xmath165 over @xmath163 to vectors of length @xmath20 over @xmath10 .",
    "the demapper acts as a post - processor to create from @xmath167 a second derived channel @xmath168 .",
    "the well - known data - processing theorem of information theory @xcite states that @xmath169 where @xmath170 , @xmath171 , and @xmath172 denote the capacities of @xmath173 , @xmath167 , and @xmath8 , respectively .",
    "there is also a data - processing theorem that applies to the cutoff rate , stating that @xmath174 the data - processing result for the cutoff rate follows from a more general result given in @xcite in the context of `` parallel channels . '' in words ,",
    "inequality states that it is impossible to boost the cutoff rate of a channel @xmath8 if one employs a _",
    "single _ sequential decoder on the channels @xmath167 or @xmath173 derived from @xmath8 by any kind of pre - processing and post - processing operations .    on the other hand , there are cases where it is possible to split the derived channel @xmath173 into @xmath165 memoryless channels @xmath175 , @xmath176 , so that the normalized cutoff rate after splitting shows a cutoff rate gain in the sense that @xmath177 both pinsker s scheme and massey s scheme are examples where is satisfied . in pinsker s scheme ,",
    "the alphabets are @xmath178 , @xmath159 is an encoder for a binary block code of rate @xmath179 , @xmath160 is an ml decoder for the block code , and the bit channel @xmath180 is the channel between @xmath181 and @xmath182 , @xmath145 . in massey s scheme , with the qec labeled as in fig .",
    "[ fig : massey1]-(a ) , the length parameters are @xmath183 and @xmath184 , @xmath159 is the identity map on @xmath185 , and @xmath160 is the identity map on @xmath186 .",
    "as we conclude this section , a word of caution is necessary about the application of the above framework for cutoff rate gains .",
    "the coordinate channels @xmath187 created by the above scheme are in general not memoryless ; they interfere with each other in complex ways depending on the specific @xmath159 and @xmath160 employed . for a channel @xmath188 with memory ,",
    "the parameter @xmath189 loses its operational meaning as the cutoff rate of sequential decoding .",
    "pinsker avoids such technical difficulties in his construction by using a _ linear _ code and restricting the discussion to a _",
    "symmetric _ channel . in designing systems that target cutoff rate gains as promised by",
    ", these points should not be overlooked .",
    "in this section , we examine the successive - cancellation ( sc ) architecture , shown in fig .  [",
    "fig : derivedchannel2 ] , as a general framework for boosting the cutoff rate .",
    "the sc architecture is more flexible than pinsker s architecture in fig .",
    "[ fig : derivedchannel ] , and may be regarded as a generalization of it .",
    "this greater flexibility provides significant advantages in terms of building practical coding schemes , as we will see in the rest of the paper . as usual",
    ", we will assume that the channel in the system is a binary - input channel @xmath190 .      as seen in fig .",
    "[ fig : derivedchannel2 ] , the transmitter in the sc architecture uses a 1 - 1 mapper @xmath191 that combines @xmath20 independent copies of @xmath8 to synthesize a channel @xmath192 with transition probabilities @xmath193    the sc architecture has room for @xmath20 ces but these encoders do not have to operate at the same rate , which is one difference between the sc architecture and pinsker s scheme",
    ". the intended mode of operation in the sc architecture is to set the rate of the @xmath142th encoder ce@xmath194 to a value commensurate with the capability of that channel .",
    "the receiver side in the sc architecture consists of a soft - decision generator ( sdg ) and a chain of sds that carry out sc decoding . to discuss the details of the receiver operation ,",
    "let us index the blocks in the system by @xmath195 . at time @xmath195 , the @xmath195th code block @xmath196 , denoted @xmath197 , is transmitted ( over @xmath20 copies of @xmath8 ) and @xmath198 is delivered to the receiver .",
    "assume that each round of transmission lasts for @xmath199 time units , with @xmath200 being sent and @xmath201 received .",
    "let us write @xmath202 to denote @xmath200 briefly .",
    "let us use similar time - indexing for all other signals in the system , for example , let @xmath203 denote the data at the input of the @xmath142th encoder ce@xmath204 at time @xmath195",
    ".    decoding in the sc architecture is done layer - by - layer , in @xmath20 layers : first , the data sequence @xmath205 is decoded , then @xmath206 is decoded , and so on . to decode the first layer of data @xmath207 , the sdg computes the soft - decision variables @xmath208 as a function of @xmath209 and feeds them into the first sequential decoder sd@xmath210 . given @xmath208 ,",
    "sd@xmath211 calculates two sequences : the estimates @xmath212 of @xmath207 , which it sends out as its final decisions about @xmath207 ; and , the estimates @xmath213 of @xmath214 , which it feeds back to the sdg .",
    "having received @xmath213 , the sdg proceeds to compute the soft - decision sequence @xmath215 and feeds them into sd@xmath216 , which , in turn , computes the estimates @xmath217 and @xmath218 , sends out @xmath217 , and feeds @xmath218 back into the sdg . in general , at the @xmath142th layer of sc decoding , the sdg computes the sequence @xmath219 and feeds it to sd@xmath194 , which in turn computes a data decision sequence @xmath220 , which it sends out , and a second decision sequence @xmath221 , which it feeds back to sdg .",
    "the operation is completed when the @xmath20th decoder sd@xmath222 computes and sends out the data decisions @xmath223 .      for capacity and cutoff rate analysis of the sc architecture",
    ", we need to first specify a probabilistic model that covers all parts of the system . as usual",
    ", we will use upper - case notation to denote the random variables and vectors in the system . in particular , we will write @xmath224 to denote the random vector at the output of the 1 - 1 mapper @xmath191 ; likewise , we will write @xmath225 to denote the random vector at the input of the sdg @xmath226",
    ". we will assume that @xmath224 is uniformly distributed , @xmath227 since @xmath224 and @xmath225 are connected by @xmath20 independent copies of @xmath8 , we will have @xmath228 the ensemble @xmath229 , thus specified , will serve as the core of the probabilistic analysis .",
    "next , we expand the probabilistic model to cover other signals of interest in the system .",
    "we define @xmath230 as the random vector that appears at the output of the ces in fig .",
    "[ fig : derivedchannel2 ] .",
    "since @xmath230 is in 1 - 1 correspondence with @xmath224 , it is uniformly distributed .",
    "we define @xmath231 as the random vector that the sds feed back to the sdg as the estimate of @xmath230 .",
    "ordinarily , any practical system has some non - zero probability that @xmath232 .",
    "however , modeling such decision errors and dealing with the consequent error propagation effects in the sc chain is a difficult problem .",
    "to avoid such difficulties , we will assume that the outer code in fig .",
    "[ fig : derivedchannel2 ] is perfect , so that @xmath233 this assumption eliminates the complications arising from error - propagation ; however , the capacity and cutoff rates calculated under this assumption will be optimistic estimates of what can be achieved by any real system .",
    "still , the analysis will serve as a roadmap and provide benchmarks for practical system design .",
    "( in the case of polar codes , we will see that the estimates obtained under the above ideal system model are in fact achievable . ) finally , we define the soft - decision random vector @xmath234 at the output of the sdg so that its @xmath142th coordinate is given by @xmath235 ( if it were not for the modeling assumption , it would be appropriate to use @xmath236 in the definition of @xmath237 instead of @xmath238 . )",
    "this completes the specification of the probabilistic model for all parts of the system .",
    "we first focus on the capacity of the channel @xmath239 created by combining @xmath20 copies of @xmath8 .",
    "since we have specified a uniform distribution for channel inputs , the applicable notion of capacity in this analysis is the _ symmetric _ capacity , defined as @xmath240 where @xmath241 is the uniform distribution , @xmath242 .",
    "likewise , the applicable cutoff rate is now the symmetric one , defined as @xmath243    in general , the symmetric capacity may be strictly less than the true capacity , so there is a penalty for using a uniform distribution at channel inputs . however , since we will be dealing with linear codes , the uniform distribution is the only appropriate distribution here .",
    "fortunately , for many channels of practical interest , the uniform distribution is actually the optimal one for achieving the channel capacity and the cutoff rate .",
    "these are the class of symmetric channels .",
    "a binary - input channel is called _ symmetric _ if for each output letter @xmath244 there exists a `` paired '' output letter @xmath245 ( not necessarily distinct from @xmath244 ) such that @xmath246",
    ". examples of symmetric channels include the bsc , the bec , and the additive gaussian noise channel with binary inputs .",
    "as shown in @xcite , for a symmetric channel , the symmetric versions of channel capacity and cutoff rate coincide with the true ones .",
    "we now turn to the analysis of the capacities of the bit - channels created by the sc architecture .",
    "the sc architecture splits the vector channel @xmath239 into @xmath20 bit - channels , which we will denote by @xmath247 , @xmath248 .",
    "the @xmath142th bit - channel connects the output @xmath249 of ce@xmath194 to the input @xmath237 of sd@xmath194 , @xmath250 and has symmetric capacity @xmath251 here , @xmath252 denotes the mutual information between @xmath249 and @xmath237 . in the following analysis",
    ", we will be using the mutual information function and some of its basic properties , such as the chain rule .",
    "we refer to ( * ? ? ?",
    "2 ) for definitions and a discussion of such basic material .",
    "the aggregate symmetric capacity of the bit - channels is calculated as @xmath253 where the equality ( 1 ) is due to the fact that @xmath249 and @xmath238 are independent , ( 2 ) is by the chain rule , ( 3 ) by the 1 - 1 property of @xmath191 , ( 4 ) by the memoryless property of the channel @xmath8 .",
    "thus , the aggregate symmetric capacity of the underlying @xmath20 copies of @xmath8 is preserved by the combining and splitting operations .",
    "our main interest in using the sc architecture is to obtain a gain in the aggregate cutoff rate .",
    "we define the normalized symmetric cutoff rate under sc decoding as @xmath254 the objective in applying the sc architecture may be stated as devising schemes for which @xmath255 holds by a significant margin",
    ". the ultimate goal would be to have @xmath256 approach @xmath257 as @xmath20 increases .    for specific examples of schemes that follow the sc architecture and achieve cutoff rate gains in the sense of",
    ", we refer to @xcite and the references therein .",
    "we must mention in this connection that the multilevel coding scheme of imai and hirakawa @xcite is perhaps the first example of the sc architecture in the literature , but the focus there was not to boost the cutoff rate . in the next section",
    ", we discuss polar coding as another scheme that conforms to the sc architecture and provides the type of cutoff rate gains envisaged by .",
    "polar coding is an example of a coding scheme that fits into the framework of the preceding section and has the property that @xmath258 the name `` polar '' refers to a phenomenon called `` polarization '' that will be described later in this section .",
    "we begin by describing the channel combining and splitting operations in polar coding .",
    "the channel combining and splitting operations in polar coding follow the general principles already described in detail in sect .",
    "[ subsect : sca ] .",
    "we only need to describe the particular 1 - 1 transformation @xmath191 that is used for constructing a polar code of size @xmath20 .",
    "we will begin this description starting with @xmath259 .",
    "the basic module of the channel combining operation in polar coding is shown in fig .",
    "[ fig : polar0 ] , in which two independent copies of @xmath260 are combined into a channel @xmath261 using a 1 - 1 mapping @xmath262 defined by @xmath263 where @xmath264 denotes modulo-2 addition in the binary field @xmath265 .",
    "we call the basic transform @xmath262 the _ kernel _ of the construction .",
    "( we defer the discussion of how to find a suitable kernel until the end of this subsection . )",
    "polar coding extends the above basic combining operation recursively to constructions of size @xmath266 , for any @xmath267 . for @xmath268 , the polar code construction",
    "is shown in fig .",
    "[ fig : polar2 ] , where 4 independent copies of @xmath8 are combined by a 1 - 1 mapping @xmath269 into a channel @xmath270 .",
    "the general form of the recursion in polar code construction is illustrated in fig .",
    "[ fig : polar3 ] and can be expressed algebraically as @xmath271 where @xmath264 denotes the componentwise mod-2 addition of two vectors of the same length over @xmath272 .",
    "the transform @xmath273 is _ linear _ over the vector space @xmath274 , and can be expressed as @xmath275 where @xmath276 and @xmath196 are row vectors , and @xmath277 is a matrix defined recursively as @xmath278 or , simply as @xmath279 where the `` @xmath280 '' in the exponent denotes kronecker power of a matrix @xcite .    the recursive nature of the mapping @xmath281 makes it possible to compute @xmath282 in time complexity @xmath283 .",
    "the polar transform @xmath191 is a `` fast '' transform over the field @xmath272 , akin to the `` fast fourier transform '' of signal processing .",
    "we wish to comment briefly on how to select a suitable kernel ( basic module such as @xmath262 above ) to get the polar code construction started . in general ,",
    "not only the kernel , but its size is also a design choice in polar coding ; however , all else being equal , it is advantageous to use a small kernel to keep the complexity low .",
    "the specific kernel @xmath262 above has been found by exhaustively studying all @xmath284 alternatives for a kernel of size @xmath259 , corresponding to all permutations ( 1 - 1 mappings ) of binary vectors @xmath285 .",
    "six of the permutations are listed in table  [ table : permutations ] .",
    ".basic permutations of @xmath286 [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]     [ table : permutations ]    the title row displays the regular order of elements in @xmath287 , each subsequent row displays a particular permutation of the same elements .",
    "each permutation listed in the table happens to be a linear transformation of @xmath286 , with a transformation matrix as shown as the final entry of the related row .",
    "the remaining 18 permutations that are not listed in the table can be obtained as affine transformations of the six that are listed .",
    "for example , by adding ( mod-2 ) a non - zero constant offset vector , such as @xmath288 , to each entry in the table , we obtain six additional permutations .",
    "the first and the second permutations in the table are trivial permutations that provide no channel combining .",
    "the third and the fifth permutations are equivalent from a coding point of view ; their matrices are column permutations of each other , which corresponds to permuting the elements of the codeword during transmission  an operation that has no effect on the capacity or cutoff rate .",
    "the fourth and the sixth permutations are also equivalent to each other in the same sense that the third and the fifth permutations are .",
    "the fourth permutation is not suitable for our purposes since it does not provide any channel combining ( entanglement ) under the decoding order @xmath289 first , @xmath290 second .",
    "for the same reason , the sixth permutation is not suitable , either .",
    "the third and the fifth permutations ( and their affine versions ) remain as the only viable alternatives ; and they are all equivalent from a capacity / cutoff rate viewpoint . here , we use the third permutation since it is the simplest one among the eight viable candidates .      for the analysis in this section , we will use the general setting and notation of sect .",
    "[ subsect : scacapacitycutoff ] .",
    "we begin our analysis with the case @xmath259 .",
    "the basic transform creates a channel @xmath291 with transition probabilities @xmath292 this channel is split by the sc scheme into two bit - channels @xmath293 and @xmath294 with transition probabilities @xmath295    here , we introduce the alternative notation @xmath296 and @xmath297 to denote @xmath298 and @xmath299 , respectively",
    ". this notation will be particularly useful in the following discussion .",
    "we observe that the channel @xmath296 treats @xmath300 as pure noise ; while , @xmath297 treats @xmath301 as an observed ( known ) entity .",
    "in other words , the transmission of @xmath301 is hampered by interference from @xmath300 ; while @xmath300 `` sees '' a channel of diversity order two , after `` canceling '' @xmath301 .",
    "based on this interpetation , we may say that the polar transform creates a `` bad '' channel @xmath296 and a `` good '' channel @xmath297 .",
    "this statement can be justified by looking at the capacities of the two channels .",
    "the symmetric capacities of @xmath296 and @xmath297 are given by @xmath302 we observe that @xmath303 which is a special instance of the general conservation law .",
    "the symmetric capacity is conserved , but redistributed unevenly .",
    "it follows from basic properties of mutual information function that @xmath304 where the inequalities are strict unless @xmath257 equals 0 or 1 . for a proof",
    ", we refer to @xcite .",
    "we will call a channel @xmath8 _ extreme _ if @xmath257 equals 0 or 1 .",
    "extreme channels are those for which there is no need for coding : if @xmath305 , one can send data uncoded ; if @xmath306 , no code will help .",
    "inequality states that , unless the channel @xmath8 is extreme , the size-2 polar transform creates a channel @xmath297 that is strictly better than @xmath8 , and a second channel @xmath296 that is strictly worse than @xmath8 . by doing so",
    ", the size-2 transform starts the polarization process .",
    "as regards the cutoff rates , we have @xmath307 where the inequality is strict unless @xmath8 is extreme .",
    "this result , proved in @xcite , states that the basic transform _",
    "always _ creates a cutoff rate gain , except when @xmath8 is extreme .",
    "an equivalent form of , which is the one that was actually proved in @xcite , is the following inequality about the bhattacharyya parameters , @xmath308 where strict inequality holds unless @xmath8 is extreme .",
    "the equivalence of and is easy to see from the relation @xmath309,\\ ] ] which is a special form of with @xmath310 .    given that the size-2 transform improves the cutoff rate of any given channel @xmath8 ( unless @xmath8 is already extreme in which case there is no need to do anything ) , it is natural to seek methods of applying the same method recursively so as to gain further improvements .",
    "this is the main intuitive idea behind polar coding .",
    "as the cutoff - rate gains are accummulated over each step of recursion , the synthetic bit - channels that are created in the process keep moving towards extremes .    to see how recursion helps improve the cutoff rate as the size of the polar transform",
    "is doubled , let us consider the next step of the construction , @xmath268 .",
    "the key recursive relationships that tie the size-4 construction to size-2 construction are the following : @xmath311 the first claim @xmath312 means that @xmath313 is equivalent to the bad channel obtained by applying a size-2 transform on two independent copies of @xmath298 .",
    "the other three claims can be interpreted similarly .    to prove the validity of and ,",
    "let us refer to fig .",
    "[ fig : polar2 ] again .",
    "let @xmath314 denote the ensemble of random vectors that correspond to the signals @xmath315 in the polar transform circuit . in accordance with the modeling assumptions of sect .  [ subsect : scacapacitycutoff ] , the random vector @xmath316 is uniformly distributed over @xmath317 . since both @xmath318 and @xmath319 are in 1 - 1 correspondence with @xmath320 , they , too ,",
    "are uniformly distributed over @xmath317 .",
    "furthermore , the elements of @xmath316 are i.i.d .",
    "uniform over @xmath321 , and similarly for the elements of @xmath322 , and of @xmath319 .",
    "let us now focus on fig .",
    "[ fig : polar5 ] which depicts the relevant part of fig .",
    "[ fig : polar2 ] for the present discussion .",
    "consider the two channels @xmath323 embedded in the diagram .",
    "it is clear that @xmath324 furthermore , the two channels @xmath325 and @xmath326 are independent .",
    "this is seen by noticing that @xmath325 is governed by the set of random variables @xmath327 , which is disjoint from the set of variables @xmath328 that govern @xmath326 .",
    "returning to the size-4 construction of fig .",
    "[ fig : polar2 ] , we now see that the effective channel seen by the pair of inputs @xmath329 is the combination of @xmath325 and @xmath326 , or equivalently , of two independent copies of @xmath330 , as shown in fig .",
    "[ fig : polar6 ] .",
    "the first pair of claims follows immediately from this figure .",
    "the second pair of claims follows by observing that , after decoding @xmath329 , the effective channel seen by the pair of inputs @xmath331 is the combination of two independent copies of @xmath332 as shown in fig .",
    "[ fig : polar7 ] .",
    "the following conservation rules are immediate from .",
    "@xmath333 likewise , we have , from , @xmath334 here , we extended the notation and used @xmath335 to denote @xmath336 , and similarly for @xmath337 , etc .    if we normalize the aggregate cutoff rates for @xmath268 and compare with the normalized cutoff rate for @xmath259 , we obtain @xmath338 these inequalities are strict unless @xmath8 is extreme .    the recursive argument given above can be applied to the situtation in fig .",
    "[ fig : polar3 ] to show that for any @xmath266 , @xmath339 , and @xmath248 , the following relations hold @xmath340 from which it follows that @xmath341    these results establish that the sequence of normalized cutoff rates @xmath342 is monotone non - decreasing in @xmath20 . since @xmath343 for all @xmath20 ,",
    "the sequence must converge to a limit .",
    "it turns out , as might be expected , that this limit is the symmetric capacity @xmath257 .",
    "we examine the asymptotic behavior of the polar code construction process in the next subsection .      as the construction size in the polar transform",
    "is increased , gradually a `` polarization '' phenomenon takes holds . all channels @xmath344 created by the polar transform , except for a vanishing fraction , approach extreme limits ( becoming near perfect or useless ) with increasing @xmath20 .",
    "one form of expressing polarization more precisely is the following .",
    "for any fixed @xmath345 , the channels created by the polar transform satisfy @xmath346 and @xmath347 as @xmath20 increases .",
    "( @xmath348 denotes the number of elements in set @xmath163 . )",
    "a proof of this result using martingale theory can be found in @xcite ; for a recent simpler proof that avoids martingales , we refer to @xcite .    as an immediate corollary to",
    ", we obtain , establishing the main goal of this analysis .",
    "while this result is very reassuring , there are many remaining technical details that have to be taken care of before we can claim to have a practical coding scheme .",
    "first of all , we should not forget that the validity of rests on the assumption that there are no errors in the sc decoding chain .",
    "we may argue that we can satisfy assumption to any desired degree of accuracy by using convolutional codes of sufficiently long constraint lengths .",
    "luckily , it turns out using such convolutional codes is unnecessary to have a practically viable scheme .",
    "the polarization phenomenon creates sufficient number of sufficiently good channels fast enough that the validity of can be maintained without any help from an outer convolutional code and sequential decoder .",
    "the details of this last step of polar code construction are as follows .",
    "let us reconsider the scheme in fig .",
    "[ fig : derivedchannel2 ] . at the outset ,",
    "the plan was to operate the @xmath142th convolutional encoder ce@xmath194 at a rate just below the symmetric cutoff rate @xmath349 .",
    "however , in light of the polarization phenomenon , we know that almost all the cutoff rates @xmath349 are clustered around 0 or 1 for @xmath20 large .",
    "this suggests rounding off the rates of all convolutional encoders to 0 or 1 , effectively eliminating the outer code .",
    "such a revised scheme is highly attractive due to its simplicity , but dispensing with the outer code exposes the system to unmitigated error propagation in the sc chain .    to analyze the performance of the scheme that has no protection by an outer code ,",
    "let @xmath163 denote the set of indices @xmath350 of input variables @xmath249 that will carry data at rate 1 .",
    "we call @xmath163 the set of `` active '' variables",
    ". let @xmath351 denote the complement of @xmath163 , and call this set the set of `` frozen '' variables .",
    "we will denote the active variables collectively by @xmath352 and the frozen ones by @xmath353 , each vector regarded as a subvector of @xmath230 .",
    "let @xmath165 denote the size of @xmath163 .",
    "encoding is done by setting @xmath354 and @xmath355 where @xmath356 is user data equally likely to take any value in @xmath357 and @xmath358 is a fixed pattern .",
    "the user data may change from block to the next , but the frozen pattern remains the same and is known to the decoder .",
    "this system carries @xmath165 bits of data in each block of @xmath20 channel uses , for a transmission rate of @xmath359 .    at the receiver ,",
    "we suppose that there is an sc decoder that computes its decision @xmath231 by calculating the likelihood ratio @xmath360 and setting @xmath361 successively , starting with @xmath362 .",
    "since the variables @xmath363 are fixed to @xmath364 , this decoding rule can be implemented at the decoder .",
    "the probability of frame error for this system is given by @xmath365    for a symmetric channel , the error probability @xmath366 does not depend on @xmath364 @xcite",
    ". a convenient choice in that case may be to set @xmath364 to the zero vector . for a general channel , we consider the average of the error probability over all possible choices for @xmath364 , namely , @xmath367 it is shown in @xcite that @xmath368 where @xmath369 is the bhattacharyya parameter of @xmath247 .",
    "the bound suggests that @xmath163 should be chosen so as to minimize the upper bound on @xmath370 .",
    "the performance attainable by such a design rule can be calculated directly from the following polarization result from @xcite .    for any fixed @xmath371 ,",
    "the bhattacharyya parameters created by the polar transform satisfy @xmath372    in particular , if we fix @xmath373 , the fraction of channels @xmath247 in the population @xmath344 satisfying @xmath374 approaches the symmetric capacity @xmath257 as @xmath20 becomes large .",
    "so , if we fix the rate @xmath375 , then for all @xmath20 sufficiently large , we will be able to select an active set @xmath376 of size @xmath377 such that holds for each @xmath378 . with @xmath376",
    "selected in this way , the probability of error is bounded as @xmath379 this establishes the feasibility of constructing polar codes that operate at any rate @xmath380 with a probability of error going to 0 exponentially as @xmath381 .",
    "this brings us to the end of our discussion of polar codes .",
    "we will close by mentioning two important facts that relate to complexity .",
    "the sc decoding algorithm for polar codes can be implemented in complexity @xmath283 @xcite .",
    "the contruction complexity of polar codes , namely , the selection of an optimal ( subject to numerical precision ) set @xmath163 of active channels ( either by computing the bhattacharyya parameters @xmath382 or some related set of quality parameters ) can be done in complexity @xmath383 , as shown in the sequence of papers @xcite , @xcite , and @xcite .",
    "in this paper we gave an account of polar coding from a historical perspective , tracing the original line of thinking that led to its development .",
    "the key motivation for polar coding was to boost the cutoff rate of sequential decoding .",
    "the schemes of pinsker and massey suggested a two - step mechanism : first build a vector channel from independent copies of a given channel ; next , split the vector channel into correlated subchannels . with proper combining and splitting , it is possible to obtain an improvement in the aggregate cutoff rate .",
    "polar coding is a recursive implementation of this basic idea .",
    "the recursiveness renders polar codes both analytically tractable , which leads to an explicit code construction algorithm , and also makes it possible to encode and decode these codes at low - complexity .",
    "although polar coding was originally intended to be the inner code in a concatenated scheme , it turned out ( to our pleasant surprise ) that the inner code was so reliable that there was no need for the outer convolutional code or the sequential decoder .",
    "however , to further improve polar coding , one could still consider adding an outer coding scheme , as originally planned .",
    "this appendix provides a proof of the pairwise error bound .",
    "the proof below is standard textbook material .",
    "it is reproduced here for completeness and to demonstrate the simplicity of the basic idea underlying the cutoff rate .",
    "let @xmath23 be a specific code , let @xmath384 be the rate of the code . fix two distinct messages @xmath77 , @xmath78 , and define @xmath79 as in .",
    "then , @xmath385\\\\ & \\stackrel{(3)}{= } \\prod_{n=1}^n z_{m , m'}(n),\\end{aligned}\\ ] ] where the inequality ( 1 ) follows by the simple observation that @xmath386 equality ( 2 ) follows by the memoryless channel assumption , and ( 3 ) by the definition @xmath387 at this point the analysis becomes dependent on the specific code structure . to continue",
    ", we consider the ensemble average of the pairwise error probability , @xmath388 , defined by .",
    "@xmath389^n \\end{aligned}\\ ] ] where the overbar denotes averaging with respect to the code ensemble , the equality ( 1 ) is due to the independence of the random variables @xmath390 , and ( 2 ) is by the fact that the same set of random variables are identically - distributed . finally , we note that , @xmath391\\,\\bigg[\\overline{\\sqrt{w(y|x_1(m'))}}\\bigg ]   \\nonumber\\\\   & = \\sum_{y}\\bigg [ \\sum_{x } q(x)\\sqrt{w(y|x)}\\bigg]^2.\\end{aligned}\\ ] ] in view of the definition , this proves that @xmath392 , for any @xmath77 .",
    "the author would like to thank senior editor m. pursley and anonymous reviewers for extremely helpful suggestions to improve the paper .",
    "this work was supported by the fp7 network of excellence newcom # under grant agreement  318306 .",
    "p.  elias , `` coding for noisy channels , '' in _ ire convention record , pt . 4 _ , pp .  3746 , march 1955 .",
    "( reprinted in : d. slepian ( ed . ) , `` key papers in the development of information theory '' , ieee press , 1974 , pp .",
    "102 - 111 ) ."
  ],
  "abstract_text": [
    "<S> polar coding was conceived originally as a technique for boosting the cutoff rate of sequential decoding , along the lines of earlier schemes of pinsker and massey . the key idea in boosting the cutoff rate </S>",
    "<S> is to take a vector channel ( either given or artificially built ) , split it into multiple correlated subchannels , and employ a separate sequential decoder on each subchannel . </S>",
    "<S> polar coding was originally designed to be a low - complexity recursive channel combining and splitting operation of this type , to be used as the inner code in a concatenated scheme with outer convolutional coding and sequential decoding . however , the polar inner code turned out to be so effective that no outer code was actually needed to achieve the original aim of boosting the cutoff rate to channel capacity . </S>",
    "<S> this paper explains the cutoff rate considerations that motivated the development of polar coding .    </S>",
    "<S> channel polarization , polar codes , cutoff rate , sequential decoding . </S>"
  ]
}