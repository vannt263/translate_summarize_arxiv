{
  "article_text": [
    "ensemble classifiers have become very popular for classification and regression tasks .",
    "they offer the potential advantages of robustness via bootstrapping , feature prioritization , and good out - of - sample performance characteristics ( @xcite ) .",
    "however , they suffer from lack of interpretability , and oftentimes features are reported as `` word bags '' - e.g. by feature importance ( @xcite ) .",
    "generalized linear models , a venerable statistical toolchest , offer good predictive performance across a range of prediction and classification tasks , well - understood theory ( advantages and modes of failure ) and implementation considerations and , most importantly , excellent interpretability . until recently ,",
    "there has been little progress in bringing together ensemble learning and glms , but some recent work in this area ( e.g. @xcite ) has resulted in publicly - available implementations of glm ensembles .",
    "nevertheless , the resulting ensembles of glms remain difficult to interpret .",
    "meantime , human understanding of models is pivotal in some fields - e.g. in translational medicine , where machine learning influences drug positioning , clinical trial design , treatment guidelines , and other outcomes that directly influence people s lives .",
    "improvement in performance without interpretability can be useless in such context . to improve performance of maximum - likelihood models , @xcite proposed to learn multiple centroids of parameter space .",
    "built bottom - up , such ensembles would have only a limited number of models , keeping the ensemble interpretable . in this paper",
    ", we work from a model ensemble down .",
    "we demonstrate that minimum description length - motivated ensemble summarization can dramatically improve interpretability of model ensembles with little if any loss of predictive power , and outline some key directions in which these approaches may evolve in the future .",
    "the problem of ml estimators being drawn to dominant solutions is well understood .",
    "likewise , an ensemble consensus can be drawn to the ( possibly infeasible ) mode , despite potentially capturing the relevant variability in the parameter space .",
    "relevant observations on this issue are made in @xcite , who have proposed centroid estimators as a solution .",
    "working from the ensemble backwards , we use this idea as the inspiration to compress ensembles to their constituent centroids .    in order to frame the problem of ensemble summarization as that of mdl - driven compression , we consider which requirements",
    "a glm ensemble must meet in order to be compressible , and what is required of the compression technique .",
    "to wit , these are :    1 .",
    "representation * the ensemble members needs to be representible as vectors in a cartesian space * the ensemble needs to be `` large enough '' with respect to its feature set * the ensemble needs to have a very non - uniform distribution over features 2 .",
    "compression : the compression technique needs to * capture ensemble as a number of overlapping or non - overlapping clusters * provide a loss measure * formulate a `` description length '' measure    it is easy to see that glm ensembles can satisfy the representation requirement very directly .",
    "it is sufficient to view ensembles of _ regularized _ glms as low - dimensional vectors in a high - dimensional space .",
    "the dimensionality of the overall space will somewhat depend on the cardinality of the ensemble , on the strictness of regularization used , on the amount of signal in the data , on the order of interactions investigated , and on other factors influencing the search space of the optimizer generating the ensemble of glms .",
    "coordinates in this space can be alternately captured by ( ideally standardized ) coefficients or , perhaps more meaningfully , by some function of statistical significance of the terms . in this work",
    ", we apply the latter .    for representation , we choose a basis vector of subnetworks . in order to identify this basis vector ,",
    "we have experimented with gaussian mixture decomposition ( gmm ) ( finding clusters of vectors in model space ) and hierarchical clustering . for performance reasons , we present results using the latter technique , despite its shortcomings : instability and inability to fit overlapping clusters ( this may lead to overfitting ) .",
    "nevertheless , in practice we find that this latter technique performs reasonably well .",
    "optionally , to summarize the clusters , centroids can be fit",
    "_ de novo _ once these groups of models are identified , or medoids can be used , obviating the need for further fitting .",
    "here we use the first method , refitting centroids from training data on just the terms occurring in the models in a given cluster .",
    "lastly , bayesian information criterion ( _ bic _ ) satisfies the representation scoring requirement .",
    "the likelihood term serves as the loss function and the penalty term captures `` description length '' ( @xcite ) .",
    "the bic - regularized glm ensembles were fit for binary - outcome datasets used in @xcite and using the software from the same paper ( number of bags = = 100 , other settings left at defaults ) .",
    "the result of this step was an ensemble @xmath0 which , ignoring the outcome variable and the intercepts , could be captured via a non - sparse matrix as follows : @xmath1 where @xmath2 , the ensemble dimensionality , refers to the number of fitted models and @xmath3 to the number of terms found in the whole fitted ensemble .",
    "importantly , @xmath2 is always an arbitrary parameter - the fact that partially motivated our study .    for each dataset , the fitted ensembles",
    "were then compressed using the following procedure .",
    "first of all , for each ensemble we created the significance matrix s : @xmath4    where @xmath5 , and the p - value is determined from the fit of the linear model @xmath6 of the glm ensemble ( s is the heatmap in figure [ figure1 ] ) .",
    "each row of @xmath7 projects every model @xmath6 into a multivariate cartesian space where each axis corresponds to model terms observed in the whole ensemble and each coordinate corresponds to log - scaled term significance .",
    "log - scaling is introduced to induce separability of models that share terms in the presence or absence of collinear covariates that would be expected to influence , but not obviate , shared terms significance .",
    "note that , in this representation , if @xmath8 is missing , @xmath9 .    as an example of what the @xmath0 and @xmath7 matrices would look like after this step , for the * compressed * ensemble in figure [ figure2 ] , @xmath0 = @xmath10 and @xmath7 = @xmath11    of course , in practice it is the full ensemble we are interested in representing in this manner en route to compressing it , so @xmath0 and @xmath7 will have their dimensions , @xmath2 and @xmath3 , of @xmath12 for a computational - biological application using a regularized glm ensemble construction approach .",
    "having constructed the matrix s for each ensemble in this manner , we clustered its rows ( the models coefficients ) using ward s clustering criterion ( @xcite ) and euclidean distance metric .",
    "we then traversed the resulting dendrogram from left to right , using each cutpoint to perform model assignment to clusters implied by the leaves of the resulting dendrogram ( figure [ figure1 ] ) .",
    "we captured the assignment of models to k clusters produced by each step in this traversal as a column vector @xmath13 expressed as a categorical variable with @xmath14 unique values , such that @xmath15 .",
    "then , for each model ensemble term @xmath16 , we extracted the vector @xmath17 - a column slice through the matrix s for term @xmath16 .",
    "we next defined a linear model @xmath18 with @xmath19 parameters .",
    "the success of ensemble compression via the @xmath14 clusters implied by @xmath13 could then be assessed by defining @xmath20@xmath21 being the log - likelihood of the model .",
    "this evaluation of cost across clustering levels to find maximal average likelihood compression , @xmath22 , could be viewed as using the bayes factor as the loss function for optimization , and the process of describing the model ensemble by centroids ( or medoids ) of the clusters of models described by @xmath13 could be described as an mdl - driven compression of the ensemble , using the bic - penalized likelihood as the measure of optimal compression .",
    "it is worth adding that the aforementioned gmm approach for cluster membership assignment , which can also be driven by bic(@xcite , @xcite ) , does not imply a specific nested membership of models in clusters , but generally results in cluster membership strongly correlated with that identified via the hierarchical model clustering technique .",
    "while the gmm approach is more robust ( e.g. , it s not path - dependent , and does nt require specification of a linkage function ) , it scales worse when number of terms in the ensemble is large .",
    "using the datasets described above , we performed 3-fold cross - validation repeated three times , and for each training fold and repeat fitted medoid- and centroid - compressed model ensembles .",
    "for each held - out fold , we then computed out - of - sample auc for every method ( table [ table : table1 ] ) .",
    "additionally , we performed paired one - tailed t - tests comparing medoid and centroid compression strategies to uncompressed ensembles across folds .",
    "all aucs arising from repeats and folds were averaged prior to t - test to avoid pseudo - replication issues .",
    "while medoids performed slightly worse , on average , than uncompressed ensembles , centroids performance was degraded only in the statistically `` suggestive '' sense ( 0.05 < p < 0.1 ) .",
    "note that in our experience using this technique on real - world datasets , larger datasets and continuous outcomes result in even smaller , if any , degradation of performance , performance being especially degraded for logistic regression .",
    "in other words , we believe that these results , reported for binary outcomes , are essentially a lower bound .",
    "maximum - likelihood methods identify the most likely fit in the parameter space . however ,",
    "unless the most likely fit is vastly superior to all others and is sharply defined , a rare scenario in practice , the total probability of this fit in the infinite model ensemble may be very small .",
    "for that reason , model ensembles are thought to be superior to individual models .",
    "ensemble construction gains power by sampling multiple models from the parameter space but , by so doing , loses interpretability by introducing alternative parameter configurations and values .",
    "we build on the understanding that model parameter space centroids should be sufficient to capture predictive power of large ensembles ( @xcite ) , while observing that such centroids exhibit better interpretability by having fewer parameters among the alternative models .    working top - down , we demonstrate and validate on several datasets a novel approach to summarizing ensembles of glms .",
    "our data shows this approach can result in models nearly identical to full ensembles in performance and vastly superior in interpretability , owing to dramatically reduced ensemble sizes ( figure [ figure2 ] ) .",
    "in addition , since this approach can operate on any models that can be shoehorned into a cartesian space , it shows promise for compressing and thus summarizing ensembles of other types - for instance , causal model ensembles with individual models represented as orderings ( @xcite ) .",
    "we posit that our approach can alter applicability of ensemble methods in general , making their use possible for a wide range of applications where the bottleneck has been the interpretability of results .",
    "future directions of research may include multivariate classification methods beyond gmm and hierarchical clustering , as well as extension of this methodology beyond ensembles of glms to other types of predictive ensembles .",
    "presented at nips 2016 workshop on interpretable machine learning in complex systems",
    ".aucs by method and dataset .",
    "last column shows p - value of one - tailed paired t - test vs randomglm ( @xmath23 : the compressed ensemble performs as well as the full ensemble ) .",
    "values are averaged over 3 folds and 3 repeats . because of multiple repeats , standard errors are not shown . [",
    "cols=\">,<,<,<,<,<,<\",options=\"header \" , ]",
    "the authors would like to acknowledge leon furchtgott and fred gruber for their invaluable feedback on the manuscript , and fred gruber for his help with latex .",
    "song , l. , langfelder , p. , horvath , s. ( 2013 ) random generalized linear model : a highly accurate and interpretable ensemble predictor . _ bmc bioinformatics _",
    "14:5 pmid : 23323760 doi : 10.1186/1471 - 2105 - 14 - 5 teyssier , m. and koller , d ( 2005 ) .",
    "ordering - based search : a simple and effective algorithm for learning bayesian networks . _ proceedings of the twenty - first conference on uncertainty in ai ( uai ) _ ( pp .",
    "584 - 590 ) .",
    "chitraa , v. , thanamani a.s .",
    "( 2013 ) , review of ensemble classification . _",
    "international journal of computer science and mobile computing a monthly journal of computer science and information technology _ , vol .",
    "2 , issue . 5 , may 2013 , pg.307  312 liaw , a. , wiener , m. ( 2002 ) classification and regression by randomforest r news 1822 hansen , m.h . , and yu , b. model selection and the principle of minimum description length .",
    "_ journal of the american statistical association _ 96.454 ( 2001 ) : 746 - 774 .",
    "carvalho , l.e . ,",
    "lawrence , c.e .",
    ", centroid estimators for inference in high - dimensional discrete spaces _ pnas _ , 105 : 32093214 ( 2008 ) ward , j. h. , jr .",
    "hierarchical grouping to optimize an objective function , _ journal of the american statistical association _ , 58 , 236244 ( 1963 ) fraley , c. , raftery , a.e . , murphy , t.b . , and scrucca , l. ( 2012 ) mclust version 4 for r : normal mixture modeling for model - based clustering , classification , and density estimation technical report no .",
    "597 , department of statistics , university of washington fraley , c. and raftery , a.e .",
    "( 2002 ) model - based clustering , discriminant analysis and density estimation journal of the american statistical association 97:611 - 631"
  ],
  "abstract_text": [
    "<S> over the years , ensemble methods have become a staple of machine learning . </S>",
    "<S> similarly , generalized linear models ( _ glms _ ) have become very popular for a wide variety of statistical inference tasks . </S>",
    "<S> the former have been shown to enhance out - of - sample predictive power and the latter possess easy interpretability . </S>",
    "<S> recently , ensembles of glms have been proposed as a possibility . on the downside </S>",
    "<S> , this approach loses the interpretability that glms possess . </S>",
    "<S> we show that minimum description length ( _ mdl_)-motivated compression of the inferred ensembles can be used to recover interpretability without much , if any , downside to performance and illustrate on a number of standard classification data sets . </S>"
  ]
}