{
  "article_text": [
    "there are a large class of data analysis problems in astrophysics involving a model of a source that has a spectrum that varies spatially on the sky . in the case of the x - ray emission from clusters of galaxies , where the individual photon energies are recorded ,",
    "the problem is especially prominent .",
    "in fact , much of the recent work in the x - ray analysis of clusters of galaxies has had the aim of obtaining a complete description of the density and temperature structure of the intracluster medium .",
    "the x - ray emission from clusters of galaxies has been recently shown , using observations with the chandra and xmm - newton telescopes , to have a very complex distribution .",
    "in general , the surface brightness lacks circular symmetry .",
    "for example , cold fronts , or cluster gas that `` sloshing '' relative to the dark matter gravitational potential , is a significant perturbation from a spherically - symmetric gas distribution @xcite .",
    "`` radio bubbles '' , which are displacements of cluster gas by relativistic radio - emitting plasma , have complex morphologies and vary from cluster to cluster .",
    "@xcite have identified a `` swirl '' in the temperature structure of the perseus cluster possibly due to rotation of the cluster gas .",
    "there may also be ripples or waves in the cluster gas @xcite . neither are clusters isothermal : @xcite and @xcite have questioned the assumption that the cluster gas can even approximately be described by a single phase as a function of radius .",
    "in fact , they find evidence that the temperature can vary at a single radius by a larger factor than the average temperature drops radially across the entire cluster .",
    "recently , the gross spectrum from clusters of galaxies was shown to be inconsistent with simple models of cooling flows ( e.g.   * ? ? ?",
    "the confusion over what this result means clearly precludes the use of any simple theoretical template in the construction of a model of the intracluster medium .",
    "our goal in this paper is therefore to provide a flexible method that is capable of reproducing all of the complexities we have mentioned above .",
    "this method must necessarily utilize both the spectral and spatial information in a dataset and should still be effective with datasets that have relatively low numbers of x - ray counts .    in the literature is a large number of diverse techniques for analyzing x - ray data from galaxy clusters .",
    "the methods range from fitting isothermal spectra to square grids of photons across the detector plane ( e.g.   * ? ? ?",
    "* ) to imaging deprojection techniques through `` onion - peeling '' methods ( e.g.   * ? ? ?",
    "fitting adaptively - binned data to isothermal models has also proved useful @xcite and the onion - peeling technique has been extended to spectroscopic deprojection @xcite .",
    "other methods include image smoothing , gross spectral fitting , and inversion of spectral hardness ratios @xcite .",
    "we break with these previous methods in two important ways .",
    "first , we use the spectrum and image of a source on an equal footing .",
    "traditional analyses extract a spectrum from a piece of the detector plane and then use the integrated number of photons as a proxy for the true source image . however , this process is only approximately correct in the presence of broad point - spread functions that are energy - dependent .",
    "even the concept that there is an `` image '' and a `` spectrum '' for a spatially - resolved source is misleading , since in reality there is a spatially - varying emissivity at a given energy .",
    "we propose to use a monte carlo calculation to forward - fold a complex model to predict detector positions and ccd energies for each photon .",
    "secondly , we construct a model that is flexible enough to allow for extremely complex gas temperature and density distributions in clusters .",
    "for example , a common approach is to assume a single temperature for a given set of photons and fit for its value .",
    "in contrast , our procedure is to allow for a multi - temperature distribution , and then allow the method to sharpen the temperature distribution if it is close to a single temperature .",
    "this approach is motivated by the observations outlined in the first paragraph that clusters are complex .",
    "we achieve this complexity by modeling the emissivity from clusters as using conglomerations of emissivity particles , smoothed for ease of interpretation .",
    "the construction of underlying model distributions from linear combinations of simple basis functions is well - founded in modern bayesian image processing : indeed , the `` massive inference '' method of @xcite has been found to be a powerful technique for the reconstruction of one dimensional spectra and two dimensional images .",
    "this work may be seen as a practical application of the basic premises of massive inference to the problem of astronomical x - ray data analysis , with some of the refinements compromised in the name of pragmatism .",
    "skilling et al .",
    "coined the term `` atom '' for the indivisible unit of image flux ; to avoid confusion with the atomic physics terminology of x - ray spectroscopy we translate atom into particle and draw an analogy with numerical simulations of galaxy clusters .",
    "where in the first instance our goal is to model x - ray data , it will become clear that this `` smoothed particle inference '' ( spi ) methodology offers great opportunities for further analysis and physical interpretation .",
    "this work also borrows heavily from some independent pioneering work by @xcite .",
    "there a monte carlo approach was used to invert x - ray data by interating a non - parametric model , while matching the simulation with the x - ray events .    in this paper",
    ", we describe the spi method in considerable detail , highlighting the two novel aspects introduced above .",
    "we then apply the method on realistic simulated datasets for both epic and rgs , and show that it works .",
    "we then demonstrate some of the benefits of the methodology with regard to extracting and displaying the information in the data .    in two companion papers ,",
    "we apply the method to xmm - newton reflection grating spectrometer ( rgs ) observations of the perseus and virgo clusters @xcite and xmm - newton european photon imaging camera ( epic ) observations of abell 1689 and rxj0658 - 55 @xcite .",
    "this section is organized as follows .",
    "we first describe our choice of model , a set of spatially gaussian x - ray plasma emitters of an assigned spectral type .",
    "then , we discuss x - ray monte carlo techniques to propagate this model through the instrument response . we discuss the ensuing two - sample likelihood statistic used to assess the statistical significance of the model .",
    "finally , we describe the use of markov chain techniques to iterate the model and explore the parameter space .",
    "we have argued in the previous section that a `` non - parametric '' method , or rather , a method with thousands of parameters , is required to describe the current x - ray data . in particular , a more unified modeling approach , embracing all the complex aspects of clusters that have recently been discovered ( cold fronts , temperature radial gradients , local non - isothermality , lack of circular symmetry in the temperature distribution ) is now required .    consider a `` blob '' , or `` smoothed particle '' , of plasma , described by a spatial position , a gaussian width , a single temperature , a set of elemental abundances , and an overall luminosity . a model of a cluster of galaxies",
    "may be constructed from a set of such particles .",
    "there are a number of advantages to doing this .",
    "the model has enough freedom to reproduce all of the salient features of cluster gas distributions , and can form complex density and temperature structures .",
    "particles with different temperatures can occupy the same position and therefore multi - phase temperature distributions can be constructed .",
    "there is also no global spherical symmetry that is imposed on the particle configuration , although they are themselves spherically symmetric .",
    "another powerful aspect of using smoothed particles is that they can be mapped from one space to another in a straightforward way : for example a collection of smoothed particles representing x - ray luminosity may be manipulated in order to derive the corresponding sz decrement map .",
    "this approach is somewhat similar to the methods used in smoothed particle hydrodynamics ( sph ) , except here the method decide the properties of the smoothed particle , rather than a set of hydrodynamical equations .",
    "for this reason , we refer to the method as `` smoothed particle inference '' .    assuming an x - ray source is optically - thin , we can write the differential luminosity per volume due to a set of smoothed gaussian particles as    @xmath0    where @xmath1 is the luminosity per particle in ergs per second , @xmath2 is the temperature of the @xmath3th particle in kev , @xmath4 is the angular diameter distance in @xmath5 , @xmath6 is the angular size in radians , @xmath7 is the location of the particle in three dimensions expressed in @xmath5 .",
    "we achieve the differential luminosity per solid angle by integrating along the line of sight , @xmath8 ,    @xmath9    this can be rewritten as    @xmath10    where @xmath7 is now the two dimensional spatial position of each smoothed particle . the differential flux per unit solid angle per unit energy , @xmath11 , in units of photons per second per @xmath12 which is the actual observable ,",
    "is then obtained by first calculating the differential flux per unit energy , @xmath13 ,    @xmath14    where @xmath15 is the luminosity distance in @xmath5 , @xmath16 is the cooling function in units of @xmath17 , @xmath18 is the differential emission measure , and @xmath19 is the line ( or continuum power ) which is predicted uniquely given a set of elemental abundances and a temperature in units of @xmath20 .",
    "note that the cooling function is the energy averaged integral of @xmath19 .",
    "the differential flux per solid angle per energy is then obtained by combining equation 3 and equation 4 ,    @xmath21    where @xmath22 are angular coordinates on the sky , @xmath23 is now in angular units , @xmath15 is the luminosity distance in @xmath5 , @xmath24 is the temperature and metallicity - dependent cooling function , and @xmath19 is the energy - dependent line ( or continuum ) power function , which describes the probability of a photon having a given energy  @xmath25 as a function of temperature .",
    "we have purposely chosen to use luminosity as the basis for our smoothed particles , rather than , say , gas density .",
    "if we had parameterized in terms of the gas density then predicting the luminosity would require , essentially , a summation over all particles to reconstruct the density field and then the squaring of the result . in this case",
    ", the complete luminosity field would have to be reconstructed following the alteration of a single particle .",
    "later , we will demonstrate that using a luminosity basis will simplify the calculation considerably but not quantitatively change the results .",
    "we are also anticipating that in using this method in combination with sz observations , it will still be preferable to use the luminosity as the basis rather than a compton - y parameter basis since the calculation of the x - ray instrument response is likely to be much more involved .",
    "we convert the luminosity at each energy and spatial position into a quantitative prediction for probability of the numbers of photons at a given detector position and energy via a monte carlo calculation .",
    "more explicitly , our goal is the calculation of the probability of detection ,  @xmath26 , given the instrument response function ,  @xmath27 , and the photon source model ,  @xmath28 .",
    "this is expressed as ,    @xmath29    where @xmath30 are the detector position vector , @xmath31 is the measured detector energy , @xmath32 is the sky position vector , and @xmath25 is the intrinsic energy of the photon .",
    "this is a three - dimensional integral : it is most efficiently calculated using a monte carlo approach . in @xcite , we describe the calculation of this integral for the reflection grating spectrometers ( rgs ) on xmm - newton . we have also written a companion monte carlo for the european photon imaging cameras ( epic ) on xmm - newton @xcite .",
    "we do not describe these calculations in detail here , but instead briefly outline their use in the context of smoothed particle simulation .    in the monte carlo calculation",
    ", photons are simulated sequentially via conditional probability functions .",
    "mirror , grating , and detector responses are included in the calculation .",
    "for the rgs simulation , photons are manipulated using a set of dispersion and cross - dispersion response functions , ccd pulse - height redistribution functions , and exposure maps .",
    "for the epic monte carlo , photons are constructed using the point - spread function , vignetting , ccd pulse - height redistribution functions , and the exposure map . in general , the functions are both energy and off - axis angle dependent and the calculation of these functions involves a sequence of convolutions .",
    "the response functions are calculated on coarse grids , and then interpolated using a monte carlo method @xcite .",
    "photons are removed in proportion to the relative response probability , in order to properly calculate the effective area , which is also off - axis angle and energy dependent .    for the purpose of simulating the x - ray emission from a set of spi particles , a set of simulated photons",
    "is assigned to a particular particle .",
    "the particle parameters are the temperature ( @xmath33 ) , set of abundances , position ( @xmath34 , @xmath35 ) , gaussian width ( @xmath23 ) , and possibly the redshift .",
    "for each photon , the wavelength is first chosen from the probability distribution described by the emissivity function corresponding to the temperature and set of elemental abundances of that particle .",
    "its position is then perturbed from the nominal center of that particle ( @xmath34 , @xmath35 ) by a random gaussian step having width equal to @xmath23 .    the photon flux per particle , @xmath36 ,",
    "is calculated by computing ,    @xmath37    where @xmath38 is the peak effective area in units of @xmath12 , @xmath39 is the exposure time in units of @xmath40 , and @xmath41 is the number of photons simulated before removing any photons in the monte carlo calculation .",
    "the luminosity , @xmath1 per particle can be calculated by computing ,    @xmath42    where    @xmath43    and @xmath44 is the emissivity per energy interval in units of @xmath45 .",
    "we quantify the goodness of fit of a given set of spi particles via the likelihood function of their parameters . in general ,",
    "the likelihood is a function dependent on both predicted and actual data , whose form embodies the analyst s understanding of the measurement errors , and is understood as the probability of getting the data given the parameters of the model .",
    "ordinarily , the predicted data are calculated to arbitrary precision within the context of the model under investigation : this is not the case here .",
    "the monte carlo simulation procedure outlined above , while needed to deal correctly with the energy - dependent and spatially varying telescope response , provides predicted data that are themselves uncertain , being the outcome of a poisson process .",
    "we can view the output of the telescope , and the output of the simulation , as two independent data streams emanating from the same underlying physical process ( emission from the cluster followed by detection at the telescope ) .",
    "the likelihood function then quantifies the misfit between the two datasets .",
    "note that given infinite simulation time , predicted data can be computed with infinite precision within the context of the limitations of the instrument model ; clearly any likelihood statistic used should revert to the more familiar form in this limit .",
    "moreover , we note that all likelihood functions are necessarily approximations to an unknown function ( that indeed , should itself form part of the data model )",
    ". we should be satisfied with a likelihood that captures the essence of the misfit , and satisfies any limits such as that above .",
    "statistics for the two poisson sample situation have not been well - developed in the statistical literature ; possibly it has been deemed preferable to find alternative ways of computing predicted data exactly rather than press on with effectively two sources of measurement error .",
    "where attempts have been made , they have made use of gaussian approximations valid only in the limit of large numbers . in the appendix we give a discussion of a number of routes to a likelihood function applicable to the low counting statistics of x - ray datasets , and show how they recover the correct form in the limit of infinite simulation time .",
    "we give below the form of the likelihood  @xmath46 that we use in practice , and note that in its normalized form ( @xmath47 ) that @xmath48 is distributed as @xmath49 with the number of degrees of freedom equal to the number of bins . while not required by the analysis , we find that this provides a valuable check on the pure goodness of fit of the models , giving confidence in the results .    in general , we find that the results are relatively insensitive to the exact choice of likelihood statistic and the differences are largest when the data is extremely sparse and there are few photons : in subsection  [ sect : adbin ] below we outline our approach for avoiding this eventuality .",
    "we experimented with the likelihood functions in the next section and found the reconstruction to produce similar results .",
    "the basic datum is a number of photons in a three dimensional bin @xmath50 .",
    "the bin size may be set by physical limits such as the extent of the ccd pixel , or be imposed given some understanding of , for example , the spectral resolution of the instrument .",
    "the simulated photons are then binned on the same grid .",
    "assume that we have @xmath51 photons in the observed dataset , and have simulated @xmath52 photons from our model : in the @xmath53 bin we have @xmath54 simulated photons , and @xmath55 observed photons . since the photons falling into a set of bins will follow a multinomial distribution , we obtain for the data photons the likelihood function    @xmath56    where @xmath57 is the probability of getting a photon in that bin .",
    "the multinomial distribution is the poisson distribution but with total number of photons fixed . treating the simulated photons in the same way",
    ", we get    @xmath58    in doing so we have suggested that the simulated photons be considered to be from a separate , independent , experiment with its own likelihood .",
    "the joint likelihood is then just the product of the two experiments likelihoods . in the appendix",
    "we discuss two routes to dealing with the value of the parameters @xmath57 : one can see that a reasonable guess is to estimate @xmath57 from the data and set @xmath59 :    @xmath60    we use this form when comparing exploring the parameter space as outlined in the next section .",
    "it is possible to re - normalize these likelihood functions , such that they will approximately follow the @xmath49 distribution  @xcite .",
    "this is done by dividing by the same expression with the probability estimates replaced by the actual number of counts .",
    "the result for the likelihood given above is written as    @xmath61    by construction , @xmath62 is approximately distributed as @xmath49 with the number of degrees of freedom equal to the number of bins .",
    "we use @xmath47 for checking that a reasonable fit is being obtained by the method ( see also section  [ sect : evidence ] ) , which will be near the number of degrees of freedom for a good fit . in the appendix",
    "we demonstrate how this statistic approaches the two sample @xmath49 statistic that we used in the large @xmath52 and @xmath51 limit .",
    "@xmath46 is used for the parameter iteration , since it analgous to the one - sample likelihood .",
    "the statistics that we have discussed in the previous section require the events to be binned on a arbitrary three - dimensional grid . for a non - dispersive spectrometer , the natural minimal grid for this would be to design bins to reasonably sample the point - spread function for the two spatial dimensions and the energy resolution kernel for the spectral dimension . for a dispersive spectrometer",
    ", the angular dispersion resolution kernel would define the bins in the dispersion direction . in either case , however , it is found that , for virtually all observations of clusters of galaxies , the numbers of minimal bins are very large indeed , and that the photons fill the three dimensional grids extremely sparsely .",
    "in fact , most bins typically contain either 0 or 1 photons .",
    "simple grouping of these minimal bins will result in severe loss of spectral and/or spatial resolution .",
    "therefore , a more efficient binning approach deserves some consideration .",
    "although the statistics discussed in the previous section are capable of handling low numbers of photon counts , we are pushed toward fuller bins to reduce computation time .",
    "additionally , we show below that the fluctuation induced on the statistic by the monte carlo calculation depends on the number of bins , such that a smaller number of bins is preferred .    for these reasons",
    ", we have developed an algorithm to adaptively bin the data in the three dimensions similar to other adaptive binning methods .",
    "to do this , we first consider the entire three - dimensional data space .",
    "we create the binning grid by bisecting the data space in one dimension at a time .",
    "the dimension to bisect is chosen from the relative length of the bin in each dimension .",
    "the longer dimension is always bisected .",
    "if the relative sizes are the same the dimension is selected randomly .",
    "each bin is bisected for as long as any bin has more than @xmath41 counts ; most of the bins will contain on average @xmath63 photon counts by the time the binning has been completed .",
    "typically , @xmath41 is chosen to be 10 or 20 , but this varies between applications .",
    "for instance in a dataset with extremely low flux the bins can be made with fewer photons .",
    "there is also the possibility of weighting the bin size in any dimension with some factor @xmath51 .",
    "for example , if spectral accuracy is preferred over spatial accuracy the bins in the spectral dimension can be chosen to be on average @xmath51 times smaller than the spatial bins .",
    "after the bins have been determined for the data photons , they remain fixed .",
    "we then use a binary tree algorithm to rapidly place new simulated photons into the appropriate bin . the two - sample statistic given in the previous section",
    "can then be calculated as a product over all the bins .",
    "after model generation and likelihood calculation , the third major component of the method is the sequence of model parameter iteration .",
    "we have found several complications to the usual methods of parameter iteration that are particularly problematic with the high - dimensional parameter model that we are considering in this paper . in the following subsections",
    "we discuss several aspects of this part of the analysis chain .",
    "we explore the parameter space of the smoothed particle model with the markov chain monte carlo ( mcmc ) method , the use of which has recently become widespread in the field of bayesian data analysis ( see * ? ? ?",
    "* for an excellent introduction ) .",
    "mcmc is a technique for sampling the probability distribution of a model s parameters ; in this case , the dimensionality of the parameter space is extremely high , prohibiting brute force grid calculations and condemning gradient - based optimization to failure by being trapped in local likelihood maxima .",
    "mcmc is a very efficient and powerful way of exploring such high - dimensional spaces , returning a list of sample cluster models all of whom are consistent with the data , and whose number density in parameter space follows the probability density .",
    "possession of the entire parameter probability distribution , in the form of a list of samples , opens up possibilities not available to other methods .",
    "the distributions of particle parameters , for example , can be used to estimate the average distributions of cluster quantities ( such as temperatures and densities ) .",
    "these averages come with well - defined uncertainties trivially derived from the samples : one may probe the range of distributions that are consistent with the data .",
    "mcmc is now the standard tool for cosmological data analysis ( see e.g.   * ? ? ?",
    "* ; * ? ? ?",
    "mcmc has been applied to simple cluster modeling using sz and lensing data @xcite , and x - ray and sz data @xcite . to date , the use of mcmc in astrophysics has been restricted to relatively modest numbers of parameters . however , it is when using large numbers of parameters to model complex datasets that the full power of the technique is realised .    as usual ,",
    "the target probability distribution is the posterior density  @xmath64 , given by bayes theorem as    @xmath65    here , @xmath66 is the likelihood function introduced above , with the notation emphasizing its dependence on the observed data @xmath26 given the parameter set @xmath67 .",
    "@xmath68 is the prior probability distribution of the model parameters : in this work we use uninformative priors on all parameters , restricting ourselves to uniform distributions over broad finite ranges .",
    "we give here the briefest of introductions to the metropolis - hastings algorithm @xcite that we use to sample the posterior density , and then provide details of the implementation in the following subsections . following the evaluation of the likelihood associated with an initial point in parameter space , @xmath69 , a new point , @xmath70 is drawn from a proposal distribution .",
    "this proposal distribution is required to be symmetric about the current position to ensure successful sampling of the target distribution .",
    "if the likelihood of the new point is higher than the likelihood of @xmath71 then the new point is accepted .",
    "otherwise , the point is accepted only if a uniform random number between 0 and 1 is less than the ratio of likelihoods , @xmath72 .",
    "this is known as the metropolis - hastings acceptance criterion .",
    "if the new point is not accepted , then the old point is repeated in the chain of parameters .",
    "while this basic algorithm is very simple , the efficiency of the exploration of the parameter space depends strongly on the form of the proposal distribution , which we discuss in the following two subsections",
    ".      as we mentioned in the previous section , the method of picking the next point in the markov chain can either be by taking a step from some fixed distribution or it can be modified dynamically . in dealing with the large number of parameters in the smoothed particle fits , we have found that an adaptive proposal distribution is necessary .",
    "initially , the proposal distribution is chosen to be a gaussian of fixed width , @xmath23 .",
    "after the markov chain has taken a few steps , we calculate the standard deviation , @xmath73 , of the new estimate of the target posterior distribution .",
    "we then use @xmath73 as an estimate for the new step in the proposal distribution .",
    "large excursions in parameter space , however , can sometimes bias this standard deviation estimation . therefore , we have constructed a different standard deviation estimator with some built - in `` memory loss '' , such that earlier steps are weighted less strongly .",
    "the proposal step width , @xmath74 is calculated according to    @xmath75    where @xmath76 is the parameter value at the @xmath53 iteration and the sum is over all the iterations .",
    "@xmath77 is a constant taken to be @xmath78 that causes the earlier iterations to be de - weighted .",
    "if @xmath77 were zero , the formula would result to the normal standard deviation formula ; an infinite value for epsilon implies the normal non - adaptive gaussian proposal distribution . with finite @xmath77",
    ", the individual steps no longer exactly satisfy detailed balance , as the proposal distribution is slightly different between the current and trial sample positions . if epsilon is set sufficiently small ( although not so small as to restrict the movement of the chain ) then a large number of past sample positions are used to determine the proposal distribution widths , and consequently the differences between any successive proposal distributions are very small .",
    "the parameter update method then asymptotically becomes a true markov chain . in practice",
    "these differences are completely negligible compared to the large deviations from unity of the likelihood ratios .",
    "quantitatively , we found that this holds whenever @xmath79 . in practice",
    "@xmath80 is much larger than one and we take @xmath77 to be 0.01 .",
    "the proposal step is then taken to be @xmath81 , where @xmath82 is the number of dimensions .",
    "this is the optimal step size for uncorrelated gaussian distributions , such that the acceptance rate times the rms step size is maximized for this value . in the large @xmath82 limit , a rejection rate around 77@xmath83",
    "is expected @xcite .",
    "a successful chain will , after the burn - in period , explore the posterior distribution with this rejection rate , and the proposal distribution in any given parameter direction will asymptote to the gaussian approximation to the marginalized posterior .      in a typical spi emission model",
    "there are two kinds of parameters .",
    "the first kind are local parameters , describing the individual properties of the particles .",
    "these are the temperature , position , size , and elemental abundances for each particle .",
    "the second kind are global parameters for describing properties of the entire data set .",
    "these parameters are the background parameters and the absorption column density .",
    "the order in which these parameters are varied was found to have a strong influence on the speed of the chain convergence : the variation sequence has been under considerable study in the development of this method .",
    "firstly , it was found that the local parameters for a particular particle should be varied separately while holding the parameters of the other particles fixed and the global parameters fixed .",
    "we believe that this is a valid technique since any particle can be interchanged with any other particle and therefore we can exploit this symmetry by not considering the full degeneracy of all of the particles parameters .",
    "we do , however , vary the set of particle parameters  position , temperature , elemental abundances , and gaussian size simultaneously since these parameters may be highly correlated .",
    "varying each set of particle parameters separately speeds up the calculation by several orders of magnitude , as only the photons associated with that particle need to be resimulated .",
    "secondly , we found that after changing the global parameters the particle parameters needed to be varied for at least one iteration in order for the system to re - equilibrate and remain in the desired high probability region .",
    "this is necessary since there may be some correlations between the positions of the particles , for example , and global background parameters .",
    "therefore , the particle parameters must be allowed to move somewhat before rejecting a new set of global parameters . without this consideration , we found some global parameters became pinned to sub - optimal values .",
    "thirdly , we found it necessary to regenerate a completely new set of photons for a particular set of parameters frequently .",
    "this is necessary due to the monte carlo nature of the calculation : there is statistical noise in the likelihood value itself .",
    "although we believe the two sample likelihood properly deals with this statistical noise in modifying the likelihood surface , it does not deal with the parameters drifting to a particular non - optimal value only because it received a favorable set of photons .",
    "frequent regeneration of the photons simply cancels this effect and restores the mobility of the markov chain .",
    "these three considerations lead us to the following sequence of parameter iteration .",
    "consider that we have @xmath51 particles with local parameters , @xmath84 , for the @xmath3th particle and the @xmath85th particle parameter .",
    "assume also that we have a set of global parameters @xmath86 for the @xmath87th parameter .",
    "the following sequence of parameter variation is then used for each iteration :    1 .",
    "vary values of @xmath84 to new values @xmath88 , holding the parameters of all other particles fixed , and compute the likelihood .",
    "take the new values if the statistic conforms to the acceptance criterion .",
    "repeat this step for each of the particles .",
    "2 .   regenerate all photons for @xmath89 and compute the likelihood .",
    "3 .   vary the values of @xmath86 to a new point @xmath90 .",
    "hold the values of @xmath84 fixed at the previous values .",
    "4 .   vary the values of @xmath84 to new values @xmath91 , holding the parameters of all other particles fixed , and compute the likelihood .",
    "take the new values , if the statistic conforms to the acceptance criterion .",
    "repeat this step for each of the particles .",
    "5 .   regenerate all of the photons @xmath90 and possibly the new values of @xmath84 .",
    "compare the value of the likelihood at this point with that obtained in the second step .",
    "take the new value of @xmath92 and values of @xmath91 , if it conforms to the acceptance criterion .",
    "otherwise , return to the previous values of both @xmath86 and @xmath88 at the second step .",
    "the absence of a metropolis comparison in step 3 indicates a departure from the standard markov chain : in fact , this step can be thought of as the splitting off of a new chain which explores the posterior conditional on the value of @xmath90 . after some exploration of the local parameter space , this chain",
    "is then compared with its parent ( left behind at the known sample position @xmath86 ) and the fittest ( in the metroplis - hasting sense ) survives in step 4 . strictly speaking this process does not conserve detailed balance :",
    "however , we have checked that the departure of this algorithm from the proper format does not affect the convergence on the target posterior distribution , at least in simple cases as we demonstrate in appendix 2 .",
    "since we have used a monte carlo calculation to calculate the likelihood statistic for the highly - complex smoothed - particle model , there is unavoidable statistical noise in the likelihood function .",
    "the same set of parameters will produce a slightly different value of the likelihood statistic every time they are simulated .",
    "this monte carlo noise can be reduced by increasing the `` over - simulate factor , '' the ratio of the simulated photons compared to the number of photons in the data set . in practice",
    ", a value of the over - simulate factor of 10 or so can be easily accommodated , but much larger values become tedious when working on a single workstation .",
    "the statistical fluctuation in the two sample normalized statistic scales empirically as    @xmath93    such that the fluctuation can be reduced by both increasing the simulation factor and reducing the number of bins .",
    "normally , this ratio is significantly larger than one , and therefore this is likely to significantly broaden the posterior distribution of any parameters .",
    "however , this merely makes the method conservative : the posterior distributions we derive contain the smaller true distribution .",
    "this is illustrated in figure  [ fig : oversim ] .",
    "initially , we experimented with methods that may change the number of particles dynamically , during the evolution of the markov chain .",
    "however , we found that any advantage that this method may have had , in reducing the number of smoothed particles and their associated parameters , was outweighed by the complications added to the fitting procedure .",
    "in particular , it was difficult to determine what to do with the parameters of the particles that were newly created or recently destroyed when the particle number changed .",
    "the use of adaptive steps within this scheme was also problematic .",
    "therefore , we fix the number of particles and do not vary this during the parameter iteration , instead opting for a more approximate approach to determining a suitable number of particles .",
    "we calculate ( albeit approximately ) the `` evidence '' of the smoothed particle model as a function of particle number .",
    "the evidence is the integral of the likelihood function @xmath66 times the prior distribution , @xmath94 over all parameters(@xmath76 ) :    @xmath95    the evidence works by penalizing overly complex models ( such as those with too many particles ) wwith a lower evidence ( see e.g.   * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* for introductions to the use of the evidence ) .",
    "we estimate the evidence by the method of thermodynamic integration @xcite , raising the likelihood to some power during the initial stages of the mcmc process and increasing this power steadily from 0 to 1 . in our studies , the evidence increased dramatically with increasing particle number for less than @xmath96 smoothed particles but was then relatively flat for larger particle numbers .",
    "this number of course varies from dataset to dataset : we use the smallest number of particles possible ( indicated by the turnover in evidence , where the improvement in goodness - of - fit starts to become insignificant ) in the interests of both the economy of hypotheses and also of saving computation time .",
    "the method requires one to choose both the over - simulate factor and the number of smoothed particles .",
    "the former is chosen to be as large as possible without the method taking too long , and the latter is chosen to be as large as possible given the criteria of the previous paragraph .",
    "having finite values for the over - simulate factor limits the precision of the method , but this was found to be unimportant for many applications . limiting the number of particles via consideration of the evidence prevents fitting of the noise with an overly complex model .",
    "there is an impressive variety of techniques that can be applied to the results of the smoothed particle inference .",
    "the output of the mcmc process consists of a set of parameters , both global and local , for the given number of smoothed particles , for each iteration .",
    "thus , the usual marginalized posterior probability distributions can be generated by histogramming the history of parameter values ( see fig .  [",
    "fig : histopars ] ) .",
    "the interpretation of the smoothed particle parameters is not always straightforward , but a wealth of information resides in the mcmc samples .",
    "spi is essentially a method for translating noisy x - ray data into uncertain emission components : the markov chain process retains the correlations between components such that the manipulation of the samples can reveal much about the gross and detailed properties of the emitter .",
    "we discuss just some of the possible inference techniques below .",
    "as always , caution has to be used in only using samples taken after the chain has reached the stationary distribution .    to demonstrate these inference techniques we have simulated a cluster model with two spatially separated isothermal components to which we apply our method .",
    "this choice of simplistic simulation reflects both our desire to demonstrate the algorithm on a relatively easily - interpreted problem , but also the difficulty of generating realistically complex mock datasets . to make a simple demonstration of the features of the method we have chosen to simulate a double cluster with two isothermal components",
    "this will test the ability of the method to estimate single temperatures as well as to separate different components in a dataset , both spatially and spectrally .",
    "the method is applied to real datasets in two companion papers , @xcite and @xcite .    in this work",
    "we perform separate inferences for the xmm - newton epic and xmm - newton rgs detectors . in the epic run",
    "we expect to be able to resolve the spatial structure of the clusters in detail due to the high spatial resolution ( psf fwhm @xmath97 ) of the instrument datasets .",
    "for the rgs run the spatial resolution will be low in the dispersion direction of the spectrometer ; however we expect to resolve spectral features in finer detail .          where @xmath99 is the source surface brightness at angular radius @xmath100 and @xmath101 is the angular core radius .",
    "the beta models are separated diagonally by @xmath102 .",
    "each of the two clusters is modeled spectrally with a mekal @xcite model for the thermal plasma with wisconsin @xcite photo - electric absorption for the galactic column .",
    "the cluster plasma temperatures were chosen separately for the epic and rgs simulations due to the different energy sensitivity of the two instruments . in the rgs",
    "run the two beta models have isothermal temperatures of and respectively whereas in the epic run these temperatures are at and .",
    "metal abundances are set at 0.5 solar , the redshift is fixed at @xmath103 and both clusters are absorbed by a corresponding hydrogen column of @xmath104 .",
    "the beta model parameters for the clusters were set to @xmath105 , @xmath106 and @xmath107 , @xmath108 for the lower left and the upper right cluster respectively ( see fig .  [",
    "fig : lummed ] ) .",
    "we display the posterior distributions of the temperature and absorption parameters in figure  [ fig : globallocal ] .",
    "in all three simulations the posterior distribution is identical .",
    "this demonstrates that the various iteration scheme approximately obey detailed balance .",
    "this suggests that it is not critically important what scheme we use and works in the limit that the posterior distribution becomes stationary .",
    "these simulations also demonstrate that the posterior temperature distribution is smaller when the model is constrained to only consist of two temperatures than the other simulations with smoothed particles because of the simplicity of the model .",
    "the posterior distribution is smallest at low temperatures because of the nature of x - ray spectra .",
    "we acknowledge many helpful discussions on the suitability of smooth particles for modelling cluster data with steve allen , ted baltz , roger blandford , sarah church , steve kahn , greg madejski , frits paerels , vahe petrosian , yoel raphaeli , and keith thompson .",
    "likewise we thank garrett jernigan , mike hobson , sarah bridle and john skilling for similar input on the statistical details of the method .",
    "financial support for ka is provided by the gran gustavsson foundation for research in natural sciences and medicine .",
    "this work was supported in part by a grant from nasa for scientific and calibration support of the rgs at stanford .",
    "this work was also supported in part by the u.s .",
    "department of energy under contract number de - ac02 - 76sf00515 ."
  ],
  "abstract_text": [
    "<S> we propose an ambitious new method that models the intracluster medium in clusters of galaxies as a set of x - ray emitting smoothed particles of plasma . </S>",
    "<S> each smoothed particle is described by a handful of parameters including temperature , location , size , and elemental abundances . </S>",
    "<S> hundreds to thousands of these particles are used to construct a model cluster of galaxies , with the appropriate complexity estimated from the data quality . </S>",
    "<S> this model is then compared iteratively with x - ray data in the form of adaptively binned photon lists via a two - sample likelihood statistic and iterated via markov chain monte carlo . </S>",
    "<S> the complex cluster model is propagated through the x - ray instrument response using direct sampling monte carlo methods . using this approach </S>",
    "<S> the method can reproduce many of the features observed in the x - ray emission in a less assumption - dependent way than traditional analyses , and it allows for a more detailed characterization of the density , temperature , and metal abundance structure of clusters . </S>",
    "<S> multi - instrument x - ray analyses and simultaneous x - ray , sunyaev - zeldovich ( sz ) , and lensing analyses are a straight - forward extension of this methodology . </S>",
    "<S> significant challenges still exist in understanding the degeneracy in these models and the statistical noise induced by the complexity of the models . </S>"
  ]
}