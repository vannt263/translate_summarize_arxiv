{
  "article_text": [
    "high - dimensional data have generated a great challenge in different fields of statistics , computer science , and machine learning . in order to consider cases where the number of covariates is larger than the sample size ,",
    "new methodologies , applicable for the model under some structural constraints , have been developed .",
    "for instance , there have been substantial works under the sparsity assumption including sparse linear regression , sparse covariance matrices estimation or sparse inverse covariance matrices estimation ( see e.g. * ? ? ? * ; * ? ? ? * ; * ? ? ? * ; * ? ? ? * ; * ? ? ? * ) . in this paper , we focus on the problem of _ low rank matrix recovery and uncertainty quantification_.    there have been quite a few work on estimating low rank matrices in the matrix regression setting ( also named the trace regression setting , the matrix compressed sensing setting , or the quantum tomography setting when the parameter is a density matrix ) . many authors ( e.g. * ? ? ? * ; * ? ? ? * ; * ? ? ? * ; * ? ? ? * ) considered the exact recovery of a low - rank matrix based on a subset of uniformly sampled entries .",
    "also @xcite considered matrix recovery based on a small number of noisy linear measurements in the framework of restricted isometry property ( rip ) .",
    "@xcite proved non - asymptotic bounds on the frobenius risk , and investigated matrix completion under a row / column weighted random sampling .",
    "@xcite proposed a nuclear norm minimisation method and derived a general sharp oracle inequality under the condition of restricted isometry property .",
    "very recently , @xcite considered a rank - one projection model and used constrained nuclear norm minimization method to estimate the matrix .",
    "@xcite considered a specific quantum tomography problem where the parameter is a density matrix ( for more details , plasase see subsection [ ss : q ] ) , and @xcite proved that the quantum tomography design setting satisfies the rip .",
    "in addition , @xcite proposed an estimator based on an entropy minimisation for solving a quantum tomography problem .",
    "@xcite adapt the iterative hard thresholding method ( first introduced in the sparse linear regression setting , see e.g.   * ? ? ?",
    "* ; * ? ? ?",
    "* ) to the problem of low rank matrix recovery in the case where the noise is non - stochastic and of small @xmath0 norm .",
    "this procedure has the advantage of being very computationally efficient . in the same vein but applied to the more challenging stochastically noisy setting , @xcite introduced a soft thresholding technique that provides efficient result in this setting in frobenius norm , see also  @xcite for other thresholding methods in related settings that provide results also in frobenius norm .",
    "another important problem is on understanding the uncertainty associated to these statistical methodologies , by e.g.  characterizing the limiting distribution of the efficient estimators . yet",
    "results in this area for high dimensional models are still scarce , available mainly for the sparse ( generalised ) linear regression models @xcite . in the papers",
    "@xcite , the authors focus first on constructing an estimator for the sparse parameter that has good properties in @xmath1 risk , and they use then this result to exhibit the limiting distribution of their estimator . knowing this limiting distribution immediately enables the construction of tests and confidence sets for low dimensional subsets of parameters .    a similar achievement ,",
    "i.e.  the construction of an estimator that has an explicit limiting distribution , does not exist in the low rank matrix recovery setting . to the best of our knowledge ,",
    "moreover , all the theoretical results from the above papers on the estimation of the parameter in the noisy setting are derived in frobenius risk  neither in the entrywise matrix @xmath2 risk , nor in the operator norm ( i.e.  the largest singular value ) .    in our paper",
    ", we consider the problem of constructing an estimator for low - rank matrix in a stochastically noisy high - dimensional setting , under the assumption that a rip - type isometry condition is satisfied ( see assumption  [ ass : designbis ] ) .",
    "we provide ( in  theorem [ th : mainthm2 ] ) error bounds for our estimator in all @xmath3 schatten norms for @xmath4 .",
    "we prove in particular that this estimator has optimal frobenius and operator norm risk by proving that this estimator has optimal @xmath2 risk performance uniformly over any change of orthonormal basis .",
    "in addition , a slight modification of our estimator has an explicit gaussian limiting distribution with bounded bias in operator norm ( see  theorem [ thm : asymnorm2 ] ) ; and in the particular case when the design consists in uncorrelated gaussian entries , we prove that the bias in @xmath1 entry wise norm is bounded as well , which is immediately useful for testing hypotheses and constructing confidence intervals for each parameter of interest , similar to the ideas in  @xcite . moreover",
    "our estimator is computationally efficient with an explicit algorithm .",
    "the proposed algorithm is inspired by the iterative hard thresholding , that refines its estimation of the matrix by iteratively estimating the low rank sub - space where the matrix s image is defined .",
    "it requires only @xmath5 iteration steps to converge approximately , and the computational complexity of the method is of order @xmath6 where @xmath7 is the dimension of the matrix , and @xmath8 is the sample size .    in the experiment section",
    "we first provide some simulations where we illustrate the efficiency of our method and explain how it can be used to create a confidence interval for the entries of the low rank matrix .",
    "we then apply our method to a specific _ quantum tomography application _ , namely multiple ion tomography  ( see , e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , where the assumptions required by our method are naturally satisfied  ( see e.g. * ? ? ?",
    "* ; * ? ? ?",
    "finally we compare our method with other existing estimation methods for the trace regression setting  @xcite using the gradient descent implementation of  @xcite and also regularized maximum likelihood based procedures  @xcite .    as a complement in the supplementary material ,",
    "we adapt our method to the setting of sparse linear regressionm and provide an estimator that has an explicit limiting distribution ( recovering the results of @xcite ) .",
    "for @xmath9 , @xmath10 and @xmath11 , we write @xmath12 for the hard thresholded version of @xmath13 at level @xmath14 , i.e.  for the vector @xmath15 such that @xmath16 for @xmath17 . for @xmath18 and @xmath19 , we write @xmath20 for the standard @xmath0 norm of @xmath13 , and @xmath21 for the standard @xmath1 norm of @xmath13 .    for a @xmath22 complex matrix @xmath23 , we write @xmath24 as the conjugate transpose of  @xmath23 .",
    "we write @xmath25 for the trace of @xmath23 , and @xmath26 for the matrix whose diagonal entries are the same as @xmath23 while its non - diagonal entries are all zeros .",
    "we write the entry - wise matrix norm of @xmath23 as @xmath27 , and its squared frobenius norm as @xmath28 .",
    "we write also the operator norm of @xmath23 as @xmath29 , where the @xmath30 are the singular values of @xmath23 , and the schatten @xmath3 norm of @xmath23 for @xmath31 as @xmath32 - and note that @xmath33 .    for @xmath9 , we write @xmath34 for the hard thresholded version of @xmath23 at level @xmath14 for each entry , i.e.  for the matrix @xmath35 such that @xmath36 for @xmath37 .",
    "let @xmath38 .",
    "let @xmath39 be the set of @xmath40 matrices , and @xmath41 be the set of @xmath40 complex matrices of rank less than or equal to @xmath42 .",
    "let us also write @xmath43 for the set of orthonormal matrices in @xmath44 .    for @xmath45",
    ", we consider the matrix regression problem where for any @xmath46 , @xmath47 where @xmath48 is an i.i.d .",
    "vector of gaussian white noise , i.e.  @xmath49 ( but our results hold in the same way for any sub - gaussian independent noise @xmath50 : see remark  [ rem : noise ] ) , and @xmath51 but @xmath52 .",
    "let us write @xmath53 for the linear operator going from @xmath44 to @xmath54 , and such that for any @xmath55 , @xmath56 the model can be rewritten as @xmath57 where @xmath58 .",
    "this matrix regression model is directly related to the quantum tomography model ( in which case the design @xmath53 is often chosen to be the random pauli design  @xcite , but it is also related to e.g.  matrix completion  @xcite .",
    "we state the following assumption on the design operator @xmath53 .    [",
    "ass : designbis ] let @xmath59 . for any @xmath60",
    ", it holds that @xmath61 where @xmath62 .",
    "[ rem : gaussian ] the above assumption is very related to the restricted isometry property . typically ,",
    "for uncorrelated gaussian design with mean @xmath63 and variance @xmath64 entries , it will hold with probability larger than @xmath65 for @xmath66 where @xmath67 is a universal constant . for the pauli design used in quantum tomography , it will hold with probability larger than @xmath65 for @xmath68 where @xmath67 is a universal constant  @xcite - see subsection  [ ss : q ] for a description of description of a quantum tomography setting in which the pauli matrices represent measurements .",
    "as a generalization of sparsity constraints in linear regression models , we impose a rank @xmath69 constraint on a matrix @xmath70 .",
    "that is , we require the rows ( or columns ) of @xmath71 lie in some @xmath42-dimensional subspace of @xmath72 .",
    "this type of rank constraint arises in numerous applications such as quantum tomography , matrix completion , and matrix compressed sensing ( see e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "our method considers the parameters @xmath73 .",
    "the parameter @xmath74 is a small probability that will calibrate the precision of the estimate : the theoretical results that we will prove later for this estimate will hold with probability @xmath65 , and the smaller @xmath74 , the larger the constant in the bound ( see theorem  [ th : mainthm2 ] ) .",
    "the parameter @xmath75 is an upper bound on two times the actual low rank of the parameter @xmath71 .",
    "it does not need to be tight , and the final results will not depend on it as long @xmath76 ( see assumption  [ ass : designbis ] and theorem  [ th : mainthm2 ] ) .",
    "the parameter @xmath77 is an upper bound on the frobenius norm of the parameter @xmath71 .",
    "it again does not need to be tight , but constants in the proof will scale with it .",
    "we set the initial values for the estimator @xmath78 and the threshold @xmath79 such that@xmath80    we update the thresholds @xmath81 where @xmath82 , @xmath83 is an universal constant ( see lemma [ helo ] ) and @xmath84 .    set now recursively , for @xmath85 , @xmath86 , @xmath87 and let @xmath88 be two orthonormal matrices that diagonalise @xmath89 .",
    "then we set @xmath90 this procedure provides a sequence of estimates , and as we will prove in the next subsection , this sequence is with high probability close to the true @xmath71 as soon as @xmath91 is of order @xmath92 ( see theorems  [ th : mainthm2 ] and  [ thm : asymnorm2 ] ) .",
    "[ rem : para ] note that although we describe this method using many quantities , in fact while implementing our method we only need to set up four quantities : @xmath93 and the stopping time @xmath91 .",
    "we describe in equation   how to implement a good stopping rule , and in subsection  [ ss : disc ] how to choose the three first parameters .",
    "in particular , @xmath79 can be chosen in a data driven way .",
    "this method is related to iterative hard thresholding ( iht ) , a method that has been developed for the sparse regression setting ( see e.g. * ? ? ?",
    "* ; * ? ? ?",
    "it is less straightforward to see this in this setting , as in the sparse regression setting where we adapt also our method in subsection  [ ss : sr ] , and for a more comprehensive discussion of the relation between our method and iht , see the remark  [ rem : iht ] .",
    "note that iht algorithms have been proved to work in settings where the noise is small and non - stochastic ( see e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , but to the best of our knowledge , there are no results on iht in a stochastically noisy setting .",
    "[ [ main - result - for - our - thresholded - estimator ] ] main result for our thresholded estimator + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we now provide a theorem that guarantees that the estimate @xmath94 after @xmath95 iterations has at most rank @xmath42 , and its entry - wise @xmath2 risk and frobenius risk are bounded with the optimal rates ( see the first point in subsection 3.3 ) .",
    "[ th : mainthm2 ] assume that assumption  [ ass : designbis ] is satisfied and that @xmath96 . let @xmath97 .",
    "we have that for a constant @xmath98 it holds that with probability larger than @xmath65 and for any @xmath99 @xmath100 and also that @xmath101 and also that for any @xmath102 @xmath103    the above theorem proves among other things that our estimate attains the minimax optimal schatten @xmath3 risk , which other estimates in the literature also attain for e.g.  @xmath104 .",
    "the first interesting property is that our proposed estimator has an explicit algorithmic form and is very computationally efficient .",
    "another interesting additional property is that it is also minimax - optimal in operator norm ( or entry - wise matrix @xmath1 risk ) , and that the entry - wise error is not more than @xmath105 with high probability _ for any orthonormal change of basis of the matrix @xmath71_. this is a strong result since the entry - wise norm is not invariant by orthonormal change of basis while the frobenius norm is .",
    "this result is already useful for measuring the uncertainty of an estimate ( in particular since it does not require the a priori knowledge of the rank of the matrix @xmath71 ) .",
    "[ [ asymptotic - normality - results ] ] asymptotic normality results + + + + + + + + + + + + + + + + + + + + + + + + + + + +    to prove asymptotic normality , we slightly modify the estimator defined in theorem [ th : mainthm2 ] . consider the estimator @xmath94 of theorem  [ th : mainthm2 ] ( with @xmath97 ) and define @xmath106.\\ ] ]    [ thm : asymnorm2 ] set @xmath107 then we have @xmath108 where @xmath109 .",
    "let @xmath97 .",
    "the two following bounds hold for the bias term @xmath110 under two different assumptions .    *",
    "assume that assumption  [ ass : designbis ] is satisfied for some @xmath111 and that @xmath112 .",
    "if the rank of @xmath71 is smaller than @xmath113 and if its frobenius norm is bounded by @xmath77 , there is a constant @xmath98 such that with probability larger than @xmath65 @xmath114 * assume that the elements in the design matrices @xmath115 are i.i.d .",
    "gaussian with mean @xmath63 and variance @xmath64 , and that @xmath116 , we have that @xmath117 note that this implies the previous result .",
    "this theorem , which is in the spirit of the works in the context of sparse linear regression of  @xcite , implies that there exists an estimator of @xmath71 that has a gaussian limiting distribution , and whose rescaled bias @xmath110 with respect to @xmath71 can be bounded @xmath118 in operator norm under our assumption  [ ass : designbis ] and @xmath119 in @xmath1 norm as well in the specific case where the design is gaussian .",
    "[ rem : noise ] theorems  [ th : mainthm2 ] and  [ thm : asymnorm2 ] are proved for a gaussian noise @xmath50 , but these results are easily generalisable to any independent , sub - gaussian noise , with a similar but more technical proof ( based on talagrand s inequality ) .",
    "the results of theorem  [ thm : asymnorm2 ] would however be modified in that the random variable @xmath120 , conditioned on the design @xmath53 , would then not be exactly gaussian , but have a limiting gaussian distribution using the central limit theorem .    [ [ stopping - rule - r ] ] stopping rule @xmath91 + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    theorem  [ th : mainthm2 ] is satisfied after @xmath121 iterations of our thresholding strategy , and so we know what is a theoretical value for @xmath91 so that our strategy works .",
    "however , it is possible to propose a data driven stopping rule that will perform well . for a desired precision @xmath122",
    ", we propose to stop the algorithm as soon ( after having thresholded a last time ) as @xmath123 let us write @xmath124 for the time where the stopping rule stops .",
    "the following result holds for the estimator stopped at this stopping rule .",
    "[ prop : srule ] assume that assumption  [ ass : designbis ] is satisfied and that @xmath125 , i.e.  @xmath126 . let @xmath127 in ( [ eq : sr ] .",
    "the estimator @xmath128 satisfies with probability larger than @xmath65 and for any @xmath99 @xmath129 and also that @xmath130 which implies for any @xmath4 @xmath131 moreover @xmath124 is such that @xmath132    this empirical stopping rule , that does not require the tuning of any additional parameters we introduced the desired precision @xmath133 .",
    "the precision @xmath133 is a very natural quantity to choose for the experimenter ",
    "one can set e.g.  @xmath134 for an @xmath135 optimal solution with respect to the solution outputted by an algorithm that runs for an infinitely long time ] , is guaranteeing minimax optimal results in less than @xmath92 iterations .",
    "note that theorem  [ thm : asymnorm2 ] would also hold using this stopping rule - this can be proved in the same way as theorem  [ prop : srule ] is proved .",
    "[ [ comparison - of - our - results - with - the - literature ] ] comparison of our results with the literature + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    our theorem  [ th : mainthm2 ] gives bounds for our estimators in all schatten @xmath4 norms ( including the operator norm , and therefore uniform entry wise bounds in all rotation basis ) .",
    "a first point is that our results are minimax optimal in both frobenius and operator norm .",
    "the corresponding lower bound in frobenius norm can be found in e.g.   theorem 5 of @xcite ( under an assumption related to our assumption  [ ass : designbis ] ) or  @xcite under an assumption that is the same as ours .",
    "the corresponding lower bound in operator norm can be found in e.g.  @xcite .",
    "in addition to this , the paper  @xcite even contains further lower bounds results proving that the operator norm rate @xmath105 ( and associated schatten @xmath136 norm @xmath137 ) is optimal also in the case of quantum tomography that we use in our experiments later on , i.e.  under the additional assumptions that the parameter is a density matrix and that the design is random pauli . to the best of our knowledge ,",
    "our method is the first iterative method that has such an optimality property in operator norm - for instance , the paper  @xcite provides results for schatten norms with @xmath138 $ ] , but not for other schatten norms .",
    "besides , we proved in theorem  [ thm : asymnorm2 ] a slight modification of our estimator has an explicit gaussian limiting distribution , and this is , again to the best of our knowledge , the first iterative method for low rank matrix recovery that has such a property . on top of that ,",
    "the computational complexity of our algorithm is low as for any procedure based on iterative hard thresholding : see the papers  @xcite .",
    "our assumption  [ ass : designbis ] is a strong rip condition .",
    "but it is in particular satisfied in the interesting application of multiple ion tomography for the natural pauli design as soon as the number of settings is large enough , see subsection  [ ss : q ] .",
    "operator norm bounds are particularly interesting since they provide an entrywise bound up to any change of orthonormal basis . in particular",
    ", they provide a bound on the eigen values - and since these bounds do not depend on the true rank @xmath42 , they can be used to implement conservative confidence sets .",
    "moreover as highlighted in the papers  @xcite , having a bound on the entrywise risk , and then an estimator with explicit limiting distribution , is interesting in that it can be used to construct tests and confidence intervals for subsets of coordinates of the parameter @xmath71 .",
    "we illustrate this point in the simulation section ( see section  [ s : sim ] ) , where a confidence set is constructed using the limiting distribution .",
    "note however that the bound on the bias term @xmath110 in @xmath1 norm in theorem  [ thm : asymnorm2 ] requires the fact that the design is gaussian . on the other hand ,",
    "the bound on the bias term @xmath110 in operator norm in theorem  [ thm : asymnorm2 ] requires only the fact that our assumption  [ ass : designbis ] is satisfied .    [",
    "[ stopping - rule - r-1 ] ] stopping rule @xmath91 + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    our theorems are satisfied after @xmath121 iterations of our thresholding strategy , and so we know what is a theoretical value for @xmath91 so that our strategy works .",
    "we also defined an empirical stopping rule , see   and theorem  [ prop : srule ] .",
    "we use this stopping rule in practice for all our experiments in section  .    [",
    "[ calibration - of - the - parameters - of - the - proposed - method ] ] calibration of the parameters of the proposed method + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    our method is not parameter free - there are three quantities that need to be calibrated .",
    "the two first ones , that we write @xmath139 and @xmath140 , enter in the definition of the thresholds sequence @xmath141 . @xmath139 controls the rate at which we make our threshold decay , and @xmath142 is the quantity toward which it converges when @xmath91 goes to infinity . the last quantity , namely @xmath79 is the initialisation of the threshold sequences .",
    "here are some comments on how to choose these quantities :    * * rate of decay @xmath139 :* in theory the parameter @xmath139 can be taken between @xmath64 and @xmath143 where @xmath75 is an upper bound on the rank of the parameter and @xmath144 is the constant associated to the design such that assumption  [ ass : design ] is satisfied",
    ". it might not be really possible to compute exactly @xmath75 or @xmath144 without more assumptions on the design .",
    "but there is at least one design that is interesting in practice , namely the random pauli design for quantum tomography , that is such that we have an upper bound on @xmath144 for all @xmath75 that is of order @xmath145 with high probability , i.e.  it decays with @xmath8 ( the same holds in gaussian design up to the @xmath146 term ) . in this design",
    "if @xmath8 is large enough , we know that taking @xmath147 will work - we do not want to take @xmath139 too close to @xmath64 since the bound on the performance of the estimator scales with @xmath148 .",
    "* * smallest threshold calibration @xmath140 * the interpretation and theoretical value of @xmath140 is clear : it should be taken to be larger than the @xmath74 quantile of the lhs quantity defined in equation   divided by @xmath149 .",
    "now since we do not have access to this quantile , we calibrate it in the experimental section of this paper as an empirical estimator of the asymptotic quantile described above ( using theorem  [ thm : asymnorm2 ] ) . *",
    "* initialisation threshold @xmath79 :* the constant @xmath79 needs to be taken as an upper bound on the frobenius norm of @xmath71 .",
    "note first that estimating from the data an upper bound on @xmath150 is easy under assumption  [ ass : design ] , i.e.  the quantity for @xmath151 @xmath152 overestimates @xmath150 . in our simulations",
    ", we propose a slightly more refined heuristic upper bound and use the same @xmath79 and @xmath153 in subsection  [ s : sim ] and [ ss : q ] .    to conclude on the practical tuning of the constants",
    ", we would like to emphasize that at least in practical situation , namely quantum tomography , we have enough information about both the design and the noise level to know that the above calibration will be working , provided that the target matrix is indeed low rank .",
    "so although the tuning of parameters is always a tricky issue for any algorithm , in at least this specific application , our algorithm can be used as it is .",
    "in this section we present some experiments , first some simulation for the construction of confidence intervals , and then a formal comparison of our thresholded estimator with other methods on specific quantum tomography problem , namely multiple ion tomography .",
    "we performed experiments for low - rank matrix recovery , with matrix dimension @xmath7 .",
    "we consider a gaussian design where each @xmath154 and are independent .",
    "we also consider a gaussian uncorrelated noise @xmath49 .",
    "we consider a parameter @xmath71 of rank @xmath42 that is stochastically generated in an isotropic way as @xmath155    we implemented our method choosing a data - driven heuristic for the choice of our parameters .",
    "we first set @xmath156 we set for any @xmath86 @xmath157 i.e.  the empirical risk , and @xmath158 where @xmath159 is the @xmath160 quantile of a @xmath161 random variable .",
    "@xmath162 replaces here @xmath140 , and is a heuristic high probability bound on the error for each coordinate .",
    "we set @xmath163 which is by construction higher than the frobenius norm of @xmath71 with high probability , and @xmath164 where we select @xmath147 ( we take @xmath165 so that the decay is not too fast , but also so that @xmath148 is not too large ) .",
    "we also use the heuristic stopping rule described in equation  , i.e.  we iterate until @xmath166 for @xmath167 .",
    "we also construct , using the limiting distribution results provided in  theorem [ thm : asymnorm2 ] , a confidence set for the all the entries of @xmath71 that is such that for any entry @xmath168 , we set the confidence interval @xmath169,\\ ] ] where @xmath170 where @xmath171 .",
    "we provide several results , depending on the values of @xmath172 , averaged over @xmath173 iterations of simulations .",
    "for these simulations , we present three kinds of results :    * a first set of graphs ( figure  [ fig:1 ] ) presents , for different values of @xmath174 , and in function of the sample size @xmath8 , the logarithm of the rescaled frobenius risk of the estimate @xmath175 , i.e. @xmath176 * a second set of graphs ( figure  [ fig:2 ] ) presents , for different values of @xmath174 , and in function of the sample size @xmath8 , the logarithm of the averaged diameter of the confidence intervals @xmath177 , i.e. @xmath178 * a last set of graphs ( figure  [ fig:3 ] ) presents , for different values of @xmath174 , and in function of the sample size @xmath8 , the averaged coverage probability of the confidence intervals @xmath177 , i.e. @xmath179    all these graphs also exhibit @xmath180 confidence intervals ( upper and lower @xmath181 quantile values from 100 iterations ) around their means ( dotted lines in the graphs , the solid line being the mean ) .    , for different values of @xmath174 .",
    "the solid line is the average over @xmath173 iterations , the dotted lines form @xmath180 confidence intervals.,scaledwidth=100.0% ]    , for different values of @xmath174 .",
    "the solid line is the average over @xmath173 iterations , the dotted lines form @xmath180 confidence intervals.,scaledwidth=100.0% ]    , for different values of @xmath174 .",
    "the solid line is the average over @xmath173 iterations , the dotted lines form @xmath180 confidence intervals.,scaledwidth=100.0% ]    , for different values of @xmath174 .",
    "the solid line is the average over @xmath173 iterations , the dotted lines form @xmath180 confidence intervals.,scaledwidth=100.0% ]    , for different values of @xmath174 .",
    "the solid line is the average over @xmath173 iterations , the dotted lines form @xmath180 confidence intervals.,scaledwidth=100.0% ]    , for different values of @xmath174 .",
    "the solid line is the average over @xmath173 iterations , the dotted lines form @xmath180 confidence intervals.,scaledwidth=100.0% ]    , for different values of @xmath174 .",
    "the solid line is the average over @xmath173 iterations , the dotted lines form @xmath180 confidence intervals.,scaledwidth=100.0% ]    , for different values of @xmath174 .",
    "the solid line is the average over @xmath173 iterations , the dotted lines form @xmath180 confidence intervals.,scaledwidth=100.0% ]    , for different values of @xmath174 .",
    "the solid line is the average over @xmath173 iterations the dotted lines form @xmath180 confidence intervals.,scaledwidth=100.0% ]    , for different values of @xmath174 .",
    "the solid line is the average over @xmath173 iterations the dotted lines form @xmath180 confidence intervals.,scaledwidth=100.0% ]    , for different values of @xmath174 .",
    "the solid line is the average over @xmath173 iterations the dotted lines form @xmath180 confidence intervals.,scaledwidth=100.0% ]    , for different values of @xmath174 .",
    "the solid line is the average over @xmath173 iterations the dotted lines form @xmath180 confidence intervals.,scaledwidth=100.0% ]    these figures exhibit different behaviours depending on the difficulty of the problems ( increasing with @xmath3 and more importantly with @xmath42 ) .",
    "the graphs in figure  [ fig:1 ] for @xmath182 ( and @xmath183 ) exhibit first a very fast decay of the risk , until some critical threshold @xmath184 where @xmath185 seems to be between @xmath186 and @xmath187 . at this point",
    ", one can actually observe that the method recovers in most case the true rank @xmath42 of the matrix , whereas it before recovered only a smaller rank approximate of @xmath71with a too small @xmath8 , it could not distinguish all the signal from the noise , and the fact that it gradually does for larger @xmath8 explains the fast decay of the logarithm of the rescaled risk .",
    "after that , the curve has a kink and the decay becomes slower ( the theory predicts that the logarithm of the rescaled risk should decrease with @xmath8 as @xmath188 ) .",
    "after this kink , all the @xmath42  rank directions \" have been identified , and the logarithm of the rescaled risk starts decreasing slower , according to the theoretical rate of @xmath188 .",
    "the graphs in figure  [ fig:1 ] for @xmath189 ( and @xmath183 ) exhibit mainly the first regime , since @xmath42 is larger and the second regimes comes for larger values of @xmath8empirically , we can observe that the method recovers most of the time all @xmath42  directions \" as soon as @xmath190 for @xmath191 , as soon as @xmath192 for @xmath193 .",
    "a parallel evolution can be observed in figure  [ fig:2 ] , for the logarithm of the average length of the confidence intervals .",
    "it is not at all surprising since this length is supposed to reflect the risk .",
    "the averaged coverage of these intervals in figure  [ fig:3 ] is in average larger than @xmath194 in all cases , and in more than @xmath180 of the cases , it is higher than @xmath195 in all cases , which makes our method reliable .",
    "an important application that satisfies our assumptions and to which our method can be applied is _ quantum tomography _",
    ", i.e.  the estimation of quantum states .",
    "we consider the popular problem of multiple ion tomography , i.e.  the problem of estimating the joint quantum state of @xmath196 two - dimensional systems ( qubits ) , as encountered in ion trap quantum tomography , see  @xcite or  @xcite for textbooks on this problem .",
    "such a system s _ quantum state _ can be represented by a positive semi - definite unit trace complex matrix @xmath71 ( a _ density matrix _ ) of dimension @xmath197 .    for each individual qubit , the experimenter can measure one of the three pauli observables described by the @xmath198 by @xmath198 _ pauli matrices _",
    "@xmath199 , where @xmath200,\\ ,      \\sigma^2=\\left [      \\begin{array}{cc }      0 & -i\\\\      i & 0      \\end{array }      \\right],\\ ,      \\sigma^3 = \\left [      \\begin{array}{cc }      1 & 0\\\\      0 & -1      \\end{array }      \\right],\\ ] ] and each of these measurements may yield one of two outcomes , denoted by @xmath201 and @xmath202 respectively .",
    "therefore , a full experiment ( i.e.  an experiment that describes the measurement for each of the @xmath196 qubits ) is then defined by a setting @xmath203 where each @xmath204 for @xmath205 , which specifies which of the @xmath206 pauli observables is measured for each qubit . for each fixed setting @xmath207 , the measurement produces random outcomes @xmath208 , with expected probability @xmath209 where @xmath210 where @xmath211 is the eigen projector of the the @xmath198 by @xmath198 pauli matrix @xmath212 associated to the eigen value @xmath213 ( we remind that the @xmath198 by @xmath198 pauli matrices have eigen values of either @xmath201 or @xmath202 ) .",
    "now set @xmath214,\\ ] ] for the last @xmath215 pauli matrix such that @xmath216 form an orthogonal basis of @xmath217 .",
    "let @xmath218 be the outcome of an experiment given a setting @xmath203 ( where each @xmath219 ) .",
    "write @xmath220 for a setting where a subset @xmath221 of the @xmath196 matrices @xmath212 of this setting have been replaced by @xmath222 , and @xmath223 for the outcome where the same subset @xmath224 of the @xmath196 elements @xmath213 have been replaced by @xmath64 .",
    "since the only eigen value of @xmath222 is @xmath64 , the outcome of the measurement of a qubit by @xmath222 is always @xmath64 .",
    "this implies in particular that the distribution of @xmath223 as described above , is the same as the distribution of the outcome of an experiment when the measurement setting is @xmath220 .",
    "for this reason , measuring according to setting @xmath207 actually gives information about all settings @xmath220 for any subset @xmath224 of @xmath225 .",
    "thus instead of measuring all settings which are tensor products of @xmath226 pauli matrices @xmath227 , it is enough to measure all settings which are tensor products of @xmath226 pauli matrices @xmath228 , as they provide information about corresponding settings that involve @xmath222 . therefore if one measures all @xmath229 settings that correspond to the settings @xmath203 where each @xmath204 , we have observations about all measurements direction , and our measurement setting is complete",
    "now we are interested in also dealing with situations where one does not want to observe all @xmath229 settings , and where one has only a number of settings @xmath230 .",
    "we consider a random measurement setting as in  @xcite , i.e.  each measurement setting @xmath207 is chosen uniformly at random ( each @xmath212 is chosen uniformly at random among @xmath231 ) .",
    "let @xmath232 be the total number of measurement setting chosen in this random way . for each chosen measurement setting @xmath207 , we perform @xmath14 repetition of the experiment and observe @xmath14 independent outcomes .",
    "so for each chosen measurement setting @xmath233 with @xmath234 , we observe @xmath14 independent outcomes @xmath235 which are observations according to setting @xmath233 .",
    "it is often convenient to express the information contained by a measurement @xmath236 in a way that involves tensor products of @xmath226 pauli matrices , rather than their spectral projections , see  @xcite .",
    "the set of matrices that are created by @xmath196 tensor products of @xmath226 pauli matrices @xmath227 is exactly the set of @xmath237 pauli matrices rescaled by @xmath238 ( introduced briefly in remark  [ rem : gaussian ] ) i.e.  the @xmath239 rescaled pauli basis .",
    "indeed let @xmath240 , then one easily verifies that @xmath241 where @xmath242 is the expectation according to the outcome @xmath218 when measurement @xmath207 is chosen .",
    "in this sense , the measurement described by the @xmath239 rescaled pauli matrix @xmath243 can be measured by the parity of the spins @xmath244 that one gets when performing measurement @xmath207 : in fact @xmath245 is a random variable such that its value is @xmath64 with probability @xmath246 and @xmath202 with probability @xmath247 - and its expectation is @xmath248 as noted .",
    "we write @xmath249 for this distribution .",
    "let us go back to our experimental setup .",
    "as remarked before , each of the @xmath232 quantum measurement settings according to setting @xmath233 ( with @xmath234 ) , where @xmath250 and where each @xmath251 for @xmath252 provides us with @xmath7 information in the sense of our trace regression model , i.e.  we observe at each measurement @xmath233 , for each replication @xmath253 and for all @xmath254 @xmath255 in our trace regression model , we can average the observations @xmath256 and we have the the following averaged observations for any @xmath257 and for all @xmath254 @xmath258 where @xmath259 is the @xmath260 repetition ( among @xmath14 iterations ) of the observation and where @xmath261 is the averaged noise and is such that @xmath262 , and such that @xmath261 is sub - gaussian has a sum of bounded random variables and satisfies @xmath263 for any @xmath264 . now in order for our assumption  [ ass : designbis ] to be satisfied for rank @xmath42 matrices for a large enough number of settings @xmath232 , we need to rescale our data .",
    "we set @xmath265 where @xmath266 is the cardinality of @xmath224 and where @xmath267 is the rescaled noise and is such that @xmath268 , and such that @xmath269 is sub - gaussian and satisfies @xmath270 for any @xmath264 .",
    "it is a direct consequence from the results of  @xcite and from our remark  [ rem : gaussian ] that if @xmath271 , then with high probability on the random draws of our settings we have that assumption  [ ass : designbis ] is satisfied for rank @xmath42 matrices .",
    "we can therefore apply our method and other low rank recovery methods such as trace regression methods to our rescaled data @xmath272      we let @xmath273 so that @xmath274 , and let @xmath275 with @xmath276 and consider @xmath277 measurement settings . these settings with small @xmath42 and @xmath278 were chosen since we are more interested in the truncated measurement setting ( such that @xmath230 ) . for the replication , we consider @xmath279 .",
    "using the data ( [ rescaleddata ] ) , we estimate @xmath71 by three methods  our proposed method ( iht ) , the truncated maximum likelihood estimator ( mle ) for the high dimensional multiple ion tomography model as described in  @xcite , and nuclear norm penalization ( nnp ) method computed using a gradient descent method ( e.g. * ? ? ?",
    "we use the same tuning parameters as in ( [ emprisk ] ) , ( [ upsilonr ] ) , ( [ t1 ] ) , and ( [ tr ] ) and also we select @xmath280 and the same stopping rule as we describe in subsection [ s : sim ] . for the stepsize of the gradient descent method used to compute the nnp estimator , we follow the recommendation in subsection 3.1 in @xcite .",
    "norm ( linf ) and shatten @xmath64 norm ( shatten ) of @xmath281 in function of @xmath278 ( and therefore in function og the number of settings @xmath232 ) , for different values of @xmath7 for replication @xmath282 and @xmath283 using the three methods described.,scaledwidth=100.0% ]    figure [ fig:4 ] and [ fig:5 ] present the result when the number @xmath14 of replications is @xmath7 and when the true rank is 1 or 2 respectively , and for four different values of @xmath7 .",
    "we provide average values of squared frobenius norm , operator norm , entrywise @xmath2 norm , and shatten @xmath64 norm of @xmath284 averaged over 100 iterations .",
    "red dots , blue blank triangles , and black asterisks are average value of iht , mle , and nnp , respectively .",
    "intuitively when @xmath278 increases these risks will decrease .",
    "our estimator shows almost comparable result to the mle except in the case where @xmath285 and @xmath286 . in this case ,",
    "iht estimates @xmath71 by @xmath63 a few times and pretty well for most cases so that on average the frobenius norm is still large .",
    "figures [ fig:6 ] and [ fig:7 ] present the result when the replication is @xmath287 and when the true rank is 1 or 2 respectively , and for four different values of @xmath7 .",
    "they show similar patterns as for the cases @xmath282 , but we can see that iht performs well especially for @xmath288 .",
    "an interesting feature that all these pictures illustrate is that iht performs the best relatively to other methods for a large number of replication @xmath14 , and for difficult problems with high @xmath7 and @xmath42 ( see in particular the plots in figure  [ fig:7 ] for large @xmath7 ) .",
    "note that our method is computationally more efficienct than the other two methods in the sense that when @xmath289 , iht takes about 40 seconds while mle ( and even nnp ) takes about 2.5 minutes for one iteration , on a regular laptop , i.e.  it is about four times slower .",
    "norm ( linf ) and shatten @xmath64 norm ( shatten ) of @xmath281 in function of @xmath278 , for different values of @xmath7 for replication @xmath282 and @xmath290 using three methods.,scaledwidth=100.0% ]     norm ( linf ) and shatten @xmath64 norm ( shatten ) of @xmath281 in function of @xmath278 , for different values of @xmath7 for replication @xmath291 and",
    "@xmath283 using three methods.,scaledwidth=100.0% ]     norm ( linf ) and shatten @xmath64 norm ( shatten ) of @xmath281 in function of @xmath278 , for different values of @xmath7 for replication @xmath291 when @xmath290 using three methods.,scaledwidth=100.0% ]",
    "for convenience in writing the proofs , we introduce the following quantities .",
    "we write , for integers @xmath292 , the vectorisation of a @xmath293 matrix ( where @xmath294 ) @xmath23 by stacking the rows of @xmath295 as @xmath296 we write the kronecker product between two matrices @xmath23 and @xmath77 as @xmath297 .",
    "consider the @xmath298 matrix @xmath299 such that @xmath300 for @xmath46 and for @xmath301 where @xmath302 :    @xmath303 = \\left [ \\begin{array}{cccccccc } x^1_{1,1 } & x^1_{1,2 } & \\ldots & x^1_{1,d } & \\ldots & x^1_{d,1 } & \\ldots & x^1_{d , d } \\\\",
    "x^2_{1,1 } & x^2_{1,2 } & \\ldots & x^2_{1,d } & \\ldots & x^2_{d,1 } & \\ldots & x^2_{d , d } \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ x^n_{1,1 } & x^n_{1,2 } & \\ldots & x^n_{1,d } & \\ldots & x^n_{d,1 } & \\ldots & x^n_{d , d } \\end{array } \\right ] .",
    "$ ]    let @xmath304 be the set of vectorization of matrices in @xmath305 , that is , @xmath306 . if @xmath307 , then @xmath308 contains a vector @xmath309 of dimension @xmath310 such that @xmath311 for @xmath312 .    assumption  [ ass : designbis ] can be rewritten as follows in this vectorized new notation .",
    "[ ass : design ] let @xmath59 . for any @xmath60",
    ", it holds that @xmath313 where @xmath62 .",
    "assumption  [ ass : design ] actually implies the following lemma that bounds the scalar products rather than the norms .",
    "[ lem : tra ] if assumption  [ ass : design ] holds , then for any @xmath314 , we have that @xmath315    the proof of this lemma is in subsection  [ proof : tra ] .      let @xmath316 be the set of vectors of @xmath317 and of norm @xmath64 .",
    "[ [ explicit - writing - of - the - quantities ] ] 1 .",
    "explicit writing of the quantities + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    for any matrices @xmath318 of dimension @xmath319 with @xmath320 , we set @xmath321 \\big)v \\\\ & = u^t \\big(\\frac{1}{n}\\sum_{i\\leq n } ( x^i)^t y_i^r \\big)v = : u^t \\hat \\psi^r v\\end{aligned}\\ ] ] where @xmath322 and @xmath323 .",
    "also we set @xmath324 and @xmath325 note that @xmath326 by linearity of the trace .",
    "let @xmath327 , then @xmath328 is a scalar : @xmath329    let @xmath330 be the column vector of dimension @xmath310 such that @xmath331 , that is , @xmath332 for @xmath333 .",
    "note that @xmath334 , and that @xmath335 .",
    "consider the @xmath298 matrix @xmath299 such that @xmath300 for @xmath46 .",
    "consider the column vector @xmath336 where @xmath337 .",
    "then we have @xmath338 where here @xmath339 is the classic vectorial scalar product on @xmath54 . also by definition of @xmath340 and @xmath341",
    "@xmath342    the last equation implies that @xmath343    [ [ bound - on - the - stochastic - term ] ] 2 . bound on the stochastic term + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we first bound the second term in ( [ eq : dec ] ) with the following lemma .    [ helo ]",
    "assume that @xmath344 ( note that @xmath345 for @xmath346 ) .",
    "it holds with probability larger than @xmath65 that @xmath347 where @xmath83 is an universal constant .",
    "its proof is in subsection  [ proof : helo ] .",
    "lemma [ helo ] implies that on an event of probability larger than @xmath65 , we can bound the stochastic term in ( [ eq : dec ] ) @xmath348 let @xmath349 be an event of probability larger than @xmath65 where the above holds .    [",
    "[ bound - on - the - first - term - in - eqdec - provided - that - the - rank - kr - of - psir - is - smaller - than-2k ] ] 3 . bound on the first term in ( [ eq : dec ] )",
    "provided that the rank @xmath350 of @xmath351 is smaller than @xmath352 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    let us assume , only for this paragraph 3 .",
    "of the proof , that the rank @xmath350 of @xmath351 is smaller than @xmath353 . by lemma",
    "[ lem : tra ] , we can apply equation   ( since @xmath354 ) , and combining this with the fact that @xmath355 , we have @xmath356    by combining equation  , and  , and using @xmath357 , we then have in the case that @xmath358 that on @xmath349 @xmath359 since the previous result holds in the worst case of @xmath360 , we directly have on @xmath349 the corresponding entrywise result whenever @xmath361 @xmath362 by definition , we know @xmath363 and @xmath364 , which gives on @xmath349 whenever @xmath361,@xmath365    note also by definition of the thresholding process , the matrix @xmath366 is such that it is diagonal with all diagonal elements smaller than @xmath153 .",
    "let @xmath367 and @xmath368 for @xmath369 . by elementary calculations , we have @xmath370 and so we have that @xmath371    by definition of @xmath94",
    ", we have @xmath372 so this implies that @xmath373 combining this with equation  , we obtain that on @xmath349 and whenever @xmath361 @xmath374    [ [ induction ] ] 4 .",
    "induction + + + + + + + + + + + +    we now stop assuming that the rank @xmath350 of @xmath351 is smaller than @xmath352 , and we consider the general case .    we are going to prove by induction that on @xmath349 , for any integer @xmath86 , we have that ( i ) the rank of @xmath375 is smaller than @xmath42 , and ( ii ) @xmath376 .    for @xmath377 , since @xmath378 , then its rank is @xmath63 and is therefore bounded by @xmath42 and ( i ) is satisfied . moreover , since @xmath379 , then ( ii ) is satisfied as well .",
    "now assume that ( i ) and ( ii ) hold on @xmath349 for a given @xmath91 ( as it holds for @xmath380 not only on @xmath349 but on the entire probability space ) .",
    "by induction assumption ( i ) , we have that on @xmath349 the rank of @xmath375 is smaller than @xmath42 , which implies that the rank of @xmath381 is smaller than @xmath382 .    because we have that @xmath361 , equation   applies and on @xmath349 @xmath383 by definition of @xmath153 and since @xmath384 ( since @xmath353 ) .",
    "moreover , in the same way , we have that on @xmath349 ( see equation   since @xmath361 ) @xmath385    let us now state the following lemma .",
    "[ lem : mati ] let @xmath386 be a matrix ( with @xmath387 ) , with singular values @xmath388 ordered in decreasing order ( all positive ) . for any @xmath389 and for any collection of orthogonal vectors",
    "@xmath390 , we have @xmath391    write @xmath392 for the singular values of @xmath393 ordered in decreasing order and all positive ( and @xmath394 for the diagonalising matrices ) .",
    "let @xmath395 be the matrices that diagonalise @xmath71 and order its singular values in decreasing order on the diagonal and write @xmath396 for its singular values ( all positive ) .",
    "by lemma  [ lem : mati ] , we know that , for any @xmath397 , @xmath398    therefore , on @xmath349 , by equation  , we know that for any @xmath397 @xmath399 so since all @xmath400 that are smaller than @xmath153 are thresholded for constructing @xmath94 ( we remind that the @xmath401 are the diagonal elements of the diagonal matrix @xmath402 , that is thresholded at level @xmath153 in the construction of @xmath94 ) , it means that on @xmath349 , the rank of @xmath94 is smaller than the rank of @xmath71 , i.e.  it is smaller than @xmath42 .",
    "this proves the first part of the induction ( i ) .",
    "now let @xmath403 be the matrices that diagonalise @xmath404 , and let @xmath405 . by , we have that on @xmath349 @xmath406 now since the rank of both @xmath94 and @xmath71 are smaller than @xmath42 on @xmath349 , we know that the rank of @xmath404 , and thus of @xmath407 , is smaller than @xmath352 . therefore , we have since @xmath407 is diagonal and has therefore only @xmath352 non - zeros elements that on @xmath349 @xmath408 which implies that on @xmath349 , since the frobenius norm is invariant by rotation @xmath409 this concludes part ( ii ) of the induction and therefore , it concludes the induction .    [ [ conclusion ] ] 5 . conclusion + + + + + + + + + + + + +    by the previous induction , we know that on @xmath349 , we have @xmath410 and also that @xmath411 and also that for any @xmath4 @xmath412    this concludes the proof since for @xmath91 larger than @xmath413 with @xmath414 a large enough constant , we have by definition of the sequence @xmath153 that @xmath415      first , note that for @xmath416 , we have @xmath417 thus , without loss of generality , we consider @xmath418 . we know that @xmath419 and @xmath420 this gives @xmath421 by assumption  [ ass : design ] ,",
    "using @xmath422 , we have for @xmath314 , @xmath423 this concludes the proof .      since @xmath49",
    ", we have that @xmath424 this implies that ( using a gaussian tail probability @xmath425 for @xmath426 when @xmath427 ) with probability larger than @xmath65 @xmath428    since @xmath299 satisfies the assumption  [ ass : design ] with @xmath346 , we have that @xmath429 which implies that for any @xmath430 , we have @xmath431    equation   implies together with equation   that for a matrix @xmath430 , with probability larger than @xmath65 , @xmath432 where @xmath433 .    to obtain the bound for the supremum of the quantity in ( [ eq : ber2 ] ) over all @xmath434 , we consider the approximating set @xmath435 whose property is described as follows",
    ". let @xmath436 .",
    "let , for any @xmath437 , @xmath438 be a @xmath439 covering set of @xmath440 .",
    "here we use a classic result ( * ? ? ? * lemma 3.1 ) , saying that the @xmath441-covering numbers of @xmath440 is bounded by @xmath442 .",
    "thus the cardinality of @xmath438 is smaller than @xmath443 .",
    "let @xmath444 be the event such that for all @xmath445 and for each vector in @xmath446 , it holds that @xmath447 where @xmath448 , where @xmath449 is a large constant . by equation  , and since @xmath450",
    "we know that @xmath451 holds with probability @xmath452 for each @xmath453 and for each vector @xmath446 . by a union bound , we have that @xmath454 since @xmath449 .",
    "let now @xmath455 such that @xmath456 .",
    "it is possible to write @xmath23 as @xmath457 where each @xmath458 belongs to @xmath438 , and where the @xmath459 are such that @xmath460 .",
    "we have on @xmath444 that @xmath461 this concludes the proof .",
    "let @xmath462 be the singular vectors of @xmath463 , i.e.  @xmath464 .",
    "let @xmath465 .",
    "the dimension of @xmath224 is @xmath466 .",
    "let now @xmath467 be the vectorial sub - space that is orthogonal to @xmath468 .",
    "its dimension is @xmath469 .",
    "since @xmath470 , there is at lest one unitary vector in @xmath471 .",
    "let @xmath472 be this vector , since it is in @xmath224 , it can be written as @xmath473 where for @xmath474 , we have @xmath475 and @xmath476 .",
    "therefore , we have that @xmath477 consider @xmath478 .",
    "so we have that @xmath479 since the @xmath480 are orthonormal , and since the singular values are positive and ordered in decreasing order .",
    "this concludes the proof .      from the proof of theorem",
    "[ th : mainthm2 ] , equation  , we know that the estimator @xmath128 satisfies @xmath481 i.e.  the desired result in operator norm and from which the results on the rank and in the other schatten norm follow because of the thresholding . also since the sequence @xmath153 is an arithmetico - geometrical sequence converging to @xmath482 , of arithmetic term @xmath483 and of geometric term @xmath139",
    ", it is clear that @xmath124 is such that @xmath484 i.e. @xmath132 this concludes the proof .      by definition , we have that @xmath485",
    "let @xmath486 and let @xmath487 be the vector with all element equal to @xmath63 except the @xmath488 entry which is equal to @xmath64 , and we consider that @xmath489 .",
    "we have by definition and using representations ( [ simple1 ] ) and ( [ simple2 ] ) that @xmath490 and @xmath491 note that given @xmath492 , @xmath493 and @xmath494    the following lemma is a concentration inequality that holds in gaussian design .    [ lem : concer ] assume that the design is gaussian .",
    "let @xmath55 .",
    "we have that with probability larger than @xmath65 ( on the design ) @xmath495    let @xmath496 .",
    "we have @xmath497 where @xmath498 is the @xmath499 identity matrix .",
    "this implies that @xmath500 where @xmath501 is the chi square distribution with @xmath466 degrees of freedom . by bernstein s inequality ,",
    "we thus have ( since the @xmath502 distribution is sub - gaussian ) that , with probability larger than @xmath65 , @xmath503 where @xmath83 is an universal constant .",
    "this implies that , with probability larger than @xmath65 , @xmath504 this concludes the proof .    combining lemma  [ lem : concer ] with pythagoras s theorem as in the proof of lemma  [ lem : tra ]",
    ", we have that for any @xmath505 , with probability larger than @xmath65 , @xmath506 by a union bound , this implies that with probability larger than @xmath65 , @xmath507 where @xmath83 is a universal constant .",
    "this concludes the proof ( in remarking that the above quantity is arbitrarily small when @xmath508 ) .",
    "[ [ acknowledgements ] ] acknowledgements + + + + + + + + + + + + + + + +    we would like to thank richard nickl , richard samworth and rajen shah for insightful comments and discussions .",
    "part of this work was produced when ac was in the statslab in the university of cambridge .",
    "ac s work is supported since 2015 by the dfg s emmy noether grant musyad ( ca 1488/1 - 1 ) .",
    "# 1    acharya , a. , t.  kypraios , and m.  guta ( 2015 ) .",
    "efficient quantum tomography with incomplete measurement settings . .",
    "agarwal , a. , s.  negahban , and m.  j. wainwright ( 2012 ) .",
    "fast global convergence of gradient methods for high - dimensional statistical recovery .  *",
    "40*(5 ) , 24522482 .",
    "bickel , p. , y.  ritov , and a.  tsybakov ( 2009 ) .",
    "simultaneous analysis of lasso and dantzig selector .  * 37 * , 17051732 .",
    "blumensath , t. and m.  e. davies ( 2009 ) .",
    "iterative hard thresholding for compressed sensing .  *",
    "27*(3 ) , 265274 .",
    "bunea , florentina , y. she , and m. wegkamp ( 2011 ) .",
    "optimal selection of reduced rank estimators of high - dimensional matrices .  *",
    "39*(2 ) , 12821309 .",
    "butucea , c. , m.  gu , and t.  kypraios ( 2015 ) .",
    "spectral thresholding quantum tomography for low rank states .  _",
    "17_(11 ) , 113050 .",
    "cai , t. and h.h  zhou ( 2012 ) .",
    "optimal rates of convergence for sparse covariance matrix estimation .  *",
    "40 * , 23892420 .",
    "cai , t. and a.  zhang ( 2015 ) .",
    "rop : matrix recovery via rank - one projections .  * 43 * , 102138 .",
    "cands , e. and t.  tao ( 2010 ) .",
    "the power of convex relazation : near - optimal matrix completion .  *",
    "56 * , 20532080 .",
    "cands , e.  j. and y.  plan ( 2011 ) .",
    "tight oracle bounds for low - rank matrix recovery from a minimal number of random measurements .  *",
    "57*(4 ) , 23422359 .",
    "cands , e. and b.  recht ( 2009 ) .",
    "exact matrix completion via convex optimization .",
    "* 9 * , 717772 .",
    "carpentier , a. , j. eisert , d. gross , and r. nickl ( 2015 ) .",
    "uncertainty quan- tification for matrix compressed sensing and quantum tomography problems . .",
    "chen , y. and m. wainwright ( 2015 ) .",
    "fast low - rank estimation by projected gradient descent : general statistical and algorithmic guarantees .",
    "flammia , s.  t , d.  gross , y .- k .",
    "liu , and j.  eisert .",
    "quantum tomography via compressed sensing : error bounds , sample complexity and efficient estimators .",
    ", 14(9):095022 , 2012 .",
    "friedman j. and t. hastie and r. tibshirani ( 2008 ) .",
    "sparse inverse covariance estimation with the graphical lasso .  *",
    "9*(3 ) , 432441 .    van  de geer , s. , p.  bhlmann , y.  ritov , and r.  dezeure ( 2014 ) .",
    "on asymptotically optimal confidence regions and tests for high - dimensional models .",
    "* 42*(3 ) , 11661202 .",
    "goldfarb , d. and s. ma ( 2011 ) .",
    "convergence of fixed - point continuation algorithms for matrix rank minimization .  * 11 * , 183210 .",
    "gross , d. ( 2011 ) . recovering low - rank matrices from few coefficients in any basis .  *",
    "57*(3 ) , 15481566 .",
    "gross , d. , y .- k .",
    "liu , s.  t flammia , s.  becker , and j.  eisert .",
    "( 2010 ) quantum state tomography via compressed sensing .",
    ", 105(15):150401 .",
    "guta , m. , t.  kypraios , and i.  dryden ( 2012 ) .",
    "rank - based model selection for multiple ions quantum tomography .",
    "_ 14 _ , 105002 .",
    "haeffner , h. , w.  haensel , c.  f. roos , j.  benhelm , d.  c. al  kar , m.  chwalla , t.  koerber , u.  d. rapol , m.  riebe , p.  o. schmidt , c.  becher , o.  ghne , w.  dur , and r.  blatt ( 2005 ) .",
    "scalable multi - particle entanglement of trapped ions .",
    "_ 438 _ , 643 .",
    "holevo , a.  s. ( 2001 ) . .",
    "springer .",
    "huang , h. and s. ma and c - h .",
    "zhang ( 2008 ) .",
    "adaptive lasso for sparse high - dimensional regression models .  * 18 * , 16031618 .",
    "javanmard , a. and a.  montanari .",
    "( 2014 ) confidence intervals and hypothesis testing for high - dimensional regression .  *",
    "15 * ( 1 ) , 28692909 .",
    "klopp , o. matrix completion by singular value thresholding : sharp bounds .",
    "* 9 * , ( 2 ) .",
    "23482369 .",
    "knight , k. and w. fu ( 2000 ) .",
    "asymptotics for lasso - type estimators .",
    "* 28 * , 13561378 .",
    "koltchinskii , v. ( 2011 ) . on neumann entropy penalization and low - rank matrix estimation .  *",
    "39*(6 ) , 2936 - 2973 .",
    "koltchinskii , v. , k.  lounici , and a.  b. tsybakov ( 2011 ) .",
    "nuclear - norm penalization and optimal rates for noisy low - rank matrix completion .  *",
    "39*(5 ) , 23022329 .",
    "koltchinskii , v. , d. xia ( 2015 ) . optimal estimation of low rank density matrices .",
    "arxiv preprint arxiv:1507.05131 .",
    "liu , y.k .",
    "universal low - rank matrix recovery from pauli measurements .",
    ", 16381646 .",
    "meinshausen , n. and p. bhlmann ( 2006 ) .",
    "high dimensional graphs and variable selection with the lasso .  *",
    "34 * , 14361462 .",
    "needell , d. and j.a .",
    "tropp ( 2009 ) .",
    "cosamp : iterative signal recovery from incomplete and inaccurate samples .",
    "* 26 * , 301321 .",
    "negahban , s. and wainwright , m. ( 2011 ) .",
    "estimation of ( near ) low - rank matrices with noise and high - dimensional scaling .  * 39 * , 10691097 .",
    "nickl , r. and van  de geer , s. ( 2014 ) .",
    "confidence sets in sparse regression .",
    "* 41*(6 ) , 2852 - 2876 .",
    "nielsen , m.  a. and i.  l. chuang ( 2000 ) . .",
    "cambridge : cambridge university press .",
    "recht , b. ( 2011 ) . a simpler approach to matrix completion .",
    "* 12 * , 34133430 .",
    "tanner , j. and k. wei ( 2012 ) .",
    "normalized iterative hard thresolding for matrix completion .",
    "* 35 * , s104s125 .",
    "zhang , c - h . and",
    "zhang , s - s . (",
    "confidence intervals for low dimensional parameters in high dimensional linear models .  *",
    "76 * , 217242 .",
    "the method that we proposed and studied in the low rank matrix recovery setting can be adapted and simplified to accommodate another setting : the sparse linear regression setting .",
    "we explain how to construct an estimator based on iht , and prove that the estimator is efficient in @xmath0 and @xmath2 norm , and provide the limiting distribution of a simple modification of our estimate .",
    "consider the linear model @xmath513 where @xmath514 is a @xmath515 matrix , the signal vector @xmath516 is @xmath42-sparse ( @xmath517 ) , and @xmath48 is an i.i.d .",
    "vector of gaussian white noise , i.e.  @xmath49 ( as in the matrix regression , we do not need the gaussian assumption and our results hold with sub - gaussian independent noise ) , and @xmath518 .",
    "we denote the sample covariance matrix by @xmath519 .",
    "[ rem : ass ] suppose @xmath514 is from a distribution whose covariance matrix is @xmath524 .",
    "let the minimum eigenvalue @xmath525 and the maximum eigenvalue @xmath526 and @xmath527 } \\sigma_{ii } \\leq 1 $ ] .",
    "assume that @xmath528 has independent sub - gaussian rows with zero mean and sub - gaussian norm @xmath529 .",
    "then from the paper @xcite , for @xmath530 , with probability larger than @xmath531 with @xmath532 , there exists a computationally feasible @xmath35 such that @xmath533 holds . in this case",
    ", we can take @xmath534 .",
    "this algorithm takes again three parameters : @xmath535 and @xmath77 .",
    "we have the same interpretation for @xmath74 as in the matrix regression setting , @xmath75 is an upper bound on two times the sparsity of @xmath536 ( again , it does not need to be tight as long as @xmath537 is small enough ) , and @xmath77 is a loose bound on the @xmath1 norm of @xmath536 .",
    "then we update thresholds in each iteration @xmath540 , by @xmath541 where @xmath542 where @xmath543 . recall that the pseudo inverse @xmath35 of @xmath544 and @xmath537",
    "are taken from assumption [ ass : matrix ] .",
    "set now recursively , @xmath545 and @xmath546 this procedure provides a sequence of estimates , and as we will prove in the next subsection , this sequence is with high probability close to the true @xmath536 as soon as @xmath91 is of order @xmath92 ( see theorems  [ th : mainthm ] and  [ thm : asymnorm ] ) .",
    "[ rem : iht ] the proposed method modifies iterative algorithms ( see e.g. * ? ? ?",
    "* ; * ? ? ?",
    "the usual ( normalised ) iht algorithm updates the estimate using @xmath547 where @xmath548 is a hard thresholding operator that keeps the largest @xmath42 elements of a vector and @xmath549 is a stepsize that can have the interpretation of a gradient step when it is much smaller than @xmath64 .",
    "the difference is in the thresholding ; we update thresholds while they pick the largest @xmath42 values after adjusting the added parts .",
    "most importantly , previous works on this estimator only considered the case of a deterministic ( small ) noise , so their analysis is not applicable in our model where the noise is stochastic .",
    "[ th : mainthm ] assume that assumption  [ ass : matrix ] is satisfied and that @xmath551 .",
    "let @xmath552 .",
    "we have that with probability larger than @xmath65 , for any @xmath99 , @xmath553 where @xmath554 and @xmath555 .",
    "to prove asymptotic normality , we slightly modify the estimator defined in theorem [ th : mainthm ] .",
    "this is similar to the de - sparsified lasso by @xcite in the sense that we also use a de - sparsified version of our estimator .",
    "consider the estimator @xmath550 of theorem  [ th : mainthm ] ( with the same @xmath560 ) and @xmath35 in assumption [ ass : matrix ] , and define @xmath561    [ thm : asymnorm ] suppose that the same assumptions and notation used in theorem [ th : mainthm ] hold .",
    "then , writing @xmath562 and @xmath563 , we have @xmath564 where @xmath565 .",
    "if @xmath566 ( e.g.  for designs as in remark  [ rem : ass ] , we have @xmath567 so it suffices that @xmath568 ) then we also have @xmath117            by an union bound ( with hoeffding s inequality ) we know that with probability larger than @xmath65 @xmath571 let @xmath349 be the event of probability @xmath65 where the previous equation is satisfied .                  by equation  ( [ eq : superpoule ] ) we know that on @xmath349 , since @xmath581 is @xmath42 sparse @xmath582 since @xmath583 . since @xmath584 , we have that on @xmath349 , by equation ( [ equ ] ) , all the coordinates @xmath466 of @xmath585 such that @xmath586 are set to @xmath63 .",
    "this implies that the support of @xmath585 ( and thus the support of @xmath587 ) is included in the support of @xmath536 on @xmath349 .",
    "therefore , @xmath585 is @xmath588sparse on @xmath349 .",
    "also , still since @xmath584 , we have that @xmath589 and this implies together with equation ( [ equ ] ) that on @xmath349 , we have @xmath590 this concludes the proof for @xmath591 .",
    "the induction is complete , and we have that the previous equation holds for all @xmath86 .",
    "it is equivalent to the fact that on @xmath349 ( and thus with probability larger than @xmath65 ) , for all @xmath86 @xmath592 and the support of @xmath550 is included in the support of @xmath536 .              by definition , @xmath597 given @xmath514 , we know that @xmath120 is a linear function of the gaussian vector @xmath50 , thus @xmath598 now we prove the bound for @xmath110 .",
    "note that using ( [ eq : pseudoinv ] ) and @xmath599 , for a sufficiently large @xmath8 , we have a constant @xmath67 such that @xmath600 then using the result from theorem [ th : mainthm ] , with probability at least @xmath65 , we have as long as @xmath601 @xmath602 as @xmath603 ."
  ],
  "abstract_text": [
    "<S> we consider the problem of low rank matrix recovery in a stochastically noisy high dimensional setting . </S>",
    "<S> we propose a new estimator for the low rank matrix , based on the iterative hard thresholding method , and that is computationally efficient and simple . </S>",
    "<S> we prove that our estimator is optimal both in terms of the frobenius risk , and in terms of the operator norm risk , i.e.  in terms of the entry - wise risk uniformly over any change of orthonormal basis . </S>",
    "<S> this result allows us to provide the limiting distribution of the estimator . in the case where the design is gaussian </S>",
    "<S> , we prove that the entry - wise bias of the limiting distribution of the estimator is small , which is of great interest for constructing tests and confidence sets for low dimensional subsets of entries of the low rank matrix . </S>"
  ]
}