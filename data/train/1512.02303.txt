{
  "article_text": [
    "complex system modeling and simulation often mandate global sensitivity analysis , which constitutes the study of how the global variation of input , due to its uncertainty , influences the overall uncertain behavior of a response of interest .",
    "most common approaches to sensitivity analysis are firmly anchored in the second - moment properties  the output variance  which is divvied up , qualitatively or quantitatively , to distinct sources of input variation @xcite .",
    "there exist a multitude of methods or techniques for calculating the resultant sensitivity indices of a function of independent variables : the random balance design method @xcite , the state - dependent parameter metamodel @xcite , sobol s method @xcite , and the polynomial dimensional decomposition ( pdd ) method @xcite , to name but four .",
    "a few methods , such as those presented by kucherenko , tarantola , and annoni @xcite and rahman @xcite , are also capable of sensitivity analysis entailing correlated or dependent input .",
    "implicit in the variance - driven global sensitivity analysis is the assumption that the statistical moments satisfactorily describe the stochastic response . in many applications",
    ", however , the variance provides a restricted summary of output uncertainty .",
    "therefore , sensitivity indicators stemming solely from the variance should be carefully interpreted .",
    "a more rational sensitivity analysis should account for the entire probability distribution of an output variable , meaning that alternative and more appropriate sensitivity indices , based on probabilistic characteristics above and beyond the variance , should be considered .",
    "addressing some of these concerns has led to a sensitivity index by exploiting the @xmath1 distance between two output probability density functions @xcite .",
    "such sensitivity analysis establishes a step in the right direction and is founded on the well - known total variational distance between two probability measures .",
    "there remain two outstanding research issues for further improvements of density - based sensitivity analysis .",
    "first , there is no universal agreement in selecting the total variational distance as the undisputed measure of dissimilarity or affinity between two output probability density functions .",
    "in fact , a cornucopia of divergence or distance measures exist in the literature of information theory .",
    "therefore , a more general framework , in the spirit of density - based measures , should provide diverse choices to sensitivity analysis @xcite .",
    "second , the density - based sensitivity indices in general are more difficult to calculate than the variance - based sensitivity indices .",
    "this is primarily because the probability density function is harder to estimate than the variance .",
    "moreover , nearly all estimation methods available today are very expensive due to the existence of the inner and outer integration loops . therefore , efficient computational methods for computing density - based sensitivity indices are desirable .",
    "the purpose of this paper is twofold .",
    "first , a brief exposition of the @xmath0-divergence measure is given in section 2 , setting the stage for a general multivariate sensitivity index , referred to as the @xmath0-sensitivity index , presented in section 3 .",
    "the section includes new theoretical results representing fundamental properties and important inequalities pertaining to the @xmath0-sensitivity index .",
    "second , section 4 introduces three distinct approximate methods for estimating the @xmath0-sensitivity index .",
    "the methods depend on how the probability densities of a stochastic response are estimated , including an efficient surrogate approximation commonly used for high - dimensional uncertainty quantification .",
    "numerical results from three mathematical functions , as well as from a computationally intensive stochastic mechanics problem , are reported in section 5 .",
    "finally , conclusions are drawn in section 6 .",
    "let @xmath2 , @xmath3 , @xmath4 , and @xmath5 represent the sets of positive integer ( natural ) , non - negative integer , real , and non - negative real numbers , respectively . for @xmath6 , denote by @xmath7 the @xmath8-dimensional euclidean space and by @xmath9 the @xmath8-dimensional multi - index space .",
    "these standard notations will be used throughout the paper .",
    "let @xmath10 be a measurable space , where @xmath11 is a sample space and @xmath12 is a @xmath13-algebra of the subsets of @xmath11 , satisfying @xmath14 and @xmath15 , and @xmath16 be a @xmath13-finite measure on @xmath10 .",
    "let @xmath17 be a set of all probability measures on @xmath18 , which are absolutely continuous with respect to @xmath16 .",
    "for two such probability measures @xmath19 , let @xmath20 and @xmath21 denote the radon - nikodym derivatives of @xmath22 and @xmath23 with respect to the dominating measure @xmath16 , that is , @xmath24 and @xmath25 .",
    "let @xmath26 $ ] be an extended real - valued function , which is    1 .",
    "continuous on @xmath27 and finite - valued on @xmath28 ; 2 .",
    "convex on @xmath27 , that is , @xmath29 for any @xmath30 and @xmath31 $ ] ; , @xmath32 , are excluded . ]",
    "strictly convex at @xmath33 , that is , @xmath34 for any @xmath35 and @xmath36 such that @xmath37 ; ; and @xmath38 evaluated on two sides of the point @xmath33 on the graph of @xmath0 lies above the function value @xmath39 . ] and 4 .",
    "equal to _ zero _ at @xmath33 , that is , @xmath40 .",
    "the @xmath0-divergence , describing the difference or discrimination between two probability measures @xmath22 and @xmath23 , is defined by the integral @xmath41 provided that the undefined expressions are interpreted by @xcite @xmath42 @xmath43 to define the @xmath0-divergence for absolutely continuous probability measures in terms of elementary probability theory , take @xmath11 to be the real line and @xmath16 to be the lebesgue measure , that is , @xmath44 , @xmath45 , so that @xmath20 and @xmath21 are simply probability density functions , denoted by @xmath46 and @xmath47 , respectively .",
    "then the @xmath0-divergence can also be defined by @xmath48 the divergence measures in ( [ 1 ] ) and ( [ 2 ] ) were introduced in the 1960s by csiszr @xcite , ali and silvey @xcite , and morimoto @xcite .",
    "similar definitions exist for discrete probability measures .",
    "vajda @xcite , liese and vajda @xcite , and sterreicher @xcite discussed general properties of the @xmath0-divergence measure , including a few axiomatic ones .",
    "the basic but important properties are as follows @xcite : +    1",
    ".   _ non - negativity and reflexivity : _ @xmath49 with equality if and only if @xmath50 . + 2 .",
    "_ duality : _",
    "@xmath51 , where @xmath52 , @xmath53 , is the * -conjugate ( convex ) function of @xmath0 .",
    "when @xmath54 , @xmath0 is * -self conjugate .",
    "_ invariance : _ @xmath55 , where @xmath56 , @xmath57 .",
    "symmetry : _",
    "@xmath58 if and only if @xmath59 , where @xmath52 , @xmath53 , and @xmath57 . when @xmath60 , the symmetry and duality properties coincide .",
    "range of values : _",
    "@xmath61 , where @xmath62 and @xmath63 .",
    "the left equality holds if and only if @xmath50 . the right equality holds if and only if @xmath64 , that is , for mutually singular ( orthogonal ) measures , and is attained when @xmath65 .",
    "+    the normalization condition @xmath40 is commonly adopted to ensure that the smallest possible value of @xmath66 is _ zero_. but fulfilling such condition by the class @xmath67 of convex functions @xmath0 is not required .",
    "this is because , for the subclass @xmath68 such that @xmath69 satisfies @xmath40 , the shift by the constant @xmath70 sends every @xmath71 to @xmath72 .",
    "indeed , some of these properties may still hold if @xmath73 or if @xmath0 is not restricted to the convexity properties .    depending on how @xmath0 is defined , the @xmath0-divergence may or may not be a true metric .",
    "for instance , it is not necessarily symmetric in @xmath22 and @xmath23 for an arbitrary convex function @xmath0 ; that is , the @xmath0-divergence from @xmath22 to @xmath23 is generally not the same as that from @xmath23 to @xmath22 , although it can be easily symmetrized when required .",
    "furthermore , the @xmath0-divergence does not necessarily satisfy the triangle inequality .",
    "it is well known that @xmath74 has a versatile functional form , resulting in a number of popular information divergence measures .",
    "indeed , many of the well - known divergences or distances commonly used in information theory and statistics are easily reproduced by appropriately selecting the generating function @xmath0 .",
    "familiar examples of the @xmath0-divergence include the forward and reversed kullback - leibler divergences @xmath75 and @xmath76 @xcite , kolmogorov total variational distance @xmath77 @xcite , hellinger distance @xmath78 @xcite , pearson @xmath79 divergence @xmath80 @xcite , neyman @xmath79 divergence @xmath81 @xcite , @xmath82 divergence @xmath83 @xcite , vajda @xmath84 divergence @xmath85 @xcite , jeffreys distance @xmath86 @xcite , and triangular discrimination @xmath87 @xcite , to name a few , and are defined as    @xmath88 d\\xi , \\label{3a}\\ ] ]    @xmath89 d\\xi = : d_{kl}\\left ( p_2 \\parallel p_1\\right ) , \\label{3b}\\ ] ]    @xmath90    @xmath91 ^ 2 d\\xi , \\label{3d}\\ ] ]    @xmath92 d\\xi , \\label{3e}\\ ] ]    @xmath93 d\\xi : = d_{p}\\left ( p_2 \\parallel p_1\\right ) , \\label{3f}\\ ] ]    @xmath94 , ~\\alpha \\in \\mathbb{r } \\setminus \\{\\pm 1\\ } , \\label{3g}\\ ] ]    @xmath95    @xmath96 \\ln \\left [ \\dfrac{f_1(\\xi)}{f_2(\\xi ) } \\right ] d\\xi , \\label{3i}\\ ] ]    @xmath97 ^ 2}{f_1(\\xi)+f_2(\\xi ) } d\\xi . \\label{3j}\\ ] ]    the definitions of some of these divergences , notably the two kullback - leibler and pearson - neyman @xmath79 divergences , are inverted when the @xmath0-divergence is defined by swapping @xmath22 and @xmath23 in ( [ 1 ] ) or ( [ 2 ] ) .",
    "there are also many other information divergence measures that are not subsumed by the @xmath0-divergence measure .",
    "see the paper by kapur @xcite or the book by taneja @xcite .",
    "nonetheless , any of the divergence measures from the class of @xmath0-divergences or others can be exploited for sensitivity analysis , as described in the following section .",
    "let @xmath98 be a complete probability space , where @xmath99 is a sample space , @xmath100 is a @xmath13-field on @xmath99 , and @xmath101 $ ] is a probability measure . with @xmath102 representing the borel @xmath13-field on @xmath103 , @xmath104 , consider an @xmath103-valued absolutely continuous random vector @xmath105 , describing the statistical uncertainties in all system and input parameters of a general stochastic problem .",
    "the probability law of @xmath106 , which may comprise independent or dependent random variables , is completely defined by its joint probability density function @xmath107 .",
    "let @xmath108 be a non - empty subset of @xmath109 with the complementary set @xmath110 and cardinality @xmath111 , and let @xmath112 , @xmath113 , be a subvector of @xmath106 with @xmath114 defining its complementary subvector .",
    "then , for a given @xmath115 , the marginal density function of @xmath116 is @xmath117 .",
    "let @xmath118 ) , a real - valued , continuous , measurable transformation on @xmath119 , define a general stochastic response of interest .",
    "define @xmath120 to be the associated output random variable . for global sensitivity analysis ,",
    "suppose that the sensitivity of @xmath121 with respect to a subset @xmath122 , @xmath115 , of input variables @xmath106 is desired .",
    "as shown by a few researchers for univariate cases only @xcite , such a multivariate sensitivity measure can be linked to the divergence between the unconditional and conditional probability measures of @xmath121 .",
    "denote by @xmath123 and @xmath124 the probability measures and by @xmath125 and @xmath126 the probability density functions of random variables @xmath121 and @xmath127 , respectively , where @xmath127 stands for @xmath121 conditional on @xmath122 , which is itself random .",
    "setting @xmath128 , @xmath129 , @xmath130 , and @xmath131 in ( [ 2 ] ) , the @xmath0-divergence becomes @xmath132 as explained in the preceding section , @xmath133 characterizes the discrimination between @xmath123 and @xmath124 , but it is random because @xmath122 is random .",
    "a general multivariate @xmath0-sensitivity index of an output random variable @xmath121 for a subset @xmath116 , @xmath115 , of input random variables @xmath134 , denoted by @xmath135 , is defined as the expected value of the @xmath0-divergence from @xmath123 to @xmath124 , that is , @xmath136 , \\label{4b}\\ ] ] where @xmath137 is the expectation operator with respect to the probability measure of @xmath116 .    from the definition of the expectation operator , the @xmath0-sensitivity index @xmath138 where @xmath139 and @xmath140 are the probability measure and probability density function , respectively , of @xmath121 conditional on @xmath141 , and @xmath142 is the joint probability density function of @xmath143 . the last equality in ( [ 5 ] ) is formed by the recognition that @xmath144 and is useful for calculating the sensitivity index , to be discussed in section 4 .    for variance - based sensitivity analysis entailing independent random variables",
    ", there exists a well - known importance measure , namely , the sobol index .",
    "one way to explain the sobol index is the analysis - of - variance ( anova ) decomposition of a square - integrable function @xmath145 , expressed by the compact form @xcite    @xmath146    [ 5a ]    which is a finite , hierarchical expansion in terms of its input variables with increasing dimensions . here",
    ", @xmath147 is a @xmath148-variate component function describing a constant or the interactive effect of @xmath116 on @xmath145 when @xmath149 or @xmath150 .",
    "the summation in ( [ 5a1 ] ) comprises @xmath151 component functions , with each function depending on a group of variables indexed by a particular subset of @xmath109 , including the empty set @xmath152 . applying the expectation operator @xmath153 on @xmath154 and its square from ( [ 5a1 ] ) and recognizing the _ zero_-mean and orthogonal properties of @xmath155 , @xmath156 , in ( [ 5a3 ] ) @xcite , the variance @xmath157\\right]^2= \\sum_{\\emptyset\\ne u\\subseteq\\{1,\\cdots , n\\ } } { \\sigma_u}^2 \\label{5b}\\ ] ] of @xmath121 splits into partial variances @xmath158= \\mathbb{e}_{\\mathbf{x}_{u}}\\left[y_{u}^{2}(\\mathbf{x}_{u})\\right ] , ~\\emptyset\\ne u\\subseteq\\{1,\\cdots , n\\ } , \\label{5c}\\ ] ] of all non - constant anova component functions .",
    "henceforth , the sobol sensitivity index of @xmath121 for a subset of variables @xmath122 is defined as @xcite @xmath159 provided that @xmath160 .",
    "the sobol index is bounded between 0 and 1 and represents the fraction of the variance of @xmath121 contributed by the @xmath148-variate interaction of input variables @xmath122 .",
    "there exist @xmath161 such indices , adding up to @xmath162 .",
    "does the @xmath0-sensitivity index provide a more useful insight than the existing variance - based sobol index into the importance of input variables ? to answer this question , consider a purely additive function @xmath163 , where @xmath164 is an arbitrary real - valued constant and the input random variables @xmath165 , @xmath166 , have _ zero _ means and identical variances @xmath167=s^2 $ ] , @xmath168 , but otherwise follow independent and arbitrary probability distributions .",
    "then , from ( [ 5a ] ) through ( [ 5d ] ) , ( 1 ) the anova component functions @xmath169 , @xmath170 , @xmath166 , and @xmath171 for @xmath172 ; ( 2 ) the variances @xmath173 , @xmath174 , @xmath166 , and @xmath175 for @xmath172 ; and ( 3 ) the sobol indices @xmath176 , @xmath166 , and @xmath177 for @xmath178 . as all univariate sobol indices are the same , so are the contributions of input variables to the variance of @xmath121 .",
    "hence , according to the sobol index , all input variables are equally important , regardless of their probability distributions .",
    "this is unrealistic , but possible because the variance is just a moment and provides only a partial description of the uncertainty of an output variable . in contrast",
    ", the @xmath0-sensitivity indices will vary depending on the choice of the input density functions , therefore , providing a more rational measure of the influence of input variables .",
    "it is important to derive and emphasize the fundamental properties of the @xmath0-sensitivity index @xmath135 inherited from the @xmath0-divergence measure .",
    "the properties , including a few important inequalities , are described in conjunction with six propositions as follows .",
    "the @xmath0-sensitivity index @xmath135 of @xmath121 for @xmath122 , @xmath115 , is non - negative and vanishes when @xmath121 and @xmath122 are statistically independent .",
    "[ p1 ]    since @xmath179 by virtue of the non - negativity property of the @xmath0-divergence and @xmath180 for any @xmath181 , the first line of ( [ 5 ] ) yields @xmath182 proving the first part of the proposition .",
    "if @xmath121 and @xmath122 are statistically independent , then @xmath183 for any @xmath184 , resulting in @xmath185 , owing to the reflexivity property or the range of values ( left equality ) of the @xmath0-divergence . in that case , @xmath186 proving the second part of the proposition .",
    "the range of values of @xmath135 is @xmath187 where @xmath62 and @xmath63 .",
    "[ p2 ]    see the proof of proposition [ p1 ] for the left inequality .",
    "the right inequality is derived from the largest value of @xmath188 , which is @xmath189 , according to the range of values ( right equality ) of the @xmath0-divergence .",
    "therefore , ( [ 4b ] ) yields @xmath190 \\le \\mathbb{e}_{\\mathbf{x}_u } \\left [ f(0)+f^*(0 ) \\right ] = f(0)+f^*(0),\\ ] ] completing the proof .    from proposition",
    "[ p2 ] , @xmath135 has a sharp lower bound , which is _ zero _ since @xmath40 . in contrast , @xmath135 may or may not have an upper bound , depending on whether @xmath189 is finite or infinite . if there is an upper bound , then the largest value @xmath189 is a sharp upper bound , and hence can be used to scale @xmath135 to vary between 0 and 1 . for instance , when @xmath191 , the result is the well - known variational distance measure @xmath192 and the upper bound of the associated sensitivity index @xmath193 ( say ) is @xmath194 . when @xmath195 or @xmath196 , then @xmath197 , meaning that the sensitivity index @xmath198 ( say ) or @xmath199 ( say ) , derived from the kullback - leibler divergence measure @xmath200 or @xmath201 , has no upper bound . no scaling is possible in such a case .",
    "the @xmath0-sensitivity index @xmath202 of @xmath121 for all input variables @xmath203 is @xmath204 where @xmath62 and @xmath63 .",
    "[ p3 ]    the probability measure @xmath205 is a dirac measure , representing an almost sure outcome @xmath206 , where @xmath207 and @xmath208 .",
    "decompose @xmath4 into two disjoint subsets @xmath209 and @xmath210 and observe that @xmath211 therefore , the probability measures @xmath123 and @xmath205 are mutually singular ( orthogonal ) , that is , @xmath212 .",
    "consequently , @xmath213 , according to the range of values ( right equality ) of the @xmath0-divergence .",
    "finally , for @xmath214 , ( [ 4b ] ) yields @xmath215 = \\mathbb{e}_{\\mathbf{x } } \\left [ f(0)+f^*(0 ) \\right ] = f(0)+f^*(0).\\ ] ]    for the special case of @xmath191 , the index derived from the total variational distance @xmath216 .",
    "therefore , when normalized , @xmath217 , which is the same value reported by borgonovo @xcite .",
    "let @xmath218 .",
    "if @xmath121 and @xmath219 are statistically independent , then @xmath220 [ p4 ] in addition , if @xmath108 and @xmath221 are disjoint subsets , that is , @xmath222 , then @xmath223 [ p4b ]    for any @xmath218 , observe that @xmath224 and @xmath225 . since @xmath121 is independent of @xmath219 , the probability measures @xmath226 and @xmath227 are the same , yielding @xmath228 .",
    "applying this condition to the expression of @xmath229  the @xmath0-sensitivity index of @xmath121 for @xmath230  in the first line of ( [ 5 ] ) and noting @xmath231 results in @xmath232 proving the first part of the proposition . here , the second equality is obtained by recognizing that @xmath233 does not depend on @xmath234 and @xmath235 .",
    "the third equality is attained by integrating out @xmath236 with respect to @xmath234 on @xmath237 , resulting in @xmath238 .",
    "the second part of the proposition results from the reduction , @xmath239 , when @xmath222 .    as a special case ,",
    "consider @xmath240 and @xmath241 , where @xmath242 , @xmath243 .",
    "then , according to proposition [ p4 ] , @xmath244 , meaning that there is no contribution of @xmath245 to the sensitivity of @xmath121 for @xmath246 if @xmath145 does not depend on @xmath245 .",
    "the @xmath0-sensitivity index @xmath135 of @xmath121 for @xmath122 , @xmath115 , is invariant under smooth and uniquely invertible transformations ( diffeomorphisms ) of @xmath121 and @xmath122 .",
    "[ p5 ]    for @xmath115 , let @xmath247 and @xmath248 be smooth and uniquely invertible , that is , diffeomorphic maps of random variables @xmath249 and @xmath121 . from elementary probability theory",
    ", the probability densities of the transformed variables @xmath250 , @xmath251 , and @xmath252 are @xmath253 respectively , where @xmath254 \\in \\mathbb{r}^{|u|\\times |u|}$ ] , @xmath255 , is the jacobian matrix of the transformation such that @xmath256 for any @xmath181 and @xmath257 .",
    "applying these relationships to the sensitivity index @xmath258 of @xmath251 for @xmath259 defined in the last line of ( [ 5 ] ) and noting @xmath260 and @xmath261 yields @xmath262 completing the proof .",
    "for a special case of @xmath240 , @xmath166 , corollary 4 of borgonovo et al .",
    "@xcite describes the monotonic invariance of a univariate sensitivity index derived from @xmath263 norm or @xmath0-divergence .",
    "in contrast , proposition [ p5 ] and its proof presented here are more general and different than those reported in the existing work @xcite",
    ".    the invariance property of the @xmath0-sensitivity index described by proposition [ p5 ] does not hold in general for the variance - based sobol index @xcite .",
    "the latter index is invariant only under affine transformations .",
    "moreover , the @xmath0-sensitivity index , unlike the sobol index , is applicable to random input following dependent probability distributions .",
    "let @xmath218 be two disjoint subsets such that @xmath222 . for probability measures",
    "@xmath123 , @xmath124 , and @xmath264 , let @xmath0 be a select convex generating function , which produces metric @xmath0-divergences from @xmath123 to @xmath264 , from @xmath123 to @xmath265 , and from @xmath124 to @xmath264 , satisfying the triangle inequality @xmath266 then @xmath267 where @xmath268 $ ] is the conditional sensitivity index of @xmath269 for @xmath230 .",
    "furthermore , if @xmath122 and @xmath219 are statistically independent , then @xmath270 [ p6 ]    applying the expectation operator @xmath271 on both sides of the triangle inequality yields    @xmath272    since , for @xmath222 , @xmath273 does not depend on @xmath274 , the first integral on the right side of ( [ sr3 ] ) reduces to @xmath275 therefore , ( [ sr3 ] ) becomes @xmath276 recognizing the sensitivity indices @xmath229 , @xmath135 , and @xmath277 to be respectively the integral on the left side , the first integral on the right side , and the second integral on the right side of ( [ sr4 ] ) produces the upper bound in ( [ sr2 ] ) . in addition , observe that the sensitivity index @xmath277 is non - negative , represents the contribution of the divergence from @xmath124 to @xmath264 , and vanishes if and only if @xmath121 and @xmath219 are statistically independent .",
    "therefore , @xmath229 reaches the lower bound , which is @xmath135 , if and only if @xmath121 and @xmath219 are statistically independent .    to obtain ( [ sr2b ] ) , use the last line of ( [ 5 ] ) to write @xmath278 where , by invoking the statistical independence between @xmath122 and @xmath219 , the numerator and denominator of the argument of @xmath0 become @xmath279 and @xmath280 respectively . applying ( [ sr4c ] ) and ( [ sr4d ] ) to ( [ sr4b ] ) results in @xmath281 which transforms ( [ sr2 ] ) to ( [ sr2b ] ) and",
    "hence completes the proof .    as a special case ,",
    "consider again @xmath240 and @xmath241 , where @xmath242 , @xmath243 .",
    "then , according to proposition [ p6 ] , applicable to sensitivity indices rooted in metric @xmath0-divergences only , @xmath282 which states the following : if @xmath121 depends on @xmath245 , then the contribution of @xmath245 to the sensitivity of @xmath121 for @xmath246 increases from @xmath283 , but is limited by the residual term @xmath284 . if @xmath121 and @xmath245 are statistically independent , then @xmath284 vanishes , resulting in @xmath285",
    ". this agrees with proposition [ p4 ] , which , however , is valid whether or not the underlying @xmath0-divergence is a metric .",
    "in addition , if @xmath165 and @xmath245 are statistically independent , then @xmath286 , yielding @xmath287 .",
    "borgonovo @xcite derived the same bounds for a special case when the sensitivity index stems from the total variational distance .",
    "proposition [ p6 ] , by contrast , is a general result and applicable to sensitivity indices emanating from all metric @xmath0-divergences .",
    "a plethora of @xmath0-sensitivity indices are possible by appropriately selecting the convex function @xmath0 in ( [ 4b ] ) or ( [ 5 ] ) . listed in table [ table1 ]",
    "are ten such sensitivity indices derived from the forward and reversed kullback - leibler divergences , total variational distance , hellinger distance , pearson @xmath79 divergence , neyman @xmath79 divergence , @xmath82 divergence , vajda @xmath84 divergence , jeffreys distance , and triangular discrimination in ( [ 3a ] ) through ( [ 3j ] ) .",
    "three prominent sensitivity indices , for example , the mutual information @xcite @xmath288 f_{\\mathbf{x}_u , y}(\\mathbf{x}_u,\\xi ) d{\\mathbf{x}_u}d\\xi = : h_{u , kl'}\\ ] ] between @xmath122 and @xmath121 , the squared - loss mutual information @xcite @xmath289 ^ 2   f_{y}(\\xi ) f_{\\mathbf{x}_u}(\\mathbf{x}_u )   d{\\mathbf{x}_u}d\\xi   \\\\       & =   & \\int_{\\mathbb{r}^{|u|}\\times\\mathbb{r } } \\dfrac{f_{\\mathbf{x}_u , y}(\\mathbf{x}_u,\\xi)}{f_{y}(\\xi ) f_{\\mathbf{x}_u}(\\mathbf{x}_u ) } \\left[1 - \\left\\ { \\dfrac{f_{y}(\\xi ) f_{\\mathbf{x}_u}(\\mathbf{x}_u)}{f_{\\mathbf{x}_u , y}(\\mathbf{x}_u,\\xi ) } \\right\\}^2 \\right ] f_{\\mathbf{x}_u , y}(\\mathbf{x}_u,\\xi ) d{\\mathbf{x}_u}d\\xi                                                      \\\\       & = : & h_{u , n } \\end{array}\\ ] ] between @xmath122 and @xmath121 , and borgonovo s importance measure @xcite @xmath290 of @xmath122 on @xmath121 , are rooted in reversed kullback - leibler , neyman , and total variational divergences or distances , respectively .",
    "indeed , many previously used sensitivity or importance measures are special cases of the @xmath0-sensitivity index derived from the @xmath0-divergence .",
    ".ten special cases of the @xmath0-sensitivity index [ cols=\"<,<,<\",options=\"header \" , ]     [ table8 ]    table [ table8 ] presents the approximate univariate sensitivity indices @xmath291 ( total variational distance ) and @xmath292 ( reversed kullback - leibler divergence ) of the maximum von mises stress by the pdd - kde - mc method .",
    "the pdd expansion coefficients were estimated by @xmath293-variate dimension - reduction integration @xcite , requiring one- ( @xmath294 ) or at most two - dimensional ( @xmath295 ) gauss quadratures .",
    "the order @xmath296 of orthogonal polynomials and number @xmath297 of gauss quadrature points in the dimension - reduction numerical integration are @xmath298 and @xmath299 , respectively .",
    "the indices are broken down according to the choice of selecting @xmath300 and @xmath301 . in all pdd approximations , the sample size @xmath302 .",
    "the sensitivity indices by the pdd - kde - mc methods in table [ table8 ] quickly converge with respect to @xmath293 and/or @xmath296 .",
    "since fea is employed for response evaluations , the computational effort of the pdd - kde - mc method comes primarily from numerically determining the pdd expansion coefficients .",
    "the expenses involved in estimating the pdd coefficients vary from 25 to 33 fea for the univariate pdd approximation and from 277 to 481 fea for the bivariate pdd approximation , depending on the two values of @xmath296 . based on the sensitivity indices in table [ table8 ] , the horizontal boundary conditions ( @xmath303 and @xmath304 ) are highly important ; the vertical load ( @xmath305 ) , elastic modulus ( @xmath306 ) , and vertical boundary conditions ( @xmath307 and @xmath308 ) are slightly important ; and the horizontal load ( @xmath309 ) and poisson s ratio ( @xmath310 ) are unimportant in influencing the maximum von mises stress .",
    "it is important to recognize that the respective univariate and bivariate pdd solutions in this particular problem are practically the same .",
    "therefore , the univariate pdd solutions are not only accurate , but also highly efficient .",
    "this is because of a realistic example chosen , where the individual main effects of input variables on the von mises stress are dominant over their interactive effects .",
    "finally , this example also demonstrates the non - intrusive nature of the pdd - kde - mc method , which can be easily integrated with commercial or legacy computer codes for analyzing large - scale complex systems .",
    "a general multivariate sensitivity index , referred to as the @xmath0-sensitivity index , is presented for global sensitivity analysis .",
    "the index is founded on the @xmath0-divergence , a well - known divergence measure from information theory , between the unconditional and conditional probability measures of a stochastic response .",
    "the index is applicable to random input following dependent or independent probability distributions .",
    "since the class of @xmath0-divergence subsumes a wide variety of divergence or distance measures , numerous sensitivity indices can be defined , affording diverse choices to sensitivity analysis .",
    "several existing sensitivity indices or measures , including mutual information , squared - loss mutual information , and borgonovo s importance measure , are shown to be special cases of the proposed sensitivity index .",
    "a detailed theoretical analysis reveals the @xmath0-sensitivity index to be non - negative and endowed with a range of values , where the smallest value is _ zero _ , but the largest value may be finite or infinite , depending on the generating function @xmath0 chosen .",
    "the index vanishes or attains the largest value when the unconditional and conditional probability measures coincide or are mutually singular .",
    "unlike the variance - based sobol index , which is invariant only under affine transformations , the @xmath0-sensitivity index is invariant under nonlinear but smooth and uniquely invertible transformations .",
    "if the output variable and a subset of input variables are statistically independent , then there is no contribution from that subset of input variables to the sensitivity of the output variable . for a metric divergence , the resultant @xmath0-sensitivity index for a group of input variables increases from the unconditional sensitivity index for a subgroup of input variables , but is limited by the residual term emanating from the conditional sensitivity index .",
    "three new approximate methods , namely , the mc , kde - mc , and pdd - kde - mc methods , are proposed to estimate the @xmath0-sensitivity index .",
    "the mc and kde - mc methods are both relevant when a stochastic response is inexpensive to evaluate , but the methods depend on how the probability densities of a stochastic response are calculated or estimated .",
    "the pdd - kde - mc method , predicated on an efficient surrogate approximation , is relevant when analyzing high - dimensional complex systems , demanding expensive function evaluations .",
    "therefore , the computational burden of the mc and kde - mc methods can be significantly alleviated by the pdd - kde - mc method . in all three methods developed",
    ", the only requirement is the availability of input - output samples , which can be drawn either from a given computational model or from actual raw data .",
    "numerical examples , including a computationally intensive stochastic boundary - value problem , demonstrate that the proposed methods provide accurate and economical estimates of density - based sensitivity indices .",
    ", _ on the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling _ ,",
    "phil . mag .",
    ", 50 ( 1900 ) , pp ."
  ],
  "abstract_text": [
    "<S> this article presents a general multivariate @xmath0-sensitivity index , rooted in the @xmath0-divergence between the unconditional and conditional probability measures of a stochastic response , for global sensitivity analysis . unlike the variance - based sobol index , </S>",
    "<S> the @xmath0-sensitivity index is applicable to random input following dependent as well as independent probability distributions . </S>",
    "<S> since the class of @xmath0-divergences supports a wide variety of divergence or distance measures , a plethora of @xmath0-sensitivity indices are possible , affording diverse choices to sensitivity analysis . commonly used sensitivity indices or measures , such as mutual information , squared - loss mutual information , and borgonovo s importance measure , are shown to be special cases of the proposed sensitivity index . </S>",
    "<S> new theoretical results , revealing fundamental properties of the @xmath0-sensitivity index and establishing important inequalities , are presented . </S>",
    "<S> three new approximate methods , depending on how the probability densities of a stochastic response are determined , are proposed to estimate the sensitivity index . </S>",
    "<S> four numerical examples , including a computationally intensive stochastic boundary - value problem , illustrate these methods and explain when one method is more relevant than the others .    </S>",
    "<S> borgonovo s importance measure , @xmath0-sensitivity index , kernel density estimation , mutual information , polynomial dimensional decomposition , squared - loss mutual information . </S>"
  ]
}