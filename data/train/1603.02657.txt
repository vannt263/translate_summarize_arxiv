{
  "article_text": [
    "the construction of a generator of realizations from a given data set related to a @xmath0-valued random vector , for which the probability distribution is unknown and is concentrated on an unknown subset @xmath1 of @xmath0 , is a central and difficult problem in uncertainty quantification and statistical data analysis , in stochastic modeling and associated statistical inverse problems for boundary value problems , in the design of experiments for random parameters , and certainly , in signal processing and machine learning . +",
    "two fundamental tools serve as building blocks for addressing this problem .",
    "first , nonparametric statistical methods @xcite can be effectively used to construct probability distribution on @xmath0 of a random vector given an initial data set of its samples .",
    "multidimensional gaussian kernel - density estimation is one efficient subclass of these methods .",
    "markov chain monte carlo ( mcmc ) procedures can then be used to sample additional realizations from the resulting probability model , and which are thus statistically consistent with the initial data set @xcite .",
    "the second building block consists of manifold embedding algorithms , where low - dimensional structure is characterized within a larger vector space .",
    "diffusion maps @xcite is a powerful tool for characterizing and delineating @xmath1 using the initial data set and concepts of geometric diffusion .",
    "+ the first tool described above , consisting of using nonparametric density estimation with mcmc , does not allow , in general , the restriction of new samples to the subset @xmath1 on which the probability distribution is concentrated .",
    "the scatter of generated samples outside of @xmath1 is more pronounced the more complex and disconnected this set is .",
    "+ the second tool consisting of diffusion maps , while effectively allowing for the discovery and characterization of subset @xmath1 on which the probability distribution is concentrated , does not give a direct approach for generating additional realizations in this subset that are drawn from a target distribution consistent with the initial data set .",
    "+ these two fundamental tools have been used independently and quite successfully to address problems of sampling from complex probability models and detecting low - dimensional manifolds in high - dimensional settings .",
    "an analysis of mcmc methods on riemann manifolds has been presented recently @xcite where the manifold is the locus of density functions and not of the data itself .",
    "this paper addresses the still open challenge of efficient statistical sampling on manifolds defined by limited data .",
    "+ it should be noted that the pca @xcite yields a statistical reduction method for second - order random vectors in finite dimension , similarly to the karhunen - love expansion ( kle ) @xcite , which yields a statistical reduction method for second - order stochastic processes and random fields , and which has been used for obtaining an efficient construction @xcite of the polynomial chaos expansion ( pce ) of stochastic processes and random fields @xcite , and for which some ingredients have more recently been introduced for analyzing complex problems encountered in uncertainty quantification @xcite . _ a priori _ and in general , the pca or the kle , which use a nonlocal basis with respect to the data set ( global basis related to the covariance operator estimated with the data set ) does not allow for discovering and characterizing the subset on which the probability law is concentrated .",
    "the present work can be viewed as an extension and generalization of previous work by the authors where the low - dimensional manifold was unduly restricted @xcite .",
    "+ after formalizing the problem in section  [ section2 ] , the proposed methodology is presented in section  [ section3 ] and developed in section  [ section4 ] . section  [ section5 ] deals with three applications : the first two applications correspond to analytical examples in dimension @xmath2 with @xmath3 given data points and in dimension @xmath4 with @xmath5 data points .",
    "the third application is related to a petro - physics database made up of experimental measurements for which the dimension is @xmath6 with @xmath7 given data points .",
    "a lower case letter such as @xmath8 , @xmath9 , or @xmath10 , is a real deterministic variable . +",
    "a boldface lower case letter such as @xmath11 , @xmath12 , or @xmath13 is a real deterministic vector .",
    "+ an upper case letter such as @xmath14 , @xmath15 , or @xmath16 , is a real random variable . +",
    "a boldface upper case letter , @xmath17 , @xmath18 , or @xmath19 , is a real random vector . +",
    "a lower case letter between brackets such as @xmath20 $ ] , @xmath21 $ ] , or @xmath22 $ ] ) , is a real deterministic matrix . + a boldface upper case letter between brackets such as @xmath23 $ ] , @xmath24",
    "$ ] , or @xmath25 $ ] , is a real random matrix .",
    "+ @xmath26 : mathematical expectation .",
    "+ @xmath27 : set of all the @xmath28 real matrices .",
    "+ @xmath29 : @xmath30 .",
    "+ @xmath31 : euclidean norm of vector @xmath11 .",
    "+ @xmath20_{kj}$ ] : entry of matrix @xmath20 $ ] .",
    "+ @xmath20^t$ ] : transpose of matrix @xmath20 $ ] .",
    "+ @xmath32\\}$ ] : trace of a square matrix @xmath20 $ ] .",
    "+ @xmath33\\vert_f$ ] : frobenius norm of matrix @xmath20 $ ] such that @xmath34^t\\ , [ x]\\}$ ] .",
    "+ @xmath35 $ ] : identity matrix in @xmath29 .",
    "+ @xmath36 : kronecker s symbol such that @xmath37 if @xmath38 and @xmath39 if @xmath40",
    "the following four ingredients serve to set the stage for the mathematical analysis required for constructing the target probability distribution and sampling from it .",
    "\\(i ) let @xmath41 be a generic point in @xmath0 and let @xmath42 be the lebesgue measure .",
    "a family of @xmath43 vectors in @xmath0 will be written as @xmath44 .",
    "+ ( ii ) let @xmath45 be a random vector defined on a probability space @xmath46 , with values in @xmath0 , for which the probability distribution is defined by a probability density function ( pdf ) on @xmath0 ( _ a priori _ and in general , the probability distribution is not gaussian ) .",
    "this pdf is unknown but is assumed to be concentrated on an unknown subset @xmath1 of @xmath0 .",
    "a specific realization of random vector @xmath17 will be denoted by @xmath47 where @xmath48 .",
    "+ ( iii ) the available information consists of a given set of @xmath43 data points specified by @xmath43 vectors @xmath49 in @xmath0 . these will be assumed to constitute @xmath43 statistically independent realizations ( or samples ) @xmath50 @xmath51 of random vector @xmath17 . for @xmath52 , the vector @xmath53 in @xmath0",
    "is written as @xmath54 .",
    "the @xmath43 data points can then be represented by the matrix @xmath55 $ ] in @xmath27 such that @xmath55_{kj } = x^{d , j}_k$ ] .",
    "+ ( iv ) the local structure of the given data set is captured via random matrix @xmath23 $ ] , defined on @xmath46 , with values in @xmath27 .",
    "specifically , @xmath23=[{{\\textbf{x}}}^1 \\ldots { { \\textbf{x}}}^n]$ ] in which the columns @xmath56 are @xmath43 independent copies of random vector @xmath17 .",
    "consequently , matrix @xmath55 $ ] can be viewed as one realization of random matrix @xmath23 $ ] + the objective of this paper then is to construct a generator of realizations of random matrix @xmath23 $ ] in @xmath27 , for which the unknown probability distribution is directly deduced from the unknown probability distribution of random vector @xmath17 , which is concentrated on the unknown subset @xmath1 of @xmath0 , and for which only one realization @xmath55 $ ] is given .",
    "+ the unknown subset @xmath1 of @xmath0 can be viewed as a manifold , which corresponds to the structure of data @xmath55 $ ] , and on which the unknown probability measure is concentrated .",
    "consequently , the objective of the paper is to perform `` data - driven probability concentration and sampling on a manifold '' .",
    "to enhance the utility of the present paper and to clarify the inter - relation between a number of intricate mathematical steps , the proposed methodology is summarized in the following seven steps .    1",
    ".   in general , the given data are heterogeneous and badly conditioned .",
    "consequently , the first step consists in performing a scaling of the given data , which yields the matrix @xmath55 $ ] in @xmath27 of the scaled given data ( the matrix introduced in section  [ section2 ] ) , and simply called the given data set ( removing the word `` scaled '' ) .",
    "the given data set are then normalized by using a principal component analysis ( but without trying to introduce a statistical reduced - order representation ) .",
    "therefore , the random matrix @xmath23 $ ] ( corresponding to scaled data @xmath55 $ ] ) is written as an affine transformation of a random matrix @xmath24 $ ] with values in @xmath57 with @xmath58 ( in general , @xmath59 , but sometimes some eigenvalues ( of the empirical estimate of the covariance matrix of @xmath17 ) exhibits zeros eigenvalues that are removed , yielding @xmath60 ) .",
    "random matrix @xmath24 $ ] can then be written as @xmath24 = [ { { \\textbf{h}}}^1 \\ldots { { \\textbf{h}}}^n]$ ] in which the columns @xmath61 are @xmath43 independent copies of a random vector @xmath18 with values in @xmath62 , whose probability density function on @xmath62 is unknown and is concentrated on an unknown subset @xmath63 of @xmath62 .",
    "the given data @xmath55 $ ] in @xmath27 ( related to @xmath23 $ ] ) are then transformed into given data , @xmath64 $ ] in @xmath57 , related to random matrix @xmath24 $ ] .",
    "the data represented by @xmath64 $ ] are thus normalized .",
    "let @xmath65 be the nonparametric estimate of the probability density function of random vector @xmath18 , which is performed by using @xmath64 $ ] ( note that @xmath65 is not the pdf of @xmath18 but is the nonparametric estimate of the pdf of @xmath18 ) .",
    "consequently , the nonparametric estimate of the probability distribution on @xmath57 of random matrix @xmath24 $ ] is written as @xmath66}([\\eta])\\ , d[\\eta]= p_{{\\textbf{h}}}({{\\boldsymbol{\\eta}}}^1)\\times \\ldots \\times p_{{\\textbf{h}}}({{\\boldsymbol{\\eta}}}^n ) \\ , d{{\\boldsymbol{\\eta}}}^1\\ldots d{{\\boldsymbol{\\eta}}}^n$ ]",
    "in which @xmath21 $ ] is any matrix in @xmath57 such that @xmath21 = [ { { \\boldsymbol{\\eta}}}^1 \\ldots { { \\boldsymbol{\\eta}}}^n]$ ] with @xmath67 .",
    "the second step consists in constructing the nonparametric statistical estimate @xmath65 of the probability density function of @xmath18 using data @xmath64\\in\\mm_{\\nu , n}$ ] .",
    "this is an usual problem that will be performed by using the classical multidimensional gaussian kernel - density estimation method .",
    "nevertheless , we will use the modification proposed in @xcite ( instead of the classical method ) in order that the nonparametric estimate @xmath65 yields , for the estimation of the covariance matrix of @xmath18 ( using @xmath64 $ ] ) , the identity matrix @xmath68 $ ] in @xmath29 .",
    "this construction is directly used in the following third step .",
    "the third step consists in introducing an adapted generator of realizations for random matrix @xmath24 $ ] , which belongs to the class of the mcmc methods such as the metropolis - hastings algorithm @xcite ( that requires the definition of a good proposal distribution ) , the gibbs sampling @xcite ( that requires the knowledge of the conditional distribution ) or the slice sampling @xcite ( that can exhibit difficulties related to the general shape of the probability distribution , in particular for multimodal distributions ) .",
    "this adapted generator will be the one derived from @xcite , which is based on a nonlinear it stochastic differential equation ( isde ) formulated for a dissipative hamiltonian dynamical system @xcite , which admits @xmath66}([\\eta])\\ , d[\\eta]$ ] as an invariant measure , and for which the initial condition depends on matrix @xmath64 $ ] .",
    "the fourth step of the methodology consists in characterizing the subset @xmath63 from scaled and normalized data @xmath64 $ ] .",
    "this will be done using the formulation of the diffusion maps , which is a very powerful mathematical tool for doing that .",
    "it should be noted that the diffusion - maps method is a local approach with respect to given data while the pca is a global approach that , in general , can not see the local geometric structure of the given data set .",
    "however , the diffusion distance , which has been introduced in @xcite for discovering and characterizing @xmath63 , and which is constructed using the diffusion maps , does not allow for constructing a generator of realizations of random matrix @xmath24 $ ] for which data @xmath64 $ ] are given but for which its probability measure and the subset @xmath63 of concentration are unknown .",
    "this step is introduced for constructing an algebraic vector basis @xmath69 of @xmath70 , depending on two parameters that are a smoothing parameter @xmath71 and an integer @xmath72 related to the analysis scale of the local geometric structure of the data set . for @xmath73 , the vectors @xmath74 are directly related to the diffusion maps .",
    "a subset of this basis will be able to characterize the subset @xmath63 of @xmath62 on which the probability measure of @xmath18 is concentrated .",
    "we will then introduce the matrix @xmath75 $ ] in @xmath76 made up of the first @xmath77 vectors @xmath78 of the diffusion - maps basis , with @xmath79 .",
    "the fifth step consists in estimating an adapted value of @xmath77 in order to capture the local geometric structure of @xmath63 and to obtain a reasonable mean - square convergence .",
    "6 .   using the first @xmath77 vectors ( represented by matrix @xmath75 $ ] ) of the diffusion - maps basis ,",
    "the sixth step consists in constructing a reduced - order isde , which allows for generating some additional realizations of the reduced - order representation of random matrix @xmath24 $ ] , by introducing the random matrix @xmath80 $ ] with values in @xmath81 such that @xmath24 = [ { { \\textbf{z}}}]\\ , [ g]^t$ ] . 7 .",
    "the last step consists in numerically solving the reduced - order isde for computing the additional realizations @xmath82 , \\ldots , [ z_s^{n_{{\\hbox{{\\pppppcarac mc}}}}}]$ ] of random matrix @xmath80 $ ] and then to deduce the additional realizations @xmath83 , \\ldots , [ x_s^{n_{{\\hbox{{\\pppppcarac mc}}}}}]$ ] of random matrix @xmath23 $ ] for which only one realization @xmath55 $ ] was given .",
    "in this section , a detailed presentation of the methodology is given that parallels the steps described in section  [ section3 ] .",
    "let @xmath84 $ ] be the matrix in @xmath27 of the unscaled given data set .",
    "the matrix @xmath55 $ ] in @xmath27 of the scaled given data set ( simply called the given data set ) is constructed ( if the data effectively require such a scaling , which will be the case for the third application presented in section  [ section5.3 ] ) such that , for all @xmath85 and @xmath52 , @xmath86_{kj } = \\frac{[x_d^{uns}]_{kj } - \\min_{j ' } [ x_d^{uns}]_{kj ' } } { \\max_{j ' } [ x_d^{uns}]_{kj ' } - \\min_{j ' } [ x_d^{uns}]_{kj ' } } + \\epsilon_s\\ , .",
    "\\ ] ] the quantity @xmath87 is added to the scaled data in order to avoid the scalar @xmath88 in the nonparametric statistical estimation of the pdf .",
    "let @xmath89 and @xmath90 $ ] be the empirical estimates of the mean vector @xmath91 and the covariance matrix @xmath92 , such that @xmath93 = \\frac{1}{n-1 } \\sum_{j=1}^n ( { { \\textbf{x}}}^{d , j}-{{\\textbf{m}}})\\ , ( { { \\textbf{x}}}^{d , j } -{{\\textbf{m}}})^t \\ , .                                                                   \\ ] ] we consider the eigenvalue problem @xmath90\\ , { { \\boldsymbol{\\varphi}}}^k = \\mu_k\\ , { { \\boldsymbol{\\varphi}}}^k$ ] .",
    "noting that matrix @xmath90 $ ] is often of rank @xmath94 , denote its @xmath95 positive eigenvalues by @xmath96 with @xmath97 and let @xmath98 $ ] be the @xmath99 matrix such @xmath98^t\\,[\\varphi]= [ i_\\nu]$ ] , whose columns are the associated orthonormal eigenvectors @xmath100 . consequently , random matrix @xmath23 $ ] can be rewritten as @xmath101 = [ \\underline x ] + [ \\varphi]\\ , [ \\mu]^{1/2}\\ , [ { { \\textbf{h } } } ] \\ , ,                                                                                    \\ ] ] in which @xmath102 $ ] is the matrix in @xmath27 for which each column is vector @xmath89 and where @xmath103 $ ] is the positive diagonal @xmath104 real matrix such that @xmath103_{kk ' }   = \\delta_{kk'}\\mu_k$ ] .",
    "the realization @xmath64\\in \\mm_{\\nu , n}$ ] of @xmath24 $ ] associated with the realization @xmath55 $ ] of @xmath23 $ ] is thus computed by @xmath105 =   [ \\mu]^{-1/2 } [ \\varphi]^t\\ , ( [ x_d ] - [ \\underline x ] ) \\ , .                                                                               \\ ] ] let @xmath106 be the @xmath43 vectors in @xmath62 such that @xmath107 = [ \\eta_d]$ ] ( the columns of @xmath64 $ ] ) .",
    "it can easily be seen that the empirical estimates @xmath108 of the mean vector @xmath109 and @xmath110 $ ] of the covariance matrix @xmath111 of random vector @xmath18 are such that @xmath112 = \\frac{1}{n-1 } \\sum_{j=1}^n { { \\boldsymbol{\\eta}}}^{d , j } \\ , ( { { \\boldsymbol{\\eta}}}^{d , j})^t = [ \\ , i_\\nu ] \\ , .                                              \\ ] ]      the estimation @xmath65 on @xmath62 of the pdf of random vector @xmath18 is carried out by using the gaussian kernel - density estimation method and the @xmath43 independent realizations @xmath113 represented by matrix @xmath64 $ ] computed with eq .  .",
    "as proposed in @xcite , a modification of the classical gaussian kernel - density estimation method is used in order that the mean vector and the covariance matrix ( computed with the nonparametric estimate @xmath114 ) are equal to @xmath115 and @xmath116 $ ] respectively ( see eq .  ) .",
    "the positive - valued function @xmath65 on @xmath62 is then defined , for all @xmath12 in @xmath62 , by @xmath117 in which @xmath118 is the positive function from @xmath62 into @xmath1190\\ , , + \\infty[$ ] defined , for all @xmath12 in @xmath62 , by @xmath120 with @xmath121 and where the positive parameters @xmath122 and @xmath123 are defined by @xmath124 parameter @xmath122 is the usual multidimensional optimal silverman bandwidth ( in taking into account that the empirical estimate of the standard deviation of each component is unity ) , and parameter @xmath123 has been introduced in order that the second equation in eq .   holds .",
    "using eqs .",
    "to , it can easily be verified that @xmath125 @xmath126 + ( \\frac{\\widehat s_\\nu}{s_\\nu})^2 \\ , \\frac{(n - 1)}{n}[c ' ] = [ \\ , i_\\nu ] \\ , .",
    "\\ ] ] a nonparametric estimate @xmath66}$ ] on @xmath57 of the probability density function of random matrix @xmath24 $ ] is then written as @xmath127}([\\eta])=p_{{\\textbf{h}}}({{\\boldsymbol{\\eta}}}^{d,1})\\times \\ldots\\times p_{{\\textbf{h}}}({{\\boldsymbol{\\eta}}}^{d , n})\\ , ,                                                          \\ ] ] in which @xmath65 is defined by eqs .   to .",
    "the probability density function defined by eqs .   to is directly used for constructing the it stochastic differential equation . let @xmath128,[{{\\textbf{v}}}(r ) ] ) , r\\in \\rr^+ \\}$ ] be the markov stochastic process defined on the probability space @xmath129 @xmath130 , indexed by @xmath131 , with values in @xmath132 , satisfying , for all @xmath133 , the following isde @xmath134 =   [ { { \\textbf{v}}}(r ) ] \\ , dr \\",
    ", ,                                                                                                           \\ ] ] @xmath135=   [ l([{{\\textbf{u}}}(r)])]\\ , dr -\\frac{1}{2 } f_0\\ , [ { { \\textbf{v}}}(r)]\\ , dr + \\sqrt{f_0}\\ , [ d{{\\textbf{w}}}(r ) ] \\ , ,                                             \\ ] ] with the initial condition @xmath136 = [ { { \\textbf{h}}}_d ] \\quad , \\quad [ { { \\textbf{v}}}(0 ) ] = [ { { \\boldsymbol{\\mathcal{n}}}}\\ , ] \\quad a.s \\ , .                                                                     \\ ] ] in eqs .   and , the different quantities are defined as follows . + ( i ) for all @xmath22 = [ { { \\textbf{u}}}^1 \\ldots { { \\textbf{u}}}^n]$ ] in @xmath57 with @xmath137 in @xmath62 , the matrix @xmath138)]$ ] in @xmath57 is defined , for all @xmath139 and for all @xmath140 , by @xmath141)]_{k\\ell}= -\\frac{\\partial}{\\partial u^\\ell_k }   { { \\mathcal{v}}}({{\\textbf{u}}}^\\ell ) \\ , ,                                                                    \\ ] ] in which the potential @xmath142 defined on @xmath62 with values in @xmath143 , is defined by @xmath144 where @xmath145 is the continuously differentiable function from @xmath62 into @xmath1190\\ , , + \\infty[$ ] such that @xmath146 from eqs .   and",
    ", it can be deduced that , @xmath147)]_{k\\ell } = \\frac{1}{q({{\\textbf{u}}}^\\ell ) } \\ , \\ { { \\boldsymbol{\\nabla}}_{\\!\\!{{\\textbf{u}}}^\\ell}\\ , q({{\\textbf{u}}}^\\ell ) \\}_k \\ , ,                               \\ ] ] @xmath148 + ( ii ) the stochastic process @xmath149 , r \\geq 0\\}$ ] with values in @xmath57 is such that @xmath150 = [ d{{\\textbf{w}}}^1(r ) \\ldots d{{\\textbf{w}}}^n(r)]$ ] in which the columns @xmath151 are @xmath43 independent copies of the normalized wiener process @xmath152 defined on @xmath46 , indexed by @xmath143 with values in @xmath62 . the matrix - valued autocorrelation function @xmath153= e\\{{{\\textbf{w}}}(r)\\,{{\\textbf{w}}}(r')^t\\}$ ] of @xmath154",
    "is then written as @xmath153 = \\min ( r , r')\\ , [ i_\\nu ] $ ] . + ( iii ) the probability distribution of the random matrix @xmath155 $ ] with values in @xmath57 is @xmath66}([\\eta])\\ , d[\\eta]$ ] .",
    "a known realization of @xmath155 $ ] is matrix @xmath64 $ ] .",
    "the random matrix @xmath156 $ ] with values in @xmath57 is written as @xmath156 = [ { { \\boldsymbol{\\mathcal{n}}}}^1 \\ldots { { \\boldsymbol{\\mathcal{n}}}}^n]$ ] in which the columns @xmath157 are @xmath43 independent copies of the normalized gaussian vector @xmath158 with values in @xmath62 ( this means that @xmath159 and @xmath160 $ ] ) .",
    "the random matrices @xmath155 $ ] and @xmath156 $ ] , and the normalized wiener process @xmath161 are assumed to be independent .",
    "+ ( iv ) the free parameter @xmath162 allows the dissipation term of the nonlinear second - order dynamical system ( dissipative hamiltonian system ) to be controlled .",
    "+ since the columns @xmath163 of random matrix @xmath24 $ ] are independent copies of random vector @xmath18 , and since the pdf of random matrix @xmath155 $ ] is @xmath66}$ ] , using theorems 4 to 7 in pages 211 to 216 of ref .",
    "@xcite , in which the hamiltonian is taken as @xmath164 , and using @xcite for proving the ergodic property , it can be proved that the problem defined by eqs .   to admits a unique invariant measure and a unique solution @xmath128,[{{\\textbf{v}}}(r)]),$ ] @xmath165 that is a second - order diffusion stochastic process , which is stationary ( for the shift semi - group on @xmath143 defined by the positive shifts @xmath166 , @xmath167 ) and ergodic , and such that , for all @xmath168 fixed in @xmath143 , the probability distribution of random matrix @xmath169 $ ] is @xmath66}([\\eta])\\ , d[\\eta]$ ] in which @xmath66}$ ] is defined by eq .  . + * remarks*. + 1 .",
    "it should be noted that the invariant measure is independent of @xmath170 .",
    "if the initial condition @xmath171 $ ] was not @xmath155 $ ] but was any other random matrix whose pdf is not @xmath66}$ ] , then the unique diffusion process @xmath128,[{{\\textbf{v}}}(r)]),$ ] @xmath172 would not be stationary , but would be asymptotic ( for @xmath173 ) to a stationary diffusion process @xmath174,$ ] @xmath175 ) , r_{{\\hbox{{\\ppcarac st}}}}\\geq 0\\}$ ] such that , for all @xmath176 , @xmath24 = [ { { \\textbf{u}}}_{{\\hbox{{\\ppcarac st}}}}(r_{{\\hbox{{\\ppcarac st } } } } ) ] = \\lim_{r\\rightarrow + \\infty } [ { { \\textbf{u}}}(r)]$ ] in probability distribution ( this implies that , for all @xmath176 , the pdf of random matrix @xmath177 $ ] is @xmath66}$ ] ) .",
    "in such a case , the free parameter @xmath162 allows the transient response generated by the initial condition to be rapidly killed in order to get more rapidly the asymptotic behavior corresponding to the stationary and ergodic solution associated with the invariant measure .",
    "+ 3 . as the nonparametric estimate @xmath66}$ ] of the pdf of @xmath24 $ ]",
    "does not explicitly take into account the local structure of data set @xmath64 $ ] , if the pdf of @xmath18 is concentrated on @xmath63 , then the generator of realizations constructed by the mcmc method defined by eqs .",
    "to ( or by any other mcmc method ) , will not give some realizations localized in the subset @xmath63 ( see the applications in section  [ section5 ] ) .",
    "+ 4 . as explained in @xcite , a variant of eq .   could be introduced in replacing it by @xmath178=   [ l([{{\\textbf{u}}}(r)])]\\ , dr -\\frac{1}{2 } f_0\\ , [ d_0]\\,[{{\\textbf{v}}}(r)]\\ , dr + \\sqrt{f_0}\\,[s_0]\\ , [ d{{\\textbf{w}}}(r)]$ ] in which @xmath179 $ ] would belong to @xmath29 and where @xmath180 $ ] would be a positive symmetric matrix such that @xmath180=[s_0]\\,[s_0]^t$ ] with @xmath181 \\leq \\nu$ ] . in the present case ,",
    "such an extension would not allow for improving the methodology proposed because the initial condition for @xmath171 $ ] is the given matrix @xmath64 $ ] that follows @xmath66}$ ] .",
    "+ 5 . for @xmath182",
    "fixed in @xmath183 , let @xmath184,r\\geq 0\\}$ ] , @xmath185 = [ \\eta_d]$ ] , and @xmath186 $ ] be independent realizations of the stochastic process @xmath187,r\\geq 0\\}$ ] , the random matrix @xmath155 $ ] , and the random matrix @xmath188 $ ] .",
    "let @xmath189,[{{\\textbf{v}}}(r;\\theta ) ] ) , r\\in \\rr^+\\}$ ] be the corresponding realization of the unique stationary diffusion process @xmath128,[{{\\textbf{v}}}(r ) ] ) , r\\in \\rr^+\\}$ ] of the isde problem defined by eqs .   to ) .",
    "then additional realizations @xmath190 , \\ldots , [ \\eta_s^{n_{{\\hbox{{\\pppppcarac mc}}}}}]$ ] of random matrix @xmath24 $ ] can be generated by @xmath191   = [ { { \\textbf{u}}}(\\ell \\rho ; \\theta ) ] \\quad , \\quad \\rho = m_0\\ , \\delta r \\quad , \\quad \\ell = 1,\\ldots , n_{{\\hbox{{\\pppppcarac mc}}}}\\ , ,                \\ ] ] in which @xmath192 is the sampling step of the continuous index parameter @xmath168 used in the integration scheme ( see section  [ section4.7.1 ] ) and where @xmath193 is a positive integer :    * if @xmath194 , then @xmath195 and the @xmath196 additional realizations are dependent , but the ergodic property of @xmath197,r\\in \\rr^+\\}$ ] can be used for obtaining the convergence of statistics constructed using @xmath190 , \\ldots , [ \\eta_s^{n_{{\\hbox{{\\pppppcarac mc}}}}}]$ ] for random matrix @xmath24 $ ] . *",
    "if integer @xmath193 is chosen sufficiently large ( such that @xmath198 is much larger than the relaxation time of the dissipative hamiltonian dynamical system ) , then @xmath190 , \\ldots ,    [ \\eta_s^{n_{{\\hbox{{\\pppppcarac mc}}}}}]$ ] can approximatively be considered as independent realizations of random matrix @xmath24 $ ] .",
    "we underscore here that each sample of random matrix @xmath199 $ ] consists of @xmath43 simultaneous samples of random vector @xmath18 inherit additional statistical properties from the matrix structure of @xmath24 $ ] to ensure their coalescence around the low - dimensional structure @xmath1 .",
    "let @xmath200 be the kernel defined on @xmath201 , depending on a real smoothing parameter @xmath71 , which verifies the following properties :    * @xmath202 ( symmetry ) .",
    "* @xmath203 ( positivity preserving ) .",
    "* @xmath204 is positive semi - definite .    a classical choice ( that we will use in section  [ section5 ] ) for the kernel that satisfies the above three properties is the gaussian kernel specified as , @xmath205 let @xmath206 $ ] be the symmetric matrix in @xmath207 with positive entries such that @xmath208_{ij } = k_\\varepsilon({{\\boldsymbol{\\eta}}}^{d , i } , { { \\boldsymbol{\\eta}}}^{d , j } ) \\quad ,",
    "\\quad i \\,\\,\\hbox{and } \\,\\ , j \\in \\{1,\\ldots , n\\}\\ , .                         \\ ] ]",
    "let @xmath209 $ ] be the positive - definite diagonal real matrix in @xmath207 such that @xmath210_{ij } = \\delta_{ij}\\,\\sum_{j'=1}^n [ k]_{ij ' } \\ , ,                                                                                         \\ ] ] and let @xmath211 $ ] be the matrix in @xmath207 such that @xmath212 = [ b]^{-1}\\ , [ k ] \\ , .                                                                                                                  \\ ] ] consequently , matrix @xmath211 $ ] has positive entries and satisfies",
    "@xmath213_{ij } = 1 $ ] for all @xmath214 .",
    "it can thus be viewed as the transition matrix of a markov chain that yields the probability of transition in one step .",
    "let @xmath215 $ ] be the symmetric matrix in @xmath207 such that @xmath216 = [ b]^{1/2}\\ , [ \\pp ] \\ , [ b]^{-1/2 } = [ b]^{-1/2}\\ , [ k ] \\ , [ b]^{-1/2 } \\ , .                                                               \\ ] ] we consider the eigenvalue problem @xmath215\\ , { { \\boldsymbol{\\phi}}}^\\alpha = \\lambda_\\alpha\\ , { { \\boldsymbol{\\phi}}}^\\alpha$ ] .",
    "let @xmath77 be an integer such that @xmath217 .",
    "it can easily be proved that the associated eigenvalues are real , positive , and such that @xmath218 let @xmath219 $ ] be the matrix in @xmath76 such that @xmath219^t\\ , [ \\phi ] = [ i_m]$ ] , whose columns are the @xmath77 orthonormal eigenvectors @xmath220 associated with @xmath221 .",
    "the eigenvalues of matrix @xmath211 $ ] are the same as the eigenvalues of matrix @xmath215 $ ] .",
    "the right eigenvectors @xmath222 associated with @xmath221 , which are such that @xmath211\\ , { { \\boldsymbol{\\psi}}}^\\alpha = \\lambda_\\alpha\\ , { { \\boldsymbol{\\psi}}}^\\alpha$ ] , are written as @xmath223^{-1/2 } \\ , { { \\boldsymbol{\\phi}}}^\\alpha \\in \\rr^n \\quad , \\quad \\alpha = 1,\\ldots , m \\ , ,                                               \\ ] ] and consequently , the matrix @xmath224 = [ { { \\boldsymbol{\\psi}}}^1 \\ldots { { \\boldsymbol{\\psi}}}^m ] = [ b]^{-1/2 } \\ , [ \\phi ] \\in\\mm_{n , m}$ ] is such that @xmath225^t\\ , [ b]\\ , [ \\psi ] = [ i_m ] \\ , ,                                                                                                           \\ ] ] which defines the normalization of the right eigenvectors of @xmath211 $ ] .",
    "+ we then define a `` diffusion - maps basis '' by @xmath75 = [ { { \\textbf{g}}}^{1 } \\ldots { { \\textbf{g}}}^m]\\in \\mm_{n , m}$ ] ( which is an algebraic basis of @xmath70 for @xmath226 ) such that @xmath227 in which @xmath72 is an integer that is chosen for fixing the analysis scale of the local geometric structure of the data set .",
    "it should be noted that the family @xmath228 of diffusion maps are defined @xcite by the vector @xmath229 in order to construct a diffusion distance , and integer @xmath72 is thus such that the probability of transition is in @xmath72 steps .",
    "however , as we have previously explained , we do not use such a diffusion distance , but we use the `` diffusion - maps basis '' @xmath230 that we have introduced for performing a projection of each column of the @xmath231-valued random matrix @xmath24^t$ ] on the subspace of @xmath70 , spanned by @xmath232 . introducing the random matrix",
    "@xmath80 $ ] with values in @xmath81 , we can then construct the following reduced - order representation of @xmath24 $ ] , @xmath233 = [ { { \\textbf{z}}}]\\ , [ g]^t \\ , .                                                                                                              \\ ] ] since the matrix @xmath75^t\\ , [ g ] \\in \\mm_m$ ] is invertible , eq .",
    "yields @xmath234 = [ { { \\textbf{h}}}]\\ , [ a ] \\quad , \\quad [ a ]   = [ g]\\ , ( [ g]^t\\ , [ g])^{-1 } \\in \\mm_{n , m}\\ , .                                                       \\ ] ] in particular , matrix @xmath64\\in \\mm_{\\nu , n}$ ] can be written as @xmath64 = [ z_d]\\ , [ g]^t$ ] in which the matrix @xmath235\\in \\mm_{\\nu , m}$ ] is written as @xmath236 = [ \\eta_d]\\ , [ a ]   \\in \\mm_{\\nu , m}\\ , .                                                                                                 \\ ] ]      because an estimation of the value of the order - reduction dimension @xmath77 must be known before beginning the generation of additional realizations of random matrix @xmath80 $ ] using the reduced - order representation of random matrix @xmath24 $ ] , we propose a methodology which is only based on the use of the known data set represented by matrix @xmath64 $ ] that is a realization of random matrix @xmath24 $ ] .",
    "+ for a given value of integer @xmath72 and for a given value of smoothing parameter @xmath71 , the decay of the graph @xmath237 of the eigenvalues of transition matrix @xmath211 $ ] , yields a criterion for choosing the value of @xmath77 that allows the local geometric structure of the data set represented by @xmath64 $ ] to be discovered .",
    "nevertheless , this criterion can be misleading as it does not capture statistical fluctuations around the embedded manifold",
    ". an additional mean - square convergence must be verified , and if necessary , the value of @xmath77 must be increased . however ,",
    "if the value of @xmath77 is chosen too large , the localization of the geometric structure of the data set is lost .",
    "consequently , a compromise must be applied between the very small value of @xmath77 given by the decreasing criteria of the eigenvalues of matrix @xmath211\\in\\mm_n$ ] and a larger value of @xmath77 which is necessary for obtaining a reasonable mean - square convergence .",
    "+ using eqs .   to allows for calculating the reduced - order representation @xmath238\\in\\mm_{\\nu , n}$ ] of @xmath64 $ ] such that @xmath238 = [ \\eta_d]\\,[a]\\,[g]^t$ ] in which @xmath239 $ ] and @xmath75 $ ] depend on @xmath77 .",
    "it should be noted that if @xmath226 , then @xmath239\\,[g]^t = [ i_n]$ ] and therefore , @xmath238 = [ \\eta_d]$ ] .",
    "in such a case , the `` reduced - order '' representation would correspond to a simple change of vector basis in @xmath70 and the localization of the geometric structure of the data set would be lost .",
    "this implies that @xmath77 must be much more less than @xmath43 for preserving the capability of the approach to localize the geometric structure of the data set , and must be chosen as the smallest possible value that yields a reasonable mean - square convergence .",
    "let @xmath240\\in\\mm_{n , n}$ ] be the matrix @xmath55 $ ] of the data set , calculated using eq .   with @xmath238 $ ] .",
    "we then have @xmath241 = [ \\underline x ] + [ \\varphi]\\ , [ \\mu]^{1/2}\\ , [ \\eta_d]\\,[a]\\,[g]^t \\ , .                                                               \\ ] ] let @xmath242 be the @xmath43 vectors in @xmath0 , which constitute the columns of matrix @xmath240\\in\\mm_{n , n}$ ] .",
    "we then introduced the empirical estimates @xmath243 and @xmath244\\in\\mm_n$ ] of the mean value and the covariance matrix calculated with the realization @xmath240\\in\\mm_{n , n}$ ] such that @xmath245 @xmath246 = \\frac{1}{n-1 } \\sum_{j=1}^n ( { { \\textbf{x}}}_{{\\hbox{{\\ppppcarac red}}}}^j(m)-{{\\textbf{m}}}_{{\\hbox{{\\ppppcarac red}}}})\\ , ( { { \\textbf{x}}}_{{\\hbox{{\\ppppcarac red}}}}^j(m ) -{{\\textbf{m}}}_{{\\hbox{{\\ppppcarac red}}}})^t \\ , .                                          \\ ] ] the mean - square convergence criterion is then defined by @xmath247 - [ c ] \\vert_f}{\\vert [ c ] \\vert_f } \\ ,   .                                                             \\ ] ] in which @xmath90 $ ] is defined by eq .  .",
    "since @xmath248 = [ x_d]$ ] , it can be deduced that @xmath249 when @xmath77 goes to @xmath43 . for",
    "a fixed reasonable value @xmath250 of the relative tolerance @xmath251 , an estimate of @xmath77 will consist in looking for the smallest value of @xmath77 such that @xmath252 .",
    "an illustration of the use of this criterion will be given in the third application presented in section  [ section5.3 ] .",
    "for @xmath77 , @xmath253 , and @xmath72 fixed , the reduced - order representation @xmath24 = [ { { \\textbf{z}}}]\\ , [ g]^t$ ] of random matrix @xmath24 $ ] , defined by eq .  , is used for constructing the reduced - order isde associated with eqs .   to . introducing the change of stochastic processes @xmath169=[{{\\boldsymbol{\\mathcal{z}}}}(r)]\\,[g]^t$ ] and @xmath254 = [ { { \\boldsymbol{\\mathcal{y}}}}(r)]\\,[g]^t$ ] into these equations , then right multiplying the obtained equations by matrix @xmath239 $ ] , and taking into account eq .",
    ", it can be seen that @xmath255,[{{\\boldsymbol{\\mathcal{y}}}}(r ) ] ) , r\\in \\rr^+ \\}$ ] is a markov stochastic process defined on the probability space @xmath129 @xmath130 , indexed by @xmath131 , with values in @xmath256 , satisfying , for all @xmath133 , the following reduced - order isde , @xmath257 =   [ { { \\boldsymbol{\\mathcal{y}}}}(r ) ] \\ , dr \\ , ,                                                                                                      \\ ] ] @xmath258=   [ { { \\mathcal{l}}}([{{\\boldsymbol{\\mathcal{z}}}}(r)])]\\ , dr -\\frac{1}{2 } f_0\\ , [ { { \\boldsymbol{\\mathcal{y}}}}(r)]\\ , dr + \\sqrt{f_0}\\ , [ d{{\\boldsymbol{\\mathcal{w}}}}(r ) ] \\ , ,                             \\ ] ] with the initial condition @xmath259 = [ { { \\textbf{h}}}_d]\\ , [ a ] \\quad , \\quad [ { { \\boldsymbol{\\mathcal{y}}}}(0 ) ] = [ { { \\boldsymbol{\\mathcal{n}}}}\\ , ] \\ , [ a]\\quad a.s \\ , ,                                                   \\ ] ] in which the random matrices @xmath260)]$ ] and @xmath261 $ ] with values in @xmath81 are such that @xmath262)]= [ l ( [ { { \\boldsymbol{\\mathcal{z}}}}(r ) ] \\ , [ g]^t ) ] \\ , [ a]\\ , ,                                                                              \\ ] ] @xmath263= [ d{{\\textbf{w}}}(r ) ] \\ , [ a]\\ , .                                                                                                   \\ ] ] from section  [ section4.3 ] , it can be deduced that the problem defined by eqs .   to admits a unique invariant measure and a unique solution @xmath255,[{{\\boldsymbol{\\mathcal{y}}}}(r)]),$ ] @xmath165 that is a second - order diffusion stochastic process , which is stationary ( for the shift semi - group on @xmath143 ) and ergodic .    for @xmath182 fixed in @xmath183 , the deterministic quantities @xmath264,r\\geq 0\\}$ ] , @xmath265   = [ \\eta_d ] \\ , [ a]$ ] , and @xmath266 = [ { { \\boldsymbol{\\mathcal{n}}}}(\\theta)]\\ , [ a]$ ] are independent realizations of the stochastic process @xmath267,r\\geq 0\\}$ ] , the random matrix @xmath268 $ ] , and the random matrix @xmath269 .",
    "let @xmath270,[{{\\boldsymbol{\\mathcal{y}}}}(r;\\theta ) ] ) , r\\in \\rr^+\\}$ ] be the corresponding realization of the unique stationary diffusion process @xmath255,[{{\\boldsymbol{\\mathcal{y}}}}(r ) ] ) , r\\in \\rr^+\\}$ ] of the reduced - order isde problem defined by eqs .   to ) . then , using eq .",
    ", some additional realizations @xmath190 , \\ldots , [ \\eta_s^{n_{{\\hbox{{\\pppppcarac mc}}}}}]$ ] of random matrix @xmath24 $ ] can be generated by @xmath271   = [ { { \\boldsymbol{\\mathcal{z}}}}(\\ell \\rho ; \\theta ) ] \\ , [ g]^t\\quad , \\quad \\rho = m_0\\ , \\delta r \\quad , \\quad \\ell = 1,\\ldots n_{{\\hbox{{\\pppppcarac mc}}}}\\ , ,                \\ ] ] and using eq .",
    ", some additional realizations @xmath83 , \\ldots , [ x_s^{n_{{\\hbox{{\\pppppcarac mc}}}}}]$ ] of random matrix @xmath23 $ ] can be generated ( using the reduced - order representation defined by eq .  ) by @xmath272 = [ \\underline x ] + [ \\varphi]\\ , [ \\mu]^{1/2}\\ , [ \\eta_s^\\ell]\\quad , \\quad \\ell = 1,\\ldots n_{{\\hbox{{\\pppppcarac mc}}}}\\ , .                                   \\ ] ]      for numerically solving the reduced - order isde defined by eqs .",
    "to , a discretization scheme must be used . for general surveys on discretization schemes for it stochastic differential equations ,",
    "we refer the reader to @xcite .",
    "concerning the particular cases related to hamiltonian dynamical systems ( which have also been analyzed in @xcite using an implicit euler scheme ) , we propose to use the strmer - verlet scheme , which is a very efficient scheme that preserves energy for nondissipative hamiltonian dynamical systems ( see @xcite for reviews about this scheme in the deterministic case , and see @xcite and the references therein for the stochastic case ) .",
    "we then propose to reuse hereinafter the strmer - verlet scheme , introduced and validated in @xcite for weakly dissipative stochastic hamiltonian dynamical system .",
    "let @xmath273 be the positive integer in which @xmath274 and @xmath193 have been introduced in remark  5 of section  [ section4.3 ] .",
    "the reduced - order it stochastic differential equation defined by eqs .   and with the initial condition defined by eq .  ,",
    "is solved on the finite interval @xmath275 $ ] , in which @xmath192 is the sampling step of the continuous index parameter @xmath168 .",
    "the integration scheme is based on the use of the @xmath276 sampling points @xmath277 such that @xmath278 for @xmath279 .",
    "the following notations are introduced : @xmath280 = [ { { \\boldsymbol{\\mathcal{z}}}}(r_\\ell)]$ ] , @xmath281 = [ { { \\boldsymbol{\\mathcal{y}}}}(r_\\ell)]$ ] , and @xmath282 = [ { { \\boldsymbol{\\mathcal{w}}}}(r_\\ell)]$ ] , for @xmath283 , with @xmath284 = [ { { \\textbf{h}}}_d]\\ , [ a ]   \\quad , \\quad [ { { \\boldsymbol{\\mathcal{y}}}}_0 ] = [ { { \\boldsymbol{\\mathcal{n}}}}\\ , ] \\ , [ a ] \\quad , \\quad [ { { \\boldsymbol{\\mathcal{w}}}}_0 ] = [ 0_{\\nu , m } ]   \\quad a.s     \\ , .        \\ ] ] for @xmath285 , let @xmath286 = [ \\delta{{\\textbf{w}}}_{\\ell+1}]\\ , [ a]$ ] be the sequence of random matrices with values in @xmath81 , in which @xmath287= [ { { \\textbf{w}}}_{\\ell+1 } ] - [ { { \\textbf{w}}}_\\ell]$ ] .",
    "the increments @xmath288 , \\ldots , [ \\delta{{\\textbf{w}}}_{m}]$ ] are @xmath289 independent random matrices . for all @xmath290 and for all @xmath52 ,",
    "the real - valued random variables @xmath291_{kj}\\}_{kj}$ ] are independent , gaussian , second - order , and centered random variables such that @xmath292_{kj}[\\delta{{\\textbf{w}}}_{\\ell+1}]_{k'j'}\\}=\\delta r \\,\\delta_{kk'}\\,\\delta_{jj'}$ ] . for @xmath293 , the strmer - verlet scheme applied to eqs .   and yields @xmath294      =     [ { { \\boldsymbol{\\mathcal{z}}}}_\\ell ] + \\frac{\\delta r}{2 } \\ , [ { { \\boldsymbol{\\mathcal{y}}}}_\\ell ] \\",
    ", ,                                             \\ ] ] @xmath295    =     \\frac{1-b}{1+b}\\ , [ { { \\boldsymbol{\\mathcal{y}}}}_\\ell ] + \\frac{\\delta r}{1+b}\\ , [ { { \\boldsymbol{\\mathcal{l}}}}_{\\ell+\\frac{1}{2 } } ] +         \\frac{\\sqrt{f_0}}{1+b}\\ , [ \\delta{{\\boldsymbol{\\mathcal{w}}}}_{\\ell+1}]\\ , ,                                                                               \\ ] ] @xmath296   =     [ { { \\boldsymbol{\\mathcal{z}}}}_{\\ell+\\frac{1}{2 } } ] + \\frac{\\delta r}{2 } \\ , [ { { \\boldsymbol{\\mathcal{y}}}}_{\\ell+1}]\\ , ,                                      \\ ] ] with the initial condition defined by , where @xmath297 , and where @xmath298 $ ] is the @xmath81-valued random variable such that @xmath299   = [ { { \\mathcal{l } } } ( [ { { \\boldsymbol{\\mathcal{z}}}}_{\\ell+\\frac{1}{2 } } ] ) ] = [ l ( [ { { \\boldsymbol{\\mathcal{z}}}}_{\\ell+\\frac{1}{2}}]\\ , [ g]^t ) ] \\ , [ a]\\ , ,         \\ ] ] in which , for all @xmath22 = [ { { \\textbf{u}}}^1 \\ldots { { \\textbf{u}}}^n]$ ] in @xmath57 with @xmath137 in @xmath62 , the entries of matrix @xmath138)]$ ] in @xmath57 are defined by eqs .   and .",
    "some estimations of the values of the parameters @xmath300 , and @xmath193 , which are used in the discretization scheme of the isde ( with and without reduced - order representation of random matrix @xmath24 $ ] , and introduced in sections  [ section4.3 ] and [ section4.7.1 ] ) are described below .",
    "+ ( i ) parameter @xmath192 is written as @xmath301 in which @xmath302 is an oversampling that has to be estimated for getting a sufficient accuracy of the strmer - verlet scheme ( for instance , @xmath303 ) .",
    "this means that a convergence analysis of the solution must be carried out with respect to @xmath304 .",
    "+ ( ii ) as the accuracy of the strmer - verlet scheme is finite , a small numerical integration error is unavoidably introduced .",
    "although that the initial conditions are chosen in order to directly construct the stationary solution ( associated with the unique invariant measure ) , a small transient response can occur and be superimposed to the stationary stochastic solution .",
    "therefore , @xmath170 is chosen in order that the damping in the dissipative hamiltonian system is sufficiently large to rapidly kill such a small transient response ( a typical value that is retained in the applications presented in section  [ section5 ] is @xmath305 ) . + ( iii ) using an estimation of the relaxation time of the underlying linear second - order dynamical system , and choosing an attenuation of @xmath306 for the transient response , parameter @xmath193 must be chosen larger than @xmath307 .",
    "a typical value that is retained in the applications presented in section  [ section5 ] is @xmath308 or @xmath309 ) .",
    "three applications are presented for random vector @xmath17 with values in @xmath0 for which :    * the dimension is @xmath310 and there are @xmath311 given data points in subset @xmath1 , for which the mean value is made up of two circles in the plane ) . * the dimension is @xmath312 and there are @xmath313 given data points in subset @xmath1 , for which the mean value is made up of a helix in three - dimensional space ) . * the third example corresponds to a petro - physics database that is made up of experimental measurements ( downloaded from @xcite ) and detailed in @xcite , for which the dimension is @xmath314 and for which @xmath315 given data points are concentrated in an unknown `` complex '' subset @xmath1 of @xmath0 , which can not be easily described once it is discovered .      for this first application ,",
    "two cases are considered : small ( case 1.1 ) and medium ( case 1.2 ) statistical fluctuations around the two circles . for every case ,",
    "the number of given data points is @xmath311 , no scaling of data is performed , but the normalization defined in section  [ section4.1 ] is done and yields @xmath316 . in figs .",
    "[ figure1 ] to [ figure5 ] , the left figures are relative to case 1.1 and the right ones to case 1.2 . fig .",
    "[ figure1 ] displays the @xmath3 given data points for random vector @xmath317 of the data set represented by matrix @xmath318 $ ] in @xmath319 , and shows that the given data points are concentrated in the neighborhood of two circles , with small ( case 1.1 ) and medium ( case 1.2 ) statistical fluctuations .",
    "given data points : case 1.1 ( left ) , case 1.2 ( right).,title=\"fig:\",width=207 ]   given data points : case 1.1 ( left ) , case 1.2 ( right).,title=\"fig:\",width=207 ]    -scale of the transition matrix for random vector @xmath18 : case 1.1 ( left ) , case 1.2 ( right).,title=\"fig:\",width=207 ] -scale of the transition matrix for random vector @xmath18 : case 1.1 ( left ) , case 1.2 ( right).,title=\"fig:\",width=207 ]     ( solid line ) and @xmath320 ( dashed line ) obtained by a nonparametric estimation from data points : case 1.1 ( left ) , case 1.2 ( right).,title=\"fig:\",width=207 ]   ( solid line ) and @xmath320 ( dashed line ) obtained by a nonparametric estimation from data points : case 1.1 ( left ) , case 1.2 ( right).,title=\"fig:\",width=207 ]     given data points ( blue symbols ) and @xmath321 additional realizations ( red symbols ) generated using the reduced - order isde with @xmath322 : case 1.1 ( left ) , case 1.2 ( right).,title=\"fig:\",width=207 ]   given data points ( blue symbols ) and @xmath321 additional realizations ( red symbols ) generated using the reduced - order isde with @xmath322 : case 1.1 ( left ) , case 1.2 ( right).,title=\"fig:\",width=207 ]     given data points ( blue symbols ) and @xmath321 additional realizations ( red symbols ) generated using the isde : case 1.1 ( left ) , case 1.2 ( right).,title=\"fig:\",width=207 ]   given data points ( blue symbols ) and @xmath321 additional realizations ( red symbols ) generated using the isde : case 1.1 ( left ) , case 1.2 ( right).,title=\"fig:\",width=207 ]    the kernel is defined by eq .  , the value of the smoothing parameter that is retained is @xmath323 , @xmath72 is chosen to @xmath324 , and the graph of the eigenvalues of the transition matrix for random vector @xmath18 is displayed in fig .",
    "[ figure2 ] .",
    "these two graphs show that dimension @xmath77 can be chosen to @xmath4 , and for @xmath322 , the value of @xmath251 ( defined by eq .  )",
    "is @xmath325 for case 1.1 and @xmath326 for case 1.2 .",
    "it can thus be considered that a reasonable mean - square convergence is reached for these two cases .",
    "[ figure3 ] displays the pdf for random variables @xmath327 and @xmath320 computed with a nonparametric estimation from the data points . for all the computation ,",
    "the numerical values of the parameters for generating @xmath321 additional realizations are @xmath328 , @xmath329 , and @xmath330 , yielding @xmath331 .",
    "the results obtained with the reduced - order isde ( for which the first @xmath322 vectors of the diffusion - maps basis are used ) are displayed in fig .",
    "[ figure4 ] , which shows the @xmath3 given data points and the @xmath321 additional realizations generated using the reduced - order isde .",
    "it can be seen that the additional realizations are effectively concentrated in subset @xmath1 .",
    "[ figure5 ] displays the @xmath3 given data points and the @xmath321 additional realizations generated using a direct simulation of the isde presented in section  [ section4.3 ] .",
    "it can be seen that the realizations are not concentrated in subset @xmath1 , but are scattered .      as previously",
    ", two cases are considered : small ( case 2.1 ) and medium ( case 2.2 ) statistical fluctuations around the helical . for every case ,",
    "the number of given data points is @xmath313 , no scaling of data is performed , but the normalization defined in section  [ section4.1 ] is done and yields @xmath332 . in figs .",
    "[ figure6 ] to [ figure10 ] , the left figures are relative to case 2.1 and the right ones to case 2.2 . fig .",
    "[ figure6 ] displays the @xmath5 given data points for random vector @xmath333 of the data set represented by matrix @xmath318 $ ] in @xmath334 .",
    "[ figure6 ] shows that the given data points are concentrated in the neighborhood of the helical , with small ( case 2.1 ) and medium ( case 2.2 ) statistical fluctuations .",
    "given data points : case 2.1 ( left ) , case 2.2 ( right).,title=\"fig:\",width=207 ]   given data points : case 2.1 ( left ) , case 2.2 ( right).,title=\"fig:\",width=207 ]    -scale of the transition matrix for random vector @xmath18 : case 2.1 ( left ) , case 2.2 ( right).,title=\"fig:\",width=207 ] -scale of the transition matrix for random vector @xmath18 : case 2.1 ( left ) , case 2.2 ( right).,title=\"fig:\",width=207 ]     ( solid line ) , @xmath320 ( dashed line ) , and @xmath335 ( dotted line ) obtained by a nonparametric estimation from data points : case 2.1 ( left ) , case 2.2 ( right).,title=\"fig:\",width=207 ]   ( solid line ) , @xmath320 ( dashed line ) , and @xmath335 ( dotted line ) obtained by a nonparametric estimation from data points : case 2.1 ( left ) , case 2.2 ( right).,title=\"fig:\",width=207 ]     given data points ( blue symbols ) and @xmath336 additional realizations ( red symbols ) generated using the reduced - order isde with @xmath337 : case 2.1 ( left ) , case 2.2 ( right).,title=\"fig:\",width=207 ]   given data points ( blue symbols ) and @xmath336 additional realizations ( red symbols ) generated using the reduced - order isde with @xmath337 : case 2.1 ( left ) , case 2.2 ( right).,title=\"fig:\",width=207 ]     given data points ( blue symbols ) and @xmath336 additional realizations ( red symbols ) generated using the isde : case 2.1 ( left ) , case 2.2 ( right).,title=\"fig:\",width=207 ]   given data points ( blue symbols ) and @xmath336 additional realizations ( red symbols ) generated using the isde : case 2.1 ( left ) , case 2.2 ( right).,title=\"fig:\",width=207 ]    the kernel is defined by eq .  , the value of the smoothing parameter that is retained is @xmath338 , @xmath72 is chosen to @xmath324 , and the graph of the eigenvalues of the transition matrix for random vector @xmath18 is displayed in fig .",
    "[ figure7 ] .",
    "these two graphs show that dimension @xmath77 can be chosen to @xmath339 , and for @xmath337 , the value of @xmath251 ( defined by eq .  )",
    "is @xmath340 for case 2.1 and @xmath341 for case 2.2 .",
    "it can thus be considered that a reasonable mean - square convergence is reached for these two cases .",
    "[ figure8 ] displays the pdf for random variables @xmath327 , @xmath320 , and @xmath335 computed with a nonparametric estimation from the data points .",
    "for all the computation , the numerical values of the parameters for generating @xmath321 additional realizations are @xmath342 , @xmath329 , and @xmath343 , yielding @xmath344 .",
    "the results obtained with the reduced - order isde ( for which the first @xmath337 vectors of the diffusion - maps basis are used ) are displayed in fig .",
    "[ figure9 ] , which shows the @xmath5 given data points and the @xmath336 additional realizations generated using the reduced - order isde .",
    "it can be seen that the additional realizations are effectively concentrated in subset @xmath1 .",
    "[ figure10 ] displays the @xmath5 given data points and the @xmath336 additional realizations generated using a direct simulation with the isde presented in section  [ section4.3 ] .",
    "it can be seen that the realizations are not concentrated in subset @xmath1 , but are scattered .",
    "given data points viewed from coordinates @xmath345 and @xmath346 ( up left ) , viewed from coordinates @xmath347 and @xmath346 ( up right ) , and viewed from coordinates @xmath348 , @xmath349 , and @xmath350 ( down).,title=\"fig:\",width=166 ]   given data points viewed from coordinates @xmath345 and @xmath346 ( up left ) , viewed from coordinates @xmath347 and @xmath346 ( up right ) , and viewed from coordinates @xmath348 , @xmath349 , and @xmath350 ( down).,title=\"fig:\",width=158 ]   given data points viewed from coordinates @xmath345 and @xmath346 ( up left ) , viewed from coordinates @xmath347 and @xmath346 ( up right ) , and viewed from coordinates @xmath348 , @xmath349 , and @xmath350 ( down).,title=\"fig:\",width=181 ]    -scale of the transition matrix for random vector @xmath18 ( left ) .",
    "graph @xmath351 in @xmath352 scale ( right).,title=\"fig:\",width=188 ] -scale of the transition matrix for random vector @xmath18 ( left ) .",
    "graph @xmath351 in @xmath352 scale ( right).,title=\"fig:\",width=188 ]     obtained by a nonparametric estimation from the data points and the simulated data points .",
    "right figures : @xmath7 given data points ( blue symbols ) and @xmath353 additional realizations ( red symbols ) generated using the reduced - order representation of @xmath24 $ ] with @xmath354 , viewed from different components of random vector @xmath17.,title=\"fig:\",width=207 ]   obtained by a nonparametric estimation from the data points and the simulated data points .",
    "right figures : @xmath7 given data points ( blue symbols ) and @xmath353 additional realizations ( red symbols ) generated using the reduced - order representation of @xmath24 $ ] with @xmath354 , viewed from different components of random vector @xmath17.,title=\"fig:\",width=188 ] +   obtained by a nonparametric estimation from the data points and the simulated data points .",
    "right figures : @xmath7 given data points ( blue symbols ) and @xmath353 additional realizations ( red symbols ) generated using the reduced - order representation of @xmath24 $ ] with @xmath354 , viewed from different components of random vector @xmath17.,title=\"fig:\",width=207 ]   obtained by a nonparametric estimation from the data points and the simulated data points .",
    "right figures : @xmath7 given data points ( blue symbols ) and @xmath353 additional realizations ( red symbols ) generated using the reduced - order representation of @xmath24 $ ] with @xmath354 , viewed from different components of random vector @xmath17.,title=\"fig:\",width=188 ] +   obtained by a nonparametric estimation from the data points and the simulated data points .",
    "right figures : @xmath7 given data points ( blue symbols ) and @xmath353 additional realizations ( red symbols ) generated using the reduced - order representation of @xmath24 $ ] with @xmath354 , viewed from different components of random vector @xmath17.,title=\"fig:\",width=207 ]   obtained by a nonparametric estimation from the data points and the simulated data points .",
    "right figures : @xmath7 given data points ( blue symbols ) and @xmath353 additional realizations ( red symbols ) generated using the reduced - order representation of @xmath24 $ ] with @xmath354 , viewed from different components of random vector @xmath17.,title=\"fig:\",width=188 ]     given data points ( blue symbols ) and @xmath353 additional realizations ( red symbols ) generated without using the reduced - order representation , viewed from different components of random vector @xmath17.,title=\"fig:\",width=166 ]   given data points ( blue symbols ) and @xmath353 additional realizations ( red symbols ) generated without using the reduced - order representation , viewed from different components of random vector @xmath17.,title=\"fig:\",width=158 ]   given data points ( blue symbols ) and @xmath353 additional realizations ( red symbols ) generated without using the reduced - order representation , viewed from different components of random vector @xmath17.,title=\"fig:\",width=181 ]    the data base used corresponds to a petro - physics data base of experimental experiments .",
    "the dimension of random vector @xmath17 is @xmath314 and the number of given data points is @xmath315 . the scaling and the normalization defined in section  [ section4.1 ] are necessary , have been done , and yield @xmath355 .",
    "[ figure11 ] displays @xmath7 given data points viewed from coordinates @xmath345 and @xmath346 , from coordinates @xmath347 and @xmath346 , and from coordinates @xmath348 , @xmath349 , and @xmath350 .",
    "although only a partial representation of the @xmath7 data points for the @xmath0-valued random vector @xmath17 is given , this figure shows that @xmath1 is certainly a complex subset of @xmath0 .",
    "the kernel is defined by eq .",
    ", the value of the smoothing parameter that has been used is @xmath356 , and @xmath72 has also been chosen to @xmath324 .",
    "the graph of the eigenvalues ( of the transition matrix relative to random vector @xmath18 ) displayed in fig .",
    "[ figure12 ] ( left ) shows that the value @xmath357 could potentially be a good choice for the value of @xmath77 .",
    "however , for @xmath357 , the value of @xmath251 is @xmath358 that shows that the mean - square convergence is not reached .",
    "consequently , an analysis has been performed in constructing the graph of function @xmath359 in order to identify the smallest value of @xmath77 for which the mean - square convergence is reasonably reached .",
    "the graph displays in fig .",
    "[ figure12 ] ( right ) clearly shows that a good choice is @xmath354 for which the value of @xmath251 is @xmath360 that can thus be considered as a reasonable mean - square convergence .",
    "for all the computation , the numerical values of the parameters for generating @xmath353 additional realizations are @xmath361 , @xmath362 , and @xmath363 , yielding @xmath364 . + for the same coordinates that those introduced in fig .",
    "[ figure11 ] , the left figures in fig .",
    "[ figure13 ] display the pdf of the considered components of random vector @xmath17 obtained by a nonparametric estimation from the data points and the simulated data points obtained with the reduced - order isde , and the right figures display the @xmath7 given data points and the @xmath353 additional realizations generated using the reduced - order isde using the first @xmath354 vectors of the diffusion - maps basis .",
    "it can be seen that the additional realizations are effectively concentrated in subset @xmath1 .",
    "[ figure15 ] displays the @xmath7 given data points and the @xmath353 additional realizations generated using a direct simulation with the isde presented in section  [ section4.3 ] .",
    "it can be seen that the realizations are not concentrated in subset @xmath1 , but are scattered . in particular , the positivity of random variable @xmath365 is not satisfied .",
    "a new methodology has been presented and validated for generating realizations of an @xmath0-valued random vector , for which the probability distribution is unknown and is concentrated on an unknown subset @xmath1 of @xmath0 .",
    "both the probability distribution and the subset @xmath1 are constructed to be statistically consistent with a specified data set construed as providing initial realizations of the random vector .",
    "the proposed method is robust and can be used for high dimension and for large initial data sets .",
    "it is expected that the proposed method will contribute to open new possibilities of developments in many areas of uncertainty quantification and statistical data analysis , in particular in the design of experiments for random parameters .",
    "part of this research was supported by the u.s .",
    "department of energy office of advanced scientific computing research .",
    "coifman , s. lafon , a.b .",
    "lee , m. maggioni , .",
    "nadler , f. warner , s.w .",
    "zucker , geometric diffusions as a tool for harmonic analysis and structure definition of data : diffusion maps , pnas 102(21 ) ( 2005 ) 7426 - 7431 .",
    "g. perrin , c. soize , d. duhamel , c. funfschilling , karhunen - love expansion revisited for vector - valued random fields : scaling , errors and optimal basis , journal of computational physics 242(1 ) ( 2013 ) 607 - 622 .",
    "r. ghanem , c. soize , remarks on stochastic properties of materials through finite deformations , international journal for multiscale computational engineering , doi : 10.1615/intjmultcompeng.2015013959 , in press ( 2015 ) .",
    "c. thimmisetty , a. khodabakhshnejad , n. jabbari , f. aminzadeh , r. ghanem , k. rose , j. bauer , c. disenhof , multiscale stochastic representation in high - dimensional data using gaussian processes with implicit diffusion metrics , lecture notes in computer science , vol .",
    "8964 , 2015 ( proceedings of the dynamic data - driven environmental systems science conference , mit , cambridge , ma , nov 5 - 7 , 2014 . )              c. soize , construction of probability distributions in high dimension using the maximum entropy principle .",
    "applications to stochastic processes , random fields and random matrices , international journal for numerical methods in engineering 76(10 ) ( 2008 ) 1583 - 1611 .",
    "r. khasminskii , stochastic stability of differential equations , series : stochastic modelling and applied probability , vol .",
    "66 , 2nd edition , springer , heidelberg , 2012 . originally published in russian , by nauka ,",
    "moskow , 1969 .",
    "first english edition published in 1980 under r.z .",
    "hasminski in the series mechanics : analysis by sijthoff & noordhoff .",
    "d. talay , simulation and numerical analysis of stochastic differential systems , pp .",
    "54 - 96 , in probabilistic methods in applied physics , lecture notes in physics , 451 , p. kree and w. wedig , eds . , springer - verlag , heidelberg , 1995 .",
    "c. soize , i.e. poloskov , time - domain formulation in computational dynamics for linear viscoelastic media with model uncertainties and stochastic excitation , _ computers and mathematics with applications _ , doi:10.1016/j.camwa . 2012.09.010 , * 64*(11 ) , 3594 - 3612 ( 2012 ) .",
    "j. guilleminot , c. soize , stochastic model and generator for random fields with symmetry properties : application to the mesoscopic modeling of elastic random media , multiscale modeling and simulation ( a siam interdisciplinary journal ) 11(3 ) ( 2013 ) 840 - 870 ."
  ],
  "abstract_text": [
    "<S> a new methodology is proposed for generating realizations of a random vector with values in a finite - dimensional euclidean space that are statistically consistent with a data set of observations of this vector . </S>",
    "<S> the probability distribution of this random vector , while a - priori not known , is presumed to be concentrated on an unknown subset of the euclidean space . </S>",
    "<S> a random matrix is introduced whose columns are independent copies of the random vector and for which the number of columns is the number of data points in the data set . </S>",
    "<S> the approach is based on the use of ( i ) the multidimensional kernel - density estimation method for estimating the probability distribution of the random matrix , ( ii ) a mcmc method for generating realizations for the random matrix , ( iii ) the diffusion - maps approach for discovering and characterizing the geometry and the structure of the data set , and ( iv ) a reduced - order representation of the random matrix , which is constructed using the diffusion - maps vectors associated with the first eigenvalues of the transition matrix relative to the given data set . </S>",
    "<S> the convergence aspects of the proposed methodology are analyzed and a numerical validation is explored through three applications of increasing complexity . the proposed method is found to be robust to noise levels and data complexity as well as to the intrinsic dimension of data and the size of experimental data sets . both the methodology and the underlying mathematical framework presented in this paper contribute new capabilities and perspectives at the interface of uncertainty quantification , statistical data analysis , stochastic modeling and associated statistical inverse problems .    </S>",
    "<S> concentration of probability , measure concentration , probability distribution on manifolds , random sampling generator , mcmc generator , diffusion maps , statistics on manifolds , design of experiments for random parameters </S>"
  ]
}