{
  "article_text": [
    "the purpose of this paper is to outline a novel bayesian approach to estimate the threshold of a generalised pareto distribution ( gpd ) by means of data dependent priors on the order statistics .",
    "the statistical model for the overall sample is a mixture model with two main components : a model for the non - extreme data below a certain threshold , also labelled as the _ bulk data _ , and the gpd to model the extreme values above the threshold .",
    "the component for the bulk data do not represent our main concern ; therefore , we will be using a finite mixture of densities where the components will somehow reflect the nature of the phenomenon of interest @xcite .",
    "in particular , a mixture of gamma densities if we are interested in positive data ( e.g. insurance losses , river floods , rainfall ) , and a mixture of normal densities for data that can take both positive and negative values ( e.g. financial returns ) .",
    "the second component of the overall model is a gpd where the threshold parameter @xmath0 , conceptually separating non - extreme from extreme observations , has an assigned uncertainty represented by a prior probability distribution .",
    "the details of the overall model will be discussed in section [ sc_model ] .",
    "+ the idea behind extreme value theory is that the main interest is in the tail ( or tails ) of a distribution . in areas such as finance , insurance , environmental sciences and engineering , the focus is often on observations that present a clear difference in value from the bulk data . due to this extremal nature of some observations , a distribution that model",
    "the whole data would not be appropriate as the majority of observations used to estimate the parameters are non - extreme .",
    "it is then necessary to use an appropriate procedure that , whilst still allowing for a reasonable inference of the bulk data , permits a precise estimate of the main characteristics of the tail observations .",
    "depending on the area of application , justifications of the adoption of extreme value distributions can be found , for example , in @xcite for finance , @xcite for insurance and actuarial science ; or , to cover a wider range of applications , including environmental sciences and engineering , refer to @xcite and @xcite .    to set the scene , suppose we have observed the sample @xmath1 from a model with distribution function @xmath2 .",
    "under some specific conditions @xcite , the distribution of @xmath3 above a certain value @xmath0 can be approximated by a gpd with distribution function    @xmath4    where @xmath5 is the scale parameter and @xmath6 is the shape parameter .",
    "the support for is @xmath7 for @xmath8 and @xmath9 for @xmath10 .",
    "although later the case @xmath10 will be briefly mentioned , the main focus of the paper is for @xmath11 , where the gpd presents a heavy - tailed behaviour .",
    "the assessment of the threshold @xmath0 is critical .",
    "in fact , if its value is not large enough , the resulting model would be incorrect as the asymptotic tail approximation discussed in @xcite is no longer valid .",
    "on the other hand , if the value @xmath0 is too high , then the number of observations above it would not be sufficient to have reasonably precise estimates of the parameters @xmath6 and @xmath12 .",
    "+ the idea of using order statistics to identify the threshold of a gpd is not new . in the context of a bayesian predictive approach",
    ", @xcite assigns a discrete prior to the number of upper order statistics , that is to the number of observations that could be classified as excedances .",
    "what we propose here has a different flavour , and it assumes that the threshold corresponds to one of the observed data points . the detailed motivations for a discrete prior for the threshold of a gpd will be given in section [ sc_model ] , when the overall statistical model is introduced .",
    "in short , if the data are modelled by a mixture of two components , one for the data below the threshold and one for the data above the threshold , then the assumption of having the threshold coinciding with one of the order statistics is sensible for the following two reasons : there is no evidence about the threshold value between any two consecutive order statistics , and the contribution of each excedance to the gpd likelihood is maximised when the threshold lies on an order statistic .",
    "we propose two different criteria to define a prior distribution on the order statistics .",
    "the first one assigns equal mass to each order statistics , and the second one assigns a mass which depends on the _ worth _ that each order statistics , as the potential threshold , has as being part of the model @xcite . although the proposed priors tend to yield posterior distributions with similar frequentist properties , we discuss some situations and reasons where either one or the other have to be preferred .",
    "in addition , although for different reasons , the proposed prior distributions can be categorized as _ objective _ , as defined in @xcite , and are suitable to be employed in scenarios of minimal prior information .",
    "+ the outline of the paper is as follows . in section [ sc_model ]",
    "we discuss the details of the mixture model for the whole data set and the prior distributions for the parameters . for the priors , we place the main focus on the prior distributions we propose for the threshold of the gpd .",
    "we then conduct simulation studies in section [ sc_simul ] by first illustrating how the proposed priors for @xmath0 apply to a single independent and identically distributed sample , and then by analysing the frequentist performances of the respective induced posterior distributions . section [ sc_real ] is dedicated to applying the defined model and the proposed prior distributions for the threshold to real data examples ; in particular , we analyse the well known data set of the danish fire loss , and the daily increments of the nasdaq-100 index over a period of more than seventeen years .",
    "finally , the concluding discussion and remarks are presented in section [ sc_disc ] .",
    "the model considered in this paper has two components : a finite mixture of parametric distributions for the data below the threshold @xmath0 , and a gpd for the data above the threshold .",
    "if we represent the distribution function for the bulk data by @xmath13 , the distribution function for the whole set of observations is given by    @xmath14g(x|\\xi,\\sigma,\\theta ) & x\\geq \\theta ,                      \\end{cases}\\ ] ]    where @xmath15 is the distribution defined in , and @xmath16 represents the parameters of the mixture for the bulk data .",
    "more in general , the mixture model in can be categorised on the basis of the nature of @xmath13 : parametric bulk model , semiparametric bulk model and nonparametric bulk model @xcite .",
    "an example of the first type consider a gamma distribution for the bulk data @xcite .",
    "of course , other parametric distribution can be considered , such as the normal , the lognormal or the weibull , so to reflect a different nature of the data . the main drawback of parametric bulk models is the lack of flexibility , resulting in a difficult identification of the threshold , except when the processes generating the bulk data and the extreme data are well discernible @xcite . to overcome this difficulty ,",
    "semiparametric bulk models have been proposed .",
    "@xcite propose a spliced model for the bulk data , while @xcite discuss a finite mixture of gamma densities .",
    "examples of a nonparametric approach for the bulk data can be found in @xcite , @xcite and @xcite .",
    "all the above references concern bayesian approaches to deal with the gpd .",
    "recent publications discussing different approaches worthwhile to be mentioned are @xcite and @xcite , among others .",
    "the focus of this paper is on the determination of the threshold @xmath0 , and we represent the bulk data with a finite mixture of distributions @xcite . as discussed by the authors , the approach allows for appropriate adaptation and , therefore , flexibility of the overall mixture model . for a more detailed discussion of this specific type of models for the bulk data and , in general , about semiparametric models",
    ", we refer to @xcite and @xcite .",
    "the main contribution of this work is in the prior for the threshold @xmath0 of a gpd .",
    "in fact , we propose to assign a prior probability to the observed order statistics by assuming that @xmath17 , where in general @xmath18 can take any value in @xmath19 .",
    "therefore , the nature of the proposed priors is of a discrete _ data dependent _ prior . before outlining how a prior on the order statistics can be defined , we need to fully motivate the choice of a distribution which support is limited to the order statistics only .",
    "the density of model has the form @xmath20g(x|\\xi,\\sigma,\\theta ) & x\\geq \\theta ,                      \\end{cases}\\ ] ] where @xmath21 is the density of the bulk data mixture , and @xmath22 the density of a gpd .",
    "note that , being a mixture model with two components , it can also be represented as @xmath23 where @xmath24 , @xmath25 , and @xmath26 . as in this work",
    "we consider quantities that can take positive values only , we will have @xmath27 .",
    "if we observe sample @xmath28 , which results in the order statistics @xmath29 , the likelihood function of model @xmath30 ( or , equivalently , model ) is given by @xmath31g\\left(x^{(j)}|\\xi,\\sigma , x^{(k)}\\right),\\ ] ] where we have assumed that the threshold of the gpd satisfies @xmath32 , with @xmath33 .",
    "note that , although the impact of the observations below the threshold on the estimates of the gpd parameters is in general not prominent @xcite , we still deem appropriate to consider it , and it is therefore included in the likelihood . for reasons due to practicality and identifiability of the model @xcite ,",
    "we assume that at least one observation contributes to the likelihood of the bulk component of the overall model .",
    "as mentioned in section [ sc_intro ] , from , we note that observations @xmath34 contribute to the bulk part of the model @xmath21 , while observations @xmath35 contribute to the gpd part of the overall mixture model . as such",
    ", there is no information for any @xmath0 within the interval @xmath36 , and the choice to assume that the threshold coincides to one of the order statistics is sensible .",
    "an additional argument , though connected to the above one , can be made by considering the following characteristics of the density of the gpd , which has the form @xmath37 at least for the case of interest in this paper , that is @xmath11 , the density is decreasing .",
    "therefore , if @xmath32 , the choice of @xmath17 is optimal in the sense that the contribution of the excesses @xmath38 to the gpd part of the likelihood is maximised .",
    "that is , any other choice of @xmath39 would yield a smaller contribution of the excesses to the gpd likelihood .",
    "we should also not forget that the threshold of the gpd is an artificial parameter @xcite , defining at what point of the support it is safe to assume tail approximation , and its determination within a given interval @xmath40 $ ] is not driven by any information in the sample beyond the interval boundaries themselves .",
    "as such , in a mixture model set up as the one considered in this work , the choice of having @xmath0 equal to an order statistics is appropriate .",
    "+ we propose two discrete priors for the threshold , both of which can be seen as the result of a choice under minimal prior information .",
    "the first proposed prior distribution is a discrete uniform prior .",
    "it is assumed that the threshold coincides with one of the observed order statistics ; in other words , the parameter space for the threshold of the gpd is @xmath41 , and the prior can be written as @xmath42 or @xmath43 , with @xmath33 . in this work",
    "we will be using the latter notation , leaving @xmath0 to represent the true ( or theoretical ) threshold value . from a practical point of view , the choice of a finite uniform prior to represent prior minimal information is obvious and , in some sense , intuitive .",
    "the prior has computational advantages and it is easy to be implemented . from @xcite , where a continuous uniform prior is proposed ,",
    "we see that the uniform should be defined over the interval @xmath44 , where @xmath45 is the number of parameters of the model for the bulk component ( i.e. the dimension of @xmath16 ) . in this case",
    ", the overall prior will be @xmath46 where the parameters ( @xmath6 , @xmath12 ) and @xmath16 are in general assumed to be independent a priori .",
    "the assumption of considering the parameters of the model for the bulk data independent from the parameters of the gpd is sensible .",
    "in fact , the general idea is that we have a set of observations that have been generated by two different processes .",
    "therefore , the information of the first process that impacts the second process ( and vice - versa ) can be assumed to be on the threshold only @xcite .",
    "in addition , parametric mixture models , such as in @xcite , @xcite @xcite , have been criticised as they do not take into consideration the dependence between the threshold and the scale @xmath12 of the gpd @xcite .",
    "however , since this dependence occurs for @xmath10 , what discussed in this paper is not subject to the above criticism .",
    "the second prior we propose is based on concepts of loss in information , therefore identified as the prior based on losses ( or as the kl prior , for reasons which will become clear below , where kl stands for kullback  leibler divergence ) . in this case",
    "the prior for the threshold depends on the parameters of the gpd ( @xmath6 and @xmath12 ) , and the overall prior has the form @xmath47 the idea used to obtain the prior @xmath48 is derived from @xcite and it is as follows .",
    "let us assume to have observed data @xmath1 .",
    "given the order statistics @xmath49 , we also assume that the true value of the threshold is @xmath17 , with @xmath33 .",
    "we consider the threshold to be @xmath50 , so that there will be at least one observation from the bulk distribution .",
    "should the inferential process suggest @xmath51 , it would then be practical ( and sensible ) to assume that a gpd is not necessary and that the model for the bulk data is a better choice .",
    "it is important to remark that the component @xmath52 of the model is not considered in the construction of the prior for the threshold .",
    "the main reason being that , with the type of problems considered in this paper , the focus is on the tail of the model , i.e. on the extreme values .",
    "in addition , the mixture model approach here discussed is thought in a way that the mixture distribution for the bulk data is included for convenience only and little consideration is given to its actual fitting to the data .",
    "as such , in order to use the prior information for @xmath0 in a relatively sharp way , it seems more appropriate to use the information of the order statistics above the threshold only , leaving the information coming from the bulk data to contribute by means of the likelihood function .",
    "the prior mass to be put on @xmath53 is derived by considering what is lost if the model @xmath54 is removed and it is the true one , where @xmath55 is the density of a gpd with threshold @xmath53 , shape parameter @xmath6 and scale parameter @xmath12 . in other words ,",
    "the approach associates a _ worth _ to each parameter value which , in this particular circumstance , is derived from the fact of having observed a particular value of @xmath3 .",
    "the _ worth _ is measured by applying a result in @xcite which states that , if a model is misspecified , i.e. if @xmath53 is removed and it is the true threshold , then the posterior distribution asymptotically accumulates at the order statistics @xmath56 such that the kullback ",
    "leibler divergence @xcite @xmath57 is minimised .",
    "that is , if the true model is removed , the estimation process will asymptotically indicate as the correct model the nearest one , in terms of the kullback ",
    "leibler divergence ; viz . , the model which is the most similar to the true one @xcite .    to link the _ worth _ of each order statistics to the prior probability we use the _ self - information _ loss function .",
    "this particular type of loss function assigns a loss to a probability statement and , say we have defined prior @xmath48 , its form is @xmath58 .",
    "more information about the self - information loss function can be found , for example , in @xcite . to formally derive the prior for the threshold we can proceed in terms of utilities , instead of losses ; this approach allows for a clearer exposition and does not impact the logic behind the prior derivation .",
    "let us then write utility @xmath59 where , to simplify the notation , we have dropped parameters @xmath6 and @xmath12 .",
    "we then let the minimum divergence from @xmath53 to be represented by utility @xmath60 .",
    "we want @xmath61 and @xmath60 to be matching utilities functions , as they measure the same utility in @xmath53 ; though as it stands @xmath62 and @xmath63 , and we want @xmath64 when @xmath65 .",
    "the scales are matched by taking exponential transformation ; so @xmath66 and @xmath67 are on the same scale .",
    "hence , we have @xmath68},\\ ] ] where @xmath69 by setting the functional form of @xmath70 in as it is defined in , we derive the objective prior distribution for the order statistics @xmath71    to identify the minimum kullback  leibler divergence in , we first consider    @xmath72 \\right . \\nonumber\\\\ & & \\left . - \\mathbb{e}\\left [ \\log\\left(1+\\frac{\\xi}{\\sigma}\\big(x - x^{(k+c)}\\big)\\right ) \\right ]   \\right\\}\\end{aligned}\\ ] ]    where @xmath73 and the expectations are taken with respect to the density @xmath54 . as is decreasing in @xmath74 , the nearest gpd to @xmath54",
    "is either @xmath75 or @xmath76 .",
    "however , given that @xmath76 is zero for @xmath77 , resulting in an infinite divergence , the prior is    @xmath78    the behaviour of the prior is obvious in the ideal case where the bulk and the extreme data have been generated by two clearly distinct processes . in this scenario there would be a large `` jump '' separating the two sets of data .",
    "prior will then put the highest mass on the most left order statistics of the extreme set of data , as its nearest model is relatively far .",
    "this value would then represent the best candidate of being the threshold separating the extreme from the bulk data , given the information coming from the observations and the choice of the model . in most realistic scenarios observed data would most likely not display an abrupt `` jump '' between the bulk and the extreme components ; rather , a smooth transition has to be expected . sections [ sc_simul ] and [ sc_real ] present both simulated and real data scenarios , where it is possible to have a feeling of the shape of the prior based on losses , and how its performances can be compared with the ones of the uniform prior .    in considering the qualitative behaviour of the prior distribution based on losses",
    ", we also need to take into account the case where there may be two or more observations with the same value .",
    "although it is possible , and perhaps advisable , to assume that the data are different from each other as this may lead to conceptual issues in the definition of the posterior distribution @xcite , it is easy to see how the proposed prior would behave in this scenario .",
    "if we have two ( or more ) order statistics with the same value , say @xmath79 , then , by the way the prior is constructed , the mass on @xmath80 would be zero , but the mass on @xmath81 would be strictly positive , provided @xmath82 .",
    "as such , the prior based on losses maintains the idea of assigning mass on the basis on how `` extreme '' a value is , even when there are repeated observations .",
    "+ given that the prior distributions proposed are data dependent , it is appropriate to briefly discuss the implications of such a choice .",
    "a definition of data dependent prior can be found in @xcite , who identifies it as a measurable mapping from the data space to the set of priors .",
    "in other words , a distribution that depends on the data obtained through avert use of the observations .",
    "the above can be accomplished in different ways ( and at different levels of depth ) , but probably the most common type of data dependent priors are the _ data - analytic _ priors , where the data is used to determine the hyperparameter(s ) of the prior distribution .",
    "examples can be found in @xcite , @xcite , @xcite and @xcite .",
    "data - analytic priors can also be used to choose the base measure and the precision of a dirichlet process in bayesian nonparametric @xcite .",
    "finally , @xcite and @xcite discuss data - analytic priors for finite mixtures of normal densities .",
    "although data dependent priors are used in practical situations , criticisms have been raised .",
    "possibly , the most important concerns are that the data is used twice , for the prior and for the likelihood , and that bayes theorem can only be approximated .",
    "an interesting discussion about the first objection can be found in @xcite , for example ; while the second objection is discussed , for example , in @xcite .",
    "we do not present here a detailed discussion on how the above objections can be rebutted or overcome ; such a discussion can be found , for example , in the work of @xcite and the reference therein .",
    "obviously , using the order statistics to determine the parameter space of the threshold categorises our priors under wasserman s definition of a data dependent prior . in the case",
    "of the uniform prior , the information drawn from the data is limited to the possible location of the threshold and , as discussed above , the choice is sensible as it yields optimal contribution of the excesses to the likelihood .",
    "for the prior based on the kullback  leibler divergence , the information drawn from the data goes beyond the possible location of the threshold , as it considers the similarity ( or diversity ) between consecutive models .",
    "however , as will be shown in section [ sc_simul ] , the frequentist performances of this prior are virtually the same as the uniform , showing that the extra information used does not add tangible performance benefits .    to conclude",
    ", we deem appropriate to point out that information from the data ( besides in the likelihood function ) has been always considered in the inferential process for the threshold .",
    "this is obvious when we consider graphical approaches @xcite , where data are plotted to determine a possible location of the threshold .",
    "when it comes to bayesian analysis , the proposed priors in the literature which claim to carry minimal information , draw some of this information from the data .",
    "for example , the continuous uniform prior proposed in @xcite has a parameter space bound by order statistics .",
    "the normal prior proposed by @xcite , and claimed to be set up in a noninformative fashion by @xcite , has to be centered on the @xmath83 data quantile in order to avoid identifiability issues when the sample size is not sufficiently large .",
    "the choice of the prior distribution for the parameters @xmath6 and @xmath12 of the gpd is straightforward . in a noninformative context , as it is the flavour of this paper , the choice is on the jeffreys independent prior defined in @xcite as @xmath85 which is defined for @xmath86 and @xmath5 . as shown by @xcite , the prior yields to the proper posterior @xmath87 for a sample size of @xmath88 . on the other hand ,",
    "if suitable prior information about @xmath6 and @xmath12 is available ( and it is practical / desirable to be exploited ) , then appropriate prior distributions can be elicited .",
    "however , as this case lies outside the scope of this work , it will not be discussed any further .",
    "@xmath16 is a vector which elements are the parameters of the mixture @xmath52 for the bulk data . thus the prior to be assigned to @xmath16",
    "depends on the components of the mixture .",
    "as already mentioned , the focus of this work is mainly in the prior for the threshold @xmath0 ; we then restrict our illustrations to the common case of positive data only and we will adopt a finite mixture of gamma densities to represent the bulk data @xcite @xmath89 we have @xmath90 , where @xmath91 denote the weights of the mixture , with @xmath92 , @xmath93 is a gamma density with shape parameter @xmath94 and rate parameter @xmath95 . to address the identifiability issue intrinsic to mixture models @xcite the gamma density",
    "can be reparametrised as @xmath96 so we can impose the constraint @xmath97 on the parameter space for the @xmath98 s , as they represent the means of the gamma densities .",
    "@xmath99 will represent the shape parameters for the @xmath100 gamma densities .    with the parametrisation in we",
    "assign an inverse gamma prior to each mean @xmath98 and a gamma prior to each shape parameter @xmath101 .",
    "although the above priors are not selected through an objective method , they will represent minimal prior information in the form of large variance .    finally , for the weights @xmath102 we chose a symmetric dirichlet prior distribution with all the parameters equal to one : @xmath103 .",
    "this choice as well represents minimal prior information , and @xmath104 .",
    "to analyse and compare the proposed discrete priors for the threshold of the gpd we perform two types of simulations . in the first simulation we detail the inferential procedure for all the parameters of the mixture on the basis of a random sample from a known model .",
    "the second part consists in a simulation study that aims to assess the frequentist performances of the posterior distributions induced by the proposed priors .",
    "this is done by repeatedly sample from mixture models that differ in the gpd component only ( i.e. threshold , shape and scale parameters ) and observe the coverage and the means square errors of the posterior distributions for the threshold .",
    "given the minimal informative nature of the paper , the analysis of the frequentist properties is a suitable way to compare the two proposed priors and assess their effectiveness .",
    "the posterior for the parameters of the mixture model in is given by @xmath105 where @xmath106 is the likelihood function specified in .",
    "the prior distribution @xmath43 , in our illustrations , would be one of the proposed discrete priors ; that is , either the uniform prior or the prior based on losses",
    ". as the marginal posterior distributions of the parameters are analytically intractable , monte carlo methods are necessary to sample from these distributions .      to illustrate in detail the entire inferential procedure , we have sampled @xmath107 observations from a mixture model as in .",
    "the bulk data component is a mixture of two gamma densities with shape parameters @xmath108 and @xmath109 , and rate parameters @xmath110 and @xmath111 .",
    "the weights of the gamma densities are , respectively , @xmath112 and @xmath113 .",
    "the extreme data component is a gpd with shape parameter @xmath114 and scale parameter @xmath115 , and the threshold has been put at the @xmath83 data quantile , with @xmath116 .",
    "figure [ fig : simulation_histandprior ] shows the histogram of the sample ( left graph ) and the prior probabilities on the order statistics ( right graph ) representing the prior for the threshold based on losses . from the histogram we see that there is a smooth transition between the bulk data and the extreme data .",
    "the behaviour of the prior for @xmath53 , which we recall being based on the kullback  leibler divergence between gpd densities with thresholds on adjacent order statistics , reflects the level of `` extremeness '' of the data : almost uniform for the lower part of the data space , with mass that is assigned increasingly on the order statistics when these become extreme . as discussed in section [ sc_thetaprior ] ,",
    "the behavior of the prior for the threshold as shown in figure [ fig : simulation_histandprior ] is sensible as more extreme observations are more likely to represent a suitable threshold .    to estimate the number of components of the mixture for the bulk data ( @xmath100 ) one could proceed as suggested in @xcite , where models with different values of @xmath100 are estimated and suitable indexes , such as the deviance information criterion ( dic ) and the bayesian information criterion ( bic ) , are computed to choose the `` best '' model on the basis of the observed sample .",
    "alternatively one could consider a hierarchical structure and assign a prior to @xmath100 to represent the uncertainty on its true value ; for this approach see , for example , @xcite .",
    "we have already mentioned that the focus of this work is on the prior for @xmath53 ; therefore , we will not further investigate this matter , and we simply show that the posterior distributions for the weights are different from zero only for @xmath117 .    for the parameters of the mixture ,",
    "as discussed in section [ sc_priorgamma ] , we use inverse gamma priors on the means of the gamma densities , and gamma priors on the shape parameters . given that we want prior distributions that somehow represent weak prior information , these distributions will have large variances . in detail",
    ", we have a gamma with parameters 6 and 0.5 for each shape @xmath118 , and an inverse gamma with parameters 2.1 and 5.5 for the each mean @xmath119 , for @xmath120 .",
    "in addition , the priors have mean equal to the average of the corresponding true values . for the weights ( section [ sc_priorgamma ] ) we choose a dirichlet distribution with all parameters equal to one , corresponding to a noninformative scenario",
    ". the estimation of @xmath53 has been performed by considering , for the same sample , both the uniform prior and the prior bases on losses in .",
    "the monte carlo procedure consists of 20,000 iterations with 10,000 iterations as burn - in period .",
    "convergence of the posterior has been assessed by several means , including monitoring the chains , running means and computing the gelman and rubin s convergence diagnostics @xcite .",
    "first , considering @xmath121 , we have seen that the value of @xmath122 converged to zero almost immediately under each prior on @xmath53 , which makes us conclude that the model with @xmath117 is the appropriate one .",
    "table [ tab : simulation ] shows the statistics of the marginal posterior distributions of all the parameters of the mixture model , namely the parameters of the mixture component , and the parameters of the gpd , for @xmath117 .",
    "these statistics have been computed for both the priors for the threshold .",
    "= 0.11 cm    .statistics of the posterior distributions under the prior based on losses ( kl prior ) and under the uniform prior . [ cols=\"^,^,^,^,^,^,^,^ \" , ]     when we compare the estimates of the threshold of the gamma mixture for the bulk data with the ones of the single gamma mixture we note some differences .",
    "they appear to be reasonable differences , especially if we consider the size of the credible intervals . however , the fact that the differences are not large it is most likely due to the large size of the sample .",
    "but , it is possible to appreciate these discrepancies which show that different models for the bulk data impacts on the threshold value , as expected .",
    "there are many processes which present heavy - tailed behaviour and , in these cases , it is not always advisable to represent the whole data by means of a parametric distribution , such as the student-@xmath123 , or by a mixture model where the components are densities of the same family , such as a mixture of gamma densities @xcite or normal densities .",
    "one way to address the above problem is to consider the asymptotic result of pickand s theorem , for which the tail of a distribution , above a certain threshold , can be represented by a gpd .",
    "however , this method raises another problem , that is the determination of an appropriate threshold . in a bayesian set up ,",
    "the idea is to represent the uncertainty about the threshold by a prior distribution .    in this paper",
    ", we present a way of defining prior distributions for the threshold of a gpd which have as support the set of observed order statistics .",
    "we propose two different methods to determine the prior : one is intuitive in the sense that every order statistic has , _ a priori _ , the same probability of being the true threshold value .",
    "the second method takes into consideration the loss that we would incur if a given order statistic were removed from the set of possible values for the threshold , and it is true value . through simulation and real data analysis",
    ", we have shown that the two priors have very similar performances , when compared on the basis of the frequentist properties of the respective posterior they yield and the estimates they generate .",
    "given that the idea behind these priors is to represent a condition of minimal prior information , the fact that both priors converge to similar results is comforting .",
    "nonetheless , it is still possible to find reasons to prefer one over the another .",
    "the uniform prior has the undoubted advantage of being easier to code and , although minimally , it allows for faster simulations .",
    "however , one has to be careful in assuming that uniformity represents no prior information @xcite .",
    "thus , although the results obtained by applying the two discrete priors for the threshold are similar , we believe that the prior based on losses has to be preferred on the basis of the following considerations .",
    "first , it has a `` meaning '' .",
    "the mass assigned to each order statistic derives from a sound consideration of the _ worth _ that each one of them has in representing the potential threshold for the gpd . on the other hand , and this connects to the second reason , by assigning a uniform prior to @xmath53 one assumes that each order statistics has the same chance of being the threshold . apart that it could be interpreted as an informative assumption , it conflicts with the idea behind the gpd , for which the threshold has to be sufficiently large to avoid model bias , as discussed in section [ sc_intro ] ; and this is not compatible with a uniform prior where lower order statistics have an _ a priori _ probability of being the threshold equal to the one of the upper order statistics .. in conclusion , when one aims for objectivity , in applied statistics problems a noninformative prior has to be based on solid motivations , not only on performance .",
    "one exception to the above argument is the case of @xmath10 ( which however is not in the scope of this paper ) .",
    "in fact , given that the parameter space for the threshold depends on the values of @xmath6 and @xmath12 , the prior based on losses would not be defined as the kullback  leibler divergence between different",
    "gpd is infinite .",
    "thus , in the case we would model a light - tail of a distribution by a gpd , the choice of the uniform prior would be the only choice between the two proposed discrete prior distributions .    as a final remark",
    ", we would like to highlight that , although the focus of the paper has been on observations that can take positive values only , the overall approach can be easily extended to quantities over the whole real line .",
    "for example , if we consider logarithmic returns of some financial index ( or price ) , the part of the data below the threshold could be modelled by a finite mixture of normal densities . similarly ,",
    "if we were interested in analysing the negative returns ( which is a common practice in risk management , for example ) , the prior would be defined over the negative order statistics , and the bulk data would be represented by the observations above the threshold .",
    "another possible development of the model and the inferential procedure would be represented by the case where both tails of the distribution would be of interest , and therefore represented by one separate , but not necessarily independent , gpd each . in all the above cases , the support of the prior has to be defined so to reflect the nature of the problem ."
  ],
  "abstract_text": [
    "<S> in this paper , we discuss a method to define prior distributions for the threshold of a generalised pareto distribution , in particular when its applications are directed to heavy - tailed data . </S>",
    "<S> we propose to assign prior probabilities to the order statistics of a given set of observations . </S>",
    "<S> in other words , we assume that the threshold coincides to one of the data points . </S>",
    "<S> we show two ways of defining a prior : by assigning equal mass to each order statistic , that is a uniform prior , and by considering the _ worth _ that every order statistic has in representing the true threshold . </S>",
    "<S> both proposed priors represent a scenario of minimal information , and we study their adequacy through simulation exercises and by analysing two applications from insurance and from finance .    * keywords . * extreme values , generalised pareto distribution , heavy tails , kullback  </S>",
    "<S> leibler divergence , self - information loss </S>"
  ]
}