{
  "article_text": [
    "compressed sensing or compressive sampling ( cs )  @xcite ,  @xcite is an emerging field in signal processing .",
    "the theory of cs suggests to use only a few random linear measurement of a sparse signal ( in a basis ) for reconstructing the original signal . the mathematical model of noise free",
    "cs is : @xmath0 where @xmath1 is the original signal with length @xmath2 and is sparse in the basis @xmath3 ( @xmath4 and @xmath5 is defined as sparsity level ) and @xmath6 is an @xmath7 random measurement matrix where @xmath8 . for near perfect recovery , in addition to the signal sparsity , the incoherence of the random measurement matrix @xmath6 with the basis @xmath3 is needed .",
    "the incoherence is satisfied with high probability for some types of random matrices such as i.i.d gaussian elements or i.i.d bernoulli @xmath9 elements .",
    "recent theoretical results show that under these two conditions ( sparsity and incoherence ) , the original signal can be recovered from only a few linear measurements of the signal within a controllable error , even in the case of noisy measurements @xcite ,  @xcite ,  @xcite ,  @xcite .    in @xcite ,",
    "some error bounds are introduced for reconstructing the original sparse ( or compressible ) signal in the noisy cs framework . in @xcite ,",
    "the performance limits of noisy cs is investigated by definition of some performance metrics which are of shannon theoretic spirit .",
    "@xcite considers the no noise cs and finds an upper bound on reconstruction error in terms of mean square error ( mse ) only for @xmath10-minimization recovery algorithm .",
    "but , @xcite finds some upper bounds in the noisy cs and for general recovery algorithms .",
    "@xcite is also investigated its own decoder which is derived based on joint typicality . moreover ,",
    "some information theoretic bounds are derived in @xcite .    in this paper , we derive a bayesian cramer - rao bound ( bcrb ) ( @xcite ,  @xcite ) , which is a lower bound , for noisy cs by a statistical view to the cs problem .",
    "this bcrb bounds the performance of any parametric estimator ( whether biased or unbiased ) of the sparse coefficient vector in terms of mean square estimation error @xcite ,  @xcite .",
    "we also introduce the notion of blind cs in contrast to the traditional cs to whom we refer on the non - blind cs .",
    "we compute bcrb for both non - blind and blind cs , where in the latter , we do not know the measurement matrix in advance . in a related direction of research ,",
    "a crb is obtained for mixing matrix estimation in sparse component analysis ( sca ) @xcite .",
    "consider the noisy cs problem : @xmath11 where @xmath12 , @xmath13 is a sparse vector and @xmath14 is a gaussian zero - mean noise vector with the covariance @xmath15 . in cs framework , we want to estimate @xmath13 , from which , @xmath1 can be reconstructed from the measurement vector @xmath16 .",
    "we nominate the traditional cs problem as non - blind cs since we know the basis @xmath3 and the measurement matrix @xmath6 and hence @xmath17 in advance . in some cases",
    ", we have no prior information about the signals in addition to their sparsity . as such",
    ", we do not know the basis @xmath3 , in which the signals are sparse .",
    "one application is a blind interceptor who intercepts the signals .",
    "the only information is that the signals have been received are sparse in some unknown domain . in these cases ,",
    "we nominate the problem as blind cs which is inspired from the well known problem of blind source separation ( bss ) . as such",
    ", each measurement will be : @xmath18 where @xmath19 is the random measurement vector and a row of @xmath6 ) and @xmath20 is the corresponding row in @xmath17 and an unknown random vector .",
    "the posterior cramer - rao bound ( pcrb ) or bayesian cramer - rao bound ( bcrb ) of a vector of parameters @xmath21 estimated from data vector @xmath16 is the inverse of the fisher information matrix , and bounds the estimation error in the following form @xcite : @xmath22\\ge{{\\textbf{j}}}^{-1}\\ ] ] where @xmath23 is the estimate of @xmath21 and @xmath24 is the fisher information matrix with the elements @xcite : @xmath25,\\ ] ] where @xmath26 is the joint probability between the observations and the parameters . unlike crb ,",
    "the bcrb ( [ eq : bcrb ] ) is satisfied for any estimator ( even for biased estimators ) under some mild conditions @xcite , @xcite which we assume that are fulfilled in our problem . using bayes rule , the fisher information matrix can be decomposed into two matrices @xcite : @xmath27 where @xmath28 represents data information matrix and @xmath29 represents prior information matrix which their elements are @xcite : @xmath30=e_{\\boldsymbol{\\theta}}(j_{s_{ij}})\\ ] ] @xmath31\\ ] ] where @xmath32 $ ] is the standard fisher information matrix @xcite and @xmath33 is the prior distribution of the parameter vector .    in this paper , we use this bcrb for our problem because we have a sparse prior information about the parameter which is estimated .",
    "we compute bcrb for two blind and non - blind cases .      in the non - blind cs case , the matrices @xmath6 and",
    "@xmath3 are assumed to be known and @xmath6 is a random matrix while @xmath3 is a fixed basis matrix .",
    "similar to @xcite , since @xmath6 is assumed to be known and random , @xmath6 can be added as an additional observation .",
    "hence , the data information matrix elements @xmath34 from model ( [ eq : ncs ] ) are of the form : @xmath35.\\ ] ] since @xmath36 , @xmath37 is independent of @xmath13 and @xmath38 , we can write @xmath39 .",
    "so , we have @xmath40 where @xmath41 denotes the elements of the matrix @xmath42 . hence , we have @xmath43 .",
    "so , the expectation ( [ eq : expec ] ) will be @xmath44=\\frac{1}{\\sigma^2_e}e_{\\boldsymbol{\\phi}}\\{g_{ij}\\}=j_{d_{ij}}=\\frac{1}{\\sigma^2_e}\\sum_{r=1}^n e_{\\boldsymbol{\\phi}}\\{d_{ri}d_{rj}\\ } $ ] .",
    "some simple manipulations show that under assumption that the elements of @xmath6 are zero mean and independent random variables , the data information matrix will be : @xmath45 where @xmath46 is the variance of the random measurement matrix elements . if @xmath47 is an orthonormal basis then @xmath48 and hence @xmath49 .    to compute the prior information matrix @xmath50 from ( [ eq : pfim ] )",
    ", we should assume a sparse prior distribution for our parameter vector elements @xmath51 .",
    "similarly to @xcite , we assume @xmath51 s are independent and have a parameterized gaussian distribution : @xmath52 in ( [ eq : pr ] ) , the variance @xmath53 enforce the sparsity of the corresponding coefficient : a small variance means that the coefficient is inactive and a large value means the activity of the coefficient .",
    "it can be easily seen that in this case , the prior information matrix is @xmath54 . finally , for orthonormal bases for @xmath3 and for prior distribution ( [ eq : pr ] ) , the bcrb results in : @xmath55\\ge \\left(n\\frac{\\sigma^2_r}{\\sigma^2_e}+\\frac{1}{\\sigma^2_i}\\right)^{-1}.\\ ] ]      in the blind cs case , the matrix @xmath3 is not known in advance and hence the elements of matrix @xmath17 are random and unknown with zero mean . if we restrict ourselves to gaussian measurements matrix elements ( @xmath56 is a zero - mean gaussian ) then different measurement samples of @xmath57 are also gaussian and independent of each other .",
    "hence , we can compute the data information matrix from only one measurement ( [ eq : blind1 ] ) . then",
    ", the information matrix elements @xmath58 $ ] will be equal to ( refer to @xcite ) : @xmath59.\\ ] ] if the elements of @xmath60 are assumed to be random with a gaussian distribution of zero mean and variance @xmath61 and the columns of the basis matrix @xmath3 have unit norms , then : @xmath62 where @xmath63 .",
    "simple manipulations show : @xmath64 and from ( [ eq : bbcrb ] ) we should compute : @xmath65p({{\\textbf{w}}})d{{\\textbf{w}}}\\ ] ] where the internal integral is @xmath66 in which @xmath67 and @xmath68 are the second and fourth order moments equal to @xmath69 and @xmath70 .",
    "so , we have @xmath71 and then : @xmath72 where the off diagonal terms are zeros @xmath73 because the integrand is an odd function .",
    "the diagonal terms are : @xmath74    following appendix  [ app1 ] , the diagonal elements are simplified as : @xmath75 where @xmath76 and @xmath77 are defined and calculated in appendix  [ app1 ] .",
    "the prior information matrix for bg distribution @xmath78 is calculated in appendix  [ app2 ] : @xmath79 finally , the blind bcrb is calculated as : @xmath80\\ge \\left(2\\frac{\\sigma^2_r}{m}\\left(a_1-\\sigma^2_ea_2\\right)+\\frac{1-p}{\\sigma^2}\\right)^{-1}\\ ] ]",
    "in this section , we compare the crb s with the results of some of the state - of - the - art algorithms for signal reconstruction in cs . in our simulations , we used sparse signals with the length @xmath81 in the time domain where @xmath82 .",
    "we used a bg distribution with the probability of being nonzero equal to @xmath83 and the variance for nonzero coefficients is equal to @xmath84 .",
    "so , in average there were 51 active coefficients .",
    "we used a gaussian random measurement matrix with elements drawn from zero mean gaussian distribution with variance equal to @xmath85 .",
    "the number of measurements are varied between 60 to 200 .",
    "we computed the mean square error ( mse ) for sparse coefficient vector over 100 different runs of the experiment : @xmath86 where @xmath87 is the experiment index .",
    "we compared this measure for various algorithms with the average value of bcrb for non - blind case which is equal to @xmath88 .",
    "the algorithms used for our simulation are orthogonal matching pursuit ( omp ) @xcite , basis pursuit ( bp ) @xcite , bayesian compressive sampling ( bcs ) @xcite and smoothed - l0 ( sl0 ) @xcite .",
    "we also computed the bcrb for blind case ( [ eq : bbcrb ] ) to compare the bcrb s in both blind and non - blind case .",
    "figure  [ fig1 ] shows the results of the simulation .",
    "it can be seen that in the low number of measurements , there is a gap between the bcrb and the performance of algorithms while one of the algorithms approximately reaches the bcrb for large number of measurements .",
    "moreover , the difference between the bcrb s for the non - blind and blind cases are very large .",
    "it shows that the blind case needs much more linear measurements than the non - blind case .    to verify the approximation @xmath89 and @xmath90 ( refer to appendix  [ app2 ] ) , we calculated the integrals numerically with parameters @xmath91 and @xmath92 .",
    "when @xmath93 then @xmath94 and @xmath95 .",
    "it shows that our approximations are true for sufficiently small value of @xmath96 .    ) with length @xmath81 and with the bg distribution with parameters @xmath91 , @xmath97 and @xmath98 .",
    "measurement matrix elements are unit variance gaussian random variables.,width=264 ]",
    "in this paper , the cs problem is divided into non - blind and blind cases and the bayesian cramer - rao bound for estimating the sparse vector of the signal was calculated in the two cases .",
    "the simulation results show a large gap between the lower bound and the performance of the practical algorithms when the number of measurements are low .",
    "there was also a large gap between the bcrb in both non - blind and blind cases .",
    "it also shows that in the blind cs framework , much more blind linear measurements of the sparse signal are needed for perfect recovery of the signal .",
    "let define @xmath99 and assume an equal prior distribution for all coefficients @xmath51 , then all @xmath100 s are the same because of the symmetry of the integral .",
    "so , we can add all the integrals and write : @xmath101 then , if we nominate the two above integrals as @xmath102 and @xmath103 , the integral @xmath100 is computed as @xmath104 . to compute @xmath76 and @xmath77 , we approximate the joint probability distribution of coefficients as : @xmath105 this approximation is based on the assumption that the value of @xmath106 which is the activity probability is very small and so we can neglect the higher order powers of @xmath106 . by this approximation , the two integrals will be approximately : @xmath107 @xmath108 where the two integrals are @xmath109 and @xmath110 . by change of variable @xmath111 ,",
    "the two integrals are equal to : @xmath112 @xmath113 where @xmath114 .",
    "the above integrals are equal to : @xmath115\\ ] ] @xmath116\\ ] ] where @xmath117 is the error function , defined as @xmath118 .",
    "since the coefficients @xmath51 s are independent , the off diagonal terms @xmath119 are zero . because of the independence of @xmath51 s , we can write @xmath120 . to calculate this term",
    ", we use a gaussian distribution with small variance @xmath121 instead of delta function @xmath122 .",
    "so , the prior is : @xmath123 where @xmath124 , @xmath125 and @xmath126 .",
    "the partial derivative can be calculated as : @xmath127 hence , we have : @xmath128 to compute the above integrals , the partial derivatives are @xmath129 and @xmath130 .",
    "simple calculations show that @xmath131 and hence : @xmath132 ^ 2}{a\\exp(-\\frac{w^2_i}{2\\sigma^2_0})+b\\exp(-\\frac{w^2_i}{2\\sigma^2})}dw_i\\ ] ] where the above integral can be decomposed to three integrals which are @xmath133 , @xmath134 and @xmath135 .",
    "since we have a term @xmath136 in the numerator of the above integrals and the gaussian term with small variance is large near zero , we can neglect the gaussian term with small variance ( delta function ) in the denominator .",
    "so , the integrals @xmath137 and @xmath138 with neglecting this term will be approximately zero .",
    "we verify this approximation in the simulation results by computing these integrals numerically .",
    "finally , the third integral will be approximately @xmath139 .",
    "calculating this integral results is @xmath140 .",
    "s.  aeron , m.  zhao , and v.  saligrama , `` information theoretic bounds to sensing capacity of sensor networks under fixed snr , '' in _ ieee information theory workshop _ , pp .",
    "8489 , lake tahoe , ca , sep 2007 ."
  ],
  "abstract_text": [
    "<S> in this paper , we address the theoretical limitations in reconstructing sparse signals ( in a known complete basis ) using compressed sensing framework . </S>",
    "<S> we also divide the cs to non - blind and blind cases . </S>",
    "<S> then , we compute the bayesian cramer - rao bound for estimating the sparse coefficients while the measurement matrix elements are independent zero mean random variables . </S>",
    "<S> simulation results show a large gap between the lower bound and the performance of the practical algorithms when the number of measurements are low .    </S>",
    "<S> _ index terms_-compressed sensing , sparse component analysis , blind source separation , cramer - rao bound . </S>"
  ]
}