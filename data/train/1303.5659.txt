{
  "article_text": [
    "vt ( viterbi training ) has been used for long time as an efficient parameter learning method in various research fields such as machine translation @xcite , speech recognition @xcite , image analysis @xcite , parsing @xcite and gene finding @xcite .",
    "although vt is np - hard even for pcfgs ( probabilistic context free grammars ) , which is proved by encoding the 3-sat problem into pcfgs @xcite , and is biased unlike mle ( maximum likelihood estimation)@xcite , it often outperforms and runs faster than the conventional em algorithm .",
    "we introduce this vt to prism which is a probabilistic extension of prolog @xcite .",
    "there are already multiple parameter learning methods available in prism .",
    "one is the em algorithm , or more generally map ( maximum a posteriori ) estimation @xcite .",
    "another is vb ( variational bayes ) @xcite which approximately realizes bayesian inference and learns pseudo counts assuming dirichlet priors over parameters .",
    "they are implemented on prism s data structure called _",
    "explanation graph_s representing and/or boolean formulas made up of probabilistic ground atoms .",
    "probabilities used in em , map and vb are all computed by running the generalized inside - outside algorithm @xcite or its variant on explanation graphs .",
    "vt in prism runs on explanation graphs just like em , map and vb but always deals with a single probability of a single explanation called _ viterbi explanation _ or most probable explanation .",
    "compared to em that updates only parameters , vt alternately updates the viterbi explanation and parameters , computing one from the other and vice versa , until the viterbi explanation stops changing .",
    "note that this results in earlier termination of the algorithm than em because a small perturbation in parameters does not change the viterbi explanation whereas it keeps em running .",
    "actually we found in our experiments in section  [ sec : vt - prism ] that em required 8 to 15 times more cycles to stop than vt .",
    "also since vt updates parameters so that they maximize the probability of the viterbi explanation , it is possible and probable that the final parameters by vt give a higher probability to the viterbi explanation than those learned by em , which intuitively explains why vt tends to yield superior performance to em in prediction tasks such as parsing that computes the viterbi explanation as a predicted value , as we see in section  [ sec : grammar ] .",
    "in addition vt brings about a favorable side effect on prism .",
    "vt does not require _ the exclusiveness condition _ which is imposed on prism programs to ensure efficient sum - product probability computation .",
    "this is because vt always deals with a single probability of viterbi explanation and hence there is no need for summing up probabilities of the non - exclusive explanations .",
    "consequently prism can learn parameters by vt for programs that do not satisfy the exclusiveness condition .",
    "we will discuss more about the exclusiveness condition in section  [ sec : exclusivness ] .",
    "vt thus improves prism in the following points :    * faster convergence due to a less number of iterations compared to em * ability to learn parameters good for prediction * the elimination of the exclusiveness condition imposed on programs    from the viewpoint of statistical machine learning and plp ( probabilistic logic programming ) , on the other hand , we can first say that prism generalizes vt .",
    "that is , the vt algorithm implemented in prism works for arbitrary probabilistic models described by prism , a turing complete language , including bns ( bayesian networks ) , hmms ( hidden markov models ) and pcfgs , and hence eliminates the need for the user to derive and implement a specific vt algorithm for a specific model that can be described as a prism program .",
    "also it makes vt easily accessible to probabilistic logic programmers because they can use vt just by setting learn_mode , one of prism s flags , appropriately . as a result , by switching the learn_mode flag he / she can choose the best parameter learning method for their models from em , map , vb and vt , all available in prism2.1 , without rewriting and adapting their programs to each parameter learning method .",
    "indeed , the exhaustive comparisons among em , map , vb and vt done in our experiments seem quite costly in other environments .    in what follows , we first review prism in section  [ sec : review - prism ] and then explain the basic idea of vt and reformulate it for prism in section  [ sec : vt - prism ] .",
    "we then apply vt to two probabilistic grammars in section  [ sec : grammar ] using the atr corpus where a hidden variable in a model is a prediction target . in section  [ sec : nbh ] , we deal with a different situation using an nbh ( naive bayes with a hidden variable ) model whose hidden variable is _ not _ a prediction target .",
    "we explain the implication of vt on the exclusiveness condition in section  [ sec : exclusivness ] .",
    "section  [ sec : discussion ] discusses related work .",
    "section  [ sec : conclusion ] is the conclusion .",
    "for the self - containedness we review prism focusing on its computation mechanism .",
    "prism is one of the srl ( statistical relational learning ) / pll ( probabilistic logic learning ) languages @xcite which aim at using rich expressions such as relations and first - order logic for complex probabilistic modeling .",
    "it is a probabilistic extension of prolog enhanced with various built - in predicates for statistical machine learning such as predicates for parameter learning , viterbi inference , model scoring , mcmc sampling and so on in addition to standard predicates equipped with prolog .",
    "syntactically a prism program @xmath4 looks like a usual prolog program except the use of probabilistic built - in predicate of the form msw(@xmath5,@xmath6 ) called `` multi - valued random switch''(with switch name @xmath5 ) that represents a probabilistic choice using simple probabilistic events such as dice throwing ; msw(@xmath5,@xmath6 ) says that throwing a dice named @xmath5 yields an outcome @xmath6 .",
    "let @xmath7 be the set of possible outcomes for @xmath5 .",
    "the set @xmath8 of msw atoms is given a joint distribution such that one of the @xmath9 s , say msw(@xmath5,@xmath6 ) , becomes exclusively true ( others false ) with probability @xmath10 ( @xmath11 ) where @xmath12 .",
    "in other words , @xmath13 stands for a discrete random variable @xmath14 taking @xmath6 with probability @xmath10 ( @xmath15 ) . in this sense",
    "we identify @xmath13 with @xmath14 and its distribution .",
    "the @xmath10 s are called _",
    "parameter_s associated with @xmath5 .",
    "they are directly specified by the user or learned from data .",
    "we define @xmath16 as an infinite product of such distributions for msws where @xmath17 stands for the set of all parameters",
    ". then @xmath18 is uniquely extended by way of the least model semantics for logic programs to a @xmath19-additive probability measure @xmath20 over possible herbrand interpretations of @xmath4 which we consider as the denotation of @xmath4(_distribution semantics _ ) @xcite . in the following we omit @xmath17 when the context is clear for the sake of brevity .",
    "let @xmath21 be a non - msw atom which is ground .",
    "@xmath22 , the probability of @xmath21 , can be naively computed as follows .",
    "first reduce the top - goal @xmath21 using prolog s exhaustive top - down proof search to an equivalent propositional dnf formula @xmath23 and @xmath24 denote the same boolean random variable in view of the distribution semantics of prism . ] as a bag @xmath25 of explanations . ] where @xmath26 ( @xmath27 ) is a conjunction @xmath28 of msw atoms such that @xmath29 .",
    "each @xmath26 is called an _ explanation for g_. then assuming    : :    msw atoms in an explanation are independent : +    @xmath30 : :    explanations are exclusive : +    @xmath31 if    @xmath32    we compute @xmath22 as @xmath33 recall here that msws with different switch names are independent by construction of @xmath34 .",
    "we may further assume that msw atoms with the same switch name are iid ( independent and identically distributed ) .",
    "that is when msw(@xmath5,@xmath6 ) and msw(@xmath5,@xmath35 ) occur in a program , we consider they are the results of sampling the same msw(@xmath5,@xmath36 ) twice .",
    "this is justified by hypothetically adding an implicit argument , trial - id @xmath37 @xcite , to msw(@xmath5,@xmath36 ) and assume that msw(@xmath5,@xmath37,@xmath36)s have a product of joint distributions just like the case of msw/2 which makes msw(@xmath5,@xmath37,@xmath36 ) and msw(@xmath5,@xmath38,@xmath36 ) ( @xmath39 ) iid .",
    "so in what follows we assume the independence condition is automatically satisfied .",
    "contrastingly the exclusiveness condition can not be automatically satisfied .",
    "it needs to be satisfied by the user , for example , by writing a program so that it generates an output solely as a sequence of probabilistic choices made by msw atoms ( modulo auxiliary non - probabilistic computation ) .",
    "although most generative models including bns , hmms and pcfgs are written this way , naturally , but there are models which are unnatural or difficult to write this way @xcite . relating to this",
    ", observe that _",
    "viterbi explanation _ ,",
    "i.e. the most likely explanation @xmath40 for @xmath21 , is computed similarly to @xmath22 just by replacing sum with argmax : @xmath41 , and does not require the exclusiveness condition to compute because it only deals with the probability of a single explanation .",
    "we will discuss more about the exclusiveness condition in section  [ sec : exclusivness ] .",
    "so far our computation is naive . since there can be exponentially many explanations , naive computation would lead to exponential time computation .",
    "prism avoids this by adopting tabled search in the exhaustive search for all explanations for the top - goal @xmath21 and applying dynamic programming to probability computation . by tabling ,",
    "a goal once called and proved is stored ( tabled ) in memory with answer substitutions and later calls to the goal return with stored answer substitutions without processing further .",
    "tabling is important to probability computation because tabled goals factor out common sub - conjunctions in @xmath24 , which results in sharing probability computation for the common sub - conjunctions , thereby realizing dynamic programming which gives exponentially faster probability computation compared to naive computation .    as a result of exhaustive tabled search for all explanations for @xmath21",
    ", prism obtains a set of propositional formulas called _ defining formula_s of the form @xmath42 for every tabled goal @xmath43 is a tabled goal .",
    "tabled goals except the top - goal are called `` intermediate goals '' in , . ]",
    "that directly or indirectly calls msws .",
    "we call the heads of defining formulas _ defined goal_s .",
    "each @xmath44 ( @xmath45 ) is recursively a conjunction @xmath46 ( @xmath47 ) of defined goals @xmath48 and msw atoms @xmath49 .",
    "we introduce a binary relation @xmath50 over defined goals such that @xmath51 holds if @xmath43 is the head of some defining formula and @xmath52 occurs in the body . assuming `` @xmath53 '' is acyclic , we extend it to a partial ordering over the defined atoms .",
    "we denote by @xmath54 the whole set of defining formulas and call @xmath54 the _ explanation graph _ for @xmath21 like the non - tabled case .    once @xmath54 is obtained , since defined goals are layered by the `` @xmath53 '' relation by our assumption where the a defining formula in the bottom layer has only msws in the body whose probabilities are known , we can compute probabilities by sum - product operation for all defined goals from the bottom layer upward in a dynamic programming manner in time linear in the size of @xmath54 , i.e.   the number of atoms appearing in @xmath54 .",
    "compared to naive computation , dynamic programming on @xmath54 can reduce time complexity for probability computation from exponential time to polynomial time . for example",
    "prism s probability computation for hmms takes @xmath55 time for a given sequence with length @xmath56 and coincides with the well - known forward - backward algorithm for hmms .",
    "likewise prism s probability computation for pcfgs takes @xmath57 time for a sentence with length @xmath56 and coincides with the computation of inside probability for pcfgs .",
    "more interestingly , bp ( belief propagation ) , one of the standard algorithms for probability computation for bns , coincides with prism s probability computation applied to prism programs that describe junction trees @xcite .",
    "viterbi inference , i.e.  the computation of the viterbi explanation and its probability , is similarly performed on @xmath54 in a bottom - up manner like probability computation stated above .",
    "the only difference is that we use argmax instead of sum . in what follows",
    "we look into how the viterbi explanation is computed .",
    "we use @xmath17 for the set of all parameters .",
    "let @xmath43 be a defined goal and @xmath58 the defining formula for @xmath43 in @xmath54 .",
    "write @xmath59 ( @xmath60 ) and suppose recursively that the viterbi explanation @xmath61 ( @xmath62 ) has already been calculated for each defined goal in @xmath63 in @xmath44",
    ". then the viterbi explanation @xmath64 for @xmath44 and the viterbi explanation @xmath65 for @xmath43 are respectively computed by @xmath66 here @xmath67 is a parameter associated with msw(@xmath68,@xmath69 ) and so on . in this way",
    "the viterbi explanation for the top - goal @xmath21 is computed in a bottom - up manner by scanning @xmath54 once in time linear in the size of @xmath54 , i.e.   exactly the same time complexity as probability computation ; for example @xmath55 for hmms and @xmath57 for pcfgs where @xmath56 is respectively the length of sequence and that of sentence .",
    "parameter learning in prism , be it em , map , vb or vt(explained next ) , is based on computation by dynamic programming on @xmath54 .",
    "for example em in prism computes generalized inside probabilities and generalized outside probabilities for defined goals in @xmath54 using dynamic programming and calculates expectations of the number of occurrences of msw atoms in an sld proof for the top - goal to update parameters in each iteration , similarly to the inside - outside algorithm for pcfgs @xcite .",
    "map ( maximum a posteriori ) estimation and vb ( variational bayes ) inference are also performed similarly @xcite .",
    "in this section we adapt vt to the distribution semantics of prism and derives the vt algorithm for prism .      here",
    "we explain the basic idea of vt without assuming specific distributions .",
    "let @xmath1 be hidden variables , @xmath0 observed ones and @xmath70 their joint distribution with parameters @xmath3 .",
    "we assume @xmath1 and @xmath0 are discrete .",
    "mle estimates parameters @xmath3 from @xmath0 as the maximizer of the ( log ) likelihood function @xmath71 : @xmath72 in the case of map ( maximum a posteriori ) estimation , we add a prior distribution @xmath73 and use @xmath74 below as an objective function : @xmath75    what vt does is similar to mle and map but it uses a different objective function @xmath76 defined as @xmath77 vt estimates parameters as the maximizer of @xmath78 by coordinate ascent that alternates the maximization of @xmath79 w.r.t .",
    "@xmath1 and the maximization of @xmath80 w.r.t .",
    "@xmath3 : @xmath81    starting with appropriate initial parameters @xmath82 , vt iterates the above two steps and terminates when @xmath83 holds ( recall that random variables @xmath1 and @xmath0 are discrete ) . proving the convergence property of vt is straightforward .",
    "@xmath84    so @xmath85 for every @xmath86 since @xmath87 is a monotonically increasing sequence with an upper bound , it converges as @xmath88 goes to infinity .      here",
    "we reformulate vt in the context of prism .",
    "let @xmath4 be a prism program with parameters @xmath17 and @xmath89 a probability measure defined by @xmath4 .",
    "also let @xmath90 be observed goals , and @xmath91 ( @xmath92 ) the set of all explanations @xmath93 for @xmath94 such that @xmath95 .",
    "@xmath90 corresponds to observed variables @xmath0 and @xmath96 to hidden variables @xmath1 in @xmath97 respectively in equations ( [ eq : max - hidden ] ) and ( [ eq : max - param ] ) in subsection  [ subsec : vt ] .",
    "let msw(@xmath5,@xmath36 ) be the set of msw atoms for a multi - valued random switch @xmath5 as before that represents a probabilistic choice from a finite set @xmath98 of possible outcomes such that msw(@xmath5,@xmath6 ) ( @xmath99 ) becomes exclusively true with probability @xmath10 is declared by values/2 - 3 predicate . ] .",
    "since @xmath100 holds , @xmath101 is a point in the probability simplex .",
    "we put @xmath102 and @xmath103 where @xmath5 ranges over possible switch names .",
    "we introduce as a prior distribution dirichlet distribution @xmath104 with hyper parameters @xmath105 over @xmath101 and their product distribution @xmath106 . in the following , to avoid the difficulty of zero - probability encountered in parameter learning , we assume _ pseudo count _ @xmath107 and use @xmath108 in place of @xmath109 .",
    "finally recall the _ viterbi explanation _",
    "@xmath110 for a goal @xmath94 is a most probable explanation for @xmath94 given by @xmath111    by substituting @xmath112 for @xmath0 and @xmath96 for @xmath1 in the definition of @xmath113 , the objective function @xmath114 for vt in prism is now computed as follows .",
    "@xmath115 where `` @xmath116 '' ranges over those such that msw(@xmath5,@xmath6 ) appears in some @xmath110 and @xmath117 is the count of msw(@xmath5,@xmath6 ) in @xmath110 .",
    "likewise by substituting @xmath118 for @xmath0 and @xmath96 for @xmath1 in equations ( [ eq : max - hidden ] ) and ( [ eq : max - param ] ) respectively and using the definition of @xmath110 , we obtain the vt algorithm for prism which alternately executes ( [ eq : max - hidden - prism ] ) and ( [ eq : max - param - prism ] ) where @xmath119 stands for the set of parameters @xmath120 at step @xmath88 .",
    "@xmath121    here ( [ eq : max - hidden - prism ] ) corresponds to ( [ eq : max - hidden ] ) and ( [ eq : max - param - prism ] ) to ( [ eq : max - param ] ) respectively .    using ( [ eq : max - hidden - prism ] ) and ( [ eq : max - param - prism ] ) ,",
    "vt in prism is performed as follows .",
    "given observed goals @xmath90 , we first perform tabled search for all explanations to build explanations graphs @xmath122 for each @xmath37 ( @xmath92 ) . then starting from the initial parameters @xmath123 , we repeat ( [ eq : max - hidden - prism ] ) and ( [ eq : max - param - prism ] ) alternately while computing the viterbi explanations @xmath124 in ( [ eq : max - hidden - prism ] ) by dynamic programming over @xmath122 as explained in section  [ sec : review - prism ] until @xmath125 holds for all @xmath37 ( @xmath126 ) .",
    "@xmath127 are then learned parameters .",
    "+ having derived the vt algorithm for prism , we examine the effect of the termination condition @xmath125 ( @xmath128 ) on the convergence of vt . as we remarked in section  [ intro ] , this condition means vt terminates as soon as the viterbi explanations converge , i.e.   there is no change of the viterbi explanations between step @xmath88 and step @xmath129 whereas em always runs until convergence of parameters . as a result since a small change of parameters does not affect the viterbi explanation but keeps em running , vt tends to converge in much less number of iterations than em .",
    "to empirically check this , we conducted parameter learning of probabilistic grammars by vt and by em using prism and compared their convergence behavior2 cpu and 72 gb ram running opensuse 11.2 , using prism2.1 . ] .",
    "we used two probabilistic grammars , a pcfg and a plcg ( probabilistic left - corner grammar ) for the atr corpus @xcite(their details are described in the next section ) , and measured the average number of iterations and learning time ) to measure learning time which returns in @xmath1 time used by the learning algorithm . ]",
    "required for convergence over ten runs .",
    "table  [ table : ave - itr ] summarizes the results with standard deviations in parentheses .",
    "ccccc    ' '' ''    & & +    ' '' ''     + [ -6pt ] & vt & em & vt & em + pcfg & 8.10(2.28 ) & 123.6(3.23 ) & 0.45(0.11 ) & 6.29(0.16 ) + plcg & 15.80(4.73 ) & 144.2(43.51 ) & 1.55(0.36 ) & 11.686(3.64 ) +    looking at the table , we see that vt required only a small number of iterations to converge compared to em ; the ratio of average number of iterations of vt to em is 1:15.2 w.r.t .",
    "the pcfg and 1:8.3 w.r.t .",
    "we also note that the ratio of average learning time ) .",
    "since such extra - time accounts for a large percent of total learning time , it can happen that the difference in total learning time between em and vt is smaller than table  [ table : ave - itr ] .",
    "] is similar to that of iterations , 1:13.8 w.r.t .",
    "the pcfg and 1:7.4 w.r.t .",
    "the plcg respectively .",
    "it therefore seems natural to conclude that vt learns parameters with much less number of iterations and thereby much faster than em .    since vt is a local maximizer , it is sensitive to the initial condition like em .",
    "so we need to carefully choose @xmath130 .",
    "uniform distributions for @xmath130 @xcite and @xmath131(@xmath128 ) @xcite are possible choices . in practice , we further add random restart to alleviate the sensitivity problem .",
    "for example in the experiments in the next section , we repeated parameter learning 50 times with random restart for each learning and selected the parameter set giving the largest value of the objective function @xmath114 computed by ( [ eq : objective - lvt ] ) .",
    "in this section we apply vt to parsing tasks in natural language processing where observable variables are sentences and hidden variables are parse trees .",
    "we predict parse trees for given sentences using probabilistic grammars ( pcfg and plcg ) whose parameters are learned by vt and compare the parsing performance with each of em , map and vb .      prior to describing the parameter learning experiment with a pcfg by vt",
    ", we briefly review how to write pcfgs in prism . in pcfgs ,",
    "sentence derivation is carried out probabilistically .",
    "when there are @xmath132 pcfg rules @xmath133 for a nonterminal @xmath134 with probabilities @xmath135 ( @xmath136 ) , @xmath134 is expanded by @xmath137 into @xmath138 with probability @xmath139 .",
    "the probability of a parse tree @xmath140 is the product of probabilities associated with occurrences of cfg rules in @xmath140 and the probability of a sentence is the sum of probabilities of parse trees for the sentence .",
    "writing pcfg programs is easy in prism .",
    "[ prog : pcfg ] is a prism program for a pcfg \\ {",
    "0.4:s@xmath141s s , 0.3:s@xmath141a , 0.3:s@xmath141b } . in general , pcfg rules such as \\ { @xmath142 } are encoded by values/3 declaration as    values(a,[@xmath143,[@xmath135 ] )    where @xmath138 ( @xmath145 ) is a prolog list of terminals and nonterminals .    ' '' ''    .... values('s',[['s','s'],[a],[b]],[0.4,0.3,0.3 ] ) .",
    "pcfg(l):- pcfg(['s'],l , [ ] ) .",
    "pcfg([a|r],l0,l2):-    ( get_values(a , _ ) - >   % msw(a , _ ) exists , so        msw(a , rhs ) ,        % a is a nonterminal        pcfg(rhs , l0,l1 )    ; l0=[a|l1 ] ) ,    pcfg(r , l1,l2 ) .",
    "pcfg([],l , l ) .",
    "....    ' '' ''    we wrote a pcfg program as shown in fig .",
    "[ prog : pcfg ] for the atr corpus @xcite using an associated cfg .",
    "the corpus contains labeled parse trees for 10,995 japanese sentences whose average length is about 10 .",
    "the associated cfg comprises 861 cfg rules ( 168 non - terminals and 446 terminals ) and yields 958 parses / sentence on average .",
    "we applied four learning algorithms , i.e.  vt , em , map and vb @xcite available in prism2.1 to the pcfg program for the atr corpus[multiblock footnote omitted ] and compared the performance of vt with other learning methods .",
    "+ we conducted eight - fold cv ( cross validation ) for each algorithm to evaluate the quality of learned parameters in terms of three performance metrics i.e.  lt(labeled tree ) , bt(bracketed tree ) and 0-cb(zero crossing brackets ) @xcite .",
    "these metrics are computed from @xmath146 , the set of parse trees in a test corpus which are considered correct and @xmath147 , the set of parse trees predicted for sentences in the test corpus by a parsing algorithm .",
    "lt is defined as @xmath148 where @xmath149 denotes the number of elements in a set @xmath150 and @xmath151 .",
    "it is the ratio of correctly predicted labeled parse trees to the total number of labeled parse trees .",
    "compared to lt , bt is a less strict metric that ignores nonterminals in parse trees .",
    "let @xmath152 be the set of unlabeled trees obtained by removing nonterminals from @xmath147 which coincide with the corresponding unlabeled trees in @xmath146 .",
    "then bt is defined as @xmath153 .",
    "finally 0-cb is the least strict metric in the three metrics .",
    "we say brackets @xmath154 in a tree @xmath140 is inconsistent with another tree @xmath155 if @xmath155 contains brackets @xmath156 such that @xmath157 or @xmath158 .",
    "otherwise they are consistent with @xmath155 .",
    "let @xmath159 be the set of trees in @xmath147 which have no inconsistent brackets with the corresponding trees in @xmath146 .",
    "then 0-cb is given by @xmath160 .",
    "+ to perform cross validation , the entire corpus is partitioned into eight sections . in each fold , one section is used as a test corpus and sentences in the remaining sections are used as training data .",
    "for each of em , map , vt and vb , parameters ( or pseudo counts ) are learned from the training data .",
    "a parse tree is predicted , i.e.  the viterbi explanation is computed for each sentence in the test corpus using learned parameters or using the approximate a posterior distribution learned by vb .",
    "the predicted trees are compared to answers , i.e.  the labeled trees in the test corpus to compute lt , bt and 0-cb respectively .",
    "the final performance figures are calculated as averages over eight folds and summarized in table  [ table : parsing - pcfg ] with standard deviations in parentheses .",
    "ccccc    ' '' ''    & +    ' '' ''     + [ -6pt ] metric & vt & em & map & vb + lt(% ) & * 74.69*(0.87 ) & 70.02(0.88 ) & 70.31(1.13 ) & 72.13(1.10 ) + bt(% ) & * 77.87*(0.84 ) & 73.10(1.01 ) & 73.45(1.20 ) & 75.46(1.13 ) + 0-cb(% ) & 83.78(0.92 ) & 84.44(0.89 ) & 84.89(0.84 ) & * 87.08*(0.87 ) +    we statistically analyzed the parsing performance by dunnett s test .",
    "the result is that vt outperformed all of em , map and vb in terms of lt and bt at the 5% level of significance but did not so in terms of 0-cb .",
    "this is understandable if we assume that there are many parse trees that can give high scores in terms of less restrictive metrics such as 0-cb but since vt concentrates probability mass on a single tree , those promising trees are allocated little probability mass by vt , which results in relatively low performance of vt in terms of 0-cb .",
    "so far we examined parsing performance by parameters obtained from incomplete data ( sentences in the corpus ) .",
    "we also examined parsing performance using 8-fold cv by parameters learned from complete data , i.e.  by parameters obtained by counting occurrences of cfg rules in the corpus .",
    "the result is lt:79.06%(1.25 ) , bt:85.28%(0.69 ) , 0-cb:95.37%(0.26)(figures in parentheses are standard deviations ) .",
    "these figures are considered as the best possible performance .",
    "we notice the gap in parsing performance between the complete data case and the incomplete data case tends to become wider as the performance metric gets less restrictive in the order of lt , bt and 0-cb .",
    "another thing to note is that the objective functions for em , map and vb are similar in the sense that they all sum out hidden variables whereas the objective function for vt retains them .",
    "this fact together with fig .",
    "[ table : parsing - pcfg ] seems to suggest that parsing performance is more affected by the difference among objective functions than the difference among learning methods .      ' '' ''    .... values(lc('s','s'),[rule ( 's ' , [ 's ' , 's ' ] ) ] ) .",
    "values(lc('s',a),[rule('s',[a ] ) ] ) .",
    "values(lc('s',b),[rule('s',[b ] ) ] ) .",
    "values(first('s'),[a , b ] ) .",
    "values(att('s'),[att , pro ] ) .",
    "plcg(l):- g_call(['s'],l , [ ] ) .",
    "g_call([],l , l ) .",
    "g_call([g|r],[wd|l],l2):-     ( g = wd - > l1 = l      % shift operation     ; msw(first(g),wd),lc_call(g , wd , l , l1 ) ) ,     g_call(r , l1,l2 ) .",
    "lc_call(g , b , l ,",
    "l2):-        % b - tree is completed        msw(lc(g , b),rule(a,[b|rhs2 ] ) ) ,     ( g = a - > true ; values(lc(g , a ) , _ ) ) ,     g_call(rhs2,l , l1 ) ,      % complete a - tree     ( g = a - > att_or_pro(a , op ) ,        ( op = att - > l2 = l1 ; lc_call(g , a , l1,l2 ) )     ; lc_call(g , a , l1,l2 ) ) .",
    "att_or_pro(a , op):-     ( values(lc(a , a ) , _ ) - >",
    "msw(att(a),op ) ; op = att ) . ....    ' '' ''",
    "pcfgs assume top - down parsing .",
    "contrastingly there is a class of probabilistic grammars based on bottom - up parsing for cfgs called plcgs ( probabilistic left - corner grammars ) @xcite .",
    "although they use the same set of cfg rules as pcfgs but attach probabilities not to expansion of nonterminals but to three elementary operations in bottom - up parsing , i.e.  shift , attach and project . as a result",
    "they define a different class of distributions from pcfgs . in this subsection",
    "we conduct an experiment for parameter learning of a plcg by vt .",
    "the objective of this subsection is two fold .",
    "one is to apply vt to a plcg , which seems not attempted before as far as we know , and to examine the parsing performance .",
    "the other is to empirically demonstrate the universality of our approach to vt that subsumes differences in probabilistic models as differences in explanation graphs and applies a single vt algorithm to the latter .",
    "programs for plcgs look very different from those for pcfgs .",
    "[ prog : plcg ] is a plcg program which is a dual version of the pcfg program in fig .",
    "[ prog : pcfg ] with the same underlying cfg \\ { s@xmath141s s , s@xmath141a , s@xmath141b}. it generates sentences using the first set of s and the left - corner relation for this cfg ( values/2 there only declares the space of outcomes ) .",
    "the program works as follows",
    ". suppose nonterminals g and b are in the left - corner relation and g is waiting for a b - tree , i.e.   a subtree with the root node labeled b , to be completed .",
    "when a b - tree is completed , the program probabilistically chooses a cfg rule of the form @xmath161 to further grow the b - tree using this rule . upon the completion of the a - tree and if g = a , the attach operation or the projection is probabilistically chosen . by replacing values declarations appropriately , this program is applicable to any plcg .",
    "ccccc    ' '' ''    & +    ' '' ''     + [ -6pt ] metric & vt & em & map & vb + lt(% ) & * 76.26*(0.96 ) & 71.81(0.91 ) & 71.17(0.93 ) & 71.15(0.90 ) + bt(% ) & * 78.86*(0.70 ) & 75.17(1.15 ) & 74.28(1.12 ) & 74.28(1.00 ) + 0-cb(% ) &",
    "* 87.45*(1.00 ) & 86.49(0.97 ) & 86.03(0.67 ) & 86.04(0.71 ) +    we have developed a plcg program similarly to the pcfg program for the atr corpus and applied vt , em , map and vb to it to learn parameters .",
    "we measured parsing performance by learned parameters in terms of lt , bt and 0-cb by eight - fold cv for each of vt , em , map and vb and obtained table  [ table : parsing - plcg ] ( standard deviations in parentheses ) .",
    "we compared the parsing performance of vt with em , map and vb by dunnett s test at the 5% level of significance similarly to the pcfg case .",
    "this time however vt outperformed all of em , map and vb by all metrics , i.e.  lt , bt and 0-cb .",
    "in the previous section , we conducted learning experiments with a pcfg and a plcg in which the prediction target was parse trees that coincide with a hidden variable in a probabilistic model . in this section",
    ", we deal with a different situation where a prediction target differs from a hidden variable .",
    "we apply vt to classification tasks using an _ nbh_(naive bayes with a hidden variable ) model whose hidden variable is summed out and instead an observable variable , class label , is predicted for the given data .        before explaining classification tasks , we review nbh for completeness @xcite .",
    "nbh is an extension of nb ( naive bayes ) with a hidden class variable @xmath162 as illustrated in fig .",
    "[ fig : nbh1 ] .",
    "it defines a joint distribution @xmath163 where is model parameters , the @xmath164 s attributes of observed data as data when the context is clear .",
    "] , @xmath52 a class and @xmath162 a hidden class .",
    "it is easily seen from the equation ( [ eq : mixture ] ) below that nbh represents the data distribution in a class @xmath52 as a mixture of data distributions indexed by @xmath162 .",
    "@xmath165 the role of @xmath162 is to cluster data in a class @xmath52 so that a distribution @xmath166 in each cluster @xmath162 satisfies the independent condition @xmath167 imposed on nb as much as possible .",
    "nbh was introduced in @xcite as a simple substitute for more complicated variants of nb such as tan @xcite , aode @xcite , bnc @xcite , fbc @xcite and hbn @xcite .",
    "given data @xmath168 , we classify @xmath168 as a class @xmath169 by @xmath170 note here that the hidden variable , @xmath162 , is _ not _ a prediction target unlike probabilistic grammars .",
    "it is just summed out .",
    "however we expect a sub - classifier @xmath171 indexed by @xmath162 performs better than @xmath172 , the original nb , in each cluster and so does their mixture ( see equation ( [ eq : expert - mixture ] ) ) . +    ' '' ''    .... values(class,[democrat , republican ] ) .",
    "% class labels are democrat or republican values(attr(_a,_c,_hc),[y , n ] ) .",
    "% attribute values are y or n    nbayes(c , vals):-     msw(class , c),msw(hclass(c),hc),nbh(1,c , hc , vals ) .",
    "nbh(j , c , hc,[v|vals]):-     ( v = = ' ? ' - > msw(attr(j , c , hc ) , _ )   % ' ? ' indicates missing value     ; msw(attr(j , c , hc),v ) ) ,     j1 is j+1 ,     nbh(j1,c , hc , vals ) .",
    "nbh ( _ , _ , _ , [ ] ) .",
    "....    ' '' ''    to evaluate the quality of parameters learned by vt for classification tasks , we conducted a learning experiment with nbh using ten data sets from the uci machine learning repository@xcite . in the experiment training data",
    "is given as a set of tuples @xmath173 consisting of a class @xmath52 and attributes @xmath168 .",
    "parameters ( or pseudo counts ) are learned by a prism program shown in fig .",
    "[ prog : nbh ] which is also used for predicting class labels in test data .",
    "a values/2 declaration ` values(class,[democrat , republican ] ) ` in the program tells prism to introduce two msw atoms , msw(class , democrat ) and msw(class , republican ) that represent a probabilistic choice between democrat and republican as a class , implicitly together with their parameters @xmath174 and @xmath175 such that @xmath174 + @xmath175 = 1 .",
    "this program assumes that attributes are numbered and missing values in a data set are replaced with ?.",
    "ccccccc    ' '' ''    & & nb & +    ' '' ''     + data set & size & em(% ) & vt(% ) & em(% ) & map(% ) & vb(% ) + nursery & 12960 & 90.23 & 92.93 & 99.40 & * 99.65 * & 97.45 + mushroom & 8124 & 99.57 & * 100.00 * & * 100.00 * & * 100.00 * & 99.99 + kr - vs - kp & 3196 & 87.86 & 88.69 & 91.59 & * 92.34 * & 88.90 + car & 1728 & 85.86 & 90.97 & 97.67 & * 97.82 * & 94.68 + votes & 435 & 90.29 & 96.00 & 95.66 & * 96.51 * & 96.05 + dermatology & 336 & 97.73 & 97.98 & 97.51 & 98.06 & * 98.17 * + glass & 214 & 72.82 & 75.86 & * 76.84 * & 76.66 & 76.53 + iris & 150 & 94.40 & 95.07 & * 95.13 * & 95.07 & 95.07 + breast - cancer & 150 & 72.52 & 72.52 & 70.07 & 72.76 & *",
    "72.83 * + zoo & 101 & 95.07 & 96.55 & * 97.42 * & 96.95 & 96.62 +    we obtained the classification accuracy of nbh for each combination of data set , learning method ( vt , em , map , vb ) , the number of clusters @xmath176 in a class @xmath52 ( from 2 to 15 ) and hyper parameters ( \\{0.1,1.0 } as @xmath109 for vb , and the same as pseudo counts @xmath108 for vt , map ) as the average over ten times ten - fold cv except nursery , mushroom and kr - vs - kp data sets in which case ten - fold cv was used .",
    "we similarly obtained the classification accuracy of nb as baseline .",
    "table  [ table : acc ] summarizes classification accuracies of nb and nbh . accuracy for nbh in the table is the best accuracy obtained by varying @xmath176 and hyper parameters as we mentioned for the given learning method and data set .",
    "figures in bold face indicate the best accuracy achieved in each data set .",
    "the table shows that for most data sets nbh performed better than nb as we expected .",
    "actually the difference in accuracy between nb and the best one for nbh is statistically significant by unpaired t - test at the 5% level with the bonferroni correction for all data sets except dermatology , iris and breast - cancer .",
    "the superiority of nbh over nb demonstrated in this experiment is interpreted as an effect of clustering in a class by introducing a hidden variable @xmath162 .    comparing the classification accuracies by four parameter learning methods applied to nbh",
    ", we notice that vt s performance is comparable to the other three , i.e.  em , map and vb except for the case of nursery , kr - vs - kp and car data sets . for these data sets vt",
    "s accuracy is worse than the best one achieved by one of the three learning methods , which is statistically confirmed by unpaired t - test at the 5% level of significance with the bonferroni correction . so from the viewpoint of a learning experiment with nbh , we can not say , regrettably , vt outperformed em , map and vb for all data sets",
    ". however , the result is understandable if we recall that while the predication target in the experiment is a class variable @xmath52 , vt optimizes parameters not for @xmath52 but for the hidden variable @xmath162 which is summed out and hence only indirectly affects prediction .",
    "prism assumes the exclusiveness condition on programs to simplify probability computation as explained in section  [ sec : review - prism ] .",
    "it means we can not write a program clause @xmath177 unless @xmath178 is guaranteed @xcite . although most of generative probabilistic models such as bns , hmms and pcfgs are naturally described as prism programs satisfying the condition , removing it certainly gives us more freedom of probabilistic modeling .",
    "theoretically it is possible to remove it by introducing bdds ( binary decision diagrams ) as problog @xcite and pita @xcite do , and their related systems , leproblog@xcite , lfi - problog@xcite and emblem@xcite , offer parameter learning based on probability computation by bdds , though with different learning frameworks from prism .",
    "if , however , we are only interested in obtaining the viterbi explanation after parameter learning as we are in many cases , vt gives us a way of doing it without bdds even for programs that do not satisfy the exclusiveness condition .",
    "this is because vt does not require the exclusiveness condition to execute equations ( [ eq : max - hidden - prism ] ) and ( [ eq : max - param - prism ] ) that always deal with a single explanation and a single probability .    ' '' ''    ....",
    "values(d_e(1,2),[on , off],[0.9,0.1 ] ) .",
    "values(d_e(2,3),[on , off],[0.8,0.2 ] ) .",
    "values(d_e(3,4),[on , off],[0.6,0.4 ] ) .",
    "values(d_e(1,6),[on , off],[0.7,0.3 ] ) .",
    "values(d_e(2,6),[on , off],[0.5,0.5 ] ) .",
    "values(d_e(6,5),[on , off],[0.4,0.6 ] ) .",
    "values(d_e(5,3),[on , off],[0.7,0.3 ] ) .",
    "values(d_e(5,4),[on , off],[0.2,0.8 ] ) .",
    "d_e(1,2):- msw(d_e(1,2),on ) .",
    "d_e(2,3):- msw(d_e(2,3),on ) .",
    "d_e(3,4):- msw(d_e(3,4),on ) .",
    "d_e(1,6):- msw(d_e(1,6),on ) .",
    "d_e(2,6):- msw(d_e(2,6),on ) .",
    "d_e(6,5):- msw(d_e(6,5),on ) .",
    "d_e(5,3):- msw(d_e(5,3),on ) .",
    "d_e(5,4):- msw(d_e(5,4),on ) .",
    "path(x , y ) : - path(x , y,[x ] ) .",
    "path(x , x , _ ) .",
    "path(x , y , a):- x\\==y , ( d_e(x , z ) ; d_e(z , x ) ) , absent(z , a ) , path(z , y,[z|a ] ) .",
    "absent ( _ , [ ] ) .",
    "x\\==y , absent(x , z ) .",
    "....    ' '' ''    we next give an example of parameter learning by vt followed by the computation of the viterbi explanation for a program that violates the exclusiveness condition .",
    "[ prog : problog_graph ] is a prism program translated from a problog program that computes a path between two nodes ( and its probability ) in a graph .",
    "the graph has six nodes .",
    "edges are assigned probabilities and we express this fact by attaching an msw atom to an atom d_e(@xmath1,@xmath0 ) representing an edge @xmath179 in the program . for example ( directed ) edge d_e(1,2 ) between node 1 and node 2",
    "is assigned probability 0.9 as indicated by msw(d_e(1,2),on ) following its value declaration values(d_e(1,2),[on , off],[0.9,0.1 ] ) in the program .",
    "observe that a ground top - goal path(x , y ) causes a call to d_e(x , z ) with x ground and z free that calls more than one clause , which leads to the violation of the exclusiveness condition .",
    "nonetheless we can learn parameters by vt and compute the viterbi explanation for this program .",
    "[ prog : sample_session ] is a sample session doing this . in fig .",
    "[ prog : sample_session ] , we first compute the viterbi path ve , i.e.  the most probable path between node 1 and node 4 and its probability p by applying the built - in predicate viterbif/3 to goal path(1,4 ) .",
    "we next renew parameters by learning them using vt from observed goals path(1,4),path(1,3 ) ...    ' '' ''    .... ?",
    "- viterbif(path(1,4),p,_x),viterbi_switches(_x , ve ) p = 0.432 ve = [ msw(d_e(1,2),on),msw(d_e(2,3),on),msw(d_e(3,4),on ) ]    ?",
    "- set_prism_flag(learn_mode , ml_vt ) .    ?",
    "- learn([path(1,4),path(1,3),path(2,4),path(2,5),path(3,6 ) ] ) . ...    ?",
    "- viterbif(path(1,4),p,_x),viterbi_switches(_x , ve ) p = 0.104 ve = [ msw(d_e(1,6),on),msw(d_e(6,5),on),msw(d_e(5,4),on ) ] ....    ' '' ''    finally we compute the viterbi path again that is determined by learned parameters and see whether the learning changes the viterbi path or not . in the session , the viterbi path changed after learning from 1 - > 2 - > 3 - > 4 to 1 - > 6 - > 5 - > 4 together with their probabilities from 0.432 to 0.161 .",
    "vt thus enables us to learn parameters from programs that violate the exclusiveness condition .",
    "however we have to recall at this point that vt has an objective function different from likelihood and is biased @xcite , and the effect of removing the exclusiveness condition on the quality of parameters estimated by vt is unknown at the moment , which remains as a future research topic .",
    "vt is closely related to k - means @xcite which is a standard clustering method for continuous data .",
    "if we apply vt to a gaussian mixture for clustering of continuous data with an assumption of a common variance to all composite gaussian distributions , the resulting algorithm is identical to k - means . in this sense",
    ", the usefulness of vt is established .",
    "actually vt has been used in various settings @xcite and also in the srl frameworks that deal with structured data @xcite where the algorithmic essence of vt , coordinate ascent on parameters and target variables with argmax operation applied to the latter , is used .    despite its popularity",
    "however , it seems that vt so far has been model - specific and only model - specific vt algorithms have been implemented . in this paper",
    "we gave a unified treatment to vt for discrete models for the first time to our knowledge , and derived the vt algorithm for prism which is a single generic algorithm applicable to any discrete model as long as the model is described by a prism program .",
    "since our derivation of vt is based on the reduction of goals to and - or propositional formulas , it seems quite possible for other logic - based modeling languages that use bdds such as problog @xcite and pita @xcite to introduce vt as a parameter learning routine .",
    "one of the unique features of vt is its affinity with discriminative modeling .",
    "write the vt s objective function @xmath76 as follows .",
    "@xmath180 this means that although prism is intended for generative modeling , vt in prism computes the viterbi explanation @xmath181 that gives the highest conditional probability @xmath182 for @xmath0 whose form is identical to the objective function in discriminative modeling and the viterbi explanation is chosen in the same way as discriminative modeling does provided the hidden variable is a prediction target . when this condition is met vt shows good performance as demonstrated by the experiments in section  [ sec : grammar ] but if not , vt does not necessarily outperform other parameter learning methods as exemplified in section  [ sec : nbh ] .",
    "it therefore seems reasonable to say that vt is effective for prediction tasks when the prediction target coincides with hidden variables in a probabilistic model , though we obviously need more experiments .    as a coordinate ascent local hill - climber",
    ", vt is sensitive to the initial parameters and also sensitive to the viterbi explanation . to mitigate the sensitivity problem with initial parameters , we used 50 time random restart in the learning experiments in section  [ sec : grammar ] . to cope with the sensitivity to the viterbi explanation , it is interesting to introduce @xmath132-best explanations as discussed in @xcite and replace the viterbi explanation in vt with them .",
    "this approach will give us control over the sensitivity and computation time by choosing @xmath132 and seems not very difficult to implement in prism as @xmath132-best explanations for a goal @xmath21 are already computed by built - in predicates such as n_viterbi(@xmath132,@xmath21 ) .",
    "since vt in prism runs on explanation graphs obtained from all solution search , it requires time for all solution search ( by tabling ) and also space to store discovered explanation graphs .",
    "it is possible , however , to implement vt without explanation graphs , and to realize much more memory saving vt by repeating search for a viterbi explanation in each cycle of vt .",
    "we note this approach particularly fits well with mode - directed tabling @xcite . in mode - directed tabling ,",
    "we can search for partial viterbi explanations for subgoals efficiently without constructing explanation graphs and put them together to form a larger viterbi explanation for the goal .",
    "currently however mode - directed tabling is not available in prism .",
    "we are planing to incorporate it in prism in the near future .",
    "we introduced vt ( viterbi training ) to prism to enhance prism s probabilistic modeling power .",
    "prism becomes the first srl ( statistical relational learning ) language @xcite in which vt is available for parameter learning to our knowledge .",
    "although vt has already been used in various models under various names @xcite , we made the following contributions to vt .",
    "one is a generalization by deriving a generic vt algorithm for prism , thereby making it uniformly applicable to a very wide class of discrete models described by prism programs ranging from bns to probabilistic grammars .",
    "the other is an empirical clarification of conditions under which vt performs well .",
    "we conducted learning experiments with a pcfg and a plcg using vt and confirmed vt s excellent parsing performance compared to em , map and vb .",
    "we also conducted a learning experiment with nbh for classification tasks . putting the results of these experiments together",
    ", we may say that vt performs well when hidden variables are a prediction target .    from the viewpoint of prism ,",
    "vt improves prism first by realizing faster convergence compared to em , second by providing the user with a parameter learning method that can learn parameters good for prediction , and third by providing a solution to the problem of the exclusiveness condition that hinders prism programming .",
    "thanks to vt , we are now able to use arbitrary programs with inclusive - or for probabilistic modeling .",
    "last but not least we can say that as vt in prism is general and applicable to any prism program , it largely reduces the need for the user to develop a specific vt algorithm for a specific model . furthermore since vt in prism can be used just by setting a prism flag appropriately , it makes vt easily accessible to ( probabilistic ) logic programmers .",
    "probabilistic inductive logic programming . in _",
    "probabilistic inductive logic programming - theory and applications _ , l.  de raedt , p.  frasconi , k.  kersting , and s.  muggleton , eds .",
    "lecture notes in computer science 4911 .",
    "springer , 127 .    ,",
    "kimmig , a. , and toivonen , h. 2007 . : a probabilistic prolog and its application in link discovery . in _ proceedings of the 20th international joint conference on artificial intelligence ( ijcai07)_. 24682473 .            ,",
    "kimmig , a. , kersting , k. , and de  raedt , l. 2008 .",
    "parameter learning in probabilistic databases : a least squares approach . in _",
    "machine learning and knowledge discovery in databases , proceedings of european conference , ecml / pkdd 2008 , part i_. 473488 .    , thon , i. , and de  raedt , l. 2011 .",
    "learning the parameters of probabilistic logic programs from interpretations . in _",
    "machine learning and knowledge discovery in databases , proceedings of european conference , ecml / pkdd 2011 , part i , lncs6911_. 581596 .",
    "discriminative training of markov logic networks . in _ in proceedings of the twentieth national conference on artificial intelligence ( aaai-05)aaai ) _ ,",
    "m.  m. veloso and s.  kambhampati , eds .",
    "868873 .    ,",
    "alshawi , h. , jurafsky , d. , and manning , c. 2010 .",
    "viterbi training improves unsupervised dependency parsing . in _ proceedings of the fourteenth conference on computational natural language learning_. 917 .              ,",
    "kameya , y. , and sato , t. 2010 .",
    "mode - directed tabling for dynamic programming , machine learning , and constraint solving . in _ proceedings of the 22th international conference on tools with artificial intelligence ( ictai-2010)_."
  ],
  "abstract_text": [
    "<S> vt ( viterbi training ) , or hard em , is an efficient way of parameter learning for probabilistic models with hidden variables . given an observation @xmath0 , it searches for a state of hidden variables @xmath1 that maximizes @xmath2 by coordinate ascent on parameters @xmath3 and @xmath1 . in this paper </S>",
    "<S> we introduce vt to prism , a logic - based probabilistic modeling system for generative models . </S>",
    "<S> vt improves prism in three ways . </S>",
    "<S> first vt in prism converges faster than em in prism due to the vt s termination condition . </S>",
    "<S> second , parameters learned by vt often show good prediction performance compared to those learned by em . </S>",
    "<S> we conducted two parsing experiments with probabilistic grammars while learning parameters by a variety of inference methods , i.e.  vt , em , map and vb . </S>",
    "<S> the result is that vt achieved the best parsing accuracy among them in both experiments . </S>",
    "<S> also we conducted a similar experiment for classification tasks where a hidden variable is not a prediction target unlike probabilistic grammars . </S>",
    "<S> we found that in such a case vt does not necessarily yield superior performance . </S>",
    "<S> third since vt always deals with a single probability of a single explanation , viterbi explanation , the exclusiveness condition that is imposed on prism programs is no more required if we learn parameters by vt .    </S>",
    "<S> last but not least we can say that as vt in prism is general and applicable to any prism program , it largely reduces the need for the user to develop a specific vt algorithm for a specific model . furthermore since vt in prism can be used just by setting a prism flag appropriately , it makes vt easily accessible to ( probabilistic ) logic programmers . to appear in theory and practice of logic programming ( tplp ) .    </S>",
    "<S> # 1    [ firstpage ]    viterbi training , prism , exclusiveness condition </S>"
  ]
}