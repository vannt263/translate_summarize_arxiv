{
  "article_text": [
    "the ability of neural nets to be universal approximators has been proved by @xcite and studied by further authors in different contexts .",
    "for instance , neurons or small neuronal groups implementing `` plane wave responses '' have been considered by @xcite and @xcite .",
    "as well , pairs of neurons implementing `` windows '' have been investigated by @xcite .",
    "any `` complete enough '' basis of functions which is able to span a sufficiently large vector space of response functions is of interest , and , for instance , the wavelet analysis has been the subject of a complete investigation by @xcite and @xcite .    in this paper , we visit again the subject of a linear reconstruction of tasks , but with an emphasis upon neglecting the usual `` translational '' parameters .",
    "we mainly use a scale parameter only .",
    "this is somewhat different from the usual wavelet approach , which takes advantage of both translation and scale .",
    "but we shall find that a multifrequency reconstruction of tasks occurs as well .",
    "simultaneously , we separate a `` radial '' from an `` angular '' analysis of the task . finally , for the sake of robustness and biological relevance , we introduce a significant amount of randomness , corrected by training , in the choice of the implemented neuronal parameters .",
    "furthermore , our basic neuronal units can be those `` window - like '' pairs advocated earlier @xcite , because of biological relevance too . such deviations from the more rigorous approaches of @xcite and @xcite are expected to make cheaper the practical implementation of such neural nets .",
    "we also investigate two training operations .",
    "the first one consists in a trivial optimization of the output synaptic layer connecting a layer of intermediate , `` elementary task neurons '' to an output , purely _ linear _",
    "the second training consists in optimizing the scale parameters of such a layer of intermediate neurons .",
    "it will be found that one may start from random values of such parameters and , however , sometimes reach solutions where some among the intermediate neurons are driven to become identical .",
    "this `` dynamical identification '' training will be discussed .    in section",
    "ii we describe our formalism , including a traditional universality theorem .",
    "we also reduce the realistic , multi - dimensional situations to a one - dimensional problem . in section iii",
    "we illustrate such considerations by numerical examples of network training .",
    "finally section iv contains our discussion and conclusion .",
    "consider an input @xmath0 which must be processed into an output ( a task ) @xmath1 this input is here taken to be a positive number , such as the intensity of a spike or the average intensity ( or frequency ) of a spike train .",
    "one may view @xmath2 as a `` radial '' coordinate in a suitable space .",
    "there is no loss of generality in restricting @xmath2 to be a positive number , because , should negative values of @xmath2 be necessary for the argument , then @xmath3 could always be split into an even and odd parts , @xmath4/2,$ ] respectively .",
    "such even and odd parts need only be known for @xmath5 obviously . outputs , in turn , will have both signs , in order to account for both excitation or inhibition .",
    "finally there is no need to tell a scalar task @xmath6 from a vector task @xmath7 since any component @xmath8 boils down to a separate scalar task , and this can be processed by a parallel architecture .",
    "consider now neuronal units which , for instance may be excitatory - inhibitory pairs of neurons providing a window - like elementary response .",
    "or they may be more complicated assemblies of neurons , providing a more elaborate `` mother wavelet '' , such as a `` mexican hat '' .",
    "we denote @xmath9 the response function of such a unit and , for short , call this unit a `` formal neuron '' ( fn ) .",
    "the traditional wavelet approach uses a set of such fn s with various thresholds @xmath10 and scale sensitivities @xmath11 hence a space of elementary responses @xmath12.$ ] the same approach expands @xmath3 in this set , @xmath13,\\ ] ] where the integral is most often reduced to a discrete sum . also , @xmath10 and @xmath14 do not need to be independent parameters .",
    "the expansion coefficients , @xmath15 are output synaptic weights and are the unknowns of the problem .",
    "this well known architecture is shown in figure 1 .",
    "the following , seemingly poorer , but simpler expansion , @xmath16 does not use the translation parameter @xmath17 here it is assumed that there exists a suitable electronic or biological tuning mechanism , able to recruit or adjust fn s with suitable gains @xmath18 but no threshold tuning .",
    "such gains are positive numbers , naturally .",
    "the outputs of such fn s are then added , via synaptic output efficiencies @xmath19 which can be both positive and negative , namely excitatory and inhibitory , respectively .",
    "the coefficient @xmath20 is introduced in eq .",
    "( [ basicinte ] ) for convenience only .",
    "it can be absorbed in @xmath21    this expansion , eq .",
    "( [ basicinte ] ) allows a universality theorem .",
    "define @xmath22 and @xmath23 the same expansion becomes , @xmath24 where @xmath25 and @xmath26 this reduces the `` scale expansion '' , eq .",
    "( [ basicinte ] ) , into a `` translational expansion '' where a basis is generated by arbitrary translations of a given function .",
    "the solution of this inverse convolution problem is trivially known as @xmath27 where the superscript @xmath28 refers to the fourier transforms of @xmath29 @xmath30 and @xmath31 respectively , and @xmath32 is the relevant `` momentum '' .",
    "this result will make our claim for universality . in the following ,",
    "this paper empirically assumes that the needed analytical properties of @xmath33 , @xmath34 , ... @xmath35 are satisfied . actually , for the sake of biological or industrial relevance , we are only concerned with discretizations of eq .",
    "( [ basicinte ] ) , with @xmath36 units , @xmath37 where we now let @xmath38 include the coefficient @xmath39      obviously , input patterns to be processed by a net can not be reduced to one degree of freedom @xmath2 only .",
    "rather , they consist of a vector @xmath40 with many components @xmath41 these may be considered as , and recoded into , a radial variable @xmath42 and , to specify a direction on the suitable hypersphere , @xmath43 angles @xmath44 enough special functions ( legendre polynomials , spherical harmonics , rotation matrices , etc . )",
    "are available to generate complete functional bases in angular space and one might invoke some formal neurons as implementing such base angular functions . the design of such fn s , and as well the design of such a polar coordinate recoding , is a little far fetched , though . in this paper",
    "we prefer to take advantage of the following argument , based upon the synaptic weights of the input layer , shown in figure 2 .    in the left part of the figure , fig .",
    "2 , all the fn s have the same input synaptic weights @xmath45 hence receive the same input @xmath46 when contributing to a global task @xmath47 for the right part of fig .",
    "2 it is again assumed that all fn s have equal input weights , with , however , weights @xmath48 deduced from @xmath49 by a sheer rotation , @xmath50 accordingly , if the output weights of the left part are the same as those of the right one , the global task @xmath51 performed by the right part is a rotated task , @xmath52 an expansion of any task @xmath53 upon the @xmath43-rotation group is thus available , @xmath54 where discretizations are in order , naturally , with suitable output weights @xmath55 here @xmath3 plays the rle of an elementary task , and it might be of some interest to study cases where @xmath3 belongs to specific representations of the rotation group .",
    "this broad subject exceeds the scope of the present paper , however , and , in the following , we restrict our considerations to scalar tasks @xmath6 of a scalar input @xmath56 according to fig . 1 only .",
    "let us return to eq .",
    "( [ pratiq ] ) , in an obvious , short notation @xmath57 two kinds of parameters can be used to best reconstruct @xmath3 : the output synaptic weights @xmath58 and , hidden inside the elementary tasks @xmath59 the scales @xmath60 let @xmath61 denote a suitable scalar product in the functional space spanned by all the @xmath62 s of interest .",
    "we assume , naturally , that the same scalar product makes sense for the @xmath3 s .",
    "incidentally , there is no loss of generality if @xmath3 is normalized , @xmath63 since the final neuron is linear .",
    "one way to define the `` best '' @xmath64 is to minimize the square norm of the error @xmath65 in terms of the @xmath58 s , this consists in solving the equations , @xmath66 let @xmath67 be that matrix with elements @xmath68 its inverse @xmath69 usually exists . even in those rare cases when @xmath67 is",
    "very ill - conditioned , or its rank is lower than @xmath70 it is easy to define a pseudoinverse such that , in all cases , the operator @xmath71 is the projector upon the subspace spanned by the @xmath62 s .",
    "then a trivial solution , @xmath72 is found for eqs .",
    "( [ linear ] ) , @xmath73 given @xmath3 and the @xmath62 s , this projection , which can be achieved by elementary trainings of the output layer of synaptic weights , will be understood in the following .",
    "it makes the @xmath74 s functions of the @xmath75 s .",
    "now we are concerned with the choice of the parameters @xmath75 of the fn s performing elementary tasks .",
    "this is of some importance , for the number @xmath36 of fn s in the intermediate layer is quite limited in practice .",
    "the subspace spanned by the @xmath62 s is thus most undercomplete .",
    "hence , every time one requests an approximator to a new @xmath76 an optimization with respect to the intermediate layer is in order , to patch likely weaknesses of the `` projector '' solution , eqs ( [ linear ] ) .",
    "let us again minimize the square norm @xmath77 of the error .",
    "we know from eqs .",
    "( [ linear ] ) that the @xmath58 s are functions of the @xmath78 s , but there is no need to use chain rules @xmath79 because the same equations , eqs .",
    "( [ linear ] ) , cancel the corresponding contributions , the @xmath58 s being optimal .",
    "derivatives of @xmath62 with respect to their scales @xmath75 are enough .",
    "the gradient of @xmath80 to be cancelled , reads , @xmath81 here @xmath82 is the straight derivative of the reference elementary task , before any scaling .",
    "there is no difficulty in implementing a training algorithm for a gradient descent in the @xmath14-space .    the next section , sec .",
    "iii , gives a brief sample of the results we obtained when solving eqs .",
    "( [ linear ] ) and ( [ gradien ] ) for many choices of the global task @xmath3 and elementary task @xmath83",
    "define for instance the scalar product in the functional space as , @xmath84 among many numerical tests we show here the results obtained when the target task reads , @xmath85 - 4.33575 \\tanh[4(x-9.56591 ) ] \\}.$ ] let the elementary task of a fn read @xmath86 a mexican hat .",
    "set @xmath87 and initial values @xmath88 for the @xmath75 s .",
    "keeping eqs .",
    "( [ linear ] ) satisfied at each step , start a gradient descent from such initial values .",
    "our increments of the @xmath75 s at each step read , @xmath89 see eqs .",
    "( [ gradien ] ) .",
    "after @xmath90 steps , a saturation of @xmath91 begins , see figure 3 .",
    "a comparison between @xmath3 and @xmath64 is provided by figure 4 .",
    "this saturation makes it reasonable to interrupt the learning process . for the sake of rigor",
    ", however , another run , with 1000 steps , was used to verify the saturation .",
    "while the saturation is confirmed , the convergence of the @xmath75 s turns out to be slower .",
    "the values of the @xmath75 s and @xmath58 s at the end of this second run read @xmath92 and @xmath93 respectively .",
    "the weakness of @xmath94 and @xmath95 is explained by the lack of a fine structure in @xmath47 the large and almost opposite values of @xmath96 and @xmath97 clearly mean a renormalization of @xmath98 since @xmath99 and @xmath100 are so close to each other .",
    "numerical difficulties linked to difference effects may therefore demand extra care in practical applications .",
    "we show in figure 5 the way in which the @xmath75 s evolved during the first 100 steps of the gradient descent .    a temporary merging of @xmath101 with @xmath102 then a final episode in which they become distinct again , are striking , as well as the merging of @xmath99 with @xmath103 it will be stressed at this stage that the whole process is invariant under any permutation of the @xmath75 s ( and of their associated @xmath58 s ) , hence a `` triangular '' rule , @xmath104 can be implemented without restricting learning flexibility . furthermore",
    ", as a symmetric function under pairwise exchanges of such parameters , the error square norm @xmath105 has a vanishing `` transverse '' derivative , @xmath106 every time @xmath107 it is thus not surprising that , at least for part of the learning process , the learning path rides lines where such parameters merge .",
    "when merging occurs , the functional basis seems to degenerate since @xmath62 and @xmath108 are not distinct .",
    "it will be recalled , however , that our output neuron is linear , and nothing prevents the process from using the strictly equivalent representations , @xmath109 a trivial renormalization of the @xmath110 term makes it that the functional basis still contains two independent vectors , namely , a new elementary response @xmath111 besides @xmath112 naturally , the renormalization has a numerical cost , since both @xmath58 and @xmath74 must diverge . in practice ,",
    "a minute modification of the `` triangular rule '' , which becomes , in our runs , @xmath113 is enough to smooth our calculations .",
    "the conclusion of this merging phenomenon , for those @xmath3 s where it occurs , is of some interest : new neuronal units ( new fn s ) may spontaneously emerge . these are `` derivative sensitive '' , and may represent a new task @xmath114 or , if @xmath115 parameters merge , any further derivative @xmath116      most choices of @xmath3 yield distinct values for the @xmath75 s .",
    "we show in figure 6 a trivial case . here",
    "@xmath117,$ ] a window - like elementary response , and the target task reads @xmath118 - 1/[1+(x^2/16)],$ ] a sum of such windows .",
    "we freeze @xmath119 @xmath120 and @xmath121 a symmetry breaking situation , and clearly a part of the obvious solution for the minimum of @xmath122 then the contour map of @xmath123 in the @xmath124,@xmath125-space does show the expected minimum for @xmath126 and @xmath127 the minimum turns out to be very flat , hence some robustness is likely for that special case .",
    "the learning process does reach this `` fully symmetry breaking '' configuration , together with the corresponding set of @xmath58 s , namely @xmath128 many other , less academic cases generate a full symmetry breaking , namely distinct @xmath75 s .      besides `` windows '' and `` mexican hats '' , we also used oscillatory shapes such as @xmath129 for @xmath83 a cut - off by an exponential decay was also sometimes introduced .",
    "the range of the scalar product integration was independently varied within one order of magnitude .",
    "sometimes the dimension @xmath36 of the elementary task basis was also taken as a random number , a test of little interest , however , which just verified that @xmath64 improves when @xmath36 increases .",
    "for @xmath76 a few among our tests involved a small amount of random noise added to a smooth main part @xmath130 furthermore we investigated a fair amount of piecewise continuous @xmath3 s , this case being of interest for image processing @xcite . alternately , we smoothed such discontinuities with a suitable definition of @xmath76 such as , @xmath131,$ ] with randomized choices of the number of terms , the coefficients @xmath132 the large  slope coefficient @xmath133 and the positions @xmath134 of the steep areas .",
    "the set of initial values for the @xmath75 s before gradient descent was also sometimes taken at random .",
    "it was often found that a traditional sequence @xmath135 is not a bad choice for a start .",
    "all our runs converge reasonably smoothly to a saturation of the norm @xmath136 provided those cases where @xmath67 becomes ill - conditioned are numerically processed .",
    "there is a significant proportion of runs where the optimum seems to be quite flat , hence some robustness of the results .",
    "local minima where the learning gets trapped do not seem to occur very often , but this problem deserves the usual caution , with the usual annealing if necessary .",
    "we did not find clear criteria for predicting whether a given @xmath3 leads to a merging of some @xmath75 s , however . despite this failure , all these results advocate a reasonably positive case for the learning process described by eqs .",
    "( [ linear ] ) and ( [ gradien ] ) and the emergence of `` derivative tasks '' .",
    "this paper tries to relate several issues .",
    "most of them are well known in the theory of neural nets , but two of our considerations , the question of symmetries and the rotational analysis , might give reasonably original results , up to our knowledge at least .",
    "the most important and well known issue is that of the universality offered by nets whose architecture is described by figures 1 and 2 , namely four layers : input weights @xmath137 fn s for elementary tasks @xmath138 with adjustable parameters @xmath139 output weights @xmath140 linear output neuron(s ) . the linearity of the output(s ) can be summarized in any dimensions by the linear transform @xmath141 ( we use here boldface symbols to stress that the linearity generalizes to any suitable vector and tensor situations for multiparameter inputs , intermediate tasks and outputs . )",
    "this linearity actually reduces the theory of such an architecture to a special case of the `` generator coordinate '' theory , well known in physics @xcite .",
    "as well , from a mathematical point of view , this boils down to the only question of the invertibility of the kernel @xmath142 actually , the invertibility problem boils down into identifying those classes of global tasks @xmath143 which belong to the functional ( sub)space spanned by the @xmath138 s . for the sake of definiteness , we proved a universality theorem for the very special case of `` scaling without translating '' , inspired by wavelets .",
    "but most of the considerations of this paper clearly hold if one replaces , _ mutatis mutandis _ ,",
    "wavelets by other responses and scaling parameters by any other parameters .    the parameters @xmath144 can be defined as including the input synaptic weight vectors @xmath145 whose dimension is necessarily the same as that of the inputs @xmath40 in order to generate the actual inputs @xmath146 received by the intermediate fn s .",
    "when @xmath144 also explicitly includes scale parameters @xmath11 there is no loss of generality in restricting the @xmath49 s to be unitary vectors .",
    "hence the linear kernel @xmath138 can imply , in a natural way , an integration upon the group of rotations transforming all the @xmath49 s into one another .",
    "this part of the theory relates to the angular momentum projections which are so familiar in the theory of molecular and nuclear rotational spectra @xcite .",
    "the well known issue of the discretization of a continuous expansion converts kernels into finite matrices , naturally .",
    "this paper studied what happens if one trains @xmath147 for a temporary optimum of the approximate task @xmath148 , while @xmath144 is not yet optimized .",
    "this implies a prejudice on training speeds : @xmath147 fast learner , @xmath144 slower .",
    "other choices , such as @xmath147 slower learner and @xmath144 faster , for instance , are as legitimate , and should be investigated too .",
    "the question is of importance for biological systems , because of obvious likely differences in the time behaviors and biochemical and metabolic factors of synapses and cell bodies .",
    "the training speed hierarchy we chose points to one technical problem only , namely whether the gram - schmidt matrix @xmath67 of scalar products @xmath149 is easily invertible or not .",
    "we do not use a gram - schmidt orthogonalization of the finite basis of such @xmath62 s , but the ( pseudo ) inversion of @xmath67 amounts to the same .",
    "once @xmath69 is obtained , temporarily optimal @xmath147 are easily derived .",
    "our further optimization of @xmath148 with respect to the parameters of the intermediate fn s takes advantage of the linearity of the output(s ) and the symmetry of the problem under any permutation of the fn s .",
    "let @xmath150 label such fn s , @xmath151 and denote @xmath152 the parameters of the @xmath150-th fn .",
    "we found cases where the gradient descent used to optimize @xmath148 induces a few @xmath152 s to become quite close to one another .",
    "such functional clusters , because of the output linearity , may yield elementary tasks corresponding to derivatives of @xmath138 with respect to components of @xmath153 this derivative process may look similar to a gram - schmidt orthogonalization , but it is actually distinct , because no rank is lost in the basis . for those @xmath143 s which induce such mergings of fn s , industrial applications should benefit from a preliminary simulation of training as a useful precaution , because , besides straight fn s implementing @xmath154 additional , more specific @xmath155 implementing `` derivative @xmath138 s '' will be necessary . for biological systems , diversifications of neurons , or groups of such , between tasks and `` derivative tasks ''",
    "might also be concepts of interest .",
    "it may be noticed that the word `` derivative '' may hold with respect to inputs as well as parameters . indeed , as found at the stage of eq .",
    "( 3 ) , scale parameters reduce , in a suitable representation , to translational parameters in a task @xmath156 the sign difference between @xmath157 and @xmath158 is obviously inconsequential .    to conclude , this emergence of `` derivative elementary tasks '' prompts us into a problem yet unsolved by our numerical studies with many different @xmath159s and many different @xmath138 s : given the shape of @xmath154",
    "can one predict whether a given @xmath143 leads to a full symmetry breaking or to a partial merging of the fn s ?",
    "a.t . thanks service de physique thorique , saclay , for its hospitality during this work .",
    "k. hornik , m. stinchcombe , h. white , _ multilayer feedforward networks are universal approximators _ , neural networks * 2 * ( 1989 ) 359 - 366 ; k. hornik , m. stinchcombe , h. white and p. auer , _ degree of approximation results for feedforward networks approximating unknown mappings and their derivatives _ , neural computation , * 6 * ( 6 ) ( 1994 ) 1262 - 1275      b.g .",
    "giraud , l.c .",
    "liu , c. bernard and h. axelrad , _ optimal approximation of square integrable functions by a flexible one - hidden - layer neural network of excitatory and inhibitory neuron pairs _ , neural networks , * 4 * ( 1991 ) 803 - 815            d.l .",
    "hill and j.a .",
    "wheeler , _ nuclear constitution and the interpretation of fission phenomena _ , phys",
    "* 89 * ( 1953 ) 1102 - 1146 ; j.j .",
    "griffin and j.a .",
    "wheeler , _ collective motion in nuclei by the method of generator coordinates _",
    ", phys . rev .",
    "* 108 * ( 1957 ) 311 - 327"
  ],
  "abstract_text": [
    "<S> neural nets are known to be universal approximators . in particular , </S>",
    "<S> formal neurons implementing wavelets have been shown to build nets able to approximate any multidimensional task . </S>",
    "<S> such very specialized formal neurons may be , however , difficult to obtain biologically and/or industrially . in this paper </S>",
    "<S> we relax the constraint of a strict `` fourier analysis '' of tasks . </S>",
    "<S> rather , we use a finite number of more realistic formal neurons implementing elementary tasks such as `` window '' or `` mexican hat '' responses , with adjustable widths . </S>",
    "<S> this is shown to provide a reasonably efficient , practical and robust , multifrequency analysis . </S>",
    "<S> a training algorithm , optimizing the task with respect to the widths of the responses , reveals two distinct training modes . </S>",
    "<S> the first mode induces some of the formal neurons to become identical , hence promotes `` derivative tasks '' . </S>",
    "<S> the other mode keeps the formal neurons distinct .    </S>"
  ]
}