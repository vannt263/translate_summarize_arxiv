{
  "article_text": [
    "hermitian positive definite ( hpd ) matrices possess a remarkably rich geometry that is a cornerstone of modern convex optimisation  @xcite and convex geometry  @xcite .",
    "in particular , hpd matrices form a convex cone , the strict interior of which is a differentiable riemannian manifold which is also a prototypical cat(0 ) space ( i.e. , a metric space of nonpositive curvature  @xcite ) .",
    "this rich structure enables `` geometric optimisation '' on the set of hpd matrices  enabling us to solve certain problems that may be nonconvex in the euclidean sense but are convex in the manifold sense ( see  [ sec.gc ] or  @xcite ) , or failing that , still have enough geometry ( see  [ sec.ln ] ) so as to admit efficient optimisation",
    ".    this paper formally develops _",
    "_ conic geometric optimisation _ _ for hpd matrices .",
    "we present key results that help recognise geodesic convexity ( g - convexity ) ; we also present sufficient conditions that place even several non geodesically convex functions within the grasp of geometric optimisation .",
    "we begin by noting that the widely studied class of _ geometric programs _ ultimately reduces to conic geometric optimisation on @xmath0 hpd matrices ( i.e. , positive scalars ; see remark  [ rmk.logc ] ) .",
    "geometric programming has enjoyed great success across a spectrum of applications ",
    "see e.g. , the survey of  @xcite ; we hope this paper helps conic geometric optimisation gain wider exposure .    perhaps the best known conic geometric optimisation problem is computation of the karcher ( frchet ) mean of a set of hpd matrices , a topic that has attracted great attention within matrix theory  @xcite , computer vision  @xcite , radar imaging  ( * ? ? ?",
    "* part ii ) , medical imaging  @xcite  we refer the reader to the recent book  @xcite for additional applications and references .",
    "another basic geometric optimisation problem arises as a subroutine in image search and matrix clustering  @xcite .",
    "conic geometric optimisation problems also occur in several other areas : statistics ( covariance shrinkage )  @xcite , nonlinear matrix equations  @xcite , markov decision processes and more broadly in the fascinating areas of nonlinear perron - frobenius theory  @xcite .    as a concrete illustration of our ideas",
    ", we discuss the task of _ maximum likelihood estimate _",
    "( mle ) for _ elliptically contoured distributions _ ( ecds )  @xcite  see  [ sec.ecd ] .",
    "we use ecds to illustrate our theory , not only because of their instructive value but also because of their importance in a variety of applications  @xcite .",
    "the main focus of this paper is on recognising and constructing certain structured nonconvex functions of hpd matrices .",
    "in particular , section  [ sec.gc ] studies the class of geodesically convex functions , while section  [ sec.ln ] introduces `` log - nonexpansive '' functions .",
    "we present a limited - memory bfgs algorithm in section  [ sec.manopt ] , where we also present a derivation for the _ parallel transport _ , which , we could not find elsewhere in the literature . even though manifold optimisation algorithms apply to both classes of functions , for log - nonexpansive functions we advance fixed - point theory and algorithms separately in section  [ sec.ln ] .",
    "we present an application of geometric optimisation in section  [ sec.ecd ] , where we consider statistical inference with elliptically contoured distributions .",
    "numerical results are the subject of section  [ sec.expt ] .",
    "geodesic convexity ( g - convexity ) is a classical concept in geometry and analysis ; it is used extensively in the study of hadamard manifolds and metric spaces of _ nonpositive curvature _",
    "@xcite , i.e. , metric spaces having a g - convex distance function . the concept of g - convexity has been previously explored in nonlinear optimisation  @xcite , but its importance and applicability in statistical applications and optimisation has only recently gained more attention  @xcite .",
    "it is worth noting that geometric programming  @xcite ultimately relies on `` geometric - mean '' convexity  @xcite , i.e. , @xmath1^\\alpha[f(y)]^{1-\\alpha}$ ] , which is nothing but logarithmic g - convexity on @xmath0 hpd matrices ( positive scalars ) .    to introduce g - convexity on @xmath2 hpd matrices",
    "we begin by recalling some key definitions ",
    "see @xcite for extensive details .",
    "let @xmath3 be a @xmath4-dimensional connected @xmath5 riemannian manifold .",
    "a set @xmath6 is called _ geodesically convex _ if any two points of @xmath7 are joined by a geodesic lying in @xmath7 .",
    "that is , if @xmath8 , then there exists a shortest path @xmath9 \\to { \\mathcal{x}}$ ] such that @xmath10 and @xmath11 .",
    "let @xmath6 be a g - convex set .",
    "a function @xmath12 is called _ geodesically convex _ , if for any @xmath8 , we have the inequality @xmath13 where @xmath14 is the geodesic @xmath9 \\to { \\mathcal{x}}$ ] with @xmath10 and @xmath11 .",
    "unlike scalar g - convexity , for matrices recognising g - convexity is not so easy .",
    "indeed , for scalars , a function @xmath15 is log - g - convex ( and hence g - convex ) if and only if @xmath16 is convex . a similar",
    "characterisation does not seem to exist for hpd matrices , primarily due to the noncommutativity of matrix multiplication .",
    "we develop some theory below for helping recognise and construct g - convex functions .    to define g - convex functions on hpd matrices recall that @xmath17 is a differentiable riemannian manifold where geodesics between points are available in closed form .",
    "indeed , the tangent space to @xmath17 at any point can be identified with the set of hermitian matrices , and the inner product on this space leads to a riemannian metric on @xmath17 . at any point @xmath18 , this metric is given by the differential form @xmath19 ; for @xmath20 there is a unique geodesic path  ( * ? ? ?",
    "6.1.6 ) @xmath21.\\ ] ] the midpoint of this path , namely @xmath22 is called the _ matrix geometric mean _ , which is an object of great interest  @xcite  we drop the @xmath23 and denote it simply by @xmath24 .",
    "starting from the geodesic  , many g - convex functions can be constructed by extending monotonic convex functions to matrices . to that end ,",
    "first recall the fundamental operator inequality  @xcite ( where @xmath25 denotes the lwner partial order ) : @xmath26 theorem  [ thm.trace.gc ] uses the operator inequality   to construct `` tracial '' g - convex functions .",
    "[ thm.trace.gc ] let @xmath27 be monotonically increasing and convex ; let @xmath28 denote the eigenvalue map and @xmath29 its decreasingly sorted version . then , @xmath30 is g - convex for each @xmath31 .",
    "it suffices to establish midpoint convexity .",
    "inequality   implies that @xmath32 since @xmath33 is monotonic , for @xmath31 it follows that @xmath34 lidskii s theorem  ( * ? ? ? *",
    "thm.iii.4.1 ) yields the majorisation @xmath35 , which combined with a celebrated result of  @xcite and convexity of @xmath33 yields @xmath36 now invoke inequality   to conclude that @xmath30 is g - convex .",
    "theorem  [ thm.trace.gc ] shows that the following functions are g - convex : ( i ) @xmath37 ; ( ii ) @xmath38 for @xmath39 ; ( iii ) @xmath40 ; ( iv ) @xmath41 for @xmath39 .",
    "we now construct examples of g - convex functions different from those obtained via theorem  [ thm.trace.gc ] .",
    "let us start with a motivating example .",
    "[ eg.one ] let @xmath42 .",
    "the function @xmath43 is g - convex .",
    "to prove this claim it suffices to verify midpoint convexity : @xmath44 for @xmath20 .",
    "since @xmath45 and @xmath46 ( @xcite ) , it follows that @xmath47 .",
    "below we substantially generalise this example ; but first some background .    a linear map @xmath48 from hilbert space @xmath49 to a hilbert space @xmath50 is called _ positive _ , if for @xmath51 , @xmath52 .",
    "it is called _ strictly positive _ if @xmath53 for @xmath54 ; finally , it is called _ unital _ if @xmath55 .",
    "[ lem.parsum ] define the _ parallel sum _ of hpd matrices @xmath56 as @xmath57^{-1}.\\ ] ] then , for any positive linear map @xmath58 , we have @xmath59    building on lemma  [ lem.parsum ] , we are ready to state a key theorem that helps us recognise and construct g - convex functions ( see thm .",
    "[ thm.gc ] , for instance ) .",
    "this result is by itself not new  e.g .",
    ", it follows from the classic paper of  @xcite ; due to its key importance we provide our own proof below for completeness .",
    "[ thm.linmap ] let @xmath60 be a strictly positive linear map .",
    "then , @xmath61,\\ \\text{for } a , b \\in { \\mathbb{p}}_d.\\ ] ]    the key insight of the proof is to use the integral identity  @xcite : @xmath62^{\\alpha+\\beta } } d\\lambda = \\frac{\\gamma(\\alpha)\\gamma(\\beta)}{\\gamma(\\alpha+\\beta)}a^\\alpha b^\\beta\\ ] ] using @xmath63 and @xmath64 , for @xmath65 this yields the integral representation @xmath66^{-1}}{\\lambda^t(1-\\lambda)^{1-t}}d\\lambda,\\ ] ] where @xmath67 is the usual gamma function . since @xmath68 , using   we may write it as @xmath69^{-1}d\\mu(\\lambda),\\ ] ] for a suitable measure @xmath70 . applying the map @xmath48 to both sides of   we obtain @xmath71^{-1}\\bigr)d\\mu(\\lambda)\\\\      & = \\sint_0 ^ 1 \\phi ( \\bar{a } : \\bar{b})d\\mu(\\lambda ) ,    \\end{aligned}\\ ] ] where @xmath72 and @xmath73 . using lemma  [ lem.parsum ] and linearity of @xmath48",
    "we see @xmath74^{-1}d\\mu(\\lambda)\\\\        & \\stackrel{\\eqref{eq.19}}{=}\\ \\phi(a ) { { \\#}}_t \\phi(b ) ,      \\end{split}\\ ] ] which completes the proof .    a corollary of theorem  [ thm.linmap ] ( that subsumes example  [ eg.one ] ) follows .    [ corollary.trace ]",
    "let @xmath20 , and let @xmath75 have full column rank ; then @xmath76^{1-t}[\\operatorname{tr}x^*bx]^{t},\\qquad t \\in [ 0,1].\\ ] ]    use the positive linear map @xmath77 in theorem  [ thm.linmap ] .",
    "[ rmk.logc ] corollary  [ corollary.trace ] actually proves a result stronger than g - convexity : it shows _ log - g - convexity _ ,",
    "i.e. , @xmath78 , so that @xmath79 is g - convex .",
    "it is easy to verify that if @xmath80 are log - g - convex , then both @xmath81 and @xmath82 are log - g - convex .",
    "[ rmk.logc2 ] more generally , if @xmath83 is nondecreasing and log - convex , then the map @xmath84 is g - convex .",
    "the proof is the same as of theorem  [ thm.trace.gc ] .",
    "for instance , if @xmath85 , we obtain the special case that @xmath86 is g - convex , i.e. , @xmath87    we mention now another corollary to theorem  [ thm.linmap ] ; we note in passing that it subsumes a more complicated result of gurvits and samorodnitsky  ( * ? ? ?",
    "* lem .  3.2 ) .",
    "[ corollary.det ] let @xmath88 with @xmath89 such that @xmath90_{i=1}^m)=k$ ] ; also let @xmath91 .",
    "then @xmath92 is g - convex on @xmath17 .    by our assumption on @xmath93 and @xmath94 ,",
    "the map @xmath95 is strictly positive .",
    "[ thm.linmap ] implies that @xmath96 . this operator inequality is stronger than what we require .",
    "indeed , since @xmath97 is monotonic and determinants are multiplicative , from this inequality it follows that @xmath98 observe that the above result extends to @xmath99 , where @xmath100 is some positive measure on @xmath101 .",
    "[ rmk.det ] corollary  [ corollary.det ] may come as a surprise to some readers because @xmath102 is well - known to be concave ( in the euclidean sense ) , and yet @xmath103 turns out to be g - convex  moreover , @xmath102 is g - linear , i.e. , both g - convex and g - concave .    in  @xcite",
    "( see also @xcite ) a dissimilarity function to compare a pair of hpd matrices is studied .",
    "specifically , for @xmath104 , this function is called the s - divergence and is defined as @xmath105 divergence   proves useful in several applications  @xcite , and very recently its joint g - convexity ( in both variables ) was discovered  @xcite .",
    "corollary  [ corollary.det ] along with remark  [ rmk.det ] yield g - convexity of @xmath106 in either @xmath107 or @xmath108 separately .",
    "we are now ready to state our next key g - convexity result .",
    "a similar result was obtained in  @xcite ; our result is somewhat more general as it allows incorporation of positive linear maps . moreover ,",
    "our proof technique is completely different .",
    "[ thm.gc ] let @xmath109 be nondecreasing ( in lwner order ) and g - convex .",
    "let @xmath110 , and let @xmath48 be a positive linear map .",
    "then , @xmath111 is g - convex .",
    "it suffices to prove midpoint g - convexity . since @xmath110 , @xmath112",
    "thus , applying theorem  [ thm.linmap ] to @xmath48 and noting that @xmath33 is nondecreasing it follows that @xmath113 by assumption @xmath33 is g - convex , so the last inequality in   yields @xmath114 notice that if @xmath33 is strictly g - convex , then @xmath115 is also strictly g - convex .",
    "let @xmath116 and @xmath117 .",
    "then , @xmath118 is g - convex . with @xmath119 for @xmath39 ,",
    "@xmath120 is g - convex .",
    "next , theorem  [ thm.gc.sval ] presents a method for creating essentially logarithmic versions of our `` tracial '' g - convexity result theorem  [ thm.trace.gc ] .",
    "[ thm.gc.sval ] if @xmath121 is convex , then @xmath122 is g - convex for each @xmath31 .",
    "if @xmath123 is nondecreasing and convex , @xmath124 is g - convex for each @xmath31 .    to prove theorem  [ thm.gc.sval ]",
    "we will need the following majorisation .",
    "[ lem.gm.maj ] let @xmath125 denote the _ log - majorisation _ order , i.e. , for @xmath126 ordered nonincreasingly , we say @xmath127 if @xmath128 and @xmath129 .",
    "then , for @xmath130 and @xmath131 $ ] , we have the log - majorisation between the eigenvalues : @xmath132    the first majorisation follows from a recent result of  @xcite .",
    "the second one follows easily from @xmath133 ( the final equality holds since @xmath130 ) .",
    "apply this inequality to the antisymmetric ( grassmann ) exterior product @xmath134 , since @xmath135 ( see e.g. ,  ( * ? ?",
    "* i ; iv.2 ) ; then we obtain @xmath136 .",
    "now set @xmath137 , @xmath138 , and use the multiplicativity @xmath139 to complete the proof .    from lemma  [ lem.gm.maj ]",
    "we have the majorisation @xmath140 on taking logarithms , this majorisation may be written equivalently as @xmath141 applying a classical result of  @xcite on majorisation under convex functions , from   we obtain the inequality @xmath142 applying the ky - fan norm @xmath143that is , the sum of top-@xmath144 singular values  to , we obtain the weak - majorisation ( see e.g. , ( * ? ? ? *",
    "ii ) for more on majorisation ) : @xmath145 \\prec_w ( 1-t)\\sigma(\\log a)+t\\sigma(\\log b).\\ ] ] since @xmath33 is monotone and convex , yields g - convexity of @xmath146 .    [ cor.norm.gc ]",
    "let @xmath147 be a symmetric gauge function ( i.e. , @xmath48 is a norm , invariant to permutation and sign changes ) .",
    "also , let @xmath148 .",
    "then , @xmath149 is g - convex .",
    "note that @xmath150 ; now apply theorem  [ thm.gc.sval ] .",
    "[ ex.riem ] consider @xmath151 the riemannian distance between @xmath152  ( * ? ?",
    "* ch . 6 ) . since @xmath153",
    ", it follows from corollary  [ cor.norm.gc ] that @xmath154 is g - convex ( see also  ( * ? ? ?",
    "6.1.11 ) ) .",
    "this immediately shows that computing the frchet ( karcher ) mean and median of hpd matrices ( also known as geometric mean and median of hpd matrices , respectively ) are g - convex optimisation problems ; formally , these problems are given by @xmath155 where @xmath156 , @xmath157 , and @xmath158 for @xmath159 .",
    "the latter problem has received great interest in the literature  @xcite , and its optimal solution is unique owing to the ( strict ) g - convexity of its objective .",
    "the former problem is less well - known but in some cases proves more favourable  @xcite  again , despite the nonconvexity of the objective , its g - convexity ensures every local solution is global .",
    "we conclude this section by using lemma  [ lem.gm.maj ] to prove the following log - convexity analogue to theorem  [ thm.gc.sval ] ( _ cf . _",
    "the scalar case studied in ( * ? ? ? * prop .",
    "2.4 ) ) .",
    "[ thm.logc.svals ] let @xmath160 be real analytic with @xmath161 and radius of convergence @xmath162 . then , @xmath163 is log - g - convex on matrices with spectrum in @xmath164 .",
    "it suffices to verify that @xmath165 . since @xmath166 , we have @xmath167 taking logarithms , we see that @xmath168 is log - g - convex ( and hence also g - convex ) .",
    "some examples of @xmath169 that satisfy conditions of theorem  [ thm.logc.svals ] are @xmath170 , @xmath171 on @xmath101 , @xmath172 and @xmath173 on @xmath174 ; see  @xcite for more examples .",
    "we describe now an extension of g - convexity to multiple matrices ; a two - variable version was also partially explored in  @xcite , though under a different name .",
    "we begin our multivariable extension by recalling a few basic properties of the kronecker product  @xcite .",
    "the obvious extension to the multivariable case is the following .",
    "a function @xmath175 is called _ jointly g - convex _ if @xmath176    recall that we introduced ordinary g - convexity using the matrix geometric mean  . using the extremely fruitful idea of multivariable matrix geometric means  @xcite",
    ", we introduce an extension to the case of multiple variables below .",
    "let @xmath177 be a probability vector , i.e. , @xmath157 and @xmath178 .",
    "let @xmath179 be given .",
    "then the _ matrix geometric mean _ is defined as @xmath180 where @xmath181 is the riemannian distance as defined in example  [ ex.riem ] .",
    "this object enjoys a host of axiomatic properties and is widely believed to be the `` correct '' generalisation of the scalar geometric mean @xmath182 to the noncommutative case ",
    "though proving several of its basic properties such as monotonicity turned out to be surprisingly hard , and was settled only very recently  @xcite .",
    "we call @xmath183 _ multivariable g - convex _ if @xmath184 a map @xmath185 such that @xmath186    next we recall a result that generalises theorem  [ thm.linmap ] to the multivariable case .    [ thm.gm.linmap ] let @xmath187 be a positive linear map .",
    "then , @xmath188    see ( * ? ? ?",
    "* thm.4.1 and  5 ) , which ultimately relies on thm .",
    "[ thm.linmap ] .",
    "the function @xmath189 is multivariable log - g - convex . indeed , using thm .",
    "[ thm.gm.linmap ] , we immediately obtain @xmath190^{w_i}.\\ ] ]    * this section is currently being expanded .",
    "*      [ lem.kron ] let @xmath191 , @xmath192 .",
    "then , @xmath193 \\in { \\mathbb{r}}^{mp\\times nq}$ ] satisfies :    a.   @xmath194 b.   @xmath195 c.   assuming that the respective products exist , @xmath196 d.   @xmath197 e.   if @xmath198 and @xmath199 then @xmath200 .",
    "f.   let @xmath201 , and @xmath202 ; then @xmath203 g.   if @xmath204 and @xmath205 , then @xmath206",
    ".    identities ( i)(iii ) are classic ; ( v ) follows easily from ( i ) and ( iv ) , while ( vi ) and ( vii ) follow from ( v ) ; ( viii ) is an easy exercise .",
    "we will need the following easy but key result on tensor products of geometric means .",
    "[ lem.gm.kron ] let @xmath207 and @xmath208 .",
    "then , @xmath209    denote @xmath210 .",
    "observe that @xmath211^{1/2}\\\\      & = [ ( a{\\otimes}c)^{-1/2}(b{\\otimes}d)(a{\\otimes}c)^{-1/2}]^{1/2}\\\\      & = \\gamma(a{\\otimes}c , b{\\otimes}d ) ,    \\end{aligned}\\ ] ] where the second equality follows from lemma  [ lem.kron]-(iii ) , while the third one from lemma  [ lem.kron]-(ii),(iii ) , and ( vi ) .",
    "a similar manipulation then shows that @xmath212 which concludes the proof .",
    "lemma  [ lem.gm.kron ] inductively extends to the multivariable case , so that @xmath213 using identity   we thus obtain the following multivariate analogue to theorem  [ thm.gc.sval ] .    [ thm.multi.spec ] let @xmath33 be an increasing convex function on @xmath214 .",
    "then , the map @xmath215 is jointly g - convex , i.e. , @xmath216 is g - convex in its variables .",
    "let @xmath217 be pairs of hpd matrices of arbitrary sizes ( such that for each @xmath218 , @xmath219 are of the same size .",
    "let @xmath220 index the eigenvalues of the tensor product @xmath221 . then , starting with identity   we obtain @xmath222 & = \\lambda_j\\left[\\left(\\nlpkron_{i=1}^m a_i\\right){{\\#}}\\left(\\nlpkron_{i=1}^m b_i\\right)\\right ] \\le { \\tfrac{1}{2}}\\lambda_j\\left[\\nlpkron_{i=1}^m a_i+\\nlpkron_{i=1}^m b_i\\right]\\\\      \\operatorname{tr}h\\left(\\nlpkron_{i=1}^m ( a_i{{\\#}}b_i)\\right)&=\\sum_j h\\left(\\lambda_j\\left[\\nlpkron_{i=1}^m ( a_i{{\\#}}b_i)\\right]\\right )       \\le \\sum_jh\\left({\\tfrac{1}{2}}\\lambda_j\\left[\\nlpkron_{i=1}^m a_i+\\nlpkron_{i=1}^m b_i\\right]\\right)\\\\      & \\le { \\tfrac{1}{2}}\\sum_j h(\\lambda_j(\\nlpkron_{i=1}^m a_i ) ) + { \\tfrac{1}{2}}\\sum_j h(\\lambda_j(\\nlpkron_{i=1}^m b_i))\\\\      & = { \\tfrac{1}{2}}\\operatorname{tr}h(\\nlpkron_{i=1}^m a_i ) + { \\tfrac{1}{2}}\\operatorname{tr}h(\\nlpkron_{i=1}^m b_i)\\\\      & = { \\tfrac{1}{2}}\\prod_{i=1}^m\\operatorname{tr}h(a_i ) + { \\tfrac{1}{2}}\\prod_{i=1}^m\\operatorname{tr}h(b_i ) ,    \\end{aligned}\\ ] ] which shows the desired multivariable g - convexity of the map @xmath216 .",
    "again , using   we obtain the following multivariate analogue to theorem  [ thm.linmap ] .    [ thm.multi.linmap ]",
    "let @xmath223 be pairs of hpd matrices of arbitrary sizes ( such that for each @xmath218 , @xmath224 are of the same size ) .",
    "let @xmath225 be a positive linear map for each @xmath218 , and @xmath48 the _ positive multilinear map _ defined by @xmath226 .",
    "then , @xmath227    expanding the definition of @xmath48 we have @xmath228\\\\      & = \\bigl[\\nlpkron_i\\phi_i(x_i)\\bigr]{{\\#}}\\bigl[\\nlpkron_i\\phi_i(y_i)\\bigr ] = \\phi(\\nlpkron_i x_i){{\\#}}\\phi(\\nlpkron_i y_i ) .",
    "\\end{aligned}\\ ] ] the operator inequality   follows upon invoking theorem  [ thm.linmap ] and lemma  [ lem.kron]-(viii ) .    building on theorem  [ thm.multi.linmap ] , we also derive a generalisation to theorem  [ thm.gc ] .",
    "let @xmath229 be nondecreasing ( in lwner order ) and g - convex let @xmath230 and let @xmath231 be a strictly positive multilinear map .",
    "then , @xmath232 is jointly g - convex ( i.e. , g - convex in @xmath233 ) .",
    "since @xmath234 is continuous , it suffices to establish midpoint g - convexity .",
    "@xmath235 since @xmath33 is nondecreasing , using theorem  [ thm.multi.linmap ] the first inequality follows .",
    "the second one follows as @xmath33 is g - convex , which completes the proof .",
    "using identities and   with lemma  [ lem.gm.maj ] we obtain the following log - majorisations .",
    "[ prop.kron.maj ] let @xmath236 be pairs of hpd matrices of compatible sizes . then , @xmath237^{1-t}[\\nlpkron_{i=1}^mb_i]^t),\\qquad",
    "[ 0,1]\\\\        \\lambda([\\nlpkron_{i=1}^ma_i]^{1-t}[\\nlpkron_{i=1}^mb_i]^t ) & \\prec_{\\log } \\lambda[\\nlpkron_{i=1}^ma_i^{1-t}]\\lambda[\\nlpkron_{i=1}^mb_i^t ] .",
    "\\end{split}\\ ] ]    proposition  [ prop.kron.maj ] grants us the following multivariate analogue to theorem  [ thm.gc.sval ] .",
    "[ thm.kron.gc.sval ] if @xmath121 is convex , then @xmath238 is g - convex on @xmath239 for each @xmath31 .",
    "if @xmath123 is nondecreasing and convex , then @xmath240 is g - convex for @xmath31 .",
    "theorem  [ thm.kron.gc.sval ] brings us to the end of our theoretical results on recognising and constructing g - convex functions .",
    "we are now ready to devote attention to optimisation algorithms . in particular , we first discuss manifold optimisation  @xcite techniques in  [ sec.manopt ]",
    ". then , in  [ sec.ln ] we introduce a special class of functions that overlaps with g - convex functions , but not entirely , and admits simpler `` conic fixed - point '' algorithms .",
    "since @xmath17 is a smooth manifold , we can use optimisation techniques based on exploiting smooth manifold structure .",
    "in addition to common concepts such as tangent vectors and derivatives along manifolds , different optimisation methods need a subset of new definitions and explicit expressions for inner products , gradients , retractions , vector transport and hessians @xcite . since @xmath17 can be viewed as a sub - manifold of the euclidean space @xmath241 , most of concepts of importance to our study can be defined by using the embedding structure of euclidean space .",
    "the tangent space at any point is the space @xmath242 of @xmath243 hermitian matrices .",
    "the derivative of a function on the manifold in any direction in the tangent space is simply the embedded euclidean derivative in that direction .    for several optimisation algorithms ,",
    "two different inner product formulations were tested in @xcite for @xmath17 .",
    "the authors observed that the intrinsic inner product leads to the best convergence speed for the tested algorithms .",
    "we too observed that the intrinsic inner product yields more than a hundred times faster convergence for our algorithms compared to the induced inner product of euclidean space .",
    "the _ intrinsic inner product _ of two tangent vectors at point @xmath107 on the manifold is given by @xmath244 this intrinsic inner product leads to geodesics of the form  . now that we have set up an inner product tensor",
    ", we can define the gradient direction as the direction of the maximum change .",
    "the inner product between the gradient vector and a vector in the tangent space is equal to the gradient of the function in that direction .",
    "if @xmath245 is the hermitian part of euclidean gradient , then the gradient in intrinsic metric is given by : @xmath246 the simplest gradient descent approach , namely steepest descent , also needs the notion of projection of a vector in the tangent space onto a point on the manifold .",
    "such a projection is called _",
    "retraction_. if the manifold is riemannian , a particular retraction is the exponential map , i.e. , moving along a geodesic .",
    "if the inner product is the induced inner product of the manifold , then the retraction is normal retraction on the euclidean space which is obtained by summing the point on the manifold and the vector on the tangent space . the intrinsic inner product of   of the riemannian manifold leads to the following exponential map : @xmath247 from a numerical perspective",
    ", our experiments revealed that the following equivalent representation of the retraction   gives the best computational speed : @xmath248    definitions of the gradient and retraction suffice for implementing steepest descent on @xmath17 . for approaches such as conjugate gradients or quasi - newton methods ,",
    "we need to relate the tangent vector at one point to the tangent vector at another point , i.e. , we need to define _",
    "vector transport_. a special case of vector transport on a riemannian manifold is parallel transport : for the induced euclidean metric , parallel transport is simply the identity map . in order to compute the parallel transport",
    "one first needs to compute the _ levi - civita connection_. this connection is a way to compute directional derivatives of vector fields .",
    "it is a map from the cartesian product of tangent bundles to the tangent bundle : @xmath249 where @xmath250 is the tangent bundle of manifold @xmath251",
    "( i.e. the space of smooth vector fields on @xmath251 )",
    ". it can be verified that for the intrinsic metric   the following connection satisfies all the needed properties ( see e.g. , @xcite ) : @xmath252-\\tfrac12 ( \\zeta_x x^{-1 } \\xi_x + \\xi_x x^{-1 } \\zeta_x),\\end{aligned}\\ ] ] where @xmath253 denotes the classical frchet derivative of @xmath254 .",
    "@xmath255 and @xmath256 are vector fields on the manifold @xmath242 .",
    "subindex @xmath107 is used to discriminate a vector field from a tangent vector .",
    "consider @xmath257 , a vector field along the geodesic curve @xmath258 .",
    "parallel transport along a curve is given by the differential equation @xmath259 for the intrinsic metric , the above equation becomes @xmath260 the geodesic passing through @xmath261 with @xmath262 is given by @xmath263 for @xmath264 we get the retraction  .",
    "it can be shown that along the geodesic curve the following equation gives the parallel transport : @xmath265 thus , parallel transport for the intrinsic inner product is given by @xmath266 it is important to note that this parallel transport can be written in a compact form that is also computationally more advantageous , namely , @xmath267    we are now ready to describe a quasi - newton method on @xmath17 . different algorithms such as conjugate - gradient , bfgs , and trust - region methods for the riemmanian manifold @xmath17 are explained in @xcite .",
    "here we only provide details for a limited memory version of riemmanian bfgs ( rbfgs ) . the rbfgs algorithm for general retraction and vector transport",
    "was originally explained in @xcite and the proof of convergence appeared in @xcite , although for a slightly different version .",
    "it was proved that for g - convex functions and with line - search that satisfies wolfe conditions , rbfgs algorithm has a ( local ) superlinear convergence rate .",
    "the rbfgs algorithm can be transformed into a limited - memory rbfgs ( l - rbfgs ) algorithm by unrolling the update step of the approximate hessian computation as shown in algorithm  [ alg.lrbfgs ] . as may be apparent from the algorithm",
    ", parallel transport and its inverse can be the computational bottlenecks .",
    "one possible speed - up is to store the matrix @xmath268 and its inverse in  .",
    "riemannian manifold @xmath3 with riemannian metric @xmath269 ; vector transport @xmath270 on @xmath3 with associated retraction @xmath162 ; initial value @xmath271 ; a smooth function @xmath169 set initial @xmath272 obtain descent direction @xmath273 by unrolling the rbfgs method + @xmath274 use line - search to find @xmath275 s.t .",
    "@xmath276 is sufficiently smaller than @xmath277 calculate @xmath278 define @xmath279 define @xmath280 update @xmath281 store @xmath282 ; @xmath283 ; @xmath284 ; @xmath285 ; @xmath286 @xmath287 + * function * @xmath288 @xmath289 @xmath290 @xmath291 @xmath292 * end function *",
    "though manifold optimisation is powerful and widely applicable ( see e.g. , the excellent toolbox  @xcite ) , for a special class of geometric optimisation problems we may be able to circumvent its heavy machinery in favour of potentially much simpler algorithms .",
    "this motivation underlies the material developed in this section , where ultimately our goal is to obtain fixed - point iterations by viewing @xmath17 as a convex cone instead of a riemannian manifold .",
    "this viewpoint is grounded in nonlinear perron - frobenius theory  @xcite , and it proves to be of practical value for our application in  [ sec.ecd ] .",
    "notably , for certain problems we can obtain globally optimal solutions even without g - convexity .",
    "we believe the general conic optimisation theory developed in this section may be of wider interest .",
    "consider thus the following minimisation problem @xmath293 where @xmath48 is a continuously differentiable real - valued function on @xmath17 .",
    "since the constraint set @xmath294 is an open subset of a euclidean space , the first - order optimality condition for   is similar to that of unconstrained optimisation .",
    "a point @xmath295 is a candidate local minimum of @xmath48 only if its gradient at this point is zero , that is , @xmath296    the nonlinear ( matrix ) equation   could be solved using numerical techniques such as newton s method .",
    "but , such approaches can be computationally more demanding than the original optimisation problem , especially because they involve the ( inverse of ) the second derivative @xmath297 at each iteration .",
    "we propose to exploit a fixed - point iteration that offers a simpler method for solving  . _",
    "more importantly , _",
    "the fixed - point technique allows one to show that under certain conditions the solution to   is unique , and therefore potentially a global minimum ( essentially , if the global minimum is attained , then it must be this unique stationary point ) .",
    "assume therefore that   is rewritten as the fixed - point equation @xmath298 then , a fixed - point of the map @xmath299 is a potential solution ( since it is a stationary point ) to the minimisation problem  .",
    "the natural question is how to find such a fixed - point , and starting with a feasible @xmath300 , whether it suffices to perform the picard iteration @xmath301 iteration   is ( usually ) _ not _ a fixed - point iteration when cast in a normed vector space  the conic geometry of @xmath17 alluded to previously suggests that it might be better to analyse the iteration using a non - vectorial metric .",
    "we provide below a class of sufficient conditions ensuring convergence of  .",
    "therein , the correct metric space in which to study convergence is neither the euclidean ( or banach ) space @xmath302 nor the riemannian manifold @xmath17 with distance  .",
    "instead , a conic metric proves more suitable , namely , the thompson part metric , an object of great interest in nonlinear perron - frobenius theory  @xcite .",
    "our sufficient conditions stem from the following key definition .",
    "let @xmath303 .",
    "we say @xmath169 is _ log - nonexpansive _ ( ln ) on a compact interval @xmath304 if there exists a constant @xmath305 such that @xmath306",
    "if @xmath307 , we say @xmath169 is _ @xmath308-log - contractive_. if for every @xmath309 it holds that @xmath310 we say @xmath169 is log - contractive .",
    "we use log - nonexpansive functions in a concrete optimisation task in section  [ sec.example ] .",
    "the proofs therein rely on core properties of the thompson metric and contraction maps in the associated metric space  we cover requisite background in section  [ sec.thompson ] .",
    "the content of section  [ sec.thompson ] is of independent interest as the theorems therein provide techniques for establishing contractivity ( or nonexpansivity ) of nonlinear maps from @xmath17 to @xmath311 .",
    "on @xmath17 , the _ thompson metric _ is defined as ( _ cf .",
    "_ @xmath312 which uses @xmath313 ) @xmath314 where @xmath315 is the usual operator norm ( largest singular value ) , and ` log ' is the matrix logarithm .",
    "let us recall some core ( known ) properties of  for details please see  @xcite .",
    "[ prop.thom ] unless noted otherwise , all matrices are assumed to be hpd . @xmath316\\\\",
    "\\label{eq:7 }        { \\delta_t}\\bigl({\\sum\\nolimits}_i w_ix_i , { \\sum\\nolimits}_i w_iy_i \\bigr ) & \\quad\\le\\quad \\max_{1",
    "\\le i \\le m } { \\delta_t}(x_i , y_i),\\qquad w_i \\ge 0 , w \\neq",
    "0\\\\        \\label{eq:10 }        { \\delta_t}(x+a , y+a ) & \\quad\\le\\quad        \\frac{\\alpha}{\\alpha+\\beta}{\\delta_t}(x , y),\\qquad a \\succeq 0 ,      \\end{aligned}\\ ] ] where @xmath317 and @xmath318 .",
    "we prove now a powerful refinement to  , which shows contraction under `` compression . ''",
    "[ thm.comp ] let @xmath319 , where @xmath320 have full column rank .",
    "let @xmath321 , @xmath322 .",
    "then , @xmath323    let @xmath324 and @xmath325 denote the `` compressions '' of @xmath321 and @xmath94 , respectively ; these compressions are invertible since @xmath107 is assumed to have full column rank .",
    "the largest generalised eigenvalue of the pencil @xmath326 is given by @xmath327 starting with   we have the following relations : @xmath328 similarly , we can show that @xmath329 .",
    "since @xmath321 , @xmath94 and the matrices @xmath330 , @xmath331 are all positive , we may conclude @xmath332 which is nothing but the desired claim @xmath333 .",
    "theorem  [ thm.comp ] can be extended to encompass more general `` compression '' maps , namely to those defined by operator monotone functions , a class that enjoys great importance in matrix theory ",
    "see e.g. ,  ( * ? ? ?",
    "v ) and @xcite .",
    "[ thm.comp2 ]",
    "let @xmath169 be an operator monotone ( i.e. , if @xmath334 , then @xmath335 ) function on @xmath101 such that @xmath336 .",
    "then , @xmath337    if @xmath169 is operator monotone with @xmath336 , then it admits the integral representation  ( * ? ? ?",
    "* ( v.53 ) ) @xmath338 where @xmath339 , @xmath340 , and @xmath341 is a nonnegative measure . using   we get @xmath342 similarly",
    ", we obtain @xmath343 .",
    "now , consider at first @xmath344 next , defining @xmath345 , we can use the above contraction to help prove contraction for the map @xmath169 as follows : @xmath346 moreover , for @xmath347 the inequality is strict if @xmath348 .",
    "let @xmath349 , let @xmath350 for @xmath351 and @xmath352 . then",
    ", @xmath353    theorem  [ thm.comp ] and theorem  [ thm.comp2 ] together yield the following general result .",
    "[ cor.comp ] let @xmath60 ( @xmath89 ) , and @xmath354 ( @xmath355 ) be completely positive ( see e.g. ,  ( * ? ?",
    "3 ) ) maps .",
    "then , @xmath356    we prove  ; the proof of is similar , hence omitted . from theorem  [ thm.comp2 ]",
    "it follows that @xmath357 .",
    "since @xmath48 is completely positive , it follows from a result of @xcite and @xcite that there exist matrices @xmath358 , @xmath359 , such that @xmath360 theorem  [ thm.comp ] and property   imply that @xmath361 , which proves  .",
    "let @xmath362 be a map from @xmath363 .",
    "analogous to  , we say @xmath362 is ( thompson ) _ log - nonexpansive _",
    "if @xmath364 the maps is called _ log - contractive _ if the inequality is strict .",
    "we present now a key result that justifies our nomenclature and the analogy to  : it shows that the sum of a log - contractive map and a log - nonexpansive map is log - contractive .",
    "this behaviour is a striking feature of the nonpositive curvature of @xmath17 ; such a result does _ not _ hold in normed vector spaces .",
    "[ thm.thomsum ] let @xmath362 be a log - nonexpansive map and @xmath365 be a log - contractive one .",
    "then , their sum @xmath366 is log - contractive .",
    "we start by writing thompson metric in an alternative form  @xcite : @xmath367 where @xmath368 . let @xmath369 ; then it follows that @xmath370 .",
    "since @xmath362 is nonexpansive in @xmath371 , using   it further follows that @xmath372 and @xmath365 is log - contractive map , we obtain the inequality @xmath373 write @xmath374 ; then , we have the following inequalities : @xmath375 as @xmath376 , using we obtain @xmath377 \\bigr).\\ ] ] we also have the following eigenvalue inequality @xmath378 combining inequalities and we see that @xmath379 \\bigr).\\ ] ] similarly , since @xmath380 , we also obtain the bound ( notice we now have @xmath381 instead of @xmath382 ) @xmath383 \\bigr).\\ ] ] combining and into a single inequality , we get @xmath384 \\bigr).\\end{gathered}\\ ] ] as the second term is @xmath385 , the inequality is strict , proving log - contractivity of @xmath386 .    using log - contractivity",
    "we can finally state our main result for this section .",
    "[ thm.ln.convg ] if @xmath362 is log - contractive and equation   has a solution , then this solution is unique and iteration converges to it .",
    "if   has a solution , then from a theorem of @xcite , it follows that the log - contractive map @xmath362 yields iterates that stay within a compact set and converge to a unique fixed point of @xmath362 .",
    "this fixed - point is positive definite by construction ( starting from a positive definite matrix , none of the operations in   violates positivity ) .",
    "thus , the unique solution is positive definite .      to illustrate how to exploit log - nonexpansive functions for optimisation ,",
    "let us consider the following minimisation problem @xmath387 which arises in maximum - likelihdood estimation of ecds ( see section  [ sec.ecd ] for further examples and details ) and also m - estimation of the scatter matrix  @xcite .",
    "the first - order necessary optimality condition for   stipulates that a candidate solution @xmath388 must satisfy @xmath389 defining @xmath390 , may be rewritten more compactly in matrix notation as the equation @xmath391 where @xmath392 , and @xmath393 $ ] .",
    "we then solve the nonlinear equation   via a fixed - point iteration .",
    "introducing the nonlinear map @xmath299 that maps @xmath394 to the right hand side of  , we use fixed - point iteration   to find the solution . in order to show that the picard iteration converges ( to the unique fixed - point ) ,",
    "it is enough to show that @xmath362 is log - contractive ( see theorem  [ thm.ln.convg ] ) .",
    "the following proposition gives sufficient condition on @xmath33 , under which the map is log - contractive .",
    "[ prop.ln ] let @xmath33 be log - nonexpansive .",
    "then , the map @xmath362 in   is log - nonexpansive .",
    "moreover , if @xmath33 is log - contractive , then @xmath362 is log - contractive .",
    "let @xmath395 be arbitrary .",
    "then , we have the following chain of inequalities @xmath396 the first inequality follows from   and theorem  [ thm.comp ] ; the second inequality follows since @xmath397 and @xmath398 are diagonal ; the third follows from  ; the fourth from another application of theorem  [ thm.comp ] , while the final equality is via  .",
    "this proves log - nonexpansivity ( i.e. , nonexpansivity in @xmath371 ) . if in addition @xmath33 is log - contractive and @xmath399 , then the second inequality above is strict , that is , @xmath400",
    "if @xmath33 is merely log - nonexpansive ( not log - contractive ) , it is still possible to show uniqueness of   up to a constant .",
    "our proof depends on the compression property of @xmath371 proved in theorem  [ thm.comp ] .",
    "[ thm.thomln ] let the data @xmath401 span the whole space .",
    "if @xmath33 is ln , and @xmath402 are solutions to equation  , then iteration converges to a solution , and @xmath403 .    without loss of generality",
    "assume that @xmath404 .",
    "let @xmath405 .",
    "theorem  [ thm.comp ] implies that @xmath406 as per assumption , the data span the whole space . since @xmath405 , we can find @xmath407 such that @xmath408 therefore , we obtain the following inequality for point @xmath407 : @xmath409 using proposition  [ prop.ln ] and invoking theorem  [ thm.thomsum ] , it then follows that @xmath410 but this means that @xmath411 can not be a solution to  , a contradiction .",
    "therefore , @xmath412 .",
    "so far we did not address computational efficacy of the fixed - point algorithm .",
    "the rate of convergence depends heavily on the contraction factor , and as we will see in the experiments , without further care one obtains poor contraction factors that can lead to a very slow convergence .",
    "we briefly discuss below a useful speedup technique that seems to have a dramatic impact on the empirical convergence speed ( see figure  [ fig.dim ] ) .    at the fixed point @xmath413",
    "we have @xmath414 , or equivalently for a new map @xmath3 we have @xmath415 therefore , one way to analyse the convergence behaviour is to assess how fast @xmath416 converges to identity .",
    "using the theory developed beforehand , it is easy to show that @xmath417 where @xmath418 is the contraction factor between @xmath283 and @xmath419 , so that @xmath420 to increase the convergence speed we may replace @xmath421 by its scaled version @xmath422 such that @xmath423 one can do a search to find a good @xmath424 .",
    "clearly , the sequence @xmath425 converges at a faster pace .",
    "we will see in the numerical results section that scaling with @xmath424 has a remarkable effect on the convergence speed .",
    "an intuitive reasoning why this happens is that the additional scaling factor can resolve the problematic cases where the contraction factor become small .",
    "these problematic cases are those where both the smallest and the largest eigenvalues of @xmath416 become smaller ( or larger ) than one , whereby the contraction factor ( for @xmath362 ) becomes small , which may lead to a very slow convergence .",
    "the scaling factor , however , makes the smallest eigenvalues of @xmath416 always smaller and its largest eigenvalue larger than one .",
    "one way to avoid the search is to choose @xmath424 such that @xmath426though with a small caveat : empirically this simple choice of @xmath424 works very well , but our convergence proof does not hold anymore . extending our convergence theory to incorporate this specific choice of scaling @xmath424",
    "is a part of our future work . in all simulations in the result section @xmath424",
    "is selected by ensuring @xmath426 .",
    "in this section we present details for a concrete application of conic geometric optimisation : mle for ecds  @xcite .",
    "we use ecds as a platform for illustrating geometric optimisation because ecds are widely important ( see e.g. , the survey  @xcite ) , and are instructive in illustrating our theory . first , some basics .",
    "if an ecd has density on @xmath427 , it assumes the form @xmath428 where @xmath429 is the scatter matrix and @xmath430 is the _ density generating function _ ( dgf ) .",
    "if the ecd has finite covariance , then the scatter matrix is proportional to the covariance matrix  @xcite .",
    "let @xmath431 ; then , reduces to the multivariate gaussian density . for @xmath432 where @xmath275 , @xmath433 , @xmath434 are fixed",
    ", density   yields the rich class called _ kotz - type distributions _ that have powerful modelling abilities  @xcite ; they include as special cases multivariate power exponentials , elliptical gamma , multivariate w - distributions , for instance .",
    "other examples include multivariate student - t , multivariate logistic , and weibull dgfs ( see  [ sec.ecd.gc ] ) .",
    "let @xmath435 be i.i.d .",
    "samples from an ecd  @xmath436 . ignoring constants ,",
    "the log - likelihood is @xmath437 to compute a mle we equivalently consider the minimisation problem  , which we restate here for convenience @xmath438 unfortunately , is in general very difficult : @xmath48 may be nonconvex and may have multiple local minima ( observe that @xmath439 is concave in @xmath440 and we are minimising ) . since statistical estimation relies on having access to globally optimal estimates , it is important to be able to solve   globally .",
    "these difficulties notwithstanding , using our theory we identify a rich class of ecds for which we can solve   globally . some examples are already known  @xcite , but our techniques yield results strictly more general : they subsume previous examples while advancing the broader idea of geometric optimisation over hpd matrices .    building on  [ sec.gc ] and  [ sec.ln ] , we divide our study into the following three classes of dgfs :    a.   geodesically convex ( g - convex ) : this class contains functions for which the negative log - likelihood @xmath441 is g - convex . some members of this class have been previously studied ( though sometimes without recognising or directly exploiting g - convexity ) ; b.   * * l**og-**n**onexpansive ( ln ) : this is a new class introduced in this paper .",
    "it exploits the `` non - positive curvature '' property of the hpd manifold . to our knowledge , this class of ecds was beyond the grasp of previous methods  @xcite . the iterative algorithm for finding the global minimum of the objective is similar to that of the class lc .",
    "c.   * * l**og-**c**onvex ( lc ) : we cover this class for completeness ; it covers the case of log - convex @xmath442 , but leads to nonconvex @xmath48 ( due to the @xmath443 term ) .",
    "however , the structure of the problem is such that one can derive an efficient algorithm for finding a local minumum of the objective function .    as illustrated in figure",
    "[ fig.fc ] , these three classes can overlap .",
    "when a function is in the overlap between classes lc and g - convex , one can be sure that the iterative algorithm derived for the class ln will converge to a unique minimum . table  [ tab : algo ] summerizes the applicability of fixed - point or manifold optimization methods on different classes of dgfs .    .",
    "]    .applicability of the different algorithms : ` * * yes * * ' means a preferred algorithm ; ` can@xmath444 ' denotes applicability on a case - by - case basis ; ` can ' signifies possible applicability of method . [ cols=\"^,^,^\",options=\"header \" , ]      if the log - likelihood is strictly g - convex then   can not have multiple solutions .",
    "moreover , for any local optimisation method that ensures a local solution to  , g - convexity ensures that this solution is globally optimal .",
    "first we state a corollary of theorem  [ thm.gc ] that helps recognise g - convexity of ecds .",
    "we remark that a result equivalent to corollary  [ cor.ecd ] was also recently discovered in  @xcite .",
    "theorem  [ thm.gc ] is more general and uses a completely different argument founded on matrix - theoretic results .",
    "[ cor.ecd ] let @xmath445 be g - convex ( i.e. , @xmath446 ) .",
    "if @xmath33 is nondecreasing , then for @xmath447 , @xmath448 is g - convex .",
    "furthermore if @xmath33 is strictly g - convex , then @xmath234 is also strictly g - convex .",
    "immediate from theorem  [ thm.gc ] since @xmath449 is a positive linear map .    for reference ,",
    "we summarise several examples of strictly g - convex ecds in corollary  [ corollary.kotz ] .",
    "[ corollary.kotz ] the negative log - likelihood   is strictly g - convex for the following distributions :    kotz with @xmath450 ( its special cases include gaussian , multivariate power exponential , multivariate w - distribution with shape parameter smaller than one , elliptical gamma with shape parameter @xmath451 ) ;    multivariate-@xmath452 ;    multivariate pearson type ii with positive shape parameter ;    elliptical multivariate logistic distribution .    ;",
    "multivariate w - distribution : @xmath453 ; elliptical gamma : @xmath454 ; multivariate t : @xmath455 ; multivariate pearson type ii : @xmath456 ; elliptical multivariate logistic : @xmath457 . ]",
    "even though g - convexity ensures that every local solution will be globally optimal , we must first ensure that there exists a solution at all , that is , _",
    "does   have a solution ? _ answering this question is nontrivial even in special cases  @xcite .",
    "we provide below a fairly general result that helps establish existence .",
    "[ thm.existmin ] let @xmath441 satisfy the following : ( i ) @xmath458 is lower semi - continuous ( lsc ) for @xmath459 , and ( ii ) @xmath460 as @xmath461 or @xmath462 , then @xmath441 attains its minimum .",
    "consider the metric space @xmath463 , where @xmath464 is the riemannian distance , @xmath465 if @xmath460 as @xmath461 or as @xmath462 , then @xmath441 has bounded lower - level sets in @xmath463 .",
    "it is a well - known result in variational analysis that an lsc function which has bounded lower - level sets in a metric space attains its minimum  @xcite .",
    "since @xmath458 is lsc and @xmath466 is continuous , @xmath441 is lsc on @xmath463 .",
    "therefore it attains its minimum .",
    "a key consequence of this theorem is its utility is in showing existence of solutions to   for a variety of different ecds .",
    "we show an example application to kotz - type distributions  @xcite below . for these distributions ,",
    "the function @xmath467 assumes the form @xmath468 lemma  [ lem.kotzinfty ] shows that @xmath469 whenever @xmath470 or @xmath471 .",
    "[ lem.kotzinfty ] let the data @xmath401 span the whole space and for @xmath472 satisfy @xmath473 where @xmath474 is an arbitrary subspace with dimension @xmath475 and @xmath476 is the number of datapoints that lie in the subspace @xmath474 . if @xmath470 or @xmath471 , then @xmath469 .",
    "if @xmath470 and since the data span the whole space , it is possible to find a datum @xmath477 such that @xmath478 . since @xmath479 for constants @xmath480,@xmath481 and @xmath482 , it follows that @xmath469 whenever @xmath470 .",
    "if @xmath471 and @xmath483 is bounded , then the third term in expression of @xmath484 is bounded .",
    "assume that @xmath485 is the number of eigenvalues of @xmath394 that go to @xmath486 and @xmath476 is the number of data that lie in the subspace span by these eigenvalues .",
    "then in the limit when eigenvalues of @xmath394 go to @xmath486 , @xmath484 converges to the following limit @xmath487 apparently if @xmath488 , then @xmath489 and the proof is complete .",
    "it is important to note that overlap condition   can be fulfilled easily by assuming that the number of data points is larger than their dimensionality and that they are noisy . using lemma  [ lem.kotzinfty ] with theorem  [ thm.existmin ]",
    "we obtain the following key result for kotz - type distributions .",
    "[ thm.kotzexist ] if the data samples satisfy condition  , then log - likelihood of kotz - type distribution has a maximiser ( i.e. , there exists an mle ) .",
    "once existence is ensured , one may use any local optimisation method to minimise   to obtain the desired mle . for members of the class g - convex that do not lie in class ln or class lc",
    ", we recommend invoking the manifold optimisation techniques summarised in  [ sec.manopt ] .      for negative log - likelihoods   in class ln , we can circumvent the heavy machinery of manifold optimisation , and obtain simple fixed - point algorithms by appealing to the contraction results developed in  [ sec.ln ] .",
    "we note that some members of class g - convex may also turn out to lie in class ln , so the discussion below also applies to them .    as an illustrative example of these results , consider the problem of finding the minimum of negative log - likelihood solution of kotz - type distribution  .",
    "if the corresponding nonlinear equation   with corresponding @xmath490 has a positive definite solution , then it is a candidate mle ; if it is unique , then it is the desired solution to  .    but",
    "how should we solve  ? this is where the theory developed in  [ sec.ln ] comes into play .",
    "convergence of the iteration   as applied to   can be obtained from theorem  [ thm.thomln ] .",
    "but in the kotz case we can actually show a stronger result that helps ensure better geometric convergence rates for the fixed - point iteration .",
    "[ lem.lnkotz ] if @xmath491 and @xmath492 , then @xmath493 is log - contractive .    without loss of generality",
    "assume @xmath494 with @xmath495 .",
    "assume that @xmath496 : @xmath497 since the second term is negative , therefore @xmath269 is log - contractive .",
    "consider the other case @xmath496 , that could happen only when @xmath498 .",
    "@xmath499 in this case , the second term is also negative . therefore @xmath33 is log - contractive .",
    "assume @xmath500 , @xmath501 and knowing that @xmath502 has the same contraction factor as @xmath503 , lemma  [ lem.lnkotz ] implies that @xmath33 in the iteration   for the kotz - type distributions with @xmath504 and @xmath505 is log - contractive .",
    "based on theorem  [ thm.kotzexist ] , @xmath484 has at least one minimum . thus using theorem  [ thm.ln.convg ]",
    ", we have the following main convergence result .    [ thm.kotz.cvg ]",
    "if the data samples satisfy  , then iteration   for kotz - type distributions with @xmath504 and @xmath505 converges to a unique fixed point .",
    "for completeness , we briefly mention class lc here , which is perhaps one of the most studied classes of ecds , at least from an algorithmic point - of - view  @xcite . therefore",
    ", we only discuss it summarily , and present our new results .    for the class lc",
    ", we assume that the dgf @xmath442 is log - convex . without assumptions that are typically made in the literature , it can be that neither the gc nor the ln analysis applies to class lc .",
    "however , the optimisation problem still has structure that allows simple and efficient algorithms",
    ". specifically , here the objective function @xmath467 can be written as a a difference of two convex functions by introducing the variable @xmath506 , wherewith we have @xmath507 . to this representation of @xmath48",
    "we may now apply the cccp procedure  @xcite to search for a locally optimal point .",
    "the method operates as follows @xmath508 which yields the update @xmath509    because @xmath510 is constructed using the cccp procedure , it can be shown that the sequence @xmath511 is monotonically decreasing . furthermore , since we assumed @xmath33 to be nonnegative , therefore the iteration stays within positive semidefinite cone .",
    "if the cost function goes to infinity whenever the covariance matrix is singular , then using theorem  [ thm.existmin ] we can conclude that iteration converges to a positive definite matrix .",
    "thus , we can state the following key result for class lc .",
    "[ thm.lcconverge ] assume that @xmath512 goes to infinity whenever @xmath513 reaches the boundary of @xmath514 , i.e. @xmath515 .",
    "furthermore if @xmath443 is concave and @xmath33 is non - negative , then each step of the iterative algorithm given in   decreases the cost function and furthermore it converges to a positive definite solution .",
    "a similar theorem but under more strict conditions was established in @xcite .",
    "knowing that the iterative algorithm in   is the same as and using theorem  [ thm.lcconverge ] with the existence result of theorem  [ thm.kotzexist ] and the uniqueness result of corollary  [ corollary.kotz ] , we can state the following theorem for kotz - type distributions ( _ cf . _ theorem  [ thm.kotz.cvg ] ) .",
    "[ thm.kotz.cvg2 ] if the data samples satisfy condition  , then iteration   for kotz - type distributions with @xmath516 and @xmath505 converges to a unique fixed point .",
    "theorem  [ thm.kotz.cvg2 ] and theorem  [ thm.kotz.cvg ] together show that the iteration   for kotz - type distributions with @xmath505 and regardless of the value of @xmath517 always converges to the unique mle estimate whenever it exists .",
    "we briefly illustrate the numerical performance of our fixed - point iteration .",
    "the key message here is that our fixed - point iterations solve nonconvex problems that are further complicated by a positive definiteness constraint .",
    "but by construction the fixed - point iterations satisfy the constraint , so no extra eigenvalue computation is needed , which can provide substantial computational savings .",
    "in contrast , a general nonlinear solver must perform constrained optimisation , which may be unduly expensive .",
    "we show two experiments ( figs .",
    "[ fig.dim ] and [ fig.two ] ) to demonstrate scalability of the fixed - point iteration with increasing dimensionality of the input matrix and for varying @xmath517 parameter of the kotz distribution which influences convergence rate of our fixed - point iteration . for all simulations",
    ", we sampled 10,000 datapoints from the kotz - type distribution with given @xmath275 and @xmath517 parameters and a random covariance matrix .",
    "we note that the problems are nonconvex with an open set as a constraint  this precludes direct application of semidefinite programming or approaches such as gradient - projection ( projection requires closed sets ) .",
    "we also tried interior - point methods but we did not include them in the comparisons because of their extremely slow convergence speed on this problem . so we choose to show the result of ( riemannian ) manifold optimisation techniques  @xcite .",
    "we compare our fixed - point iteration against four different manifold optimisation methods : ( i ) steepest descent ( sd ) ; ( ii ) conjugate gradients ( cg ) ; ( iii ) trust - region ( tr ) ; and ( iv ) lbfgs , which implements algorithm  [ alg.lrbfgs ] .",
    "all methods are implemented in matlab ( including the fixed - point iteration ) ; for manifold optimisation we extend the manopt toolbox  @xcite to support the hpd manifold as well as algorithm  [ alg.lrbfgs ] .    from figure  [ fig.dim ]",
    "we see that the basic fixed - point algorithm ( fp ) does not perform better than sd , the simplest manifold optimisation method",
    ". moreover , even when fp performs better than cg , tr , or lbfgs ( figure  [ fig.two ] ) , it seems to closely follow sd .",
    "however , the scaling idea introduced in  [ sec.example ] leads to a fixed - point method ( fp2 ) that outperforms all other methods , both with increasing dimensionality and varying @xmath517 . the scale is chosen by ensuring @xmath518 .",
    "these results merely indicate that the fixed - point approach can be competitive . a more thorough experimental study to assess",
    "our algorithms remains to be undertaken .",
    "0.3     0.3     0.3",
    "we studied geometric optimisation for minimising certain nonconvex functions over the set of positive definite matrices .",
    "we showed key results that help recognise geodesic convexity ; we also introduced a new class of log - nonexpansive functions which contains functions that need not be geodesically convex , but can still be optimised efficiently .",
    "key to our ideas was a construction of fixed - point iterations in a suitable metric space on positive definite matrices .",
    "additionally , we developed and applied our results in the context of maximum likelihood estimation for elliptically contoured distributions , covering instances substantially beyond the state - of - the - art .",
    "we believe that the general geometric optimisation techniques that we developed in this paper will prove to be of wider use and interest beyond our motivating examples and applications .",
    "moreover , developing a more extensive geometric optimisation numerical package is an ongoing project .",
    "nicolas boumal , bamdev mishra , p .- a .",
    "absil , and rodolphe sepulchre .",
    "anopt , a matlab toolbox for optimization on manifolds .",
    "_ journal of machine learning research _ , 15:0 14551459 , 2014 .",
    "url http://www.manopt.org .",
    "g.  cheng , h.  salehian , and b.  c. vemuri .",
    "efficient recursive algorithms for computing the mean diffusion tensor and applications to dti segmentation . in _",
    "european conference on computer vision ( eccv ) _ , volume  7 , pages 390401 , 2012 .",
    "a.  cherian , s.  sra , a.  banerjee , and n.  papanikolopoulos .",
    "jensen - bregman logdet divergence for efficient similarity computations on positive definite tensors .",
    "_ ieee transactions pattern analysis and machine intelligence _ , 2012 .",
    "b.  jeuris , r.  vandebril , and b.  vandereycken .",
    "a survey and comparison of contemporary algorithms for computing the matrix geometric mean . _ electronic transactions on numerical analysis _ , 39:0 379402 , 2012 .",
    "s.  kotz , n.  l. johnson , and d.  w. boyd .",
    "series representations of distributions of quadratic forms in normal variables .",
    "i. central case . _",
    "the annals of mathematical statistics _ , 380 ( 3):0 823837 , june 1967 .",
    "e.  ollila , d.e .",
    "tyler , v.  koivunen , and h.  v. poor .",
    "complex elliptically symmetric distributions : survey , new results and applications . _ ieee transactions on signal processing _ , 600 ( 11):0 55975625 , 2011 .      c.  qi , k.  a. gallivan , and p .- a .",
    "riemannian bfgs algorithm with applications . in _ recent advances in optimization and its applications in engineering _ ,",
    "pages 183192 .",
    "springer berlin heidelberg , january 2010 .",
    "h.  zhu , h.  zhang , j.g .",
    "ibrahim , and b.s .",
    "statistical analysis of diffusion tensors in diffusion - weighted magnetic resonance imaging data .",
    "_ journal of the american statistical association _ , 1020 ( 480):0 10851102 , 2007 ."
  ],
  "abstract_text": [
    "<S> we develop _ geometric optimisation _ on the manifold of hermitian positive definite ( hpd ) matrices . </S>",
    "<S> in particular , we consider optimising two types of cost functions : ( i ) geodesically convex ( g - convex ) ; and ( ii ) log - nonexpansive ( ln ) . </S>",
    "<S> g - convex functions are nonconvex in the usual euclidean sense , but convex along the manifold and thus allow global optimisation . </S>",
    "<S> ln functions may fail to be even g - convex , but still remain globally optimisable due to their special structure . </S>",
    "<S> we develop theoretical tools to recognise and generate g - convex functions as well as cone theoretic fixed - point optimisation algorithms . </S>",
    "<S> we illustrate our techniques by applying them to maximum - likelihood parameter estimation for elliptically contoured distributions ( a rich class that substantially generalises the multivariate normal distribution ) . </S>",
    "<S> we compare our fixed - point algorithms with sophisticated manifold optimisation methods and obtain notable speedups .    </S>",
    "<S> * keywords : * manifold optimisation , geometric optimisation , geodesic convexity , log - nonexpansive , conic fixed - point theory , thompson metric , vector transport , riemannian bfgs </S>"
  ]
}