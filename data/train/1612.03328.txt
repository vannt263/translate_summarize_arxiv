{
  "article_text": [
    "datasets with a small number of samples @xmath0 and a large number of variables @xmath1 are nowadays common . statistical learning , for example regression , in these kinds of problems",
    "is ill - posed , and it is known that statistical methods have limits in how low in sample size they can go @xcite .",
    "a lot of recent research in statistical methodology has focused on finding different kinds of solutions via well - motivated trade - offs in model flexibility and bias .",
    "these include strong assumptions about the model family , such as linearity , low rank , sparsity , meta - analysis and transfer learning from related datasets , efficient collection of new data via active learning , and , less prominently , prior elicitation .",
    "there is , however , a certain disconnect between the development of state - of - the - art statistical methods and their application in challenging data analysis problems .",
    "many applications have significant amounts of previous knowledge to incorporate into the analysis , but this is often unstructured and tacit .",
    "building it into the analysis would require tailoring the model and eliciting the knowledge in a suitable format for the analysis , which would be burdensome for both experts in statistical methods and experts in the problem domain .",
    "more commonly , new methods are developed to work well in some broad class of problems and data , and domain experts use default approaches and apply their previous knowledge post - hoc for interpretation and discussion .",
    "even when experts in both fields are directly collaborating , the feedback loop between the method development and application is often slow .",
    "we propose to directly integrate the user into the modelling loop by formulating knowledge elicitation as a probabilistic inference process .",
    "we study a specific case of sparse linear regression with the aim of solving prediction problems where the number of available samples ( `` training data '' ) is insufficient for statistically accurate prediction .",
    "a core characteristic of the formulation is that it adapts to the feedback obtained from the expert and it sequentially integrates every piece of information before deciding on the next query for the expert .",
    "the sequential aspect of the approach efficiently reduces the burden on the expert , since the most informative queries will be asked first , thus reducing the overall number of needed interactions and allowing knowledge elicitation for high - dimensional parameters ( such as the regression weights ) . by interactively eliciting and incorporating expert knowledge , our approach fits into the interactive learning literature .",
    "our focus is here on the probabilistic modelling part of the interaction and we leave the design of supporting user interfaces for future work .      the outline of the paper and our main contributions are as follows . after discussing related work ( sect .",
    "[ sec : related_work ] ) , we rigorously formulate the expert knowledge elicitation as a probabilistic inference process ( sect .",
    "[ sec : framework ] ) .",
    "we study a specific case of sparse linear regression , and in particular , consider cases where the user has knowledge about the values of the regression coefficients and about the relevance of the features ( sect .",
    "[ sec : sparse_linear_regression ] ) .",
    "we present an algorithm for efficient interactive sequential knowledge elicitation for high - dimensional models that makes knowledge elicitation in `` small @xmath0 , large @xmath1 '' problems feasible ( sect .",
    "[ sec : algo ] ) .",
    "we describe an efficient computational approach using deterministic posterior approximations allowing real - time interaction for the sparse linear regression case ( sect .  [",
    "sec : computation ] ) .",
    "simulation studies are presented to demonstrate the performance and to gain insight into the behaviour of the approach ( sect .  [",
    "sec : experiments ] ) . finally , we demonstrate that real users are able to improve the predictive performance of sparse linear regression in a proof - of - concept experiment ( sect .",
    "[ sec : expes_real_user ] ) .",
    "the problem we study relates to several topics studied in the literature , either by the method , goal , or by the considered setting . in this section ,",
    "we highlight the main connections .",
    "* interactive learning . *",
    "interactive machine learning includes a variety of ways to employ user s knowledge , preferences , and human cognition to enhance statistical learning @xcite .",
    "these methods have been used successfully in several applications , such as learning user intent  @xcite and preferential clustering .",
    "for instance , the semi - supervised clustering method in  @xcite uses feedback on pairs of items that should or should not be in the same cluster , to learn user preferences .",
    "in addition to the differences coming from the learning task , one notable contrast between these works and our method is that their aim is to identify user preferences or opinions , whereas our goal is to use expert knowledge as an additional source of information for an improved prediction model , by integrating it with the knowledge coming from the ( small @xmath0 ) data . as a probabilistic approach , our work relates to  @xcite and  @xcite , where expert feedback is used for improved learning of bayesian networks and for visual data exploration , respectively . in sect .",
    "[ sec : framework_examples ] , we show how these works can be seen as instances of the general approach we propose .",
    "* active learning . *",
    "the method we propose for efficiently using expert feedback is related to active learning techniques ( for a survey , see , for instance , @xcite ) , where the algorithms actively select the most informative data points to be used in prediction tasks .",
    "our method similarly queries the user for information with the goal of maximising the information gain from each feedback and thus learning more accurate models with less feedback .",
    "the same definition of efficiency with respect to the use of samples , also connects our work with experimental design techniques , recently used for linear settings by seeger  @xcite , hernndez - lobato et al .",
    "@xcite , and ravi et al .",
    "our task , however , is different as we do not aim at collecting new data samples , but the additional information comes from a different source , the expert , with its respective bias and uncertainty . indeed , our method will be most useful in cases where obtaining additional input samples would be too expensive",
    ".    * prior elicitation and privileged information . *",
    "many works have studied approaches for efficient elicitation of human and , in particular , expert knowledge . in prior elicitation  @xcite ,",
    "the goal is to use expert knowledge to construct a prior distribution for bayesian data analysis and restrict the range of parameters to be later used in learning models .",
    "notably , an important line of work @xcite studies methods of quantifying subjective opinion about the coefficients of linear regression models through the assessment of credible intervals .",
    "our approach goes beyond pure prior elicitation as the training data is used to facilitate efficient user interaction .",
    "another line of work considers expert feedback as privileged information  @xcite , where additional human knowledge is allowed in the training phase only . differently to our method , these works typically do not consider an interactive integration of the expert knowledge with the training data , and do not model the reliability of the human feedback thus received , rather , they use it as a guideline for improving the performance of learning tasks .",
    "in the following , we formulate expert knowledge elicitation as a probabilistic inference process .",
    "let @xmath2 and @xmath3 denote the outputs ( target variables ) and inputs ( covariates ) , and @xmath4 and @xmath5 the model parameters .",
    "let @xmath6 encode input from the user ( feedback based on the user s knowledge ) and @xmath7 be related model parameters .",
    "we identify the following key components :    1 .",
    "an observation model @xmath8 for @xmath2 .",
    "a feedback model @xmath9 for user s knowledge .",
    "a prior model @xmath10 completing the hierarchical model description .",
    "4 .   a query algorithm and user interface that facilitate gathering @xmath6 iteratively from the user . 5 .",
    "update process of the model after user interaction .",
    "the observation model can be any appropriate probability model .",
    "it is assumed that there is some parameter @xmath4 , possibly high - dimensional , that the user has knowledge about .",
    "the user s knowledge is encoded as ( possibly partial ) feedback @xmath6 that is transformed into information about @xmath4 via the feedback model .",
    "of course , there could be a more complex hierarchy tying the observation and feedback models , and the feedback model can also be used to model more user - centric issues , such as the quality of or uncertainty in the knowledge or user s interests .    the feedback model , together with a query algorithm and a user interface , is used to facilitate an efficient interaction with the user .",
    "the term `` query algorithm '' is used here in a broad sense to describe any mechanism that is used to intelligently guide the user s focus in providing feedback to the system .",
    "this enables considering a high - dimensional @xmath6 without overwhelming the user as the most useful feedbacks can be queried first .",
    "crucially , this enables going beyond pure prior elicitation as the observed data can be used to inform the queries via the dependence of the feedback and observation models .",
    "for example , the queries can be formed as solutions to decision or experimental design tasks that maximize the expected information gain from the interaction .",
    "finally , as the user s feedback is modelled as additional data , bayes theorem can be used to sequentially update the model during the interaction . for real - time interaction ,",
    "this may present a challenge as computation in probabilistic models can be demanding .",
    "it is known that slow computation can impair effective interaction @xcite and , thus , efficient computational approaches are important .",
    "figure  [ fig : infoflow ] depicts the information flow .",
    "first , the posterior distribution given the observations @xmath11 is computed .",
    "then , the user is queried iteratively for feedback via the user interface and the query algorithm .",
    "the feedback is used to sequentially update the posterior distribution . the query algorithm has access to the latest beliefs about the model parameters and the predicted user behaviour , that is , the posterior predictive distribution of @xmath6 , @xmath12 where @xmath13 are possibly partial observations of @xmath6 , to formulate queries and highlight the most informative interactions in the user interface .    at ( -4.5,2 )",
    "( dm ) model @xmath1 ; at ( -4.5,0 ) ( d ) data @xmath14 ; at ( -3,1 ) ( p1 ) @xmath15 ; ( p2 ) @xmath16 ; ( p3 ) @xmath17 ; ( p4 ) ;    ( q1 ) query @xmath18 ; ( q2 ) query @xmath19 ;    \\(e ) expert knowledge ;    ( dm ) edge ( [ yshift=0.1cm]p1.west ) ; ( d ) edge ( [ yshift=-0.1cm]p1.west ) ;    ( [ yshift=-0.1cm]p1.east ) edge ( [ yshift=-0.1cm]p2.west ) ; ( [ yshift=-0.1cm]p2.east ) edge ( [ yshift=-0.1cm]p3.west ) ; ( p3 ) edge ( p4 ) ;    ( [ yshift=0.1cm]p1.east ) edge ( [ xshift=-0.1cm]q1.south ) ; ( [ xshift=0.1cm]q1.south ) edge ( [ yshift=0.1cm]p2.west ) ;    ( [ yshift=0.1cm]p2.east ) edge ( [ xshift=-0.1cm]q2.south ) ; ( [ xshift=0.1cm]q2.south ) edge ( [ yshift=0.1cm]p3.west ) ;    ( [ xshift=0.1cm]q1.north ) edge ( [ xshift=-0.2cm]e.south ) ; ( [ xshift=-0.4cm]e.south ) edge ( [ xshift=-0.1cm]q1.north ) ;    ( [ xshift=0.1cm]q2.north ) edge ( [ xshift=0.4cm]e.south ) ; ( [ xshift=0.2cm]e.south ) edge ( [ xshift=-0.1cm]q2.north ) ;      the goal in this paper is to use the interaction scheme to help solve prediction problems in the `` small @xmath0 , large @xmath1 '' setting .",
    "the approach as described above is , however , more general and applicable to other problems .",
    "we briefly describe two earlier works that can be seen as instances of it .",
    "@xcite present a method for integrating expert knowledge into learning of bayesian networks .",
    "the observation model is a multinomial bayesian network with dirichlet priors .",
    "the user provides answers to queries about the presence or absence of edges in the graph and the feedback model assumes the answers to be correct with some probability . which edge to query about next is selected by maximising the information gain with regard to the inclusion probability of the edges .",
    "monte carlo algorithms are used for the computation .",
    "@xcite present a framework for interactive visual data exploration .",
    "they describe two observation models , principal component analysis and multidimensional scaling , that are used for dimensionality reduction to visualise the observations in a two dimensional plot .",
    "they do not have a query algorithm , but their user interface allows moving points in a low - dimensional plot closer or further apart , which is interpreted by a feedback model that transforms the feedback into appropriate changes in the shared parameters with the observation model to allow exploration of different aspects of the data .",
    "their model affords closed form updates .",
    "[ sec : sparse_linear_regression ] we next introduce the knowledge elicitation approach for sparse linear regression .",
    "let @xmath20 be the observed output values and @xmath21 the matrix of covariate values .",
    "the regression is modelled with gaussian observation model and a spike - and - slab sparsity - inducing prior @xcite on the regression coefficients @xmath22 , and a gamma prior on the inverse of the residual noise variance @xmath23 : @xmath24    here , the @xmath25 are latent binary variables indicating inclusion or exclusion of the covariates in the regression ( @xmath26 is a point mass at zero ) and @xmath27 is the prior inclusion probability controlling the expected sparsity .",
    "the @xmath28 , @xmath29 , @xmath30 , and @xmath27 are assumed fixed hyperparameters .",
    "we consider two simple and natural feedback models encoding knowledge about the individual regression coefficients :    * user has knowledge about the value of the coefficient ( @xmath31 ) : @xmath32 * user has knowledge about the relevance of coefficient ( @xmath33 for not - relevant , relevant ) : @xmath34    here , @xmath35 and @xmath36 control the uncertainty or strength of the knowledge . in detail , @xmath35 is the uncertainty in the user s estimate of the coefficient , and @xmath36 is the probability that the user gives correct feedback relative to the state of the covariate inclusion indicator @xmath25 .",
    "our aim is to improve prediction .",
    "thus , the user interaction should focus on aspects of the model ( here , predictive features ) that would be most beneficial towards this goal .",
    "we use the query algorithm to rank the features for choosing which feature to ask feedback about next .",
    "the ranking is formulated as a bayesian experimental design task  @xcite .",
    "more specifically , the feature @xmath37 that maximizes the expected information gain is chosen next : @xmath38\\!,\\ ] ] where @xmath39 indexes the features , @xmath40 is the set of feedbacks that have already been given ( to simplify notation , those are here included in @xmath41 ) , and the summation over @xmath42 goes over the training dataset .",
    "the information gain is defined as the kullback ",
    "leibler divergence ( @xmath43 ) between the current posterior predictive distribution @xmath44 , where @xmath45 , and the posterior predictive distribution with the new feedback @xmath13 , @xmath46 .",
    "the bigger the information gain , the bigger impact the new feedback has on the predictive distribution .",
    "since the feedback itself will only be observed after querying the user , we take the expectation over the posterior predictive distribution of the feedback @xmath47 .",
    "more details about the bayesian experimental design are provided in the supplementary material ( sec .",
    "[ sec : bayes_ed ] ) .",
    "we note that , were the predictive distribution of @xmath2 gaussian , the problem would be simple .",
    "the expected information gain would be independent of @xmath2 and the actual values of the feedbacks ( when feedback is on values of the regression coefficients ) and would only depend on the @xmath3 and which features were given feedback on @xcite .",
    "the sparsity - promoting prior , however , makes the problem non - trivial .",
    "the model does not have a closed form posterior distribution , predictive distribution , or solution to the information gain maximization problem . to achieve fast computation , we use deterministic posterior approximations .",
    "expectation propagation @xcite is used to approximate the spike - and - slab prior @xcite and the feedback models , and variational bayes ( e.g. , ( * ? ? ?",
    "* chapter 10 ) ) is used to approximate the residual variance @xmath23 .",
    "the form of the posterior approximation for the regression coefficients @xmath48 is gaussian . the posterior predictive distribution for @xmath2 is also approximated as gaussian .",
    "details are provided in the supplementary material  ( sect .",
    "[ sec : posterior_approximation ] ) .",
    "expectation propagation has been found to provide good estimates of uncertainty , which is important in experimental design @xcite . in evaluating the expected information gain for a large number of candidate features , running the approximation iterations to full convergence for each , however , is too slow .",
    "we follow the approach of @xcite in computing only a single iteration of updates on the essential parameters for each candidate .",
    "we show in the results that this already provides a good performance for the query algorithm in comparison to random queries .",
    "details on the computations are provided in the supplementary material  ( sect .",
    "[ sec : single_iteration_update ] ) .",
    "the performance of the proposed method ( sect .",
    "[ sec : sparse_linear_regression ] ) is evaluated in several `` small @xmath0 , large @xmath1 '' regression problems on both simulated and real data .",
    "a proof - of - concept user study is presented to demonstrate the feasibility of the method with real users .",
    "we use synthetic data to study the behaviour of the approach in a wide range of controlled settings .",
    "* setting . * the covariates of @xmath0 training data points are generated from @xmath49 .",
    "out of the @xmath50 regression coefficients @xmath51 , @xmath52 are generated from @xmath53 and the rest are set to zero .",
    "the observed output values are generated from @xmath54 .",
    "we consider cases where the user has knowledge about the value of the coefficients ( eq .  [ eq : feedback_coefficient ] with noise value @xmath55 ) and where the user has knowledge about non - relevant / relevant features ( eq .  [ eq : feedback_relevance ] with @xmath56 if @xmath57 is non - zero , and @xmath58 otherwise , and @xmath59 ) . for a generated set of training data , all algorithms query feedback about one feature at a time .",
    "mean squared error ( mse ) is used as the performance measure to compare the query algorithms .",
    "for the simulated data setting , we use the known data - generating values for the fixed hyperparameters , namely : @xmath60 ( here we do not use the distribution assumption on @xmath23 ) .",
    "* results . * in fig .",
    "[ fig : heatmap_dimensions ] , we consider a `` small @xmath0 , large @xmath1 '' scenario , with @xmath61 and with increasing dimensionality ( hence also increasing sparsity ) from @xmath62 .",
    "the heatmaps show the average mse values over 100 runs ( repetitions of the data generation ) for both feedback models , as obtained by our sequential experimental design algorithm and by a strategy that randomly selects the sequence of features on which to ask for expert feedback .",
    "the result shows that our method achieves a faster improvement in the prediction , starting from the very first user feedbacks , for both feedback types , and at all the dimensionalities .",
    "notably , in the case of the random strategy , the performance decreases rapidly with the growing dimensionality ( even with 50 feedbacks , in the setting with 200 dimensions , the prediction error for random strategy stays high ) , while the user feedback via the sequential experimental design is informative enough to provide good predictions even in large dimensionalities . comparing the two types of feedback ,",
    "the feedback on the coefficient values gives better performance for both strategies .",
    "[ sec : heatmap_traindata ] in the supplement shows heatmaps for the same setting but with a fixed dimension @xmath63 and a varying number of training data @xmath64 . for those experiments",
    ", we can again see superior improvement for the sequential experimental design compared to random , for both feedback models , and in particular for small sample sizes . moreover , a comparison of the sequential experimental design algorithm to its non - sequential version ( sect .",
    "[ sec : seq_vs_nonseq ] in the supplement ) shows that the former achieves a better performance , indicating that the user feedback affects the next query . finally , for further insight into the behaviour of the approach , a simulation experiment with @xmath65 in sect .",
    "[ sec : trainingerror ] in the supplementary material shows that the training set error begins to increase as a function of the number of feedbacks while the test error decreases .",
    "this happens because the initial fit exhausts the information in the training data , but at this small sample size is insufficient to provide good generalization performance .",
    "we test our method for the task of predicting review ratings from textual reviews in subsets of amazon and yelp datasets .",
    "each review is one data point , and each distinct word is a feature with the corresponding covariate value given by the number of appearances of the word in the review .",
    "in addition to being fit for sparse linear regression models ( as shown in previous studies , for instance , in  @xcite ) , we also chose this type of dataset due to the uncomplicated interpretation of the features , which allows us to easily test our method on real users .",
    "the amazon data is a subset of the sentiment dataset of  @xcite .",
    "this dataset contains textual reviews and their corresponding 1 - 5 star ratings for amazon products . here",
    ", we only consider the reviews for products in the _ kitchen appliances _",
    "category , which amounts to 5149 reviews .",
    "the preprocessing of the data follows the method described in  @xcite , where this dataset was used for testing the performance of a sparse linear regression model .",
    "each review is represented as a vector of features , where the features correspond to unigrams and bigrams , as given by the data provided by  @xcite . for each distinct feature and for each review",
    ", we created a matrix of occurrences and only kept for our analysis the features that appeared in at least 100 reviews , that is , 824 features .",
    "the second dataset we use is a subset of the yelp ( academic ) dataset .",
    "the dataset contains 2.7 million restaurant reviews with ratings ranging from 1 to 5 stars ( rounded to half - stars ) . here",
    ", we consider the 4086 reviews from the year 2004 .",
    "similarly to the preprocessing done for amazon data , each review is represented as a vector of features ( distinct words ) . after removing non - alphanumeric characters from the words and removing words that appear fewer than 100 times , we have 465 words for our analysis .          first , to compare the convergence speed of different methods , we normalised the mse improvements at each iteration by the amount of total improvement obtained by each of the users , when considering all their individual feedback .",
    "figure [ fig : user_study_percentage ] depicts the convergence speed of methods based on this measure . as can be seen from the figure , for all participants , the proposed method was able to capture most of the participants s knowledge with small budget of feedback queries ( stabilizing at around 200 out of the total 824 features in the considered subset of amazon data ) .",
    "then , in figure [ fig : user_study_suggestions ] , we show the average percentage of relevant words that were asked from the participants at each iteration .",
    "it is evident from the figure that the proposed algorithm started by mostly asking about limited relevant words .",
    "the relevant words were identified by considering all the data in amazon dataset and training an spike and slab model and then choosing words with @xmath66>0.7 $ ] ( words with high posterior inclusion probability ) . based on this threshold , only 39 words from 824 words",
    "were considered as relevant .",
    "the coefficients of all words in the spike - and - slab model , along with the names of relevant words are shown in fig .",
    "[ fig : user_study_gt ] ."
  ],
  "abstract_text": [
    "<S> prediction in a small - sized sample with a large number of covariates , the `` small @xmath0 , large @xmath1 '' problem , is challenging . </S>",
    "<S> this setting is encountered in multiple applications , such as precision medicine , where obtaining additional samples can be extremely costly or even impossible , and extensive research effort has recently been dedicated to finding principled solutions for accurate prediction . however </S>",
    "<S> , a valuable source of additional information , domain experts , has not yet been efficiently exploited . </S>",
    "<S> we formulate knowledge elicitation generally as a probabilistic inference process , where expert knowledge is sequentially queried to improve predictions . in the specific case of sparse linear regression , where we assume the expert has knowledge about the values of the regression coefficients or about the relevance of the features </S>",
    "<S> , we propose an algorithm and computational approximation for fast and efficient interaction , which sequentially identifies the most informative features on which to query expert knowledge . </S>",
    "<S> evaluations of our method in experiments with simulated and real users show improved prediction accuracy already with a small effort from the expert . </S>"
  ]
}