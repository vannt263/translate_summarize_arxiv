{
  "article_text": [
    "processing large - scale real - world graphs has become significantly important for mining valuable information and learning knowledge in many areas , such as data analytics , web search , and recommendation systems .",
    "the most frequently used algorithmic kernels , including path exploration ( e.g. traversal , shortest paths computation ) and topology - based iteration ( e.g. page rank , clustering ) , are driven by graph structures .",
    "parallelization of these algorithms is intrinsically different from traditional scientific computation that appeals to a data - parallel model , and has emerged as so - called _ graph - parallel _ @xcite problems .    [",
    "cols=\"^,^,^,^\",options=\"header \" , ]     [ tab_comp_all ]    in recent years we have witnessed an explosive growth of graph data . for example , the world wide web graph currently has at least 15 billion pages and one trillion urls  @xcite . also , the social network of facebook  @xcite has over 700 million users and 140 billion social links . even to store only the topology of such a graph ,",
    "the volume is beyond terabytes ( tb ) , let alone rich metadata on vertices and edges .",
    "the efficient processing of these graphs , even with linear algorithmic complexity , has scaled out capacity of any single commodity machine .",
    "thus , it is not surprising that distributed computing has been a popular solution to graph - parallel problems  @xcite .",
    "however , since the scale - free nature of real - world graphs , we are facing the following two major challenges to develop high performance graph - parallel algorithms on distributed memory systems @xcite .",
    "* parallelism expressing . * _ graph - parallel algorithms often exhibit random data access , very little work per vertex and a changing degree of parallelism over the course of execution@xcite , making it hard to express parallelism efficiently_. note the facts that graph - parallel computation is data - driven or dictated by the graph topology , and real - world graphs are unstructured and highly irregular ( known as scale - free , e.g. low - diameter , power - law degree distribution ) .",
    "thus , on one hand , from the view of programming , graph - parallel computation ca nt fit well in traditional parallelization methods based on decomposing either computational structure or data structure .",
    "for example , mapreduce  @xcite , the widely - used data - parallel model , ca nt efficiently process graphs due to the lack of support to random data access and iterative execution . on the other hand , from the view of performance ,",
    "the lack of locality makes graph - parallel procedures memory - bound on the shared memory system and network - bound on the distributed system .",
    "meanwhile , current computer architecture is evolving to deeper memory hierarchy and more processing cores , seeking more locality and parallelism in programs . considering both ease of programming and affinity to the system architecture",
    ", it is challenging to express parallelism efficiently for graph - parallel algorithms .",
    "* graph data layout . * _ real - world graphs are hard to partition and represent in a distributed data model . _",
    "the difficulty is primarily from graph s scale - free nature , especially the power - law degree distribution .",
    "the power - law property implies that a small fraction of vertices connect most of edges .",
    "for example , in a typical power - law graph with degree distribution @xmath1 where @xmath2 , 20% of vertices connect more than 80% of edges .",
    "as identified by previous work  @xcite , this skew of edge distribution makes a balanced partitioning with low edge - cut difficult and often impossible for large - scale graphs in practice .",
    "reference  @xcite thoroughly investigated streaming partitioning methods with all popular heuristics on various datasets .",
    "however , their released results show that for scale - free graphs the edge - cut rate is very high , only slightly lower than a random hashing method . as a consequence",
    ", graph distribution leads to high communication overhead and memory consumption .    to address the above challenges ,",
    "several distributed graph - parallel frameworks have been developed .",
    "basically , a framework provides a specific compution abstraction with a functional api to express graph - parallel algorithms , leaving details of parallelization and data transmission to the underlying runtime system .",
    "representative frameworks include pregel@xcite , graphlab@xcite , powergraph@xcite and matrix - based packages  @xcite@xcite .",
    "these efforts target three aspects of graph - parallel applications : _ graph data model , computation model _ and _ communication model_. table  [ tab_comp_all ] summarizes key technical features of the three frameworks ( pregel , graphlab , powergraph ) , as well as gre to be presented in this paper .",
    "pregel  @xcite acts as a milestone of graph - parallel computing .",
    "it firstly introduced the _ vertex - centric _ approach that has been commonly adopted by most of later counterparts  @xcite including ours .",
    "this idea is so - called _ think like a vertex _ philosophy , where all active vertices perform computation independently and interact with their neighbors typically along edges .",
    "pregel organizes high - level computation in bulk synchronous parallel ( bsp ) super - steps and adopts message passing model for inter - vertex communication .",
    "graphlab supports asynchronous execution any more , but uses distributed shared memory such that the vertex can directly operates its edges and neighbors . for both pregel and graphlab",
    ", the vertex computation follows a common pattern where an active vertex 1 ) collects data from its in - edges , 2 ) updates its states , 3 ) puts data on its out - edges and signals the downwind neighbors .",
    "moreover , powergraph , the descent of graphlab , summarized the above vertex procedure as a gather - apply - scatter ( gas ) paradigm , and explicitly decomposes it into three split phases , which exposes potential edge - level parallelism .",
    "however , the gas abstraction inherently handles each edge in a two - sided way that requires two vertices involvement ( scatter and gather respectively ) , which leads to intermediate data storage and extra operations . instead , gre proposes a new scatter - combine computation model , where the previous two - sided edge operations are reduced to one active message@xcite .",
    "besides , compared to pure message passing or distributed shared memory model , active message has better affinity to both local multi - core processors and remote network communication .",
    "with respect to distributed graph data model , most of previous frameworks including pregel and graphlab use a simple hash - mapping strategy where each vertex and its edges are evenly assigned to a random partition . while being fast to load and distribute graph data , this method leads to a significantly high edge - cut rate .",
    "recently , powergraph introduces _ vertex - cut _ in which vertex rather than edge spans multiple partitions . _",
    "vertex - cut _ can partition and represent scale - free graphs with significantly less resulting communication .",
    "however , it requires to maintain strict data consistency between master vertex and its mirrors . instead , gre proposes a new agent - graph model , where data on agent is temporal so that the consistency issue is avoided .    in a summary",
    ", gre inherits the _ vertex - centric _ programming model , and specifically makes the following major contributions :    scatter - combine , a new graph - parallel computation model .",
    "it is based on active message and fine - grained edge - level parallelism .",
    "sec : comp_model ] )    agent - graph , a novel distributed directed graph model .",
    "it extends the original graph with _",
    "agent _ vertices . specifically , it has no more and typically much less communication than powergraph s _ vertex - cut_. ( sec .",
    "[ sec : graph_model ] )    implementation of an efficient runtime system for gre abstractions .",
    "it incorporates several key components of data storage , one - sided communication and fine - grained data synchronization .",
    "( sec .  [ sec : runtime ] )    a comprehensive evaluation of three benchmark programs ( pagerank , sssp , cc ) and graph partitioning on real - world graphs , demonstrating gre s excellent performance and scalability . compared to powergraph , gre s performance is 2.5@xmath017.0 times better on 8@xmath016 machines .",
    "specifically , gre s pagerank takes @xmath3 seconds per iteration on 192 cores ( 16 machines ) while powergraph reported @xmath4 seconds on 512 cores ( 64 machines)@xcite .",
    "gre can process a large graph of one billion vertices on our machine with 768 gb memory while powergraph can not make it .",
    "[ sec : evaluation ] )",
    "in this section , we formalize the procedure of vertex - centric graph computation and then present motivation of this work .      graph topology can be represented as @xmath5 , where @xmath6 is the set of vertices and @xmath7 the set of edges .",
    "associating @xmath8 with metadata on vertices and edges , we have a property graph @xmath9 , where @xmath10 and @xmath11 are properties of vertices and edges respectively .",
    "property graph is powerful enough to support all known graph algorithms . in this paper ,",
    "all edges are considered as directed ( one undirected edge can be transformed into two directed edges ) . for simplicity , we define the following operations :    @xmath12 : return the set of a vertex s out - edges ;    @xmath13 : return the set of a vertex s in - edges ;    @xmath14 : return the source vertex of an edge ;    @xmath15 : return the target vertex of an edge ;    @xmath16 : filter a set of vertices or edges with rule @xmath17 , and return a subset .    as an abstraction , the computation @xmath18 on some vertex @xmath19 can be described as : @xmath20 for example , an instance of pagerank  @xcite computation on @xmath19 can be described as : @xmath21 in the vertex - centric approach , calculation of equation .",
    "[ e1 ] is encoded as so - called _ vertex - program_. each vertex executes its _ vertex - program _ and can communicate with its neighbors . depending on pre - defined synchronization policies ,",
    "active vertices are scheduled to run in parallel .",
    "a common pattern followed by _ vertex - program _ is gather - apply - scatter ( gas)@xcite ( or signal / collect@xcite ) .",
    "it translates equation .",
    "[ e1 ] to the following three conceptual steps : , as gas requires that the vertex computation can be factored into a generalized sum of products . ]    * g*ather .",
    "vertex @xmath19 collects information from in - edges and upwind neighbor vertices by a generalized sum @xmath22 , resulting in @xmath23 : @xmath24    * a*pply .",
    "vertex @xmath19 recomputes and updates its state : @xmath25    * s*catter .",
    "vertex @xmath19 uses its newest state to update the out - edges states and signals its downwind neighbors : @xmath26    in steps of gather and scatter , the vertex @xmath19 communicates with its upwind and downwind neighbors respectively . conventionally , there are two methods to do the communication , that message passing ( pregel  @xcite and other similar systems like spark  @xcite , trinity  @xcite ) and distributed shared memory ( graphlab  @xcite and its descendant powergraph  @xcite ) . in distributed shared memory , remote vertices are replicated in local machine as _ ghost_s or _",
    "mirror_s , and data consistency among multiple replications is maintained implicitly .",
    "although gas model provides a clear abstraction to reason about parallel program semantics , it may sacrifice storage and performance . note that gas conceptually split the information transferring on an edge into two phases",
    "executed by two vertices , i.e. the source vertex changes the edge state in its scatter phase and then the target vertex reads the changed state in its gather phase .",
    "the above asynchrony of operations on the edge requires storage of intermediate edge states and leads to extra operations that hurt performance . in pregel",
    "this happens across two super - steps and leads to the large storage of intermediate messages , while in graphlab it requires not only out - edge storage but also redundant in - edge storage and polling on all in - edges .",
    "however , we identify that in a message model the separation of gather and scatter is not necessary , as long as the operator @xmath22 in gather is commutative . to illustrate this point , again we take pagerank as an example and rewrite its vertex computation in equation .  [ pr-1 ] as follow : @xmath27 equation .",
    "[ pr-2]a is the message sent to @xmath19 by vertex @xmath28 . in equation .",
    "[ pr-2]b , the @xmath29 operation is commutative , which means the order of computing @xmath30 does no matter .",
    "once a message comes , it can be immediately computed . in practice ,",
    "given the fact that _ graph - parallel computation is essentially driven by data flow on edges _ , the @xmath22 is naturally commutative .        -6pt [ gre - arch ]    from the above analysis , we can induce a dataflow execution model that leverages active message approach  @xcite .",
    "an active message is a message containing both data and encoded operations .",
    "now the gather phase then can be broken into a series of discrete asynchronous active messages .",
    "an active message can be scheduled to run once it reaches the destination vertex .",
    "specifically , for a message on the edge @xmath31 , when @xmath32 and @xmath19 are owned by the same machine , the message can be computed in - place during @xmath32 s scatter phase .",
    "note that multiple active messages may operate on the same destination vertex simultaneously , leading to data race .",
    "as detailed in later sections , we shall handle this issue with a vertex - grained lock mechanism .    basically , the dataflow model has two significant advantages .",
    "first , it transforms the two - sided communication to one - sided , bypassing the intermediate message storage and signaling .",
    "this optimization dramatically reduces the overhead in both shared memory and distributed environment .",
    "second , it enables fine - grained edge - level parallelism that takes advantage of multi - core architecture .",
    "therefore , we propose scatter - combine ( sec.[sec : comp_model ] ) , an alternative graph - parallel abstraction that is more performance - aware . besides",
    ", we notice that in equation .",
    "[ pr-2]b , the @xmath29 is also associative .",
    "in fact , for most graph - parallel problems , not only commutativity but associativity is also satisfied by the generalized sum @xmath22 in equation .",
    "[ gather ] .",
    "this fact has been widely realized as the basis of pregel s message _ combiner _ and powergraph s _ vertex - cut _ mechanism . based on the associativity and commutativity of @xmath22",
    ", we develop agent - graph ( sec.[sec : graph_model ] ) , a novel distributed graph data model that can effectively partition and represent scale - free graphs in the distributed environment .",
    "agent - graph is closely coupled with active message approach .",
    "this evolution motivates the development of * g*raph * r*untime * e*ngine ( gre ) .",
    "in this section , we give an overview of gre system , as shown in fig .",
    "[ gre - arch ] .",
    "gre is implemented in c++ templates .",
    "it consists of graph loader , abstraction layer and underlying runtime layer .",
    "the essentials of gre are the scatter - combine computation model and distributed agent - graph data model .",
    "as noted in the previous sections , gre inherits the _ vertex - centric _ programming model .",
    "the programming interface is a set of simple but powerful functional primitives . to define a graph - parallel algorithm",
    ", users only need to define the vertex computation with these primitives . in fig .",
    "[ gre - arch ] , user - defined pagerank program is presented as an example , which is as simple as its mathematic form in equation .",
    "[ pr-2 ] . the user - defined program , as template parameters , is then integrated into gre framework and linked with runtime layer .",
    "internally , gre adopts distributed agent - graph to represent graph topology , and column - oriented storage  @xcite to store vertex / edge property .",
    "the graph loader component loads and partitions graphs into internal representation .",
    "the runtime layer provides infrastructure supporting gre s abstractions . with thread pool ( or thread groups ) and fine - grained data synchronization , gre can effectively exploit the massive edge - level parallelism expressed in scatter - combine model . besides , gre adopts one - sided communication to support active message and override the network communication overhead with useful computation .",
    "gre models the graph - parallel computation in scatter- combine abstraction . as noted in section  [ sub : motivation ] , it realizes the fact that for a broad set of graph algorithms , vertex - centric computation can be factored into independent edge - level functions , and thus transforms the bulk of vertex computation into a series of active messages .      in scatter - combine , each active vertex is scheduled to compute independently and interacts with others by active messages .",
    "like in pregel and graphlab , the major work of gre programming is to define vertex - computation .",
    "scatter - combine provides four primitives : _ scatter _ , _ combine _ , _ apply _ and _ assert_to_halt_. their abstract semantics are specified in alg .",
    "[ alg : primitives ] , and run in the context of alg .",
    "[ alg : logics ] . to implement a specific algorithm",
    ", users only need to instantiate these primitives .",
    "each vertex alternately carries computation following the logics in alg .",
    "[ alg : logics ] , where the procedure is divided into two phases , scatter - combine and apply .",
    "each vertex implicitly maintains two state variables , one for scatter and the other for apply , to decide whether to participate in the computation of the relevant phase .    during scatter - combine phase , a vertex , if being active to scatter , modifies its outgoing edges s states and scatters data to its downwind neighbors by active messages . as emphasized before",
    ", active message is the essential of scatter - combine computation .",
    "it is edge - grained and defined by primitives _ scatter _ and _ combine_. as shown in alg .",
    "[ alg : primitives ] , the _",
    "scatter _ primitive generates an active message that carries a _ combine _ operation , while the _ combine _ primitive defines how the message operates on its destination vertex . besides , _ combine _ is able to activate the destination vertex for a future _ apply_. note that unlike previous message passing in pregel , active message is one - sided .",
    "that is , when the vertex @xmath32 sends a message to @xmath19 , it directly operates on @xmath19 without @xmath19 s involvement .",
    "conceptually , the _ scatter _",
    "does nt necessarily wait its paired _ combine _ to return .",
    "an active vertex may execute _ scatter _ on all or a subset of its out - edges .",
    "after finishes all desired _ scatter _",
    "operations , the vertex calls user - defined _ assert_to_halt _ to deactivates itself optionally . generally , in traversal - based algorithms _",
    "assert_to_halt _ is defined to deactivate for scatter , and for iterative algorithms such as pagerank it is defined to keep the vertex active .    during apply phase , a vertex , if being active to apply , executes an _ apply _ and then sets its apply - phase state inactive . in",
    "the _ apply _ procedure , the vertex recomputes its state ( _ v.state _ ) with intermediate results ( _ v.sum _ ) accumulated in the previous phase , and optionally activates itself to participate in the scatter - combine phase of next round .",
    "gre adopts bulk synchronous parallel execution .",
    "like pregel , gre divides the whole computation into a sequence of conceptual super - steps . in each super - step",
    ", gre executes the above two phases in order . during each phase , all active vertices run in parallel .",
    "the computation is launched by initializing any subset of vertices as source that are activated for scatter . during the course of whole execution , more vertices",
    "are either activated to compute or deactivated . at the end of a super - step ,",
    "if no vertex is active for further scatter - combine , the whole computation terminates .",
    "note that there are essential differences between gas and scatter - combine .",
    "[ fig : compare - models ] illustrates their execution flow on an vertex @xmath19 in bulk synchronous parallel execution .",
    "we assume @xmath19 s state is computed in super step @xmath33 .",
    "in gas model , at super - step @xmath33-@xmath34 , the upwind neighbors ( @xmath35 , @xmath36 and @xmath37 ) have executed _ scatter _ and put data on @xmath19 s in - edges ( @xmath38 , @xmath39 and @xmath40 ) . at super - step @xmath33 when @xmath19 is active , it _",
    "gather_s data by polling its in - edges and accumulates them in a local variable ( @xmath23 here ) .",
    "we can see that processing an edge needs a pair of _ scatter _ and _ gather _ executed by two vertices respectively , which crosses two super - steps and requires storage of all intermediate data on edges . as a significant progress , in scatter - combine ,",
    "the operation of _ gather _ is encoded in an active message that can automatically execute without the target vertex s involvement , namely _",
    "combine_. in this example , during scatter - combine phase of super - step @xmath33 , @xmath19 s upwind neighbors _ scatter _ active messages that directly execute _ combine _ on @xmath19 , and finally @xmath19 simply updates itself by an _ apply _ during the apply phase .",
    "programming with scatter - combine model is very convenient .",
    "for instance , to implement pagerank , we directly translate the formulas [ pr-2]-a , [ pr-2]-b and [ pr-2]-c in equation .",
    "[ pr-2 ] into primitives _ scatter _ , _",
    "combine _ , and _ apply _ , as shown in fig.[subfig : pr - code ] .",
    "besides , this figure presents implementations of other two algorithms that we use as benchmark in later evaluation .",
    "gre s sssp implementation , given in fig .",
    "[ subfig : sssp - code ] , is a variant of bellman ford label correcting algorithm .",
    "it is a procedure of traversal , starting from a given source vertex , visiting its neighbors and then neighbors s neighbors in a breadth first style , and continuing until no vertices change their states .",
    "when a vertex is visited , if its stored distance to source is larger than that of the new path , its path information is updated .",
    "gre implements connected components as an example of label propagation .",
    "[ subfig : cc - code ] shows the connected components on undirected graphs . for each connected component ,",
    "it is labeled by the smallest i d of its vertices . in the beginning , each vertex is initialized as a component labeled with its own vertex i d .",
    "the procedure then iteratively traverses the graph and combines new found connected components .",
    "its algorithmic procedure is similar to sssp , but initiates all vertices as sources and typically converges in fewer number of iterations .",
    "+   +    [ fig : applications ]    scatter - combine model can express most graph - parallel computation efficiently , including traversal - based path exploration and most iterative algorithms .",
    "in fact , since graph - parallel computation is internally driven by data flow on edges  @xcite , edge - parameterized vertex factorization is widely satisfied .",
    "we found that all examples of pregel in @xcite satisfy vertex factorization , such as bipartite matching  @xcite and semi - clustering  @xcite , thus can be directly implemented in gre . for some graph algorithms",
    "that contain other computation except for graph - parallel procedure , extension to basic scatter - combine model is required .",
    "for example , with simple extension of backward traversal on transposed graphs , gre implements multi - staged algorithms like betweenness centrality  @xcite and strong connected components .    besides , with technologies that were proposed in reference @xcite to complement vertex - centric parallelism in pregel - like systems , gre is able to efficiently implement algorithms including minimum spanning forests , graph coloring and approximate maximum weight matching . however , like pregel , gre only supports bsp execution , and thus ca nt express some asynchronous algorithms in powergraph .",
    "in this section we propose the distributed agent - graph model that extends original directed graphs with @xmath41 vertices .",
    "agent - graph is coupled with the message model , and is able to efficiently partition and represent scale - free graphs .",
    "there is a consensus that the difficulty of partitioning a real - world graph comes from its scale - free property . for big - vertex whose either in - degree or out - degree is high ,",
    "amounts of vertices in remote machines send messages to it or it sends messages to amounts of remote vertices . lack of low - cut graph partition , these messages lead to heavy network communication and degrades performance significantly .",
    "to crack the big - vertex problem , gre introduces a new strategy ",
    "_ agent_. the basic idea is demonstrated in fig .",
    "[ fig : agent ] .",
    "there are two kinds of agents , i.e. _ combiner _ and _ scatter_. in the given examples , there are two machines ( or graph partitions ) where @xmath19 is the big - vertex and owned by machine 2 . in fig .",
    "[ subfig_combiner ] , @xmath19 is a high in - degree vertex , and many vertices in machine 1 send messages to it . by introducing a _ combiner _ @xmath42 , now messages previously sent to @xmath19 are first _ combined _ on @xmath42 and later @xmath42 sends a message to @xmath19 . in this example",
    ", the _ combiner _ reduces network communication cost from three messages to one .",
    "similarly , in fig .",
    "[ subfig_scatter ] , @xmath19 no longer directly sends messages to remote vertices in machine 1 but only one message to a _ scatter _",
    "agent @xmath42 who then delivers messages to vertices in machine 1 .",
    "also , the _ scatter _ agent reduces messages on network from three to one .",
    "based on the idea of _ agent _ , we propose agent - graph , simply denoted as @xmath43 .",
    "@xmath43 treats _ agent_s as special vertices and extends the original graph topology @xmath8 . for simplicity , we call the vertex in original graph as _ master _ vertex . each _ master _ vertex",
    "is uniquely owned by one partition but can have arbitrary _",
    "agent_s in any other partitions .",
    "agent _ has an directed edge connected with its _",
    "master_. one thing to note is that the term of _ agent _ is completely transparent to programmers , and only makes sense to gre s runtime system .",
    "now we give the formal description of @xmath44 .",
    "we assume graph @xmath8 has been divided into @xmath45 parts , say @xmath46 .    in @xmath47",
    ", for any set of directed edges pointing to @xmath48 , if @xmath19 is not owned by @xmath47 , we set an agent @xmath49 for @xmath19 and do the following transformation : let the edges redirect to @xmath49 , and add a directed edge @xmath50 .",
    "then @xmath49 is a combiner of @xmath19 .",
    "a combiner may have arbitrary in - edges but only one out - edge that points to its _ master _",
    "vertex .    in @xmath47",
    ", for any set of directed edges that start from @xmath48 and point to a set of vertices owned by another partition @xmath51 , we set an agent @xmath52 on remote @xmath51 , and do the following transformation : move these edges to @xmath51 , and add a directed edge @xmath53 on @xmath47 .",
    "then @xmath52 is a scatter of @xmath19 .",
    "scatter _ may have arbitrary out - edges but only one in - edge that comes from its _ master _ vertex .",
    "let @xmath54 be the set of _ scatter_s and @xmath55 the set of _",
    "combiner_s , an agent - graph @xmath56 , where @xmath57 and @xmath58 .",
    "note that according to the definitions of _ scatter _ and _ combiner _ , an edge from @xmath52 to @xmath49 is allowed , but there never exist an edge from @xmath49 to @xmath52 .",
    "note that vertex - cut model in powergraph is another way to address distributed placement of scale - free graphs . in vertex - cut",
    ", a vertex can be cut into multiple replicas distributed in different machines , where the remote replica of vertex is called _",
    "mirror_. both _ agent _ and _ mirror _ are based on vertex factorization .",
    "conceptually , agent - graph can be built from vertex - cut partitions by simply splitting one _ mirror _ into one _ scatter _ and one _ combiner_. however , their mechanisms are fundamentally different .",
    "fig.[fig : compare - graph - models ] shows an example to illustrate difference of agent - graph and vertex - cut .",
    "we argue that _ agent _ has obvious advantage over _ mirror _ for expressing message model on directed graphs .",
    "first , _ agent _ has no overhead on maintaining its data consistency with _ master _ while _ mirror _ has to periodically synchronize data with _",
    "master_. this is because _ mirror _ holds an integrated copy of its _ master _",
    "s runtime states , while _ agent _ , as comparison , is a message proxy that can only temporally cache and forward messages in single direction .",
    "thus , for traversal - based algorithms on directed graphs , agent - graph has much less communication than vertex - cut .",
    "second , the communication cost of _ agent _ is lower than _ mirror _ s in most of cases . in vertex - cut , each _ master _ first accumulates all its _ mirror_s data , and then sends the new result to all its _ mirror_s .",
    "thus , the communication is @xmath59 ( r is the total number of all vertices replicas ) . in agent - graph ,",
    "one _ agent _ only involves in one direction message delivery , either receive ( _ scatter _ ) or send ( _ combine _ ) .",
    "thus , it has less communication cost since @xmath60 .",
    "take the example in fig .",
    "[ fig : compare - graph - models ] . in vertex - cut the _ master _",
    "vertex @xmath19 has to collect all changes of its _ mirror_s and then spread the newest value to them , while in agent - graph @xmath19 only receives messages from its agents ( combiners here ) and has no need to update agents .",
    "as shown by the number of dashed lines , agent - graph requires just half communication of that in vertex - cut .      on agent - graph model",
    ", we thoroughly investigated various streaming and 2-pass semi - streaming partitioning methods in  @xcite . with the agent - extension ,",
    "both traditional edge - cut and vertex - cut partitioning methods perform much better for scale - free graphs . in this paper",
    ", we only give the pure vertex - cut approach and a streaming partitioning method adapted from powergraph s greedy vertex - cut heuristics .    in a pure vertex - cut model",
    ", none of edges in original graph @xmath8 is cut .",
    "all cut edges are @xmath43 extended edges , i.e. \\{@xmath61 } or \\{@xmath62 } .",
    "the extended edges represent communication overhead while original edges represent computational load .",
    "we construct agent - graph by loading edges from the original graph .",
    "assuming that the goal is @xmath45 partitions , we formalize the objective of k - way balanced partition objective as follow : @xmath63 where @xmath64 is an imbalance factor , @xmath65 and @xmath66 are sets of _ scatter_s and _ combiner_s of vertex @xmath19 respectively .    the loader reads edge list in a stream way and greedily places an edge @xmath67 to the partition which minimizes number of new added _",
    "agents _ and keeps edge load balance .",
    "the current best @xmath68 of partition is calculated by the following heuristic : @xmath69 , @xmath70 @xmath71 and @xmath72 @xmath73 @xmath74 @xmath75    in default , each machines independently loads a subset of edges , partitions them into @xmath45 parts , and finally sends remote partitions to their owner machines . during the procedure",
    ", machines do nt exchange information of heuristic computing .",
    "this is the same with the _ oblivious _ mode in powergraph .",
    "also , gre supports the _ coordinated _ mode of powergraph , where partitioning information are periodically synchronized among all machines .",
    "gre s abstractions , scatter - combine computation model and distributed agent - graph model , are built on the runtime layer .",
    "the runtime system is designed for contemporary distributed systems in which each single machine has multiple multi - core processors sharing memory and is connected to other machines with high performance network .",
    "gre follows an owner - compute rule , that launches one single process on each machine and assigns a graph partition to it . within each machine , the process has multiple threads : one master thread in charge of inter - process communication and multiple worker threads that are forked and scheduled by the master thread to do actual computation .",
    "now , we describe the runtime design from three aspects , with an emphasis on how it bridges gre abstractions and the underlying platform .      from the view of users , graph - parallel applications run on a directed property graph where each vertex is identified by an unique 64-bit integer .",
    "internally , however , gre stores runtime graph data in a distributed way .",
    "gre manages three types of in - memory data : graph topology , graph property and runtime states .",
    "each machine storers a partition of agent - graph .",
    "the local topology storage is compact and highly optimized for fast retrieve .",
    "it includes three parts : graph structure , vertex - id index and agent - extended edges .",
    "first , the graph structure stores all assigned ordinary edges in the csr ( compressed sparse row ) format  @xcite , where all vertices are renumbered with local 32-bit integer ids .",
    "local vertex ids are assigned by the following rule .",
    "assuming that there are @xmath76 local vertices ( i.e. _ master _ ) , _",
    "master_s are numbered from 0 to @xmath76-@xmath34 in order . both _",
    "scatter_s and _ combiner_s are then continuously numbered from @xmath76 .",
    "second , the vertex - id index provides bidirectional translation between local i d and global i d for all vertices .",
    "third , gre stores agent - extended edges implicitly . for any @xmath77@xmath78@xmath79 type edge",
    ", the @xmath77 induces it by retrieving a data structure recording all machines that hold its _ scatter_s . for any @xmath80@xmath78@xmath77 type edge ,",
    "the _ combiner _ induces it by local - to - global vertex - id index .",
    "graph property ( i.e. meta data associated with vertices and edges ) is decoupled with graph topology .",
    "it is separately stored in the column - oriented storage ( cos )  @xcite approach . in cos , each type of graph property is stored as a flat array of data items .",
    "the local vertex ( edge ) i d serves as primary key and can directly index the array .",
    "for example , in a social network , given one person s _ local _ i d , say @xmath81 , to retrieve his / her _ name _ we directly locate the information by _",
    "name_[@xmath81 ] .",
    "cos provides fast data load / store between disk and memory , as well as optimizations like streaming data compression . with cos ,",
    "gre can load or store arbitrary types of graph property in need , and run multiple ad - hoc graph analysis continuously .",
    "vertex runtime states play a crucial role in implementing gre s scatter - combine computation model .",
    "like graph property , runtime states are stored in flat arrays and indexed by local vertex i d . conceptually , there are three types of runtime vertex states :    _ vertex_data _ is the computing results only owned by _ master _",
    "vertex , and updated by _",
    "apply_.    _",
    "scatter_data _ is the data that one vertex wants to _ scatter _ by messages , owned by _ master _ and _",
    "scatter_.    _ combine_data _ is the data on which an active message executes _ combine _ , owned by _",
    "master _ and _ combiner_.     +    tabel .",
    "[ tab : runtime - states ] is the runtime state setting of algorithms in fig .",
    "[ fig : applications ] .",
    "note that for performance optimization , gre allows the _ vertex_data _ vector refer to _ scatter_data _ or _",
    "combine_data_. however , for data consistency , gre requires _ scatter_data _ and _ combine_data _ be physically different .    [ tab : runtime - states ]    data consistency of vertex states is automatically maintained by the specification of scatter - combine primitives .",
    "( 1 ) for an ordinary vertex(_master _ ) , its _",
    "scatter_data _ can only be updated by initialization or _",
    "apply _ , while for a _",
    "scatter - agent _ its _ scatter_data _ can only be updated by the message from its _",
    "master_. during scatter - combine phase , the _ scatter_data _ does nt change , and is valid only when the vertex is active for _ scatter_. ( 2 ) for a vertex , either _ master _ or _ combiner - agent _ , _ combine _ operation may change and can only change its _",
    "combine_data_. if the vertex is a @xmath77 , _ combine _ on it incurs a future _ apply_. if the vertex is a _ combiner - agent _ , in the future it will send an active message to its remote _ master _ and then reset its _ combine_data_. ( 3 ) during apply phase , each active _ master _ executes an _ apply _ in which it updates the _ vertex_data _",
    ", optionally recomputes _",
    "scatter_data _ , and resets the _ combine_data_.      active message hides underlying details and difference of intra - machine shared - memory and inter - machine distributed memory . with one - sided communication and fine - grained data synchronization ,",
    "gre s runtime layer provides efficient support to active messages .      a daemon thread ( master of local process ) keeps monitoring the network and receiving incoming data , meanwhile sending data prepared by _",
    "gre supports asynchronous communication of multiple message formats simultaneously .",
    "the communication unit is a memory block whose format is shown in fig.[fig : comm_protocol ] .",
    "it consists of two parts , header and messages .",
    "the header is a 64-bit structure that implements a protocol to support user - defined communication patterns .",
    "messages with the same format are combined into one buffer and the format registration information is encoded in the header . the fields _",
    "op_(8 bits ) and _ flag_(8 bits ) decide what actions to take when receive the message .",
    "the filed _",
    "count_(32 bits ) is the number of messages in the buffer .",
    "message is vertex - grain , containing destiny vertex and message data . besides , a buffer with zero message is legal , where the header - only buffer is used to negotiate among processes .",
    "each process maintains a buffer pool where the buffer size is predefined globally .",
    "one buffer block can be filled with arbitrary messages not exceeding the capacity . as the block has encoded all information in its header , all interactions either between local _",
    "master _ and _ worker_s or among distributed processes are one - sided and asynchronous .      in gre",
    "s master - workers multi - threading mode , all computation ( _ scatter _ and _ combine _ ) are carried by worker threads in parallel .",
    "note that multiple active messages may do _ combine _ operation on the same vertex simultaneously , which leads to data race .",
    "according to our previous statistics  @xcite , given the sparse and irregular connection nature of real graphs the probability of real conflicts on any vertex is very low . in default , gre uses the proven high performance virtual lock mechanism ( _ vlock _",
    "@xcite ) for vertex - grained synchronization .",
    "gre provides two methods of worker thread organization , i.e. thread pool and thread groups . in thread pool mode ,",
    "the process maintains a traditional thread pool and a set of virtual locks .",
    "all _ combine _ operations are implicitly synchronized by the _ vlock_. thread groups is an alternative method , addressing the issue that for multi - socket machine , frequent atomic operations and data consistency across sockets may lead to high overhead . in thread groups mode ,",
    "worker threads are further divided into groups .",
    "each thread group runs on one socket , computes on one of local vertices disjoint subsets , and communicates with other groups by fastforward channels  @xcite .",
    "besides , each thread group has an independent set of _ vlock _ privately .",
    "fault tolerance in gre is achieved by checkpointing , i.e. saving snapshot of runtime states periodically in a given interval of super steps .",
    "the process is similar to that in pregel but much simpler . during checkpointing",
    ", gre only needs to backup for native vertex runtime states and active vertex bitmap , abandoning all agent data and temporal messages . besides , thanks to the column - oriented - storage , gre can dump and recover runtime data image very fast .",
    "for gre , since it keeps all runtime data in - memory , failures rarely happen and typically incurred by message loss over network which can be caught by a communication component at the end of a super - step .",
    "the experimental platform is a 16-node cluster .",
    "each node has two six - core intel xeon x5670 processors , coupled with 48 gb ddr-1333 ram .",
    "all nodes are connected with mlx4 infiniband network of 40gb / s bandwidth .",
    "the operating system is suse server linux 10.0 .",
    "all applications of both gre and powergraph ( graphlab 2.2 ) are compiled with gcc 4.3.4 and openmpi 1.7.2 .",
    "we choose three representative algorithms ",
    "pagerank , single source shortest path(sssp ) and connected components(cc ) .",
    "we use 9 real - world and a set of synthetic graph datasets .",
    "the real - world datasets we used are summarized in table .",
    "[ datasets ] . to the best of our knowledge",
    ", this set includes all available largest graphs in public . the synthetic graphs are r - mat graphs generated using graph500 benchmark with parameters a=0.57 , b = c=0.19 and d=0.05 .",
    "they have fixed out - degree 16 , and varying numbers of vertices from 64 million to 1 billion .",
    "[ datasets ]    since graph partitioning strategies are closely related to parallel performance , we implemented two graph partition settings on gre when comparing to powergraph . table .",
    "[ tab : partition ] gives the name notation of different partition strategies .",
    "the graph partitioning is evaluated in terms of both _",
    "equivalent edge - cut rate _ and _ cut - factor_. the _ edge - cut rate _ is defined as the rate of communication edges count to total number of edges , while the _ cut - factor _ is the rate of communication edge count over total number of vertices .",
    "[ tab : partition ]    our evaluation focuses on gre s performance and scalability on different machine scales and problem sizes .",
    "the results of three benchmark programs and graph partitioning are summarized as following .",
    "gre achieves good performance on all three benchmark programs , 2.5@xmath07.6 ( 6.6@xmath017.0 ) times better than powergraph when running on 8 ( 16 ) machines ( fig .",
    "[ fig : runtime ] ) . specifically , compared to other systems , gre achieves the best performance for pagerank on twitter graph ( table .",
    "[ tab : pr - relative - perf ] ) .",
    "gre can efficiently scale to either hundreds of cpu cores ( fig .",
    "[ fig : runtime ] ) or billions of vertices ( fig .",
    "[ fig : scalability - problem - sizes ] ) .",
    "powergraph , however , can scale to neither 16 machines due to communication overhead , nor the synthetic graph with 512 million vertices and 8 billion edges due to its high memory cost .",
    "gre shows significant advantage on graph partitioning .",
    "compared to random hash method , gre s agent - graph can partition 9 real - world graphs with 2@xmath011 times improvement on equivalent edge - cut ( fig .",
    "[ subfig : graphs - par - rate ] ) . compared to vertex - cut method in powergraph - s(p ) , when partitioning twitter and sk-2005 into 4@xmath016 parts , with the same greedy heuristics gre - s(p ) shows 12%@xmath035% ( 29%@xmath0 58% ) improvement on equivalent edge - cut ( fig .",
    "[ subfig : twitter - par ] , [ subfig : sk - par ] ) .",
    "we evaluate gre and compare it with other frameworks in terms of _ strong scalability _ and _ weak scalability_. in strong scalability test , the problem size ( graph size ) is constant while the number of machines is increased . in weak scalability test",
    ", the problem size increases with a given number of machines .      for powergraph",
    ", we adopt the reference implementation in the latest package , with minor modification to ensure its vertex computation same with that in gre . in powergraph",
    ", the pagerank is implemented in a traditional graphlab way , while sssp and cc are implemented by emulating pregel s combiner of messages .",
    "we use two types of real graphs , twitter social network and sk-2005 web graph , as input graphs .",
    "results of one iteration runtime are shown in fig .",
    "[ subfig : twitter - pr - runtime ] and fig .",
    "[ subfig : sk - pr - runtime ] respectively .",
    "first , for both graphs , gre overwhelmingly outperforms powergraph with 1.6@xmath09.5 times better performance on 2@xmath016 machines .",
    "second , gre shows nearly linear scalability over increasing machines . in fig .",
    "[ subfig : twitter - pr - runtime ] , gre - s and gre - p on 16 machines show the speedup of 5.82 and 5.10 over 2 machines respectively .",
    "similarly in fig .",
    "[ subfig : sk - pr - runtime ] , gre - s and gre - p on 16 machines show a speedup of 6.68 and 6.15 over 2 machines .    we evaluate it on the twitter graph whose edge weights are generated by randomly sampling integers from @xmath83 $ ] .",
    "the program records both distance and predecessor in shortest path for each vertex .",
    "results are shown in fig.[subfig : twitter - sssp - runtime ] .",
    "first , gre shows very good performance , e.g. on 16 machines gre - s and gre - p take only 15.46 and 18.08 seconds respectively .",
    "also , like in pagerank , gre outperforms powergraph with significant advantage ",
    "3.7@xmath017.0 times better performance on 2@xmath016 machines .",
    "we scrutinize powergraph s implementation and find that for traversal - based algorithms ( e.g. sssp and cc ) on the directed graph , its graph data model requires much more communication(during the apply phase ) than that of gre , which leads to the performance loss .",
    "second , gre shows medium speedup .",
    "specifically , gre - s and gre - p on 16 machines show the speedup of 4.85 and 3.89 over 2 machines respectively , lower than that in pagerank .",
    "we run cc on the synthetic undirected graph ( namely , graph500 - 27 ) with 128 m vertices and 2b edges , generated by graph500 benchmark . the results , as shown in fig .",
    "[ subfig:27 - 08-cc - runtime ] , are similar to that of pagerank and sssp .",
    "specifically , gre - p achieves a speedup of 6.51 on 16 machines over 2 machines , comparable to pagerank and better than sssp .    gre s performance benefits from its efficient computation and communication model , that exploits fine - grained parallelism to overlap computation with communication and adopts active message to reduce communication overhead .",
    "compared to powergraph s rpc - based multiple - handshake methods , gre one - sided communication is faster .",
    "as we known , parallel scalability is limited by network communication , or in other words , high ratio of computation to communication implies high scalability .",
    "we investigate the local computing time of three benchmark programs in gre .",
    "as expected , results in fig .",
    "[ subfig : twitter - pr - comp]@xmath0fig .",
    "[ subfig:27 - 08-cc - comp ] show that the ratio of computation time is as high as @xmath84 for the three benchmark programs .",
    "that means they can continue to scale well with larger number of machines .    especially , pagerank on twitter graph is widely used for performance comparison between different systems in public literatures .",
    "we collect these data in table  [ tab : pr - relative - perf ] , including gre s . compared to other distributed frameworks",
    ", gre shows competitive performance .",
    "[ tab : pr - relative - perf ]      we further investigate gre s scalability over problem sizes on given number of machines . without loss of generality , we consider gre - p . here",
    "we use synthetic graphs generated by graph500 benchmark  @xcite .",
    "the graph scales from 64 m to 1b vertices with the fixed out - degree 16 ( may reach 17 in practice ) .",
    "the edge weights are integers sampled from @xmath83 $ ] . due to the limit of memory capacity",
    ", the sssp program only records the distance of each vertex to source .",
    "run - time of three benchmark programs are shown in fig .",
    "[ subfig : graphs - pr]@xmath0[subfig : graphs - cc ] .",
    "we can see that for all three programs , gre shows excellent scalability on problem sizes , with close to or lower than linear increasing runtime .",
    "specifically , for the largest graph with 1 billion vertices and 17 billion edges , gre can compute one pagerank iteration in 40s , sssp in 255s , and cc in 139s .",
    "an important phenomena we observed is that powergraph can not scale to such a large graph size on the 16 machines because the memory consumption exceeds the physical memory capacity ( 768 gb ) .",
    "compared to gre , powergraph requires at least 2 times more memory space as it needs to store redundant in - edges and lots of intermediate data .",
    "+      the performance advantage of gre is also reflected by the quality of graph partitioning .",
    "we first investigate agent - graph partitioning on a broad set of real graphs summarized in table.[datasets ] .",
    "[ subfig : graphs - par ] shows the average count of agents per vertex , which is translated into _ equivalent edge - cut rates _ in fig.[subfig : graphs - par - rate ] by dividing average vertex degree . as shown in fig.[subfig : graphs - par - rate ] , compared to the traditional random vertex sharding by hashing ( red dashed line ) , agent - graph in both gre - p and gre - s fundamentally reduces cutting edges by 50%@xmath091% .",
    "now , we evaluate scalability of agent - graph in terms of the number of partitions , with a comparison to powergraph s vertex - cut . the partitioning quality metrics _ cut - factor _ is computed according to communication measure : for agent -graph the _ cut - factor _ is number of agents ( both scatters and combiners ) per vertex , while for vertex - cut it is @xmath85/@xmath86 .",
    "higher _ cut - factor _ implies more communication .    without loss of generality , we choose twitter social network and sk-2005 web graph for detailed analysis . in real world , social network and web graph are two representative types of graphs . generally , social networks have comparatively balanced out- and in - degree distribution , while web graphs are typically fan - in . results of 2@xmath016 partitions are given in fig .",
    "[ fig : twitter - par ] and fig .",
    "[ fig : sk - par ] . for both twitter and sk-2005 , gre - s performs best , followed by powergraph - s ,",
    "gre - p and powergraph - p in order .",
    "except for powergraph - p , all partitioning methods show good scalability over increasing machines ( partitions ) .    to investigate why gre - s / p have better partitioning results than counterparts of powergraph - s / p , we analyze the percentage distribution of two agent types , i.e. scatter and combiner . as shown in fig .",
    "[ subfig : twitter - par - skew ] and fig .",
    "[ subfig : sk - par - skew ] , for both gre - s and gre - p , rates of scatters to combiners have an obvious skew .",
    "as explained in section [ sub : agent - graph - model ] , powergraph s data model fails to realize this phenomena , while agent - graph model can differentiate .",
    "thus , with respect to communication measure , gre has an advantage over powergraph .",
    "gre adopts the well - known _ vertex - centric _ programming model  @xcite .",
    "essentially , it is reminiscent of the classic actor model  @xcite .",
    "previously , the most representative vertex - centric abstractions are pregel  @xcite and graphlab  @xcite , whose comparison with gre was summarized in table .",
    "[ tab_comp_all ] . here",
    "we describe how gre evolves .",
    "pregel is the first bulk synchronous distributed message passing system .",
    "it has been widely cloned in giraph@xcite , gps@xcite , goldenorb@xcite , trinity@xcite and mizan@xcite . besides , frameworks extending hadoop ( mapreduce ) with in - memory iterative execution , such like spark@xcite , twister@xcite and haloop@xcite , also adopt a pregel way to do graph analysis .",
    "meanwhile , graphlab uses distributed shared memory model and supports both synchronous and asynchronous vertex computation .",
    "vertex computation in both pregel and graphlab internally follows the common gas ( gather - apply - scatter)@xcite pattern . besides , powergraph@xcite adopts a phased gas , and can emulate both graphlab and pregel .",
    "however , gas model handles each edge in a two - sided way that requires two vertices involvement , leading to amounts of intermediate data storage and extra operations . to address this problem , message combiner in pregel and delta - caching in powergraph",
    "are proposed as complement to basic gas . instead of gas , gre proposes a new scatter - combine model , which explicitly transforms gas s two - sided edge operation into one - sided active message . in the worst case",
    ", active message can degrade to message passing of pregel .",
    "gre s agent - graph model is derived from optimizing message transmission on directed graphs .",
    "previously , pregel has introduced _ combiner _ for combining messages to the same destination vertex .",
    "gre further introduces _ scatter _ to reduce messages from the same source vertex .",
    "motivated by _ ghost _ in pbgl@xcite and graphlab , we finally develop ideas of message _ agent _ into a distributed directed graph model .",
    "note that gps@xcite , an optimized implementation of pregel , supports large adjacency - list partitioning where the _ subvertex _ is similar to scatter on reducing messages but not well - defined for vertex or edge computation . the closest match to agent - graph is powergraph s vertex - cut which however is used only in undirected graphs and coupled with different computation and data consistency models .    besides the vertex - centric model , generalized spmv ( sparse matrix - vector ) computation is another popular graph - parallel abstraction , which is used by pegasus @xcite and knowledge discovery toolbox @xcite .",
    "note that since spmv computation is naturally bulk synchronous and factorized over edges , their applications can be described in gre . however , unlike gre , the matrix approach is not suitable for handling abundant vertex / edge metadata .    for shared memory environment , there are also numerous graph - parallel frameworks .",
    "ligra  @xcite proposes an abstraction of edgemap and vertexmap which is simple but efficient to describe traversal - based algorithms .",
    "graphchi  @xcite uses a sliding window to process large graphs from disks in just a pc .",
    "x - stream @xcite proposes a novel edge - centric scatter - gather programming abstraction for both in - memory and out - of - core graph topology , which essentially , like gre and powergraph , leverages vertex factorization over edges .",
    "gre s computation on local machine is highly optimized for massive edge - grained parallelism , based on technologies such as vlock  @xcite fine - grained data synchronization and fastforward  @xcite thread - level communication .",
    "emerging _ graph - parallel _ applications have drawn great interest for its importance and difficulty .",
    "we identify that the performance - related difficulty lies on two aspects , i.e. irregular parallelism expressing and graph partitioning .",
    "we propose gre to address these two problems from both computation model and distributed graph model .",
    "first , the scatter - combine model retains the classic vertex - centric programing model , and realizes the irregular parallelism by factorizing vertex computation into a series of active messages in parallel .",
    "second , along the idea of vertex factorization , we develop distributed agent - graph model that can be constructed by a vertex - cut way like in powergraph .",
    "compared to traditional edge - cut partitioning methods or even powergraph s vertex - cut approach , agent - graph significantly reduces communication .",
    "finally , we develop an efficient runtime system implementation for gre s abstractions and experimentally evaluate it with three applications on both real - world and synthetic graphs .",
    "experiments on our 16-node cluster system demonstrate gre s advantage on performance and scalability over other counterpart systems .",
    "m. stonebraker , d.j .",
    "abadi , a. batkin , x. chen , m. cherniack , m. ferreira , e. lau , a. lin , s. madden , e. oneil , p. oneil , a. rasin , n. tran , and s. zdonik .",
    "c - store : a column - oriented dbms . in vldb , 2005"
  ],
  "abstract_text": [
    "<S> large - scale distributed _ graph - parallel _ computing is challenging . on one hand , due to the irregular computation pattern and lack of locality , it is hard to express parallelism efficiently . on the other hand , due to the scale - free nature , real - world graphs are hard to partition in balance with low cut . to address these challenges , </S>",
    "<S> several graph - parallel frameworks including pregel and graphlab ( powergraph ) have been developed recently . in this paper , we present an alternative framework , graph runtime engine ( gre ) . while retaining the vertex - centric programming model , gre proposes two new abstractions : 1 ) a scatter - combine computation model based on active message to exploit massive fined - grained edge - level parallelism , and 2 ) a agent - graph data model based on vertex factorization to partition and represent directed graphs . </S>",
    "<S> gre is implemented on commercial off - the - shelf multi - core cluster . </S>",
    "<S> we experimentally evaluate gre with three benchmark programs ( pagerank , single source shortest path and connected components ) on real - world and synthetic graphs of millions@xmath0billion of vertices . compared to powergraph </S>",
    "<S> , gre shows 2.5@xmath017 times better performance on 8@xmath016 machines ( 192 cores ) . specifically , the pagerank in gre is the fastest when comparing to counterparts of other frameworks ( powergraph , spark , twister ) reported in public literatures . </S>",
    "<S> besides , gre significantly optimizes memory usage so that it can process a large graph of 1 billion vertices and 17 billion edges on our cluster with totally 768 gb memory , while powergraph can only process less than half of this graph scale . </S>"
  ]
}