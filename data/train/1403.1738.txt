{
  "article_text": [
    "the problem of finding sparse solutions to large underdetermined linear systems of equations has received a lot of attention in the last decades .",
    "this is due to the fact that several real - world applications can be formulated as linear inverse problems .",
    "a standard approach is the so called @xmath1-@xmath0 unconstrained optimization problem : @xmath2 where @xmath3 , @xmath4 , @xmath5 @xmath6 and @xmath7 .",
    "we denote by @xmath8 the standard @xmath1 norm and by @xmath9 the @xmath0 norm defined as @xmath10 .",
    "several classes of algorithms have been proposed for the solution of problem  ( [ l2l1 ] ) . among the others",
    ", we would like to remind iterative shrinkage / thresholding ( ist ) methods ( see e.g. @xcite ) , augmented lagrangian approaches ( see e.g. @xcite ) , second order methods ( see e.g. @xcite ) , sequential deterministic ( see e.g. @xcite ) and stochastic ( see e.g. @xcite and references therein ) block coordinate approaches , parallel deterministic ( see e.g. @xcite and references therein ) and stochastic ( see e.g. @xcite and references therein ) block coordinate approaches , and active - set strategies ( see e.g. @xcite ) .",
    "the main feature of this class of problems is the fact that the optimal solution is usually very sparse ( i.e. it has many zero components ) .",
    "then , quickly building and/or correctly identifying the active set ( i.e. the subset of zero components in an optimal solution ) for problem  ( [ l2l1 ] ) is becoming a crucial task in the context of big data optimization , since it can guarantee relevant savings in terms of cpu time . as a very straightforward example",
    ", we can consider a huge scale problem having a solution with just a few nonzero components . in this case",
    ", both the fast construction and the correct identification of the active set can considerably reduce the complexity of the problem , thus also giving us the chance to use more sophisticated optimization methods than the ones usually adopted.various attempts have been made in order to use active set technique in the context of @xmath0-regularized problems .    in @xcite ,",
    "wen et al . proposed a two - stage algorithm , fpc - as , where an estimate of the active variables set is driven by using a first - order iterative shrinkage method . in @xcite , a block - coordinate relaxation approach",
    "with proximal linearized subproblems yields convergence to critical points , while identification of the optimal manifold ( under a nondegeneracy condition ) allows acceleration techniques to be applied on a reduced space .    in @xcite , the authors solve an @xmath0-regularized log determinant program related to the problem of sparse inverse covariance matrix estimation combining a second - order approach with a technique to correctly identifying the active set .    an efficient version of the two - block nonlinear constrained gauss - seidel algorithm that at each iteration fixes some variables to zero according to a simple active set rule has been proposed in @xcite for solving @xmath0-regularized least squares .",
    "in a recent paper @xcite , nocedal et al . described an interesting family of second order methods for @xmath0-regularized convex problems .",
    "those methods combine a semi - smooth newton approach with a mechanism to identify the active manifold in the given problem .    in the case one wants to solve very large problems , block coordinate descent algorithms ( both sequential and parallel ) represent a very good alternative and , sometimes , the best possible answer @xcite .",
    "an interesting coordinate descent algorithm combining a newton steps with a line search technique was described by yuan et al . in @xcite . in this context",
    ", the authors also proposed a shrinking technique ( i.e. a heuristic strategy that tries to fix to zero a subset of variables according to a certain rule ) , which can be seen as a way to identify the active variables . in @xcite , some ideas on how to speed up their block coordinate descent algorithm by including an active set identification strategy are described , but no theoretical analysis is given for the resulting approach .",
    "what we want to highlight here is that all the approaches listed above , but the one described in @xcite , estimate the final active set by using the current active set and perform subspace minimization on the remaining variables . in @xcite ,",
    "the authors define an estimate that performs multiple changes in the active manifold by also including variables that are nonzero at a given point and satisfy some specific condition .",
    "since this active set mechanism , due to the aggressive changes in the index set , can cause cycling , including the estimate into a globally converging algorithmic framework is not always straightforward .",
    "in this work , we adapt the active set estimate proposed in @xcite for constrained optimization problems to the @xmath0-regularized least squares case .",
    "our estimate , similarly to the one proposed in @xcite , does not only focus on the zero variables of a given point .",
    "instead it tries to quickly identify as many active variables as possible ( including the nonzero variables of the point ) , while guaranteeing that some approximate optimality conditions are satisfied .",
    "the main feature of the proposed active set strategy is that a significant reduction of the objective function is obtained when setting to zero all those variables estimated active .",
    "this global property , which is strongly related to the fact that the components estimated active satisfy an approximate optimality condition , makes easy to use the estimate into a given globally converging algorithmic framework .",
    "furthermore , inspired by the papers @xcite , we describe a new block coordinate descent algorithm that embeds the considered active set estimate . at each iteration",
    ", the method first sets to zero the active variables , then uses a decomposition strategy for updating a bunch of the non - active ones . on the one hand , decomposing the non - active variables enables to handle huge scale problems that other active set approaches can not solve in reasonable time . on the other hand , since the subproblems analyzed at every iteration explicitly take into account the @xmath0-norm , the proposed algorithmic framework does not require a sign identification strategy ( for the non - active variables ) , which is tipically needed when using other active set methods from the literature .",
    "the paper is organized as follows . in section",
    "[ estimate ] , we introduce our active set strategy . in section",
    "[ algorithm ] , we describe the active set coordinate descent algorithm , and prove its convergence .",
    "we further analyze the convergence rate of the algorithm . in section  [ numres ] , we report some numerical results showing the effectiveness of the approach .",
    "finally , we draw some conclusions in section  [ conclusions ] .",
    "throughout the paper we denote by @xmath11 , @xmath12 , @xmath13 and @xmath14 the original function in problem  , the quadratic term of the objective function in problem  , the @xmath15 gradient vector and the @xmath16 hessian matrix of @xmath17 respectively .",
    "explicitly @xmath18 given a matrix @xmath19 , we further denote by @xmath20 and @xmath21 the maximum and the minimum eigenvalue of the matrix @xmath22 , respectively .",
    "furthermore , with @xmath23 we indicate the set of indices @xmath24 , and with @xmath25 we indicate the submatrix of @xmath22 whose rows and columns indices are in @xmath26 .",
    "we also report the optimality conditions for problem  :    @xmath27 is an optimal solution of problem   if and only if @xmath28    furthermore , we define a continuous function @xmath29 that measures the violation of the optimality conditions in @xmath30 ( and is connected to the gauss - southwell - r rule proposed in @xcite ) , that is @xmath31 where mid@xmath32 indicates the median of @xmath33 .    finally , we recall the concept of strict complementarity .",
    "[ stc ] strict complementarity holds if , for any @xmath34 , we have @xmath35",
    "all the algorithms that adopt active set strategies need to estimate a particular subset of components of the optimal solution @xmath36 . in nonlinear constrained minimization problems , for example , using an active set strategy usually means correctly identifying the set of active constraints at the solution . in our context , we deal with problem  ( [ l2l1 ] ) and the active set is considered as the subset of zero - components of @xmath36 .",
    "[ def : activeset ] let @xmath27 be an optimal solution for problem  ( [ l2l1 ] ) .",
    "we define the active set as follows : @xmath37 we further define as non - active set the complementary set of @xmath38 : @xmath39    in order to get an estimate of the active set we rewrite problem  ( [ l2l1 ] ) as a box constrained programming problem and we use similar ideas to those proposed in @xcite .",
    "problem  ( [ l2l1 ] ) can be equivalently rewritten as follows : @xmath40 where @xmath41 .",
    "indeed , we can transform a solution @xmath27 of problem   into a solution @xmath42 of by using the following transformation : @xmath43 equivalently , we can transform a solution @xmath42 of into a solution @xmath27 of problem   by using the following transformation : @xmath44 the lagrangian function associated to is @xmath45 with @xmath46 vectors of lagrangian multipliers .",
    "let @xmath47 be an optimal solution of problem  ( [ prob1 ] ) .",
    "then , from necessary optimality conditions , we have @xmath48    from , we can introduce the following two multiplier functions @xmath49 by means of the multiplier functions , we can recall the non - active set estimate @xmath50 and active set estimate @xmath51 proposed in the field of constrained smooth optimization ( see @xcite and references therein ) : @xmath52 @xmath53 where @xmath54 is a positive scalar .",
    "we draw inspiration from and to propose the new estimates of active and non - active set for problem",
    ". indeed , by using the relations @xmath55 we can give the following definitions .",
    "[ def : est ] let @xmath5 .",
    "we define the following sets as estimate of the non - active and active variables sets : @xmath56 @xmath57    in the next subsections , we first discuss local and global properties of our estimate , then we compare it with other active set estimates .",
    "now , we describe some local properties ( in the sense that those properties only hold into a neighborhood of a given point ) of our active set estimate . in particular , the following theoretical result states that when the point is sufficiently close to an optimal solution the related active set estimate is a subset of the active set calculated in the optimal point ( and it includes the optimal active variables that satisfy strict complementarity ) .",
    "furthermore , when strict complementarity holds the active set estimate is actually equal to the optimal active set .",
    "[ activeest ] let @xmath27 be an optimal solution of problem  .",
    "then , there exists a neighborhood of @xmath36 such that , for each @xmath58 in this neighborhood , we have @xmath59 with @xmath60 .",
    "+ furthermore , if strict complementarity holds in @xmath36 , then there exists a neighborhood of @xmath36 such that , for each @xmath58 in this neighborhood , we have @xmath61    the proof follows from theorem 2.1 in @xcite .      here ,",
    "we analyze a global property of the active set estimate .",
    "in particular , we show that , for a suitably chosen value of the parameter @xmath62 appearing in definition  [ def : est ] , by starting from a point @xmath63 and fixing to zero all variables whose indices belong to the active set estimate @xmath64 , it is possible to obtain a significant decrease of the objective function .",
    "this property , which strongly depends on the specific structure of the problem under analysis , represents a new interesting theoretical result , since it enables to easily embed the active set estimate into any globally converging algorithmic framework ( in the next section , we will show how to include it into a specific block coordinate descent method ) .",
    "furthermore , the global property can not be deduced from the theoretical results already reported in @xcite .",
    "+    [ ass1 ] parameter @xmath62 appearing in definition  [ def : est ] satisfies the following condition : @xmath65    [ prop1 ] let assumption [ ass1 ] hold .",
    "given a point @xmath63 and the related sets @xmath64 and @xmath66 , let @xmath67 be the point defined as @xmath68 then , @xmath69    see appendix [ appmainresact ] .      our active set estimate is somehow related to those proposed respectively by byrd et al . in @xcite and by yuan",
    "et al . in @xcite .",
    "it is also connected in some way to the ist algorithm ( ista ) , see e.g.  @xcite .",
    "indeed , an ista step can be seen as a simple way to set to zero the variables in the context of @xmath0-regularized least - squares problems .    here",
    ", we would like to point out the similarities and the differences between those strategies and the one we propose in the present paper .",
    "first of all , we notice that , at a generic iteration @xmath70 of a given algorithm , if @xmath71 is the related iterate and @xmath72 is an index estimated active by our estimate , that is , @xmath73 this is equivalent to write @xmath74 \\quad\\mbox{and } \\quad -\\tau\\leq g_i(x^k)\\leq\\tau,\\ ] ] which means that @xmath75 is sufficiently small and satisfies the optimality condition associated with a zero component ( see ) .",
    "as we will see , the estimate , due to the way it is defined , tends to be more conservative than other active set strategies ( i.e. it might set to zero slightly smaller sets of variables ) . on the other hand , the global property analyzed in the previous section",
    "( i.e. decrease of the objective function when setting to zero the active variables ) seems to indicate that the estimate truly contains indices related to variables that will be active in the optimal solution . as we will see later on , this important property",
    "does not hold when considering the other active set strategies analyzed here .    in the block active set algorithm for quadratic @xmath0-regularized problems proposed in @xcite , the active set estimate , at a generic iteration @xmath70 , can be rewritten in the following way : @xmath76 let @xmath77 and @xmath78 be an index estimated active by our estimate , from , we get @xmath79 $ ]",
    ".    then , in the case @xmath80 , @xmath81 implies @xmath82 .",
    "in fact , let @xmath81 . if @xmath80 we have @xmath83 so that @xmath82 .",
    "it is easy to see that the other way around is not true .",
    "other differences between the two estimates come out when considering indices @xmath84 such that @xmath85 .",
    "let @xmath81 and , in particular , @xmath86 . if @xmath87 , then we get @xmath88 so that @xmath89 . using the same reasoning we can see that , in the case @xmath81 and , in particular , @xmath90 , it can happen @xmath91 so that @xmath89 .",
    "+ in @xcite , the active set estimate is defined as follows @xmath92 where @xmath93 is a positive scalar that measures the violation of the optimality conditions .",
    "it is easy to see that our active set contains the one proposed in @xcite .",
    "furthermore , we have that variables contained in our estimate are not necessarily contained in the estimate .",
    "in particular , a big difference between our estimate and the one proposed in @xcite is that we can also include variables that are non - zero at the current iterate .    as a final comparison",
    ", we would like to point out the differences between the ista strategy and our estimate .",
    "consider the generic iteration of ista with the same @xmath54 used in our active set strategy : @xmath94 from the optimality conditions of the inner problem in , we have that the zero variables at @xmath95 belong to the following set : @xmath96 we can easily see that @xmath97 .",
    "the opposite is not always true , apart from the variables @xmath80 . as a matter of fact ,",
    "let us consider @xmath98 and @xmath99 .",
    "then , we have that @xmath100 in order to have @xmath82 it should be @xmath101 that is a tighter requirement with respect to the one within @xmath102 .",
    "a similar reasoning applies also to variables @xmath103 with @xmath99 .",
    "we would also like to notice that the ista step might generate unnecessary projections of variables to zero , thus being not always effective as a tool for identifying the active set .",
    "in this final remark , we show that , when using the active set strategies analyzed above , a sufficient decrease of the objective function can not be guaranteed by setting to zero the variables in the active set ( i.e. proposition  [ prop1 ] does not hold ) .",
    "this fact makes hard , in some cases , to include those active set strategies into a globally convergent algorithmic framework .",
    "[ remark : asproperty ] proposition  [ prop1 ] does not hold for the active set strategies described above .",
    "this can be easily seen in the following case .",
    "let us assume that , at some iteration @xmath70 , it exists only one index @xmath104 , with @xmath105 , @xmath106 and @xmath107 .",
    "let @xmath108 and @xmath67 be the point defined as @xmath109 for all @xmath110 , and @xmath111 .",
    "then , @xmath112 since @xmath113 and @xmath114 , we have @xmath115 , so that by setting to zero the active variable we get an increase of the objective function value .",
    "the same reasoning applies also to the ista step , assuming that at some iteration @xmath70 , there exists only one index @xmath116 such that @xmath117 and @xmath107 .",
    "finally , it is easy to notice that , at each iteration @xmath70 , the active set estimate @xmath118 defined in @xcite only keeps fixed to zero , at iteration @xmath70 , some of the variables that are already zero in @xmath71 , thus not changing the objective function value .",
    "in this section , we describe our fast active set block coordinate descent algorithm ( ` fast - bcda ` ) and analyze its theoretical properties .",
    "the main idea behind the algorithm is that of exploiting as much as possible the good properties of our active set estimate , more specifically :    * the ability to identify , for @xmath70 sufficiently large , the `` strong '' active variables ( namely , those variables satisfying the strict complementarity , see theorem  [ activeest ] ) ; * the ability to obtain , at each iteration , a sufficient decrease of the objective function , by fixing to zero those variables belonging to the active set estimate ( see proposition  [ prop1 ] of the previous section ) .    as we have seen in the previous section , the estimate , due to the way it is defined , tends to be more conservative than other active set strategies ( i.e. it might set to zero a slightly smaller set of variables at each iteration )",
    "anyway , since for each block we exactly solve an @xmath0-regularized subproblem , we can eventually force to zero some other variables in the non - active set .",
    "another important consequence of including the @xmath0-norm in the subproblems is that we do not need any sign identification strategy for the non - active variables .    at each iteration @xmath70 ,",
    "the algorithm defines two sets @xmath119 , @xmath120 and executes two steps :    * it sets to zero all of the active variables ; * it minimizes only over a subset of the non - active variables , i.e. those which violate the optimality conditions the most .",
    "more specifically , we consider the measure related to the violation of the optimality conditions reported in",
    ". we then sort in decreasing order the indices of non - active variables ( i.e. the set of indices @xmath121 ) with respect to this measure and define the subset @xmath122 containing the first @xmath123 sorted indices .    the set @xmath124 is then partitioned into @xmath125 subsets @xmath126 of cardinality @xmath127 , such that @xmath128",
    ". then the algorithm performs @xmath125 subiterations . at the @xmath129-th subiteration",
    "the algorithm considers the set @xmath130 and solves to optimality the subproblem we get from , by fixing all the variables but the ones whose indices belong to @xmath131 .",
    "below we report the scheme of the proposed algorithm ( see algorithm [ fig : fast - cda ] ) .",
    "@xmath132 @xmath133 , * set * @xmath134",
    ".    @xmath135 @xmath136    @xmath137 @xmath138 , @xmath121 , @xmath124 ;    @xmath139 @xmath140 and @xmath141 ;    @xmath142 @xmath143    @xmath144 @xmath145 , with @xmath130 , solution of problem @xmath146    @xmath147 @xmath148 if @xmath149 ;    @xmath150    @xmath151 @xmath152 ;    @xmath153    the convergence of fast - bcda is based on two important results .",
    "the first one is proposition  [ prop1 ] , which guarantees a sufficient decrease of the objective function by setting to zero the variables in the active set . the second one is reported in the proposition below .",
    "it shows that , despite the presence of the nonsmooth term , by exactly minimizing problem   with respect to a subset @xmath154 of the variables ( keeping all the other variables fixed ) , it is possible to get a sufficient decrease of the objective function in case @xmath155 .",
    "[ lemma2 ] given a point @xmath156 and a set @xmath157 , let @xmath158 be the solution of problem  , where all variables but the ones whose indices belong to @xmath154 are fixed to @xmath159 .",
    "let @xmath160 be defined as @xmath161    then we have @xmath162    see appendix [ appcon ] .",
    "now , we introduce an assumption that will enable us to prove global convergence of our algorithm .",
    "[ ass2 ] the matrix @xmath3 satisfies the following condition @xmath163 where @xmath154 is any subset of @xmath164 such that @xmath165 , with @xmath127 cardinality of the blocks used in fast - bcda .",
    "we notice that even though there are some similarities between condition and the well - known restricted isometry property ( rip ) condition with fixed order @xmath127 ( see e.g. @xcite for further details ) , condition is weaker than the rip condition .",
    "finally , we are ready to state the main result concerning the global convergence of fast - bcda .",
    "[ teorema1 ] let assumption [ ass1 ] and assumption [ ass2 ] hold .",
    "let @xmath166 be the sequence produced by algorithm ` fast - bcda ` .",
    "then , either an integer @xmath167 exists such that @xmath168 is an optimal solution for problem  , or the sequence @xmath166 is infinite and every limit point @xmath36 of the sequence is an optimal point for problem  .    see appendix [ appcon ] .",
    "now , we discuss assumptions [ ass1 ] and [ ass2 ] that are needed to guarantee convergence of fast - bcda .      assumption [ ass1 ]",
    "requires the evaluation of @xmath169 , which is not always easily computable for large scale problems .",
    "hence , we describe an updating rule for the parameter @xmath62 , that enables to avoid any `` a priori '' assumption on @xmath62 .    in practice , at each iteration @xmath70 we need to find the smallest @xmath170 such that the value @xmath171 and the corresponding sets @xmath138 , @xmath121 give a point @xmath172 satisfying @xmath173 with @xmath174 .",
    "then , we can introduce a variation of fast - bcda , namely fast - bcda-@xmath62 , that includes the updating rule for the parameter @xmath62 in its scheme , and prove its convergence .",
    "let assumption [ ass2 ] hold .",
    "let @xmath166 be the sequence produced by algorithm ` fast - bcda-\\eps ` .",
    "then , either an integer @xmath167 exists such that @xmath168 is an optimal solution for problem  , or the sequence @xmath166 is infinite and every limit point @xmath36 of the sequence is an optimal point for problem  .",
    "the proof follows by repeating the same arguments of the proof of theorem [ teorema1 ] by replacing the relation ( [ app_prop1 ] ) with ( [ eps - alg ] ) .",
    "assumption [ ass2 ] , which we need to satisfy in order to guarantee convergence of both fast - bcda and fast - bcda-@xmath62 , is often met in practice if we consider blocks of 1 or 2 variables ( i.e. @xmath127 equal to 1 or 2 ) . indeed ,",
    "when solving blocks of 1 variable , we need to guarantee that any column @xmath175 of matrix @xmath176 is such that @xmath177 this is often the case when dealing with overcomplete dictionaries for signal / image reconstruction ( as the columns of matrix @xmath176 are usually normalized , see e.g. @xcite ) .",
    "when using 2-dimensional blocks , we want no parallel columns in the matrix @xmath176 .",
    "this is a quite common requirement in the context of overcomplete dictionaries ( as it corresponds to ask that mutual coherence is lower than 1 , see e.g. @xcite ) .",
    "furthermore , the solution of 1-dimensional block subproblems can be determined in closed form by means of the well - known scalar soft - threshold function ( see e.g. @xcite ) .",
    "similarly , we can express in closed form the solution of 2-dimensional block subproblems .",
    "summarizing , thanks to the possibility to use an updating rule for @xmath62 , and due to the fact that we only use blocks of dimensions 1 or 2 in our algorithm , we have that assumptions [ ass1 ] and [ ass2 ] are quite reasonable in practice .      here",
    ", we report a result related to the convergence rate of ` fast - bcda ` with 1-dimensional blocks ( namely ` fast-1cda ` ) .",
    "in particular , we show that it converges at a linear rate . in order to prove the result",
    ", we make an assumption that is common when analyzing the convergence rate of both algorithms for @xmath0-regularized problems ( see e.g. @xcite ) and algorithms for general problems ( see e.g. @xcite ) :    [ ass3 ] let @xmath166 be the sequence generated by ` fast-1cda ` .",
    "we have that @xmath178 where @xmath36 is an optimal point of problem .",
    "now , we state the theoretical result related to the linear convergence .",
    "[ mainconvres ] let assumptions [ ass1 ] , [ ass2 ] and [ ass3 ] hold .",
    "let @xmath166 be the sequence generated by ` fast-1cda ` .",
    "then @xmath179 converges at least q - linearly to @xmath180 , where @xmath181 .",
    "furthermore , @xmath166 converges at least r - linearly to @xmath36 .",
    "see appendix [ appconrate ] .",
    "in this section , we report the numerical experiments related to ` fast - bcda ` .",
    "we implemented our method in matlab , and considered four different versions of it in the experiments :    * ` fast-1cda ` and ` fast-2cda ` , basic versions of ` fast - bcda ` where blocks of dimension @xmath132 and @xmath135 are respectively considered ; * ` fast-1cda - e ` and ` fast-2cda - e ` , `` enhanced '' versions of ` fast - bcda ` where again blocks of dimension @xmath132 and @xmath135 are respectively considered ( see subsection  [ sec : acc ] for further details ) .",
    "we first analyzed the performance of these four versions of our algorithm .",
    "then , we compared the best one with other algorithms for @xmath0-regularized least squares problems .",
    "namely , we compared ` fast-2cda - e ` with ista  @xcite , fista  @xcite , pssgb  @xcite , sparsa  @xcite and fpc@xmath182as  @xcite .",
    "+ all the tests were performed on an intel xeon(r ) cpu e5 - 1650 v2 3.50 ghz using matlab r2011b .",
    "we considered two different testing problems of the form , commonly used for software benchmarking ( see e.g.  @xcite ) .",
    "in particular , we generated artificial signals of dimension @xmath183 , with a number of observations @xmath184 and we set the number of nonzeros @xmath185 , with @xmath186 .",
    "the two test problems ( p1 and p2 ) differ in the way matrix @xmath176 is generated :    * considering @xmath187 as the gaussian matrix whose elements are generated independently and identically distributed from the normal distribution @xmath188 , the matrix @xmath176 was generated by scaling the columns of @xmath187 .",
    "* considering @xmath187 as the matrix generated by using the matlab command @xmath189 with @xmath190 , the matrix @xmath176 was generated by scaling the columns of @xmath187 .",
    "we would like to notice that the hessian matrices @xmath191 related to instances of problem p1 have most of the mass on the diagonal .",
    "then , those instances are in general easier to solve than the ones of problem p2 .",
    "once the matrix @xmath176 was generated , the true signal @xmath36 was built as a vector with @xmath192 randomly placed @xmath193 spikes , with zero in the other components .",
    "finally , for all problems , the vector of observations @xmath194 was chosen as @xmath195 , where @xmath196 is a gaussian white noise vector , with variance @xmath197 .",
    "we set @xmath198 as in @xcite .",
    "we produced ten different random instances for each problem , for a total of @xmath199 instances .",
    "the comparison of the overall computational effort is carried out by using the performance profiles proposed by dolan and mor in @xcite , plotting graphs in a logarithmic scale .",
    "for the value of @xmath123 ( number of non - active variables to be used in @xmath200 ) we set @xmath201 for ` fast-1cda ` and @xmath202 for ` fast-2cda ` ( these @xmath123 values are the ones that guarantee the best performances among the ones we tried ) . for what concerns the choice of the @xmath62 parameter used in the active set estimate , the easiest choice is that of setting @xmath62 to a fixed value .",
    "we tested several values and obtained the best results with @xmath203 and @xmath204 for ` fast-1cda ` and ` fast-2cda ` respectively .",
    "we further tested an implementation of both ` fast-1cda-\\eps ` and ` fast-2cda-\\eps ` .",
    "since there were no significant improvements in the performance , we decided to keep the @xmath62 value fixed .",
    "we would also like to spend a few words about the criterion for choosing the variables in @xmath205 . in some cases",
    ", we found more efficient using the following measure : @xmath206 in place of the one reported in , which we considered for proving the theoretical results .",
    "the main feature of this new measure is that it only takes into account first order information ( while considers proximity of the component value to zero too ) .",
    "anyway , replacing with the new measure is not a big deal , since convergence can still be proved using .",
    "furthermore , linear rate can be easily obtained assuming that strict complementarity holds . intuitively , considering only first order information in the choice of the variables",
    "should make more sense in our context , since proximity to zero is already taken into account when using the estimate to select the active variables .      by running our codes , we noticed that the cardinality of the set related to the non - active variables decreases quickly as the iterations go by .",
    "in general , very few iterations are needed to obtain the real non - active set . by this evidence , and keeping in mind the theoretical result reported in section  [ estimate ] , we decided to develop an `` enhanced '' version of our algorithms , taking inspiration by the second stage of fpc - as algorithm @xcite . once a `` good '' estimate @xmath121 of @xmath207",
    "was obtained , we solved the following smooth optimization subproblem @xmath208 in practice , we considered an estimate @xmath121 `` good '' if both there are no changes in the cardinality of the set with respect to the last two iterations , and @xmath209 is lower or equal than a certain threshold @xmath210 ( we fixed @xmath211 in our experiments ) .      in order to pick the best version among the four we developed , we preliminary compared the performance of ` fast-1cda ` ( fast1 ) ,",
    "` fast-2cda ` ( fast2 ) , ` fast-1cda - e ` ( fast1-e ) and ` fast-2cda - e ` ( fast2-e ) . in figure",
    "[ fig : ppcompprel ] , we report the performance profiles with respect to the cpu time .",
    ".5 [ l][l]fast1 [ l][l]fast2 [ l][l]fast1-e [ l][l]fast2-e    .5 [ l][l]fast2-e [ l][l]ista [ l][l]fista [ l][l]pssgb [ l][l]sparsa [ l][l]fpc - as    as we can see , even if the four version of ` fast - bcda ` have similar behaviour , ` fast-2cda - e ` is the one that gives the overall best performance .",
    "we then choose ` fast-2cda - e ` as the algorithm to be compared with the other state - of - the art algorithms for @xmath0-regularized problems .",
    "in this section , we report the numerical experience related to the comparison of ` fast-2cda - e ` with ista  @xcite , fista  @xcite , pssgb  @xcite , sparsa  @xcite and fpc@xmath182as  @xcite .    in our tests , we first ran ` fast-2cda ` to obtain a target objective function value , then ran the other algorithms until each of them reached the given target ( see e.g. @xcite ) . any run exceeding the limit of @xmath212 iterations is considered failure .",
    "default values were used for all parameters in sparsa  @xcite and fpc@xmath182as  @xcite .",
    "for pssgb  @xcite we considered the two - metric projection method and we set the parameter ` options.quadraticinit ` to @xmath132 , since this setting can achieve better performance for problems where backtracking steps are required on each iteration ( see http://www.cs.ubc.ca/~schmidtm/software/thesis.html ) . in all codes , we considered the null vector as starting point and all matrices were stored explicitly . in figure",
    "[ fig : ppmatavec ] , we report the plot of the performance profiles related to the cpu time for all instances . from these profiles",
    "it is clear that ` fast-2cda - e ` outperforms all the other algorithms and that sparsa and pssgb are the two best competitors .",
    "we then further compare , in figure  [ fig : boxplots - all ] , ` fast-2cda - e ` , sparsa and pssgb reporting the box plots related to the distribution of the cpu time . on each box ,",
    "the central mark is the median , the edges of the box are the 25th and 75th percentiles , the whiskers extend to the most extreme data points not considered outliers , and outliers are plotted individually .",
    "[ l][l]fast2-e [ l][l]pssgb [ l][l]sparsa [ c][c]cpu time ( sec ) [ c][c]all instances [ c][c]p1 instances [ c][c]p2 instances    in particular , figure  [ fig : boxplots - all ] shows the plots related to the distribution of the cpu time for all instances , for p1 instances and for p2 instances , respectively .",
    "for what concerns p1 instances , sparsa and pssgb show a similar behavior , while observing the plot related to p2 instances sparsa shows a better performance . for both classes , `",
    "fast-2cda - e ` shows the lowest median . as a further comparison among ` fast-2cda - e ` , sparsa and pssgb",
    ", we report in figure  [ fig : errp1 ] and in figure  [ fig : errp2 ] , the plots of the relative error vs. the cpu time for the p1 and the p2 instances respectively . in each plot",
    ", the curves are averaged over the ten runs for fixed @xmath213 and @xmath15 . observing these plots",
    ", we notice that ` fast-2cda - e ` is able to reach better solutions with lower cpu time .      in this subsection",
    ", we test the efficiency of our algorithm on realistic image reconstruction problems .",
    "we considered six images : a shepp ",
    "logan phantom available through the matlab image processing toolbox and five widely used images downloaded from http://dsp.rice.edu/cscamera ( the letter r , the mandrill , the dice , the ball , the mug ) .",
    "each image has @xmath214 pixels .",
    "we followed the procedure described in @xcite to generate the instances ( i.e. matrix @xmath176 and vector @xmath194 ) .",
    "what we want to highlight here is that the optimal solutions are unknown .",
    "hence the reconstructed images can only be compared by visual inspection . also in this case , we first ran ` fast-2cda ` to obtain a target objective function value , then ran the other algorithms until each of them reached the given target . the cpu - time",
    "needed for reconstructing the images is reported in table [ tabtempi ] . in figure",
    "[ fig : realim2 ] , we report the images of the dice and of the mandrill reconstructed by ` fast-2cda - e ` , pssgb and sparsa .",
    "it is interesting to notice that the quality of the reconstructed images can depend on the algorithm used . in table",
    "[ tabtempi ] , we can easily see that ` fast - bcda ` was faster in all problems .",
    "[ c][c]cpu time ( sec ) [ c][c]relative error [ l][l]fast2-e [ l][l]pssgb [ l][l]sparsa +   +   +    [ c][c]cpu time ( sec ) [ c][c]relative error [ l][l]fast2-e [ l][l]pssgb [ l][l]sparsa +   +   +     +   +    .real examples experiment - cpu time . [ cols=\"^,^,^,^,^,^,^,^,^\",options=\"header \" , ]",
    "in this paper , we devised an active set - block coordinate descent method ( ` fast - bcda ` ) for solving @xmath0-regularized least squares problems . the way the active set estimate",
    "is calculated guarantees a sufficient decrease in the objective function at every iteration when setting to zero the variables estimated active . furthermore , since the subproblems related to the blocks explicitly take into account the @xmath0-norm , the proposed algorithmic framework does not require a sign identification strategy for the non - active variables .",
    "global convergence of the method is established .",
    "a linear convergence result is also proved .",
    "numerical results are presented to verify the practical efficiency of the method , and they indicate that ` fast - bcda ` compares favorably with other state - of - the - art techniques .",
    "we further would like to remark that the proposed active set strategy is independent from the specific algorithm we have designed and can be easily included into other algorithms for @xmath0-regularized least squares , both sequential and parallel , to improve their performance .",
    "we finally highlight that the algorithmic scheme we described can be easily modified in order to work in a parallel fashion . future work will be devoted to adapt the presented approach to handle convex @xmath0-regularized problems .    *",
    "acknowledgments * the authors would like to thank the associate editor and the anonymous reviewers for their thorough and useful comments that significantly helped to improve the paper .",
    "here , we prove the main theoretical result related to the active set estimate .    _ proof of proposition [ prop1 ] . _",
    "we first define the sets @xmath215 and @xmath216=@xmath64 . by taking into account the definitions of the sets @xmath216 and @xmath217 and the points @xmath67 and @xmath218",
    ", we have : @xmath219 from which @xmath220 where @xmath221 is the unit vector , and @xmath222 is the diagonal matrix defined as @xmath223 with the function @xmath224 intended componentwise .",
    "since @xmath225 we have that the following inequality holds @xmath226 recalling ( [ eps - prop2 ] ) we obtain : @xmath227 then , we can write @xmath228 in order to prove the proposition , we need to show that @xmath229 inequality follows from the fact that @xmath230 : @xmath231 we distinguish two cases :    * if @xmath232 , we have that @xmath233 and , since @xmath234 , @xmath235 . + then , from the fact that @xmath236 , we have @xmath237 so that @xmath238 and is satisfied . * if @xmath239 , we have that @xmath240 and , since @xmath234 , @xmath241 . + then , by reasoning as in case a ) , from the fact that @xmath236 , we can write @xmath242 from which we have : @xmath243 again , we have that is satisfied .    @xmath244",
    "first , we prove the result that guarantees a sufficent decrease when minimizing with respect to a given block",
    ".    _ proof of proposition . _",
    "let us consider the subproblem obtained by fixing all variables in @xmath23 but the ones whose indices belong to @xmath154 to @xmath159 .",
    "let @xmath158 be a solution of this subproblem .",
    "we consider the set @xmath245 as the union of two sets @xmath246 where @xmath247 and @xmath248 let @xmath249 , with @xmath250 , be the following function : @xmath251 then , @xmath252 can be equivalently seen as the solution of the following problem @xmath253 by introducing the diagonal matrix @xmath254 , where @xmath255 is the vector defined as @xmath256 problem   can be written in a more compact form as @xmath257 from the kkt condition for problem   at @xmath252 we have : @xmath258 where @xmath259 is the vector of multipliers with respect to the constraints @xmath260 .",
    "we now analyze for each index @xmath261 .",
    "we distinguish two cases :    * @xmath262 . in this case",
    "we have that @xmath263 and @xmath264 .",
    "then , from we have @xmath265 * @xmath266 . in this case",
    "we have that @xmath267 and @xmath268 .",
    "therefore , @xmath269 @xmath270 the previous inequalities and the fact that @xmath271 for all @xmath266 imply that , whatever is the sign of @xmath272 , we have @xmath273 taking into account and , we have that @xmath274 now , consider the difference between @xmath275 and @xmath276 .",
    "we have that @xmath277 which can be rewritten as @xmath278 recalling and the fact that @xmath279 we have @xmath280 since @xmath281 by definition of @xmath282 we have that @xmath283 and @xmath284 now ( [ new2 ] ) , ( [ new3 ] ) and ( [ new4 ] ) prove the proposition .",
    "@xmath244    then , we prove the main convergence result related to fast - bcda .    _ proof of theorem [ teorema1 ] .",
    "_ we first prove that fast - bcda is well defined ( in the sense that @xmath285 iff the point @xmath71 is not an optimum ) .",
    "let @xmath71 not be optimum , then by contradiction we assume that @xmath286 .",
    "thus we have that either @xmath287 , or forall @xmath288 .",
    "this , in turns , implies that @xmath289 and , by taking into account the definition of @xmath290 , we have that @xmath71 is optimal , thus getting a contradiction .",
    "the proof of the other implication easily follows from propositions [ prop1 ] and [ lemma2 ] .",
    "let @xmath291 , with @xmath292 be the sequence of points produced by algorithm ` fast - bcda ` . by setting @xmath293 and @xmath108 in proposition  [ prop1 ] , we have : @xmath294 by setting @xmath295 and @xmath296 , for @xmath297 in proposition  [ lemma2 ]",
    ", we have : @xmath298 by using ( [ app_prop1 ] ) and ( [ app_lemma2 ] ) , we can write @xmath299 from which we have : @xmath300 from the coercivity of the objective function of problem   we have that the level set @xmath301 is compact . hence , the sequence @xmath166 has at least a limit point and @xmath302 now , let @xmath36 be any limit point of the sequence @xmath303 and @xmath304 be the subsequence such that @xmath305 let us assume , by contradiction , that @xmath36 is not an optimal point of problem  . by taking into account that inequality @xmath306 holds for the squared norm of sums of @xmath307 vectors @xmath308 , and by recalling ( [ app_prop1 ] ) , ( [ app_lemma2 ] ) and ( [ funzione-1 ] ) , we have @xmath309 with @xmath310 .",
    "now , ( [ funzione-2 ] ) , ( [ sub - sequence-1 ] ) , ( [ funzione-3 ] ) and ( [ funzione-4 ] ) imply @xmath311 for @xmath292 .    for every index @xmath312",
    ", we can define the point @xmath313 as follows : @xmath314 recalling the definition of points @xmath313 and @xmath315 , we have @xmath316 from the last inequality and ( [ convy ] ) we obtain @xmath317 for all @xmath312 .    to conclude the proof",
    ", we consider the function @xmath29 , defined in , that measures the violation of the optimality conditions for a variable @xmath30 .",
    "since , by contradiction , we assume that @xmath36 is not an optimal point there must exists an index @xmath116 such that latexmath:[\\[\\label{cont0 }     taking into account that the number of possible different choices of @xmath138 and @xmath121 is finite , we can find a subset @xmath319 such that @xmath320 and @xmath321 for all @xmath322 .",
    "we can have two different cases : either @xmath323 or @xmath324 for @xmath70 sufficiently large .",
    "suppose first that @xmath323 for @xmath70 sufficiently large .",
    "then , by definition  [ def : activeset ] , we have for all @xmath322 : @xmath325 for all @xmath322 , let @xmath326 be the point defined as in ( [ def - y ] ) .",
    "by construction we have that @xmath327 now we consider three different subcases :    1 .   @xmath328 . in this case ,",
    "( [ def - y ] ) and ( [ prop - y ] ) imply @xmath329 recalling ( [ eps - prop2 ] ) in assumption [ ass1 ] , there exists @xmath330 , such that @xmath331 furthermore , since @xmath323 , we can write @xmath332 then we have : @xmath333 which can be rewritten as follows @xmath334 that is @xmath335 on the other hand , since @xmath336 we have that @xmath337 and , as @xmath338 and holds , we get @xmath339 by , and , we have that @xmath340 furthermore , by and the continuity of @xmath341 , we can write @xmath342 thus we get a contradiction with . 2 .   @xmath343 .",
    "it is a verbatim repetition of the previous case .",
    "3 .   @xmath344 .",
    "since @xmath323 we have @xmath345 which imply that @xmath346 by the continuity of @xmath347 and the fact that @xmath348 we get a contradiction with .",
    "suppose now that @xmath324 for @xmath70 sufficiently large .",
    "we can choose a further subsequence @xmath349 with @xmath350 such that @xmath351 hence , latexmath:[\\[\\label{cont1 }    which , by continuity of @xmath347 , implies @xmath353 , a set of indices @xmath354 exists such that @xmath355 for all @xmath356 , algorithm ` fast - bcda ` produces a vector @xmath357 by minimizing problem  ( [ l2l1 ] ) with respect to all the variables whose indices belong to @xmath354 .",
    "therefore , the point @xmath357 satisfies @xmath358 furthermore , by , the continuity of @xmath347 , and taking into account , we can write @xmath359 which contradicts ( [ cont0 ] ) . @xmath244",
    "here , following the ideas in @xcite , we prove that the convergence rate of ` fast - bcda ` with 1-dimensional blocks ( namely ` fast-1cda ` ) is linear .",
    "first , we try to better analyze the indices in the set @xmath360 by introducing the following two sets : @xmath361 we further introduce the sets : @xmath362 which satisfy the following equality : @xmath363 we further notice that @xmath364        let us assume there exists a sequence @xmath366 , @xmath367 , a related sequence of neighborhoods @xmath368 and a sequence of points @xmath166 such that @xmath369 for all @xmath70 , satisfying the following : @xmath370 then , since the number of indices is finite , there exist subseqences @xmath371 and @xmath372 such that an index @xmath116 can be found , satisfying the following : @xmath373 from theorem [ activeest ] , for @xmath70 sufficiently large , @xmath374 therefore , we have that @xmath375 by continuity of the gradient , @xmath376 for @xmath70 sufficiently large . on the other hand , since @xmath377 , we have @xmath378 . this gives a contradiction , and proves .",
    "a similar reasoning can be used for proving .",
    "recalling , for @xmath70 sufficently large , we have @xmath383 therefore , by taking into account the steps of ` fast-1cda ` algorithm , we have @xmath384 furthermore , by continuity of @xmath385 and , we obtain @xmath386 and we can write @xmath387 hence , and still hold for @xmath388 , and so on .",
    "b ) . let us consider an index @xmath389 . by contradiction",
    ", we assume that there exists a subsequence @xmath390 such that @xmath391 for all @xmath392 . without any loss of generality",
    ", we can consider another subsequence @xmath393 related to @xmath394 , such that @xmath395 and @xmath396 for all @xmath397 and @xmath398          _ _ proof of theorem [ mainconvres].__first of all , for ease of notation we set @xmath402 , and @xmath403 . without any loss of generality , we can assume @xmath404 for all @xmath70 .",
    "we then notice that the objective function @xmath11 can be rewritten as follows : @xmath405 we further introduce the function @xmath406 by taking into account proposition [ resultuseful ] , we have , for @xmath70 sufficiently large , @xmath407 furthermore , when @xmath70 is sufficiently large , by definition of @xmath408 and @xmath409 , and recalling again proposition [ resultuseful ] , we can write @xmath410 @xmath411 then , by considering , and , it follows @xmath412 and , taking into account , we can write @xmath413 with @xmath414 . then , recalling theorem [ activeest ] and [ signs ] , for @xmath70 sufficiently large the problem we actually solve is @xmath415 now , let @xmath315 be the point obtained at step 4 of algorithm [ fig : fast - cda ] ( i.e. after fixing to zero the active variables ) and @xmath416 the component that most violates condition in the non - active set .",
    "we notice that finding the most violating variable according to condition is equivalent , when considering problem , to get the component that most violates the following condition @xmath417_+|,\\ ] ] see @xcite for further details .",
    "thus , we can write @xmath418_+\\|&\\leq & |y^{0,k}_s-[y^{0,k}_s-\\nabla_s \\tilde f(y^{0,k})]_+| \\nonumber\\\\    & = & |y^{0,k}_s-[y^{0,k}_s-\\nabla_s \\tilde",
    "f(y^{0,k})]_+ -x^{k+1}_s+[x^{k+1}_s-\\nabla_s \\tilde f(x^{k+1})]_+| \\nonumber \\\\    & \\leq & 2|y^{0,k}_s - x^{k+1}_s|+|\\nabla_s \\tilde f(y^{0,k})-\\nabla_s \\tilde f(x^{k+1})| \\nonumber\\\\    & \\leq & 2\\|y^{0,k}-x^{k+1}\\|+\\|\\nabla \\tilde f(y^{0,k})-\\nabla \\tilde f(x^{k+1})\\|\\nonumber \\\\    & \\leq & m\\|y^{0,k}-x^{k+1}\\|=m \\|x^k - x^{k+1}\\|_{{\\cal n}^k } , \\end{aligned}\\ ] ] where @xmath419_+$ ] is the projection on the set of inequalities in problem , and , with @xmath420 lipschitz constant of @xmath421 . by using propositions [ prop1 ] and [ lemma2 ]",
    ", we can also write : @xmath422 with @xmath423 . by",
    "taking into account inequality and the definition of @xmath315 , we can write , for @xmath70 sufficiently large , @xmath424_+\\|^2 .                    \\end{aligned}\\ ] ] now , considering theorem 2.1 in @xcite we have , for @xmath70 sufficiently large , @xmath425_+\\|\\geq \\|y^{0,k}-x^\\star\\|=\\|x^k - x^\\star\\|_{{\\cal n}^k},\\ ] ] with @xmath426 .",
    "therefore , by taking into account inequality , we can write @xmath427 with @xmath428 . by combining inequalities , and , we can write @xmath429 with @xmath430 . after rearranging the terms in ,",
    "we obtain @xmath431 with @xmath432 then , @xmath179 converges at least linearly to @xmath180 ."
  ],
  "abstract_text": [
    "<S> the problem of finding sparse solutions to underdetermined systems of linear equations arises in several applications ( e.g. signal and image processing , compressive sensing , statistical inference ) . </S>",
    "<S> a standard tool for dealing with sparse recovery is the @xmath0-regularized least - squares approach that has been recently attracting the attention of many researchers .    in this paper </S>",
    "<S> , we describe an active set estimate ( i.e. an estimate of the indices of the zero variables in the optimal solution ) for the considered problem that tries to quickly identify as many active variables as possible at a given point , while guaranteeing that some approximate optimality conditions are satisfied . </S>",
    "<S> a relevant feature of the estimate is that it gives a significant reduction of the objective function when setting to zero all those variables estimated active . </S>",
    "<S> this enables to easily embed it into a given globally converging algorithmic framework .    </S>",
    "<S> in particular , we include our estimate into a block coordinate descent algorithm for @xmath0-regularized least squares , analyze the convergence properties of this new active set method , and prove that its basic version converges with linear rate .    finally , we report some numerical results showing the effectiveness of the approach .    </S>",
    "<S> @xmath0-regularized least squares , active set , sparse optimization    65k05 , 90c25 , 90c06 </S>"
  ]
}