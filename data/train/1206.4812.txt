{
  "article_text": [
    "one of the main functions of the brain is prediction @xcite .",
    "this function is generally thought to rely on the idea that cortical regions learn a model of the world and simulate it to generate predictions of future events @xcite .",
    "several recent experimental findings support this view , showing in particular that the spontaneous neuronal activity after presentation of a stimulus is correlated with the evoked activity @xcite , and that this similarity increases along development and learning @xcite .",
    "moreover , at the scale of neuronal networks , prediction can also be seen as a general organization principle : it has been argued @xcite that the brain would contain a hierarchy of predictive units which are able to predict their direct stimuli or inputs , through the modification of the synaptic connections .",
    "understanding the mechanisms and principles underlying this prediction function is a key challenge , not only from a neuroscience perspective but also for machine learning where the number of applications requiring prediction is significant .    in the field of machine learning ,",
    "recurrent neural networks have been successfully proposed as candidates for these predictive units @xcite . in most cases , these algorithms aim at creating a neural network that autonomously and spontaneously reproduces a given time series .",
    "the bayesian approach is also useful in designing predictors @xcite and has also been mapped to neural networks @xcite . however , apart from a rough conceptual equivalence , this paper is devoid of bayesian terminology and directly focuses on neural networks . in this framework ,",
    "prediction is often achieved by minimizing a distance between the activity of the neural network and the target time series .",
    "although neural networks were originally studied in a feedforward framework @xcite , the most efficient networks for prediction shall involve recurrent connections giving the network some memory properties .",
    "so called gradient descent algorithms in recurrent neural network @xcite involve the learning of the entire connectivity matrix .",
    "they minimize the distance between a target trajectory and the trajectory of the network .",
    "on the other hand , researchers in the field of reservoir computing @xcite only optimize some connections in the network whereas the others are randomly drawn and fixed . to do prediction ,",
    "they minimize the `` one - step ahead ''",
    "error corresponding to the distance between the network predictions and the next time step of the target time series .",
    "thus , these algorithms are derived to optimize an accuracy criterion , with learning rules generally favoring prediction efficiency over biological meaning .    in the field of neuroscience",
    ", these last years have seen many discoveries in the study of synaptic plasticity , in particular providing experimental evidences and possible mechanisms for two major concepts in the current biology of learning .",
    "the first is the discovery of spike - timing dependent plasticity ( stdp )  @xcite .",
    "it is a temporally asymmetric form of hebbian learning induced by temporal correlations between the spikes of pre- and post - synaptic neurons .",
    "the general principle is that if a neuron fires before ( resp .",
    "after ) another then the strength of the connection from the former to the latter will be increased ( resp .",
    "decreased ) .",
    "the summation of all these modification leads to the strengthening of causality links between neurons .",
    "although stdp is originally based on spiking network , it has several extensions or analogs for rate - based networks ( those used in machine learning ) @xcite .",
    "the functional role of stdp is still discussed , for instance : reducing latency @xcite , optimizing information transfer @xcite , invariant recognition @xcite and even learning temporal patterns @xcite ( non exhaustive list ) .",
    "second , the notion of homeostatic plasticity @xcite , including mechanisms such as synaptic scaling , has proved to be important to moderate the growth of connection strength .",
    "in contrast to previously theory - motivated normalization of the connectivity @xcite , there is a need of a biologically plausible means to prevent the connectivity from exploding under the influence of shaping mechanisms like hebbian learning or stdp .    from a theoretical viewpoint , stdp and homeostatic plasticity",
    "are almost always studied independently .",
    "an extensive bottom - up numerical analysis of the combination of such learning mechanisms , done by triesch and colleagues , has already lead to biologically relevant behaviors @xcite .",
    "however , the mathematical understanding of their combination in terms of functionality still stands as an undocumented challenge to researchers .",
    "this letter aims at bridging the gap between biological mechanisms and machine learning regarding the issue of predictive neural networks .",
    "we rigorously show how a biologically - inspired learning rule , made of an original combination of stdp mechanism and homeostatic plasticity , mimics the gradient descent of a distance between the activity of the neural network and its direct stimuli .",
    "this results in capturing the underlying dynamical behavior of the stimuli into a recurrent neural network and therefore in designing a biologically plausible predictive network .",
    "the letter is organized as follows . in section [ sec :",
    "gradient descent ] , we construct a theoretical learning rule designed for prediction , based on an appropriate gradient descent method . then , in section [ sec : biological ] , we introduce a biologically inspired learning rule , combining the concepts of stdp and homeostatic plasticity , whose purpose is to mimic the theoretical learning rule .",
    "we discuss the various biological mechanisms which may be involved in this new learning rule .",
    "finally in section [ sec : link ] , we provide a mathematical justification of the link between the theoretical and the biologically inspired learning rule , based on the key idea that stdp can be seen as a differential operator .",
    "in a machine learning approach , we introduce here a procedure to design a neural network which autonomously replays a target time series .      we consider a recurrent neural network made of @xmath0 neurons which is exposed to a time dependent input @xmath1 of the same dimension .",
    "our aim is to construct a learning rule which will enable the network to reproduce the input s behavior .",
    "our approach is focused on learning the underlying dynamics of the input .",
    "therefore , we assume that @xmath2 is generated by an arbitrary dynamical system : @xmath3 with @xmath4 a smooth vector field from @xmath5 to @xmath5 .",
    "we also assume that the trajectory of the inputs or stimuli is @xmath6-periodic .",
    "the key mathematical assumptions on the input @xmath2 is in fact ergodicity , but we restrict our study to periodic inputs for simplicity .",
    "in particular , periodic inputs can be constructed by the repetition of a given finite - time sample .",
    "although the following method virtually works with any network equations , we focus on a neural network composed of @xmath0 neurons and governed by @xmath7 where @xmath8 is a vector representing neuronal activity , @xmath9 is the connectivity matrix , @xmath10 is a decay constant and @xmath11 is an entry - wise sigmoid function .",
    "the idea behind our learning rule is to find the best connectivity matrix @xmath12 which will minimize a distance between the two functions @xmath13 and @xmath14 . in this perspective",
    ", we define the following quantity : @xmath15 when @xmath16 , the vector fields of systems and are equal on the trajectories of the inputs .",
    "this quantity may be viewed as a distance between the two vector fields defining the dynamics of the inputs and of the neuronal network along the trajectories of the inputs .",
    "one shall notice that it is similar to classical gradient methods  @xcite , except that the norms are applied to the flows of inputs and neural network instead of their activity .",
    "thus , it focuses more specifically on the dynamical structure of the inputs . moreover , it is possible to show , using girsanov s theorem , that this definition coincides with the concept of _ relative entropy _ between two diffusion processes , namely the ones obtained by adding a standard gaussian perturbation to both equations .",
    "therefore , we will call @xmath17 the relative entropy .    in order to capture the dynamics of the inputs into the network ,",
    "it is natural to look for a learning rule minimizing this quantity . to this end",
    ", we consider the gradient of this measure with respect to the connectivity matrix : @xmath18 \\label{eq : entropy gradient}\\ ] ] where the component @xmath19 of @xmath20 is @xmath21 for @xmath22 functions from @xmath23 to @xmath5 .",
    "equivalently , these functions can be seen as semi continuous matrices in @xmath24}$ ] and @xmath25 is the transpose of @xmath26 .",
    "+ thus , an algorithm implementing the gradient descent @xmath27 is a good candidate to minimize the relative entropy between inputs and spontaneous activity .",
    "since @xmath28 is quadratic in @xmath12 , it follows that @xmath29 is a convex function , thus excluding situations with multiple local minima .",
    "moreover , if @xmath30 is invertible , one can compute directly the minimizing connectivity as @xmath31.\\big[s(\\u)\\cdot s(\\u)'\\big]^{-1 } \\label{eq : equilbrium connectivity}\\ ] ]    implementing a gradient descent based on equation does not immediately lead to a biologically relevant mechanism .",
    "first , it requires a direct access to the inputs @xmath2 , whereas synaptic plasticity mechanisms shall only rely on the network activity @xmath32 .",
    "second , it is a batch learning algorithm which requires an access to the entire history of the inputs .",
    "third , it requires the ability to compute @xmath13 .",
    "therefore , we will see in section [ sec : biological ] how to overcome these issues combining biologically inspired synaptic plasticity mechanisms .      ;",
    "@xmath33 ; @xmath34.,scaledwidth=100.0% ]    in order to illustrate the idea that learning rule enables the network to learn dynamical features of the input , we have constructed the following experiment .",
    "we present to the network an input movie displaying sequentially the writing of the letter a ( figure [ fig : ecrire ] - top row ) . to each pixel",
    "we assign one neuron , so that the input and the network share the same dimension .",
    "this input movie is repeated periodically until the connectivity matrix of the network , evolving under rule , stabilizes .",
    "then the input is turned off and we set the initial state of the network to a priming image showing the bottom left part of letter a. the network evolving without input strikingly reproduces the dynamical writing of letter a as displayed in figure [ fig : ecrire ] .",
    "thus , with this example we have illustrated the ability of the learning rule we have derived from a theoretical principle to capture a dynamic input into a connectivity matrix .",
    "we now introduce a biological learning rule made of the combination of stdp and homeostatic plasticity . later in section [ sec : link ] , we show that this learning rule minimizes @xmath17 . here",
    ", we first give a mathematical description of this learning rule and , second , relate the different terms to biological mechanisms .",
    "learning corresponds to a modification of the connectivity simultaneous to the network activity evolution .",
    "the result is a coupled system of equations .",
    "the learning rate @xmath35 is chosen to be small : @xmath36 , so that learning can be considered slow compared to the evolution of the activity .",
    "the full online learning system is @xmath37}_{\\mbox{stdp } } - \\underbrace{\\sum_k \\w_{ik } s(\\bar{\\v}_k ) s(\\bar{\\v}_j)}_{\\mbox{homeostatic plasticity } } \\end{array } \\right.\\ ] ] where @xmath38 and @xmath39 = \\frac{\\gamma + l}{2 } x ( y*g_\\gamma ) - \\frac{\\gamma - l}{2 } ( x*g_\\gamma ) y \\label{eq : delta}\\ ] ] where the notation @xmath40 denotes the convolution operator .",
    "the function @xmath41 is defined as @xmath42 with @xmath43 the heaviside function and for any positive number @xmath44 as shown in the left picture of figure [ fig : profiles ] . as illustrated in the right picture of figure [ fig : profiles ] , the operator @xmath45 roughly corresponds to the classical stdp window @xcite ( taking into account a y - axis symmetry corresponding to the symmetric formalism we are using ) .",
    "the constant @xmath46 is a time constant corresponding to the width of the stdp window used for learning .     for @xmath47 .",
    "( right ) plot of the function @xmath48 for @xmath49 and @xmath50 .",
    "this function corresponds to the operator @xmath45 as shown in section [ sec : temporal averaging ] . ]",
    "the variable @xmath51 can be seen as a spatio - temporal differential variable which approximates the inputs @xmath2 .",
    "although unsupervised learning rules are often algebraic combinations of element - wise functions applied to the activity of the network  @xcite , it is not precisely the case here . indeed , learning is based on the variable @xmath51 which corresponds to the subtraction of the temporally integrated synaptic drive @xmath52 from the activity of the neurons @xmath53 . for each neuron",
    ", this variable takes into account the past of all the neurons which are then spatially averaged through the connectivity to be subtracted from the current activity .",
    "this gives a differential flavor to this variable which is reminiscent of former learning rules  @xcite for the temporal aspect and  @xcite for the spatial aspect .",
    "note that this variable is not strictly speaking local ( i.e. the connection @xmath54 needs the values of @xmath55 to be updated ) , yet it is biologically plausible since the term @xmath56 is accessible for neuron @xmath57 on its dendritic tree , which is a form of locality in a broader sense .",
    "the first term @xmath58 $ ] in can be related to stdp .",
    "the antisymmetric part of this term is responsible for retrieving the drift @xmath4 in equation .",
    "the symmetric part ( corresponding to hebbian learning ) is responsible for retrieving the second term in .",
    "thus , it captures the causality structure of the inputs which is a task generally attributed to stdp  @xcite . beyond the simple similarity of functional role , we believe",
    "a simplification of this term may shed light on the deep link it has with stdp .",
    "the main difference between our setup and stdp is that the former is based on a rate - based dynamics , whereas the latter is based on a spiking dynamics . in a pure spike framework ,",
    "i.e the activity is a sum of diracs , the stdp can be seen as this simple learning rule @xmath59 = \\frac{\\gamma + l}{2 } \\v_i ( \\v_j \\ast g_\\gamma ) - \\frac{\\gamma - l}{2}(\\v_i \\ast g_\\gamma ) \\",
    "indeed , the term @xmath60 is non - null only when the post - synaptic neuron @xmath57 is firing and then , via the factor @xmath61 , it counts the number of preceding pre - synaptic spikes that might have caused @xmath57 s excitation and weight them by the decreasing exponential @xmath62 .",
    "thus , this term exactly accounts for the positive part of the stdp curve .",
    "the negative term @xmath63 takes the opposite perspective and accounts for the negative part of the stdp curve . a loose extension of this rule to the case where the activity is smoothly evolving leads to identifying the function @xmath45 to the stdp mechanism for rate - based networks  @xcite .",
    "the second term @xmath64 in the previous stdp term seems to be a powerful mechanism to shape the response of the network .",
    "however , there is a need of a regulatory process to prevent from uncontrolled growth of the network connectivity  @xcite .",
    "it has been argued that stdp could be self regulatory  @xcite , but it is not the case in our framework and an explicit balancing mechanism is necessary to avoid the divergence of the system .",
    "this last term is the only one with a negative sign and is multiplicative with respect to the connectivity .",
    "thus , according to  @xcite , it is a reasonable candidate for homeostasis .",
    "it has been argued  @xcite that homeostatic plasticity might keep the relative synaptic weights by dividing the connectivity with a common scaling factor , theoretically preventing from a possible information loss .",
    "in contrast to these _ ad hoc _ re - normalizations often introduced in other learning rules  @xcite , our relative entropy minimizing learning rule thus introduces naturally an original form of homeostatic plasticity .",
    "although we have separated the description of the various terms in , our approach suggests that homeostasis may be seen , not necessarily as a scaling term , but as a constitutive part of a learning principle , deeply entangled @xcite with the other learning mechanisms .",
    "although the focus of the paper is on theory , we introduce a simple numerical example to illustrate the predictive properties of the biological learning rule .",
    "more precisely , we investigate the question of retrieving the connectivity of a neural network based on the observation of the time series of its activity .",
    "this is an inverse problem which is a usual challenging topic in computational neuroscience  @xcite since it may give access to large scale effective connectivities simply from the observation of a neuronal activity .",
    "here we address it in an elementary framework .",
    "the network generating the activity patterns is referred as input network and evolves according to @xmath65 . for this example , the network is made of @xmath66 neurons and its connectivity @xmath67 is shown in figure [ fig : example 1].a ) .",
    "these parameters were chosen so that the activity is periodic as shown by the dashed curves in figure [ fig : example 1].c ) .",
    "then , we simulate the entire system with a decay constant @xmath68 and observe that its connectivity @xmath69 converges to @xmath67 .     of the input network .",
    "b ) evolution of the difference between current connectivity and input connectivity through learning .",
    "the black dot - dashed curve corresponds to the online learning rule in the homogeneous case , i.e. @xmath70 in both equations in .",
    "the grey dashed curve corresponds to the online learning system in the hybrid framework , i.e. @xmath71 for the learning equation and @xmath72 for the network equation .",
    "for this simulation we chose @xmath73 .",
    "the black plain curve ( superposed to the grey dashed curve ) corresponds to the batch relative entropy minimization .",
    "c ) the dashed curves correspond to the activity of the inputs .",
    "it is a three dimensional input ( i.e. @xmath66 ) and the three different grey levels correspond to the different dimensions .",
    "the plain curves correspond to the simulation of the network ( top equation in ) post learning , in the hybrid framework , and without inputs .",
    "the parameters for these simulations are @xmath74 , @xmath75 and @xmath76.,scaledwidth=100.0% ]    as shown in the next section , it is necessary @xmath77 in order to approximate accurately the input s activity with the online learning rule . given that the time constant of the inputs is governed by @xmath78 , the previous approximation holds only if @xmath79 .",
    "if this assumption is broken , e.g. @xmath80 , then the final connectivity matrix is different from @xmath67 , see the black dot - dashed curve in figure [ fig : example 1].b ) .",
    "indeed , in this case , the network only learns to replay the slow variation of the inputs .    a method to recover the precise time course of the inputs consists in artificially changing the time constants at different steps of an algorithm described in the following .",
    "first , simulate the network equation ( top equation in ) with a constant @xmath81 .",
    "yet , the time constant in the learning equation ( bottom equation in ) is to be kept at @xmath78 , thus introducing a hybrid model . in this framework",
    ", the connectivity converges exactly to @xmath67 as shown in the grey dashed curve in figure [ fig : example 1].b ) . after learning ,",
    "simulate the network equation with the learned connectivity and with a time constant switched back to @xmath78 .",
    "this gives an activity as shown in the plain curves in figure [ fig : example 1].c ) .",
    "in this part , we show how the biological learning rule implements the gradient descent based on equation .",
    "first , we introduce three technical results which are the mathematical cornerstones of the paper and then combine them to obtain the desired result .      as mentioned previously , equation is not biologically plausible for three main reasons : ( i ) it requires a direct access to the inputs @xmath2 , whereas synaptic plasticity mechanisms shall only rely on the network activity @xmath32 ; ( ii ) it is a batch learning algorithm which requires an access to the entire history of the inputs ; ( iii ) it requires the ability to compute @xmath13 ( equal to the time - derivative of the inputs according to equation ) .    on the contrary , the biological learning rule does not have these problems .",
    "first , it uses an estimate of the inputs , noted @xmath82 , which is based on the activity variable only .",
    "second , it is an online learning rule , i.e. it takes input on the fly , and relies on a slow - fast mechanism to temporally average the variables .",
    "third , it approximates the computation of @xmath13 with a function @xmath45 inspired from stdp convolution . in the following , we mathematically address these three points successively .      the online learning rule is expressed by means of the activity of the neural network @xmath32 governed by the top equation in .",
    "however , to be comparable to the gradient of the relative entropy , we first need to make explicit the dependency on the inputs @xmath2 .",
    "therefore , we show that the network dynamics induces a simple relation between @xmath51 and the inputs @xmath2 : a simple computation in the fourier domain shows that the convolution between the temporal operator @xmath83 and @xmath84 results in @xmath85 .",
    "applying this result to the neural dynamics leads to reformulating the top equation in as @xmath86 .",
    "we recognize the definition of the variable @xmath51 ( which was originally defined according to this relation ) such that the network s dynamics of the fast equation in is equivalent to @xmath87      to prove that implements the gradient descent of the relative entropy , we need to use a time - scale separation assumption , enabling the input to reveal its dynamical structure through ergodicity . indeed ,",
    "the fact that learning is very slow compared to the activity @xmath32 corresponds to the assumption @xmath36 . in this case , we can apply classical results of periodic averaging  @xcite ( see also @xcite in the context of neural networks ) to show that the evolution of @xmath12 is well - approximated by @xmath88\\cdot s(\\bar{\\v } ) ' \\label{eq : ze learning rule averaged 2}\\ ] ] where @xmath89 and @xmath90 .",
    "the right picture of figure [ fig : profiles ] shows a plot of the function @xmath91 .      here",
    ", we prove the two following equalities @xmath92\\cdot \\y ' = ( \\x\\ast g_\\gamma ) \\cdot ( \\y \\ast g_\\gamma)'\\ ] ] and @xmath93\\cdot \\y ' = ( \\dot{\\x}\\ast g_\\gamma ) \\cdot ( \\y \\ast g_\\gamma ) ' \\label{eq : stdp diff op}\\ ] ] this second equation is the key mathematical mechanism that makes stdp a good approximation of the temporal derivative of the inputs .",
    "both proofs consist in going in the fourier domain , where convolutions are turned into multiplications .",
    "we use the unitary , ordinary frequency convention for the fourier transform .",
    "observe that the fourier transform of @xmath62 is @xmath94 .",
    "and we define can be written as a matrix multiplication with a continuous toeplitz matrix @xmath95 whose component @xmath96 is @xmath97 . in this framework",
    ", the convolution with @xmath98 corresponds to the multiplication with the transpose of the previous toeplitz operator . ]",
    "@xmath99 such that @xmath100 and @xmath101 .    1 .",
    "let us show that @xmath102\\cdot \\y ' = ( \\x \\ast g_\\gamma ) \\cdot ( \\y \\ast g_\\gamma)'$ ] + we proceed in two steps : 1 .",
    "let us show that @xmath103 + the fourier transform of the convolution @xmath104 is the product @xmath105 . from the usual fourier tables we observe that the right hand side is the fourier transform of @xmath106 .",
    "this immediately leads to the result .",
    "2 .   let us show that @xmath107 + compute @xmath108 + using the result ( a ) and applying result ( b ) to @xmath109 and @xmath110 leads to the result @xmath102\\cdot \\y ' = ( \\x \\ast g_\\gamma ) \\cdot ( \\y \\ast g_\\gamma)'$ ] .",
    "2 .   to prove equation , let us first show that @xmath111 .",
    "+ the fourier transform of the convolution @xmath112 is the product @xmath113 . besides , @xmath114 because @xmath115 , taking the inverse fourier transform of @xmath116 gives the intermediary result @xmath111 .",
    "+ using it and applying the first result to @xmath117 and @xmath110 leads to equation .",
    "we prove here that the biological learning rule implements the gradient descent based on .",
    "the temporally averaged version of the biological learning rule is given by .",
    "we simultaneously use equations and on this formula to show that the solution of the biological learning rule is well approximated by @xmath118\\cdot \\big[s(\\u \\ast g_l ) \\ast g_\\gamma\\big ] ' + l ( \\u \\ast g_l \\ast g_\\gamma)\\cdot \\big(s(\\u \\ast g_l)\\ast g_\\gamma\\big)'\\\\   - \\w\\cdot s(\\u \\ast g_l)\\cdot s(\\u \\ast g_l)'\\end{gathered}\\ ] ]    if @xmath2 is slow enough , i.e. @xmath77 , then this equation is precisely the gradient descent of @xmath119 . if @xmath2 is too fast , then the network will only learn to predict the slow variations of the inputs .",
    "some fast - varying information is lost in the averaging process , mainly because the network equation in acts as a relaxation equation which filters the activity .",
    "note that this loss of information is not necessarily a problem for the brain , since it may be extracting and treating information at different time scales  @xcite .",
    "actually , the choice of the parameter @xmath120 specifies the time scale under which the inputs are observed .",
    "both theoretical and biological learning rules are assured to make the connectivity converge to an equilibrium point whatever the initial condition . in particular , this method does not suffer from the bifurcation issues often encountered in recurrent neural network learning @xcite . in most cases ,",
    "problems arise when learning leads the network activity to a bifurcation .",
    "the bifurcation suddenly changes the value of the quantity being minimized and this intricate coupling leads to very slow convergence rates .",
    "the reason why this method has such an unproblematic converging property is because the relative entropy ( as opposed to other energy functions traditionally used ) is independent from the network activity @xmath32 .",
    "besides , for any network pattern @xmath32 equation ensures that @xmath82 is always equal to the convolved inputs .",
    "these two arguments break the pathologic coupling preventing from getting interesting convergence results .    from a mathematical perspective",
    ", the krasovskii - lasalle invariance principle  @xcite ensures that the theoretical learning rule @xmath121 converges to a equilibrium point .",
    "indeed , the relative entropy acts as an energy or lyapunov function .",
    "if @xmath122 is invertible then the equilibrium point is unique and defined by equation .",
    "if it is not invertible then the equilibrium depends on the initial condition .",
    "if the inputs are slow enough , it has been shown that the biological learning is well approximated by the theoretical gradient descent .",
    "therefore , the former exhibits the same stable converging behavior as the latter .",
    "thus , the biological learning rule is stable . in practice",
    ", we have never encountered a diverging situation even when the inputs are not slow enough .",
    "we have shown that a biological learning rule shapes a network into a predictor of its stimuli .",
    "this learning rule is made of a combination of a stdp mechanism and homeostatic plasticity .",
    "after learning the network would spontaneously predict and replay the stimuli it has previously been exposed to .",
    "this was achieved by showing that this learning rule minimizes a quantity analogous to the relative entropy ( or kullback - leibler divergence ) between the stimuli and the network activity as for other well - known algorithms @xcite .",
    "we believe this letter brings interesting arguments in the debate concerning the functional role of stdp .",
    "we have shown that the antisymmetric part of stdp can be seen as a differential operator .",
    "when its effect is moderated by an appropriate scaling term , we argue that it could correspond to a generic mechanism shaping neural networks into predictive units .",
    "this idea is not new , but this letter may give it a stronger theoretical basis .",
    "this study also gives central importance to the time constants characterizing the network .",
    "indeed , the fact the biological learning rule implements a gradient descent is rigorously true for slow inputs .",
    "inputs are slow if they are significantly slower than both the time window of the stdp and the decay constant of the network .",
    "this means that sub - networks in the brain could select the frequency of the information they are predicting .",
    "given that the brain may process information at different time scales @xcite , this may be an interesting feature . besides , note that the proposed biological learning rule is partly characterized by the activity decay parameter @xmath120 .",
    "this is surprising because it links the intrinsic dynamics of the neurons to the learning processes corresponding to the synapse .",
    "therefore , it may provide a direction to experimentally check this theory : is the symmetric part of stdp ( i.e. hebbian learning ) stronger between faster neurons ?",
    "one of the characteristic features of this research is the combination of rate - based networks and the concept of spike timing dependent plasticity .",
    "obviously , this made impossible to consider the rigorous definition of stdp .",
    "however , we have argued that the function @xmath45 in is an alternative formulation which is equivalent to the others in the spiking framework and which can be trivially extended to analog networks . besides , it does capture the functional mechanism of the stdp in the rate - based framework : when the activation of a neuron precedes ( resp .",
    "follows ) the activation of another the strength of the synapse from the former to the latter is increased ( resp",
    ". decreased ) .",
    "finally , we believe the theory presented in this paper gives support to this rate - based stdp since it appears to fill a gap between machine learning and biology by implementing a differential operator .",
    "this approach can be applied to other forms of network equations than such as the wilson - cowan or kuramoto models , leading to different learning rules . in this perspective",
    ", learning can be seen as a projection of a given arbitrary dynamical system to a versatile neuronal network model , and the learning rule will depend on the chosen model .",
    "however , we shall remark that any network equation with an additive structure  intrinsic dynamics + communication with other neurons  as in will lead to a similar structure for the learning rule , with terms that may share similar biological interpretations as developed above .",
    "a special case is the linear network @xmath123 for which various statistical methods to estimate the connectivity matrix have been applied e.g. in climate modeling  @xcite , gene regulatory networks  @xcite and spontaneous neuronal activity  @xcite . in this simpler case ,",
    "the biological learning rule in equation would be @xmath124 - \\sum_k \\w_{ik } \\bar{\\v}_k \\bar{\\v}_j$ ] with @xmath125 .",
    "the method developed in this article can be used to extend the _ inverse modeling _",
    "approach previously developed in the linear case to models with non - linear interactions .",
    "one of the main restrictions of this approach is that the dimensionality of the stimuli and that of the network have to be identical . therefore , as such , the accuracy of this biological mechanism does not match the state of the art machine learning algorithms.this is not necessarily a big issue since a high dimensional projections of the inputs may be used as preprocessing . from a biological viewpoint and",
    "taking the example of vision , this would correspond to the retino - cortical pathway which is not one to one and very redundant .",
    "but this limitation also puts forward the necessity to study a network containing hidden neurons in a similar fashion .",
    "ongoing research is revealing that the mathematical formalism is well suited to extend this approach to the field of reservoir computing @xcite .",
    "the authors thank herbert jaeger for his helpful comments on the manuscript .",
    "doya , k. ( 1992 ) .",
    "bifurcations in the learning of recurrent neural networks . in _ circuits and systems , 1992 .",
    "iscas92 . proceedings .",
    ", 1992 ieee international symposium on _ , volume  6 , pages 27772780 ."
  ],
  "abstract_text": [
    "<S> identifying , formalizing and combining biological mechanisms which implement known brain functions , such as prediction , is a main aspect of current research in theoretical neuroscience . in this letter , </S>",
    "<S> the mechanisms of spike timing dependent plasticity ( stdp ) and homeostatic plasticity , combined in an original mathematical formalism , are shown to shape recurrent neural networks into predictors . following a rigorous mathematical treatment </S>",
    "<S> , we prove that they implement the online gradient descent of a distance between the network activity and its stimuli </S>",
    "<S> . the convergence to an equilibrium , where the network can spontaneously reproduce or predict its stimuli , does not suffer from bifurcation issues usually encountered in learning in recurrent neural networks . </S>"
  ]
}