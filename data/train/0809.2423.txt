{
  "article_text": [
    "the web - like pattern certainly is the most striking feature of matter distribution on megaparsecs scale in the universe .",
    "the existence of the `` cosmic web '' @xcite @xcite has been confirmed more than twenty years ago by the first cfa catalog @xcite and the more recent catalogs such as sdss @xcite or 2dfgrs @xcite .",
    "these observations , together with the dramatic improvement of computer simulations ( e.g. @xcite @xcite ) have largely improved the picture of a universe formed by an intricate network of voids ( _ i.e. _ globular under - dense regions ) embedded in a complex filamentary web which nodes are the location of denser halos .",
    "the traditional way of understanding large scale structures ( lss ) formation and evolution relies on friedman equations and assumes that lss are the outcome of the growth of very small primordial quantum fluctuations by gravitational instability ( see _ e.g. _ @xcite or @xcite and references therein ) .",
    "in this theory , the solution for structure formation is described in terms of a mass distribution that one needs to grasp ( _ i.e. _ by following the evolution of its most important features ) and compare these to observations .",
    "comprehending the mass distribution as a whole , especially at non - linear stages , is a very difficult task . a possible solution therefore consists in extracting and studying simple characteristic features of matter distribution such as voids , halos and filaments as individual physical objects .",
    "so far , mainly because of the relatively higher complexity of the filaments , most theoretical and computational researches have focused on the voids and halos .",
    "the dark matter halos have arguably been the most studied component of the cosmic web .",
    "their density profiles for instance are very well described by so - called nfw profiles @xcite and non - parametric models are still under investigation @xcite .",
    "the dependence of these density profiles on the halos mass ( _ e.g. _ @xcite , @xcite ) has also been investigated thoroughly and its relationship with redshift and environmental properties are a very active topics ( _ e.g. _ @xcite , @xcite , @xcite , @xcite , @xcite or @xcite ) . from a computational point of view",
    ", much effort has been put into the development of various algorithms to identify halos in simulations and galaxies in spectroscopic redshift galaxy surveys .",
    "the friend - of - friend algorithm @xcite is now widely spread , as well as more complex hierarchical sub - structures identifiers such as hfof @xcite , subfind @xcite , voboz @xcite or adaptahop @xcite .",
    "voids are another feature of cosmological matter distribution that also have a long history of theoretical and computational modeling .",
    "the first voids were observed by @xcite and are in some sense the counterpart of halos : the initial quantum perturbations collapsing into halos at non - linear stages leave room to voids in the under - dense regions .",
    "the first theoretical voids models where developed by @xcite , @xcite or @xcite among others , while numerical void finders exist , such as the one described in @xcite , zobov @xcite , based on voronoi tessellation , or the recent watershed void finder , based on the watershed transform ( _ e.g. _ @xcite , @xcite ) , by @xcite ( see the introduction and references therein for a more complete review of the subject ) .",
    "the improvements in our understanding of voids and halos properties led to the formulation of powerful theories such as the patches theory @xcite the extended press - schechter theory ( _ e.g. _ @xcite and @xcite ) or the skeleton - tree formalism @xcite .",
    "but our investigation of the filaments as individual objects is not yet as thorough as for the halos and voids : the definition of a well established mathematical framework for their study could therefore lead to significant improvements in our understanding of matter distribution in the universe .",
    "the first attempts date from @xcite , who used a graph - theory construction : the minimal spanning tree ( mst ) .",
    "this method defines the cosmic web as the network linking galaxies ( or particles from a numerical simulation ) , having the property of being loop - free and of minimal total length .",
    "this technique was later developed in order to try quantifying in an objective way the properties of the cosmic web ( see _ e.g. _ @xcite , @xcite and a review on the subject can be found in @xcite ) .",
    "the so - called shape finders ( @xcite , @xcite or @xcite ) allow a statistical study of the filaments and another method , based on the candy model , commonly used to detect road networks , uses a marked point process and a simulated annealing algorithm to trace the filaments @xcite .",
    "more recently , the skeleton formalism and its local approximation , that describe the filaments as particular field lines of the density field , was introduced by @xcite and @xcite with the advantage of framing a well - defined mathematical ground for theoretical predictions of the filaments properties as well as an efficient numerical identification algorithm .",
    "finally , an interesting first attempt to unify halos , voids and filaments identification using the multiscale morphology filter ( mmf ) technique was also proposed by @xcite .    in this paper",
    ", we introduce a framework and algorithm to identify the full hierarchy of critical lines , surfaces , volumes ... of density distribution in the general case of @xmath0-dimensional spaces . for 3d space",
    ", these critical subspaces can be identified to the void and peak patches , as well as filaments and other primary critical lines of the distribution .",
    "the algorithm extracts the filaments as a differentiable and , by definition , fully connected networks that traces the backbone of the cosmic web .",
    "this method is closely related to the skeleton formalism presented in @xcite and @xcite and is also based on both morse theory ( see _ e.g. _ @xcite , @xcite or @xcite ) and an improved watershed segmentation algorithm that uses a probability propagation scheme . + this paper",
    "is organized as follows . in section [ sec : algo ] , we present a general definition of the critical sub - spaces that we use as well as a method to extract them from sampled density field with a sub - pixel precision ( focusing more specifically on the filaments in the 2d and 3d case ) . in section [ sec : applications ] , we use this formalism to study the time evolution of the cosmic web , and understand the change of its properties as a specific object via the truncated zeldovich approximation @xcite . finally , in section",
    "[ sec : conclusion ] , we summarize our findings and discuss a few possible applications to n - body simulations and observational spectroscopic galaxy surveys .",
    "the details of a general simplex minimization algorithm used in section [ sec : algo ] are presented in appendix a while the general behavior of the inter - skeleton pseudo - distance as defined in section [ sec : applications ] is given in appendix b.",
    "the main goal of the algorithm presented here is to allow a robust extraction of the non - local primary critical lines ( among which the skeleton ) as introduced in @xcite and @xcite . in these papers ,",
    "the skeleton was defined as the set of points that can be reached by following the gradient of the field , starting from the filament type saddle points ( i.e. those where only one eigenvalue of the hessian is positive ) .",
    "let @xmath3 be the density field , and @xmath4 its gradient at position @xmath5 , the skeleton can be retrieved by solving the following differential equation :    @xmath6    using the  filament \" type saddle points as initial boundary conditions .",
    "because of the difficulty of designing a robust algorithm to solve this equation , it was achieved only in @xmath7 in @xcite and a solution to a local approximation in @xmath8 was proposed in @xcite .",
    "this local approximation allowed the extraction of a more general set of critical lines linking critical points together , the subset of this lines linking saddle points and maxima together corresponding to the skeleton ( i.e. the `` filaments '' in the large scale distribution of matter in the universe ) .",
    "see @xcite for a discussion of these various sets .",
    "+ this method works in a very general framework and allows the extraction of a fully connected _ non - local _ skeleton as well as an extension of the primary critical lines introduced in @xcite and @xcite to a hierarchy of critical surface .",
    "following the idea , already present in equation ( [ eq : skel ] ) , that the topology of a field can be expressed in terms of the properties its field lines , it takes ground in morse theory @xcite and is roughly based on an extension of the patches theory @xcite . for a sufficiently smooth and non - degenerate field    of dimension @xmath0 , the peak patches  pp hereafter ",
    "_ void patches ",
    "vp hereafter ) are defined as the set of points from which the field lines solution of equation ( [ eq : skel ] ) all converge to the same maximum ( _ resp . _",
    "minimum ) of the field . within this framework",
    ", we shall qualitatively show that in a @xmath0-dimensional space , the skeleton can be thought of as the result of @xmath1 successive identifications of vps or , equivalently , as the one dimensional interface between at least @xmath0 vps ( an actual rigorous demonstration can be found in _ e.g. _",
    "@xcite ) . using this definition , extracting the skeleton of a distribution",
    "thus simply amounts to finding a way of robustly and consistently identifying the patches . + whether considering a particle distribution obtained from a numerical simulation or a density field sampled on a grid , the major difficulty arises from the discrete nature of the data .",
    "in fact , even if the underlying density field is supposedly smooth and continuous , the discreteness of the sampling implies a relatively large uncertainty on the precise location of the patches boundaries , as sampling is limited by computational power , which is even more true when considering higher dimensions space .",
    "the algorithm we use is an improved version of the watershed transform method @xcite , based on a probability propagation scheme and aims at attributing a probability of belonging to a given patch to every sampled point of the density field .",
    "this scheme is very general and efficient as it allows dealing with discrete dataset in a naturally continuous fashion and on manifolds of arbitrary dimensions .",
    "[ fig_algo_tile ]    the initial idea beyond our patches identification algorithm is that a patch can be defined as the set of field lines ( i.e. curves that follow the gradient of a field ) that originate from a given minimum ( vp ) or maximum ( pp ) of a field .",
    "considering a sampled field , being able to identify the patches thus amounts to being able to decide , for any given pixel @xmath9 , from which extremum all field lines that cross @xmath9 originate .",
    "it is therefore easy to understand that the discrete nature of the sampling rapidly plagues such a task : for each pixel , considering the measured gradient , one has to decide from which , in the fixed number of neighbouring pixels , the field line comes from . within a @xmath0-dimensional space ,",
    "having to select between only @xmath10 possibly different direction for field lines is a crude approximation that leads , because of accumulation , to a largely wrong answer for pixels located far away from the extrema .",
    "although we present the algorithm in the general case here , the reader can refer to figure  [ fig_algo_steps ] and its legend for a simpler and more visual explanation of the algorithm in the @xmath7 case .",
    "more generally , our algorithm involves considering each pixel of a sampled field in the order of their increasing ( _ resp . _ decreasing ) value , depending on whether we want to compute the vps or pps and , for each of them , computing the probability that it belongs to a given vp ( _ resp . _ pp ) .",
    "this probability map is simply computed by scanning the probability distribution of its @xmath11 neighbours ( within a @xmath0-dimensional space , here @xmath12 ) and deducing the current pixel patch probability distribution from it .",
    "two cases are possible :    1",
    ".   none of the neighbours has already been considered ( i.e their respective densities are all higher -_resp . _",
    "lower- than that of the current pixel ) .",
    "this means that the pixel is a local minimum ( _ resp . _",
    "maximum ) of the field : a new vp ( _ resp . _",
    "pp ) index is created and the probability that the current pixel belongs to it is set to @xmath13 .",
    "2 .   at least one neighbour has already been considered ( i.e its density is lower -_resp .",
    "_ higher- than that of the current pixel ) .",
    "the current pixel probability distribution is computed as a gradient weighted average of its lower -_resp .",
    "_ higher- density neighbours probability distributions .",
    "once all pixels have been visited , a number @xmath14 of patches have thus been created and a list of @xmath14 probabilities @xmath15,@xmath16 , has been computed for each pixel , @xmath17 .",
    "these probabilities quantify the odds that a given pixel @xmath17 belongs to a given patch @xmath18 .",
    "figure  [ fig_proba_check ] illustrates the advantages of our probability list scheme compared to the naive approach : without it , the patches borders have a strong tendency to be aligned with the sampling grid and the problem tend to get much worse when considering lower sampling and of course higher dimensions .",
    "+    , width=264 ]    figure  [ fig_vpatch ] presents the results obtained by applying this algorithm to the @xmath7 gaussian random field of figure  [ fig_dfield ] . on this picture ,",
    "each patch is assigned a different shade , and the colour of each pixel is the probability weighted average colour of its possible patches . as expected , a majority of pixels seems to belong to a definite void patch with high probability ( close to @xmath13 ) .",
    "in fact , considering two neighbouring void patches a and b , all the pixels that belong to one of these patches and have a value lower than that of the first kind saddle point(s ) on their border ( i.e. where the hessian only has one positive eigenvalue ) have a @xmath13 probability of belonging to either a or b. hence , the probabilities of belonging to different patches only starts mixing above first kind saddle points .",
    "this can be seen on the top right zoomed panel of figure  [ fig_vpatch ] where probabilities only start blending mildly for densities above this threshold ( the saddle point are represented by the probability `` nodes '' on the picture ) .",
    "this results in a complex distribution of patch index probabilities in the vicinity of higher density borders ( see upper left panel of figure  [ fig_vpatch ] ) , and thus a higher uncertainty of the location of the void patches border .",
    "this uncertainty on the precise patch index is directly linked to the location of the skeleton . in fact , as explained in the next section , the skeleton can also be defined as the set of field lines that do not belong to any patch , or in other terms , where sampled pixels have an equal probability of belonging to several distinct patches .",
    "+     +      as one can easily see , the major strengths of this simple patch extraction algorithm are that it is robust and can be trivially extended to spaces of any dimensions and topology , the only requirement being that one needs to be able to define neighbouring relationships between pixels and measure distances between them .",
    "so we now have a robust algorithm for extracting the vp and pp of , in practice , nearly arbitrary scalar fields .",
    "in this subsection , we show that it is possible to generalize the definition of the skeleton @xcite to spaces of arbitrary dimension and present a simple method to compute the skeleton , as well as critical lines and surfaces , based on our patches extraction algorithm .",
    "+      let us first present important results of the morse theory without demonstrating them .",
    "the more thorough reader can refer to @xcite for a mathematical demonstration .",
    "+ let us consider the general case of a sufficiently smooth and non - degenerate @xmath0-dimensional scalar field @xmath19 , with @xmath20 and @xmath21 a manifold ( i.e @xmath22 , the sphere @xmath23 , ... ) .",
    "following @xcite , the field lines of @xmath19 fill @xmath21 and a vp can be defined as the set of points that can be reached by following the field lines originating from a given minimum of @xmath19 .",
    "the vps of @xmath19 thus segment a set of @xmath0-dimensional volumes that completely fill @xmath21 , each of them encompassing exactly one minimum of @xmath19 .",
    "the interface of the vps , @xmath24 , defines a @xmath25-surface ( i.e. a surface of dimension @xmath1 embedded in @xmath21 ) .",
    "it is therefore possible to apply our probabilistic algorithm to @xmath26 , the restriction of @xmath19 to @xmath24 , in order to extract the vps on this interface . for clarity",
    ", we will call the vps of @xmath26 the first order vps of @xmath19 , noted @xmath27-vps hereafter .",
    "recursively , the @xmath27-vps define @xmath25-dimensional volumes that pave @xmath24 , each of them encompassing , by definition of a vp , exactly one minimum of @xmath26 , with coordinates @xmath28 , and the reasoning can be applied to the whole hierarchy of @xmath29-vps , @xmath30 .",
    "+ starting from a @xmath0-dimensional @xmath31 scalar field @xmath19 , it is thus possible to define a complete hierarchy of sets of @xmath29-vps , @xmath32 .",
    "these @xmath29-vps are @xmath33-dimensional volumes that partition @xmath34 , where @xmath34 is defined as the @xmath33-dimensional interface of the @xmath35-patches .",
    "each set of @xmath29-vps is defined as the set of void patches of @xmath36 , the restriction of @xmath19 to @xmath34 .",
    "let us call a critical point , @xmath37 , of kind @xmath2 a critical point with morse index @xmath38 ( _ i.e. _ where the hessian @xmath39 has exactly @xmath2 positive eigenvalues ) .",
    "then , @xmath34 encompasses the whole set of saddle points of kind @xmath40 , of @xmath19 , the minima of @xmath36 associated to each @xmath29-patch being the saddle points of @xmath19 of kind @xmath41 .",
    "the interface @xmath42 is thus a curve embedded in @xmath21 that links the maxima of @xmath19 to its saddle points of kind @xmath27 : the skeleton of @xmath19 .",
    "it is interesting to note that this approach also allows a rigorous definition of the whole set of critical lines similar to the one introduced with the local approximation of the skeleton in @xcite ( see also @xcite ) , as well as their extension to critical hyper - surfaces of any number of dimensions .",
    "+ although we have only addressed the @xmath29-vps case so far , the exact same argumentation holds for the whole hierarchy of @xmath29-pps , which leads to @xmath21 being the skeleton of the voids that links minima to saddle points of kind @xmath1 .",
    "moreover , alternating a selection of @xmath43 @xmath44-vps and @xmath45 @xmath46-vps , @xmath47 , leads to @xmath21 being the curve that links saddle points of kind @xmath45 to saddle points of kind @xmath48 : a peculiar set of critical lines of the field .",
    "one can note that , as rigorously demonstrated in morse theory @xcite , critical lines defined in such a way can only link critical points whose morse index only differ by unity .",
    "the representation of the critical lines of a given scalar field as a peculiar limit of a peak or void patches hierarchy certainly has some mathematical appeal . from a practical point of view ,",
    "although apparently straightforward , its direct numerical implementation can nevertheless be somewhat problematic .",
    "let @xmath49 be an initial sampling grid and @xmath50 its reciprocal ( i.e. @xmath49 shifted by half the size of the pixels in every direction ) . using our patch computation algorithm on a scalar field @xmath19 sampled over @xmath49 , we obtain for every pixel , @xmath17 , of @xmath49 a probability @xmath15 that it belongs to a given patch , @xmath18 .",
    "those sets of probability distributions could be used to define a border between the patches and thus to compute the 1-pps and 1-vps .",
    "nevertheless , this is in general not an easy task : one in fact first needs a very precise localization of the 1-pps and 1-vps ( those living on the ( hyper-)surface of the initial vps or pps ) to be able to compute the following segmentation of the hierarchy ( as opposed to a density probability ) . in order to overcome this issue",
    ", we chose first to base our implementation on a subset only of the different patches probabilities and only keep for every pixel the index of its most probable patch .",
    "this way , we are able to simply define the borders between patches as the set of pixels of @xmath50 that overlap at least @xmath51 pixels of @xmath49 with different most probable patch index",
    ". the patches extraction algorithm can then be applied again over that border , restraining pixels examination to the ones that lie on its surface .",
    "identifying pixels of @xmath49 that overlap at least @xmath51 pixels of @xmath50 with different most probable patch index , one can thus identify the 2-pp or 2-vp and , repeating this procedure , all orders of the patches hierarchy . + for 2d gaussian random fields , as pictured on figure  [ fig_ppatch ] and [ fig_vpatch ] , the skeleton ( _ resp .",
    "_ anti - skeleton ) are identical to the vp ( _ resp",
    ". _ pp ) borders and the direct implementation of this algorithm leads to a very precise and smooth skeleton .",
    "but the implementation in spaces of higher dimensions raises a critical issue with this simplified method , due to the fact that the borders of the @xmath29-pps and @xmath29-vps are only defined by the index of the pixels they cross : thus they are jagged and considered locally flat ( on the scale of one pixel and its direct neighbourhood ) .",
    "figure  [ fig_1vp ] presents the 1-vps obtained by applying this algorithm to a 3d gaussian random field , each colour corresponding to a different 1-vp index .",
    "the 1-vps live on the 2d surface which is the border between the cells formed by the void patches of the field , each of this cell encompassing exactly one minimum of the field .",
    "this surface is complex : it can be multiply connected at the interface of more than two different void patches and its curvature is locally significant .",
    "although neighbouring relationships between pixels are easily obtained even where the surface is multiply connected , only a rough approximation of the actual distances along the surface can be computed , as the local curvature is not taken into account . figure  [ fig_sklrecurse ] shows the corresponding skeleton , computed as the border of the 1-vps of figure  [ fig_1vp ] .",
    "this skeleton is clearly not very well defined , the uncertainty in distance computation leading to errors in the probability propagation algorithm .",
    "this bias results in multiple skeleton branches that seem to oscillate and cross each other along the true skeleton location .",
    "+ in the end , it appears that dropping the full probability distribution and approximating borders between patches is too coarse an approximation .",
    "one solution would involve trying to compute the precise location of the @xmath29-vps and @xmath29-pps using the full set of probabilities , but , as it will be discussed in section [ sec : smooth ] , this raises complex issues . as it is the patches interface computation that seems to be difficult , the alternative we chose to implement involves computing directly the skeleton from the @xmath52vps and @xmath52pps of the field , without having to consider the full hierarchy of @xmath29-vps and @xmath29-vps .",
    "a close examination of figure  [ fig_1vp ] led us to formulate the conjecture that the @xmath53-vps or @xmath53-pps interface corresponds in fact to the subspace of @xmath24 where the manifold is sufficiently multiply connected ( i.e. where the @xmath25-surface defined by @xmath24 folds onto itself ) .",
    "equivalently , this locus can be defined in @xmath8 as the interface of at least @xmath54 different pps or vps ( see figure  [ fig_1vp ] ) .",
    "this is formally demonstrated in @xcite . in the general case of @xmath55",
    ", the skeleton should thus be the 1d interface between at least @xmath0 vps or pps of @xmath19 .",
    "figure  [ fig_sklnormal ] presents the skeleton obtained using this method on the same gaussian random field as the one used for figures  [ fig_1vp ] and  [ fig_sklrecurse ] .",
    "as expected , as there is no need to recursively compute the full hierarchy of vps , the resulting skeleton is much more precise and well defined .",
    "moreover , a quick comparison to figure  [ fig_sklrecurse ] confirms that it is in fact the approximation of the @xmath29-patches interfaces by individual pixels that plagues the algorithm , each recursive step exponentially increasing the error .",
    "+      the concepts introduced above allow the definition and extraction of the skeleton as a _ fully connected _",
    "network that continuously link maxima and saddle - points of a scalar field together .",
    "it is certainly of interest to try understanding the topological and geometrical properties of this scalar field through the connectivity and hierarchy relationship that it introduces between the critical points .",
    "applied to cosmology , it also allows a formal definition of the concept of individual filaments . considering matter distribution on large scales in the universe , a natural definition of a _ single",
    "_ filament would be a subset of the cosmic web that directly links two halos together .",
    "the transposition of such a definition to the skeleton would allow the introduction of useful concepts such as neighbouring relationship between halos in the cosmic web sense .",
    "it would also make possible the study of filaments as individual physical objects , similarly to what has been done for years in the literature with the halos and voids .",
    "+ on figure  [ fig_dfield ] , the skeleton ( coloured thick network , where the colour corresponds to the underlying pp index ) and anti - skeleton ( black network ) are superimposed on the density field from which they where extracted .",
    "let us define a filament as a subset of the skeleton continuously linking two maxima together while going through one - and only one - first kind saddle point .",
    "these saddle points can be easily extracted as they are located on the skeleton , at the border between the peak ( _ resp .",
    "_ void ) patches ( i.e where the patch index along the skeleton changes , this definition being valid for any number of dimensions ) .",
    "this way , all the filaments of an n - dimensional distribution can be extracted individually by starting from each maximum of the field , following all the branches of the skeleton , and storing only the paths that cross one saddle point before reaching another maximum .",
    "this algorithm thus allows the individual extraction of filaments as well as a continuous wander of the filamentary structure of a distribution , which should be very useful in a wide range of applications in cosmology .",
    "let us first consider for simplicity a cartesian sampling grid ( even though this sub pixel smoothing does not critically depend on this geometry , see below ) .",
    "the implementation of the procedure of section  [ sec : dskel ] naturally leads to a skeleton that lives along pixel edges and is thus jagged at the pixels scale .",
    "the differentiability of the skeleton is nonetheless a feature which may be critical for a number of its characteristics : its length , curvature , general connectivity ... in order to enforce this differentiability , we developed two smoothing methods which we use in practice in turn . the first one is based on a multi - linear interpolation of the patches probability distribution which flows naturally from the original algorithm used to create the skeleton .",
    "it provides sub - pixel resolution consistently with the probabilistic framework , thus allowing a precise extraction of the skeleton even when the sampling is low .",
    "the other is used to control the level of smoothness away from fixed points ( the maxima or the bifurcation points ) and can be used to enforce sufficient differentiability .",
    "let us first find a way to obtain a sub - pixel resolution on the skeleton position based on the patches probability distribution of each pixel .",
    "the raw skeleton is made of individual segments located on the edges of the pixels of a cartesian grid @xmath49 .",
    "each segment is defined by its two end points , and each of them is surrounded by @xmath56 pixels with a full list of possible patches index , together with their respective probabilities .",
    "recall that the probabilistic algorithm we use works on individual pixels so the resulting skeleton position , defined as the position of the border between several patches , is computed with a precision of one pixel .",
    "this implies that the smoothing procedures may not move the skeleton on more than half the size of a pixel . in other words , if we consider the dual sampling grid , @xmath50 , of @xmath49 , the skeleton can be freely moved within the pixels of @xmath50 that its jagged approximation crosses . so it is sufficient to consider individually each of these pixels",
    ". let @xmath57 be one of these pixels .",
    "we then know for each of its vertices , @xmath58 with @xmath59 , the probability distribution of the different vps , @xmath15 , where k is the index of a vp . in order to obtain sub - pixel resolution",
    ", these probabilities can be interpolated within @xmath60 .    for simplicity , we will only use a multi - linear interpolation and define @xmath61 , the probability distribution of patch @xmath18 , interpolated at point @xmath62^d$ ] within @xmath60 as : @xmath63 where @xmath64 if the @xmath65 coordinate of @xmath58 within @xmath60 is @xmath27 and @xmath66 if it is @xmath67 .",
    "ideally , the skeleton should not belong to any vp , so it should be located where all the non null values of @xmath61 are equal .",
    "let us define the arithmetic mean of the probability ( over the vps with index @xmath18 ) over the pixel @xmath68 and its root mean square , @xmath69 where the sum is over all the @xmath14 subscripts @xmath18 such that there exist a pixel @xmath58 where @xmath70 .",
    "clearly , all patches with dominating probabilities @xmath61 in @xmath60 are equal when @xmath71 equation ( [ eq_pvar0 ] ) is of fourth order and is thus difficult to solve in general .      insights into the solution of equation ( [ eq_pvar0 ] ) can be found while considering the intersection sets of points where pairs of probabilities are equal instead of equating them all at the same time .",
    "these sets are solution of the set of equations @xmath72 where @xmath73 and @xmath74 are subscripts of the patches that dominate on at least one vertex of @xmath60 .    for clarity ,",
    "let us consider the @xmath12 case first . with a proper indexing of the four pixels @xmath58 , @xmath75 equation ( [ eq_equalprob ] )",
    "writes in this case : @xmath76 where @xmath77 , @xmath78 , @xmath79 and @xmath80 only depend on the values of @xmath15 .",
    "equation ( [ eq_quadric2d ] ) is quadratic and its solutions are well known curves of dimension @xmath81 called quadrics .",
    "figure  [ fig_propersmoothing ] illustrates solutions of equation ( [ eq_quadric2d ] ) when @xmath60 is located at the intersection of @xmath82 , @xmath83 or @xmath84 different vps . in the most frequent configuration where @xmath60 is at the intersection of @xmath51 vps , equation ( [ eq_quadric2d ] ) directly gives the first order approximation of the intersection of the skeleton and @xmath60 , and we may approximate it by a straight segment . finding the end points of this segment",
    "is easily achieved by computing the location of equal probability along the two sides of @xmath60 that link vertices with different patches ( figure  [ fig_smooth2p ] ) .",
    "the @xmath83 configuration is rarer , and concerns only the maxima of the field as well as all bifurcation points of the skeleton . in this case , we know that three different branches of the skeleton merge within the pixel , at a point where all probabilities are equal .",
    "so , we may set the bifurcation point as the locus where all the @xmath85 quadrics of equation ( [ eq_equalprob ] ) intersect ( note that the three of them always intersect in a single point as @xmath86 and @xmath87 implies @xmath88 ) . the three branches of the skeleton in @xmath60 are thus obtained by linking the bifurcation point to the iso - probability along the three sides of @xmath60 that link vertices associated to different patches ( figure  [ fig_smooth3p ] ) .",
    "finally , the @xmath84 configuration is very rare and also more problematic . as previously , we know that there exist a bifurcation point within @xmath60 , but this time with @xmath89 different skeleton branches .",
    "since there are now @xmath90 different equations ( [ eq_equalprob ] ) , and given that the solution of each of them is a 1d quadric , this system is clearly over constrained to find the precise location of the bifurcation point . a solution may well be to use a higher order interpolation , allowing more complex curves than quadrics for equal probabilities regions , or to try solving directly equation ( [ eq_pvar0 ] ) . as this case",
    "is clearly rare , it would also be possible to approximate the bifurcation point as the barycenter of the three points of intersection of the subsets of equations ( [ eq_equalprob ] ) taken in pairs .",
    "again , the smoothed skeleton would therefore be derived by linking the bifurcation point to the four iso - probability points along the four sides of @xmath60 ( figure  [ fig_smooth4p ] ) .",
    "having discussed the underlying geometry of the sub - pixel multi linear interpolation , let us now turn to our actual sub - pixel smoothing algorithm .",
    "indeed , in @xmath0-dimensions , equation ( [ eq_equalprob ] ) is of order @xmath0 and is linear in each of the @xmath0 space coordinates @xmath91 .",
    "its solutions are thus @xmath1 dimensional quadrics whose intersections , as in @xmath7 , can be used to recover the skeleton position down to a sub - pixel precision .",
    "finding intersections of quadrics in general remains nonetheless a highly difficult ( or even untractable ! ) problem and even state - of - the - art solvers can only achieve such a performance for @xmath92 at most . to circumvent this difficulty , we thus opted in practice for a different solution that consists in a recursive numerical minimization of the value of @xmath93 over the hierarchy of n - cubes ( i.e hypercubes of dimension @xmath2 ) , @xmath94 , that are the faces of each cell of the sampling grid .",
    "the trick is to always reduce the problem to a 1d minimization of a polynomial of order @xmath0 ( see appendix a ) .",
    "figure  [ fig_subp ] illustrates the full process in 3d .",
    "let us consider the grid cell of figure  [ fig_subp1 ] , located at the interface of @xmath89 different patches .",
    "the skeleton extraction algorithm produces the jagged skeleton represented in red . in order to improve its resolution",
    ", we first consider each of the @xmath95 edges individually ( see figure  [ fig_subp2 ] ) and determine for each of them the point of equal probability for the two patches that dominate at the end points of segment .",
    "of course this point only exists if different patches dominate at the end points of a segment and we thus obtain at most @xmath95 new points ( @xmath96 in this instance , represented by the red crosses ) .",
    "the edges of a cube can be considered as its `` one dimensional faces '' or @xmath27-faces .",
    "the following step consists in examining the configuration of its @xmath51-faces , usually called faces for a 3d cube .",
    "figure  [ fig_subp3 ] illustrates the configuration of these @xmath97 faces together with the iso - probability points computed over their edges .",
    "we know that at least @xmath54 different patches have to dominate on at least one of the @xmath89 vertices of each face for a skeleton branch to enter the cell through this face . using the minimization algorithm presented in appendix a and the iso - probability points on the edges",
    ", it is thus possible to compute , over these faces , the location of the minimum of @xmath98 ( represented as blue crosses on figure  [ fig_subp3 ] ) . finally , considering the @xmath54-face of the cell ( i.e. the cube itself ) , one can determine the point of minimal value of @xmath98 over the cube , which is the point where the skeleton branches connect ( see figure  [ fig_subp4 ] ) .",
    "+ the generalization of this algorithm is relatively straightforward .",
    "let us again consider a cell that is a hypercube of dimension @xmath0 .",
    "we know that the skeleton intersects this cell if at least @xmath0 of its vertices have different maximal probability patches index . in that case , the sub - pixel resolved skeleton can be recovered by considering all the @xmath2-faces of the hypercube , @xmath99 , in ascending order of their dimension @xmath2 . when considering a @xmath9-face , we minimize the value of @xmath98 in order to obtain the point where its vertices respective patches have equal probability , using the points obtained from the @xmath100-faces .",
    "this point only exist for a @xmath9-face if at least @xmath101 vertices most probably belong to different patches . in the end ,",
    "one thus obtains a number of points from the @xmath25-faces that are the points where the skeleton enters the cell and one point for the @xmath0-face ( i.e. the cell itself ) , which gives the location where different branches of the skeleton connect .",
    "figure  [ fig_sklpost_2d ] illustrates the result of applying this algorithm in the @xmath7 case .    ,",
    "width=264 ]      , while the pixels colour represents their dominating patches and the initial raw skeleton is represented in red .",
    "the green skeleton is the result of applying the sub - pixel resolution algorithm while the blue one was obtained from the green one , after removing one pixel sized loops and smoothing.[fig_subp_fail],width=264 ]    though the method presented above to obtain sub - pixel resolution works most of the time , there nonetheless exist situations where it can fail due to sampling effects .",
    "figure  [ fig_subp_fail ] illustrates such a situation , which can sometimes occur when the sampling grid pixel size is not totally negligible compared to the average extension of the patches . when the thickness of a peak or void patch is smaller than a pixel size , it can in fact lead to mistakingly isolated subregions of size one pixel , implying the creation of spurious loops in the skeleton ( in red ) .",
    "this phenomenon , although rare , occurs in spaces of arbitrary dimension and triggers artifacts when applying our sub - pixel resolution algorithm .",
    "the green skeleton on figure  [ fig_subp_fail ] presents such an example of a spurious skeleton loop . + in order to fix these anomalous segments , we chose to post - treat the skeletons by opening - up all one - pixel sized loops and smooth the resulting skeleton to enforce a desired level of differentiability in the skeleton trajectory ( see the blue skeleton of figure  [ fig_subp_fail ] ) .",
    "the smoothing method that we use presents the advantage of being quite robust , and involve fixing some specific points of the skeleton , and averaging the position of each non - fixed segments end points with the position of its closest neighbouring end points a number of times .",
    "let @xmath102 be the @xmath65 coordinates of the @xmath103 sampled skeleton location ( among @xmath14 ) between two fixed points .",
    "before smoothing , all @xmath102 are located on the edges of @xmath49 and we can define their smoothed counterparts as @xmath104 , computed as : @xmath105 with @xmath106 where equation ( [ eq_smoothing ] ) is applied @xmath107 times in order to smooth over @xmath107 segments . basically , equation ( [ eq_smoothing ] ) is used to compute smoothed coordinates @xmath104 as a weighted average of the original coordinates @xmath102 together with the coordinates of its @xmath51 direct neighbours , @xmath108 and @xmath109 . applying this scheme @xmath107 times thus produces the final smoothed coordinates @xmath104 to be a weighted average of @xmath102 and its @xmath107 closest neighbours along the skeleton .",
    "this smoothing technique introduces two parameters of importance : the skeleton smoothing length @xmath107 , and the type of fixed points . in order to determine the optimal value of @xmath107 , it is possible to minimize the reduced @xmath110 corresponding to the discrepancy between @xmath104 and @xmath102 supplemented by a penalty corresponding to the total length of the skeleton ( over - smoothing will increases the discrepancy , under - smoothing will increase the total length ) . in practice , though , as a post treatment to an already smooth skeleton ( using the sub pixel probabilities ) , this choice is not critical .",
    "the choice of the skeleton points that should be fixed before smoothing depends of the planned application ; in practice , we implemented two possibilities : ( i ) fixing the field extrema , or ( ii ) the bifurcation points of the skeleton ( i.e the points of the skeleton where two filaments merge into one ) .",
    "figure  [ fig_smoothing ] illustrates the influence of this choice on the shape of the smoothed skeleton . by fixing the extrema of the field , one ensures that the skeleton subsets that link these extrema are treated independently : this is the solution used to study the properties of individual filaments in the dark matter distribution on cosmological scales .",
    "one should note that , in this case , the parts of the skeleton that belong to several individual filaments are duplicated ( see the red skeleton on figure  [ fig_smoothing ] ) , affecting global properties of the skeleton such as its total skeleton length .",
    "in contrast , fixing bifurcation points enforces the smoothing of the skeleton while conserving its global properties .",
    "times , while fixing the field maxima and the bifurcation ( i.e multiply connected ) points respectively . in both cases ,",
    "the smoothed versions always stay within half the size of a pixel distance from the original non - smoothed skeleton . on this illustration ,",
    "the smoothed skeleton was computed _ directly _ from its raw jagged version to emphasize the effect of the choice of different fixed points .",
    "this discrepancy between the two options is considerably weakened if the skeleton is previously post - treated .",
    "the background colour corresponds to the weighted probability each pixel has to belong to a definite patch.[fig_smoothing],width=264 ]          let us finally recap the main steps involved in the extraction of the fully connected skeleton in a @xmath0-dimensional space .    1 .",
    "the density field is sampled and smoothed in order to ensure sufficient differentiability",
    ". a smoothing scale of at least @xmath111 pixels is recommended when using a cartesian grid .",
    "all pixels are considered in the order of their ascending ( or descending ) density .",
    "depending on their neighbours , they are labelled as minima ( or maxima ) , or assigned a list of probability to belong to a given vp ( or pp ) following the algorithm of section [ sec : patch ] .",
    "3 .   considering only the patch index with highest probability for each pixel , skeleton segments are created on pixel edges when at least @xmath0 surrounding pixels among @xmath56 have a different most probable patch index .",
    "4 .   calling a vertex connected to more than two segments a node of the skeleton and considering each node , the sets of connected segments that link them to other nodes are recorded in order to later recover the information on the skeleton connectivity ( and allow a continuous wander along the fully connected skeleton ) .",
    "the sub - pixel smoothing procedure of sec .",
    "[ sec : smooth - algo ] is implemented .",
    "all the vertices of the skeleton segments are considered one by one together with the value of the probability distribution in the center of the surrounding pixels . according to the sub - pixel algorithm ,",
    "the extremities are moved in order to obtain a differentiable skeleton .",
    "configurations that are identified as problematic are corrected for following the method described in section [ sec : artifact ] , and the resulting skeleton is smoothed over a few pixels ( usually @xmath0 of them ) while fixing either bifurcation points or maxima / minima .",
    "eventually , individual filaments can be extracted ( and tagged ) following the method of section [ sec : filaments ] .",
    "figures  [ fig_3dskl_proj ] and [ fig_3dskl_simu ] show a 3d skeleton computed from a simulated density field at @xmath112 , sampled over only @xmath113 pixels .",
    "+     mpc box with gadget-2 .",
    "this @xmath114 mpc thick section of skeleton was computed from a @xmath113 pixels sampling grid smoothed over @xmath111 pixels ( @xmath115 mpc ) .",
    "the skeleton colour represents the index of the peak patch .",
    "note that the 2d projection of a 3d skeleton differs from the skeleton of the 2d projection , hence the discrepancy between the skeleton and apparent filaments .",
    "[ fig_3dskl_proj],width=321 ]        note that in this paper , we did not address the issue of shot noise that has for long been known to be a problem for most segmentation algorithms , and in particular for watershed techniques ( see _ e.g. _",
    "@xcite for a review on the subject ) .",
    "in fact , shot noise often leads to over segmentation , and numerous complex techniques have been developed to try and compensate for it .",
    "instead , we chose here to follow the approach used in @xcite , @xcite and @xcite , that involve simply filtering the sampled fields using a gaussian kernel on large enough scales ( in terms of number of sampled pixels ) so that it is possible to consider that the sampled field is a smooth enough representation of the underlying field .",
    "a clear disadvantage of this method is that it introduces a particular smoothing scale and thus adds one parameter ( the smoothing scale ) to take into account when considering sets of critical lines an surfaces computed on a field .",
    "a short study of the robustness of the skeleton extraction in the case of a smoothed scalar field is presented in appendix c. improvements over this shortcoming are postponed to further investigations .",
    "+ regarding performance , the computing time and memory consumption for the extraction of the skeleton mainly depends on three parameters : the number of pixels @xmath116 , the smoothing length @xmath117 in units of pixel size and the number of dimensions @xmath118 .",
    "most of the computational power is spent during the first step of the process : the propagation of the probabilities to compute the patches . for a constant value of @xmath117 and @xmath118 ,",
    "the algorithm speed is linear in @xmath116 , and so is the memory consumption .",
    "a smaller value of @xmath117 implies more smaller patches , which therefore have proportionally more borders with each other thus increasing the number of different probabilities to propagate .",
    "indeed , for very small values of @xmath117 , memory consumption is largely increased as well as the computational time ; it seems reasonable to keep @xmath117 above a minimal threshold of @xmath119 pixels ( which in any case is also necessary to ensure sufficient differentiability of the sampled field ) .",
    "finally , the value of @xmath118 is most critical to memory consumption and speed , not only because @xmath116 should increase with @xmath118 to keep a constant sampling resolution , but also because the number of neighbours for each pixels scales as @xmath120 for a cartesian grid . the computational time and",
    "the memory consumption follows , as the number of different probabilities to keep track of is also much increased ( each pixels having many more neighbours , the ratio of patches interface surface to their volume increases and so does the number of different probabilities to propagate , on larger distances ) . for the different skeletons presented in this paper , to give and order of magnitude , for a single modern cpu , @xmath7 skeletons of @xmath121 pixels smoothed over @xmath122 pixels are computed in a matter of few seconds and the memory needed is of order @xmath123 mbytes . computing a @xmath8 skeleton on a @xmath113 pixels",
    "grid with @xmath124 takes approximately @xmath27 minute and a hundred of megabytes of memory , while for a @xmath125 grid , it takes about an hour and around @xmath126 gbytes of memory are used . while @xmath127 skeletons are still tractable at a descent resolution , higher dimensionality seems difficult to reach with present facilities without implementing a fully parallel version of the code .",
    "the scope of application of the algorithm presented in sec .",
    "[ sec : algo ] is vast ( see sec .  [",
    "sec : conclusion ] for a discussion ) .",
    "here we shall focus on a simple example which makes use of one of the clear virtues of the above implementation : it allows us to identify as physical objects the filaments present in the matter distribution on cosmological scales , and see how these objects evolve with time .",
    "specifically , we intend to show , using the skeleton as a diagnostic tool , that a relatively simple but powerful model , namely the truncated zeldovich approximation mapping @xcite , can capture the main features of the _ cosmic evolution _ of the web . indeed predicting the evolution of matter distribution from the point of view of the topology and",
    "the geometry of the cosmic web has been a recurrent issue in cosmology ( e.g. @xcite ) and is becoming critical as the geometry of the cosmic environment is now believed to play a key role in shaping galaxies ( see , e.g. @xcite ) .",
    "being able to carry such an extrapolation from the initial condition to the present day distribution of filaments should lead to a simplified and broader understanding of large scale structures in the universe , in the same way the concept of clusters as important physical objects gave birth to the hierarchical model of structure formation .",
    "the fully connected skeleton encompasses both the geometry and the topology of the cosmic web : it is therefore the ideal tool to validate this mapping between the initial condition and the present day distribution of filaments .",
    "understanding and partially correcting for the distorsion induced by the proper motions of the structures is also of prime importance when dealing with observationnal data sets ( see e.g. @xcite ) .",
    "the principle of the zeldovich approximation ( za hereafter ) is to make a first order approximation , in lagrangian coordinates , of the motion of the collisionless dark matter ( dm ) particles .",
    "the motion of these particles from the initial mass distribution in lagrangian coordinates @xmath128 to their eulerian coordinates @xmath129 can therefore be described as : @xmath130 where @xmath131 is the redshift , @xmath132 the growth factor , and @xmath133 the displacement field , computed in the initial matter distribution as : @xmath134 where @xmath135 is the hubble constant , @xmath136 the quantity of energy in the universe , @xmath137 the gravitational potential and the subscript `` in '' stands for initial conditions .",
    "the truncated zeldovich approximation simply consists in filtering short scale modes of the initial power spectrum before computing the displacement field in order to prevent shell crossing effects .",
    "it has been shown to improve the precision of the approximation @xcite . as we are mainly interested in the large scale behavior of the cosmic web , the smoothing scale , @xmath138 mpc , that we use hereafter to compute @xmath139 roughly corresponds to the scale of non linearity at @xmath112 , as the so called _ truncated _",
    "zeldovich approximation has been shown to work best above this scale @xcite .",
    "it was computed as the scale at which , in the simulation , the smoothed density field , @xmath140 , is such that @xmath141 at @xmath112 .",
    "+      the numerical simulation that we use in this section was computed with the publicly available n - body code gadget2 @xcite .",
    "it corresponds to a dark matter only cosmological simulation of @xmath125 particles within a @xmath142 mpc cubic box , considering a @xmath143cdm concordant model ( @xmath144 , @xmath145 , @xmath146 , @xmath147 & @xmath148 ) . in order to study the evolution of the cosmic web , a set of reference skeletons , @xmath149 ,",
    "were computed from different snapshots , at redshift @xmath150 , where @xmath151 corresponds to the redshift of the initial conditions of the simulation .",
    "these skeletons were computed on density fields generated by sampling the particle distribution of the respective snapshots on a @xmath125 grid and after smoothing with a gaussian kernel of size @xmath138 . in order to understand if the truncated zeldovich approximation is able to capture the essential features of cosmic web , these skeleton are compared to different sets of skeletons , generated using the truncated zeldovich approximation in different ways :    * @xmath152 : this set of skeletons is generated by applying the zeldovich approximation to the dm particles of the simulation in the initial conditions .",
    "the displacement field is computed after smoothing over the scale @xmath153 and the resulting distribution is sampled and smoothed over the same scale to generate the skeletons . + * @xmath154 : these skeletons are generated by applying the zeldovich approximation directly to the skeleton of the initial conditions . the initial condition simulation ( @xmath155 ) is sampled and smoothed over the scale @xmath153 to compute its skeleton . the displacement field is computed on the same field , but smoothed over a scale @xmath156 mpc ( such that @xmath157 at @xmath112 ) and the zeldovich approximation is applied to each segment of the initial condition skeleton .",
    "we use a larger truncation scale for the zeldovich approximation here in order to prevent shell crossing , which can be tolerated when applied to particles but would result in a very fuzzy displaced skeleton . + * @xmath158 : same as @xmath159 , but with a displacement field smoothed over the scale @xmath160 , in order to check the influence of this choice on @xmath154 . + * @xmath161 : same as @xmath159 , but the sampled field is smoothed on a scale @xmath162 instead of @xmath153 to take into account that the zeldovich approximation introduces an artificial additional smoothing scale ( see below ) .      .",
    "the length density was measured on the simulation ( _ purple discs _ ) , its truncated zeldovich approximation whose displacement field was computed using a smoothing length @xmath163 such that @xmath164 ( _ red squares _ ) , or @xmath165 such that @xmath166 ( _ green crosses _ ) , and finally using the displacement field of the zeldovich approximation at scale @xmath167 , applied directly to the skeleton of the initial condition , at @xmath168 ( _ blue triangles _ ) . the black dashed line stands for the length of the skeleton in the initial conditions ( at @xmath168 ) , while the dotted line represents the length measured using the zeldovich approximation on the initial condition while taking into account the effective smoothing introduced by using the zeldovich approximation .",
    "this recipe yields the best match with the simulation . except for this last case , the skeletons where computed after smoothing the density field with a gaussian kernel of width @xmath117 .",
    "[ fig_skl_length],width=340 ]    there exist many different ways to compare one dimensional sets of lines within a @xmath8 space , but one of the simplest certainly involves comparing their lengths .",
    "figure  [ fig_skl_length ] presents the measured length per unit volume of the different sets of skeletons ( described above ) as a function of redshift .",
    "let us first consider the length of @xmath169 ( _ purple curve with discs symbols _ ) .",
    "it was shown in @xcite and @xcite that , whereas for scale invariant fields such as the initial conditions of the simulation , the length of the skeleton is expected to grow as @xmath170 ( @xmath117 being the smoothing length ) , it grows in fact as @xmath171 around @xmath112 for @xmath143cdm simulation .",
    "note that the fact that the length of @xmath169 decreases with time seems consistent with the expected evolution of matter distribution in the case a cosmological constant , where the expansion accelerates around @xmath172 . in that case ,",
    "matter in fact tends to form separate distant heavy halos : more numerous small filaments on smaller scales shrink and melt into each other as dark matter halos merge , while larger scale filaments tend to stretch : the net result is a total length decrease .",
    "this process seems to be well captured by the zeldovich approximation as the length of @xmath152 ( _ red curve , square markers _ ) exhibits the same time evolution as the length of @xmath173 .",
    "the discrepancy between the measured length in the simulation and with zeldovich s approximation is nonetheless of the order of @xmath174 at @xmath112 .",
    "this disagreement should be explained in part by the fact that the zeldovich approximation uses a displacement field computed from a smoothed version of the initial condition density field , thus introducing an additional smoothing that one should take into account when computing @xmath175 .",
    "+    , width=302 ]    the measure of the ratio , @xmath176 , of the length of @xmath177 to the length of @xmath178 as a function of time , @xmath179 , is displayed on figure  [ fig_fit_r_a ] .",
    "it appears that @xmath176 is approximately a linear function of time , @xmath179 , and can thus be fitted as @xmath180 where @xmath181 is the time of the initial conditions from which the zeldovich approximation was computed . moreover",
    ", the fact that the value of @xmath176 is relatively close to unity confirms that the artificial smoothing introduced by the zeldovich approximation is small ; we chose to model it as a convolution with a gaussian kernel of size @xmath182 .",
    "the effective gaussian smoothing used on zeldovich s approximation has scale @xmath183 and is thus the result of the composition of two gaussian smoothing of scale @xmath182 and @xmath153 : @xmath184 using equations ( [ eq_rfit ] ) and ( [ eq_leff ] ) , and the fact that the skeleton length grows with smoothing scale as @xmath171 @xcite , the value of @xmath182 one should chose to get the best match with @xmath143cdm simulations is thus @xmath185 in order to compute a skeleton that is comparable to @xmath169 , one should therefore smooth the distribution obtained using the zeldovich approximation on a scale @xmath162 such that @xmath186 on figure  [ fig_skl_length ] , the dotted black curve represents the measure of the length of @xmath161 , when the effective smoothing introduced by the zeldovich approximation is taken into account . the agreement with the measurements in the simulation",
    "is significantly improved compared to the naive approach ; this suggest that the zeldovich approximation can be used to predict the shape of the evolved cosmic web from the initial conditions distribution only .",
    "of course , the length is only a global characteristic of the skeleton and it certainly does not fully constraint its shape .",
    "higher order estimators that can compare the relative position and shapes of skeletons are needed to quantify how good an approximation the skeleton obtained by zeldovich s approximation is . +",
    "before doing so , let us consider an alternative form of the zeldovich approximation , where , instead of displacing the particles from the initial conditions of the simulation to derive the evolved density field , we directly use the displacement field to evolve the skeleton of the initial conditions .",
    "this method will be called here the skeleton zeldovich approximation ( sza hereafter ) , and the resulting skeleton @xmath187 . studying the properties of @xmath188",
    "is interesting as it should make it possible to distinguish between two different processes that affect the properties of the cosmic web : the simple deformation of the initial cosmic web on the one hand and the creation or annihilation of filaments on the other hand .",
    "indeed , @xmath187 reflects only the modification of the skeleton due to its deformation while @xmath189 also takes into account the merging and annihilation of filaments .",
    "note nonetheless that by definition , the locus of the skeleton for the sza is biased toward higher density regions ; in these regions , non - linear effects inducing shell - crossings in the zeldovich approximation are more likely . to be conservative",
    ", we thus use a larger smoothing length than @xmath153 to compute the displacement field .",
    "this smoothing length , @xmath190 mpc , was chosen such that @xmath191 ; the green curve ( _ cross markers _ ) of figure  [ fig_skl_length ] shows that using @xmath160 or @xmath153 does not make any difference regarding the length of the skeleton . on this figure , the blue curve ( _ triangle markers _ ) depicts the evolution of the length of @xmath192 : its behavior is clearly opposite to the @xmath189 case , as the length rises with time .",
    "although surprising at first sight , this result only confirms our previous interpretation of the evolution of the cosmic web .",
    "in fact , if the sza can nicely capture the large scale evolution of long filaments , the smaller ones can not melt into each other , which induces several small scale filaments to be located at the same loci , where only one piece of filaments should have been measured .",
    "the disappearance of the smaller scale filaments does not compensate anymore for the expansion of large scale filament : the net result is thus an increase of the total measured length of @xmath187 with time .",
    "let us now define a way to compute a pseudo - distance between two different skeletons ( see also @xcite ) .",
    "in practice , a skeleton @xmath193 is always computed from a sampled density and thus has a maximal resolution @xmath194 .",
    "it can therefore be described , without loss of information , as the union of a set of @xmath14 straight segments @xmath195 of size @xmath194 .",
    "we define a pseudo - distance from a skeleton @xmath196 to a skeleton @xmath197 , @xmath198 , as the probability distribution function ( pdf ) of the minimal distance from the @xmath199 segments @xmath200 to any of the @xmath201 segments @xmath202 . in practice ,",
    "our algorithm applied to a density field sampled on a cartesian grid naturally leads to a skeleton described as a set of segments of size the order of the sampling resolution .",
    "hence we directly use these segments to compute inter - skeleton distances .",
    "note that there is no reason , in general , for @xmath198 to be identical to @xmath203 ; this discrepancy , together with the value of the different modes of the pdfs , do in fact quantify the differences between @xmath196 and @xmath197 ( see appendix b for details on how to interpret pseudo - distances pdfs ) .",
    "the upper and lower panels of figure  [ fig_skl_dist ] present the pseudo distance measurements obtained by comparing @xmath204 to @xmath175 and @xmath187 respectively .",
    "a close examination of figure  [ fig_dist_z_t ] confirms the hypothesis we made in previous subsection .",
    "first , the high correlation of @xmath175 and @xmath204 ( _ bold curves _ ) for any redshift , is demonstrated by the localization of the mode around @xmath205 kpc , well below the smoothing length @xmath206 mpc .",
    "second , the asymmetry between the pdfs of @xmath207 and @xmath208 follows from the fact that @xmath209 has smaller scale filaments that have no counterpart in @xmath189 ( the mode intensity is higher for @xmath207 than @xmath208 ) .",
    "this is exactly what should happen if @xmath175 was effectively smoothed on a scale larger than @xmath204 .",
    "the thin curves , for which the effective zeldovich approximation smoothing was taken into account , confirms this , as the asymmetry is completely removed in that case .",
    "+ it is also interesting to look at the distance pdfs between @xmath187 and @xmath204 ( see figure  [ fig_dist_zd_t ] ) . except for high redshifts ( @xmath210 ) ,",
    "the general intensity of the modes are lower for @xmath211 than for @xmath207 , suggesting that the zeldovich approximation is a better description of the evolution of the filaments on large scales , and that filaments mergers and creation are important processes .",
    "the general position of the modes is still comparable , which means that sza is nonetheless successful in describing the evolution of the general shape of the cosmic web .",
    "also , the asymmetry between @xmath211 and @xmath212 suggests that @xmath187 has more small scale filaments than @xmath204 .",
    "these observations confirm our previous assumption that although the cosmic web evolves in a simple inertial way on larger scales ( a process captured by sza ) , the shrinking and fusion of the more numerous smaller scale filaments is the cause of the general length decrease of the cosmic web ( as suggested by a simple visual examination of a @xmath213 @xmath214 subregion of @xmath215 , @xmath187 and @xmath175 on figure  [ fig_cmp_zeldo3d ] ) .",
    "+        the above investigation opens the prospect of correcting for the peculiar velocities of galaxies induced by gravitational clustering , and carry an alcock - paczynski ( ap ) test @xcite on the skeleton of the large scale structures of the universe . in short , the ap test compares observed transverse and longitudinal distances to constrain the global geometry of the universe .",
    "galaxy positions are usually observed in redshift space which induces an important distortion between the distances measured along and orthogonally to the line of sight , which plagues the regular ap - test .",
    "our analysis suggests that it is in fact possible to correct through the zeldovich approximation for the distortions induced on the cosmic web .",
    "having carried such a correction , we expect that the measure of the anisotropy of the observed skeleton length ratio could yield a good constraint on the value of the cosmological parameters .",
    "note finally that the zeldovich mapping smoothed with @xmath162 ( see equation  ( [ eq : deflcor ] ) ) could be used to generate synthetically sets of extremely large cosmic skeletons probing exotic cosmologies using codes such as mpgrafic @xcite to generate the initial conditions and their zeldovich displacement .",
    "this construction could then be populated with halos and substructure using semi analytical models .",
    "note finally that the total length and the skeleton s distance are two probes amongst many on how to characterize the difference between two skeletons .",
    "moreover , there are other means to quantify the evolution of the cosmic web . for instance",
    ", an interesting statistics would be to find out how often the reconnection of the skeleton occurs as a function of redshift .",
    "we have presented a method , based on an improved watershed technique , to efficicently compute the full hierarchy of critical subsets from a density field within spaces of arbitrary dimensions .",
    "our algorithm uses a fast one pass probability propagation scheme that is able to improve significantly the quality of the segmentation by circumventing the discreteness of the sampling .",
    "we showed that , following morse theory , a recursive segmentation of space yields , for a @xmath0-dimensional space , a succession of @xmath1 @xmath2-dimensional subspaces that characterize the topology of the density field . in 3d for cosmological matter density distribution",
    ", we particularly focused on the 3d subspaces which are the peak and void patches of the field ( i.e. the attraction / repulsion pools ) and the 1d critical lines which trace the filaments as well as the whole primary cosmic web structure ( _ i.e. _ a _ fully - connected _ , non - local skeleton as defined in @xcite ) .",
    "for the primary critical lines , we also demonstrated that it is possible to use the probabilities distribution from our algorithm to derive a smooth and differentiable skeleton with a sub - pixel level resolution .",
    "thus this method allows us to consider the cosmic web as a precise physical object and makes it possible to compute any of its properties such as length , curvature , halos connectivity etc ... + as an application , we used our algorithm to study the evolution of the cosmic web , while comparing the time evolution of the skeleton ( a proxy to the cosmic web ) of a simulation , to those corresponding to different versions of its zeldovich approximation .",
    "we first compared the evolution of the respective lengths of the different skeletons and then introduced a method to compute pseudo - distances between different skeletons .",
    "this pseudo distance makes it possible to compare different features of the skeleton such as the size of their filaments and the similarity of their locations . using these measurements",
    ", we showed that two effects were competing , with net result a decrease of the cosmic length with time : a general dilation of the larger scales filaments that could be captured by a simple deformation of the skeleton of the initial conditions on the one hand , and the shrinking , fusion and disappearance of the more numerous smaller scales filaments on the other hand .",
    "we also showed that a simple zeldovich approximation could accurately capture most features of the evolution of the cosmic web for all scales larger than a few megaparsecs ( provided an effective smoothing introduced by the approximation is taken into account ) . hence in this context ,",
    "the skeleton has proven to be a useful tool both for insight and as a quantitative probe and diagnostic .",
    "conversely , the match between the simulated and the mapped skeleton confirms and extends geometrically the ( point process elliptical ) peak patch theory @xcite since both the peaks and their frontiers ( the skeleton in 2d and the peak patch volumes in 3d ) are well mapped by the zeldovich approximation .",
    "+ the domain of interest of the skeleton is quite vast and offer the prospect of a number of applications .    from a theoretical point of view , using the points presented in this paper and in @xcite , we are presently developing a general theory of the skeleton and its statistical properties @xcite that aims to understand the properties of the critical lines of scale invariant gaussian random fields as mathematical objects .",
    "in particular , this companion paper provides quantitative analytic predictions for the length per unit volume ( resp .",
    "curvature ) of the critical lines and its scaling with the shape parameter of the field , and checks successfully the current algorithm against these . in this paper we focussed on the skeleton .",
    "one could clearly investigate on the rest of the peak patch hierarchy and measure , say , the surface or volume of the ( hyper)-surfaces of the recursion ( whose last intersection is given by the primary critical lines ) .",
    "another interesting issue would be to estimate the fraction of special ( degenerate ) points which do not satisfy the morse condition , where the fields behaves pathologically @xcite .",
    "for instance , one of the shortcoming of the present algorithm concerns special fields where critical lines disappear , a situation which occurs , say , in the context of tracing dendrites in a neural network , or blood vessels within a liver .",
    "note also that there exist other sets of ( geometrical ) critical lines that are not topological invariants such as the lines of steepest ascent connecting directly minima to maxima which are not accounted for by the present formalism .",
    "in contrast , the algorithm is well suited to identify bifurcation points , and the connectivity of the network .",
    "in particular , in an astrophysical context , it would be worthwhile to make use of this feature and study statistically how the skeleton connects onto dark matter halos as a function of , say , they mass or spin , and investigate the details of local spin accretion in the context of the cosmic web superhighways , hence completing the spin alignment measurements of @xcite on smaller scales .",
    "more generally , the algorithm provides a neat bridge , via the provided connectivity , between the theory of continuous fields on the one hand , and graph theory for discrete networks on the other .",
    "this could prove to be of importance in the context of percolation theory .",
    "for instance , the percolation threshold can be explained in terms of the properties of the connectivity of the relevant nodes ( see _ e.g. _ @xcite ) . + here ,",
    "as argued in section  2.4 we deliberately chose not to consider the issue of shot noise and its consequences on segmentation , for which no definitive solution yet exists , though many improvements have been proposed in the literature ( see _ e.g. _ @xcite ) .",
    "instead , we followed the approach of @xcite , that simply involves convolving the sampled density field with a large enough ( in terms of sampling scale ) gaussian kernel so that the field can be considered smooth and differentiable ; the probabilistic algorithm allows for the removal of sampling effects and small intensity residual shot noise . in appendix  c",
    "we show that the corresponding fully connected skeleton is nonetheless quite robust ( the core of the network remains quasi unchanged ) , so long as the snr is above one .",
    "a possible drawback of this method is that it introduces a smoothing scale attached to the skeleton .",
    "this is not necessarily a problem in cosmology as the scaling of the skeleton properties with scale yields information on the distribution over these scales .",
    "moreover , one is usually interested by the properties of the skeleton on a given scale ( typically larger than the halo scale , a few megaparsecs ) .",
    "nonetheless , there exist more complex multi - scale sampling and smoothing techniques such as the one presented in @xcite or @xcite that could straightforwardly be adapted to our implementation .",
    "all the algorithm requires is a structured sampling grid where one can recover a one to one pixel neighbourhood ( _ i.e. _ one needs to be able to find the neighbouring pixels of any pixel and these pixels must have the former as neighbour as well ) .",
    "for instance , we already implemented the algorithm for an healpix @xcite pixelisation of the sphere ( see figure  [ healpix ] ) , while a direct implementation on a delaunay tesselation network is clearly an option .",
    "+ a natural extension of the theoretical component of this work would be to investigate numerically the properties of the bifurcation points in abstract space or anisotropic settings ( see @xcite for a theoretical discussion for isotropic gaussian random fields ) .",
    "for instance , in the context of cosmic structure formation , @xcite , relied on the parallel between the skeleton of the density field in position - time 4d space and in position - scale 4d space to relate the two . in the former ,",
    "the skeleton is a natural way of computing what is known as a halos merger tree , commonly used in semi - analytical galaxy formation models ( see @xcite for instance ) : the skeleton traces the evolution of the critical points of the density field in time .",
    "the peak theory @xcite tells us that the smoothing scale can be linked to time evolution on scales where gravitational effects remain weakly non - linear . a worthwhile goal is to establish the parallel between the properties of 4d skeleton in this position - smoothing scale space ( which can be computed from the gaussian initial conditions only ) and the halo merger tree.finally ,",
    "note that classical bifurcation theory is concerned with the evolution of a critical point as a function of a control parameter . in the language of the skeleton",
    ", this evolution may correspond to the skeleton in the extended `` phase space '' .    from the physical and observational point of view",
    ", an other interesting venue would be to apply the skeleton to actual galaxy catalogs such as the sdss @xcite to characterize the ( universal ) statistics of filaments as physical objects , like halos or voids , and describe them in terms of their thickness , length , curvature and environmental properties ( galaxies types , halo proximity , color and morphology gradient ... ) , both in virtual and observed catalogs .",
    "it could also be used as a diagnosis tool for inverse methods which aim at reconstructing the three dimensional distribution of the igm from say qso bundles @xcite or upcoming radio surveys ( lofar , ska etc ... ) clearly the peak patch segmentation developed in this paper will also be useful in the context of the upcoming surveys such as the lsst , or the the sdss-3 bao surveys , for instance to identify rare events such as large walls or voids and study their shape .",
    "its application to cmb related full sky data , such as wmap or planck should provide insight into , e.g. the level of non gaussianity in these maps .",
    "similarly , upcoming large scale weak lensing surveys ( dune , snap ... ) could be analysed in terms of these tools ( see @xcite for the validation of a reconstruction method in this context ) . using the skeleton ,",
    "the geometry of cold gas accretion that fuel stellar formation in the core of galaxies could be probed .",
    "the properties of the distribution of metals on smaller scales could be also investigated using peak patches , to see how they influence galactic properties ; one could compare these statistical results to those obtained through whim detection by oxygen emission lines ( @xcite @xcite ) .",
    "indeed it has been claimed ( see e.g. @xcite @xcite ) that the geometry of the cosmic inflow on a galaxy ( its mass , temperature and entropy distribution , the connectivity of the local filaments network etc . ) is strongly correlated to its history and nature .",
    "+ in closing , let us emphasize again that the scope of application of the algorithm presented in this paper extends well beyond the context of the large scale structure of the universe : it could be used in any scientific of engineering context ( medical tomography , geophysics , drilling ... ) where the geometrical structure of a given field needs to be characterized .           _",
    "we thank d. pogosyan , d.  aubert , j.  devriendt , j.  blaizot , s.  peirani and s.  prunet for fruitful comments during the course of this work , and d.  munro for freely distributing his yorick programming language and opengl interface ( available at _ http://yorick.sourceforge.net/_ ) .",
    "this work was carried within the framework of the horizon project , ` www.projet-horizon.fr ` .",
    "_    adelman - mccarthy , j.  k. , et al .",
    "2008 , , 175 , 297 alcock , c. , & paczynski , b.  1979 , , 281 , 358 b. , petitjean p. , pichon c. , bergeron j. , 2004 , , 419 , 811 aragn - calvo , m.  a. , jones , b.  j.  t. , van de weygaert , r. , & van der hulst , j.  m.  2007 , , 474 , 315 aragn - calvo , m.  a. , van de weygaert , r. , jones , b.  j.  t. , & van der hulst , j.  m.  2007 , , 655 , l5 aubert , d. , & pichon , c.  2006 , eas publications series , 20 , 37 aubert , d. , pichon , c. , & colombi , s.  2004 , , 352 , 376 barrow , j.  d. , bhavsar , s.  p. , & sonoda , d.  h.  1985 , , 216 , 17 bertschinger , e.  1985 , , 58 , 1 s. , lantuejoul c. , 1979 , in proceedings international workshop on image processing , ccett / irisa , rennes , france s. , meyer f. , 1993 , mathematical morphology in image processing , ed .",
    "m. dekker , new york , ch .",
    "12 , 433 bharadwaj , s. , sahni , v. , sathyaprakash , b.  s. , shandarin , s.  f. , & yess , c.  2000 , , 528 , 21 bond , j.  r. , & myers , s.  t.  1996 , , 103 , 1 bond , j.  r. , cole , s. , efstathiou , g. , & kaiser , n.  1991 , , 379 , 440 colberg , j.  m.  2007 , , 375 , 337 coles , p. , melott , a.  l. , & shandarin , s.  f.  1993 , , 260 , 765 colless , m. , et al .   2003 , arxiv astrophysics e - prints , arxiv : astro - ph/0306581 colombi , s. , pogosyan , d. , & souradeep , t.  2000 , physical review letters , 85 , 5515 s. , colombi s. , pichon c. , rollinde e. , petitjean p. , sousbie t. , 2008 , in press , pp 000000 a. , birnboim y. , engel g. , freundlich j. , goerdt t. , mumcuoglu m. , neistein e. , pichon c. , teyssier r. , zinger e. , 2008 , arxiv e - prints , 808 de lapparent , v. , geller , m.  j. , & huchra , j.  p.  1986",
    ", , 302 , l1 el - ad , h. , & piran , t.  1997 , , 491 , 421 forero - romero , j.  e. , hoffman , y. , gottloeber , s. , klypin , a. , & yepes , g.  2008 , arxiv:0809.4135 gottloeber , s.  1998 , large scale structure : tracks and traces , 43 k.  m. , hivon e. , banday a.  j. , wandelt b.  d. , hansen f.  k. , reinecke m. , bartelmann m. , 2005 , , 622 , 759 graham , m.  j. , clowes , r.  g. , & campusano , l.  e.  1995 , , 275 , 790 hahn , o. , carollo , c.  m. , porciani , c. , & dekel , a.  2007 , , 381 , 41 hanami , h.  2001 , , 327 , 721 harker , g. , cole , s. , helly , j. , frenk , c. , & jenkins , a.  2006 , , 367 , 1039 s. , devriendt j.  e.  g. , ninin s. , bouchet f.  r. , guiderdoni b. , vibert d. , 2003 , , 343 , 75 hoffman , y. , & shaham , j.  1982 , , 262 , l23 huchra , j.  p. , & geller , m.  j.  1982 , , 257 , 423 icke , v.  1984 , , 206 , 1p , jorgen . , riemannian geometry and geometric analysis , fourth edition , 1995 , springer kirshner , r.  p. , oemler , a. , jr . , schechter , p.  l. , & shectman , s.  a.  1981 , , 248 , l57 kofman , l. , pogosyan , d. , shandarin , s.  f. , & melott0 , a.  l.  1992 , , 393 , 437 lacey , c. , & cole , s.  1993 , , 262 , 627 martinez , v. , & saar , e.  2002 , , 4847 , 86 merritt , d. , graham , a.  w. , moore , b. , diemand , j. , & terzi , b.  2006 , , 132 , 2685 milnor , j. , 1963 , morse theory ( princeton university , princeton , nj ) navarro , j.  f. , frenk , c.  s. , & white , s.  d.  m.  1997 , , 490 , 493 neyrinck , m.  c.  2008 , , 386 , 2101 neyrinck , m.  c. , gnedin , n.  y. , & hamilton , a.  j.  s.  2005 , , 356 , 1222 novikov , d. , colombi , s. , & dor , o.  2006 , , 366 , 1201 ocvirk p. , pichon c. , teyssier r. , 2008 , arxiv e - prints , 803 peebles , p.  j.  e.  1980 , research supported by the national science foundation .",
    "princeton , n.j . , princeton university press , 1980 .",
    "435 p. , peebles , p.  j.  e.  1993 , princeton series in physics , princeton , nj : princeton university press , |c1993 , c. , vergely j.  l. , rollinde e. , colombi s. , petitjean p. , 2001",
    ", , 326 , 597 pichon c. , thibaut e. , prunet s. , benabed k. , sousbie , t . , teyssier r. , colombi s. _ in prep . _",
    "pogosyan d. , pichon c. , prunet s. , gay c. , sousbie , t .",
    ", colombi s. _ in prep .",
    "_ platen , e. , van de weygaert , r. , & jones , b.  j.  t.  2007 , , 380 , 551 s. , pichon c. , aubert d. , pogosyan d. , teyssier r. , gottloeber s. , 2008 , arxiv e - prints , 804 jos , b.  t.  m. , roerdink & arnold meijster ,  2001 , _ fundamenta informaticae _",
    ", 41 , 187 - 228 sahni , v. , sathyaprakash , b.  s. , & shandarin , s.  f.  1998 , , 495 , l5 sathyaprakash , b.  s. , sahni , v. , & shandarin , s.  f.  1996 , , 462 , l5 sheth , r.  k.  1998 , , 300 , 1057 sousbie , t. , pichon , c. , colombi , s. , novikov , d. , & pogosyan , d.  2008a , , 383 , 1655 sousbie , t. , pichon , c. , courtois , h. , colombi , s. , & novikov , d.  2008b , , 672 , l1 springel , v.  2005 , , 364 , 1105 springel , v. , white , s.  d.  m. , tormen , g. , & kauffmann , g.  2001 , , 328 , 726 stoica , r.  s. , martnez , v.  j. , mateu , j. , & saar , e.  2005 , , 434 , 423 teyssier , r , pires , s , prunet , s , aubert , d. pichon , c prunet , amara , a benabed , k colombi , s refregier , a. & starck , j.l .",
    "2008 , submitted .",
    "van de weygaert , r. , & schaap , w.  2007 , arxiv e - prints , 708 , arxiv:0708.1441 wang , h.  y. , mo , h.  j. , & jing , y.  p.  2007",
    ", , 375 , 633 zeldovich , y.  b.  1970 , a&a , 5 , 84",
    "in this appendix , we present a generic algorithm that aims at minimizing a multi - linear scalar function @xmath216 of @xmath0 variables within a polygonal volume , in a @xmath0-dimensional space , by reducing the problem to finding the respective minima of a set of polynomials of order @xmath0 .",
    "it takes as input the location of the minima , @xmath217 , of @xmath216 on the edges of the square and simply consists in recursively minimizing the value of @xmath216 along the lines joining them .",
    "let us first consider the 2d case illustrated by figure  [ fig_minimization ] , where the cell is a square . in this case ,",
    "three minima , @xmath218 , @xmath219 and @xmath220 ( represented by red crosses ) can be easily found on the edges of the square from the linearly interpolated value of @xmath221 along them .",
    "one can then compute the location of the minima along the three lines linking them ( the red triangle ) , noting that because of the multi - linearity of @xmath221 , its value along a line can be expressed as a second order polynomial .",
    "one thus obtains @xmath54 new points , @xmath222 , @xmath223 and @xmath224 , and the process can be repeated , as represented by the blue and black sets of lines , until convergence to the solution , represented by the blue cross ( _ i.e. _ when the three points are close enough to each other ) .    .",
    "the reader can refer to the legend of figure  [ fig_propersmoothing ] for more details .",
    "the scalar field to minimize is represented by the blue shading in the background while its minimum is located at the intersection of the 3 quadrics .",
    "the red crosses locate the field minima along the edges while the red , blue and black sets of lines result from the first three recursion steps.[fig_minimization],width=321 ]    this algorithm can be generalized to the case of the @xmath9-face of an @xmath2-cubic cell , @xmath225 , thus providing the solution over the @xmath9-face from the @xmath18 solutions , @xmath226 , over the sets of @xmath100-faces that are its edges . as explained in section [ sec : smooth - algo ] , this algorithm is thus recursively applied to the edges of the cell , starting from the @xmath27-faces , in the order of their increasing dimensionality .",
    "the @xmath65 step of the algorithm thus goes as follows :    1 .",
    "compute the equations of the @xmath227 lines joining pairs of @xmath228 .",
    "2 .   evaluate the value of @xmath216 at @xmath229 points along these lines using multi - linear interpolation , and fit a polynomial of order @xmath9 .",
    "3 .   find the minima of these polynomials that belong to the cell and keep the @xmath18 lowest among them , with coordinates @xmath230 .",
    "4 .   if these points are all contained in a sphere of radius a given fraction of the cell , stop , else start over .",
    "note that although only the case of a cartesian sampling grid was presented here , the algorithm is easily transposable to any type of grid , such as the one produced by voronoi tessellation on a manifold , which is composed of simplex shape cells .",
    "the inter - skeleton pseudo - distance from one skeleton @xmath196 to another skeleton @xmath197 was defined in the main text by the probability distribution function ( pdf ) of the minimum of the distance from each segment of @xmath196 to any segments of @xmath197 . in this appendix",
    ", we show how this measure can be interpreted using realizations of scale invariant gaussian random fields ( grfs ) with different power spectrum index @xmath2 ( such that @xmath231 ) and different smoothing lengths @xmath117 .",
    "all the skeletons that we use were computed from @xmath125 pixels realizations of grfs , smoothed over a scale @xmath232 pixels or @xmath233 pixels .",
    "these scales are defined as the width of the gaussian kernel that we used to smooth the fields and the value of @xmath117 roughly corresponds , in number of pixels , to the smoothing scale we used in the main text , @xmath153 .",
    "a total of six different skeletons were computed :    * @xmath234 and @xmath235 : skeletons computed from two realizations ( grf0 and @xmath236 ) of grfs with spectral index @xmath237 , smoothed over a scale @xmath232 pixels . + * @xmath238 and @xmath239 : skeletons computed from two realizations ( grf3 and @xmath240 ) of grfs with spectral index @xmath241 , smoothed over a scale @xmath232 pixels . + * @xmath242 : this skeleton was computed from the field grf0 , smoothed on scale @xmath117 .",
    "the resulting skeleton was then translated by @xmath243 . +",
    "* @xmath244 : this skeleton was computed from the field grf0 , smoothed on scale @xmath245 pixels .",
    "+     +   +    figure  [ fig_grf ] presents the different pseudo - distances between these skeletons , @xmath198",
    ". figures  [ fig_grf00 ] and  [ fig_grf33 ] present the results obtained when comparing uncorrelated fields ( i.e. different realizations of grfs ) .",
    "as expected in that case , @xmath246 and @xmath247 and the position of the mode is about the smoothing length .",
    "one should also note that the mode intensity differs between @xmath237 and @xmath241 , which can be explained by the fact that in the latter case , small scale fluctuations are suppressed together with smaller scale filaments , thus making it less probable for a segment of one realization to be very close to one of the other realization .",
    "figure  [ fig_grf03 ] shows that these pseudo distance measurements make it possible to distinguish the different nature of two skeletons .",
    "in fact , whereas @xmath234 has filaments on any scales , only the larger scales are present in @xmath248 , which translates into an asymmetry between @xmath249 and @xmath250 . whereas in the first case , there is no reason why every segment of @xmath234 should be close to a segment of @xmath238 , the reciprocal is not true : @xmath251 spreads on all scales and every segment of @xmath252 should be as close as any other from a segment of @xmath234 ( hence the higher intensity of the mode for @xmath250 ) . when comparing a skeleton @xmath253 with less filaments to a skeleton @xmath254 with more filaments , the intensity of the mode is thus expected to be higher for @xmath198 than for @xmath203 .",
    "+ this observation is confirmed by figure  [ fig_grf00l ] where @xmath255 is compared to @xmath234 , which has small scale filaments that @xmath244 does not have .",
    "but in that case , the two skeletons are correlated as only the smoothing length changes .",
    "this results in a higher intensity of the mode of @xmath256 : the larger scale filaments are present in both skeletons .",
    "it also results in a shift in the position of the mode , located at a distance smaller than the smoothing length .",
    "figure  [ fig_grf00 t ] illustrates the case of a simple translation of length half the smoothing length @xmath117 : in that case , both pdfs are identical and a very asymmetric and high intensity mode is present at distance @xmath257 . finally , it is also interesting to note that the comparison of @xmath244 to @xmath242 almost gives the exact same result as the one for @xmath244 to @xmath234 and it is difficult to distinguish one from the other .",
    ", width=321 ]    in order to investigate the robustness of the fully connected skeleton with respect to small changes in the underlying field , the following experiment is carried . a given 2d white random field of size @xmath258 is generated .",
    "it is then smoothed over 10 pixels , and its reference skeleton , @xmath259 is computed .",
    "a white random field of amplitude snr is added to the reference field , and the corresponding skeleton , @xmath260 , is computed after smoothing over 10 pixels .",
    "the pdf of the pseudo - distances @xmath261 and @xmath262 is then calculated ( see appendix b ) .",
    "the distance at the maximum ( its mode ) of both pdf remains unchanged for all the snr considered ( @xmath263 ) , which demonstrates that the core of the skeleton is quite robust : the reference skeleton is always shadowed by its noisy counter part .",
    "the amplitude of the pdf at its maximum is plotted in figure  [ sklnoise ] .",
    "this amplitude is sensitive to the high distance tail of mismatch between the two skeleton since the pdf is normalized . in short , within the network there is a small subset of filaments which are sensitive to any small variation of the field . for the vast majority of the network ,",
    "the skeleton is globally only weakly affected by changes of the underlying field so long as the amplitude of the change has a snr above one .",
    "when the snr drops bellow one , spurious filaments occur more and more .",
    "the discrepancy between the two plateaux at larger snr reflects the fact that weaker filaments will occur somewhat randomly from one realisation to another , depending on very small details in the field ."
  ],
  "abstract_text": [
    "<S> a method to compute the full hierarchy of the critical subsets of a density field is presented . </S>",
    "<S> it is based on a watershed technique and uses a probability propagation scheme to improve the quality of the segmentation by circumventing the discreteness of the sampling . </S>",
    "<S> it can be applied within spaces of arbitrary dimensions and geometry . </S>",
    "<S> this recursive segmentation of space yields , for a @xmath0-dimensional space , a @xmath1 succession of @xmath2-dimensional subspaces that fully characterize the topology of the density field . </S>",
    "<S> the final 1d manifold of the hierarchy is the fully connected network of the primary critical lines of the field : the skeleton . </S>",
    "<S> it corresponds to the subset of lines linking maxima to saddle points , and provides a definition of the filaments that compose the cosmic web as a precise physical object , which makes it possible to compute any of its properties such as its length , curvature , connectivity etc ...    when the skeleton extraction is applied to initial conditions of cosmological n - body simulations and their present day non linear counterparts , it is shown that the time evolution of the cosmic web , as traced by the skeleton , is well accounted for by the zeldovich approximation . comparing this skeleton to the initial skeleton undergoing the zeldovich </S>",
    "<S> mapping shows that two effects are competing during the formation of the cosmic web : a general dilation of the larger filaments that is captured by a simple deformation of the skeleton of the initial conditions on the one hand , and the shrinking , fusion and disappearance of the more numerous smaller filaments on the other hand . </S>",
    "<S> other applications of the n dimensional skeleton and its peak patch hierarchy are discussed . </S>",
    "<S> +    cosmology : simulations , statistics , observations , galaxies : formation , dynamics . </S>"
  ]
}