{
  "article_text": [
    "studies of variability in astronomy typically use image subtraction techniques in order to characterize the magnitude and type of the variability .",
    "this practice involves subtracting a prior  epoch ( generally high signal  to  noise ) template image from a recent science image ; any flux remaining in their difference may be attributed to phenomena that have varied in the interim .",
    "this technique is sensitive to both photometric and astrometric variability , and can uncover variability of both point ",
    "sources ( such as stars or supernovae ; e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ) and extended  sources ( such as comets or light echoes ; e.g. * ? ? ?",
    "successful application of this technique shows that it is sensitive to variability at the poisson noise limit in a variety of astrophysical conditions @xcite , and in this regard may be considered optimal .",
    "there are several reasons for preferring such an approach over catalog  based searches .",
    "first , many types of variability are found in confused regions of the sky , and it may be difficult to deblend the time  variable signal from the non  temporally  variable surrounding area .",
    "this is particularly true for supernovae and active galactic nuclei , which are typically blended with light from their host galaxies .",
    "however , such confusion is not limited to stationary objects . moving solar system bodies",
    "may serendipitously yield false brightness enhancements in the measurement of a background object if the impact parameter is small compared to the image s point spread function ( psf ) .",
    "for this reason , removal of non ",
    "variable objects is preferred before attempting to characterize variable sources in images .",
    "image subtraction is also an efficient technique as the vast majority of pixels in an image do not contain signatures of astrophysical variability . any pixel ",
    "level analysis of a difference image will , therefore , be restricted to those sources that are temporally variable ( as opposed to analyzing all sources within an image ) . while many variants of this technique have been published @xcite , and many versions implemented in automated variability ",
    "detection pipelines @xcite , there does remain room for improvement in the robustness of the image subtraction , and in the reduction of subtraction artifacts .",
    "we refer the reader to @xcite for an in  depth summary on the practical application of these image subtraction techniques .      in image subtraction we assume that we have two images of the same portion of the sky , taken at different epochs , but in the same filter",
    ". we will call the image that contains the variability of interest the `` science '' image , and the template image to be subtracted the `` reference '' image .",
    "the images will , in general , be astrometrically misaligned , but this can be resolved by using sinc  based image registration methods that preserve the noise properties of the original image .",
    "after astrometric alignment , a given astrophysical object will be represented in the reference image as a sub - array of pixels @xmath1 and in the science image as @xmath2 , with the same span in @xmath3 and @xmath4 . each image will , however , have a different point spread function ( psf ) , which is the spatial response of a point source due to the atmosphere , telescope optics , and instrumental signatures .",
    "psf  matching of the images is required before we can subtract one image from the other , and is the essence of the image subtraction technique .",
    "we typically assume that the reference image is a high signal  to  noise ( s / n ) representation of the field , for example an image coadd made through a mosaicing process , or a single image taken on a night with particularly good seeing . a standard assumption ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ) is that @xmath2 can be modeled as a convolution of @xmath1 by a single psf  matching kernel @xmath5 , with an additional noise component @xmath6 ; @xmath7 our goal in this paper is to develop an effective method for determining @xmath5 .      as inputs to the psf  matching technique , we assume images are astrometrically registered , and background subtracted ( while this latter constraint is not a necessity , it does enable us to restrict our analysis here to the respective shapes of the psfs ) .",
    "to proceed , we make the assumption that @xmath5 may be modeled as a linear combination of basis functions @xmath8 , such that @xmath9 @xcite .",
    "the basis components do not have to be orthonormal , nor does the basis need to be complete ( indeed , it may be overcomplete ) .",
    "however , it is desirable to choose a shape set that compactly describes @xmath5 , such that the number of required terms is small .    by formulating the kernel decomposition as a linear expansion",
    ", we may recast equation  [ eqn - conv ] as the vectorized equation @xmath10 where c is the matrix of functions @xmath11 evaluated at each pixel . for any given kernel basis set",
    ", the goal is to find the coefficients @xmath12 associated with each @xmath13 .",
    "we proceed using standard linear least squares analysis .",
    "we assume that the noise is uncorrelated and known ; @xmath14 is therefore the product of a diagonal matrix @xmath15 , which represents the square root of the known per ",
    "pixel variance , and a zero  mean unit  variance random variable @xmath16 . by reweighting by the inverse square root of @xmath17 ( which must exist as covariance matrices are positive definite and hence invertible )",
    "we obtain the modified equation @xmath18 which is just another linear model , now with the error term @xmath16 having the identity matrix for the covariance .",
    "this reduces to the weighted linear least squares equation @xmath19 with @xmath20    the normal equations for estimating @xmath21 are : @xmath22    this may be cast in the familiar form of @xmath23 with @xmath24 in discrete pixel coordinates , this corresponds to @xmath25 where @xmath26 represents the known variance per pixel . the creation of the matrices @xmath27 and @xmath28 therefore requires a convolution of the reference image with each basis kernel .",
    "the least ",
    "squares estimate for @xmath21 is @xmath29 .",
    "a difference image is then constructed as @xmath30 . because the estimate of @xmath31 is explicitly dependent on both @xmath2 and @xmath1",
    ", the residuals in the difference image may _ not _ necessarily follow a normal @xmath32 distribution , with @xmath33 due to this covariance .",
    "the residuals should however have a flat power spectral density .",
    "when a large set of basis functions is used , the matrix @xmath35 may be ill  conditioned or even singular .",
    "this can be quantified by the `` condition number '' of @xmath36 , which we define as the ratio of the largest to the smallest eigenvalues .",
    "when the condition number is large , inversion of @xmath36 will be numerically unstable or infeasible .    a common approach",
    "when trying to invert an ill  conditioned matrix is to compute instead a _ pseudo ",
    "inverse _ , or an approximation to one in which eigenvalues that are numerically small are zeroed out . as @xmath36 is symmetric",
    ", we can decompose it as @xmath37 with @xmath38 an orthogonal matrix and @xmath39 with eigenvalues @xmath40 .",
    "we define @xmath41 to be a truncation of @xmath42 where @xmath43 becomes too large .",
    "then , we define the pseudo  inverse of @xmath44 as @xmath45 .",
    "note this allows for the definition of a pseudo - inverse of @xmath36 as @xmath46 .",
    "analogous to @xmath44 , define @xmath47 to be the same as the matrix @xmath38 in the first @xmath48 columns , and zero elsewhere . typically this truncation threshold is defined by the machine precision of the computation ( e.g. for double  valued calculations , @xmath49 ) .",
    "however , significantly larger limits for @xmath50 may be used to avoid underconstrained parameters , such as in section  [ sec - sure ] .",
    "the original psf  matching bases proposed by @xcite and @xcite ( referred to here as `` alard  lupton '' or al bases ) used a sum of multiple gaussians , each modified by a 2dimensional polynomial : @xmath51 where the index @xmath48 runs over all permutations of @xmath52 .",
    "this basis set effectively uses @xmath53 gaussian components , each with width @xmath54 , and each modified by a set of gauss  hermite polynomials ( e.g. * ? ? ?",
    "* ) expanded out to order @xmath55 ( @xmath56 ) .",
    "the total number of basis functions in the set is @xmath57 .",
    "the number @xmath58 and width @xmath54 of the gaussians , as well as spatial order of the polynomials @xmath55 , are configurable but are not fitted parameters in the linear least - squares minimization .",
    "therefore these are tuning parameters of the model .",
    "typically , _ a - priori _ information such as the widths of the image psfs is used to choose these values ( e.g. * ? ? ?",
    "in a representative implementation @xcite , three gaussians are used , with the narrowest gaussian expanded out to order 6 , the middle to order 4 , and the widest to order 2 .",
    "this leads to a total of 49 basis functions used in the kernel expansion .",
    "the practical application of this algorithm has been very successful , and it has been used by various time ",
    "domain surveys such as macho @xcite , ogle @xcite , moa @xcite , supermacho @xcite , the deep lens survey @xcite , essence @xcite , the sdss ",
    "ii supernova survey @xcite , and most recently analysis of commissioning data from pan  starrs @xcite .    the top row of figure  [ fig - al ] shows an instance of successful psf matching using this sum  of  gaussians basis .",
    "the first column represents a high signal  to  noise image of a star @xmath1 generated from an image coaddition process applied to data from the canada ",
    "hawaii telescope ( cfht ) .",
    "the second column shows this same star , aligned with the template image to sub  pixel accuracy , in a single science image @xmath2 .",
    "the star is obviously asymmetric , potentially due to optical distortions such as focus or astigmatism , or due to tracking problems during acquisition of the image .",
    "the psf  matching kernel thus will need to take the symmetric @xmath1 and elongate it along a vector oriented approximately @xmath59 from horizontal .",
    "the first row , third column shows the best ",
    "fit psf  matching kernel using @xmath60 gaussians with @xmath54 = [ 0.75 , 1.5 , 3.0 ] pixels , and each modified by hermite polynomials of order @xmath55 = [ 4 , 3 , 2 ] , respectively .",
    "the total number of terms in the expansion is 31 .",
    "the first row , right column shows the resulting difference image @xmath61 .",
    "the subtraction is obviously very good , with the remaining pixels described by a @xmath62 distribution .",
    "the intrinsic symmetries of hermite polynomials ( symmetric for even order , anti  symmetric for odd order ) means that the gauss  hermite bases possess a high degree of symmetry about the central pixel .",
    "this makes it difficult to concentrate the kernel power off ",
    "center when using an incomplete basis expansion .",
    "such functionality is necessary when the flux needs to be redistributed on the scale of the kernel size , such as when there are astrometric misalignments .",
    "while it is possible to compensate for misalignment using kernels derived from this basis , this requires concentrating the kernel strength in the high  order terms .",
    "there are practical limitations to the efficacy of this including the scale and orientation of the required shift , and the number of basis terms used .    as a concrete example",
    ", the second row in figure  [ fig - al ] shows the best ",
    "fit kernel derived when there is a 3pixel shift in both the @xmath3 and @xmath4 directions .",
    "the kernel needs to have power in the first quadrant ( upper right ) at the scale of 3 pixels .",
    "the image of the kernel ( third column ) shows that while it is obviously able to do so , the matching suffers in the third quadrant , as the difference image shows obvious residuals .",
    "these pixels result in an unacceptable @xmath63 distribution ; recall we were able to yield @xmath64 for well  registered images ( top row ) .",
    "another limitation of the model is that there are a variety of tuning parameters .",
    "this includes the number of gaussians in the basis , their widths , and their spatial orders .",
    "these parameters are typically chosen using a set of heuristics .",
    "if there is a mismatch compared to the true underlying kernel , this process will fail .",
    "the third row of figure  [ fig - al ] shows psf  matching results when the basis gaussians are _ too big _ and are unable to reproduce the small  scale differences in the psfs .",
    "this yields obvious residuals in the difference image , which follow a @xmath65 distribution .",
    "the fourth row of figure  [ fig - al ] shows results when the gauss ",
    "hermite polynomials are not allowed to vary to high enough order , also yielding unacceptable @xmath66 residuals in the difference image .    clearly",
    "the results of this process are sensitive to the choice of several tuning parameters , which makes this difficult to implement robustly . in a statistical sense ,",
    "selection of tuning parameters ( which includes selecting the number of basis functions used ) usually has a much larger effect on performance than does the choice of basis functions .",
    "a process that results in a reduction in the number of kernel tuning parameters , while maintaining the quality of the difference images , would greatly improve the effectiveness of this method .",
    "the most general technique for modeling @xmath5 is to use a `` shape free '' basis , which consists of a delta function at each kernel pixel index : @xmath67 . a kernel of size @xmath68",
    "will then have 361 orthonormal , single  pixel bases .",
    "in this situation there are _ no _ tuning parameters , which is an obvious benefit . however , in any choice of basis there is a trade off between flexibility in the forms the fitted function can take , and variability in the resulting fit ( the so  called `` bias  variance '' trade off ) .",
    "the delta ",
    "function basis provides complete flexibility , and as such can account for features such as arbitrary off - center power required to compensate for astrometric misregistration ( e.g. * ? ? ?",
    "* ) . but to avoid gross overfitting , that flexibility needs to be tempered to keep the variance in check",
    ".    figure  [ fig - df ] shows the results of psf  matching using such a basis , using the same objects as in figure  [ fig - al ] .",
    "the top row demonstrates the results for exactly aligned images , while the bottom row demonstrates the results for images misaligned by 3 pixels in both @xmath3 and @xmath4 .",
    "the difference images are qualitatively similar .",
    "however , the best  fit solutions obviously yield large variations within the kernels themselves , and do not match expectations of what the actual kernel should look like . the reason for this",
    "can be found in the distribution of pixels residuals in the difference image .",
    "both images follow a @xmath69 distribution .",
    "this indicates that the residuals have lower variance than gaussian statistics would suggest .",
    "indeed , in figure  [ fig - df ] column  4 the residuals appear smoother than random noise . this is impossible unless we have overestimated the variance in our images , or unless the kernels themselves are removing some fraction of the noise .",
    "the large numbers of basis shapes ( 361 degrees of freedom vs. 31 for the sum  of  gaussians ) makes it highly likely that we are over  fitting the problem .",
    "the kernel thus has the ability to match both the underlying signal _ and _ the associated noise in the two images .",
    "so while this technique is optimal for matching pixels in two images  where those pixels are a combination of signal and noise  it is not necessarily optimal for uncovering the true psf  matching kernel .",
    "a consequence of this is that the psf  matching kernel derived for any given object may not be directly applied to neighboring objects , since the solution is significantly driven by the local noise properties .",
    "high variance estimators are particularly poor as inputs to interpolation routines , or to a spatial model of the kernel @xmath70 , that find the matching kernel at _ all _ locations as a function of the fitted kernels at particular locations .",
    "below , we explore how introducing a certain amount of bias into this estimator can improve its performance .",
    "the delta  function basis can flexibly fit a kernel of any form , but as we have shown , this flexibility is both its strength and weakness . as is , the method significantly overfits , absorbing substantial noise fluctuations into the fit and thus giving estimated kernels with excessive variance .",
    "a solution is to introduce some amount of bias into the fit to reduce the solution variance by a much larger factor ( if `` bias '' sounds pejorative , note that this is just a kind of smoothing ) . when fitting a smooth function such as @xmath70 , we prefer fitted kernels for which nearby solutions do not vary too greatly",
    ". this bias will enable such a fit with vastly reduced mean  squared error .    among the various approaches to dealing with overfitting ,",
    "the most common are through linear regularization techniques ( e.g. section 18.5 ; * ? ? ?",
    "using these , we may penalize undesirable features of the fit , usually by adding a penalty term to our optimization criterion .",
    "for instance , when fitting a smooth function , we want to penalize fits @xmath71 that are too rough or irregular . one way to do this is to add to the least squares objective a term penalizing the second derivative , @xmath72 .",
    "the one  dimensional second derivative of a function @xmath71 around pixel @xmath3 may be approximated using the central finite difference @xmath73 . since the delta ",
    "function bases have unit height and no intrinsic shape , regularizing the coefficients @xmath12 is equivalent to regularizing the shape of the resulting kernel ( care must be taken to apply the regularization penalty to only those pixels that are associated spatially ) . in matrix terms , this one  dimensional regularization may be represented by @xmath74 , with @xmath75 which is of dimension @xmath76 , where @xmath77 here is the total number of pixels in the kernel .. ] a generalization of this to two dimensions results in a 5point stencil that sums the local derivative along both axes , @xmath78 , with an associated matrix @xmath79 .",
    "the finite calculation of this penalty is implemented through the matrix equations latexmath:[\\[\\begin{aligned } \\label{eqn - r }    @xmath21 represents the amplitude of each delta function , and @xmath79 encapsulates the coefficients that approximate the local @xmath81 derivative of the resulting kernel .",
    "we define the matrix @xmath82 , which makes the second derivative penalty @xmath83 .",
    "this matrix is used to regularize the normal equations ( equation  [ eqn - lin4 ] ) with strength @xmath0 @xmath84 note the similarity to equation  [ eqn - lin3 ] , with the only difference being @xmath85 . here",
    "@xmath0 represents the strength of the regularization penalty , and is the sole tuning parameter in this model .",
    "figure  [ fig - dfr ] shows results for the same set of objects displayed in figure  [ fig - al ] and figure  [ fig - df ] , but using regularization of the delta ",
    "function basis set .",
    "the top row shows the results for aligned images , and @xmath86 .",
    "note that the kernel looks very much as anticipated , being compact and having a shape aligned approximately @xmath87 from horizontal .",
    "residuals in the difference image follow a @xmath88 distribution .",
    "the second row shows the results when the images are misaligned by 3 pixels in @xmath3 and @xmath4 .",
    "the kernel merely appears shifted by the same amount compared to the aligned images , and the difference image follows a quantitatively similar @xmath89 distribution .",
    "this effectively demonstrates that this method can reproduce kernels with off  center power .",
    "the third row shows the results with @xmath90 ; the shape of the psf  matching component of the kernel is just barely discernible above its noise , suggesting the regularization is too weak .",
    "the difference image is , however , acceptable ( @xmath91 ) .",
    "the fourth row shows the results with @xmath92 .",
    "the kernel is far smoother than in previous runs .",
    "however , this appears to be at the expense of residuals in the difference image , which follow a @xmath93 distribution .",
    "this suggests that too much weight has been given the smoothness of the kernel compared to the residuals in the difference image , indicating that the regularization is too strong .",
    "the general trend is that with increasing lambda , the variance in the difference image increases .",
    "the noise properties of the difference image evolve from being too smooth , to approximately white in spectrum , to having residual features at a similar scale as the kernel .",
    "overall , this technique appears very effective .",
    "we are able to create general , compact kernels that represent the underlying shape of the psf  matching kernel with only one tuning parameter , the strength of the regularization @xmath0 .",
    "the role of @xmath0 is effectively to exchange variance in the resulting difference image with variance in the kernel itself . by increasing the value of @xmath0 , we are able to smooth the kernel while increasing the variance in the difference image .",
    "we explore various methods to establish the optimal value of @xmath0 below .      choosing",
    "a good tuning parameter is essential for good performance of a regularization method .",
    "if @xmath0 is too high , the fit will be too smooth ( high bias , low variance ) ; if @xmath0 is too low , the fit will be too rough ( low bias , high variance ) .",
    "the goal of data  driven methods for choosing tuning parameters is to find the sweet spot in the bias  variance trade off .",
    "while choosing a good value for @xmath0 is a hard statistical problem , there are a variety of methods that have proven successful in practice .",
    "these methods construct a statistical estimate of mean  squared error and choose @xmath0 to minimize it .",
    "for instance in cross  validation ( reviewed in * ? ? ?",
    "* ) , the data set is broken into pieces , and each piece is left out in turn during the fit .",
    "the ( prediction ) mean  squared error is derived from the average squared error of the fits in predicting the part of the data that was left out .",
    "another approach , called empirical risk estimation @xcite , uses the data itself to compute an ( unbiased ) estimate of original fit s mean  squared error and chooses @xmath0 to minimize it .",
    "the theoretical justification for these methods is that , when properly done and with sufficiently large data sets , the chosen @xmath0 is close to the value that minimizes the corresponding mean  squared error function .",
    "a second tuning consideration is that frequently a set of fitted kernels will be used to constrain a spatial model @xmath70 that will be applied to _ all _ pixels in an image .",
    "therefore we must give a large weight to our ability to interpolate between the ensemble of kernel realizations used to constrain @xmath70 .",
    "one metric for this is to examine the predictive power of a kernel derived from one object , and applied to a neighboring object . at small separations ,",
    "the quality of each difference image should be similar , indicating that the initial solution was not significantly driven by the local noise properties .",
    "we explore the practical application of these ideas below using several sets of ccd images from the canada  france ",
    "hawaii telescope plus megacam imager , calibrated using the elixir pipeline of @xcite .",
    "the template image used is the median of several images into a single high s / n representation of the field .",
    "the variance per pixel is determined from the image pixel values divided by the gain .",
    "we first construct a loss function that represents the sum of squared differences between the true ( unknown ) kernel coefficients @xmath21 and @xmath94 , which is our estimate of the kernel coefficients when the tuning parameter is set to the value @xmath0 : @xmath95 the expectation value of @xmath96 is the statistical risk we will minimize through our choice of @xmath0 . when @xmath36 is well  conditioned , we can construct an unbiased estimator of the true risk @xmath97 as ( section 2 , * ? ? ?",
    "* ) @xmath98    we note that this estimator of risk does not require tuning parameters .",
    "if we let @xmath99 be the minimizer of @xmath100 , then we choose @xmath101 as the estimate of @xmath21 .    for the circumstance that @xmath36 is ill - conditioned , we present an adjustment to @xmath100 from equation  [ eqn - sure ] .",
    "following the notation from section  [ sec - invert ] , for any @xmath48 define @xmath102 .",
    "this corresponds to @xmath103 being a projection matrix onto the space of the eigenvectors of @xmath36 that correspond to its @xmath48 largest eigenvalues . note that @xmath48 is now an additional tuning parameter , corresponding to choice of condition number ( denoted by symbol @xmath104 ) for matrix @xmath36 ( section  [ sec - invert ] ) .",
    "a biased estimate of the statistical risk is then : @xmath105 while introducing bias into the estimator of statistical risk seems bad , it can be necessary in situations where @xmath36 is ill  conditioned .",
    "small eigenvalues of @xmath36 corresponds to there being very little information along the associated eigenvectors . by zeroing out these eigenvalues",
    "we are effectively saying we can not reliably estimate with this little amount of information .",
    "hence , we concentrate on getting the estimation correct on the eigenvectors with larger eigenvalues .    for each object detected in the cfht images , and for given values of condition number @xmath104 ranging from @xmath106 , we evaluate @xmath107 at values of @xmath108 .",
    "figure  [ fig - risk ] shows a typical outcome of this analysis for a single object . along the y ",
    "axis we show the associated value of the conditioning parameter @xmath104 , and along the x  axis the value of @xmath0 at which @xmath107 is evaluated .",
    "the _ solid _ line shows the minimum value of @xmath107 for each @xmath104 .",
    "we note that as we decrease the acceptable matrix condition number , thereby truncating more eigenvalues from the matrix pseudoinverse , the optimum value of @xmath0 increases . for matrices with effectively no conditioning ( large @xmath104 )",
    ", the optimal value of @xmath0 is near @xmath109 .",
    "this is in fact the global minimum of the risk .",
    "a similar result is obtained by looking at all objects within an image and summing their cumulative risk surfaces .",
    "we regard @xmath109 as the value preferred by the empirical risk estimation technique , with a range of nearly  equivalent risk between @xmath110 .      in most psf  matching implementations , several dozen objects across a pair of registered images are used to create individual @xmath5 ; ideally these should evenly sample the spatial extent of the images . due to spatial variation in the psfs of the images , caused by optical aberrations or bulk atmospheric effects , the single kernel that psf  matches _ all _ objects in an image",
    "must itself vary spatially . in this case",
    "each of the kernels @xmath5 are used to build spatially varying psf  matching kernel @xmath70 .",
    "this is typically implemented as spatial variation on the kernel coefficients @xmath111 .",
    "therefore an additional consideration in the choice of @xmath0 is the ability to build spatial models for the coefficients @xmath112 .    to quantify this",
    ", we examine the predictive ability of the kernel solution @xmath94 . in all cfht images ,",
    "we identify object pairs separated by more than 5 pixels but less than 50 , a range of separations where we expect the intrinsic spatial variation of the underlying kernel to be minimal .",
    "the kernel derived for each object in a pair is applied to its complement , and the quality of each difference image assessed . for components a and b of each object pair , this yields difference image @xmath113 which is the difference image of object a with kernel a , @xmath114 which is the difference image of object a with kernel b , and analogous images @xmath115 and @xmath116 .",
    "we assess the quality of each difference image using the width of the pixel distribution normalized by the noise , defined as e.g. @xmath117 , within the central @xmath118 pixels of the difference image . while we do nt expect this distribution to have a width of exactly 1.0 due to covariance between the solution and the input images , we do desire that the quality of @xmath114 and @xmath115 should not be significantly worse than that of @xmath113 and @xmath116 .",
    "we aggregate the `` even '' statistics @xmath117 and @xmath119 into distribution @xmath120 , and the `` odd '' statistics ( @xmath121 , @xmath122 ) into @xmath123 .",
    "we further examine the distribution of @xmath124 , which is created from all measurements of @xmath125 and @xmath126 .",
    "this statistic reflects the deterioration in an object s difference image when using a counterpart s kernel , compared to the optimal kernel derived for that object .",
    "we plot the distributions of these values in figure  [ fig - stats ] .",
    "the _ top _",
    "panel provides the median values of these distributions for the sum  of  gaussian ( al ) basis ( _ left _ ) , for the unregularized delta function basis ( @xmath127 ; _ center _ ) , and for delta  function regularization strengths of @xmath128 ( _ right _ ) . the _ bottom _ panel plots the effective standard deviation of the distribution , defined as @xmath129 of the interquartile range .",
    "the lowest median residual variance @xmath120 comes from difference images made using an unregularized @xmath127 basis , the reasons for which we have examined in detail in section  [ sec - df ] .",
    "however , as expected the predictive ability of this basis is by far the worst , having the highest median @xmath124 , as well as large variance within this distribution . as we ramp up the regularization strength , the predictive ability of the kernels increases ( low @xmath124 ) , but at the expense of the quality of the difference image itself ( large @xmath120 ) .    to find an acceptable medium between these two considerations",
    ", we will use the results from the sum  of  gaussian ( al ) basis as a benchmark , since it has been shown to produce effective spatial models ( section  [ sec - al ] ) .",
    "for the al basis , the median values of @xmath120 , @xmath123 , and @xmath124 are 0.99 , 1.14 , and 0.28 , respectively .",
    "similar results are obtained with delta  function regularization strengths of @xmath130 , and @xmath131 . for al",
    "the @xmath132 values of @xmath120 , @xmath123 , and @xmath124 are 0.14 , 0.33 , and 0.74 , respectively .",
    "these are matched ( or bested ) in the regularized basis for @xmath133 , @xmath134 , and @xmath135 , respectively .    in summary , using delta ",
    "function regularization strengths of @xmath136 , we are able to achieve difference images with a similar quality to those yielded by the sum  of  gaussian al basis ( using @xmath120 as our metric ) . these models have similar predictive ability when applied to neighboring objects ( quantified using @xmath123 and @xmath124 ) , making them useful for full  image spatial modeling .",
    "finally , they are seen to be generally applicable , having a small variance in the above statistics when evaluated over several hundreds of object pairs .",
    "we ve examined here the choice of basis set on the quality of psf  matching kernels and their resulting difference images . these include the traditional sum  of  gaussian ( `` alard  lupton '' ) basis and a digital basis based upon delta  functions .",
    "we find that while the delta ",
    "function kernels are the most expressive , they are also the least compact in terms of localization of power within the kernel . having one basis component per pixel in the kernel , they tend to overfit the data and are more sensitive to the noise in the images instead of the intrinsic psf  matching signal .    we introduce a new technique of linear regularization to impose smoothness on these delta ",
    "function kernels , at the expense of slightly higher noise in the difference images .",
    "these regularized shapes are shown to be flexible , and yield solutions with sufficient predictive power to prove useful for spatial interpolation .",
    "we outline two methods to determine the strength of this regularization that minimize the statistical risk of the kernel estimate , and that examine the predictive ability of the derived kernels .",
    "both methods suggest values of @xmath0 that are between 0.1 and 1.0 .    given the large range of image qualities used in image subtraction pipelines compared to the small number of images used in the analysis here , we caution that these estimates may not be applicable under all conditions and should really be estimated on a dataset  by  dataset basis",
    ". the optimal value of @xmath0 will be a function of the s / n in the template and science images , which should affect the level of kernel smoothing needed , and of the respective seeings in the input images , which may impact the suitability of our finite  difference smoothness approximation .    while this implementation appears successful and practical ,",
    "there are various improvements we might consider in our regularization efforts .",
    "this includes changing the scale over which the regularization stencil is calculated based upon the seeing in the images ; currently this is being done in pixel  based coordinates , and not adjusted depending on the full  width at half  maximum of the input psfs .",
    "we also plan to examine additional metrics to determine the optimal value of @xmath0 , including the power spectrum of noise in the resulting difference image , which should be flat .",
    "ultimately , the overall quality of the _ entire _ difference image is the optimal metric to use in assessing choice of basis ; we will be expanding our analysis to include full  image metrics and spatial modeling of the kernel .",
    "finally , the wealth of statistical techniques to efficiently choose basis shapes has not been exhausted .",
    "other potential methods include the use of overcomplete bases , where the choice of the correct subset of components to use is made though through basis pursuit @xcite , as well as the process of `` basis shrinkage '' through the use of multi  scale wavelets @xcite . in all considerations",
    ", it is an advantage to yield solutions that , as an ensemble , have a low dimensionality so that spatial modeling is efficient and spatial degrees of freedom are not being used to compensate for an inefficient choice of basis .",
    "however , for any given basis set the choice of regularization ( none at all or using a fixed set of functions ) is likely to be the proper place for optimization .                                                    , c. , et  al .",
    "2002 , in presented at the society of photo - optical instrumentation engineers ( spie ) conference , vol . 4836 , society of photo - optical instrumentation engineers ( spie ) conference series , ed . j.  a.  tyson & s.  wolff , 395405"
  ],
  "abstract_text": [
    "<S> we review current methods for building psf  matching kernels for the purposes of image subtraction or coaddition . </S>",
    "<S> such methods use a linear decomposition of the kernel on a series of basis functions . </S>",
    "<S> the correct choice of these basis functions is fundamental to the efficiency and effectiveness of the matching  </S>",
    "<S> the chosen bases should represent the underlying signal using a reasonably small number of shapes , and/or have a minimum number of user  adjustable tuning parameters . </S>",
    "<S> we examine methods whose bases comprise multiple gauss  hermite polynomials , as well as a form free basis composed of delta  functions . </S>",
    "<S> kernels derived from delta  functions are unsurprisingly shown to be more expressive ; they are able to take more general shapes and perform better in situations where sum  of  gaussian methods are known to fail . </S>",
    "<S> however , due to its many degrees of freedom ( the maximum number allowed by the kernel size ) this basis tends to overfit the problem , and yields noisy kernels having large variance . </S>",
    "<S> we introduce a new technique to regularize these delta  function kernel solutions , which bridges the gap between the generality of delta  </S>",
    "<S> function kernels , and the compactness of sum  of  gaussian kernels . through this regularization </S>",
    "<S> we are able to create general kernel solutions that represent the intrinsic shape of the psf  matching kernel with only one degree of freedom , the strength of the regularization @xmath0 . </S>",
    "<S> the role of @xmath0 is effectively to exchange variance in the resulting difference image with variance in the kernel itself . </S>",
    "<S> we examine considerations in choosing the value of @xmath0 , including statistical risk estimators and the ability of the solution to predict solutions for adjacent areas . </S>",
    "<S> both of these suggest moderate strengths of @xmath0 between 0.1 and 1.0 , although this optimization is likely dataset dependent . </S>",
    "<S> this model allows for flexible representations of the convolution kernel that have significant predictive ability , and will prove useful in implementing robust image subtraction pipelines that must address hundreds to thousands of images per night . </S>"
  ]
}