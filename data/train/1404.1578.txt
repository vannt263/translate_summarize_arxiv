{
  "article_text": [
    "the classical gaussian linear model reads as follows : @xmath1 important for the present focus are two aspects of how the model is commonly interpreted : ( 1 )  the model is assumed correct , that is , the conditional response means are a linear function of the predictors and the errors are independent , homoskedastic and gaussian ; ( 2 )  the predictors are treated as known constants even when they arise as random observations just like the response .",
    "statisticians have long enjoyed the fruits that can be harvested from this model and they have taught it as fundamental at all levels of statistical education . curiously little known to many statisticians is the fact that a different framework is adopted and a different statistical education is taking place in the parallel universe of econometrics . for over three decades , starting with halbert white s ( 1980a , b;1981;1982 ) seminal articles , econometricians have used multiple linear regression without making the many assumptions of classical linear models theory .",
    "while statisticians use * assumption - laden exact finite sample inference * , econometricians use * assumption - lean asymptotic inference * based on the so - called `` sandwich estimator '' of standard error . in our experience most statisticians have heard of the sandwich estimator but do not know its purpose , use , and underlying theory .",
    "a first goal of the present exposition is therefore to convey an understanding of the assumption - lean framework of basic econometrics in a language that is intelligible to statisticians .",
    "the approach is to interpret linear regression in a semi - parametric fashion as extracting the parametric linear part of a general nonlinear response surface .",
    "the modeling assumptions can then be reduced to i.i.d .  sampling from largely arbitrary joint @xmath2 distributions that satisfy a few moment conditions .",
    "it is in this assumption - lean framework that the sandwich estimator produces asymptotically correct standard errors .",
    "a second goal of this exposition is to connect the assumption - lean econometric framework to a form of statistical inference in linear models that is known to statisticians but appreciated by few : the `` pairs bootstrap . ''",
    "as the name indicates , the pairs bootstrap consists of resampling pairs @xmath3 , in contrast to the `` residual bootstrap '' which resamples residuals  @xmath4 . among the two ,",
    "the pairs bootstrap is the less promoted even though asymptotic theory exists to justify both under different assumptions ( see , for example , freedman 1981 , mammen 1993 ) .",
    "it is intuitively clear that the pairs bootstrap can be asymptotically justified in the asumption - lean framework , and for this reason it produces standard error estimates that solve the same problem as the sandwich estimator .",
    "indeed , we establish a connection that shows the sandwich estimator to be the asymptotic limit of the @xmath5-of-@xmath6 pairs bootstrap when @xmath7 .",
    "we will use the general term `` * assumption - lean estimator * '' to refer to either the sandwich estimator or the pairs bootstrap estimator of standard error .",
    "a third goal of this article is to theoretically and practically compare the assumption - lean estimators with the linear models estimator .",
    "we define a ratio of asymptotic variances  `` @xmath8 '' for short  that describes the discrepancies between the two types of standard error estimators in the asymptotic limit .",
    "if there exists a discrepancy , @xmath9 , it will be assumption - lean estimators ( sandwich or pairs bootstrap ) that are asymptotically correct , and the linear models estimator is then indeed asymptotically incorrect .",
    "if @xmath9 , there exist deviations from the linear model in the form of nonlinearities and/or heteroskedasticities . if @xmath10 , the linear models estimator is asymptotically correct , but this does not imply that the linear model is correct , the reason being that nonlinearities and heteroskedasticities can conspire to produce a coincidentally correct size for the linear models estimator",
    "importantly , the @xmath8 is specific to each regression coefficient because the discrepancies between the two types of standard error estimators vary from coefficient to coefficient .",
    "a fourth goal is to estimate the @xmath8 for use as a test statistic .",
    "we derive an asymptotic null distribution to test the presence of model violations that invalidate the classical standard error of a specific coefficient .",
    "while the result can be called a `` misspecification test '' in the tradition of econometrics , it is more usefully viewed as guidance to the better standard error .",
    "the sandwich estimator comes with a cost  vastly increased non - robustness  which makes it desirable to use the linear models standard error when possible .",
    "however , a procedure that chooses the type of standard error depending on the outcome of a pre - test raises new performance issues that require future research .",
    "( simpler is angrist and pischke s proposal ( 2009 ) to choose the larger of the two standard errors ; their procedure forfeits the possibility of detecting the classical standard error as too conservative .",
    "mackinnon and white ( 1985 ) on the other hand recommend using a sandwich estimator even if no misspecification is detected . )",
    "a fifth and final goal of this article is to propose answers to questions and objections that would be natural to statisticians who hold the following tenets :    * models need to be `` nearly correct '' for inference to be meaningful .",
    "the implication is that assumption - lean standard errors are misguided because inference in a `` misspecified model '' is meaningless .",
    "* predictors in regression models should or can be treated as fixed even if they are random .",
    "here the implication is that inference which treats predictors as random is unprincipled or at a minimum superfluous .",
    "a strong proponent of the first tenet is the late david freedman ( 2006 ) .",
    "while his insistence on intellectual honesty and rigor is admirable , we will counter argue based on white ( 1980a , b ; 1981 ; 1982 ) that inference under misspecification can be meaningful and rigorous . to begin with , we should avoid the negative rhetoric of the term `` misspecification '' .",
    "an alternative view , to be discussed below , can be summarized as follows :    1 .",
    "models are always approximations , not truths ( box 1979 ; cox 1995 ) .",
    "2 .   if models are approximations , it is still possible to give regression slopes meaningful interpretations .",
    "3 .   if models are approximations , it is prudent to use inference that is less dependent on model correctness .",
    "in fact , neither of the second and the third point depends on the degree of approximation : meaningful interpretation and valid inference can be provided for regression slopes whether models are good or bad approximations .",
    "( this is of course not to say that every regression analysis is meaningful ! )    the second tenet above , conditionality on predictors , has proponents to various degrees .",
    "more forceful ones hold that conditioning on the predictors is a necessary consequence of the ancillarity principle ; others hold that the principle confers license , not a mandate .",
    "the ancillarity principle says in simplified terms that valid inference results from conditioning on statistics whose distribution does not involve the parameters of interest . when the predictors in a regression are random , their distribution is ancillary for the regression parameters , hence conditioning on the predictors is necessary or at least permitted .",
    "this argument , however , fails when the parametric model is only an approximation and the predictors are random .",
    "it will be seen that under these circumstances the population slopes do depend on the predictor distribution which is hence not ancillary .",
    "this effect does not exist when the conditional mean of the response is a linear function of the predictors or the predictors are truly nonrandom .",
    "this article continuous as follows : section  [ sec : data - example ] illustrates discrepancies between standard error estimates with real data examples . section  [ sec : population - framework ] sets up the semi - parametric population framework in which ls approximation extracts the parametric linear component .",
    "section  [ sec : non - ancillarity ] shows how nonlinear conditional expectations invalidate ancillarity of the predictor distribution .",
    "sections  [ sec : observational ] derive a decomposition of asymptotic variance of ls estimates into contributions from noise and from nonlinearity , and central limit theorems associated with the decomposition . section  [ sec : bootstrap - vs - sandwich ] introduces the sandwich estimator of standard error and shows how it is a limiting case of the @xmath5-of-@xmath6 pairs bootstrap .",
    "section  [ sec : adjustment ] expresses parameters , estimates , asymptotic variances and clts in the language of predictor adjustment .",
    "this is critical in order to arrive at expressions that speak to individual predictors in a transparent fashion . in section  [ sec : ses ]",
    "the language of adjustment allows us to define the @xmath8 , that is , the ratio of proper ( assumption - lean ) and improper ( linear models - based ) asymptotic variances and to analyze conditions under which linear models theory yields standard errors that are too liberal ( often ) or too conservative ( rarely ) .",
    "section  [ sec : sandwich - adjusted - and - test ] turns the @xmath8 into a simple test statistic with an asymptotically normal null distribution under `` well - specification '' . the penultimate section  [ app : ave - slopes ] proposes an answer to the question of what the meaning of regression coefficients is when the linear model is only an approximation .",
    "the final section  [ sec : conclusion ] has a brief summary and ends with a pointer to problematic aspects of the sandwich estimator .",
    "throughout we use precise notation for clarity .",
    "the notation may give the impression of a technical article , but all technical results are elementary and all limit theorems are stated informally without regularity conditions .",
    "table  [ tab : la ] shows regression results for a dataset in a sample of 505 census tracts in los angeles that has been used to examine homelessness in relation to covariates for demographics and building usage ( berk et al .",
    "we do not intend a careful modeling exercise but show the raw results of linear regression to illustrate the degree to which discrepancies can arise among three types of standard errors : @xmath11 from linear models theory , @xmath12 from the pairs bootstrap ( @xmath13 ) and @xmath14 from the sandwich estimator ( according to mackinnon and white s ( 1985 ) hc2 proposal ) . ratios of standard errors that are far from + 1 are shown in bold font .",
    ".__comparison of standard errors for the la homeless data . _ _ [ cols=\"<,>,>,>,>,>,>,>,>,>,>\",options=\"header \" , ]     tables  [ tab : la - rav ] and [ tab : boston - rav ] show the results for the two datasets of section  [ sec : data - example ] .",
    "values of @xmath15 that fall outside the middle 95% range of their permutation null distributions are marked with `` * '' .",
    "an objection against using linear fits in the presence of nonlinearities is that slopes lose their common interpretation : no longer is @xmath16 the average difference in @xmath17 associated with a unit difference in @xmath18 at fixed levels of all other @xmath19 . yet",
    ", there exists a simple alternative interpretation that is valid and intuitive even in the presence of nonlinearities , both for the parameters of the population and their estimates from samples : slopes are weighted averages of case - wise slopes or pairwise slopes .",
    "this holds for simple linear regression and also for multiple linear regression for each predictor after linearly adjusting it for all other predictors .",
    "this is made precise as follows :    * * sample estimates * : in a multiple regression based on a sample of size @xmath6 , consider the ls estimate @xmath20 : this is the empirical simple regression slope through the origin with regard to the empirically adjusted predictor @xmath21 ( for @xmath22 as we only consider actual slopes , not the intercept , but assume the presence of an intercept ) . to simplify notation we write @xmath23 for @xmath21 , as well as @xmath24 for the response vector @xmath25 and @xmath26 for the ls estimate @xmath20 .",
    "then the representation of @xmath26 as a weighted average of case - wise slopes is @xmath27 are case - wise slopes and weights , respectively .",
    "+ the representation of @xmath26 as a weighted average of pairwise slopes is @xmath28 are pairwise slopes and weights , respectively .",
    "the summations can be over @xmath29 or @xmath30 .",
    "see figure  [ fig : ave - slopes ] for an illustration .",
    "+      these formulas provide intuitive interpretations of regression slopes that are valid without the first order assumption of linearity of the response as a function of the predictors .",
    "they support the intuition that , even in the presence of a nonlinearity , a linear fit can be used to infer the overall direction of the association between the response and the predictors .",
    "the above formulas were used and modified to produce alternative slope estimates by gelman and park ( 2008 ) , also with the `` goal of expressing regressions as comparisons that can be understood by the general reader '' ( see their sections 1.2 and 2.2 ) .",
    "earlier , wu ( 1986 ) used generalizations from pairs to tuples of size @xmath38 for the analysis of jackknife and bootstrap procedures ( see his section 3 , theorem 1 ) .",
    "the formulas have a history in which stigler ( 2001 ) includes edgeworth , while berman ( 1988 ) traces it back to a 1841 article by jacobi written in latin .",
    "in this article we compared statistical inference from classical linear models theory with inference from econometric theory .",
    "the major differences are that the former is a finite - sample theory that relies on strong assumptions and treats the predictors as fixed even when they are random , whereas the latter uses asymptotic theory that relies on few assumptions and treats the predictors as random . on a practical level , inferences differ in the type of standard error estimates they use : linear models theory is based on the `` ususal '' standard error which is a scaled version of the error standard deviation , whereas econometric theory is based on the so - called `` sandwich estimator '' of standard error which derives from an assumption - lean asymptotic variance . in comparing and contrasting the two modes of statistical inference we observe the following :    * as econometric theory does not assume the correctness of the linearity and homoskedasticity assumptions of linear models theory , a new interpretation of the targets of estimation is needed : linear fits estimate the best linear approximation to a usually nonlinear response surface . *",
    "if statisticians are willing to buy into this semi - parametric view of linear regression , they will accept sandwich - based inference as asymptotically correct . ",
    "if they are unwilling to go down this route , they must have strong belief in the correctness of their models and/or rely on diagnostic methodology to ascertain that linearity and homoskedasticity assumptions are not violated in ways that affect `` usual '' statistical inference . * while regression is rich in model diagnostics , a more targeted approach in this case may be based on misspecification tests which are well - established in econometrics .",
    "we described one such test which permits testing the adequacy of the linear models standard error , one coefficient at a time .",
    "* the discrepancies between standard errors from assumption - rich linear models theory and assumption - lean econometric theory can be of arbitrary magnitude in the asymptotic limit , but real data examples indicate discrepancies by a factors of 2 to be common .",
    "this is obviously relevant because such factors can change a @xmath39-statistic from significant to insignificant and vice versa . *",
    "the pairs bootstrap is seen to be an alternative to the sandwich estimate of standard error .",
    "the latter is the asymptotic limit in the @xmath5-of-@xmath6 bootstrap as @xmath40 .",
    "assumption lean inference is not without its problems .",
    "a major issue is its non - robustness : compared to the standard error from linear models theory the sandwich standard error relies on higher order moments .",
    "the non - robustness is fundamentally a consequence of the ls method , which may suggest that solutions should be obtained through a revival of robustness theory .",
    "9    aldrich ( 2005 ) .",
    "fisher and regression . _ statistical science _ * 20 * ( 4 ) , 4001417 .",
    "angrist , j. d. and pischke , j .- s .",
    "_ mostly harmless econometrics _ , princeton : princeton university press .",
    "belsley , d. a .. , kuh , e. and welsch , r. e. ( 1980 ) .",
    "_ regression diagnostics : identifying influential data and sources of collinearity_. wiley series in probability and mathematical statistics .",
    "hoboken , nj : john wiley & sons , inc .",
    "berk , r. a. , kriegler , b. and yilvisaker , d. ( 2008 ) . counting",
    "the homeless in los angeles county . in _ probability and statistics",
    ": essays in honor of david a. freedman _ , monograph series for the institute of mathematical statistics , d. nolan and s. speed ( eds . )    berman , m. ( 1988 ) . a theorem of jaboci and its generalization .",
    "_ biometrika _ * 75 * ( 4 ) , 779783 .    box , g. e. p. ( 1979 ) .",
    "robustness in the strategy of scientific model building . in _ robustness in statistics : proceedings of a workshop _",
    "( launer , r. l. , and wilkinson , g. n. , eds . ) amsterdam : academic press ( elsevier ) , 201236 .",
    "buja , a. , hastie , t. and tibshirani , r. ( 1989 ) .",
    "linear smoothers and additive models ( with discussions and rejoinder ) . _",
    "the annals of statistics _ , * 17 * ( 2 ) , 453555 .",
    "cox , d. r. and hinkley , d. v. ( 1974 ) . _ theoretical statistics _ , london : chapman & hall .",
    "cox , d.r .",
    "discussion of chatfield ( 1995 ) . _",
    "journal of the royal statistical society , series a _ * 158 * ( 3 ) , 455 - 456 .",
    "freedman , d. a. ( 1981 ) .",
    "bootstrapping regression models . _",
    "the annals of statistics _ * 9 * ( 6 ) , 12181228 .    freedman , d. a. ( 2006 ) . on the so - called `` huber sandwich estimator '' and `` robust standard errors . ''",
    "_ the american statistician _ * 60 * ( 4 ) , 299302 .",
    "gelman , a. and park , d .. k. ( 2008 ) .",
    "splitting a predictor at the upper quarter or third and the lower quarter or third , _ the american statistician _ * 62 * ( 4 ) , 18 .",
    "harrison , x. and rubinfeld , x. ( 1978 ) .",
    "hedonic prices and the demand for clean air .",
    "_ journal of environmental economics and management _ * 5 * , 81102 .",
    "efron , b. ( 1982 ) .",
    "_ the jackknife , the bootstrap and other resampling plans_. philadelphia , pa : society for industrial and applied mathematics ( siam ) .",
    "efron , b. and tibshirani , r. j. ( 1994 ) .",
    "_ an introduction to the bootstrap _ , boca raton , fl : crc press .",
    "hastie , t. j. and tibshirani , r. j. ( 1990 ) .",
    "_ generalized additive models _ , london : chapman & hall / crc monographs on statistics & applied probability .",
    "hausman , j. a. ( 1978 ) .",
    "specification tests in econometrics .",
    "_ econometrica _ * 46 * ( 6 ) , 1251 - 1271 .",
    "hinkley , d. v. ( 1977 ) .",
    "jackknifing in unbalanced situations .",
    "_ technometrics _ * 19 * , 285292 .",
    "huber , p. j. ( 1967 ) . the behavior of maximum likelihood estimation under nonstandard conditions .",
    "proceedings of the fifth berkeley symposium on mathematical statistics and probability , vol . 1 ,",
    "berkeley : university of california press , 221233 .",
    "kauermann , g. and carroll , r. j. ( 2001 ) . a note on the efficiency of sandwich covariance matrix estimation , _ journal of the american statistical association _ * 96*(456 ) , 1387-1396 .",
    "mammen , e. ( 1993 ) . bootstrap and wild bootstrap for high dimensional linear models . _",
    "the annals of statistics _ * 21 * ( 1 ) , 255285 .    mackinnon , j. and white , h. ( 1985 ) . some heteroskedasticity - consistent covariance matrix estimators with improved finite sample properties . _ journal of econometrics _ * 29 * , 305325 .    stigler , s. m. ( 2001 ) . ancillary history . in _ state of the art in probability and statistics :",
    "festschrift for willem r. van zwet _ ( m.  degunst , c.  klaassen and a.  van der vaart , eds . ) , 555567 .",
    "weber , n.c .",
    "( 1986 ) . the jackknife and heteroskedasticity ( consistent variance estimation for regression models ) .",
    "_ economics letters _ * 20 * , 161-163 .",
    "white , h. ( 1980 ) .",
    "using least squares to approximate unknown regression functions . _ international economic review _ * 21 * ( 1 ) , 149-170 .",
    "white , h. ( 1980 ) . a heteroskedasticity - consistent covariance matrix estimator and a direct test for heteroskedasticity .",
    "_ econometrica _ * 48 * , 817-838 .",
    "white , h. ( 1981 ) .",
    "consequences and detection of misspecified nonlinear regression models . _ journal of the american statistical association _ * 76 * ( 374 ) , 419-433 .",
    "white , h. ( 1982 ) . maximum likelihood estimation of misspecified models .",
    "_ econometrica _ * 50 * , 125 .",
    "wu , c. f. j. ( 1986 ) .",
    "jackknife , bootstrap and other resampling methods in regression analysis .",
    "_ the annals of statistics _ * 14 * ( 4 ) , 12611295 .",
    "the facts as layed out in section  [ sec : non - ancillarity ] amount to an argument against conditioning on predictors in regression .",
    "the justification for conditioning derives from an ancillarity argument according to which the predictors , if random , form an ancillary statistic for the linear model parameters @xmath41 and @xmath42 , hence conditioning on @xmath43 produces valid frequentist inference for these parameters ( cox and hinkley 1974 , example  2.27 ) . indeed , with a suitably general definition of ancillarity , it can be shown that in _ any _ regression model the predictors form an ancillary . to see this we need an extended definition of ancillarity that includes nuisance parameters .",
    "the ingredients and conditions are as follows :    * @xmath44 : the parameters , where @xmath45 is of interest and @xmath46 is nuisance ; * @xmath47 : a sufficient statistic with values @xmath48 ; * @xmath49 : the condition that makes @xmath50 an ancillary .",
    "we say that the statistic @xmath50 is ancillary for the parameter of interest , @xmath45 , in the presence of the nuisance parameter , @xmath46 .",
    "condition ( 3 ) can be interpreted as saying that the distribution of @xmath51 is a mixture with mixing distribution @xmath52 .",
    "more importantly , for a fixed but unknown value @xmath46 and two values @xmath53 , @xmath54 , the likelihood ratio @xmath55 has the nuisance parameter @xmath46 eliminated , justifying the conditionality principle according to which valid inference for @xmath45 can be obtained by conditioning on  @xmath50 .",
    "when applied to regression , the principle implies that in _ any _ regression model the predictors , when random , are ancillary and hence can be conditioned on : @xmath56 where @xmath43 acts as the ancillary @xmath50 and @xmath57 as the mixing distribution @xmath58 with a `` nonparametric '' nuisance parameter that allows largely arbitrary distributions for the predictors .",
    "( the predictor distribution should grant identifiability of @xmath59 in general , and non - collinearity in linear models in particular . )",
    "the literature does not seem to be rich in crisp definitions of ancillarity , but see , for example , cox and hinkley ( 1974 , p.32 - 33 ) .",
    "for the interesting history of ancillarity see the articles by stigler ( 2001 ) and aldrich ( 2005 ) .    as explained in section  [ sec : non - ancillarity ] , the problem with the ancillarity argument is that it holds only when the regression model is correct . in practice , whether models are correct is never known .",
    "* error @xmath60 : assuming constancy of the conditional distribution we obtain independence of the errors as follows : @xmath61 = { { \\boldsymbol{e } } } [ { { \\boldsymbol{e}}}[f({{{\\epsilon } } } ) | { { \\vec{\\boldsymbol{x } } } } ] g({{\\vec{\\boldsymbol{x } } } } ) ]                           = { { \\boldsymbol{e } } } [ { { \\boldsymbol{e}}}[f({{{\\epsilon } } } ) ] g({{\\vec{\\boldsymbol{x } } } } ) ]                           = { { \\boldsymbol{e}}}[f({{{\\epsilon } } } ) ] { { \\boldsymbol{e } } } [ g({{\\vec{\\boldsymbol{x } } } } ) ] \\ ] ] conversely , if the conditional distribution of the errors is not constant , there exists @xmath62 such that @xmath63 >    { { \\boldsymbol{e}}}[f({{{\\epsilon}}})]$ ] for @xmath64 for some @xmath65 with @xmath66 > 0 $ ] .",
    "let @xmath67 , and it follows @xmath68 >    { { \\boldsymbol{e}}}[f({{{\\epsilon}}})]\\ , { { \\boldsymbol{e}}}[g({{\\vec{\\boldsymbol{x}}}})]$ ] . *",
    "nonlinearity @xmath69 : the conditional distribution of @xmath69 given @xmath70 is a point mass . the same argument as for errors applies , but restricted to point masses . because @xmath71 = 0 $ ] ( due to the presence of an intercept ) the point masses must be at zero . * total deviation @xmath72 : again , the conditional distribution must be identical across predictor space , which results in both of the previous cases .",
    "the linear case is trivial : if @xmath73 is linear , that is , @xmath74 for some @xmath41 , then @xmath75 irrespective of @xmath76 according to .",
    "the nonlinear case is proved as follows : for any set of points @xmath77 in general position and with 1 in the first coordinate , there exists a unique linear function @xmath78 through the values of @xmath79 .",
    "define @xmath76 by putting mass @xmath80 on each point ; define the conditional distribution @xmath81 as a point mass at @xmath82 ; this defines @xmath83 such that @xmath84 .",
    "now , if @xmath85 is nonlinear , there exist two such sets of points with differing linear functions @xmath86 and @xmath87 to match the values of @xmath85 on these two sets ; by following the preceding construction we obtain @xmath88 and @xmath89 such that @xmath90 .      an important difference between @xmath91 and @xmath92",
    "is that nonlinearities are constrained by orthogonalities to the predictors , whereas conditional error variances are not .    consider first nonlinearities @xmath93 : we construct a one - parameter family of nonlinearities @xmath94 for which @xmath95 = \\infty$ ] and @xmath96 = 0 $ ] . generally in the construction of examples",
    ", it must be kept in mind that nonlinearities are orthogonal to ( adjusted for ) all other predictors : @xmath97 = { { \\boldsymbol{0}}}$ ] . to avoid uninsightful complications arising from adjustment due to complex dependencies among the predictors",
    ", we construct an example for simple linear regression with a single predictor @xmath98 and an intercept @xmath99 . w.l.o.g .",
    "we will further assume that @xmath100 is centered ( population adjusted for @xmath101 , so that @xmath102 ) and standardized .",
    "in what follows we write @xmath32 instead of @xmath100 , and the assumptions are @xmath103=0 $ ] and @xmath104=1 $ ] .",
    "* proposition : *      * @xmath110 = \\infty$ ] ; * @xmath111 = 0 $ ] if the distribution of @xmath32 has no atom at the origin : @xmath112=0 $ ] .    by construction these nonlinearities are centered and standardized , @xmath113 = 0 $ ] and @xmath114 = 1 $ ] .",
    "they are also orthogonal to @xmath32 , @xmath115=0 $ ] , due to the assumed symmetry of the distribution of @xmath32 , @xmath116 = p[x",
    "< -t]$ ] , and the symmetry of the nonlinearities , @xmath117 .    consider next heteroskedastic error variances @xmath92 : the above construction for nonlinearities can be re - used . as with nonlinearities , for @xmath118 $ ] to rise with no bound , the conditional error variance @xmath119 needs to place its large values in the unbounded tail of the distribution of @xmath32 . for @xmath118 $ ] to reach down to zero , @xmath119 needs to place its large values in the center of the distribution of  @xmath32 .",
    "* proposition : *      * @xmath123 = \\infty$ ] ; * @xmath124 = 0 $ ] if the distribution of @xmath32 has no atom at the origin : @xmath112=0 $ ]",
    ".        we write @xmath32 instead of @xmath31 and assume it has a standard normal distribution , @xmath129 , whose density will be denoted by @xmath130 . in figure",
    "[ fig : rav - f ] the base function is , up to scale , as follows : @xmath131 these functions are normal densities up to normalization for @xmath132 , constant 1 for @xmath133 , and convex for @xmath134 .",
    "conveniently , @xmath135 and @xmath136 are both normal densities ( up to normalization ) for @xmath137 : @xmath138 where we write @xmath139 for scaled normal densities . accordingly we obtain the following moments : @xmath140           & = & { { s_1}}\\ , { { \\boldsymbol{e}}}[\\,1\\ , | n(0,{{s_1}}^2 ) ] & = & { { s_1}}&= & ( 1+t/2)^{-1/2 } , \\\\      { { \\boldsymbol{e}}}[f(x ) \\ , x^2 ]    & = & { { s_1}}\\ , { { \\boldsymbol{e}}}[x^2    | n(0,{{s_1}}^2 ) ]   & = & { { s_1}}^3 & = & ( 1+t/2)^{-3/2 } , \\\\      { { \\boldsymbol{e}}}[f^2(x ) ]         & = & { { s_2}}\\ , { { \\boldsymbol{e}}}[\\,1\\ , | n(0,{{s_2}}^2 ) ]   & = & { { s_2}}&= & ( 1+t)^{-1/2 }    , \\\\      { { \\boldsymbol{e}}}[f^2(x ) \\ , x^2 ] & = & { { s_2}}\\ , { { \\boldsymbol{e}}}[x^2     | n(0,{{s_2}}^2 ) ]   & = & { { s_2}}^3 & = & ( 1+t)^{-3/2 }     ,    \\end{array}\\ ] ] and hence @xmath141 ~=~ \\frac { { { \\boldsymbol{e}}}[f^2(x ) \\ , x^2 ] } { { { \\boldsymbol{e}}}[f^2(x ) ] \\ , { { \\boldsymbol{e}}}[x^2 ] }                   ~=~ { { s_2}}^2 ~=~ ( 1+t)^{-1}\\ ] ] figure [ fig : rav - f ] shows the functions as follows : @xmath142 = f(x)^2/{{s_2}}$ ] .      we recall for reference in the following form : @xmath143 for the denominators it is easy to show that @xmath144 ,      \\\\[2pt ]      \\frac{1}{n } \\| { { { { \\boldsymbol{x}}}_{\\!j { { \\hat{\\scriptscriptstyle \\bullet}}}}}}\\|^2      & \\overset{{{\\boldsymbol{p}}}}{\\longrightarrow } &      { { \\boldsymbol{e}}}[\\,{{x_{\\!j { { \\scriptscriptstyle \\bullet}}}}}^2\\ , ] .",
    "\\end{array}\\ ] ] for the numerator a clt holds based on @xmath145 for a proof outline see * details * below .",
    "it is therefore sufficient to show asymptotic normality of @xmath146 . here",
    "are first and second moments : @xmath147        & = &       { { \\boldsymbol{e}}}[{{\\delta}}^2 \\ , { { x_{\\!j { { \\scriptscriptstyle \\bullet}}}}}^2 ]       & = &       { { \\boldsymbol{e}}}[{{\\delta}}^2 ] \\ , { { \\boldsymbol{e}}}[{{x_{\\!j { { \\scriptscriptstyle \\bullet}}}}}^2 ] ,       \\\\[4pt ]      { { \\boldsymbol{v}}}[\\frac{1}{n^{1/2 } } \\langle { { \\boldsymbol{\\delta}}}^2 , { { { { \\boldsymbol{x}}}_{\\!j { { \\scriptscriptstyle \\bullet}}}}}^2 \\rangle ]        & = &       { { \\boldsymbol{e}}}[{{\\delta}}^4 \\ , { { x_{\\!j { { \\scriptscriptstyle \\bullet}}}}}^4 ] -   { { \\boldsymbol{e}}}[{{\\delta}}^2 \\ , { { x_{\\!j { { \\scriptscriptstyle \\bullet}}}}}^2]^2      & = &       { { \\boldsymbol{e}}}[{{\\delta}}^4 ] \\ , { { \\boldsymbol{e}}}[{{x_{\\!j { { \\scriptscriptstyle \\bullet}}}}}^4 ] - { { \\boldsymbol{e}}}[{{\\delta}}^2]^2 \\ , { { \\boldsymbol{e}}}[{{x_{\\!j { { \\scriptscriptstyle \\bullet}}}}}^2]^2 .",
    "\\end{array}\\ ] ] the second equality on each line holds under the null hypothesis of independent @xmath148 and @xmath70 . for the variance one observes that we assume that @xmath149 to be i.i.d .",
    "sampled pairs , hence @xmath150 are @xmath6 i.i.d .",
    "sampled pairs as well . using the denominator terms and slutsky s theorm",
    ", we arrive at the first version of the clt for  @xmath15 : @xmath151 }                                 { { { \\boldsymbol{e}}}[\\,{{\\delta}}^2]^2 } \\ ,                            \\frac{{{\\boldsymbol{e}}}[{{x_{\\!j { { \\scriptscriptstyle \\bullet}}}}}^4 ] }                                 { { { \\boldsymbol{e}}}[{{x_{\\!j { { \\scriptscriptstyle \\bullet}}}}}^2]^2 }                      \\,-\\ , 1 }           \\right )     \\end{array}\\ ] ] with the additional null assumption of normal errors we have @xmath152 = 3 { { \\boldsymbol{e}}}[\\,{{\\delta}}^2]^2 $ ] , and hence the second version of the clt for  @xmath15 : @xmath153 }                                 { { { \\boldsymbol{e}}}[{{x_{\\!j { { \\scriptscriptstyle \\bullet}}}}}^2]^2 }                      - 1 }           \\right ) .",
    "\\end{array}\\ ] ]    * details for the numerator * , using notation of sections  [ sec : population - adjustment ] and  [ sec : sample - adjustment ] , in particular @xmath154 and @xmath155 : @xmath156      & = &          \\langle\\ ,         { { \\boldsymbol{\\delta}}}^2         + ( { { \\boldsymbol{x}}}({{\\boldsymbol{\\hat{\\beta}}}}- { { \\boldsymbol{\\beta}}}))^2         - 2\\ , { { \\boldsymbol{\\delta}}}\\ , ( { { \\boldsymbol{x}}}({{\\boldsymbol{\\hat{\\beta}}}}- { { \\boldsymbol{\\beta } } } ) )        , \\ ,        \\\\        & & ~~        { { { { \\boldsymbol{x}}}_{\\!j { { \\scriptscriptstyle \\bullet}}}}}^2         + ( { { \\boldsymbol{x}}}_{{{\\!\\scriptscriptstyle -}\\!j}}({{\\boldsymbol{\\hat{\\beta}}}}_{{{{\\!\\scriptscriptstyle -}\\!j}}{{\\hat{\\scriptscriptstyle \\bullet } } } } - { { \\boldsymbol{\\hat{\\beta}}}}_{{{{\\!\\scriptscriptstyle -}\\!j}}{{\\scriptscriptstyle \\bullet } } } ) ) ^2         - 2\\ , { { { { \\boldsymbol{x}}}_{\\!j { { \\scriptscriptstyle \\bullet } } } } } ( { { \\boldsymbol{x}}}_{{{\\!\\scriptscriptstyle -}\\!j}}({{\\boldsymbol{\\hat{\\beta}}}}_{{{{\\!\\scriptscriptstyle -}\\!j}}{{\\hat{\\scriptscriptstyle \\bullet } } } } - { { \\boldsymbol{\\beta}}}_{{{{\\!\\scriptscriptstyle -}\\!j}}{{\\scriptscriptstyle \\bullet } } } ) )      \\,\\rangle      \\\\[4pt ]      & = &          \\langle\\ ,         { { \\boldsymbol{\\delta}}}^2 , { { { { \\boldsymbol{x}}}_{\\!j { { \\scriptscriptstyle \\bullet}}}}}^2      \\,\\rangle      + ...        \\end{array}\\ ] ] among the 8 terms in `` ... '' , each contains at least one subterm of the form @xmath157 or @xmath158 , each being of order @xmath159 .",
    "we first treat the terms with just one of these subterms to first power , of which there are only two , normalized by @xmath160 : @xmath161      & = & \\sum_{k=0 ... p } \\ , o_p(1 ) \\",
    ", o_p(n^{-1/2 } )   ~=~ o_p(n^{-1/2 } ) ,      \\\\[8pt ]      \\frac{1}{n^{1/2 } } \\ ,      \\langle\\ ,        { { \\boldsymbol{\\delta}}}^2         , \\ ,        - 2\\ , { { { { \\boldsymbol{x}}}_{\\!j { { \\scriptscriptstyle \\bullet } } } } } ( { { \\boldsymbol{x}}}_{{{\\!\\scriptscriptstyle -}\\!j}}({{\\boldsymbol{\\hat{\\beta}}}}_{{{{\\!\\scriptscriptstyle -}\\!j}}{{\\hat{\\scriptscriptstyle \\bullet } } } } - { { \\boldsymbol{\\beta}}}_{{{{\\!\\scriptscriptstyle -}\\!j}}{{\\scriptscriptstyle \\bullet } } } ) )            \\,\\rangle      & = &      -2 \\ ,        \\sum_{k ( \\neq j ) } \\ ,          \\left (            \\frac{1}{n^{1/2 } }            \\sum_{i=1 ... n }               { { \\delta}}_i^2 x_{i , j{{\\scriptscriptstyle \\bullet } } } x_{i , k }          \\right )          ( { { \\hat{\\beta}}}_{{{{\\!\\scriptscriptstyle -}\\!j}}{{\\hat{\\scriptscriptstyle \\bullet}}},k } - \\beta_{{{{\\!\\scriptscriptstyle -}\\!j}}{{\\scriptscriptstyle \\bullet}},k } )            \\\\[4pt ]      & = &      \\sum_{k ( \\neq j ) } \\ , o_p(1 ) \\ , o_p(n^{-1/2 } ) ~=~ o_p(n^{-1/2 } ) .    \\end{array}\\ ] ] the terms in the big parens are @xmath162 because they are asymptotically normal .",
    "this is so because they are centered under the null hypothesis that @xmath163 is independent of the predictors @xmath164 : in the first term we have @xmath165 = { { \\boldsymbol{e}}}[{{\\delta}}_i ] \\ , { { \\boldsymbol{e}}}[x_{i , k } x_{i , j{{\\scriptscriptstyle \\bullet}}}^2 ] = 0\\ ] ] due to @xmath166 = 0 $ ] .",
    "in the second term we have @xmath167 = { { \\boldsymbol{e}}}[{{\\delta}}_i^2 ] \\ , { { \\boldsymbol{e}}}[x_{i , j{{\\scriptscriptstyle \\bullet } } } x_{i , k } ] = 0\\ ] ] due to @xmath168 = 0 $ ] as  @xmath169 .",
    "we proceed to the 6 terms in that contain at least two @xmath33-subterms or one @xmath33-subterm squared . for brevity",
    "we treat one term in detail and assume that the reader will be convinced that the other 5 terms can be dealt with similarly . here",
    "is one such term , again scaled for clt purposes : @xmath170 the term in the paren converges in probability to @xmath171 $ ] , accounting for `` const '' ; the term @xmath172 is asymptotically normal and hence @xmath162 ; and the term @xmath173 is @xmath159 due to its clt .",
    "for the la homeless data ]"
  ],
  "abstract_text": [
    "<S> we review and interpret the early insights of halbert white who over thirty years ago inaugurated a form of statistical inference for regression models that is asymptotically correct even under `` model misspecification , '' that is , under the assumption that models are approximations rather than generative truths . </S>",
    "<S> this form of inference , which is pervasive in econometrics , relies on the `` sandwich estimator '' of standard error . </S>",
    "<S> whereas linear models theory in statistics assumes models to be true and predictors to be fixed , white s theory permits models to be approximate and predictors to be random . </S>",
    "<S> careful reading of his work shows that the deepest consequences for statistical inference arise from a synergy  a `` conspiracy ''  of nonlinearity and randomness of the predictors which invalidates the ancillarity argument that justifies conditioning on the predictors when they are random . </S>",
    "<S> unlike the standard error of linear models theory , the sandwich estimator provides asymptotically correct inference in the presence of both nonlinearity and heteroskedasticity . </S>",
    "<S> an asymptotic comparison of the two types of standard error shows that discrepancies between them can be of arbitrary magnitude . </S>",
    "<S> if there exist discrepancies , standard errors from linear models theory are usually too liberal even though occasionally they can be too conservative as well . a valid alternative to the sandwich estimator </S>",
    "<S> is provided by the `` pairs bootstrap '' ; in fact , the sandwich estimator can be shown to be a limiting case of the pairs bootstrap . </S>",
    "<S> we conclude by giving meaning to regression slopes when the linear model is an approximation rather than a truth . </S>",
    "<S>   in this review we limit ourselves to linear least squares regression , but many qualitative insights hold for most forms of regression .    </S>",
    "<S> * models as approximations : + how random predictors and model violations + invalidate classical inference in regression *    * dedicated to halbert white ( @xmath02012 ) *    * keywords : * ancillarity of predictors ; first and second order incorrect models ; model misspecification ; econometrics ; sandwich estimator ; pairs bootstrap . </S>"
  ]
}