{
  "article_text": [
    "the long - standing puzzle of  what causes what \" , formally known as the problem of causality inference , is challenging yet central in science  @xcite . understanding causal relationship between events",
    "has important implications in a wide range of areas including as examples social perception  @xcite , epidemiology  @xcite , and econometrics  @xcite .",
    "it is the reliable inference of causality that allows one to untangle complex causal interactions , make predictions , and ultimately design intervention strategies .",
    "traditional approach of inferring causality between two stochastic processes is to perform the granger causality test  @xcite .",
    "a main limitation of this test is that it can only provide information about _ linear _ dependence between two processes , and therefore fails to capture intrinsic nonlinearities that are common in real - world systems . to overcome this difficulty , schreiber developed the concept of _ transfer entropy _ between two processes  @xcite .",
    "transfer entropy measures the uncertainty reduction in inferring the future state of a process by learning the ( current and past ) states of another process .",
    "being an asymmetric measure by design , transfer entropy is often used to infer the directionality of information flow and further the causality between two processes  @xcite .",
    "recently , it becomes increasingly popular to use transfer entropy for causality inference in networks of neurons  @xcite and in coupled dynamical systems with parameter mismatches  @xcite , anticipatory couplings  @xcite , and time delays  @xcite .",
    "however , despite the overwhelming number of proposed applications , a clear interpretation of the resulting relationship inferred by transfer entropy is lacking .    in this paper , we study information transfer in the dynamics of small - scale coupled oscillators networks .",
    "we show by several examples that causal relationship inferred by transfer entropy are often misleading when the underlying system contains indirect connections , dominance of neighboring dynamics , or anticipatory couplings . to account for these effects , we develop a measure called _ causation entropy ( cse ) _ , and",
    "show that its appropriate application reveals true coupling structures of the underlying dynamics .",
    "in this section we introduce the mathematical tools used in this study , which include elements from both dynamical systems and information theory .",
    "our focus of this paper is on discrete dynamical systems of the form @xmath0 where @xmath1 is the state variable and @xmath2 is the dynamic rule of the system . a trajectory ( or orbit ) @xmath3 of eq .",
    "naturally represents a time series . for a continuous dynamical system @xmath4 , a time series can be obtained by sampling its continuous trajectory at discrete time points .",
    "the time points are often chosen to spread uniformly in time or to be the times instances at which the trajectory intersects a given manifold that is transversal to the trajectory , called a _",
    "_  @xcite .    a natural bridge between dynamical systems and information",
    "theory is the formulation of symbolic dynamics , which requires discretization of the phase space . in particular , a finite _ topological partition _",
    "@xmath5 of the phase space @xmath6 is a collection of pairwise disjoint sets in @xmath6 whose union is @xmath6  @xcite .",
    "defining the associated set of symbols @xmath7 , one can transform a trajectory @xmath3 into a _",
    "symbolic sequence _ @xmath8 , where @xmath9 is defined by  @xcite @xmath10    viewing @xmath11 as the sample space , the symbolic sequence @xmath8 can be seen as a time series of a stochastic process .",
    "define a probability measure over the partition @xmath12 , as @xmath13 if @xmath14 is _ invariant _ under the dynamics , then  @xcite @xmath15 a partition @xmath12 is called a _ markov partition _ if it gives rise to a stochastic process that is markovian , i.e. , future states of the process depends only on its current state , and not the past states  @xcite .",
    "consider a discrete random variable @xmath16 whose probability mass function is denoted by @xmath17 . to quantify the unpredictability of @xmath16",
    ", one can calculate its _ ( information ) entropy _ , defined as @xmath18 where by convention , we use  @xmath19 \" to represent  @xmath20 \" . in general , @xmath21 approximates the minimal binary description length @xmath22 of the random variable @xmath16 , with the following inequality  @xcite : @xmath23 it follows that , among all random variables with @xmath24 elements , the one with uniform distribution yields the maximum entropy , @xmath25 .",
    "consider now two random variables @xmath16 and @xmath26 with joint distribution @xmath27 and conditional distribution @xmath28 the _ joint entropy _ @xmath29 and _ conditional entropy _",
    "@xmath30 for @xmath16 and @xmath26 are defined , respectively , as @xmath31 and @xmath32 similar definition holds for @xmath33 .",
    "it is easy to verify that conditioning reduces entropy , i.e. , knowledge of @xmath26 will reduce ( or at least can not increase ) the uncertainty about @xmath16 , i.e. , @xmath34 similarly , @xmath35 .",
    "the reduction of uncertainty of @xmath16 ( @xmath26 ) given full information about @xmath26 ( @xmath16 ) can be measured by the _ mutual information _ between @xmath16 and @xmath26 , as  @xcite @xmath36 the mutual information is symmetric in @xmath16 and @xmath26 , and measures their deviation from independence : if @xmath16 and @xmath26 are fully dependent , then @xmath37 and thus @xmath38 ; on the other hand , if @xmath16 and @xmath26 are independent , then @xmath39 and @xmath40 and therefore @xmath41 . in general",
    ", we have  @xcite @xmath42.\\ ] ]    it is convenient to visualize the relationship between entropy , joint entropy , conditional entropy , and mutual information by a venn - like diagram , as shown in fig .",
    "[ fig:1](a ) .",
    "we now turn to stochastic processes . for a stationary process @xmath43",
    ", its _ entropy rate _",
    "@xmath44 can be defined as @xmath45 which can be thought of as the ( asymptotic ) growth rate of the joint entropy @xmath46 .",
    "if the process is markovian , then  @xcite @xmath47    for two stochastic processes @xmath43 and @xmath48 , the reduction of uncertainty about @xmath49 due to the information of the past @xmath50 states of @xmath26 , represented by @xmath51 in addition to the information of the past @xmath52 states of @xmath16 , represented by @xmath53 is measured by the _ transfer entropy _ from @xmath26 to @xmath16 , defined as  @xcite @xmath54 one can similarly define @xmath55 , which does not necessarily equal to @xmath56 .",
    "note that @xmath56 can also be interpreted as the mutual information between @xmath49 and @xmath57 conditioned on @xmath58 . in this paper",
    ", we focus on the case where @xmath59 unless specified otherwise .",
    "the relationship between transfer entropy , entropy and conditional entropy are illustrated in fig .",
    "[ fig:1](b ) .",
    "coupled oscillator networks are commonly used for modeling the dynamic behavior of complex systems in various areas  @xcite . here",
    "we consider discrete dynamics of coupled oscillator networks , in the form @xmath60 + \\epsilon\\sum_{j\\neq{i}}c_{ij}g[x^{(i)}_t , x^{(j)}_t],~~i=1,2,\\dots , n.\\ ] ] here @xmath61 is the state of oscillator @xmath62 at time @xmath63 , @xmath2 is the dynamics of individual oscillators , @xmath64 is the coupling function , and @xmath65 is the coupling strength .",
    "term @xmath66 represents the coupling from @xmath67 to @xmath62 . in this paper , we use @xmath68 with parameter @xmath69 .",
    "the coupling function is chosen to be @xmath70 the choice of @xmath71 $ ] and normalization condition @xmath72 guarantees that @xmath73\\mbox{~for all $ i$ and $ t$}.\\ ] ]    we first explore information transfer in two coupled oscillators , with bidirectional and unidirectional couplings , respectively . with a slight abuse of notation ,",
    "we use @xmath16 and @xmath26 to represent oscillators @xmath74 and @xmath75 . in terms of eq .",
    ", the bidirectional coupling corresponds to having @xmath76 and unidirectional coupling corresponds to @xmath77 .",
    "results from numerical simulation are shown in fig .",
    "[ fig:2 ] .",
    "one direct observation is that mutual information can be used as a measure of synchrony between two oscillators @xmath16 and @xmath26 . when @xmath16 and @xmath26 are synchronized , their mutual information @xmath78 when they are not synchronized , @xmath79.\\ ] ] we remark that this observation suggests a new and alternative way of measuring generalized synchronization or synchronization among a partial set of nodes in a large - scale network  @xcite .    for bidirectionally",
    "coupled oscillators , synchrony occurs when the coupling strength  @xcite @xmath80 as shown in fig .",
    "[ fig:2](a ) .",
    "the mutual information reaches its maximum for the same range of @xmath65 .",
    "similarly , synchronization and maximum mutual information both occur when @xmath81,\\ ] ] in the case where @xmath16 and @xmath26 are unidirectionally coupled [ fig .  [ fig:2](c ) ] .",
    "figure  [ fig:2](b , d ) show typical time series of the bidirectionally and unidirectionally coupled oscillators with @xmath82 ( unsynchronized trajectories ) and @xmath83 ( synchronized trajectories ) , respectively .",
    "when two oscillators synchronize , the transfer entropy from either one of them to the other becomes zero because no extra information can be gained by learning the past trajectory of the other oscillator ( in addition to that from one s own ) . as a consequence , detection of coupling by transfer entropy ( or any other measure ) is valid only when the oscillators are not synchronized .",
    "oscillators that are synchronized produce identical trajectories and therefore appear indistinguishable .",
    "when the two oscillators are not synchronized , there is a positive transfer entropy following the directionality of coupling . for bidirectionally coupled oscillators , @xmath84,\\ ] ] except for a few parameters at which the trajectories of @xmath16 and @xmath26 settle into a periodic orbit [ fig .",
    "[ fig:2](a ) ] . for unidirectionally",
    "coupled oscillators , positive transfer entropy @xmath85 is observed when @xmath86 this absolute asymmetry of transfer entropy confirms the dominant direction of information flow from @xmath16 to @xmath26 , and not the other way around [ fig .",
    "[ fig:2](c ) ] .",
    "having studied the application of transfer entropy in systems of two coupled oscillators , we now turn to networks .",
    "first we explore information transfer under the presence of indirect couplings .",
    "consider a directed linear chain @xmath87 where @xmath88 indirectly influences @xmath16 through @xmath26 [ fig .",
    "[ fig:4](a ) ] .",
    "we focus on the dynamics of this three - node network according to eq .  , with @xmath89 $ ] , a regime where coupling has a non - negligible effect on the dynamics but not strong enough to result in synchronization .    in fig .",
    "[ fig:5](a ) we plot values of the transfer entropies @xmath90 , @xmath91 , and @xmath92 ( @xmath93 are not plotted ) . by definition , @xmath94 .",
    "the direct influence of @xmath26 on @xmath16 is validated by the positive values of @xmath91 .",
    "interestingly , values @xmath92 are also positive , despite the fact that there is no direct coupling from @xmath88 to @xmath16 .",
    "similar results are found for other networks that contain the direct linear chain @xmath95 but without the direct coupling @xmath96 .",
    "see fig .",
    "[ fig:4](b - c ) for the other two networks and fig .",
    "[ fig:5](c , e ) for the corresponding results .    one important implication of these results is that , the use of transfer entropy for inferring network structure can be inappropriate under the presence of indirect influences . since indirect couplings are common in many networks , _ directed edges that are inferred by measuring transfer entropy",
    "can often be  false positive \" . _      we note that the key reason transfer entropy often fails in identifying indirect couplings from direct ones is that it is a pairwise measure between two processes .",
    "for example , the transfer entropy @xmath97 shown in fig .  [ fig:5](a , c , e ) does not account for the fact that the observed information transfer from @xmath88 to @xmath16 is indeed a consequence of the direct information transfer from @xmath88 to @xmath26 , and then @xmath26 to @xmath16 .",
    "here we propose a new measure , which we call _ causation entropy_. the causation entropy from @xmath88 to @xmath16 ( conditioned on @xmath16 and @xmath26 ) is defined as @xmath98 thus , @xmath99 measures the extra information provided to @xmath16 by @xmath88 _ in addition _ to the information that is already provided to @xmath16 by other means .    for an arbitrary set of processes ,",
    "causation entropy is defined as follows .",
    "the causation entropy from process @xmath100 to process @xmath101 conditioned on the set of processes @xmath102 is defined as @xmath103    causation entropy @xmath104 is a generalization of transfer entropy .",
    "in fact , by letting @xmath105 , we have @xmath106 in general , causation entropy @xmath104 measures the reduction in uncertainty in @xmath101 due to the extra knowledge of @xmath100 in addition to that of @xmath102 .    if @xmath107 , we simply write @xmath108 it follows that @xmath109 which is the mutual information between @xmath110 and @xmath111 . when @xmath112 , causation entropy @xmath104 can be interpreted as the mutual information shared between @xmath110 and @xmath111 conditioned on @xmath113 .",
    "figure  [ fig:5](b , d , f ) shows that , for the networks in fig .",
    "[ fig:4](a - c ) , both @xmath114 and @xmath115 are positive , as a result of the influence of @xmath16 on itself ( self - dynamics ) and the direct influence of @xmath26 on @xmath16 . on the other hand , and by design , the causation entropy @xmath116 , in sharp contrast to the positive transfer entropy , @xmath117 [ fig .  [ fig:5](a , c , e ) ] .",
    "the reason @xmath99 is close to zero is that , the information provided by @xmath88 ( to @xmath16 ) is merely a subset of the information provided by @xmath26 .",
    "no extra information about @xmath16 s future state can be gained by learning the current state of @xmath88 if those of @xmath16 and @xmath26 are already known .",
    "dominance of neighbors refers to a scenario where an oscillator s future state is dominantly determined by the state of its neighboring nodes , rather than by itself . in terms of eq .",
    ", this occurs when the coupling strength @xmath118 .",
    "we here explore its effect on information transfer .",
    "as an example , we consider dynamics by eq .   on the network shown in fig .",
    "[ fig:4](d ) , where node @xmath16 receives input from @xmath26 , but not from @xmath88 ( even indirectly ) .    as shown in fig .",
    "[ fig:6](a ) , transfer entropy @xmath56 is positive , due to the direct influence of @xmath26 on @xmath16 .",
    "surprisingly , transfer entropy @xmath97 is also found to be positive , despite the fact that no information flows from @xmath88 to @xmath16 , either directly or indirectly .",
    "the reason positive transfer entropy @xmath97 is found in the absence of influence of @xmath88 on @xmath16 is that , @xmath97 is taken to be the difference between @xmath119 and @xmath120 . here since",
    "@xmath49 is dominantly determined by @xmath121 and only depends weakly on @xmath122 , the conditional entropies @xmath123 a closer inspection of the network reveals that , under the strong coupling regime where the dynamics of an oscillator depends dominantly on its neighbors dynamics , the state of @xmath49 depends mostly on @xmath121 ( and not @xmath122 ) . since @xmath121 depends mostly on @xmath124 by the very same argument , we conclude that the mutual information between @xmath49 and @xmath124 is high .",
    "similarly , since @xmath125 depends mostly on @xmath124 and @xmath126 , there is high mutual information between @xmath125 and @xmath124 .",
    "based on this analysis , the mutual information between @xmath124 and @xmath122 should be low and that between @xmath125 and @xmath49 should be nonnegilible , which is confirmed in fig .",
    "[ fig:6](c - d ) .",
    "although information in the network flows directly from @xmath16 to @xmath88 , without accounting for the dominant factors that determine the value of @xmath49 , one would indeed infer a directed link from @xmath88 to @xmath16 based on the calculation of the transfer entropy @xmath97 .",
    "we note that , because of the dominance of @xmath26 on @xmath16 ( as opposed to @xmath16 on itself ) , one should indeed measure the causation entropies @xmath127 , @xmath128 , and @xmath99 , respectively .",
    "results are shown in fig .",
    "[ fig:6](b ) .",
    "the value @xmath129 , as expected .",
    "the value @xmath130 , due to the dominant influence of @xmath26 ( rather than @xmath16 itself ) on @xmath16 .",
    "the value @xmath131 as well , suggesting the absence of information transfer from @xmath88 to @xmath16 , which is consistent with the structure of the network shown in fig .",
    "[ fig:4](d ) .",
    "the determination of causation entropies ( i.e. , the order @xmath133 ) can in fact be done _ a priori _ , by first choosing the process @xmath134 that maximizes the causation entropy @xmath135 , and then iteratively select @xmath136 as the process that maximizes @xmath137 ( see the following paragraph for details ) . for the example used in fig .",
    "[ fig:6 ] , we found that @xmath138 , and @xmath139 .",
    "therefore , contrast to transfer entropy , causation entropy can successfully identify the dominance of neighbors and in turn avoid erroneous inference of couplings due to its effect .    for a network of @xmath132 coupled stochastic processes @xmath140",
    ", we propose to identify the set of causal processes of a given process @xmath62 by iterative maximization of causation entropy .",
    "let @xmath141 .",
    "we first find process @xmath142 that satisfies @xmath143 then we iteratively seek for @xmath144 ( @xmath145 ) that satisfies @xmath146 we stop the search at step @xmath147 when @xmath148 where @xmath149 is a preselected tolerance value .",
    "the processes @xmath150 ( in the decreasing order of dominance ) form the set of causal processes of @xmath62 .    note that in theory the value of @xmath151 will be exactly zero if the dynamics of node @xmath144 does not causal - determine the dynamics of node @xmath62 . in practice , however , the numerical estimation of @xmath151 is based on the estimation of probability distributions from finite sample , and will be close to ( but not necessarily equal to ) zero for finite number of data points . a rigorous way of determining whether the numerically computed causation entropy should be identified as zero is to perform a hypothesis test . it can be challenging to do such a test in practice and often times one can instead use a shuffle test to obtain approximate confidence intervals  @xcite .",
    "our last example is a unidirectionally coupled dynamical system with anticipatory coupling  @xcite @xmath152 , \\end{cases}\\ ] ] where @xmath153 with @xmath69 , parameter @xmath71 $ ] is the coupling strength , and parameter @xmath154 $ ] is the strength of anticipatory coupling .",
    "notation @xmath155 means that the map @xmath156 is applied twice .",
    "here we adopt the concept and notation of transfer entropy to define @xmath157 and @xmath158 figure  [ fig:7](a ) shows that both @xmath159 and @xmath160 are positive , with comparable values .",
    "does this suggest that both @xmath122 and @xmath49 independently influence @xmath161 ?",
    "standard interpretation ( of transfer entropy ) would suggest that the answer to this question is yes .",
    "by use of causation entropy , we find that @xmath161 is primarily determined by @xmath121",
    ". the second dominant influence on @xmath161 is @xmath49 , as confirmed by the values of @xmath162 it turns out that additional information of @xmath122 ( beyond @xmath121 and @xmath49 ) does not contribute to the reduction of uncertainty of @xmath161 .",
    "this is validated by the causation entropy @xmath163 which remain close to zero , as shown in fig .",
    "[ fig:7](b ) .    therefore ,",
    "in contrast to transfer entropy analysis , which would suggest that both @xmath122 and @xmath49 participate in the determination of @xmath161 , causation entropy analysis reveals that information of @xmath122 is indeed completely redundant in inferring @xmath161 .",
    "in fact , by expressing @xmath164 as @xmath165 in eq .",
    ", it appears that the value of @xmath166 depends solely on @xmath167 and @xmath165 , and not on @xmath168 .",
    "the effects of time - dependent structures on network dynamics are often intriguing and pose considerable challenges for analysis . for example , the problem of synchronization stability of coupled oscillators in time - dependent networks has been fully addressed only for a few specific cases  @xcite . here , our focus is to measure information transfer among oscillators that are coupled through a time - dependent network structure ( that is , a network whose edges change in time ) . in particular",
    ", we generalize eq .   to allow for time - dependent interactions in between oscillators , as @xmath169 + \\epsilon\\sum_{j\\neq{i}}c_{ij}(t)g[x^{(i)}_t , x^{(j)}_t],~~i=1,2,\\dots , n.\\ ] ] here all terms in eq .   except for @xmath170",
    "are the same as those in eq .  .",
    "the term @xmath170 represents the coupling from @xmath67 to @xmath62 at time @xmath63 and explicitly accounts for the time - dependent network structure .",
    "we consider time - dependent networks constructed as follows .",
    "start with a baseline static network whose adjacency matrix is @xmath171_{n\\times n}$ ] . the edges in the network are then allowed to  blink \" according to the following rule , to generate a time - dependent network : at each time @xmath63 , @xmath172 therefore , when the blinking probability @xmath173 , the network is the same as the baseline static network ; on the other extreme , when @xmath174 , no edge exists and the network becomes empty ( i.e. , each oscillator is isolated and does not couple to other oscillators ) . for the values of @xmath175 in between @xmath176 and @xmath74 ,",
    "the network structure changes in time in a stochastic fashion ( see fig .",
    "[ fig:9 ] for a few illustrative examples ) .",
    "our interest lie in the information transfer within such time - dependent networks .",
    "different from its static counterpart , the flow of information in a time - dependent network often can not be directly obtained from examining the edges @xmath170 , because it is possible for a network to be disconnected at all times and yet be able to transfer information from one node to another .",
    "such scenario has been previously considered in the synchronization of coupled oscillators in time - dependent networks with edges being switched on and off  @xcite and in moving - neighbor networks whose edges are defined by the local interactions between agents that move in space  @xcite . in both cases , even though the original static network is connected , the corresponding time - dependent network obtained by blinking the edges might not be ( see fig .",
    "[ fig:9 ] for examples ) .",
    "the connection between these time - dependent networks and the original static network is that the asymptotic temporal average of each directed edge , @xmath177 , is proportional to the weight of the same edge in the static network : @xmath178    we ran numerical simulation on several time - dependent networks and focus on the information flow measured both by transfer entropy and causation entropy .",
    "figure  [ fig:8](a ) shows that , for the directed linear chain @xmath95 with fixed coupling strength @xmath179 , when the blinking probability @xmath175 increases , the transfer entropy @xmath92 becomes increasingly nonnegligible , indicating direct information transfer from @xmath88 to @xmath16 from standard interpretation . on the other hand ,",
    "the causation entropy @xmath180 remains essentially zero , suggesting that the information transferred from @xmath88 to @xmath16 is merely a redundancy of the information that are transferred from @xmath88 to @xmath26 and @xmath26 to @xmath16 , respectively , possibly at different times .",
    "figure  [ fig:8](b - c ) show similar comparison between transfer entropy and causation entropy for meaning the information flow in time - dependent networks that originate from the networks shown in fig .",
    "[ fig:4](b - c ) with the fixed coupling strength @xmath179 .",
    "the possible misinterpretation of transfer entropy becomes more evident under the dominance of neighbors scenario , where the coupling strength @xmath65 is close to @xmath74 .",
    "as shown in fig .",
    "[ fig:8](d ) , under such scenario , transfer entropy identifies a strong information transfer from @xmath88 to @xmath16 whereas in the average network of the time - dependent network , it is the exact opposite .",
    "causation entropy , on the other hand , successfully identifies the dominant nodes that influence the dynamics of @xmath16 , namely , its neighbor @xmath26 and then @xmath16 itself .",
    "our main message here is that while being an essential problem in science in general , and dynamical systems in particular , the question of what is cause and what is influence in complex system analysis is challenging , not due to the lack of methodology , but rather due to the lack of clear and comprehensible understanding of the applicability of proposed methods , in particular when the underlying system involves complex interactions . the popular concept of transfer entropy has been used lately to serve as a way of inferring causality , without much understanding about its domain of success .",
    "we here explored information flow measured from the dynamics of small - scale coupled oscillators network , attempting to gain insights into the validity of transfer entropy as well as its limitations . for two coupled oscillators ,",
    "transfer entropy is found to successfully detect the directionality of information flow , even in cases where the couplings are blinking ( time - dependent ) . however , its validity breaks down under the presence of indirect couplings , dominance of neighboring dynamics , and anticipatory couplings , which are common in large - scale complex systems .",
    "to overcome the limitations of transfer entropy , we introduced a new measure of information flow called causation entropy , which is designed to allow inference of causation despite the presence of primary and secondary influences between elements of a larger coupled system .",
    "we highlighted the success of our approach with several examples where specifically the transfer entropy can not distinguish between causation and independence but causation entropy successfully infers the true causal relationships .",
    "given the recent advancements in estimating transfer entropy in rather general settings including multivariate time series and infinite time delay  @xcite , it is our hope to build on the idea of causation entropy to explore information flow and coupling inference in larger - scale systems , which are important for a wide range of applications across scientific fields .",
    "one challenge is that , for large - scale systems , naive binning methods would require an exponential number of data points with respect to the number of variables , in order to reliably calculating entropies ( including joint entropy , transfer entropy , and also causation entropy ) .",
    "nonparametric density estimation methods previously developed for mutual information  @xcite are likely to offer a route towards the reliable estimation of causation entropies in large - scale dynamical systems .",
    "we thank dr samuel stanton from the aro complex dynamics and systems program for his ongoing and continuous support .",
    "this work is funded by aro grant no .",
    "61386-eg .",
    "bollt , t.  stanford , y - c .",
    "lai , k.  yczkowski , what symbolic dynamics do we get with a misplaced partition ? on the validity of threshold crossings analysis of chaotic time - series , physica d 154 ( 2001 ) 259286 .                           and @xmath181 , joint entropy @xmath29 , conditional entropies @xmath30 and @xmath33 , and mutual information @xmath182 , of two random variables @xmath16 and @xmath26 .",
    "( b ) relations between : transfer entropy @xmath91 , entropies of random variables @xmath49 , @xmath122 , and @xmath121 , and their joint and conditional entropies .",
    "the transfer entropy is the difference between the conditional entropies @xmath183 and @xmath119 , which measures the extra information provided by @xmath121 ( in addition to @xmath122 ) in the determination of @xmath49 . , scaledwidth=80.0% ]     and transfer entropies @xmath85 and @xmath91 on coupling strength @xmath65 for two bidirectionally coupled oscillators .",
    "synchronization occurs when @xmath184 , which is the same region where the mutual information reaches its maximum . due to the symmetry of coupling , @xmath185 .",
    "( b ) typical time series for two bidirectionally coupled oscillators , with @xmath82 ( top , unsynchronized trajectories ) and @xmath83 ( bottom , synchronized trajectories ) . ( c - d ) same as ( a - b ) , but for two oscillators with unidirectional coupling from oscillator @xmath16 to oscillator @xmath26 . in this case",
    ", synchronization appears when @xmath186 . for @xmath187 , @xmath188 , indicating that the dominant direction of information flow between @xmath16 and @xmath26 is from @xmath16 to @xmath26 . in all simulations of the paper ,",
    "we generate trajectories of length @xmath189 and discard the initial @xmath190 segments for all information measures .",
    "the interval @xmath191 $ ] is divided evenly into @xmath192 subintervals for the estimation of discrete probabilities . in our simulations , we made the choice of @xmath192 based on the balance between the length of the time series and the number of variables in the joint distribution : too few subintervals will only reveal limited information about the true dynamics and on the other hand , too many of them will lead to statistical under - sampling  @xcite .",
    "note that this problem of finding an appropriate number of subintervals for the estimation of entropy is analogous to the problem of finding an appropriate number of bins to construct a histogram , for which no  best \" solution exists in general . , scaledwidth=80.0% ]       and causation entropies @xmath195 for the network shown in fig .",
    "[ fig:4](a ) with dynamics  . note that the transfer entropy @xmath92 is positive despite the absence of direct coupling from @xmath88 to @xmath16 . on the other hand , the causation entropy @xmath196 ,",
    "since information that are being indirected transferred from @xmath88 to @xmath16 all go through @xmath26 .",
    "( c - d ) same as ( a - b ) , for the network in fig .",
    "[ fig:4](b ) .",
    "( e - f ) same as ( a - b ) , for the network in fig .  [ fig:4](c ) .",
    ", scaledwidth=80.0% ]    .",
    "( b ) causation entropies @xmath197 for the network in fig .",
    "[ fig:4](d ) whose dynamics follow eq .  .",
    "( c ) scatter plot between @xmath124 and @xmath122 for @xmath198 .",
    "( d ) scatter plot between @xmath199 and @xmath122 for @xmath198 . in ( c ) and",
    "( d ) , points are taken from a randomly select trajectory segment ( of the full trajectory ) of length @xmath200 .",
    ", scaledwidth=80.0% ]      ) .",
    "second to the last ( rightmost ) columns : typical network structures at different times , obtained from keeping each directed edge of the static network independently with probability @xmath204 at each time @xmath63 . , scaledwidth=80.0% ]     and causation entropies @xmath195 for the time - dependent network originates from the network in fig .",
    "[ fig:4](a ) via eq .   and endowed with dynamics  , for the fixed coupling strength @xmath179 .",
    "( c - f ) same as ( a - b ) , for the networks in fig .",
    "[ fig:4](b ) and fig .",
    "[ fig:4](c ) , respectively , and @xmath179 .",
    "( g - h ) same as ( a - b ) , for the network in fig .",
    "[ fig:4](d ) and @xmath205 .",
    ", scaledwidth=80.0% ]"
  ],
  "abstract_text": [
    "<S> inference of causality is central in nonlinear time series analysis and science in general . a popular approach to infer causality between two processes is to measure the information flow between them in terms of transfer entropy . using dynamics of coupled oscillator networks , </S>",
    "<S> we show that although transfer entropy can successfully detect information flow in two processes , it often results in erroneous identification of network connections under the presence of indirect interactions , dominance of neighbors , or anticipatory couplings . </S>",
    "<S> such effects are found to be profound for time - dependent networks . to overcome these limitations </S>",
    "<S> , we develop a measure called _ causation entropy _ , and show that its application can lead to reliable identification of true couplings .    </S>",
    "<S> causality inference , causation entropy , coupled oscillator networks , blinking couplings </S>"
  ]
}