{
  "article_text": [
    "today s multicore x86 processors bear multiple complexities when aiming for high performance . conventional performance tuning tools like intel vtune , oprofile , codeanalyst , openspeedshop , etc .",
    ", require a lot of experience in order to get sensible results . for this reason they are usually unsuitable for the scientific user , who would often be satisfied with a rough overview of the performance properties of their application code .",
    "moreover , advanced tools often require kernel patches and additional software components , which makes them unwieldy and bug - prone .",
    "additional confusion arises with the complex multicore , multicache , multisocket structure of modern systems ( see fig .  [",
    "fig : nehalem_socket ] ) ; users are all too often at a loss about how hardware thread ids are assigned to resources like cores , caches , sockets and numa domains . moreover ,",
    "the technical details of how threads and processes are bound to those resources vary strongly across compilers and mpi libraries .",
    "likwid ( `` like i knew what i m doing '' ) is a set of easy to use command line tools to support optimization .",
    "it is targeted towards performance - oriented programming in a linux environment , does not require any kernel patching , and is suitable for intel and amd processor architectures .",
    "multithreaded and even hybrid shared / distributed - memory parallel code is supported .",
    "it comprises the following tools :    * ` likwid - features ` can display and alter the state of the on - chip hardware prefetching units in intel x86 processors . * ` likwid - topology ` probes the hardware thread and cache topology in multicore , multisocket nodes .",
    "knowledge like this is required to optimize resource usage like , e.g. , shared caches and data paths , physical cores , and ccnuma locality domains , in parallel code . * ` likwid - perfctr ` measures performance counter metrics over the complete runtime of an application or , with support from a simple api , between arbitrary points in the code . counter multiplexing allows the concurrent measurement of a large number of metrics , larger than the ( usually small ) number of available counters .",
    "although it is possible to specify the full , hardware - dependent event names , some predefined event sets simplify matters when standard information like memory bandwidth or flop counts is needed .",
    "* ` likwid - pin ` enforces thread - core affinity in a multi - threaded application `` from the outside , ''",
    "i.e. , without changing the source code .",
    "it works with all threading models that are based on posix threads , and is also compatible with hybrid `` mpi+threads '' programming .",
    "sensible use of likwid - pin requires correct information about thread numbering and cache topology , which can be delivered by likwid - topology ( see above ) .",
    "although the four tools may appear to be partly unrelated , they solve the typical problems application programmers have when porting and running their code on complex multicore / multisocket environments .",
    "hence , we consider it a natural idea to provide them as a single tool set .",
    "this paper is organized as follows .",
    "section  [ sec : tools ] describes the four tools in some detail and gives hints for typical use . in section  [ sec : papi ] we briefly compare likwid to the papi feature set .",
    "section  [ sec : cases ] demonstrates the use of likwid in three different case studies , and section  [ sec : conc ] gives a summary and an outlook to future work .",
    "likwid only supports x86-based processors . given the strong prevalence of those architectures in the hpc market ( e.g. , 90% of all systems in the latest top 500 list are of x86 type ) we do not consider this a severe limitation . in other areas like , e.g. , workstations or desktops , the x86 dominance is even larger .    in the following",
    "we describe the four tools in detail .",
    "hardware - specific optimization requires an intimate knowledge of the microarchitecture of a processor and the characteristics of the code .",
    "while many problems can be solved with profiling , common sense , and runtime measurements , additional information is often useful to get a complete picture .",
    "performance counters are facilities to count hardware events during code execution on a processor .",
    "since this mechanism is implemented directly in hardware there is no overhead involved .",
    "all modern processors provide hardware performance counters , but their primary purpose is to support computer architects during the implementation phase .",
    "still they are also attractive for application programmers , because they allow an in - depth view on what happens on the processor while running applications .",
    "there are generally two options for using hardware performance counter data : either event counts are collected over the runtime of an application process ( or probably restricted to certain code parts via an appropriate api ) , or overflowing hardware counters can generate interrupts , which can be used for ip or call - stack sampling .",
    "the latter option enables a very fine - grained view on a code s resource requirements ( limited only by the inherent statistical errors ) .",
    "however , the first option is sufficient in many cases and also practically overhead - free .",
    "this is why it was chosen as the underlying principle for likwid - perfctr .",
    "the probably best known and widespread existing tool is the papi library @xcite , for which we provide a detailed comparison to likwid - perfctr in section  [ sec : comp_papi ] .",
    "a lot of research is targeted towards using performance counter data for automatic performance analysis and detecting potential performance bottlenecks @xcite",
    ". however , those solutions are often too unwieldy for the common user , who would prefer a quick overview as a first step in performance analysis .",
    "a key design goal for likwid - perfctr was ease of installation and use , minimal system requirements ( no additional kernel modules and patches ) , and  at least for basic functionality  no changes to the user code .",
    "a prototype for the development of likwid - perfctr is the sgi tool `` perfex , '' which was available on mips - based irix machines as part of the `` speedshop '' performance suite .",
    "cray provides a similar , papi - based tool ( craypat ) on their systems .",
    "likwid - perfctr offers comparable or improved functionality with regard to hardware performance counters on x86 processors , and is available as open source .",
    "hardware performance counters are controlled and accessed using processor - specific hardware registers ( also called _ model specific registers _",
    "( msr ) ) .",
    "likwid - perfctr uses the linux `` msr '' module to modify the msrs from user space .",
    "the msr module is available in all linux distributions with a 2.6 linux kernel and implements the read / write access to msrs based on device files .",
    "likwid - perfctr is a command line tool that can be used as a wrapper to an application .",
    "it allows simultaneous measurements on multiple cores .",
    "events that are shared among the cores of a socket ( this pertains to the `` uncore '' events on core i7-type processors ) are supported via `` socket locks , '' which enforce that all uncore event counts are assigned to one thread per socket .",
    "events are specified on the command line , and the number of events to count concurrently is limited by the number of performance counters on the cpu .",
    "these features are available without any changes in the user s source code . a small instrumentation ( `` marker '' ) api allows one to restrict measurements to certain parts of the code ( named regions ) with automatic accumulation over all regions of the same name .",
    "an important difference to most existing performance tools is that event counts are strictly core - based instead of process - based : everything that runs and generates events on a core is taken into account ; no attempt is made to filter events according to the process that caused them .",
    "the user is responsible for enforcing appropriate affinity to get sensible results .",
    "this could be achieved via likwid - pin ( see below for more information ) :    .... $ likwid - perfctr -c 1 \\",
    "-g simd_comp_inst_retired_packed_double : pmc0,\\        simd_comp_inst_retired_scalar_double : pmc1 \\           likwid - pin -c 1 ./a.out ....    ( see below for typical output in a more elaborate setting . ) in this example , the computational double precision packed and scalar sse retired instruction counts on an intel core 2 processor are assigned to performance counters 0 and 1 and measured on core 1 over the duration of ` a.out ` s runtime .",
    "the ` likwid - pin ` command is used here to bind the process to this core . as a side effect , it becomes possible to use likwid - perfctr as a monitoring tool for a complete shared - memory node , just by specifying all cores for measurement and , e.g. , `` ` sleep ` '' as an application :    .... $ likwid - perfctr -c 0 - 7 \\     -g simd_comp_inst_retired_packed_double : pmc0,\\        simd_comp_inst_retired_scalar_double : pmc1 \\           sleep 1 ....    apart from naming events as they are documented in the vendor s manuals , it is also possible to use preconfigured _ event sets _ ( groups ) with derived metrics .",
    "this provides a simple abstraction layer in cases where standard information like memory bandwidth , flops per second , etc .",
    ", is sufficient :    .... $ likwid - perfctr -c 0 - 3   \\",
    "-g flops_dp   ./a.out ....    at the time of writing , the following event sets are defined :    r|m5.5 cm * event set & * function + flops_dp & double precision mflops / s + flops_sp & single precision mflops / s + l2 & l2 cache bandwidth in mbytes / s + l3 & l3 cache bandwidth in mbytes / s + mem & main memory bandwidth in mbytes / s + cache & l1 data cache miss rate / ratio + l2cache & l2 data cache miss rate / ratio + l3cache & l3 data cache miss rate / ratio + data & load to store ratio + branch & branch prediction miss rate / ratio + tlb & translation lookaside buffer miss rate / ratio * *    the event groups are partly inspired from a technical report published by amd @xcite .",
    "we try to provide the same preconfigured event groups on all supported architectures , as long as the native events support them .",
    "this allows the beginner to concentrate on the useful information right away , without the need to look up events in the manuals ( similar to papi s high - level events ) .",
    "the interactions between event sets , hardware events , and performance counters are illustrated in fig .",
    "[ fig : perfctr ] . in the usage scenarios",
    "described so far there is no interference of likwid - perfctr while user code is being executed ,",
    "i.e. , the overhead is very small ( apart from the unavoidable api call overhead in marker mode ) . if the number of events is larger than the number of available counters , this mode of operation requires running the application more than once .",
    "for ease of use in such situations , likwid - perfctr also supports a _ multiplexing mode _ , where counters are assigned to several event sets in a `` round robin '' manner . on the downside ,",
    "short - running measurements will then carry large statistical errors .",
    "multiplexing is supported in wrapper and marker mode .",
    "the following example illustrates the use of the marker api in a serial program with two named regions ( `` ` main ` '' and `` ` accum ` '' ) :    .... # include < likwid.h > ... int coreid = likwid_processgetprocessorid ( ) ; printf(\"using likwid\\n \" ) ; likwid_markerinit(numberofthreads , numberofregions ) ; int mainid   = likwid_markerregisterregion(\"main \" ) ; int accumid = likwid_markerregisterregion(\"accum \" ) ;    likwid_markerstartregion(0 , coreid ) ; //",
    "measured code region likwid_markerstopregion(0 , coreid , mainid ) ;    for ( j = 0 ; j",
    "< n ; j++ ) {     likwid_markerstartregion(0 , coreid ) ;     // measured code region     likwid_markerstopregion(0 , coreid , accumid ) ; }    likwid_markerclose ( ) ; ....    event counts are automatically accumulated on multiple calls .",
    "nesting or partial overlap of code regions is not allowed .",
    "the api requires specification of a thread i d ( 0 for one process only in the example ) and the core i d of the thread / process .",
    "the likwid api provides simple functions to determine the core i d of processes or threads .",
    "the following listing shows the output of likwid - perfctr after measurement of the ` flops_dp ` event group on four cores of an intel core 2 quad processor in marker mode with two named regions ( `` ` init ` '' and `` ` benchmark ` , '' respectively ) :    .... $ likwid - perfctr -c 0 - 3 -g flops_dp -m ./a.out ------------------------------------------------------------- cpu type :        intel core 2 45 nm processor cpu clock :       2.83 ghz ------------------------------------------------------------- measuring group flops_dp ------------------------------------------------------------- % region : init%   + --------------------------------------+--------+--------+--------+--------+    + --------------------------------------+--------+--------+--------+--------+         + --------------------------------------+--------+--------+--------+--------+ + -------------+-------------+-------------+-------------+-------------+    + -------------+-------------+-------------+-------------+-------------+        + -------------+-------------+-------------+-------------+-------------+ % region : benchmark%   + -----------------------+-------------+-------------+-------------+-------------+    + -----------------------+-------------+-------------+-------------+-------------+         + -----------------------+-------------+-------------+-------------+-------------+ + -------------+-----------+------------+------------+------------+    + -------------+-----------+------------+------------+------------+        + -------------+-----------+------------+------------+------------+ ....    note that the ` instr_retired_any ` and ` cpu_clk_unhalted_core ` events are always counted ( using two unassignable `` fixed counters '' on the core 2 architecture ) , so that the derived ` cpi ` metric ( `` cycles per instruction '' ) is easily obtained .    the following architectures are supported at the time of writing :    * intel pentium m ( banias , dothan ) * intel atom * intel core 2 ( all variants ) * intel nehalem ( all variants , including uncore events ) * intel westmere * amd k8 ( all variants ) * amd k10 ( barcelona , shanghai , istanbul )      multicore / multisocket machines exhibit complex topologies , and this trend will continue with future architectures .",
    "performance programming requires in - depth knowledge of cache and node topologies , e.g. , about which caches are shared between which cores and which cores reside on which sockets .",
    "the linux kernel numbers the usable cores and makes this information accessible in ` /proc / cpuinfo ` .",
    "still how this numbering maps to the node topology depends on bios settings and may even differ for otherwise identical processors .",
    "the processor and cache topology can be queried with the ` cpuid ` machine instruction .",
    "likwid - pin is based directly on the data provided by ` cpuid ` .",
    "it extracts machine topology in an accessible way and can also report on cache characteristics .",
    "the thread topology is determined from the apic ( advanced programmable interrupt controller ) i d . starting with the nehalem processor ,",
    "intel introduced a new ` cpuid ` leaf ( 0xb ) to account for today s more complex multicore chip topologies .",
    "older intel and amd processors both have different methods to extract this information , all of which are supported by likwid - topology .",
    "similar considerations apply for determining the cache topology . starting with",
    "the core 2 architecture intel introduced the ` cpuid ` leaf 0x4 ( deterministic cache parameters ) , which allows to extract the cache characteristics and topology in a systematic way . on older intel processors the cache parameters where provided by means of a lookup table ( ` cpuid ` leaf 0x2 ) .",
    "amd again has its own ` cpuid ` leaf for the cache parameters .",
    "the core functionality of likwid - topology is implemented in a c module , which can also be used as a library to access the information from within an application .",
    "likwid - topology outputs the following information :    * clock speed * thread topology ( which hardware threads map to which physical resource ) * cache topology ( which hardware threads share a cache level ) * extended cache parameters for data caches .",
    "the following output was obtained on an intel nehalem ep westmere processor and includes extended cache information :    .... $ likwid - topology -c ------------------------------------------------------------- cpu name :        unknown intel processor   cpu clock :       2.93 ghz     * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * hardware thread topology * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * sockets :                 2   cores per socket :        6   threads per core :        2   ------------------------------------------------------------- hwthread         thread           core             socket 0                0                0                0 1                0                1                0 2                0                2                0 3                0                8                0 4                0                9                0 5                0                10               0 6                0                0                1 7                0                1                1 8                0                2                1 9                0                8                1 10               0                9                1 11               0                10               1 12               1                0                0 13               1                1                0 14               1                2                0 15               1                8                0 16               1                9                0 17               1                10               0 18               1                0                1 19               1                1                1 20               1                2                1 21               1                8                1 22               1                9                1 23               1                10               1 ------------------------------------------------------------- socket 0 : ( 0 12 1 13 2 14 3 15 4 16 5 17 ) socket 1 : ( 6 18 7 19 8 20 9 21 10 22 11 23 ) -------------------------------------------------------------    * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * cache topology * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * level :    1 size :     32 kb type :     data cache associativity :    8 number of sets :   64 cache line size : 64 inclusive cache shared among 2 threads cache groups :    ( 0 12 ) ( 1 13 ) ( 2 14 ) ( 3 15 ) ( 4 16 ) ( 5 17 ) ( 6 18 ) ( 7 19 ) ( 8 20 ) ( 9 21 ) ( 10 22 ) ( 11 23 ) ------------------------------------------------------------- level :    2 size :     256 kb type :     unified cache associativity :    8 number of sets :   512 cache line size : 64 inclusive cache shared among 2 threads cache groups :    ( 0 12 ) ( 1 13 ) ( 2 14 ) ( 3 15 ) ( 4 16 ) ( 5 17 ) ( 6 18 ) ( 7 19 ) ( 8 20 ) ( 9 21 ) ( 10 22 ) ( 11 23 ) ------------------------------------------------------------- level :    3 size :     12 mb type :     unified cache associativity :    16 number of sets :   12288 cache line size : 64 non inclusive cache shared among 12 threads cache groups :    ( 0 12 1 13 2 14 3 15 4 16 5 17 ) ( 6 18 7 19 8 20 9 21 10 22 11 23 ) ------------------------------------------------------------- ....    one can also get an accessible overview of the node s cache and socket topology in ascii art ( via the ` -g ` option ) . the following listing fragment shows the output for the same chip as above . note that only one socket is shown ( belonging to the first l3 cache group above ) :    .... + -------------------------------------------------------------+                         + -------------------------------------------------------------+ ....          thread / process affinity is vital for performance . if topology information is available , it is possible to pin threads according to the application s resource requirements like bandwidth , cache sizes , etc .",
    "correct pinning is even more important on processors supporting smt , where multiple hardware threads share resources on a single core .",
    "likwid - pin supports thread affinity for all threading models that are based on posix threads , which includes most openmp implementations . by overloading the ` pthread_create ` api call with a shared library wrapper",
    ", each thread can be pinned in turn upon creation , working through a list of core ids .",
    "this list , and possibly other parameters , are encoded in environment variables that are evaluated when the library wrapper is first called .",
    "likwid - pin simply starts the user application with the library preloaded .",
    "the overall mechanism is illustrated in fig .",
    "[ fig : likwid - pin ] .",
    "no code changes are required , but the application must be dynamically linked .",
    "this mechanism is independent of processor architecture , but the way the compiled code creates application threads must be taken into account : for instance , the intel openmp implementation always runs ` omp_num_threads+1 ` threads but uses the first newly created thread as a management thread , which should not be pinned .",
    "this knowledge must be conveyed to the wrapper library .",
    "the following example shows how to use likwid - pin with an openmp application compiled with the intel compiler :    .... $ export omp_num_threads=4 $ likwid - pin -c 0 - 3 % -t intel% ./a.out ....    currently , posix threads , intel openmp , and gnu ( gcc ) openmp are supported , and the latter is assumed as the default if no ` -t ` switch is given .",
    "other threading implementations are supported via a `` skip mask . ''",
    "this mask is interpreted as a binary pattern and specifies which threads should not be pinned by the wrapper library ( the explicit mask for intel binaries would by ` 0x1 ` ) .",
    "the skip mask makes it possible to pin hybrid applications as well by skipping mpi shepherd threads . for intel - compiled binaries using the intel mpi library ,",
    "the appropriate skip mask is ` 0x3 ` :    .... $ export omp_num_threads=8 $ mpiexec -n 64 -pernode \\",
    "likwid - pin -c 0 - 7 % -s 0x3% ./a.out ....    this would start 64 mpi processes on 64 nodes ( via the ` -pernode ` option ) with eight threads each , and not bind the first two newly created threads .    in general ,",
    "likwid - pin can be used as a replacement for the ` taskset ` tool , which can not pin threads individually .",
    "note , however , that likwid - pin , in contrast to ` taskset ` , does not establish a linux cpuset in which to run the application .",
    "some compilers have their own means for enforcing thread affinity . in order to avoid interference effects",
    ", those mechanisms should be disabled when using likwid - pin . in case of recent intel compilers ,",
    "this can be achieved by setting the environment variable ` kmp_affinity ` to ` disabled ` the current version of likwid does this automatically .",
    "the big advantage of likwid - pin is its portable approach to the pinning problem , since the same tool can be used for all applications , compilers , mpi implementations , and processor types . in section  [ sec : case_1 ]",
    "the usage model is analyzed in more detail on the example of the stream triad .",
    "an important hardware optimization on modern processors is to hide data access latencies by hardware prefetching .",
    "intel processors not only have a prefetcher for main memory ; several prefetchers are responsible for moving data between cache levels .",
    "often it is beneficial to know the influence of the hardware prefetchers . in some situations turning off",
    "hardware prefetching even increases performance . on the intel core 2 processor",
    "this can be achieved by setting bits in the ` ia32_misc_enable msr ` register .",
    "likwid - features allows viewing and altering the state of these bits . besides the ability to toggle the hardware prefetchers ,",
    "likwid - features also reports on the state of switchable processor features like , e.g. , intel speedstep :    .... $ likwid - features ------------------------------------------------------------- cpu name :        intel core 2 65 nm processor   cpu core i d :     0   ------------------------------------------------------------- fast - strings :                    enabled automatic thermal control :       enabled performance monitoring :          enabled hardware prefetcher :             enabled branch trace storage :            supported pebs :                            supported intel enhanced speedstep :        enabled monitor / mwait :                   supported adjacent cache line prefetch :    enabled limit cpuid maxval :              disabled xd bit disable :                  enabled dcu prefetcher :                  enabled intel dynamic acceleration :      disabled ip prefetcher :                   enabled ------------------------------------------------------------- ....    disabling , e.g. , adjacent cache line prefetch then works as follows :    .... $ likwid - features % -u cl\\_prefetcher% [ ... ] cl_prefetcher :   disabled ....    likwid - features currently only works for intel core 2 processors , but support for other architectures is planned for the future .",
    "[ sec : comp_papi ]    [ cols=\"<,<,<\",options=\"header \" , ]",
    "likwid is a collection of command line applications supporting performance - oriented software developers in their effort to utilize today s multicore processors in an effective manner .",
    "likwid does not try to follow the trend to provide yet another complex and sophisticated tooling environment , which would be difficult to set up and would overwhelm the average user with large amounts of data .",
    "instead it tries to make the important functionality accessible with as few obstacles as possible .",
    "the focus is put on simplicity and low overhead .",
    "likwid - topology and likwid - pin enable the user to account for the influence of thread and cache topology on performance and pin their application to physical resources in all possible scenarios with one single tool and no code changes .",
    "prototypically we have shown the influence of thread topology and correct pinning on the example of the stream triad benchmark .",
    "moreover thread pinning and performance characteristics were reviewed for an optimized topology - aware stencil code using likwid - perfctr .",
    "likwid is open source and released under gpl2 .",
    "it can be downloaded at http://code.google.com / p / likwid/.    likwid is still in alpha stage .",
    "near - term goals are to consolidate the current features and release a stable version , and to include support for more processor types .",
    "an important feature missing in likwid - topology is to include numa information in the output .",
    "likwid - pin will be equipped with cpuset support , so that logical core ids may be used when binding threads .",
    "further goals are the combination of likwid with one of the available mpi profiling frameworks to facilitate the collection of performance counter data in mpi programs .",
    "most of these frameworks rely on the papi library at the moment .",
    "future plans include applying the philosophy of likwid to other areas like , e.g. , profiling ( also on the assembly level ) and low - level benchmarking with a tool creating a `` bandwidth map . ''",
    "this will allow a quick overview of the cache and memory bandwidth bottlenecks in a shared - memory node , including the ccnuma behavior .",
    "it is also planned to port parts of likwid to the windows operating system .",
    "on popular demand , future releases will also include support for xml output .",
    "we are indebted to intel germany for providing test systems and early access hardware for benchmarking .",
    "many thanks to michael meier , who had the basic idea for likwid - pin , implemented the prototype , and provided many useful thoughts in discussions .",
    "this work was supported by the competence network for scientific and technical high performance computing in bavaria ( konwihr ) under the project `` omi4papps . ''",
    "1 g. jost , j. haoqiang , j. labarta , j. gimenez , j. caubet : _ performance analysis of multilevel parallel applications on shared memory architectures .",
    "_ proceedings of the parallel and distributed processing symposium , 2003 .",
    "s. browne , c. deane , g. ho , p. mucci : _ papi : a portable interface to hardware performance counters .",
    "_ proceedings of department of defense hpcmp users group conference , june 1999 .",
    "paul j. drongowski : _ basic performance measurements for amd athlon 64 , amd opteron and amd phenom processors .",
    "_ technical note , advanced micro devices , inc .",
    "boston design center , september 2008 .",
    "j. treibig , g. wellein , g. hager : _ efficient multicore - aware parallelization strategies for iterative stencil computations .",
    "_ submitted to journal of computational science , 2010 .",
    "preprint http://arxiv.org/abs/1004.1741 .",
    "m.  wittmann , g.  hager and g.  wellein .",
    "_ multicore - aware parallel temporal blocking of stencil codes for shared and distributed memory_. workshop on large - scale parallel processing 2010 ( ipdps2010 ) , atlanta , ga , april 23 , 2010 ."
  ],
  "abstract_text": [
    "<S> exploiting the performance of today s processors requires intimate knowledge of the microarchitecture as well as an awareness of the ever - growing complexity in thread and cache topology . </S>",
    "<S> likwid is a set of command - line utilities that addresses four key problems : probing the thread and cache topology of a shared - memory node , enforcing thread - core affinity on a program , measuring performance counter metrics , and toggling hardware prefetchers . </S>",
    "<S> an api for using the performance counting features from user code is also included . </S>",
    "<S> we clearly state the differences to the widely used papi interface . to demonstrate the capabilities of the tool set we show the influence of thread pinning on performance using the well - known openmp stream triad benchmark , and use the affinity and hardware counter tools to study the performance of a stencil code specifically optimized to utilize shared caches on multicore chips . </S>"
  ]
}