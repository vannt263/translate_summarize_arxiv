{
  "article_text": [
    "suppose we observe data @xmath3 which are real numbers on @xmath4 .",
    "the aim is to classify these data into @xmath5 groups and to determine which ones are in the same group .",
    "this is a classic problem and current bayesian approaches rely on mixture models , such as described in @xcite , or the mixture of dirichlet process model ( see , for example , @xcite ) . in the richardson and green model the @xmath6",
    "is modeled explicitly via @xmath7 where the @xmath8 are weights which sum to one .",
    "prior distributions are assigned to @xmath9 and @xmath10 and inference is made possible via reversible jump mcmc , @xcite .",
    "the likelihood function for @xmath11 observations is given by @xmath12 where the @xmath13 are latent variables which pick out the component , less than or equal to @xmath6 , from which the @xmath14th observation is coming from .",
    "on the other hand , the mixture of dirichlet process ( mdp ) model is based on the density function @xmath15 where the weights @xmath16 sum to one .",
    "the parameters @xmath17 are assigned distributions and , since the classification ideas we have are based on this model , we will elaborate .",
    "so , the @xmath18 and , for @xmath19 , @xmath20 with the @xmath21 being independent and identically distributed as @xmath22 random variables for some @xmath23 .",
    "the @xmath24 are also independent and identically distributed ( the prior ) and we consider the prior as @xmath25 in this case the corresponding likelihood function is given by @xmath26 so , in the richardson and green model , the @xmath6 is explicit , but in the mdp model it is implicit , and taken to be the number of distinct @xmath27 .",
    "however , we are not convinced that either of these models are useful for classification purposes .",
    "the key to the problem is that locations of the normal distributions , the @xmath28 , can be arbitrarily close to each other and therefore register as different clusters .",
    "so two @xmath29 close to each other register as two clusters with a certain probability on the mdp model , since the @xmath30 liking this location may be all one or the other ; but does register as two clusters in the richardson and green model .",
    "such a scenario may well happen when clusters are not normal based , for example .",
    "we would also from this point of view expect the mdp model to perform slightly better than the richardson and green model when using as classification modeling . but both methods would over  estimate the number of clusters .",
    "we will discuss this issue later in section 4 when we do some numerical illustrations .",
    "nevertheless , the issue of overestimation of the number of clusters for the mdp has already been known ; see , for example , @xcite .",
    "our approach is not model based yet the starting point is the mdp model ; since we believe a classification procedure based on the @xmath27 is preferable .",
    "hence , from the mdp model we compute @xmath31 , by integrating out the @xmath32 . but this @xmath31 will include many arrangements which are strange for classification purposes . for example , there is positive probability on @xmath30 and @xmath33 both being the same @xmath34 yet @xmath35 and @xmath36 can be the largest and smallest observation , and observations in between these two extremes are being allocated to different groups .",
    "so , at this point we simply study @xmath31 as a classification probability model and adjust it to eliminate groupings which just do nt make any sense .",
    "indeed , it is these such types of @xmath37 which cause the problems with the mdp model as a classifier in the first place .",
    "so , we first order the @xmath38 , so that @xmath39 is the smallest observation and @xmath40 is the largest observation .",
    "we then constrain the @xmath37 so that the @xmath27 are non  decreasing .",
    "this ensures that any group contains only consecutive @xmath38 s .",
    "for example , group 1 would contain a number of the smallest observations ; group 2 would contain a number of the next smallest observations ; while group @xmath6 would contain a number of the largest observations .",
    "thus , for any trio of @xmath41 , if @xmath42 and @xmath43 are in the same group , then so is @xmath44 .",
    "it follows then that our @xmath45 , with the ordered @xmath38 s , is given by @xmath46 .",
    "we then show how to sample from @xmath45 in order to compute classifications with high probability , and obviously the mode .",
    "in section 2 we derive and explain our probability model for classification . section 3 then describes a mcmc algorithm for sampling from this probability model ; since for large @xmath11 the number of possible clusterings is prohibitively large to compute directly .",
    "section 4 then presents numerical illustrations based on a toy example of 10 data points whereby all probabilities can be computed and the well known and widely studied galaxy data set .",
    "given the outline in the introduction , our first task is to compute @xmath31 based on the mdp model .",
    "now @xmath47 and so @xmath48 here , @xmath49 and @xmath50 . the first term in the product",
    "is given by @xmath51 and the second term is easily found to be given by @xmath52 where @xmath53 and @xmath54 hence , @xmath55 this then is the probability of classification based on the mdp model .    at this point",
    ", we simply focus on @xmath31 and assess it as a probability model for classification .",
    "so , without loss of generality , we take the @xmath38 s to be ordered , with @xmath39 being the smallest observation and @xmath40 being the largest . for reasons then given in the introduction , we would now for classification purposes",
    "only wish to consider the @xmath27 to be non  decreasing .",
    "hence , we consider @xmath56 .",
    "we also impose the constraint that if there are @xmath6 distinct @xmath27 then @xmath57 .",
    "our observation now is that @xmath37 is completely determined by @xmath58 whereby @xmath6 is the number of distinct @xmath27 and @xmath59 is the number of the @xmath30 equal to @xmath34 .",
    "hence , @xmath60 where @xmath61 is the normalizing constant , and now we define @xmath62 and @xmath63 and @xmath64 with @xmath65 and @xmath66 .",
    "note that , for a given sample size @xmath11 , the support of this probability runs over the set of compositions of the integer @xmath11 rather than on the number of partitions of a set with @xmath11 elements typically found in the mdp or other exchangeable partition probability functions settings encounter in the bayesian nonparametric literature .",
    "this probability model for classification is a highly suitable and necessary adaption of the probability model for classification based on the mdp model .",
    "it can be seen to depend fundamentally on the sample variances of the observations in the same group .",
    "so the lower the sample variances , the higher the probability .",
    "the rule of having @xmath67 groups is countered by the probability being a product of @xmath6 terms .",
    "the probability depends on the parameters @xmath68 , which would basically have the same interpretation as if we were using a mdp model for the data .",
    "so , for example , if @xmath69 is big , which implies a large number of groups in the mdp model , its role can be seen explicitly in @xmath70 , since we would have the term @xmath71 and so encourages large @xmath6 .",
    "we also note that attempts have been made to emphasize the suitable @xmath58 by using alternative nonparametric mixing prior distributions to the dirichlet process , which constitutes the mdp model , and which put more weight on configurations which are realistic , see @xcite .",
    "however , positive mass is still being put on  ridiculous \" configurations which will lead to overestimation of @xmath6 .",
    "our approach , in light of this , is remarkably obvious in that we put zero weight on all but realistic configurations .",
    "here we also mention the problem of what happens if a new piece of data arrives .",
    "our approach is not to assume a clustering for the existing data has been set and we decide into which group , possibly a new one , the extra piece of data should be put ; but rather we merely recompute @xmath70 with all the data , including the additional piece .",
    "we do not see any other approach as being relevant here .",
    "we will compare our approach with a routine in the package _",
    "r _ , a hierarchical clustering routine based on local sums of squares , so in principle is not unlike the idea of working with sample variances .",
    "the routine is labeled ` hclust ` in _",
    "r _ and is based on an original algorithm appearing in @xcite .",
    "the basic idea for sampling from @xmath70 will be a split  merge mcmc algorithm .",
    "so at each iteration one of 2 types of move will be proposed : a split , whereby a group of size bigger than 1 is divided into 2 groups so @xmath6 is increased by 1 ; and a merge , whereby 2 groups are combined into 1 group so @xmath6 is decreased by 1 .",
    "the idea for sampling from @xmath70 can be seen as a reversible jump mcmc algorithm , and for ease of exposition we will describe the algorithm using latent variables and the specification of a joint density for a configuration conditional on a @xmath6 : so let @xmath72 for @xmath73 be a clustering for @xmath34 groups , and consider @xmath74 which is based on a recent idea described in @xcite .",
    "the concern is that the marginal density for @xmath58 is unchanged ; which is the case , as is evidently obvious .",
    "now given a @xmath6 and @xmath75 we propose a move to @xmath76 with probability 1/2 and to @xmath77 with probability 1/2 ( with obvious modifications if @xmath78 or @xmath67 ) .",
    "we need to therefore sample @xmath79 from @xmath80 and @xmath81 from @xmath82 .",
    "the former is achieved by finding an existing group with size @xmath83 , and then we split this group into 2 . if uniform distributions are used for both operations then @xmath84 where @xmath85 is the number of groups of size @xmath83 , from @xmath86 , and @xmath87 is the size of this group chosen .",
    "the latter is obtained by merging two neighboring groups and so with a uniform distribution we have @xmath88 therefore , the sampler carries out each step through a metropolis - hastings scheme .",
    "when at state @xmath89 the acceptance probability to move to state @xmath90 is @xmath91 where @xmath92 . on the other hand ,",
    "if instead two groups , @xmath93 , in @xmath94 are selected and we attempt to merge them , then the acceptance probability for this move is @xmath95 where @xmath96 is the cardinality of the set containing all groups with more than one observation in @xmath97 .",
    "to improve the algorithm we then shuffle the @xmath75 by selecting adjacent groups , @xmath98 and attempting to change them into @xmath99 in such a way that both @xmath100 and @xmath101 .",
    "the shuffle is based on the idea of putting the two groups together and then uniformly splitting into 2 groups .",
    "the acceptance probability is then given by @xmath102 these acceptance probabilities all follow from the expression @xmath103 and the cancelations which occur when evaluating the ratios of neighboring @xmath6 .",
    "the algorithm is effectively a joint metropolis  hastings and gibbs algorithm , rather than a reversible jump mcmc algorithm .",
    "the dimension is fixed and so no special considerations arise on this issue .",
    "the only necessary consideration is that if @xmath104 then @xmath105 , and vice versa .",
    "neither , if one is even needed , have we had to worry about a jacobian , since we are not basing the moves on transformations of variables between different dimensions .",
    "we believe it is more explicit to understand reversible jump mcmc from this perspective .    here",
    "we consider a more general idea for sampling , based on the notion of a joint density @xmath106 then it is easy to see how the reversible jump mcmc arises from this model .",
    "but we can seek alternative , and more general strategies , and one such is based on the idea of @xmath107 where @xmath108 is the probability density for a split move described earlier , and @xmath109 is the correct density for @xmath110 given @xmath111 , and is easy to sample since @xmath110 can be represented by a single number between @xmath112 and @xmath113 .",
    "then it is easy to see that a move from @xmath94 to @xmath114 , with @xmath115 can be achieved by first sampling @xmath116 from @xmath117 and @xmath97 from the density @xmath118 , which is done by sampling @xmath110 , then @xmath119 , and so on , up to @xmath81 . if @xmath120 then the proposed move is rejected and @xmath121 is kept . on the other hand ,",
    "if @xmath104 then a move to @xmath76 , proposed with probability @xmath122 is accepted with probability @xmath123 or else a move to @xmath77 is proposed and is accepted with probability @xmath124 while in this particular case we do not claim an improvement using this alternative , the point is that there are alternatives to be considered . in this way",
    "it can be seen that the reversible jump mcmc methodology can be viewed as a special case of a particular idea formulated by the notion of a joint density @xmath106 to see how the algorithms work here ; let us ease the notation by writing @xmath125 and @xmath126 be the @xmath127 .",
    "then , at @xmath128 , we sample @xmath126 from the full conditional , but in the original algorithm only need @xmath129 and @xmath130 , and then do a metropolis ",
    "hastings step for @xmath128 where the proposal is to complete the joint density with @xmath76 , or @xmath77 , and the retention of @xmath131 .",
    "in order to underline the kind of results that can be obtained by our approach we first consider a small data set ; small enough @xmath132 so that we can provide exact computations of probabilities for all @xmath58 . we then illustrate our approach with a real data set ; the galaxy data set .",
    "suppose the set of ordered observations is @xmath133 - 1.522 , -1.292 , -0.856 , -0.104 , 2.388 , 3.080 , 3.313 , 3.415 , 3.922 , 4.194@xmath134 , and a histogram of the data is shown in figure  [ fig : small ] , from this it is evident that 2 groups are the most likely option .",
    "table  [ tab : toy ] gives the probabilities of having @xmath6 groups using the mdp approach , and our proposed approach . in both cases",
    "we use the prior specification of parameters as @xmath135 and @xmath136 . for the mdp model we computed the exact probabilities for each of the 115,975 possible partitions of @xmath38 , and then using them to obtain the exact posterior probabilities for each @xmath137 . the highest probability in this case is allocated to @xmath138 .",
    "further inspection among the partitions indicates that the highest posterior probability of @xmath139 corresponds to the classification involving 2 groups ; @xmath140,[y_5,\\ldots , y_{10}]\\right\\}$ ] , which corresponds to @xmath141 .",
    "on the other hand , the exact probabilities , @xmath142 , computed over all the 512 possible configurations , assign the highest probability to @xmath111 . as with the mdp case",
    ", the classification with the highest probability corresponds to @xmath140,[y_5,\\ldots , y_{10}]\\right\\}$ ] ; but in this case with the considerably higher probability of @xmath143 . clearly , considering the order of the @xmath38 limits the support considerably , from set partitions to integer compositions , withdrawing all inadequate partitions for classification purposes and hence leading to an improved estimator for the number of groups .",
    "the last four columns in table  [ tab : toy ] show the estimates of @xmath142 based on the mcmc schemes described in section  [ sec : sampling ] , with @xmath144 and @xmath145 iterations following a burn in period of @xmath146 and @xmath144 iterations respectively .",
    "the results are matching the exact probabilities and from these it is evident that both schemes are valid , although the first appears to converge faster than the second .",
    "our results are in agreement with the hierarchical agglomerative clustering , using ward s ( 1963 ) approach , which reduces the number of groups from @xmath6 to @xmath77 by minimizing the local sum of squares ; see figure  [ fig : dendo ] .",
    "it is obvious from this that 2 groups are by far and away the preferred choice .",
    "here we consider the galaxy data set , studied in @xcite .",
    "it is widely used in the literature to illustrate methodology for mixture modeling . in this case",
    "the sample size is @xmath147 and so we would need to compute @xmath148 probabilities to obtain all the possible configurations .",
    "therefore , we will use the first mcmc algorithm proposed in section  3 to obtain the probabilities .",
    "we undertake this approach , with the same parameter specifications as in the above small data set example and 10000 iterations after 1000 of burn in period .",
    "the mcmc estimates result in @xmath149 and @xmath150 , with the highest probability of 0.677 on the configuration @xmath151 .",
    "the same results are attained with the second scheme of section  [ sec : sampling ] but with a higher number of simulations .",
    "depending on the parameter values , e.g. the total mass parameter @xmath69 ; the bayesian nonparametric mixture model favors between 5 and 6 groups ; see for example @xcite and @xcite .",
    "similar results are achieved in the finite mixture setting as in @xcite .",
    "all of these approaches seem to be overestimating the number of groups , as noted from results reported in @xcite .",
    "the first author gratefully acknowledges the mexican mathematical society and the sofia kovalevskaia fund , and the second author gratefully acknowledges conacyt for grant no .",
    "j50160-f , for allowing them to travel to uk , where the work was completed during a visit to the university of kent .                        .",
    "[ tab : toy ] probabilities on the different number of groups for the small data set example .",
    "the mdp results correspond to exact posterior probabilities .",
    "the probabilities @xmath142 and @xmath152 for the classification model correspond to the exact and mcmc estimates , respectively .",
    "the columns labeled m1 and m2 refer to the two sampling schemes described in section  [ sec : sampling ] with ( a ) 10000 iterations after a 1000 burn in period and ( b ) 100000 iterations after a 10000 burn in period .",
    "[ cols=\"<,<,<,^,^,^,^ \" , ]"
  ],
  "abstract_text": [
    "<S> in this paper , we provide an explicit probability distribution for classification purposes . it is derived from the bayesian nonparametric mixture of dirichlet process model , but with suitable modifications which remove unsuitable aspects of the classification based on this model . the resulting approach </S>",
    "<S> then more closely resembles a classical hierarchical grouping rule in that it depends on sums of squares of neighboring values . </S>",
    "<S> the proposed probability model for classification relies on a simulation algorithm which will be based on a reversible mcmc algorithm for determining the probabilities , and we provide numerical illustrations comparing with alternative ideas for classification .    _ </S>",
    "<S> keywords : _ classification ; mcmc sampling ; mdp model .    * a probability for classification based on the mixture of dirichlet process model * +   +    @xmath0 facultad de ciencias , universidad nacional autnoma de mxico . </S>",
    "<S> mxico , d.f . </S>",
    "<S> 04510 , mxico . </S>",
    "<S> + @xmath1 iimas , universidad nacional autnoma de mxico . </S>",
    "<S> mxico , d.f . </S>",
    "<S> 04510 , mxico . </S>",
    "<S> + @xmath2 university of kent , canterbury , kent , ct2 7nz , uk . </S>"
  ]
}