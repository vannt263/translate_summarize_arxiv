{
  "article_text": [
    "low - rank approximations are widely used in data mining , machine learning , and signal processing , as a tool for dimensionality reduction , feature extraction , and classification . in system identification ,",
    "signal processing , and computer algebra , in addition to having low rank , the matrices are often structured , e.g. , they have ( block ) hankel , ( block ) toeplitz , or ( block ) sylvester structure",
    ". motivated by this fact , in this paper , we consider the problem of approximating a given structured matrix @xmath0 by a matrix @xmath1 with the same structure and with a pre - specified reduced rank @xmath2 .      formally , we consider the following problem @xmath3 where @xmath4 , @xmath5 , @xmath6 and @xmath7 is a semi - norm on the space of matrices @xmath8 , induced by a positive semidefinite matrix @xmath9 as @xmath10    being able to deal with weights in has a number of advantages in practice .",
    "first , due to sensor failure , malfunctioning of a communication channel , or simply due to unseen events , real - world data can have unknown ( missing ) elements .",
    "if repeating the experiments until all data are collected is not an option , for example because of high price of the experiments or high computational time , the missing data have to be approximated as well .",
    "the problem of estimating missing data is also known as the matrix completion problem and is well - studied in the case of unstructured matrices . in the case of structured matrices , however , this problem has few solutions @xcite .",
    "a natural way to deal with missing elements is to introduce zeros in the weight matrix at the positions corresponding to the missing elements .",
    "in addition , if prior knowledge is available about the importance or the correctness of each ( noisy ) element , this knowledge can be encoded in the weight matrix .",
    "note also that finding the closest structured matrix to a given structured matrix with respect to the frobenius norm can be encoded with @xmath11 being the identity matrix .",
    "the structures considered in are affine structures .",
    "this class of structures includes many structures of interest and contains all linear structures .",
    "moreover , it allows us to deal with fixed elements , i.e. , to keep some elements of @xmath0 in the approximating matrix @xmath1 . unlike other approaches in the literature , we do not use infinite weights , but rather incorporate the fixed values in a special matrix .",
    "existing algorithms to solve problem can be classified into three groups : i ) based on local optimization , ii ) using relaxations , or iii ) using heuristics , such as the widely used cadzow method @xcite or @xcite .",
    "relaxation methods include subspace - based methods @xcite and , more recently , nuclear norm based methods @xcite .",
    "local optimization algorithms use kernel or input / output ( i / o ) ( also known as the structured total least squares ( stls ) problem ) representations of the rank constraint , as described in table  [ tab : loc_opt_approaches ] .",
    "some of these algorithms can not deal with fixed or missing elements or solve only special cases of problem , e.g. the case of frobenius norm .",
    ".existing local optimization approaches for the structured low - rank approximation problem [ cols= \" < , < , < , < \" , ]     [ tab : sysid_misssing_comparison_square ]    ) , missing data ( subset of @xmath12 ) , true data @xmath12 and the trajectories obtained from algorithm  [ alg : reg_slra ] ( @xmath13 ) and slra ( @xmath14 ) for the example  with @xmath15 and missing data . ]    for @xmath16 , algorithm  [ alg : reg_slra ] had the smallest approximation error . for @xmath15 ,",
    "the results of algorithm  [ alg : reg_slra ] and slra ( initialized with kung s method ) were similar , although algorithm  [ alg : reg_slra ] was still better .",
    "different realizations of the example lead to slightly different numerical results .",
    "we also observed that for smaller noise variance , slra and algorithm  [ alg : reg_slra ] compute the same solution , but for higher values of the noise algorithm  [ alg : reg_slra ] is generally more robust .",
    "another application of structured low - rank approximation and thus of algorithm  [ alg : reg_slra ] is finding approximate common divisors of a set of polynomials .",
    "existence of a nontrivial common divisor is a nongeneric property . given a noisy observation of the polynomials coefficients ( or due to round - off errors in storing the exact polynomials coefficients in a finite precision arithmetic ) , the polynomials have a nontrivial common divisor with probability zero . assuming that the noise free polynomials have a common divisor of a known degree , our aim in the approximate common divisor problem is to estimate the common divisor from the noisy data .",
    "the problem can be formulated and solved as sylvester structured low - rank approximation problems , see @xcite .",
    "since the sylvester matrix has fixed zero elements , in this example , we use the feature of algorithm  [ alg : reg_slra ] to work with structured matrices having _",
    "fixed elements_.      for a polynomial @xmath17 define the multiplication matrix @xmath18 we consider three polynomials @xmath19 , @xmath20 , and @xmath21 and for simplicity let they be of the same degree  @xmath22 .",
    "a basic result in computer algebra , see , e.g. , @xcite , is that @xmath19 , @xmath20 , and @xmath21 have a nontrivial common divisor if and only if the generalized sylvester matrix @xmath23      s_{n}(a ) & \\mathbf{0}\\\\[1 mm ]      \\mathbf{0 } & s_{n}(a ) \\end{bmatrix } \\label{eq : sylvester3a}\\ ] ] has rank at most @xmath24 .",
    "alternatively , one can consider another type of generalized sylvester matrix  @xcite @xmath25 s_{n}(b ) \\\\[1 mm ] s_{n}(c ) \\end{bmatrix } , \\label{eq : sylvester4}\\ ] ] whose rank deficiency is equal to the degree of the greatest common divisor of @xmath19 , @xmath20 , and @xmath21 .",
    "formulation ( [ eq : sylvester4 ] ) is more compact than the one in ( [ eq : sylvester3a ] ) , especially if the number of polynomials is large .",
    "these results are generalizable for arbitrary number of polynomials ( @xmath26 ) of possibly different degrees and arbitrary order of the required common divisor .    in the case of inexact coefficients ,",
    "the generalized sylvester matrices are generically of full rank . the problem of finding the approximate common divisor is then transformed to the problem of approximating the matrix in ( [ eq : sylvester3a ] ) or in ( [ eq : sylvester4 ] ) by low - rank matrices with the same structure .",
    "this can be done with algorithm  [ alg : reg_slra ] and with the alternative method slra . for simplicity ,",
    "in our example we take three polynomials of degree @xmath27 and desired ( greatest ) common divisor of order one ( one common root ) .",
    "let @xmath28      b(z ) & = & 10.8 - 7.4z + z^2 & = & ( 2 - z)\\,(5.4 - z),\\\\[1 mm ]      c(z ) & = & 15.6 - 8.2z    +   z^2 & = & ( 3 - z)\\,(5.2 - z ) .",
    "\\end{array}\\ ] ] aiming at a common divisor of degree one , we approximate ( [ eq : sylvester3a ] ) and ( [ eq : sylvester4 ] ) with , respectively , rank-@xmath29 and rank-@xmath30 matrices .",
    "the obtained solution with algorithm  [ alg : reg_slra ] , applied on is @xmath31      \\hat b(z ) & = & 10.8010   -7.3946\\,z + 1.0277\\,z^2          & = & 1.0277\\,\\,(2.0378 - z)\\,\\textcolor{blue}{(5.1572 - z)},\\\\[1 mm ]      \\hat c(z ) & = & 15.6001   -8.1994\\,z +   1.0033\\,z^2          & = & 1.0033\\,\\,(3.0149 - z)\\,\\textcolor{blue}{(5.1572 - z ) } , \\end{array}\\ ] ] with a common root @xmath32 .",
    "the approximation error was @xmath33 where @xmath34 is the vector of @xmath35 initial coefficients ( @xmath36 ) and @xmath37 is the vector of @xmath35 coefficients of the approximations ( @xmath38 ) .",
    "the roots of the original and approximating polynomials , as well as the polynomials themselves are plotted in figure  [ fig : gcd_v2 ] .",
    "the same results were obtained with slra , applied on . due to the non - convexity of the problem , algorithm  [ alg : reg_slra ] applied on computed a slightly different solution with comparable accuracy ( @xmath39 ) .",
    "applying slra on ( [ eq : sylvester4 ] ) resulted in computing a common divisor of degree @xmath27 , i.e. , @xmath40 .",
    "cadzow s algorithm performed well in both cases ( ( [ eq : sylvester4 ] ) and ( [ eq : sylvester3a ] ) ) and resulted in two other approximations with @xmath41 and @xmath39 , respectively .",
    "structured low - rank approximation can be applied to decompose and approximate complex symmetric tensors into a sum of symmetric rank - one terms . with this application",
    "we also demonstrate how the proposed algorithm can deal with _ missing _ and _ fixed _ elements on quasi - hankel structured matrices .",
    "a complex tensor of dimension @xmath22 and order @xmath42 , @xmath43 is called symmetric if it is invariant under any permutation of the modes .",
    "a symmetric tensor @xmath44 admits a symmetric decomposition of rank @xmath2 if it can be represented as a sum of @xmath2 rank - one symmetric terms : @xmath45 where @xmath46 .",
    "the problem of symmetric tensor decomposition is to find a ( minimal ) decomposition .    in @xcite",
    ", it was shown that @xmath44 has a decomposition of rank @xmath2 if and only if ( excluding some nongeneric cases ) @xmath47 where @xmath48 is a quasi - hankel structured matrix constructed from the tensor and the vector @xmath49 has unknown entries ( latent variables ) .",
    "therefore , the tensor decomposition problem is a low - rank matrix completion of the structured matrix @xmath48 .",
    "moreover , symmetric low - rank tensor approximation is equivalent to structured low - rank approximation with missing data .",
    "the main difficulty for this reformulation is that filling in missing data is nontrivial , and gives rise to multivariate polynomial systems of equations @xcite .",
    "symmetric tensors are often represented as homogeneous polynomials @xcite using @xmath50 where @xmath51 and ` @xmath52 ' is the tensor - vector product with respect to the @xmath53th mode of the tensor . with this representation , symmetric tensor decomposition is equivalent to the waring problem @xcite of decomposing a homogeneous polynomial @xmath54 of degree @xmath42 into a sum of @xmath42-th powers of linear forms @xmath55 we will represent our results in the polynomial form .",
    "consider the example from @xcite , which is decomposition of a polynomial ( a tensor of dimension @xmath30 and order @xmath56 ) , with @xmath57 , @xmath58 into a sum of @xmath59 symmetric rank - one terms .",
    "( for dimension @xmath30 and degree @xmath56 complex symmetric tensors , the generic rank is @xmath59 @xcite ) . in this case , in order to compute the decomposition with the theory of @xcite it is crucial to fill in the missing data .",
    "we considered the @xmath60 submatrix of the matrix in @xcite .",
    "we fixed the non - missing data ( by setting them in @xmath61 ) , and computed the missing elements with algorithm  [ alg : reg_slra ] .",
    "the error on the deviation from the structure was around machine precision ( @xmath62 ) .",
    "the computed tensor decomposition was @xmath63      & + 6.94\\ , { \\left(x_0 + 0.895\\ , x_1 - 0.604\\ , x_2\\right)}^4 & \\\\[1 mm ]      & - 4.94\\ , { \\left(x_0 - 0.982\\ , x_1 + 0.657\\ , { i}\\ , x_2\\right)}^4 & \\\\[1 mm ]      & + 4.94\\ , { \\left(x_0 - 0.982\\ , x_1   - 0.657\\ , { i}\\ , x_2\\right)}^4 & \\\\[1 mm ]      & -\\left(1.99 - 11.9\\ , { i}\\right){\\left(x_0 + \\left(0.128 + 0.308\\ , { i}\\right)x_1\\right)}^4 & \\\\[1 mm ]       & -\\left(1.99 + 11.9\\ , { i}\\right){\\left(x_0 + \\left(0.128 - 0.308\\ , { i}\\right)x_1\\right)}^4 & \\hspace*{-3 mm } , \\end{array } \\label{eq : tensor_example}\\ ] ] ( where we have removed coefficients smaller than @xmath64 ) .",
    "this is a different expansion from the one reported in @xcite .",
    "this can be expected , because for generic ranks the tensor decompositions are usually nonunique @xcite .",
    "the approximation error of on the normalized polynomial coefficients is @xmath65 .    instead of the method used in @xcite , we computed the vectors @xmath66 by joint diagonalization of matrices @xmath67 and @xmath68 of the quotient algebra ( see @xcite for the definition of these matrices ) . for joint diagonalization",
    "we used the method @xcite from signal processing , where approximate joint eigenvectors are computed by taking the eigenvectors of a linear combination of @xmath67 and @xmath68 .",
    "this was needed because in the case of multiple eigenvalues of @xmath67 ( which is the case of our computed decomposition ) , the method in @xcite does not work correctly .",
    "note that in case of ( structured ) matrix completion with exact data , the given data are incorporated in the matrix @xmath61 .",
    "thus , since no elements are being approximated , @xmath69 and the first term in the objective function @xmath70 vanishes .",
    "the parameter @xmath71 would not affect the resulting optimization problem and can also be removed .",
    "the problem is thus reduced to the following problem @xmath72 and can be solved faster ( since no iterations over @xmath71 are necessary ) .    in this example , the data were exact and the goal was to compute exact rank-@xmath59 decomposition .",
    "however , algorithm  [ alg : reg_slra ] with the full objective function can be used to solve the more general problem of tensor low - rank approximation as well .",
    "in this paper , we introduced a novel approach for solving the structure - preserving low - rank approximation problem .",
    "we used the image representation to deal with the low - rank constraint , and a penalty technique , to impose the structure on the approximation .",
    "the original problem has been reduced to solving a series of simple least squares problems with exact solutions .",
    "we have discussed the properties of the proposed local optimization algorithm and ensured that it can solve the weighted problem and deal with the cases of missing or fixed elements . the proposed algorithm was tested on a set of numerical examples from system identification , computer algebra and symmetric tensor decomposition and compared favorably to existing algorithms .",
    "the penalized structured low - rank approximation algorithm proposed in this paper is an attractive alternative to the kernel approach : it is more robust to the initial approximation ( section  [ ex : sysid ] ) , allows us to use a simpler sylvester matrix in the gcd setting , and can be used for symmetric tensor decompositions , where the alternative slra method experiences difficulties .",
    "in contrast to algorithms based on the kernel representation , the proposed structured low - rank approximation is designed for the problems requiring low ranks ( small @xmath2 ) .",
    "it is also worth noting that there are no restrictions on the values of the rank @xmath2 .",
    "an efficient implementation of the algorithm and more detailed analysis of its applications in case of missing data and gcd computation are a topic of future research .",
    "the closest matrix in @xmath73 to an unstructured matrix @xmath74 is the solution of the minimization problem @xmath75 where @xmath76 stands for the frobenius norm .",
    "equivalently , we need to solve @xmath77 which can be written as @xmath78 since , by assumption * ( a ) * , @xmath79 has full column rank and @xmath80 ( [ min : projection ] ) is a least squares problem with unique solution @xmath81 thus , @xmath82 which completes the proof.@xmath83    the effect of applying @xmath84 on a vectorized @xmath85 matrix @xmath74 is producing a structure parameter vector by averaging the elements of @xmath86 corresponding to the same @xmath87 .",
    "indeed , the product @xmath88 results in a vector containing the sums of the elements corresponding to each  @xmath87 . by assumption * ( a ) * , @xmath89 is a diagonal matrix , with elements on the diagonal equal to the number of nonzero elements in each @xmath87 , i.e. , @xmath90 where nnz stands for the number of nonzero elements .",
    "therefore multiplying by @xmath91 corresponds to averaging .        consider first problem ( [ def : subproblem1 ] ) .",
    "problem ( [ def : subproblem2 ] ) can be solved in a similar way . using and , ( [ def : subproblem1 ] ) can be reformulated as @xmath95      \\longleftrightarrow\\quad\\mbox { } & \\displaystyle{\\min_{l } \\|\\overline m(p-{\\mathbf{s}}^\\dagger\\,{\\textnormal{vec}(pl)})\\|^2_2 +          \\lambda\\|{\\textnormal{vec}(pl ) } - { \\textnormal{vec}({\\cal p}_{{{\\cal s}}}({pl}))}\\|^2_2,}\\\\[3 mm ]      \\longleftrightarrow & \\displaystyle{\\min_{l } \\|\\overline m",
    "( p-{\\mathbf{s}}^\\dagger\\,{\\textnormal{vec}(pl)})\\|^2_2          + \\lambda\\|{\\textnormal{vec}(pl ) } -   { \\textnormal{vec}(s_0 ) } - \\pi_{{\\mathbf{s}}}\\,{\\textnormal{vec}(pl)}\\|^2_2,}\\\\[3 mm ]      \\longleftrightarrow & \\displaystyle{\\min_{l } \\|\\overline m{\\mathbf{s}}^\\dagger\\,{\\textnormal{vec}(pl ) } -\\overline mp\\|^2_2           + \\|\\sqrt{\\lambda}\\pi_{{\\mathbf{s}}_\\perp}\\,{\\textnormal{vec}(pl ) }           -\\sqrt{\\lambda}{\\textnormal{vec}(s_0)}\\|^2_2,}\\\\[3 mm ]      \\longleftrightarrow & \\displaystyle{\\min_{l } \\left\\|          \\begin{bmatrix}\\overline m\\,{\\mathbf{s}}^\\dagger\\\\[2mm]\\sqrt{\\lambda}\\pi_{{\\mathbf{s}}_\\perp}\\end{bmatrix}\\,{\\textnormal{vec}(pl ) }          -\\begin{bmatrix}\\overline mp\\\\[2mm]\\sqrt{\\lambda}{\\textnormal{vec}(s_0)}\\end{bmatrix}\\right\\|^2_2,}\\\\[6 mm ]              \\longleftrightarrow & \\displaystyle{\\min_{l } \\left\\|              \\begin{bmatrix } \\overline m\\ , { \\mathbf{s}}^\\dagger\\\\[2mm]\\sqrt{\\lambda}\\pi_{{\\mathbf{s}}_\\perp}\\end{bmatrix}\\,(i_n\\otimes p)\\,{\\textnormal{vec}(l ) }          -\\begin{bmatrix } \\overline m p\\\\[2mm]\\sqrt{\\lambda}{\\textnormal{vec}(s_0)}\\end{bmatrix}\\right\\|^2_2 . }",
    "\\end{array } \\label{min2_}\\ ] ]      we need to prove that @xmath97 recall from and that @xmath98 \\calj_c(p , l ) & = \\pi_{\\bfs_\\bot } \\bmx l^{\\top } \\otimes i_m   & i_n \\otimes p \\emx .",
    "\\end{split}\\ ] ] the expression @xmath99 is then equivalent to the system of equations @xmath100 & ( i_n \\otimes p^{\\top})\\ ,   \\pi_{\\bfs_\\bot } \\pi_{\\bfs_\\bot } ( i_n \\otimes p ) \\mvec(l ) = 0 .",
    "\\end{cases}\\ ] ] if we denote @xmath101 , @xmath102 , is equivalent to @xmath103 which completes the proof .    a vector @xmath104 , with @xmath105 , is in the left kernel of @xmath106 when @xmath107          &        & \\mbox{vec}^\\top\\!(x)(i_n\\otimes p ) = \\mathbf{0}\\\\[1 mm ]          & \\iff & xl^\\top = \\mathbf{0 } \\mbox { and } p^\\top x = \\mathbf{0 } \\\\[1 mm ]          & \\iff & x = p_{\\bot } y l_{\\bot } , \\mbox { for some } y \\\\[1 mm ]          & \\iff & { \\textnormal{vec}(x ) } = ( l_{\\bot}^{\\top } \\otimes p_{\\bot } ) { \\textnormal{vec}(y ) }   \\\\[1 mm ]          & \\iff & x \\in\\mathop{\\mathrm{image } } ( l_{\\bot}^{\\top } \\otimes p_{\\bot } ) ,      \\end{array}\\ ] ] where we have used the well - known equality @xmath108 , @xmath109 and @xmath110 are orthogonal complements of @xmath96 and @xmath111 , respectively , and @xmath112 and @xmath113 then , @xmath114          & = & mn - ( m - r_p)(n - r_l)\\\\[1 mm ]          & \\leq & mr+nr - r^2 .",
    "\\end{array }      \\label{eq : rank_2nd_matrix}\\ ] ] the equality holds if and only if @xmath115 .    1 .",
    "since @xmath116 , it follows from lemma  4.4 that the row span of @xmath117 coincides with the row span of @xmath118 .",
    "we note that the cost function @xmath119 can be expressed as @xmath120 , and thus its gradient at the limit point can be expressed as @xmath121 and is also in the row span of @xmath118 .",
    "thus the equation @xmath122 is consistent and its set of solutions is an affine subspace of dimension @xmath123 2 .",
    "since @xmath124 , from lemma  4.4 we have that @xmath125 . since @xmath126 depends continuously on @xmath127 , then @xmath128 in a neighborhood of @xmath129 .",
    "height 2pt depth -1.6pt width 23pt , _ variable projection methods for approximate ( greatest ) common divisor computations _ , tech .",
    "report , vrije univ .",
    "brussel , 2013 .",
    "available from http://arxiv.org/abs/1304.6962 , submitted to _",
    "journal of symbolic computation_."
  ],
  "abstract_text": [
    "<S> we consider the problem of approximating an affinely structured matrix , for example a hankel matrix , by a low - rank matrix with the same structure . </S>",
    "<S> this problem occurs in system identification , signal processing and computer algebra , among others . </S>",
    "<S> we impose the low - rank by modeling the approximation as a product of two factors with reduced dimension . </S>",
    "<S> the structure of the low - rank model is enforced by introducing a penalty term in the objective function . </S>",
    "<S> the proposed local optimization algorithm is able to solve the weighted structured low - rank approximation problem , as well as to deal with the cases of missing or fixed elements . </S>",
    "<S> in contrast to approaches based on kernel representations ( in linear algebraic sense ) , the proposed algorithm is designed to address the case of small targeted rank . </S>",
    "<S> we compare it to existing approaches on numerical examples of system identification , approximate greatest common divisor problem , and symmetric tensor decomposition and demonstrate its consistently good performance .    * key words . * </S>",
    "<S> low - rank approximation , affine structure , penalty method , missing data , system identification , approximate greatest common divisor , symmetric tensor decomposition + * ams subject classifications . </S>",
    "<S> * 15a23 , 15a83 , 65f99 , 93b30 , 37m10 , 37n30 , 11a05 , 15a69 </S>"
  ]
}