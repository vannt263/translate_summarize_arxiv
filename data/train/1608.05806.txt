{
  "article_text": [
    "reweighting distributions ( also known as event reweighting or importance weighting ) is a general procedure , but its major use - case for particle physics is to modify the output of the monte carlo ( mc ) simulation to reduce disagreement with real data ( rd ) collected at a collider .",
    "there are many applications in hep including searching for rare decays ( decays with an extremely low probability in the standard model of elementary particles ) , when a classifier is trained on mc data to discriminate signal decays from background .",
    "however , the simulation is often imperfect ( see  @xcite for more details ) and corrections should be introduced . to calibrate the reweighting a similar physics process is considered , for which both real data and simulation can be obtained . for instance , in rare decays a normalization channel is selected  a decay with the similar kinematic characteristics ( see  @xcite for an example ) .",
    "reweighting techniques have applications outside hep : i.e.  in sociology a survey reweighting is used to reduce a non - response bias  @xcite .",
    "in what follows hep terminology is used , but approaches discussed in the paper are applicable to any reweighting .    mathematically , the problem is equivalent to estimating the density ratio @xmath0 as a function of the variables participating in reweighting .",
    "a density ratio estimation is a general problem in machine learning ( ml ) with numerous applications ( see @xcite ) .",
    "an approach widely used in high energy physics is reweighting with bins .",
    "the space of variables is split into bins , in each bin the weights of the simulated events are multiplied by @xmath1 to compensate the difference ( @xmath2 and @xmath3  total weight of events in a bin for rd and mc distributions ) . in other words , both densities @xmath4 and @xmath5 are estimated using histograms and then divided ( this gives another name of this approach  `` histogram division '' ) .    reweighting using bins is intuitive and easy - to - use , however , has very strong limitations :    * very few variables can be reweighted in practice , typically one or two ; * choosing which variable(s ) to use in reweighting is complex : reweighting one variable often brings disagreement in others ; * the amount of data needed to reliably estimate a density function with a histogram grows exponentially with the number of variables , which is commonly referred to as the `` curse of dimensionality '' .    to fight the last problem , one can reduce the number of bins along each variable , but this drives to a rough reweighting rule , insufficient to cover discrepancies .",
    "density estimation is a complex problem and it should be avoided in cases when only the ratio is of interest . a general and natural method of density ratio estimation is based on reusing general - purpose ml techniques . in",
    "@xcite this method was proposed and was successfully applied to particle physics problems .",
    "some general - purpose classification techniques ( i.e.  bdt and ann ) trained to discriminate mc and rd can provide probabilities @xmath6 , @xmath7 that a given event @xmath8 belongs to mc or rd .",
    "the probabilities can be used to estimate required density ratio : @xmath9    this approach successfully overcomes the curse of dimensionality , but provides inaccurate predictions when density ratio is high .",
    "one possible explanation is as follows : while regions with the high ratio are significant for reweighting , those are not of high importance for classification task : guessing correct class within regions with high / low ratio is easier , since most of the events belong to one class , and classification algorithms focus on the other regions .",
    "for example , when training ann or gbdt , these regions provide smaller contribution to the loss function , thus are given less attention .",
    "in this section a machine learning algorithm is proposed to solve the specific problem of reweighting . to address the problems of the histogram reweighting approach ,",
    "the space of variables is split into a few large regions .",
    "these regions are not obtained by a simple splitting of each variable into several bins , but in correspondence with the problem .",
    "a decision tree is used to split the regions .",
    "recall that decision trees split the space of variables into the regions by checking simple conditions .",
    "each region is associated with some leaf of the tree .    to find the regions that are suitable for reweighting , the symmetrized  @xmath10 is greedily optimized :",
    "@xmath11 this metric is maximized to find the regions important for reweighting .",
    "if in some leaf ( region ) the amount of mc events @xmath12 is much higher than the amount @xmath13 of rd events , the mc weights in this region must be decreased .",
    "the corresponding summand in the @xmath10 will be high reflecting the importance of this region for reweighting .",
    "the bdt reweighter makes use of many such trees which are trained one - by - one by repeating the following steps many times :    1 .",
    "build a shallow tree to maximize the symmetrized @xmath10 2 .",
    "compute predictions in the leaves : @xmath14 3 .   reweight the mc distribution ( compare this step with adaboost  @xcite ) :",
    "@xmath15 for each event @xmath16 is equal to the prediction of a leaf containing this event .",
    "the last two steps work in the same way as reweighting with bins , the distinction being that the bins are selected differently .",
    "also , since logarithm is taken , the predictions of the different trees are summed up , as is usually done in the boosting .    in the bdt reweighter",
    ", each tree in the sequence is trying to cover the discrepancies that were not resolved on the previous iterations .",
    "the complexity of a decision tree can be adjusted by varying the depth and the minimal number of samples in the leaf , making this approach highly tunable .",
    "the goal of reweighting is to have the mc distribution coincide with the rd distribution .",
    "comparing one - dimensional distributions is simple and can be done either by looking at the distributions or by computing one of well - known two - sample tests like kolmogorov - smirnov , anderson - darling or cramer  von mises . however , in the applications all the distributions are multidimensional .",
    "comparing only projections is obviously not enough to be sure that the distributions are identical .    at the same time",
    ", there are no useful multidimensional two - sample tests . given the whole pipeline of our analysis , two - samples tests are not necessary , because the question of interest is not whether the mc and rd distributions are different ( those are different ) .",
    "the question is whether an ml technique used later in the analysis ( i.e.  to detect signal decay , typically it is bdt or ann ) is able to use the discrepancy between rd and mc . thus it is only needed to check that after reweighting a classifier used in the analysis is not able to find the difference between the distributions . for this purpose a classifier is trained to discriminate rd and mc .",
    "its quality is checked by inspecting the roc curves on a holdout sample .",
    "overfitting is an issue that becomes obvious when using advanced methods of reweighting .",
    "one should measure the quality of the classification on a holdout ( data sample that was not participating in training ) to get unbiased estimations .",
    "the same approach works for reweighting : the quality of reweighting should be checked on the data that did not participate in training of the reweighting rule .",
    "the different cross - validation techniques like folding are also applicable .",
    "as an example the 11-dimensional distribution is taken ( simulated and real data ) . figure  [ fig : reweightdistrib ] demonstrates how the distribution of different features ( variables ) has changed after reweighting with the new method .",
    "comparison of real data ( blue ) and simulated ( green ) distributions before and after using the bdt reweighter . ]",
    "comparison of real data ( blue ) and simulated ( green ) distributions before and after using the bdt reweighter . ]    [ cols=\"<,>,>,>,>\",options=\"header \" , ]     in table [ fig : ksdistances ] the kolmogorov - smirnov distances are provided .",
    "reweighting with bins is done for last two variables , while two other approaches use all 11 variables .",
    "finally , the quality of reweighting is checked as it was proposed earlier and the roc curves are built on a holdout ( figure [ fig : roccurves ] ) .",
    "checking the quality of reweighting with ml .",
    "the roc curves for the classifier trained to discriminate rd and mc are computed on a holdout .",
    "reweighting with bins significantly reduces initially high discrepancy , but the classifier still can easily find the difference .",
    "ml - based solutions provide significantly better results , though reusing ml approach has minor issues in the left bottom corner . ]",
    "two problems are discussed in the paper :    1 .   event reweighting for multidimensional distributions 2 .",
    "the comparison of multidimensional distributions    it is demonstrated that both problems are effectively addressed by means of machine learning , while typically these steps in the analysis are considered outside of the scope of ml .    also , the novel method of reweighting is proposed : a modification of bdt algorithm , which alters the procedures of boosting and decision tree building .",
    "this method outperforms known reweighting approaches and makes it possible to reweight dozen of variables .",
    "when compared on the same problems , it requires less data to achieve the same quality .",
    "ready - to - use implementation of introduced algorithm is available in the ` hep_ml ` package @xcite .",
    "99 likhomanenko , t. , ilten , p. , khairullin , e. , rogozhnikov , a. , ustyuzhanin , a. , williams , m. ( 2015 ) .",
    "lhcb topological trigger reoptimization . in journal of physics : conference series ( vol .",
    "8 , p. 082025 ) .",
    "iop publishing .",
    "martschei , d. , feindt , m. , honc , s. , wagner - kuhr , j. ( 2012 ) .",
    "advanced event reweighting using multivariate analysis . in journal of physics : conference series ( vol .",
    "1 , p. 012028 ) . iop publishing .",
    "goodfellow , i. , pouget - abadie , j. , mirza , m. , xu , b. , warde - farley , d. , ozair , s. , courville , a. , bengio , y. ( 2014 ) .",
    "generative adversarial nets .",
    "advances in neural information processing systems ( pp . 2672 - 2680 ) ."
  ],
  "abstract_text": [
    "<S> machine learning tools are commonly used in modern high energy physics ( hep ) experiments . </S>",
    "<S> different models , such as boosted decision trees ( bdt ) and artificial neural networks ( ann ) , are widely used in analyses and even in the software triggers  @xcite .    in most cases , these are classification models used to select the `` signal '' events from data . </S>",
    "<S> monte carlo simulated events typically take part in training of these models . </S>",
    "<S> while the results of the simulation are expected to be close to real data , in practical cases there is notable disagreement between simulated and observed data . in order to use available simulation in training </S>",
    "<S> , corrections must be introduced to generated data . </S>",
    "<S> one common approach is reweighting  assigning weights to the simulated events .    </S>",
    "<S> we present a novel method of event reweighting based on boosted decision trees . </S>",
    "<S> the problem of checking the quality of reweighting step in analyses is also discussed . </S>"
  ]
}