{
  "article_text": [
    "in 1961 , james and stein @xcite considering the problem of estimating the mean vector @xmath4  of a @xmath5-dimensional  normal distributed random vector @xmath2  with a covariance matrix @xmath6  introduced an estimator @xmath7 which outperforms the maximum likelihood estimate ( mle ) @xmath8 for dimension @xmath9 ,  under the common quadratic risk @xmath10 in the sense that for all parameter values @xmath4",
    "@xmath11 this unexpected result draw a great interest of mathematical statisticians and stimulated a number of authors to contribute to the theory of improved estimation by extending the problem of james and stein in different directions to more general models with unknown covariance matrix and considering other types of estimates ( see @xcite for more details and other references ) .",
    "a considerable effort has been directed towards the problems of improved estimation in non - gaussian models with the spherically symmetric distributions ( see @xcite ) and in the non - parametric regression models @xcite .",
    "now the james  stein estimator and other improved shrinkage estimators are widely used in econometrics and the problems associated with the signal processing .    in this paper",
    "we will consider the problem of estimating the mean in a conditionally gaussian distribution .",
    "suppose that the observation @xmath2  is a @xmath5-dimensional random vector which obeys the equation @xmath12 where @xmath4  is a constant vector parameter , @xmath13  is a conditionally gaussian random vector with a zero mean and the covariance matrix @xmath14 ,  i.e. @xmath15 ,  where @xmath16  is some fixed @xmath17-algebra .",
    "we propose to consider a shrinkage estimator of the form @xmath18 where @xmath19  is a positive constant which will be specified below .",
    "it will be shown that such an estimator allows one to obtain an explicit upper bound for the quadratic risk in case of the regression model with a conditionally gaussian noise .",
    "theorem [ le.sec:gas.1 ] in section [ sec : gas ] claims that the estimator outperforms the maximum likelihood estimate @xmath20  uniformly in @xmath4  from any compact set @xmath21  for any dimension @xmath5  starting from two . in section [ sec : per ] , we apply the estimator to solve the problem of improved parametric estimation in the regression model in continuous time with a non - gaussian noise .    the rest of the paper is organized as follows . in section [ sec : gas ] , we impose some conditions on the random covariance matrix @xmath14  and derive to upper bound for the difference of risks @xmath22 corresponding to @xmath23  and @xmath20  respectively . in section [ sec : aut ] , the estimate is used for the parameter estimation in a discrete time regression with a gaussian noise depending on some nuisance parameters .",
    "appendix contains some technical results .",
    "in this section we will derive an upper bound for the risk of estimate under some conditions on the random covariance matrix @xmath14 .",
    "assume that    @xmath24  there exists a positive constant @xmath25 ,  such that the minimal eigenvalue of matrix @xmath14  satisfies the inequality @xmath26",
    "@xmath27  the maximal eigenvalue of the matrix @xmath14  is bounded on some compact set @xmath21  from above , i.e. @xmath28 where @xmath29  is some known positive constant .",
    "let denote the difference of the risks of estimate and that of as    @xmath30    we will need also the following constant @xmath31 where @xmath32,@xmath33    [ le.sec:gas.1 ]    let the noise @xmath13  in have a conditionally gaussian distribution @xmath34  and its covariance matrix @xmath14  satisfy conditions @xmath35  with some compact set @xmath21 .",
    "then the estimator with @xmath36  dominates the mle @xmath20  for any @xmath37 ,  i.e. @xmath38 ^ 2.\\ ] ]    first we will establish the lower bound for the random variable @xmath39 .",
    "[ le.sec:gas.2 ] under the conditions of theorem 2.1 @xmath40    the proof of lemma is given in the appendix .    in order to obtain the upper bound for @xmath41  we will adjust the argument in the proof of stein s lemma @xcite to the model with a random covariance matrix .",
    "we consider the risks of mle and of @xmath42 r(\\theta^{*},\\theta)=r(\\hat{\\theta}_{ml},\\theta)+\\mathbf{e}_{\\theta}[\\mathbf{e}((g(y)-1)^{2}\\|y\\|^{2}|\\mathcal{g})]\\\\[2 mm ] + 2\\sum_{j=1}^{p}\\mathbf{e}_{\\theta}[\\mathbf{e}((g(y)-1)y_{j}(y_{j}-\\theta_j)|\\mathcal{g})],\\end{gathered}\\ ] ] where @xmath43 .    denoting @xmath44  and applying the conditional density of distribution of a vector @xmath2  with respect to @xmath17-algebra",
    "@xmath16 @xmath45 one gets @xmath46    making the change of variable @xmath47  and assuming @xmath48 ,  one finds that @xmath49 where @xmath50  denotes the @xmath51-th  element of matrix @xmath52 .",
    "these quantities can be written as @xmath53    thus , the risk for an estimator takes the form @xmath54|_{u = y}\\right).\\end{gathered}\\ ] ]    therefore one has @xmath55 where @xmath56 this implies that @xmath57    since @xmath58 ,  one comes to the inequality @xmath59    from here it follows that @xmath60 taking into account the condition @xmath24  and the lemma  [ le.sec:gas.2 ] , one has @xmath61    minimizing the function @xmath62  with respect to @xmath19 ,  we come to the desired result @xmath63 ^ 2.\\ ] ] hence theorem  [ le.sec:gas.1 ] .",
    "[ le.sec:gas.3 ] let in the noise @xmath64  with the positive definite non random covariance matrix @xmath65  and @xmath66",
    ".  then the estimator with @xmath67  dominates the mle for any @xmath37  and compact set @xmath21 ,  i.e. @xmath38 ^ 2.\\ ] ]    note that if @xmath68  then @xmath69 ^ 2.\\ ] ]    [ le.sec:gas.4 ] if @xmath70 and @xmath71  in model then the risk of estimate is given by the formula @xmath72 ^ 2=:r_p.\\ ] ]    by applying the stirling s formula for the gamma function @xmath73 one can check that @xmath74  as @xmath75 .",
    "the behavior of the risk for small values of @xmath5  is shown in fig.1 .",
    "it will be observed that in this case the risk of the james  stein estimate remains constant for all @xmath9 ,  i.e. @xmath76 and the risk of the mle @xmath20  is equal to @xmath5  and tends to infinity as @xmath75 .    at @xmath71 .",
    ", scaledwidth=90.0% ]",
    "in this section we apply the proposed estimate to a non - gaussian continuous time regression model .",
    "let observations @xmath77  obey the equation @xmath78 here a vector @xmath79  of unknown parameters from some compact set @xmath21.assume that @xmath80  is a one - periodic @xmath81 functions , bounded and orthonormal in @xmath82 $ ] .",
    "the noise @xmath83  in is a non - gaussian ornstein  uhlenbeck process given by stochastic differential equation @xmath84 where @xmath85  is unknown parameter , @xmath86  is a levy process satisfying the equation @xmath87 here @xmath88  are unknown constants , @xmath89  is a standard brownian motion , @xmath90  is a compound poisson process defined as @xmath91 where @xmath92  is a poisson process with unknown intensity @xmath93 and @xmath94  is a sequence of i.i.d .",
    "gaussian random variables with parameters ( 0,1 ) .",
    "the problem is to estimate the unknown vector parameter @xmath4  on the basis of observations @xmath77 .",
    "let @xmath95  denote the @xmath17-algebra  generated by the poisson process .",
    "it will be noted that the model is conditionally gaussian given the @xmath17-algebra  @xmath16 .  therefore one can use estimate to obtain an improved estimate of the unknown vector parameter @xmath4 .  to this end",
    "we have to reduce the initial continuous time regression model to a discrete time model of the form .",
    "the quality of an estimator @xmath96  will be measured by the quadratic risk @xmath97    a commonly used estimator of an unknown vector @xmath4  in model is the least squares estimate ( lse ) @xmath98  with @xmath99 from here taking into account , one has @xmath100 where @xmath101  and @xmath102  is the random vector with coordinates @xmath103 note that the vector @xmath102  has a conditionally gaussian distribution with a zero mean and conditional covariance matrix @xmath104  with the elements @xmath105    thus the initial problem of estimating parameter @xmath4  in can be reduced to the that of estimating parameter @xmath4  in conditionally gaussian regression model .",
    "[ le.sec:per.1 ] let the regression model be given by the equations  , @xmath106 .  then , for any @xmath107  and @xmath108 ,  the estimator of @xmath4@xmath109 dominates the lse @xmath110:@xmath111 ^ 2.\\ ] ]    to proved this theorem one can apply theorem [ le.sec:gas.1 ] and it suffices to check conditions @xmath24 ,  @xmath27  on the matrix @xmath112 .",
    "the proof of conditions @xmath24  and @xmath27  is given in the appendix .",
    "in this section we consider the problem of improved estimating the unknown mean of a multivariate normal distribution when the dispersion matrix is unknown and depends on some nuisance parameters .",
    "let in the noise @xmath113 ,  be described by a gaussian autoregression process @xmath114 where @xmath115 ,  @xmath116  and @xmath117  are independent gaussian ( 0,1 ) random variables .",
    "assume that the parameter @xmath118  in is unknown and belongs to interval @xmath119 $ ] ,  where @xmath120  is known number .",
    "it is easy to check that the covariance of the noise @xmath13  has the form    @xmath121       a & 1 & ... & a^{p-2 } \\\\[2 mm ]       &   \\ddots & &    \\\\[2 mm ] a^{p-1 } & a^{p-2 } & ... & 1 \\end{array } \\right ) \\ ] ]    let @xmath13 in be specified by with @xmath122 $ ] .  then for any @xmath123",
    "the mle is dominated by the estimator @xmath124 and @xmath125    one has that @xmath126",
    ".  now we find the estimation of the maximal eigenvalue of matrix @xmath127 .  by definition @xmath128 one",
    "has @xmath129 by applying the cauchy  bunyakovskii inequality",
    "we obtain that @xmath130 thus , @xmath131 hence , taking into account the theorem [ le.sec:gas.1 ] we come to assertion of proposition .",
    "in this paper we propose a new type improved estimation procedure . the main difference from the well - known james  stein estimate is that in the dominator in the corrected term we take the first power of the observation norm @xmath132 .",
    "this allow us to improve estimation with respect to mle begining with any dimension @xmath0 .",
    "moreover , we apply this procedure to the estimation problem for the non - gaussian ornstein  uhlenbeck  levy regression model .",
    "6.1 . proof of the lemma [ le.sec:gas.2 ] .    from one",
    "has @xmath133 using a repeated conditional expectation and since the random vector @xmath13  is distributed conditionally normal with a zero mean , then @xmath134 making the change of variable @xmath135  and applying the estimation @xmath136  we find @xmath137            6.2 .",
    "the proof of conditions @xmath24  and @xmath27  on the matrix @xmath112 .",
    "the elements of matrix @xmath112  can be written as @xcite @xmath140 where @xmath141 and @xmath142  are the jump times of the poisson process @xmath92 ,  i.e. @xmath143      notice that by one can the matrix @xmath112  present as @xmath146 where @xmath147  is non random matrix with elements @xmath148 and @xmath149  is a random matrix with elements @xmath150 .",
    "\\end{gathered}\\ ] ] this implies that @xmath151 therefore @xmath152 and we come to the assertion of lemma [ lem.sec:app.1 ] .",
    "[ lem.sec:app.2 ] let @xmath83  be defined by with @xmath85 .",
    "then a maximal eigenvalue of the matrix @xmath153  with elements defined by , satisfy the following inequality @xmath154 where @xmath155  and @xmath156 .",
    "one has @xmath157 where @xmath158 since the @xmath159 is a one - periodic orthonormal functions therefore the first integral is equal to @xmath160  and in view of the inequality @xmath161 for any @xmath85   @xmath162 from here denoting @xmath163 we obtain that @xmath164 hence lemma [ lem.sec:app.2 ] .",
    "w. james , c. stein , estimation with quadratic loss , in : proceedings of the fourth berkeley symposium on mathematics statistics and probability , vol . 1 , university of california press , berkeley , 1961 , pp .",
    "361 - 380 ."
  ],
  "abstract_text": [
    "<S> the paper considers the problem of estimating a @xmath0  dimensional mean vector of a multivariate conditionally normal distribution under quadratic loss . </S>",
    "<S> the problem of this type arises when estimating the parameters in a continuous time regression model with a non - gaussian ornstein  uhlenbeck process driven by the mixture of a brownian motion and a compound poisson process . </S>",
    "<S> we propose a modification of the james  stein procedure of the form @xmath1  where @xmath2  is an observation and @xmath3  is a special constant . </S>",
    "<S> this estimate allows one to derive an explicit upper bound for the quadratic risk and has a significantly smaller risk than the usual maximum likelihood estimator for the dimensions @xmath0 .  </S>",
    "<S> this procedure is applied to the problem of parametric estimation in a continuous time conditionally gaussian regression model and to that of estimating the mean vector of a multivariate normal distribution when the covariance matrix is unknown and depends on some nuisance parameters .    </S>",
    "<S> _ keywords _ : conditionally gaussian regression model ; improved estimation ; james  stein procedure ; non - gaussian ornstein  uhlenbeck process .    _ ams 1991 subject classifications _ : primary : 62c20 ; secondary : 62c15 </S>"
  ]
}