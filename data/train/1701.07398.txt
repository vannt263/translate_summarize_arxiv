{
  "article_text": [
    "neuroscientists and cognitive scientists have many tools at their disposal to study the brain and neural networks in general , including electroencephalography ( eeg ) , single - photon emission computed tomography ( spect ) , functional magnetic resonance imaging ( fmri ) and microelectrode arrays ( mea ) , to name a few .",
    "however , the amount of information and level of control afforded by these tools do not remotely resemble what is available to an engineer working on an artificial neural network .",
    "the engineer can manipulate any neuron at any time , force certain excitations , intervene in ongoing processes , and collect as much data about the network as needed , at any level of detail .",
    "this wealth of information has enabled reverse engineering research on artificial neural networks , leading to insights into the inner workings of trained artificial neural networks .",
    "this suggests an indirect approach to studying the brain : training a biologically plausible neural network model to exhibit complex behavior observed in real brains , and reverse engineering the result . in line with this approach",
    ", we designed an artificial visual system based on a fully recurrent unlayered neural network that learns to perform saccadic eye movements .",
    "saccadic eye movements are quick , unconscious , task - dependent @xcite motions following the demand of attention @xcite , that direct the eye to new targets that require the high resolution of the fovea .",
    "these targets are usually detected within the peripheral visual system @xcite .",
    "once a target is centered at the fovea , the eye fixates for a fraction of a second while the visual system extracts the necessary information .",
    "most eye movements are proactive rather than reactive , predict actions in advance and do not merely respond to visual stimuli @xcite .",
    "there is good evidence that much of the active vision in humans results from reinforcement learning ( rl ) @xcite , as part of and organism s attempt to maximize its performance while interacting with the environment @xcite .",
    "accordingly , we train the artificial visual system within the rl paradigm .",
    "the network was not explicitly engineered to perform a certain task , and does not contain an explicit memory component rather it has memory only by virtue of its recurrent topology .",
    "learning takes place in a model - free setting using policy gradient techniques .",
    "we find that the network displays attributes of human learning such as : ( a ) decision making and gradual confidence increase along with accumulated evidence , ( b ) skill transfer , namely the ability to use a pre - learned skill in a certain task in order to improve learning on a related but more difficult task , ( c ) selectively attending information relevant for the task at hand , while ignoring irrelevant objects in the field of view .",
    "we designed an artificial visual system ( avs ) with the task of learning an attention model to control saccadic eye movements , and subsequent classification of digits .",
    "we refer to this task as the _ attention - classification task_. the avs is similar in many ways to that presented in @xcite .",
    "it is a simplified model of the human visual system , consisting of a small region in the center with high resolution , analogous to the human fovea , and two larger concentric regions which are sub - sampled to lower resolution and are analogous to the peripheral visual system in humans .",
    "the avs was trained and tested on the classification of handwritten digits from the mnist data set @xcite .",
    "only a small part of the image is visible to the avs at any one time .",
    "specifically , full resolution is only available at the fovea , which is 9-by-9 pixels , as in @xcite , or 5-by-5 pixels ( about 69% smaller ) .",
    "the first peripheral region is double the size of the fovea , but sub - sampled with period 2 to match the size of the fovea in pixels .",
    "similarly , the second peripheral region is quadruple the size of the fovea but sub - sampled with period 4 . for comparison , a typical digit in the mnist",
    "database occupies about 20-by-20 pixels of the image .",
    "the location of the observation within the image is not available to the avs ( unlike @xcite ) , and movements of the observation location are not constrained to image boundaries .",
    "instead , locations outside the image boundaries are observed as black pixels .    ]",
    "the avs consists of inputs ( observations ) projected upon the network via input weights @xmath0 , a neural network consisting of @xmath1 neurons connected by the recurrent weights @xmath2 , and two outputs : a classifier @xmath3 , responsible for identifying the digit after the avs has explored the image , and the attention model output @xmath4 , responsible for directing the eye to new locations based on the information represented in the network state ( see figure [ fig : artificial - visual - system ] ) .",
    "the output @xmath5 consists of one neuron for each possible digit . at the end of the trial ,",
    "the identity of the highest valued neuron is interpreted as the network s classification .",
    "the progression of a single trial follows these principal stages :    1 .",
    "a random digit is selected from the mnist training database .",
    "2 .   a location across the image is randomly selected .",
    "the observation ( called ` glimpse ' @xcite ) from the current location is projected upon the network through @xmath0 , along with any pre - existing information within the network state through the recurrent weights @xmath2 .",
    "the attention model output @xmath4 is fed back as a saccade of the eye , i.e. as the size of movement from the current location in the horizontal and vertical axes .",
    "5 .   if a predefined number of glimpses has passed ( or by network decision ) , compare the classifier output @xmath3 to the true label , otherwise return to stage 3 .",
    "reward the avs if the classification was correct , and continue to the next trial .",
    "the avs is implemented by a fully recurrent neural network .",
    "its network topology is similar the echo state network ( esn ) @xcite in that the recurrent neural connections are drawn randomly and are not constrained to a particular topology such as in layered feedforward networks , or long short - term memory networks .",
    "the network state evolves according to    @xmath6    where    * @xmath7 is the state of network at time step @xmath8 , each element representing the state of a single neuron , * @xmath9 $ ] is the leak rate , * @xmath10 is the internal connections weight matrix , * @xmath11 is the input weight matrix , * @xmath12 is the observation ( network input ) , * @xmath13 and @xmath14 are independent discrete - time gaussian white noise processes with independent components , each having variance @xmath15 respectively , * @xmath16 is the state of the @xmath17 output neurons , * @xmath18 is the output weight matrix ( consisting of blocks @xmath19 for the corresponding output components ) .",
    "the gradient of the expected reward @xmath20 is estimated as in @xcite , @xmath21 where @xmath22 is a random trajectory of @xmath23 glimpses , @xmath24 is the probability of trajectory @xmath25 , @xmath26 the observed ( usually binary ) reward , @xmath27 a fixed baseline computed as in @xcite , @xmath28 is the random location of the first glimpse , and @xmath29 indicates averaging over trajectories . viewed as a partially observable markov decision process ( pomdp ) , we can write the distributions describing the agent : @xmath30 where @xmath31 are the probability density functions of @xmath32 respectively",
    ". the pomdp dynamics are deterministic : the glimpse position @xmath33 evolves as @xmath34 .    for the avs",
    "the probability density of a trajectory @xmath25 is @xmath35 here only the output probabilities @xmath36 depend on @xmath37 , and , using and the gaussian distribution of the noise , we find that the log likelihood gradient with respect to @xmath37 takes the form @xmath38 where @xmath23 is the number of glimpses .",
    "stochastic gradient ascent is performed only for the output weights @xmath37 .",
    "recurrent weights are randomly selected , with spectral radius 1 , and remain fixed throughout training . the log likelihood with respect to",
    "the internal weight matrix @xmath2 takes a similar form .",
    "however , the recurrent connections were not learned in our simulations .",
    "since information accumulated by the neural network over time is mixed into the state of the network , it is not obvious that the potential to extract useful historic information can be exploited within the attention model solution .",
    "training uses gradient ascent to the local maxima of the estimated expected reward and therefore may converge to sub - optimal maxima that do not make use of the full potential of the system . in order to test the use of memory by the trained network ,",
    "two similar avs were trained on the attention - classification task . in the first avs ,",
    "recurrent weights were random , whereas the second avs was set to ` forget ' historic information , by setting the recurrent weights matrix to zero .    ]",
    "use of memory was found to depend on the size of the fovea .",
    "[ fig : exploitation - of - memory - r5 ] shows the performance of the system across training epochs , for the case of a large ( 9@xmath399 pixels ) fovea .",
    "initially , the avs with memory has the advantage as the attention model is still poor at this stage , leading to relatively uninformative glimpses , so the use of information from several glimpses results in better classification .",
    "however , as the attention mechanism improves the last glimpse becomes highly informative , so the memoryless network , where information from the last glimpse is not corrupted by memory of previous glimpses , has the advantage .",
    "in fact , we found that information from a well - placed glimpse suffices to classify the digit with over 90% success rate in this case , driving the network to a solution of finding a single good glimpse location across the digit , and classification based on that glimpse , without regard to the rest of the trajectory .",
    "the situation is different with a smaller fovea ( 5@xmath395 pixels ) , where classification from a single glimpse becomes harder .",
    "as seen in figure [ fig : exploitation - of - memory - r3 ] , the avs with memory outperforms the one without memory in the small fovea case .    ]",
    "the human visual system acts to maximize the information relevant to the task @xcite . in order to assess",
    "whether our avs behaves similarly , we have to characterize the relevant information in the context of our task .",
    "since the network classification @xmath3 depends linearly on the network state in the last time step , we quantify the task - relevant information as the best linear separation of the network state , between each class and the other classes .",
    "accordingly , we use linear discriminant analysis ( lda ) @xcite , which acts to find the projection that minimizes the distance between samples of the same cluster @xmath40 while at the same time maximizes the distance between clusters @xmath41 . the distance within each class",
    "is measured by the variance of samples belonging to that class , and @xmath40 is taken to be the mean of these distances across all classes .",
    "the distance between classes @xmath41 is defined as the variance of the set of class centers .",
    "we trained an avs on the attention - classification task with 5 glimpses per digit .",
    "after the avs was trained , it was tested in two cases . in the first case",
    ", the system was run as usual and the network state vector was recorded after the last glimpse of each digit in the test set . in the second case ,",
    "the location of the last glimpse was chosen randomly rather than following the learned attention model .",
    "the results are illustrated in figure [ fig : gathering - information ] , where the state of the network is projected on the first two eigenvectors of @xmath42 .",
    "separation is significantly better with the full attention model compared to the one with the random last glimpse .",
    "we conclude that , at the very least , the attention model acts to maximize task - relevant information in the last glimpse better than a random walk .",
    "results of linear discriminant analysis of the avs state at the last time step .",
    "each dot corresponds to single trial and represents the projection of the network state on the first two eigenvectors of @xmath42 .",
    "dots are colored according to the digit presented to the network .",
    "left : random last glimpse .",
    "right : full attention model . ]",
    "biological learning often displays the ability to use a skill learned on a simple task in order to improve learning of a harder yet related task , e.g. , proficiency at tennis is beneficial when learning racquetball and even seemingly unrelated tasks such as skiing for example @xcite . to test whether transfer learning is possible in the avs",
    ", we trained it to learn the attention model and classification of 3 digits ( out of 10 in the mnist database ) .",
    "the resulting solution served as an initial condition for learning the full task of classifying all 10 digits .",
    "as seen in fig .",
    "[ fig : transferable - skill ] , not only did the avs with pre - learned attention learn much faster , but it also achieved a better result at the end of training .    ]      the eyes are not directed to the most visually salient points in the field of view , but rather to the ones that are most relevant for the task at hand @xcite .",
    "accordingly , we introduced an highly salient object into the training images .",
    "the object is a square , approximately the size of a digit but with maximum brightness , whereas the digits are handwritten and displayed in grayscale .",
    "the object is inserted at a fixed position relative to the digit , always on the right hand side of the digit .",
    "the trained network successfully avoids unnecessary fixations on the salient object . in cases where the first glimpse falls upon an area where both the digit and the object are within the peripheral visual region",
    ", the object seems to be completely ignored .",
    "perhaps more interesting is the case where only the object is visible in the first glimpse , within the peripheral view .",
    "in such a case , the avs learned to exploit the fact that the digit is always located on the left of the object and consistently performs saccades to the left .",
    "thus , not only was the presence of a distracting object not harmful to performance , but it was actually beneficial .    ]    in fig .",
    "[ fig : fixed - distracting - object ] , the colored squares represent the foveal view of 5-by-5 pixels at each time step going from blue to red .",
    "the left and middle images show cases where the first glimpse only observes the distracting object within the peripheral view .",
    "the left image shows a case where the first glimpse observes both the distracting object and the digit within the peripheral view .",
    "next , we test the network with a distracting object in a random position around the digit .",
    "the observed behavior was similar when the first glimpse happened to fall on a location where both the object and digit are within the peripheral view : the avs ignored the distraction and directed itself towards the digit .",
    "however , in the case where the first glimpse falls on a location where only the distracting object is visible in the peripheral view , the avs failed to locate the digit .    ]",
    "an example is seen in fig .",
    "[ fig : free - distracting - object ] .",
    "the lines are trajectories of the avs each starting from a different point on a test grid and followed until the last glimpse ( blue / magenta dot ) .",
    "green lines are trajectories that led to a correct classification while red lines are trajectories that led to a false classification .",
    "when the avs happen to fall at a location where the digit is not seen , it directs its gaze towards the square , which it then chooses to classify as `` 1 '' thus earning 10% expected reward which is better than nothing .",
    "learning by demonstration ( or learning with guidance ) was implemented in the avs .",
    "demonstration differs from supervision in two key ways .",
    "first , demonstration is not continuous , and is applied sparsely in time in order to suggest new trajectories to the system .",
    "second , demonstration is not required to provide the best solution to the system , because the system maintains its freedom to explore and even improve upon it .",
    "demonstration was achieved by providing the network with a sparse and naive suggestion for the attention model . for example , on @xmath43 of trajectories , the system was directed to the center of the digit on the last glimpse .",
    "such partial direction resulted in a significant improvement of both speed of learning and the final success rate , as can be observed in figure [ fig : demonstraion - learning ] .",
    "demonstration in the avs system was made possible by manipulating the exploration noise .",
    "the exploration noise is a gaussian white noise and as such has probability greater than zero to accept any value .",
    "since the output of the system at any given time is a function of that noise , we can force the output to a specific value by setting the exploration noise in that particular time step to be @xmath44 where @xmath45 is now the demonstrated output of the attention model and @xmath46 is the determined exploration noise that will bring the system to that desired output .",
    "as long as the demonstration is kept sparse enough , it would in practice not break the assumption that the noise is a gaussian white noise .",
    "the noise in the system is an essential part of the log likelihood gradient and therefore the system would not only arrive to the desired output at that particular time step , but also learn from that experience .    ]",
    "we have shown that a simple artificial visual system , implemented through a recurrent neural network using policy gradient reinforcement learning , can be trained to perform classification of objects that are much larger than its central region of high visual acuity .",
    "while receiving only classification based reward , the system develops an active vision solution , which directs attention towards relevant parts of the image in a task - dependent way .",
    "importantly , the internal network memory plays an essential role in maintaining information across saccades , so that the final classification is achieved by combing information from the current visual input and from previous inputs represented in the network state . within a generic active vision system , without any specifically crafted features ,",
    "we have been able to explain several features characteristic of biological vision : _ ( i ) _ good classification performance using reinforcement learning based on highly limited central vision and low resolution peripheral vision , _ ( ii ) _ gathering task - relevant information through active search , _ ( iii ) _ transfer learning , _",
    "( iv ) _ ignoring task - irrelevant distractors , _ ( v ) _ learning through guidance . beyond providing a model for biological vision ,",
    "our results suggest possible avenues for cost - effective image recognition in artificial vision systems ."
  ],
  "abstract_text": [
    "<S> the human visual perception of the world is of a large fixed image that is highly detailed and sharp </S>",
    "<S> . however , receptor density in the retina is not uniform : a small central region called the fovea is very dense and exhibits high resolution , whereas a peripheral region around it has much lower spatial resolution . </S>",
    "<S> thus , contrary to our perception , we are only able to observe a very small region around the line of sight with high resolution . the perception of a complete and stable view is aided by an attention mechanism that directs the eyes to the numerous points of interest within the scene . </S>",
    "<S> the eyes move between these targets in quick , unconscious movements , known as `` saccades '' . once a target is centered at the fovea , the eyes fixate for a fraction of a second while the visual system extracts the necessary information . </S>",
    "<S> an artificial visual system was built based on a fully recurrent neural network set within a reinforcement learning protocol , and learned to attend to regions of interest while solving a classification task . </S>",
    "<S> the model is consistent with several experimentally observed phenomena , and suggests novel predictions . </S>"
  ]
}