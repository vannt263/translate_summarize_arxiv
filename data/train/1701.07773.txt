{
  "article_text": [
    "the tevatron was a proton - antiproton collider located at fermi national accelerator laboratory ( fermilab ) .",
    "run ii of the tevatron , occurring from 2001 to 2011 and having collisions with a center - of - mass energy of 1.96 tev , saw the cdf and d0 collaborations @xcite record datasets corresponding to an integrated luminosity of approximately 11 fb@xmath0 per experiment .",
    "these datasets helped make groundbreaking contributions to high energy physics including the most precise measurements of the @xmath1 boson and top quark masses , observation of electroweak production of top quarks , observation of @xmath2 oscillations , and first evidence of higgs boson decay to fermions .",
    "the unique nature of the tevatron s proton - antiproton collisions and large size of the datasets means that the cdf and d0 data will retain their scientific value for years to come , both as a vehicle to perform precision measurements as newer theoretical calculations appear , and to potentially validate any new discoveries at the lhc .",
    "the fermilab run ii data preservation project ( r2dp ) aims to ensure that both experimental collaborations have the ability to perform complete physics analyses on their full datasets through at least the year 2020 . to retain full analysis capability ,",
    "the project must preserve not only the experimental data themselves , but also their software and computing environments .",
    "this requires ensuring that the data remain fully accessible in a cost - effective manner and that experimental software and computing environments are supported on modern hardware .",
    "furthermore user jobs must be able to run at newer facilities when dedicated computing resources are no longer available and job submission and data movement to these new facilities must be accomplished within the familiar software environment with a minimal amount of effort on the part of the end - user .",
    "documentation is also a critical component of r2dp and includes not only the existing web pages , databases , internal documents , but also requires writing clear , concise instructions detailing how users need to modify their usual habits to work in the r2dp computing infrastructure .",
    "at the time of the tevatron shutdown , the data for both cdf and d0 were stored on lto4 tapes @xcite , which have a per - tape capacity of 800  gb . an analysis of then - available tape technologies concluded that t10k tapes @xcite , with a capacity of up to 5  tb per tape , would be the near - term choice for archival storage at fermilab . while it was theoretically possible to leave the cdf and d0 data on lto4 storage , a decision was made to migrate these data to t10k storage for two reasons .",
    "first , if the tevatron data were accessed for a long period of time after data taking ended , the lto4 storage may be an unsupportable configuration ; as lto4 tapes decline in usage industry - wide there may not be replacement storage easily available .",
    "second , as storage media and drives for older technology become scarce , their costs rise , potentially increasing the overall long - term cost of staying with lto4 storage . due to these concerns",
    ", the commitment was made to purchase t10k tapes and migrate all of the cdf and d0 data .",
    "it took approximately two years for the migration to be completed ( fig .",
    "[ fig : tapemigration ] ) and the cdf and d0 data now share tape access resources with active fermilab experiments .",
    "both cdf and d0 migrated all data stored on tape , including raw detector data , reconstructed detector data and derived datasets , and simulation .",
    "cdf has also made an additional copy of its raw detector data at the national centre for research and development in information technology ( cnaf ) in italy  @xcite .",
    "table  [ tab : datatypes ] shows the amount of each type of data that cdf and d0 have stored over run ii .",
    ".types of data stored on tape for cdf and d0 along with approximate size in pb .",
    "the numbers do not include the additional 4 pb of cdf raw data stored at cnaf . [ cols=\"<,^,^\",options=\"header \" , ]",
    "both cdf and d0 have complex software frameworks to carry out simulation , reconstruction , calibration , and analysis . most of the core software was developed in the early- to mid-2000s on scientific linux running on 32-bit x86 architectures . during the operational period of the tevatron",
    ", releases of the software framework were maintained on dedicated storage elements that were mounted on the experiments respective dedicated computing clusters .",
    "as these dedicated resources are no longer maintained , cdf and d0 have migrated their software releases to cern virtual machine file system ( cvmfs ) repositories  @xcite . as cvmfs has been widely adopted by current fermilab experiments",
    ", this move allows for maintaining cdf and d0 software releases for the foreseeable future without a significant investment in dedicated resources .",
    "furthermore , as the fermilab - based cvmfs repositories are distributed to a variety of computing facilities away from fermilab , this approach lends to cdf and d0 computing environments that exist on a variety of remote sites .    at the time of the tevatron shutdown",
    ", both cdf and d0 were running software releases that , while operational on scientific linux 5 ( `` sl5 '' ) , depended on compatibility libraries that were built in previous os releases dating back to scientific linux 3 .",
    "the two experiments chose different strategies to ensure the functionality of their software releases throughout the data preservation period .      at cdf , stable software releases were available under two flavors : one was used in reconstruction of collision data and analysis and another for monte carlo generation and simulation . to ensure the long - term viability of cdf analysis capability ,",
    "the cdf software team chose to prepare brand new `` legacy '' releases of both flavors .",
    "these legacy releases were stripped of any long - obsolete packages that were no longer used for any analysis and also shed any compatibility libraries built prior to sl5 . once validated and distributed , older releases of cdf software which still depended on compatibility libraries were removed from the cvmfs repository .",
    "this meant that usage of cdf code for analysis on any centrally available resources was guaranteed to be fully buildable and executable on sl5 .",
    "furthermore , it meant a relatively simple process for further ensuring the legacy release is buildable and executable exclusively on scientific linux 6 ( `` sl6 '' ) , the target os for r2dp .",
    "the total size of the cdf code base in the legacy releases is 326  gb , which includes compiled code and most external dependencies .      after careful study , the d0 software team chose to stay with its software releases that were current at the time of the tevatron shutdown , but updated common tools where possible , and also made sure that 32-bit compatibility system libraries are installed on worker nodes at fermilab , where d0 plans to run jobs throughout the r2dp project lifetime . if d0 physicists should wish to run analysis jobs outside of fermilab in future years",
    ", they will need to ensure that 32-bit versions of system libraries such as glibc are available at any future remote sites .",
    "resources at fermilab , however , are sufficient to meet the projected demand over the project lifetime . required pre - sl6 compatibility libraries can also be added to the cvmfs repository if needed in the future .",
    "the total size of the d0 software repository in cvmfs , including code base , executables , and external product dependencies , is currently 227  gb .",
    "during run ii , cdf and d0 both had large dedicated analysis farms ( cdfgrid and cab , respectively ) of several thousand cpu cores each .",
    "since the end of the run , these resources have been steadily diminishing as older nodes are retired and some newer ones are repurposed . while the computing needs of the experiments have declined over the years ( fig .",
    "[ fig : gridjobs ] ) , preserving full analysis capability requires that both experiments have access to opportunistic resources and a way to submit jobs to them .",
    "the fermilab general purpose grid ( `` gpgrid '' ) , used by numerous other experiments based at fermilab , is a natural choice for the tevatron experiments .",
    "both cdf and d0 have worked with the fermilab scientific computing division to add the ability to run their jobs on gpgrid , by adopting the fermilab jobsub product  @xcite used by other fermilab experiments . having users submit their analysis jobs via jobsub solves the issue of long - term support , but introduces an additional complication for users who are unfamiliar with the new system , or who may return to do a tevatron analysis many years from now and will not have time to learn an entirely new system .",
    "thus , both cdf and d0 have implemented wrappers around the jobsub tool that emulate job submission commands each experiment normally uses .        in the case of d0 ,",
    "users who wish to submit to gpgrid instead of cab ( which will be required once cab is retired ) can simply do so by adding an extra command line option .",
    "the d0 submission tools will then generate and issue the appropriate jobsub commands without any direct user intervention .",
    "users can switch to submitting jobs to gpgrid with a minimum amount of effort , and future analyzers will not need to spend time learning an entirely new system .",
    "we have successfully tested submission of all common job types ( simulation , reconstruction , user analysis ) to gpgrid using the modified d0 submission tools .    in the case of cdf",
    ", the existing cdfgrid gateway was retired with all remaining hardware ( worker nodes ) absorbed into gpgrid .",
    "analysis and monte carlo generation jobs are now submitted entirely using jobsub and a cdf - specific wrapper in front .",
    "this also allows for cdf analysis jobs to go to remote sites on the open science grid , specifically to sites that previously had to operate cdf - specific gateways .",
    "these sites can now support cdf computing either opportunistically or via dedicated quotas without the need to support a separate gateway .",
    "this transition was completed in early 2015 and cdf computing use did not diminish in 2015 as compared to 2014 despite moving to an environment with no dedicated computing nodes .",
    "cdf physicists consumed over 5 million cpu hours on gpgrid in the twelve months following the transition .",
    "both cdf and d0 use the sequential access to metadata ( sam ) service  @xcite for data handling .",
    "older versions of sam used oracle backends with a corba - based communication infrastructure , while more recent versions use a postgresql - based backend , with communication over http . throughout run ii cdf and d0 used corba - based versions of sam , but can now also communicate with their existing oracle databases using the new http interfaces as part of r2dp , eliminating the requirement of supporting the older corba - based communication interface through the life of the project .",
    "while cdf and d0 code bases had to be modified to interface with these new communication interfaces , these changes are transparent to the end user .",
    "for d0 , part of the sam infrastructure included dedicated cache disks on the d0 cluster worker nodes that allowed for rapid staging of input files to jobs .",
    "as files were requested through sam , they would be copied in from one of the cache disks if they were present . if they were not already on one of the cache disks sam",
    "would fetch them from tape . at its peak",
    "this cache space totaled approximately 1  pb , but this cache space was only available to d0 and would have been too costly to maintain over the life of the data preservation project .",
    "d0 has therefore deployed a 100 tb dcache  @xcite instance for staging input files to worker nodes , as cdf and numerous other fermilab experiments are already doing .",
    "the test results showed no degradation in performance relative to the dedicated sam caches , and the d0 dcache instance has been in production for approximately two years .",
    "as cdf was already using dcache for tape - backed caching , once the necessary code changes were made to use newer versions of sam , data access continued to be possible with no hardware infrastructure changes needed .",
    "preserving the experiments institutional knowledge is a critically important part of the project .",
    "here we define this knowledge to be internal documentation and notes , presentations in meetings , informational web pages and tutorials , meeting agendas , and mailing list archives .",
    "the largest step in this part of the project was transferring each experiment s internal documents to long - term repositories . both cdf and d0",
    "have partnered with inspire  @xcite to transfer their internal notes to experiment - specific accounts on inspire . for internal meeting agendas",
    ", d0 has moved to an indico instance hosted by the fermilab scientific computing division , while cdf has virtualized their mysql - based system .",
    "fermilab will see to it that archives of each experiment s mailing lists are available through the life of the project , and wiki / twiki instances are being moved to static web pages to facilitate ease of movement to new servers if needed in the future .",
    "in addition to moving previous documentation to modern platforms , both experiments have written new documentation specifically detailing the infrastructure changes that the r2dp project has made , along with instructions for adapting legacy workflows to the new systems .",
    "the run ii data preservation project aims to enable full analysis capability for the cdf and d0 experiments through at least the year 2020 .",
    "both experiments have modernized their software environments and job submission procedures in order to be able to run jobs in current operating systems and to take advantage of non - dedicated computing resources .",
    "wherever possible they have adopted elements of the computing infrastructure now in use by the majority of active fermilab experiments .",
    "they have also made significant efforts at preserving institutional knowledge by moving documentation to long - term archives .",
    "materials costs for the project were dominated by media for migrating the data to t10k tapes , while the vast majority of the project s other costs came from salaries .",
    "the implementation phase of the project is complete and both experiments are actively using the r2dp infrastructure for their current and future work .",
    "the authors thank the computing and software teams of both cdf and d0 as well as the staff of the fermilab computing sector that made this project possible .",
    "fermilab is operated by fermi research alliance , llc under contract number de - ac02 - 07ch11359 with the united states department of energy .",
    "the work was supported in part by data and software preservation for open science ( daspos ) ( nsf - phy-1247316 ) .",
    "99 d. acosta _ et al .",
    "_ ,  measurement of the @xmath3 meson and @xmath4-hadron production cross sections in @xmath5 collisions at @xmath6  gev \" , phys . rev .",
    "d 71 , 032001 ( 2005 ) .",
    "a. abulencia _",
    "_ ,  measurements of inclusive w and z cross sections in @xmath5 collisions at @xmath7  tev \" , journal of physics g : nuclear and particle physics , 34 , number 12 ( 2007 ) .",
    "v. m. abazov _",
    "_ ,  the upgraded d0 detector `` , nucl .",
    "methods in phys .",
    "a 565 , 463 ( 2006 ) .",
    "the linear tape open consortium , http://www.lto.org .",
    "oracle , ' ' storagetek t10000 data cartridge family data sheet \" , http://www.oracle.com/us/products/servers-storage/storage/tape-storage/storagetek-t10000-t2-cartridge-296699.pdf .",
    "s. amerio _",
    "et al . _ , `` the long term data preservation ( ltdp ) project at infn cnaf : cdf use case , '' j. phys .",
    "( 2015 ) no.1 , 012012 .",
    "p. buncic _",
    "et al . _ , `` cernvm - a virtual appliance for lhc applications . ''",
    "proceedings of the xii .",
    "international workshop on advanced computing and analysis techniques in physics research ( acat08 ) , erice , 2008 pos(acat08)012 dennis box _",
    "_ , `` progress on the fabric for frontier experiments project at fermilab '' , journal of phyics .",
    ": conference series , 664 062040 ( 2015 ) .",
    "r. a. illingworth , `` a data handling system for modern and future fermilab experiments '' , journal of physics : conference series , 513 , 032045 ( 2014 ) .",
    "et al . _ ,",
    "`` dcache , a distributed data storage caching system '' , in computing in high energy physics 2001 ( chep 2001 ) , beijing , china . ."
  ],
  "abstract_text": [
    "<S> the fermilab tevatron collider s data - taking run ended in september 2011 , yielding a dataset with rich scientific potential . </S>",
    "<S> the cdf and d0 experiments each have approximately 9 pb of collider and simulated data stored on tape . </S>",
    "<S> a large computing infrastructure consisting of tape storage , disk cache , and distributed grid computing for physics analysis with the tevatron data is present at fermilab . </S>",
    "<S> the fermilab run ii data preservation project intends to keep this analysis capability sustained through the year 2020 and beyond . to achieve this goal , </S>",
    "<S> we have implemented a system that utilizes virtualization , automated validation , and migration to new standards in both software and data storage technology and leverages resources available from currently - running experiments at fermilab . </S>",
    "<S> these efforts have also provided useful lessons in ensuring long - term data access for numerous experiments , and enable high - quality scientific output for years to come . </S>"
  ]
}