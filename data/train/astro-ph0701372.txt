{
  "article_text": [
    "understanding the formation and the evolution of early - type galaxies is one of the most important open problems in present - day astrophysics and cosmology . within the standard @xmath3cdm paradigm , massive ellipticals",
    "are predicted to be formed via hierarchical merging of lower mass galaxies @xcite . despite the many theoretical and observational successes of this scenario , several important features of early - type galaxies",
    "are still left unexplained . in particular , the origin of the ( often strikingly tight ) empirical scaling laws that correlate the global properties of ellipticals remain unexplained : ( i ) the fundamental plane @xcite , relating effective radius , velocity dispersion and effective surface brightness ; ( ii ) the @xmath4 @xcite , relating the mass of the central supermassive black hole hosted by the galaxy with its velocity dispersion ; ( iii ) the color-@xmath5 @xcite and the @xmath6 @xcite relating the velocity dispersion with the stellar ages and the chemical composition of the galaxy .",
    "each of these scaling relations relate structural , ( spectro- ) photometric , and dynamical ( i.e.  stellar velocity dispersion ) quantities . whereas the first two are solely based on the observed stellar component , the latter is a function of the stellar _ and _ dark matter mass distribution .",
    "hence , a detailed study of the inner mass profile of early - type galaxies at different redshifts is undoubtedly necessary to properly address the numerous issues related with the formation and the evolution of these objects and their scaling relations , and it would also constitute an excellent test bed for the validity of the @xmath3cdm scenario on small ( i.e.  non linear ) scales .    today , a large number of thorough stellar dynamic and x - ray studies have been conducted to probe the mass structure of nearby ( @xmath7 ) early - type galaxies @xcite , in most ( but not all ) cases finding evidence for the presence of a dark matter halo component and for a flat equivalent rotation curve in the inner regions .    when it comes to distant ( @xmath8 ) early - type galaxies , however , only relatively little is known .",
    "the two main diagnostic tools which can be employed to carry out such studies , namely gravitational lensing @xcite and stellar dynamics @xcite , both suffer from limitations and degeneracies .",
    "gravitational lensing provides an accurate and almost model independent determination of the total mass projected within the einstein radius , but a reliable determination of the mass density profile is often prevented by the effects of the mass sheet @xcite and the related mass profile and shape degeneracies ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "despite these limitations , strong gravitational lensing has provided the first , although heterogeneous , constraints on the internal mass distribution of stellar and dark matter in high - redshift galaxies ( e.g. * ? ? ? * ; * ? ? ? * ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "analyses based on stellar dynamics are limited by the paucity of bright kinematic tracers in the outer regions of early - type galaxies ( e.g. * ? ? ?",
    "moreover , there is a severe degeneracy ( the mass - anisotropy degeneracy ) between the mass profile of the galaxy and the anisotropy of its stellar velocity dispersion tensor ( e.g. * ? ? ? * ) .",
    "higher order velocity moments , which could be used to overcome this difficulty , can not be measured in reasonable integration time for such distant systems with any of the current instruments .",
    "a viable solution to break these degeneracies to a large extent is a joint analysis which combines constraints from both gravitational lensing and stellar dynamics @xcite : the two approaches can be described as almost `` orthogonal '' , in other words they complement each other very effectively ( see e.g. * ? ? ? * for some simple scaling relations ) , allowing a trustworthy determination of the mass density profile of the lens galaxy in the inner regions ( i.e.  within the einstein radius or the effective radius , whichever is larger ) .",
    "a joint gravitational lensing and stellar dynamics study of the lenses structure and dynamics ( lsd ) and sloan lens acs ( slacs ) samples of early - type galaxies @xcite , for example , showed that the total mass density has an average logarithmic slope extremely close to isothermal ( @xmath9 , assuming @xmath10 ) with a very small intrinsic spread of at most 6% .",
    "they also find no evidence for any secular evolution of the inner density slope of early - type galaxies between @xmath11 and @xmath12 @xcite .",
    "these authors , in their combined analysis , make use of both gravitational lensing and stellar dynamics information , but treat the two problems as _",
    "specifically , the projected mass distribution of the lens galaxy is modeled as a singular isothermal ellipsoid ( sie ; e.g. * ? ? ?",
    "* ) , and the obtained value for the total mass within the einstein radius is used as a constraint for the dynamical modeling ; in the latter the galaxy is assumed to be spherical and the stellar orbit anisotropy to be constant ( or follow an osipkov  merritt profile ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) in the inner regions and therefore the spherical jeans equations can be solved in order to determine the slope @xmath13 ( see * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* for a full discussion of the methodology ) .",
    "this approach , however , while being robust and successful , is not fully self - consistent : gravitational lensing and the stellar dynamics use different assumptions about the symmetry of the system and different potentials for the lens galaxy .",
    "this has motivated us to develop a completely rigorous and self - consistent framework to carry out combined gravitational lensing and stellar dynamics studies of early - type galaxies , the topic of the present paper .",
    "the methodology that we introduce , in principle , is completely general and allows  given a set of data from gravitational lensing ( i.e.  the surface brightness distribution of the lensed images ) and stellar dynamics ( i.e.  the surface brightness distribution and the line - of - sight projected velocity moments of the lens galaxy )  the `` best '' parameters , describing the gravitational potential of the galaxy and its stellar phase - space distribution function , to be recovered .    in practice , because of technical and computational limitations , we restrict ourselves to axisymmetric potentials and two - integral stellar phase - space distribution functions ( i.e.  @xmath0 ; see * ? ? ?",
    "* ) , in order to present a fast and efficiently working algorithm .",
    "we introduce a new monte carlo approach to schwarzschild s orbital superposition method , that allows the @xmath0 to be solved in axisymmetric potentials in a matter of seconds .",
    "all of these restrictions , however , should be seen just as one particular implementation of the much broader general framework .",
    "the very core of the methodology lies in the possibility of formulating both the lensed image reconstruction and the dynamical modeling as formally analogous linear problems .",
    "the whole framework is coherently integrated in the context of bayesian statistics , which provides an objective and unambiguous criterion for model comparison , based on the _ evidence _ merit function @xcite .",
    "the bayesian evidence penalty function allows one to both determine the `` best '' ( i.e. the most plausible in an occam s razor sense ) set of non linear parameters for an assigned potential and compare and rank different potential families , as well as set the optimal regularization level in solving the linear equations and find the best point - spread function ( psf ) model , pixel scales , etc .",
    "the paper is organized as follows : in sect .",
    "[ bayes ] we first review some relevant aspects of the theory of bayesian inference , and we elucidate how these apply to the framework of combined lensing and stellar dynamics . in sect .",
    "[ outline ] we present a general overview of the framework , with particular focus on the case of our implementation for axisymmetric potentials . in sects .",
    "[ len ] and  [ dyn ] , respectively , we provide a detailed description of the methods for the lensed image reconstruction and the dynamical modeling . in sect .",
    "[ test ] we describe the testing of the method , showing that it allows a reliable recovery of the correct model potential parameters . finally , conclusions are drawn in sect .",
    "the application of the algorithm to the early - type lens galaxies of the slacs sample ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) will be presented in forthcoming papers .",
    "before describing the code itself in more detail , in this section we first review the basics of bayesian statistics .",
    "this is relevant , because  as will be shown in more detail in the coming sections  both the lensing and stellar dynamics parts of the algorithm can be formalized as linear sets of equations @xmath14 that can be solved within a bayesian statistical framework . in the above equation , @xmath15 represents the data , @xmath16 $ ] is the model , which will in general depend on the physics involved ( e.g. , in our case , the lens - galaxy potential @xmath17 , a function of the non linear parameters @xmath18 ) , @xmath19 are the linear parameters that we want to infer , and @xmath20 is the noise in the data , characterized by the covariance matrix @xmath21 .",
    "first , we aim to determine the linear parameters @xmath19 given the data and a fixed model and , on a more general level , to find the non linear parameters @xmath18 corresponding to the `` best '' model .",
    "note that the choices of grid size , psf , etc .",
    ", are also regarded as being ( discrete ) parameters of the model family ( changing these quantities or assumptions formally is equivalent to adopting a different family of models ) .    both of these problems can quantitatively be addressed within the framework of bayesian statistics . following the approach of mackay ( @xcite @xcite , @xcite , @xcite @xcite ; see also @xcite @xcite ) , it is possible to distinguish between three different levels of inference for the data modeling .    1 .   at the first level of inference , the model @xmath22 is assumed to be true , and the most probable @xmath23 for the linear parameters is obtained by maximizing the posterior probability , given by the expression ( derived from bayes rule ) @xmath24 where @xmath25 is a regularization operator , which formalizes a conscious a priori assumption about the degree of smoothness that we expect to find in the solution ( refer to appendix  [ reg ] for the construction of the regularization matrix ) ; the level of regularization is set by the hyperparameter value @xmath26 . the introduction of some kind of prior ( the term @xmath27 in eq .",
    "[ [ bayes.1 ] ] ) is inescapable , since , due to the presence of noise in the data , the problem from equation  ( [ ax+n = b ] ) is an ill - conditioned linear system and , therefore , can not simply be solved through a direct inversion along the line of @xmath28 ( see for example @xcite @xcite for a clear introductory treatment on this subject ) .",
    "note that besides @xmath25 , any a priori assumption ( psf , pixel scales , etc . ) can be treated and ranked through the evidence .",
    "traditional likelihood methods do not allow a proper quantitative ranking of model families or other assumptions . in eq .",
    "( [ bayes.1 ] ) , the probability @xmath29 is the likelihood term , while the normalization constant @xmath30 is called the evidence and plays a fundamental role in bayesian analysis , because it represents the likelihood term at the higher levels of inference .",
    "2 .   at the second level",
    ", we infer the most probable hyperparameter @xmath31 for the model @xmath22 by maximizing the posterior probability function @xmath32 which is equivalent to maximizing the likelihood @xmath30 ( i.e.  the evidence of the previous level ) if the prior @xmath33 is taken to be flat in logarithm , as is customarily done , ( see e.g. @xcite @xcite and references therein ) . ] , because its scale is not known .",
    "3 .   finally ,",
    "at the third level of inference the models are objectively compared and ranked on the basis of the evidence ( of the previous level ) , @xmath34 where the prior @xmath35 is assumed to be flat .",
    "it has been shown by @xcite that this evidence - based bayesian method for model comparison automatically embodies the principle of occam s razor , i.e.  it penalizes those models which correctly predict the data , but are unnecessarily complex .",
    "hence , it is in some sense analogous to the reduced @xmath36 .    in sections  [ lin.opt ] and  [ nlin.opt ]",
    "we illustrate how this general framework is implemented under reasonable simplifying assumptions and how the maximization of the evidence is done in practice .",
    "if the noise @xmath20 in the data can be modeled as gaussian , it is possible to show @xcite that the posterior probability ( eq .  [ [ bayes.1 ] ] ) can be written as @xmath37 } { \\int \\exp \\left [ -{e_{\\mathcal{p}}}({\\vec{x } } )   \\right ] { { \\rm d}}{\\vec{x } } } , \\ ] ] where @xmath38    in the penalty function from equation  ( [ pen.f ] ) , @xmath39 ( i.e.  half of the @xmath36 value ) is a term proportional to the logarithm of the likelihood , which quantifies the agreement of the model with the data , and the term @xmath40 is the regularizing function , which is taken to have a quadratic form with the minimum in @xmath41 ( see the seminal paper of @xcite  @xcite for the use of the regularization method in solving ill - posed problems ) . the term @xmath42 formalizes the only a priori assumption regarding the smoothness of the solution , and therefore corresponds to the prior term of eq .",
    "( [ bayes.1 ] ) .",
    "the most probable solution @xmath23 is obtained by maximizing the posterior from equation  ( [ posterior ] ) .",
    "the calculation of @xmath43 / \\partial { \\vec{x}}= \\vec{0}$ ] yields the linear set of normal equations @xmath44 which maximizes the posterior ( a solution exists and is unique because of the quadratic and positive definite nature of the matrices ) .",
    "if the solution @xmath23 is unconstrained , the set of equations  ( [ ata.2 ] ) can be effectively and non - iteratively solved using e.g.  a cholesky decomposition technique .",
    "however , in our case the solutions have a very direct physical interpretation , representing the surface brightness distribution of the source in the case of lensing ( e.g.  section  [ len ] ) , or the weighted distribution function in the case of dynamics ( e.g.  section  [ dyn ] ) .",
    "the solutions must therefore be non - negative .",
    "hence , we compute the solution of the constrained set of equations  ( [ ata.2 ] ) using the freely - available l - bfgs - b code , a limited memory and bound constrained implementation of the broyden - fletcher - goldfarb - shanno ( bfgs ) method for solving optimization problems @xcite .      the implementation of the higher levels of inference ( i.e.  the model comparison ) requires an iterative optimization process . at every iteration  @xmath45 , a different set @xmath46 of the non linear parameters is considered , generating a new model @xmath47 $ ] for which the most probable solution @xmath48 for the linear parameters is recovered as described in sect .",
    "[ lin.opt ] , and the associated evidence @xmath49 is calculated ( the explicit expression for the evidence is straightforward to compute but rather cumbersome and is , therefore , given in appendix  [ evidence ] ) .",
    "then a nested loop , corresponding to the second level of inference , is carried out to determine the most probable hyperparameter @xmath31 for this model , by maximization of @xmath50 .",
    "the evidence @xmath51 for the model @xmath52 can now be calculated by marginalizing over the hyperparameters .",
    "assuming that @xmath53 is a @xmath54 function centered on @xmath31 , so that the evidence is obtained simply as @xmath55 ( see * ? ? ?",
    "* ; * ? ? ?",
    "] the different models @xmath52 can now be ranked according to the respective value of the evidence @xmath56 ( this is the third level of inference ) , and the best model is the one which maximizes the evidence .",
    "this procedure is in fact very general , and can be applied to compare models with different types of potentials , regularization matrices , psfs , grid sizes , etc .      in principle , for each @xmath46 one has to determine the corresponding set of most probable hyperparameters @xmath57 by means of a nested optimization loop . in practice , however , the values of the hyperparameters change only slightly when @xmath18 is varied in the optimization process , and therefore it is not necessary to perform a nested loop for @xmath26 at each @xmath18-iteration .",
    "what we do is to start the evidence maximization by iteratively changing @xmath18 , while the hyperparameters are kept fixed at a quite large initial value @xmath58 ( so that the solutions are assured to be smooth and the preliminary exploration of the @xmath18 space is faster ) .",
    "this is followed by a second loop in which the best @xmath18-model found so far is fixed , and we optimize only for @xmath26 .",
    "the alternate loop procedure is iterated until the maximum for the evidence is reached .",
    "our tests show that the hyperparameters generally remain very close to the values found at the second loop .",
    "hence , this approximation ( which is somewhat equivalent to a line - search minimization along  alternatively  the @xmath18-parameters and the hyperparameters ) works satisfactorily , significantly reducing the number of iterations necessary to reach convergence .",
    "the maximization of the evidence is , in general , not an easy task , because obtaining the function value for a given model can be very time consuming and the gradient of the evidence over the non linear parameters @xmath18 is in general not known or too expensive to calculate .",
    "we have therefore tailored a hybrid optimization routine which combines a ( modified ) markov chain monte carlo ( mcmc ) search with a downhill - simplex method ( see e.g. * ? ? ? * for a description of both techniques ) .",
    "the preliminary run of the simplex method ( organized in the loops described above ) is the crucial step , because even when launched from a very skewed and unrealistic starting point @xmath59 , it usually allows one to reach a good local maximum and from there to recover the `` best set '' of parameters , i.e. the @xmath18-set which corresponds to the absolute maximum of the evidence function .",
    "the outcome of this first - order optimization is then used as the starting point for a modified mcmc exploration of the surrounding evidence surface with an acceptance ratio based on the evaluation of the normalized quantity @xmath60 ( where @xmath61 is the evidence of the considered point and @xmath62 is the evidence of the best maximum found so far ; higher maxima are always accepted ) .",
    "when a point is `` accepted '' , a fast simplex routine is launched from that point to determine the evidence value of the local maximum to which it belongs .",
    "if this turns out to be a better ( i.e.  higher ) maximum than the best value found so far , it becomes the new starting point for the mcmc .",
    "in practice , the whole procedure can be sped up by about 1 order of magnitude if some phases of the mcmc exploration and the subsequent local peak climbing are limited to the optimization of the evidence of lensing only ( whose evaluation is much faster than the calculation of the total evidence ) , while the evidence of dynamics provides the decisive criterion to discriminate between the obtained set of local maxima of the lensing evidence surface .    as for the implementation of the downhill - simplex method , we have made use of both the freely available optimization packages",
    "minuit @xcite and appspack , a parallel derivative - free optimization package ( see * ? ? ?",
    "* ; * ? ? ?",
    "* for an exhaustive description of the algorithm ) , receiving similar results",
    ". the effectiveness of this hybrid approach for our class of problems will be demonstrated in sect .",
    "[ test.nlin ] , where a test case is considered .",
    "in this section we describe the general outline of the framework for joint gravitational lensing and stellar dynamics analysis ( see fig .",
    "[ scheme ] for a schematic flow chart of the method ) .",
    "we develop an implementation of this methodology ( the cauldron algorithm ) which applies specifically to axisymmetric potentials and two - integral stellar distribution functions .",
    "this restriction is a good compromise between extremely fast lensing plus dynamical modeling and flexibility .",
    "fully triaxial and three - integral methods ( e.g. * ? ? ?",
    "* ) , although possible within our framework , are not yet justified by the data quality and would make the algorithms much slower .",
    "of course , the true potential might be neither axisymmetric nor even triaxial .",
    "hence , if significative departures from the assumption of axisymmetry are present in the lens galaxy , it will have an effect on the lensing and stellar kinematic reconstructions , which can not be accounted for by these simple models . in that case",
    ", the model needs to be revised correspondingly .",
    "however , if the model fits the data , in a bayesian sense , such that no signicant residuals are left , then a more complex model is not yet warranted by the data .",
    "we emphasize that this does not prove that the galaxy is not triaxial , has three integrals of motion , or is even more complex , but only that the data can not make a statement about it ( see e.g. * ? ? ?",
    "* for an illustrative discussion on this topic ) .",
    "the technical details and a more exhaustive explanation of the terminology are given in the following sections and in the appendices .",
    "first , consider a set of observational data for an elliptical lens galaxy consisting of ( 1 ) the surface brightness distribution of the lensed images ( which we will refer to as the _ lensing data _",
    "@xmath63 ) and ( 2 ) the surface brightness distribution and the first and second velocity moments map of the lens galaxy itself ( hereafter the _ kinematic data _",
    "@xmath64 ) .",
    "it is also assumed that the redshift of the source ( @xmath65 ) and of the lens ( @xmath66 ) are known .",
    "second , we choose a family of gravitational potentials @xmath67 which we expect to provide a reasonably good description of the true potential of elliptical galaxies for some set of model parameters @xmath18 , such as the normalization constant , core radius , oblateness , slope , etc",
    ". could even be the potential values on a grid of @xmath68 and , hence , be grid - based itself . ]",
    "the vector @xmath18 can also include other `` non intrinsic '' quantities , such as the inclination angle , the position angle and the coordinates of the center of the elliptical galaxy .    in order to understand the physical characteristics of the lens galaxy ,",
    "the first problem consists of finding the specific values of parameters @xmath18 that optimize some penalty function based on the mismatch between the data and the model , within the solution space allowed by the chosen potential model .",
    "a more general and difficult , but also much more interesting problem , is an objective and quantitative ranking of a set of different families of potentials @xmath69 .",
    "our method is designed to address both of these questions ; given the data and a choice for the potential , the algorithm yields the most likely solution of linear parameters and a value for bayesian evidence @xmath70 .",
    "the maximum evidence solution @xmath71 allows direct and objective model family comparison .",
    "the model family can also include different choices of pixel scales , regularization , psf models , etc .",
    "a comparison based on the evidence , as was described in section  [ bayes ] , overcomes many of the traditional hurdles in regularization and objective model comparison ( see also e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* for a thorough discussion of the bayesian framework in the context of non parametric lensing ) and is , therefore , extremely powerful .      whereas the lensed - image reconstruction is general and",
    "can straightforwardly be applied to an arbitrary potential , in the case of the dynamical modeling we describe how it can be coherently integrated into the general framework , but then focus on the implementation for axisymmetric potentials and two - integral distribution functions .",
    "the algorithm requires a set of observables @xmath63 and @xmath64 , and the choice of a parametric model @xmath72 .",
    "an initial set @xmath46 ( with @xmath73 ) for the non linear parameters is selected .",
    "we indicate the corresponding potential as @xmath74 .",
    "this potential is used for both the lensed - image reconstruction and the dynamical modeling , so that the method is completely self - consistent and makes full use of all the data constraints .",
    "the basic idea for the lensed - image reconstruction is the following .",
    "( 1 ) one describes the source brightness distribution by a set of ( possibly irregularly spaced ) pixels in the source plane @xmath75 with each pixel value representing the source surface brightness at the location of the pixel .",
    "( 2 ) a matrix operator  @xmath76 is then constructed for a given lens potential , which multiplied by a given @xmath75 ( and blurred by the psf ) represents the observed lensed image ( see e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) .    in practice ,",
    "the three - dimensional potential @xmath77 is integrated along the line of sight @xmath78 to obtain the projected potential @xmath79 .",
    "the deflection angle is then determined from @xmath79 . the particular grid - based source reconstruction method introduced in @xcite and @xcite",
    "then allows one to quickly construct the lensing operator @xmath80 and to determine the most probable pixelized source @xmath81 by maximizing the posterior probability ( section  [ lin.opt ] ) .",
    "more discussion of the lensing routine is presented in section  [ len ] .      to construct the axisymmetric dynamical model ( and the corresponding operator @xmath82 ,",
    "see sect .",
    "[ linear.optim ] ) that reproduces the kinematic data set , a schwarzschild method @xcite is used . within this flexible and powerful framework ,",
    "a library of stellar orbits is integrated in an arbitrary potential @xmath72 and the specific weighted superposition of orbits is determined which best reproduces the observed surface brightness distribution and line - of - sight velocity moments of the galaxy ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ) .    in the case of the cauldron algorithm for axisymmetric potentials",
    "@xmath83 , we use the two - integral schwarzschild method developed by @xcite and @xcite .",
    "in contrast to the `` classical '' schwarzschild method , here the building blocks for the galaxy are constituted not by orbits , but by `` two - integral components '' ( tics ) , derived from dirac @xmath54 two - integral distribution functions ( i.e.  as functions of the orbital energy @xmath1 and the axial component of the angular momentum @xmath84 ) .",
    "a tic can be thought of as an elementary building block of toroidal shape , characterized by a combination of orbits that produces a @xmath85 radial density distribution and very simple ( analytically calculable ) velocity moments . for any tic the projected observables ( e.g.  in our case surface brightness and line - of - sight first and second velocity moments )",
    "can then be straightforwardly calculated given  @xmath77 .",
    "the projected axisymmetric density distribution and velocity moments can be obtained as a weighted superposition of tics @xcite .",
    "the aim of the dynamical modeling is therefore to recover a set of weights which describe how the stellar orbits ( or , in the case of the axisymmetric implementation , the tic observables ) are linearly superposed to match the data . in analogy to the lensing case , this is done by maximizing the posterior probability ( sect .  [ lin.opt ] ) . for a more extended description of the dynamics routine and the generation of the tics , we refer to section  [ dyn ] and appendix  [ mcmc ] .",
    "a consequence of the previous considerations and an important feature of the algorithm is that both the gravitational lensing source reconstruction and the dynamical modeling can be expressed in a formally analogous way as sets of coupled linear equations of the form    @xmath86    the `` lensing operator '' @xmath76 encodes how the source surface brightness distribution @xmath87 is mapped on to the observed image @xmath63 .",
    "each of the @xmath88 columns of @xmath76 describe how a point source localized on the corresponding pixel is lensed and possibly multiple imaged on the image plane grid .",
    "similarly , the `` dynamical operator '' @xmath82 contains along its @xmath89 columns all the information about the observables generated by each individual orbit or tic ( i.e.  the surface brightness and the weighted line - of - sight velocity moments , written in pixelized form on the same grid as the data ) , which are superposed with weights @xmath90 to generate the data set @xmath64 .",
    "the crucial advantage of this formulation lies in the fact that each element of eq .",
    "( [ ax = b ] ) is a linear system of equations which can be solved in a fast and non iterative way , using the standard linear optimization techniques . because both @xmath76 and @xmath82 are built from the same lens potential @xmath91 , both sets of equations are coupled through the non linear parameters @xmath92 .",
    "because of the ill - posed nature of the set of equations  ( [ ax = b ] ) ( i.e.  the data are noisy ) , as was discussed in section  [ lin.opt ] , finding the solution for @xmath75 and @xmath93 that maximimize the posterior probability translates into solving a set of ( regularized ) linear equations @xmath94 where @xmath95 and @xmath96 are the covariance matrices for lensing and dynamics , respectively , @xmath97 and @xmath98 are a choice for the regularization matrix , and @xmath99 and @xmath100 are the corresponding regularization hyperparameters .",
    "note that , for @xmath101 ( i.e. in absence of regularization ) , the solution of eqs .",
    "( [ ata ] ) is equivalent to the maximum likelihood solution in the case of gaussian errors .",
    "once @xmath76 and @xmath82 have been constructed , from eqs .",
    "( [ ata ] ) we can derive the solutions @xmath81 and @xmath102 , relative to the choice of the non linear parameters @xmath46 and of the hyperparameters  @xmath103 .",
    "this , however , represents just a single possible model , namely , @xmath104 , belonging to one family of potentials @xmath105 and , in general , will not be the `` best '' model given the data @xmath63 and @xmath64 .      in the framework of the bayesian statistics ,",
    "the `` plausibility '' for each considered model can be objectively quantified through the evidence ( see section  [ bayes ] ) , a merit function which includes ( but is not limited to ) a likelihood penalty function , but can also take into account the effects of the choice of regularization , grid , psf , etc .",
    "the set of non linear parameters  @xmath106 , and the corresponding best model @xmath107 , is obtained by maximizing the evidence through an iterative optimization loop .",
    "for each cycle @xmath45 , a different set of non linear parameters @xmath46 is chosen , and the most probable solutions @xmath108 and @xmath109 are found as described before . the evidence @xmath56 which characterizes this model is then calculated to allow an objective comparison between different models .",
    "once the evidence is maximized , we are left with the best model @xmath110 ( characterized by the value @xmath111 for the evidence ) and the best reconstruction for the source brightness distribution @xmath112 and the tic weights @xmath113 ( which is one - to - one related to the distribution function ) .",
    "at this point , the algorithm has provided us with the best set of non linear parameters @xmath106 , i.e.  the unknowns that we are most interested in .",
    "nevertheless , we might wish to tackle the more general problem of model family comparison , by considering a different family of potentials @xmath114 and applying again the full algorithm to it .",
    "the result will be a vector @xmath115 and a value for the evidence @xmath116 that we can directly compare to the value @xmath111 that we had previously found , determining in this way whether the potential @xmath17 or @xmath114 represents a better model given the constraints .",
    "we now proceed to describe the lensed - image reconstruction and the dynamical modeling routines in much greater detail .",
    "the reader less interested in the technical details and more in the application of the algorithm , can continue with sect .",
    "[ test ] without loss of continuity .",
    "gravitational lensing can be formulated as the reconstruction of an unknown source brightness distribution @xmath87 ( pixelized on a grid composed of @xmath88 pixels ) given the observed and psf - convoluted image brightness distribution @xmath63 ( sampled on a grid of dimension @xmath117 ) . to tackle this problem we have made use of the implementation of the method of non - parametric source reconstruction initially developed by @xcite and further refined and/or adapted by @xcite    each pixel @xmath45 ( with @xmath118 ) on the image grid , located at position @xmath19 , is cast back to the source plane ( at position @xmath119 ) through the lensing equation @xmath120 where @xmath121 is the deflection angle , calculated from the gravitational potential @xmath17 as described in appendix  [ defl.angle ] . since gravitational lensing conserves the surface brightness @xmath122 , the equivalence @xmath123 holds ( if the effect of the psf is neglected ) .",
    "in general , however , @xmath119 will not exactly correspond to the position of a pixel of the fixed source grid .",
    "therefore @xmath124 is expressed as a weighted linear superposition of the surface brightness values at the four pixels @xmath125 ( where the index @xmath126 runs in the interval @xmath127 ) which delimit the position @xmath119 ( see * ? ? ?",
    "the weights @xmath128 for each of the source pixels are the bilinear interpolation weights ( whose sum adds to unity to conserve flux ) , and they are stored as the elements @xmath129 of a ( very ) sparse matrix @xmath76 of dimension @xmath130 , which represents the lensing operator . if the image pixel @xmath45 is cast outside the borders of the source grid , all the elements of the @xmath45-th row of @xmath76 are put to zero . in case",
    "we need to be more accurate , we can split each image grid pixel into @xmath131 subpixels and apply the same procedure as before to construct an @xmath132-factor oversampled lensing matrix of dimension @xmath133 .",
    "the lensing operator is therefore a non linear function of the parameter vector @xmath18 through the potential , i.e. @xmath134 , and must be constructed at each iteration of the cauldron algorithm . from @xmath76",
    "we then construct the _ blurred _ lensing matrix @xmath135 , where @xmath136 is a blurring operator ( this is a square matrix of order equal to the number of rows of  @xmath76 ) which accounts for the effects of the psf - th row of the matrix @xmath136 contains a discretized description of how the psf is going to spread the surface brightness at the @xmath45-th pixel of the image @xmath63 over the surrounding grid pixels ( see discussion in * ? ? ?",
    "if we are dealing with an oversampled lensing matrix , we need to include also a resampling operator @xmath137 ( of dimension @xmath138 ) that sums the oversampled pixels together so that in the end the blurred lensing matrix is defined as @xmath139 .    within this framework",
    ", the mapping of the source into the lensed image can be expressed as the set of linear equations ( cfr .",
    "[ [ ax = b ] ] ) @xmath140 as discussed in section  [ lin.opt ] , for gaussian errors the solution @xmath141 of the ill - conditioned linear system from equation  ( [ ls = d ] ) is found by minimizing the quadratic penalty function @xmath142 =      \\frac{1}{2 } { ( { \\mathbf{m}}{\\vec{s}}- { \\vec{d}})}^{\\rm t } { { \\mathbf{c}}^{-1}_{\\rm l}}({\\mathbf{m}}{\\vec{s}}- { \\vec{d } } ) + \\frac{{\\lambda_{\\rm len}}}{2 }      { || { \\mathbf{h}}{\\vec{s}}||}^{2}\\ ] ] ( cf .",
    ".  [ [ pen.f ] ] ) by varying @xmath87 and finding @xmath143 . here",
    "@xmath144 is the ( diagonal ) lensing covariance matrix , @xmath25 is the lensing regularization matrix , and @xmath145 is the corresponding regularization hyperparameter .",
    "this problem translates ( see again sect .  [ lin.opt ] ) into solving the set of linear equations @xmath146 ( cf .",
    "eq .  [ [ ata.2 ] ] ) .",
    "although in eqs .",
    "( [ pen.f.len ] ) and  ( [ ltl ] ) we have indicated the regularization matrix simply as @xmath25 for the sake of clarity , it should be noted that , since @xmath87 represents a two - dimensional grid , it is necessary in practice to consider regularization both in the @xmath147- and @xmath148-directions , as described , respectively , by matrices @xmath149 and @xmath150 .",
    "therefore , the regularization term in eq .",
    "( [ ltl ] ) becomes @xmath151 , where @xmath152 is the ratio between the horizontal and vertical pixel scales in the case of `` curvature regularization '' ( see * ? ? ?",
    "* for a discussion of different forms of regularization ) .",
    "for a specific description of how the actual regularization matrices are constructed , we refer to appendix  [ reg ] .",
    "in this section we describe the details of the fast two - integral schwarzschild method for the dynamical modeling of axisymmetric systems , which is implemented in the cauldron algorithm . it should be noted , however , that eqs .",
    "( [ qg = p])-([qtq ] ) are also valid in the general case of arbitrary potentials , provided that @xmath90 is interpreted as the vector of the weights of the different ( numerically integrated ) stellar orbits of some orbit library .",
    "however , the actual construction of the `` dynamical operator ''  @xmath82 , as described in sect .  [ q ] , is specific to this particular implementation .    as already shown in sect .",
    "[ algorithm ] , from a formal point of view the problem of dynamical modeling is identical to that of lensing ( sect .",
    "[ len ] ) , essentially consisting of finding the most probable solution @xmath153 for the ill - constrained linear system @xmath154 ( cf .",
    "eqs.[[ls = d ] ] and  [ [ ax = b ] ] ) . here",
    "@xmath82 is the dynamical operator which is applied to the vector @xmath90 of the weights of the building - block @xmath54 distribution functions @xmath155 ( the tics ) to generate the set of observables @xmath64 .",
    "see sect .",
    "[ q ] for a more in - depth description of the meaning of the mentioned quantities and the construction of the matrix @xmath82 . as described in section  [ lin.opt ] , one derives the solution @xmath153 by minimizing the quadratic penalty function @xmath156      & = &     \\frac{1}{2 } { ( { \\mathbf{q}}{\\vec{\\gamma}}- { \\vec{p}})}^{\\rm t } { { \\mathbf{c}}^{-1}_{\\rm d}}({\\mathbf{q}}{\\vec{\\gamma}}- { \\vec{p } } ) + \\nonumber\\\\     & &     \\frac{1}{2 } \\left (      { \\lambda^{\\rm dyn}_{l}}{|| { \\mathbf{k}_{l}}{\\vec{\\gamma}}||}^{2 } +     { \\lambda^{\\rm dyn}_{e}}{|| { \\mathbf{k}_{e}}{\\vec{\\gamma}}||}^{2 }     \\right ) , \\end{aligned}\\ ] ] which corresponds to solving the set of linear equations @xmath157 ( note the equivalence with eqs .",
    "( [ pen.f.len ] ) and  ( [ ltl ] ) for lensing ) .",
    "here we have indicated the ( diagonal ) dynamics covariance matrix as @xmath158 and the regularization matrices along the @xmath1- and @xmath159-axes of the @xmath90-grid as @xmath160 and @xmath161 , respectively .",
    "the regularization along these two directions are ( assumed to be ) uncorrelated , and the corresponding hyperparameters @xmath162 and @xmath163 must therefore be considered independently .    with respect to their physical meaning",
    ", however , the construction of the linear operators @xmath76 and @xmath82 is a markedly distinct problem : the lensing operator describes the distortion to which the source surface brightness distribution is subjected , while the dynamics operator is effectively a library which stores the projected observables ( surface brightness distribution and unweighted velocity and velocity dispersion maps ) associated with the set of elementary ( i.e. dirac @xmath54 ) two - integral distribution functions .      in this section",
    "we describe how to construct the dynamics operator @xmath82 ( which is the most complex part of implementing eq .  [ [ qtq ] ] explicitly ) , introducing a new and extremely fast monte carlo implementation of the two - integral components schwarzschild method , as proposed by @xcite and @xcite .",
    "the schwarzschild method is a powerful and flexible numerical method to construct numerical galaxy models without having to make any prior assumptions regarding the shape , the symmetry , the anisotropy of the system , etc .    in its original implementation @xcite ,",
    "the procedure works as follows . an arbitrary density distribution ( possibly motivated by observations )",
    "is chosen for the galaxy and the corresponding potential is computed by means of the poisson equation .",
    "one then calculates a library of stellar orbits within this potential and finds the specific superposition of orbits which reproduces the initial density distribution .",
    "the method can be generalized to also treat cases in which density and potential are not a self - consistent pair and to include kinematic constraints ( see e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "orbits , however , are not the _ only _ possibility for the building blocks of a schwarzschild method if one only aims to construct two - integral axisymmetric models for galaxies . given an axisymmetric potential @xmath164 , one can also consider more abstract constituents called two - integral components or tics ( see * ? ? ? * ; * ? ? ? * ) which correspond to elementary dirac @xmath54 distribution functions completely specified by a choice of energy @xmath165 and angular momentum @xmath166 , @xmath167 where @xmath168}$ ] is a normalization coefficient chosen such that all the tics have equal mass ( see appendix  [ normalization ] for an explicit expression for @xmath169 ) .",
    "the zero - velocity curve ( zvc ) is the curve in the meridional plane @xmath68 for which @xmath170 where @xmath1 is the relative energy and @xmath171 is the relative potential ; another frequently useful quantity is the effective potential @xmath172 .",
    "a tic - based schwarzschild method has two main advantages .",
    "first , the @xmath126-th tic can be interpreted as a particular combination of all orbits ( both regular and irregular , see @xcite  @xcite ) with energy @xmath165 and angular momentum @xmath166 which can be integrated in the potential @xmath164 and completely fill the zvc .",
    "therefore , the tics constitute a family of building blocks smoother than the regular orbits ( which may have sharp edges ) and automatically take into account the irregular orbits .",
    "second , the unprojected density and velocity moments for the tics have simple analytic expressions , which makes this method much faster than the ordinary orbit integration .    from the definition from equation  ( [ tic ] )",
    ", the density distribution in the meridional plane generated by the @xmath126-th tic is given by ( see * ? ? ?",
    "* ) @xmath173 while the only non - zero velocity moments have the following expressions inside the zvc : @xmath174 @xmath175 @xmath176 , \\ ] ] and they vanish elsewhere .",
    "note that we can not directly compare the quantities described by eqs .",
    "( [ tic.rho])-([tic.vz ] ) with the observations . before",
    "this can be done , we need to calculate the projected quantities , grid them , and convolve them with the appropriate psf ( and possibly regrid them again in the case of subgridding ; see text ) .    the surface brightness @xmath177 ( sampled on a grid of @xmath178 elements ) and the weighted line - of - sight velocity moments @xmath179 and",
    "@xmath180 ( both sampled on a grid of @xmath181 elements ) can be obtained semi - analytically through multi - dimensional integrals ( refer to * ? ? ?",
    "because the same semi - analytic approach to calculate @xmath82 is rather time consuming , we have developed a very efficient monte carlo implementation of the two - integral schwarzschild method , detailed in appendix  [ mcmc ] , which is several orders of magnitude faster .",
    "whatever technique is adopted , the projected and psf - convoluted quantities for the @xmath126-th tic , sampled on the appropriate grids , constitute the @xmath126-th column of the @xmath82 matrix . therefore ,",
    "if the galaxy model is built using a library of @xmath182 tic building blocks ( where @xmath183 and @xmath184 are the number of samplings , respectively , in energy and angular momentum ) , the dynamics operator @xmath82 turns out to be a dense matrix of dimensions @xmath185 .",
    "the meaning of the vector @xmath90 in eq .",
    "( [ qg = p ] ) is now clear : its @xmath89 dimensionless non negative elements @xmath186 describe the weights of the linear superposition of the model observables generated by the library of tics , and we look for the solution @xmath187 ( given by eq .  [",
    "[ qtq ] ] ) which best reproduces the data set @xmath64 .",
    "moreover , as explained in appendix  [ normalization ] , the weights @xmath186 are proportional to the light contributed by the tic to the galaxy and related to the reconstructed dimensional distribution function @xmath188 when they are normalized with the tic area @xmath189 in the meridional plane and the surface of the cell @xmath190}}$ ] in integral space .",
    "in this section we describe several of the tests that we have performed to show the proper functioning of the method .",
    "we construct a simulated data set ( including realistic noise ) for both lensing and dynamics by adopting the potential given in eq .",
    "( [ evans.pot ] ) @xcite and a particular choice for the non linear parameters @xmath18 ( see sect .",
    "[ test.setup ] for a description of the setup ) . then in sect .",
    "[ test.lin ] , keeping the non linear parameters fixed , we test the linear optimization part of the method by showing that the code is able to faithfully reconstruct the source surface brightness distribution and the distribution function used to generate the mock data . finally , in sect .",
    "[ test.nlin ] we execute a full run of the code .",
    "we use the mock set of simulated data as input and adopt a starting set of non linear parameters considerably skewed from the `` true '' values , in order to verify how reliably the linear and non linear parameters are recovered . note that , for conciseness , what we refer to as the `` evidence '' @xmath191 is , rigorously speaking , the logarithm of the evidence as presented in sect .",
    "[ bayes ] .",
    "see appendix  [ evidence ] for its precise definition .",
    "as mentioned , for testing purposes it is convenient to make use of the evans power - law potentials @xcite @xmath192 where @xmath193 is the central potential , @xmath194 is a core radius and @xmath195 is the axis ratio of the equipotentials . for @xmath196 this becomes the @xcite logarithmic potential .",
    "what makes this class of axisymmetric galaxy models suitable for the setup of a test case , is the overall simplicity of their properties .",
    "the power - law galaxy models are characterized by elementary two - integral distribution functions and provide fully analytical expressions for the density distribution , associated with the potential via the poisson equation , the second velocity moments and the deflection angle ( see appendix  [ evans ] ) . moreover ,",
    "even the projected quantities ( i.e.  the projected surface density and the line - of - sight second velocity moment ) are analytical .",
    "the mean stellar streaming velocity @xmath197 is assigned through a physically motivated prescription ( see sect .  2.3 of * ? ? ?",
    "* ) which leads to a simple odd part for the distribution function , which does not contribute to the density and the second velocity moment .    with the choice of the potential from equation  ( [ evans.pot ] ) for the lens galaxy , the elements of the @xmath18 vector of non linear parameters are @xmath198 , @xmath195 , @xmath194 and @xmath193 ( or equivalently , through eq .",
    "[ [ lens.str ] ] , the lens strength @xmath199 ) .",
    "in addition , @xmath18 includes the parameters that determine the observed geometry of the system : the inclination  @xmath45 , the position angle  @xmath200 , and the coordinates  @xmath201 of the center of lens galaxy on the sky grid .",
    "we note that @xmath18 can also include the external shear strength and position angle , although in our current tests we assume negligible shear for the sake of simplicity and to fully concentrate on the parameters of the lens galaxy only , in order to test whether their degeneracies can be broken by the combination of lensing and stellar kinematic data .",
    "we also need to make a choice for the size of the grids in pixels .",
    "this actually constitutes an explicit prior choice ( just like the type of regularization ) .",
    "the evidence however can be used to test exactly these types of assumptions .",
    "explicitly , our test setup is the following .    * _ lensing : _ for the test setup , we adopt a @xmath202 pixel grid ( @xmath203 ) in the source plane and a @xmath204 pixel grid ( @xmath205 ) in the image plane .",
    "the lensing operator is built using an oversampling factor of  @xmath206 to improve the quality of the reconstruction . *",
    "_ dynamics : _ in the two - integral phase space we consider a grid of @xmath207 elements logarithmically sampled in @xmath208 and @xmath209 elements linearly sampled in angular momentum , i.e.  a total of @xmath210 tics ( note that the grid must be mirrored for negative @xmath84 ) .",
    "each tic is populated with @xmath211 particles .",
    "this number of tics ( when compared to the grid size for the lensing source for example ) has been verified to be a good compromise between the quality of the distribution function reconstruction and the heavy computational power needed in the iterative scheme for the construction of many tics .",
    "the surface brightness and the line - of - sight velocity moments are sampled on different grids of , respectively , @xmath212 and @xmath213 elements ( @xmath214 and @xmath215 ) .",
    "analogous to the case of lensing , an oversampling factor of  @xmath206 is adopted in the construction of the operator  @xmath82 .",
    "we select and fix the following arbitrary ( albeit physically plausible ) set of values for the  @xmath18 vector : @xmath216 , @xmath217 , @xmath218 , @xmath219 ( and the ratio , @xmath220 and @xmath221 the angular diameter distance of , respectively , observer - source , observer - lens and lens - source . ] @xmath222 is taken to be @xmath223 ) , @xmath224 , @xmath225 and @xmath226 .",
    "this makes it possible to construct the lensing operator , which is the _ blurred _ lensing operator , it is necessary to have the blurring operator @xmath136 , which we construct from the @xmath227 pixel psf model . ]",
    "@xmath228 and the dynamics operator  @xmath82 ( sects .",
    "[ len ] and  [ dyn ] ) , which are then used to generate a simulated data set . with a 3 ghz machine ,",
    "the construction of the sparse lensing matrix is a very fast process ( less than 1 s ) .",
    "constructing the dynamics operator is more time consuming , although requiring in the above case still only about 7  s , i.e.  of the order of 100  ms per tic ( it should be noted that this is a very short time for building a dynamical library and is a direct consequence of the monte carlo implementation described in appendix  [ mcmc ] . ) in addition :    * _ lensing : _ we adopt an elliptical gaussian source surface brightness distribution @xmath229 and using the evans potential from equation  ( [ evans.pot ] ) ( see eq .  [ [ evans.alpha ] ] for the analytic expression of the corresponding deflection angle ) we generate the blurred lensed image . from this",
    "we obtain the final mock image @xmath230 by adding a gaussian noise distribution with @xmath231  times the image peak value .",
    "this is illustrated in the first column of fig .",
    "[ fig.len.reclin ] .",
    "+ the reconstruction of the non - negative source @xmath232 , from the simulated data @xmath230 , is obtained by solving the linear system of equations  ( [ ltl ] ) , with the adoption of a fiducial value for the regularization hyperparameter ( @xmath233 ) .",
    "although the matrix @xmath234 in eq .",
    "( [ ltl ] ) is large ( @xmath235 ) , it is very sparse and therefore , using l - bfgs - b , it only takes @xmath236 s to find the solution . the result is shown in fig .  [ fig.len.reclin ] ( _ middle column _ ) together with the residuals ( _ right column _ ) .",
    "* _ dynamics : _ we generate the simulated data set @xmath237 for a self - consistent dynamical system described by the distribution function of the evans power - law potential .",
    "we adopt the same parameters used for the above potential .",
    "this kind of assumption , in general , is not required by the method .",
    "however , we adopt it here because it immediately and unambiguously allows us to check the correctness of the simulated data in comparison to the analytic expressions . in this way , it is possible to verify that the considered tic library @xmath238 , although composed of only @xmath239 elements , indeed represents a fair approximation of the true distribution function .",
    "+ the first two panels in the first row of fig .",
    "[ fig.dyn.reclin ] show the tic weights @xmath238 ( i.e.  the weighted distribution function ) sampled over the two - integral space grid .",
    "the remaining panels display the simulated data : the surface brightness distribution , the line - of - sight streaming velocity and the line - of - sight velocity dispersion , the calculated quantity is not the line - of - sight velocity dispersion @xmath240 , but instead @xmath241 , which is additive and can therefore be directly summed up when the tics are generated and superposed . the line - of - sight velocity dispersion to compare with the observation",
    ", however , can be simply obtained in the end as : @xmath242 ] as can be seen , non - uniform gaussian noise has been added to the simulated data ( its full characterization is taken into account in the covariance matrix @xmath158 ) .",
    "the noise on the surface brightness is a few percent of the value of this quantity in the inner region , while , in order to make the test case more realistic , the noise on the velocity moments is increasing outwards and becomes quite severe in the external regions .",
    "+ the reconstruction of the non negative tic weights @xmath243 is given by the solution of eq .",
    "( [ qtq ] ) ( the chosen values for the hyperparameters are @xmath244 and @xmath245 ) .",
    "the reconstruction of the dynamical model constitutes , together with the generation of the tic library , the most time consuming part of the algorithm , requiring almost @xmath246 s. this is a consequence of the fact that the @xmath247 dynamics operator @xmath82 , although much smaller than @xmath76 , is a fully dense matrix . if the number of tics is significantly increased , the time required for calculating the term @xmath248 in eq .",
    "( [ qtq ] ) and to solve that set of linear equations with the l - bfgs - b method increases very rapidly ( as does the time needed to generate the tics , although less steeply ) , and therefore the dynamical modeling is typically the bottleneck for the efficiency of the method .",
    "+ the results of the reconstruction are shown in the second row of fig .",
    "[ fig.dyn.reclin ] ( whereas the third row shows the residuals ) .",
    "the reconstruction is generally very accurate .",
    "having verified the soundness of the linear reconstruction algorithms , the next step is to test how reliably the method is able to recover the `` true '' values of the non linear parameters @xmath18 from the simulated data , through the maximization of the evidence penalty function @xmath70 .",
    "we first run the linear reconstruction for the reference model @xmath249 described in sect .",
    "[ test.lin ] , optimized for the hyper - parameters , to determine the value of the total evidence @xmath250 ( reported in the first column of table  [ table.evid ] ) .",
    "since this is by definition the `` true '' model , it is expected ( provided that the priors , i.e.  grids and form of regularization , are not changed ) that every other model will have a smaller value for the evidence .",
    "c c c c c c & & @xmath249 & @xmath251 & @xmath252 & @xmath253 + & @xmath254}$ ] & 60.0 & 25.0 & 75.6 & 62.6 + non linear & @xmath199 & 4.05 & 5.60 & 3.86 & 3.99 + parameters & @xmath198 & 0.280 & 0.390 & 0.288 & 0.285 + & @xmath195 & 0.850 & 0.660 & 0.880 & 0.857 + & @xmath255 & -1.04 & 0.00 & -1.04 & -1.04 + hyper- & @xmath256 & 8.00 & 12.0 & 7.99 & 8.00 + parameters & @xmath257 & 9.39 & 12.0 & 9.55 & 9.39 + & @xmath258 & -22663 & -58151 & -22668 & -22661 + evidence & @xmath259 & 12738 & -156948 & 12672 & 12856 + & @xmath260 & -9925 & -215099 & -9996 & -9805 +    [ table.evid ]    the reference model in the second column is the `` true '' model which generated the simulated data sets , and is shown for comparison .",
    "the first group of rows list the non linear parameters @xmath18 which are varied in the loop .",
    "the second group shows the hyperparameters .",
    "the last group shows the evidence relative to the considered model ; the contributions of the lensing and dynamics part to the total evidence are also indicated .",
    "second , we construct a `` wrong '' starting model @xmath261 by significantly skewing the values , indicated in the second column of table  [ table.evid ] , of the four non linear parameters : the inclination  @xmath45 , the lens strength  @xmath199 , the slope  @xmath198 and the axis ratio  @xmath195 .",
    "figures  [ fig.len.opt ] and  [ fig.dyn.opt ] show that @xmath251 is clearly not a good model for the data in hand .",
    "we do not set boundaries on the values that the non linear parameters can assume during the iterative search , except for those which come from physical or geometrical considerations ( i.e. inclination comprised between edge - on and face - on , @xmath262 , @xmath263 , @xmath264 ) .",
    "the position angle , in general , is reasonably well - constrained observationally , and therefore including it with tight constraints on the interval of admitted values would only slow down the non linear optimization process without having a relevant impact on the overall analysis .",
    "it is therefore kept fixed during the optimization .",
    "we also keep the core radius of the mass distribution fixed even though in general it is not well constrained by the surface brightness distribution and in real applications it should be varied .",
    "it always remains possible , once the best model has been found , to optimize for the remaining parameters in case a second - order tuning is required .",
    "adopting  @xmath59 as the starting point for the exploration , the non linear optimization routine for the evidence maximization is run as described in sect .",
    "[ nlin.opt ] .",
    "the model @xmath252 ( third column of table  [ table.evid ] ) is what is recovered after three major loops ( the first and the last one for the optimization of the varying non linear parameters , the intermediate one for the hyperparameters ) of the preliminary downhill - simplex optimization , for a total of @xmath265 iterations , requiring about  @xmath266 s on a 3 ghz machine . from this intermediate stage , the final model @xmath253 is obtained through the combined mcmc+downhill - simplex optimization routine , in a time of the order of @xmath267 hours .",
    "further testing has shown that increasing the number of loops does not produce relevant changes in the determination of the non linear parameters nor of the hyperparameters , and therefore extra loops are in general not necessary .",
    "we also note that , in all the considered cases , the first loop is the crucial one for the determination of @xmath18 , and the one which requires the largest number of iterations before it converges .",
    "it is generally convenient to set the initial hyperparameters to high values so that the system is slightly over - regularized .",
    "this has the effect of smoothing the evidence surface in the @xmath18-space , and , therefore , facilitates the search for the maximum .",
    "the successive loops will then take care of tuning down the regularization parameters to more plausible values .",
    "a comparison of the retrieved non linear parameters for @xmath268 ( last column of table  [ table.evid ] ) with the corresponding ones for the reference model reveals that all of them are very reliably recovered within a few percent ( the most skewed one is the inclination  @xmath45 , being only @xmath269 different from the `` true '' value ) .",
    "the panels in the second and fourth rows of figures  [ fig.len.opt ] and  [ fig.dyn.opt ] clearly show that the two models indeed look extremely similar , to the point that they hardly exhibit any difference when visually examined .    the evidence of the final model , when compared with the value for the reference model , turns out to be higher by @xmath270 , which might look surprising since @xmath249 is by construction the `` true '' model .",
    "the explanation for this is given by the cumulative effects of numerical error ( which enters mainly in the way the tics are generated times the number of particles , i.e. @xmath271 , the evidence increases for both @xmath249 and @xmath253 , and the reference model is now favored ( of a @xmath272 ) as expected .",
    "obviously , this comes at a significant cost in computational time , with the dynamical modeling becoming approximately an order of magnitude slower . ] ) , finite grid size , and noise ( in particular on the velocity moments ) , which all contribute to slightly shift the model .",
    "such a shift is , however , well within the model uncertainties ( sect .",
    "[ error.analysis ] ) and therefore not significant .",
    "extensive mcmc exploration reveals that the lensing evidence surface @xmath273 is characterized by multiple local maxima which effectively are degenerate even for very different values of @xmath18 .",
    "indeed , for the potential eq .",
    "( [ evans.pot ] ) , one can easily observe from eq .",
    "( [ evans.alpha ] ) that a relation between @xmath195 , @xmath45 , and @xmath274 exists that leaves the deflection angles and , hence , the lens observable invariant .    in this situation , the dynamics plays a crucial role in breaking the degeneracies and reliably recovering the best values for the non linear parameters .",
    "a clear example is shown in figure  [ fig.degen ] , where cuts along the evidence surfaces are presented .",
    "the left panel of figure  [ fig.degen ] displays two almost degenerate maxima in @xmath258 ; indeed , the maximum located at the linear coordinate @xmath275 ( corresponding to the set of non linear parameters @xmath276 , @xmath277 , @xmath278 , @xmath279 ) has a value for the lensing evidence of @xmath280 , and therefore , judging on lensing alone , this model would be preferred to the reference model ( see table  [ table.evid ] ) which corresponds to the other maximum at @xmath281 . when the evidence of the dynamics ( fig .",
    "[ fig.degen ] _ central panel _ ) is considered , however , the degeneracy is broken and the `` false '' maximum is heavily penalized with respect to the true one , as shown by the resulting total evidence surface ( fig .",
    "[ fig.degen ] _ right panel _ ) .",
    "test cases with a denser sampling of the integral space ( e.g. @xmath282 , @xmath283 , @xmath284 ) were also considered .",
    "this analysis revealed that , although one obtains more detailed information about the reconstructed two - integral distribution function at the cost of a longer computational time ( e.g. in the @xmath285 case the loop phase is slower by more than a factor of 2 ) , the accuracy of the recovered non linear parameters  which is what we are primarily interested in  does not ( significantly ) improve .",
    "as a consequence , the @xmath210 case is assumed to be the standard grid for the integral space as far as the dynamical modeling is concerned to give an unbiased solution .",
    "once the non linear parameters have been recovered through the evidence maximization routine , it is possible to start from the obtained results to make a more detailed and expensive study of the distribution function using a higher number of tics .      to determine the scatter in the recovered parameters",
    "@xmath18 we performed the full non linear reconstruction on a set of @xmath286 random realizations of the test data .",
    "the statistical analysis of the results is summarized in table  [ table.error ] .",
    "on examining the @xmath287 confidence intervals , the parameters appear to be quite tightly constrained , with the partial exception of the inclination angle which spans an interval of approximately @xmath288 . the means ( and the medians ) of the parameters @xmath289 , @xmath198 and @xmath195 are very close to the `` true values '' of the reference model , while the mean of parameter @xmath45 is slightly more skewed .",
    "the two sets of parameters ( @xmath45 , @xmath195 ) and ( @xmath289 , @xmath198 ) are clearly significantly correlated when plotted against each other ( see fig .",
    "[ fig.corr.matrix ] for a graphical representation of the correlation matrix ) .",
    "both correlations can be understood .",
    "the former is due to the fact that the projected lens potential is nearly invariant when the inclination is increased and flattening is decreased simultaneously ; whereas , in the latter case the requirement that the lens mass enclosed by the lensed images be very similar between lens models causes the lens strength and density slopes to vary in concordance .",
    "although the starting parameters of model @xmath251 are very different from the `` true '' solution given by @xmath249 , in all the considered cases the recovered parameters end up close to the @xmath249 ones , and there is no case in which the solution remains anchored in a really far - off local minimum .",
    "hence , even in the case of our chosen lens potential , which has global degeneracies in the lensing observables ( see section  [ biases ] ) , these degeneacies are broken through the inclusion of stellar kinematic data .",
    "this indeed shows that the combination of lensing and stellar kinematic information is a very promising tool to break degeneracies of the galaxy mass models .",
    "c c c c c c c c & & median & mean & @xmath287 c.i .",
    "& @xmath249 + & @xmath254}$ ] & 63.0 & 63.3 & [ 59.5 , 69.5 ] & 60.0 + non linear & @xmath199 & 4.02 & 4.02 & [ 3.94 , 4.10 ] & 4.05 + parameters & @xmath198 & 0.276 & 0.277 & [ 0.266 , 0.293 ] & 0.280 + & @xmath195 & 0.861 & 0.861 & [ 0.849 , 0.873 ] & 0.850 +    [ table.error ]",
    "we have presented and implemented a complete framework to perform a detailed analysis of the gravitational potential of ( elliptical ) lens galaxies by combining , for the first time , in a fully self - consistent way both gravitational lensing and stellar dynamics information .",
    "this method , embedded in a bayesian statistical framework , enables one to break to a large extent the well - known degeneracies that constitute a severe hindrance to the study of the early - type galaxies ( in particular those at large distances , i.e.  @xmath290 ) when the two diagnostic tools of gravitational lensing and stellar dynamics are used separately . by overcoming these difficulties",
    ", the presented methodology provides a new instrument to tackle the major astrophysical issue of understanding the formation and evolution of early - type galaxies .",
    "the framework is very general in its scope and in principle can accommodate an arbitrary ( e.g.  triaxial ) potential @xmath291 .",
    "in fact , if a combined set of lensing data ( i.e.  surface brightness distribution of the lensed images ) and kinematic data ( i.e.  surface brightness distribution and velocity moments maps of the galaxy ) is provided for an elliptical lens galaxy , it is always possible , making use of the same potential , to formulate the two problems of lensed - image reconstruction and dynamical modeling as sets of coupled linear equations to which the linear and non linear optimization techniques described in sect .",
    "[ outline ] and  [ bayes ] can be directly applied .",
    "more specifically , in the case of gravitational lensing the non - parametric source reconstruction method ( as illustrated in sect .  [ len ] ) straightforwardly applies to the general case of any potential @xmath291 . in the case of dynamical modeling , a full schwarzschild method with orbital integration would be required ; this would constitute , however , only a mere technical complication which does not modify the overall conceptual structure of the method .      in practical applications ,",
    "however , technical difficulties and computational constraints must also be taken into account .",
    "this has motivated the development , from the general framework , of the specific working implementation described in this paper , which restricts itself to axisymmetric potentials @xmath292 and two - integral stellar phase - space distribution functions .",
    "this choice is an excellent compromise between efficiency and generality . on one hand",
    "it allows one to study models which go far beyond the simple spherical jeans - modeling case , and on the other hand , it has the invaluable advantage of permitting a dynamical modeling by means of the two - integrals schwarzschild method of @xcite .",
    "this method ( see sect .  [ dyn ] ) is based on the superposition of elementary building blocks ( i.e.  the tics ) directly obtained from the two - integral distribution function , which do not require computationally expensive orbit integrations .",
    "more specifically , we have sped up this method by several orders of magnitude by designing a fully novel monte carlo implementation ( see appendix  [ mcmc ] ) .",
    "hence , we are now able to construct a realistic two - integral dynamical model and its observables ( surface brightness and line - of - sight projected velocity moments ) in a time of the order of @xmath293 s on a typical 3 ghz machine .",
    "the bayesian approach ( e.g. sect .  [ bayes ] ) constitutes a fundamental aspect of the whole framework . on a first level",
    ", the maximization of the posterior probability , by means of linear optimization , provides the most probable solution for a given data set and an assigned model ( which will be , in general , a function of some non linear parameters @xmath18 , e.g.  the potential parameters , the inclination , the position angle ) in a fast and efficient way . a solution ( i.e.  the source surface brightness distribution for lensing and the distribution function for dynamics ) , however , can be obtained for _ any _ assigned model .",
    "the really important and challenging issue is instead the model comparison , that is , to objectively determine which is the `` best '' model for the given data set or , in other words , which is the `` best '' set of non linear parameters @xmath18 .",
    "bayesian statistics provides the tool to answer these questions in the form of a merit function , the `` evidence '' , which naturally and automatically embodies the principle of occam s razor , penalizing not only mismatching models but also models which correctly predict the data but are unnecessarily complex ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "the problem of model comparison thus becomes a non linear optimization problem ( i.e.maximizing the evidence ) , for which several techniques are available ( see sect .",
    "[ nlin.opt ] ) .    as reported in sect .",
    "[ test ] , we have conducted successful tests of the method , demonstrating that both the linear reconstruction and the non linear optimization algorithms work reliably . it has been shown that it is possible to recover within a few percent the values of the non linear parameters of the reference model ( i.e. the `` true '' model used to generate the simulated data set ) , even when starting the reconstruction from a very skewed and implausible ( in terms of the evidence value ) initial guess for the non linear parameters . such an accurate reconstruction is a direct consequence of having taken into account , beyond the information coming from gravitational lensing , the constraints from stellar dynamics . indeed",
    ", when the algorithm is run considering _ only _ the lensing data , degenerate solutions with comparable or almost coincident values for the evidence are found , making it effectively impossible to distinguish between these models .",
    "the crucial importance of the information from dynamics is exhibited by the fact that , when it is included in the analysis , the degeneracies are fully broken and a solution very close to the true one is unambiguously recovered ( see fig .",
    "[ fig.degen ] for an example ) .",
    "bearing in mind these limitations and their consequences , however , it should also be noted that the full modularity of the presented algorithm makes it fit to be used also in those situations in which either the lensing or the kinematic observables are not available .",
    "this would allow one , for example , to restrict the plausible models to a very small subset of the full space of non linear parameters , although a single non - degenerate `` best solution '' would probably be hard or impossible to find .",
    "eventhough the methodology that we introduced in this paper works very well and is quite flexible , we can foresee a number of improvements for the near and far future , in order of perceived complexity .",
    "( i ) exploration of the errors on the non linear parameters @xmath92 through a mcmc method , even though this requires an extension of the mcmc framework in the context of the bayesian evidence , since one also needs to explore variations in the lens parameters due to changes in the hyperparameters , and not only changes in the posterior for fixed hyperparameters .",
    "( ii ) implementation of additional potential ( or density ) models or even a non - parametric or multi - pole expansion description of the gravitational potential in the @xmath294)-plane for axisymmetric models .",
    "this allows more freedom for the galaxy potential description .",
    "( iii ) implementing an approximate three - integral method in axisymmetric potentials ( e.g. * ? ? ? * ) .",
    "( iv ) including an additional iterative loop around the posterior probability optimization , one can construct stellar phase - space distribution functions that are self - consistent , i.e.  they generate the potential for which they are solutions to the collisionless boltzmann equation .",
    "this would allow the stellar and dark - matter potential contributions to be separated , a feature not yet part of the current code .",
    "( v ) a full implementation of schwarzschild s method for arbitrary potentials through orbital integration .",
    "besides these technical improvements , which are all beyond the scope of this methodological paper , we also plan , in a future publication , a set of additional performance tests to see to what level each of the degeneracies ( e.g. the mass sheet and mass anisotropy ) in lensing and stellar dynamics are broken and how far the simpler lensing plus spherical jeans approach ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ) gives ( un)biased results . such studies would allow us to better interpret the results obtained in cases where spatially resolved stellar kinematics is not available ( e.g.  for faint very high redshift systems ; * ? ? ?",
    "* ) .    as for the application , the algorithm described in this paper will be applied in a full and rigorous analysis of the slacs sample of massive early - type lens galaxies ( see * ? ? ?",
    "* ; * ? ? ?",
    "* ) for which the available data include _ hubble space telescope _",
    "advanced camera for survey and nicmos images of the galaxy surface brightness distribution and lensed image structure and maps of the first and second line - of - sight projected velocity moments ( obtained with vlt - vimos two - dimensional integral field spectroscopy , as part of the large program and as a series of keck long - slit spectra ) .",
    "we are grateful to the anonymous referee for the insightful comments .",
    "we acknowledge the kavli institute for theoretical physics at the university of california , santa barbara for the warm hospitality and lively scientific environment provided over the course of the _ applications of gravitational lensing _ workshop .",
    "we are grateful to tommaso treu , chris fassnacht , giuseppe bertin , luca ciotti , phil marshall , sherry suyu , and claudio grillo for fruitful discussions , as well as to the other slacs team members , adam bolton , scott burles , and lexi moustakas .",
    "m.  b. acknowledges the support from an nwo program subsidy ( project 614.000.417 ) .",
    "l.  v.  e.  k. is supported in part through an nwo - vidi program subsidy ( project 639.042.505 ) .",
    "we also acknowledge the continuing support of the european community s sixth framework marie curie research training network programme , contract mrtn - ct-2004 - 505183 `` angles '' .",
    "arnaboldi , m. , et al .",
    "1996 , , 472 , 145    barnes , j.  e.  1992 , , 393 , 484    bender , r. , burstein , d. , & faber , s.m .",
    "1993 , , 411 , 153    bernardi , m. , et al .",
    "2003 , , 125 , 1882    bertin , g. , et al .",
    "1994 , , 292 , 381    binney , j.  1981 , , 196 , 455    binney , j. & tremaine , s.  1987 , _ galactic dynamics _ ( princeton : princeton univ . press )    bolton , a.  s. , burles , s. , koopmans , l.  v.  e. , treu , t. , & moustakas , l.  a.   2006 , , 638 , 703    borriello , a. , salucci , p. , & danese , l.  2003 , , 341 , 1109    bower , r.g . ,",
    "lucey , j.r . , & ellis , r.s .",
    "1992 , , 254 , 589    brewer , b.  j. , & lewis , g.  f.  2006 , , 637 , 608    brewer , b.  j. , & lewis , g.  f.  2006 , , 651 , 8    byrd , r.  h. , lu , p. , & nocedal , j.  1995 , siam journal on scientific and statistical computing , 16 , 5 , 1190    cappellari , m. , bacon , r. , bureau , m. , damen , m.  c. , davies , r.  l. , de zeeuw , p.  t. , emsellem , e. , falcn - barroso , j. , krajnovi , d. , kuntschner , h. , mcdermid , r.  m. , peletier , r.  f. , sarzi , m. , van den bosch , r.  c.  e. , van de ven , g.  2006 , 366 , 1126    carollo , c.  m. , de zeeuw , p.  t. , van der marel , r.  p. , danziger , i.  j. , & qian , e.  e.  1995 , , 441 , l25    cohn , j.  d. , kochanek , c.  s. , mcleod , b.  a. , & keeton , c.  r.  2001 , , 554 , 1216    cole , s. , lacey , c.  g. , baugh , c.  m. , & frenk , c.  s.  2000 , , 319 , 168    cousins , r.  d.  1995 , american journal of physics , 63 , 398    cretton , n. , de zeeuw , p.  t. , van der marel , r.  p. , & rix , h .- w .",
    "1999 , , 124 , 383    de zeeuw , p.  t. , bureau , m. , emsellem , e. , bacon , r. , carollo , c.  m. , copin , y. , davies , r.  l. , kuntschner , h. , miller , b.  w. , monnet , g. , peletier , r.  f. , verolme , e.  k.  2002 , , 329 , 513    dehnen , w. , & gerhard , o.  e.  1993 , , 261 , 311    djorgovski , s. , & davis , m. 1987 , , 313 , 59    dobke , b.  m. , & king , l.  j.  2006 , , 460 , 647    dressler , a. , lynden - bell , d. , burstein , d. , davies , r.  l. , faber , s.m .",
    ", terlevich , r. , & wegner , g. 1987 , , 313 , 42    dye , s. , & warren , s.  j.  2005 , , 623 , 31    evans , n.  w.  1994 , , 267 , 333    evans , n.  w. & de zeeuw , p.  t.  1994 , , 271 , 202    evans , n.  w. , & witt , h.  j.  2003 , , 345 , 1351    fabbiano , g.  1989 , , 27 , 87    falco , e.  e. , gorenstein , m.  v. , & shapiro , i.  i. 1985 , apj , 289 , l1    ferreras , i. , saha , p. , & williams , l.  l.  r.  2005 , , 623 , l5    ferrarese , l. , & merritt d. 2000 , , 539 , l9    franx , m. , van gorkom , j.  h. , & de zeeuw , t.  1994 , , 436 , 642    frenk , c.  s. , white , s.  d.  m. , davis , m. , & efstathiou , g.  1988 , , 327 , 507    gavazzi , r. , treu , t. , rhodes , j.  d. , koopmans , l.  v.  e. , bolton , a.  s. , burles , s. , massey , r. , & moustakas , l.  a. 2007 , apj , in press    gebhardt , k. , et al .",
    "2000 , , 539 , l13    gerhard , o.  e.  1993 , , 265 , 213    gerhard , o. , kronawitter , a. , saglia , r.  p. , & bender , r.  2001 , , 121 , 1936    gerhard , o.  2006 , planetary nebulae beyond the milky way , 299    gray , g.  a. & kolda , t.  g.  2006 , acm t. math .",
    "software , 32 , 3 , 485    guzman , r. , lucey , j.r . , carter , d. , & terlevich , r.j .",
    "1992 , , 257 , 187    james , j.  1994 , minuit - function minimization and error analysis , cern program library entry d506    kochanek , c.  s.  1995 , , 445 , 559    kolda , t.  g.  2005 , siam j. optimization , 16 , 2 , 563    koopmans , l. v. e. & treu , t.  2002 , , 568 , l5    koopmans , l. v. e. & treu , t.  2003 , , 583 , 686    koopmans , l.v.e .",
    ", proceedings of science , published by sissa ; conference : `` baryons in dark matter haloes '' , novigrad , croatia , 5 - 9 october 2004 ; editors : r .- j .",
    "dettmar , u. klein , p. salucci , 66    koopmans , l.  v.  e.  2005 , , 363 , 1136    koopmans , l.  v.  e. , treu , t. , bolton , a.  s. , burles , s. , & moustakas , l.  a.   2006 , , 649 , 599    kormann , r. , schneider , p. , & bartelmann , m.  1994 , , 284 , 285    liddle , a.  r , corasaniti , p.  s. , kunz , m. , mukherjee , p. , parkinson , d. , & trotta , r.  2007 , astro - ph/0703285v1    loewenstein , m. , & white , r.  e.  1999 , , 518 , 50    ma , c .- p .",
    "2003 , , 584 , l1    mackay , j. c.  1992 , phd thesis    mackay , j. c.  1999 , neural comp , 11 , 1035    mackay , d.  j.  c.  2003 , _ information theory , inference and learning algorithms _ ( cambridge : cambridge university press )    magorrian j. , et al .",
    "1998 , , 115 , 2285    marshall , p.  2006",
    ", , 372 , 1289    matsushita , k. , makishima , k. , ikebe , y. , rokutanda , e. , yamasaki , n. , & ohashi , t.  1998 , , 499 , l13    merritt , d. 1985a , , 90 , 1027    merritt , d. 1985b , , 214 , 25    mould , j.  r. , oke , j.  b. , de zeeuw , p.  t. , & nemec , j.  m.  1990 , , 99 , 1823    muoz , j.  a. , kochanek , c.  s. , & keeton , c.  r.  2001 , , 558 , 657    osipkov l.  .p . , 1979 , pisma astron .",
    ", 5 , 77    pfenniger , d.  1984 , , 141 , 171    press , w.  h. , teukolsky , s.  a. , vetterling , w.  t. , flannery , b.  p.  1992",
    ", _ numerical recipes in c _ ( cambridge : cambridge university press )    richstone , d.  o.  1980 , , 238 , 103    richstone , d.  o.  1984 , , 281 , 100    rix , h .- w . , de zeeuw , p.  t. , cretton , n. , van der marel , r.  p. , & carollo , c.  m.  1997 , , 488 , 702    romanowsky , a.  j. , douglas , n.  g. , arnaboldi , m. , kuijken , k. , merrifield , m.  r. , napolitano , n.  r. , capaccioli , m. , & freeman , k.  c.  2003 , science , 301 , 1696    rusin , d. , & ma , c .- p .",
    "2001 , , 549 , l33    rusin , d. , norbury , m. , biggs , a.  d. , marlow , d.  r. , jackson , n.  j. , browne , i.  w.  a. , wilkinson , p.  n. , & myers , s.  t.  2002 , , 330 , 205    rusin , d. , et al .",
    "2003 , , 587 , 143    rusin , d. , & kochanek , c.  s.  2005 , , 623 , 666    saglia , r.  p. , bertin , g. , & stiavelli , m.  1992 , , 384 , 433    saha , p. , & williams , l.  l.  r.  2006 , , 653 , 936    schneider , p. , ehlers , j. , & falco e.  e.  1992 , _ gravitational lenses _ ( berlin : springer verlag )    schneider , p. , kochanek , c.  s. , & wambsganss , j.  2006 , saas - fee advanced course 33 : gravitational lensing : strong , weak and micro    schwarzschild , m.  1979 , , 232 , 236    seljak , u.  2002 , , 334 , 797    suyu , s.  h. , marshall , p.  j. , hobson , m.  p. , & blandford , r.  d.  2006 , , 371 , 983    tikhonov , a.  n.  1963 , soviet math .",
    "dokl . , 4 , 1035    toomre , a.  1977 , in `` evolution of galaxies and stellar populations '' , ed .",
    "tinsley & r.b .",
    "larson ( new haven : yale university observatory ) , 401    treu , t. & koopmans , l. v. e.  2002 , , 575 , 87    treu , t. & koopmans , l. v. e.  2003 , , 343 , l29    treu , t. & koopmans , l. v. e.  2004 , , 611 , 739    treu , t. , koopmans , l.  v.  e. , bolton , a.  s. , burles , s. , & moustakas , l.  a.   2006 , , 640 , 662    verolme , e.  k. & de zeeuw , p.  t.  2002 , , 331 , 959    warren , s.  j. & dye , s.  2003 , , 590 , 673    wayth , r.  b. , warren , s.  j. , lewis , g.  f. , & hewett , p.  c.  2005 , , 360 , 1333    wayth , r.  b. , & webster , r.  l.  2006 , , 372 , 1187    white , s.  d.  m. , & frenk , c.  s.  1991 , , 379 , 52    winn , j.  n. , rusin , d. , & kochanek , c.  s.  2003 , , 587 , 80    wucknitz , o.  2002 , , 332 , 951    wucknitz , o. , biggs , a.  d. , & browne , i.  w.  a.  2004 , , 349 , 14    zhu , c. , byrd , r.  h. , & nocedal , j.  1997 , acm t. math .",
    "software , 23 , 4 , 550",
    "we make use of a curvature regularization . this form of regularization tries to put the second derivative between a pixel and the two adjacent ones to zero and has been shown by @xcite to be optimal for the reconstruction of smooth distributions .",
    "the curvature regularization has been chosen since we do not expect , in the majority of cases , to have sharp intensity variations in the surface brightness distribution of an extended source ( for lensing ) or in the distribution function of a galaxy .",
    "however , other choices of regularization can easily be implemented and ranked according to their evidence @xcite .    following the notation of sect .",
    "[ bayes ] , we indicate as @xmath19 the linear parameter vector ( or , more simply , the source ) and as @xmath25 the regularization matrix . since the source is defined on a rectangular grid of @xmath295 elements ,",
    "@xmath25 actually consists of two matrices , @xmath296 and @xmath297 , which regularize the grid pixels along the rows and the columns , respectively .",
    "the horizontal regularization operator @xmath296 is a square matrix of rank @xmath298 . in each row",
    "@xmath45 the only non zero elements are @xmath299 ; the only exceptions are the rows @xmath300 and @xmath301 ( with @xmath302 ) , where a zeroth - order regularization is performed ( i.e. @xmath303 is the only non zero term ) to prevent the connection of pixels belonging to different rows which are therefore physically uncorrelated .",
    "similarly , the vertical regularization operator @xmath297 is constructed such that in each row  @xmath45 all the elements are zero with the exclusion of @xmath304 ; a zeroth - order regularization is applied for the rows @xmath305 and @xmath306 .",
    "for the assigned three - dimensional gravitational potential @xmath17 , the reduced deflection angle @xmath121 is given by ( e.g. * ? ? ?",
    "* ) @xmath307 where @xmath78 is the line - of - sight coordinate , @xmath308 are the sky coordinates and @xmath309 denotes the two - dimensional gradient operator in the plane of the sky ; @xmath310 is the speed of light expressed in the same units as the value of @xmath311 .",
    "if the gradient operator , which does not depend on @xmath78 , is taken out of the integral and the potential is conveniently written as @xmath312 where @xmath193 is the normalization constant in the most suitable physical unit ( in our case km s@xmath313 ) and @xmath314 is a function of the dimensionless coordinates expressed as angles in arcseconds ( @xmath315 , @xmath316 ) , then the deflection angle assumes the expression @xmath317,\\ ] ] where @xmath318 is the lens strength in arcseconds .",
    "the parameter @xmath199 sets the scale for the lensing and , therefore , is always included in the parameter vector @xmath18 .",
    "( [ lens.str ] ) openly displays how intimately the lens strength is connected to the normalization of the three - dimensional potential ( the same used for the dynamical modeling ) within our joint method .",
    "in this appendix we describe the numerical implementation of the two - integral axisymmetric schwarzschild method of @xcite that we developed to significantly accelerate the construction of the dynamical model , i.e.  the projected and psf - convoluted model observables generated by the tic library .    as a first step ,",
    "we construct a library composed of @xmath319 tics in the given potential @xmath17 ( here @xmath184 is an even number ) .",
    "we consider a grid ( linear or logarithmic ) in the circular radius @xmath320 with @xmath183 samplings between @xmath321 and @xmath322 . the range is chosen to include most of the mass or , if the mass is infinite for the potential @xmath17 , to provide a satisfactory sampling of the potential profile in the radial direction . for each @xmath320",
    "the circular speed @xmath323 is calculated as @xmath324 and the angular momentum of the circular orbit @xmath325 is set .",
    "computing the energy @xmath326 of the circular orbit at @xmath320 , the radial grid is immediately translated into a sampling in energy . for each value of @xmath327",
    ", the grid in the normalized angular momentum @xmath328 is constructed by sampling ( linearly or logarithmically ) @xmath329 values between the minimum @xmath330 and the maximum @xmath331 .",
    "( for numerical reasons , the grid is actually not sampled between these extrema , but between @xmath332 and @xmath333 , with @xmath334 ) . to take the odd part of the distribution function into account as well ,",
    "we likewise consider the @xmath329 negative values for the angular momentum , @xmath335 , on a mirror grid .    we also need to define a suitable coordinate frame for the galaxy .",
    "since the system is axisymmetric , we adopt the cylindrical coordinates @xmath336 , with the origin on the center of the galaxy .",
    "if the galaxy is observed at an inclination  @xmath45 and @xmath200 is the position angle ( defined as the angle measured counterclockwise between the north direction and the projected major axis of the galaxy ) , the projected coordinates @xmath337 are given by @xmath338 here @xmath78 is measured along the line of sight , while @xmath339 and @xmath340 are in the plane of sky and are directed ( respectively ) along the projected major and minor axes of the galaxy .    for any tic",
    ", we populate the surface inside the zero velocity curve with @xmath341 particles of mass ( or equivalently luminosity ) @xmath342 by means of a markov - chain monte carlo routine whose probability distribution is given by eq .",
    "( [ tic.rho ] ) for the density .",
    "this effectively corresponds to numerically reproducing the density @xmath343 , fixing at the same time the total mass @xmath344 for each tic .",
    "however , since @xmath345 , the surface density of the torus `` wrapped '' onto the meridional plane ( denoted as @xmath346 ) is constant , i.e. , @xmath347 one can take advantage of this property to greatly simplify the markov - chain monte carlo routine .",
    "now it is only necessary to uniformly populate the meridional plane .",
    "for each particle a pair of coordinates @xmath68 ( within some interval which encompasses the zvc ) is randomly generated .",
    "if it falls outside the zvc , the particle is `` rejected '' and another one is generated .",
    "if the coordinates are located inside the zvc , a random value in the interval @xmath348 is chosen for the azimuthal coordinate in order to have a complete tern @xmath349 , and the particle counts toward the total of @xmath341 drawings .",
    "this procedure yields at the same time the surface @xmath189 enclosed by the zvc in the meridional plane ( effectively obtained via monte carlo integration ) , which is required for the normalization of the @xmath186 ( see appendix  [ normalization ] ) .    with this method",
    "the computation of all the projected quantities is fast and straightforward . for each",
    "`` accepted '' mass point we know the cylindrical coordinates @xmath349 ; associated with it are also the velocity moments defined by eqs .",
    "( [ tic.vphi])-([tic.vz ] ) . using the first two equations of the transformation from equation  ( [ coord ] ) , the projected coordinates @xmath350",
    "are directly calculated .",
    "casting the points on a grid on the sky plane and summing up all the points in the same pixel then reproduces numerically the projected surface brightness distribution @xmath177 ( see figure  [ fig.tic ] for an illustration ) . the line - of - sight velocity moments associated with each point in the sky plane ( but possibly on a different grid )",
    "are obtained in an analogous way ( but making use now of the third equation of [ [ coord ] ] ) from the corresponding unprojected quantities @xmath351 @xmath352 in analogy with the surface brightness , the first and second line - of - sight moments associated with each mass point inside a given pixel are summed up .",
    "this gives the quantities @xmath353 and @xmath354 .",
    "the effect of the psf is taken into account by simply convolving the projected surface brightness or weighted velocity moments calculated on their respective grids ( preferably oversampled ) with the psf profile sampled on the same grid .",
    "this operation can be numerically performed in a very efficient way through several ffts ( fast fourier transforms ) .",
    "this numerical implementation is dramatically faster than the semi analytic approach ( at the expense of some numerical noise ) . on a machine with a 3 ghz processor ,",
    "the whole process of calculating the projected quantities @xmath355 , @xmath356 , and @xmath241 convolved with the psf for 1400 tics takes about 3 minutes ( with @xmath211 ) .",
    "this figure should be compared with the @xmath357 minutes required by @xcite to calculate ( on a 1 ghz machine ) _ only _ the projected density without psf convolution for an equal number of tics .",
    "in this appendix we illustrate how the reconstructed adimensional weights @xmath186 are translated into the dimensional distribution function values @xmath188 . for a two - integral distribution function ,",
    "the density is given by the formula ( e.g. * ? ? ?",
    "* ) @xmath358 where @xmath359 is the surface density `` wrapped '' in the meridional plane ( cf . definition  [ [ tic.wrap ] ] ) .",
    "if we assume that @xmath360 can be considered approximately constant over the cell @xmath361 of area @xmath190}}$ ] in the integral space , and we remember the properties of the tics , then we have @xmath362    in the previous formula , @xmath363 is the `` wrapped '' surface density generated by @xmath364 , specified by the pair @xmath365 , which is constant inside the zvc and zero elsewhere ( see eq .",
    "[ [ tic.wrap ] ] ) : @xmath366 here @xmath189 denotes the area enclosed by the zvc in the meridional plane ( which can be calculated as described in appendix  [ mcmc ] ) and @xmath344 is the fixed tic mass ( all the tics have equal mass by construction ) .    combining eqs .",
    "( [ sig - df ] ) and  ( [ cj ] ) we find the desired relation @xmath367 } } } , \\ ] ] which translates the weights @xmath186 into distribution function values expressed in the standard physical units of mass length@xmath368 velocity@xmath368 .",
    "if @xmath342 , in the numerator of the right - hand side of eq .",
    "( [ gamma - df ] ) , is omitted or divided by a mass - to - light ratio coefficient @xmath369 , the resulting distribution function is expressed in terms of ( respectively ) number or luminosity phase - space density .",
    "in this appendix we use the same notation as sect .",
    "[ bayes ] , and we indicate as @xmath370 and @xmath371 , respectively , the number of elements in the linear parameter vector @xmath19 and in the data vector @xmath15 . if the assumptions made in sect .",
    "[ bayes ] , viz .",
    ", gaussian noise and quadratic functional form of the regularization term @xmath372 with minimum in @xmath373 , are valid , then the logarithm of the evidence has the expression ( e.g. * ? ? ?",
    "* )    @xmath374         \\nonumber \\\\ & & + \\frac{{n_{\\rm x}}}{2 } \\log \\lambda      + \\frac{1}{2 } \\log \\left [ \\det \\left ( { \\mathbf{h}^{\\rm t}}{\\mathbf{h}}\\right ) \\right ]       - \\frac{{n_{\\rm b}}}{2 } \\log ( 2 \\pi )      + \\frac{1}{2 } \\log \\left ( \\det { { \\mathbf{c}}^{-1}}\\right).\\end{aligned}\\ ] ]    the expression of the evidence in the case of lensing and dynamics is immediately obtained by rewriting eq .",
    "( [ evid ] ) with the notation of sections  [ len ] and  [ dyn ] .",
    "all the relevant quantities for the evans power - law galaxy models which are used in sect .",
    "[ test ] are analytic ( refer to @xcite @xcite and @xcite @xcite for the full expressions ) .",
    "the lensing deflection angle @xmath121 can be calculated from the potential from eq .",
    "( [ evans.pot ] ) , resulting in : @xmath375 where @xmath350 are the coordinates in the sky plane ( which is defined as the plane orthogonal to the line of sight @xmath78 ) , @xmath376 is the projected axis ratio , and @xmath369 is the gamma function ."
  ],
  "abstract_text": [
    "<S> gravitational lensing and stellar dynamics are two independent methods , based solely on gravity , to study the mass distributions of galaxies . </S>",
    "<S> both methods suffer from degeneracies , however , that are difficult to break . in this paper , we present a new framework that self - consistently unifies gravitational lensing and stellar dynamics , breaking some classical degeneracies that have limited their individual usage , particularly in the study of high - redshift galaxies .    for any given galaxy potential , the mapping of both the unknown lensed source brightness distribution and the stellar phase - space distribution function onto the photometric and kinematic observables can be cast as a single set of coupled linear equations , which are solved by maximizing the likelihood penalty function . </S>",
    "<S> the bayesian evidence penalty function subsequently allows one to find the best potential - model parameters and to quantitatively rank potential - model families or other model assumptions ( e.g. psf ) . </S>",
    "<S> we have implemented a fast algorithm that solves for the maximum - likelihood pixelized lensed source brightness distribution and the two - integral stellar phase - space distribution function @xmath0 , assuming axisymmetric potentials . to make the method practical , </S>",
    "<S> we have devised a new monte carlo approach to schwarzschild s orbital superposition method , based on the superposition of two - integral ( @xmath1 and @xmath2 ) toroidal components , to find the maximum - likelihood two - integral distribution function in a matter of seconds in any axisymmetric potential . </S>",
    "<S> the non linear parameters of the potential are subsequently found through a hybrid mcmc and simplex optimization of the evidence . </S>",
    "<S> illustrated by the power - law potential models of evans , we show that the inclusion of stellar kinematic constraints allows the correct linear and non linear model parameters to be recovered , including the potential strength , oblateness _ and _ inclination , which , in the case of gravitational - lensing constraints only , would otherwise be fully degenerate . </S>"
  ]
}