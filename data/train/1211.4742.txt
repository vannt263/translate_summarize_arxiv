{
  "article_text": [
    "we consider the statistical problem of functional linear regression ( flr ) . in its standard version ,",
    "one observes the data @xmath0 where @xmath1 are i.i.d .",
    "random variables taking their values in @xmath2)$ ] , that is , the set consisting of all continuous functions on the interval @xmath3 $ ] , and @xmath4 with @xmath5 where @xmath6 denotes the @xmath7)$]-inner product throughout this work .",
    "the i.i.d .",
    "error variables @xmath8 are assumed to be centered and normally distributed with the variance @xmath9 .",
    "moreover , all @xmath10 are independent .",
    "the goal is to estimate the regression function @xmath11)$ ] . in general , we allow for such a structure of the function class @xmath12 which does not determine @xmath13 up to finitely many real - valued parameters .",
    "thus we consider a nonparametric estimation problem .",
    "moreover we assume that @xmath14 and @xmath15    \\leq c_{x,0 } \\exp(-c_{x,1 } x^{c_{x,2}})$ ] for all @xmath16 and some finite constants @xmath17 where @xmath18 , @xmath19 denotes the @xmath20)$]-norm of some element of that space .",
    "thus the tails of the design distribution are restricted .",
    "such conditions are usual in nonparametric regression problems .",
    "the flr model has obtained considerable attention in the statistical community during the last years , which is reflected in the large amount of literature on this topic .",
    "various of estimation procedures have been proposed to make the regression function @xmath13 empirically accessible ( see , e.g. ,  @xcite ) .",
    "the minimax convergence rates in flr are investigated , for example , in  @xcite . in @xcite , adaptive estimation in flr",
    "is considered .",
    "generalizations of flr are discussed in @xcite .",
    "a central limit theorem for flr is derived in  @xcite . in  @xcite",
    ", practical applications of flr in the field of medical statistics are described ; the authors consider two real data sets on primary biliary cirrhosis and systolic blood pressure .",
    "for a comprehensive introduction to the field of functional data analysis in general , see  @xcite .    in order to compare two statistical models , it is useful to prove asymptotic equivalence between those models .",
    "for the basic concept and a detailed description of this strong asymptotic property , we refer to @xcite and  @xcite .",
    "also , a review on this topic is given in the following section .",
    "as an important feature , if two models @xmath21 and @xmath22 are asymptotically equivalent , then @xmath21 adopts optimal convergence rates and sharp asymptotic constants with respect to _ any _ bounded loss function from model @xmath22 and vice versa .",
    "thus , the theory of asymptotic equivalence does not only capture special loss functions such as the mean integrated squared error ( mise ) or the pointwise mean squared error ( mse ) but includes various types of semi - metrics between the estimator and the target function @xmath13 and also addresses the estimation of characteristics of @xmath13 , such as its support or its mode .",
    "furthermore , superefficiency phenomena also coincide in both models when considering subclasses @xmath23 of the target parameter space @xmath12 . in particular",
    ", research has focussed on proofs of asymptotic equivalence of experiments where @xmath24 i.i.d .",
    "data are observed , whose distribution depends on some parameter @xmath25 , and experiments where @xmath13 occurs in the drift of an empirically accessible it process .",
    "for instance , nussbaum @xcite considers an asymptotically equivalent white noise model for density estimation , while brown and low  @xcite introduce such a model for nonparametric regression . in recent related literature on regression problems , carter  @xcite studies the case of unknown error variance , and reiss  @xcite extends asymptotic equivalence to the multivariate setting .",
    "returning to model ( [ eq:1.1 ] ) , we suppose that the nuisance parameters @xmath26 and @xmath27 , that is , the distribution of the @xmath28 , are known .",
    "that allows us to exclude those quantities from the parameter space of the experiment and to fully concentrate on the estimation of @xmath13 .",
    "this condition is also imposed in most papers dealing with asymptotic equivalence for nonparametric regression experiments .",
    "the work of  @xcite represents an exception where the corresponding white noise model becomes more difficult and , apparently , less useful to derive adoptable asymptotic properties .",
    "with respect to asymptotic equivalence , we restrict our consideration to the case of known @xmath27 .",
    "however , in section  [ s : new ] , we will show that the sharp minimax asymptotics with respect to the mise are extendable to the case of unknown design distribution .",
    "the main purpose of the current work is to prove asymptotic equivalence of model ( [ eq:1.1 ] ) and a statistical inverse problem in the white noise setting .",
    "that latter model is described by the observation of an it process @xmath29 , @xmath30 $ ] , @xmath31 , driven by the stochastic differential equation @xmath32(t)\\,dt + n^{-1/2 } \\sigma \\,dw(t ) , \\ ] ] where @xmath33 denotes a standard wiener process on the interval @xmath3 $ ] , and @xmath34 denotes a linear operator mapping from the hilbert space @xmath7)$ ] to itself .",
    "these models are also widely studied in mathematical statistics ( see , e.g. ,  @xcite and  @xcite ) .",
    "they have their applications in the field of signal deblurring and econometrics .",
    "we will concentrate on a specific version of model ( [ eq:1.1.1 ] ) where @xmath34 is equal to the unique positive symmetric square root @xmath35 of the covariance operator @xmath36 , that is , @xmath37 and @xmath38 for any @xmath39)$ ] .",
    "thus , the observation @xmath29 , @xmath30 $ ] , is defined by @xmath31 and @xmath40(t)\\,dt + n^{-1/2 } \\sigma \\,dw(t ) .\\ ] ] in  @xcite , the authors remark on the similarity of models ( [ eq:1.1 ] ) and ( [ eq:1.1.2 ] ) . in the current paper",
    ", we will rigorously establish asymptotic equivalence between those models . as an interesting feature , additional observation of the data",
    "@xmath41 would be redundant in model ( [ eq:1.1.2 ] ) .",
    "all information about the design points is recorded by @xmath36 in ( [ eq:1.1.2 ] ) .",
    "therefore , all what is observed in the corresponding white noise experiment is the process @xmath29 , @xmath42 $ ] .",
    "after the general introduction to the property of asymptotic equivalence as used in the current paper in section  [ s1.0 ] , we will first prove ( nonasymptotic ) equivalence of model ( [ eq:1.1 ] ) and an empirical version of model ( [ eq:1.1.2 ] ) where @xmath36 is replaced by a noisy counterpart in section  [ s2 ] .",
    "in section  [ s3 ] , we prove asymptotic equivalence of ( [ eq:1.1 ] ) and ( [ eq:1.1.2 ] ) under some additional technical conditions . in section",
    "[ s : new ] , we show that the sharp lower bound which follows from the results of the previous section can be attained by specific estimators in the realistic case of unknown design distribution .",
    "a discussion of the findings and their conclusions are provided in section  [ s5 ] .",
    "to recall the definition of asymptotic equivalence , we consider two ( sequences of ) statistical experiments @xmath43 , @xmath44 , with a joint parameter space @xmath12 , which may depend on @xmath24 .",
    "the lecam distance between @xmath45 and @xmath46 is defined by @xmath47 where @xmath48 is the total variation distance , and @xmath49 denotes the collection of so - called transitions ( see  @xcite and  @xcite for their exact definition ) .",
    "the statistical experiments @xmath45 and @xmath46 are called asymptotically equivalent if @xmath50 converges to zero as @xmath51 , while they are called equivalent if @xmath52 for all @xmath24 .    in the framwork of our note , we will not use that general definition of ( asymptotic ) equivalence but our proofs lean on following sufficient conditions for these properties :    we consider the following sufficient condition for asymptotic equivalence of @xmath45 and @xmath53 : we define the sets @xmath54 , @xmath44 , @xmath55 , which contains all real - valued integrable random variables @xmath56 on the domain @xmath57 satisfying @xmath58 a.s .",
    "thus any kind of bounded loss functions are captured by the classes @xmath54 so that the expectation @xmath59 with respect to the distribution @xmath60 describes an arbitrary bounded and normalized statistical risk for estimating the parameter @xmath13 under the observation scheme @xmath61 .",
    "now we define two sequences @xmath62 , @xmath63 , of @xmath64-measurable mappings from @xmath65 to @xmath66 . as an essential condition ,",
    "the @xmath67 must not depend on @xmath13 .",
    "hence , @xmath67 may be interpreted as a transformation of the data from an observation contained in the space @xmath57 to an observation which lies in @xmath68 .",
    "thus a statistician who intends to construct an estimation procedure for @xmath13 may always apply this transformation @xmath67 to an observation @xmath69 .",
    "then we obtain asymptotic equivalence of @xmath70 and @xmath46 when we can show the existence of such transformation sequences @xmath62 , @xmath63 , so that @xmath71 as @xmath72 for all @xmath63 .",
    "accordingly , we have equivalence if the left - hand side in ( [ eq : aseq ] ) equals @xmath73 for any @xmath24 . intuitively speaking , after transforming the data drawn from model @xmath45 according to @xmath74 ,",
    "the distance between any bounded statistical risk in model @xmath46 on one hand and for the transformed data from model @xmath45 becomes small for large @xmath24 or is equal to zero for any  @xmath24 , respectively .",
    "the same condition must also hold true when exchanging @xmath75 and @xmath76 .    in the specific framework of our note , we assume , in addition , that all transformations @xmath67 must not depend on the nuisance parameter @xmath26 .",
    "that compensates the unrealistic condition of known @xmath26 .",
    "in particular , @xmath26 is not used to transform the data or to construct decision procedures or estimators .",
    "therefore , our results also addresses the case of unknown @xmath26 .",
    "nevertheless , @xmath26 must be viewed as uninteresting for the statistician , that is , it must not explicitly occur in the loss functions @xmath77 .",
    "thus , the problem of estimating @xmath26 is not covered by our approach .",
    "assume that the experiment @xmath46 describes the observation of @xmath78 for @xmath79 in experiment @xmath45 where @xmath80 is a sufficient statistic for @xmath13 in experiment @xmath45 .",
    "then @xmath45 and @xmath46 are statistically equivalent ( i.e. , their lecam distance vanishes ) and , hence , asymptotically equivalent .",
    "that assertion holds true whenever the experiments are polish spaces .",
    "this criterion is satisfied as all probability spaces considered in the current work are @xmath81 , @xmath2)$ ] , @xmath7)$ ] and some set products of those classes ( see lemma 3.2 in  @xcite ) .    if some experiments @xmath45 and @xmath46 on one hand , and @xmath46 and @xmath82 on the other hand , are ( asymptotically ) equivalent , then @xmath45 and @xmath82 are ( asymptotically ) equivalent , too .",
    "also , ( asymptotic ) equivalence of @xmath45 and @xmath46 is a symmetric relation between the experiments .",
    "assume that some experiments @xmath45 and @xmath46 may be decomposed into two independent experiments @xmath83 and @xmath84 , @xmath63 , respectively .",
    "moreover , we suppose that the experiments @xmath85 and @xmath86 on one hand and the experiments @xmath87 and @xmath88 on the other hand are ( asymptotically ) equivalent .",
    "then , the combined experiments @xmath45 and @xmath46 are ( asymptotically ) equivalent as well .",
    "now , @xmath21 denotes the underlying experiment of the flr model ( [ eq:1.1 ] ) ; it is defined by @xmath89)^{(n ) } \\times\\mathbb{r}^{(n)}$ ] , @xmath90 denotes the borel @xmath91-algebra when considering the uniform metric on the functional components and the euclidean on the real - valued components of @xmath92 .",
    "the corresponding probability measures @xmath93 are well defined by the assumptions of the model  ( [ eq:1.1 ] ) .",
    "the parameter space @xmath94)$ ] will be specified later .",
    "still , the observations @xmath0 may be viewed as random variables having their domain on some basic probability space @xmath95 .",
    "we define the linear covariance operator @xmath96 ) \\to l_2([0,1])$ ] by @xmath97 ) .\\ ] ] writing @xmath98 , we realize that @xmath36 is a hilbert ",
    "schmidt integral operator where @xmath99 by the cauchy ",
    "schwarz inequality and the tail condition imposed on the distribution of @xmath100 .",
    "hence @xmath36 is a continuous and compact operator .",
    "we have @xmath101 for all @xmath102 $ ] so that the operator @xmath36 is self - adjoint .",
    "furthermore , it is positive ; that is , by fubini s theorem we have @xmath103 of the separable hilbert space @xmath7)$ ] which consists of eigenfunctions of @xmath36 .",
    "the corresponding eigenvalues are denoted by @xmath104 .",
    "the sequence @xmath105 converges to zero and may be viewed as monotonously decreasing without loss of generality .",
    "those results are also used , for example , in  @xcite .",
    "furthermore , for @xmath36 as for any compact self - adjoint positive operator from @xmath7)$ ] to itself , there exists a unique compact self - adjoint positive operator @xmath106 from @xmath7)$ ] to itself such that @xmath107 ; then @xmath106 is called the square root of @xmath36 .",
    "we have @xmath108 for any @xmath109 .",
    "we may define an empirically accessible version @xmath110 of @xmath36 by replacing the expectation by the average ; more precisely , we have @xmath111 ) .\\ ] ] thus , @xmath110 may be viewed as the operator @xmath36 when @xmath27 equals the uniform distribution on the discrete set @xmath112 .",
    "therefore , all properties derived for @xmath36 in the previous paragraph can be taken over to @xmath110 .",
    "in particular , @xmath113 , integer @xmath109 , denotes the orthonormal basis of the eigenfunctions of @xmath110 with the eigenvalues @xmath114 .",
    "now we consider the conditional probability density @xmath115 of the data @xmath116 given the design functional observations @xmath41 in model ( [ eq:1.1 ] ) .",
    "this density shall be understood with respect to the @xmath24-dimensional lebesgue measure .",
    "we derive that @xmath117 with the vectors @xmath118 and @xmath119 .",
    "moreover , denotes the euclidean norm .",
    "expanding @xmath120)$ ] in the orthonormal basis @xmath121 gives us that @xmath122 we impose the following condition on the distribution @xmath27 : @xmath123 & = & 0 ,    \\qquad   \\mbox{for any deterministic linear subspace } \\nonumber \\\\[-8pt ] \\\\[-8pt ]    & &      \\hspace*{32pt}l \\subseteq l_2([0,1])\\mbox { with } \\dim l <",
    "\\nonumber\\end{aligned}\\ ] ] intuitively , this assumption provides that the probability mass of the @xmath28 fills the whole of @xmath7)$ ] . somehow , ( [ eq : cond_x ] ) is the functional data analog for continuity of a distribution of some real - valued random variables .",
    "it is satisfied when we take an appropriate gaussian process for @xmath124 , for instance .",
    "condition ( [ eq : cond_x ] ) yields that the linear space generated by @xmath41 is @xmath24-dimensional almost surely .",
    "otherwise , at least one of the @xmath28 must be included in the linear hull of the other design variables . according to ( [ eq : cond_x ] ) that occurs with probability zero when employing the conditional probability measure given the data @xmath125 . finally ,",
    "applying the expectation , we obtain the desired result for the unconditional distribution .",
    "we realize that the range of @xmath110 is included in the linear hull of @xmath41 . by definition , @xmath113",
    "is contained in that @xmath24-dimensional space whenever @xmath126 . as the @xmath113 form an orthonormal basis at most @xmath24 of the eigenvalues @xmath114",
    "are nonvanishing .",
    "furthermore , the linear independence of the @xmath41 implies that the functions @xmath127 , @xmath128 , are linearly independent , too , so that the range of @xmath110 is equal to the linear hull of @xmath129 .",
    "clearly , the range of @xmath110 also coincides with the linear hull of all @xmath113 with @xmath126 , from what follows @xmath130 for all @xmath131 and @xmath132 . also , we have @xmath126 for @xmath131 and @xmath133 for @xmath134 . hence , ( [ eq : s2.n.2 ] ) leads to the representation @xmath135 for all @xmath131 .",
    "equation ( [ eq : s2.n.3 ] ) is equivalent to the system of linear equations @xmath136 with the vector @xmath137 and the matrix @xmath138 with the components @xmath139 , @xmath140 .",
    "then the conditional density @xmath141 as in ( [ eq : s2.n.1 ] ) may be written as @xmath142 we consider that the @xmath143th component of the matrix @xmath144 is equal to @xmath145 thus @xmath144 is a diagonal matrix containing @xmath146 as its @xmath147th component .",
    "we denote the diagonal matrix having @xmath148 as its @xmath147th component by @xmath149 .",
    "obviously , @xmath149 is invertible , and we define @xmath150 .",
    "we have @xmath151 where @xmath152 denotes the identity matrix . also",
    ", this yields that @xmath153 and that @xmath154 is an orthogonal matrix .",
    "thus , @xmath155 for any vector @xmath156 .",
    "equality ( [ eq : s2.n.4 ] ) provides that @xmath157    referring to the notation of ( [ eq : aseq ] ) , we consider the expectation @xmath158 where @xmath159 .",
    "we derive that @xmath160 \\\\[-8pt ]     & = &    e { \\int\\cdots\\int}r_{1,n,\\theta}(x_1,\\ldots , x_n;\\mathbf{a}\\mathbf{z } ) ( 2\\pi)^{-n/2 } \\nonumber\\\\ & & \\hphantom{e { \\int\\cdots\\int } } { } \\times\\sigma^{-n }    \\exp \\bigl ( - \\|\\mathbf{z } - \\mathbf{d } \\mathbf{f}\\| ^2 / ( 2\\sigma^2 ) \\bigr ) \\ ,   dz_1 \\cdots",
    "dz_n \\nonumber\\\\     & = &    e r_{1,n,\\theta}(\\mathbf{x},\\mathbf{a}\\mathbf{z } ) , \\nonumber\\end{aligned}\\ ] ] where @xmath161 denotes a vector consisting of independent normally distributed random variables where @xmath162 has the mean @xmath163 and the variance @xmath9 , conditionally on the @xmath91-algebra generated by @xmath164 .",
    "therefore , the @xmath162 may be represented as @xmath165 where @xmath166 are i.i.d . @xmath167-distributed random variables .",
    "the @xmath8 are independent of the @xmath26-algebra generated by @xmath164 .",
    "we have applied the integral transformation @xmath168 where @xmath169 due to the orthogonality of @xmath154 .",
    "note that the sign of the eigenfunctions @xmath170 may still be chosen ; we can arrange that @xmath171 .",
    "now we define the statistical experiment @xmath22 with the same parameter space @xmath12 as @xmath21 , @xmath172 and @xmath173 as the probability measure generated by the random variable @xmath174 with @xmath175 as in ( [ eq : s2.n.6 ] ) . in the notation of section  [ s1.0 ] , paragraph ( i ) , we use the mapping @xmath176 defined by @xmath177 , @xmath178)^{(n)}$ ] , @xmath179 , as the data transformation from @xmath22 to @xmath21 . by definition ,",
    "the matrix @xmath154 does not depend on the parameter @xmath13 but only on the data @xmath41 and the known orthonormal basis @xmath180 .",
    "also , it does not depend on @xmath26 as requested in the previous section .",
    "we have already derived that @xmath154 is an orthogonal matrix so that @xmath181 is a bijective mapping from the set @xmath182)^{(n ) } \\times\\mathbb{r}^n$ ] to itself .",
    "hence , its reverse mapping @xmath183 may be used as the data transformation @xmath184 .",
    "then , according to ( [ eq : aseq ] ) , we have proved the following lemma .",
    "[ l:1 ] under condition ( [ eq : cond_x ] ) , the statistical experiments @xmath21 and @xmath22 are equivalent .",
    "the random variables @xmath8 , integer @xmath185 , as occurring in ( [ eq : nn.1 ] ) , may be represented by @xmath186 where @xmath187 denotes a standard wiener process on @xmath3 $ ] which is independent of @xmath164 .",
    "we deduce that the @xmath188 are an independent sequence of @xmath167-distributed random variables .",
    "moreover , they are independent of @xmath41 although @xmath113 depends on these design variables . that can be shown via the conditional characteristic function of @xmath189 given @xmath129 ; that is , @xmath190   &    = &    \\exp \\biggl(- \\frac12 \\sigma^2   \\biggl\\|\\sum_{j=1}^\\infty s_j \\hat{\\varphi}_j   \\biggr\\| _ 2 ^ 2 \\biggr ) \\\\   &    = &    \\exp \\biggl(- \\frac12 \\sigma^2 \\sum_{j=1}^\\infty s_j^2 \\biggr)\\end{aligned}\\ ] ] for all real - valued sequences @xmath191 with @xmath192 . applying the expectation to the above equality , the unconditional characteristic function of @xmath193 turns out to coincide with the conditional one .",
    "we have @xmath194 for all @xmath131 where @xmath195 , @xmath30 $ ] , denotes an it process satisfying @xmath196(t)\\,dt   + \\sigma\\ , dw(t ) , \\ ] ] and @xmath197 .",
    "the differential @xmath198 shall be understood in the it sense .",
    "now we define the statistical experiment @xmath199 with a completely functional observation structure .",
    "we fix that @xmath200)^{(n+1)}$ ] with the corresponding borel @xmath26-algebra @xmath201 .",
    "the probability measure @xmath202 is defined via the observation of @xmath164 as in @xmath22 and the it process @xmath195 , @xmath30 $ ] , as defined in  ( [ eq : s2.n.7 ] ) . the definition ( [ eq : s2.n.6.1 ] ) of @xmath203 can be extended to @xmath134 straightforwardly . as @xmath204",
    ", we obtain that @xmath205 moreover , @xmath195 is uniquely determined by the @xmath203 for all integers @xmath109 and vice versa . that can be seen as follows : @xmath206}(s)\\,dz(s )    =    \\sum_{j=1}^\\infty\\langle 1_{[0,t ] } , \\hat{\\varphi}_j \\rangle   z_j\\ ] ] for all @xmath30 $ ] where the infinite sum must be understood as an @xmath207-limit .",
    "that seems to cause some troubles as we only observe one element of the probability space .",
    "however , convergence in probability implies almost sure convergence of a subsequence so that @xmath195 is fully accessible by the observation of all @xmath203 .",
    "on the other hand , by a similar argument , all @xmath203 are accessible ( in practice , that means approximable arbitrarily precisely ) by a trajectory of the process @xmath208 .",
    "hence the data set @xmath209 is independent of the @xmath210 , conditionally on the @xmath26-algebra generated by @xmath164 .",
    "furthermore , the distribution of the @xmath203 , @xmath211 , does not depend on the target parameter @xmath13 so that @xmath203 , for @xmath134 , does not contain any information about @xmath13 .",
    "we conclude that @xmath212 is a sufficient statistic for the observation scheme in the experiment @xmath199 .",
    "we can utilize result ( ii ) from section  [ s1.0 ] in order to prove equivalence of the experiments @xmath22 and @xmath213 . considering paragraph ( iii ) from section  [ s1.0 ]",
    ", we may establish equivalence of the experiments @xmath21 and @xmath199 .",
    "this result is presented in the following theorem .",
    "[ t:1 ] under condition ( [ eq : cond_x ] ) , the flr statistical experiment @xmath21 is equivalent to the model @xmath199 where one observes @xmath164 and the it process @xmath195 , @xmath214 $ ] , as defined in ( [ eq : s2.n.7 ] ) .",
    "in the previous section , we have derived a statistically equivalent white noise model for the flr problem .",
    "however , the it process @xmath208 in ( [ eq : s2.n.6.1 ] ) contains the noisy operator @xmath110 in its construction . in the current section",
    ", we will replace it by the covariance operator @xmath36 .    for that purpose",
    ", we split the original experiment @xmath21 into two independent parts @xmath215 and @xmath216 where @xmath215 is based on the observation of the data @xmath217 , @xmath218 , and @xmath219 consists of the residual data @xmath217 , @xmath220 .",
    "the selection of the integer parameter @xmath221 is deferred .",
    "the strategy of splitting the sample in the current context leans on @xcite . applying theorem  [ t:1 ] to each of the experiments @xmath222 , @xmath63",
    ", we obtain equivalence of @xmath223 and the experiments @xmath224 for @xmath63 .",
    "therein , @xmath225 is defined by the observation of @xmath226 and the it process @xmath227 , @xmath214 $ ] , specified by @xmath228 and @xmath229(t)\\,dt +    \\sigma\\ , dw_1(t ) , \\ ] ] and accordingly @xmath230 is defined by the observation of @xmath231 and the it process @xmath232 , @xmath233 $ ] , specified by @xmath234 and @xmath235(t)\\,dt    +    \\sigma\\ , dw_2(t ) .\\ ] ] furthermore , @xmath236 , @xmath63 , denotes the empirical covariance operator constructed by the data @xmath237 and @xmath238 , respectively .",
    "also note that @xmath239 and @xmath240 are two independent standard wiener processes . using criterion ( iv ) in section  [ s1.0 ] , the experiment @xmath241 , which combines the independent experiments @xmath225 and",
    "@xmath230 , we deduce that @xmath241 and @xmath21 are equivalent .    from the experiment",
    "@xmath225 we construct an estimator @xmath242 for @xmath13 .",
    "we define that @xmath243(t)\\,dz_1(t )    \\varphi_k , \\ ] ] where @xmath34 is an integer - valued smoothing parameter still to be selected .",
    "condition ( [ eq : cond_x ] ) guarantees that all @xmath244 are positive since , otherwise , @xmath245 would yield that @xmath246 a.s . so that @xmath124 would lie in the linear hull of @xmath247 .",
    "thus the estimator @xmath242 is well defined .",
    "we introduce the data transformation @xmath248 where @xmath249(t)\\,dt\\\\ & & \\hspace*{62pt } \\qquad   \\quad { } + ( n - m)^{1/2 } \\int_0^\\cdot[\\gamma ^{1/2 } \\hat{\\theta}_1](t)\\,dt \\biggr ) .\\end{aligned}\\ ] ] the transformation is fully accessible by the data drawn from the experiment @xmath225 and the assumed knowledge of the distribution of @xmath164 .",
    "we set @xmath250 where @xmath251 ) \\times c_0([0,1 ] ) \\times c_0^{n - m}([0,1 ] ) \\times c_0([0,1])$ ] and @xmath252 is the corresponding borel @xmath26-algebra .",
    "the data structure of @xmath241 is represented by @xmath253 when inserting the data set as an argument of the mapping @xmath254 .",
    "note that @xmath255 may be inserted in the definition of the estimator @xmath242 .",
    "the integral occurring in the definition of @xmath242 is not defined for all continuous functions @xmath256 but for almost all trajectories of @xmath257 . for the other negligible trajectories",
    "the integral may conventionally be put equal to zero to make the mapping @xmath254 well defined on the whole of its domain .",
    "we define the experiment @xmath258 where ones observes the data @xmath259 where the data @xmath260 are obtained under experiment @xmath241 .",
    "the experiment @xmath261 is defined on the probability space @xmath262 . considering the definition of @xmath254",
    ", we realize that the shift contained in the forth component is still available in the experiment @xmath258 as the other components are kept .",
    "therefore , @xmath254 is an invertible transformation so that the experiments @xmath241 and @xmath258 are equivalent .    in the experiment @xmath258 ,",
    "the component @xmath263 is still an it process conditionally on @xmath264 .",
    "now we introduce the experiment @xmath265 with @xmath266 where one observes the data @xmath267 and the it process @xmath268 , @xmath30 $ ] with @xmath269 and @xmath270(t)\\,dt    + \\sigma\\ , dw_2(t ) .\\ ] ] in the notation of section  [ s1.0 ] , we consider that @xmath271 \\\\[-8pt ]   & & \\qquad    \\leq   2   e   \\biggl\\{1 - \\exp \\biggl(- \\frac1{2\\sigma^2 } \\|\\delta_{5,6}\\| _ 2 ^ 2 \\biggr ) \\biggr\\}^{1/2}\\nonumber \\\\    & & \\qquad     \\leq   2   \\biggl\\{1 - \\exp   \\biggl ( - \\frac1{2\\sigma^2 } e \\|\\delta_{5,6}\\|_2 ^ 2 \\biggr ) \\biggr\\}^{1/2 } , \\nonumber\\end{aligned}\\ ] ] where @xmath272 therein , we have used girsanov s theorem , @xmath273 for @xmath274 as @xmath275 , the bretagnolle  huber inequality and jensen s inequality in the last step .",
    "now we study the expectation occurring in ( [ eq:4.1.t ] ) by parseval s identity with respect to the basis @xmath276 and the orthogonal expansion of @xmath277 with respect to @xmath278 where @xmath276 denotes the eigenfunctions of @xmath279 and @xmath280 the corresponding eigenvalues .",
    "@xmath281 where we have used the cauchy ",
    "schwarz inequality for sums and the elementary inequality @xmath282 for all @xmath283 .",
    "therein @xmath284 is still to be selected .",
    "also , the independence of @xmath279 and @xmath242 has been utilized .",
    "then , we apply the cauchy ",
    "schwarz inequality with respect to the discrete random variable @xmath285 satisfying @xmath286 = |\\langle\\varphi_j , \\hat{\\varphi}_{k,2}\\rangle|^2 $ ] for all integers @xmath287 and some fixed integer @xmath185 , conditionally on @xmath238 .",
    "we conclude that @xmath288 we consider that @xmath289 \\nonumber\\\\ &    \\leq & \\mathrm{const.}\\cdot(n - m)^{-1 }   \\bigl(c_j^2 \\lambda_j    + c_{x,0 } \\exp(-c_{x,1 } c_j^{c_{x,2 } } /2 )",
    "\\bigr )   \\nonumber\\end{aligned}\\ ] ] for @xmath24 sufficiently large where the sequence @xmath290 remains to be determined . in order to obtain those results ,",
    "we impose the following :    [ condx ] we assume that condition ( [ eq : cond_x ] ) holds true ; @xmath291 for all integer @xmath109 and some @xmath292 ; @xmath293 ; @xmath294    \\leq   c_{x,0 } \\exp(-c_{x,1 } x^{c_{x,2}})$ ] for all @xmath16 and some finite constants @xmath17 .",
    "condition  [ condx ] imposes a polynomial lower bound on the sequence of the eigenvalues of @xmath36 .",
    "this assumption is very common in flr ( see , e.g. ,  @xcite ) .",
    "when condition  [ condx ] is fixed the underlying inverse problem can be viewed as a moderately ill - posed problem unlike severely ill - posed problems where exponential decay of the eigenvalues occurs .",
    "condition  [ condx ] also corresponds to the deconvolution setting with ordinary smooth error densities in the related field of density estimation based on contaminated data .",
    "as an example for a stochastic process which satisfies condition  [ condx ] , we mention the random variables @xmath295 where the @xmath296 , integer @xmath185 , form an arbitrary orthonormal basis of @xmath7)$ ] ; the @xmath297 are i.i.d . real - valued centered random variables with a continuous distribution which is concentrated on some compact interval , and @xmath298 .",
    "we stipulate that @xmath299 .",
    "easy calculations yield that the coefficients @xmath300 and @xmath296 are the eigenvalues and the eigenvectors of the corresponding covariance operator @xmath36 , respectively . stipulating that the sequence @xmath301 is bounded above ( as satisfied , e.g. , by the fourier polynomials )",
    ", we can show that condition  [ condx ] is fulfilled . in particular , the random variable @xmath302 is continuously distributed for any @xmath303)\\setminus \\{0\\}$ ] since @xmath304 for at least one integer @xmath185 so that the distribution of @xmath302 is just the convolution of an absolutely continuous distribution and some other distribution ; hence the distribution of @xmath305 has a lebesgue density so that condition ( [ eq : cond_x ] ) can be verified .",
    "all other assumptions contained in condition  [ condx ] can easily be checked .",
    "another even more important example for design distributions are the gaussian processes @xmath306 , @xmath42 $ ] , where @xmath187 denotes a standard wiener process and @xmath26 is a sufficiently smooth function which is bounded from above and below by positive constants .",
    "these processes satisfy condition  [ condx ] as well where @xmath307 .",
    "the decay condition can be verified via the famous reflection principle of wiener processes .",
    "returning to the investigation of an upper bound on @xmath308 , the following inequality is evident : @xmath309 \\\\[-8pt ]   & & { }    \\times \\sum _ { j=1}^\\infty j^{-\\gamma }   \\bigl(c_j^2 \\lambda_j    +    c_{x,0 } \\exp ( -c_{x,1 } c_j^{c_{x,2 } } ) \\bigr)^{1/2 } . \\nonumber\\end{aligned}\\ ] ] we deduce that @xmath310(t)\\,dz_1(t ) \\biggr|^2 \\nonumber\\\\ & & \\qquad   = 1_{\\{j ' > k\\ } }   |\\langle\\varphi_{j ' } , \\theta\\rangle |^2 \\nonumber\\\\ & & \\qquad   \\quad    { } + 1_{\\{j'\\leq k\\}}\\nonumber\\\\ & & \\hphantom { { } + } \\qquad   \\quad { } \\times e   \\biggl|\\lambda_{j'}^{-1 } \\langle \\hat{\\gamma}_1 \\theta,\\varphi_{j'}\\rangle- \\langle\\theta,\\varphi_{j ' } \\rangle+ \\sigma m^{-1/2 } \\lambda_{j'}^{-1 } \\int[\\hat{\\gamma}_1^{1/2 } \\varphi_{j'}](t)\\,dw_1(t ) \\biggr|^2 \\nonumber\\\\ & & \\qquad   = 1_{\\{j ' > k\\ } }   |\\langle\\varphi_{j ' } , \\theta\\rangle |^2 \\nonumber \\\\[-8pt ] \\\\[-8pt ] & & \\qquad   \\quad   { } + 1_{\\{j'\\leq k\\ } } \\lambda_{j'}^{-2 }   \\{e   | \\langle\\hat{\\gamma}_1 \\theta,\\varphi_{j'}\\rangle- \\langle\\gamma \\theta,\\varphi_{j ' } \\rangle |^2 + \\sigma^2 m^{-1 } e \\|\\hat{\\gamma } _ 1^{1/2 } \\varphi_{j ' } \\|_2 ^ 2 \\}\\nonumber \\\\ & & \\qquad   \\leq 1_{\\{j ' > k\\ } }   |\\langle\\varphi_{j ' } , \\theta\\rangle    & & \\qquad   \\quad { } + 1_{\\{j'\\leq k\\ } } \\lambda_{j'}^{-2 }   \\{m^{-1 } \\|\\theta\\|_2 ^ 2 e \\| x_1\\|_2 ^ 2 |\\langle x_1,\\varphi_{j'}\\rangle|^2 + \\sigma^2 m^{-1 }",
    "\\lambda _ { j ' } \\ } \\nonumber\\\\   & & \\qquad   \\leq 1_{\\{j ' > k\\ } }   |\\langle\\varphi _ { j ' } , \\theta\\rangle |^2 \\nonumber\\\\   & & \\qquad   \\quad   { } +    1_{\\ { j'\\leq k\\}}\\nonumber\\\\ & & \\hphantom { { } + } \\qquad   \\quad { } \\times \\lambda_{j'}^{-2 } \\bigl \\{m^{-1 } \\|\\theta\\|_2 ^ 2   \\bigl(c_{j'}^2 \\lambda_{j ' }    +    c_{x,0 } \\exp(-c_{x,1 } c_{j'}^{c_{x,2}}/2 ) \\bigr ) + \\sigma^2 m^{-1 } \\lambda_{j ' } \\bigr\\ } .",
    "\\nonumber\\end{aligned}\\ ] ]    for further investigation of the asymptotic quality of the estimator @xmath242 , some conditions on @xmath27 and @xmath12 are required .",
    "they are stated such that  combined with condition  [ condx]all previously imposed assumptions concerning those characteristics are included .",
    "[ condt ] we assume that @xmath311 for all @xmath55 and some constants @xmath312 and @xmath313 , which are uniform with respect to @xmath55 .",
    "condition  [ condt ] says that the @xmath55 are uniformly well approximable with respect to the orthonormal basis consisting of the eigenfunctions of @xmath36 .",
    "the parameter @xmath314 describes the degree of this approximability .",
    "if the @xmath315 were some fourier polynomials , then condition  [ condt ] could be interpreted as sobolev constraints on the set of the target functions .",
    "we apply the parameter selection @xmath316 , and we fix that @xmath317 with some constants @xmath318 sufficiently large and that @xmath319 .",
    "also , we choose @xmath320 . inserting that result into ( [ eq:4.3.t ] )",
    ", we deduce by conditions  [ condx ] and  [ condt ] that @xmath321 for some @xmath322 , due to the inequality @xmath323 and the suitable selection of @xmath324 . revisiting inequality ( [ eq:4.1.t ] ) ,",
    "we have finally proved by section  [ s1.0 ] , paragraph ( i ) that the experiments @xmath258 and @xmath265 are asymptotically equivalent .    in the experiment @xmath265 , the observation of @xmath325 allows us to construct an estimator @xmath326 for @xmath13 as well .",
    "it is given by @xmath327(t)\\,ds_2(t ) \\varphi_k , \\ ] ] where the parameter @xmath34 can be adopted from the estimator @xmath242 .",
    "we specify the transformation @xmath328 with @xmath329(t)\\,dt + m^{1/2 } \\int_0^\\cdot[\\gamma^{1/2 } \\hat{\\theta}_2](t)\\,dt,\\mathbf{x}_2,s_2 \\biggr ) .\\end{aligned}\\ ] ] again the shift of the second component is accessible by the other components which are maintained under the mapping so that @xmath330 is invertible",
    ". therefore , we define the experiment @xmath331 by the observation of @xmath332 with @xmath333 as under the experiment @xmath265 . hence , we put @xmath334 and obtain that @xmath331 is equivalent to @xmath265 .",
    "we define the experiment @xmath335 by the observation of @xmath336 on the probability space @xmath337 where @xmath338 denotes the it process with @xmath339 and @xmath340(t)\\,dt    + \\sigma\\ , dw_1(t ) .\\ ] ] we can show that @xmath335 is asymptotically equivalent to @xmath331 analogously to the proof of the asymptotic equivalence of @xmath258 and @xmath341 .",
    "the only remarkable modification concerns the application of the estimator @xmath326 instead of @xmath242 .",
    "however , even for that term we establish an upper bound at the same rate as for estimator @xmath242 since the asymptotic order of @xmath221 and @xmath342 coincide .",
    "taking a closer look at the data drawn from @xmath335 , we realize that the random variables @xmath343 are independent .",
    "that occurs as we have replaced the empirical covariance operators by the true deterministic one .",
    "furthermore the data sets @xmath344 do not carry any information on @xmath13 so that @xmath345 represent a sufficient statistic for the whole empirical information obtained under @xmath335 . by section  [ s1.0 ] , paragraph ( ii ) , we conclude that @xmath335 is equivalent to the experiment @xmath346 in which only the observations @xmath345 are available .",
    "thus we put @xmath347 ) \\times c_0([0,1])$ ] and @xmath348 equal to the corresponding borel @xmath26-algebra .",
    "we define the transformation @xmath349 with @xmath350 by @xmath351 with the matrix @xmath352 we easily verify that @xmath154 is invertible so that the experiment @xmath353 which is defined by the observation of @xmath354 is equivalent to the experiment @xmath346 .",
    "we consider the characteristic function of the @xmath7)\\times l_2([0,1])$]-valued random variable @xmath355 ,",
    "@xmath356(t)\\,dt \\biggr\\rangle \\biggr ) \\\\   & & \\quad \\qquad { } \\times\\exp \\biggl(-\\frac1{2n } \\sigma^2 { \\int\\!\\!\\int}t_1(x_1 ) \\min\\{x_1,x_2\\ } t_1(x_2)\\,dx_1 \\,dx_2 \\biggr ) \\\\ & & \\quad \\qquad { } \\times\\exp \\biggl(-\\frac1{2 } \\biggl[\\frac1 m + \\frac1{n - m } \\biggr ] \\sigma ^2 { \\int\\!\\!\\int}t_2(x_1 ) \\min\\{x_1,x_2\\ } t_2(x_2)\\,dx_1 \\,dx_2 \\biggr)\\end{aligned}\\ ] ] for any @xmath357)$ ] so that @xmath358 and @xmath359 are two it processes satisfying @xmath360 and @xmath361(t)\\,dt    +    n^{-1/2 } \\sigma\\ , dw_3(t ) , \\\\",
    "dt_2(t ) &    = &   \\sigma \\biggl(\\frac1 m + \\frac1{n - m } \\biggr)^{1/2}\\,dw_4(t ) , \\end{aligned}\\ ] ] where @xmath362 and @xmath363 are two independent wiener processes .",
    "thus @xmath358 and @xmath359 are independent , and @xmath359 is totally uninformative with respect to the target function  @xmath13 . applying section  [ s1.0 ] , paragraph ( ii ) again , we have established equivalence of @xmath353 and the experiment @xmath364 , which is equipped with @xmath365)$ ] and the corresponding borel @xmath26-algebra @xmath366 , and characterized by the observation of the process @xmath358 , which coincides with the process @xmath367 as defined in ( [ eq:1.1.2 ] ) .",
    "summarizing we have shown asymptotic equivalence of the experiments @xmath21 and @xmath368 .",
    "that provides our final main result , which will be given as a theorem below .",
    "[ t:2 ] under the conditions  [ condx ] and  [ condt ] , the flr experiment @xmath21 with known design distribution and independent @xmath369-distributed regression errors is asymptotically equivalent to the white noise experiment @xmath368 where only the it process @xmath367 as in ( [ eq:1.1.2 ] ) is observed .",
    "we can combine our results with theorem 1 in @xcite , which is due to  @xcite , in order to derive a sharp minimax result with respect to the mise for the flr problem under known covariance operator .",
    "it follows from there that this sharp minimax risk corresponds to the sequence @xmath370 where @xmath324 is the unique solution of the equation @xmath371 under the conditions of theorem  [ t:2 ] .",
    "more concretely , there exists an estimator @xmath372 of @xmath13 in the flr model , which satisfies @xmath373 thus , any other estimator in the underlying model satisfies the above equation when @xmath374 is replaced by @xmath375 .",
    "we have established sharp asymptotic constants .",
    "critically , we mention that the loss function @xmath376 is apparently not bounded .",
    "still , asymptotic equivalence yields coincidence of sharp minimaxity for the loss function @xmath377 for some @xmath378 sufficiently slowly .",
    "we can show that , in the white noise inverse problem , the sharp constant result is extendable to the truncated loss function . using theorem  [ t:2 ] , we have a sharp lower bound for the flr model even for the truncated loss function .",
    "however , the design distribution @xmath27 is assumed to be known and occurs in the definition of the minimax estimator . on the other hand , the lower bound as derived from theorem  [ t:2 ] in the previous section",
    "provides a lower bound for the flr model in the case of unknown @xmath27 as well since nonknowledge of @xmath27 can not improve this lower bound .",
    "thus if we succeed in showing that some estimator achieves these asymptotic properties under the assumption of unknown @xmath27 , then sharp asymptotic minimaxity is extended to this more realistic condition .",
    "assuming that all conditions of theorem  [ t:2 ] except the knowledge of @xmath27 hold true , we propose the estimator @xmath379 for @xmath13 where @xmath380 for some @xmath381 .",
    "the weights @xmath382 remain to be specified . using the techniques of the papers of @xcite and  @xcite , the mise of @xmath383 is equal to @xmath384 we stipulate that for @xmath385 all weights @xmath386 shall be put equal to zero . for all other @xmath387",
    "we have @xmath388 for @xmath24 sufficiently large so that @xmath389 \\\\[-2pt ] &    \\leq &   \\lambda_k^{-1 } + \\lambda_k^{-1 }   \\biggl(\\frac1{\\log k + 1 } + ( 2+\\log k)^2 o(n^{2\\rho-1 } ) \\biggr ) , \\end{aligned}\\ ] ] where we have used markov s inequality and that @xmath390 is bounded from above by the expected squared hilbert ",
    "schmidt norm of @xmath391 and , hence , by @xmath392 ( see , e.g. ,  @xcite ) . that requires the following assumption : @xmath393 ( see also  @xcite ) .",
    "we conclude that the second term in equation ( [ eq : sh2 ] ) has the same asymptotic order as @xmath394 under the above restriction with respect to the selection of the weights .",
    "focusing on the first term in ( [ eq : sh2 ] ) , we deduce by the cauchy  schwarz inequality that @xmath395 & & \\hphantom { \\biggl\\ { \\biggl ( } { }    +    \\mathrm{const.}\\cdot \\biggl(\\sum_k e   |\\langle\\hat { \\varphi}_k - \\varphi_k,\\theta\\rangle |^2 \\biggr)^{1/2 } \\biggr\\}^2 .\\end{aligned}\\ ] ] we consider that @xmath396 &    \\leq &   \\mathrm{const.}\\cdot c_\\theta\\sum_{k , j } j^{-2\\beta } e    &    = &   \\mathrm{const.}\\cdot\\sum_j j^{-2\\beta } e |\\langle\\hat { \\varphi}_j - \\varphi_j , \\varphi_j\\rangle |^2 \\nonumber\\\\[-2pt ] & & { }   +    \\mathrm { const.}\\cdot\\sum_j j^{-2\\beta } \\sum_{k\\neq j } e |\\langle\\hat { \\varphi}_k - \\varphi_k , \\varphi_j\\rangle |^2 \\\\[-2pt ] &    \\leq &   \\mathrm{const.}\\cdot\\sum_j j^{-2\\beta } e\\|\\hat{\\varphi}_j - \\varphi_j \\|_2 ^ 2 \\nonumber\\\\[-2pt ] & & { }   +    \\mathrm{const.}\\cdot\\sum_j j^{-2\\beta } \\sum _ { k\\neq j } e |\\langle\\hat{\\varphi}_k , \\hat{\\varphi}_j - \\varphi _",
    "j\\rangle |^2 \\nonumber\\\\[-2pt ]   \\nonumber &    \\leq & \\mathrm{const.}\\cdot\\sum_j j^{-2\\beta } e\\|\\hat{\\varphi}_j - \\varphi_j \\|_2 ^ 2   \\nonumber\\end{aligned}\\ ] ] by exploiting the orthonormality of the @xmath113 and the @xmath296 as well as condition  [ condt ] and parseval s identity .",
    "bhatia , davis and mcintosh  @xcite provide that the squared @xmath7)$]-distance between @xmath113 and @xmath296 is bounded from above by the squared hilbert ",
    "schmidt norm of @xmath397 multiplied by @xmath398 via condition  ( [ eq : as01 ] ) .",
    "thus we have @xmath399 do not depend on @xmath13 whenever @xmath400 returning to the consideration of the first term in ( [ eq : sh2 ] ) , we conclude that its asymptotic order reduces to that of @xmath401 then this term is bounded from above by @xmath402   & & \\hphantom{\\sum_k   |w_k - 1 |^2 |\\langle\\varphi_k,\\theta\\rangle|^2    +    \\mathrm{const.}\\cdot\\sum_{k=1}^{\\lfloor n^{\\rho/\\alpha}/\\log n\\rfloor } }    { } +    o ( n^{-2\\beta\\rho/\\alpha } ( \\log n)^{2\\beta } ) \\\\[-2pt ] & & \\qquad    \\leq   o ( n^{-2\\beta\\rho/\\alpha } ( \\log n)^{2\\beta } )   +    \\mathrm{const.}\\cdot c_\\theta / n    +    \\sum_k   |w_k - 1 |^2    condition  [ condt ] and again the results of @xcite .",
    "the term @xmath403 is asymptotically negligible [ i.e. , bounded by @xmath404 whenever @xmath405 as we have already imposed the condition ( [ eq : sh4 ] ) .",
    "it follows that the mise of  ( [ eq : sh1 ] ) may be reduced to its asymptotically efficient terms , that is , @xmath406 \\\\[-9pt ] & & { } + o ( n^{-1 } \\log^{\\alpha+1}n ) .",
    "\\nonumber\\end{aligned}\\ ] ] the right - hand side of ( [ eq : sh5 ] ) , however , corresponds to the mise of an oracle estimator which uses the true versions of the eigenvalues and eigenfunctions of @xmath36 instead of the empirical ones .",
    "also , it follows from  @xcite and @xcite that the estimator @xmath372 as in ( [ eq : sh1 ] ) attains the sharp asymptotic minimax risk when the weights are chosen as @xmath407 when writing @xmath408 with an appropriate deterministic parameter  @xmath324 .",
    "more precisely , we consider @xmath409 which we define by the unique zero of the function @xmath410 with @xmath411 for @xmath412 where @xmath413 and @xmath414 are continuous montonically decreasing and increasing , respectively .",
    "the selection @xmath415 leads to asymptotic sharp optimality ( see , e.g. , @xcite ) .",
    "clearly , we have @xmath416 . otherwise , not even the convergence rates are optimal as the required balance between the bias and the variance term is violated . by condition ( [ eq : sh4 ] ) our additional assumption saying that that @xmath417 for @xmath418 is verified under this optimal selection of the weights when stipulating that @xmath419 as we have assumed @xmath420 .",
    "still , the suggested selector is an oracle choice as it requires knowledge of the true eigenvalues @xmath421 .",
    "that motivates us to consider a data - driven selector @xmath422 of @xmath324 .",
    "first we split the sample @xmath0 into two independent data sets @xmath423 , @xmath424 .",
    "the first data set @xmath425 consists of @xmath221 pairs @xmath426 where @xmath427 , and @xmath428 contains all the other observations .",
    "we employ @xmath425 to estimate the function @xmath13 while the second data set ( training data ) is used to provide an selector of @xmath324 .",
    "concretely , we fix @xmath429 as the unique zero of @xmath430 where @xmath431 therein , @xmath432 indicates that the estimator is based on the second data set .",
    "then we define our selector of @xmath422 as @xmath433 .",
    "this truncation takes into account the a priori knowledge about the true @xmath409 so that @xmath434 almost surely for @xmath24 sufficiently large .",
    "thus determining @xmath422 does not require knowledge of @xmath27 .",
    "now let us consider the mise of the estimator @xmath435 where the index indicates the incorporated choice of the parameter @xmath324 . by ( [ eq : sh5 ] )",
    ", we derive that @xmath436 where the terms contained in @xmath437 do not depend on @xmath438 . as the asymptotic order of @xmath221 and @xmath24 coincides , the estimator based on @xmath221 data attains the same asymptotic rates and constants as the estimator which uses even @xmath24 data , so our above calculations remain valid .",
    "therefore , the estimator @xmath439 attains sharp minimax rates and constants whenever @xmath440 uniformly with respect to @xmath55 . the first term in ( [ eq : nnew1 ] ) is bounded from above by @xmath441 .",
    "the second term has the upper bound @xmath442\\ ] ] for some sequence @xmath443 tending to infinity sufficiently slowly .",
    "we deduce by markov s inequality that ( [ eq : nnew1 ] ) is satisfied if @xmath444 for some fixed integer @xmath445 .",
    "the assertion @xmath446 , for some positive - valued sequence @xmath447 with @xmath448 , implies that @xmath449 or @xmath450 we have already imposed that @xmath451 so that @xmath452 for all @xmath453 . that , however , yields @xmath454 where @xmath455 denotes the hilbert  schmidt norm of an operator . therein we have used the findings of @xcite again and the monotonicity of the functions @xmath456 , @xmath413 , @xmath414 as well as the definitions of @xmath409 and  @xmath429 .",
    "we deduce by markov s inequality that @xmath457 \\\\   & = &   s_n^{2\\nu } + \\mathrm{const.}\\cdot n^{2\\rho\\mu } s_n^{-2\\mu } \\gamma _ n^{2\\mu }    e \\|\\hat{\\gamma } ' - { \\gamma } \\|_{\\mathrm{hs}}^{2\\mu}\\end{aligned}\\ ] ] for any integer @xmath458 . as all moments of @xmath100",
    "are finite by condition  [ condt ] we derive that @xmath459 where we recall that @xmath460 is based on the training data set , thus on @xmath461 observations .",
    "as @xmath462 we conclude by suitable choice of @xmath463 that @xmath464^{2\\nu } ) , \\ ] ] where @xmath465 denotes some sequence tending to zero at an algebraic rate .",
    "choosing @xmath445 sufficiently large , we can finally verify ( [ eq : nnew1 ] ) yielding the following proposition which summarizes the investigation carried out in this section .",
    "[ p:1 ] we consider the flr model in the setting of theorem  [ t:2 ] except the condition that @xmath27 is known .",
    "in addition , suppose that ( [ eq : as01 ] ) , ( [ eq : sh4 ] ) and @xmath466 .",
    "then , estimator ( [ eq : sh1 ] ) with the weight selector ( [ eq : weight ] ) , which does not use @xmath27 in its definition , attains the sharp minimax rate and constant with respect to the mean integrated squared error ; viewed uniformly over the function class @xmath12 which is defined via condition  [ condt ] .",
    "hence , under some additional conditions on the model , we have established sharp minimaxity in the case where @xmath27 is unknown . only an arbitrary number between @xmath467 and @xmath468",
    "is supposed to be known .",
    "we have proved equivalence of the flr model and a white noise model involving an empirical covariance operator in theorem  [ t:1 ] .",
    "we mention that @xmath26 and @xmath27 can be treated as real nuisance parameters in section  [ s2 ] ; more precisely , knowledge of those quantities is not needed to apply the data transformations .",
    "in contrast , for the asymptotic approximation in section  [ s3 ] , @xmath27 must be known .",
    "nevertheless , section  [ s : new ] shows that , with respect to the mise , the sharp asymptotic minimax risk can be taken over to the case of unknown design distribution .",
    "furthermore , under specific parametric assumption on @xmath27 , the condition of known @xmath27 can obviously be justified .",
    "cai and hall  @xcite explicitly mention gaussian processes as examples for the random design functions @xmath28 .",
    "for instance , assuming that @xmath28 can be represented as @xmath469 with independent standard wiener processes @xmath470 as already suggested in the previous section , we realize that the function @xmath471 is precisely reconstrucable based on only one observation @xmath124 .",
    "then as @xmath471 is known the distribution @xmath27 is known as well . therefore , under this shape of @xmath27 , the assumption of known @xmath27 is not unrealistic at all .",
    "this phenomenon is typical for the functional data approach and does not occur in multivariate linear regression with finite - dimensional covariates . from that point of view",
    ", the assumption of known design distribution causes less trouble in flr compared to more standard regression problems . still this does not address the completely nonparametric case for @xmath27 and @xmath13 .    as an interesting restriction",
    ", we have assumed that @xmath472 in condition  [ condt ] .",
    "therefore , the quality of the approximation of the target curve @xmath13 in the orthonormal basis consisting of the eigenvalues of the covariance operator of the design variables must be sufficiently high .",
    "if this basis consisted of fourier polynomials then that assumption could be interpreted as a smoothness condition on  @xmath13 .",
    "that corresponds to the theorems in  @xcite and @xcite where hlder conditions are imposed , which correspond to @xmath473 , in order to prove asymptotic equivalence of the white noise model on one hand and density estimation and nonparametric regression on the other hand .",
    "otherwise , counterexamles can be constructed ( see  @xcite ) . to our best knowledge",
    "our work represents the first proof of white noise equivalence in a statistical inverse problem .",
    "it seems reasonable that the essential condition is extended to @xmath474 in this setting as the selection @xmath475 describes the setting of direct estimation ( noninverse problems ) .",
    "still , the question of whether our results are extendable to some @xmath476 remains open . in section  [",
    "s : new ] we have studied the case of unknown @xmath27 ; however , the regularity parameter @xmath314 is still assumed to be known .",
    "therefore , another interesting problem , which can not be addressed within the framework of this paper , is whether this sharp risk can be achieved by an adaptive estimator , which does not use @xmath314 and @xmath477 in its construction .",
    "approaches to adaptivity in flr are studied in  @xcite ; however , that report seems to focus on optimal rates rather than optimal constants .",
    "also , combining theorem  [ t:2 ] and the results of brown and low @xcite , we conclude that , under reasonable conditions , the flr model is also equivalent to the standard nonparametric regression problem , under which the data @xmath478(x_j )    +    \\sigma \\varepsilon_j , \\qquad   j=1,\\ldots , n , \\ ] ] are observed where the @xmath8 are i.i.d . and @xmath167-distributed , and the homogeneous fixed design setting @xmath479 , @xmath131 , is applied .",
    "the author is grateful to markus reiss for a discussion on this paper and to the reviewers for their inspiring comments ."
  ],
  "abstract_text": [
    "<S> we consider the statistical experiment of functional linear regression ( flr ) . </S>",
    "<S> furthermore , we introduce a white noise model where one observes an it process , which contains the covariance operator of the corresponding flr model in its construction . </S>",
    "<S> we prove asymptotic equivalence of flr and this white noise model in lecam s sense under known design distribution . </S>",
    "<S> moreover , we show equivalence of flr and an empirical version of the white noise model for finite sample sizes . as an application </S>",
    "<S> , we derive sharp minimax constants in the flr model which are still valid in the case of unknown design distribution .    .    </S>"
  ]
}