{
  "article_text": [
    "over a petabyte of raw astronomical data is expected to be collected in the next decade ( see szalay & gray 2001 ) .",
    "this explosion of data also extends to the volume of parameters measured from these data including their errors , quality flags , weights and mask information .",
    "furthermore , these massive datasets facilitate more complex analyses , e.g. nonparametric statistics , which are computationally intensive . a key question",
    "therefore is : can existing statistical software scale - up to cope with such large datasets and massive calculations ?",
    "we address this question here by focusing on two exciting new technologies , namely the virtual observatory ( vo ) and computational grids .",
    "as a case study of the types of massive calculations planned for the next generation of astronomical surveys and analyses , we discuss here the galaxy n - point correlation functions .",
    "these have a long history in cosmology and are used to statistically quantify the degree of spatial clustering of a set of data points ( e.g. galaxies ) .",
    "there are a hierarchy of correlation functions , starting with the 2-point correlation function , which measures the joint probability of a data pair , as a function of their separation @xmath0 , compared to a poisson distribution ,    @xmath1    where @xmath2 is the joint probability of an object being located in both search volumes @xmath3 & @xmath4 , and @xmath5 is the space density of objects .",
    "@xmath6 is the 2-point correlation function and is zero for a poisson distribution .",
    "if @xmath6 is positive , then the objects are more clustered on scales of @xmath0 than expected , and vica versa for negative values .",
    "the next in the series is the 3-point correlation function , which is defined as ,    @xmath7    where @xmath8 are the 2-point functions for the three sides ( @xmath9 ) of the triangle and @xmath10 is the 3point function .",
    "likewise , one can define a 4-point , 5-point etc . , correlation function .",
    "the reader is referred to peebles ( 1980 ) for a full discussion of these n - point correlation functions including their importance to cosmology ( see also the recent lecture notes of szapudi 2005 ) .",
    "we also refer the reader to landy & szalay ( 1993 ) and szapudi & szalay ( 1998 ) for a discussion of the practical details of computing the n  point functions .    naively , the computation of the n ",
    "point correlation functions scale as @xmath11 , where @xmath12 is the number of data  points in the sample .",
    "as one can see , even with existing galaxy surveys from the sloan digital sky survey ( sdss ) , where @xmath13@xmath14 , such correlation functions quickly become untractable to compute . in recent years",
    ", there has been a number of more efficient algorithms developed to beat this naive scaling .",
    "for example , the international computational astrostatistics ( inca ; www.incagroup.org ) group has developed a new algorithm based on the use of the multi ",
    "resolutional kd - tree data structure ( mrkdtrees ) .",
    "this software , known as _ npt _",
    ", is publicly available ( www.autonlab.org ) , and has been discussed previously in gray et al .",
    "( 2003 ) , nichol et al .",
    "( 2001 ) and moore et al .",
    "briefly , mrkdtrees represent a condensed data structure in memory , which is used to efficiently answer as much of any data query as possible , i.e. , pruning the tree in memory .",
    "the key advance of our _ npt _ algorithm is the use of `` n '' trees in memory together to compute an n  point function .",
    "even with an efficient algorithm , the computation of higher  order correlation functions is intensive . in detail , the n ",
    "point correlation functions require a large number of sequential calls to the _ npt _ code .",
    "these include computing the cross ",
    "correlation between the real data ( called @xmath15 ) and a random dataset ( called @xmath12 ) , which is used to mimic the edge effects in the real data . as outlined in szapudi & szalay ( 1998 ) , each estimation of a 3point correlation functions , for a given bin of triangle shapes ( i.e. , @xmath16 , @xmath17 , @xmath18 , requires seven separate source counts over the whole dataset , namely @xmath19 .",
    "therefore , if one wished to probe @xmath20 triangle configuration , then @xmath21 sequential _ npt _ jobs are required , each of which could take several minutes to run .",
    "this can rise rapidly if one wishes to estimate errors on the n  point functions using either jack - knife resampling ( i.e. , removing subregions of the data and then re - computing the correlation functions ) , or a large ensemble of mock catalogs ( derived from simulations ) .",
    "such computations are well - suited to large clusters or grid of computers .    in recent years , we have used resources like teragrid ( www.teragrid.org ) and cosmos ( www.damtp.cam.ac.uk/cosmos/ ) to perform the computation of the n ",
    "point correlation functions for the sdss main galaxy sample and the sdss lrg sample .",
    "our experience shows that the management and scheduling of such a large number of jobs on these massive machines is laborious and tedious . to ease this problem",
    ", we are working on _ votechbroker _ , which is a tool that joins two new and emerging technologies ; the vo and computational grids .",
    "astrogrid ( www.astrogrid.org ) is a pparc - funded project to create a working virtual observatory for uk and international astronomers .",
    "astrogrid works closely with other vo initiatives around the world ( via the international virtual observatory alliance ; ivoa ) and is part of the euro ",
    "vo initiative in europe .",
    "in particular , the work outlined here has been performed as part of the eu ",
    "funded votech project , which aims to complete the technical preparation work for the construction of a european virtual observatory .",
    "specifically , votech is undertaking r&d into data  mining and visualization tools , which can be integrated into the emerging vo and computational grid infrastructure .",
    "therefore , votech will build upon existing or emerging standards and infrastructure ( e.g. ivoa standards and astrogrid middleware ) , as well as looking at standards from w3c and ggf .    as part of the votech research ,",
    "we are engaged in developing the _ votechbroker_. the key design goals of the broker are to : _",
    "i ) _ remove the execution and management of a large number of jobs ( like _ npt _ ) from the user in a transparent and reusable way ; _ ii ) _ accommodate different grid infrastructures ( e.g. condor , globus etc . ) ; _ iii ) _ locate suitable resources on the grid and optimize the submission of jobs ; _ iv ) _ monitor the status and success of jobs ; _ v ) _ combine with astrogrid myspace and workflow environments to allow easy management of job submission and final results ( as well as utilizing other algorithms within the vo ) . in figure [ fig1 ]",
    ", we show the schematic design of the broker archtecture which illustrates the modular and `` plug - in '' design philosophy we have adopted .",
    "this is required as one of the key requirements of _ votechbroker _ is that it should be straightforward to add new algorithms , resources and middleware ( e.g. a different job submission tool or protocol ) .",
    "we have implemented the core functionality of _ votechbroker _ and are presently testing it by submitting @xmath22 _ npt _ jobs on both the uk national grid servise ( www.ngs.ac.uk ) , cosmos supercomputer and a local condor pool of machines .",
    "the key ingredients of the present _ votechbroker _ include gridsam ( an open - source job submission and monitoring web servise from the london e - science centre ) , the uk e - science x.509 certificates , myproxy ( a repository for x.509 public key infrastructure security credentials ) , and the job submission description language ( jsdl ; a standard description of job execution requirements to a range of resource managers from the global grid forum ) . at present",
    ", the _ votechbroker _ provides a web - form interface to just the _ npt _ algorithm discussed above but is modular in design so other algorithms can be easily added via other web  forms .",
    "results from the _ votechbroker _ will soon be placed in a users astrogrid myspace . in the near future",
    ", we will interface the broker with other computational resources , e.g. , teragrid ( see below ) , and the astrogrid workflow .",
    "in addition to the need for new statistical software that scales - up to petabyte datasets , we also require new algorithms and computational resources that exploit the emerging power of nonparametric statistics . as discussed in wasserman et al .",
    "( 2001 ) , such nonparametric methods are statistical techniques that make as few assumptions as possible about the process that generated the data .",
    "such methods are more flexible than more traditional parametric methods that impose rigid and often unrealistic assumptions . with large sample sizes ,",
    "nonparametric methods make it possible to find subtle effects which might otherwise be obscured by the assumptions built into parametric methods .    in genovese et",
    "2004 ) , we discuss the application of nonparametric techniques to the analysis of the power spectrum of anisotropies in the cosmic microwave background ( cmb ) .",
    "for example , one can ask the simple question : how many peaks are detected in the wmap cmb power spectrum ?",
    "this question is hard to answer using parametric models for the cmb ( e.g. cmbfast models ) as these models possess multiple peaks and troughs , which could potentially be fit to noise rather than real peaks in the data . to solve this ,",
    "we have performed a nonparametric analysis of the wmap power spectrum ( miller et al . 2003 ) , which involves explaining the observed data ( @xmath23 ) as @xmath24 where @xmath25 is a orthogonal function ( expanded as a cosine basis @xmath26 ) and @xmath27 is the covariance matrix . the challenge is to `` shrink '' @xmath25 to keep the number of coefficients ( @xmath28 ) to a minimum . we achieve this using the method of beran ( 2000 ) , where the number of coefficients kept is equal to the number of data points . this is optimal for all smooth functions and provides valid confidence intervals .",
    "we also use monotonic shrinkage of @xmath28 , specifically the nested subset selection ( nss ) .",
    "the main advantage of this methodology is that it proves a `` confidence ball '' ( in n dimensions ) around @xmath25 , allowing non - parametric interferences like : is the second peak in the wmap power spectrum detected ?",
    "in addition , we can test parametric models against the `` confidence ball '' thus quickly assessing the validity of such models in n dimensions .",
    "this is illustrated in figure [ fig2 ] .",
    "we are embarked on a major effort to jointly search the 7dimensional cosmological parameter  space of @xmath29 , neutrino fraction , spectral index and h@xmath30 using parametric models created by cmbfast and thus determine which of these models fit within the confidence ball around our @xmath25 at the 95% confidence limit .",
    "traditionally , this is done by marginalising over the other parameters to gain confidence intervals on each parameter separately .",
    "this is a problem in high - dimensions where the likelihood function can be degenerate , ill - defined and under - identified .",
    "unfortunately , the nonparametric approach is computational intense as millions of models need to searched , each of which takes @xmath31 minute to run .",
    "to mitgate this problem , we have developed an intelligent method for searching for the surface of the confidence ball in high - dimensions based on kriging .",
    "briefly , kriging is a method of interpolation which predicts unknown values from data observed at known locations ( also known as gaussian process regression , which is a form of bayesian inference in statistics ) .",
    "there are many different metrics for evaluating the kriging success including variance and entropy , yet we employ the `` straddle '' method which picks new test points based both on the overall distance from other searched points and are predicted to be near the boundary .",
    "we have also developed a heuristic algorithm for searching for `` missed peaks '' in the likelihood space by searching models along the path jointing already detected peaks .",
    "we find no `` missed peaks '' , which illustrates our kriging algorithm is effective in finding the surface of the confidence ball in this high dimensional space .",
    "we have distributed the cmbfast model computations over a local condor pool of computers . in figure",
    "[ fig3 ] , we show preliminary results from this high - dimension search for the surface of the confidence ball and present * joint * 2d confidence limits on pairs of the aforementioned cosmological parameters . these calculations represent 6.8 years of cpu time to calculate over one million cmbfast models . in the near future",
    ", we will move this analysis to teragrid , using _",
    "votechbroker _ , and plan 10 million models to fully map the surface of the confidence ball .",
    "we will also make available a java  based web servise for accessing these models , and the wmap confidence ball , thus allowing other users to rapidly combine their data with our wmap constraints e.g. , doing a joint constraint from lss and cmb data .",
    "we are also working on possible convergence tests , and visualization tools within votech , to access this high - dimensional data .",
    "we present here two other examples of where massive computations are needed and could greatly benefit from the _",
    "votechbroker_. first , the xmm cluster survey ( xcs ) is a project dedicated to uniformily analysing all xmm pointings in search of clusters of galaxies i.e. , extended x - ray sources .",
    "dedicated software has been written to find clusters and a cluster target list created .",
    "one of the key components for the cosmological analysis of the xcs is the selection function , which is the efficiency ( and therefore , the effective volume of the survey ) in finding clusters as function of cluster properties ( cluster profile , redshift , temperature ) and observational constraints ( exposure time , background etc . ) .",
    "the most comprehensive method of determining such selection functions is via extemsive monte carlo simulations i.e. , adding fake clusters to the real data and measure the efficiency in re - detecting them ( see adami et al .",
    "2000 for such simulations for the sharc survey ) .",
    "we estimate that over a million simulations will be required to confidently estimate the xcs selection function because of the large number of parameters involved .",
    "this translates to over 4 years of cpu time to complete , but could be trivially parallelized over a large grid of computers using _",
    "votechbroker_.    next , we discuss the our independent analysis of the wmap time - stream data to create cmb maps of the sky ( see freeman et al .",
    "such independent confirmation is important and allows use to apply the same non - parametric techniques discussed above to the wmap map - making procedures , and thus assess their effect on the maps and power spectrum analyses .",
    "we found small differences between our maps and wmap ( 10 microkelvin ) , which are primary because of the residual dipole uncertainty and second - order terms in the doppler shift ( see freeman et al .",
    "2005 for details ) .",
    "each map would take a cpu day on a single processor , but we have parallalized the map - making code to exploit teragrid and can make maps in minutes .",
    "we provide here several examples of massive astronomical data analyses that require significant computational resources .",
    "our plan is to develop the _ votechbroker _ to provide a power framework within which such analyses can be performed",
    ". as discussed , the main goals of the _ votechbroker _ are to abstract from the user ( either a person or another program ) the complexities of job submission and management on computational grids , as well as being a modular `` plug  in '' design so other algorithms and software can be easily added .",
    "finally , we plan to integrate _ votechbroker _ into the astrogrid workflow and myspace environments , so it becomes a natural repository for a host of advanced statistical algorithms than scale - up in preparation for petabyte - scale datasets and analyses .",
    "we thank all our collaborators and colleagues in inca , votech , astrogrid , sdss and vo projects . the work presented here",
    "was partly funded by nsf itr grant 0121671 and through the eu votech and marie curie programs .",
    "rcn thanks the organisers of the adass meeting for their invitation .",
    "gs thanks the votech and university of edinburgh for his funding ( see eurovotech.org for details ) .",
    "adami , c. , et al .",
    ", 2000 , apjs , 131 , 391 beran , r. , 2000 , _ j. amer",
    "_ , 95 , 155 genovese , c. , _ et al . _ , 2004 ,",
    "_ statistical science _ , astro - ph/0410140 gray , a. , _ et al .",
    "_ , 2003 _ conference proceeding for adass xiii _ , astro - ph/0401121 freeman , p. , et al . , 2005 ,",
    "apj accepted , astro - ph/05xxxx landy , s. d. & szalay , a. , 1993 , _ astrophysical journal _ , 412 , 64 miller , c. j. , _ et al . _ , 2002 , _ astrophysical journal _ , 565 , 67 .",
    "moore , a. w. , _ et al .",
    "_ , 2000 , _ conference proceeding for `` mining the sky '' _ , astro - ph/0012333 nichol , r. c. , _ et al .",
    "_ , 2001 , _ conference proceedings for  statistical challenges in modern astronomy iii \" _ , astro - ph/0110230    peebles , p. j. e. , 1980 , _ large - scale structure in the universe _ , princeton university press szalay , a. & gray j. , 2001 , _ science _ , 293 , 2037 szapudi , i. 2005 , astro - ph/0505391 szapudi , i. & szalay , a. , 1998 , _ astrophysical journal _ , 494 , 41 .",
    "wasserman , l. , _ et al .",
    "_ 2001 , _ conference proceedings for  statistical challenges in modern astronomy iii \" _ , astro - ph/0112050"
  ],
  "abstract_text": [
    "<S> there is a growing need for massive computational resources for the analysis of new astronomical datasets . to tackle this problem , we present here our first steps towards marrying two new and emerging technologies ; the virtual observatory ( e.g , astrogrid ) and the computational grid ( e.g. teragrid , cosmos etc . ) . </S>",
    "<S> we discuss the construction of _ votechbroker _ , which is a modular software tool designed to abstract the tasks of submission and management of a large number of computational jobs to a distributed computer system </S>",
    "<S> . the broker will also interact with the astrogrid workflow and myspace environments . </S>",
    "<S> we discuss our planned usages of the _ votechbroker _ in computing a huge number of n  point correlation functions from the sdss data and massive model - fitting of millions of cmbfast models to wmap data . </S>",
    "<S> we also discuss other applications including the determination of the xmm cluster survey selection function and the construction of new wmap maps . </S>"
  ]
}