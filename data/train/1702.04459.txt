{
  "article_text": [
    "feed - forward neural networks have been popular and widely applied in many domain applications due to its universal approximation capability .",
    "basically , a large amount of learning algorithms are developed on the basis of the minimum of a mean squared error ( mse ) criterion , which indeed is sufficient and used in uncertain data modelling problems with assumption on noise fitting a gaussian function .",
    "however , it is generally known that , in many real applications data are not only noisy but may inevitably contain outliers @xcite .",
    "in fact , the outliers can cause a great threat to the training process of neural networks as just one notorious outlier could severely distort the training , which consequently make the resulting learner model based on mse useless .    over the last decades",
    ", a lot of robust techniques have been established in the field of robust statistics @xcite , including m - estimates ( maximum likelihood estimates ) , l - estimates ( linear combination of order statistics ) , r - estimates ( estimates based on rank transformations ) , and lmeds estimates ( least median square ) .",
    "in essence , a majority of these efforts lay great emphasize on using new cost function in training the neural network model , where the conventional back - propagation ( bp ) algorithm is applied in tuning the model parameters , such as hidden parameters and output weights .",
    "for example in @xcite , m - estimator and hampels hyperbolic tangent estimates were employed in the objective function , aiming to alleviating the impacts of outliers .",
    "liano @xcite made use of the mean log squared error ( mlse ) criterion as a robust cost function with assumption that the errors follow cauchy distribution . in @xcite , the author proposed a robust neural network algorithm based on a m - estimator cost function with consideration of the random sample consensus ( ransac ) framework that has become the standard method of dealing with outliers in the computer vision and image processing community @xcite .",
    "however , robust neural network methods based on bp algorithm still suffer from some inherent drawbacks , including the ambiguity in selecting the initialization weights and the problem of a local minimum and slow convergence speed . on the other hand , although some methods based on svm are also obtained and successfully verified for function approximation problems with data corrupted with outliers , @xcite , its effectiveness and efficiency may meet challenge when dealing with training data with high level outliers or other large scale robust modeling problems .",
    "that means more flexible learning tool together with some advanced learning algorithms are desirable .    with the increasing demands of dealing with large scale data processing ,",
    "randomized methodology becomes a pathway to access a fast and effective solution in computing exercises @xcite .",
    "the use of randomness in neural networks can be traced back to later 80 s @xcite and some milestones on this topic can be read in our survey paper @xcite .",
    "there is few work addressing robust modeling problem based on random vector functional - link ( rvfl ) networks @xcite . in @xcite ,",
    "the authors employed a hybrid regularization loss function with assumption of the sparsity among outliers , and proposed a probabilistic robust learning algorithm for neural networks with random weights ( prnnrw ) , where some parameters must be set properly and this is quite difficult to be done in practice . in @xcite , the authors suggested an improved version of the original rvfl networks ( imporoved rvfl ) , where a weighted cost function is used to improve the model s robustness by differentiating each sample s contribution for the whole objective function . specifically , kernel density estimation ( kde ) method , which acts as a non - parametric way to estimate the probability density function of a random variable by computing a smooth density estimation from data samples by placing on a each sample point a function representing its contribution to the density , is used to renew the associated penalty weights .",
    "however , these two algorithms , i.e. , prnnrw and improved rvfl do not clarify the random selection strategy of the hidden parameters , which indeed could not fully guarantee the effectiveness of the robust algorithms .",
    "our recent work reported in @xcite proposed a new framework of randomized learner model , termed as stochastic configuration networks ( scns ) , which has been proved as universal approximators .",
    "the main contribution of the work in @xcite lies in the supervisory mechanism for randomly assigning the hidden parameters throughout the training process , which can guarantee the universal approximation property of the randomized learner model .",
    "it should be pointed out that our proposed supervisory mechanism in @xcite is original and unique in literature up to date .",
    "this paper is built on @xcite and attempts to develop a robust version of scns , aiming to copy with modeling problems with data corrupted by outliers .",
    "based on the construction process of scn , we employ a weighted least squares objective function that consequently leads to a weighted configuration criterion ( compared with the original scn ) for randomly assigning the hidden parameters . at the meantime",
    ", the penalty weights representing the degree of contribution of individual data samples to the objective function will be updated by using a well - defined kernel density estimation ( kde ) method . in this paper , alternating optimization ( ao ) technique",
    "is employed to renew the penalty weights and the scn model .",
    "finally , the performance of our proposed rsc - kde algorithm is extensively evaluated on synthetic and real data contaminated by artificial outliers , including some benchmark datasets from the uci repository of standard machine learning datasets @xcite and a dataset from engineering application @xcite .",
    "comparative studies indicate that rsc - kde outperforms other existing randomized neural networks in terms of effectiveness and robustness .",
    "the rest of paper is organized as follows : section ii briefly reviews stochastic configuration networks and the kernel density estimation ( kde ) method .",
    "section iii details our proposed rsc - kde algorithm .",
    "section iv reports experimental results with comparisons and discussions .",
    "section v concludes this paper with some remarks .",
    "this section briefly reviews scn framework , of which the highlight is the configuration of hidden parameters under a supervisor mechanism . in general",
    ", scn performs in two interrelated phases , i.e. , input parameter ( inner weights and biases ) configuration and output parameters ( output weights ) determination .",
    "details of the scn framework can be read in @xcite .",
    "also , as a non - parametric way to estimate the probability density function of a random variable , the kernel density estimation ( kde ) method is introduced here .",
    "suppose that we consider the approximation problem of unknown function @xmath0 .",
    "let @xmath1 denote the space of all lebesgue - measurable vector - valued functions @xmath2:r^{d}\\rightarrow r^{m}$ ] on a compact set @xmath3 , with the @xmath4 norm defined as @xmath5 the constructive process of scn starts with a small size of network then adds hidden nodes and output weights incrementally ( automatically ) until an acceptable approximation performance is achieved .",
    "that does not require any prior knowledge about sufficient complexity of the network for the objective tasks .",
    "for instance , given a target function @xmath0 , assume a slfn with @xmath6 hidden nodes ( @xmath7 ) have already been constructed , that is , @xmath8 ( @xmath9 ) , where @xmath10^\\texttt{t}$ ] .",
    "the current residual error can be denoted as @xmath11 $ ] .",
    "it can be easily obtained that @xmath12 .",
    "basically , scn can provide an effective solution for how to add @xmath13 , @xmath14 ( @xmath15 and @xmath16 ) leading to @xmath17 until the residual error @xmath18 is suitable for the given task , that is , @xmath19 is smaller than an expected specific tolerance @xmath20 .",
    "the following theorem 1 restates the universal approximation property of scn , corresponding to theorem 7 in @xcite",
    ".    * theorem 1 . *",
    "suppose that span(@xmath21 ) is dense in @xmath22 space and @xmath23 , @xmath24 for some @xmath25 .",
    "given @xmath26 and a nonnegative real number sequence @xmath27 with @xmath28 and @xmath29 .",
    "for @xmath7 , denoted by @xmath30 if the random basis function @xmath14 is generated to satisfy the following inequality : @xmath31 and the output weights are constructively evaluated by @xmath32=\\arg \\min_{\\beta}\\|f-\\sum_{j=1}^{l}\\beta_jg_j\\|.\\ ] ] then , we have @xmath33 where @xmath34 , @xmath35^{\\mathrm{t}}$ ] .      basically , the kernel density estimator computes a smooth density estimation from data samples by placing on a each sample point a function representing its contribution to the density .",
    "the distribution is obtained by summing all these contributions .",
    "readers may refer to @xcite and @xcite for further details on the kernel density estimation method .    using kde method ,",
    "the underlying probability density function of a random variable @xmath36 can be estimated as @xmath37 where @xmath38 is a `` kernel function '' ( typically a gaussian ) centered at the data points , @xmath39 , @xmath40 , and @xmath41 are weighting coefficients ( typically uniform weights are used , i.e. , @xmath42 ) .",
    "in some industrial process , the collected samples are always contaminated by the outliers caused by the failure of the measuring or transmission devices or unusual disturbances .",
    "that means a robust neural network learner that could fit a functional model to data corrupted with outliers is of high importance in practical applications , especially on some industrial system design .",
    "this section contributes to the development of robust stochastic configuration networks ( rscns ) by using kernel density estimation ( kde ) . to begin with ,",
    "let us shed some light on the technique of weighted least squares ( wls ) , which has been received considerable amount of attention in robust modeling .",
    "in general , a weighted least squares ( wls ) learning strategy when training a neural network model can be formulated as follows . for a target function @xmath0 ,",
    "given a training dataset with inputs @xmath43",
    ", @xmath44^\\texttt{t}\\in r^{d}$ ] and outputs @xmath45 , where @xmath46^\\texttt{t}\\in r^{m}$ ] , @xmath47 , a robust neural network model as an approximator of @xmath48 can be obtained by taking consideration of the weighted least squares problem , i.e. , @xmath49 where @xmath50 is the penalty weight representing the contribution of the corresponding sample to the objective function . @xmath51 is the activation function and @xmath52 is the number of hidden neurons . for @xmath53 , @xmath54 , @xmath55",
    "are the input weights and biases , respectively .",
    "@xmath56 represent the output weights .",
    "generally , the penalty weights @xmath57 ( @xmath58 ) can be determined according to the reliability of the sample @xmath59 .",
    "it is obvious that , high reliability means the sample should be a normal data that correctly represents the process behavior , whereas low reliability indicates that the sample might be a suspected outlier or noise embodying worthless or even wrong information . decreasing and enlarging the weight of low and high reliability samples , respectively , can eliminate and even remove the impact of the outliers . as such , the built robust model will possess extremely small sensitivity to outlying data .",
    "it is easy to combine the new objective ( [ wls ] ) with our scn framework shown in theorem 1 . before introducing wls into the procedures based on scn , some specified notations are needed . in practice ,",
    "as the exact functional form of @xmath60 is unavailable , we use its consistent estimation on @xmath43 , @xmath44^\\texttt{t}\\in r^{d}$ ] , i.e. , @xmath61^\\texttt{t}\\in r^{n\\times m}$ ] where @xmath62\\in r^n$ ] with @xmath63 . let @xmath64^\\texttt{t}$ ]",
    "consists the activation of the new hidden node for each input @xmath59 , @xmath58 .",
    "then the current hidden output matrix @xmath65 $ ] .    according to ( [ wls ] ) ,",
    "a weighted form of @xmath66 should be concerned , aiming to implanting the penalty weights during the constructive process of scn . here",
    "we can denote @xmath67^\\texttt{t}\\nonumber\\\\ & = & \\theta e_{l-1}(x)\\\\ \\label{weighted_error } \\tilde{h}_l(x)&=&\\theta h_l(x),\\label{hiddennode}\\end{aligned}\\ ] ] where @xmath68 .",
    "given a set of penalty weights @xmath69 , we denote @xmath70 according to the configuration mechanism in theorem 1 and taking into account the contribution of each sample to the global objective function , the hidden parameters ( @xmath71 and @xmath72 ) should be selected when @xmath73 .",
    "now it is obvious that the remaining question is how to assign penalty weights @xmath69 along with the processes of scn .",
    "in fact , if the probability density function of the residuals can be obtained or estimated , the reliabilities of the samples will be determined .",
    "inspired by [ 25 ] , we attempt to incorporate kde method ( as reviewed in section 2 ) into our scn framework , aiming to estimate the reliability of the sample and then assign @xmath57 in terms of the estimated reliability of the sample . based on ( [ kde_def ] ) , the probability density function of the residuals @xmath74 can be calculated as ( here @xmath74 is regarded as a random variable ) @xmath75 where @xmath76^\\texttt{t}\\in r^{m}$ ] , @xmath77 is the estimation window width which exhibits a strong influence on the resulting estimate , @xmath78 is the standard deviation of the residual ; @xmath38 is the gaussian function defined as : @xmath79 with the above equations , the probability of each residual @xmath80 ( @xmath58 ) can be achieved as @xmath81 .",
    "also , it is clear that the larger the probability , the higher the reliability . concretely",
    ", the penalty weight @xmath57 thus can be assigned as ( @xmath58 ) @xmath82 once the set of penalty weights @xmath57 is assigned and fixed , the output weights @xmath83 can be determined by solving the followed minimization problem @xmath84 where @xmath85 $ ] , @xmath65 $ ] , @xmath86 .",
    "it is easy to obtain the solution of ( [ wls2 ] ) below @xmath87    so far , we can generally describe the whole learning process in building a robust scn model , where alternating optimization ( ao ) technique works effectively in finding the most appropriate values of penalty weights . to begin with ,",
    "we assign equal penalty weight for each sample ( i.e. , @xmath88 ) and conduct the procedures of scn to obtain a learner with @xmath52 hidden neurons , of which the hidden parameters @xmath71 and @xmath72 are configured based on ( [ factor1 ] ) , and @xmath89 is determined by ( [ itera2 ] )",
    ". then the penalty weights are updated by new residuals caused by the resultant learner .",
    "finally , a rscn is established by repeating this procedure until some user - defined accuracy or stopping criterion is reached .",
    "it should be clarified that , the renew process of penalty weights corresponds to the whole process similar to the original sc algorithm .",
    "that means at each step of iteration , the penalty weights remain unchanged throughout the process of building the neural network architecture using scn framework . once a temp learner is designed , the penalty weights will be updated and the new setting will be employed in the new round for constructing an universal approximator by scn principles .    based on the idea of alternating optimization ( ao ) , the penalty weight @xmath57 and the output weight @xmath89 can be calculated iteratively by @xmath90 and @xmath91 where @xmath92 denotes the @xmath92th iteration and @xmath93 .",
    "here we use @xmath94 to represent the changing ( with @xmath52 increase ) residual vector throughout the constructing process in which @xmath95 are utilized as the present penalty weights .",
    "different from our previously proposed scn framework that all samples will contribute equally to the value of the objective function , this newly designed method based on kde treats individual samples differently and give more emphasis on the entry with higher reliability , which is equivalent to , lager value of penalty weights .",
    "this means that if the output @xmath96 is occluded or corrupted , the input entry corresponding to outliers will provide small contributions to the cost function . as a result",
    ", the noise can be handled uniformly and smoothly .",
    "now , our proposed robust stochastic configuration algorithm , namely rsc - kde , can be summarized as followed :    [ cols=\"<,^,<\",options=\"header \" , ]",
    "this paper contributes a development of randomized method for robust data modelling , which is an important research topic in many domain applications .",
    "stochastic configuration networks , which was recently proposed and reported in @xcite , are employed to build robust predictive models against some uncertain outputs caused by noises and/or outliers .",
    "a weighted least squares method with the kernel density estimation ( kde ) is used in the proposed rsc - kde algorithm , and the alternating optimization ( ao ) technique is employed to implement the robust scn model through several rounds of iterations .",
    "simulation results including a case study are quite promising , and that imply a good potential of our rsc - kde algorithm for resolving robust data modelling problems .",
    "this work sets a basis for future researches on building robust learner models with random parameters .",
    "extensions of the present algorithm to online version is being expected .",
    "in addition , the proposed algorithm in this paper can be easily extended to distributed learning framework for dealing with large scale robust modelling problems ."
  ],
  "abstract_text": [
    "<S> this paper aims at developing robust data modelling techniques using stochastic configuration networks ( scns ) , where a weighted least squares method with the well - known kernel density estimation ( kde ) is used in the design of scns . </S>",
    "<S> the alternating optimization ( ao ) technique is applied for iteratively building a robust scn model that can reduce some negative impacts , caused by corrupted data or outliers , in learning process . </S>",
    "<S> simulation studies are carried out on a function approximation and four benchmark datasets , also a case study on industrial application is reported . </S>",
    "<S> comparisons against other robust modelling techniques , including the probabilistic robust learning algorithm for neural networks with random weights ( prnnrw ) and an improved rvfl , demonstrate that our proposed robust stochastic configuration algorithm with kde ( rsc - ked ) perform favourably . </S>"
  ]
}