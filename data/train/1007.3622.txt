{
  "article_text": [
    "besides their classical and traditional applications in signal processing and communications @xcite ( cf . also further references in @xcite ) and speech recognition @xcite , hidden markov models have recently become indispensable in computational biology and bioinformatics @xcite as well as in natural language modelling @xcite and information security @xcite .    at the same time , their spatial extensions , known as hidden markov random field models ( hmrfm ) , have also been immensely influential in spatial statistics @xcite , and particularly in image analysis , restoration , and segmentation @xcite . indeed , hidden markov models are called ` one of the most successful statistical modelling ideas that have [ emerged ] in the last forty years ' @xcite .",
    "hm(rf)ms owe much of their success on the one hand to the persistence of the markov property of the unobserved , or hidden , layer in the presence of observed data , and on the other , to the richness of the observed system @xcite . namely , in bayesian terms , in addition to the prior , the posterior distribution of the hidden layer also possesses a markov property ( albeit generally inhomogeneous even with homogeneous priors ) , whereas the marginal law of the observed layer can still include global , i.e. non - markovian , dependence .",
    "the markov property of the posterior distribution and the conditional independence of the observed variables given the hidden ones , have naturally led to a number of computationally feasible methods for inference about the hidden realizations as well as model parameters ( if any ) .",
    "hmms are naturally a special case of _ graphical models _",
    "@xcite , ( * ? ? ?",
    "* ch . 8) .",
    "hmms , or one dimensional hmrfms , have been particularly popular not least due to the fact that linear order of the indexing set ( usually associated with time ) makes exploration of hidden realizations relatively straightforward from the computational viewpoint .",
    "in contrast , higher dimensional hmrfms generally require approximate , possibly stochastic , techniques in order to compute optimal configurations of the hidden field @xcite .",
    "in particular , _ maximum a posteriori _ ( map ) estimator of the hidden layer of an hmm is efficiently and exactly computed by a dynamic programming algorithm bearing the name of viterbi , whereas a general higher dimensional hmrfm would commonly employ a simulated annealing type method @xcite to produce approximate solutions to the same task .",
    "we adopt the machine and statistical learning convention and therefore refer to the hidden and observed processes as @xmath0 and @xmath1 , respectively , in effect reversing the convention that is more commonly used in the hmm context . thus , let @xmath2 be a markov chain with state space @xmath3 , @xmath4 . even though we include inhomogeneous chains in most of what follows , for brevity we will still be suppressing the time index wherever this",
    "does not cause ambiguity .",
    "hence , we write @xmath5 for all transition matrices .",
    "let @xmath6 be a process with the following properties .",
    "first , given @xmath7 , the random variables @xmath8 are conditionally independent .",
    "second , for each @xmath9 , the distribution of @xmath10 depends on @xmath11 ( and @xmath12 ) only through @xmath13 .",
    "the process @xmath1 is sometimes called the _ hidden markov process _ ( hmp ) and the pair @xmath14 is referred to as a _ hidden markov model _ ( hmm ) .",
    "the name is motivated by the assumption that the process @xmath0 ( sometimes called a _ regime _ ) is generally non - observable .",
    "the conditional distribution of @xmath15 given @xmath16 is called an _ emission distribution _ , written as @xmath17 , @xmath18 .",
    "we shall assume that the emission distributions are defined on a measurable space @xmath19 , where @xmath20 is usually @xmath21 and @xmath22 is the corresponding borel @xmath23-algebra .",
    "without loss of generality , we assume that the measures @xmath17 have densities @xmath24 with respect to some reference measure @xmath25 , such as the counting or lebesgue measure",
    ".    given a set @xmath26 , integers @xmath27 and @xmath28 , @xmath29 , and a sequence @xmath30 , we write @xmath31 for the subsequence @xmath32 . when @xmath33 , it will be often suppressed .",
    "thus , @xmath34 and @xmath35 stand for the fixed observed and unobserved realizations , respectively , of the hmm @xmath36 up to time @xmath37 .",
    "any sequence @xmath38 is called a _",
    "path_. we shall denote by @xmath39 the joint probability density of @xmath40 , i.e. @xmath41 overloading the notation , for every @xmath38 and for every sequence of observations @xmath42 , let @xmath43 and @xmath44 stand for the marginal probability mass function @xmath45 of path @xmath46 and probability density function @xmath47 of the data",
    "@xmath42 , respectively .",
    "it is a standard ( see , e.g. @xcite , ( * ? ? ?",
    "* ch . 13 ) ) in this context to define the so - called _ forward _ and _ backward _ variables @xmath48 where @xmath49 and @xmath50 are the conditional densities of the data segments @xmath51 and @xmath52 , respectively , given @xmath53 .",
    "_ segmentation _ here refers to estimation of the hidden path @xmath54 . treating @xmath54 as missing data @xcite , or parameters , a classical and by far the most popular solution to the segmentation problem is to maximize @xmath55 in @xmath38 . often , especially in the digital communication literature",
    "@xcite , @xmath55 is called the likelihood function which might become potentially problematic in the presence of any genuine model parameters .",
    "such `` maximum likelihood '' paths are also called _ viterbi paths _ or _ alignments _ after the viterbi algorithm @xcite commonly used for their computation .",
    "if @xmath56 is thought of as the prior distribution of @xmath57 , then viterbi path also maximizes @xmath58 , the probability mass function of the posterior distribution of @xmath59 , hence the term ` _ maximum a posteriori ( map ) path _ ' .    in spite of its computational attractiveness",
    ", viterbi inference may be unsatisfactory for a number of reasons , including its suboptimality with regard to the number of correctly estimated states @xmath60 .",
    "also , using the language of information theory , there is no reason to expect a viterbi path to be typical @xcite . indeed , `` there might be many similar paths through the model with probabilities that add up to a higher probability than the single most probable path '' @xcite .",
    "the fact that a map estimate need not be representative of the posterior distribution has also been recently discussed in a more general context in @xcite .",
    "atypicality of viterbi paths particularly concerns situations when estimation of @xmath54 is combined with inference about model parameters , e.g. transition probabilities @xmath61 @xcite .",
    "even when estimating , say , the probability of heads from independent tosses of a biased coin , we naturally hope to observe a typical realization and not the constant one of maximum probability .    an alternative and very natural way to estimate @xmath54 is by maximizing the posterior probability @xmath62 of each individual hidden state @xmath13 , @xmath63 .",
    "we refer to the corresponding estimator as _ pointwise maximum a posteriori ( pmap)_. pmap is well - known to maximize the expected number of correctly estimated states ( section [ sec : risk ] ) , hence the characterization ` _ optimal accuracy _ ' @xcite . in statistics , especially spatial statistics and image analysis ,",
    "this type of estimation is known as _ marginal posterior mode _",
    "@xcite or _ maximum posterior marginals _",
    "@xcite ( mpm ) estimation . in computational biology , this is also known as the _ posterior decoding _",
    "( pd ) @xcite and has been reported to be particularly successful in pairwise sequence alignment @xcite . in the wider context of biological applications of discrete high - dimensional probability models ,",
    "this has also been called _ consensus _ estimation , and in the absence of constraints , _ centroid _ estimation @xcite . in communications applications of hmms , largely influenced by @xcite , the terms ` _ optimal symbol - by - symbol decoding _ ' @xcite , ` _ symbol - by - symbol map estimation _ ' @xcite , and ` _ map state estimation _ ' @xcite have been used for this .",
    "although optimal in the sense of maximizing the expected number of correctly estimated states , a pmap path might at the same time have low , in principle zero , probability @xcite .",
    "it is actually not difficult to constrain the pmap decoder to _ admissible _ paths , i.e. of positive posterior probabilities as described in @xcite ( albeit in a slightly more general form allowing for state aggregation ) and also in subsection [ sec : pvdmore ] , , below .",
    "a variation on this idea has been applied in @xcite for prediction of membrane proteins , giving rise to the term ` _ _ posterior viterbi decoding ( pvd ) _ _ ' @xcite .",
    "pvd , however , maximizes the product @xmath64 @xcite ( and also below ) and not the sum @xmath65 , whereas the two criteria are _ no longer equivalent in the presence of path constraints _ ( subsection [ sec : pvdmore ] ) . in @xcite ,",
    "a pmap decoder is proposed to obtain optimal pairwise sequence alignments .",
    "the authors of @xcite use the term `` a legitimate alignment '' which suggests admissibility , but the description of the actual algorithm ( * ? ? ?",
    "* section 3.8 ) appears to be insufficiently detailed to verify if the algorithm indeed enforces admissibility , or , if inadmissible solutions are altogether an issue in that context .",
    "in many applications , e.g. gene identification , the pointwise ( e.g. nucleotide level ) error rate is not necessarily the main measure of accuracy , hence the constrained pmap need not be an ultimate answer .",
    "together with the above problem of atypicality of map paths , this has been addressed by moving from single path inference towards _ envelops _ @xcite .",
    "thus , for example , in computational biology the most common approach would be to aggregate individual states into a smaller number of semantic labels ( e.g. codon , intron , intergenic ) . in effect , this would realise the notion of path similarity by mapping many `` similar '' state paths to a single label path , or _ annotation _ @xcite . however , this leads to the problem of _ multiple paths _ , which in many practically important hmms renders the dynamic programming approach of the viterbi algorithm np - hard @xcite .",
    "unlike the viterbi / map decoder , the pmap decoder handles annotations as easily as it does state paths , including the enforcement of admissibility @xcite .",
    "a number of alternative heuristic approaches are also known in computational biology , but none appears to be fully satisfactory @xcite .",
    "evidently , mapping optimal state paths to the corresponding annotations need not lead to optimal annotation and can actually give poor results @xcite .",
    "overall , although the original viterbi decoder has still been the most popular paradigm in many applications , and in computational biology in particular , alternative approaches have demonstrated significantly higher performance , e.g. , in predicting various biological features .",
    "for example , @xcite suggested the _",
    "1-best _ algorithm for optimal labelling .",
    "more recently , @xcite have demonstrated pvd to be superior to the 1-best algorithm , and not surprisingly , to the viterbi and pmap decoders , on tasks of predicting membrane proteins .",
    "a starting point of this paper is that restricting the pmap decoder to paths of positive probability is but one of _ numerous ways to combine the useful features of the map and pmap path estimators_. indeed , as a sensible remedy against vanishing probabilities , in his popular tutorial @xcite rabiner briefly mentions maximization of the expected number of correctly decoded ( overlapping ) blocks of length two or three , rather than single states . with @xmath66 and @xmath67 being the block length and corresponding path estimate , respectively",
    ", this approach yields viterbi inference as @xmath68 increases to @xmath69 ( with @xmath70 corresponding to pmap ) . therefore",
    ", this approach could be interpreted as interpolating between the pmap and viterbi inferences .",
    "intuitively , one might also expect @xmath71 to be strictly increasing with @xmath68 .",
    "this is not exactly so as can be seen from example  [ sec : example ] where @xmath72 .",
    "however , we find the idea of interpolation between the pmap and viterbi inferences worth a further investigation . to the best of our knowledge ,",
    "the only published work which explicitly proposes a solution to such interpolation is @xcite .",
    "the approach of @xcite is algorithmic , directly based on continuous mappings , and also deserves an analysis which we present in subsection [ subsec : alg ] .      in this paper",
    ", we consider the segmentation problem in the more general framework of statistical learning .",
    "namely , we consider sequence _ classifier _ mappings @xmath73 and optimality criteria for their selection . in section [ sec : risk ] , criteria for optimality of @xmath74 are naturally formulated in terms of risk minimization whereby @xmath75 , the _ risk of _",
    "@xmath46 , derives from a suitable _ loss function_. in section [ sec : combrisk ] , we consider families of risk functions which naturally generalize those corresponding to the viterbi and pmap solutions ( subsection [ subsec : viterbipmap ] ) .",
    "furthermore , as shown in section [ sec : bridge ] , these risk functions define a family of path decoders parameterized by an integer @xmath68 with @xmath76 and @xmath77 corresponding to the pmap and viterbi cases , respectively ( theorem [ k - block ] ) .",
    "we also show the close connection between the aforementioned family of decoders and the rabiner @xmath68-block approach .",
    "if needed , then the new family of decoders can easily be embedded into a yet wider class with a principled criterion of optimality .",
    "all these decoders ( classifiers ) would only be of theoretical interest if they could not be easily calculated . in section",
    "[ sec : combrisk ] , we show that all of the newly defined decoders can be implemented efficiently as a dynamic programming algorithm in the usual forward - backward manner with essentially the same ( computational as well as memory ) complexity as the pmap or viterbi decoders ( theorem [ dyn ] ) .",
    "given a sequence of observations @xmath42 , we define the ( posterior ) _ risk _ to be a function @xmath78.\\ ] ] naturally , we seek a state sequence with minimum risk : @xmath79 following the _ statistical decision and pattern recognition theories _ , the classifier @xmath80 will be referred to as the _ bayes classifier _",
    "( relative to risk @xmath81 ) . within the same framework",
    ", the risk is often specified via a _ loss - function _",
    "@xmath82,\\ ] ] interpreting @xmath83 as the loss incurred by the decision to predict @xmath84 when the actual state sequence was @xmath85 .",
    "therefore , for any state sequence @xmath38 , the risk is given by @xmath86=\\sum_{a^t\\in s^t}l(a^t , s^t)p(a^t|x^t).\\ ] ]      the most popular loss function is the so - called _ symmetrical _ or _ zero - one _ loss @xmath87 defined as follows : @xmath88 we shall denote the corresponding risk by @xmath89 . with this loss , clearly @xmath90 thus @xmath91 is minimized by a viterbi path , i.e. a sequence of maximum posterior probability .",
    "let @xmath92 stand for the corresponding classifier , i.e. @xmath93 with a suitable tie - breaking rule .",
    "evidently , viterbi paths also minimize the following risk @xmath94 it can actually be advantageous to use the log - likelihood based risk since , as we shall see later , it leads to various natural generalizations ( sections [ sec : combrisk ] and [ sec : bridge ] ) . when sequences are compared pointwise , it is common to use additive loss - functions of the form @xmath95 where @xmath96 is the loss associated with classifying the @xmath12-th element @xmath97 as @xmath98 .",
    "typically , for every state @xmath99 , @xmath100 .",
    "it is not hard to see that , with @xmath101 as in , the corresponding risk can be represented as follows @xmath102 where @xmath103 .",
    "most commonly , @xmath104 is again symmetrical , or zero - one , i.e. @xmath105 , where @xmath106 stands for the indicator function of set @xmath107 . in this case",
    ", @xmath108 is naturally related to the _ hamming distance _ @xcite .",
    "then also @xmath109 so that the corresponding risk is @xmath110 let @xmath111 stand for the bayes classifier relative to the @xmath112-risk .",
    "it is easy to see from the above definition of @xmath112 , that @xmath111 delivers pmap paths , which clearly minimize the expected number of misclassification errors .",
    "in addition to maximizing @xmath113 , @xmath114 also maximizes the _ pseudolikelihood _ @xmath115 , and therefore minimizes the following _ log - pseudolikelihood _ risk @xmath116      recall ( subsection [ sec : segment ] ) that pmap paths can be of zero probability ( i.e. not admissible ) . to ensure admissibility , @xmath112-risk",
    "can simply be minimized over the admissible paths : @xmath117 assuming that @xmath118 , @xmath63 , @xmath119 , have been precomputed ( by the classical forward - backward recursion @xcite ) , the solution of can be easily found by a viterbi - like recursion @xmath120 where @xmath121 , @xmath122 .",
    "the recursion is also equivalent to @xmath123 however , in the presence of path constraints , minimization of the @xmath112-risk is no longer equivalent to minimization of the @xmath124-risk .",
    "in particular , the problem is not equivalent to the following problem ( _ posterior - viterbi decoding _ ) @xmath125 a solution to can be computed by a related recursion given in below @xmath126 recursion is clearly equivalent to @xmath127 although admissible minimizers of @xmath112 and @xmath124 risk are by definition of positive probability , this probability might still be very small .",
    "indeed , in the above recursions , the weight @xmath128 is 1 even when @xmath61 is very small .",
    "we next replace @xmath128 ( @xmath129 ) by the true transition ( initial ) probability @xmath61 ( @xmath130 ) in minimizing the @xmath124-risk ( i.e. maximization of @xmath115 ) .",
    "then the solutions remain admissible and now also tend to maximize the prior path probability . with the above replacements , recursions and now solve the following _ seemingly unconstrained _ optimization problem ( see theorem [ dyn ] ) @xmath131\\quad \\leftrightarrow \\quad \\min_{s^t}\\big[\\bar{r}_1(s^t|x^t)+h(s^t)\\big],\\ ] ] where the penalty term @xmath132 is the prior log - likelihood risk which does not depend on the data .",
    "the thereby modified recursions immediately generalize as follows : @xmath133 solving @xmath134,\\ ] ] where @xmath135 is a regularization constant and @xmath136 ( see section [ sec : combrisk ] and theorem [ dyn ] ) . then , pvd , i.e. the problem solved by the original recursions and , can be recovered by taking @xmath137 sufficiently small .",
    "( alternatively , the pvd problem can also be formally written in the form with @xmath138 and @xmath139 given , for example , by @xmath140 . )",
    "what if the actual probabilities @xmath61 ( @xmath130 ) were also used in the optimal accuracy / pmap decoding , i.e. optimization - ?",
    "it appears more sensible to replace the indicators @xmath128 ( @xmath129 ) with @xmath61 ( @xmath130 ) in ( and not in ) .",
    "this solves the following problem : @xmath141\\quad \\leftrightarrow \\quad \\min_{s^t}\\big[r_1(s^t|x^t ) + \\r_{\\infty}(s^t)\\big].\\ ] ] a more general problem can be written in the form @xmath142,\\ ] ] where @xmath143 is some penalty function ( independent of the data @xmath42 ) .",
    "thus , the problem of optimal accuracy / pmap decoding over the admissible paths is obtained by taking @xmath137 sufficiently small and @xmath136 .",
    "( setting @xmath144 also reduces the problem back to . )",
    "motivated by the previous section , we consider the following general problem @xmath145,\\ ] ] where @xmath146 , @xmath147 , @xmath148 .",
    "this is also equivalent to @xmath149,\\ ] ] where @xmath150,\\nonumber\\\\ & = -{1\\over t}[\\log \\pi_{s_1}+ \\sum_{t=1}^{t-1}\\log p_{s_t s_{t+1}}+\\sum_{t=1}^t \\log f_{s_t}(x_t)],\\nonumber \\\\",
    "\\bar{r}_{\\infty}(s^t|x^t)&={1\\over t}\\log p(s^t|x^t),\\quad\\text{recalling}~\\eqref{loglikerisk},\\nonumber \\\\                           & = \\r_{\\infty}(s^t , x^t)-{1\\over t}\\log p(x^t),\\nonumber \\\\",
    "\\bar{r}_1(s^t)&:=-{1\\over t}\\sum_{t=1}^t\\log p_t(s_t),\\label{eqn : barr1prior}\\\\ \\bar{r}_{\\infty}(s^t)&=-{1\\over t}\\log p(s^t),\\quad\\text{recalling}~\\eqref{eq : rinftyprior},\\nonumber \\\\                       & = -{1\\over t}[\\log \\pi_{s_1}+",
    "\\sum_{t=1}^{t-1}\\log p_{s_t s_{t+1}}].\\end{aligned}\\ ] ] the newly introduced risk @xmath151 is the prior log - pseudo - likelihood .",
    "evidently , the combination @xmath152 corresponds to the map / viterbi decoding ; the combination @xmath153 yields the pmap case , whereas the combinations @xmath154 and @xmath155 give the _",
    "maximum a priori _ decoding and _ marginal prior mode _ decoding , respectively . the case @xmath156 subsumes and the case @xmath157 is the problem @xmath158.\\ ] ] thus , a solution to is a generalization of the viterbi decoding that allows one to suppress ( @xmath135 ) contribution of the data . _",
    "it is important to note that with @xmath159 every solution of ( [ gen - problem ] ) is admissible . _",
    "_ no less important , and perhaps a bit less obvious , is that @xmath160 also guarantees admissibility of the solutions _ , as stated in proposition [ prop : admit ] below .",
    "[ prop : admit]let @xmath160 .",
    "then , for _ almost every",
    "_ realization @xmath40 of the hmm process @xmath161 , the minimized risk is finite and any minimizer @xmath46 is admissible , i.e. satisfies @xmath162 .    without loss of generality ,",
    "assume @xmath156 .",
    "suppose the problem has no finite solution .",
    "then for any @xmath46 with @xmath163 , we would have some @xmath12 , @xmath63 , such @xmath164 .",
    "this would imply that @xmath165 for all @xmath46 with @xmath163 , contradicting the hypothesis that @xmath166 . now",
    ", suppose that @xmath46 is a minimizer of but @xmath167 .",
    "since @xmath163 , we would have some @xmath12 , @xmath63 , such @xmath168 .",
    "this would imply @xmath169 , and subsequently that @xmath164 and @xmath170 , contradicting optimality of @xmath46 .",
    "[ rem : pvd ] thus , note that the posterior - viterbi decoding @xcite can be obtained by either setting @xmath171 and taking @xmath172 , or setting @xmath156 and taking @xmath173 .",
    "if the smoothing probabilities @xmath118 , @xmath174 and @xmath119 , have been already computed ( say , by the usual forward - backward algorithm ) , a solution to can be found by a standard dynamic programming algorithm .",
    "let us first introduce more notation .",
    "for every @xmath175 and @xmath119 , let @xmath176 note that the function @xmath177 depends on the entire data @xmath42 .",
    "next , let us also define the following scores @xmath178 using the above scores @xmath179 and a suitable tie - breaking rule , below we define the back - pointers @xmath180 , terminal state @xmath181 , and the optimal path @xmath182.@xmath183 , \\quad \\hbox{when $ t=1,\\ldots , t-1 $ ; } \\nonumber   \\\\   i_t&:=           \\arg\\max_{i\\in s}\\delta_t(i).\\\\ \\label{st } \\hat s^{t}(j)&:=\\left\\ {                  \\begin{array}{ll }                    i_1(j ) , & \\hbox{when $ t=1 $ ; } \\\\",
    "\\big(\\hat s^{t-1}(i_{t-1}(j)),j \\big ) & \\hbox{when $ t=2,\\ldots , t$. }                  \\end{array }                \\right.\\end{aligned}\\ ] ] the following theorem formalizes the dynamic programming argument ; its proof is standard and we state it below for completeness only .",
    "[ dyn ] any solution to can be represented in the form @xmath184 provided the ties in are broken accordingly .    with a slight abuse of notation , for every @xmath185 ,",
    "let @xmath186,\\ ] ] where @xmath187 and @xmath188 .",
    "hence , @xmath189=u(s^t)\\ ] ] and any maximizer of @xmath190 is clearly a solution to and .",
    "next , note that @xmath191 for all @xmath119 , and that @xmath192 for @xmath193 and also @xmath185 . by induction on @xmath12 , these yield @xmath194 for every @xmath195 and for all @xmath119 .",
    "clearly , every maximizer @xmath196 of @xmath190 over the set @xmath197 must satisfy @xmath198 , or , more precisely @xmath199 , allowing for non - uniqueness . continuing to interpret @xmath200 as a set",
    ", recursion implies recursions and , hence any maximizer @xmath196 can indeed be computed in the form @xmath201 via the _ forward _ ( recursion ) -_backward _ ( recursion ) procedure .",
    "1similarly to the generalized risk minimization of , the generalized problem of accuracy optimization can also be further generalized as follows : @xmath202,\\ ] ] where risk @xmath203 is the error rate relative to the prior distribution .",
    "this problem apparently can be solved by the following recursion @xmath204 where now @xmath205 as in the generalized _ posterior - viterbi decoding _",
    ", here @xmath159 also implies admissibility of the optimal paths .",
    "however , unlike in , @xmath160 is not sufficient to guarantee admissibility of the solutions .",
    "we have been discussing a set of related ideas which allow us to balance path accuracy against path probabilities .",
    "next , we extend this discussion by presenting a couple of notably different approaches .      recall ( subsection [ sec : segment ] ) that rabiner s compromise between map and pmap is to maximize the expected number of correctly decoded pairs or triples of ( adjacent ) states . with",
    "@xmath68 being the length of the overlapping block ( @xmath206 ) this means to minimize the conditional risk @xmath207 which derives from the following loss function : @xmath208 obviously , for @xmath76 this gives the usual @xmath112 maximization  the pmap decoding  which is known to fault by allowing inadmissible paths .",
    "it is natural to think that minimizers of @xmath209 `` move '' towards viterbi paths `` monotonically '' as @xmath68 increases to @xmath69 . indeed , when @xmath210 , minimization of @xmath209 is equivalent to minimization of @xmath211 achieved by the viterbi decoding .",
    "however , as example [ sec : example ] shows below , minimizers of are not guaranteed to be admissible for @xmath212 , which is a drawback of using the loss @xmath213 .",
    "we now show that this drawback is easily overcome when the sum in is replaced by the product .",
    "certainly , these problems are not equivalent , and in particular with the product in place of the sum the @xmath68-block idea works well .",
    "namely , the longer the block , the larger the resulting path probability , which is also now guaranteed to be positive already for @xmath214 . moreover ,",
    "this gives another interpretation of the risks @xmath215 ( see also remark [ rem : pvd ] above ) and , though perhaps less interestingly , the prior risks @xmath216 .    let @xmath68 be a positive integer .",
    "for the time being , let @xmath217 represent any first order markov chain on @xmath197 , and let us define @xmath218 thus @xmath219 where @xmath220 thus , @xmath221 is a natural generalization of @xmath124 ( introduced first for the posterior distribution in ) since when @xmath76 , @xmath222 .",
    "[ k - block ] let @xmath68 be such that @xmath223",
    ". then the following recursion holds @xmath224    note that @xmath225 next , for all @xmath226 such that @xmath227 , the markov property gives @xmath228 and @xmath229 hence , @xmath230 the second equality above also follows from the markov property .",
    "taking logarithms on both sides and dividing by @xmath231 completes the proof .",
    "now , we specialize this result to our hmm context , and , thus , @xmath43 and @xmath232 are again the prior and posterior hidden path distributions .",
    "[ corollary0]let @xmath68 be such that @xmath223 .",
    "for all paths @xmath38 the prior risks @xmath221 and @xmath233 satisfy . for every @xmath234 and for all paths @xmath38 , the posterior risks @xmath221 and @xmath233 satisfy .",
    "@xmath235    clearly , conditioned on the data @xmath42 , @xmath59 remains a first order markov chain ( generally inhomogeneous even if it was homogeneous _ a priori _ ) .",
    "hence , theorem [ k - block ] applies .",
    "below , we focus on the posterior distribution and risks , even though the following would readily extend to any first order markov chain",
    ".    let @xmath236 be a classifier that minimizes @xmath237 .",
    "thus , @xmath238 we refer to such classifiers as _ k - block _ pvd or _",
    "k - block _ pmap as they naturally extend @xmath111 , the pmap / optimal accuracy / posterior decoder ( section [ subsec : viterbipmap ] ) . to be consistent with applications @xcite ,",
    "the term ` @xmath68-block posterior - viterbi decoding ' , however , is perhaps more accurate given the use of the product - based risk @xmath239 as opposed to @xmath240 .",
    "now , we present some properties of the new risks and decoders .",
    "[ corollary ] for every @xmath234 , and for every @xmath38 , we have @xmath241    equation follows immediately from equation of corollary [ corollary0 ] .",
    "inequality follows from inequalities below @xmath242 which in turn follow from equation of corollary [ corollary0 ] .",
    "also , equation implies that for every @xmath243 , @xmath244 which , together with inequality , implies .",
    "inequality means that the posterior path probability @xmath245 increases with @xmath68 .",
    "equation is also of practical significance showing that @xmath236 is a solution to with @xmath246 , @xmath247 , @xmath171 , and as such can be computed in the same fashion for all @xmath68 ( see theorem [ dyn ] above ) .",
    "thus , increasing @xmath68 increases @xmath124-risk , i.e. decreases the product of the ( conditional ) marginal probabilities of states along the path @xmath236 .",
    "inequalities and clearly show that as @xmath68 increases , @xmath248 monotonically moves from @xmath111 ( pmap ) towards the viterbi decoder , i.e. @xmath249 .",
    "however , the maximum block length is @xmath210 .",
    "a natural way to complete this bridging of pmap with map is by embedding the collection of risks @xmath221 into the family @xmath250 via @xmath251 $ ] .",
    "thus , extends to @xmath252 with @xmath253 and @xmath254 corresponding to the viterbi and pmap cases , respectively . given @xmath42 and a sufficiently small @xmath255 ( equivalently , large @xmath68 ) , @xmath236 , the minimizer of @xmath256 ( or , the right hand side of )",
    "would produce a viterbi path @xmath257 ( since @xmath197 is finite ) .",
    "however , such @xmath255 ( and @xmath68 ) would generally depend on @xmath42 , and in particular @xmath68 may need to be larger than @xmath69 , i.e. @xmath258 may be different from @xmath257 . at the same time , we clearly have @xmath259 on which we comment more in section below .",
    "an alternative that does not involve the risk functions is to simply transform the forward and backward variables @xmath260 and @xmath261 defined in .",
    "consider , for one example , the recursively applied power transformations given in below @xmath262^\\frac{1}{q}f_j(x_{t+1}),\\quad 1\\le t < t \\nonumber\\\\",
    "\\beta_{t}(j ; q ) & : = & \\left[\\sum_{i\\in    s}\\left(p_{ji}f_i(x_{t+1})\\beta_{t+1}(i ; q)\\right)^q\\right]^\\frac{1}{q},\\quad 1\\le t < t",
    "\\\\ \\beta_t(j ; q)&:=&\\beta_t(j)=1.\\nonumber\\end{aligned}\\ ] ] clearly , @xmath263 and @xmath264 , for all @xmath119 and all @xmath265 .",
    "thus , @xmath266 leads to the pmap decoding . using induction on @xmath12 and continuity of the power transform",
    ", it can also be seen that the following limits exist and are finite for all @xmath267 and all @xmath265 : @xmath268 and @xmath269 , where for @xmath270 @xmath271 and therefore , any viterbi path @xmath272 has the following property : @xmath273 this has been already been pointed out by @xcite , who , to the best of our knowledge , @xcite have been the only group to publish on the idea of hybridization of the pmap and viterbi decoders via a continuous transformation . ignoring potential non - uniqueness of viterbi paths , @xcite state , based on , that the viterbi path can be found _ symbol - by - symbol_.",
    "certainly , when viterbi paths are non - unique , symbol - by - symbol decoding based on can produce suboptimal , and in principle inadmissible , paths .",
    "in contrast to viterbi , non - unique pmap paths ( in the absence of constraints ) can certainly be found symbol - by - symbol .",
    "note also that for their hybrid of pmap with viterbi , @xcite use the following transformations : @xmath274 where @xmath275 ( in our notation ) and @xmath276 are required to be continuous on @xmath277 with finite limits as @xmath278 .",
    "these @xmath279 are then substituted for by appropriate expressions in terms of the recursively transformed forward and backward variables .",
    "the following points with regard to this hybridization idea have also motivated our present work :    1 .",
    "transformations appear to be somewhat more sophisticated than the power transforms .",
    "it appears that the only reason explicitly stated in @xcite for making their choice of transformation is to deliver the correct limits ( in that case pmap with @xmath280 and viterbi with @xmath281 ) . besides and , there are other ( single parameter ) transformations meeting this condition .",
    "2 .   implicitly , the authors of @xcite do recognize the usual problem of numerical underflow , but somehow appear to suggest that the rescaling trick @xmath282 would be sufficient to resolve this problem when computing their transformed variables .",
    "we could not experimentally confirm this optimism even with simple models .",
    "in fact , without addressing the scaling issue in full , the expressions and are short of defining practically meaningful path decoders .",
    "true , it is not difficult to renormalize these , or similar , expressions while preserving their limiting behavior .",
    "however , unlike in the original , i.e. untransformed , forward - backward algorithm , _ renormalization of the transformed forward and backward variables will alter the original decoders for intermediate values of the tuning parameter_. this can already be suspected by examining equations below @xmath283^\\frac{1}{q}f_j(x_{t+1 } ) } { \\sum_{l\\in s}\\left[\\sum_{i\\in    s}\\left(\\tilde \\alpha_t(i ; q)p_{il}\\right)^q\\right]^\\frac{1}{q}f_l(x_{t+1})},\\quad 1\\le t < t",
    "\\nonumber\\\\ \\tilde \\beta_{t}(j ; q ) & : = & \\frac{\\left[\\sum_{i\\in    s}\\left(p_{ji}f_i(x_{t+1})\\tilde \\beta_{t+1}(i ;    q)\\right)^q\\right]^\\frac{1}{q } } { \\sum_{l\\in s}\\left[\\sum_{i\\in    s}\\left(\\tilde \\alpha_t(i ;    q)p_{il}\\right)^q\\right]^\\frac{1}{q}f_l(x_{t+1})},\\quad 1\\le t <",
    "\\tilde \\beta_t(j ; q)&:=&\\beta_t(j)=1.\\nonumber\\end{aligned}\\ ] ] which implement the usual rescaling @xcite .",
    "3 .   moreover , algorithmically defined estimators are generally hard to analyze rigorously @xcite . in our context ,",
    "optimization criteria for intermediate members of the above interpolating families are indeed not clear , making it difficult to interpret the corresponding decoders .",
    "this might be discouraging should such decoders be included in more complex inference cycles ( i.e. when any genuine model parameters are estimated as well , e.g. viterbi training @xcite ) .",
    "other recursion schemes ( for example , cf .",
    "@xcite for derin s formula ) can surely be experimented with in a similar manner . however , now more than ten years after appearance of @xcite , we find that _ the value of any such interpolation is yet to be demonstrated .",
    "as already mentioned above , the symbol - by - symbol implementation of the transform - based hybrids is problematic when the solution is non - unique and full path probability is a factor .",
    "given a classifier @xmath74 and a risk function @xmath81 , the quantity @xmath284 evaluates the risk when @xmath74 is applied to a given sequence @xmath42 . when @xmath74 is optimal in the sense of risk minimization , then @xmath285 .",
    "we are also interested in the random variables @xmath286 .",
    "thus , in @xcite , convergence of several risks of the viterbi decoding has been considered .",
    "based on the asymptotic theory of viterbi processes @xmath287 @xcite , it has been shown that under fairly general assumptions on the hmm , the random variables @xmath288 , @xmath289 , @xmath290 as well as @xmath291 , @xmath292 and @xmath293 all converge to constant limits , _ a.s._. convergence of these risks obviously imply convergence of @xmath294 and @xmath295 the risks appearing in the generalized problems and , respectively .",
    "actually , convergence of @xmath296 is also proved ( and used in the proof of convergence of @xmath297 ) .",
    "hence , the risk in , evaluated at viterbi paths , converges as well .    the limits  _ asymptotic risks _  are ( deterministic ) constants that depend only on the model and evaluate the viterbi inference .",
    "for example , let @xmath298 be the limit of @xmath288 , which is the asymptotic misclassification rate of the viterbi decoding .",
    "thus , for big @xmath69 , the viterbi decoding makes about @xmath299 misclassification errors .",
    "the asymptotic risks might be , in principle , found theoretically , but as the limit theorems show , the limiting risks can also be estimated by simulations .    in @xcite , it has been also shown that under the same assumptions @xmath300 converges to a constant limit , say @xmath301 . in @xcite , @xmath302 has been also shown to converge .",
    "clearly @xmath303 , and even if their difference is small , the number of errors made by the viterbi decoder in excess of pmap in the long run can still be found significant .",
    "presently , we are not aware of a universal method for proving the limit theorems for these risks . convergence of the risks of the viterbi decoding is possible due to the existence of the so - called viterbi process ( see @xcite ) that has nice ergodic properties .",
    "the question whether infinite pmap processes have similar properties , is still open .",
    "therefore , convergence of @xmath304 was proven with a completely different method based on the smoothing probabilities .",
    "in fact , all of the limit theorems obtained thus far have been proven with different methods .",
    "we conjecture that these different methods can be combined so that convergence of the minimized combined risk or could be proven as well . in summary , as mentioned before , thus far convergence of the minimized combined risks has been obtained for trivial combinations only , i.e. with three of the four constants being zero .",
    "note that while convergence of the intermediate case @xmath305\\ ] ] with its minimizer @xmath306 is an open question , gives    @xmath307    this , together with the _ a.s . _ convergence of @xmath289 , implies that in the long run , for most sequences @xmath42 , @xmath308 will not exceed @xmath309 by more than @xmath310 .",
    "since this limit is finite , letting @xmath137 increase with @xmath69 , @xmath311 obviously approaches @xmath312 _ a.s .",
    "_ , i.e. as the intuition predicts , the likelihood of @xmath313 approaches to that of @xmath314",
    "certainly , the logarithmic risks , , , on the one hand , and the ordinary risks , , @xmath315 , , on the other , can be respectively combined into a single parameter family by , for example , the power transformation as shown below .",
    "let @xmath217 for the moment be any probability distribution on @xmath197 .",
    "@xmath316 thus , our two generalized problems and are naturally members of the same family of problems : @xmath317,\\ ] ] where @xmath146 and @xmath318 , @xmath147 , and @xmath148 .",
    "clearly , the dynamic programming approach of theorem  [ dyn ] and immediately applies to any member of the above family with @xmath319 .",
    "theorem [ k - block ] and corollaries [ corollary0 ] and [ corollary ] obviously generalize to higher order markov chains as can be seen below .",
    "let @xmath217 represent a markov chain of order @xmath27 , @xmath320 , on @xmath197 .",
    "then for any @xmath38 and for any @xmath321 , we have @xmath322    this is a straightforward extension of the proof of theorem .",
    "the present risk - based discussion of hmm path inference also naturally extends to the problem of optimal _ labelling _ or _ annotation _ ( section [ sec : segment ] ) .",
    "namely , the state space @xmath323 can be partitioned into subsets @xmath324 , @xmath325 , ",
    ", @xmath326 , for some @xmath327 , in which case @xmath328 assigns label @xmath25 to every state @xmath329 .",
    "the fact that the pmap problem is as easily solved over the label space @xmath330 as it is over @xmath197 has already been used in practice . indeed , adding the admissibility constraint , @xcite in effect average @xmath331 s within the label classes and",
    "then use recursions to obtain the pmap labelling , say , @xmath332 , of admissible state paths .",
    "this approach clearly corresponds to using the point loss @xmath333 in when solving @xmath334 .",
    "importantly , our generalized problem also immediately incorporates the above pointwise label - loss in either the prior @xmath335 or posterior risk @xmath336 , or both .",
    "since computationally these problems are essentially as light as and since @xcite report their special case to be useful in practice , we believe that the above generalizations offer yet more useful possibilities to practitioners . note the different kinds of averaging corresponding to different values of @xmath337 to be used with the @xmath112 risks : @xmath338    certainly , the choice of the basic loss functions , inflection parameters @xmath339 and weights @xmath340 of the respective risks , is application dependent , and can be tuned with the help of labelled data , using cross - validation .",
    "finally , these generalizations are presented for the standard hmm setting , and work on extending them to more complex and practically more useful hmm - based settings ( e.g semi - markov , autoregressive , etc . ) is underway .",
    "consider the following four - state mc transition matrix @xmath341 suppose observations @xmath342 and the emission densities @xmath24 @xmath343 are such that @xmath344 hence every admissible path begins and ends with 2 .",
    "thus , to simplify the notation , we assume without loss of generality that @xmath345 . amongst the paths that begin and end with 2 , the paths whose probabilities are listed below , are",
    "the only ones of positive ( prior ) probability ( the probabilities below are calculated up to the normalization constant ) :            the viterbi paths here are @xmath357 . also note that , for every @xmath147 , @xmath358 and @xmath359 .",
    "thus @xmath360 is also the unique pmap path .",
    "minimization of @xmath361 over all @xmath362 here is equivalent to maximization of @xmath363 , and the optimal paths are @xmath364 and @xmath365 .",
    "l.  bahl , j.  cocke , f.  jelinek , and j.  raviv .",
    "optimal decoding of linear codes for minimizing symbol error rate ( corresp . ) .",
    "_ information theory , ieee transactions on _ , 200 ( 2):0 284287 , 1974 .",
    "url http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1055186 .        christopher  m. bishop .",
    "_ pattern recognition and machine learning_. information science and statistics .",
    "springer , new york , 2006 .",
    "isbn 978 - 0387 - 31073 - 2 ; 0 - 387 - 31073 - 8 .",
    "doi : 10.1007/978 - 0 - 387 - 45528 - 0 .",
    "url http://dx.doi.org/10.1007/978-0-387-45528-0 .",
    "broa brejov , daniel  g. brown , and tom vina .",
    "the most probable annotation problem in hmms and its application to bioinformatics . _",
    "j comput syst sci _",
    ", 730 ( 7):0 1060  1077 , 2007 .",
    "issn 0022 - 0000 .",
    "doi : doi : 10.1016/j.jcss.2007.03.011 .",
    "url http://www.sciencedirect.com / science / article / b6wj0 - 4n85bd6 - 1/2/ffd66145% 030eb2db405304f3de9bf6c4[http://www.sciencedirect.com / science / article / b6wj0 - 4n85bd6 - 1/2/ffd66145% 030eb2db405304f3de9bf6c4 ] .",
    "bioinformatics iii .",
    "chris burge and samuel karlin .",
    "prediction of complete gene structures in human genomic dna .",
    "_ journal of molecular biology _ , 2680 ( 1):0 78  94 , 1997 .",
    "issn 0022 - 2836 .",
    "doi : doi : 10.1006/jmbi.1997.0951 .",
    "url http://www.sciencedirect.com / science / article / b6wk7 - 45vgf7t-9/2/0db81327% 54939b6d0d07e85a6276d801[http://www.sciencedirect.com / science / article / b6wk7 - 45vgf7t-9/2/0db81327% 54939b6d0d07e85a6276d801 ] .",
    "olivier capp , eric moulines , and tobias rydn .",
    "_ inference in hidden markov models_. springer series in statistics .",
    "springer , new york , 2005 .",
    "isbn 978 - 0387 - 40264 - 2 ; 0 - 387 - 40264 - 0 . with randal douc s contributions to chapter 9 and christian p. robert s to chapters 6 , 7 and 13 , with chapter 14 by gersende fort , philippe soulier and moulines , and chapter 15 by stphane boucheron and elisabeth gassiat .",
    "yariv ephraim and neri merhav .",
    "hidden markov processes .",
    "_ ieee trans .",
    "inform . theory _",
    ", 480 ( 6):0 15181569 , june 2002 .",
    "issn 0018 - 9448 .",
    "special issue on shannon theory : perspective , trends , and applications .",
    "piero fariselli , pier martelli , and rita casadio .",
    "a new decoding algorithm for hidden markov models improves the prediction of the topology of all - beta membrane proteins . _",
    "bmc bioinformatics _ , 60 ( suppl 4):0 s12 , 2005 .",
    "issn 1471 - 2105 .",
    "doi : 10.1186/1471 - 2105 - 6-s4-s12 .",
    "j.  hayes , t.  cover , and j.  riera .",
    "optimal sequence detection and optimal symbol - by - symbol detection : similar algorithms . _ communications , ieee transactions on _ , 300 ( 1):0 152157 , january 1982 .",
    "issn 0090 - 6778 .",
    "url http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=1095391 .",
    "g.  ji and j.  bilmes . backoff model training using partially observed data : application to dialog act tagging . in _ proc .",
    "human language techn .",
    "naacl , main conference _ , pages 280287 , new york city , usa , 2006 .",
    "association for computational linguistics .",
    "url http://www.aclweb.org/anthology/n/n06/n06-1036 .",
    "dhiraj joshi , jia li , and james  z. wang . a computationally efficient approach to the estimation of two- and three - dimensional hidden markov models .",
    "_ ieee trans . image process .",
    "_ , 150 ( 7):0 18711886 , 2006 .",
    "issn 1057 - 7149 .",
    "lukas kll , anders krogh , and erik l.  l. sonnhammer . an hmm posterior decoder for sequence feature prediction that includes homology information .",
    "_ bioinformatics _ , 210 ( suppl_1):0 i251257 , 2005 .",
    "doi : 10.1093/bioinformatics / bti1014 .",
    "url http://bioinformatics.oxfordjournals.org / cgi / content / abstract/21/suppl% _ 1/i251[http://bioinformatics.oxfordjournals.org / cgi / content / abstract/21/suppl% _ 1/i251 ] .",
    "alexey koloydenko and jri lember .",
    "infinite viterbi alignments in the two state hidden markov models .",
    "_ acta comment .",
    "_ , 0 ( 12):0 109124 , 2008 .",
    "issn 1406 - 2283 .",
    "8th tartu conf .",
    "multivariate statist .",
    "june 2007 .",
    "steffen  l. lauritzen .",
    "_ graphical models _ , volume  17 of _ oxford statistical science series_. the clarendon press oxford university press , new york , 1996 .",
    "isbn 0 - 19 - 852219 - 3 .",
    "oxford science publications .",
    "jri lember and alexey koloydenko .",
    "http://projecteuclid.org/handle/euclid.bj/1202492790/[the adjusted viterbi training for hidden markov models ] .",
    "_ bernoulli _ , 140 ( 1):0 180206 , february 2008 .",
    "issn 1350 - 7265 .",
    "doi : 10.3150/07-bej105 .",
    "jia li , robert  m. gray , and richard  a. olshen .",
    "multiresolution image classification by hierarchical modeling with two - dimensional hidden markov models .",
    "_ ieee trans .",
    "inform . theory _",
    ", 460 ( 5):0 18261841 , 2000 .",
    "issn 0018 - 9448 .",
    "information - theoretic imaging .",
    "shu lin and daniel  j. costello  jr . _ error control coding : fundamental and applications_. computer applications in electrical engineering .",
    "prentice - hall , inc .",
    ", englewood cliffs , new jersey 07632 , 1983 .",
    "isbn 0 - 13 - 283796-x .",
    "william  h. majoros and uwe ohler . _ knowledge discovery and emergent complexity in bioinformatics _ ,",
    "volume 4366/2007 of _ lecture notes in computer science _ , chapter advancing the state of the art in computational gene prediction , pages 81106 .",
    "springer berlin / heidelberg , 2007 .",
    "doi : 10.1007/978 - 3 - 540 - 71037 - 0_6 .",
    "url http://www.springerlink.com/content/x7h8126560375r25 .",
    "jose  l. marroquin , edgar arce  santana , and salvador botello .",
    "hidden markov measure field models for image segmentation .",
    "_ ieee trans . pattern anal .",
    "_ , 250 ( 11):0 13801387 , 2003 .",
    "issn 0162 - 8828 .",
    "doi : http://dx.doi.org/10.1109/tpami.2003.1240112 .",
    "joshua mason , kathryn watkins , jason eisner , and adam stubblefield . a natural language approach to automated cryptanalysis of two - time pads . in _",
    "ccs 06 : proceedings of the 13th acm conference on computer and communications security _ , pages 235244 , new york , ny , usa , 2006 .",
    "isbn 1 - 59593 - 518 - 5 .",
    "doi : http://doi.acm.org/10.1145/1180405.1180435 .",
    "c.  a. mcgrory , d.  m. titterington , r.  reeves , and a.  n. pettitt .",
    "variational bayes for estimating the parameters of a hidden potts model . _",
    "statistics and computing _ , 190 ( 3):0 329340 , 2009 .",
    "issn 0960 - 3174 .",
    "doi : http://dx.doi.org/10.1007/s11222-008-9095-6 .",
    "h.  ney , v.  steinbiss , r.  haeb - umbach , and _ et .",
    "_ an overview of the philips research system for large vocabulary continuous speech recognition . _",
    ". j. pattern recognit .",
    "_ , 80 ( 1):0 3370 , 1994 .",
    "patrick robertson , emmanuelle villebrun , and peter hoeher .",
    "a comparison of optimal and sub - optimal map decoding algorithms operating in the log domain . in _ communications , 1995 .",
    "icc 95 seattle , gateway to globalization , 1995 ieee international conference on _ , volume  2 , pages 10091013 , 1995 .",
    "doi : 10.1109/icc.1995.524253 .",
    "i.  shu , l.  hetherington , and j.  glass .",
    "baum - welch training for segment - based speech recognition . in _ proc .",
    "ieee asru workshop _ ,",
    "pages 4348 , st .",
    "thomas , u. s. virgin islands , http://groups.csail.mit.edu/sls/publications/2003/asru_shu.pdf , 2003 .",
    "a.  viterbi .",
    "http://ieeexplore.ieee.org/search/wrapper.jsp?arnumber=1054010[error bounds for convolutional codes and an asymptotically optimum decoding ] algorithm .",
    "_ ieee trans .",
    "inform . theory _",
    ", 130 ( 2):0 260269 , april 1967 .",
    "gerhard winkler .",
    "_ image analysis , random fields and markov chain monte carlo methods _ , volume  27 of _ applications of mathematics ( new york)_. springer - verlag , berlin , second edition , 2003 .",
    "isbn 3 - 540 - 44213 - 8 . a mathematical introduction , with 1 cd - rom ( windows ) ,",
    "stochastic modelling and applied probability ."
  ],
  "abstract_text": [
    "<S> motivated by the continuing interest in discrete time hidden markov models ( hmms ) , this paper reexamines these models using a risk - based approach . simple modifications of the classical optimization criteria for hidden path inference lead to a new class of hidden path estimators . </S>",
    "<S> the estimators are efficiently computed in the usual forward - backward manner and a corresponding dynamic programming algorithm is also presented . </S>",
    "<S> a particularly interesting subclass of such alignments are sandwiched between the most common _ maximum a posteriori _ ( map ) , or viterbi , path estimator and the minimum error , or _ </S>",
    "<S> pointwise maximum a posteriori _ ( pmap ) , estimator . </S>",
    "<S> similar to previous work , the new class is parameterized by a small number of tunable parameters . unlike their previously proposed relatives , the new parameters and class are more explicit and have clear interpretations , and bypass the issue of numerical scaling , which can be particularly valuable for applications .    </S>",
    "<S> risk , hmm , hybrid , interpolation , map sequence , viterbi algorithm , symbol - by - symbol , posterior decoding </S>"
  ]
}