{
  "article_text": [
    "graphical models such as markov random fields ( mrfs ) are widely used in many application domains , including spatial statistics , statistical signal processing , and communication theory .",
    "a fundamental limitation to their practical use is the infeasibility of computing various statistical quantities ( e.g. , marginals , data likelihoods etc . ) ; such quantities are of interest both bayesian and frequentist settings .",
    "sampling - based methods , especially those of the markov chain monte carlo ( mcmc ) variety  @xcite , represent one approach to obtaining stochastic approximations to marginals and likelihoods .",
    "a disadvantage of sampling methods is their relatively high computational cost .",
    "for instance , in applications with severe limits on delay and computational overhead ( e.g. , error - control coding , real - time tracking , video compression ) , mcmc methods are likely to be overly slow .",
    "it is thus of considerable interest for various application domains to consider less computationally intensive methods for generating approximations to marginals , log likelihoods , and other relevant statistical quantities .",
    "variational methods are one class of techniques that can be used to generate deterministic approximations in markov random fields ( mrfs ) . at the foundation of these methods",
    "is the fact that for a broad class of mrfs , the computation of the log likelihood and marginal probabilities can be reformulated as a convex optimization problem ( see  @xcite for an overview ) .",
    "although this optimization problem is intractable to solve exactly for general mrfs , it suggests a principled route to obtaining approximations ",
    "namely , by relaxing the original optimization problem , and taking the optimal solutions to the relaxed problem as approximations to the exact values . in many cases ,",
    "optimization of the relaxed problem can be carried out by `` message - passing '' algorithms , in which neighboring nodes in the markov random field convey statistical information ( e.g. , likelihoods ) by passing functions or vectors ( referred to as messages ) .",
    "estimating the parameters of a markov random field from data poses another significant challenge .",
    "a direct approach  for instance , via ( regularized ) maximum likelihood estimation",
    " entails evaluating the cumulant generating ( or log partition ) function , which is computationally intractable for general markov random fields .",
    "one viable option is the pseudolikelihood method  @xcite , which can be shown to produce consistent parameter estimates under suitable assumptions , though with an associated loss of statistical efficiency .",
    "other researchers have studied algorithms for ml estimation based on stochastic approximation  @xcite , which again are consistent under appropriate assumptions , but can be slow to converge .      as illustrated in figure  [ figestpred ] ,",
    "the problem domain of interest in this paper is that of joint estimation and prediction in a markov random field .",
    "more precisely , given samples @xmath0 from some unknown underlying model @xmath1 , the first step is to form an estimate of the model parameters . now suppose that we are given a noisy observation of a new sample path @xmath2 , and that we wish to form a ( near)-optimal estimate of @xmath3 using the fitted model , and the noisy observation ( denoted @xmath4 )",
    ". examples of such prediction problems include signal denoising , image interpolation , and decoding of error - control codes . disregarding any issues of computational cost and speed",
    ", one could proceed via route a in figure  [ figestpred]that is , one could envisage first using a standard technique ( e.g. , regularized maximum likelihood ) for parameter estimation , and then carrying out the prediction step ( which might , for instance , involve computing certain marginal probabilities ) by monte carlo methods .    this paper , in contrast ,",
    "is concerned with the _ computation - limited _ setting , in which both sampling or brute force methods are overly intensive . with this motivation ,",
    "a number of researchers have studied the use of approximate message - passing techniques , both for problems of prediction  @xcite as well as for parameter estimation  @xcite . however , despite their wide - spread use , the theoretical understanding of such message - passing techniques remains limited , especially for parameter estimation .",
    "consequently , it is of considerable interest to characterize and quantify the loss in performance incurred by using computationally tractable methods versus exact methods ( i.e. , route b versus a in figure  [ figestpred ] ) . more specifically , our analysis applies to variational methods that are based _",
    "convex relaxations_. this class includes a broad range of extant methods  among them the tree - reweighted sum - product algorithm  @xcite , reweighted forms of generalized belief propagation  @xcite , and semidefinite relaxations  @xcite .",
    "moreover , it is straightforward to modify other message - passing methods ( e.g. , expectation propagation  @xcite ) so as to `` convexify '' them .    at a high level ,",
    "the key idea of this paper is the following : given that approximate methods can lead to errors at both the estimation and prediction phases , it is natural to speculate that these sources of error might be arranged to partially cancel one another .",
    "the theoretical analysis of this paper confirms this intuition : we show that with respect to end - to - end performance , it is in fact beneficial , even in the infinite data limit , to learn the `` wrong '' the model by using _ inconsistent _",
    "methods for parameter estimation .",
    "en route to this result , we analyze the asymptotic properties of m - estimators based on convex variational relaxations , and establish a lipschitz stability property that holds for a broad class of variational methods .",
    "we show that joint estimation / prediction based on the reweighted sum - product algorithm substantially outperforms a commonly used heuristic based on ordinary sum - product .",
    "the remainder of this paper is organized as follows .",
    "section  [ secbackground ] provides background on markov random fields and associated variational representations , as well as the problem statement . in section  [ secconvsurr ] ,",
    "we introduce the notion of a convex surrogate to the cumulant generating function , and then illustrate this notion via the tree - reweighted bethe approximation  @xcite . in section  [ secjoint ] ,",
    "we describe how any convex surrogate defines a particular joint scheme for parameter estimation and prediction .",
    "section  [ secanal ] provides results on the asymptotic behavior of the estimation step , as well as the stability of the prediction step .",
    "section  [ secmixgauss ] is devoted to the derivation of performance bounds for joint estimation and prediction methods , with particular emphasis on the mixture - of - gaussians observation model . in section  [ secexperiments ] ,",
    "we provide experimental results on the performance of a joint estimation / prediction method based on the tree - reweighted bethe surrogate , and compare it to a heuristic method based on the ordinary belief propagation algorithm .",
    "we conclude in section  [ secdiscussion ] with a summary and discussion of directions for future work .",
    "consider an undirected graph @xmath5 , consisting of a set of vertices @xmath6 and an edge set @xmath7 .",
    "we associate to each vertex @xmath8 a multinomial random variable @xmath9 taking values in the set @xmath10 .",
    "we use the lower case letter @xmath11 to denote particular realizations of the random variable @xmath9 in the set @xmath12 .",
    "this paper makes use of the following exponential representation of a pairwise markov random field over the multinomial random vector @xmath13 .",
    "we begin by defining , for each @xmath14 , the @xmath15-valued indicator function @xmath16 & \\defn & \\begin{cases } 1 & \\mbox{if $ x_s = j$ } \\\\ 0 & \\mbox{otherwise } \\end{cases}\\end{aligned}\\ ] ] these indicator functions can be used to define a potential function @xmath17 via @xmath18\\end{aligned}\\ ] ] where @xmath19 is the vector of exponential parameters associated with the potential .",
    "our exclusion of the index @xmath20 is deliberate , so as to ensure that the collection of indicator functions @xmath21 , \\ ; j = 1 , \\ldots , { \\ensuremath{m}}- 1\\}$ ] remain affinely independent . in a similar fashion , we define for any pair @xmath22 the pairwise potential function @xmath23 \\ ; \\ind_k[x_t],\\end{aligned}\\ ] ] where we use @xmath24 to denote the associated collection of exponential parameters , and @xmath25 \\ ,",
    "\\ind_k[x_s ] , \\ ; j , k=1,2 , \\ldots , { \\ensuremath{m}}- 1 \\}$ ] for the associated set of sufficient statistics .",
    "overall , the probability mass function of the multinomial markov random field in exponential form can be written as @xmath26 here the function @xmath27\\end{aligned}\\ ] ] is the logarithm of the normalizing constant associated with @xmath28 .",
    "the collection of distributions thus defined can be viewed as a regular and minimal exponential family  @xcite . in particular , the exponential parameter @xmath29 and the vector of sufficient statistics @xmath30 are formed by concatenating the exponential parameters ( respectively indicator functions ) associated with each vertex and edge  viz .",
    "[ eqndefnover ] @xmath31    this notation allows us to write equation   more compactly as @xmath32 .",
    "a quick calculation shows that @xmath33 , where @xmath34 is the dimension of this exponential family .",
    "the following properties of @xmath35 are well - known : [ lembasic ] ( a ) the function @xmath35 is convex , and strictly so when the sufficient statistics are affinely independent .",
    "+ ( b ) it is an infinitely differentiable function , with derivatives corresponding to cumulants . in particular , for any indices @xmath36 , we have @xmath37 , \\qquad \\pdifftwo{\\partc}{\\eparam_\\sumind}{\\eparam_\\beta } = \\covv{\\clipot_\\sumind(x)}{\\clipot_\\beta(x)}{\\eparam},\\ ] ] where @xmath38 and @xmath39 denote the expectation and covariance respectively",
    ". we use @xmath40 to denote the vector of _ mean parameters _ defined element - wise by @xmath41 $ ] for any @xmath42 .",
    "a convenient property of the sufficient statistics @xmath30 defined in equations   and   is that these mean parameters correspond to marginal probabilities .",
    "for instance , when @xmath43 or @xmath44 , we have respectively    [ eqnover ] @xmath45 = { \\ensuremath{p(x_s = j \\ , ; \\ , \\eparam ) } } , \\quad \\mbox{and } \\\\",
    "\\label{eqnoverb } \\meanpar_{st;jk } & = & \\exs_\\eparam \\big \\ { \\ind_j[x_s]\\ , \\ind_k[x_t ] \\big \\ }",
    "= { \\ensuremath{p(x_s = j , x_t = k \\ , ; \\ , \\eparam)}}.\\end{aligned}\\ ] ]",
    "this section is devoted to a systematic procedure for constructing convex functions that represent approximations to the cumulant generating function .",
    "we begin with a quick development of an exact variational principle , one which is intractable to solve in general cases .",
    "( more details on this exact variational principle can be found in the papers  @xcite . ) nonetheless , this exact variational principle is useful , in that various natural relaxations of the optimization problem can be used to define convex surrogates to the cumulant generating function . after a high - level description of such constructions in general ,",
    "we then illustrate it more concretely with the particular case of the `` convexified '' bethe entropy  @xcite .",
    "since @xmath35 is a convex and continuous function ( see lemma  [ lembasic ] ) , the theory of convex duality  @xcite guarantees that it has a variational representation , given in terms of its conjugate dual function @xmath46 , of the following form @xmath47 in order to make effective use of this variational representation , it remains determine the form of the dual function .",
    "a useful fact is that the exponential family   arises naturally as the solution of an entropy maximization problem .",
    "in particular , consider the set of linear constraints @xmath48 \\",
    ", \\defn \\ ; \\sum_{x \\in \\statespn } p(x ) \\clipot_\\sumind(x ) & = & \\meanpar_\\sumind \\quad \\mbox{for $ \\sumind = 1 , \\ldots , \\df$},\\end{aligned}\\ ] ] where @xmath40 is a set of target mean parameters . letting @xmath49",
    "denote the set of all probability distributions with support on @xmath50 , consider the _ constrained entropy maximization problem _ : maximize the entropy @xmath51 subject to the constraints  .",
    "a first question is when there any distributions @xmath52 that satisfy the constraints  .",
    "accordingly , we define the set @xmath53 \\quad \\mbox{for some $ p \\in \\mathcal{p}$ } \\big \\},\\end{aligned}\\ ] ] corresponding to the set of @xmath54 for which the constraint set   is non - empty . for any the optimal value of the constrained maximization problem",
    "is @xmath55 ( by definition , since the problem is infeasible ) .",
    "otherwise , it can be shown that the optimum is attained at a unique distribution in the exponential family , which we denote by @xmath56 .",
    "overall , these facts allow us to specify the conjugate dual function as follows : @xmath57 see the technical report  @xcite for more details of this dual calculation . with this form of the dual function",
    ", we are guaranteed that the cumulant generating function @xmath35 has the following variational representation : @xmath58 however , in general , solving the variational problem   is intractable .",
    "this intractability should not be a surprise , since the cumulant generating function is intractable to compute for a general graphical model .",
    "the difficulty arises from two sources .",
    "first , the _ constraint set _",
    "@xmath59 is extremely difficult to characterize exactly for a general graph with cycles . for the case of a multinomial markov random field  , it can be seen ( using the minkowski - weyl theorem ) that @xmath59 is a polytope , meaning that it can be characterized by a finite number of linear constraints .",
    "the question , of course , is how rapidly this number of constraints grows with the number of nodes @xmath60 in the graph .",
    "unless certain fundamental conjectures in computational complexity turn out to be false , this growth must be non - polynomial ( see deza and laurent  @xcite for an in - depth discussion of the binary case ) .",
    "tree - structured graphs are a notable exception , for which the junction tree theory  @xcite guarantees that the growth is only linear in @xmath60 .",
    "second , the _ dual function _",
    "@xmath61 lacks a closed - form representation for a general graph .",
    "note in particular that the representation   is not explicit , since it requires solving a constrained entropy maximization problem in order to compute the value @xmath62 .",
    "again , important exceptions to this rule are tree - structured graphs . here",
    "a special case of the junction tree theory guarantees that any markov random field on a tree @xmath63 can be factorized in terms of its marginals as follows @xmath64 consequently , in this case , the negative entropy ( and hence the dual function ) can be computed explicitly as @xmath65 where @xmath66 and @xmath67 are the singleton entropy and mutual information , respectively , associated with the node @xmath68 and edge @xmath69 . for a general graph with cycles , in contrast , the dual function lacks such an explicit form , and is not easy to compute . given these challenges , it is natural to consider approximations to @xmath61 and @xmath59 .",
    "as we discuss in the following section , the resulting relaxed optimization problem defines a convex surrogate to the cumulant generating function .",
    "we now describe a general procedure for constructing convex surrogates to the cumulant generating function , consisting of two main ingredients . given the intractability of characterizing the marginal polytope @xmath59 , it is natural to consider a relaxation .",
    "more specifically , let @xmath70 be a convex and compact set that acts as an outer bound to @xmath59 .",
    "we use @xmath71 to denote elements of @xmath70 , and refer to them as _ pseudomarginals _ since they represent relaxed versions of local marginals .",
    "the second ingredient is to designed to sidestep the intractability of the dual function : in particular , let @xmath72 be a strictly convex and twice continuously differentiable approximation to @xmath61 .",
    "we require that the domain of @xmath72 ( i.e. , @xmath73 ) be contained within the relaxed constraint set @xmath70 .    by combining these two approximations",
    ", we obtain a convex surrogate @xmath74 to the cumulant generating function , specified via the solution of the following relaxed optimization problem @xmath75 note the parallel between this definition   and the variational representation of @xmath35 in equation  .",
    "the function @xmath74 so defined has several desirable properties , as summarized in the following proposition : [ propbasicprop ] any convex surrogate @xmath74 defined via   has the following properties :    1 .   for each @xmath76 ,",
    "the optimum defining @xmath74 is attained at a unique point @xmath77 .",
    "the function @xmath74 is convex on @xmath78 .",
    "it is differentiable on @xmath78 , and more specifically : @xmath79",
    "\\(i ) by construction , the constraint set @xmath70 is compact and convex , and the function @xmath72 is strictly convex , so that the optimum is attained at a unique point @xmath77 .",
    "+ ( ii ) observe that @xmath74 is defined by the maximum of a collection of functions linear in @xmath29 , which ensures that it is convex  @xcite .",
    "+ ( iii ) finally , the function @xmath80 satisfies the hypotheses of danskin s theorem  @xcite , from which we conclude that @xmath74 is differentiable with @xmath81 as claimed .",
    "given the interpretation of @xmath77 as a pseudomarginal , this last property of @xmath74 is analogous to the well - known cumulant generating property of @xmath35namely , @xmath82as specified in lemma  [ lembasic ] .",
    "the following example provides a more concrete illustration of this constructive procedure , using a tree - based approximation to the marginal polytope , and a convexifed bethe entropy approximation  @xcite . as with the ordinary bethe approximation  @xcite ,",
    "the cost function and constraint set underlying this approximation are exact for any tree - structured markov random field .    [",
    "[ relaxed - polytope ] ] relaxed polytope : + + + + + + + + + + + + + + + + +    we begin by describing a relaxed version @xmath70 of the marginal polytope @xmath83 .",
    "let @xmath84 and @xmath85 represent a collection of singleton and pairwise pseudomarginals , respectively , associated with vertices and edges of a graph @xmath86 .",
    "these quantities , as locally valid marginal distributions , must satisfy the following set of local consistency conditions : @xmath87 by construction , we are guaranteed the inclusion @xmath88 .",
    "moreover , a special case of the junction tree theory  @xcite guarantees that equality holds when the underlying graph is a tree ( in particular , any @xmath89 can be realized as the marginals of the tree - structured distribution of the form  ) .",
    "however , the inclusion is strict for any graph with cycles ; see appendix  [ appstrict ] for further discussion of this issue .",
    "[ [ entropy - approximation ] ] entropy approximation : + + + + + + + + + + + + + + + + + + + + + +    we now define an entropy approximation @xmath90 that is finite for any pseudomarginal @xmath71 in the relaxed set @xmath91 .",
    "we begin by considering a collection @xmath92 of spanning trees associated with the original graph . given @xmath89",
    ", there is  for each spanning tree @xmath93a unique tree - structured distribution that has marginals @xmath84 and @xmath85 on the vertex set @xmath94 and edge set @xmath95 of the tree . using equations   and  ,",
    "the entropy of this tree - structured distribution can be computed explicitly .",
    "convexified bethe entropy _ approximation is based on taking a convex combination of these tree entropies , where each tree is weighted by a probability @xmath96 $ ] . doing so and expanding the sum yields @xmath97 where @xmath98 $ ] are the _ edge appearance probabilities _ defined by the distribution @xmath99 over the tree collection . by construction ,",
    "the function @xmath90 is differentiable ; moreover , it can be shown  @xcite that it is strictly convex for any vector @xmath100 of strictly positive edge appearance probabilities .",
    "[ [ bethe - surrogate - and - reweighted - sum - product ] ] bethe surrogate and reweighted sum - product : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we use these two ingredients  the relaxation @xmath91 of the marginal polytope , and the convexified bethe entropy approximation  to define the following convex surrogate @xmath101 since the conditions of proposition  [ propbasicprop ] are satisfied , we are guaranteed that @xmath102 is convex and differentiable on @xmath78 , and moreover that @xmath103 , where ( for each @xmath76 ) the quantity @xmath77 denotes the unique optimum of problem  .",
    "perhaps most importantly , the optimizing pseudomarginals @xmath77 can be computed efficiently using a _ tree - reweighted variant _ of the sum - product message - passing algorithm  @xcite .",
    "this method operates by passing `` messages '' , which in the multinomial case are simply @xmath104-vectors of non - negative numbers , along edges of the graph .",
    "we use @xmath105 to represent the message passed from node @xmath106 to node @xmath107 . in the tree - reweighted variant ,",
    "these messages are updated according to the following recursion @xmath108^{\\rho_{ut } } } { \\big[{\\ensuremath{m}}_{st}(x_t)\\big]^{1-\\rho_{st}}}.\\end{aligned}\\ ] ] upon convergence of the updates , the fixed point messages @xmath109 yield the unique global optimum of the optimization problem   via the following equations    [ eqntrwmarg ] @xmath110^{\\rho_{us } } , \\quad \\mbox{and } \\\\",
    "\\taupar_{st}(x_s , x_t ; \\eparam ) & \\propto & \\exp \\big\\{\\eparam_s(x_s ) + \\eparam_t(x_t ) + \\frac{\\eparam_{st}(x_s , x_t)}{\\rho_{st } } \\big \\ }",
    "\\frac{\\prod \\limits_{u \\in \\neigh(s ) } \\big[{\\ensuremath{m}}_{us}(x_s)\\big]^{\\rho_{us } } \\ ;   \\prod \\limits_{v \\in \\neigh(s ) } \\big[{\\ensuremath{m}}_{vs}(x_s)\\big]^{\\rho_{vs } } } { { \\ensuremath{m}}_{st}(x_t ) \\ ; { \\ensuremath{m}}_{ts}(x_s ) } \\end{aligned}\\ ] ]    further details on these updates and their properties can be found in the paper  @xcite .",
    "we now turn to consideration of how convex surrogates , as constructed by the procedure described in the previous section , are useful for both approximate parameter estimation as well as prediction .",
    "suppose that we are given i.i.d .",
    "samples @xmath111 from an mrf of the form  , where the underlying true parameter @xmath112 is unknown .",
    "one standard way in which to estimate @xmath112 is via maximum likelihood ( possibly with an additional regularization term ) ; in this particular exponential family setting , it is straightforward to show that the ( normalized ) log likelihood takes the form @xmath113 where function @xmath114 is a regularization term with an associated ( possibly data - dependent ) weight @xmath115 .",
    "the quantities @xmath116 are the empirical moments defined by the data . for the indicator - based exponential representation  ,",
    "these empirical moments correspond to a set of singleton and pairwise marginal distributions , denoted @xmath117 and @xmath118 respectively .",
    "it is intractable to maximize the regularized likelihood directly , due to the presence of the cumulant generating function @xmath35 .",
    "thus , a natural thought is to use the convex surrogate @xmath74 to define an alternative estimator obtained by maximizing the regularized _ surrogate likelihood _ : @xmath119 by design , the surrogate @xmath74 and hence the surrogate likelihood @xmath120 , as well as their derivatives , can be computed in a straightforward manner ( typically by some sort of message - passing algorithm ) .",
    "it is thus straightforward to compute the parameter @xmath121 achieving the maximum of the regularized surrogate likelihood ( for instance , gradient descent would a simple though naive method ) .    for the tree - reweighted bethe surrogate  ,",
    "we have shown in previous work  @xcite that in the absence of regularization , the optimal parameter estimates @xmath122 have a very simple closed - form solution , specified in terms of the weights @xmath123 and the empirical marginals @xmath124 .",
    "( we make use of this closed form in our experimental comparison in section  [ secexperiments ] ; see equation  . ) if a regularizing term is added , these estimates no longer have a closed - form solution , but the optimization problem   can still be solved efficiently using the tree - reweighted sum - product algorithm  @xcite .      using such an estimator ,",
    "we now consider a joint approach to estimation and prediction . recalling the basic set - up , we are given an initial set of i.i.d .",
    "samples @xmath125 from @xmath126 , where the true model parameter @xmath112 is unknown .",
    "these samples are used to form an estimate of the markov random field .",
    "we are then given a noisy observation @xmath127 of a new sample @xmath128 , and the goal is to use this observation in conjunction with the fitted model to form a near - optimal estimate of @xmath129 .",
    "the key point is that the same convex surrogate @xmath74 is used both in forming the surrogate likelihood   for approximate parameter estimation , and in the variational method   for performing prediction .    for a given fitted model parameter @xmath76 ,",
    "the central object in performing prediction is the posterior distribution @xmath130 . in the exponential family",
    "setting , for a fixed noisy observation @xmath127 , this posterior can always be written as a new exponential family member , described by parameter @xmath131 .",
    "( here the term @xmath132 serves to incorporate the effect of the noisy observation . ) with this set - up , the procedure consists of the following steps : +    examples of the prediction task in the final step include smoothing ( e.g. , denoising of a noisy image ) and interpolation ( e.g. , in the presence of missing data ) .",
    "we provide a concrete illustration of such a prediction problem in section  [ secmixgauss ] using a mixture - of - gaussians observation model .",
    "the most important property of this joint scheme is that the _ convex surrogate _",
    "@xmath74 underlies both the parameter estimation phase ( used to form the surrrogate likelihood ) , and the prediction phase ( used in the variational method for computing approximate marginals ) .",
    "it is this matching property that will be shown to be beneficial in terms of overall performance .",
    "in this section , we turn to the analysis of the surrogate - based method for estimation and prediction .",
    "we begin by exploring the asymptotic behavior of the parameter estimator .",
    "we then prove a lipschitz stability result applicable to any variational method that is based on a strongly concave entropy approximation .",
    "this stability result plays a central role in our subsequent development of bounds on the performance loss in section  [ secmixgauss ] .",
    "we begin by considering the asymptotic behavior of the parameter estimator @xmath121 defined by the surrogate likelihood  .",
    "since this parameter estimator is a particular type of @xmath133-estimator , its asymptotic behavior can be investigated using standard methods , as summarized in the following : [ propvan ] let @xmath74 be a strictly convex surrogate to the cumulant generating function , defined via equation   with a strictly concave entropy approximation @xmath134 . consider the sequence of parameter estimates @xmath135 given by @xmath136 where @xmath114 is a non - negative and convex regularizer , and the regularization parameter satisfies @xmath137 .",
    "then for a general graph with cycles , the following results hold :    1 .",
    "we have @xmath138 , where @xmath139 is ( in general ) distinct from the true parameter @xmath112 .",
    "the estimator is asymptotically normal : @xmath140 & { \\stackrel{d}{\\longrightarrow } } & n \\biggr(0 , \\ , \\big(\\nabla^2 { \\ensuremath{b}}({\\ensuremath{\\estim{\\eparam}}})\\big)^{-1 } \\nabla^2 \\partc(\\eparams ) \\big(\\nabla^2 { \\ensuremath{b}}({\\ensuremath{\\estim{\\eparam}}})\\big)^{-1 } \\biggr)\\end{aligned}\\ ] ]    by construction , the convex surrogate @xmath74 and the ( negative ) entropy approximation @xmath72 are a fenchel - legendre conjugate dual pair . from proposition",
    "[ propbasicprop ] , the surrogate @xmath74 is differentiable .",
    "moreover , the strict convexity of @xmath74 and @xmath72 ensure that the gradient mapping @xmath141 is one - to - one and onto the relative interior of the constraint set @xmath70 ( see section 26 of rockafellar  @xcite )",
    ". moreover , the inverse mapping @xmath142 exists , and is given by the dual gradient @xmath143 .",
    "let @xmath144 be the moment parameters associated with the true distribution @xmath112 ( i.e. , in the limit of infinite data , the asymptotic value of the parameter estimate is defined by @xmath145 note that @xmath144 belongs to the relative interior of @xmath59 , and hence to the relative interior of @xmath70 .",
    "therefore , equation   has a unique solution @xmath146 .    by strict convexity",
    ", the regularized surrogate likelihood   has a unique global maximum .",
    "let us consider the optimality conditions defining this unique maximum @xmath121 ; they are given by @xmath147 , where @xmath148 denotes an arbitrary element of the subdifferential of the convex function @xmath114 at the point @xmath121 .",
    "we can now write @xmath149 - \\lambda^n \\partial { \\ensuremath{r}}({\\ensuremath{\\estim{\\eparam}}}^{\\ensuremath{n}}).\\end{aligned}\\ ] ] taking inner products with the difference @xmath150 yields @xmath151^t \\ ; \\left[{\\ensuremath{\\estim{\\eparam}}}^{\\ensuremath{n}}- { \\ensuremath{\\estim{\\eparam}}}\\right ] \\ ; \\leq \\ ; \\left[{\\ensuremath{\\estim{\\meanpar}}}^{\\ensuremath{n}}- \\meanpar^ * \\right]^t",
    "\\left[{\\ensuremath{\\estim{\\eparam}}}^{\\ensuremath{n}}- { \\ensuremath{\\estim{\\eparam}}}\\right ] + \\lambda^n \\partial { \\ensuremath{r}}({\\ensuremath{\\estim{\\eparam}}}^{\\ensuremath{n}})^t \\left[{\\ensuremath{\\estim{\\eparam}}}- { \\ensuremath{\\estim{\\eparam}}}^{\\ensuremath{n}}\\right],\\ ] ] where inequality ( a ) follows from the convexity of @xmath74 . from the convexity and non - negativity of @xmath114 , we have @xmath152 \\ ; \\leq \\ ; \\lambda^{\\ensuremath{n}}\\left [ { \\ensuremath{r}}({\\ensuremath{\\estim{\\eparam } } } ) - { \\ensuremath{r}}({\\ensuremath{\\estim{\\eparam}}}^{\\ensuremath{n}})\\right ] \\ ; \\leq \\ ; \\lambda^{\\ensuremath{n}}{\\ensuremath{r}}({\\ensuremath{\\estim{\\eparam}}}).\\ ] ] applying this inequality and cauchy - schwartz to equation   yields @xmath153^t \\ ; \\left[\\frac{{\\ensuremath{\\estim{\\eparam}}}^{\\ensuremath{n}}- { \\ensuremath{\\estim{\\eparam}}}}{\\|{\\ensuremath{\\estim{\\eparam}}}^{\\ensuremath{n}}- { \\ensuremath{\\estim{\\eparam}}}\\| } \\right ] \\ ; \\leq \\ ; \\|{\\ensuremath{\\estim{\\meanpar}}}^{\\ensuremath{n}}- \\meanpar^ * \\| + \\lambda^n { \\ensuremath{r}}({\\ensuremath{\\estim{\\eparam}}})\\ ] ] since @xmath154 by assumption and @xmath155 by the weak law of large numbers , the quantity @xmath156^t \\ ; \\left[\\frac{{\\ensuremath{\\estim{\\eparam}}}^{\\ensuremath{n}}- { \\ensuremath{\\estim{\\eparam}}}}{\\|{\\ensuremath{\\estim{\\eparam}}}^{\\ensuremath{n}}- { \\ensuremath{\\estim{\\eparam}}}\\| } \\right]$ ] converges in probability to zero . by the strict convexity of @xmath74",
    ", this fact implies that @xmath121 converges in probability to @xmath139 , thereby completing the proof of part ( a ) .    to establish part ( b ) , we observe that @xmath157 { \\stackrel{d}{\\longrightarrow}}n(0 , \\nabla^2 { \\ensuremath{a}}(\\eparams))$ ] by the central limit theorem .",
    "using this fact and applying the delta method to equation   yields that @xmath158 { \\stackrel{d}{\\longrightarrow}}n \\left(0 , \\nabla^2 \\partc(\\eparams ) \\right),\\ ] ] where we have used the fact that @xmath159 .",
    "the strict convexity of @xmath74 guarantees that @xmath160 is invertible , so that claim ( b ) follows .",
    "a key property of the estimator is its _ inconsistency_i.e .",
    ", the estimated model differs from the true model @xmath112 even in the limit of large data . despite this inconsistency",
    ", we will see that the approximate parameter estimates @xmath121 are nonetheless useful for performing prediction .",
    "a desirable property of any algorithm  particularly one applied to statistical data  is that it exhibit an appropriate form of stability with respect to its inputs .",
    "not all message - passing algorithms have such stability properties .",
    "for instance , the standard sum - product message - passing algorithm , although stable for relatively weakly coupled mrfs  @xcite , can be highly unstable in other regimes due to the appearance of multiple local optima in the non - convex bethe problem .",
    "however , previous experimental work has shown that methods based on convex relaxations , including the reweighted sum - product ( or belief propagation ) algorithm  @xcite , reweighted generalized bp  @xcite , and log - determinant relaxations  @xcite appear to be very stable .",
    "for instance , figure  [ figunstable ] provides a simple illustration of the instability of the ordinary sum - product algorithm , contrasted with the stability of the tree - reweighted updates .",
    "wiegerinck  @xcite provides similar results for reweighted forms of the generalized belief propagation .",
    "here we provide theoretical support for these empirical observations : in particular , we prove that , in sharp contrast to non - convex methods , any variational method based on a strongly convex entropy approximation is globally stable .",
    "this stability property plays a fundamental role in providing a performance guarantee on joint estimation / prediction methods .",
    "we begin by noting that for a multinomial markov random field  , the computation of the exact marginal probabilities is a globally lipschitz operation : [ lemexact ] for any discrete markov random field  , there is a constant @xmath161 such that @xmath162 this lemma , which is proved in appendix  [ appexact ] , guarantees that small changes in the problem parameters  that is , `` perturbations '' @xmath163lead to correspondingly small changes in the computed marginals .",
    "our goal is to establish analogous lipschitz properties for variational methods . in particular , it turns out that any variational method based on a suitably concave entropy approximation satisfies such a stability condition . more precisely , a function @xmath164 is _ strongly convex _ if there exists a constant @xmath165 such that @xmath166 for all @xmath167 . for a twice continuously differentiable function ,",
    "this condition is equivalent to having the eigenspectrum of the hessian @xmath168 be uniformly bounded below by @xmath169 . with this definition",
    ", we have : [ proplipstable ] consider any strictly convex surrogate @xmath74 based on a strongly concave entropy approximation @xmath134 .",
    "then there exists a constant @xmath170 such that @xmath171 from proposition  [ propbasicprop ] , we have @xmath172 , so that the statement is equivalent to the assertion that the gradient @xmath141 is a lipschitz function . applying the mean value theorem to @xmath141 , we can write @xmath173 where @xmath174 $ ] . consequently , in order to establish the lipschitz condition , it suffices to show that the spectral norm of @xmath175 is uniformly bounded above over all @xmath176 .",
    "since @xmath74 and @xmath72 are a strictly convex legendre pair , we have @xmath177^{-1}$ ] . by the strong convexity of @xmath72 , we are guaranteed that the spectral norm of @xmath178 is uniformly bounded away from zero , which yields the claim .",
    "many existing entropy approximations can be shown to be strongly concave . in appendix",
    "[ appconv ] , we provide a detailed proof of this fact for the convexified bethe entropy  . [ lemconv ] for any set @xmath179 of strictly positive edge appearance probabilities , the convexified bethe entropy   is strongly concave .",
    "we note that the same argument can be used to establish strong concavity for the reweighted kikuchi approximations studied by wiegerinck  @xcite .",
    "moreover , it can be shown that the gaussian - based log - determinant relaxation considered in wainwright and jordan  @xcite is also strongly concave .",
    "for all of these variational methods , then , proposition  [ proplipstable ] guarantees that the pseudomarginal computation is globally lipschitz stable , thereby providing theoretical confirmation of previous experimental results  @xcite .",
    "in this section , we develop theoretical bounds on the performance loss of our approximate approach to joint estimation and prediction , relative to the unattainable bayes optimum . so as not to unnecessarily complicate the result , we focus on the performance loss in the infinite data limit corrections , hold for the finite data setting . ] ( i.e. , for which the number of samples @xmath180 ) .    in the infinite data setting ,",
    "the bayes optimum is unattainable for two reasons :    1 .",
    "it is based on knowledge of the exact parameter @xmath112 , which is not easy to obtain .",
    "2 .   it assumes ( in the prediction phase ) that computing exact marginal probabilities @xmath54 of the markov random field is feasible .    of these two difficulties ,",
    "it is the latter assumption  regarding the computation of marginal probabilities  that is the most serious .",
    "as discussed earlier , there do exist computationally tractable estimators of @xmath112 that are consistent though not statistically efficient under appropriate conditions ; one example is the pseudolikelihood method  @xcite mentioned previously . on the other hand , mcmc methods may be used to generate stochastic approximations to marginal probabilities , but may require greater than polynomial complexity .    recall from proposition  [ propvan ] that the parameter estimator based on the surrogate likelihood @xmath120 is _ inconsistent _ , in the sense that the parameter vector @xmath139 returned in the limit of infinite data is generally not equal to the true parameter @xmath112 .",
    "our analysis in this section will demonstrate that this inconsistency is beneficial .",
    "although the ideas and techniques described here are more generally applicable , we focus here on a special observation model so as to obtain a concrete result .    [ [ observation - model ] ] observation model : + + + + + + + + + + + + + + + + + +    in particular , we assume that the multinomial random vector @xmath181 defined by the markov random field   is a label vector for the components in a finite mixture of gaussians . for each node @xmath68 , we specify a new random variable @xmath182 by the conditional distribution @xmath183 so that @xmath182 is a mixture of @xmath104 gaussians .",
    "such gaussian mixture models are widely used in spatial statistics as well as statistical signal and image processing  @xcite .",
    "now suppose that we observe a noise - corrupted version of @xmath184 namely , a vector @xmath4 of observations with components of the form @xmath185 where @xmath186 is additive gaussian noise , and the parameter @xmath187 $ ] specifies the signal - to - noise ratio ( snr ) of the observation model .",
    "note that @xmath188 corresponds to pure noise , whereas @xmath189 corresponds to completely uncorrupted observations .",
    "[ [ optimal - prediction ] ] optimal prediction : + + + + + + + + + + + + + + + + + + +    our goal is to compute an optimal estimate @xmath190 of @xmath129 as a function of the observation @xmath191 , using the mean - squared error as the risk function",
    ". the essential object in this computation is the posterior distribution @xmath192 , where the conditional distribution @xmath193 is defined by the observation model  . as shown in the sequel , the posterior distribution ( with @xmath127 fixed ) can be expressed as an exponential family member of the form @xmath194 ( see equation  ) .",
    "disregarding computational cost , it is straightforward to show that the optimal bayes least squares estimator ( blse ) takes the form @xmath195,\\end{aligned}\\ ] ] where @xmath196 denotes the marginal probability associated with the posterior distribution , and @xmath197 is the usual blse weighting for a gaussian with variance @xmath198 .",
    "[ [ approximate - prediction ] ] approximate prediction : + + + + + + + + + + + + + + + + + + + + + + +    since the marginal distributions @xmath196 are intractable to compute exactly , it is natural to consider an approximate predictor , based on a set @xmath71 of pseudomarginals computed from a variational relaxation .",
    "more explicitly , we run the variational algorithm on the parameter vector @xmath199 that is obtained by combining the new observation @xmath127 with the fitted model @xmath139 , and use the outputted pseudomarginals @xmath200 as weights in the approximate predictor @xmath201,\\end{aligned}\\ ] ] where the weights @xmath202 are defined in equation  .",
    "we now turn to a comparison of the bayes least - squares estimator ( blse ) defined in equation   to the surrogate - based predictor  . since ( by definition ) the blse is optimal for the mean - squared error ( mse ) , using the surrogate - based predictor will necessarily lead to a larger mse .",
    "our goal is to prove an upper bound on the maximal possible increase in this mse , where the bound is specified in terms of the underlying model @xmath112 and the snr parameter @xmath203 .",
    "more specifically , for a given problem , we define the mean - squared errors @xmath204 of the bayes - optimal and surrogate - based predictors , respectively .",
    "we seek upper bounds on the increase @xmath205 of the approximate predictor relative to bayes optimum .      before providing a technical statement and proof ,",
    "we begin with some intuition underlying the bounds , and the role of lipschitz stability .",
    "first , consider the low snr regime ( @xmath206 ) in which the observation @xmath4 is heavily corrupted by noise . in the limit @xmath207",
    ", the new observations are pure noise , so that the prediction of @xmath3 should be based simply on the estimated model ",
    "namely , the true model @xmath126 in the bayes optimal case , and the `` incorrect '' model @xmath208 for the method based on surrogate likelihood .",
    "the key point here is the following : by properties of the mle and surrogate - based estimator , the following equalities hold : @xmath209 here equality ( a ) follows from lemma  [ lembasic ] , whereas equality ( b ) follows from the moment - matching property of the mle in exponential families . equalities ( c ) and ( d ) hold from the proposition  [ propbasicprop ] and the pseudomoment - matching property of the surrogate - based parameter estimator ( see proof of proposition  [ propvan ] ) . as a key consequence , it follows that the combination of surrogate - based estimation and prediction is _ functionally indistinguishable _ from the bayes - optimal behavior in the limit of @xmath188 .",
    "more specifically , in the limiting case , the errors systematically introduced by the inconsistent learning procedure are cancelled out exactly by the approximate variational method for computing marginal distributions .",
    "of course , exactness for @xmath188 is of limited interest ; however , when combined with the lipschitz stability ensured by proposition  [ proplipstable ] , it allows us to gain good control of the low snr regime . at the other extreme of high snr ( @xmath210 ) ,",
    "the observations are nearly perfect , and hence dominate the behavior of the optimal estimator .",
    "more precisely , for @xmath203 close to @xmath211 , we have @xmath212 for all @xmath213 , so that @xmath214 .",
    "consequently , in the high snr regime , accuracy of the marginal computation has little effect on the accuracy of the predictor .",
    "although bounds of this nature can be developed in more generality , for simplicity in notation we focus here on the case of @xmath215 mixture components .",
    "we begin by introducing the factors that play a role in our bound on the performance loss @xmath216 .",
    "first , the lipschitz stability enters in the form of the quantity : @xmath217 where @xmath218 denotes the maximal singular value . following the argument in the proof of proposition  [ proplipstable ]",
    ", it can be seen that @xmath219 is finite .",
    "second , in order to apply the lipschitz stability result , it is convenient to express the effect of introducing a new observation vector @xmath127 , drawn from the additive noise observation model  , as a perturbation of the exponential parameterization . in particular , for any parameter @xmath33 and observation @xmath127 from the model  , the conditional distribution @xmath220 can be expressed as @xmath221 , where the exponential parameter @xmath222 has components    @xmath223    see appendix  [ appgamform ] for a derivation of these relations .",
    "third , it is convenient to have short notation for the gaussian estimators of each mixture component : @xmath224 with this notation , we have the following [ thmbound ] the mse increase is upper bounded by @xmath225 before proving the bound  , we begin by considering its behavior in a few special cases .    [ [ effect - of - snr ] ] effect of snr : + + + + + + + + + + + + + +    first , consider the low snr limit in which @xmath226 . in this limit , it can be seen that @xmath227 , so that the the overall bound @xmath228 tends to zero .",
    "similarly , in the high snr limit as @xmath229 , we see that @xmath230 for @xmath231 , which drives the differences @xmath232 , and in turn the overall bound @xmath228 to zero .",
    "thus , the surrogate - based method is optimal in both the low and high snr regimes ; its behavior in the intermediate regime is governed by the balance between these two terms .",
    "[ [ effect - of - equal - variances ] ] effect of equal variances : + + + + + + + + + + + + + + + + + + + + + + + + + +    now consider the special case of equal variances @xmath233 , in which case @xmath234 .",
    "thus , the difference @xmath235 simplifies to @xmath236 , so that the bound   reduces to @xmath237 as shown by the simpler expression  , for @xmath238 , the mse increase is very small , since such a two - component mixture is close to a pure gaussian .",
    "[ [ effect - of - mean - difference ] ] effect of mean difference : + + + + + + + + + + + + + + + + + + + + + + + + + +    finally consider the case of equal means @xmath239 in the two gaussian mixture components . in this case , we have @xmath240 \\ ; [ y_s - { \\ensuremath{\\nu}}]$ ] , so that the bound   reduces to @xmath241 ^ 2 \\ ; \\exs \\left \\ { \\min \\left ( 1 , \\ ; { \\ensuremath{l}}(\\eparams ; { \\ensuremath{\\estim{\\eparam } } } ) \\frac{\\|{\\ensuremath{\\gamma}}(\\ysca ; { \\ensuremath{\\alpha}})\\|_2}{\\sqrt{{\\ensuremath{n } } } } \\right ) \\sqrt{\\frac{\\sum_s ( \\ysca_s - { \\ensuremath{\\nu}})^4}{{\\ensuremath{n } } } } \\right \\}.\\end{aligned}\\ ] ] here the mse increase depends on the snr @xmath203 and the difference @xmath242 \\ ; \\left[{\\ensuremath{\\alpha}}^2 \\sigma^2_1 + ( 1-{\\ensuremath{\\alpha}}^2)\\right]}.\\end{aligned}\\ ] ] observe , in particular , that the mse increases tends to zero as the difference @xmath243 decreases .      by the pythagorean relation that characterizes the bayes least squares estimator @xmath244",
    ", we have @xmath245 using the definitions of @xmath246 and @xmath247 , some algebraic manipulation yields @xmath248 ^ 2 & = & \\left[\\tau_s({\\ensuremath{\\estim{\\eparam}}}+{\\ensuremath{\\gamma } } ) - \\mu_s(\\eparams + { \\ensuremath{\\gamma } } ) \\right]^2 \\ ; \\left [ { \\ensuremath{{\\ensuremath{g}}_1}}(y_s ) - { \\ensuremath{{\\ensuremath{g}}_0}}(y_s ) \\right]^2 \\\\ & \\leq & \\left |\\tau_s({\\ensuremath{\\estim{\\eparam}}}+ { \\ensuremath{\\gamma } } ) - \\mu_s(\\eparams+ { \\ensuremath{\\gamma } } ) \\right | \\left [ { \\ensuremath{{\\ensuremath{g}}_1}}(y_s ) - { \\ensuremath{{\\ensuremath{g}}_0}}(y_s ) \\right]^2,\\end{aligned}\\ ] ] where the second inequality uses the fact that @xmath249 since @xmath250 and @xmath251 are marginal probabilities . next we write @xmath252 ^ 2 \\nonumber \\\\ \\label{eqninter } & \\leq & \\frac{1}{\\sqrt{{\\ensuremath{n } } } } \\|\\tau({\\ensuremath{\\estim{\\eparam}}}+ { \\ensuremath{\\gamma } } ) - \\mu(\\eparams + { \\ensuremath{\\gamma } } ) \\|_2 \\ ; \\sqrt { \\frac{\\sum_{s=1}^{\\ensuremath{n}}\\left    where the last line uses the cauchy - schwarz inequality .",
    "it remains to bound the 2-norm @xmath253 .",
    "an initial naive bound follows from the fact @xmath254 $ ] implies that @xmath255 , whence @xmath256 an alternative bound , which will be better for small perturbations @xmath257 , can be obtained as follows . using the relation @xmath258 guaranteed by the definition of the ml estimator and surrogate estimator , we have @xmath259 + \\left [ \\mu(\\eparams ) - \\mu(\\eparams + { \\ensuremath{\\gamma}})\\right ] \\right \\|_2 \\\\ & = & \\left \\| \\left [ \\nabla^2 { \\ensuremath{b}}({\\ensuremath{\\estim{\\eparam}}}+ s { \\ensuremath{\\gamma } } ) - \\nabla^2 \\partc(\\eparams + t { \\ensuremath{\\gamma } } ) \\right ] { \\ensuremath{\\gamma}}\\right \\|_2,\\end{aligned}\\ ] ] for some @xmath260 $ ] , where we have used the mean value theorem .",
    "thus , using the definition   of @xmath261 , we have @xmath262 combining the bounds   and   and applying them to equation  , we obtain @xmath263 taking expectations of both sides yields the result .",
    "in order to test our joint estimation / prediction procedure , we have applied it to coupled gaussian mixture models on different graphs , coupling strengths , observation snrs , and mixture distributions . here",
    "we describe both experimental results to quantify the performance loss of the tree - reweighted sum - product algorithm  @xcite , and compare it to both a baseline independence model , as well as a closely related heuristic method that uses the ordinary sum - product ( or belief propagation ) algorithm .      in section  [ secjointprocedure ] , we described a generic procedure for joint estimation and prediction . here",
    "we begin by describing the special case of this procedure when the underlying variational method is the tree - reweighted sum - product algorithm  @xcite .",
    "any instantiation of the tree - reweighted sum - product algorithm is specified by a collection of edge weights @xmath123 , one for each edge @xmath264 of the graph .",
    "the vector of edge weights must belong to the spanning tree polytope ; see wainwright et al .",
    "@xcite for further background on these weights and the reweighted algorithm .",
    "given a fixed set of edge weights @xmath99 , the joint procedure based on the tree - reweighted sum - product algorithm consists of the following steps :    1 .",
    "given an initial set of i.i.d .",
    "data @xmath0 , we first compute the empirical marginal distributions @xmath265 , \\qquad \\estim{\\meanpar}_{st}(j , k ) \\defn \\frac{1}{{\\ensuremath{n } } } \\sum_{i=1}^{\\ensuremath{n}}\\ind[x^i_s = j ] \\ ; \\ind[x^i_t = k],\\ ] ] and use them to compute the approximate parameter estimate @xmath266 as shown in our previous work  @xcite , the estimates   are the global maxima of the surrogate likelihood   based on the convexified bethe approximation   without any regularization term ( i.e. , @xmath267 ) .",
    "2 .   given the new noisy observation @xmath4 of the form  , we incorporate it by by forming the new exponential parameter @xmath268 where equation   defines @xmath269 for the gaussian mixture model under consideration .",
    "we then compute approximate marginals @xmath270 by running the trw sum - product algorithm with edge appearance weights @xmath123 , using the message updates  , on the graphical model distribution with exponential parameter @xmath199 .",
    "we use the approximate marginals ( see equation  ) to construct the prediction @xmath271 in equation  .",
    "we evaluated the tree - reweighted sum - product based on its increase in mean - squared error ( mse ) over the bayes optimal predictor  .",
    "moreover , we compared the performance of the tree - reweighted approach to the following alternatives :    1 .   as a baseline , we used the _ independence model _ in which the mixture distributions at each node are all assumed to be independent . in this case , ml estimates of the parameters are given by @xmath272 , with all of the coupling terms @xmath273 equal to zero .",
    "the prediction step reduces to computing the bayes least squares estimate at each node independently , based only on the local data @xmath274 .",
    "the _ standard sum - product or belief propagation _ ( bp ) approach is closely related to the tree - reweighted sum - product method , but based on the edge weights @xmath275 for all edges . in particular , we first form the approximate parameter estimate @xmath139 using equation   with @xmath275 . as shown in our previous work  @xcite ,",
    "this approximate parameter estimate uniquely defines the markov random field for which the empirical marginals @xmath276 and @xmath277 are fixed points of the ordinary belief propagation algorithm .",
    "we note that a parameter estimator of this type has been used previously by other researchers  @xcite . in the prediction step",
    ", we then use the ordinary belief propagation algorithm ( i.e. , again with @xmath275 ) to compute approximate marginals of the graphical model with parameter @xmath199 .",
    "finally , based on these approximate bp marginals , we compute the approximate predictor using equation  .",
    "although our methods are more generally applicable , here we show representative results for @xmath278 components , and two different types of gaussian mixtures .    1 .",
    "mixture ensemble a is bimodal , with components @xmath279 and @xmath280 .",
    "mixture ensemble b was constructed with mean and variance components @xmath281 and @xmath282 ; these choices serve to mimic heavy - tailed behavior .    in both cases ,",
    "each mixture component is equally weighted ; see figure  [ figmixensemble ] for histograms of the resulting mixture ensembles .    [ cols=\"^,^,^ \" , ]     panel ( a ) provides the comparison ensembles of type a , with fixed variances @xmath283 and mean vectors @xmath284 ranging from @xmath285 to @xmath286 . note how the bounds capture the qualitative behavior for low snr , for which the difficulty of the problem increases as the mean separation",
    "is increased .",
    "in contrast , in the high snr regime , the bounds are extremely conservative , and fail to predict that the sharp drop - off in error as the snr parameter @xmath203 approaches one .",
    "this drop - off is particularly pronounced for the ensemble with largest mean separation ( marked with @xmath287 ) .",
    "panel ( b ) provides a similar comparison for ensembles of type b , with fixed mean vectors @xmath288 , and variances @xmath289 ranging from @xmath290 to @xmath291 . in this case , although the bounds are still very conservative in quantitative terms , they reasonably capture the qualitative behavior of the error over the full range of snr .",
    "key challenges in the application of markov random fields include the estimation ( learning ) of model parameters , and performing prediction using noisy samples ( e.g. , smoothing , interpolation , denoising ) . both of these problems present substantial computational challenges for general markov random fields . in this paper ,",
    "we have described and analyzed methods for joint estimation and prediction that are based on convex variational methods .",
    "our central result is that using inconsistent parameter estimators can be beneficial in the computation - limited setting .",
    "indeed , our results provide rigorous confirmation of the fact that using parameter estimates that are `` systematically incorrect '' is helpful in offsetting the error introduced by using an approximate method during the prediction step . in concrete terms",
    ", we demonstrated that a joint prediction / estimation method using the tree - reweighted sum - product algorithm yields good performance across a wide range of experimental conditions .",
    "although our work has focused on a particular scenario , we suspect that similar ideas and techniques will be useful in related applications of approximate methods for learning and prediction .",
    "this work was supported by an alfred p. sloan foundation fellowship , an okawa foundation research fellowship , an intel corporation equipment grant , and nsf grant dms-0528488 .",
    "as an illustration on the single cycle on @xmath292 vertices , the pseudomarginal vector with elements @xmath293 belongs to @xmath91 for all choices @xmath294 $ ] , but fails to belong to @xmath59 , for instance , when @xmath295 .",
    "using lemma  [ lembasic ] and the mean value theorem , we write @xmath296 for some @xmath297 . hence , it suffices to show that the eigenspectrum of the hessian is uniformly bounded above by @xmath298 .",
    "the functions @xmath30 are all 0 - 1 valued indicator functions , so that the diagonal elements of @xmath299 are bounded above  in particular , @xmath300 for any index @xmath301 .",
    "consequently , we have @xmath302 as required .",
    "consider a spanning tree @xmath93 of @xmath86 with edge set @xmath95 . given a vector @xmath303 , we associate with @xmath93 a subvector @xmath304 formed by those components of @xmath71 associated with vertices @xmath94 and edges @xmath95 .",
    "note that by construction @xmath305 .",
    "the mapping @xmath306 can be represented by a projection matrix @xmath307 with the block structure @xmath308 in this definition , we are assuming for convenience that @xmath71 is ordered such that the @xmath309 components corresponding to the tree @xmath93 are placed first . with this notation",
    ", we have @xmath310",
    ".    by our construction of the function @xmath102 , there exists a probability distribution such that @xmath311 , where @xmath312 denotes the negative entropy of the tree - structured distribution defined by the vector of marginals @xmath304 .",
    "hence , the hessian of @xmath102 has the decomposition @xmath313 to check dimensions of the various quantities , note that @xmath314 is a @xmath315 matrix , and recall that each matrix @xmath316 .",
    "now by lemma  [ lemexact ] , the eigenvalues of the @xmath317 are uniformly bounded above ; hence , the eigenvalues of @xmath318 are uniformly bounded away from zero .",
    "hence , for each tree @xmath93 , there exists a constant @xmath319 such that for all @xmath320 @xmath321 substituting this relation into our decomposition   and expanding the sum over @xmath93 yields @xmath322 \\sum_{s \\in \\vertex } \\|z\\{s\\}\\|^2 + \\sum_{(s , t ) \\in \\edge } \\big [ \\sum_{\\treegr \\in \\tract } { \\ensuremath{\\rho}}(\\treegr ) { \\ensuremath{c}}_\\treegr \\ind[(s , t ) \\in \\edge(\\treegr ) ] \\ ; \\big ] \\ ; \\|z\\{(s , t ) \\ } \\|^2 .",
    "\\qquad\\end{aligned}\\ ] ] defining @xmath323 , we have the lower bounds @xmath324 & \\geq & { \\ensuremath{c}}^ * \\sum_{\\treegr \\in \\tract } { \\ensuremath{\\rho}}(\\treegr ) \\ ; = \\ ; { \\ensuremath{c}}^ * \\ ; > \\ ; 0\\\\ \\sum_{\\treegr \\in \\tract } { \\ensuremath{\\rho}}(\\treegr ) { \\ensuremath{c}}_\\treegr \\ind[(s , t ) \\in \\edge(\\treegr ) ] & \\geq & { \\ensuremath{c}}^ * { \\treegr \\in \\tract } { \\ensuremath{\\rho}}(\\treegr ) \\ind[(s , t ) \\in \\edge(\\treegr ) ] \\ ; = \\ ; { \\ensuremath{c}}^ * { \\ensuremath{\\rho}}_{st } \\ ; \\geq \\ ; { \\ensuremath{c}}^ * { \\ensuremath{\\rho}}^ * > 0,\\end{aligned}\\ ] ] where @xmath325 . applying these bounds to equation   yields the final inequality @xmath326 with @xmath327 , which establishes that the eigenvalues of @xmath328 are bounded away from zero",
    "consider the observation model @xmath329 , where @xmath330 and @xmath184 is a mixture of two gaussians @xmath331 and @xmath332 . conditioned on the value of the mixing indicator @xmath333 , the distribution of @xmath274 is gaussian with mean @xmath334 and variance @xmath335 .",
    "let us focus on one component @xmath336 in the factorized conditional distribution @xmath337 . for @xmath338",
    ", it has the form @xmath339 } } \\exp \\big \\ { - \\frac{1}{2 \\big[{\\ensuremath{\\alpha}}^2 \\sigma^2_j + ( 1 - { \\ensuremath{\\alpha}}^2 ) \\big ] } ( y_s - { \\ensuremath{\\alpha}}{\\ensuremath{\\nu}}_{j } ) ^2 \\big \\}.\\end{aligned}\\ ] ] we wish to represent the influence of this term on @xmath11 in the form @xmath340 for some exponential parameter @xmath341 .",
    "we see that @xmath341 should have the form @xmath342 } { \\big[{\\ensuremath{\\alpha}}^2 \\sigma^2_1 + ( 1 - { \\ensuremath{\\alpha}}^2 ) \\big ] } + \\frac{(y_s - { \\ensuremath{\\alpha}}{\\ensuremath{\\nu}}_0)^2 } { 2 \\big[{\\ensuremath{\\alpha}}^2 \\sigma^2_0 + ( 1 - { \\ensuremath{\\alpha}}^2 ) \\big ] } - \\frac{(y_s - { \\ensuremath{\\alpha}}{\\ensuremath{\\nu}}_1)^2}{2 \\big[{\\ensuremath{\\alpha}}^2 \\sigma^2_1 + ( 1 - { \\ensuremath{\\alpha}}^2 ) \\big ] } \\end{aligned}\\ ] ]                                                            m.  j. wainwright , t.  s. jaakkola , and a.  s. willsky .",
    "tree - reweighted belief propagation algorithms and approximate ml estimation by pseudomoment matching . in _ workshop on artificial intelligence and statistics _ , january 2003 ."
  ],
  "abstract_text": [
    "<S> consider the problem of joint parameter estimation and prediction in a markov random field : i.e. , the model parameters are estimated on the basis of an initial set of data , and then the fitted model is used to perform prediction ( e.g. , smoothing , denoising , interpolation ) on a new noisy observation . working under the restriction of limited computation , we analyze a joint method in which the _ same convex variational relaxation _ </S>",
    "<S> is used to construct an m - estimator for fitting parameters , and to perform approximate marginalization for the prediction step . </S>",
    "<S> the key result of this paper is that in the computation - limited setting , using an inconsistent parameter estimator ( i.e. , an estimator that returns the `` wrong '' model even in the infinite data limit ) can be provably beneficial , since the resulting errors can partially compensate for errors made by using an approximate prediction technique . </S>",
    "<S> en route to this result , we analyze the asymptotic properties of m - estimators based on convex variational relaxations , and establish a lipschitz stability property that holds for a broad class of variational methods . </S>",
    "<S> we show that joint estimation / prediction based on the reweighted sum - product algorithm substantially outperforms a commonly used heuristic based on ordinary sum - product .    * inconsistent parameter estimation in markov random fields : benefits in the computation - limited setting * +    department of statistics , uc berkeley + technical report 690 </S>"
  ]
}