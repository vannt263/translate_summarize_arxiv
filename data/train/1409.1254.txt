{
  "article_text": [
    "the past decade has seen the emergence of a concordance cosmology , @xmath9cdm , in which the contents of the universe are dominated by dark matter and dark energy .",
    "even though the basic parameters appear to be robustly measured , more stringent measurements are sought as a way to improve our understanding of the nature of these mysterious components , as well as a way to test the model against signatures of new physics @xcite .",
    "achieving better cosmography means two things . on the one hand ,",
    "increasingly higher quality data are being obtained ( e.g. * ? ? ?",
    "* ) in order to improve the precision of each method . on the other hand ,",
    "independent observational methods are being exploited to break the degeneracies inherent to each method and to uncover unknown systematic uncertainties , thus improving accuracy . with precision and accuracy rigorously under control",
    ", potential inconsistencies might reveal new physics , such as the presence of additional families of neutrinos or deviations from general relativity .    in the past few years",
    ", strong lens time delays @xcite have made something of a comeback , becoming an increasingly popular probe of cosmography @xcite .",
    "the configuration most suitable for this work consists of a quasar with variable luminosity , being lensed by a foreground elliptical galaxy that creates multiple images of the quasar ( e.g. , * ? ? ? * for a recent review ) .",
    "differences in optical paths and gravitational potentials give rise to time delays between the images . in turn , the observable time delays , combined with a model of the mass distribution in the main deflector and along the line of sight , provide information on the so - called time - delay distance , which is a combination of angular diameter distances .",
    "the time delay distance is primarily sensitive to the hubble constant @xcite , but can also constrain other cosmological parameters , especially with large numbers of time delay systems and in combination with other methods @xcite .    at the time of writing , only a fraction of the hundred or so known gravitationally lensed quasars has well - measured time delays , owing to the considerable observational challenge associated with this measurement .",
    "accurate time delays in the optical require long and well - sampled light curves as well as sophisticated algorithms that account for data irregularities and astrophysical effects such as microlensing ( e.g. , * ? ? ?",
    "radio wavelength light curves have been used to determine time delays with great accuracy ( e.g. , * ? ? ?",
    "* ) , but unfortunately are restricted to the radio - loud subset of systems . in all cases ,",
    "the success rate is limited by the intrinsic variability of the sources .",
    "the number of systems with known time delays is about to increase dramatically . in the immediate future ,",
    "as more lensed quasars are discovered ( e.g. via the strides program ) , there will be more opportunities to identify highly variable systems in cosmologically favorable configurations for targeted follow - up .",
    "the state - of - the - art project cosmograil with its newly developed methods @xcite has shown the potential power of extracting time delay data from sparsely sampled photometric data @xcite . in the near future",
    ", the upcoming cadenced optical imaging surveys will provide light curves for large samples of lensed quasars .",
    "for example , the large synoptic survey telescope ( lsst ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) will repeatedly observe approximately 18000 deg@xmath10 of sky for ten years , and is predicted to find and monitor several thousand time delay lens systems @xcite .    in preparation for this wealth of light curves , it is crucial to carry out a systematic study of the current algorithms for time delay determination .",
    "such an investigation has two main goals .",
    "the first is to determine whether current methods have sufficient precision and accuracy to exploit the kind of data anticipated in the next decade .",
    "identifying limitations and failure modes of current methods is a necessary step to develop the next generation of measurement algorithms . in parallel ,",
    "the second goal is to test the impact of different observational strategies .",
    "for example , what kind of cadence , duration , and sensitivity is required to obtain precise and accurate time delays ? is the lsst baseline strategy sufficient to meet the goals of time delay cosmography or can we identify changes that would improve the outcome ?    with these two goals in mind , a time delay challenge ( tdc ) was initiated in october 2013 . the challenge `` evil '' team ( gd , cdf , kl , pjm , nr , tt ) simulated large numbers of time delay light curves , including all anticipated physical and experimental effects .",
    "the wider community was then invited to extract time delay signals from these mock light curves , blindly , using their own algorithms as `` good teams . ''",
    "this invitation was made by the posting of an initial version of paper i  of this series @xcite on the arxiv.org preprint server , and on the tdc website ( http://timedelaychallenge.org/ ) .",
    "the two first ladders of this challenge are tdc0 and tdc1 .",
    "tdc0 consisted of a small set of simulated data , which was used mostly as a debugging and validation tool .",
    "tdc0 is discussed in detail in paper i. four statistics were used to evaluate the performance of every method s submitted time delays @xmath11 and uncertainties @xmath12 , in light of the the true time delay value ( defined as positive in the input ) , @xmath13 .",
    "these four metrics are : the success fraction @xmath14 where @xmath15 is the total number of light curves available for analysis in the ladder , the @xmath1 value : @xmath16 the `` precision '' @xmath17 and the `` accuracy '' or `` bias '' @xmath18    in addition to the sample metrics we also define the analogous metrics for each individual point @xmath19 , @xmath20 and @xmath21 . thus , * @xmath0 , @xmath2 and @xmath1 defined above are the averages of the individual point values .",
    "*    target thresholds in each of these sample metrics were set for the teams entering tdc0 .",
    "the seven  `` good '' teams whose methods passed these thresholds were given access to the tdc1 dataset , which consisted of several thousand light curves .",
    "this large number was motivated by the goals of revealing the potential biases of each algorithm at the sub - percent level and testing the ability of current pipelines to handle large volumes of data .    to put this challenge in cosmological context , absolute distance measurements with 1% precision and accuracy are highly desirable for the study of dark energy @xcite and other cosmological parameters .",
    "therefore , in order for the time delay method to be competitive it has to be demonstrated that the delays can be measured with sub - percent accuracy _ and _ that the combination of precision for each system and the available sample size is sufficient to bring the statistical uncertainties to sub - percent level in the near future . the total uncertainty on the time delay distance , and therefore on the derived cosmology , depends on both the time delay and on the residual uncertainties from modeling the lens potential and the structure along the line of sight .",
    "thus , controlling the precision and accuracy of the time delay measurement is a necessary , but not sufficient , condition . in this first challenge we focus on just the time delay aspect of the measurement .",
    "the assessment of residual systematic uncertainties in the other components of time delay lens cosmography , and the distillation of the time delay measurement biases and uncertainties into a single cosmology metric is left for future work .",
    "this paper focuses on tdc1 , the analysis period of which closed on 1 july 2014 , and it is structured as follows . contains a brief recap of the light curve generation process , and describes the design of tdc1 .",
    "in we describe the response of the community to the challenge and give a brief summary of each method that was applied , and then in we analyze the submissions .",
    "we look at some of the apparent implications of the tdc1 results for future survey strategies in , and briefly discuss our findings in . in we",
    "summarize our conclusions .",
    "in tdc1 , the `` evil '' team simulated several thousand realistic mock light curve pairs , using the methods outlined in paper i. in this section , we first describe the general 5-rung design of tdc1 , and then describe the process of generating these light curves step by step , revealing quantitative details of all the elements considered .",
    "* we emphasize that tdc1 was purely a light curve analysis challenge ; no additional information regarding the gravitational lensing configuration , such as positions of the multiple images , or redshifts of the source and deflector , was given .",
    "this choice was motivated by the goal of performing the simplest possible test of time delay algorithms . as discussed at the end of this paper",
    ", the inclusion of additional lensing information could provide means to further improve the performance of the methods .",
    "*      each rung of tdc1 represents a possible wide - field survey that has monitored sufficient sky area that we are in possession of light curves for 1000 gravitationally - lensed agn image pairs .",
    "the number of lens systems in this sample is somewhat less than 1000 : quad systems are presented as 2 pairs , flagged as coming from the same system but enabling two independent time delay measurements .",
    "the five rungs of tdc1 span a selection of possible observing strategies , ranging from a high cadence , long season dedicated survey ( such as cosmograil might evolve into ) , to the kind of `` universal cadence '' strategy that might be adopted for an `` all - sky '' synoptic imaging survey ( such as is being designed for lsst ) .",
    "the challenge allows four control variables to be investigated ( within small plausible ranges ) : cadence , sampling regularity , observing season length , and campaign duration . gives the values of these control variables for each rung .    to make the mock data generation more efficient , and to better enable comparison of results between the different rungs ,",
    "we re - used the same catalog of lenses for all the rungs .",
    "this trick was disguised from the `` good '' teams by randomly re - allocating the lightcurve identification labels in each rung .",
    "in addition , the random noise was independently generated in each rung . as a consequence ,",
    "the submissions for different rungs may be deemed independent , as if they had addressed 5000 lensed image pairs .    [",
    "cols=\"^,^,^,^,^,^ \" , ]                             shows some interesting diversity between methods . despite this",
    ", some approximate general trends can be seen .",
    "greater accuracy and success fractions seem to be associated primarily with longer seasons , but there is considerable scatter between submissions , perhaps due to residual outliers in some cases . in most methods , little dependence of accuracy on cadence ,",
    "campaign lengths beyond 5 years , or the regularity of the sampling was seen .",
    "the success fraction seems to be somewhat dependent on cadence but less so on campaign length . in general , the trends in precision with cadence and season length seem to be less marked , and show less scatter , than those in accuracy and success fraction . in general , cadence seems to be the most important factor for precision .",
    "while the variation of time delay measurement with observing strategy seems to be somewhat algorithm - dependent , we can nevertheless hope to capture the general trends just described .",
    "focusing on the the pycs - spl results , we derived a very approximate power - law model for how the @xmath0 , @xmath2 and @xmath3 metrics varied with the main three quantities that describe the observing strategies in the rungs , mean cadence ( cad ) , mean season length ( sea ) , and campaign length ( camp ) .",
    "we find : @xmath22 we can see that in this model , the accuracy metric @xmath0 is the most sensitive to the observing strategy .",
    "it is also the case that it is the metric most sensitive to how the outliers are rejected .",
    "rejecting outliers that have @xmath23 gives similar conclusions to those drawn here , but slightly different model parameters , in the sense that there is even stronger dependence of @xmath0 on the observing strategy . in both cases",
    "the dependence of @xmath0 on cadence is relatively weak .",
    "the season length and campaign length seem to be more important parameters : doubling either of these results in approximately a factor of two improvement in @xmath0 .",
    "we note that constraining the total number of observations weakens these dependencies somewhat : for example , at fixed cadence , lengthening the season means shortening the campaign , and in our model , @xmath24 then decreases only as the ratio of the season length to the campaign length to the power of 0.1 .",
    "the results of the fixed epoch number tests in bore this out .",
    "the precision and success fraction metrics dependence on observing strategy is weaker , but it is interesting to note that the precision depends more strongly on cadence than the season length , while the opposite is true for the success fraction .",
    "this can be understood qualitatively as the presence of large gaps reducing the overlap between light curves , making it more difficult to reliably and uniquely identify common features between them .",
    "conversely , if the signal is properly identified , then the precision is driven by the total number of observation points , i.e. a combination of cadence and campaign duration . as a rough rule of thumb , we might have in mind that season length largely determines bias , while cadence controls precision .",
    "the precision of an ensemble average parameter , such as the cosmological parameters , may yet depend primarily on season length , however , through the success fraction .",
    "these simple model conclusions represent small extrapolations ",
    "we did not , for example , test doubling the season length and cadence simultaneously  but they represent a first approximation to the response of the more accurate time delay estimation routines to variations in observing strategy .    finally , we note briefly the implications of this model for the sample of lensed quasars that was forecast for lsst by @xcite .",
    "rung  4 represents something like the `` universal cadence '' planned for lsst @xcite , albeit with slightly shorter seasons .",
    "a cadence of 6 days would be well within the reach of such a strategy , but would require using observations from most of the filters in the set . while in this work we have only simulated and analyzed single filter lightcurves , agn variability has been observed to be significantly correlated across the optical and near infra - red bands ( see e.g. , * ? ? ?",
    "* ) , and microlensing variability is expected , and observed , to vary smoothly with wavelength due to source size effects ( e.g. , * ? ? ?",
    "with sufficiently sophisticated algorithms we might expect to be able to measure time delays from multi - filter light curves with fidelity not dissimilar to that shown by the tdc1 methods tested here .",
    "the 3-day cadence of rung  1 could be achieved by lsst without changing the total number of visits ; the impact of such a strategy on the various different lsst science cases would need to be investigated .",
    "we take rungs  1 and  4 to span the range of possibilities for the lsst time sampling .",
    "our model suggests that , if outliers with @xmath25 can be rejected ( perhaps during a joint analysis of the ensemble ) , the cadence is effectively unimportant for time delay measurement bias , and with lsst we might expect to achieve an accuracy metric of @xmath26 .",
    "such a small time delay measurement bias is well below the systematic errors expected from lens modeling .",
    "meanwhile , the expected precision achievable per lens in the rung 1 and 4 cadences would be 2.64.3% , and the success fractions would be 2026% .",
    "the mock lenses used in this data challenge were not quite randomly drawn from the om10 catalog , but instead had approximately uniformly distributed @xmath27 image magnitudes within four broad magnitude bins ( ) . correcting for this , we find that we might , with the present - day algorithms ( tested here and represented by our simple model ) , expect to be able to make time delay measurements with the above accuracy in at least 20% of an lsst sample of 1990 lenses selected to have @xmath28 and @xmath29 days .",
    "this would correspond to a well - measured sample of around 400 lensed quasars .",
    "we must expect these numbers to be refined as the lsst observing strategy is defined , and further time delay measurement tests are carried out .",
    "in this section , we give a brief analysis of each method s performance , discussing how they performed and what can be improved in the future .",
    "we note that the performance of each method must be evaluated in multi - dimensional metric space .",
    "each `` good '' team had to make choices with respect to which metric to optimize .",
    "some teams decided to favor inclusiveness ( high @xmath3 ) at the cost of a higher fraction of outliers ( lower @xmath30 ) or lower precision @xmath2 , and vice - versa .",
    "in fact , some of the teams submitted multiple entries spanning the range of parameter space , and illustrating these competitive needs .",
    "therefore , at this stage it is not possible , nor useful , to identify a `` best '' submission , not even within each method .",
    "it is more fruitful to understand the tradeoffs and explore the range covered by each method , and then identify areas for improvement .",
    "the gp method attained its twin goals of an automated fitting pipeline and very good fit accuracy .",
    "the main issue to address is one of outliers , which can be handled in two ways : global clipping and image information .",
    "this team found that the outliers were not due to multi - modal fit distributions  indeed the fits often have better likelihood for the data than the truth .",
    "however , the cosmology derived from the outliers would be discrepant from the cosmology from the global fit ensemble , and in this way , outliers could be recognized and clipped .",
    "another approach would be to use information such as image separation ( not provided in tdc1 ) to recognize and discard discrepant fits .",
    "while these considerations would lower the accepted fraction of fits , the correction of the mean function discussed in raises the fraction over those given here .",
    "this , and a set of new but simple selection criteria ( no limits on precision were imposed by this team for tdc1 submissions ) , discussed in a follow - up paper by @xcite , give considerable improvement in the precision and fraction , and further improvement in accuracy .      the unblinding of the tdc1 simulated data provided valuable information on the behavior of this team s bayesian inference algorithm . for the most part ,",
    "the technique identified catastrophic outliers .",
    "however , some light curve pairs still resulted in large contributions to the @xmath1 estimator . identifying this subset of outliers that pass the quality cuts has provided valuable insight into the behavior of this technique , and will allow for future refinement and development to reduce the probability of mis - reconstructions .      throughout",
    "the challenge this team s main concern was to achieve a high @xmath3 value without having any outliers .",
    "this was achieved with @xmath31 for all five rungs .",
    "this conservative approach yielded average @xmath1 values of around  @xmath32 for different rungs with @xmath2 of about @xmath33 to  @xmath34 .",
    "as noted before , since @xmath1 and @xmath2 are correlated , by simply dividing all estimated errors by a factor of @xmath35 , @xmath1 of @xmath36 and @xmath2 of @xmath37 could be achieved trivially .",
    "after the true time delays were revealed , a calibration bias of 0.5 days for all the submissions was discovered , resulting in @xmath38 ( the method had been calibrated only on tdc0 rung  0 , owing to lack of time ) . by adding a calibration correction of @xmath39 days to all this team s submissions delay estimates , the bias was removed , improving @xmath0 to @xmath40 . to summarize , this method seems promising in both reliability and precision , and is automated in all steps .",
    "there is also the potential to improve the error estimation by doing appropriate simulations for each set of light curves separately .      after the release of the true time delays , this submission was re - examined to try to understand the reasons for the most severe errors , especially those in which the true time delay differed from the inference by @xmath41 ( between 9 and 18 cases in each rung out of a few hundred submitted ) . in four of the worst cases",
    ", the problem appeared to be unrealistically low errors fitted during the resampling process , possibly due to a small number of anomalous points , and not corrected by eye .",
    "this suggests that for a given set of light - curves , a minimum error based on the fits to the ensemble should be adopted .",
    "a significant fraction of the remaining severe errors were characterised by a pelt statistic vs.  time delay plot with a relatively bumpy and irregular minimum , even when the eye detected a good fit in terms of the number of coincident points of inflection .",
    "this is more difficult to quantify , but suggests that an addition to the resampling - derived error based on the shape of the pelt statistic may be useful .",
    "the d3cs classification of the light - curve pairs into different confidence categories proved valuable .",
    "all the resulting `` doubtless '' ( dou ) submissions ( @xmath42 , averaging accross all rungs ) are free from any catastrophic outliers . as an example , none of the point estimates from the vanilla spl method is farther than @xmath43 or @xmath44 days from the truth .",
    "for this same method , the less pure doupla submission ( @xmath45 ) is contaminated by @xmath46 of delays that are off by more than 20 days , or , alternatively , @xmath47 .",
    "interestingly , the d3cs estimates for time delays shorter than 50 days are systematically biased low , leading to a significant @xmath0 of approximately @xmath48 for d3cs .",
    "we speculate that this bias is perceptual and due to users involuntarily trying to maximize the overlap in the light curves .",
    "the sdi and spl techniques were not influenced by this bias in their initial conditions , and both reached a high accuracy , consistent with being unbiased .",
    "for these two numerical techniques , the @xmath1 metric values are close to unity , suggesting adequate to slightly over - estimated delay uncertainties .",
    "the implemented simplifications to the original techniques from @xcite seem therefore acceptable for the level of complexity present in the tdc1 data .      from the tdc1 feedback",
    ", it was realized that this procedure overestimates the uncertainties in the measured time delays , and hence was more prone to reporting catastrophic failures .",
    "this problem can be solved by using a gaussian filter of width equal to the median rather than the mean temporal sampling of the light curves in the process of simulating light curves having known time delays . with this choice",
    ", the intrinsic variability in the simulated light curves does not get smoothed out on short timescales .",
    "also , there were a few cases in the submissions where the measured and true time delays were discrepant at the level of @xmath49 .",
    "this points to a need to increase the plausible range of time delays around the measured delay over which the simulated light curves are generated to at least the 3@xmath50 confidence interval implied by the inferred uncertainty , rather than the 2@xmath50 confidence interval used in the tdc1 submissions .",
    "the time delay measurements can be improved further by exploring a range of reasonable values of free parameters , and selecting those which result in the smallest uncertainty in the measured time delay .",
    "these changes are now being rigorously tested on the tdc1 light curves and will be described in the paper by @xcite during the revision process .",
    "this team considered tdc1 to be a great opportunity to develop and improve their bayesian approach .",
    "considering the team s late entry into the challenge , the pragmatic bayesian perspective was taken @xcite , developing the approximate gibbs sampling scheme ( algorithm1 ) and applying it only to the most realistic rung ( rung  4 ) .",
    "the main advantage of this pragmatic approach was the fast convergence of its markov chains , saving some computational time , a desirable characteristic for analyzing large number of data sets .",
    "the method performs well in terms of precision and accuracy .",
    "however it produces error bars that are smaller than those from a fully - bayesian approach , though larger than an empirical bayesian approach , leading to a relatively high  @xmath1 . to be balanced ,",
    "several gibbs sampling schemes are being tested for the future .",
    "in the next decade , dedicated efforts and the lsst survey will deliver thousands of light curves for lensed quasars , ushering in a revolution in time - delay cosmology @xcite . in order to prepare for and make the most of this wealth of data , it is essential to ascertain whether current algorithms are sufficiently accurate , fast , and precise .",
    "it is also important to investigate the optimal observing strategies for time delay determination , both in dedicated monitoring campaigns and for lsst .    in order to investigate these two issues",
    ", we carried out the first strong lens time delay challenge ( tdc ) .",
    "after the preliminary time delay challenge tdc0 @xcite , the challenge `` evil '' team simulated several thousand time delay light curves and made them available to the community on the challenge website .",
    "seven `` good '' teams responded to the challenge , and blindly measured the time delays for tdc1 using 9 independent algorithms . a simple method implemented by the `` evil '' team as a baseline",
    "was also included .",
    "our main findings from analyzing the the blind tdc1 submissions can be summarized as follows .    *",
    "the measurement of time delays from thousands of realistic light curves in manageable amounts of cpu and investigator time has been demonstrated .",
    "this is a considerable achievement given that traditionally this process has been carried out only for very small numbers of light curves ( allowing investigators to spend significant amounts of time on each system ) .",
    "several independent approaches were successful , ranging from cross - correlation , to scatter minimization , to data modeling with gaussian processes and other suitable sets of basis functions . some methods relied heavily on visual inspection , while others were almost completely automated . * in rung  0  which simulates the typical observing parameters of a dedicated monitoring campaign like cosmograil  the best current algorithms can recover time delays with negligible bias ( often sub - percent ) and 3% precision for over 50% of the light curves .",
    "the error bars are generally reasonable , resulting in @xmath1 of order unity , while the fraction of outliers is also just a few percent .",
    "these were the requirements for a method to be competitive , as described in paper i. when enough information was present in the light curves , typically 6 independent algorithms were able to recover time delays within 10% of the truth .",
    "* as the data quality was degraded in the subsquent rungs  1 - 4 ( emulating some observing strategies possible with lsst ) , the fraction of usable light curves also decreased , hovering between 20 and 30% .",
    "outliers became more common , although they can be contained by suitably conservative algorithms , or by visual inspection .",
    "once outliers are excluded , the algorithms perform as well as in rung  0 , albeit with a smaller fraction of the light curves ( 10 - 30% ) yielding robust results with competitive precision and accuracy .",
    "a success fraction of 20% translates to an expected sample size of around 400 lensed quasars detected and measured by lsst to very high accuracy ",
    "well within the systematic error requirements of time delay cosmography .",
    "* we have derived approximate scalings for the time delay metrics as a function of observing parameters .",
    "season and campaign length appear to be the dominant terms controlling accuracy ( or bias ) and success rate , while the precision of the time delay is most strongly related to the cadence and campaign duration .",
    "much has been learned from this first blind time delay challenge , and the results provide useful guidance and reference for designing future experiments and improving the measurement algorithms .",
    "however , it should be emphasized that this challenge was designed to be somewhat simplistic . in particular , tdc1 consisted of a pure time delay estimation challenge from light curves alone : teams were not given the image positions , nor the deflector and source redshifts .",
    "it is likely therefore that the results of this challenge might be overly pessimistic . in real life",
    ", investigators will have access to the full lensing configurations , and will be able to use this information as a prior for their time delay inference ( for example using the lensing geometry for quads ) .    furthermore",
    ", a fully cosmological challenge should enable outlier rejection based on cosmological self - consistency in a joint analysis of the ensemble of lenses .",
    "it should be possible to identify and reject outliers that lead to cosmological parameters ( chiefly h@xmath51 ) that are inconsistent with those inferred from the majority of sample .",
    "another limitation of the simplicity of tdc1 is that the metrics measure how well an algorithm performs on time - delay estimation , not directly on cosmological parameter inference .",
    "given the encouraging results of tdc1 , we plan to overcome these two limitations in the future . in the short term",
    ", we plan to translate the simple metrics adopted here into a full cosmological estimation tool by introducing the available additional information , and justifiable assumptions about the underlying lens models . in the longer term , we plan to organize a second time delay challenge , to further test our ability to handle outliers , and to investigate the measurement of time delays from multi - band data , * and in which more information will be provided for each system with the ultimate goal for the `` good '' teams of inferring cosmological parameters , rather than just time delays .",
    "*    the tdc0 and tdc1 data will remain available at http://timedelaychallenge.org for any team who might be interested in using them for developing algorithms for strong lens time delay measurement .",
    "we acknowledge the lsst dark energy science collaboration for hosting several meetings of the `` evil '' team , and the private code repository used in this work .",
    "we thank the referee for constructive criticism which helped improved this paper .",
    "tt , cdf , kl acknowledge support from the national science foundation collaborative grant `` collaborative research : accurate cosmology with strong gravitational lens time delays '' ( ast-1312329 and ast-1450141 ) .",
    "tt gratefully acknowledges support by the packard foundation through a packard research fellowship .",
    "kl is supported by china scholarship council .",
    "the work of pjm was supported by the u.s .",
    "department of energy under contract number de - ac02 - 76sf00515 .",
    "vb and fc are supported by the swiss national science foundation ( snsf ) .",
    "mt acknowledges support by the dfg grant hi 1495/2 - 1 .",
    "a. a and a. s. wishes to acknowledge support from the korea ministry of education , science and technology , gyeongsangbuk - do and pohang city for independent junior research groups at the asia pacific center for theoretical physics .",
    "would like to acknowledge the support of the national research foundation of korea ( nrf-2013r1a1a2013795 ) .",
    "el is supported by doe grant de - sc-0007867 and contract no . de - ac02 - 05ch11231 .",
    "ah is supported by an nserc grant and thanks the institute for the early universe , korea for computational resources .",
    "aa , as , ah , el thank ibs korea for hospitality .",
    "the work of lam and arw was carried out at the jet propulsion laboratory , california institute of technology , under a contract with the national aeronautics and space administration .",
    "km is supported at harvard by nsf grant ast-1211196 .",
    "44 natexlab#1#1 , a. , & shafieloo , a. 2014 , arxiv:1410.8122    bostock , m. , ogievetsky , v. , & heer , j. 2011 , ieee trans .",
    "visualization & comp . graphics ( proc .",
    "infovis )    , d. , & moustakas , l.  a. 2009 , , 706 , 45    , b.  m. , king , l.  j. , fassnacht , c.  d. , & auger , m.  w. 2009 , , 397 , 311    , g. , fassnacht , c. , treu , t. , marshall , p. , liao , k. , hojjati , a. , linder , e.  v. , & rumbaugh , n. 2014 , , submitted , arxiv:1310.4830    , c.  d. , xanthopoulos , e. , koopmans , l.  v.  e. , & rusin , d. 2002 , , 581 , 823    , d. , hogg , d.  w. , lang , d. , & goodman , j. 2013 , , 125 , 306    harva , m. , & raychaudhury , s. 2006 , machine learning for signal processing . proceedings of the 2006 16th ieee signal processing society workshop    hojjati , a. , kim , a.  g. , & linder , e.  v. 2013 , phys.rev . , d87 , 123512    , a. , & linder , e.  v. 2014 , arxiv:1408.5143    , z. , tyson , j.  a. , acosta , e. , allsman , r. , anderson , s.  f. , andrew , j. , angel , r. , axelrod , t. , & others .",
    "2008 , arxiv:0805.2366    , b.  c. , bechtold , j. , & siemiginowska , a. 2009 , , 698 , 895    . 2009 , , 698 , 895    , c.  s. 2002 , , 578 , 25    , h. , kashyap , v.  l. , van dyk , d.  a. , connors , a. , drake , j.  j. , izem , r. , meng , x .-",
    "l . , min , s. , park , t. , ratzlaff , p. , siemiginowska , a. , & zezas , a. 2011 , , 731 , 126    , e.  v. 2011 , , 84 , 123529    .",
    "2012 , arxiv:1211.0310    , abell , p.  a. , allison , j. , anderson , s.  f. , andrew , j.  r. , angel , j.  r.  p. , armus , l. , arnett , d. , asztalos , s.  j. , axelrod , t.  s. , & others .",
    "2009 , arxiv:0912.0201    , c.  l. , ivezi ,  . , kochanek , c.  s. , kozowski , s. , kelly , b. , bullock , e. , kimball , a. , sesar , b. , westman , d. , brooks , k. , gibson , r. , becker , a.  c. , & de vries , w.  h. 2010 , , 721 , 1014    , m. 2007 , , 660 , 1    , m. , & marshall , p.  j. 2010 , , 405 , 2579    , d. , hjorth , j. , burud , i. , jakobsson , p. , & eliasdottir , a. 2006 , , 455 , l1    , d. , & hjorth , j. 2009 , , 507 , l49    , d. , & hjorth , j. 2010 , , 712 , 1378    , j. , hoff , w. , kayser , r. , refsdal , s. , & schramm , t. 1994 , , 286 , 775    , ade , p.  a.  r. , aghanim , n. , armitage - caplan , c. , arnaud , m. , ashdown , m. , atrio - barandela , f. , aumont , j. , baccigalupi , c. , banday , a.  j. , & et  al .",
    "2013 , arxiv:1303.5076    , s. , morgan , n. , & kochanek , c.  s. 2008 , , 673 , 34    , s. , stalin , c.  s. , & prabhu , t.  p. 2014 , arxiv:1404.2920    , s. , tewes , m. , stalin , c.  s. , courbin , f. , asfandiyarov , i. , meylan , g. , eulaers , e. , prabhu , t.  p. , magain , p. , van winckel , h. , & ehgamberdiev , s. 2013 , , 557 , a44    , s. 1964 , , 128 , 307    , p.  l. 2003 , arxiv:0304480    , p.  l. , wambsganss , j. , & lewis , g.  f. 2004 , , 613 , 77    , k.  b. , rix , h .- w . , shields , j.  c. , knecht , m. , hogg , d.  w. , maoz , d. , & bovy , j. 2012 , , 744 , 147    , m. , & paraficz , d. 2014 , , 437 , 600    , a. 2007 , , 380 , 1573    . 2012 , j. cosm .",
    ", 8 , 2    , a. , alam , u. , sahni , v. , & starobinsky , a.  a. 2006 , , 366 , 1081    , a. , & clarkson , c. 2010 , , 81 , 083537    , c. , lupton , r.  h. , bernardi , m. , blanton , m.  r. , burles , s. , castander , f.  j. , connolly , a.  j. , eisenstein , d.  j. , & others .",
    "2002 , , 123 , 485    , s.  h. , auger , m.  w. , hilbert , s. , marshall , p.  j. , tewes , m. , treu , t. , fassnacht , c.  d. , koopmans , l.  v.  e. , sluse , d. , blandford , r.  d. , courbin , f. , & meylan , g. 2013 , , 766 , 70    , s.  h. , treu , t. , blandford , r.  d. , freedman , w.  l. , hilbert , s. , blake , c. , braatz , j. , courbin , f. , & others .",
    "2012 , arxiv:1202.4459    , s.  h. , treu , t. , hilbert , s. , sonnenfeld , a. , auger , m.  w. , blandford , r.  d. , collett , t. , courbin , f. , fassnacht , c.  d. , koopmans , l.  v.  e. , marshall , p.  j. , meylan , g. , spiniello , c. , & tewes , m. 2014 , , 788 , l35    , m. , courbin , f. , & meylan , g. 2013 , , 553 , a120    , m. , courbin , f. , meylan , g. , kochanek , c.  s. , eulaers , e. , cantale , n. , mosquera , a.  m. , magain , p. , van winckel , h. , sluse , d. , cataldi , g. , vrs , d. , & dye , s. 2013 , , 556 , a22    , t. 2010 , , 48 , 87    , t. , marshall , p.  j. , cyr - racine , f .- y . , fassnacht , c.  d. , keeton , c.  r. , linder , e.  v. , moustakas , l.  a. , bradac , m. , & others .",
    "2013 , arxiv:1306.1272    , d.  h. , mortonson , m.  j. , eisenstein , d.  j. , hirata , c. , riess , a.  g. , & rozo , e. 2013 , , 530 , 87",
    "how sensitive is the distribution of mock lightcurves to the random realizations of the positions of the stars in the lens ?",
    "we generated 30 star field realizations , over fields 30 einstein radii ( @xmath52 ) by 30 @xmath52 in area , with different random seeds for each fixed @xmath53 or @xmath54 , and calculated the mean of their standard deviations as a characteristic measure of the rms fluctuation in the microlensing magnification .",
    "shows how this rms fluctuation varies as a function of @xmath53 .",
    "the top panel shows the case where the image arises at the minimum of the time delay surface ( where the eigenvalues of the hessian matrix are both positive and the image has positive parity ) ; the bottom panel shows the case where the image arises at a saddle point of the time delay surface ( where the eigenvalues have opposite signs and the parity is flipped compared to the original source ) . for both figures , significant trends , increasing when @xmath53 is small , and decreasing at larger @xmath53 , are apparent .",
    "these can be explained as follows .                    at small @xmath53 , when there are few stars , sparsely distributed in the field ,",
    "the magnification of each position is dominated by the nearest individual star , and the variation of the map increases with more stars that bring more caustics . however",
    ", when @xmath53 grows large , the magnification at any position becomes less affected by the addition of more stars , and the magnification and demagnification attributed to different stars will average away .",
    "the saddle - point images are more vulnerable to demagnification and hence show larger variations in their magnification maps ( see * ? ? ?",
    "* for more on the differences of microlensing between minima and saddle - point images ) .",
    "the lefthand panel of shows the effect of the macrolens convergence @xmath54 on the standard deviation of the source plane magnification map .",
    "@xmath54 affects the variation in two ways , changing the stellar density fraction , and also the macro magnification .",
    "these effects appear to approximately balance each other at high  @xmath54 . at low convergence ,",
    "the magnification and shear are also low , and the microlensing effects weaker .",
    "meanwhile , the righthand panel of shows the rms microlensing magnification fluctuation as a function of source size .",
    "as expected , the fluctuations are smoothed out at large source size , reducing the amplitude of the microlensing fluctuations and ensuring that the average microlensing magnification is unity ."
  ],
  "abstract_text": [
    "<S> we present the results of the first strong lens time delay challenge . </S>",
    "<S> the motivation , experimental design , and entry level challenge are described in a companion paper . </S>",
    "<S> this paper presents the main challenge , tdc1 , which consisted of analyzing thousands of simulated light curves blindly . </S>",
    "<S> the observational properties of the light curves cover the range in quality obtained for current targeted efforts ( e.g. ,  cosmograil ) and expected from future synoptic surveys ( e.g. ,  lsst ) , and include simulated systematic errors . </S>",
    "<S> seven  teams participated in tdc1 , submitting results from 78  different method variants . after a describing each method </S>",
    "<S> , we compute and analyze basic statistics measuring accuracy ( or bias ) @xmath0 , goodness of fit @xmath1 , precision @xmath2 , and success rate @xmath3 . for some methods we identify outliers as an important issue . </S>",
    "<S> other methods show that outliers can be controlled via visual inspection or conservative quality control . </S>",
    "<S> several methods are competitive , i.e. , give @xmath4 , @xmath5 , and @xmath6 , with some of the methods already reaching sub - percent accuracy . </S>",
    "<S> the fraction of light curves yielding a time delay measurement is typically in the range @xmath72040% . </S>",
    "<S> it depends strongly on the quality of the data : cosmograil - quality cadence and light curve lengths yield significantly higher @xmath3 than does sparser sampling . taking the results of tdc1 at face value , we estimate that lsst should provide around 400 robust time - delay measurements , each with @xmath5 and @xmath8 , comparable to current lens modeling uncertainties . in terms of observing strategies , we find that @xmath0 and @xmath3 depend mostly on season length , while p depends mostly on cadence and campaign duration .    . </S>"
  ]
}