{
  "article_text": [
    "one is often confronted with displaying an empirical probability density ( pd ) @xmath0 of a continuous variable @xmath1 .",
    "most commonly this is done using histograms .",
    "while that is entirely appropriate and parameter free when @xmath1 is confined to discrete values , in case of a continuous variable @xmath1 , one is faced with choosing a binsize , or even several binsizes .",
    "this is a frustrated problem : on one side one would like the binsize infinitesimally small , so that the resolution of the curve becomes perfect .",
    "on the other side the binsize has to be sufficiently big so that statistical fluctuations do not destroy the smoothness of the underlying curve .",
    "here we do not attempt to fine - tune the binsize parameter(s ) , but propose to by - pass the entire problem by using a method based on the cumulative distribution function ( cdf ) @xmath2 @xmath3 is a monotonically increasing function as @xmath0 can not be negative . in a range with @xmath4 , @xmath3 is strictly monotone .    given a time series of @xmath5 real numbers ( data ) , a parameter free estimate of the cdf , called empirical cdf ( ecdf ) , is well - known @xcite : the step function @xmath6 which , after sorting the data , increases by @xmath7 at each data point . unfortunately ,",
    "that does not help directly in getting a good estimate of the probability density .",
    "the derivative of a step function is a sum of dirac delta functions , whereas the probability density is often known to be a smooth function , as will be assumed in the forthcoming .",
    "so it appears that one needs some kind of interpolation of the cdf before taking the derivative .",
    "this is no fun , as one has to decide whether the interpolation of 2 , 3 , 4 , or @xmath8 points will work best .",
    "in contrast , plotting a histogram of the data is simple and robust .",
    "however , it is tedious to guess a smooth function from a histogram . typically , either the statistical errors on the bins are small , but the resolution of the function is bad , or the resolution is good , but the statistical errors are large .    let @xmath9 be the straight line , which is zero for @xmath10 and one for @xmath11 , where @xmath12 is the smallest and @xmath13 the largest data point .",
    "after subtracting @xmath9 from @xmath6 , a function is left over , which is zero for @xmath14 and @xmath11 , and in - between well - suited for a fourier series expansion .",
    "this leads to the desired smooth approximation as long as the expansion is sufficiently short , but will imitate every wiggle of the data , when carried too far .",
    "therefore , one needs a cut - off criterion .",
    "we provide this by using the kolmogorov @xcite test , which tells us whether the difference between the ecdf and an analytical approximation of the cdf is explained by chance .",
    "the result is a well - defined , smooth empirical estimate of the pd .",
    "our approach is so simple and straightforward , that one can hardly imagine that it is original , but we have not seen it in use before . whether there exists previous literature on it or not , it is certainly desirable to bring it to the attention of a general science community . with this paper",
    "we also distribute startup software , which should help to bridge initial barriers against applying the approach .    in the next section we review the cdf , its empirical estimate , and other preliminaries .",
    "section  [ pd ] explains our construction of pds and gives numerical examples .",
    "we consider independent events sampled from ( 1 )  a gaussian distributions , ( 2 )  a cauchy distribution , and data from ( 3 )  a markov chain monte carlo simulation , which creates an autocorrelated time series . summary and conclusions follow in section  [ conclusions ] .",
    "the appendix gives a listing of the main routine and explains web access to fortran  77 code , which reproduces the examples of this paper .",
    "assume we generate @xmath5 random numbers @xmath15 , @xmath16 , @xmath17 according to a distribution function @xmath3 .",
    "we may re - arrange the @xmath18 in increasing order . denoting the smallest value by @xmath12 , the next smallest by @xmath19 , etc .",
    ", we arrive at @xmath20 where @xmath21 is a permutation of @xmath22 . as long as the data are not yet created ,",
    "the @xmath23 are random variables , afterwards they are data .",
    "an estimator for the distribution function @xmath3 is the ecdf @xmath24 with @xmath25 and the definitions @xmath26 , @xmath27 .",
    "confidence limits can be obtained from the bimodal distribution : with @xmath28 defined by @xmath29 , @xmath30 , the probability to find @xmath8 data points with values @xmath31 and @xmath32 data points with @xmath33 is given by @xmath34    ( 150,155 ) ( 0 , 0 )    figure  [ fig_gaucdf100 ] shows an ecdf from 100 gaussian distributed random numbers together with the exact cdf . using marsaglia @xcite random numbers , the data were created for the probability density @xmath35 and sorted with the heapsort algorithm .",
    "the cdf is in this case determined by the error function : @xmath36 the computer code used was that of ref .",
    "@xcite .",
    "in contrast to histograms as estimators of a pd from data , the ecdf has the advantage that no free parameters are involved in its definition , but the probability density of events is encoded in its slope .",
    "this makes it often impossible to read off from graphs like fig .",
    "[ fig_gaucdf100 ] high probability regions , in particular , the point of maximum likelihood .",
    "an ecdf is well - suited for determining confidence intervals , however , and that can be further improved by switching from the cdf to the peaked cdf @xcite defined by @xmath37 by construction the maximum of the peaked cdf @xmath38 is at the median @xmath39 and @xmath40 .",
    "therefore , @xmath38 has two advantages over the cdf @xmath3 : the median is clearly exhibited and the accuracy of the ordinate is doubled .",
    "it looks a bit like a pd , but it is in essence still the integrated pd .",
    "( 150,155 ) ( 0 , 0 )    the peaked ecdf @xmath41 is defined in the same way , just replacing @xmath3 in eq .",
    "( [ pcdfdef ] ) by @xmath6 .",
    "figure  [ fig_gaupcdf100 ] displays the peaked ecdf and peaked cdf for the same data as used in fig .",
    "[ fig_gaucdf100 ] . while in these two figures deviations of the estimates from the exact function due to statistical deviations are clearly visible , the picture changes dramatically when one increases the number of data by a factor of 100 .",
    "( 150,155 ) ( 0 , 0 )    figure  [ fig_gaupcdf10000 ] shows the peaked ecdf from 10@xmath42000 gaussian random numbers together with the exact peaked cdf .",
    "a difference is no longer visible to the naked eye .",
    "note that for large @xmath5 use of a fast sorting algorithm is mandatory . for a heapsort the cpu time scales with @xmath43 .",
    "for details see , e.g. , ref .",
    "@xcite .",
    "estimation of confidence intervals is also illustrated in fig .",
    "[ fig_gaupcdf10000 ] : just pick the desired likelihood on the ordinate and follow the arrows to their termination points .",
    "call such a point @xmath44 , then the probability for @xmath45 is given by the value on the ordinate for arrows emerging from the left , and the probability @xmath46 for arrows emerging from the right .",
    "using one arrow from the left , and one arrow from the right , the probability content for the interval between these @xmath1 values is obtained by subtracting the appropriate numbers from one ( 70% for the @xmath1 values from the two inner and 95% for the @xmath1 values from the two outer arrows of fig .",
    "[ fig_gaupcdf10000 ] ) .",
    "do the empirical and exact cdfs of our figures agree ?",
    "the kolmogorov test @xcite answers this question .",
    "it returns the probability @xmath47 , that the difference between the analytical cdf and an ecdf from statistically independent data is due to chance .",
    "if the analytical cdf is exactly known and the data are indeed sampled from this distribution , @xmath47 is a uniformly distributed random variable in the range @xmath48 .",
    "turned around , if one is not sure about the exact cdf , or the data , or both , and @xmath47 is small ( say , @xmath49 ) one concludes that the difference between the proposed cdf and the data is not due to chance , but that one of the assumptions made is false .",
    "kolmogorov s ingenious test relies on no more than the maximum difference @xmath50 between the ecdf and the cdf . for the one - sided tests",
    "the relationship to @xmath47 is analytically known @xcite for any value of  @xmath5 . in practice one",
    "prefers the two - sided test for which @xmath51 is defined by @xmath52 an exact analytical conversion into @xmath47 is then not known , but an asymptotic expansion due to stephens @xcite gives satisfactory results for @xmath53 , which is for practically all applications sufficient .",
    "the test yields @xmath54 for the data of fig .",
    "[ fig_gaucdf100 ] and  [ fig_gaupcdf100 ] , and @xmath55 for the data of fig .",
    "[ fig_gaupcdf10000 ] .",
    "both values are consistent with the assumption that the difference between the data and the exact cdf is due to chance .",
    "we would like to construct an empirical probability density ( epd ) from an ecdf given by eq .",
    "( [ ecdfdef ] ) .",
    "the first idea , that comes to mind , is to differentiate smooth interpolations of @xmath6 .",
    "this turns out to be tedious as long as one is unable to find a simple , generic rule , which determines the optimal interpolation for the purpose at hand , and we do not pursue this line .    the method we propose consists of two well - defined steps .    1 .",
    "define as an initial approximation to @xmath6 a differentiable , monotonically increasing function @xmath56 , with details as specified below .",
    "fourier expand the remainder until the kolmogorov test yields @xmath57 ( there may be some flexibility in lowering @xmath58 ) .    for @xmath56",
    "we require @xmath59 where the interval @xmath60 $ ] has to lie within the range of the data : @xmath61 for pds with support on a compact interval , or with fast fall - off like for a gaussian distribution , the natural choice is @xmath62 and @xmath63 . in case of slow fall - off , like for a cauchy distributions , or other distributions with outliers , one has to restrict the analysis to @xmath60 $ ] regions , which are sufficiently populated by data .",
    "this can be interpreted as considering instead of @xmath0 the pd @xmath64 here the constant @xmath65 is defined by the normalization @xmath66 and empirically obtained by the left - out probability content of @xmath6 , i.e. , @xmath67 , where @xmath68 is the number of data in @xmath60 $ ] and @xmath5 the total number of data .",
    "we denote the cdf of @xmath69 by @xmath70 and its ecdf by @xmath71 . after calculating from @xmath71 an estimate @xmath72 of @xmath69 ,",
    "the estimate of @xmath0 is for @xmath73 $ ] given by @xmath74 .    for @xmath73 $ ]",
    "we restrict our choice of @xmath56 in this paper to the straight line , @xmath75 more elaborate definitions will likely give improvements in a number of situations ( we comment on that in the conclusions ) , but would at the present state just discourage applications .",
    "our point here is to show that good results are already obtained with the definition  ( [ f0linear ] ) .",
    "once @xmath56 is defined , the remainder of the ecdf is given by @xmath76 which we expand into the fourier series @xmath77 the cosine terms are not present due to the boundary conditions @xmath78 .",
    "the fourier coefficients follow from @xmath79 because in our case @xmath80 is the difference of a step function and a linear function , the integrals over the flat regions of the step function are easily calculated , and the integrals ( [ fcof ] ) are solved by adding them up .",
    "the fourier expansion ( [ rx ] ) is useless for a too large value of @xmath81 , because it will then reproduce all statistical fluctuations of the data . to get around this problem",
    ", we perform the two - sided kolmogorov test first between @xmath71 and @xmath56 ( @xmath82 ) , and then each time @xmath81 is incremented by @xmath83 .",
    "once @xmath84 is reached for the kolmogorov @xmath47 , we know that the difference between the data and our analytical approximation is explained by statistical fluctuations .",
    "the other way round , the only information left in the data is statistical noise .",
    "therefore , the expansion is terminated at that point .",
    "the thus obtained smooth estimate of the cdf , @xmath85 yields @xmath72 by differentiation , and our final estimate of the desired pd is : @xmath86 .",
    "one likes to attach error bars to the estimate of the pd .",
    "we do this by dividing the ( unsorted ) original data into jackknife @xcite bins and repeat the analysis for each bin . comparing the function values thus obtained at selected points , error bars follow in the usual jackknife way ( see @xcite for technical details ) .",
    "( 150,155 ) ( 0 , 0 )    ( 150,155 ) ( 0 , 0 )    figure  [ fig_gauhist ] shows a histogram of 51 bins for 2@xmath42000 random numbers generated according to the gaussian distribution ( the error bars follow follow from the variance @xmath87 of the bimodal distribution  ( [ bimodal ] ) with @xmath88 ) .",
    "figure  [ fig_gauj ] shows the estimate @xmath89 obtained from the same data with the method described in this section .",
    "we used @xmath62 and @xmath63 . for the estimate from all 2@xmath42000 data ,",
    "@xmath90 was reached with @xmath91 ( @xmath92 with @xmath93 ) .",
    "twenty jackknife bins were used to calculate the error bars . as all results of this section ,",
    "the analysis is fully reproducible with the programs provided on the web as described in the appendix .",
    "we consider the cauchy distribution defined by the pd @xmath94 which leads to the cdf @xmath95 due to the slow @xmath96 fall - off of the cauchy pd , neither its mean nor its variance are defined .",
    "( 150,155 ) ( 0 , 0 )    ( 150,155 ) ( 0 , 0 )    therefore , it comes to no surprise that we encounter considerably more difficulties in applying our method for cauchy data than for gaussian data or for the data of our next example . to overcome instabilities",
    ", the analysis has to be restricted to the central region and far more data than before are needed for stable estimates .",
    "we ended up with using 20@xmath42000 cauchy distributed random number and restricting the analysis to @xmath97 which came out to be @xmath98 and @xmath99 .",
    "figure  [ fig_cauhist ] depicts the 51-bins histogram of the data in this region .",
    "the estimate of the pd @xmath100 with our method is then shown in fig .",
    "[ fig_cauj ] . as before , twenty jackknife bins were used for the error estimates and for the estimate from all data @xmath101 was obtained for @xmath102 ( @xmath103 for @xmath104 ) .",
    "markov chain monte carlo ( mcmc ) simulations are widely employed in physics and other disciplines .",
    "data in a mcmc time series are autocorrelated , which makes straightforward application of the kolmogorov test questionable .",
    "here we illustrate for an example from lattice gauge theory ( lgt ) a way to deal with this problem .    in 4d u(1 ) lgt ,",
    "a double peak in the action has been observed on symmetric lattices @xcite .",
    "this is characteristic for a first order phase transition , although the situation in 4d u(1 ) lgt is somewhat questionable due to the weakness of the transition and other circumstances .    for this paper we have used the biased metropolis - heatbath algorithm of ref .",
    "@xcite to generate u(1 ) data at @xmath105 on an @xmath106 lattice .",
    "these are parameters appropriate for producing a double peak . in lgt and statistical physics",
    "a standard unit for the time series is one `` sweep '' , which corresponds for sequential updating ( as used in our simulations ) to updating each dynamical variable once .",
    "we have generated a time series of 2560@xmath42000 sweeps , which takes about twenty hours on a 2ghz pc .",
    "we calculated the integrated autocorrelation time @xmath107 of our time series and found @xmath108 sweeps .",
    "as the effective number of statistically independent data in an autocorrelated time series is given by @xcite @xmath109 , our measurement of @xmath107 implies that we have generated approximately @xmath110 independent measurements . in the spirit of data reduction",
    ", we then selected @xmath111 action values , separated by steps of @xmath112 sweeps , from the time series .",
    "( 150,155 ) ( 0 , 0 )    ( 150,155 ) ( 0 , 0 )    for these @xmath111 measurements , the histogram with 51 entries is plotted in fig .",
    "[ fig_u1hist ] , where @xmath113 is the action density .",
    "figure  [ fig_u1j ] shows the estimate @xmath114 obtained from the same data with our method , using @xmath62 and @xmath63 . for the estimate from all 2@xmath42000 data @xmath115",
    "was reached with @xmath116 ( @xmath117 with @xmath91 ) , and twenty jackknife bins were used to calculate the error bars .",
    "the improvement from fig .",
    "[ fig_u1hist ] to fig .",
    "[ fig_u1j ] is as good as the corresponding improvement for the gaussian distribution .",
    "this result sheds new light on an effect , which is well - known to workers in mcmc simulations , but has to our knowledge not been satisfactorily explained , the amazing smoothing , which one obtains when one includes all autocorrelated data in the histogram . in our case",
    "this is 640@xmath42000 data points , as we have taken measurements every 4 sweeps .",
    "the histogram error bars then fall almost on top of the jackknife error bars of fig .",
    "[ fig_u1j ] .",
    "how can that be when there is little or no additional information in the autocorrelated data ?",
    "part of the answer appears to be that most of this information is already in the statistically independent data , but is usually not exploited .",
    "to someone used to plotting histogram , our method may appear difficult , but indeed it is not .",
    "true , first one has to sort the data to calculate their ecdf , then from it the empirical pd , using fourier series expansion and kolmogorov tests , and finally jackknife error bars need to be calculated , but all these steps are standard . besides the subroutine shown in the appendix , another short subroutine , and the main program , we had no programming to do .",
    "all other routines were already available in the web based fortran  77 code of ref .  @xcite .",
    "once the archive described in the appendix is downloaded from the web , and the program is up and running , the extra effort compared to plotting histograms is almost negligible . and the differences between fig .",
    "[ fig_gauhist ] and fig .",
    "[ fig_gauj ] , fig .",
    "[ fig_cauhist ] and fig .",
    "[ fig_cauj ] , and finally fig .",
    "[ fig_u1hist ] and fig .",
    "[ fig_u1j ] speak for themselves .",
    "it is astonishing that these results could be obtained with a simple straight line ( [ f0linear ] ) as initial approximation for the cdf .",
    "there is certainly space for improvement at the price of giving up the presently achieved simplicity . for our examples one and two ,",
    "the exact cdfs are known and pointed out by eqs .",
    "( [ gaucdf ] ) and ( [ cauchycdf ] ) .",
    "obviously , one does not test our method by constructing @xmath56 from them in these examples , but they could be good choices , when one has reasons to expect them to be a close , albeit not exact , approximation of the investigated distribution .",
    "for instance , for the double peak of our third example , a good initial guess could be a sum of two gaussians , and one could start off by fitting their cdf to the data .",
    "afterwards care has to be taken that eqs .",
    "( [ f0a ] ) and  ( [ f0b ] ) remain valid . that this can always be achieved follows from the interpretation ( [ fab ] ) of that requirement .",
    "we abstained from investigating a double gaussian @xmath56 , because it starts to get tedious and there is nothing really to improve on our result .",
    "but , one can easily imagine that there are more complicated situations , accompanied by limited statistics , where an improvement of @xmath56 becomes essential .",
    "if the problem for which this happens is important too , it will become worthwhile to explore more @xmath56 functions .    with our @xmath118 rule , we are slightly overexpanding the fourier transformation ( [ fourier ] ) . in the average @xmath47",
    "should be 1/2 , but all our final values are @xmath119 .",
    "that gives some flexibility to lower @xmath58 , which should be used with discretion in situations were the @xmath81 of the fourier expansion ( [ fourier ] ) appears to be too large .",
    "a warning right away : the only situation in which we did not see a rapid approach towards @xmath120 turned out to be one in which we had not noticed that the underlying distribution was discrete , while the kolmogorov test did notice that .",
    "we did not develop a statistically rigorous approach .",
    "we address physicists and others , who do not hesitate to use whatever works , not those , who want to forbid numerical recipes .",
    "we rely on the assumption that the fourier series ( [ fourier ] ) would be rapidly convergent , when the ecdf in eq .",
    "( [ rx ] ) would be replaced by the corresponding ( unknown ) exact cdf .",
    "that is in spirit similar to picking a primer in bayesian statistics , when a rigorous one is not known , a procedure , which can modify the probability content of confidence intervals .",
    "* acknowledgments : * bb likes to thank alexei bazavov for useful discussions .",
    "this work was in part supported by the u.s .",
    "department of energy under contract de - fg02 - 97er41022 .",
    "an archive with fortran  77 example runs can presently be downloaded from the website of bb .",
    "google bernd berg , or go directly to      take from there the research link and download the gzipped archive @xmath121 unfold the archive .",
    "( if instructions for that are needed , they can be found on the website of ref .",
    "@xcite , which is also linked on the main website of bb . ) go then to the folder @xmath122 all three examples of this paper can be run as special cases of the program cdf_to_pd.f in this folder , and more instructions are given in its readme.txt file . to show that at its heart our method is indeed quite simple , we list in the following our main subroutine .    ....",
    "subroutine cdf_pd(iuo , ndat , sdat , fxct ) c bernd berg , robert harris dec 16 2007 .",
    "c transforms an empirical cumulative distribution function ( cdf ) into   c a corresponding probability density using and initial function plus   c fourier series expansion for the cdf .",
    "c on input : c iuo      write unit , unchanged on exit . c ndat     number of input data , unchanged on exit .",
    "c sdat     sorted input data , unchanged on exit .",
    "c internal : c qcut     cut - off .",
    "fourier expansion is terminated for q > qcut",
    ". c          cumulative distribution function .",
    "c nmax     maximum number of terms in the fourier series .",
    "c on output : c fxct     exact cdf ( means here analytical approximation of the cdf ) .        include ' .. / ..",
    "/libs / fortran / implicit.sta '        include ' .. / ..",
    "/libs / fortran / constants.par '        parameter(qcut = half , nmax=100 ) ! change also in functions .",
    "dimension sdat(ndat),fxct(ndat )        common /cdfprob/ xmin , xrange , dn(nmax),m !",
    "expansion parameters .",
    "c        xmin = sdat(1 ) ! initializations .",
    "xrange = sdat(ndat)-sdat(1 )        do j=1,ndat          fxct(j)=(sdat(j)-sdat(1))/xrange        end do        do k=1,nmax          dn(k)=zero        enddo        do m=1,nmax !",
    "integration for the fourier series coefficients :          do i=1,ndat-1             x1=(sdat(i)-sdat(1))/xrange             x2=(sdat(i+1)-sdat(1))/xrange             dn(m)=dn(m)+(one*i / ndat - x1)*cos(m*pi*x1)/(m*pi )             dn(m)=dn(m)-(one*i / ndat - x2)*cos(m*pi*x2)/(m*pi )             dn(m)=dn(m)+sin(m*pi*x1)/(m*m*pi*pi )             dn(m)=dn(m)-sin(m*pi*x2)/(m*m*pi*pi )          enddo          dn(m)=dn(m)*two          do k=1,ndat !",
    "cdf in fourier series approximations :             xrange1=sdat(k)-sdat(1 )             fxct(k)=fxct(k)+dn(m)*sin(m*pi / xrange*xrange1 )          enddo          call kolm2_as(ndat , fxct , del , q ) ! kolmogorov test .",
    "if(q.gt.qcut ) goto 1        enddo        write(iuo,'(/ , \" cdf_pd failed m , q = \" , i6,g12.3 ) ' ) m , q        stop \" cdf_pd : expansion failed .",
    "\" 1      write(iuo,'(/ , \" cdf_pd : final m , q = \" , i6,g12.3,/ ) ' ) m , q c             return        end ....    with exception of the jackknife routine @xmath123 to be found in libs / fortran of the package , all other subroutines used are from the code of ref .",
    "@xcite and for convenience included here . for the kolmogorov test in the form of stephens",
    "the routine kolm2_as.f of @xcite has been modified to abort in case of no convergence .",
    "( note also that the related routine kolm2_as2.f of @xcite , which is not used in this paper , does not work for large values of the input arguments @xmath124 , @xmath125 due to bad coding of a multiplication of @xmath124 and @xmath125 , which can be easily corrected . ) all routines are copyrighted by their authors , who are listed in one of the first lines of each routine .",
    "limited permission of their use is given under the conditions stated on the fortran download page of ref .",
    "@xcite .",
    "stephens , j. royal stat .",
    "b * 32 * , 115 ( 1970 ) . m.h .",
    "quenville , biometrika * 43 * , 353 ( 1956 ) .",
    "tukey , ann .",
    "* 29 * , 614 ( 1958 ) .",
    "j. jersak , t. neuhaus , and p.m. zerwas , phys .",
    "b * 133 * , 103 ( 1983 ) ."
  ],
  "abstract_text": [
    "<S> when one deals with data drawn from continuous variables , a histogram is often inadequate to display their probability density . </S>",
    "<S> it deals inefficiently with statistical noise , and binsizes are free parameters . </S>",
    "<S> in contrast to that , the empirical cumulative distribution function ( obtained after sorting the data ) is parameter free . </S>",
    "<S> but it is a step function , so that its differentiation does not give a smooth probability density . </S>",
    "<S> based on fourier series expansion and kolmogorov tests , we introduce a simple method , which overcomes this problem . </S>",
    "<S> error bars on the estimated probability density are calculated using a jackknife method . </S>",
    "<S> we give several examples and provide computer code reproducing them . </S>",
    "<S> you may want to look at the corresponding figures  [ fig_gauhist ] to  [ fig_u1j ] first .    </S>",
    "<S> display of data , probability densities , histograms , continuous variables , cumulative distribution functions . </S>"
  ]
}