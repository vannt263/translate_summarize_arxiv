{
  "article_text": [
    "regularization has been studied extensively since the theory of ill - posed problems @xcite . by adding a penalty associated to a choice of regularization parameter , one penalizes overfitted models and can achieve good generalized performance with the training data .",
    "this has been a crucial feature in current modeling frameworks : for example , tikhonov regularization @xcite , lasso regression @xcite , smoothing splines @xcite , regularization networks @xcite , svms @xcite , and ls - svms @xcite .",
    "svms in particular are characterized by dual optimization reformulations , and their solutions follow from convex programs .",
    "standard svms reduce to solving quadratic problems and ls - svms reduce to solving a set of linear equations .",
    "many general purpose methods to measure the appropriateness of a regularization parameter for given data exist : cross - validation ( cv ) , generalized cv , mallows s @xmath1 , minimum description length ( mdl ) , akaike information criterion ( aic ) , and bayesian information criterion ( bic ) .",
    "recent interest has also been in discovering closed - form expressions of the solution path @xcite , and developing homotopy methods @xcite .    like svms , this paper takes the perspective of convex optimization  following @xcite  in order to tune the regularization parameter for optimal model selection .",
    "classical tikhonov regularization schemes require two steps : 1 .",
    "( training ) choosing a grid of fixed parameter values , find the solution for each constant regularization parameter ; 2 . ( validating )",
    "optimize over the regularization constants and choose the model according to a model selection criterion .",
    "the approach in this paper reformulates the problem as one for constrained optimization .",
    "this allows one to compute both steps simultaneously : minimize the validation measure subject to the training equations as constraints .",
    "this approach offers noticeable advantages over the ones above , of which we outline a few here :    * * automation * : practical users of machine learning tools may not be not interested in tuning the parameter manually .",
    "this brings us closer to fully automated algorithms . *",
    "* convexity * : it is ( usually ) much easier to examine worst case behavior for convex sets , rather than attempting to characterize all possible local minima . * * performance * : the algorithmic approach of training and validating simultaneously is occasionally more efficient than general purpose optimization routines .    for this write - up , we focus only on ridge regression , although the same approach can be applied to more complex model selection problems ( which may benefit more as they suffer more often from local minima ) .",
    "we introduce the standard approach to ridge regression and prove key properties for a convex reformulation . for a more rigorous introduction ,",
    "see @xcite .",
    "let @xmath2 and @xmath3 be a @xmath4 positive semi - definite matrix . for a fixed @xmath5 , recall that tikhonov regularization schemes of linear operators lead to the solution @xmath6 , where the estimator @xmath7 solves @xmath8    _ for a fixed value @xmath9 , we define the _ ridge solution set _",
    "@xmath10 as the set of all solutions @xmath7 corresponding to a value @xmath11 .",
    "that is , @xmath12 _    the value @xmath13 can be thought of as the minimal regularization parameter allowed in the solution set ; this would speed up computation should the user already know a lower bound on their optimal choice of @xmath14 .",
    "let @xmath15 denote the reduced singular value decomposition ( svd ) of @xmath3 , i.e. , @xmath16 is orthogonal and @xmath17 contains all the ordered positive eigenvalues with @xmath18 .",
    "[ lemma ] the solution function @xmath19 is lipschitz continuous .    for two values @xmath20 such that @xmath21 , @xmath22 consider the function",
    "@xmath23 then by the mean value theorem , there exists a value @xmath24 $ ] such that @xmath25      we now examine the the convex hull of the solution set @xmath26 .",
    "this will allow us to search efficiently through a convex set of hypotheses as in section [ 3 ] .",
    "we decompose the solution function into a sum of low rank matrices : @xmath27 then define @xmath28 for all @xmath29 .",
    "the following proposition provides linear constraints on the set of @xmath30 s following from this reparameterization .",
    "let @xmath31 for @xmath29 .",
    "then the polytope @xmath32 parametrized by @xmath33 as follows @xmath34 is convex , and moreover , forms a convex hull to @xmath10 .",
    "it is easy to verify the first two constraints by looking at the function @xmath35 defined previously , which is strictly increasing .",
    "set @xmath36 , and @xmath37 as before .",
    "then for all @xmath38 , @xmath39 hence the third inequality is also true .",
    "moreover , the set @xmath40 is characterized entirely by equalities and inequalities which are linear in the unknown @xmath30 s .",
    "so it forms a polytope .",
    "then by the above result together with the convex property of polytopes , it follows that @xmath40 is a convex relaxation to the set @xmath26 .",
    "we now find the relationship between solutions in @xmath26 and solutions in its convex relaxation @xmath40 .",
    "the maximal distance between a solution in @xmath41 and its closest counterpart in @xmath26 is bounded by the maximum range of the inverse eigenvalue spectrum : @xmath42    following similar steps to prove lemma [ lemma ] , one can show that the maximal difference between a solution @xmath43 for a given @xmath44 and its corresponding closest @xmath45 is @xmath46 for any @xmath47 , the value @xmath48 is bounded by the worst case scenario that the solution @xmath14 passes through @xmath30 or through @xmath49 .",
    "then @xmath50 where @xmath51 satisfies @xmath52 .",
    "that is , @xmath53 which is greater than zero by construction .",
    "hence for all @xmath54 , there exists @xmath55 such that @xmath56      let @xmath57 be a given data set .",
    "the ridge regression estimator @xmath58 with @xmath59 minimizes the regularized loss function @xmath60 where @xmath61 is some loss function .",
    "set @xmath62 and fix @xmath63 . for @xmath59 to be the unique global minimizer of , it is necessary and sufficient that @xmath64 satisfies @xmath65 where @xmath66 is the @xmath67 design matrix and @xmath2 is the response vector formulated from @xmath68 .",
    "note that we use the kkt notation in order to hint to the extension to other learning machines , which reduce to solving a similar convex optimization problem but with inequality constraints .",
    "let @xmath69 be a validation data set .",
    "the optimization problem of finding the optimal regularization parameter @xmath70 with respect to a validation performance criterion @xmath71 can then be written as @xmath72 that is , we find the least squared error among all @xmath64 s in the solution set @xmath10 , or equivalently all @xmath64 s satisfying the kkt conditions .",
    "for the convex approach , we simply replace the non - convex solution set @xmath73 with its convex relaxation @xmath74 .",
    "then one obtains the convex optimization problem @xmath75 this has the immediate advantage of simultaneously training and validating ( 1 step ) ; in comparison the original method requires finding a grid of points @xmath76 in @xmath10 and then minimizing among those ( 2 steps ) .",
    "furthermore , the convex hull is defined by @xmath77 equality / inequality constraints , whose complexity is not any higher than the original problem .",
    "for example , can be solved with a qp solver when @xmath78 as before , or with a lp solver when @xmath79 ( the latter of which may be preferred for sparsenesses or feature selection ) .",
    "the convex relaxation constitutes the solution path for the modified ridge regression problem @xmath80 where @xmath81 and @xmath82 for all @xmath83 , and the following inequalities hold by translating : @xmath84      the above applies to a single training and validation set , and we now extend it to @xmath0-fold cv in general .",
    "let @xmath85 and @xmath86 denote the set of training and validation data respectively , corresponding to the @xmath87 fold for @xmath88 : that is , they satisfy @xmath89 let @xmath90 . then in order to tune the parameter @xmath14 according to @xmath0-fold cv",
    ", we have the optimization problems @xmath91 for all @xmath88",
    ".    then we need only relax the kkt conditions independently for each @xmath92 .",
    "the convex optimization problem according to a @xmath0-fold cv is @xmath93 for all @xmath88 , each of which is solved as before , and so with @xmath94 constraints .",
    "then just as in typical @xmath0-fold cv , we take the average of the folds @xmath95 as the final model .",
    "we show two simulation studies as a benchmark in order to compare it to current methods .",
    "the first figure below provides intuition behind the solution paths : the curve @xmath96 and its convex relaxation @xmath97 . in the second figure",
    ", we simulate data with the function @xmath98 , where @xmath99 are sampled i.i.d . , @xmath100 observations , and @xmath101 .",
    "the figure compares the performance of the method with one which uses basis functions , another with cv , and another with generaized cv ( gcv ) .",
    "cv and gcv are implemented with standard gradient descent , and the convex algorithm outlined here uses the interior point method .",
    "this toy example demonstrates that the relaxation does not result in an increase in true error .",
    "we also conduct a monte carlo simulation : every iteration constructs a simulated model for a given value of @xmath102 defined as @xmath103 for random values of @xmath104 where @xmath105 , @xmath106 , and @xmath107 is a @xmath108 covariance matrix with @xmath109 . a data set of size @xmath102",
    "is constructed such that @xmath110 and @xmath111 is sampled i.i.d . for all @xmath29 .",
    "we compare three methods for tuning the parameter with respect to the ordinary least squares ( ols ) estimate : 10-fold cv with gradient descent ( ` rr+cv ` ) , generalized cv with gradient descent ( ` rr+gcv ` ) , and 10-fold cv criterion which applies the convex method as in ( 18 ) ( ` frr+cv ` ) .",
    "we run it for 20,000 iterations .",
    "the figure on the left compares the true error as the condition number grows ; the figure on the right compares the true error as the number of observations @xmath102 increases .    as we can see , the method is comparable to both cv and gcv for variable changes in the data set .",
    "we expect that the ols worsens drastically over ill - conditioned matrices , and our method compensates for that via the optimal tuning parameter which is roughly the same as cv s .",
    "viewing the model selection problem as one in constrained optimization gives rise to a natural approach to tuning the regularization parameter . according to the simulation results ,",
    "the convex program provides comparable performance to popular methods .",
    "moreover , global optimality is guaranteed and efficient convex algorithms can be employed as suited to the modeling problem at hand .",
    "this would especially outperform general purpose techniques when there is a high number of local optima , or if finding each individual solution for fixed regularization parameter is more costly than optimizing it simultaneously with validation as we do here .",
    "further extensions to this framework can be used as a generic convex hull method @xcite , and it can also be applied for constructing stable kernel machines , feature selection , and other possibilities related to simultaneous training and validating ."
  ],
  "abstract_text": [
    "<S> we develop a robust convex algorithm to select the regularization parameter in model selection . in practice </S>",
    "<S> this would be automated in order to save practitioners time from having to tune it manually . </S>",
    "<S> in particular , we implement and test the convex method for @xmath0-fold cross validation on ridge regression , although the same concept extends to more complex models . </S>",
    "<S> we then compare its performance with standard methods . </S>"
  ]
}