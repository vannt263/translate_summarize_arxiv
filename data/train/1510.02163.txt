{
  "article_text": [
    "at the end of its life , the core of a massive star collapses under its own gravity and releases a gigantic amount of energy which blows up the rest of the star in a ( core - collapse ) supernova @xcite .",
    "supernovae are essential to the chemical evolution of the universe . during a supernova chemical elements such as carbon , oxygen , gold and uranium",
    "are ejected into the inter - stellar medium in which new generations of stars such as our sun are born .",
    "it turns out that 99% of the energy of a supernova is carried away by a group of nimble particles called neutrinos and their anti - particles or antineutrinos .",
    "there are three kinds or `` flavors '' of neutrinos : electron - flavor , mu - flavor and tau - flavor neutrinos .",
    "it has been well established by various experiments that a neutrino of one flavor can mutate into another flavor during propagation @xcite .",
    "this phenomenon is known as neutrino ( flavor ) oscillation or transformation .",
    "because neutrinos play an influential role in supernovae , neutrino oscillations can also be important in , e.g. , supernova dynamics and the synthesis of chemical elements .    in a typical supernova approximately @xmath0 neutrinos are emitted in just tens of seconds .",
    "these neutrinos form a dense neutrino medium surrounding the collapsed stellar core which becomes a neutron star . under appropriate conditions",
    "the neutrino medium can experience collective oscillations in which neutrinos of different energies and initial flavors , emitted from different positions and propagating in different directions , can all become coupled @xcite . computing collective neutrino oscillations",
    "is a very challenging problem , and it has been done only in some greatly simplified models .",
    "the most sophisticated supernova model by now in which collective neutrino oscillations can be computed self - consistently is the ( neutrino ) _ bulb model _",
    "[ fig : bulb ] ) @xcite . for simplicity",
    ", the bulb model assumes spherical symmetry about the center of the neutron star and no time dependence .",
    "the neutron star in this model is described as a sphere of radius @xmath1 from the surface of which neutrinos are emitted . in the bulb model",
    "the flavor quantum state @xmath2 of a neutrino at radius @xmath3 is determined by the schrdinger equation @xmath4 where @xmath5 , @xmath6 and @xmath7 are the initial flavor , ( the unit vector of ) the propagation direction , and the energy of the neutrino , respectively , @xmath8 is the hamiltonian in the absence of the neutrino medium , and @xmath9 is the neutrino potential because of the ambient neutrinos .",
    "the propagation direction @xmath6 is fully described by the polar angle @xmath10 between @xmath6 and the radial direction when axial symmetry about the radial direction is imposed . in the _ extended bulb model _",
    "@xcite where axial symmetry is not imposed , @xmath6 is determined by both @xmath10 and the azimuthal angle @xmath11 about the radial axis . for @xmath12",
    "neutrino flavors , @xmath13 is a vector of @xmath12 complex variables , and @xmath8 and @xmath9 are both @xmath14 hermitian matrices .",
    "the difficulty in solving eq .   stems from the neutrino potential : @xmath15 , \\label{eq : hv}\\end{aligned}\\ ] ] where the quantities with primes are associated with the ambient neutrinos , the quantities with bars are associated with antineutrinos , @xmath16 , and @xmath17 is the number flux of the neutrino .",
    "eqs .   and",
    "require the self - consistent solution for the flavor quantum states of all neutrinos simultaneously .",
    "neutrino oscillations in the neutrino bulb model can be solved numerically by using , e.g.  flat , which is a numerical code developed by one of us @xcite . in flat",
    "the quantum flavor states @xmath18 of neutrinos at a given radius @xmath3 are described by a multi - dimensional object array ` psi_alpha[theta , e ] ` . at each radius @xmath3 summations over the elements of this array with different weighting functions",
    "are performed to obtain the neutrino potential @xmath9 , and a modified midpoint method is subsequently employed to evolve @xmath18 over one radial step . in a typical run @xmath19 discrete energy bins are needed to achieve the desired energy resolution , and @xmath20 polar angle ( @xmath10 ) bins are required to achieve numerical convergence . in other words , millions",
    "to tens of millions nonlinear differential equations need to be solved simultaneously in computing neutrino oscillations in the bulb model .",
    "the problem sizes of more realistic models can be significantly larger .",
    "for example , the inclusion of the azimuthal ( @xmath11 ) dimension in the extended bulb model can increase the problem size significantly .    because the flavor quantum states of all neutrinos are required to compute @xmath9 , it is desirable that these calculations are run on as few compute nodes as possible so as to minimize the inter - node communication . at the same time",
    ", the larger problem sizes of better supernova models require greater computing power .",
    "this balance can be achieved by employing accelerators or co - processors such as graphics processing units ( gpu ) and the xeon phi , which in principle have much higher throughput than cpus . to maintain the same code set for both cpu and co - processor we chose to develop for the intel xeon phi co - processor which is based on the intel many integrated core architecture ( mic ) .",
    "xflat is written in c++ and uses an algorithm similar to that of flat . however , unlike flat , which uses only mpi , xflat implements three levels of parallelism . in fig .",
    "[ fig : xflat ] we illustrate the high level structure of xflat for the extended neutrino bulb model . at the top level ,",
    "the @xmath10 angle bins are distributed among mpi nodes which can be either a cpu or a xeon phi co - processor . at the middle level ( i.e.  on a cpu or xeon phi )",
    "@xmath10 angle bins are dispatched to openmp threads . at the bottom level ( i.e.  in a thread ) the loop over energy bins",
    "is performed using vectorization or single instruction with multiple data ( simd ) . in order to use simd",
    "efficiently we use multiple double - precision floating - point arrays ` ar_alpha[e ] ` , ` ai_alpha[e ] ` , etc .  to represent the real and imaginary parts of the variables in complex vectors @xmath21 of the same @xmath11 and @xmath10 and at a given @xmath3 .",
    "these arrays are then grouped into an object element of neutrino beam array ` nbeam[theta , phi ] ` .    .... ///",
    "distribute theta bins among mpi nodes . ... /// main computation loop . while ( ! termination_conditions ) {    /// dispatch local theta bins among openmp threads .",
    "# pragma omp parallel for    for ( theta : local_theta_count )    {      ///",
    "loop over phi bins .      for ( phi :",
    "phi_count )      {        ///",
    "loop over energy bins using simd .",
    "# pragma simd        for ( e : energy_count )        {          /// evolve psi over one radial step .",
    "...          /// perform sums over energy bins .          ...        }        /// perform sums over phi bins .        ...      }    /// perform partial sums over local theta bins .    ...    }    /// exchange partial sums among mpi nodes .",
    "...    /// compute neutrino potential .    ... } ....",
    "we validated xflat against flat using the bulb model . in this paper",
    "we present xflat benchmarks using the _ extended _ bulb model with the azimuthal angle ( @xmath11 ) dependence which is not supported by flat .",
    "most of the benchmarks are presented for a `` standard problem '' with 2 neutrino flavors , 100 energy bins , 10 azimuthal angle ( @xmath11 ) bins and 10,000 polar angle ( @xmath10 ) bins .",
    "this problem size fits on the memory of a single xeon phi .",
    "when both the cpu and xeon phi were used in a benchmark , the number of polar angle bins on the xeon phi is three times as large as that on the cpu unless stated otherwise .",
    "xflat benchmarks were performed on the stampede supercomputer at the texas advanced computing center ( tacc ) ( see table  [ tab : stampede ] for stampede specifications ) .",
    "double precision was used exclusively in all floating point calculations .",
    "xflat and all kernels were compiled with the intel c++ compiler v13.0.2 with flags ` -o3 -openmp -xhost ` for cpu and ` -o3 -openmp -mmic ` for xeon phi , respectively . one xflat process was run on each cpu and/or xeon phi ( i.e.  the xeon phi was run in _ native _ mode @xcite ) .",
    "each process on the cpu was run with 8 openmp threads since hyperthreading is disabled on stampede @xcite .",
    "each process on xeon phi was run with 244 openmp threads to fully utilize its hyperthreading capability .",
    "for mpi - enabled benchmarks the intel mpi library v4.1.3.049 was used , and for i / o - enabled benchmarks the netcdf v4.3.2 and hdf5 v1.8.13 libraries were used .",
    ".stampede dell poweredge c8220z compute node specifications @xcite .",
    "[ cols=\"<,<\",options=\"header \" , ]     [ tab : stampede ]    ....    ///",
    "8/244 openmp threads for cpu / xeon phi .",
    "# pragma omp parallel for for ( t : num_threads ) {    /// repeat 10 million times .    for ( itr : loop_count )    {      ///",
    "vector_width is a multiple of 4/8       /// for cpu",
    "/ xeon phi .      # pragma simd      for ( s : vector_width )      {        /// floating point operations        ...      }    } } ....      we first benchmarked the raw performance of the cpu and xeon phi on stampede using a code adapted from @xcite with the structure illustrated in fig .",
    "[ fig : benchmark ] . in the innermost loop of this code simple floating point operations ( i.e.  additions and multiplications ) are performed on an array or vector of double - precision ( dp ) floating - point numbers .",
    "the widths of the vectors are taken to be a multiple of that of the simd registers of the computing component ( 256 bits or 4 dp for the xeon cpu , and 512 bits or 8 dp for the xeon phi ) .",
    "the same vector operations are repeated 10 million times in the middle loop to maintain data locality . in the outermost loop",
    "all of the hardware threads are utilized to achieve the best performance .",
    "the results of these benchmarks with different vector widths in the innermost loop are shown in fig .",
    "[ fig : flops ] .",
    "these results show that the floating point performance of the xeon phi is highly sensitive to the width of the vector , while the performance of the cpu is relatively stable .",
    "the floating point performance of the xeon phi is best when the width of the dp vectors is 64 ; in that test , the xeon phi ran 10 times as fast as the cpu .",
    "however , the performance of the xeon phi degrades substantially as the vector width increases .    because xflat uses transcendental functions such as ` sin ( ) ` and ` exp ( ) ` , we also benchmarked the transcendental function performance of the cpu and xeon phi using a code similar to that of fig .",
    "[ fig : benchmark ] with the simple floating point operations replaced by a pair of ` sin ( ) ` and ` cos ( ) ` functions in one series of tests , and ` exp ( ) ` in the other .",
    "the results of these benchmarks are shown in fig .",
    "[ fig : sin_exp ] .",
    "these results suggest that the transcendental function performance on the xeon phi is relatively stable against the width of the vectors . for the tests on ` sin ( ) ` and ` cos ( ) ` the xeon phi ran 68 times as fast as the cpu , but for ` exp ( ) ` the xeon phi is only 34 times better .",
    "we then benchmarked the performance of xflat on a single compute node without i / o .",
    "we ran xflat on a single cpu , dual cpus , a single xeon phi , dual xeon phis , and both dual cpus and dual xeon phis , respectively . in all of these tests we used the same input configuration as the `` standard problem '' mentioned previously , except that we varied the number of polar angle bins from 1,000 up to 12,000 ( the memory on a single xeon phi can not support more than 13,000 polar angle bins ) .",
    "the results of these tests are shown in fig .",
    "[ fig : snode ] .",
    "the run times of these tests scale approximately linearly with problem size , although there are variations in the benchmarks with xeon phis . in single - component tests",
    "xflat ran 2.52.9 times as fast on single xeon phi as on a single 8-core ( sandy bridge ) xeon cpu .",
    "in dual - component tests xflat ran 2.12.6 times as fast on dual xeon phis as on dual cpus . when both dual cpus and dual xeon phis were used , xflat ran 3.53.9 times as fast as on dual cpus",
    ".          the performance of xflat can be significantly affected by i / o . in its _ direct _ i / o configuration the xflat process on each mpi node , which can be either a cpu or xeon phi , save its local snapshots of the quantum flavor states of neutrinos to the global ` $ scratch ` storage space of stampede .",
    "we ran xflat with direct i / o and without i / o for 1,000 radial steps on a single node with dual cpus and dual xeon phis . in the test with direct i",
    "/ o each mpi node dumps 10 snapshots .",
    "the size of a snapshot is 164  mb for a cpu process and 476  mb for the xeon phi because the xeon phi process has 2.9 times as many polar angle bins as the cpu process does . as shown fig .",
    "[ fig : io ] xflat ran more than 10 times slower with direct i / o than without i / o .",
    "most of the extra time in the test with direct i / o was spent in the i / o module of the xeon phi process .    to mitigate the poor i / o performance on xeon phi",
    ", we designed an _",
    "i / o configuration in which the i / o module of the xeon phi process sends the data to the i / o module of the corresponding cpu process which then writes to ` $ scratch ` for the xeon phi .",
    "we benchmarked xflat with indirect i / o , and the result",
    "is also shown in fig .",
    "[ fig : io ] . in this test both the overall run time and the time spent in the i / o module of the xeon phi process were significantly reduced compared to that with direct i / o , although the cpu process had to spend more time in its i / o module to communicate with the xeon phi .",
    "unfortunately , the indirect i / o configuration breaks the symmetry between the cpu and xeon phi and increases the complexity of code maintenance .",
    "we benchmarked xflat on multiple compute nodes in the cpu - only , xeon phi - only and hybrid modes which utilize dual cpus , dual xeon phis and both dual cpus and dual xeon phis for each node , respectively . in each of the tests we ran xflat for approximately 100 seconds and computed the number of calculated steps per second .",
    "the results of these benchmarks are shown in fig .",
    "[ fig : hom ] .",
    "these results show that the performance of xflat scales well in the cpu - only mode up to 32 nodes , the maximum number of nodes in our tests . in the xeon phi",
    "- only mode , however , the performance scales reasonably well only up to 12 nodes for the studied problem size , where it ran 22.6 times as fast as in the cpu - only mode . in the hybrid mode",
    "the performance of xflat scales reasonably well up to 8 nodes where it ran 33.5 times as fast as in the cpu - only mode .",
    "the reason for the poor scaling behavior in the xeon phi - only and hybrid modes is because the 244 hardware threads of the xeon phi can not not be fully utilized for the studied problem size when many compute nodes are used . for example , in the test with 20 compute nodes in the xeon phi - only mode each xeon phi process received @xmath22 polar angle bins , which is just a few more than the 244 threads available on the xeon phi .",
    "as a result , the openmp parallel ` for ` loop in fig .  [",
    "fig : xflat ] iterated twice in each step with most of the threads idle in the second iteration .",
    "when 22 compute nodes were used , however , each xeon phi process received 227 or 228 polar angle bins , and the computation for these angle bins was completed in one iteration by employing most of the threads .",
    "the performance of xflat decreases when more compute nodes are used because there is not enough load on each xeon phi .            in all previous hybrid - mode tests we fixed the load ratio between the xeon phi and cpu to be about 3:1 .",
    "we also benchmarked the multi - node performance of xflat with different load ratios using the method described above . in fig .",
    "[ fig : het ] we show the results of three such benchmarks .",
    "these results show that , in the regime where there is enough load on the xeon phi , xflat generally performs best with three times as much load on the xeon phi as on the cpu .",
    "the performance of xflat changes modestly when the xeon phi to cpu load ratio is changed from 3:1 to 2:1 or 4:1 .",
    "varying the load ratio can have a large impact when the load on the xeon phi is less than is necessary to keep most of its hardware threads busy for two iterations .",
    "for example , the test with 14 compute nodes and load ratio 3:1 does not perform well because each xeon phi process has 267 or 268 polar angle bins , which is again just a few more than 244 .",
    "better performance was achieved by lowering the load ratio and moving some of the polar angle bins from the xeon phi to the cpu .",
    "indeed , the test with the 2:1 load ratio ( with 238 polar angle bins on each xeon phi ) has better performance than that with the 3:1 load ratio .",
    "we have developed the xflat astrophysical simulation code which fully utilizes all three levels of parallelism available on xeon phi - equipped supercomputers . in general",
    "we have found it very helpful to maintain a single code set which can run efficiently on both the cpu and the co - processor .",
    "we benchmarked both the single - node and multi - node performance of xflat in various configurations .",
    "when there is no i / o involved , each xeon phi co - processor can boost the performance of xflat by the equivalent of two or more 8-core ( sandy bridge ) xeon cpus . when there is i / o involved , however , the direct i / o from the xeon phi can reduce the performance of xflat substantially .",
    "this disadvantage can be mitigated by redirecting the i / o from the xeon phi through the cpu , but a better fix may need an update in the xeon phi and/or its software . because the xeon phi co - processor has many more hardware threads than the cpu does , it is essential to have sufficient number of parallel tasks to keep most of its threads busy .",
    "this feature limits the number of compute nodes for which xflat can be run efficiently for the extended neutrino bulb model .",
    "this limitation will be removed with the development and implementation of next - generation multi - dimensional supernova models for neutrino oscillations in xflat .",
    "this work was supported in part by doe grant de - sc0008142 ( h.d . ) and nsf grant oci-1040530 ( s.r.a . ) at the university of new mexico .",
    "we are grateful to the texas advanced computing center and the unm center for advanced research computing for providing computational resources used in this work .",
    "we thank dr .",
    "john cherry , dr .  shashank shalgar , and sajad abbar for their assistance during the development of this project ."
  ],
  "abstract_text": [
    "<S> we have developed the astrophysical simulation code xflat to study neutrino oscillations in supernovae . </S>",
    "<S> xflat is designed to utilize multiple levels of parallelism through mpi , openmp , and simd instructions ( vectorization ) . </S>",
    "<S> it can run on both cpu and xeon phi co - processors based on the intel many integrated core architecture ( mic ) . </S>",
    "<S> we analyze the performance of xflat on configurations with cpu only , xeon phi only and both cpu and xeon phi . </S>",
    "<S> we also investigate the impact of i / o and the multi - node performance of xflat on the xeon phi - equipped stampede supercomputer at the texas advanced computing center ( tacc ) . </S>"
  ]
}