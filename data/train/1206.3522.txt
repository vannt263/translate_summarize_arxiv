{
  "article_text": [
    "due to the increasing number of cpu cores , exploiting possible speedups by parallel computations is nowadays more important than ever .",
    "parallel evolutionary algorithms ( eas ) form a popular class of heuristics with many applications to computationally expensive problems  @xcite .",
    "this includes _ island models _ , also called _ distributed eas _ , _ multi - deme eas _ or _ coarse - grained eas_. evolution is parallelized by evolving subpopulations , called _ islands _ , on different processors .",
    "individuals are periodically exchanged in a process called _ migration _ , where selected individuals , or copies of these , are sent to other islands , according to a migration topology that determines which islands are neighboring .",
    "also more fine - grained models are known , where neighboring subpopulations communicate in every generation , first and foremost in _ cellular eas _",
    "@xcite .    by restricting the flow of information through spatial structures and/or infrequent communication , diversity in the whole system is increased .",
    "researchers and practitioners frequently report that parallel eas speed up the computation time , and at the same time lead to a better solution quality  @xcite .    despite these successes , a long history  @xcite and very active research in this area  @xcite ,",
    "the theoretical foundation of parallel eas is still in its infancy .",
    "the impact of even the most basic parameters on performance is not well understood  @xcite .",
    "past and present research is mostly empirical , and a solid theoretical foundation is missing .",
    "theoretical studies are mostly limited to artificial settings . in the study of _ takeover times",
    "_ , one asks how long it takes for a single optimum to spread throughout the whole parallel ea , if the ea uses only selection and migration , but neither mutation nor crossover  @xcite .",
    "this gives a useful indicator for the speed at which communication is spread , but it does not give any formal results about the running time of evolutionary algorithms with mutation and/or crossover .",
    "one way of gaining insight into the capabilities and limitations of parallel eas is by means of rigorous running time analysis  @xcite . by asymptotic bounds on the running time",
    "we can compare different implementations of parallel eas and assess the speedup gained by parallelization in a rigorous manner .    in",
    "@xcite the authors presented the first running time analysis of a parallel evolutionary algorithm with a non - trivial migration topology .",
    "it was demonstrated for a constructed problem that migration is essential in the following way .",
    "a suitably parametrized island model with migration has a polynomial running time while the same model without migration as well as comparable panmictic populations need exponential time , with overwhelming probability .",
    "neumann , oliveto , rudolph , and sudholt  @xcite presented a similar result for island models using crossover . if islands perform crossover with immigrants during migration , this can drastically speed up optimization .",
    "this was demonstrated for a pseudo - boolean example as well as for instances of the vertexcover problem  @xcite .    in this work",
    "we take a broader view and consider the speedup gained by parallelization for various common pseudo - boolean functions and function classes of varying difficulty .",
    "a general method is presented for proving upper bounds on the parallel running time of parallel eas .",
    "the latter is defined as the number of generations of the parallel ea until a global optimum is found for the first time .",
    "this allows us to estimate the speedup gained by parallelization , defined as the ratio of the expected parallel running time of an island model and the expected running time for a single island .",
    "it also can be used to determine how to choose the number of islands such that the parallel running time is reduced as much as possible , while still maintaining an asymptotically optimal speedup .",
    "our method is based on the _ fitness - level method _ or _ method of @xmath0-based partitions _ , a simple and well - known tool for the analysis of evolutionary algorithms  @xcite . the main idea of this method is to divide the search space into sets @xmath1 , strictly ordered according to fitness values of elements therein .",
    "elitists eas , i.e. , eas where the best fitness value in the population can never decrease , can only increase their current best fitness .",
    "if , for each set @xmath2 we know a lower bound @xmath3 on the probability that an elitist ea finds an improvement , i.e. , for finding a new search point in a new best fitness - level set @xmath4 , this gives rise to an upper bound @xmath5 on the expected running time .",
    "the method is described in more detail in section  [ sec : preliminaries ] .    in section  [ sec : general - upper - bounds ] we first derive a general upper bound for parallel eas , based on fitness levels .",
    "our general method is then tailored towards different spatial structures often used in fine - grained or cellular evolutionary algorithms and parallel architectures in general : ring graphs ( theorem  [ the : method - ring ] in section  [ sec : ring ] ) , torus graphs ( theorem  [ the : method - torus ] in section  [ sec : torus ] ) , hypercubes ( theorem  [ the : method - hypercube ] in section  [ sec : hypercube ] ) and complete graphs ( theorems  [ the : method - completegraph ] and  [ the : method - completegraph - refined ] in section  [ sec : perfectparallelization ] ) .",
    "the only assumption made is that islands run elitist algorithms , and that in each generation each island has a chance of transmitting individuals from its best current fitness level to each neighboring island , independently with probability at least @xmath6 .",
    "we call the latter the _ transmission probability_. it can be used to model various stochastic effects such as disruptive variation operators , the impact of selection operators , probabilistic migration , probabilistic emigration and immigration policies , and transient faults in the network .",
    "this renders our method widely applicable to a broad range of settings .",
    "our estimates of parallel running times from theorems  [ the : method - ring ] ,  [ the : method - torus ] ,  [ the : method - hypercube ] ,  [ the : method - completegraph ] , and  [ the : method - completegraph - refined ] are summarized in the following theorem , hence characterizing our main results . throughout this work",
    "@xmath7 always denotes the number of islands .    [",
    "the : generalbounds ] consider an island model with @xmath7 islands where each island runs an elitist ea . for each island",
    "let there be a fitness - based partition @xmath1 such that for all @xmath8 all points in @xmath2 have a strictly worse fitness than all points in @xmath9 , and @xmath10 contains all global optima .",
    "we say that an island is in  @xmath2 if the best search point on the island is in  @xmath2 .",
    "let @xmath3 be a lower bound for the probability that in one generation a fixed island in @xmath2 finds a search point in @xmath4 .",
    "further assume that for each edge in the migration topology in every iteration there is a probability of at least @xmath6 that the following holds , independently from other edges and for all  @xmath8 .",
    "if the source island is in @xmath2 then after the generation the target island is in  @xmath11 .",
    "then the expected parallel running time of the island model is bounded by    1 .",
    "@xmath12 for every ring graph or any other strongly connected there is a directed path from  @xmath13 to  @xmath14 and vice versa . ] topology , 2 .",
    "@xmath15 for every undirected grid or torus graph with side lengths at least @xmath16 , 3 .",
    "@xmath17 for the @xmath18-dimensional hypercube graph , 4 .",
    "@xmath19 for the complete topology @xmath20 , as well as @xmath21 .",
    "a remarkable feature of our method is that it can automatically transfer upper bounds for panmictic eas to parallel versions thereof .",
    "the only requirement is that bounds on panmictic eas have been derived using the fitness - level method , and that the partition @xmath1 and the probabilities for improvements @xmath22 used therein are known .",
    "then the expected parallel time of the corresponding island model can be estimated for all mentioned topologies simply by plugging the @xmath3 into theorem  [ the : generalbounds ] .",
    "fortunately , many published runtime analyses use the fitness - level method  either explicitly or implicitly  and the mentioned details are often stated or easy to derive .",
    "hence even researchers with limited expertise in runtime analysis can easily reuse previous analyses to study parallel eas .",
    "further note that we can easily determine which choice of @xmath7 , the number of islands , will give an upper bound of order  @xmath23the best upper bound we can hope for , using the fitness - level method . in all bounds from theorem  [ the : generalbounds ]",
    "we have a first term that varies with the topology and @xmath6 , and a second term that is always @xmath23 .",
    "the first term reflects how quickly information about good fitness levels is spread throughout the island model .",
    "choosing @xmath7 such that the second term becomes asymptotically as large as the first one , or larger , we get an upper bound of @xmath24 . for settings where @xmath25 is an asymptotically tight upper bound for a single island , this corresponds to an asymptotic linear speedup .",
    "the maximum feasible value for @xmath7 depends on the problem , the topology and the transmission probability  @xmath6 .",
    ".asymptotic bounds on expected parallel ( @xmath26 , number of generations ) and sequential ( @xmath27 , number of function evaluations ) running times and expected communication efforts ( @xmath28 , total number of migrated individuals ) for various @xmath29-bit functions and island models with @xmath7 islands running the and using migration probability  @xmath30 .",
    "the number of islands @xmath7 was always chosen to give the best possible upper bound on the parallel running time , while not increasing the upper bound on the sequential running time by more than a constant factor . for unimodal functions @xmath31 denotes the number of function values . see  @xcite for bounds for the . results for @xmath32 were restricted to @xmath33 for simplicity . all upper bounds for and stated here are asymptotically tight , as follows from general results in  @xcite . [ cols=\"<,<,<,<,<,<,<\",options=\"header \" , ]"
  ],
  "abstract_text": [
    "<S> we present a new method for analyzing the running time of parallel evolutionary algorithms with spatially structured populations . based on the fitness - level method , </S>",
    "<S> it yields upper bounds on the expected parallel running time . </S>",
    "<S> this allows to rigorously estimate the speedup gained by parallelization . </S>",
    "<S> tailored results are given for common migration topologies : ring graphs , torus graphs , hypercubes , and the complete graph . </S>",
    "<S> example applications for pseudo - boolean optimization show that our method is easy to apply and that it gives powerful results . in our examples </S>",
    "<S> the possible speedup increases with the density of the topology . </S>",
    "<S> surprisingly , even sparse topologies like ring graphs lead to a significant speedup for many functions while not increasing the total number of function evaluations by more than a constant factor . </S>",
    "<S> we also identify which number of processors yield asymptotically optimal speedups , thus giving hints on how to parametrize parallel evolutionary algorithms .    </S>",
    "<S> parallel evolutionary algorithms , runtime analysis , island model , spatial structures </S>"
  ]
}