{
  "article_text": [
    "kinetic monte carlo ( kmc ) @xcite has proven an efficient and powerful tool to study non - equilibrium processes , and is used in fields as different as population dynamics , irradiation damage , or crystal growth @xcite .",
    "the most widely used variant of the method is the monte carlo _ time residence _",
    "algorithm @xcite , also known as rejection - free @xmath0-fold method , or bkl in reference to its authors @xcite . although kmc is generally capable of advancing the time scale significantly faster than direct , time - driven methods , it suffers from numerical limitations such as _ stiffness _",
    "@xcite , and time asynchronicity .",
    "this has spurred the development of more powerful variants such as coarse - grained kmc @xcite , first - passage kmc @xcite , and other accelerated methods @xcite . in this sense ,",
    "a number of parallelization schemes for kmc have been proposed , including rigorous and semi - rigorous algorithms based on asynchronous kinetics @xcite .",
    "these methods rely on cumbersome roll - back procedures to avoid causality errors , _",
    "i.e. _ event time incompatibilities associated with processor communications .",
    "for this reason , most applications of interest are studied using approximate schemes ( non - rigorous ) for computational convenience . in spite of this , calculations using asynchronous parallel kmc have provided numerous insights in several studies , most notably crystal growth @xcite .",
    "recently , we have developed and alternative algorithm based on a synchronous time decomposition of the master equation @xcite .",
    "our parallel kinetic monte carlo method , eliminates time conflicts by recourse to _ null _ events that advance the internal clock of each processor in a synchronized fashion without altering the stochastic trajectory of the system .",
    "the method has been demonstrated for continuum diffusion / reaction systems , which represents a worst - case application scenario for two reasons .",
    "first , the maximum time step gain is limited by the intrinsic length scale of the problem at hand , which in concentrated systems may not be large ; second , spatial boundary errors are difficult to eliminate due to the unbounded nature of diffusion in a continuum setting .",
    "this latter feature also limits the parallel efficiency of the algorithm , as global communications are needed during every monte carlo step . in our synchronous parallel kmc method ( spkmc ) ,",
    "the parallel error can always be computed intrinsically and reduced arbitrarily ( albeit at the expense of efficiency ) . in this paper",
    ", we extend spkmc to lattice systems , where diffusion lengths are quantized and boundary errors can be eliminated altogether .",
    "first , we adapt the algorithm proposed in ref .",
    "@xcite to discrete systems .",
    "due to its relevance and well - known properties , we have chosen the three - dimensional ( 3d ) ising system as our underlying lattice model .",
    "second , we analyze the performance of the method in terms of stochastic bias ( error ) and parallel efficiency .",
    "we then apply spkmc to large systems near the critical point , which provides a demanding testbed for the method , as this is where fluctuations are exacerbated and convergence is most difficult .",
    "the ising model is one of the most extensively studied lattice systems in statistical mechanics .",
    "it consists of a lattice of @xmath1 sites occupied by particles whose state is described by a variable @xmath2 that represents the spin of each particle and can take only the value @xmath3 . for pair - interactions , the hamiltonian that gives the energy of configurational state @xmath4 takes the form : @xmath5 where @xmath6 is the coupling constant between neighboring pairs @xmath7 and @xmath8 is an external ( magnetic ) field .",
    "the time evolution of the system is assumed to be described by a master equation with glauber dynamics @xcite , which states that the probability @xmath9 of finding the system in state @xmath10 at time @xmath11 obeys the equation : @xmath12      \\label{eq : glauber}\\ ] ] where @xmath13 denotes the configuration obtained from @xmath10 by flipping the @xmath14 spin with transition rate @xmath15 : @xmath16      \\label{eq : rate}\\ ] ] here @xmath17 is a positive constant that represents the natural frequency of the system , @xmath18 is the reciprocal temperature , and @xmath19 is the energy associated with spin @xmath20 , which follows directly from eq .",
    "( [ eq : isingh ] ) . in what follows",
    "we consider only internally - driven systems ( @xmath21 ) .",
    "many discrete systems can be mapped exactly or approximately to the ising system .",
    "the grand canonical ensemble formulation of the lattice gas model , for example , can be mapped exactly to the canonical ensemble formulation of the ising model .",
    "also , binary alloy hamiltonians with nearest - neighbor interactions in rigid lattices can also be expressed as ising hamiltonians .",
    "these mappings allow us to exploit results and behaviors of the ising model to answer questions about the related models .",
    "in addition , the ising system is particularly useful to study second - order phase transitions .",
    "the temperature @xmath22 at which such transitions occur is known as the critical temperature . during the phase transition ,",
    "thermodynamic quantities diverge according to power laws of @xmath23 , whose exponents are known as the critical exponents . the nature of the phase transition is determined by whether the order parameter is continuous at @xmath22 @xcite . in a ferromagnetic system such as the ising model ,",
    "the order parameter is the net magnetization @xmath24 : @xmath25 for simple cubic lattices in 3d , @xmath26 is the number of sites , with @xmath27 the lattice size . as we approach the critical temperature from @xmath28 , uncorrelated groups of spins align themselves in the same direction .",
    "these clusters grow in size , known as the correlation length @xmath29 , which too diverges at the critical point . at @xmath30 , one may theoretically encounter arbitrarily large areas with correlated spins pointing in one direction . in finite systems the upper limit of @xmath29 is the system s dimension @xmath27 .",
    "thus , the challenge associated with simulating ising systems during the phase transition is then ensuring that the error incurred by simulating a finite - size lattice is sufficiently small for the critical exponents to be calculated with certainty .",
    "this has spurred a great many monte carlo simulations of very large lattices in the hope of finding converged critical exponents ( cf . , _",
    "e.g. _ , ref .",
    "@xcite ) .",
    "when the system is in a ferromagnetic state , @xmath31 decays from the spontaneous magnetization value @xmath32 with time as @xmath33 , where @xmath34 and @xmath35 are the critical exponents for @xmath32 and @xmath29 , respectively . from the known value of the ratio @xmath36 @xcite in 3d",
    ", one can obtain @xmath37 from the slope of the @xmath31-@xmath11 curve , obtained for several @xmath27 , at the critical point . to study the finite - size dependence of @xmath22 ,",
    "high - order dimensionless ratios such as the _ binder _ cumulant have been proposed : @xmath38 which takes a value of @xmath39 when @xmath28 ( when the magnetization oscillates aggressively around zero ) , and goes to zero at low temperatures , when @xmath40 . as mentioned earlier , at the critical point the correlation length diverges , and therefore @xmath41 does not depend on @xmath27 .",
    "kmc equilibrium calculations of @xmath41 for several lattice sizes can then be used to calculate the value of the reciprocal critical temperature @xmath42 , whose most accurate estimate is presently @xmath43 @xcite .",
    "the basic structure of the algorithm is identical to that described in ref .",
    "first , the entire configurational space is partitioned into @xmath44 subdomains @xmath45 .",
    "note that , in principle , this decomposition need not be necessarily spatial ( although this is the most common one ) , and partitions based on some other kind of load balancing can be equally adopted .",
    "however , without loss of generality , in what follows it is assumed that the system is spatially partitioned :    1 .   *",
    "a frequency line is constructed for each @xmath45 as the aggregate of the individual rates , @xmath46 , corresponding to all the possible events within each subdomain : @xmath47 * where @xmath48 and @xmath49 are , respectively , the number of possible events and the total rate in each subdomain @xmath50 . here",
    "@xmath51 and @xmath52 .",
    "* we define the maximum rate , @xmath53 , as : @xmath54 * this value is then communicated globally to all processors .",
    "* we assign a _",
    "null _ event with rate @xmath55 to each frequency line in each subdomain @xmath50 such that : @xmath56 * where , in general , the @xmath55 will all be different .",
    "we showed in ref .",
    "@xcite that the condition for maximum efficiency is that step ( 2 ) become strictly an equality , such that : @xmath57 _ i.e. _ there is no possibility of null events .",
    "however , in principle , each subdomain can have any arbitrary @xmath55 as long as all the frequency lines in each @xmath45 sum to the same global value .",
    "this flexibility is one of the most important features of our algorithm .",
    "4 .   * in each @xmath45 an event is chosen with probability @xmath58 , including null events with @xmath59 . * for this step , we must ensure that independent sequences of random numbers be produced for each @xmath45 , using appropriate parallel pseudo random number generators . 5 .",
    "* as in standard bkl , a time increment is sampled from an exponential distribution : @xmath60 where @xmath61 is a suitable random number*. here , by virtue of poisson statistics , @xmath62 becomes the global time step for all of the parallel processes .",
    "* communicate boundary events .",
    "* a global call will always achieve communication of boundary information .",
    "however , depending on the characteristics of the problem at hand , local calls may suffice , typically enhancing performance .",
    "as we have shown , this algorithm solves the master equation exactly for non - interacting particles .",
    "when particles are allowed to interact across domain boundaries , suitable corrections must be implemented to avoid boundary conflicts . for lattice - based kinetics with short - ranged interaction distances",
    "this is straightforwardly achieved by methods based on the chessboard sublattice technique .",
    "this spatial subdivison method has been used in multispin calculations of the kinetic ising model since the early 1990s @xcite . in the context of parallel kmc algorithms , shim and amar",
    "were the first to implement such procedure @xcite , in which a sublattice decomposition was used to isolate interacting domains in each cycle .",
    "the minimum number of sublattices to ensure non - interacting adjacent domains depends on a number of factors , most notably system dimensionality . in 3d",
    ", the chessboard method requires a subdivison into a minimum of either two or eight sublattices , depending on whether only first or farther nearest neighbor interactions are considered .",
    "this is schematically shown in figure [ fig : coloring ] , where each sublattice is defined by a specific color .",
    "the figure shows the minimum sublattice block ( white wireframe ) to be assigend to each processor .",
    "these blocks are indivisible and each processor can be assigned only integer multiples of them for accurate spkmc simulations .",
    "the implementation of the sublattice algorithm is as follows . because here eq .",
    "[ eq : isingh ] only involves first nearest neighbor interactions , the spatial decomposition performed in this work is such that it enables a regular sublattice construction with exactly two colors . in this fashion , each @xmath45 becomes a subcell of a given sublattice , which imposes that each processor must have a multiple of two ( or eight , for longer range interactions ) number of subcells . using a sublattice size greater than the particle interaction distance guarantees that no two particles in adjacent @xmath45 interact .",
    "step ( 4 ) above might then be substituted by the following procedure :    1 .   *",
    "a given sublattice is chosen for all subdomains*. this choice may be performed in several ways such as fully random or using some type of color permutation so that every sublattice is visited in each kmc cycle .",
    "here we have implemented the former , as , for example , in the synchronous sublattice algorithm ( ssl ) of shim and amar @xcite .",
    "the sublattice selection is performed with uniform probability thanks to the flexibility furnished by the spkmc algorithm , which takes advantage of the null rates to avoid global calls to communicate each sublattice s probability . restricting each processor s sampling to only one lattice , however , while avoiding boundary conflicts , results in a systematic error associated with spatial correlations .",
    "the errors incurred by this procedure will be analyzed in section [ sec : bias ] .",
    "* an event is chosen in the selected sublattice with the appropriate probability , including null events*. when the rate changes in each @xmath45 after a kmc cycle are unpredictable , a global communication of @xmath53 in step ( 2 ) is unavoidable .",
    "when the cost of global communications becomes a considerable bottleneck in terms of parallel efficiency , it is worth considering other alternatives .",
    "for the ising system , we consider the following : + 12 cm * the simplest way to avoid global communications is to prescribe @xmath53 to a very large value so as to ensure that it is never surpassed regardless of the kinetics being simulated . for the ising model ,",
    "this amounts to _ calculating the maximum theoretical aggregate rate for an ensemble of ising spins_. for a given subdomain @xmath45 , this is : @xmath63\\ ] ] where @xmath64 is the theoretical maximum energy increment due to a single spin change : @xmath65 and @xmath66 is the lattice coordination number .",
    "this procedure is very conservative and may result in a poor parallel performance . * _ perform a self - learning process to optimize @xmath67_. this procedure is aimed at refining the upper estimate of @xmath53 by recording the history of rate changes over the course of a pkmc simulation .",
    "for example , one can start with the maximum theoretical aggregate ising rate and start decreasing the upper bound to improve the efficiency .",
    "for this procedure , a tolerance to ensure that @xmath68 must be prescribed .",
    "a sufficiently - long time history of this comparison must be stored to perform regular checks and ensure that the inequality holds .",
    "this algorithm solves the same master equation as the serial case but it is not strictly rigorous . as we have pointed out , the sampling strategies adopted to solve boundary conflicts introduce spatial correlations that result in stochastic bias . under certain conditions spkmc does behave rigorously in the sense that this bias is smaller than the intrinsic statistical error .",
    "we explore these issues in the following section .",
    "the algorithm introduced above eliminates the occurrence of boundary conflicts at the expense of limiting the sampling configurational space of the system in each kmc step .",
    "because boundary conflicts are inherently a spatial process , this introduces a spatial bias that must be quantified to understand the statistical validity of the spkmc results .",
    "next , we analyze this bias by testing the behavior of the magnetization when the system is close to the critical point ( @xmath69 ) .",
    "all the results shown in this section correspond to the sublattice algorithm using two colors with random selection .",
    "the bias is defined as the difference between a parallel calculation and a reference calculation usually taken as the mean of a sufficient number of serial runs : @xmath70 where @xmath71 and @xmath72 are the averages of a number of independent runs in parallel and in serial , respectively , for a given total ising system size .",
    "the initial ( @xmath73 ) and boundary ( periodic ) conditions in both cases are identical . in figure",
    "[ fig : mag ] we show the time evolution of the magnetization of an @xmath1=262,144-spin ( @xmath74 ) ising system , averaged over 20 serial runs , used as the reference for the calculation of the bias .",
    "the purple shaded region gives the extent of the standard deviation , which is initially very small , when @xmath32 is very close to one , but grows with time as the system approaches its paramagnetic state and fluctuations are magnified .",
    "the shaded region ( in gold ) between @xmath75@xmath76@xmath77 has been chosen for convenience and marks the time interval over which eq .",
    "[ eq : bias ] is solved is measured . ] . the same exercise has been repeated for a 2,097,152-spin ( @xmath78 ) sample with 5 serial runs performed ( not shown ) .    , is represented by the purple shaded area about the magnetization curve .",
    "the golden shaded area marks the region over which the bias is computed.,width=491 ]    figure [ fig : b - t ] shows the time evolution of the bias for a number of parallel runs corresonding to the two system sizes studied .",
    "the shaded area in the figure corresponds to the interval contained within the standard deviation of the serial case ( cf .",
    "[ fig : mag ] ) . therefore , this analysis yields the maximum number of parallel events that can be considered to obtain a solution statistically equivalent to that given by the serial case .",
    "the figure shows up to what number of parallel processes can the serial and sublattice methods be considered statistically equivalent in the entire range where the bias is calculated . for the 262,144-spin system",
    "this is 32 , whereas for the 2,097,152 one it is approximately 256 .",
    "however , runs whose errors are larger than the serial standard deviation at short time scales ( _ e.g. _ @xmath7964 and @xmath79512 for , respectively , the 262,144 and 2,097,152-spin systems ) gradually reduce their bias as time progresses .",
    "in fact , at @xmath80 , all parallel runs fall within @xmath81 .",
    "it appears , therefore , that fluctuations play an important a role in the parallel runs for low numbers of processes an spkmc cycles . as the accumulated statistics increases ( more cycles ) , this effect gradually disappears . in any event , the bias is never larger than @xmath822% for all cases considered here .",
    "although fig .",
    "[ fig : b - t ] ) provides an informative quantification of the errors introduced by the parallel method , it is also important to separate this systematic bias from the statistical errors associated with each set of independent parallel runs .",
    "this is quantified by the standard deviation of the _ time - integrated _ bias , defined as : @xmath83 where the terms inside the square root are the parallel and serial variances respectively .",
    "we next solve eqs .",
    "( [ eq : bias ] ) and ( [ eq : dev ] ) during the time interval prescribed above for the two system sizes considered in fig .",
    "[ fig : b - t ] .",
    "the absolute value of the systematic bias is extracted from a number of independent runs ( 10 and 5 respectively ) and plotted in figure [ fig : bias ] as a function of the number of parallel processes .",
    "note that the number of parallel processes is equal to the total number of subcells divided by the number of different sublattices ( = 2 , in our case ) .",
    "the figure shows that the absolute value of the bias is always smaller than the statistical error ( _ i.e. _ the error bars always encompass zero bias ) .",
    "this implies that , in the range explored , a given problem may be solved in parallel and the result obtained can be considered statistically equivalent to a serial run .",
    "the bias is roughly constant and always below @xmath84 in the entire range explored for both cases .",
    "however , the bias is consistently lower for the larger system size , as are the error bars .",
    "this is simply related to the moderation of fluctuations with system size .",
    "an analysis such as that shown in figure [ fig : bias ] allows the user to control the parallel error by choosing the problem size and the desired number of particles per subcell .",
    "consequently , our method continues to be a controlled approximation in the sense that the error can be intrinsically computed and arbitrarily reduced .",
    "the algorithm s performance can be assessed via its two fundamental contributions , namely , one that is directly related to the implementation of the minimal process method ( mpm ) through the null events @xcite , and the parallel performance _ per se_. the effect of the null events is quantified by the utilization ratio ( ur ) : @xmath85 which gives the relative weight of null events on the overall frequency line .",
    "the ur determines the true time step gain associated with the implementation of the mpm as @xcite : @xmath86 where @xmath87 and @xmath88 are , respectively , the mpm and standard time steps .",
    "this procedure is intrinsically serial , and will result in superlinear scalar behavior if not taken into account for parallel performance purposes .",
    "next , we show in figure [ fig : ur ] the evolution of the ur for 524,288 ( @xmath89 ) and 1,048,576 ( @xmath90 ) spin systems .",
    "we have done calculations for several numbers of processors and number of particles per subcell . we find that the determining parameter is the latter , _",
    "i.e. _ for a fixed system size and number of processors used , the ur displays a strong dependence with the number of particles per subcell .",
    "the figure shows results for 512 and 4096 particles per subcell , which in the 524,288(1,048,576)-spin system amounts to , respectively , 1024(2048 ) and 128(256 ) subcells per processor . in the latter case",
    ", the ur eventually oscillates around @xmath91 , whereas in the former it is approximately 90% ( _ i.e. _ on average , @xmath92 and 10% of events , respectively , are _ null _ events ) .    ) and 1,048,576 ( @xmath90 ) particle ising system .",
    "calculations have been done varying the number of sublattices per processor , which is shown to have a significant impact on the ur.,width=491 ]    for its part , the parallel efficiency is defined as the wall clock time employed in a serial calculation relative to the wall clock time of a parallel calculation with @xmath44 processors involving a @xmath44-fold increase in the problem size : @xmath93 the inverse of the efficiency gives the weak - scaling behavior of the algorithm .",
    "due to the absence of fluctuations that exist in other parallel algorithms based on intrinsically asynchronous kinetics ( cf . , _",
    "e.g. _ , ref .",
    "@xcite ) , the ideal parallel efficiency of pkmc is always 100% .",
    "let us now consider the efficiency for the following weak - scaling problem . assuming that frequency line searches scale linearly with the number of walkers in our system , the serial time expended in simulating a system of @xmath1 spins to a total time @xmath23 is : @xmath94 where @xmath95 is the number of cycles required to reach @xmath23 , and @xmath96 is the _ computation _ time during each kmc cycle .",
    "for its part , the total parallel time for the @xmath44-fold system is : @xmath97 where @xmath98 is the counterpart of @xmath95 and @xmath99 is the communications overhead due to global and local calls . in the most general case ,",
    "the efficiency is then : @xmath100 as mentioned in the paragraph above , when it is ensured that the serial algorithm also take advantage of the time step gain furnished by the minimal process method , the number of cycles to reach @xmath23 is the same in both cases , @xmath101 .",
    "the parallel efficiency then becomes : @xmath102 next , by virtue of the @xmath103 model @xcite , we assume that the cost of global communications is @xmath104 , with @xmath105 a constant , while the local communication time , @xmath106 , is independent of the number of processors used and scales with the problem size as @xmath107 @xcite .",
    "if we consider the execution time @xmath96 negligible compared to the communication time , we have : @xmath108 where @xmath109 , @xmath110 and @xmath111 are architecture - dependent constants the final expression then reduces to : @xmath112 where @xmath113 and @xmath114 are an architecture and problem dependent constants .    in the case where @xmath53 is overdimensioned _ a priori _ to a prescribed tolerance tol of the _ true _ value , @xmath115 ,",
    "then @xmath116 and eq .  [ eq : initial ] becomes : @xmath117 stemming from the fact that now the ratio @xmath118 .",
    "assuming again that @xmath96 is negligible with respect to @xmath106 and the @xmath119 term for frequency line searches , the expression for the efficiency takes the form : @xmath120 where @xmath114 is the same as in eq .",
    "[ eq : pe3 ] .    combining eqs .",
    "( [ eq : pe3 ] ) and ( [ eq : pe5 ] ) , we arrive at the criterion to choose the optimum algorithm : @xmath121 _ i.e. _ as long as the above inequality is satisfied , avoiding global calls by conservatively setting @xmath53 at the beginning of the simulation results in a more efficient use of parallel resources .",
    "note that , via the constants @xmath113 , @xmath105 , and @xmath114 , this is problem and machine - dependent , and establishing these with confidence may require considerable testing prior to engaging in production runs .",
    "next , we perform scalability tests for the case where @xmath53 is communicated globally , _ i.e. _ the efficiency is governed by eq .",
    "[ eq : pe3 ] .",
    "the tests have been carried out on llnl s distributed - memory parallel platforms , specifically the `` hera '' cluster using intel compilers @xcite .",
    "the scalabilitry calculations were all performed for 512 particles per subcell , regardless of the number of processors used , for systems with three different numbers of spins per processor , namely 4,194,304 ( @xmath78 ) , 2,097,152 ( @xmath90 ) , and 1,048,576 ( @xmath89 ) .",
    "this means that as the number of particles per processor is increased , more subcells are assigend to each processor .",
    "figure [ fig : scaling ] shows the parallel efficiency of the pkmc algorithm as a function of the number of processors used for three reference ising systems at the critical point .",
    "the fitting constants @xmath113 , @xmath105 and @xmath114 are given for each case .",
    "as the figure shows , the number of spins per processor has a significant impact on the parallel efficiency , with larger sizes resulting in better performances .",
    "the efficiency at @xmath122 is upwards of 80% for the largest system , and @xmath82 60% for the smallest system size .",
    "the leap in efficiency observed in all cases between 2 and 4 processors is caused by the nodal interconnects ( band width ) connecting quad cores in the platforms used . as expected from eq .",
    "[ eq : pe2.5 ] , the fitting constant @xmath113 scales roughly as @xmath123 , while @xmath105 does not display a large variability and takes value between 0.41 and 0.49 .",
    "@xmath114 can be considered equal to one for all three cases within the least - squares error , which implies negligible local communication costs ( @xmath124 in eq .",
    "[ eq : pe2.5 ] ) , and simplifies the tolerance criterion above to @xmath125 .    , one with @xmath78 , and another with @xmath126 particles per processor of an ising system at the critical point .",
    "the fitting constants @xmath113 , @xmath105 and @xmath114 in eq .",
    "[ eq : pe3 ] are given for each case.,width=453 ]    the idea behind using a tolerance to minimize or contain global calls forms the basis of the so - called _ optimistic _ algorithms @xcite , where the parameter(s ) controlling the parallel evolution of the simulation are set conservatively either by a self - learning procedure or by accepting some degree of error and monitored sparingly .",
    "for example , for the @xmath90-spin ising system , @xmath127 , @xmath128 , @xmath129 , tol varies between @xmath1300.09 and @xmath1300.22 in the range @xmath131",
    ". a value of 0.09 may not be sufficient to encompass the time fluctuations in @xmath53 , but it is expected that for a higher number of processors the efficiency will improve , although at the cost of the ur .",
    "these and more aspects about the parallel efficiency and its behavior will be discussed in section [ sec : conc ] .",
    "we now apply the method to study the time relaxation of large ising systems near the critical point . as anticipated in section [ sec : ising ] , at the critical point , the relaxation time @xmath132 diverges as @xmath133 , where @xmath134 .",
    "the scaling at @xmath30 is then : @xmath135 in 3d , we use the known critical temperature @xmath136 @xcite to find @xmath37 .",
    "we start with all spins @xmath137 and let @xmath138 decay from its initial value of unity down to zero . at each time point , we can find the critical exponent @xmath37 from : @xmath139^{-1}\\ ] ] where the ratio @xmath140 takes the known value of @xmath141 @xcite .",
    "we have carried out simulations with lattices containing 1024@xmath76512@xmath76512 ( @xmath142 ) , 1024@xmath761024@xmath76512 ( @xmath143 ) , and 1024@xmath761024@xmath761024 ( @xmath144 ) spins .",
    "the results are shown in figure [ fig : z2 ] for critical exponents calculated during @xmath145 , from time derivatives averaged over 300 to 500 timsteps .    ) ,",
    "half ( @xmath143 ) , and one ( @xmath144 ) billion - spin ising systems for @xmath145 .",
    "the horizontal line at @xmath146 marks the consensus value in 3d from the literature.,width=453 ]    at long time scales , the critical exponent oscillates around values that range from , roughly , 2.06 to 2.10 depending on system size .",
    "this in good agreement with the converged consensus value of @xmath1472.04 published in the literature @xcite ( shown for reference in fig .  [ fig : z2 ] ) . however , as time increases , the oscillations increase their amplitude with inverse system size .",
    "oscillations of this nature also appear in multi - spin calculations , both for smaller @xcite and larger @xcite systems , where the inverse proportionality with system size is also observed .",
    "these may be caused by insufficient statistics due to size limitations , as we have shown that , under the conditions chosen for the simulations , our calculations are statistically equivelent to serial ones ( cf .",
    "[ fig : bias ] ) .",
    "the effect of the system size is also clearly manifested in the relative convergence rate of @xmath37 . as size increases , convergence to the expected value of 2.04",
    "is achieved on much shorter time scales , _",
    "i.e. _ fewer kmc cycles .",
    "we now discuss the main characteristics of our method .",
    "we start by considering the three factors that affect the performance of our algorithm :    a.   _ number of particles per subcell_. this is the most important variable affecting the algorithm s performance , as it controls the intrinsic parallel bias and the utiliation ratio .",
    "higher numbers of spins per subcell both reduce the bias ( cf .  figs .",
    "[ fig : b - t ] and [ fig : bias ] ) and increase the ur ( fig .",
    "[ fig : ur ] ) , bolstering performance .",
    "however , this also results in an increase of the value of @xmath53 , which causes a reduction in @xmath62 .",
    "thus , decreasing the bias and increasing the time step are actions that may work in opposite directions in terms of performance , and a suitable balance between both should be found for each class of problems .",
    "b.   _ number of particles per processor_. this parameter affects the parallel efficiency via the number of spins per processor @xmath148 ( for regular space decompositions , @xmath149 ) . as @xmath148 increases , a significant improvement is observed .",
    "this is directly related to the parameter @xmath113 in eq.[eq : pe3 ] , which scales inversely with @xmath148 and is related to the cost of linear searches .",
    "c.   _ total system size_. as figs .",
    "[ fig : b - t ] and [ fig : bias ] show , for a given sublattice decomposition , a larger system incurs in smaller relative fluctuations in the magnetization , which results in a more contained bias .    through the constants @xmath113 , @xmath105 , and @xmath114 , the parallel efficiency",
    "strongly depends on the latency and bandwidth of the communication network used .",
    "that is why we have explored other efficiency - increasing alternatives that contain the number of global calls and the associated overhead .",
    "prescribing a tolerance on the expected fluctuations of @xmath53 is in the spirit of so - called optimistic kmc methods , and ideally its value is set by way of a self - learning procedure that maximizes the efficiency .    in any case , the intersection of items ( i)-(iii ) above configures the _ operational _ space that determines the class of problems that our method is best suited for : large ( multimillion ) systems , with preferrably a sublattice division that achieves an optimum compromise between time step gain and lowest possible bias , with the maximum possible number of particles per processor .",
    "these are precisely the conditions under which we have simulated critical dynamics of 3d ising systems , with very good results .",
    "we conclude that spkmc is best designed to study this class of dynamic problems where fluctuations are important and there is unequivocal size scaling .",
    "this includes applications on many other areas of physics , such as crystal growth , irradiation damage , plasticity , biological systems , etc . ,",
    "although other difficulties due to the distinctiveness of each problem may arise that may not be directly treatable with the algorithm presented here .",
    "we note that , because the parallel bias is seen to saturate for large numbers of parallel processes , and the efficiency is governed by the inverse logarithmic term , the only limitation to using spkmc is given by the number of available processors .",
    "we have developed an extension of the synchronous parallel kmc algorithm presented in ref .",
    "@xcite to discrete lattices .",
    "we use the chess sublattice technique to rigorously account for boundary conflicts , and have quantified the resulting spatial bias .",
    "the algorithm displays a robust scaling , governed by the global communications cost as well as by the spatial decomposition adopted .",
    "we have applied the method to multimillion - atom three - dimensional ising systems close to the critical point , with very good agreement with published state - of - the - art results .",
    "we thank m.  h.  kalos for his invaluable guidance , suggestions , and inspiration , as this work would not have been possible without him . this work performed under the auspices of the us department of energy by lawrence livermore national laboratory under contract de - ac52 - 07na27344 .",
    "e. m. acknowledges support from the spanish ministry of science and education under the `` juan de la cierva '' programme .          a. f. voter , `` introduction to the kinetic monte carlo method '' , in _ radiation effects in solids _ , edited by k. e. sickafus , e. a. kotomin , and b. p. uberuaga ( springer , nato publishing unit , dordrecht , the netherlands , 2006 ) pp .  1 - 24",
    ".                                                  e. schwabe , v. e. taylor , m. hribar , `` an in - depth analysis of the communication costs of parallel finite element applications '' , technical report cse-95 - 005 , northwestern university , eecs department , 1995 ( http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.31.5316 ) .",
    "llnl s `` hera '' cluster ( https://computing.llnl.gov/tutorials/linux_clusters/#systemsocf ) , on which all kmc simulations were performed , uses chaos linux as o / s and runs version 1 of the mpi libraries , with two - sided communications .."
  ],
  "abstract_text": [
    "<S> an extension of the synchronous parallel kinetic monte carlo ( pkmc ) algorithm developed by martinez _ et al _ [ _ j .  comp .  </S>",
    "<S> phys . </S>",
    "<S> _  * 227 * ( 2008 ) 3804 ] to discrete lattices is presented . </S>",
    "<S> the method solves the master equation synchronously by recourse to null events that keep all processors time clocks current in a global sense . </S>",
    "<S> boundary conflicts are rigorously solved by adopting a chessboard decomposition into non - interacting sublattices . </S>",
    "<S> we find that the bias introduced by the spatial correlations attendant to the sublattice decomposition is within the standard deviation of the serial method , which confirms the statistical validity of the method . </S>",
    "<S> we have assessed the parallel efficiency of the method and find that our algorithm scales consistently with problem size and sublattice partition . </S>",
    "<S> we apply the method to the calculation of scale - dependent critical exponents in billion - atom 3d ising systems , with very good agreement with state - of - the - art multispin simulations .    ,    , and    kinetic monte carlo , parallel computing , discrete lattice , ising system 02.70.-c , 02.70.uu , 61.72.ss </S>"
  ]
}