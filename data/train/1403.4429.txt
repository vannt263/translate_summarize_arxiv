{
  "article_text": [
    "la statistique baysienne est une approche spcifique de la statistique infrentielle qui propose une rponse  la fois unitaire et globale au problme infrentiel , dans le cadre paramtrique comme dans le cadre non - paramtrique .",
    "elle se distingue des approches dites  classiques \" par la construction et lutilisation dune loi de probabilit sur lensemble des notions pouvant faire lobjet dune infrence . on peut lgitimement se demander pourquoi la distinction entre cette",
    "approche baysienne et les mthodologies ( plus ) classiques est ncessaire  et pourquoi elle nest pas ",
    "classique\" , dautant quelle saccompagne de dbats philosophiques particulirement virulents et de positions militantes pouvant parfois voquer des drives sectaires , attitudes qui ne se retrouvent pas dans les autres approches de la statistique infrentielle .",
    "le principe de linfrence baysienne se rsume assez simplement : tant donn un modle statistique , lensemble des constituants ( paramtres et / ou fonctions ) inconnus de ce modle est trait comme une variable alatoire ",
    "potentiellement de dimension infinie , donc munie dune loi de probabilit , et lensemble des rponses infrentielles se fonde sur la loi de cette variable alatoire , conditionnellement aux donnes .",
    "ce qui fait la beaut de cette approche et explique en partie son attractivit est que la dmarche infrentielle est alors quasi - automatique , tant donn cette loi et une mesure des performances des procdures dite _ fonction de cot_. dans les approches alternatives , la seule variable alatoire est celle correspondant aux donnes et la contruction des procdures infrentielles est gnralement ouverte ( entre estimateurs des moments , du maximum de vraisemblance , etc . )",
    "la contrepartie  ce caractre automatique , que certains pourraient qualifier de beaut vnneuse ! , est que la modlisation des inconnues en variable alatoire exige un choix de loi de probabilit , choix subjectif opr par le statisticien baysien car ne reposant pas directement sur lobservation .",
    "cet aspect de lapproche baysienne concentre la majorit des critiques de la communaut statistique , une critique secondaire ( et plus rpandue dans la communaut du _ machine learning _ ) tant lobligation faite au statisticien de construire intgralement le modle statistique sur les donnes , ce qui empcherait le traitement de structures complexes et / ou de grandes tailles .    ce chapitre prsente brivement le dveloppement historique de lapproche baysienne , depuis bayes et laplace jusqu nos jours , ainsi que les motivations philosophiques et mthodologiques qui la sous - tendent .",
    "la seconde partie prsente quelques lments de mise en uvre au travers dexemples simples .",
    "nous dirigeons le lecteur vers @xcite pour une couverture plus complte ( en franais ) de cette mthodologie spcifique , de nombreux autres ouvrages tant disponibles en anglais .",
    "quelques avertissements au lecteur sont de mise quant au contenu de ce chapitre : ne pouvant y dfinir correctement les concepts de probabilit ncessaires ,",
    "je suppose que les lecteurs sont suffisamment familiers avec ceux - ci ,  un niveau de fin de licence ( 3ime anne duniversit ) , pour manipuler les concepts de probabilit conditionnelles .",
    "par ailleurs , ces mmes lecteurs devront se rfrer  leur source favorite en ce qui concerne les distributions usuelles ( wikipdia , bien sr , ou lannexe de @xcite ) .",
    "le concept de  statistique baysienne \" part du nologisme  baysien \" , tir du nom de thomas bayes , qui introduisit le thorme qui porte  prsent son nom dans un article posthume de 1763 , il y a 250 ans .",
    "ce thorme exprime une probabilit conditionnelle en termes de la probabilit conditionnelle inverse pondre par les probabilits marginales , @xmath0 ce qui a valu  la statistique ( alors uniquement envisage sous cet angle baysien ) dtre appele  probabilits inverses \" pendant plus dun sicle , de @xcite  @xcite , avant que fisher nintroduise le terme  baysien \" @xcite .",
    "bien que la formule ( ou thorme ) de bayes soit une consquence directe de la dfinition des probabilits ( et densits ) conditionnelles , son application  des problmatiques statistiques , o une observation @xmath1 dpend dun paramtre inconnu @xmath2 est effectivement approprie , au sens o le contexte _ inverse _ ce qui est connu et ce qui est inconnu .",
    "effectuer linversion pour obtenir linformation contenue dans @xmath1  propos de @xmath2 conduit  dfinir la loi _ a posteriori _ , loi conditionnelle de @xmath2 sachant @xmath1 .",
    "pour que la loi a posteriori soit dfinie , le modle doit non seulement comprendre une loi des observations , de densit @xmath3 ou @xmath4 mais galement une loi de probabilit sur le paramtre @xmath2 , de densit @xmath5 et appele loi a priori .",
    "dans ce cas , la loi a posteriori sobtient par la formule de bayes , @xmath6 cette formule , prsente dans lessai de thomas bayes de 1763 , est la version gnrale du thorme dinversion ci - dessus .",
    "le dnominateur de la fraction est  la fois la densit marginale associe  lobservation @xmath1 et la constante de normalisation permettant de transformer le numrateur en densit de probabilit .",
    "dans ce contexte , o @xmath2 est trait comme une variable alatoire , la loi des observations apparat comme une loi conditionnelle  la valeur du paramtre , plutt que comme une loi indice par @xmath2 comme dans lapproche classique",
    ".    cette distinction entre paramtre inconnu ( mais fixe ) et paramtre alatoire peut  la fois paratre fondamentale et sembler condamner lapproche baysienne comme inapproprie vis -  - vis de la comprhension moderne ",
    "labore par kolmogorov et ses successeurs",
    " de la notion de probabilit .",
    "la distinction faite entre probabilit comme stabilisation des frquences ( loi des grands nombres ) et probabilit comme quantification dun degr dincertitude met en lumire limprcision et la subjectivit lies  la seconde approche .",
    "cest certainement la perspective quadopta fisher trs rapidement dans sa carrire et la raison de sa querelle avec jeffreys au cours des annes 1930 @xcite .",
    "bien que fonde elle aussi sur",
    "la notion de vraisemblance , @xmath7 qui reflte galement un principe dinversion , lapproche de fisher refuse linterprtation de @xmath4 comme densit conditionnelle et la probabilisation de @xmath2 .",
    "il est cependant abusif de faire de cet aspect de lanalyse baysienne autre chose quune source de discussion philosophique .",
    "en effet , le passage de la notion de paramtre _ inconnu _  la notion de paramtre _ alatoire _ est incompatible avec la plupart des expriences en particulier dans les sciences physiques .",
    "la modlisation statistique suppose au contraire lexistence dun paramtre fixe @xmath2 , sur lequel elle vise  obtenir une information aussi prcise que possible .",
    "la constante de hubble , la vitesse de la lumire , le coefficient de diffraction dun prisme sont autant dexemples o le concept dala sur le paramtre ne fait pas sens .",
    "cependant , ce que propose lapproche baysienne se situe  un autre niveau smantique qui ne remet pas en cause cette modlisation de la ralit .",
    "la loi a posteriori est utilise comme un nouvel ( et efficace ) outil de rsum de linformation disponible sur @xmath2 , sans remettre en cause lexistence de ce paramtre inconnu et _",
    "non alatoire_. en dautres termes , la loi a posteriori est un outil de reprsentation de linformation disponible sur @xmath2 une fois les observations obtenues , elle reflte lincertitude inhrente aux donnes et  lexprimentateur plutt quun ala physique sur ce paramtre",
    ".    cette distinction entre outil de reprsentation et vritable ala a cependant presque caus lextinction dfinitive de lapproche baysienne lorsque , au tournant du sicle , et comme dans de nombreuses autres branches des mathmatiques , kolmogorov et ses collgues probabilistes ont formalis la thorie des probabilits .",
    "il devenait alors difficile de donner un sens mathmatique  la loi a priori si elle ne correspond pas  un vritable phnomne alatoire mais plutt  une traduction de ce que lexprimentateur est prt  parier sur les valeurs possibles du paramtre .",
    "cet aspect subjectif de la loi a priori , rsultant du choix de lexprimentateur , peut apparatre comme un lmnt rducteur de la perspective baysienne , mais il permet de reflter les informations ( et leur degr dimprcision ) do nt dispose cet exprimentateur et donc conduit  une infrence plus prcise et plus riche , de ce fait . que deux exprimentateurs adoptent deux lois a priori diffrentes ne devrait pas plus porter  critique que le fait que deux expriences ( donc deux sries dobservations diffrentes ) conduisent  des fonctions de vraisemblance diffrentes .",
    "ni que deux statisticiens  classiques \" choisissent lune un estimateur du maximum de vraisemblance et lautre un estimateur des moments .",
    "il est fondamental dobserver que lapproche baysienne ne dispose pas dune _ seule loi a priori _ , une sorte de graal quil conviendrait dobtenir aprs de longues recherches !",
    "cette vision , souvent observe dans la littrature applique , est du mme ordre que celle qui attribue au paramtre @xmath2 un vritable caractre alatoire .",
    "( bien entendu , dans certains contextes comme ceux des modles  effets alatoires ou de la prvision , certaines composantes de @xmath2 sont effectivement considres comme alatoires , mais elles le sont aussi dans lapproche classique de ces modles . )",
    "dans lapproche baysienne , comme dans dans la plupart des autres coles de statistique , parler de la  vraie \" valeur du paramtre fait sens .",
    "comme nous le verrons dans les sections suivantes , disposer dune loi a posteriori permet de construire des procdures infrentielles ( tests , intervalles de confiance , densits prdictives ) de la manire la plus naturelle possible , ce qui explique entre autres la persistance de cette approche , contre vents et mares @xcite , depuis 250 ans .      tandis que la vision de fisher , fonde sur la vraisemblance , nadmet pas une extension vers une modlisation baysienne , il existe un principe de vraisemblance , formalis par @xcite , do nt dcoule comme implmentation premire la mthodologie baysienne .",
    "bien que ce principe soit rgulirement remis en cause @xcite , il sous - tend suffisamment cette approche pour que nous le rappelions brivement ici .",
    "le _ principe de vraisemblance _ impose  lexprimentateur confront  deux sries dobservations @xmath8 et @xmath9 portant sur le mme paramtre @xmath2 ayant des fonctions de vraisemblance proportionnelles , donc telles que , pour tout @xmath2 , @xmath10 de conduire  la mme infrence sur ce paramtre @xmath2 .",
    "cest naturellement le cas si lexprimentateur adopte une dmarche baysienne puisque les lois a posteriori sont alors identiques .",
    "il en va de mme pour lestimation par maximum de vraisemblance .",
    "lintuition derrire le principe est que linformation apporte par les deux chantillons sur le paramtre @xmath2 est la mme .",
    "cette situation de fonctions de vraisemblance proportionnelles se produit par exemple lorsquon compare un chantillonnage binomial , @xmath11 ,  un chantillonnage binomial ngatif , @xmath12 , ce qui correspond  un sondage avec un nombre fixe de personnes interroges contre un sondage avec un nombre",
    "fixe de personnes avec une rponse donne .",
    "le nombre dessais total @xmath13 et le nombre de succs @xmath14 sont les mmes , mais lala portant sur deux parties diffrentes , les fonctions de vraisemblance sont proportionnelles , @xmath15 bien que cet exemple puisse paratre trs artificiel , il sous - tend la classe des problmes de rgles darrt , o un chantillon @xmath16 est de taille @xmath13 alatoire , la loi de @xmath13 ne dpendant pas du paramtre @xmath2 .",
    "une infrence distinguant un chantillon @xmath16 iid dun chantillon @xmath16 produit par une rgle darrt contredit le principe de vraisemblance . plus",
    "gnralement , des mthodologies fondes sur des proprits frquentistes comme les @xmath17-values ne saccordent pas avec ce principe .",
    "@xcite a dmontr que le principe de vraisemblance dcoulait logiquement de la conjonction de deux autres principes , le principe de conditionalit et le principe dexhaustivit ,  savoir que , ( i ) si deux expriences sont possibles pour mesurer @xmath2 et que lune est choisie au hasard , seule compte lexprience effectivement ralise , et que ( ii ) linfrence ne doit dpendre des donnes que via des statistiques exhaustives . comme le discutent @xcite , la mise en uvre du principe de vraisemblance conduit naturellement  une construction baysienne ( qui ne dpend effectivement que de la fonction de vraisemblance ) , mme sil existe des mthodologies alternatives ( comme le test du rapport de vraisemblance ) respectant le principe de vraisemblance et couvrant certains aspects de linfrence .",
    "comme signal ci - dessus , la loi a priori est choisie par lexprimentateur et tmoigne  un degr ou un autre dun choix ( comme peut ltre la slection de la procdure infrentielle voire de la loi des observations ) .",
    "ces lois sont souvent choisies dans des classes de lois standard pour faciliter leur utilisation , mme aprs lavnement de techniques de simulation plus puissantes . limpact de ce choix sur la rponse infrentielle est non - nulle , mme sil disparat  mesure que la taille de lchantillon grandit , du fait de la consistance  convergence de lestimation vers la vraie valeur du paramtre quand la taille de lchantillon tend vers linfini  de lapproche baysienne dans un grand nombre de situations .",
    "il peut tre valu de manire analytique ou numrique , mais aussi compar  des solutions dites de rfrence @xcite , dcrites ci - dessous .      dans le cadre de lois dobservations standard comme la loi normale , il existe des familles de lois a priori facilitant le calcul de la loi a posteriori .",
    "bien que leur motivation soit surtout fonde sur le fait quelles simplifient de la drivation de la loi a posteriori ( plutt que sur une caractristique particulire de linformation a priori ) , elles nen constituent pas moins un lment de base dans la construction effective de lois a priori plus personnelles .",
    "par ailleurs , et dun point de vue beaucoup plus pratique elles sont llment constitutif du logiciel bugs @xcite , principal outil de lanalyse baysienne de donnes car ces lois conjugues permettent au logiciel de construire automatiquement lalgorithme correspondant de monte carlo par chantillonnage de gibbs .",
    "ces lois sont dites conjugues (  une famille de fonctions de vraisemblance ) .",
    "les lois conjugues sont intrinsquement associes aux lois de familles exponentielles : @xmath18 o @xmath2 est un paramtre de dimension @xmath19 et @xmath20 une fonction  valeurs dans @xmath21 , @xmath22 permettant la normalisation de la densit de probabilit .",
    "ainsi , ces densits sont associes aux lois a priori de la forme @xmath23 o @xmath24 et @xmath25 sont contraints par lintgrabilit de la fonction , puisque @xmath26 - [ \\lambda+1]\\psi(\\theta ) \\}\\,.\\ ] ] la loi a posteriori est donc dfinie par un changement de paramtres , de @xmath27  @xmath28 , et de @xmath29  @xmath30 .",
    "un point crucial est que ces lois conjuguees ne peuvent exister _",
    "que _ pour les vraisemblances dcoulant des familles exponentielles , en vertu du lemme de pitman - koopman sur lexistence de statistiques exhaustives ( trop technique pour tre dtaill ici , voir par exemple @xcite )",
    ". elles sont cependant disponibles pour un grand nombre de lois classiques , comme la loi normale  un ou deux paramtres , les lois binomiale , binomiale ngative , de poisson , gamma  un ou deux paramtres , dirichlet , de wishart ... par exemple , pour des observations @xmath31 normales de loi @xmath32 , o @xmath33 est connu , la loi conjugue est aussi normale @xmath34 .",
    "en effet , la loi de @xmath2 a posteriori est la loi normale @xmath35    les lois conjugues ayant une forme _ impose _ par la loi des observations , elles ne sont",
    "pas  mme de reflter toute sorte dinformation a priori , tout en demandant la spcification des hyperparamtres @xmath27 et @xmath29 .",
    "une extension efficace et robuste de ces lois est fournie par les mlanges , discrets ou continus , obtenus par lintroduction de lois sur les paramtres @xmath27 et @xmath29 , en particulier parce quelles autorisent  leur tour des implmentations par lalgorithme de gibbs @xcite .",
    "dans une situation o linformation a priori nest pas disponible , et o lexprimentateur renacle  faire un choix , il est tentant de chercher  proposer une loi refltant ce manque dinformation .",
    "hlas ( ou pas ! ) , il nexiste pas de loi  la moins informative \" et on peut seulement , au mieux , dfinir une procdure automatique de construction dune loi a priori de rfrence ,  laune de laquelle les lois a priori subjectives peuvent tre values .",
    "donc dune convention et non dune optimalit quelconque au titre de la faible information a priori .",
    "mme si ces lois de rfrence sont souvent appeles non - informatives , elles contiennent nanmoins des informations sur le paramtre et ne peuvent prtendre reprsenter une complte ignorance @xcite",
    ".    une illustration de cette impossibilit est fournie par le principe de la raison insuffisante de @xcite : par extension du cas dun ensemble fini de paramtres , laplace propose dutiliser une loi uniforme dans toute situation non - informative .",
    "si lespace des paramtres nest pas compact , cela exige lemploi dune mesure de lebesgue en lieu et place dune loi de probabilit , ce qui ne pose pas problme en soi du moment que la loi a posteriori est dfinie , et surtout la solution dpend de la paramtrisation adopte , puisque la loi uniforme ne  rsiste \" pas  un changement de variable .",
    "bien que cette solution reste celle adopte tout au long du 19ime sicle , elle cristallise les critiques naissantes sur le paradigme baysien .",
    "il faut en fait attendre les annes 1930 et harold jeffreys pour voir apparatre une perspective globale sur le choix des lois de rfrence .",
    "ironiquement , cette solution emprunte  fisher en ce quelle est fonde sur linformation du mme nom .",
    "si linformation de fisher associe  un modle est dfinie par @xmath36\\ ] ] la loi de jeffreys associe est donne par @xmath37 soit donc la racine du dterminant de linformation .",
    "plusieurs remarques sont de mise :    1 .",
    "la solution de jeffreys est invariante par changement de paramtrisation du modle , grce  la formule du jacobien ; 2 .",
    "la loi est directement dpendante de linformation , ce qui signifie que les rgions o linformation est plus importante sont privilgies , car les donnes y sont plus discriminantes ; 3 .",
    "cette construction est en gnral compatibles avec lutilisation de lois invariantes sous laction dun groupe ( comme celui des translations ) , les mesures de haar ; 4 .",
    "cette construction ne garantit en rien lintgrabilit de @xmath38 , ce qui signifie que les lois de jeffreys seront souvent des mesures ; 5 .",
    "les lois de jeffreys dpendent de lintgralit de la loi des observations , donc ne respectent pas le principe de vraisemblance .",
    "une illustration de ce dernier point est fournie par lopposition entre la loi binomiale , @xmath39 pour qui la loi de jeffreys est la loi @xmath40 , et la loi binomiale ngative , @xmath41 , pour qui la loi de jeffreys est la mesure ( impropre , cest  dire ne pouvant tre normalise en une loi de probabilit ) de densit @xmath42 .    comme cette construction universelle de lois de rfrence peut donner naissance  des  monstres \" , par exemple dans le cas de lestimation de @xmath43 quand @xmath44 et @xmath17 grand , il existe des dterminations de lois de rfrence qui distinguent entre paramtres dintret et paramtres de nuisance avant de construire des lois de jeffreys conditionnelles et marginales sur les deux groupes .",
    "nous renvoyons le lecteur  @xcite et @xcite pour des entres sur cette extension qui demeure peu utilise  ce jour .",
    "voir aussi @xcite pour une revue de certains principes de dtermination des lois  non - informatives \" .",
    "en lien avec ces principes , insistons une nouvelle fois sur le fait que , de mme quil nexiste pas  une \" loi a priori unique quil faudrait dcouvrir , il nexiste pas non plus  une \" seule loi a priori non - informative .",
    "il sagit bien de poser une rfrence , qui peut servir  la fois  analyser des problmes sans information visible et  comparer diffrentes lois a priori , si besoin .",
    "( cette rfrence se trouve faire dfaut dans le cadre des tests ,  moins daccepter quelques compromis avec le paradigme baysien , @xcite . )",
    "nous avons mentionn dans lintroduction que lapproche baysienne permettait galement de traiter de problmes dans un cadre non - paramtrique .",
    "cela signifie par exemple que ,  partir dun chantillon iid @xmath31 de loi inconnue - uplet , suivant la reprsentation de bruno de finetti .",
    "voir par exemple @xcite . ]",
    "@xmath45 , une loi a posteriori peut tre construite sur @xmath45 .",
    "les lois a priori qui sous - tendent ces constructions sont donc des lois sur des espaces de fonctions comme lensemble des densits de probabilit .",
    "par exemple , la loi de dirichlet _ fonctionnelle _ , @xmath46 , est dfinie  partir dun coefficient de prcision @xmath47 et dune moyenne a priori @xmath48 , loi de probabilit .",
    "cette loi @xcite gnralise la loi de dirichlet sur le simplexe @xmath49 au sens o , pour toute partition @xmath50 de lespace des observations , si @xmath51 , alors @xmath52 la loi a posteriori correspondante est conjugue , @xmath53 o @xmath54 dnote la distribution empirique associe  lchantillon @xmath31 , ce qui signifie que la variable alatoire nest pas absolument continue par rapport  la mesure de lebesgue , mme si @xmath48 lest .",
    "en particulier , si @xmath55 , la loi conditionnelle de @xmath56 sachant @xmath57 est de la forme @xmath58 lvaluation complte de la loi a posteriori de @xmath45 exige lemploi doutils de simulation qui ne seront pas abords ici ( voir par exemple @xcite et @xcite ) .",
    "une reprsentation quivalente du processus de dirichlet est appele _ processus du restaurant chinois _ pour la raison suivante : gnrer un chantillon de taille @xmath13 suivant le processus de dirichlet est quivalent  simuler un flux darrives de clients dans un restaurant chinois stylis .",
    "chaque client de ce restaurant sassied  une table dj occupe avec une probabilit en proportion du nombre de clients prsents  cette table et  une nouvelle table avec probabilit proportionnelle  @xmath59 .",
    "dautres versions du processus de dirichlet sont fondes sur des successions de partitions de lintervalle @xmath60 $ ] appeles  stick breaking \" en raison de leur construction rcurrente @xcite .",
    "ces diffrentes interprtations sont utilises soit au niveau thorique , pour valider la convergence des estimateurs rsultants @xcite , soit au niveau computationnel , pour simuler ces processus @xcite .",
    "de nombreuses extensions de cette loi a priori existent dans la littrature , fondes sur diffrents processus comme les processus gaussiens ou le processus de lvy .",
    "des reprsentations par mlanges permettent en particulier de dpasser le dfaut des processus de dirichlet de ne simuler que des lois  support discret .",
    "une extension de la reprsentation du restaurant chinois sappelle le buffet indien ( ! ) et permet le mlange de plusieurs caractristiques au lieu dimposer une partition @xcite .",
    "enfin , notons que les outils traditionnels de modlisation",
    "mathmatique despaces de fonctions , comme par exemple lemploi de bases dondelettes , autorisent une autre forme de reprsentation baysienne de linfrence fonctionnelle @xcite .",
    "nous dcrivons dans cette partie quelques mises en uvre de linfrence baysienne dans des modles standard .",
    "il sagit en grande partie dillustrations et nous renvoyons le lecteur par exemple  @xcite pour une perspective plus complte .",
    "si la loi a priori @xmath5 est llment central qui concentre lessentiel des critiques sur la perspective baysienne , la loi a posteriori @xmath61 est llment central permettant de conduire linfrence baysienne .",
    "une fois cette loi construite , et le modle accept ( voir ci - dessous ) , il nest plus ncessaire de conserver les donnes ! en effet ,",
    "les diverses procdures infrentielles sont toutes construites ( automatiquement )  partir de cette loi : estimateurs , variations , intervalles de confiance , tests , choix de variables , choix de modle , prvisions , etc .",
    "certains dfendent en fait la dfinition de la loi a posteriori comme ultime but de linfrence baysienne , considrant que les procdures ci - dessus ne sont que des rsums qui dgradent le contenu informatif de la loi a posteriori .",
    "bien quune loi de probabilit ne puisse en effet se rsumer  certains de ses moments , il est nanmoins pertinent de fournir des rponses aux questions des clients et des dcideurs , pour lesquels la loi a posteriori demeure un concept abstrait .",
    "cette perspective pragmatique est particulirement pertinente dans les problmes  paramtres de grande dimension , qui pour la plupart correspondent  des paramtres de nuisance .    la loi a posteriori @xmath61 tant construite , on peut driver ( analytiquement ou non ) les estimateurs ponctuels que sont la moyenne et la mdiane a posteriori de nimporte quelle transformation @xmath62 , ainsi que les valuations de leur variabilit fournies par les cart - types a posteriori . de plus , la loi a posteriori induit des lois a posteriori marginales pour toute transformation @xmath62 , dduites par projection du modle probabiliste joint , @xmath63 . de ces lois marginales peuvent se dduire des intervalles ou des rgions de confiance naturelles , les rgions de plus forte densit a posteriori ( ou rgions hpd pour _ highest posterior density _ ) @xmath64 do nt la borne @xmath65 peut se calibrer en fonction du taux de couverture @xmath59 souhait pour cette rgion .",
    "( elle dpendra alors de @xmath1 . ) contrairement aux intervalles fonds sur lapproximation normale des  deux sigma \" , ces rgions ont une couverture exacte et peuvent tre assymtriques .",
    "elles sont par contre dpendantes de la paramtrisation choisie ( ou , ce qui est quivalent , de la mesure utilise pour mesurer les volumes , voir @xcite ) .",
    "il est mme possible de discuter de la vraisemblance dune hypothse comme @xmath66 en examinant si la valeur @xmath67 appartient  la rgion hpd , bien que cette perspective ne soit pas conseille",
    "puisquelle ne prend pas en compte laction rsultant dun rejet de @xmath68 .",
    "comme simple illustration , prenons lexemple dun phnomne binaire ,  valeurs dans @xmath69 , observ durant @xmath13 rptitions indpendantes et identiquement distribues ( i.i.d . ) .",
    "le nombre de @xmath70 , @xmath71 , est alors modlis par une loi binomiale @xmath72 o @xmath17 est la probabilit dobtenir une valeur @xmath70 .",
    "si on associe  ce paramtre @xmath17 une loi de jeffreys , @xmath73 qui correspond  une loi bta @xmath74 une fois normalise , la construction de la loi a posteriori est immdiate : @xmath75 qui correspond  une loi bta @xmath76 ( toujours aprs normalisation ) .",
    "si , par exemple , @xmath77 et @xmath78 , la loi a posteriori est la loi bta @xmath79 , de mode _ et _",
    "moyenne @xmath80 .",
    "lintervalle de crdibilit  queues gales , valant @xmath81 $ ] pour un niveau de crdibilit @xmath82 , est plus facile  calculer que la rgion hpd.$ ] , soit donc le mme intervalle ( pour cette prcision ) que la solution symtrique ! ] si  prsent nous cherchons  tester lhypothse nulle et ponctuelle @xmath83 , le facteur de bayes en faveur de @xmath68 ( voir section 4.3 ) est donn par @xmath84 o @xmath85 dnote la constante de normalisation de la densit de la loi @xmath86 .",
    "dans le mme exemple numrique que ci - dessus , ce facteur de bayes vaut @xmath87 , donc exprime un soutien assez important en faveur de @xmath68 .",
    "mme si les estimateurs ci - dessus sont des produits de la loi a posteriori , il est lgitime de se demander comment les comparer et quel estimateur choisir comme",
    " meilleur rsum \" .",
    " lexception de cas spcifiques , il nexiste pas de rponse  cette question sans passer par une modlisation de la dcision prise , qui donne un sens au terme  meilleur \" .",
    "il nest gure difficile de se convaincre quil nexiste pas destimateur optimal  lexception du trivial @xmath88 .",
    "suivant les perspectives adoptes pour classer les estimateurs , certains seront prfrables  dautres mais trs trs rarement prfrables  tous les autres pour tous les critres de classement .",
    "cest du moins le cas dans le cadre frquentiste o coexistent en gnral des classes destimateurs faiblement optimaux .",
    "a loppos , la perspective baysienne permet",
    "de dfinir une forme plus forte doptimalit et daboutir  une solution _",
    "unique_.    la drivation de cette proprit repose sur la notion de fonction de perte ou de cot , emprunte  la thorie des jeux , @xmath89 , qui value lerreur ou la pnalit rsultant de la dcision @xmath19 lorsque la vritable valeur du paramtre est @xmath2 .",
    "par exemple , si @xmath19 est la dcision destimer @xmath2 , la formalisation de lerreur peut tre la somme ou le maximum des erreurs absolues sur toutes les composantes , soit @xmath90 mme si des problmes vritables peuvent conduire  une dtermination unique de la fonction de cot , dicte par des impratifs financiers par exemple , il est plus frquent quon doive adopter des fonctions de cot abstraites comme celles ci - dessus ou la plus traditionnelle , dj adopte par legendre et laplace , @xmath91 do nt le principal intrt est de fournir des solutions explicites ( voir ci - dessous ) .    tant donn un problme modlis par une fonction  valeurs relles @xmath89 , un estimateur @xmath92 propose une dcision @xmath93 ( ou estimation ) pour chaque observation @xmath1 .",
    "la comparaison des estimateurs ne peut se faire au travers de @xmath94 , puisque @xmath1 est alatoire et @xmath2 est inconnu .",
    "tandis que lapproche classique value les estimateurs au travers de lerreur moyenne @xmath95 $ ] ou risque , lapproche baysienne de la thorie de la dcision consiste  prendre lerreur moyenne par rapport aux paramtres , @xmath96 lintrt fondamental de cette perspective est quelle permet  la fois dliminer le paramtre inconnu et de conditionner par rapport  lobservation @xmath1 .",
    "les estimations @xmath93 sont alors toutes comparables  @xmath1 donn , ce qui permet dobtenir la meilleure dcision au sens de la fonction de cot : un _ estimateur de bayes _ est ainsi dfini comme la fonction qui associe  tout @xmath1 la dcision @xmath97 par exemple , le cot @xmath98 ci - dessus conduit  un estimateur de bayes gal  la moyenne a posteriori , @xmath99 $ ] .",
    "cet estimateur classique est cependant peu recommand dans le cas de lois a posteriori multimodales , comme dans le cas des mlanges de distribution @xcite .",
    "sous certaines hypothses , do nt la convexit stricte de la fonction de cot et labsolue continuit de la loi a priori , lestimateur rsultant est unique .",
    "il a de plus la proprit de minimiser le risque intgr , @xmath100\\,\\text{d}\\theta\\,,\\ ] ] ce qui lui permet de plus de bnficier de proprits frquentistes doptimalit comme la minimaxit  atteindre la plus petite des erreurs maximales et ladmissibilit ",
    "ne pas connatre dautre estimateur dominant @xmath92 uniformment ( voir * ? ? ?",
    "bien que cette approche de la construction des procdures statistiques optimales produise des arguments additionnels pour la justification de lapproche baysienne ( les thormes de classes compltes de @xcite dmontrant ainsi que les seuls estimateurs admissibles sont les estimateurs de bayes ) , le recours  une perspective dcisionnelle nest gure suivi dans les ouvrages rcents de statistique baysienne ( voir par exemple @xcite ) .",
    "mon point de vue est quil est quelque peu regrettable de ne pas prendre en compte les aspects dcisionnels , car ils conduisent  une formulation rigoureuse du choix des estimateurs et illustrent la dualit entre loi a priori et fonction de cot .",
    "en effet , dans la minimisation de @xmath101 seul compte le produit entre fonction de cot et loi a priori .",
    "le transfert de masse entre @xmath102 et @xmath103 na donc aucun effet sur la dcision finale .",
    "plus fondamentalement , connatre les rgions de lespace des paramtres o une erreur est la plus dommageable est similaire  favoriser dans la loi a priori cette rgion .",
    "cest dailleurs une manire daboutir  la loi de jeffreys  partir dune fonction de cot intrinsque @xcite .",
    "le cas particulier des tests statistiques , si souvent utiliss en statistique classique pour dmontrer des  diffrences significatives \" , met en lumire une distinction majeure entre cette approche classique et lapproche baysienne , distinction qui les rend incompatibles sur ce plan . dans lapproche classique ( des tests ) , il sagit didentifier des vnements improbables , cest  dire des observations incompatibles avec une certaine loi de probabilit ( ou une famille de lois ) .",
    "par exemple , dans lillustration binomiale ci - dessus , on recherche les valeurs de @xmath71 les plus improbables lorsque @xmath104 .",
    "dans cette approche , lhypothse alternative y joue un rle mineur voire ngligeable , servant au mieux  dfinir la zone des statistiques extrmes .",
    "lapproche baysienne vise  une rsolution complte en modlisant les deux hypothses simultanment , permettant dune part la rsolution du problme dcisionnel du choix de lhypothse la plus probable ( choix que certains baysiens rcusent ) et dautre part la construction dune loi a posteriori sur les paramtres du modle retenu .",
    "le choix dune hypothse comme @xmath105 se fait donc _ contre _ ou relativement  une hypothse alternative comme @xmath106 et non pas dans un absolu mal dfini et vitant la modlisation des contraires ( et des dcisions  prendre en cas de rejet de lhypothse nulle ) .",
    "cette opposition entre approche classique et approche baysienne est souvent mal comprise , dune part  cause du caractre relatif associ  la seconde  o deux modlisations du phnomne sont opposes ",
    "et dautre part du caractre apparemment absolu de la validation frquentiste du taux derreur de premire espce des tests de neyman - pearson .",
    "un test de neyman - pearson au niveau 5%  a en effet une probabilit de @xmath107 de rejeter lhypothse nulle  tort .",
    "ce taux est en fait trs souvent interprt  tort comme la probabilit de lhypothse nulle , ce qui na pas de sens en dehors dune perspective baysienne .",
    "au del de lopposition philosophique , et des interprtations diffrentes des rsultats , les deux principes conduisent aussi  des oppositions claires dans les dcisions , en particulier parce que les tests classiques tendent  rejeter plus frquemment les hypothses nulles .",
    "la procdure de dcision baysienne est fonde sur la probabilit a posteriori de lhypothse nulle @xmath108 ou , de manire ( presque ) quivalente , sur le facteur de bayes @xcite @xmath109 ce dernier se compare  @xmath70 comme la probabilit a posteriori se compare  @xmath110 .",
    "dans lexemple binomial , nous avons ainsi vu une valeur de @xmath111 qui nous permet de conclure en faveur de @xmath68 .",
    "( la probabilit a posteriori serait alors @xmath112 . )",
    "dans un cadre classique , la @xmath17-value est la probabilit de dpasser @xmath78 sous lhypothse nulle , soit @xmath113 , qui savre beaucoup moins favorable  @xmath68 .",
    "lopposition mentionne ci - dessus est des plus claires dans une tude mene par @xcite : ils dmontrent que la borne infrieure des probabilits a posteriori est suprieure  la @xmath17-value ( les deux quantits sont comparables dun point de vue dcisionnel , en utilisant une fonction de cot 01 , voir @xcite , chapitre 5 ) .",
    "cela signifie que pour _ toute _ loi a priori donnant le mme poids aux deux hypothses , lhypothse nulle @xmath68 sera _ toujours _ juge plus improbable par lapproche classique .",
    "@xcite remarquent par exemple que , en simulant les donnes et paramtres suivant une loi a priori spcifique , dans 20% des cas o lhypothse nulle est rejete , elle est en fait correcte .",
    "plus explicitement encore , le paradoxe de @xcite exprime cette opposition : dans un modle normal de moyenne @xmath2 et variance connue avec @xmath13 observations , lorsque @xmath114 , pour une @xmath17-value donne , @xmath115 , sous une loi a priori normale arbitraire , la probabilit a posteriori de lhypothse nulle tend vers @xmath70 avec la taille de lchantillon @xmath13 . les interprtations de ce paradoxe abondent , allant du rejet des @xmath17-values ( car pourquoi faudrait - il garder",
    "cette @xmath17-value ou le seuil dacceptation constant ? )",
    " celui des probabilits a posteriori @xcite , voire les deux @xcite .",
    "une autre spcificit des tests baysiens est de ncessiter une attention particulire envers les lois impropres , mesures de masse infinie ne pouvant tre transformes en lois de probabilit .",
    "dun point de vue strictement mathmatique , ces lois ne peuvent tre employes car elles introduisent une constante de normalisation arbitraire dans le calcul de la loi a posteriori . cela conduit  la seconde forme du paradoxe de @xcite : tant donn une observation @xmath1 arbitraire dune loi normale , la probabilit a posteriori de lhypothse nulle @xmath114 tend vers @xmath70 quand la variance a priori sous lhypothse alternative tend vers @xmath116 .",
    "en effet , faire tendre la variance vers linfini revient  utiliser la mesure de lebesgue comme loi a priori .",
    "il existe de nombreuses rponses  ce dilemme , de la prohibition des lois impropres @xcite  diverses stratgies de validation croise ( ou non - croise ) utilisant une partie des donnes pour contruire une vritable loi ( a priori ?",
    "a posteriori ?",
    "a mediori ? ) et le reste des donnes pour effectuer le test @xcite , voire utilisant _ toutes _ les donnes pour contruire une loi et les rutilisant une seconde fois pour le test @xcite .",
    "je ne mentionnerai pas ici les alternatives sur des ",
    "critres dinformation \" comme le bic ( pour _ bayesian information criterion _ )",
    "et le dic ( pour _ deviance information criterion _ ) , qui fournissent des classements de modles ou dhypothses en opposition en dehors des cadres dcisionnel _ et _ baysien .",
    "ce bref chapitre na pas abord de nombreux points et modles o lapproche baysienne fait sens , soit pour des raisons de modlisation ( comme lmergence des modles graphiques  la fin des annes 1980 , dfinis uniquement par des lois conditionnelles , voir par exemple @xcite ) , soit pour des raisons de complexit gnrique des modles ( comme dans le cadre de modles hirarchiques rencontrs en marketing comme en slection animale , en climatologie comme en pidmiologie ) , soit encore grce  sa dmarche intgre permettant de mler description stochastique et impratifs de dcision ( comme par exemple dans la construction de plans dexprience dans @xcite ) .",
    "je nai pas mentionn non plus les nouvelles capacits de lanalyse baysienne  la frontire entre statistique et machine learning , comme le classificateur bart de @xcite , techniques qui , associes avec des mthodologies non - paramtriques aussi rapides , autorisent le traitement de grands jeux de donnes , la norme de lre du  big data \" .",
    "bien que la statistique baysienne ait principalement grandi dans un terreau philosophique et polmique , de bayes et price sopposant  hume ,  @xcite contre fisher , @xcite rejettant laplace @xcite , & tc .",
    ", il est assez paradoxal que lmergence dune composante baysienne significative dans la statistique ( ou science des donnes ? ) moderne soit surtout d  sa maniabilit face  des modles complexes et  lmergence de techniques de calcul par ordinateur finalement assez simples",
    " comprendre et implmenter pour que des disciplines utilisatrices de la statistique sen saisissent .",
    "on peut le regretter du point de vue des fondements de la discipline , mais la dissmination des ides baysiennes dans les autres sciences et au - del fournit de nouvelles perspectives et propositions",
    " mme dassurer la continuation de la discipline dans les dcennies  venir : _ from practice stems theory ! _"
  ],
  "abstract_text": [
    "<S> this book chapter ( written in french ) is a review of the foundations of the bayesian approach to statistical inference , relating to its historical roots and some philosophical arguments , as well as a short presentation of its practical implementation .    </S>",
    "<S> ce chapitre dun ouvrage de philosophie des sciences portant sur les mthodes baysiennes ,  paratre aux ditions matriologiques , vise  donner les fondements de lapproche baysienne en statistique infrentielle , ses racines historiques et ses justifications philosophiques , ainsi qu prsenter des illustrations de sa mise en uvre pratique . </S>"
  ]
}