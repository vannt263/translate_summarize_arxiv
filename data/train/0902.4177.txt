{
  "article_text": [
    "network coding was introduced in @xcite as a means to improve the rate of transmission in networks , and often achieve capacity in the case of single source networks .",
    "linear network coding was introduced in @xcite .",
    "network - error correction , which involved a trade - off between the rate of transmission and the number of correctable network - edge errors , was introduced in @xcite as an extension of classical error correction to a general network setting .",
    "along with subsequent works @xcite and @xcite , this generalized the classical notions of the hamming weight , hamming distance , minimum distance and various classical error control coding bounds to their network counterparts . an algebraic formulation of network coding was discussed in @xcite for both instantaneous networks and networks with delays . in all of these works , it is assumed that the sinks and the source know the network topology and the network code , which is referred to as _ coherent network coding_.    random network coding , introduced in @xcite presented a distributed network coding scheme where nodes independently chose random coefficients ( from a finite field ) for the linear mixing of their inputs .",
    "subspace codes and rank metric codes were constructed for the setting of random network coding in @xcite and @xcite .",
    "convolutional network codes were discussed in @xcite and a connection between network coding and convolutional coding was analyzed in @xcite . in this work ,",
    "convolutional coding is introduced to achieve network - error correction .",
    "we assume an acyclic , single source , instantaneous ( delay - free ) network with coherent linear network coding to multicast information to several sinks .",
    "we define _ a network use _ as a single usage of all the edges of the network to multicast utmost min - cut number of symbols to each of the sinks .",
    "an _ error pattern _ is a subset of the set of edges of the network which are in error .",
    "it is seen that when the source implements a convolutional code to send information into the network , every sink sees a different convolutional code .",
    "we address the following problem .",
    "_ given an acyclic , delay - free , single - source network with a linear multicast network code , and a set of error patterns @xmath0 , how to design a convolutional code at the source which shall correct the errors corresponding to the error patterns in @xmath0 , as long as consecutive errors are separated by a certain number of network uses ?",
    "_    the main contributions of this paper are as follows .    * for networks with a specified network code , convolutional codes have been employed to achieve network - error correction for the first time . * an explicit convolutional code construction ( for the network with a given network code ) that corrects a given pattern of network - errors ( provided that the occurrence of consecutive errors are separated by certain number of network uses ) is given . * the convolutional codes constructed in this paper are found to offer certain advantages in field size and decoding over the previous approaches of block network - error correction codes ( bneccs ) of @xcite for network error correction . *",
    "some bounds are derived on the minimum field size required , and on the minimum number of network uses that two error events must be separated by in order that they get corrected .",
    "the rest of the paper is organized as follows .",
    "section [ sec2 ] gives a primer on convolutional codes and mds convolutional codes . in section [ sec3 ]",
    ", we discuss the general network coding set - up and network - errors . in section [ sec4 ] , we give a construction for a input convolutional code which shall correct errors corresponding to a given set of error patterns . in section [ sec5 ] ,",
    "we give some examples for this construction . in section [ sec5.5 ]",
    ", we compare the advantages and disadvantages of our network error correcting codes with that of @xcite .",
    "finally , a short discussion on the construction of section [ sec4 ] constitutes section [ sec6 ] along with several directions for further research .",
    "in this section , we review the basic concepts related to convolutional codes , used extensively throughout the rest of the paper . for @xmath1 power of a prime ,",
    "let @xmath2 denote the finite field with @xmath3 elements .",
    "for a convolutional code , the _ information sequence _",
    "@xmath4(\\boldsymbol{u}_i\\in\\mathbb{f}_q^b)$ ] and the _ codeword sequence _ ( output sequence ) @xmath5\\left(\\boldsymbol{v}_i\\in\\mathbb{f}_q^c\\right)$ ] can be represented in terms of the delay parameter @xmath6 as @xmath7    a _ convolutional code _ , @xmath8 of rate @xmath9 is defined as @xmath10\\text { } |\\text { } \\boldsymbol{v}(z)=\\boldsymbol{u}(z)g(z ) \\right\\}\\ ] ] where @xmath11 is a @xmath12 _ generator matrix _ with entries from @xmath13(the field of rationals functions over @xmath2 ) and rank @xmath14 over @xmath13 , and @xmath15 being the code sequence arising from the information sequence , @xmath16 $ ] , the set of all @xmath14-tuples with elements from the formal power series ring @xmath17 $ ] over @xmath18    two generator matrices are said to be _ equivalent _ if they encode the same convolutional code . a _ _ polynomial generator matrix__@xcite for a convolutional code @xmath19 is a generator matrix for @xmath19 with all its entries from @xmath20 $ ] , the ring of polynomials over @xmath18 it is known that every convolutional code has a polynomial generator matrix @xcite . also , a generator matrix for a convolutional code is _ _",
    "catastrophic__@xcite if there exists an information sequence with infinitely many non - zero components , that results in a codeword with only finitely many non - zero components . for a polynomial generator matrix @xmath11 ,",
    "let @xmath21 be the element of @xmath11 in the @xmath22 row and the @xmath23 column , and @xmath24 be the @xmath22 _ row degree _ of @xmath11 .",
    "let @xmath25 be the _ degree _ of @xmath26    a polynomial generator matrix is called _ basic _ if it has a polynomial right inverse .",
    "it is called _ minimal _ if its degree @xmath27 is minimum among all generator matrices of @xmath19 .",
    "forney in @xcite showed that the ordered set @xmath28 of row degrees ( indices ) is the same for all minimal basic generator matrices of @xmath19 ( which are all equivalent to one another ) .",
    "therefore the ordered row degrees and the degree @xmath27 can be defined for a convolutional code @xmath29 a rate @xmath30 convolutional code with degree @xmath27 will henceforth be referred to as a @xmath31 code . also , any minimal basic generator matrix for a convolutional code is non - catastrophic .",
    "a _ convolutional encoder _ is a physical realization of a generator matrix by a linear sequential circuit .",
    "two encoders are said to be _ equivalent encoders _ if they encode the same code .",
    "minimal encoder _ is an encoder with minimal delay elements among all equivalent encoders .",
    "the _ free distance _ of the convolutional code @xmath19 is given as @xmath32 where @xmath33 indicates the hamming weight over @xmath18      in this subsection , we discuss some results on the existence and construction of maximum distance separable ( mds ) convolutional codes . in subsection",
    "[ sec4e ] , we use these results to obtain some bounds on the field size and the error correcting capabilities of such mds convolutional codes when they are used for network - error correction .",
    "the following bound on the free distance , and the existence of codes meeting the bound , called mds convolutional codes , was proved in @xcite .",
    "[ gensingbound ] for every base field @xmath34 and every rate @xmath35 convolutional code @xmath19 of degree @xmath27 , the free distance is bounded as @xmath36    theorem [ gensingbound ] is known as the _",
    "generalized singleton bound_.    for any positive integers @xmath37 , @xmath27 and for any prime @xmath38 there exists a field @xmath2 of characteristic @xmath38 , and a rate @xmath35 convolutional code @xmath19 of degree @xmath27 over @xmath2 , whose free distance meets the generalized singleton bound .",
    "a method of constructing mds convolutional codes based on the connection between quasi - cyclic codes and convolutional codes was given in @xcite .",
    "it is known @xcite that the field size @xmath3 required for a @xmath39 mds convolutional code @xmath8 in the construction in @xcite should be a prime power such that @xmath40",
    "we consider only acyclic networks in this paper the model for which is as in @xcite .",
    "an acyclic network can be represented as a acyclic directed multi - graph @xmath41 = ( @xmath42 ) where @xmath43 is the set of all vertices and @xmath44 is the set of all edges in the network .",
    "we assume that every edge in the directed multi - graph representing the network has unit _ capacity _ ( can carry utmost one symbol from @xmath2 ) . network links with capacities greater than unit are modeled as parallel edges .",
    "the network is assumed to be instantaneous , i.e , all nodes process the same _ generation _ ( the set of symbols generated at the source at a particular time instant ) of input symbols to the network in a given coding order ( ancestral order @xcite ) .",
    "let @xmath45 be the source node and @xmath46 be the set of all receivers .",
    "let @xmath47 be the unicast capacity for a sink node @xmath48 i.e the maximum number of edge - disjoint paths from @xmath49 to @xmath50 .",
    "then @xmath51 is the max - flow min - cut capacity of the multicast connection .",
    "we follow @xcite in describing the network code . for each node @xmath52 , let the set of all incoming edges be denoted by @xmath53 .",
    "then @xmath54 is the in - degree of @xmath55 .",
    "similarly the set of all outgoing edges is defined by @xmath56 , and the out - degree of the node @xmath55 is given by @xmath57 . for",
    "any @xmath58 and @xmath59 , let @xmath60 , if @xmath55 is such that @xmath61 .",
    "similarly , let @xmath62 , if @xmath55 is such that @xmath63 .",
    "we will assume an ancestral ordering on @xmath64 of the acyclic graph @xmath41 .",
    "the network code can be defined by the local kernel matrices of size @xmath65 for each node @xmath52 with entries from @xmath2 .",
    "the global encoding kernels for each edge can be recursively calculated from these local kernels .",
    "the network transfer matrix , which governs the input - output relationship in the network , is defined as given in @xcite . towards this end ,",
    "the matrices @xmath66,@xmath67,and @xmath68(for every sink @xmath69 are defined as follows : the entries of the @xmath70 matrix @xmath66 are defined as @xmath71 where @xmath72 is the local encoding kernel coefficient at the source coupling input @xmath73 with edge @xmath74 .",
    "the entries of the @xmath75 matrix @xmath67 are defined as @xmath76 where the set of @xmath77 is the local encoding kernel coefficient between @xmath78 and @xmath79 , at the node @xmath80.for every sink @xmath81 , the entries of the @xmath82 matrix @xmath68 are defined as @xmath83 where all @xmath84 .    for instantaneous networks , we have @xmath85 where @xmath86 is the @xmath75 identity matrix .",
    "now we have the following :    [ nettransfermatrix ] _ the network transfer matrix _",
    ", @xmath87 , corresponding to a sink node @xmath88 is a full rank @xmath89 matrix defined as @xmath90    definition [ nettransfermatrix ] implies that if @xmath91 is the input to the instantaneous network at any particular instant , then at any particular sink @xmath92 , we have the output , @xmath93 , at the same instant , to be @xmath94 .      assuming that a @xmath95-dimensional linear network code multicast has been implemented in the network , we define the following terms-    an _ input convolutional code _",
    ", @xmath96 is a convolutional code of rate @xmath97 with a _ input generator matrix _",
    "@xmath98 implemented at the source of the network .    the _ output convolutional code _",
    "@xmath99 , corresponding to a sink node @xmath88 is the @xmath97 convolutional code generated by the _ output generator matrix _ @xmath100 which is given as @xmath101 , with @xmath102 being the full rank network transfer matrix corresponding to a @xmath95-dimensional network code .    [ exm1 ]",
    "combination network over a ternary field .",
    "the global kernels of the edges coming from the source are indicated .",
    "all the intermediate nodes have local kernels unity.,width=288 ]    consider the @xmath103 combination @xmath104 network as shown in fig .",
    "[ fig:4c2network ] . for this network ,",
    "let the input convolutional code over @xmath105 $ ] be generated by @xmath106.$ ] the network transfer matrices at each sink and their corresponding output convolutional matrices are calculated and tabulated in table [ tab1 ] .    .@xmath107",
    "@xmath104 network for the input convolutional code @xmath106.$ ] [ cols=\"^,^ , < \" , ]     [ tab2 ]",
    "the approach of @xcite can also be used to obtain network error correcting codes that correct @xmath108 edge errors once in every @xmath109 network uses ( for some positive integer @xmath109 ) . a time - expanded graph",
    "would then be used , i.e , with the network nodes ( those except the source and sinks ) and edges replicated for each additional time instant .",
    "suppose the network has been replicated @xmath109 times .",
    "then the algorithm in @xcite can be employed to obtain a @xmath108-error correcting bnecc for the time - expanded network , which equivalently for the original network gives a network error correcting code that corrects @xmath108 errors once in every @xmath109 network uses .",
    "it is noted that the sufficient field size @xmath3 required by the technique of @xcite to construct a @xmath108-error correcting bnecc for the time - expanded graph ( @xmath46 being the set of all sinks ) is such that @xmath110    our approach demands a field size according to theorem [ fieldsizebound ] , which is independent of the number of edges in the network .",
    "although the error correcting capability might not be comparable to that offered by the bnecc , the reduction in field size is a considerable advantage in terms of the computation to be performed at each coding node of the network .",
    "also , the use of convolutional codes permits decoding using the viterbi decoder , which is readily available .",
    "for example , one could design network error correcting codes according to @xcite for the butterfly network by using the twice replicated butterfly network as shown in fig . [",
    "fig : butterflytwice ] .",
    "the time - expanded network has min - cut 4 , and thus the technique in @xcite can be used to obtain bneccs , which correct single or double edge errors in the butterfly network once in @xmath111 network uses . in either case , the sufficient field size @xmath3 is such that @xmath112 , although by trial and error a code could be found over a smaller field size . on the other hand ,",
    "the convolutional code that we used here in our paper for the butterfly network is over the binary and ternary fields .",
    "in the construction of subsection [ construction ] , the maximum hamming weight @xmath113 of the vectors in the set @xmath114 , is such that @xmath115 clearly the actual value of @xmath113 is governed by the network code and hence the network code influences the choice of the network - error correcting convolutional code .",
    "therefore the network code designed should be such that @xmath113 is minimal , so that the free distance demanded of the network - error correcting convolutional code in the construction of subsection [ construction ] is minimal .",
    "also , for a particular error pattern set , the decoding procedure at the sinks ( case - a or case - b of decoding as in subsection [ decoding ] ) is influenced by the field size , the network code and the network - error correcting convolutional code chosen .",
    "the examples given in section [ sec5 ] illustrate the construction of subsection [ construction ] and also compare the effects of change in field size and the convolutional code chosen to correct errors corresponding to a given fixed error pattern set .",
    "this work was supported partly by the drdo - iisc program on advanced research in mathematical engineering through a research grant to b.  s.  rajan .",
    "ho , t. medard , m. koetter , r. karger , d.r .",
    "effros , m. jun shi and leong , b. , `` a random linear network coding approach to multicast '' , ieee transactions on information theory , vol .",
    "2006 , pp .",
    "4413 - 4430 ."
  ],
  "abstract_text": [
    "<S> in this work , we introduce convolutional codes for network - error correction in the context of coherent network coding . we give a construction of convolutional codes that correct a given set of error patterns , as long as consecutive errors are separated by a certain interval . </S>",
    "<S> we also give some bounds on the field size and the number of errors that can get corrected in a certain interval . compared to previous network error correction schemes , using convolutional codes </S>",
    "<S> is seen to have advantages in field size and decoding technique . </S>",
    "<S> some examples are discussed which illustrate the several possible situations that arise in this context . </S>"
  ]
}