{
  "article_text": [
    "in order to function and survive in the world , cells must make decisions about the reading out or `` expression '' of genetic information .",
    "this happens when a bacterium makes more or less of an enzyme to exploit the variations in the availability of a particular type of sugar , and when individual cells in a multicellular organism commit to particular fates during the course of embryonic development . in all such cases ,",
    "the control of gene expression involves the transmission of information from some input signal to the output levels of the proteins encoded by the regulated genes .",
    "although the notion of information transmission in these systems usually is left informal , the regulatory power that the system can achieve  the number of reliably distinguishable output states that can be accessed by varying the inputs  is measured , logarithmically , by the actual information transmitted , in bits @xcite . since",
    "relevant molecules often are present at relatively low concentrations , or even small absolute numbers , there are irreducible physical sources of noise that will limit the capacity for information transmission .",
    "cells thus face a tradeoff between regulatory power ( in bits ) and resources ( in molecule numbers ) .",
    "what can cells do to maximize their regulatory power at fixed expenditure of resources ?",
    "more precisely , what can they do to maximize information transmission with bounded concentrations of the relevant molecules ?",
    "we focus on the case of transcriptional regulation , where proteins  called transcription factors ( tfs)bind to sites along the dna and modulate the rate at which nearby genes are transcribed into messenger rna . because many of the regulated genes themselves code for tf proteins , regulatory interactions form a network .",
    "the general problem of optimizing information flow through such regulatory networks is quite hard , and we have tried to break this problem into manageable pieces . given the signal and noise characteristics of the regulatory interactions , cells can try to match the distribution of input transcription factor concentrations to these features of the regulatory network ; even simple versions of this matching problem make experimentally testable predictions @xcite . assuming that this matching occurs",
    ", some regulatory networks still have more capacity to transmit information , and we can search for these optimal networks by varying both the topology of the network connections and the strengths of the interactions along each link in the network ( the `` numbers on the arrows '' @xcite ) . we have addressed this problem first in simple networks where a single input transcription factor regulates multiple non  interacting genes @xcite , and then in interacting networks where the interactions have a feedforward structure @xcite .",
    "but real genetic regulatory networks have loops , and our goal here is to study the simplest such case , where a single input transcription factor controls a single self  interacting gene .",
    "does feedback increase the capacity of this system to transmit information ?",
    "are self  activating or self  repressing genes more informative ?",
    "since networks with feedback can exhibit multistability or oscillation , and hence a nontrivial phase diagram as a function of the underlying parameters , where in this phase diagram do we find the optimal networks ?",
    "auto  regulation , both positive and negative , is one of the simplest and most commonly observed motifs in genetic regulatory networks @xcite , and has been the focus of a number of experiments and modeling studies ( see , for example , refs @xcite ) .",
    "a number of proposals have been advanced to explain its ubiquitous presence .",
    "negative feedback ( self  repression ) can speed up the response of the genetic regulatory element @xcite , and can reduce the steady state fluctuations in the output gene expression levels @xcite .",
    "positive feedback ( self  activation ) , on the other hand , slows down the dynamics of gene expression and sharpens the response of a regulated gene to its external input .",
    "activating genes could thus threshold graded inputs , transforming them into discrete , almost `` digital '' outputs @xcite , allowing the cell to implement binary logical functions @xcite .",
    "if self  activation is very strong , it can lead to multistability , or switch  like behavior of the response , so that the genetic regulatory element can store the information for long periods of time @xcite ; such elements will also exhibit hysteretic effects .",
    "weak self  activation , which does not cause multistability , has been studied less extensively , but could play a role in allowing the cell to implement a richer set of input / output relations @xcite . alternatively ,",
    "if the self  activating gene product can diffuse into neighboring nuclei of a multicellular organism , the sharpening effect of self  activation can compensate for the `` blurring '' of responses due to diffusion and hence open more possibilities for noise reduction through spatial averaging @xcite .",
    "many of the ideas about the functional role of auto  regulation are driven by considerations of noise reduction .",
    "the physical processes by which the regulatory molecules find and bind to their regulatory sites on the dna , the operation of the transcriptional machinery , itself subject to thermal fluctuations , and the unavoidable shot noise inherent in producing a small number of output proteins all contribute towards the stochastic nature of gene expression and thus place physical limits on the reliability of biological computation @xcite . in the past decade",
    "the advance of experimental techniques has enabled us to measure the total noise in gene expression and sometimes parse apart the contributions of various molecular processes towards this `` grand total '' @xcite . with the detailed knowledge about noise in gene expression we can revisit the original question and ask : can both forms of auto - regulation help mitigate the deleterious effects of noise on information flow through the regulatory networks and if so , how ?    all of our previous work in information transmission in transcriptional regulation has been in the steady state limit .",
    "a similar approach was taken in ref @xcite , where the authors analyze information flow in elementary circuits including feedback , but with different model assumptions about network topology and noise .",
    "more recently , de ronde and colleagues have systematically reexamined the role of feedback regulation on the fidelity of signal transmission for time varying , gaussian signals in cases where the ( nonlinear ) behavior of the genetic regulatory element can be linearized around some operating point @xcite .",
    "they found that auto ",
    "activation increases gain  to  noise ratios for low frequency signals , whereas auto ",
    "repression yields an improvement for high frequency signals .",
    "while many of the functions of feedback involve dynamics , as far as we know all analyses of information transmission with dynamical signals resort to linear approximations . here",
    "we return to the steady state limit , where we can treat gene regulatory elements as fully nonlinear devices . while we hope that our analysis of the self  regulated gene is interesting in itself",
    ", we emphasize that our goal is to build intuition for the analysis of more general networks with feedback .",
    "figure [ f1 ] shows a schematic of the system that we will analyze in this paper , a gene @xmath0 that is controlled by two regulators : directly by an external transcription factor , as well as in a feedback fashion by its own gene products .",
    "we will refer to the transcription factor as the regulatory _ input _ ; its concentration in the relevant ( cellular or nuclear ) volume @xmath1 will be denoted by @xmath2 . in addition , the gene products of @xmath0 , whose number in the relevant volume @xmath1 we denote by @xmath3 and to which we refer to as the _ output _ , can also bind to the regulatory region of @xmath0 , thereby activating or repressing the gene s expression . as we attempt to make our description of this system mathematically precise , the heart of our model will be the _ regulatory function _ that maps the concentrations of the two regulators at the promotor region of @xmath0 to the rate at which output molecules are synthesized .",
    "is depicted by a thick black line and a promoter start signal .",
    "gene products @xmath4 denoted as blue circles can bind to the regulatory sites ( one in this example ) that control the expression of @xmath0 .",
    "direct control over the expression of @xmath0 is exerted by molecules of the transcription factor @xmath2 ( green diamonds , two binding sites ) . ]",
    "we can write the equation for the dynamics of gene expression from @xmath0 by assuming that synthesis and degradation of the gene products are single kinetic steps , in which case we have @xmath5 here , @xmath6 is the maximum rate for production of @xmath3 , @xmath7 is the protein degradation time , and @xmath8 is the concentration of the output molecules in the relevant volume @xmath1 . to include the noise effects inherent in creating and degrading single molecules of @xmath3 we introduce the langevin force @xmath9 , and",
    "we will discuss the nature of this and other noise sources in detail later .",
    "importantly , departures from our simplifying assumptions about the kinetics can , in part , be captured by proper treatment of the noise terms , as discussed below .",
    "we are interested in the information that the steady state output of @xmath0 provides about the input concentration @xmath2 .",
    "following our previous work @xcite , we address this problem in stages .",
    "first we relate information transmission to the response properties and noise in the regulatory element , using a small noise approximation to allow analytic progress ( section [ infsecta ] ) .",
    "then we show how the relevant noise variances can be computed from the model in eq ( [ eqd ] ) , taking advantage of our understanding of the physics underlying the essential sources of noise ( section [ insectb ] ) ; this discussion is still quite general , independent of the details of the regulation function .",
    "then we explain our choice of the regulation function , adapted from the monod ",
    "changeux description of allosteric interactions ( section [ mwc ] ) .",
    "because feedback allows for bifurcations , we have to map the phase diagram of our model ( section [ phasediag ] ) , and develop approximations for the information transmission near the critical point ( section [ critinfo ] ) and in the bistable regime ( section [ infobi ] ) .",
    "our discussion reviews some earlier results , in the interest of being self  contained , but the issues in sections [ phasediag][infobi ] are all new to the case of networks with feedback .",
    "we are interested in computing the mutual information between the input and the output of a regulatory element , in steady state .",
    "we have agreed that the input signal is the concentration @xmath2 of the transcription factor , and we will take the output to be the concentration @xmath4 of the gene products , which we colloquially call the expression level of the gene @xmath0 .",
    "an important feature of the information transmission is that its mathematical definition is independent of the units that we use in measuring these concentrations , so when we later choose some natural set of units we wo nt have to worry about substituting into the formulae we derive here .    following shannon @xcite ,",
    "the mutual information between @xmath2 and @xmath4 is defined by @xmath10 \\ , { \\rm bits } , \\label{isym}\\ ] ] where input concentrations @xmath2 are drawn from the distribution @xmath11 , the output expression levels that we can observe are drawn from the distribution @xmath12 , and the joint distribution of these two quantities is @xmath13 .",
    "we think of the expression level as responding to the inputs , but this response will be noisy , so given the input @xmath2 there is a conditional distribution @xmath14 . then the symmetric expression for the mutual information in eq ( [ isym ] ) can be rewritten as a difference of entropies , @xmath15 - \\int dc\\ ; p_{\\rm in}(c )   s[p(g|c ) ] , \\label{info_ent}\\ ] ] where the entropy of a distribution is defined , as usual , by @xmath16 = -\\int dx\\ , p(x ) \\log_2 p(x ) .\\ ] ] finally , we recall that @xmath17 notice that the mutual information is a functional of two probability distributions , @xmath11 and @xmath14 .",
    "the latter distribution describes the response and noise characteristics of the regulatory element , and is something we will be able to calculate from eq ( [ eqd ] ) .",
    "following refs  @xcite , we may then ask : given that @xmath14 is determined by the biophysical properties of the genetic regulatory element , what is the optimal choice of @xmath11 that will maximize the mutual information @xmath18 ? to this end we have to solve the problem of extremizing @xmath19 = i(c;g ) - \\lambda\\int dc\\ ; p_{\\rm in } ( c ) , \\label{extremal}\\ ] ] where the lagrange multiplier @xmath20 enforces the normalization of @xmath11 .",
    "other `` cost '' terms are possible , such as adding a term proportional to @xmath21 , which would penalize the average cost of input molecules @xmath2 , although here we take the simpler approach of fixing the maximum possible value of @xmath2 , which is almost equivalent @xcite . if the noise were truly zero ,",
    "we could write the distribution of outputs as @xmath22 , \\ ] ] where @xmath23 is the average output as a function of the input , i.e. the mean of the distribution @xmath14 .",
    "then if the function @xmath23 is invertible , we can write the entropy of the output distribution as @xmath24 & \\equiv & -\\int dg\\ , p_{\\rm out}(g)\\log_2 p_{\\rm out}(g)\\nonumber\\\\ & \\rightarrow & -\\int dc\\ , p_{\\rm in}(c ) \\log_2\\left [ p_{\\rm in}(c ) { \\bigg | } { { d\\bar g ( c)}\\over { dc}}{\\bigg |^{-1}}\\right ] , \\end{aligned}\\ ] ] and we can think of this as the first term in an expansion in powers of the noise level @xcite . keeping only this leading term , we have    @xmath25 = -\\int dc\\ , p_{\\rm in}(c ) \\log_2\\left [ p_{\\rm in}(c ) { \\bigg | } { { d\\bar g ( c)}\\over { dc}}{\\bigg |^{-1 } } \\right ]   -   \\int dc\\ , p_{\\rm in}(c )   s[p(g|c ) ] - \\lambda\\int dc\\ ; p_{\\rm in } ( c ) , \\label{pointback}\\ ] ]    and one can then show that the extremum of @xmath26 occurs at @xmath27}\\left| \\frac{d\\bar{g}(c)}{dc}\\right| , \\label{gensol}\\ ] ] where the entropy is measured in bits , as above , and the normalization constant @xmath28 } .\\ ] ] the maximal value of the mutual information is then simply @xmath29 .",
    "in the case where @xmath14 is gaussian , @xmath30 , \\ ] ] the entropy is determined only by the variance @xmath31 , @xmath32={1\\over 2 } \\log_2\\left [ 2\\pi e \\sigma_g^2(c)\\right ] .\\ ] ] it is useful to think about propagating this ( output ) noise variance back through the input / output relation @xmath23 , to define the effective noise at the input , @xmath33 then we can write @xmath34 as before , @xmath35 is the normalization constant , @xmath36^{1/2 } , \\label{eq2}\\ ] ] where @xmath37 is the maximal value of the input concentration , and again we have the information @xmath38 .      equation ( [ eq2 ] ) relates @xmath35 , and hence the information transmission @xmath29 , to the steady state response and noise in our simple regulatory element .",
    "these quantities are calculable from the dynamical model in eq ( [ eqd ] ) , if we understand the sources of noise .",
    "there are two very different kinds of noise that we need to include in our analysis .",
    "first , we are describing molecular events that synthesize and degrade individual molecules , and individual molecules behave randomly .",
    "if we say that there is synthesis of @xmath39 molecules per second on average , then if the synthesis is limited by a single kinetic step , and if all molecules behave independently , then the actual rate @xmath40 will fluctuate with a correlation function @xmath41 . similarly ,",
    "if on average there is degradation of @xmath42 molecules per second , then the actual degradation rate @xmath43 will fluctuate with @xmath44 . thus if we want to describe the time dependence of the number of molecules @xmath45 , we can write @xmath46 where @xmath47 if we are close to the steady state , @xmath48 , and if synthesis and degradation reactions are independent , we have @xmath49 if some of the reactions involve multiple kinetic steps , or if the molecules we are counting are amplified copies of some other molecules , then the noise will be proportionally larger or smaller , and we can take account of this by introducing a `` fano factor '' @xmath50 , so that @xmath51 for more about the langevin description of noise in chemical kinetics , see ref @xcite .",
    "the second irreducible source of noise is that the synthesis reactions are regulated by transcription factor binding to dna , and these molecules arrive randomly at their targets .",
    "one way to think about this is that the concentrations of tfs which govern the synthesis rate are not the bulk average concentrations over the whole cell or nucleus , but rather concentrations in some small `` sensitive volume '' determined by the linear size @xmath52 of the targets themselves @xcite . concretely ,",
    "if we write the synthesis rate as @xmath53 where @xmath54 is the local concentration of the input transcription factor and @xmath8 is the concentration of the gene product that feeds back to regulate itself , we should really think of these concentrations as @xmath55 and @xmath56 , where we separate the mean values and the local fluctuations ; note that the mean gene product concentration is the ratio of the molecule number @xmath3 to the relevant volume @xmath1 .",
    "the local concentration fluctuations are also white , and the spectral densities are given accurately by dimensional analysis @xcite , so that @xmath57 where @xmath58 is the diffusion constant of the transcription factor molecules , which we assume is the same for the input and output proteins .",
    "we can put all of these factors together if the noise is small , so that it drives fluctuations which stay in the linear regime of the dynamics .",
    "then if the steady state solution to eq ( [ eqd ] ) in the absence of noise is denoted by @xmath59 , we can linearize in the fluctuations @xmath60 :    @xmath61 \\delta g + \\xi_{\\rm eff}(t ) , \\,{\\rm where } \\label{ll2}\\\\ \\langle \\xi_{\\rm eff}(t ) \\xi_{\\rm eff}(t')\\rangle & = & 2 \\left [ \\nu \\frac{\\bar g}{\\tau } +   \\left ( r_{\\rm max } { { \\partial f(c,\\gamma)}\\over{\\partial \\gamma}}{\\bigg |}_{\\gamma = \\bar g /\\omega}\\right)^2",
    "\\frac{2\\bar g}{\\omega d \\ell } +   \\left ( r_{\\rm max } { { \\partial f(c,\\bar g /\\omega)}\\over{\\partial c}}\\right)^2 \\frac{2c } { d \\ell } \\right ] \\delta(t - t ' ) \\label{ll3}.\\end{aligned}\\ ] ]    to solve this problem and compute the variance in the output number of molecules @xmath62 , it is useful to recall the langevin equation for the position @xmath63 of an overdamped mass tethered by a spring of stiffness @xmath64 , subject to a drag force proportional to the velocity , @xmath65 : @xmath66 from equipartition we know that these dynamics predict the variance @xmath67 . identifying terms with our langevin description of the synthesis and degradation reactions , we find @xmath68   , \\ ] ] where we understand that the partial derivatives of @xmath69 are to be evaluated at the steady state @xmath70 .",
    "we have defined @xmath6 as the maximum synthesis rate , so that the regulation function @xmath69 is in the range @xmath71 , and hence the maximum mean expression level is @xmath72 .",
    "thus it makes sense to work with a normalized expression level @xmath73 , and to think of the regulation function @xmath69 as depending on @xmath4 rather than on the absolute concentration @xmath8 .",
    "then we have @xmath74   , \\ ] ] where @xmath75 is the maximal mean concentration of output molecules .",
    "as discussed previously @xcite , we can think of @xmath76 as the maximum number of independent output molecules , and this combines with the other parameters in the problem to define a natural concentration scale , @xmath77 . once we choose units where @xmath78 , we have a simpler expression , @xmath79   , \\ ] ] where we notice that almost all the parameters have been eliminated by our choice of units .",
    "finally , we need to use the variance to compute the information capacity of the system , @xmath29 , where from eq ( [ eq2 ] ) we have @xmath80^{1/2 }   =   \\left [ \\frac{n_g}{2\\pi e}\\right]^{1/2}\\tilde z,\\\\ \\tilde z & = &   \\int_0^cdc\\;\\left [ \\left(\\frac{d\\bar{g}}{dc}\\right)^2 \\frac{1   -   ( \\partial f/\\partial g ) } { \\bar{g }   +   (    \\partial f/\\partial g ) ^2    ( \\bar{g}/\\gamma_{\\rm max } ) +   ( \\partial f/ \\partial c ) ^2   c    } \\right]^{1/2 } ; \\end{aligned}\\ ] ]    @xmath37 is the maximum concentration of input transcription factor molecules , in units of @xmath81 .",
    "notice that the parameter @xmath82 just scales the noise and ( in the small noise approximation ) thus adds to the information , @xmath83 ; the problem of optimizing information transmission thus is the problem of optimizing @xmath84 .",
    "further , because @xmath85 the total derivative @xmath86 can be expressed though @xmath87 putting all of these pieces together , we find @xmath88 in what follows we will start with the assumption that , since both input and output molecules are transcription factor proteins , their maximal concentrations are the same , and hence @xmath89 ; we will return to this assumption at the end of our discussion .    if a regulatory function @xmath90 is chosen from some parametric family , eq  ( [ cap1 ] ) allows us to compute the information transmission as a function of these parameters and search for an optimum . before embarking on this path",
    ", however , we note that the integrand of @xmath84 can have a divergence if @xmath91 .",
    "this is a condition for the existence of a _",
    "critical point _ , and in this simple system the critical point or bifurcation separates the regime of monostability from the regime of bistability .",
    "we expect that at this point the fluctuations around @xmath92 are no longer gaussian , and we need to compute higher order moments .",
    "thus , eq ( [ cap1 ] ) , as is , can safely be used only in the monostable regime away from the critical point ; in section [ critinfo ] we compute the expression for the mutual information near to and at the critical point for a particular choice of @xmath69 .",
    "there are even more problems in the bistable regime , since there are multiple solutions to eq ( [ ss ] ) , and in section [ infobi ] we discuss information in the bistable regime .      to continue",
    ", we must choose a regulatory function . in ref  @xcite , where we analyzed genetic networks with feedforward interactions , we studied hill  type regulation @xcite and monod  wyman  changeaux  like ( mwc ) regulation @xcite , and found that the mwc family encompasses a broader set of functions than the hill family ; for a related discussion see ref @xcite .",
    "mwc functions also allow for a natural introduction of convergent control , where a node in a network is simultaneously regulated by several types of regulatory molecules . briefly",
    ", in the mwc model one assumes that the molecule or supermolecular complex being considered has two states , which we identify here with on and off states of the promoter .",
    "the binding of each different regulatory factor is always independent , but the binding energies depend on whether the complex is in an off or on state , so that ( by detailed balance ) binding shifts the equilibrium between these two states .    in our case",
    ", we have two regulatory molecules , the input transcription factor with concentration @xmath2 and the gene product with concentration @xmath4 .",
    "if there are , respectively , @xmath93 and @xmath94 identical binding sites for these molecules then the probability of being in the on state is    @xmath95    where @xmath96 are the binding constants in the on state , and similarly @xmath97 are the binding constants in the off state ; @xmath98 reflects the `` bare '' free energy difference between the two states .",
    "if the binding of the regulatory molecules has a strong activating effect , then we expect @xmath99 , and similarly for @xmath100 , which means that only one binding constant is relevant for each molecule , and we will refer to these as @xmath101 and @xmath102",
    ". then we can write @xmath103 where @xmath104 is the input concentration at which @xmath105 .",
    "notice that if binding of @xmath4 strongly represses the gene , then we have @xmath106 , but this can be simulated by changing the sign of @xmath94 .",
    "thus we should think of the parameters @xmath93 and @xmath94 as being not just the number of binding sites , but also an index of activation vs. repression .",
    "we will also treat these parameters as continuous , which is a bit counterintuitive but allows us to describe , approximately , situations in which the multiple binding sites are inequivalent , or in which @xmath107 and @xmath108 are not infinitely different .    from the discussion in the previous section",
    ", we will need to evaluate the partial derivatives of @xmath90 with respect to it arguments .",
    "for the mwc model , these derivatives take simple forms : @xmath109      let us start by examining the stability properties of eq ( [ ss ] ) , which determines the steady state @xmath110 . viewed as a function of @xmath4 , @xmath90 is sigmoidal , and so if we try to solve @xmath111 graphically we are looking for the intersection of a sigmoid with the diagonal , as a function of @xmath4 . in doing this",
    "we expect that , for some values of the parameters , there will be exactly one solution , but that as we change parameters ( or the input @xmath2 ) , there will be a transition to multiple solutions .",
    "this transition happens when @xmath69 just touches the diagonal , that is , when for some @xmath112 it holds true that @xmath113 and @xmath114 . using eq ( [ g_derivative ] ) , these two conditions can be combined to yield an equation for @xmath112 : @xmath115 this is a quadratic in @xmath112 for which no real solution on @xmath116 $ ] exists if either @xmath117 or @xmath118 .",
    "when either of these conditions are fulfilled , the gene is in the monostable regime . at the critical point , @xmath119 and @xmath120",
    "this is illustrated in fig  [ f - bistableregion ] , as a function of the effective input @xmath121 , where , from eq ( [ f_def ] ) , @xmath122 .    for the special case of @xmath123",
    "it is not hard to compute the analytical approximations for the boundary of the bistable domain .",
    "first , eq  ( [ mwc ] ) can be expanded for large @xmath102 , yielding a quadratic equation for @xmath92 that has two solutions only when @xmath124 to get the lower bound , we expand eq  ( [ mwc ] ) for small @xmath92 and retain terms up to the quadratic order in @xmath92 ; the resulting quadratic equation yields two solutions only if @xmath125 both approximations are plotted as circles and crosses , respectively , in fig  [ f - bistableregion ] , and match the exact curves well . for other values of @xmath94 we solve eq  ( [ ss ] ) exactly , using a bisection method to get all solutions for a given @xmath2 and we partition the range of @xmath2 adaptively into a denser grid where the derivative @xmath126 is large . for integer values of @xmath94",
    "when the equation can be rewritten as a polynomial in @xmath92 , it is technically easier to find the roots of the polynomial ; alternatively one can solve for @xmath2 given @xmath92 using a simple bisection , because @xmath127 is an injective function .    . *",
    "a ) * the phase diagram as a function of @xmath102 and the input - dependent term . in the region between the black solid lines two solutions",
    "@xmath128 exist for every value of the input @xmath2 ( y - axis ) .",
    "the corresponding critical value is at @xmath129 ( cusp of the black solid lines ) .",
    "circles and crosses represent analytical approximations to the exact boundary of the bistable region for @xmath123 and large @xmath130 ( see text ) . for three choices of @xmath102 denoted by vertical dashed lines ,",
    "the input / output relations @xmath131 are plotted in b. * b)*. the critical solution ( red ) has an infinite total derivative @xmath126 at @xmath132 , @xmath133 .",
    "the bistable system ( blue ) has three solutions , two stable and one unstable , for a range of inputs that can be read out from the plot in a. , width=192 ]      in this section we will generalize the computation of noise and information in the region close to the critical point , where the gaussian noise approximation breaks down .",
    "we start by rewriting eq s ( [ ll1][ll2 ] ) in our normalized units , @xmath134 .\\end{aligned}\\ ] ] this is equivalent to brownian motion of the coordinate @xmath4 in a potential @xmath135 defined by @xmath136 with an effective temperature @xmath137 that varies with position .",
    "if we simulate this langevin equation , we will draw samples out of the distribution @xmath14 , but we can construct this distribution directly by solving the equivalent diffusion or fokker ",
    "planck equation , @xmath138 + \\frac{\\partial^2 } { \\partial g^2}\\left[t(g)p(g , t)\\right];\\ ] ] the steady  state solution is then @xmath14 , and this is @xmath139 .",
    "\\label{intg}\\ ] ]    the `` small noise approximation '' in this extended framework corresponds to expanding the integrand in eq  ( [ intg ] ) around the mean , @xmath140 .",
    "if we write @xmath141 , we will find @xmath142 where our previous approximations correspond to keeping only @xmath143 .",
    "the critical point is where @xmath144 , and we have to keep higher order terms . in principle",
    "the expansion coefficients have contributions from the @xmath4dependence of the effective temperature , but we have checked that these contributions are negligible near criticality .",
    "then we have @xmath145 , \\label{eqset}\\\\ a_3 & = & -\\frac{1}{3 ! }",
    "\\frac{f''}{t } , \\nonumber\\\\ a_4 & = & -\\frac{1}{4!}\\frac{f'''}{t } , \\nonumber\\end{aligned}\\ ] ] where primes denote derivatives with respect to @xmath4 , and all terms should be evaluated at @xmath146 .    for the monod  wyman ",
    "changeaux regulatory function in eq ( [ mwc ] ) , all these derivatives can be evaluated explicitly : @xmath147 \\label{dfdg3}\\end{aligned}\\ ] ]    from eq  ( [ ccrit ] ) , the critical point occurs at @xmath148 when @xmath149 , and at this point the derivatives simplify : @xmath150 now we want to explore behavior in the vicinity of the critical point ; we will fix @xmath102 to its critical value , @xmath151 , and compute the derivatives in eqs  ( [ dfdg1]-[dfdg3 ] ) as @xmath152 . consider therefore a small positive @xmath153 such that @xmath154 in a system with chosen @xmath102 and @xmath94 that yield critical behavior , the deviation from criticality above will happen at @xmath155 . to find the relation between @xmath153 and @xmath156",
    ", we evaluate the derivative in eq ( [ dfdg1 ] ) at @xmath92 to form a function @xmath157 , which evaluates to 1 at @xmath112 .",
    "this function can be expanded in taylor series around @xmath112 ; the first order in @xmath156 vanishes and we find : @xmath158 therefore , the derivative deviates by @xmath153 from criticality at 1 when @xmath92 deviates by @xmath159 from the @xmath112 .",
    "we now perform similar expansions on the second- and third - order derivatives in eqs ( [ dfdg2],[dfdg3 ] ) , and evaluate the factors at the critical point : @xmath160 these expressions have been evaluated for @xmath161 , but we could have easily repeated the calculation by assuming that @xmath102 itself can deviate a bit from the critical value , i.e. @xmath162 , which would yield somewhat more complicated results that we do nt reproduce here .",
    "equations  ( [ perturb1][perturb3 ] ) can be used in eq ( [ eqset ] ) to write down the probability distribution @xmath14 . far away from the critical point",
    "the gaussian approximation is assumed to hold , and @xmath163 can be set to 0 .",
    "close to the critical point the higher order terms @xmath164 and @xmath165 need to be included . to assess the range where this switchover occurs",
    ", we compare in eqs ( [ perturb1][perturb3 ] ) the leading to the subdominant correction : we insist that the quadratic correction in eq  ( [ perturb2 ] ) is always smaller than linear , and that the linear correction in eq ( [ perturb3 ] ) is always smaller than constant ( we drop the quadratic correction there ) .",
    "we found empirically that including the higher - order corrections yields good results when the following conditions are simultaneously satisfied : @xmath166 in eq  ( [ gensol ] ) , together with the quartic ansatz for @xmath14 in eq  ( [ pquartic ] ) .",
    "for each @xmath2 , we evaluate two entropies of the conditional distribution @xmath14 : @xmath167 & = & \\log_2\\sqrt{2\\pi e\\sigma_g^2(c ) } \\label{ngauss } \\\\",
    "s_4[p(g|c ) ]   & = & -\\int dg \\;p(g|c ) \\log_2 p(g|c ) . \\label{nquartic}\\end{aligned}\\ ] ] @xmath168 is the noise entropy with higher - order terms included whenever conditions eq ( [ cond ] ) are met , and @xmath169 is the noise entropy in the gaussian approximation .    equation  ( [ gensol ] ) can be rewritten in a numerically stable fashion by realizing that @xmath170 , that is , that the optimal distribution of mean output levels is given by @xmath171}/z .",
    "\\label{cap2}\\ ] ] to join the gaussian and higher - order approximations consistently in the regimes away and near the critical point , the noise entropy in eq  ( [ cap2 ] ) is chosen to be the pointwise minimum of @xmath172 and @xmath173 .",
    "finally , the information is again @xmath174 , with @xmath175}. \\label{cap2}\\ ] ]      we now discuss the information capacity in the bistsable regime , away from the critical line . in this regime",
    ", each value of the input @xmath2 can give rise to multiple solutions of the steady state equation , eq  ( [ ss ] ) .",
    "in the simplest case ( which includes the mwc regulatory functions ) , there will be two stable solutions , @xmath176 and @xmath177 , and a third solution , @xmath178 , that is unstable . in equilibrium",
    ", the system will be on the first branch with weight @xmath179 and on the second with weight @xmath180 .",
    "here we place upper bound on the information @xmath18 , again in the small noise approximation .",
    "this will be useful since , as we will see , even this upper bound is always less than the information which can be transmitted in the monostable or critical regime , and so we will be able to conclude that the optimal parameters for which we are searching are never in the bistable regime .    in the bistable regime , the small noise approximation ( again , away from the critical line ) means that the conditional distributions are well approximated by a mixture of gaussians , @xmath181 to compute the information we need two terms , the total entropy and the conditional entropy .",
    "the conditional entropy takes a simple form if we assume the noise is small enough that the gaussians do nt overlap .",
    "then a direct calculation shows that , as one might expect intuitively , the conditional entropy is just the weighted sum of the entropies of the gaussian distributions , plus a term that reflects the uncertainty about which branch the system is on , @xmath182 & = & { 1\\over 2}\\sum_{{\\rm i}=1}^2 w_{\\rm i}(c ) \\log_2 \\left [ 2\\pi e \\sigma_{\\rm i}^2 ( c)\\right ]   \\nonumber\\\\ & & \\,\\,\\,\\,\\,\\,\\,\\,\\,\\ , - \\sum_{{\\rm i}=1}^2 w_{\\rm i}(c ) \\log_2 w_{\\rm i}(c ) .\\end{aligned}\\ ] ]    implementing the small noise approximation for the total entropy is a bit more subtle .",
    "we have , as usual , @xmath183   \\nonumber\\\\ & & \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,+   \\int dc\\ , p(c ) { { w_2(c)}\\over\\sqrt{2\\pi\\sigma_2 ^ 2(c ) } } \\exp\\left [ - { { ( g - \\bar g_2 ( c))^2}\\over{2\\sigma_2 ^",
    "2(c ) } } \\right ] .\\nonumber\\\\ & & \\end{aligned}\\ ] ] if the noise is small , each of the two integrals is dominated by values of @xmath2 near the solution of the equation @xmath184 ; let s call these solutions @xmath185 .",
    "notice that these solutions might not exist over the full range of @xmath4 , depending on the structure of the branches .",
    "nonetheless we can write @xmath186_{c = \\hat c_1(g)}\\nonumber\\\\ & & \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,+ \\left [ w_2 ( c)p(c )   { \\bigg | } { { d\\bar g_2 ( c)}\\over { dc } } { \\bigg |}^{-1}\\right]_{c = \\hat c_2(g ) } , \\end{aligned}\\ ] ] with the convention that if we try to evaluate @xmath187 at a non  existent value of @xmath188 , we get zero .",
    "thus , the full distribution @xmath189 is also a mixture ,",
    "@xmath190 the fractional contributions of the two distributions are @xmath191 where @xmath192 denotes an integral over the regions along the @xmath2 axis where the function @xmath185 exists , and the ( normalized ) component distributions are @xmath193_{c = \\hat c_{\\rm i}(g)}\\ ] ] the entropy of a mixture is always less than the average entropy of the components , so we have an upper bound    @xmath194 & \\leq & -\\sum_{{\\rm i}=1}^2 f_{\\rm i}\\int dg\\ , p_{\\rm i}(g ) \\log_2 p_{\\rm i}(g)\\\\ & = & -\\sum_{{\\rm i}=1}^2   \\int dg\\ , \\left [ w_{\\rm i } ( c)p(c )   { \\bigg | } { { d\\bar g_{\\rm i } ( c)}\\over { dc } } { \\bigg |}^{-1}\\right]_{c = \\hat c_{\\rm i}(g)}\\log_2   \\left [ { 1\\over { f_{\\rm i } } } w_{\\rm i } ( c)p(c )   { \\bigg | } { { d\\bar g_{\\rm i } ( c)}\\over { dc } } { \\bigg |}^{-1}\\right]_{c = \\hat c_{\\rm i}(g)}\\\\ & = & -\\sum_{{\\rm i}=1}^2 \\int_{\\rm i } dc\\ , p(c ) w_{\\rm i}(c ) \\log_2   \\left [ { 1\\over { f_{\\rm i } } } w_{\\rm i } ( c)p(c )   { \\bigg | } { { d\\bar g_{\\rm i } ( c)}\\over { dc } } { \\bigg |}^{-1}\\right ]   .\\end{aligned}\\ ] ]    an upper bound on the total entropy is useful because it allows us to bound the mutual information : @xmath195 - \\int dc\\ , p(c ) s[p(g|c)]\\\\ & \\leq & -\\sum_{{\\rm i}=1}^2 \\int_{\\rm i } dc\\ , p(c ) w_{\\rm i}(c ) \\log_2   \\left [ { 1\\over { f_{\\rm i } } } w_{\\rm i } ( c)p(c )   { \\bigg | } { { d\\bar g_{\\rm i } ( c)}\\over { dc } } { \\bigg |}^{-1}\\right ]   \\nonumber\\\\ & & \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,-   { 1\\over 2}\\sum_{{\\rm i}=1}^2 \\int dc\\ , p(c ) w_{\\rm i}(c ) \\log_2 \\left [ 2\\pi e \\sigma_{\\rm i}^2 ( c)\\right ] + \\sum_{{\\rm i}=1}^2 \\int dc\\ , p(c ) w_{\\rm i}(c ) \\log_2 w_{\\rm i}(c)\\\\ & = & -\\int dc\\ , p(c ) \\log_2 p(c ) + { 1\\over 2}\\int dc\\ , p(c ) \\sum_{{\\rm i}=1}^2   w_{\\rm i}(c ) \\log_2 \\left [     { \\bigg | } { { d\\bar g_{\\rm i } ( c)}\\over { dc } } { \\bigg |}^2 { 1\\over{2\\pi e \\sigma_{\\rm i}^2 ( c)}}\\right ] - \\left ( -   \\sum_{{\\rm i}=1}^2 f_{\\rm",
    "i } \\log_2 f_{\\rm i } \\right)\\\\ & \\leq &   -\\int dc\\ , p(c ) \\log_2 p(c ) + { 1\\over 2}\\int dc\\ , p(c ) \\sum_{{\\rm i}=1}^2   w_{\\rm i}(c ) \\log_2 \\left [     { \\bigg | } { { d\\bar g_{\\rm i } ( c)}\\over { dc } } { \\bigg |}^2 { 1\\over{2\\pi e \\sigma_{\\rm i}^2 ( c)}}\\right ] , \\label{ibound_n}\\end{aligned}\\ ] ]    where in the last step we use the positivity of the entropy associated with the mixture weights @xmath196 .",
    "we can now ask for the probability distribution @xmath197 that maximizes the upper bound on @xmath18 , and in this way we can bound the capacity of the system .",
    "happily , the way in which the bound depends on @xmath197 , in eq ( [ ibound_n ] ) , is not so different from the dependencies that we have seen in the monostable case [ eq  ( [ pointback ] ) ] , so we can follow a parallel calculation to show that @xmath198\\\\   \\phi(c ) & = &    \\sum_i   w_i(c ) \\ln \\left[\\sqrt{2 \\pi e \\sigma^2_{g_i}(c ) }   \\left|\\frac{d \\bar{g}_i(c)}{d c } \\right|^{-1 } \\right ] .",
    "\\label{multii}\\end{aligned}\\ ] ] finally , to find the weights @xmath187 we can numerically integrate the fokker ",
    "planck solution in eq  ( [ intg ] ) to find @xmath199    to summarize , we have derived an upper bound on the information transmitted between the input and the output . the tightness of the bound is related to the applicability of the `` no overlap '' approximation , which for mwc  like regulatory functions should hold very well , as we have verified numerically . if only one of the weights @xmath200 , our results reduce to those in the monostable case , as they should .",
    "we begin by showing that the analytical calculations presented in the previous section can be carried out numerically in a stable fashion , both away from and in the critical regime .",
    "we recall that the information transmission is determined by an integral @xmath84 [ eq  ( [ cap1 ] ) ] , and that because we are working in the small noise approximation we have a choice of evaluating this as an integral over the input concentration @xmath2 or an integral over the mean output concentrations @xmath201 .",
    "figure [ f-3 ] shows the behavior of the integrands in these two equivalent formulations when we have chosen parameters that are close to the critical point in a self  activating gene .",
    "the key result is that , once we include terms beyond the gaussian approximation to @xmath14 following the discussion in section [ critinfo ] , we really do have control over the calculation , and find smooth results as the parameter values approach criticality .",
    "thus , we can compute confidently , and search for parameters of the regulatory function @xmath202 that maximize information transmission .     for @xmath123 , @xmath203 and @xmath204 , showing a self - activating gene with an almost critical value of @xmath102 . *",
    "b ) * noise in the input from eq  ( [ sigmac ] ) , which is the integrand in the expression for information in eq  ( [ cap1 ] ) , shows an incipient divergence between red vertical bars .",
    "inset : zoom - in of the peak shows that it can be sampled well by increasing the number of bins .",
    "different plot symbols indicate domain discretization into 500 ( black dots ) and 5000 ( red circles ) bins . at the critical point",
    "the divergence is hard to control numerically .",
    "* c ) * an alternative way of computing the same information , by integrating in the output domain as in eq  ( [ cap2 ] ) . shown is the integrand in the gaussian noise approximation [ black , @xmath173 from eq  ( [ ngauss ] ) ] and with quartic corrections [ red , @xmath172 from eq  ( [ nquartic ] ) ] . at the critical point",
    "higher order corrections regularize the integrand , while away from the critical point the integrand smoothly joins with the gaussian approximation .",
    "this approach is stable numerically both away from and in the critical regime .",
    "* d ) * information @xmath205 as a function of @xmath102 for @xmath123 .",
    "critical @xmath129 is denoted by a dashed red line .",
    "integration across @xmath92 in the output domain with quartic corrections ( squares ) agrees well with the integration across @xmath2 in the input domain ( crosses ) away from @xmath161 , but also smoothly extends to the critical @xmath161 .",
    "this is a cut across the capacity plane in fig  [ f-4]a ( denoted by a dashed yellow line ) for @xmath203 . ]",
    "we start the optimization by choosing the parameter values @xmath206 which describe the self  interaction term , and then holding these fixed while we optimize the remaining ones , @xmath207 . in all these optimizations the parameter @xmath101 is driven to zero , and in this limit the mwc regulatory function of eq  ( [ mwc ] ) simplifies to something more like the hill function , @xmath208 once we have optimized @xmath209 , we can explore the information capacity as a function of @xmath210 , at varying values of the remaining parameter in the problem , the maximal concentration @xmath37 of transcription factors .",
    "figure  [ f-4]a maps out the `` capacity planes , '' @xmath211 at fixed @xmath37 . in detail , we show @xmath212 for three choices of our parameter @xmath37 , where @xmath213 is the information obtained with the optimal choice of @xmath94 and @xmath102 ; the best choice of parameters is depicted as a yellow circle in the capacity plane .        for large values of @xmath37 , @xmath214 ,",
    "the optimal solution is at @xmath215 or @xmath216 ( magenta square in the lower right corner ) , which drives the self  activation term in eq  ( [ mwc ] ) to zero , towards a noninteracting solution .",
    "we have checked that these solutions correspond to optimal solutions for a single noninteracting gene found in our previous work @xcite . as @xmath37 is decreased , however , the optimal combination @xmath217 shifts towards the left in the capacity plane ( cyan square for @xmath218 ) , exhibiting a shallow but distinct maximum in information transmission .",
    "if we examine the mean input / output relations in fig  [ f-4]b , we find nothing dramatic : the critical ( red ) solutions seem to have lower capacities ( which we carefully reexamine blow ) , while other quite distinct parameter choices for @xmath217 nevertheless generate very similar mean input / output relations , because of the freedom to optimally choose @xmath219 parameters .",
    "the behavior of effective noise in the input , @xmath220 , given by eq  ( [ sigmac ] ) and shown in fig  [ f-4]c , is more informative ; recall that @xmath221 is proportional to the information transmission .",
    "noninteracting ( magenta ) solutions always have the lowest amount of noise at high input concentrations ( @xmath222 ) .",
    "as the self  interaction turns on , the noise at high input increases , but that increase can be traded off for a decrease in noise at medium and low @xmath2 .",
    "while for low @xmath218 the critical ( red ) solution is never optimal , the solution with some self  activation manages to deliver an additional @xmath223 bits of information .",
    "we have verified that for @xmath224 smaller value of @xmath225 the capacity plane is qualitatively the same , exhibiting the peak at a nontrivial ( but still not critical ) choice of @xmath226 ( not shown ) .",
    "intuitively , the self - activation parameters @xmath227 have three direct effects on the information transmission : they change the shape of the input / output curve , the self ",
    "activation feeds some of the output noise back into the input , and the time @xmath7 ( protein lifetime ) that averages the input noise component gets renormalized to @xmath228 .",
    "the changes in the mean input / output relation can be partially compensated for by the correlated changes in the @xmath219 , as we observed in our optimal solutions , suggesting that regardless of the underlying microscopic parameters , it is the shape of @xmath131 itself that must be optimal . the increase in averaging time acts to increase the information , thus favoring self  activation .",
    "however , this will simultaneously increase the noise in the output that feeds back , as well as drive @xmath131 towards infinite steepness at criticality , restricting the dynamic range of the output . at low @xmath37",
    "there is a parameter regime where increasing the integration time will help decrease the ( dominant ) input noise enough to result in a net gain of information . at high",
    "@xmath37 , input noise is minimal and thus this averaging effect loses its advantage ; instead , feedback simply acts to increase the total noise by reinjecting the output noise at the input , so that optimizing information transmission drives the self ",
    "interaction to zero .",
    "next we examine in detail the behavior of information transmission close to the critical region . close to , but not at , the critical point we perform very fine discretization of the input range to evaluate the integral in eq  ( [ cap1 ] ) , as reported in fig  [ f-3]b . to validate that the information indeed reaches a maximum at nontrivial values of @xmath217 when @xmath218 , we cut through the capacity plane in fig  [ f-4]a along the yellow line at @xmath123 , and display the resulting capacity values in fig  [ f-5]a ( the results are numerically stable when integrated on @xmath229 or @xmath230 points ) .",
    "unlike for @xmath203 and @xmath231 , for @xmath218 the maximum is clearly achieved for a nontrivial value of @xmath102 , but away from the critical line , confirming our previous observations .",
    "we further examine the capacity directly on the critical line , @xmath119 , as a function of @xmath94 at @xmath203 ( denoted in fig  [ f-4]a with dashed red line ) . the capacity in this case",
    "can be calculated using eq  ( [ cap2 ] ) and is shown in fig  [ f-5]b .",
    "the capacity that includes quartic corrections is higher by @xmath232 bits than in the gaussian approximation , making the effect small but noticeable .",
    "we also confirmed that the capacity at the critical line joints smoothly with the capacity near the line , i.e. that there is no jump in capacity exactly at criticality , which presumably would be a sign of numerical errors .",
    "figure  [ f-5]c finally validates that across the whole range of @xmath94 for @xmath203 , small increases in @xmath102 above the critical value @xmath151 always lead to an increase of information , demonstrating that the maximum is _ not _ achieved on the critical line .",
    "are optimized as a function of the maximal input concentration @xmath37 .",
    "the self - interacting system ( red ) allows for an arbitrary mwc - like regulatory function [ eq  ( [ mwc ] ) ] with parameters @xmath233 .",
    "the noninteracting system ( black ) only has the mwc parameters @xmath234 and the leak @xmath98 ( see ref  @xcite ) which can be reexpressed in terms of @xmath104 .",
    "bright red line with circles shows self - activating solutions which are optimal for @xmath235 , while dark red line with crosses shows self - repressing solutions , optimal for @xmath236 .",
    "plotted on the secondary vertical axis in green is the ratio between the self - interacting contribution to @xmath237 , and the input contribution to @xmath237 in the expression for the mwc regulatory function [ eq  ( [ mwc ] ) ] . for @xmath238 where the interacting and noninteracting solution join ,",
    "this term falls to 0 , as expected . ]",
    "we next turn to the joint optimization of all parameters and plot the information transmission as a function of @xmath37 in fig  [ f-6 ] .",
    "as we have discussed , optimization drives the strength of self  activation to zero for @xmath236 ( but see below for self  repression ) , and at these high values of @xmath37 the result of full optimization coincides with the non  interacting case . as @xmath37 falls below one , the gain in information due to self  activation",
    "is increased , reaching a significant value of about a bit for @xmath225 .",
    "as we have noted in section [ mwc ] , the self  activating effect of @xmath4 on its own expression can be changed into a self  repressing effect by simply flipping the sign of the parameter @xmath94 . to explore the optimization of such self  repressing genes , we thus optimized the parameters as before , now constraining @xmath239 .",
    "results in @xmath240 plane are shown in fig [ f-7 ] , for @xmath203 and @xmath231 .     for @xmath203",
    "( left column ) and @xmath231 ( right column ) ; plot conventions are the same as in fig  [ f-4 ] .",
    "* a ) * the capacity decrease from the maximum value ( achieved at the parameter choice indicated by a yellow circle ) as a function of @xmath217 .",
    "the maximum information transmission is achieved for a non - interacting case ( magenta ) , for @xmath203 .",
    "in contrast , for @xmath231 , there is a non - trivial optimum for small values of @xmath102 and @xmath241 ( red ) . *",
    "b ) * the mean input / output solutions for three example systems from a ( red , magenta , cyan ) .",
    "* c ) * the effective noise in the input , @xmath220 , for example solutions in a. ]    we find that , for large @xmath37 , the optimization process drives both @xmath101 and @xmath102 toward zero , so that the effective input / output relation is given by @xmath242 with nonzero values of @xmath94 being optimal . why is self ",
    "repression optimal at large @xmath37 , when self  activation is not ?",
    "repression suppresses noise at high concentrations of the input ( red vs magenta curves fig  [ f-7]c for @xmath231 ) and allows the mean input / output curve to be more linear than in the non  interacting case ( fig  [ f-7]b ) , extending the dynamic range of the response .",
    "both these effects serve to increase information transmission .",
    "it is remarkable that when we put together the self  activating and self  repressing solutions , we see that they join smoothly at @xmath203 ( fig [ f-6 ] ) : self  activation is optimal for @xmath235 , and self  repression is optimal for @xmath236 , while precisely at @xmath203 the system that transmits the most information is non  interacting .",
    "all of this discussion has been in the limit where the maximal concentration of output molecules , @xmath75 , is the same as the maximal concentration of input molecules , @xmath37 , so there is only one parameter that governs the structure of the optimal solution .",
    "this makes sense , at least approximately , since both input and output molecules are transcription factors , presumably with similar costs , but nonetheless we would like to see what happens when we relax this assumption . intuitively ,",
    "if we let @xmath75 become large , the system can achieve the advantages of feedback while the impact of noise being fed back into the system should be reduced .",
    "if we look at eq ( [ cap1 ] ) for @xmath84 , which controls the information capacity , we can take the limit @xmath243 to find @xmath244 now the only place where feedback plays an explicit role is in the term @xmath245 , which comes from the lengthening of the integration time , which in turn serves to average out the noise in the system .",
    "all other things being equal ( which may be hard to arrange ) , this suggests that information transmission will be maximized if the system approaches the critical point , where @xmath246 .",
    "the difficulty is that the system ca nt stay at the critical point for all values of the input @xmath2 , so there must be a tradeoff between lengthening the integration time and using the full dynamic range .    to explore more quantitatively ,",
    "we treat @xmath247 as a parameter . when @xmath37 is small , we know that self  activation is important , and in this regime we see from fig  [ f-8 ] that changing @xmath247 matters . on the other hand , for large values of @xmath37 we know that ( at @xmath248 ) optimization drives self  activation to zero , so we expect that there is less or no impact of allowing @xmath249 .",
    "we also see that , for a fixed small @xmath37 , increasing @xmath247 drives the system closer towards the signatures of criticality  nonmonotonic behavior in the noise and a steepening of the input / output relation . in more detail , we can plot the value of @xmath250 as a function of @xmath37 and @xmath247 ,",
    "that is , check for each of the solutions in fig  [ f-8]a how close the partial derivative @xmath251 comes to 1 , which is a direct measure of criticality .",
    "we confirm that , for the simultaneous choice of small @xmath37 and large @xmath247 , we indeed have @xmath246 . in the extreme , if we choose @xmath225 and @xmath252 , we find that the optimal @xmath101 and @xmath102 are driven towards small values ( but since @xmath253 are small @xmath101 is not negligible ) ; the optimal @xmath254",
    ". with this value of @xmath94 , the corresponding critical value for @xmath102 would be @xmath255 , and the numerically found optimal value in our system is @xmath256 .",
    "the critical value for @xmath112 would be @xmath257 , and indeed at this small value the optimal mean input / output relation has a strong kink , the effective noise @xmath258 has a sharp dip and @xmath259 at this point climbs to 0.9936 .",
    "numerically , therefore , we have all the expected indications of emerging criticality at very large @xmath75 . for less extreme values",
    ", we expect the optimum to result from the interplay between the input and transmitted noise contributions , which in general need not be on the critical line .    .",
    "* a ) * for various choices of @xmath37 indicated on the plot , the information transmission with the optimal choice with respect to all parameters @xmath260 is shown as a function of @xmath247 .",
    "two special systems of interest ( blue , green ) are chosen for the lowest value of @xmath37 . *",
    "b ) * the mean input / output relation for the blue and green system . the green system has a higher transmission , a steeper activation curve but a smaller dynamic range .",
    "* c ) * the effective noise in the input , @xmath220 , for the blue and green systems .",
    "the green system is closer to critical at the point where the mean input / output curve has the highest curvature and the noise exhibits a dip . ]    to complete our exploration of the optimization problem , we have to consider parameter values for which the output has two locally stable values given a single input .",
    "quantitatively , in the bistable regime we have to solve for both stable solutions @xmath261 , with @xmath262 , and for the unstable branch @xmath178 .",
    "we can then evaluate the equilibrium probabilities @xmath187 of being on either of the stable branches using eq  ( [ weights ] ) , and use eq  ( [ multii ] ) to compute the capacity . as shown in an example in fig  [ f-9]a",
    ", we never find the optimal solutions in the bistable region ",
    "the capacity starts decreasing after crossing the critical line . consistently with our argument that output and feedback noise must become negligible for the regime of small @xmath37 and large @xmath247 , we find that optimization drives the system towards achieving maximal transmission closer and closer the the critical line ( which is approached from the monostable side ) , as shown in fig  [ f-9]b .",
    "to summarize , we have analyzed in detail a single , self  interacting genetic regulatory element .",
    "as in previous work , we based our analysis on three assumptions : ( i ) that the readout of the information @xmath18 between the input and output happens in steady state , ( ii ) that noise is small ; and ( iii ) that the constraint limiting the information flow is the finite number of signaling molecules . in addressing a system with feedback , assumption ( ii )",
    "requires technical elaboration near the critical point , as discussed above . but",
    "( i ) requires a qualitatively new discussion for systems with feedback , because of the possibility of multistability .     at fixed @xmath263 , with the optimal choice of @xmath264 parameters , for three values of @xmath37 ( @xmath265 dark to bright red , respectively ) .",
    "dots show capacity calculation using the bistable code that can handle multiple branches using eq  ( [ multii ] ) , solid line uses the monostable integration as in eq  ( [ cap1 ] ) . *",
    "b ) * optimal capacity at very high ratios @xmath266 for different value of @xmath37 ( @xmath267 : circles , crosses , squares , stars , respectively ) . the optimum",
    "is pushed towards the critical line from the monostable side for @xmath247 large and @xmath37 small . in all cases ,",
    "information in the bistable regime is smaller than in the monostable regime . ]",
    "our analysis , with the steady state assumption , shows that truly bistable systems do not maximize the information .",
    "intuitively , this stems from the branch ambiguity : for a given input concentration @xmath2 a bistable system can sit on either one of the stable branches with some probability , and this uncertainty contributes to the noise entropy , thereby reducing the transmitted information . but",
    "reaching steady state involves waiting for two very different processes .",
    "first , the system reaches a steady distribution of fluctuations in the neighborhood of each stable point , and then the populations of the two stable states equilibrate with one another . as with brownian motion in a double well potential ( or a chemical reaction ) , these two processes can have time scales that are separated by an exponentially large factor .",
    "alternatively , the timescales of real regulatory and readout processes could be such that the system does not have the time to equilibrate between the stable branches . in that case ,",
    "the history ( initial condition ) of the system will matter , and the final value of the output @xmath4 will be determined jointly by the input @xmath2 and the past state of the output , @xmath268 .",
    "such regulatory elements can be very useful , because they retain memory of the past and are able to integrate it with the new input ; a much studied biological example is that of a toggle switch .",
    "the information measure we use here , @xmath18 , will not properly capture the abilities of such elements , unless we modify it to include the past state , e.g. into @xmath269 : here both the input and current state together determine the output .",
    "such computations are beyond the scope of this paper , but could make precise our intuitions about switches with memory .",
    "multistability also allows for qualitatively new effects at higher noise levels . in our previous work we found that full information flow optimization ( without assuming small noise ) leads to higher capacities than a small  noise calculation for an identical system and , moreover , that as noise grows , the optimal solutions start resembling a ( noisy ) binary switch where only the minimum and maximum states of input are populated in the optimal @xmath270 @xcite . at high noise , positive ( even bistable )",
    "autoregulation could stabilize these two states and make them more distinguishable . in this case",
    "the design constraint for the genetic circuit is to use the smallest number of molecules that will prevent spontaneous flipping between the two branches on the relevant biological timescales @xcite . in this limit regulatory elements",
    "can operate at high noise , with perhaps as few as tens of signaling molecules .",
    "with these caveats in mind , our main results can be summarized as follows . except at @xmath238 , the possibility of self ",
    "interaction always increases the capacity of genetic regulatory elements . for @xmath235 ,",
    "the optimal strategy is self  activation , while for @xmath236 it is self  repression , as shown in fig  [ f-6 ] .",
    "repression allows the system to reduce the effective noise at high input levels and straighten the input / output relation , packing more `` distinguishable '' signaling levels into a fixed input range .",
    "activation for small @xmath37 lengthens the effective integration time over which the ( dominant ) input noise contribution is averaged , thereby increasing information . the optimal level of self  activation is never so strong as to cause bistability , but does , for small @xmath37 and large @xmath247 , push the optimal system towards the critical state .",
    "an interesting observation about the nature of the optimal solutions is that self  activation which is strong enough to enhance information transmission may nonetheless not result in a functional input / output relation that looks very different from a system without self  activation , albeit with different parameters . in such cases ,",
    "information transmission is enhanced primarily by the longer integration time and reduced effective noise level .",
    "this means that there need be no dramatic signature of self  activation , so that diagnosing this operating regime requires a detailed quantitative analysis .",
    "more generally , this result emphasizes that the same phenomenology can result from different parameter values , or even networks with different topology  in this case , with and without feedback .    stepping back from the detailed results , our goal in this paper was to make progress on understanding the optimization of information flow in systems with feedback by studying the simplest example .",
    "the hope is that our results provide one building block for a theory of real genetic networks , on the hypothesis that they have been selected for maximizing information transmission . as discussed in previous work",
    "@xcite , a natural target for such analysis is the well studied gap gene network in the early _ drosophila _ embryo @xcite .",
    "but we can also hope to connect with a broader range of examples .",
    "the prevailing view of self  activation has been that its utility stems from the possibility of creating a toggle ( or a flip - flop ) switch .",
    "this explanation , however , can only be true if self  activation is strong enough to actually push the system into the bistable regime .",
    "de ronde and colleagues @xcite have improved on this intuition and have shown , in the linear response limit , that weak self  activation will increase the signal to noise ratio for dynamic signals , a function very different from the switch . here",
    "we show that in the fully nonlinear , but steady state treatment , monostable self  activation can be advantageous for information transmission .",
    "furthermore , we show that there is a single control parameter , the ratio @xmath37 between the output and input noise strengths , which determines whether self  activation or self  repression is optimal . since more and more quantitative expression data is available , especially for bacteria and yeast",
    ", one could try assessing how the use of both motifs correlates with the concentrations of input and output signaling molecules .",
    "we thank t gregor , ef wieschaus , and especially cg callan for helpful discussions .",
    "work at princeton was supported in part by nsf grants phy0957573 and ccf0939370 , by nih grant r01 gm077599 , and by the wm keck foundation . for part of this work",
    ", gt was supported in part by nsf grant ef0928048 , and by the vice provost for research at the university of pennsylvania .",
    "m ronen , r rosenberg , bi shraiman & u alon ( 2002 ) assigning numbers to the arrows : parameterizing a gene regulation network by using accurate expression kinetics .",
    "_ proc natl acad sci usa _ * 99 : * 1055510560 .",
    "mb elowitz , aj levine , ed siggia & pd swain ( 2002 ) stochastic gene expression in a single cell . _ science _ * 297 * 11831186 .",
    "e ozbudak , m thattai , i kurtser , ad grossman & a van oudenaarden ( 2002 ) regulation of noise in the expression of a single gene .",
    "_ nature gen _ * 31 : * 6973 .",
    "wj blake , m kaern , cr cantor & jj collins ( 2003 ) noise in eukaryotic gene expression",
    ". _ nature _ * 422 : * 633637 .",
    "jm raser & ek oshea ( 2004 ) control of stochasticity in eukaryotic gene expression",
    ". _ science _ * 304 : * 18111814 .",
    "n rosenfeld , jw young , u alon , ps swain & mb elowitz ( 2005 ) gene regulation at the single cell level . _ science _ * 307 : * 19621965 .",
    "jm pedraza & a van oudenaarden ( 2005 ) noise propagation in gene networks .",
    "_ science _ * 307 : * 19651969 ."
  ],
  "abstract_text": [
    "<S> living cells must control the reading out or `` expression '' of information encoded in their genomes , and this regulation often is mediated by transcription factors  proteins that bind to dna and either enhance or repress the expression of nearby genes . </S>",
    "<S> but the expression of transcription factor proteins is itself regulated , and many transcription factors regulate their own expression in addition to responding to other input signals . </S>",
    "<S> here we analyze the simplest of such self  regulatory circuits , asking how parameters can be chosen to optimize information transmission from inputs to outputs in the steady state . some nonzero level of self  regulation is almost always optimal , with self  activation dominant when transcription factor concentrations are low and self  repression dominant when concentrations are high . in steady  state </S>",
    "<S> the optimal self  activation is never strong enough to induce bistability , although there is a limit in which the optimal parameters are very close to the critical point . </S>"
  ]
}