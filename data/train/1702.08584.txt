{
  "article_text": [
    "in the past few decades , reinforcement learning ( rl)-based techniques have been established as primary tools for online real - time optimization @xcite .",
    "rl techniques are valuable not only for optimization but also for control synthesis in complex systems such as a distributed network of cognitive agents .",
    "combined efforts from multiple autonomous agents can yield tactical advantages including : improved munitions effects ; distributed sensing , detection , and threat response ; and distributed communication pipelines @xcite .",
    "while coordinating behaviors among autonomous agents is a challenging problem that has received mainstream focus , unique challenges arise when seeking optimal autonomous collaborative behaviors .",
    "for example , most collaborative control literature focuses on centralized approaches that require all nodes to continuously communicate with a central agent , yielding a heavy communication demand that is subject to failure due to delays , and missing information@xcite .",
    "furthermore , the central agent is required to carry enough on - board computational resources to process the data and to generate command signals .",
    "these challenges motivate the need to minimize communication for guidance , navigation and control tasks , and to distribute the computational burden among the agents .",
    "since all the agents in a network have independent collaborative or competitive objectives , the resulting optimization problem is a multi - objective optimization problem .",
    "differential game theory is often used to define optimality in multi - objective optimization problems @xcite .",
    "for example , a nash equilibrium solution to a multi - objective optimization problem is said to be achieved if none of the players can benefit from a unilateral deviation from the equilibrium @xcite .",
    "thus , nash equilibrium solutions provide a secure set of strategies in the sense that none of the players have an incentive to diverge from their equilibrium policy .",
    "hence , nash equilibrium has been a widely used solution concept in differential game - based control techniques .",
    "online real - time solutions to differential games with centralized objectives are presented in results such as @xcite ; however , since these results solve problems with centralized objectives ( i.e. , each agent minimizes or maximizes a cost function that penalizes the states of all the agents in the network ) , they are not applicable for a network of agents with independent decentralized objectives ( i.e. , each agent minimizes or maximizes a cost function that penalizes only the error states corresponding to itself ) .    in this paper ,",
    "the objective is to obtain an online forward - in - time feedback - nash equilibrium solution ( cf .",
    "@xcite ) to an infinite - horizon formation tracking problem , where each agent desires to follow a mobile leader while the group maintains a desired formation .",
    "the agents try to minimize cost functions that penalize their own formation tracking errors and their own control efforts .",
    "various methods have been developed to solve optimal tracking problems for linear systems .    in @xcite , optimal controllers are developed to cooperatively control agents with linear dynamics . in @xcite ,",
    "a differential game - based approach is developed for unmanned aerial vehicles to achieve distributed nash strategies . in @xcite , an optimal consensus algorithm is developed for a cooperative team of agents with linear dynamics using only partial information .    for nonlinear systems ,",
    "a mpc - based approach is presented in @xcite ; however , no stability or convergence analysis is presented .",
    "a stable distributed mpc - based approach is presented in @xcite for nonlinear discrete - time systems with known nominal dynamics .",
    "asymptotic stability is proved without any interaction between the nodes ; however , a nonlinear optimal control problem needs to be solved at every iteration to implement the controller .",
    "an optimal tracking approach for formation control is presented in @xcite using single network adaptive critics where the value function is learned offline .",
    "recently , a leader - based consensus algorithm is developed in @xcite where exact model of the system dynamics is utilized , and convergence to optimality is obtained under a persistence of excitation condition .    for multi - agent problems with decentralized objectives",
    ", the desired action by an individual agent depends on the actions and the resulting trajectories of its neighbors ; hence , the error system for each agent is a complex nonautonomous dynamical system .",
    "nonautonomous systems , in general , have non - stationary value functions . since non - stationary functions are difficult to approximate using parameterized function approximation schemes such as neural networks ( nns ) , designing optimal policies for nonautonomous systems is challenging .    since the external influence from neighbors renders the dynamics of each agent nonautonomous , optimization in a network of agents presents challenges similar to optimal tracking problems . using insights gained from the authors previous work on optimal tracking problems @xcite , this paper develops a model - based rl technique to generate feedback - nash equilibrium policies online , for agents in a network with cooperative or competitive objectives . in particular",
    ", the network of agents is separated into autonomous subgraphs , and the differential game is solved separately on each subgraph .    the primary contribution of this paper is the formulation and online approximate feedback - nash equilibrium solution of an optimal network formation tracking problem .",
    "a relative control error minimization technique is introduced to facilitate the formulation of a feasible infinite - horizon total - cost differential graphical game .",
    "dynamic programming - based feedback - nash equilibrium solution of the differential graphical game is facilitated via the development of a set of coupled hamilton - jacobi ( hj ) equations .",
    "the developed approximate feedback - nash equilibrium solution is analyzed using a lyapunov - based stability analysis to demonstrate ultimately bounded formation tracking in the presence of uncertainties .",
    "throughout the paper , @xmath0 denotes @xmath1dimensional euclidean space , @xmath2 denotes the set of real numbers strictly greater than @xmath3 , and @xmath4 denotes the set of real numbers greater than or equal to @xmath3 .",
    "unless otherwise specified , the domain of all the functions is assumed to be @xmath5 .",
    "functions with domain @xmath5 are defined by abuse of notation using only their image .",
    "for example , the function @xmath6 is defined by abuse of notation as @xmath7 . by abuse of notation , the state variables are also used to denote state trajectories .",
    "for example , the state variable @xmath8 in the equation @xmath9 is also used as @xmath10 to denote the state trajectory , i.e. , the general solution @xmath6 to @xmath9 evaluated at time @xmath11 .",
    "unless otherwise specified , all the mathematical quantities are assumed to be time - varying .",
    "unless otherwise specified , an equation of the form @xmath12 is interpreted as @xmath13 for all @xmath14 , and a definition of the form @xmath15 for functions @xmath16 , @xmath17 and @xmath18 is interpreted as @xmath19 .",
    "the total derivative @xmath20 is denoted by @xmath21 and the partial derivative @xmath22 is denoted by @xmath23 .",
    "an @xmath24 identity matrix is denoted by @xmath25 , @xmath26 matrices of zeros and ones are denoted by @xmath27 and @xmath28 , respectively , and @xmath29 denotes the indicator function of the set @xmath30 .",
    "consider a set of @xmath31 autonomous agents moving in the state space @xmath0 .",
    "the control objective is for the agents to maintain a desired formation with respect to a leader .",
    "the state of the leader is denoted by @xmath32 .",
    "the agents are assumed to be on a network with a fixed communication topology modeled as a static directed graph ( i.e. digraph ) .",
    "each agent forms a node in the digraph .",
    "the set of all nodes excluding the leader is denoted by @xmath33 and the leader is denoted by node 0 . if node @xmath34 can receive information from node @xmath35 then there exists a directed edge from the @xmath36 to the @xmath34^th^ node of the digraph , denoted by the ordered pair @xmath37",
    ". let @xmath38 denote the set of all edges .",
    "let there be a positive weight @xmath39 associated with each edge @xmath37 .",
    "note that @xmath40 if and only if @xmath41 the digraph is assumed to have no repeated edges , i.e. , @xmath42 , which implies @xmath43 .",
    "the neighborhood sets of node @xmath34 are denoted by @xmath44 and @xmath45 , defined as @xmath46 and @xmath47 .    to streamline the analysis ,",
    "an adjacency matrix @xmath48 is defined as @xmath49 $ ] , a diagonal pinning gain matrix @xmath50 is defined as @xmath51\\right)$ ] , an in - degree matrix @xmath52 is defined as @xmath53 where @xmath54 , and a graph laplacian matrix @xmath55 is defined as @xmath56 . the graph is assumed to have a spanning tree , i.e. , given any node @xmath34 , there exists a directed path from the leader @xmath57 to node @xmath34 .",
    "a node @xmath35 is said to be an extended neighbor of node @xmath34 if there exists a directed path from node @xmath35 to node @xmath34 .",
    "the extended neighborhood set of node @xmath34 , denoted by @xmath58 , is defined as the set of all extended neighbors of node @xmath59 formally , @xmath60 .",
    "let @xmath61 , and let the edge weights be normalized such that @xmath62 for all @xmath63 . note that the sub - graphs are nested in the sense that @xmath64 for all @xmath65 .",
    "the state @xmath66 of each agent evolves according to the control affine dynamics @xmath67 where @xmath68 denotes the control input , and @xmath69 and @xmath70 are locally lipschitz continuous functions .",
    "[ ass : clnnleader]the dynamics of the leader are described by @xmath71 where @xmath72 is a locally lipschitz continuous function . the function @xmath73 , and the initial condition @xmath74 are selected such that the trajectory @xmath75 is uniformly bounded for all @xmath76 .    the control objective is for the agents to maintain a predetermined formation ( with respect to an inertial reference frame ) around the leader while minimizing their own cost functions . for all @xmath63 ,",
    "the @xmath77 agent is aware of its constant desired relative position @xmath78 with respect to all its neighbors @xmath79 , such that the desired formation is realized when @xmath80 for all @xmath81 .",
    "are assumed to be fixed in an inertial reference frame , i.e. , the final desired formation is rigid and its motion in an inertial reference frame can be described as pure translation . ] to facilitate the control design , the formation is expressed in terms of a set of constant vectors @xmath82 where each @xmath83 denotes the constant final desired position of agent @xmath34 with respect to the leader .",
    "the vectors @xmath84 are unknown to the agents not connected to the leader , and the known desired inter agent relative position can be expressed in terms of @xmath84 as @xmath85 .",
    "the control objective is thus satisfied when @xmath86 for all @xmath63 . to quantify the objective , local neighborhood tracking error signals",
    "are defined as @xmath87    to facilitate the analysis , the error signals in ( [ eq : clnne ] ) are expressed in terms of the unknown leader - relative desired positions as @xmath88 stacking the error signals in a vector @xmath89{cccc } e_{1}^{t } , & e_{2}^{t } , & \\cdots , & e_{n}^{t}\\end{array}\\right]^{t}\\in\\mathbb{r}^{nn}$ ] the equation in ( [ eq : clnneunk ] ) can be expressed in a matrix form @xmath90 where @xmath91 @xmath92 @xmath93 @xmath94 @xmath95^{t}$]@xmath96 , @xmath97 @xmath98 @xmath99 @xmath94 @xmath100^{t}$]@xmath96 , @xmath101",
    "@xmath102 @xmath103 @xmath94 @xmath104^{t}$]@xmath96 , and @xmath105 denotes the kronecker product . using ( [ eq : clnnbige ] ) , it can be concluded that provided the matrix @xmath106 is nonsingular , @xmath107 implies @xmath86 for all @xmath63 , and hence , the satisfaction of control objective .",
    "the matrix @xmath108 is nonsingular provided the graph has a spanning tree with the leader at the root @xcite . to facilitate the formulation of an optimization problem",
    ", the following section explores the functional dependence of the state - value functions for the network of agents .",
    "the dynamics for the open - loop neighborhood tracking error are    @xmath109 under the temporary assumption that each controller @xmath110 is an error - feedback controller , i.e. @xmath111 , the error dynamics are expressed as @xmath112 thus , the error trajectory @xmath113 , where @xmath114 denotes the initial time , depends on @xmath115 , @xmath116 .",
    "similarly , the error trajectory @xmath117 depends on @xmath118 .",
    "recursively , the trajectory @xmath113 depends on @xmath115 , and hence , on @xmath119 . thus , even if the controller for each agent is restricted to use local error feedback , the resulting error trajectories are interdependent .",
    "in particular , a change in the initial condition of one agent in the extended neighborhood causes a change in the error trajectories corresponding to all the extended neighbors .",
    "consequently , the value function corresponding to an infinite - horizon optimal control problem where each agent tries to minimize @xmath120 , where @xmath121 and @xmath122 are positive definite functions , is dependent on the error states of all the extended neighbors .",
    "since the steady - state controllers required for formation tracking are generally nonzero , quadratic total - cost optimal control problems result in infinite costs , and hence , are infeasible . in the following section ,",
    "relative steady - state controllers are derived to facilitate the formulation of a feasible optimal control problem .",
    "when the agents are perfectly tracking the desired trajectory in the desired formation , even though the states of all the agents are different , the time - derivatives of the states of all the agents are identical .",
    "hence , in steady state , the control signal applied by each agent must be such that the time derivatives of the states corresponding to the set of extended neighbors are identical .",
    "in particular , the relative control signal @xmath123 that will keep node @xmath34 in its desired relative position with respect to node @xmath124 , i.e. , @xmath125 , must be such that the time derivative of @xmath126 is the same as the time derivative of @xmath127 .",
    "using the dynamics of the agents from ( [ eq : clnndyn ] ) , and substituting the desired relative positions @xmath128 for the states @xmath126 , the relative control signals @xmath129 must satisfy @xmath130 the relative steady - state control signals can be expressed in an explicit form provided the following assumption is satisfied .",
    "[ ass : pseudo ] the matrix @xmath131 is full rank for all @xmath63 and for all @xmath7 ; furthermore , the relative steady - state control signal expressed as    @xmath132 satisfies ( [ eq : clnnreldyn ] ) along the desired trajectory , where @xmath133 , @xmath134 , @xmath135 for all @xmath7 , @xmath136 for all @xmath63 , and @xmath137 denotes a pseudoinverse of the matrix @xmath131 for all @xmath7 and for all @xmath63 .",
    "assumption [ ass : pseudo ] places restrictions on the control - effectiveness matrices .",
    "the matrices @xmath131 are full rank for a large class of systems including , but not limited to , kinematic wheels and fully actuated euler - lagrange systems with invertible inertia matrices .",
    "the second part of assumption 2 requires the existence of a feedback controller that can keep the system on the desired trajectory if the system starts on the desired trajectory .",
    "this assumption depends on the systems , the network , the desired formation , and the desired trajectory ; hence , insights into its satisfaction are hard to obtain in general .",
    "the satisfaction of this assumption needs to be verified on a case - by - case basis .",
    "for example , consider a kinematic wheel modeled as @xmath138 in this case , provided the formation satisfies @xmath139 , that is , the target formation is such that all the kinematic wheels have the same steering angle , the functions @xmath140 and @xmath141 can be computed as @xmath142 and @xmath143 .",
    "the relative steady - state control is then @xmath144 , which satisfies @xmath145 , and hence , assumption [ ass : pseudo ] holds .    to facilitate the formulation of an optimal formation tracking problem ,",
    "define the control errors @xmath146 as @xmath147 the control errors @xmath148 are treated as the design variables in the remainder of this paper . since the control errors @xmath148 are designed and the controllers @xmath149 are implemented in practice , it is essential to invert the relationship in ( [ eq : clnnmu_i ] ) . to facilitate the inversion ,",
    "let @xmath150 , where @xmath151 .",
    "let @xmath152 be a bijective map such that @xmath153 . for notational brevity ,",
    "let @xmath154 denote the concatenated vector @xmath155^{t}$ ] , let @xmath156 denote the concatenated vector @xmath157^{t}$ ] , let @xmath158 denote @xmath159 , let @xmath160 denote @xmath161 , let @xmath162^{t}\\in\\mathbb{r}^{n\\left(s_{i}+1\\right)}$ ] , and let @xmath163^{t}\\in\\mathbb{r}^{ns_{i}}$ ] .",
    "then , the control error vectors @xmath164 can be expressed as    @xmath165    where the matrices @xmath166 are defined by @xmath167_{kl}=\\begin{cases } -a_{\\lambda_{i}^{k}\\lambda_{i}^{l}}g_{\\lambda_{i}^{k}\\lambda_{i}^{l}}\\left(x_{\\lambda_{i}^{l}}\\right ) , & \\forall l\\neq k,\\\\ \\sideset{}{^{\\lambda_{i}^{k}}}\\sum a_{\\lambda_{i}^{k}j}i_{m_{\\lambda_{i}^{k } } } , & \\forall",
    "l = k , \\end{cases}\\ ] ] where @xmath168 ,    and @xmath169 are defined as @xmath170^{t}.\\ ] ]    [ ass : clnnlgiinvertible]the matrix @xmath171 is invertible for all @xmath172 and for all @xmath63 .",
    "assumption [ ass : clnnlgiinvertible ] is a controllability - like condition .",
    "intuitively , assumption [ ass : clnnlgiinvertible ] requires the control effectiveness matrices to be compatible to ensure the existence of relative control inputs that allow the agents to follow the desired trajectory in the desired formation .",
    "assumption [ ass : clnnlgiinvertible ] depends on the systems , the network , the desired formation , and the desired trajectory ; hence , insights into its satisfaction are hard to obtain in general .",
    "the satisfaction of this assumption needs to be verified on a case - by - case basis .",
    "for example , consider the kinematic wheel in ( [ eq : clnnwheel ] ) . provided",
    "the formation satisfies @xmath139 , that is , the target formation is such that all the kinematic wheels have the same steering angle , we have @xmath173 and hence , the matrices @xmath174 are given by @xmath167_{kl}=\\begin{cases } -a_{\\lambda_{i}^{k}\\lambda_{i}^{l}}i_{2 } , & \\forall l\\neq k,\\\\ \\sideset{}{^{\\lambda_{i}^{k}}}\\sum a_{\\lambda_{i}^{k}j}i_{2 } , & \\forall l = k , \\end{cases}\\ ] ] it can be shown that @xmath175 , where @xmath176 denotes the laplacian matrix corresponding to the subgraph @xmath177 .",
    "hence , the graph connectivity condition ensures that the matrices @xmath174 are invertible , and in this specific case , assumption [ ass : clnnlgiinvertible ] holds .",
    "using assumption [ ass : clnnlgiinvertible ] , the control vectors can be expressed as @xmath178 let @xmath179 denote the @xmath180^th^ block row of @xmath181 .",
    "then , the controllers @xmath110 can be implemented as @xmath182 and for any @xmath79 , @xmath183 using ( [ eq : clnnu_i ] ) and ( [ eq : clnnu_j ] ) , the error and the state dynamics for the agents can be represented as    @xmath184    and    @xmath185    where @xmath186 @xmath187 @xmath188 and @xmath189 .",
    "let @xmath190 and @xmath191 denote the trajectories of ( [ eq : clnne_idot ] ) and ( [ eq : clnnx_idot ] ) , respectively , with the initial time @xmath114 , initial condition @xmath192 , and policies @xmath193 , and let @xmath194^{t}$ ] .",
    "define the cost functionals @xmath195 where @xmath196 denote the local costs defined as @xmath197 where @xmath198 are positive definite functions , and @xmath199 are constant positive definite matrices .",
    "the objective of each agent is to minimize the cost functional in ( [ eq : clnnji ] ) . to facilitate the definition of a feedback - nash equilibrium solution ,",
    "define the value functions @xmath200 as @xmath201 where @xmath202 denotes the total cost - to - go for agent @xmath34 under the policies @xmath203 , when the sub - graph @xmath177 starts from the state @xmath204 .",
    "note that the value functions in ( [ eq : clnnv_i ] ) are time - invariant because the dynamical systems @xmath205 and @xmath206 together form an autonomous dynamical system .",
    "a graphical feedback - nash equilibrium solution within the subgraph @xmath177 is defined as the tuple of policies @xmath207 such that the value functions in ( [ eq : clnnv_i ] ) satisfy @xmath208 for all @xmath65 , for all @xmath209 and for all admissible policies @xmath210 .",
    "provided a feedback - nash equilibrium solution exists and the value functions ( [ eq : clnnv_i ] ) are continuously differentiable for all @xmath63 , the feedback - nash equilibrium value functions can be characterized in terms of the following system of hj equations : @xmath211 where @xmath212 is defined as @xmath213 .",
    "[ thm : clnnnessuf]provided a feedback - nash equilibrium solution exists and that the value functions in ( [ eq : clnnv_i ] ) are continuously differentiable , the system of hj equations in ( [ eq : clnnhjb ] ) constitutes a necessary and sufficient condition for @xmath207 to be a feedback - nash equilibrium solution within the subgraph @xmath177 .",
    "consider the cost functional in ( [ eq : clnnji ] ) , and assume that all the extended neighbors of the @xmath77 agent follow their feedback - nash equilibrium policies .",
    "the value function corresponding to any admissible policy @xmath214 can be expressed as @xmath215^{t}\\right)=\\\\ \\intop_{t}^{\\infty}r_{i}\\left(h_{ei}^{\\overline{\\mu}_{i},\\mu_{\\mathcal{s}_{-i}}^{*}}\\left(\\sigma , t,\\mathcal{e}_{i}\\right),\\overline{\\mu}_{i}\\left(\\mathcal{h}_{i}^{\\overline{\\mu}_{i},\\mu_{\\mathcal{s}_{-i}}^{*}}\\left(\\sigma , t,\\mathcal{e}_{i}\\right)\\right)\\right)\\textnormal{d}\\sigma.\\end{gathered}\\ ] ] treating the dependence on @xmath216 as explicit time dependence define @xmath217^{t}\\right),\\label{eq : clnnvbari}\\ ] ] for all @xmath218 and for all @xmath14 . assuming that the optimal controller that minimizes ( [ eq : clnnji ] ) when all the extended neighbors follow their feedback - nash equilibrium policies exists , and that the optimal value function @xmath219 exists and is continuously differentiable , optimal control theory for single objective optimization problems ( cf .",
    "@xcite ) can be used to derive the following necessary and sufficient condition @xmath220 using ( [ eq : clnnvbari ] ) , the partial derivative with respect to the state can be expressed as @xmath221 for all @xmath218 and for all @xmath14 , and the partial derivative with respect to time can be expressed as @xmath222 for all @xmath218 and for all @xmath14 . substituting ( [ eq : clnnpartialvi1 ] ) and ( [ eq : clnnpartialvi2 ] ) into ( [ eq : clnnhjb1 ] ) and repeating the process for each @xmath34 , the system of hj equations in ( [ eq : clnnhjb ] ) is obtained .    minimizing the hj equations using the stationary condition , the feedback - nash equilibrium solution is expressed in the explicit form    @xmath223    for all @xmath209 , where @xmath224 , and @xmath225 . since an analytical solution of system of hj equations in ( [ eq : clnnhjb ] ) is generally infeasible to obtain , the feedback - nash value functions and the feedback - nash policies are approximated using parametric approximation schemes @xmath226 and @xmath227 , respectively , where @xmath228 and @xmath229 are parameter estimates .",
    "substitution of the approximations @xmath230 and @xmath231 in ( [ eq : clnnhjb ] ) leads to a set of bellman errors ( bes ) @xmath232 defined as @xmath233 approximation of the feedback - nash equilibrium policies is realized by tuning the estimates @xmath230 and @xmath231 so as to minimize the bes @xmath232 .",
    "however , computation of @xmath232 in ( [ eq : clnndeltai ] ) and @xmath129 in ( [ eq : clnnmu_i ] ) requires exact model knowledge . in the following",
    ", a cl - based system identifier is developed to relax the exact model knowledge requirement and to facilitate the implementation of model - based rl via be extrapolation ( cf .",
    "@xcite ) . in particular",
    ", the developed controllers do not require the knowledge of the system drift functions @xmath234 .",
    "on any compact set @xmath235 the function @xmath234 can be represented using a nn as @xmath236 for all @xmath7 , where @xmath237 denote the unknown output - layer nn weights , @xmath238 denotes a bounded nn basis function , @xmath239 denotes the function reconstruction error , and @xmath240 denotes the number of nn neurons . using the universal function approximation property of single layer nns , provided the rows of @xmath241 form a proper basis , there exist constant ideal weights @xmath242 and positive constants @xmath243 and @xmath244 such that @xmath245 and @xmath246 , where @xmath247 denotes the frobenius norm , i.e. , @xmath248 .    [ ass : clnnnnknown]the bounds @xmath249 and @xmath250 are known for all @xmath251    using an estimate @xmath252 of the weight matrix @xmath253 the function @xmath234 can be approximated by the function @xmath254 defined by @xmath255    based on ( [ eq : clnnfnn ] ) , an estimator for online identification of the drift dynamics is developed as @xmath256 where @xmath257 , and @xmath258 is a positive constant learning gain .",
    "the following assumption facilitates concurrent learning ( cl)-based system identification .",
    "@xcite [ ass : clnnsigmabar]a history stack containing recorded state - action pairs @xmath259 along with numerically computed state derivatives @xmath260 that satisfies @xmath261 is available a priori . in ( [ eq : clnnsingularcond ] ) , @xmath262 , @xmath263 are known positive constants , and @xmath264 denotes the minimum eigenvalue .",
    "the weight estimates @xmath265 are updated using the following cl - based update law : @xmath266 where @xmath267 , @xmath268 is a constant positive cl gain , and @xmath269 is a constant , diagonal , and positive definite adaptation gain matrix .    to facilitate the subsequent stability analysis , a candidate lyapunov function @xmath270 is selected as @xmath271 where @xmath272 and @xmath273 denotes the trace of a matrix . using ( [ eq : clnnsysid])-([eq : clnnthetahatdot ] ) , the identity @xmath274 , and the facts that @xmath275 @xmath276 @xmath277 and @xmath278 @xmath279 ( cf .",
    "* theorem 4.2.12 ) ) , the following bound on the time derivative of @xmath280 is established :    @xmath281    where @xmath282 . using ( [ eq : clnnv0 ] ) and ( [ eq : clnnv0dot ] ) , a lyapunov - based stability analysis can be used to show that @xmath265 converges exponentially to a neighborhood around @xmath242 .",
    "using the approximations @xmath283 for the functions @xmath234 , the bes in ( [ eq : clnndeltai ] ) can be approximated as @xmath284 in ( [ eq : clnnbe ] ) ,    @xmath285,\\\\ \\hat{f}_{ij}\\left(x_{i},\\hat{\\theta}_{i},x_{j},\\hat{\\theta}_{j}\\right)\\triangleq g_{i}^{+}\\left(x_{j}+x_{dij}\\right)\\hat{f}_{j}\\left(x_{j},\\hat{\\theta}_{j}\\right)\\\\ -g_{i}^{+}\\left(x_{j}+x_{dij}\\right)\\hat{f}_{i}\\left(x_{j}+x_{dij},\\hat{\\theta}_{i}\\right).\\end{gathered}\\ ] ] the approximations @xmath286 , @xmath287 , and @xmath288 are related to the original unknown functions as @xmath289 , @xmath290 , and @xmath291 , where @xmath292 , @xmath293 , and @xmath294 are @xmath295 terms that denote bounded function approximation errors .    using the approximations @xmath283 , an implementable form of the controllers in ( [ eq : clnnu_s_i ] )",
    "is expressed as @xmath296 using ( [ eq : clnnmu_s_i ] ) and ( [ eq : clnnu_s_i ] ) , an unmeasurable form of the virtual controllers implemented on the systems ( [ eq : clnne_idot ] ) and ( [ eq : clnnx_idot ] ) is given by    @xmath297",
    "on any compact set @xmath298 , the value functions can be represented as @xmath299 where @xmath300 are ideal nn weights , @xmath301 are nn basis functions and @xmath302 are function approximation errors . using the universal function approximation property of single layer nns ,",
    "provided @xmath303 forms a proper basis , there exist constant ideal weights @xmath304 and positive constants @xmath305 and @xmath306 such that @xmath307 , @xmath308 , and @xmath309 .    [ ass : clnnvfnnknown ] the constants @xmath310 @xmath311 and @xmath312 are known for all @xmath63 .    using ( [ eq : clnnmu_i * ] ) and ( [ eq : clnnv_i*nn ] ) , the feedback - nash equilibrium policies are    @xmath313 for all @xmath314 where @xmath315 and @xmath316 the value functions and the policies are approximated using nns as    @xmath317    where @xmath318 and @xmath319 are estimates of the ideal weights @xmath304 , introduced in ( [ eq : clnndeltai ] ) .",
    "a consequence of theorem [ thm : clnnnessuf ] is that the be provides an indirect measure of how close the estimates @xmath318 and @xmath319 are to the ideal weights @xmath304 . from a reinforcement learning perspective , each evaluation of the be along the system trajectory can be interpreted as experience gained by the critic , and each evaluation of the be at points not yet visited can be interpreted as simulated experience . in previous results such as @xcite , the critic",
    "is restricted to the experience gained ( in other words bes evaluated ) along the system state trajectory .",
    "the development in @xcite can be extended to employ simulated experience ; however , the extension requires exact model knowledge . in results such as @xcite ,",
    "the formulation of the be does not allow for simulation of experience .",
    "the formulation in ( [ eq : clnnbe ] ) employs the system identifier developed in section [ sec : clnnsysid ] to facilitate approximate evaluation of the be at off - trajectory points .    to simulate experience , a set of points @xmath320",
    "is selected corresponding to each agent @xmath34 , and the instantaneous be in ( [ eq : clnndeltai ] ) is approximated at the current state and at the selected points using ( [ eq : clnndeltahat_i ] ) .",
    "the approximation at the current state is denoted by @xmath321 and the approximation at the selected points is denoted by @xmath322 , where @xmath321 and @xmath322 are defined as @xmath323 note that once @xmath324 and @xmath126 are selected , the @xmath77 agent can compute the states of all the remaining agents in the sub - graph . for notational brevity ,",
    "the arguments to the functions @xmath325 , @xmath287 , @xmath326 , @xmath327 , @xmath288 , @xmath231 , @xmath328 , @xmath329 , and @xmath330 are suppressed hereafter .",
    "the critic uses simulated experience to update the value function weights using a least squares - based update law @xmath331 where @xmath332 , @xmath333 denotes the time - varying least - squares learning gain , @xmath334 denotes the saturation constant , @xmath335 and @xmath336 are constant positive learning gains . in ( [ eq : clnncriticupdate ] ) ,",
    "@xmath337 where for a function @xmath338 , the notation @xmath339 indicates evaluation at @xmath340 ; i.e. , @xmath341 .",
    "the actor updates the policy weights using the following update law derived based on the lyapunov - based stability analysis in section [ sec : stability - analysis ] .",
    "@xmath342 where @xmath343 are constant positive learning gains .",
    "the following assumption facilitates simulation of experience .",
    "[ ass : clnnclw]@xcite for each @xmath63 , there exists a finite set of points @xmath320 such that @xmath344 where @xmath345 denotes the minimum eigenvalue , and @xmath346 is a positive constant .",
    "to facilitate the stability analysis , the left hand side of ( [ eq : clnnhjb ] ) is subtracted from ( [ eq : clnnbe ] ) to express the bes in terms of the weight estimation errors as    @xmath347    where @xmath348 , @xmath349 , and @xmath350\\right)$ ] are block diagonal matrices .",
    "consider a set of extended neighbors @xmath351 corresponding to the @xmath352^th^ agent . to analyze asymptotic properties of the agents in @xmath353",
    "consider the following candidate lyapunov function @xmath354 where @xmath355 is defined as @xmath356^{t},\\ ] ] @xmath357 denotes the vectorization operator , and @xmath358 is defined as @xmath359^{t}\\right),\\label{eq : clnnv_ti}\\ ] ] for all @xmath360 and for all @xmath76 . since @xmath361 depends on @xmath11 only through uniformly bounded leader trajectories    , lemma 1 from @xcite can be used to show that @xmath362 is a positive definite and decrescent function . for some function @xmath363 .",
    "thus , the value function can be expressed as @xmath364 .",
    "then , @xmath361 can be alternatively defined as @xmath365 since @xmath366 is a uniformly bounded function of @xmath11 by assumption , lemma 1 from @xcite can be used to conclude that @xmath362 is a positive definite and decrescent function . ] thus , using lemma 4.3 from @xcite , the following bounds on the candidate lyapunov function in ( [ eq : clnnv_lp ] ) are established @xmath367 for all @xmath355 and for all @xmath76 , where @xmath368 are class @xmath369 functions .    to facilitate the stability analysis , given any compact ball @xmath370 of radius @xmath371 centered at the origin , a positive constant @xmath372",
    "is defined as @xmath373 where for any function @xmath374 , @xmath375 , the notation @xmath376 denotes @xmath377 and @xmath378 , @xmath379 , and @xmath380 are uniformly bounded state - dependent terms .",
    "the following sufficient gain conditions facilitate the subsequent stability analysis . @xmath381",
    "@xmath382 @xmath383 where @xmath384 , @xmath385 , and @xmath386 are uniformly bounded state - dependent terms .",
    "[ thm : clnnmain]provided assumptions [ ass : clnnleader ] - [ ass : clnnclw ] hold and the sufficient gain conditions in ( [ eq : clnnsuffcond1])-([eq : clnnsuffcond3 ] ) are satisfied , the controller in ( [ eq : clnnmuhatvhat ] ) along with the actor and critic update laws in ( [ eq : clnncriticupdate ] ) and ( [ eq : clnnactor update ] ) , and the system identifier in ( [ eq : clnnsysid ] ) along with the weight update laws in ( [ eq : clnnthetahatdot ] ) ensure that the local neighborhood tracking errors @xmath387 are ultimately bounded and that the policies @xmath231 converge to a neighborhood around the feedback - nash policies @xmath388 for all @xmath63 .",
    "the time derivative of the candidate lyapunov function in ( [ eq : clnnv_lp ] ) is given by @xmath389 using ( [ eq : clnnhjb ] ) , ( [ eq : clnnv0dot ] ) , ( [ eq : clnnmu_s_iunm ] ) , and ( [ eq : clnndeltahat_i ] ) , the update laws in ( [ eq : clnncriticupdate ] ) and ( [ eq : clnnactor update ] ) , and the definition of @xmath362 in ( [ eq : clnnv_ti ] ) , the derivative in ( [ eq : clnnvdot1 ] ) can be bounded as    @xmath390    let @xmath391 be a class @xmath369 function such that @xmath392 where @xmath393 are class @xmath369 functions such that @xmath394 .",
    "then , the lyapunov derivative can be bounded as @xmath395 for all @xmath396 such that @xmath397 and @xmath398 . using the bounds in ( [ eq : clnnvbounds ] ) ,",
    "the sufficient conditions in ( [ eq : clnnsuffcond1])-([eq : clnnsuffcond3 ] ) , and the inequality in ( [ eq : clnnvdot4 ] ) , theorem 4.18 in @xcite can be invoked to conclude that every trajectory @xmath399 satisfying @xmath400 , is bounded for all @xmath76 and satisfies @xmath401    since the choice of the subgraph @xmath351 was arbitrary , the neighborhood tracking errors @xmath387 are ultimately bounded for all @xmath63 . furthermore , the weight estimates @xmath319 converge to a neighborhood of the ideal weights @xmath304 ; hence , invoking theorem [ thm : clnnnessuf ] , the policies @xmath231 converge to a neighborhood of the feedback - nash equilibrium policies @xmath388 for all @xmath63 .",
    "this section provides a simulation example to demonstrate the applicability of the developed technique .",
    "the agents are assumed to have the communication topology as shown in figure [ fig : clnntopology ] with unit pinning gains and edge weights .",
    "the motion of the agents is described by identical nonlinear one - dimensional dynamics of the form ( [ eq : clnndyn ] ) where @xmath402 , and @xmath403 for all @xmath404 the ideal values of the unknown parameters are selected to be @xmath405 @xmath406 @xmath407 @xmath408 and @xmath409 , and @xmath410 , @xmath408 @xmath411 @xmath411 and @xmath411 for @xmath412 , respectively .",
    "the agents start at @xmath413 for all @xmath34 , and their final desired locations with respect to each other are given by @xmath414 @xmath415 @xmath416 and @xmath417 .",
    "the leader traverses an exponentially decaying trajectory @xmath418 .",
    "the desired positions of agents 1 and 3 with respect to the leader are @xmath419 and @xmath420 , respectively .",
    "communication topology : a network containing five agents . ]    ]    for each agent @xmath421 five values of @xmath387 , three values of @xmath126 , and three values of errors corresponding to all the extended neighbors are selected for be extrapolation , resulting in @xmath422 total values of @xmath204 .",
    "all agents estimate the unknown drift parameters using history stacks containing thirty points recorded online using a singular value maximizing algorithm ( cf .",
    "@xcite ) , and compute the required state derivatives using a fifth order savitzky - golay smoothing filter ( cf . @xcite ) .    ]",
    "trajectories of the control input and the relative control error for all agents for the one - dimensional example.,title=\"fig:\"]trajectories of the control input and the relative control error for all agents for the one - dimensional example.,title=\"fig : \" ]    value function weights and drift dynamics parameters estimates for agent 1 for the one - dimensional example .",
    "the dotted lines in the drift parameter plot are the ideal values of the drift parameters.,title=\"fig:\"]value function weights and drift dynamics parameters estimates for agent 1 for the one - dimensional example .",
    "the dotted lines in the drift parameter plot are the ideal values of the drift parameters.,title=\"fig : \" ]    figures [ fig : clnnx ] - [ fig : clnnuandmu ] show the tracking error , the state trajectories compared with the desired trajectories , and the control inputs for all the agents demonstrating convergence to the desired formation and the desired trajectory .",
    "note that agents 2 , 4 , and 5 do not have a communication link to the leader , nor do they know their desired relative position with respect to the leader .",
    "the convergence to the desired formation is achieved via cooperative control based on decentralized objectives .",
    "figure [ fig : clnnwcth1 ] shows the evolution and convergence of the value function weights and the parameters estimates for the drift dynamics for agent 1 .",
    "the errors between the ideal drift parameters and their respective estimates are large , however , as demonstrated by figure [ fig : clnne ] , the resulting dynamics are sufficiently close to the actual dynamics for the developed technique to generate stabilizing policies .",
    "it is unclear whether the value function and the policy weights converge to their ideal values .",
    "since an alternative method to solve this problem is not available to the best of the author s knowledge , a comparison between value function and policy weight estimates and their corresponding ideal values is infeasible .",
    "a simulation - based actor - critic - identifier architecture is developed to obtain feedback - nash equilibrium solutions to a class of differential graphical games__. _ _ it is established that in a cooperative game based on minimization of the local neighborhood tracking errors , the value function corresponding to an agent depends on information obtained from all their extended neighbors .",
    "a set of coupled hj equations are developed that serve as necessary and sufficient conditions for feedback - nash equilibrium , and closed - form expressions for the feedback - nash equilibrium policies are developed based on the hj equations .",
    "the fact that the developed technique requires each agent to communicate with all of its extended neighbors motivates the search for a decentralized method to generate feedback - nash equilibrium policies .",
    "h.  modares , f.  l. lewis , and m .- b .",
    "naghibi - sistani , `` integral reinforcement learning and experience replay for adaptive optimal control of partially - unknown constrained - input continuous - time systems , '' _ automatica _ , vol .",
    "50 , no .  1 ,",
    "pp . 193202 , 2014 .",
    "h.  zhang , l.  cui , and y.  luo , `` near - optimal control for nonzero - sum differential games of continuous - time nonlinear systems using single - network adp , '' _ ieee trans .",
    "_ , vol .",
    "43 , no .  1 ,",
    "pp . 206216 , 2013 .",
    "a.  heydari and s.  balakrishnan , `` finite - horizon control - constrained nonlinear optimal control using single network adaptive critics , '' _ ieee trans .",
    "neural netw .",
    "_ , vol .  24 , no .  1 ,",
    "pp . 145157 , 2013 .",
    "r.  isaacs , _ differential games : a mathematical theory with applications to warfare and pursuit , control and optimization _ , ser .",
    "dover books on mathematics.1em plus 0.5em minus 0.4emdover publications , 1999 .",
    "d.  vrabie and f.  l. lewis , `` integral reinforcement learning for online computation of feedback nash strategies of nonzero - sum differential games , '' in _ proc .",
    "ieee conf .",
    "control _ , 2010 , pp .",
    "30663071 .        x.",
    "lin and c.  g. cassandras , `` an optimal control approach to the multi - agent persistent monitoring problem in two - dimensional spaces , '' _ ieee trans .",
    "autom . control _",
    "60 , no .  6 , pp . 16591664 , june 2015",
    ".                k.  g. vamvoudakis , f.  l. lewis , and g.  r. hudas , `` multi - agent differential graphical games : online adaptive learning solution for synchronization with optimality , '' _ automatica _ , vol .",
    "48 , no .  8 , pp .",
    "1598  1611 , 2012 .",
    "h.  zhang , t.  feng , g.  h. yang , and h.  liang , `` distributed cooperative optimal control for multiagent systems on directed graphs : an inverse optimal approach , '' _ ieee trans .",
    "_ , vol .  45 , no .  7 , pp",
    "13151326 , july 2015 .",
    "e.  semsar - kazerooni and k.  khorasani , `` optimal consensus algorithms for cooperative team of agents subject to partial information , '' _ automatica _ , vol .",
    "44 , no .  11 , pp .",
    "2766  2777 , 2008 .",
    "l.  magni and r.  scattolini , `` stabilizing decentralized model predictive control of nonlinear systems , '' _ automatica _ , vol .",
    "42 , no .  7 , pp .",
    "1231  1236 , 2006 .",
    "http://www.sciencedirect.com/science/article/pii/s000510980600104x      h.  zhang , j.  zhang , g.  h. yang , and y.  luo , `` leader - based optimal coordination control for the consensus problem of multiagent differential games via fuzzy adaptive dynamic programming , '' _ ieee trans .",
    "fuzzy syst .",
    "_ , vol .  23 , no .  1 , pp . 152163 , february 2015 .",
    "r.  kamalapurkar , h.  t. dinh , p.  walters , and w.  e. dixon , `` approximate optimal cooperative decentralized control for consensus in a topological network of agents with uncertain nonlinear dynamics , '' in _ proc . am .",
    "control conf .",
    "_ , washington , dc , jun .",
    "2013 , pp . 13221327 .",
    "g.  chowdhary , t.  yucelen , m.  mhlegg , and e.  n. johnson , `` concurrent learning adaptive control of linear systems with exponentially convergent bounds , '' _ int . j. adapt .",
    "control signal process .",
    "_ , vol .",
    "27 , no .  4 , pp .",
    "280301 , 2013 .",
    "rushikesh kamalapurkar received his m.s . and his ph.d . degree in 2011 and 2014 , respectively , from the mechanical and aerospace engineering department at the university of florida .",
    "after working for a year as a postdoctoral research fellow with dr .",
    "warren e. dixon , he was selected as the 2015 - 16 mae postdoctoral teaching fellow . in 2016 he joined the school of mechanical and aerospace engineering at the oklahoma state university as an assistant professor .",
    "his primary research interest has been intelligent , learning - based control of uncertain nonlinear dynamical systems .",
    "his work has been recognized by the 2015 university of florida department of mechanical and aerospace engineering best dissertation award , and the 2014 university of florida department of mechanical and aerospace engineering outstanding graduate research award .",
    "justin r. klotz received the ph.d .",
    "degree in mechanical engineering from the university of florida , gainesville , fl , usa , in 2015 , where he was awarded the science , mathematics and research for transformation ( smart ) scholarship , sponsored by the department of defense .",
    "his research interests include the development of lyapunov - based techniques for reinforcement learning - based control , switching control methods , delay - affected control , and trust - based cooperative control .",
    "patrick walters received the ph.d .",
    "degree in mechanical engineering from the university of florida , gainesville , fl , usa , in 2015 .",
    "his research interests include reinforcement learning - based feedback control , approximate dynamic programming , and robust control of uncertain nonlinear systems with a focus on the application of underwater vehicles .",
    "warren e. dixon received his ph.d . in 2000 from the department of electrical and computer engineering from clemson university .",
    "he was selected as a eugene p. wigner fellow at oak ridge national laboratory ( ornl ) . in 2004",
    ", he joined the university of florida in the mechanical and aerospace engineering department .",
    "his main research interest has been the development and application of lyapunov - based control techniques for uncertain nonlinear systems .",
    "he has published 3 books , over a dozen chapters , and approximately 125 journal and 230 conference papers .",
    "his work has been recognized by the 2015 & 2009 american automatic control council ( aacc ) o. hugo schuck ( best paper ) award , the 2013 fred ellersick award for best overall milcom paper , a 2012 - 2013 university of florida college of engineering doctoral dissertation mentoring award , the 2011 american society of mechanical engineers ( asme ) dynamics systems and control division outstanding young investigator award , the 2006 ieee robotics and automation society ( ras ) early academic career award , an nsf career award , the 2004 department of energy outstanding mentor award , and the 2001 ornl early career award for engineering achievement .",
    "he is a fellow of asme and ieee and is an ieee control systems society ( css ) distinguished lecturer .",
    "he has served as the director of operations for the executive committee of the ieee css board of governors and as a member of the u.s .",
    "air force science advisory board .",
    "he is currently or formerly an associate editor for asme journal of journal of dynamic systems , measurement and control , automatica , ieee control systems magazine , ieee transactions on systems man and cybernetics : part b cybernetics , and the international journal of robust and nonlinear control ."
  ],
  "abstract_text": [
    "<S> this paper seeks to combine differential game theory with the actor - critic - identifier architecture to determine forward - in - time , approximate optimal controllers for formation tracking in multi - agent systems , where the agents have uncertain heterogeneous nonlinear dynamics . </S>",
    "<S> a continuous control strategy is proposed , using communication feedback from extended neighbors on a communication topology that has a spanning tree . </S>",
    "<S> a model - based reinforcement learning technique is developed to cooperatively control a group of agents to track a trajectory in a desired formation . </S>",
    "<S> simulation results are presented to demonstrate the performance of the developed technique . </S>"
  ]
}