{
  "article_text": [
    "importance weighting is a crucially important tool used in many areas of machine learning , and specifically online learning with partial feedback .",
    "while most work assumes that importance weights are readily available or can be computed with little effort during runtime , this is often not the case in many practical settings , even when one has cheap sample access to the distribution generating the observations . among other cases ,",
    "such situations may arise when observations are generated by complex hierarchical sampling schemes , probabilistic programs , or , more generally , black - box generative models . in this paper",
    ", we propose a simple and efficient sampling scheme called _ geometric resampling _",
    "( ` gr ` ) to compute reliable estimates of importance weights _ using only sample access_.    our main motivation is studying a specific online learning algorithm whose practical applicability in partial - feedback settings had long been hindered by the problem outlined above . specifically , we consider the well - known _ follow - the - perturbed - leader _ ( ` fpl ` ) prediction method that maintains implicit sampling distributions that usually can not be expressed in closed form . in this paper",
    ", we endow ` fpl`with our geometric resampling scheme to construct the first known computationally efficient reduction from offline to online combinatorial optimization under an important partial - information scheme known as _ semi - bandit feedback_. in the rest of this section , we describe our precise setting , present related work and outline our main results .",
    "we consider a special case of online linear optimization known as online combinatorial optimization ( see figure  [ fig : protocol ] ) . in every round @xmath0 of this sequential decision problem",
    ", the learner chooses an _",
    "@xmath1 from the finite action set @xmath2 , where @xmath3 holds for all @xmath4 . at the same time",
    ", the environment fixes a loss vector @xmath5^d$ ] and the learner suffers loss @xmath6 .",
    "the goal of the learner is to minimize the cumulative loss @xmath7 . as usual in the literature of online optimization ( @xcite )",
    ", we measure the performance of the learner in terms of the _ regret _ defined as @xmath8 that is , the gap between the total loss of the learning algorithm and the best fixed decision in hindsight . in the current paper , we focus on the case of _ non - oblivious _ ( or _ adaptive _ ) environments , where we allow the loss vector @xmath9 to depend on the previous decisions @xmath10 in an arbitrary fashion . since it is well - known that no deterministic algorithm can achieve sublinear regret under such weak assumptions",
    ", we will consider learning algorithms that choose their decisions in a randomized way .",
    "for such learners , another performance measure that we will study is the _ expected regret _ defined as @xmath11 } = { \\mathbb{e}\\left[\\sum_{t=1}^t{\\bm{v}}_t{^\\mathsf{\\scriptscriptstyle t}}{\\bm\\ell}_t\\right ] } - \\min_{{\\bm{v}}\\in{\\mathcal{s } } } { \\mathbb{e}\\left[\\sum_{t=1}^t{\\bm{v}}{^\\mathsf{\\scriptscriptstyle t}}{\\bm\\ell}_t\\right]}.\\ ] ]    the framework described above is general enough to accommodate a number of interesting problem instances such as path planning , ranking and matching problems , finding minimum - weight spanning trees and cut sets .",
    "accordingly , different versions of this general learning problem have drawn considerable attention in the past few years .",
    "these versions differ in the amount of information made available to the learner after each round @xmath12 . in the simplest",
    "setting , called the _ full - information _ setting , it is assumed that the learner gets to observe the loss vector @xmath9 regardless of the choice of @xmath1 . as this assumption",
    "does not hold for many practical applications , it is more interesting to study the problem under _ partial - information _ constraints , meaning that the learner only gets some limited feedback based on its own decision . in the current paper , we focus on a more realistic partial - information scheme known as _ semi - bandit feedback _",
    "@xcite where the learner only observes the components @xmath13 of the loss vector for which @xmath14 , that is , the losses associated with the components selected by the learner . and @xmath13 are the @xmath15 components of the vectors @xmath1 and @xmath9 , respectively . ]",
    "the most well - known instance of our problem is the _ multi - armed bandit _ problem considered in the seminal paper of @xcite : in each round of this problem , the learner has to select one of @xmath16 _ arms _ and minimize regret against the best fixed arm while only observing the losses of the chosen arms . in our framework , this setting corresponds to setting @xmath17 and @xmath18 . among other contributions concerning this problem ,",
    "@xcite  propose an algorithm called ` exp3`(exploration and exploitation using exponential weights ) based on constructing loss estimates @xmath19 for each component of the loss vector and playing arm @xmath20 with probability proportional to @xmath21 at time @xmath12 , where @xmath22 is a parameter of the algorithm , usually called the learning rate .",
    "however , this modification is not needed when one is concerned with the total expected regret , see , e.g. , ( * ? ? ?",
    "* section  3.1 ) . ] .",
    "this algorithm is essentially a variant of the exponentially weighted average ( ` ewa ` ) forecaster ( a variant of weighted majority algorithm of @xcite , and aggregating strategies of @xcite , also known as ` hedge`by @xcite ) . besides proving that the _ expected _ regret of ` exp3`is @xmath23 ,",
    "@xcite  also provide a general lower bound of @xmath24 on the regret of any learning algorithm on this particular problem .",
    "this lower bound was later matched by a variant of the implicitly normalized forecaster ( ` inf ` ) of @xcite by using the same loss estimates in a more refined way .",
    "@xcite also show bounds of @xmath25 on the regret that hold with probability at least @xmath26 , uniformly for any @xmath27 .",
    "the most popular example of online learning problems with actual combinatorial structure is the shortest path problem first considered by @xcite in the full information scheme .",
    "the same problem was considered by @xcite , who proposed an algorithm that works with semi - bandit information . since then , we have come a long way in understanding the `` price of information '' in online combinatorial optimization  see @xcite for a complete overview of results concerning all of the information schemes considered in the current paper .",
    "the first algorithm directly targeting general online combinatorial optimization problems is due to @xcite : their method named ` component hedge ` guarantees an optimal regret of @xmath28 in the full information setting .",
    "as later shown by @xcite , this algorithm is an instance of a more general algorithm class known as online stochastic mirror descent ( ` osmd ` ) .",
    "taking the idea one step further , @xcite also show that ` osmd`-based methods can also be used for proving expected regret bounds of @xmath29 for the semi - bandit setting , which is also shown to coincide with the minimax regret in this setting . for completeness",
    ", we note that the ` ewa`forecaster is known to attain an expected regret of @xmath30 in the full information case and @xmath31 in the semi - bandit case .",
    "while the results outlined above might suggest that there is absolutely no work left to be done in the full information and semi - bandit schemes , we get a different picture if we restrict our attention to _ computationally efficient _ algorithms .",
    "first , note that methods based on exponential weighting of each decision vector can only be efficiently implemented for a handful of decision sets @xmath32see @xcite and @xcite for some examples .",
    "furthermore , as noted by @xcite , ` osmd`-type methods can be efficiently implemented by convex programming if the convex hull of the decision set can be described by a polynomial number of constraints .",
    "details of such an efficient implementation are worked out by @xcite , whose algorithm runs in @xmath33 time , which can still be prohibitive in practical applications . while @xcite list some further examples where ` osmd`can be implemented efficiently , we conclude that there is no general efficient algorithm with near - optimal performance guarantees for learning in combinatorial semi - bandits .    the follow - the - perturbed - leader ( ` fpl ` ) prediction method ( first proposed by @xcite and later rediscovered by @xcite ) offers a computationally efficient solution for the online combinatorial optimization problem given that the _",
    "static _ combinatorial optimization problem @xmath34 admits computationally efficient solutions for any @xmath35 .",
    "the idea underlying ` fpl`is very simple : in every round @xmath12 , the learner draws some random perturbations @xmath36 and selects the action that minimizes the perturbed total losses : @xmath37 despite its conceptual simplicity and computational efficiency , ` fpl`have been relatively overlooked until very recently , due to two main reasons :    * the best known bound for ` fpl`in the full information setting is @xmath38 , which is worse than the bounds for both ` ewa`and ` osmd`that scale only logarithmically with @xmath39 .",
    "* considering bandit information , no efficient ` fpl`-style algorithm is known to achieve a regret of @xmath40 .",
    "on one hand , it is relatively straightforward to prove @xmath41 bounds on the expected regret for an efficient ` fpl`-variant ( see , e.g. , @xcite and @xcite ) .",
    "@xcite proved bounds of @xmath23 in the @xmath16-armed bandit setting , however , the proposed algorithm requires @xmath42 numerical operations per round .",
    "the main obstacle for constructing a computationally efficient ` fpl`-variant that works with partial information is precisely the lack of closed - form expressions for importance weights . in the current paper , we address the above two issues and show that an efficient ` fpl`-based algorithm using independent exponentially distributed perturbations can achieve as good performance guarantees as ` ewa`in online combinatorial optimization .",
    "our work contributes to a new wave of positive results concerning ` fpl ` . besides the reservations towards ` fpl`mentioned above , the reputation of ` fpl`has been also suffering from the fact that the nature of regularization arising from perturbations is not as well - understood as the explicit regularization schemes underlying ` osmd`or ` ewa ` .",
    "very recently , @xcite have shown that ` fpl`implements a form of strongly convex regularization over the convex hull of the decision space .",
    "furthermore , @xcite showed that ` fpl`run with a specific perturbation scheme can be regarded as a relaxation of the minimax algorithm .",
    "another recently initiated line of work shows that intuitive _",
    "parameter - free _ variants of ` fpl`can achieve excellent performance in full - information settings ( @xcite and @xcite ) .      in this paper",
    ", we propose a loss - estimation scheme called geometric resampling to efficiently compute importance weights for the observed components of the loss vector .",
    "building on this technique and the ` fpl`principle , resulting in _ an efficient algorithm for regret minimization under semi - bandit feedback_. besides this contribution , our techniques also enable us to improve the best known regret bounds of ` fpl`in the full information case .",
    "we prove the following results concerning variants of our algorithm :    * a bound of @xmath43 on the expected regret under semi - bandit feedback ( theorem  [ thm : bandit_exp ] ) , * a bound of @xmath44 on the regret that holds with probability at least @xmath26 , uniformly for all @xmath45 under semi - bandit feedback ( theorem  [ thm : highprob ] ) , * a bound of @xmath46 on the expected regret under full information ( theorem  [ thm : fullinfo ] ) .",
    "we also show that both of our semi - bandit algorithms access the optimization oracle @xmath47 times over @xmath48 rounds with high probability , increasing the running time only by a factor of @xmath39 compared to the full - information variant .",
    "notably , our results close the gaps between the performance bounds of ` fpl`and ` ewa`under both full information and semi - bandit feedback .",
    "table  [ tab : table ] puts our newly proven regret bounds into context .",
    ".upper bounds on the regret of various algorithms for online combinatorial optimization , up to constant factors .",
    "the third row roughly describes the computational efficiency of each algorithm  see the text for details .",
    "new results are presented in boldface . [ cols=\"<,<,<,<\",options=\"header \" , ]",
    "in this section , we introduce the main idea underlying geometric resampling in the specific context of @xmath16-armed bandits where @xmath17 , @xmath18 and the learner has access to the basis vectors @xmath49 as its decision set @xmath32 . in this",
    "setting , components of the decision vector are referred to as _",
    "arms_. for ease of notation , define @xmath50 as the unique arm such that @xmath51 and @xmath52 as the sigma - algebra induced by the learner s actions and observations up to the end of round @xmath53 . using this notation ,",
    "we define @xmath54}$ ] .",
    "most bandit algorithms rely on feeding some loss estimates to a sequential prediction algorithm .",
    "it is commonplace to consider _ importance - weighted _ loss estimates of the form @xmath55 for all @xmath56 such that @xmath57 .",
    "it is straightforward to show that @xmath58 is an unbiased estimate of the loss @xmath13 for all such @xmath56 . otherwise , when @xmath59 , we set @xmath60 , which gives @xmath61 } = 0 \\le { \\ell}_{t , i}$ ] .    to our knowledge ,",
    "all existing bandit algorithms operating in the non - stochastic setting utilize some version of the importance - weighted loss estimates described above .",
    "this is a very natural choice for algorithms that operate by first computing the probabilities @xmath62 and then sampling @xmath50 from the resulting distributions .",
    "while many algorithms fall into this class ( including the ` exp3`algorithm of @xcite , the ` green`algorithm of @xcite and the ` inf`algorithm of @xcite , one can think of many other algorithms where the distribution @xmath63 is specified implicitly and thus importance weights are not readily available . arguably , `",
    "fpl`is the most important online prediction algorithm that operates with implicit distributions that are notoriously difficult to compute in closed form . to overcome this difficulty",
    ", we propose a different loss estimate that can be efficiently computed _ even when @xmath64 is not available for the learner_.    our estimation procedure dubbed geometric resampling ( ` gr ` ) is based on the simple observation that , even though @xmath65 might not be computable in closed form , one can simply generate a geometric random variable with expectation @xmath66 by repeated sampling from @xmath63 .",
    "specifically , we propose the following procedure to be executed in round @xmath12 :    observe that @xmath67 generated this way is a geometrically distributed random variable given @xmath50 and @xmath52 .",
    "consequently , we have @xmath68 } = 1/{p}_{t , i_t}$ ] .",
    "we use this property to construct the estimates @xmath69 for all arms @xmath20 .",
    "we can easily show that the above estimate is unbiased whenever @xmath57 : @xmath70 } & = \\sum_{j } p_{t , j } { \\mathbb{e}\\left[\\left.{{\\widehat}{\\ell}}_{t , i}\\right|{\\mathcal{f}}_{t-1},i_t = j\\right ] } \\\\ & = p_{t , i } { \\mathbb{e}\\left[{\\ell}_{t , i } k_t\\left|{\\mathcal{f}}_{t-1},i_t = i\\right.\\right ] } \\\\ & = p_{t , i } { \\ell}_{t , i } { \\mathbb{e}\\left[k_t\\left|{\\mathcal{f}}_{t-1},i_t = i\\right.\\right ] } \\\\ & = { \\ell}_{t , i}. \\end{split}\\ ] ] notice that the above procedure produces @xmath71 almost surely whenever @xmath72 , giving @xmath73 } = 0 $ ] for such @xmath56 .",
    "one practical concern with the above sampling procedure is that its worst - case running time is unbounded : while the expected number of necessary samples @xmath67 is clearly @xmath16 , the actual number of samples might be much larger . in the next section ,",
    "we offer a remedy to this problem , as well as generalize the approach to work in the combinatorial semi - bandit case .",
    "in this section , we present our main result : an efficient reduction from offline to online combinatorial optimization under semi - bandit feedback .",
    "the most critical element in our technique is extending the geometric resampling idea to the case of combinatorial action sets . for defining the procedure ,",
    "let us assume that we are running a randomized algorithm mapping histories to probability distributions over the action set @xmath32 : letting @xmath52 denote the sigma - algebra induced by the history of interaction between the learner and the environment , the algorithm picks action @xmath4 with probability @xmath74}$ ] . also introducing @xmath75}$ ]",
    ", we can define the counterpart of the standard importance - weighted loss estimates of equation  [ eq : oldest ] as the vector @xmath76 with components @xmath77 again , the problem with these estimates is that for many algorithms of practical interest , the importance weights @xmath78 can not be computed in closed form .",
    "we now extend the geometric resampling procedure defined in the previous section to estimate the importance weights in an efficient manner .",
    "one adjustment we make to the procedure presented in the previous section is capping off the number of samples at some finite @xmath79 .",
    "while this capping obviously introduces some bias , we will show later that for appropriate values of @xmath80 , this bias does not hurt the performance of the overall learning algorithm too much .",
    "thus , we define the geometric resampling procedure for combinatorial semi - bandits as follows :    based on the random variables output by the ` gr`procedure , we construct our loss - estimate vector @xmath81 with components @xmath82 for all @xmath83 . since @xmath84 are nonzero only for coordinates for which @xmath13 is observed , these estimates are well - defined .",
    "it also follows that the sampling procedure can be terminated once for every @xmath20 with @xmath14 , there is a copy @xmath85 such that @xmath86 .",
    "now everything is ready to define our algorithm : ` fpl+gr ` , standing for follow - the - perturbed - leader with geometric resampling . defining @xmath87 , at time step @xmath12 `",
    "fpl+gr`draws the components of the perturbation vector @xmath88 independently from a standard exponential distribution and selects action @xmath89 where @xmath22 is a parameter of the algorithm . as we mentioned earlier , the distribution @xmath63 , while implicitly specified by @xmath88 and the estimated cumulative losses @xmath90 , can not usually be expressed in closed form for ` fpl`.-dimensional simplex : in this case , ` fpl`is known to be equivalent with ` ewa`see , e.g. , @xcite for further discussion .",
    "] however , sampling the actions @xmath91 can be carried out by drawing additional perturbation vectors @xmath92 independently from the same distribution as @xmath88 and then solving a linear optimization task .",
    "we emphasize that the above additional actions are _",
    "never actually played by the algorithm _ , but are only necessary for constructing the loss estimates .",
    "the power of ` fpl+gr`is that , unlike other algorithms for combinatorial semi - bandits , its implementation only requires access to a linear optimization oracle over @xmath32 .",
    "we point the reader to section  [ sec : runningtime ] for a more detailed discussion of the running time of ` fpl+gr ` .",
    "pseudocode for ` fpl+gr`is shown on as algorithm  [ alg : fplgr ] .",
    "* input * : @xmath2 , @xmath93 , @xmath94 * initialization * : @xmath95    as we will show shortly , ` fpl+gr`as defined above comes with strong performance guarantees that hold _ in expectation_. one can think of several possible ways to robustify ` fpl+gr`so that it provides bounds that hold with high probability .",
    "one possible path is to follow @xcite and define the loss - estimate vector @xmath96 with components @xmath97 for some @xmath98 .",
    "the obvious problem with this definition is that it requires perfect knowledge of the importance weights @xmath78 for all @xmath20 .",
    "while it is possible to extend geometric resampling developed in the previous sections to construct a reliable proxy to the above loss estimate , there are several downsides to this approach .",
    "first , observe that one would need to obtain estimates of @xmath99 for every single @xmath20even for the ones for which @xmath100 .",
    "due to this necessity , there is no hope to terminate the sampling procedure in reasonable time .",
    "second , reliable estimation requires multiple samples of @xmath101 , where the sample size has to explicitly depend on the desired confidence level .",
    "thus , we follow a different path : motivated by the work of @xcite , we propose to use a loss - estimate vector @xmath102 with components of the form @xmath103 with an appropriately chosen @xmath104 . then , defining @xmath105 , we propose a variant of ` fpl+gr`that simply replaces @xmath90 by @xmath106 in the rule for choosing @xmath1 .",
    "we refer to this variant of ` fpl+gr`as ` fpl+gr.p ` . in the next section ,",
    "we provide performance guarantees for both algorithms .",
    "now we are ready to state our main results .",
    "proofs will be presented in section  [ sec : analysis ] .",
    "first , we present a performance guarantee for ` fpl+gr`in terms of the _ expected regret _ :    [ thm : bandit_exp ] the expected regret of ` fpl+gr`satisfies @xmath107 under semi - bandit information . in particular , with @xmath108 the expected regret of ` fpl+gr`is bounded as @xmath109    our second main contribution is the following bound on the regret of ` fpl+gr.p ` .    fix an arbitrary @xmath27 . with probability at least @xmath26 , the regret of ` fpl+gr.p`satisfies @xmath110 in particular , with @xmath111",
    "the regret of ` fpl+gr.p`is bounded as @xmath112 with probability at least @xmath26 .",
    "[ thm : highprob ]      let us now turn our attention to computational issues .",
    "first , we note that the efficiency of ` fpl`-type algorithms crucially depends on the availability of an efficient oracle that solves the static combinatorial optimization problem of finding @xmath113 .",
    "computing the running time of the full - information variant of ` fpl`is straightforward : assuming that the oracle computes the solution to the static problem in @xmath114 time , ` fpl`returns its prediction in @xmath115 time ( with the @xmath39 overhead coming from the time necessary to generate the perturbations ) .",
    "naturally , our loss estimation scheme multiplies these computations by the number of samples taken in each round . while terminating the estimation procedure after @xmath80 samples helps in controlling the running time with high probability ,",
    "observe that the nave bound of @xmath116 on the number of samples becomes way too large when setting @xmath80 as suggested by theorems  [ thm : bandit_exp ] and  [ thm : highprob ] .",
    "the next proposition shows that the amortized running time of geometric resampling remains as low as @xmath117 even for large values of @xmath80 .",
    "let @xmath118 denote the number of sample actions taken by ` gr`in round @xmath12 .",
    "then , @xmath119 } \\le d$ ] .",
    "also , for any @xmath27 , @xmath120 holds with probability at least @xmath26 .    for proving the first statement ,",
    "let us fix a time step @xmath12 and notice that @xmath121 now , observe that @xmath122 } \\le 1/{\\mathbb{e}\\left[\\left.v_{t , j}\\right|{\\mathcal{f}}_{t-1}\\right]}$ ] , which gives @xmath119}\\le d$ ] , thus proving the first statement .",
    "for the second part , notice that @xmath123}\\right)}$ ] is a martingale - difference sequence with respect to @xmath124 with @xmath125 and with conditional variance @xmath126 } & = { \\mathbb{e}\\left[\\left.{\\left(s_t - { \\mathbb{e}\\left[\\left.s_t\\right|{\\mathcal{f}}_{t-1}\\right]}\\right)}^2\\right|{\\mathcal{f}}_{t-1}\\right ] } \\le    { \\mathbb{e}\\left[\\left.s_t^2\\right|{\\mathcal{f}}_{t-1}\\right ] }   \\\\   & = { \\mathbb{e}\\left[\\left.\\max_j { \\left(v_{t , j } k_{t , j}\\right)}^2\\right|{\\mathcal{f}}_{t-1}\\right ] }   \\le { \\mathbb{e}\\left[\\left.\\sum_{j=1}^d v_{t , j } k_{t , j}^2\\right|{\\mathcal{f}}_{t-1}\\right ] }    \\\\   & \\le \\sum_{j=1}^d \\min{\\left\\{\\frac{2}{q_{t , j}},m\\right\\ } } \\le dm , \\end{split}\\ ] ] where we used @xmath127 } = \\frac{2-q_{t , i}}{q_{t , i}^2}$ ] .",
    "then , the second statement follows from applying a version of freedman s inequality due to @xcite ( stated as lemma  [ lem : mart ] in the appendix ) with @xmath128 and @xmath129 .",
    "notice that choosing @xmath130 as suggested by theorems  [ thm : bandit_exp ] and  [ thm : highprob ] , the above result guarantees that the amortized running time of ` fpl+gr`is @xmath131 with high probability .",
    "this section presents the proofs of theorems  [ thm : bandit_exp ] and  [ thm : highprob ] . in a didactic attempt , we present statements concerning the loss - estimation procedure and the learning algorithm separately : section  [ sec : analysis_rw ] presents various important properties of the loss estimates produced by geometric resampling , section  [ sec : analysis_fpl ] presents general tools for analyzing follow - the - perturbed - leader methods .",
    "finally , sections  [ sec : analysis_exp ] and  [ sec : analysis_hp ] put these results together to prove theorems  [ thm : bandit_exp ] and  [ thm : highprob ] , respectively .",
    "the basic idea underlying geometric resampling is replacing the importance weights @xmath99 by appropriately defined random variables @xmath101 . as we have seen earlier ( section  [ sec : rw ] ) , running `",
    "gr`with @xmath132 amounts to sampling each @xmath101 from a geometric distribution with expectation @xmath99 , yielding an unbiased loss estimate . in practice",
    ", one would want to set @xmath80 to a finite value to ensure that the running time of the sampling procedure is bounded .",
    "note however that early termination of ` gr`introduces a bias in the loss estimates .",
    "this section is mainly concerned with the nature of this bias .",
    "we emphasize that the statements presented in this section remain valid no matter what randomized algorithm generates the actions @xmath1 .",
    "our first lemma gives an explicit expression on the expectation of the loss estimates generated by ` gr ` .",
    "[ lem : bias ] for all @xmath133 and @xmath12 such that @xmath134 , the loss estimates satisfy @xmath135 } = \\left(1-(1-q_{t , j})^m \\right ) { \\ell}_{t , j}.\\ ] ]    fix any @xmath136 satisfying the condition of the lemma . setting @xmath137 for simplicity , we write @xmath138 } = & \\sum_{k=1}^\\infty k ( 1-q)^{k-1 } q - \\sum_{k = m}^\\infty ( k - m ) ( 1-q)^{k-1 } q \\\\ = & \\sum_{k=1}^\\infty k ( 1-q)^{k-1 } q - ( 1-q)^m \\sum_{k = m}^\\infty ( k\\ ! - \\!m ) ( 1-q)^{k\\!-\\!m\\!-\\!1 } q \\\\ = & \\left(1-(1-q)^m \\right)\\sum_{k=1}^\\infty k ( 1-q)^{k-1 } q = \\frac{1-(1-q)^m } { q}. \\end{split}\\ ] ] the proof is concluded by combining the above with @xmath139 } \\!=\\ !",
    "q_{t , j } { \\ell}_{t , j } { \\mathbb{e}\\left[\\left.k_{t , j}\\right|{\\mathcal{f}}_{t-1}\\right]}$ ] .",
    "the following lemma shows two important properties of the ` gr`loss estimates . roughly speaking ,",
    "the first of these properties ensure that any learning algorithm relying on these estimates will be _ optimistic _ in the sense that the loss of any _ fixed _ decision will be underestimated in expectation .",
    "the second property ensures that the learner will not be _ overly optimistic _ concerning its own performance .",
    "[ lem : rwprops ] for all @xmath4 and @xmath12 , the loss estimates satisfy the following two properties : @xmath140 } & \\le & { \\bm{v}}{^\\mathsf{\\scriptscriptstyle t}}{\\bm\\ell}_t,\\\\    { \\mathbb{e}\\left[\\left.\\sum_{{\\bm{u}}\\in{\\mathcal{s}}}p_t({\\bm{u } } ) { \\left({\\bm{u}}{^\\mathsf{\\scriptscriptstyle t}}{{\\widehat}{{\\bm\\ell}}}_t\\right)}\\right|{\\mathcal{f}}_{t-1}\\right ] } & \\ge &     \\sum_{{\\bm{u}}\\in{\\mathcal{s}}}p_t({\\bm{u } } ) { \\bigl({\\bm{u}}{^\\mathsf{\\scriptscriptstyle t}}{\\bm\\ell}_t\\bigr ) } - \\frac{d}{em}.   \\end{aligned}\\ ] ]    fix any @xmath4 and @xmath12 .",
    "the first property is an immediate consequence of lemma  [ lem : bias ] : we have that @xmath141}\\le { \\ell}_{t , k}$ ] for all @xmath142 , and thus @xmath143 } \\le { \\bm{v}}{^\\mathsf{\\scriptscriptstyle t}}{\\bm\\ell}_t$ ] . for the second statement , observe that @xmath144 } & =   \\sum_{i=1}^d q_{t , i } { \\mathbb{e}\\left[\\left.{{\\widehat}{\\ell}}_{t , i}\\right|{\\mathcal{f}}_{t-1}\\right ] } = \\sum_{i=1}^d q_{t , i } \\left(1-(1-q_{t , i})^m \\right ) { \\ell}_{t , i } \\end{split}\\ ] ] also holds by lemma  [ lem : bias ] . to control the bias term @xmath145 , note that @xmath146 . by elementary calculations , one can show that @xmath147 takes its maximum at @xmath148 and thus @xmath149    our last lemma concerning the loss estimates bounds the conditional variance of the estimated loss of the learner .",
    "this term plays a key role in the performance analysis of ` exp3`-style algorithms ( see , e.g. , @xcite ) , as well as in the analysis presented in the current paper .",
    "[ lem : quad ] for all @xmath12 , the loss estimates satisfy @xmath150 } \\le 2md.\\ ] ]    before proving the statement , we remark that the conditional variance can be bounded as @xmath151 for the standard ( although usually infeasible ) loss estimates . that is , the above lemma shows that , somewhat surprisingly , the variance of our estimates is only twice as large as the variance of the standard estimates .",
    "fix an arbitrary @xmath12 . for simplifying notation below ,",
    "let us introduce @xmath152 as an independent copy of @xmath1 such that @xmath153 } = p_t({\\bm{v}})$ ] holds for all @xmath4 . to begin , observe that for any @xmath20 @xmath154 } \\le \\frac{2-q_{t , i}}{q_{t , i}^2 } \\le \\frac{2}{q_{t , i}^2}\\ ] ] holds , as @xmath101 has a truncated geometric law .",
    "the statement is proven as @xmath155 } & = { \\mathbb{e}\\left[\\left.\\sum_{i=1}^d\\sum_{j=1}^d \\left({\\widetilde}{v}_{i}{{\\widehat}{\\ell}}_{t , i}\\right)\\left({\\widetilde}{v}_{j}{{\\widehat}{\\ell}}_{t , j}\\right)\\right|{\\mathcal{f}}_{t-1}\\right ] } \\\\ & = { \\mathbb{e}\\left[\\left.\\sum_{i=1}^d\\sum_{j=1}^d \\left({\\widetilde}{v}_{i}k_{t , i}v_{t , i}{\\ell}_{t , i}\\right ) \\left({\\widetilde}{v}_{j}k_{t , j}v_{t , j}{\\ell}_{t , j}\\right)\\right|{\\mathcal{f}}_{t-1}\\right ] } \\\\ & \\qquad\\qquad\\mbox{(using the definition of $ { { \\widehat}{{\\bm\\ell}}}_t$ ) } \\\\ & \\le { \\mathbb{e}\\left[\\left.\\sum_{i=1}^d\\sum_{j=1}^d \\frac{k_{t , i}^2+k_{t , j}^2}{2}\\left({\\widetilde}{v}_{i}v_{t , i}{\\ell}_{t , i}\\right)\\left({\\widetilde}{v}_{j}v_{t , j}{\\ell}_{t , j}\\right)\\right|{\\mathcal{f}}_{t-1}\\right ] } \\\\ & \\qquad\\qquad\\mbox{(using $ 2ab\\le a^2 + b^2 $ ) } \\\\ & \\le 2{\\mathbb{e}\\left[\\left.\\sum_{i=1}^d\\frac{1}{q_{t , i}^2}\\left({\\widetilde}{v}_{i}v_{t , i}{\\ell}_{t , i}\\right ) \\sum_{j=1}^d v_{t , j}{\\ell}_{t , j}\\right|{\\mathcal{f}}_{t-1}\\right ] } \\\\ & \\qquad\\qquad\\mbox{(using symmetry , eq.~\\eqref{eq : k2bound } and $ { \\widetilde}{v}_j\\le 1 $ ) } \\\\ & \\le 2m{\\mathbb{e}\\left[\\left.\\sum_{j=1}^d{\\ell}_{t , j}\\right|{\\mathcal{f}}_{t-1}\\right ] } \\le 2md , \\end{split}\\ ] ] where the last line follows from using @xmath156 , @xmath157 , and @xmath158}={\\mathbb{e}\\left[\\left.{\\widetilde}{v}_{i}\\right|{\\mathcal{f}}_{t-1}\\right]}=q_{t , i}$ ] .      in this section , we present the key tools for analyzing the ` fpl`-component of our learning algorithm . in some respect ,",
    "our analysis is a synthesis of previous work on ` fpl`-style methods : we borrow several ideas from @xcite and the proof of corollary  4.5 in @xcite .",
    "nevertheless , our analysis is the first to directly target combinatorial settings , and yields the tightest known bounds for ` fpl`in this domain .",
    "indeed , the tools developed in this section also permit an improvement for ` fpl`in the full - information setting , closing the presumed performance gap between ` fpl`and ` ewa`in both the full - information and the semi - bandit settings .",
    "the statements we present in this section are not specific to the loss - estimate vectors used by ` fpl+gr ` .    like most other known work , we study the performance of the learning algorithm through a",
    "_ virtual algorithm _ that ( _ i _ ) uses a time - independent perturbation vector and ( _ ii _ )  is  allowed to peek one step into the future .",
    "specifically , letting @xmath159 be a perturbation vector drawn independently from the same distribution as @xmath160 , the virtual algorithm picks its @xmath161 action as @xmath162 in what follows , we will crucially use that @xmath163 and @xmath164 are conditionally independent and identically distributed given @xmath165 . in particular , introducing the notations @xmath166 }   & { { \\widetilde}{q}}_{t , i } & = { \\mathbb{e}\\left[\\left.{\\widetilde{v}}_{t , i}\\right|{\\mathcal{f}}_{t}\\right]}\\\\ p_{t}({\\bm{v } } ) & = { \\mathbb{p}\\left[\\left.{\\bm{v}}_t={\\bm{v}}\\right|{\\mathcal{f}}_{t-1}\\right ] }     & { { \\widetilde}{p}}_{t}({\\bm{v } } ) & = { \\mathbb{p}\\left[\\left.{\\widetilde{{\\bm{v}}}}_t={\\bm{v}}\\right|{\\mathcal{f}}_{t}\\right]},\\end{aligned}\\ ] ] we will exploit the above property by using @xmath167 and @xmath168 numerous times in the proofs below .",
    "first , we show a regret bound on the virtual algorithm that plays the action sequence @xmath169 .    [",
    "lem : cheat ] for any @xmath4 , @xmath170    although the proof of this statement is rather standard , we include it for completeness",
    ". we also note that the lemma slightly improves other known results by replacing the usual @xmath171 term by @xmath172 .",
    "fix any @xmath4 . using lemma  3.1 of @xcite ( sometimes referred to as the _ `` follow - the - leader / be - the - leader '' _ lemma ) for the sequence @xmath173",
    ", we obtain @xmath174 reordering and integrating both sides with respect to the distribution of @xmath159 gives @xmath175}. \\end{split}\\ ] ]",
    "the statement follows from using @xmath176 } \\le m(\\log(d / m ) + 1)$ ] , which is proven in appendix  [ app : proofs ] as lemma  [ lem : expmax ] , noting that @xmath177 is upper - bounded by the sum of the @xmath178 largest components of @xmath159 .",
    "the next lemma relates the performance of the virtual algorithm to the actual performance of ` fpl ` .",
    "the lemma relies on a `` sparse - loss '' trick similar to the trick used in the proof corollary  4.5 in @xcite , and is also related to the `` unit rule '' discussed by @xcite .",
    "[ lem : price ] for all @xmath0 , assume that @xmath179 is such that @xmath180 for all @xmath181 .",
    "then , @xmath182    fix an arbitrary @xmath12 and @xmath183 , and define the `` sparse loss vector '' @xmath184 with components @xmath185 and @xmath186 using the notation @xmath187}$ ] , we show in lemma  [ lem : modified - loss ] ( stated and proved in appendix  [ app : proofs ] ) that @xmath188 . also , define @xmath189 letting @xmath190 @xmath191 be the density of the perturbations , we have @xmath192^d } { \\mathbbm{1}_{\\left\\{{\\bm{u}}({\\bm{z}})={\\bm{u}}\\right\\ } } } f({\\bm{z } } ) \\,d{\\bm{z}}\\\\ & = e^{\\eta \\left\\|{{\\widehat}{{\\bm\\ell}}}^-_{t}({\\bm{u}})\\right\\|_1 } \\int\\limits_{{\\bm{z}}\\in[0,\\infty]^d } { \\mathbbm{1}_{\\left\\{{\\bm{u}}({\\bm{z}})={\\bm{u}}\\right\\ } } }",
    "f\\left({\\bm{z}}+\\eta{{\\widehat}{{\\bm\\ell}}}^-_t({\\bm{u}})\\right )   \\,d{\\bm{z}}\\\\ & = e^{\\eta \\left\\|{{\\widehat}{{\\bm\\ell}}}^-_{t}({\\bm{u}})\\right\\|_1 } \\idotsint\\limits_{z_i\\in[{{\\widehat}{\\ell}}^-_{t , i}({\\bm{u}}),\\infty ] } { \\mathbbm{1}_{\\left\\{{\\bm{u}}\\left({\\bm{z}}-\\eta{{\\widehat}{{\\bm\\ell}}}^-_t({\\bm{u}})\\right)={\\bm{u}}\\right\\ } } } f({\\bm{z } } )   \\,d{\\bm{z}}\\\\ & \\le e^{\\eta \\left\\|{{\\widehat}{{\\bm\\ell}}}^-_{t}({\\bm{u}})\\right\\|_1 } \\int\\limits_{{\\bm{z}}\\in[0,\\infty]^d } { \\mathbbm{1}_{\\left\\{{\\bm{u}}\\left({\\bm{z}}-\\eta{{\\widehat}{{\\bm\\ell}}}^-_t({\\bm{u}})\\right)={\\bm{u}}\\right\\ } } } f({\\bm{z } } )   \\,d{\\bm{z}}\\\\ & \\le e^{\\eta \\left\\|{{\\widehat}{{\\bm\\ell}}}^-_{t}({\\bm{u}})\\right\\|_1 } p^-_{t}({\\bm{u } } ) \\le e^{\\eta \\left\\|{{\\widehat}{{\\bm\\ell}}}^-_{t}({\\bm{u}})\\right\\|_1 } { { \\widetilde}{p}}_{t}({\\bm{u } } ) .",
    "\\end{split}\\ ] ] now notice that @xmath193 , which gives @xmath194 the proof is concluded by repeating the same argument for all @xmath183 , reordering and summing the terms as @xmath195      now , everything is ready to prove the bound on the expected regret of ` fpl+gr ` .",
    "let us fix an arbitrary @xmath4 . by putting together lemmas  [ lem :",
    "quad ] , [ lem : cheat ] and  [ lem : price ] , we immediately obtain @xmath196}\\le \\frac{m\\left(\\log ( d / m)+1\\right)}{\\eta } + 2\\eta mdt,\\ ] ] leaving us with the problem of upper bounding the expected regret in terms of the left - hand side of the above inequality .",
    "this can be done by using the properties of the loss estimates stated in lemma  [ lem : rwprops ] : @xmath197 } \\le { \\mathbb{e}\\left[\\sum_{t=1}^t \\sum_{{\\bm{u}}\\in{\\mathcal{s } } } p_t({\\bm{u } } ) { \\left(\\left({\\bm{u}}- { \\bm{v}}\\right){^\\mathsf{\\scriptscriptstyle t}}{{\\widehat}{{\\bm\\ell}}}_t\\right)}\\right ] } + \\frac{dt}{em}.\\ ] ] putting the two inequalities together proves the theorem .",
    "we now turn to prove a bound on the regret of ` fpl+gr.p`that holds with high probability .",
    "we begin by noting that the conditions of lemmas  [ lem : cheat ] and  [ lem : price ] continue to hold for the new loss estimates , so we can obtain the central terms in the regret : @xmath198 the first challenge posed by the above expression is relating the left - hand side to the true regret with high probability .",
    "once this is done , the remaining challenge is to bound the second term on the right - hand side , as well as the other terms arising from the first step .",
    "we first show that the loss estimates used by ` fpl+gr.p`consistently underestimate the true losses with high probability .",
    "[ lem : bias_hp ] fix any @xmath199 . for any @xmath4",
    ", @xmath200 holds with probability at least @xmath201 .    the simple proof is directly inspired by appendix c.9 of @xcite .",
    "fix any @xmath12 and @xmath20 .",
    "then , @xmath202 } = { \\mathbb{e}\\left[\\left.\\exp{\\left(\\log{\\left(1+\\beta{{\\widehat}{\\ell}}_{t , i}\\right)}\\right)}\\right|{\\mathcal{f}}_{t-1}\\right ] } \\le 1 + \\beta{\\ell}_{t , i } \\le \\exp(\\beta { \\ell}_{t , i } ) ,   \\end{split}\\ ] ] where we used lemma  [ lem : bias ] in the first inequality and @xmath203 that holds for all @xmath204 . as a result , the process @xmath205 is a supermartingale with respect to @xmath124 : @xmath206 } \\le w_{t-1}$ ] .",
    "observe that , since @xmath207 , this implies @xmath208 } \\le { \\mathbb{e}\\left[w_{t-1}\\right ] } \\le \\ldots \\le 1 $ ] .",
    "applying markov s inequality gives that @xmath209 } & =   { \\mathbb{p}\\left[{{\\widetilde}{l}}_{t , i } - l_{t , i } > \\varepsilon\\right ] }   \\\\   & \\le { \\mathbb{e}\\left[\\exp{\\left(\\beta { \\left({{\\widetilde}{l}}_{t , i } - l_{t , i}\\right)}\\right)}\\right ] } \\exp(-\\beta \\varepsilon ) \\le",
    "\\exp(-\\beta \\varepsilon ) \\end{split}\\ ] ] holds for any @xmath210 .",
    "the statement of the lemma follows after using @xmath211 , applying the union bound for all @xmath20 , and solving for @xmath212 .",
    "the following lemma states another key property of the loss estimates .",
    "[ lem : quad2 ] for any @xmath12 , @xmath213    the statement follows trivially from the inequality @xmath214 that holds for all @xmath215 .",
    "in particular , for any fixed @xmath12 and @xmath20 , we have @xmath216 multiplying both sides by @xmath217 and summing for all @xmath20 proves the statement .",
    "the next lemma relates the total loss of the learner to its total estimated losses .",
    "fix any @xmath199 .",
    "with probability at least @xmath218 , @xmath219    we start by rewriting @xmath220 now let @xmath221}$ ] for all @xmath20 and notice that @xmath222 is a martingale - difference sequence with respect to @xmath223 with elements upper - bounded by @xmath178 ( as lemma  [ lem : bias ] implies @xmath224 and @xmath156 ) .",
    "furthermore , the conditional variance of the increments is bounded as @xmath126 } \\le & { \\mathbb{e}\\left[\\left.{\\left(\\sum_{i=1}^d q_{t , i } v_{t , i}k_{t , i}\\right)}^2\\right|{\\mathcal{f}}_{t-1}\\right ] }   \\le{\\mathbb{e}\\left[\\left.\\sum_{j=1}^d v_{t , j } { \\left(\\sum_{i=1}^d q_{t , i}^2 k_{t , i}^2\\right)}\\right|{\\mathcal{f}}_{t-1}\\right ] } \\le 2 m , \\end{split}\\ ] ] where the second inequality is cauchy  schwarz and the third one follows from @xmath127 } \\le 2/q_{t , i}^2 $ ] and @xmath225 .",
    "thus , applying lemma  [ lem : mart ] with @xmath226 and @xmath227 we get that for any @xmath228 , @xmath229 holds with probability at least @xmath201 , where we have used @xmath156 . after setting @xmath230",
    ", we obtain that @xmath231 holds with probability at least @xmath201 .    to proceed , observe that @xmath232 holds by lemma  [ lem : bias ] , implying @xmath233 together with eq .",
    ", this gives @xmath234 finally , we use that , by lemma  [ lem : rwprops ] , @xmath235 , and @xmath236 is a martingale - difference sequence with respect to @xmath124 with increments bounded in @xmath237 $ ] .",
    "then , by an application of hoeffding  azuma inequality , we have @xmath238 with probability at least @xmath201 , thus proving the lemma .    finally , our last lemma in this section bounds the second - order terms arising from lemmas  [",
    "lem : price ] and  [ lem : quad2 ] .",
    "[ lem : secondorder ] fix any @xmath199 . with probability at least @xmath218 , the following hold simultaneously : @xmath239    first",
    ", recall that @xmath240 } \\le 2md\\ ] ] holds by lemma  [ lem : price ] .",
    "now , observe that @xmath241}\\right)}\\ ] ] is a martingale - difference sequence with increments in @xmath242 $ ] .",
    "an application of the hoeffding ",
    "azuma inequality gives that @xmath243}\\right ) } \\le mm\\sqrt{2 t\\log\\frac{1}{\\delta ' } } + 2md\\sqrt{t\\log \\frac{1}{\\delta'}}\\ ] ] holds with probability at least @xmath201 .",
    "reordering the terms completes the proof of the first statement .",
    "the second statement is proven analogously , building on the bound @xmath244 } \\le & { \\mathbb{e}\\left[\\left.\\sum_{i=1}^d q_{t , i } v_{t , i } k_{t , i}^2\\right|{\\mathcal{f}}_{t-1}\\right ] } \\le 2d .",
    "\\end{split}\\ ] ]    theorem  [ thm : highprob ] follows from combining lemmas  [ lem : bias_hp ] through  [ lem : secondorder ] and applying the union bound .",
    "our proof techniques presented in section  [ sec : analysis_fpl ] also enable us to tighten the guarantees for ` fpl`in the full information setting . in particular , consider the algorithm choosing action @xmath245 where @xmath246 and the components of @xmath88 are drawn independently from a standard exponential distribution .",
    "we state our improved regret bounds concerning this algorithm in the following theorem .",
    "[ thm : fullinfo ] for any @xmath4 , the total expected regret of ` fpl`satisfies @xmath247}\\ ] ] under full information .",
    "in particular , defining @xmath248 and setting @xmath249 the regret of ` fpl`satisfies @xmath250    in the worst case , the above bound becomes @xmath251 , which improves the best known bound for ` fpl`of @xcite by a factor of @xmath252 .",
    "the first statement follows from combining lemmas  [ lem : cheat ] and  [ lem : price ] , and bounding @xmath253 while the second one follows from standard algebraic manipulations .",
    "in this paper , we have described the first _ general and efficient _ algorithm for online combinatorial optimization under semi - bandit feedback .",
    "we have proved that the regret of this algorithm is @xmath31 in this setting , and have also shown that ` fpl`can achieve @xmath254 in the full information case when tuned properly . while these bounds are off by a factor of @xmath255 and @xmath256 from the respective minimax results , they exactly match the best known regret bounds for the well - studied exponentially weighted forecaster ( ` ewa ` ) . whether the remaining gaps can be closed for ` fpl`-style algorithms ( e.g. , by using more intricate perturbation schemes or a more refined analysis )",
    "remains an important open question .",
    "nevertheless , we regard our contribution as a significant step towards understanding the inherent trade - offs between computational efficiency and performance guarantees in online combinatorial optimization and , more generally , in online optimization .",
    "the efficiency of our method rests on a novel loss estimation method called geometric resampling ( ` gr ` ) .",
    "this estimation method is not specific to the proposed learning algorithm . while ` gr`has no immediate benefits for ` osmd`-type algorithms where the ideal importance weights are readily available , it is possible to think about problem instances where ` ewa`can be efficiently implemented while importance weights are difficult to compute .",
    "the most important open problem left is the case of efficient online linear optimization with _ full bandit feedback _ where the learner only observes the inner product @xmath6 in round @xmath12 .",
    "learning algorithms for this problem usually require that the ( pseudo-)inverse of the covariance matrix @xmath257}$ ] is readily available for the learner at each time step ( see , e.g. , @xcite ) . computing this matrix , however , is at least as challenging as computing the individual importance weights @xmath99 .",
    "that said , our geometric resampling technique can be directly generalized to this setting by observing that the matrix geometric series @xmath258 converges to @xmath259 under certain conditions .",
    "this sum can then be efficiently estimated by sampling independent copies of @xmath1 , which paves the path for constructing low - bias estimates of the loss vectors . while it seems straightforward to go ahead and use these estimates in tandem with ` fpl ` , we have to note that the analysis presented in this paper does not carry through directly in this case .",
    "the main limitation is that our techniques only apply for loss vectors with _ non - negative _ elements ( cf .",
    "lemma  [ lem : price ] ) . nevertheless , we believe that geometric resampling should be a crucial component for constructing truly effective learning algorithms for this important problem .",
    "[ lem : expmax ] let @xmath260 be i.i.d",
    ".  exponentially distributed random variables with unit expectation and let @xmath261 be their permutation such that @xmath262 . then , for any @xmath263 , @xmath264 } \\le m { \\left(\\log { \\left(\\frac{d}{m}\\right ) } + 1\\right)}.\\ ] ]    let us define @xmath265 . then , as @xmath266 is nonnegative , we have for any @xmath267 that @xmath268 } = & \\int_{0}^\\infty { \\mathbb{p}\\left[y > y\\right ] } dy    \\\\",
    "\\le & a + \\int_{a}^\\infty { \\mathbb{p}\\left[\\sum_{i=1}^m z_i^*>y\\right ] } dy    \\\\",
    "\\le & a + \\int_{a}^\\infty { \\mathbb{p}\\left[z_1^*>\\frac ym\\right ] } dy      \\\\",
    "\\le & a + d \\int_{a}^\\infty { \\mathbb{p}\\left[z_1>\\frac ym\\right ] } dy    \\\\    = & a + d e^{-a / m }    \\\\",
    "\\le & m\\log{\\left(\\frac dm\\right ) } + m ,   \\end{split}\\ ] ] where in the last step , we used that @xmath269 minimizes @xmath270 over the real line .",
    "[ lem : modified - loss ] fix any @xmath4 and any vectors @xmath271 and @xmath272 .",
    "define the vector @xmath273 with components @xmath274 .",
    "then , for any perturbation vector @xmath275 with independent components , @xmath276 } \\\\ & \\qquad\\le { \\mathbb{p}\\left[{\\bm{v}}{^\\mathsf{\\scriptscriptstyle t}}\\left({\\bm{l}}+ { \\bm\\ell}- { \\bm{z}}\\right)\\le",
    "{ \\bm{u}}{^\\mathsf{\\scriptscriptstyle t}}\\left({\\bm{l}}+ { \\bm\\ell}- { \\bm{z}}\\right)\\ , \\left(\\forall { \\bm{u}}\\in{\\mathcal{s}}\\right)\\right]}. \\end{split}\\ ] ]    fix any @xmath277 and define the vector @xmath278 . define the events @xmath279 and @xmath280 we have @xmath281 where we used @xmath282 and @xmath283 .",
    "now , since @xmath284 , we have @xmath285 , thus proving @xmath286 } \\le { \\mathbb{p}\\left[\\cap_{{\\bm{u}}\\in{\\mathcal{s } } } a({\\bm{u}})\\right ] } $ ] as claimed in the lemma .",
    "[ lem : mart ] assume @xmath287 is a martingale - difference sequence with respect to the filtration @xmath288 with @xmath289 for @xmath290 .",
    "let @xmath291 $ ] and @xmath292 then , for any @xmath27 , @xmath293}\\le \\delta.\\ ] ] furthermore , for any @xmath294 , @xmath295}\\le \\delta.\\ ] ]        c.  allenberg , p.  auer , l.  gyrfi , and .",
    "hannan consistency in on - line learning in case of unbounded losses under partial monitoring . in _ proceedings of the 17th international conference on algorithmic learning theory ( alt )",
    "_ , pages 229243 , 2006 .",
    "b.  awerbuch and r.  d. kleinberg .",
    "adaptive routing with end - to - end feedback : distributed learning and geometric approaches . in _ proceedings of the 36th annual acm symposium on theory of computing _ , pages 4553 , 2004 .",
    "a.  beygelzimer , j.  langford , l.  li , l.  reyzin , and r.  e. schapire .",
    "contextual bandit algorithms with supervised learning guarantees . in _ proceedings of the 14th international conference on artificial intelligence and statistics ( aistats ) _ , pages 1926 , 2011 .",
    "s.  bubeck , n.  cesa - bianchi , and s.  m. kakade .",
    "towards minimax policies for online linear optimization with bandit feedback . in _ proceedings of the 25th conference on learning theory ( colt ) _ , pages 114 , 2012 .",
    "d.  suehiro , k.  hatano , s.  kijima , e.  takimoto , and k.  nagano .",
    "online prediction under submodular constraints . in _ proceedings of the 23rd international conference on algorithmic learning theory ( alt )",
    "_ , pages 260274 , 2012 .",
    "t.  uchiya , a.  nakamura , and m.  kudo .",
    "algorithms for adversarial bandit problems with multiple plays . in _ proceedings of the 21st international conference on algorithmic learning theory ( alt ) _ , pages 375389 , 2010 ."
  ],
  "abstract_text": [
    "<S> we propose a sample - efficient alternative for importance weighting for situations where one only has sample access to the probability distribution that generates the observations . our new method , called geometric resampling ( ` gr ` ) , is described and analyzed in the context of online combinatorial optimization under semi - bandit feedback , where a learner sequentially selects its actions from a combinatorial decision set so as to minimize its cumulative loss . in particular , </S>",
    "<S> we show that the well - known follow - the - perturbed - leader ( ` fpl ` ) prediction method coupled with geometric resampling yields the first computationally efficient reduction from offline to online optimization in this setting . </S>",
    "<S> we provide a thorough theoretical analysis for the resulting algorithm , showing that its performance is on par with previous , inefficient solutions . </S>",
    "<S> our main contribution is showing that , despite the relatively large variance induced by the ` gr`procedure , our performance guarantees hold with high probability rather than only in expectation . as a side result </S>",
    "<S> , we also improve the best known regret bounds for ` fpl`in online combinatorial optimization with full feedback , closing the perceived performance gap between ` fpl`and exponential weights in this setting .    </S>",
    "<S> online learning , combinatorial optimization , bandit problems , semi - bandit feedback , follow the perturbed leader , importance weighting </S>"
  ]
}