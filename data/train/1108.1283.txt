{
  "article_text": [
    "we prove the following theorem .",
    "[ thm : mainthm ] for every large enough integer @xmath0 , there exists an @xmath0-point subset of @xmath1 such that for every @xmath2 , embedding it into @xmath3 with distortion @xmath4 requires dimension @xmath5 at least @xmath6 .",
    "moreover , for every @xmath7 and large enough integer @xmath0 , there exists an @xmath0-point subset of @xmath1 such that embedding it into @xmath3 with distortion @xmath8 requires dimension @xmath5 at least @xmath9 .",
    "both parts of theorem  [ thm : mainthm ] were previously known . the first part ( embedding with large distortion ) was first shown by brinkman and charikar  @xcite , and later with a simpler proof by lee and naor  @xcite . the second part ( embedding with low distortion )",
    "was recently shown by andoni , charikar , neiman , and nguyen  @xcite .",
    "our proof is based on an entropy argument , and is arguably more intuitive .",
    "the set of points we use is identical to the one used by andoni et al .",
    "@xcite . for completeness",
    ", we briefly describe it here ( see also figure  [ fig : graph ] for an illustration ) . for integers",
    "@xmath10 , @xmath11 , we define the so - called  recursive cycle \" graph @xmath12 , and associate with each vertex a label in @xmath13 .",
    "the set of all labels will be our point set @xmath14 in @xmath15 .",
    "first , for @xmath10 , let @xmath16 be the cycle of length @xmath17 , with two distinguished antipodal vertices ( i.e. , of distance @xmath18 ) , call them  left \" and  right \" . for @xmath19",
    ", the @xmath20th vertex on the top path from the left to the right vertex is labeled with the vector @xmath21 with @xmath22 zeros and @xmath20 ones , and the @xmath20th vertex on the bottom path is associated with the vector @xmath23 with @xmath20 ones and @xmath22 zeros .",
    "notice that the @xmath15 distance between the labels of any two adjacent vertices is @xmath24 , whereas that between the labels of any two antipodal vertices is @xmath18 .    for @xmath25 ,",
    "define @xmath12 as the graph obtained from @xmath26 by replacing each edge with a copy of @xmath16 and identifying the distinguished vertices with the original endpoints of the edge .",
    "the number of vertices in @xmath12 is easily seen to be @xmath27 for the labels , we first take the labels in @xmath26 and duplicate each coordinate @xmath18 times .",
    "this defines the labels for those vertices coming from @xmath26 . for the newly added vertices on each cycle that replaced an edge of @xmath26",
    ", we replace the @xmath18 coordinates on which the two distinguished nodes of that cycle differ with the same labeling of @xmath16 described earlier .",
    "notice the following two properties : the @xmath15 distance between the labels of any two adjacent vertices is @xmath24 , and for @xmath28 , the distance between any two antipodal vertices in level @xmath29 is @xmath30 .",
    "we remark that these two properties are also satisfied by the shortest path metric on @xmath12 , but since that metric is not in @xmath15 , it is not good enough for the purpose of proving dimension reduction in @xmath15 .    finally , we label the edges of @xmath16 by elements of @xmath31 $ ] starting from the left vertex and going along the cycle , and extend this to a labeling of @xmath12 by elements of @xmath31^n$ ] in a recursive way , with the coordinates labeling the location of the edge from the top layer to the bottom layer ( see figure  [ fig : graph ] ) .     with our labeling and orientation of the edges and the labels on vertices in @xmath32.,scaledwidth=80.0% ]    the idea of",
    "the proof is the following . given a low - distortion embedding of @xmath14 into @xmath3 , we naturally obtain a mapping that maps each edge of the graph @xmath12 to a @xmath5-dimensional vector ( namely , the difference between the two embedded endpoints ) whose @xmath15 norm is close to @xmath24 .",
    "assume for simplicity that this norm is exactly @xmath24 ; assume moreover that the vector has non - negative coordinates .",
    "( in the proof we will show how to reduce the general case to this case . )",
    "so we can equivalently view this mapping as an encoding from @xmath31^n$ ] to probability distributions over @xmath33 $ ] . using the second property mentioned above ,",
    "one can obtain the following crucial property of the encoding : for any @xmath34 $ ] and any @xmath35 $ ] , if we are given @xmath36 together with the encoding of @xmath37^n$ ] , where @xmath38 are chosen uniformly , then we have a good probability to guess @xmath39 ( perfect probability in case of no distortion ) . a basic information theoretic argument",
    "now provides a lower bound on @xmath5 of any such encoding .",
    "for instance , in the case there is no distortion , the encoding allows us to predict @xmath39 as above with certainty , and the information theoretic argument gives the tight bound @xmath40 .",
    "we note that this simple yet powerful information theoretic argument appears in various different contexts , such as that of quantum random access codes  @xcite .",
    "all logarithms are base @xmath41",
    ". we use @xmath42 $ ] to denote the set @xmath43 .",
    "we now list a few basic definitions and facts from information theory .",
    "although not really needed for our proof , the interested reader can find an introduction to the area in  @xcite .",
    "we let @xmath44 denote the _ binary entropy function_. for a random variables @xmath45 on a domain @xmath33 $ ] obtaining each value @xmath46 $ ] with probability @xmath47 , the _ entropy _ of @xmath45 is given by @xmath48 , and is always at most @xmath49 . for two random variables @xmath50 , the _ conditional entropy _",
    "@xmath51 is the expectation of @xmath52 over @xmath53 chosen according to @xmath54 ; this can be seen to equal @xmath55 . finally , the _ mutual information _ @xmath56 is defined as @xmath57 , and the _ conditional mutual information _ @xmath58 is the expectation of @xmath59 over @xmath60 chosen according to @xmath61 , or equivalently , @xmath62 .",
    "the _ data processing inequality _ says that applying a function can not increase mutual information , @xmath63 .",
    "the following claim ( which is essentially what is known as fano s inequality ) shows that if one random variable can be used to predict another random variable , then their mutual information can not be too small .",
    "[ clm : fano ] assume @xmath45 is a random variable uniformly distributed over @xmath42 $ ] .",
    "let @xmath54 be another random variable , and assume that there exists some function @xmath64 with range @xmath42 $ ] such that @xmath65 with probability at least @xmath66",
    ". then @xmath67 .    by the data processing inequality",
    ", @xmath68 so it suffices to bound @xmath69 from above .",
    "since conditioning can not increase entropy , @xmath70",
    "our main technical theorem is the following .",
    "[ thm : main ] for any @xmath10 , @xmath11 the following holds .",
    "assume @xmath71^n \\to \\r^d$ ] satisfies that for all @xmath72 $ ] , @xmath73 and , moreover , that for some @xmath74 , and for all @xmath34 $ ] , @xmath35 $ ] , and @xmath75 $ ] , @xmath76 where @xmath77 denotes the average of @xmath78 over @xmath79 chosen uniformly in @xmath31 $ ] .",
    "then @xmath80 where @xmath81 .",
    "before proving the theorem , let us explain how it implies theorem  [ thm : mainthm ] .",
    "consider any embedding @xmath82 of @xmath14 into @xmath3 with distortion at most @xmath83 for some @xmath74 . by scaling @xmath82",
    ", we can assume that it is @xmath24-lipschitz ( i.e. , it does not expand any distance ) and that distances are not contracted by more than @xmath84 . let @xmath64 be the function that maps each @xmath85^n$ ] to @xmath86 , where @xmath87 is the label of the right endpoint of the edge labeled by @xmath88 and @xmath89 is the label of its left endpoint . since @xmath82 is @xmath24-lipschitz , @xmath90 for all @xmath85^n$ ] .",
    "moreover , it is not difficult to see that @xmath64 satisfies eq .",
    "( see figure  [ fig : graph2 ] ) .",
    "hence , theorem  [ thm : main ] implies that the bound in eq .   holds .",
    "for the first part of theorem  [ thm : mainthm ] we fix @xmath92 . we obtain that for any @xmath93 , any distortion-@xmath4 embedding of @xmath94 ( so @xmath95 and @xmath96 ) must have dimension at least @xmath97 for the second part of theorem  [ thm : mainthm ] , choosing @xmath98 and noting that @xmath99 , we obtain that the dimension must be at least @xmath100    we start by considering the case that for all @xmath72 $ ] , @xmath78 has non - negative coordinates and @xmath15-norm @xmath24 .",
    "we will later see how this implies the general case . making this assumption allows us to think of @xmath78 as a probability distribution over @xmath33 $ ] .",
    "let @xmath101 and @xmath102 be two random variables where @xmath45 is uniformly distributed over @xmath31^n$ ] and @xmath102 is distributed over @xmath33 $ ] according to @xmath103 .",
    "using the chain rule for mutual information we obtain @xmath104 the following lemma implies that for any @xmath34 $ ] , @xmath105 ( this is true even conditioned on any fixed value of @xmath106 , and not just on average ) and therefore @xmath107    let @xmath108 and @xmath109 be two random variables such that @xmath108 is uniformly distributed over @xmath31 $ ] and for any @xmath110 $ ] , conditioned on @xmath111 , @xmath109 is distributed according to some probability distribution @xmath112 on @xmath33 $ ] .",
    "assume that for all @xmath75 $ ] , @xmath113 then @xmath114 .",
    "let @xmath115 , and notice that @xmath116 is uniformly distributed on @xmath42 $ ] . by the data processing inequality , @xmath117 .",
    "for any @xmath118 $ ] , let @xmath119 be the distribution of @xmath109 conditioned on @xmath120 .",
    "our assumption says that for all @xmath75 $ ] , @xmath121 we need the following easy claim .        by applying the inequality to each of the @xmath5",
    "coordinates of the probability distributions @xmath128 , and summing the results , we obtain @xmath129 and hence @xmath130 consider the function that maps each @xmath131 $ ] to the @xmath118 $ ] that maximizes @xmath132 $ ] .",
    "this function correctly predicts @xmath116 from @xmath109 with probability @xmath133 .",
    "the lemma now follows from claim  [ clm : fano ] .",
    "we now show how to derive a similar bound for any @xmath64 as in the statement of the theorem .",
    "let @xmath71^n \\to \\r^{d}$ ] be such that for all @xmath85^n$ ] , @xmath134 has @xmath15 norm at most 1 .",
    "define @xmath135^n \\to \\r^{2d+1}$ ] by the concatenation @xmath136 obviously , for all @xmath88 , @xmath137 is non - negative and has @xmath15 norm @xmath24 . moreover , the linear operator that maps any @xmath138 to the vector @xmath139 can not increase the @xmath15 norm and maps @xmath137 to @xmath134 for all @xmath88 . therefore eq",
    ".   holds for @xmath140 , and the theorem follows .",
    "i thank the organizers of the workshop ",
    "metric embeddings , algorithms and hardness of approximation \" in the institut henri poincar , where this work started .",
    "i also thank moses charikar for the inspiring talk he gave there , and assaf naor and ofer neiman for useful discussions ."
  ],
  "abstract_text": [
    "<S> we show that for every large enough integer @xmath0 , there exists an @xmath0-point subset of @xmath1 such that for every @xmath2 , embedding it into @xmath3 with distortion @xmath4 requires dimension @xmath5 at least @xmath6 , and that for every @xmath7 and large enough integer @xmath0 , there exists an @xmath0-point subset of @xmath1 such that embedding it into @xmath3 with distortion @xmath8 requires dimension @xmath5 at least @xmath9 . </S>",
    "<S> these results were previously proven by brinkman and charikar [ jacm , 2005 ] and by andoni , charikar , neiman , and nguyen [ focs 2011 ] . </S>",
    "<S> we provide an alternative and arguably more intuitive proof based on an entropy argument .    </S>",
    "<S> = 1 </S>"
  ]
}