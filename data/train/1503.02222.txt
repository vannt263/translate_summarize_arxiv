{
  "article_text": [
    "adaptive reject metropolis sampling ( arms ) is a combination of adaptive rejection sampling ( ars ) @xcite and a metropolis - hastings sampling @xcite .",
    "ars was proposed for sampling from univariate log - concave conditional distributions . as an extension of ars ,",
    "arms @xcite removes the log - concavity restriction on the simulated probability density functions . when the density to be sampled from is multidimensional , a straight - forward approach is to embed arms within a gibbs sampler , provided that all one - dimensional conditional densities can be simulated by arms .",
    "this type of approaches have been used widely in various areas since the first paper on arms @xcite . for",
    "some more examples , @xcite , @xcite,@xcite , @xcite , and @xcite are all using arms in gibbs sampler for multivariate distributions .",
    "however , if the probability density function is multimodal in a multidimensional space , gibbs sampler can be easily trapped around some of the modes indefinitely , resulting in inefficient and even unreliable samples . in section 2 , we showed that applying gibbs sampler with arms algorithm to sample from a 2-dimensional distribution is trapped around 3 modes , while the target distribution has total 4 similar modes and is fairly smooth .",
    "although , in practice , there are methods , such as using different proposal distributions , to prevent gibbs or metropolis - hastings sampler being trapped in some subset of the support of the target distribution , whose probability is distinctly less than 1 .",
    "these methods usually implemented by trial and error",
    ". they may still give impressions of convergence while some important aspects of the target distributions are missing .",
    "hence , there is no guarantee from them that the samplers have sampled from all the subspaces around ( almost ) every local modes , especially sampling from multidimensional spaces and the locations of the modes are unknown priori .",
    "while metropolis - hastings algorithm may be stuck in only part of the support of the distribution , applying arms to sampling from one - dimensional distribution does not generally have this problem , since it is an accept - reject method with an adaptive instrumental density function .",
    "in multidimensional situation , it is mainly due to the approach through gibbs sampler that the arms can be trapped in some distinctly less than 1 probability subset of the support of the target distribution .",
    "particularly , we believe that such being trapped disadvantage is due to that the gibbs sampler updates the multidimensional samples in a fixed order and only in one dimension for each step .",
    "note that gibbs sampler does not necessarily sample in each dimension in every step , it could sample some dimension fewer times than the others and could sample in different order of the dimensions .",
    "such alternative sampling scheme may relieve the being trapping issue at some level . however , such alternative schemes are not standard and need to be done by trial and fail .",
    "therefore , again , this is not a very reliable way to solve the being trapped issue of arms in multi - dimensional distribution sampling .",
    "we propose an algorithm , which incorporate hit and run method and arms , instead of embedding arms in gibbs sampler , to sample from general multivariate distributions .",
    "hit and run algorithm was introduced by @xcite and @xcite its property has been studied by @xcite and @xcite . also , hit and run and gibbs samplers",
    "were compared by @xcite .",
    "although there s no theoretical result , the empirical study showed that the hit and run method estimators have less bias and standard errors than the corresponding gibbs sampler estimators .",
    "the authors suggested that this is due to the less autocorrelations of the hit and run method than the gibbs sampler . plus , since hit and run method generates uniformly distributed directions , so it suffers the problem of being stuck in only part of the support of the distribution much less likely . due to",
    "the difficulty of generating a signed distance for the hit and run method , griddy , acceptance / rejection , or metropolis methods have been used and the resulting algorithms have been studied .",
    "these methods more or less scarifice efficiency . in this paper ,",
    "arms algorithm is used for generate the signed distance .",
    "the resulting algorithm has not been proposed and studied explicitly in literature based on the authors best knowledge .",
    "combining the reliability of hit and run algorithm and the efficiency of arms , we can avoid the unreliable and inefficient issue due to the gibbs sampler .",
    "it is worth to mention that the hit and run is just one of the algorithms that deals with direction sampling .",
    "for example , adaptive direction sampling ( ads ) was introduced in @xcite , where the authors mentioned the gibbs sampler has much slower convergence rate in high dimensional situation compared to the ads .",
    "incorporating ads and arms looks promising and worth to be studied and compared with the hit and run method with arms , but is out of the scope of this paper .    as an example of the application of hararms ,",
    "it was applied to solve a free knots regression spline problem .",
    "spline function is widely used in the analysis of two - dimensional data @xmath0 .",
    "early papers @xcite @xcite have addressed the importance of the role of splines in smoothing and regression analysis .",
    "ruppert et al .",
    "a method of equally spaced knots was proposed in @xcite .",
    "this knot placement method is simple and straightforward to implement .",
    "however , the nature of this method does not guarantee that knots are placed at all critical locations , at which the underlying regression function possesses sharp changes .",
    "the other type of methods is called curve - fitting with free - knot splines , where the number of knots and their locations are determined from the data . to discuss the full benefits of the spline approach , free - knot methods become an important and difficult problem .",
    "traditional methods @xcite for the optimal knot selection is to add knots in intervals where the residuals are inadmissible large . in stead of parameters , the positions of the knots can be taken as the selection of functional type in an ordinary curve fitting problem @xcite .",
    "the knots should be chosen as to correspond to the overall performance of the data fitting .",
    "a new knot is assigned until the sum of squared residuals gets to a minimum .",
    "the recent literature propose several approaches to automatic knot selection that needs to search all possible models .",
    "many of them are based on stepwise regression ideas .",
    "for example , @xcite , @xcite , @xcite .",
    "a stochastic search method was proposed in @xcite for optimizing the knot locations by using a continuous genetic algorithm .",
    "bayesian methods for fitting free - knot splines have been considered in some literatures @xcite , @xcite , @xcite , and @xcite .",
    "these approaches employ continuous random search methodology through the use of monte carlo markov chain algorithms although the objective is not minimization of equation .",
    "a review and comparison of some of these approaches is given by @xcite .",
    "although most of the automatic knot selection procedures mentioned here have exhibited good performance , they all face a nonlinear problem with a lethargy property @xcite .",
    "the `` lethargy '' property is intrinsic to free - knot spline problems . in the other word",
    ", there may be local minima , saddle points , or even local maxima for the optimal knot placement .",
    "the `` lethargy '' typically cause many problems for the derivative - based optimization methodology and could lead to a poor estimation .",
    "we show that the proposed hararm algorithm has good properties for solving global optimization problems , in particular , finding the the numbers and locations of the free knot regression model efficiently and reliably .",
    "the overall procedure of the free - knot idea is investigated by simulated data sets .",
    "the proposed hararms algorithm is described in detail in section 2 , where a brief description , which has more details than above , to the related algorithms are also given .",
    "then by comparing and discussing the performances of sampling in multidimensional space by arms with gibbs sampler and hit and run algorithm in section 3 , we show that haearms is significantly better in multidimensional situation , especially there are multi - modes , which is the case in almost all real world problems . in the last section ,",
    "the hararms is used to fit the data to the free knots regression spline , and the performance is assessed .",
    "in the bayesian context , the objectives of the modeling are usually posterior distributions of the unknown parameters .",
    "high dimensional distribution samples can be generated from the gibbs sampling @xcite straightforwardly for bayesian inference . at each iteration of the gibbs sampling ,",
    "the parameter is updated by sampling a new value from its full conditional distribution .",
    "the full conditional distribution of a parameter is its distribution conditional on the data and on the current values of all the other parameters . for the completeness",
    ", we introduce the adaptive rejection sampling(ars ) and the adaptive rejection metropolis sampling(arms ) first .",
    "we define @xmath1 to be the random variable to be sampled .",
    "the ars extends the rejection sampling @xcite by adaptively adjusting the sampling distribution . for a basic rejection sampling method",
    ", it requires that a sampling distribution @xmath2 for a random variable @xmath3 can be easily drawn and @xmath4 given a finite constant @xmath5 .",
    "the rejection sample method is very useful for sampling a full conditional distribution since the integration of @xmath6 is not needed . in practice",
    ", this algorithm involves an evaluation and potential rejection step after a sample is drawn from @xmath2 . for most of the time ,",
    "it is a challenging task to pick up an appropriate sample distribution @xmath2 and the constant @xmath5 in order to reduce the ratio of the rejection .",
    "the ars @xcite is proposed to use a sequence of sampling distributions @xmath7 defined by piecewise linear functions @xmath8 : @xmath9 , \\ : { { \\rm x}}_l \\leq { { \\rm x } } < { { \\rm x}}_{l+1},\\ ] ] where @xmath10 is a set of points in the support of @xmath11 .",
    "@xmath12 denotes the straight line through points @xmath13 $ ] and @xmath14 $ ] . defining @xmath15 ,",
    "the sampling distribution is given by : @xmath16 now the algorithm of ars is derived as the following :    * initialize @xmath17 and @xmath18 .",
    "* generate @xmath19}$ ] * if @xmath20 , update @xmath18 to @xmath21 ; otherwise , accept @xmath22 .    for univariate cases ,",
    "the average of iterations of the ars to accept one point depends on the initial @xmath18 and the target distribution @xmath11 .",
    "the biggest limitation of ars is to require @xmath11 to be log - concave since @xmath23 needs to be an envelope of @xmath24 .",
    "the arms @xcite was proposed to deal with non - log - concave densities .",
    "the arms incorporates a metropolis - hastings algorithm step to ars .",
    "the metropolis - the hastings algorithm is briefly covered as the following :    * initialize @xmath25 * generate @xmath26}$ ] . * if @xmath27 , @xmath28 ; otherwise accept @xmath22 .",
    "the metropolis - hastings algorithm(mh ) @xcite improved the metropolis algorithm @xcite by generating samples from a proposal distribution @xmath29 .",
    "the performance of the mh algorithm depends on the quality of the proposal distribution .",
    "similar to other mcmc algorithms , samples from the mh algorithm may stay in a local maximum of @xmath11 due to an unsuitable proposal distribution .    to carry out the arms algorithm @xcite ,",
    "let @xmath30 denote a current set of points in ascending order , for @xmath31 , let @xmath32 denote the straight line through points @xmath13 $ ] and @xmath33 $ ] .",
    "a piecewise linear function @xmath34 $ ] , where @xmath35 .",
    "let @xmath37 and @xmath38 define the first and the last piecewise linear functions , respectively .",
    "the sampling distribution is then @xmath39 , where @xmath40 .",
    "we further define @xmath41 and @xmath42 as the current value and new sample from @xmath6 .",
    "arms starts from @xmath37 and construct @xmath43 ; we include a new point and relabel points ; we then reconstruct @xmath44 . for multivariate distribution , we reserve the bold font @xmath45 for the objective random variable vector .",
    "let @xmath46 denote the dimension of the variable and @xmath47 .",
    "assuming @xmath48 is a target univariate distribution to be selected , where @xmath49 is the parameters except @xmath50 .",
    "for ease of notation we write @xmath51 below instead of @xmath48 .",
    "the algorithm of arms embedded in gibbs sampler for multivariate distribution is described as the following :    * for @xmath52 to @xmath53 , let @xmath54 denote @xmath55 in the following steps .",
    "do * simulate @xmath56 from @xmath57 and @xmath58}$ ] until @xmath59 * generate @xmath58}$ ] and take + @xmath60}{f({{\\rm x}}_{cur } ) , \\min[f({{\\rm x}}_{a})\\exp(h_m({{\\rm x}}_{a } ) ) ] } \\biggr\\ } , \\\\ { \\rm x}_{cur } & \\qquad otherwise .",
    "\\end{array } \\right .\\ ] ]    the hit - and - run algorithm can be thought of as random - direction gibbs : in each step of the hit - and - run algorithm , instead of updating @xmath3 along one of the dimensions , we update it along a randomly generated direction that is not necessarily parallel to any dimension .",
    "more precisely , the sampler is defined in two steps : first , choose a direction @xmath61 from some positive density on the unit sphere @xmath62 .",
    "then , similar to gibbs , sample the new point @xmath63 along the line specified by @xmath64 and the distance by @xmath65 , where @xmath65 is from the marginal one - dimensional density on the line that specified by @xmath64 .",
    "the arms with hit - and - run ( hararms ) algorithm is as the following :    * initiate @xmath25 , generate @xmath66 , @xmath67}$],and set @xmath68 , initiate @xmath5 and @xmath18 for @xmath69 . *",
    "simulate @xmath70 from @xmath71 and @xmath58}$ ] until @xmath72 * generate @xmath58}$ ] and take + @xmath73}{f({z}_{cur } ) , \\min[f({z}_{a})\\exp(h_m({z}_{a } ) ) ] } \\biggr\\ } , \\\\ { { \\rm x}}_{s-1}+ d^t z_{cur } & \\qquad otherwise .",
    "\\end{array } \\right .\\ ] ]",
    "the arms with hit - and - run reduces a multiple dimensional question into one dimensional arms sampling .",
    "it breaks the procedure into two steps : ( 1 ) .",
    "pick up a random direction d ; ( 2 ) . implement a one dimensional arms for the random variable z. an important advantage of arms with hit - and - run over regular multiple dimensional arms",
    "is that it is much more likely to reach the isolated local areas by evaluating one dimensional arms in a random direction searching , while regular multiple dimensional arms only searches the space in the direction that is parallel to one of dimensions in each updating step .",
    "for example , in a two dimensional case , the gibbs sampler sampling the new points either in the vertical or the horizontal direction from the current point .",
    "the following example shows that even for sampling from a mixture of 4 bivariate normal distribution with similar mixing probability mass and variances , the gibbs sampler with arms can be trapped around only 3 of the 4 modes and totally missing the forth one .",
    "this is mainly due to that the forth mode is hardly reached by searching in the 2 dimensional space along the directions only parallel to the axes in each step .    assuming that we have a two dimensional random variable , @xmath74 , and the the objective sampling distribution is @xmath75 as below : @xmath76 in this example , we set @xmath77 , @xmath78 , @xmath79 , and @xmath80 .",
    "@xmath81 , @xmath82 , @xmath83 , @xmath84 and @xmath85 .",
    "sampling @xmath45 from @xmath75 is an easy task if we know that @xmath75 is a mixture of normal distributions .",
    "we can pick one sampling distribution among the four in terms of their probabilities @xmath86 , and then sample @xmath45 from the selected normal distribution .",
    "consider @xmath75 as if it did not have the explicit and easy form of mixture to sampling form , and treat it as a general distribution , which is the case as sampling from general complex functions .",
    "then we use both the gibbs sampler with arms embedded and the hararms to sample @xmath45 from the @xmath75 , and compare their performances .",
    "after @xmath87=10000 iteration , we generate the sampling of @xmath45 from both gibbs sampler with arms embedded and hararms . to make a reference , contour plots are made by the grid search on @xmath88 and @xmath89 , which refer as x1 and x2 in the figure @xmath90 .",
    "the generated data is following a quadratic spline function with 6 knots at 0.2 , 0.4 , 0.5 , 0.7 and 0.9 .",
    "the @xmath91-th big row shows the estimation for the location of the knots , corresponding to the fitted model has @xmath91 knots .",
    "also , the 90% credible intervals to these estimation are reported . in the last column ,",
    "the value of the log - likelihood function corresponding to the estimated values and the boundaries of the intervals are listed .",
    "not surprisingly , the log - likelihood function value of the estimations increases as the number of the knots increases .",
    "however , when the number of the knots in the model reaches the truth , the increasing in log - likelihood could be neglected , compared to the increasing before the number of knots in the model reaches the true number .",
    "[ table : nonlin4 ]      gilks wr , wild p. adaptive rejection sampling for gibbs sampling . applied statistics 1992;41:337 - 348 .",
    "hastings wk .",
    "monte carlo sampling methods using markove chains and their applications .",
    "biometrika , ( 1970).;57:97 - 109 .",
    "gilks wr , best ng , tan kkc .",
    "adaptive rejection metropolis sampling . _ applied statistics _ , 1995;44:455 - 472 meyer r , millar br .",
    "bayesian stock assessment using a statespace implementation of the delay difference model .",
    "canadian journal of fisheries and aquatic sciences , 1999 ; 56(1):37 - 52 .",
    "meyer r , christensen n. bayesian reconstruction of chaotic dynamical systems .",
    "physical review e , 2000 ; 62(3):3535 .",
    "putter h , heisterkamp sh , lange jma , de wolf f. a bayesian approach to parameter estimation in hiv dynamical models .",
    "statistics in medicine , 2002;21:2199 - 2214 .",
    "miranda mf , zhu h , ibrahim jg .",
    "bayesian spatial transformation models with applications in neuroimaging data .",
    "biometrics , 2013;69(4):1074 - 1083 .",
    "yelland pm , dong x. forecasting demand for fashion goods : a hierarchical bayesian approach .",
    "intelligent fashion forecasting systems : models and applications .",
    "springer berlin heidelberg , 2014;71 - 94 .",
    "schmeiser bw .",
    "random variate generation : a suvey .",
    "simulation with discrete models : state of the art view , t.i .",
    "oren , c.m.shub , p.f.roth(eds ) , ieee , 1980;:79 - 104 .",
    "rubinstein ry .",
    "generating random vectors uniformly distributed inside and on the surface of different regions .",
    "european journal of operations research , 1982 ; 10:205 - 209 .",
    "andersen hc , diaconis p. hit and run as a unifying device .",
    "yournal de la socit franaise de statistique , 2007;148(5):5 - 28 .",
    "lovsz , lszl , and santosh vempala .",
    "hit - and - run is fast and fun .",
    "preprint , microsoft research , 2003 .",
    "chen mh , schmeiser b. performance of the gibbs , hit - and - run , and metropolis samplers .",
    "journal of computational and graphical statistics , 1993;2(3):251 - 272 .",
    "roberts go , gilks wr .",
    "convergence of adaptive direction sampling .",
    "journal of multivariate analysis , 1994;49(2):287 - 298 .",
    "the approximation of functions , vol .",
    "2 . , addison - wesley , reading , mass ; 1969 . smith pl .",
    "splines as a useful and convenient statistical tool .",
    "the american statistician , 1979;33(2):57 - 62 .",
    "ruppert d , wand m , carroll r. semiparametric regression , cambridge university press , cambridge , 2003 .",
    "wold s. spline functions in data analysis .",
    "technometrics , 1974;16(1):1 - 11 .",
    "hypothesis testing in b - spline regression .",
    "communications in statistics , part b , 1982;11:143 - 57 .",
    "friedman jh , silverman bw .",
    "flexible parsimonious smoothing and additive modeling .",
    "technometrics , 1989;31:3 - 21 .",
    "stone cj , hansen mh , kooperberg c , truong yk .",
    "polynomial splines and their tensor products in extended linear modeling .",
    "annals of statistics , 1997;25:1371 - 1425 .",
    "spiriti s , eubank r , smith pw , young d. knot selection for least - squares and penalized splines .",
    "journal of statistical computation and simulation , 2013;83:1020 - 1036 .",
    "smith mc kohn r. nonparametric regression using bayesian variable selection . journal of econometrics , 1996;75(2):317 - 343 .",
    "denison , dgt , mallick , bk , and smith , afm .",
    "automatic bayesian curve fitting .",
    "journal of the royal statistical society : series b ( statistical methodology ) , 1997;60(2 ) , 333 - 350 .",
    "dimatteo i , genovese cr , kass re .",
    "bayesian curve fitting with free - knot splines .",
    "biometrika , 2001 ; 88(4):1055 - 1071 .",
    "lindstrom , mj .",
    "bayesian estimation of free knot splines using reversible jumps .",
    "computational statistics & data analysis , 2002;41(2):255 - 269 .",
    "a comparison of regression spline smoothing procedures .",
    "computational statistics , 2000;15:44362 .",
    "gelfand ae , smith af .",
    "sampling - based approaches to calculating marginal densities .",
    "journal of the american statistical association , 1990;85(410):398 - 409 .",
    "ripley , b. stochastic simulation .",
    "new york , wiley ; 1987 .",
    "metropolis n , rosenblth aw , rosenbluth mn , teller ah , teller e. equations of state calculations by fast computing machines j. chem phys .",
    ", 1953;21:1087 - 1092 .",
    "approximation to data by splines with free knots .",
    "siam journal on numerical analysis , 1978;15(2):328 - 343 ."
  ],
  "abstract_text": [
    "<S> an algorithm for sampling from non - log - concave multivariate distributions is proposed , which improves the adaptive rejection metropolis sampling ( arms ) algorithm by incorporating the hit and run sampling . </S>",
    "<S> it is not rare that the arms is trapped away from some subspace with significant probability in the support of the multivariate distribution . while the arms updates samples only in the directions that are parallel to dimensions , our proposed method , the hit and run arms ( hararms ) , updates samples in arbitrary directions determined by the hit and run algorithm , which makes it almost not possible to be trapped in any isolated subspaces . </S>",
    "<S> the hararms performs the same as arms in a single dimension while more reliable in multidimensional spaces . </S>",
    "<S> its performance is illustrated by a bayesian free - knot spline regression example . </S>",
    "<S> we showed that it overcomes the well - known ` lethargy ' property and decisively find the global optimal number and locations of the knots of the spline function .    </S>",
    "<S> * keywords * : adaptive rejection metropolis sampling ; hit - and - run algorithm ; regression splines ; free - knot splines , empirical bayesian method . + </S>",
    "<S> * classcode*62g08 , 62c12 . </S>"
  ]
}