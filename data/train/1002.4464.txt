{
  "article_text": [
    "modern graphics processors ( _ _ gpu__s ) have evolved into highly parallel and fully programmable architectures .",
    "current many - core gpus such as nvidia s gtx and tesla gpus can contain up to 240 processor cores on one chip and can have an astounding peak performance of up to 1 tflop .",
    "the upcoming fermi gpu recently announced by nvidia is expected to have more than 500 processor cores .",
    "however , gpus are known to be hard to program and current general purpose ( i.e. non - graphics ) gpu applications concentrate typically on problems that can be solved using fixed and/or regular data access patterns such as image processing , linear algebra , physics simulation , signal processing and scientific computing ( see e.g. @xcite ) .",
    "the design of efficient gpu methods for discrete and combinatorial problems with data dependent memory access patterns is still in its infancy .",
    "in fact , there is currently still a lively debate even on the best _ sorting _ method for gpus ( e.g. @xcite ) .    until very recently ,",
    "the comparison - based thrust merge method @xcite by nadathur satish , mark harris and michael garland of nvidia corporation was considered the best sorting method for gpus .",
    "however , an upcoming paper by nikolaj leischner , vitaly osipov and peter sanders @xcite ( to appear in proc .",
    "ipdps 2010 ) presents a randomized sample sort method for gpus that significantly outperforms thrust merge .",
    "a disadvantage of the randomized sample sort method is that its performance can vary with the input data distribution because the data is partitioned into buckets that are created via _",
    "randomly _ selected data items .    in this paper",
    ", we present and evaluate a _ deterministic _ sample sort algorithm for gpus , called gpu bucket sort , which has the same performance as the randomized sample sort method in @xcite . an experimental performance comparison on nvidia",
    "s gtx 285 and tesla architectures shows that for uniform data distribution , the _ _ best case _ _ for randomized sample sort , our deterministic sample sort method is in fact _ exactly _ as fast as the randomized sample sort method of @xcite .",
    "however , in contrast to @xcite , the performance of gpu bucket sort remains the same for any input data distribution because buckets are created deterministically and bucket sizes are guaranteed .",
    "the remainder of this paper is organized as follows .",
    "section [ sec : review :- gpu - architectures ] reviews the _",
    "tesla _ architecture framework for gpus and",
    "the cuda programming environment , and section [ sec : previous - work ] reviews previous work on gpu based sorting .",
    "section [ sec : deterministic - sample - sort ] presents gpu bucket sort and discusses some details of our cuda @xcite implementation . in section [ sec : experimental - results - and ] , we present an experimental performance comparison between our deterministic sample sort implementation , the randomized sample sort implementation in @xcite , and the thrust merge implementation in @xcite .",
    "in addition to the performance improvement discussed above , our deterministic sample sort implementation appears to be more memory efficient as well because gpu bucket sort is able to sort considerably larger data sets within the same memory limits of the gpus .",
    "as in @xcite and @xcite , we will focus on nvidia s unified graphics and computing platform for gpus known as the _ tesla _ architecture framework @xcite and associated _ cuda _ programming model @xcite",
    ". however , the discussion and methods presented in this paper apply in general to gpus that support the opencl standard @xcite which is very similar to cuda . a schematic diagram of the tesla unified gpu architecture is shown in figure [ fig : nvidia - tesla - architecture ] .",
    "a tesla gpu consists of an array of streaming processors called _ streaming multiprocessors _ _ _ ( sm__s ) .",
    "each sm contains eight processor cores and a small size ( 16 kb ) low latency local",
    "_ shared memory _ that is shared by its eight processor cores .",
    "all sms are connected to a _",
    "global _ dram _ memory _ through an interconnection network .",
    "for example , an nvidia geforce gtx 260 has 27 sms with a total of 216 processor cores while gtx 285 and tesla gpus have 30 sms with a total of 240 processor cores .",
    "a gtx 260 has approximately 900 mb global dram memory while gtx 285 and tesla gpus have up to 2 gb and 4 gb global dram memory , respectively ( see table [ tab : gpu - performance - characteristics ] ) .",
    "a gpu s global dram memory is arranged in independent memory partitions .",
    "the interconnection network routes the read / write memory requests from the processor cores to the respective global memory partitions , and the results back to the cores .",
    "each global memory partition has its own queue for memory requests and arbitrates among the incoming read / write requests , seeking to maximize dram transfer efficiency by grouping read / write accesses to neighboring memory locations .",
    "memory latency to global dram memory is optimized when parallel read / write operations can be grouped into a minimum number of arrays of contiguous memory locations .",
    "it is important to note that data accesses from processor cores to their sm s local shared memory are at least an order of magnitude faster than accesses to global memory .",
    "this is an important consideration for any efficient sorting method .",
    "another critical issue for the performance of cuda implementations is conditional branching .",
    "cuda programs typically execute very large numbers of threads .",
    "in fact , a large number of threads is critical for hiding latencies for global memory accesses .",
    "the gpu has a hardware thread scheduler that is built to manage tens of thousands and even millions of concurrent threads .",
    "all threads are divided into blocks of up to 512 threads , and each block is executed by an sm .",
    "an sm executes a thread block by breaking it into groups of 32 threads called _ warps _ and executing them in parallel using its eight cores .",
    "these eight cores share various hardware components , including the instruction decoder .",
    "therefore , the threads of a warp are executed in simt ( single instruction , multiple threads ) mode , which is a slightly more flexible version of the standard simd ( single instruction , multiple data ) mode .",
    "the main problem arises when the threads encounter a conditional branch such as an if - then - else statement .",
    "depending on their data , some threads may want to execute the code associated with the `` true '' condition and some threads may want to execute the code associated with the `` false '' condition . since the shared instruction decoder can only handle one branch at a time , different threads can not execute different branches concurrently .",
    "they have to be executed in sequence , leading to performance degradation .",
    "gpus provide a small improvement through an instruction cache at each sm that is shared by its eight cores .",
    "this allows for a `` small '' deviation between the instructions carried out by the different cores .",
    "for example , if an if - then - else statement is short enough so that both conditional branches fit into the instruction cache then both branches can be executed fully in parallel . however",
    ", a poorly designed algorithm with too many and/or large conditional branches can result in serial execution and very low performance .",
    "sorting algorithms for gpus started to appear a few years ago and have been highly competitive .",
    "early results include gputerasort @xcite based on bitonic merge , and adaptive bitonic sort @xcite based on a method by bilardi et.al .",
    "hybrid sort @xcite used a combination of bucket sort and merge sort , and d. cederman et.al .",
    "@xcite proposed a quick sort based method for gpus . both methods ( @xcite )",
    "suffer from load balancing problems . until very recently , the comparison - based thrust merge method @xcite by nadathur satish , mark harris and michael garland of nvidia corporation was considered the best sorting method for gpus .",
    "thrust merge uses a combination of odd - even merge and two - way merge , and overcomes the load balancing problems mentioned above .",
    "satish et.al .",
    "@xcite also presented an even faster gpu radix sort method for the special case of integer sorting . yet , an upcoming paper by nikolaj leischner , vitaly osipov and peter sanders @xcite ( to appear in proc .",
    "ipdps 2010 ) presents a randomized sample sort method for gpus that significantly outperforms thrust merge @xcite .",
    "however , as discussed in section [ sec : introduction ] , the performance of randomized sample sort can vary with the distribution of the input data because buckets are created through randomly selected data items .",
    "indeed , the performance analysis presented in @xcite measures the runtime of their randomized sample sort method for six different data distributions to document the performance variations observed for different input distributions .",
    "in this section we present gpu bucket sort , a _ deterministic _ sample sort algorithm for gpus , and discuss its cuda implementation .",
    "an outline of gpu bucket sort is shown in algorithm [ alg : deterministic - sample - sort ] below .",
    "it consists of a local sort ( step 1 ) , a selection of samples that define balanced buckets ( steps 3 - 5 ) , moving all data into those buckets ( steps 6 - 8 ) , and a final sort of each bucket .",
    "[ alg : deterministic - sample - sort ] gpu bucket sort ( deterministic sample sort for gpus )    _ input _ : an array @xmath0 with @xmath1 data items stored in global memory .    _ output _ : array @xmath0 sorted .    1",
    ".   split the array @xmath0 into @xmath2 sublists @xmath3 containing @xmath4 items each where @xmath4 is the shared memory size at each sm .",
    "local sort _ : sort each sublist @xmath5 ( @xmath6=1 , ... , @xmath2 ) locally on one sm , using the sm s shared memory as a cache .",
    "3 .   _ local sampling _ : select @xmath7 equidistant samples from each sorted sublist @xmath5 ( @xmath6=1 , ... ,",
    "@xmath2 ) for a total of @xmath8 samples . 4 .",
    "_ sorting all samples _ : sort all @xmath8 samples in global memory , using all available sms in parallel .",
    "_ global sampling _ : select @xmath7 equidistant samples from the sorted list of @xmath8 samples",
    ". we will refer to these @xmath7 samples as _",
    "global samples_. 6 .",
    "_ sample indexing _ : for each sorted sublist @xmath5 ( @xmath6=1 , ... ,",
    "@xmath2 ) determine the location of each of the @xmath7 global samples in @xmath5 .",
    "this operation is done for each @xmath5 locally on one sm , using the sm s shared memory , and will create for each @xmath5 a partitioning into @xmath7 buckets @xmath9 , ... ,",
    "@xmath10 of size @xmath11 , ... , @xmath12",
    "_ prefix sum _ : through a parallel prefix sum operation on @xmath13 , ... ,",
    "@xmath14 , @xmath15 , ... , @xmath16 , ... , @xmath17 , ... , @xmath18 calculate for each bucket @xmath19(@xmath20 , @xmath21 , ) its starting location @xmath22 in the final sorted sequence .",
    "_ data relocation _ : move all @xmath8 buckets @xmath19(@xmath23 , 1@xmath24 ) to location @xmath22 .",
    "the newly created array consists of @xmath7 sublists @xmath25 , ... , @xmath26 where @xmath27 for 1@xmath24 .",
    "_ sublist sort _ : sort all sublists @xmath28 , 1@xmath24 , using all sms .",
    "our discussion of algorithm [ alg : deterministic - sample - sort ] and its implementation will focus on gpu performance issues related to shared memory usage , coalesced global memory accesses , and avoidance of conditional branching .",
    "consider an input array @xmath0 with @xmath29 data items and a local shared memory size of @xmath30 data items . in steps 1 and 2 of algorithm [",
    "alg : deterministic - sample - sort ] , we split the array a into @xmath31 sublists of @xmath32 data items each and then locally sort each of those @xmath31 sublists . more precisely , we create 16 k thread blocks of 512 threads each , where each thread block sorts one sublist using one sm .",
    "each thread block first loads a sublist into the sm s local shared memory using a coalesced parallel read from global memory .",
    "note that , each of the 512 threads is responsible for @xmath33 data items .",
    "the thread block then sorts a sublist of @xmath30 data items in the sm s local shared memory .",
    "we tested different implementations for the local shared memory sort within an sm , including quicksort , bitonic sort , and adaptive bitonic sort @xcite . in our experiments ,",
    "bitonic sort was consistently the fastest method , despite the fact that it requires @xmath34 work .",
    "the reason is that , for step 2 of algorithm [ alg : deterministic - sample - sort ] , we always sort @xmath32 data items only , irrespective of @xmath1 .",
    "for such a small number of items the simplicity of bitonic sort , it s small constants in the running time , and it s perfect match for simd style parallelism outweigh the disadvantage of additional work . in step 3 of algorithm [ alg : deterministic - sample - sort ] , we select @xmath35 equidistant samples from each sorted sublist .",
    "the choice of value for @xmath7 is discussed in section [ sec : experimental - results - and ] .",
    "the implementation of step 3 is built directly into the final phase of step 2 when the sorted sublists are written back into global memory . in step 4",
    ", we are sorting all @xmath36 selected samples in global memory , using all available sms in parallel . here , we compared gpu bitonic sort @xcite , adaptive bitonic sort @xcite based on @xcite , and randomized sample sort @xcite .",
    "our experiments indicate that for up to 16 m data items , simple bitonic sort is still faster than even randomized sample sort @xcite due to its simplicity , small constants , and complete avoidance of conditional branching .",
    "hence , step 4 was implemented via bitonic sort . in step 5 ,",
    "we select @xmath35 equidistant _ global samples _ from the sorted list of @xmath36 samples . here , each thread block / sm loads the @xmath35 global samples into its local shared memory where they will remain for the next step . in step 6 , we determine for each sorted sublist @xmath5 ( @xmath6=1 , ... , @xmath2 ) of @xmath30 data items the location of each of the @xmath35 global samples in @xmath5 . for each @xmath5",
    ", this operation is done locally by one thread block on one sm , using the sm s shared memory , and will create for each @xmath5 a partitioning into @xmath35 buckets @xmath9 , ... , @xmath10 of size @xmath11 , ... , @xmath12 . here",
    ", we apply parallel binary search in @xmath5 for each of the global samples .",
    "more precisely , we first take the @xmath37-th global sample element and use one thread to perform a binary search in @xmath5 , resulting in a location @xmath38 in @xmath5 .",
    "then we use two threads to perform two binary searches in parallel , one for the @xmath39-th global sample element in the part of @xmath5 to the left of location @xmath38 , and one for the @xmath40-th global sample element in the part of @xmath5 to the right of location @xmath38 .",
    "this process is iterated @xmath41 times until all @xmath35 global samples are located in @xmath5 . with this , each @xmath5 is split into @xmath35 buckets @xmath9 , ... , @xmath10 of size @xmath11 , ... ,",
    "note that , we do not simply perform all @xmath7 binary searches fully in parallel in order to avoid memory contention within the local shared memory@xcite .",
    "step 7 uses a prefix sum calculation to obtain for all buckets their starting location in the final sorted sequence .",
    "the operation is illustrated in figure [ fig : illustration - of - step-7 ] and can be implemented with coalesced memory accesses in global memory .",
    "each row in figure [ fig : illustration - of - step-7 ] shows the @xmath11 , ... , @xmath12 calculated for each sublist .",
    "the prefix sum is implemented via a parallel column sum ( using all sms ) , followed by a prefix sum on the columns sums ( on one sm in local shared memory ) , and a final update of the partial sums in each column ( using all sms ) . in step 8 ,",
    "the @xmath36 buckets are moved to their correct location in the final sorted sequence .",
    "this operation is perfectly suited for a gpu and requires one parallel coalesced data read followed by one parallel coalesced data write operation .",
    "the newly created array consists of @xmath35 sublists @xmath25 , ... , @xmath26 where each @xmath27 has at most @xmath42 data items @xcite . in step 9",
    ", we sort each @xmath28 using the same bitonic sort implementation as in step 4 .",
    "note that , since each @xmath28 is smaller than @xmath43 data items , simple bitonic sort is faster for each @xmath28 than even randomized sample sort @xcite due to bitonic sort s simplicity , small constants , and complete avoidance of conditional branching .",
    "[ fig : illustration - of - step-7],width=264 ]",
    "for our experimental evaluation , we executed algorithm [ alg : deterministic - sample - sort ] on three different gpus ( nvidia tesla , gtx 285 , and gtx 260 ) for various data sets of different sizes , and compared our results with those reported in @xcite and @xcite which are the current best gpu sorting methods .",
    "table [ tab : gpu - performance - characteristics ] shows some important performance characteristics of the three different gpus .",
    "the tesla and gtx 285 have more cores than the gtx 260 .",
    "the gtx 285 has the highest core clock rate and in summary the highest level of core computational power .",
    "the tesla has the largest memory but the gtx 285 has the best memory clock rate and memory bandwidth .",
    "in fact , even the gtx 260 has a higher clock rate and memory bandwidth than the tesla c1060 .",
    "figure [ fig : det-260 - 285-tesla - comp ] shows a comparison of the runtime of our gpu bucket sort method on the tesla c1060 , gtx 260 and gtx 285 ( with 2 gb memory ) for varying number of data items .",
    "each data point shows the average of 100 experiments .",
    "the observed variance was less than 1 ms for all data points since gpu bucket sort is deterministic and any fluctuation observed was due to noise on the gpu ( e.g. operating system related traffic ) .",
    "all three curves show a growth rate very close to linear which is encouraging for a problem that requires @xmath44 work .",
    "gpu bucket sort performs better on the gtx 285 than both tesla and gtx 260 .",
    "furthermore , it performs better on the gtx 260 than on the tesla c1060 .",
    "this indicates that gpu bucket sort is memory bandwidth bound which is expected for sorting methods since the sorting problem requires only very little computation but a large amount of data movement . for individual steps of gpu bucket sort , the order can sometimes be reversed .",
    "for example , we observed that step 2 of algorithm [ alg : deterministic - sample - sort ] ( local sort ) runs faster on the tesla c1060 than on the gtx 260 since this step is executed locally on each sm and its performance is largely determined by the number of sms and the performance of the sm s cores .",
    "however , the gtx 285 remained the fastest machine , even for all individual steps .",
    "we note that gpu bucket sort can sort up to @xmath45 data items within the 896 mb memory available on the gtx 260 ( see figure [ fig : det-260 - 285-tesla - comp ] ) . on the gtx 285 with 2",
    "gb memory and tesla c1060 our gpu bucket sort method can sort up to @xmath46 and @xmath47 data items , respectively ( see figures [ fig : det - rand - merge-285]&[fig : det - rand - merge - tesla ] ) .",
    "figure [ fig : det - steps ] shows in detail the time required for the individual steps of algorithm [ alg : deterministic - sample - sort ] when executed on a gtx 285 .",
    "we observe that _ sublist sort _ ( step 9 ) and _ local sort _ ( step 2 ) represent the largest portion of the total runtime of gpu bucket sort .",
    "this is very encouraging in that the `` overhead '' involved to manage the deterministic sampling and generate buckets of guaranteed size ( steps 3 - 7 ) is small .",
    "we also observe that the _ data relocation _ operation ( step 8) is very efficient and a good example of the gpu s great performance for data parallel access when memory accesses can be coalesced ( see section [ sec : review :- gpu - architectures ] ) .",
    "note that , for algorithm [ alg : deterministic - sample - sort ] the sample size @xmath7 is a free parameter . with increasing @xmath7 , the sizes of sublists @xmath28 created in step 8 of algorithm [ alg : deterministic - sample - sort ] decrease and the time for step 9 decreases as well .",
    "however , the time for steps 3 - 7 grows with increasing @xmath7 .",
    "this trade - off is illustrated in figure [ fig : runtime - sample - size ] which shows the total runtime for algorithm [ alg : deterministic - sample - sort ] as a function of @xmath7 for fixed @xmath48 .",
    "as shown in figure [ fig : runtime - sample - size ] , the total runtime is smallest for @xmath35 , which is the parameter chosen for our gpu bucket sort code .",
    "figures [ fig : det - rand - merge-285 ] and [ fig : det - rand - merge - tesla ] show a comparison between gpu bucket sort and the current best gpu sorting methods , randomized sample sort @xcite and thrust merge sort @xcite .",
    "figure [ fig : det - rand - merge-285 ] shows the runtimes for all three methods on a gtx 285 and figure [ fig : det - rand - merge - tesla ] shows the runtimes of all three methods on a tesla c1060 .",
    "note that , @xcite and @xcite did not report runtimes for the gtx 260 . for gpu bucket",
    "sort , all runtimes are the averages of 100 experiments , with less than 1 ms observed variance . for randomized sample",
    "sort and thrust merge sort , the runtimes shown are the ones reported in @xcite and @xcite . for thrust",
    "merge sort , performance data is only available for up to @xmath49 data items . for larger values of @xmath1 , the current thrust",
    "merge sort code shows memory errors @xcite . as reported in @xcite , the current randomized sample sort code can sort up to @xmath50 data items on a gtx 285 with 1 gb memory and up to @xmath51 data items on a tesla c1060 .",
    "our gpu bucket sort code appears to be more memory efficient .",
    "gpu bucket sort can sort up to @xmath46 data items on a gtx 285 with 2 gb memory and up to @xmath47 data items on a tesla c1060 .",
    "therefore , figures [ fig : det - rand - merge-285]a and [ fig : det - rand - merge - tesla]a show the performance comparison with higher resolution for up to @xmath45 and @xmath52 , respectively , while figures [ fig : det - rand - merge-285]b and [ fig : det - rand - merge - tesla]b show the performance comparison for the entire range up to @xmath46 and @xmath47 , respectively .",
    "we observe in figures [ fig : det - rand - merge-285]a and [ fig : det - rand - merge - tesla]a that , as reported in @xcite , randomized sample sort @xcite significantly outperforms thrust merge sort @xcite .",
    "most importantly , we observe that randomized sample sort @xcite and our deterministic sample sort ( gpu bucket sort ) show nearly identical performance on both , the gtx 285 and tesla c1060 .",
    "note that , the experiments in @xcite used a gtx 285 with 1 gb memory whereas we used a gtx 285 with 2 gb memory .",
    "as shown in table [ tab : gpu - performance - characteristics ] , the gtx 285 with 1 gb has a slightly better memory clock rate and memory bandwidth than the gtx 285 with 2 gb which implies that the performance of deterministic sample sort ( gpu bucket sort ) on a gtx 285 is actually a few percent better than the performance of randomized sample sort .",
    "the data sets used for the performance comparison in figures [ fig : det - rand - merge-285 ] and [ fig : det - rand - merge - tesla ] were uniformly distributed , random data items .",
    "the data distribution does not impact the performance of deterministic sample sort ( gpu bucket sort ) but has an impact on the performance of randomized sample sort .",
    "in fact , the uniform data distribution used for figures [ fig : det - rand - merge-285 ] and [ fig : det - rand - merge - tesla ] is a _",
    "best case _",
    "scenario for randomized sample sort where all bucket sizes are nearly identical .",
    "figures [ fig : det - rand - merge-285]b and [ fig : det - rand - merge - tesla]b show the performance of gpu bucket sort for up to @xmath46 and @xmath47 , respectively . for both architectures ,",
    "gtx 285 and tesla c1060 , we observe a very close to linear growth rate in the runtime of gpu bucket sort for the entire range of data sizes .",
    "this is very encouraging for a problem that requires @xmath44 work . in comparison with randomized sample",
    "sort , the linear curves in figures [ fig : det - rand - merge-285]b and [ fig : det - rand - merge - tesla]b show that our gpu bucket sort method maintains a fixed _ sorting rate _ ( number of sorted data items per time unit ) for the entire range of data sizes , whereas it is shown in @xcite that the sorting rate for randomized sample sort fluctuates and often starts to decrease for larger values of @xmath1 .",
    "in this paper , we presented a _",
    "deterministic _ sample sort algorithm for gpus , called gpu bucket sort .",
    "our experimental evaluation indicates that gpu bucket sort is considerably faster than thrust merge @xcite , the best comparison - based sorting algorithm for gpus , and it is exactly as fast as the new randomized sample sort for gpus @xcite when the input data sets used are uniformly distributed , which is a _ best case _ scenario for randomized sample sort .",
    "however , as observed in @xcite , the performance of randomized sample sort fluctuates with the input data distribution whereas gpu bucket sort does not show such fluctuations .",
    "in fact , gpu bucket sort showed a fixed _ sorting rate _ ( number of sorted data items per time unit ) for the entire range of data sizes tested ( up to @xmath47 data items ) .",
    "in addition , our gpu bucket sort implementation appears to be more memory efficient because gpu bucket sort is able to sort considerably larger data sets within the same memory limits of the gpus .",
    "n.  govindaraju , j.  gray , r.  kumar , and d.  manocha . : high performance graphics co - processor sorting for large database management . in _ proc .",
    "international conference on management of data ( sigmod ) _ , pages 325  336 , 2006 .",
    "n.  leischner , v.  osipov , and p.  sanders .",
    "sample sort . in _ proc . intl parallel and distributed processing symposium ( ipdps ) , to appear _ , 2010 ( currently available at http://arxiv1.library.cornell.edu/abs/0909.5649 ) ."
  ],
  "abstract_text": [
    "<S> we present and evaluate gpu bucket sort , a parallel _ deterministic _ sample sort algorithm for many - core gpus . </S>",
    "<S> our method is considerably faster than thrust merge ( satish et.al . </S>",
    "<S> , proc . </S>",
    "<S> ipdps 2009 ) , the best comparison - based sorting algorithm for gpus , and it is as fast as the new _ randomized _ sample sort for gpus by leischner et.al . </S>",
    "<S> ( to appear in proc . </S>",
    "<S> ipdps 2010 ) . </S>",
    "<S> our _ deterministic _ sample sort has the advantage that bucket sizes are guaranteed and therefore its running time does not have the input data dependent fluctuations that can occur for randomized sample sort . </S>"
  ]
}