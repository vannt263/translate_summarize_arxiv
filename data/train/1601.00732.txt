{
  "article_text": [
    "in machine learning it is common to interpret each data point as a vector in euclidean space @xcite .",
    "such a discretisation is chosen because it allows for easy closed form solutions and fast computation , even with large datasets .",
    "however these methods ignore the fact that the data may not naturally fit into this assumption .",
    "in fact much of the data collected for practical machine learning are actually functions i.e. curves .",
    "for example financial data such as stock or commodity prices are functions of monetary value over time .",
    "functional data have become increasingly important in many scientific and engineering research areas such as ecg ( electrocardiogram ) or eeg ( electroencephalography ) in healthcare , biology data analysis , weather or climate data and motion trajectories from computer vision .",
    "analyzing functional data has been an emerging topic in statistical research @xcite and has attracted great attention from machine learning community in recent years @xcite .",
    "one of important challenges in analyzing functional data for machine learning is to efficiently cluster and to learn better representations for functional data .",
    "theoretically the underlying process for functional data is of infinite dimension , thus it is difficult to work with them with only finite samples available . a desired model for functional data",
    "is expected to properly and parsimoniously characterize the nature and variability hidden in the data . the classic functional principal component analysis ( fpca )",
    "@xcite is one of such examples to discover dominant modes of variation in the data .",
    "however fpca may fail to capture patterns if the functional data are not well aligned in its domain . for time",
    "series , a special type of functional data , dynamic time warping ( dtw ) has long been proposed to compare time series based on shape and distortions ( e.g. , shifting and stretching ) along the temporal axis @xcite .",
    "another important type of functional data is shape @xcite .",
    "shape is an important characterizing feature for objects and in computer vision shape has been widely used for the purpose of object detection , tracking , classification , and recognition . in fact , a natural and popular representation for shape analysis is to parametrize boundaries of planar objects as 2d curves . in object recognition , images of the same object should be similar regardless of resolution , lighting , or orientation . hence an efficient shape representation or",
    "shape analysis scheme must be invariant to scale , translation and rotation .",
    "a very useful shape representation is the square - root velocity function ( srvf ) representation @xcite . in general , the resulting srvf of a continuous shape is square integrable , a well - defined hilbert space where appropriate measurement can be applied , refer to section  [ sec:2 ] for more details . by acknowledging the true nature of the data we can develop more powerful methods that exploit features that would otherwise be ignored or lead to erroneous results with simple linear models .",
    "our intention in this study is to consider functional data clustering by accounting for the possible invariance in scaling / stretching , translation and rotation of functional data to help maintain shape characteristics .",
    "the focus of this paper is upon functional data where data sets consist of continuous real curves including shapes in euclidean spaces .",
    "more specifically we propose a method of subspace analysis for functional data based on the idea developed in recent subspace clustering .",
    "the idea is to apply a feature mapping such as the aforementioned srvf to the curves so that they are transformed onto the curve manifold , where the subspace analysis can be conducted based on the geometry on the manifold .",
    "in particular , we adapt the well known low - rank representation ( lrr ) framework @xcite to deal with data that lie on the manifold of open curves by implementing the classical lrr in tangent spaces of the manifold @xcite .",
    "lrr on euclidean spaces @xcite is closely related to several state - of - the - art subspace analysis approaches such as sparse subspace clustering ( ssc ) @xcite , robust pca ( rpca ) @xcite and low - rank matrix completion ( mc ) @xcite methods .",
    "this mixture of subspaces model has naturally led to the development of subspace segmentation methods .",
    "such methods aim to segment the data into clusters with each cluster corresponding to a unique subspace . more formally , given a data matrix of observed column - wise data samples @xmath0 \\in \\mathbb{r}^{d \\times n}$ ] ,",
    "the objective of subspace clustering is to assign each data sample to its underlying subspace .",
    "the basic assumption is that the data within @xmath1 is drawn from a union of @xmath2 subspaces @xmath3 of dimensions @xmath4 .",
    "the core of both ssc and lrr is to learn an affinity matrix for the given dataset and the learned affinity matrix will be pipelined to a spectral clustering method like ncut @xcite to obtain the final subspace labels . to learn the affinity matrix",
    ", ssc relies on the self expressive property @xcite , which is that    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ each data point in a union of subspaces can be efficiently reconstructed by a linear combination of other points in the data_. _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    in other words , each point can be written as a linear combination of the other points i.e.  @xmath5 , where @xmath6 is a matrix of coefficients .",
    "most methods however assume the data generation model @xmath7 , where @xmath8 is the observed data and @xmath9 is noise .",
    "since it is difficult to separate the noise from the data the solution is to relax the self - expressive model to @xmath10 , where @xmath11 is a fitting error and is different from @xmath9 .",
    "similarly lrr @xcite exploits the self expressive property but attempts to learn the global subspace structure by computing the lowest - rank representation of the set of data points .",
    "in other words , data points belonging to the same subspace should have similar coefficient patterns . in the presence of noise lrr attempts to minimise the following objective @xmath12",
    "however rank minimisation is an intractable problem",
    ". therefore lrr actually uses the nuclear norm @xmath13 ( sum of the matrix s singular values ) as the closest convex relation @xmath14 where @xmath15 is a placeholder for the norm most appropriate to the expected noise type .",
    "for example in the case of gaussian noise the best choice is the @xmath16 norm i.e.  @xmath17 and for sparse noise the @xmath18 norm should be used .",
    "both ssc and lrr rely on the linear self expressive property .",
    "this property is no longer available in the nonlinear manifold , e.g. the manifold of open curves as mentioned previously . to generalize lrr or ssc for data in the manifold space",
    ", we explicitly explore the underlying nonlinear data structure and utilize the techniques of exponential and logarithm mappings to bring data to a local linear space .",
    "the rest of the paper is organized as follows . in section [ sec:2 ] ,",
    "we review the preliminaries about the manifold of open curves and introduce the curve low - rank representation ( clrr ) model .",
    "section [ sec:3 ] is dedicated to explaining an efficient algorithm for solving the optimization proposed in clrr based on the linearized alternative direction method with adaptive penalty ( ladmap ) and the algorithm convergence and complexity are also analyzed . in section [ sec:4 ] ,",
    "the proposed model is assessed on both synthetic and real world databases against several state - of - the - art methods .",
    "finally , conclusions are discussed in section [ sec:5 ] .",
    "as previously discussed lrr is limited to a linear model and its current version can only be applied to vector data from a euclidean space .",
    "matrix @xmath19 in or encodes the affinity / similarity between data points .",
    "however this assumption is often unnatural and quite limiting .",
    "much of the data encountered in real world is functional . in other words",
    "it exhibits a curve like structure over a domain .",
    "euclidean linear models are unable to capture the nonlinear invariance embedded in each data point .",
    "for example in thermal infra - red data of geological substances a curve may contain a key identifying feature such as a dip near a particular frequency .",
    "this dip may shift or vary position over time even for the same substance due to impurities . under a linear vector model",
    "this variation may cause the vector to drastically move in the ambient euclidean space and cause poor results . or in other cases the feature may be elongated , shrunk or be subject to some non - uniformly warping or scaling . in all these cases the linear model will fail to accurately represent the non - linear affinity in the data .    exploring these unique non - linear invariance in functional data is the focus of this paper .",
    "we now discuss how to adapt lrr ( similar approach appliable to ssc ) such that it easily accepts curve data and nonlinear relationships within clusters can be easily discovered .      given a smooth parameterized @xmath20-dimension curve @xmath21 \\to \\mathbb r^n$ ] , we represent it using he square - root velocity function ( srvf ) representation @xcite , which is given by @xmath22 the srvf mapping transforms the original curve @xmath23 into a gradient based representation , which facilitates the comparing of the shape information .    in this paper , we focus on the set of open curves , e.g. the curves do not form a loop ( @xmath24 ) . for handling general curves",
    ", we refer readers to @xcite .",
    "the srvf facilitates a measure and geometry bearing invariance to scaling , shifting and reparameterization in the curves domain .",
    "for example , all the translated curves from a curve @xmath23 will have the same srvf .",
    "robinson @xcite proved that if the curve @xmath23 is absolutely continuous , then its srvf @xmath25 is square - integrable , i.e. , @xmath25 is in a functional hilbert space @xmath26 .",
    "conversely for each @xmath27 , there exists a curve @xmath23 whose srvf corresponds to @xmath25 .",
    "thus the set @xmath26 is a well - defined representation space of all the curves .",
    "the most important advantage offered by the srvf framework is that the natural and widely used @xmath28-measure on @xmath26 is invariant to the reparameterization .",
    "that is , for any two srvfs @xmath29 and @xmath30 and a randomly chosen reparametrization function ( non - decreasing ) @xmath31 , we have @xmath32    this property has been exploited in @xcite for functional data clustering under the subspace clustering framework .",
    "different from the work proposed in @xcite , we will adopt the newly developed lrr on manifolds framework to the model of curves lrr , see @xcite . to see this",
    ", we introduce some more notation .",
    "let @xmath33 be the set of all diffeomorphisms from @xmath34 $ ] to @xmath34 $ ] .",
    "this set collects all the reparametrization mappings .",
    "@xmath33 is a lie group with the composition as the group operation and the identity mapping as the identity element",
    ". then all the orbits @xmath35 = \\ { q\\circ \\gamma = q(\\gamma(t ) ) \\;|\\ ; \\forall \\gamma\\in \\gamma\\}$ ] together define the quotient manifold @xmath36 . without loss of generality ,",
    "all curves are normalized to have unit length , i.e. , @xmath37 .",
    "the srvfs associated with these curves are elements of a unit hypersphere in the hilbert space @xmath26 , i.e. , @xmath38 .",
    "therefore , under the curve normalization assumption , instead of @xmath26 , we consider the following unit hypersphere manifold @xmath39    the manifold @xmath40 has some nice properties , see @xcite .",
    "for any two points @xmath41 and @xmath29 in @xmath40 , a geodesic connecting them is given by @xmath42 \\rightarrow \\mathcal c^o$ ] , @xmath43 where @xmath44 is the length of the geodesic .",
    "if we take derivative of @xmath45 w.r.t to @xmath29 , the tangent vector at @xmath41 is @xmath46 .",
    "\\label{tangent1}\\end{aligned}\\ ] ] the above formula is regarded as the logarithm mapping @xmath47 on the manifold @xmath40 .",
    "as we are concerned with the shape invariance , i.e. , we need to additionally remove the shape - preserving transformations : rotation and curve reparametrization .",
    "the manifold concerning us is the quotient space of the manifold @xmath48 , where @xmath49 is the rotation group .",
    "each element @xmath35\\in \\mathcal{s}^o$ ] is an equivalent class defined by @xmath50 = \\left\\{o q(\\gamma(t))\\sqrt{\\dot{\\gamma}(t)}\\ ; |\\ ; o\\in so(n ) \\text { and } \\gamma \\in\\gamma \\right\\}.\\ ] ]    given any two points @xmath51 $ ] and @xmath52 $ ] in @xmath53 , a tangent representative @xcite in the tangent space @xmath54 } ( \\mathcal s^o)$ ] can be calculated in the following way , as suggested in @xcite based on , @xmath55 .",
    "\\label{tangent2}\\end{aligned}\\ ] ] where @xmath56 is the representative of @xmath52 $ ] given by the well - defined algorithm in @xcite and @xmath57 .",
    "in fact , @xmath58 is the lifting representation of abstract tangent vector @xmath59}([q_1])$ ] on @xmath54}(\\mathcal{s}^o)$ ] at @xmath29 .",
    "given a set of @xmath60 unit - length curves @xmath61 , denote their srvfs by @xmath62 such that @xmath63\\in \\mathcal s^o$ ] and @xmath64 is a representative of the equivalent class @xmath63 $ ] .",
    "we can not apply the standard lrr model directly on the quotient manifold @xmath53 .",
    "this is because indeed relies on the following individual linear combination @xmath65 which is invalid for @xmath63 $ ] s on @xmath53 .",
    "note that @xmath66 can be explained as the affinity or similarity between data points @xmath67 and @xmath68 .    on any manifold ,",
    "the tangent space at a given point is linearly local approximation to the manifold around the point and the linear combination is valid in the tangent space .",
    "this prompts us to replace the affinity relation in by the following @xmath69}([q_i ] ) = \\sum_{j=1}^n w_{ij } \\log_{[q_i ] } ( [ q_j ] ) + \\mathbf{e}_i\\label{tangentlinear}\\end{aligned}\\ ] ] with the constraint @xmath70 to maintain consistency at different locations .",
    "the meaning of @xmath71 in is the similarity between curves @xmath72 and @xmath73 via the `` affinity '' between tangent vectors @xmath74}([q_i ] ) $ ] and @xmath74}([q_j])$ ] at the first order approximation accuracy .",
    "each @xmath74}([q_j])$ ] can be calculated by and it is obvious that @xmath74}([q_i])=0 $ ] for any @xmath75 .",
    "with all the ingredients at hand , we are fully equipped to propose the curve lrr ( clrr ) model as follows @xmath76 } ( [ q_j ] ) \\|_{[q_i]}^2 , \\\\ \\textrm{s.t . } \\ ; \\sum_{j=1}^n w_{ij } = 1 , i = 1 , 2 , \\dots , n. \\end{aligned}\\label{model}\\end{aligned}\\ ] ] where @xmath77}$ ] is the metric defined on the manifold , which is defined by the classic @xmath28 hilbert metric on the tangent space .    denote @xmath78 the @xmath75-th row of matrix @xmath79 and define @xmath80 } ( [ q_j ] ) , \\log_{[q_i ] } ( [ q_k ] ) \\rangle.\\end{aligned}\\ ] ] then with some algebraic manipulation we can re - write the model into the following simplified form , @xmath81 where @xmath82 .",
    "effectively this objective allows for similarity between curves to be measured in their tangent spaces .",
    "our highly accurate segmentation results in section [ sec:4 ] have demonstrated that this is an effective way to learn non - linear similarity .",
    "to solve the clrr objective we use the linearized alternative direction method with adaptive penalty ( ladmap ) @xcite .",
    "first take the augmented lagrangian of the objective @xmath83 where @xmath84 is the lagrangian multiplier ( vector ) corresponding to the equality constraint @xmath85 , @xmath86 is the matrix frobebius - norm , and we will update @xmath87 as well in the iterative algorithm to be introduced .    denote by @xmath88 the function defined by except for the first term @xmath89 . to solve ,",
    "we adopt a linearization of @xmath90 at the current location @xmath91 in the iteration process , that is , we approximate @xmath88 by the following linearization with a proximal term @xmath92 where @xmath93 is an approximate constant with a suggested value given by @xmath94 , and @xmath95 is a gradient matrix of @xmath88 at @xmath91 .",
    "denote by @xmath96 the 3-order tensor whose @xmath75-th front slice is given by @xmath97 .",
    "let us define @xmath98 the matrix whose @xmath75-row is given by @xmath99 , then it is easy to show @xmath100    then can be approximated by linearization and @xmath101 will be updated by the following @xmath102    @xmath103 , @xmath104    initialise : @xmath105 , @xmath106 , @xmath107 , @xmath108 , @xmath109 , @xmath110 , @xmath111 , @xmath112    construct each @xmath97 as per    update @xmath79 using    check convergence criteria @xmath113    update lagrangian multiplier @xmath114    update @xmath115 @xmath116    update @xmath87 @xmath117    @xmath79    problem admits a closed form solution by using svd thresholding operator @xcite , given by @xmath118 where @xmath119 is the svd of @xmath120 and @xmath121 is the singular value thresholding ( svt ) @xcite operator defined by @xmath122    the updating rule for @xmath84 @xmath123 and the updating rule for @xmath124 @xmath125 where @xmath126    we summarize the above as algorithm  [ curve_lrr_alg ] .",
    "once the coefficient matrix @xmath79 is found , a spectral clustering like ncut @xcite is applied on the affinity matrix @xmath127 to obtain the segmentation of the data .      for ease of analysis , we firstly define some symbols used in the following .",
    "let @xmath128 and @xmath129 denote the total number of iterations and the lowest rank of the matrix @xmath79 , respectively .",
    "the size of @xmath79 is @xmath130 .",
    "the major computation cost of our proposed method contains two parts , calculating all @xmath97 s and updating @xmath79 . in terms of the formula through and ,",
    "the computational complexity of log algorithm is @xmath131 where @xmath132 is the number of terms in a discretized curves ; therefore , the complexity of @xmath133 is at most @xmath131 and @xmath97 s computational complexity is @xmath134 .",
    "thus the total for all the @xmath97 is @xmath135 . in each iteration of the algorithm",
    ", the singular value thresholding is adopted to update the low rank matrix @xmath79 whose complexity is @xmath136  @xcite .",
    "suppose the algorithm is terminated after @xmath128 iterations , the overall computational complexity is given by @xmath137      algorithm  [ curve_lrr_alg ] is adopted from the algorithm proposed in  @xcite .",
    "however due to the terms of @xmath97 s in the objective function  , the convergence theorem proved in  @xcite can not be directly applied to this case as the linearization is implemented on both the augmented lagrangian terms and the term involving @xmath97 s .",
    "fortunately we can employ the revised approach , presented in @xcite , to prove the convergence for the algorithm . without repeating all the details , we present the convergence theorem for algorithm  [ curve_lrr_alg ] as follows .",
    "if @xmath138 , @xmath139 , @xmath140 , where @xmath141 is a given constant and @xmath142 is the matrix spectral norm , then the sequence @xmath143 generated by algorithm  [ curve_lrr_alg ] converges to an optimal solution to problem  .    in all the experiments we have conducted , the algorithm converges very fast with @xmath144 .",
    "in this section we show three sets of experiments to evaluate the newly proposed clrr .",
    "the performance of the proposed method is compared with the same type of subspace clustering algorithm lrr @xcite . to compare segmentation accuracy we use the subspace clustering accuracy ( sca ) metric @xcite , which is defined as @xmath145 therefore a higher sca @xmath146 means greater clustering accuracy .",
    "the parameters used were fixed across all experiments with @xmath104 for lrr set at @xmath147 and @xmath148 for clrr . a wide range of parameters were tested for each algorithm .",
    "overall we found that the segmentation accuracy of lrr did not vary that much with changes in @xmath104 .",
    "+    .synthetic results [ cols=\"^,^,^,^,^\",options=\"header \" , ]",
    "in this paper , we extended the conventional lrr model on euclidean space to a new lrr model for the manifold of open curves .",
    "the new lrr formulation is based on the tangent space approximation to the manifold so that the classic data self expressive can be well preserved for the manifold of curves at relevant high accuracy .",
    "the resulting optimization problem can be solved using the ladmap technique and algorithm convergence and complexity were presented .",
    "finally we tested the new model by conducting experiments on synthetic , semi - synthetic and real world data , and the experimental results show the outstanding performance against the conventional lrr .",
    "our next work is further extended the lrr model to the manifold of general closed curves .",
    "funding information hidden for the review process .",
    "m.  t. bahadori , d.  kale , y.  fan , and y.  liu .",
    "functional subspace clustering with application to time series . in _ proceedings of the 32nd international conference on machine learning _ , pages 228237 , 2015 .",
    "s.  h. joshi , e.  klassen , a.  srivastava , and i.  jermyn . a novel representation for riemannian analysis of elastic curves in @xmath149 .",
    "in _ ieee conference on computer vision and pattern recognition _ , pages 17 , 2007 .",
    "f.  petitjean , g.  forestier , g.  i. webb , a.  e. nicholson , y.  chen , and e.  keogh .",
    "dynamic time warping averaging of time series allows faster and more accurate classification . in icdm , 2014 . in",
    "international conference on data mining _",
    ", 2014 .",
    "j.  su and a.  srivastava .",
    "rate - invariant analysus of trajectories on riemannian manifolds with application in visual speech recognition . in _ proceedings of international conference on computer vision and pattern recognition _ , 2014 .",
    "b.  williams , m.  toussaint , and a.  j. storkey . modelling motion primitives and their timing in biologically executed movements . in _ advances in neural information processing systems _ , pages 16091616 , 2008 .",
    "l.  wu , a.  ganesh , b.  shi , y.  matsushita , y.  wang , and y.  ma .",
    "convex optimization based low - rank matrix completion and recovery for photometric stereo and factor classification . , xx : xxx  xxx , august 2012 ."
  ],
  "abstract_text": [
    "<S> in machine learning it is common to interpret each data point as a vector in euclidean space . </S>",
    "<S> however the data may actually be functional i.e.  each data point is a function of some variable such as time and the function is discretely sampled . </S>",
    "<S> the naive treatment of functional data as traditional multivariate data can lead to poor performance since the algorithms are ignoring the correlation in the curvature of each function . in this paper </S>",
    "<S> we propose a method to analyse subspace structure of the functional data by using the state of the art low - rank representation ( lrr ) . </S>",
    "<S> experimental evaluation on synthetic and real data reveals that this method massively outperforms conventional lrr in tasks concerning functional data . </S>"
  ]
}