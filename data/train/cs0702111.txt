{
  "article_text": [
    "belief propagation ( bp ) provides maximum - likelihood ( ml ) decoding over a cycle - free factor - graph representation of a code as shown in @xcite and @xcite . in some cases ,",
    "bp over loopy factor graphs of channel codes has been shown to have near ml performance .",
    "bp performs well on the bi - partite factor graphs composed of variable nodes and check nodes that describe ldpc codes .",
    "however , loopy bp is an iterative algorithm and therefore requires a message - passing schedule .",
    "flooding , or simultaneous scheduling , is the most popular scheduling strategy . in",
    "every iteration flooding simultaneously updates all the variable nodes ( with each update using the same set of pre - update data ) and then , updates all the check nodes ( again , with each update using the same pre - update information ) .",
    "recently , several papers have addressed the effects of different types of sequential , or non - simultaneous , scheduling strategies in bp ldpc decoding .",
    "the idea was introduced as a sequence of check - node updates in @xcite and @xcite and as a sequence of variable - node updates in @xcite .",
    "it is also presented in @xcite under the name of layered bp ( lbp ) , in @xcite as serial schedule , in @xcite as shuffled bp , in @xcite as row message passing , column message passing and row - column message passing , among others .",
    "simulations and theoretical tools in these works show that sequential scheduling converges twice as fast as flooding when used in ldpc decoding .",
    "it has also been shown that sequential updating does nt increase the decoding complexity per iteration , thus allowing the convergence speed increase at no cost @xcite . in @xcite , where a global framework for the analysis of ldpc decoders is introduced , the complexity is assumed to be independent from the sequential scheduling chosen .",
    "furthermore , different types of sequential schedules such as sequential check - node updating , sequential variable - node updating and sequential message updating have very similar performance results @xcite .",
    "given their similarities , the different types of sequential updates will be referred in this paper as standard sequential scheduling ( sss ) . in the simulations presented in this paper",
    "the sss strategy used for comparison is lbp , a sequence of check - node updates , as presented in @xcite and @xcite .",
    "sequential updating poses the problem of finding the ordering of message updates that results in the best convergence speed and/or code performance .",
    "the current state of the messages in the graph can be used to dynamically update the schedule , producing what we call an informed dynamic schedule ( ids ) and presented in @xcite . to our knowledge ,",
    "the only well defined informed sequential scheduling is the residual belief propagation ( rbp ) algorithm presented by elidan et al . in @xcite .",
    "they proposed it for general sequential message passing , not specifically for bp decoding .",
    "rbp is a greedy algorithm that organizes the message updates according to how different is the message generated in the current iteration from the message generated in the previous iteration .",
    "the intuition is that the bigger this difference , the further from convergence this part of the graph is .",
    "therefore , propagating this message first will make bp converge at a higher speed .",
    "simulations show that rbp ldpc decoding has a higher convergence speed than sss but its error - rate performance for a large enough number of iterations is worse .",
    "this behavior is commonly found in greedy algorithms , which tend to arrive at a solution faster , but arrive at the correct solution less often .",
    "we propose a less - greedy ids in which all the outgoing messages of a check - node are generated simultaneously .",
    "we call this ids node - wise rbp .",
    "it converges both faster and more often than sss .",
    "both rbp and node - wise rbp require the knowledge of the message to be updated in order to pick which message to update .",
    "this means that several messages are computed and not passed .",
    "thus , increasing the complexity of the decoding per iteration .",
    "we propose using the min - bp check - node update algorithm explained in @xcite and @xcite to simplify the ordering metric and significantly decrease the complexity for both informed scheduling strategies while maintaining the same performance . also , an analysis of the hardware issues that may arise in a parallel implementation of these informed sequential scheduling strategies is presented .    this paper is organized as follows .",
    "section [ sec : ldpc ] reviews ldpc decoding and the flooding and sss schedules .",
    "section [ sec : rbp ] explains how to implement rbp decoding for ldpc codes .",
    "this section also introduces and justifies node - wise rbp .",
    "section [ sec : hardware ] addresses some complexity and implementability issues .",
    "the simulation results of all the message - passing schedules are compared and discussed in section [ sec : results ] .",
    "section [ sec : conclusions ] delivers the conclusions .",
    "in general , bp consists of the exchange of messages between the nodes of a graph . each node generates and propagates messages to its neighbors based on its current incoming messages .",
    "the vector of all the messages in the graph is denoted by * m * and @xmath0 denotes the @xmath1th message in the vector * m*. the function that generates @xmath0 from * m * is denoted by @xmath2 .",
    "the ldpc code graph is a bi - partite graph composed by @xmath3 variable nodes @xmath4 for @xmath5 that represent the codeword bits and @xmath6 check nodes @xmath7 for @xmath8 that represent the parity - check equations .",
    "the exchanged messages correspond to the log - likelihood ratio ( llr ) of the probabilities of the bits .",
    "the sign of the llr indicates the most likely value of the bit and the absolute value of the llr gives the reliability of the message . in this fashion ,",
    "the channel information llr of the variable node @xmath4 is @xmath9 , where @xmath10 is the received signal .",
    "then , for any @xmath7 and @xmath4 that are connected , the two message generating functions , @xmath11 and @xmath12 , are :    @xmath13    @xmath14    where @xmath15 denotes the neighbors of @xmath4 excluding @xmath7 , and @xmath16 denotes the neighbors of @xmath7 excluding @xmath4 .",
    "bp decoding consists of the iterative update of the messages until a stopping rule is satisfied . in flooding scheduling",
    ", an iteration consists on the simultaneous update of all the messages @xmath17 followed by the simultaneous update of all the messages @xmath18 . in sss , an iteration consists on the sequential update of all the messages @xmath17 as well as all the messages @xmath18 in a specific pre - defined order .",
    "the algorithm stops if the decoded bits satisfy all the parity - check equations or a maximum number of iterations is reached .",
    "rbp , as introduced in @xcite , is an informed scheduling strategy that updates first the message that maximizes an ordering metric called the residual .",
    "a residual is the norm ( defined over the message space ) of the difference between the values of a message before and after an update . for a message @xmath0",
    ", the residual is defined as :    @xmath19    the intuitive justification of this approach is that as loopy bp converges , the differences between the messages before and after an update go to zero .",
    "therefore if a message has a large residual , it means that it s located in a part of the graph that hasnt converged yet .",
    "therefore , propagating that message first should speed up the process .",
    "_ create a priority queue , ordered by the value of the residual , so at each step the first message in the queue is updated and then the queue is reordered using the new information .      in llr bp decoding ,",
    "all the message spaces are one - dimensional ( the real line ) .",
    "therefore , the residuals are the absolute values of the difference of the llrs .",
    "let us analyze the behavior of rbp decoding for ldpc codes in order to simplify the decoding algorithm .",
    "initially , all the messages @xmath20 are set to the value of their correspondent channel message @xmath21 .",
    "no operations are needed in this initialization .",
    "this implies that the residuals of all the variable - to - check messages @xmath22 are equal to 0 .",
    "then , without loss of generality , we assume that the message @xmath23 has the residual @xmath24 , which is the biggest of the graph .",
    "after @xmath23 is propagated , only residuals @xmath25 change , with @xmath26 .",
    "the new residuals @xmath27 are equal to @xmath24 , because @xmath24 was the change in the message @xmath23 and eq .",
    "[ eq : v2c ] shows that the message update operations of a variable node are only sums .",
    "therefore , the messages @xmath28 have now the biggest residuals in the graph .    assuming that propagating the messages @xmath29 wo nt generate any new residuals bigger than @xmath24",
    ", rbp can be simplified .",
    "every time a message @xmath30 is propagated , the outgoing messages from the variable node @xmath31 will be updated and propagated . this facilitates the scheduling since we need only to maintain a queue @xmath32 of messages @xmath30 , ordered by the value of their residuals , in order to find out the next message to be propagated .",
    "rbp ldpc decoding is formally described in algorithm [ alg : rbp ] , the stopping rule will be discussed in section [ sec : conclusions ] .",
    "[ 1 ] initialize all @xmath33 initialize all @xmath34 compute all @xmath35 and generate @xmath32 let @xmath36 be the first message in @xmath32 generate and propagate @xmath23 set @xmath37 and re - order @xmath32 generate and propagate @xmath38 compute @xmath39 and re - order @xmath32 position=4 ;    there is an intuitive way to see how rbp decoding works for ldpc codes .",
    "let us assume that at a certain moment in the decoding , there is a check node @xmath7 with residuals @xmath40 for every @xmath41 .",
    "now let us assume that there is a change in the value of the message @xmath42 .",
    "the biggest change in a check - to - variable message out of @xmath7 ( therefore the largest residual ) will happen in the edge that corresponds to the incoming variable - to - check message with the lowest reliability ( excluding the message @xmath42 ) .",
    "let us denote by @xmath43 the variable node that is the destination of the message that has the largest residual @xmath44 .",
    "then , the message @xmath45 has the smallest reliability out of all messages @xmath46 , with @xmath47 .",
    "this implies that , for this particular scenario , once there s a change in a variable - to - check message , rbp will propagate first the message to the variable node with the lowest reliability .",
    "this makes sense intuitively .",
    "in some sense the lowest reliability variable node needs to receive new information more than the higher reliability ones .",
    "the negative effects of the greediness of rbp are apparent in the case of unsatisfied check nodes .",
    "rbp will schedule to propagate first the message that will  correct \" the variable node with the lowest reliability .",
    "this is the most likely variable node to be in error .",
    "however , if that variable node was already correct , changing its sign will likely generate new errors , making the bp convergence more difficult .",
    "this analysis helps us see why rbp corrects the most likely errors faster but has trouble correcting  difficult \" errors as will be seen in section [ sec : results ] .",
    "we define  difficult \" errors as the errors that need a large number of message updates to be corrected .      in order to obtain a better performance for all types of errors ,",
    "perhaps a less greedy scheduling strategy must be used .",
    "as noted earlier , some of the greediness of rbp came from the fact that it tends to propagate first the message to the less reliable variable nodes .",
    "we propose to simultaneously update and propagate all the check - to - variable messages that correspond to the same check node @xmath7 , instead of only propagating the message with the largest residual @xmath48 .",
    "it can be seen , using the analysis presented earlier , that this algorithm is less likely to generate new errors .",
    "we call this less greedy strategy node - wise rbp and it s performance can be seen in section [ sec : results ] .",
    "node - wise rbp is similar to lbp ; it is a sequence of check - node updates .",
    "however , unlike lbp , which follows a pre - determined order , the check node to be updated next is chosen dynamically according to the residuals of the check - to - variable messages .",
    "node - wise rbp is formally described in algorithm [ alg : nwrbp ] .",
    "[ 1 ] initialize all @xmath33initialize all @xmath34 compute all @xmath35 and generate @xmath32 let @xmath36 be the first message in @xmath32 generate and propagate @xmath49 set @xmath50 and re - order @xmath32 generate and propagate @xmath51 compute @xmath39 and re - order @xmath32 position=4 ;    node - wise rbp converges both faster than sss ( in terms of number of messages updated ) and better than sss ( in terms of fer of the code for a large number of iterations ) .",
    "we can explain intuitively and demonstrate experimentally that the lower error rates are achieved because informed scheduling allows the ldpc decoder to overcome many `` trapping sets '' .",
    "trapping sets , or near - codewords , as defined in @xcite , are small variable - node sets such that the induced sub - graph has a small number of odd degree neighbors . in @xcite , richardson also mentions that the most troublesome trapping set errors are those where the odd degree neighbors have degree 1 ( in the induced sub - graph ) , and the even - degree neighbors have degree 2 ( in the induced sub - graph ) .",
    "it is likely that node - wise rbp solves the variable nodes in error by sequentially updating the degree-1 check - nodes connected to them .",
    "when a variable node in a trapping set is corrected , the induced sub - graph of the variable - nodes - in - error will change as follows .",
    "at least one check node that was degree-2 , becomes degree-1 after the variable node correction .",
    "this check node is likely to be picked as the next check - node to be updated by node - wise rbp because its messages will have large residuals .",
    "this update will probably correct another variable node in the trapping set .",
    "we corroborated this analysis by studying the behavior of the decoders for the noise realizations that the sss decoder could not solve for 200 iterations and that node - wise rbp solved in a very small number of iterations ( less than 10 ) .",
    "we found that a large majority of the sss errors in these cases are caused by trapping sets that node - wise rbp solved in the manner mentioned before .",
    "given the surge in popularity of ldpc codes for practical implementations , it is interesting to address some issues about the complexity of rbp and node - wise rbp when compared to sss and flooding .      for an ids we consider one iteration to have occurred after the number of updates equals the number of updates completed in an sss or flooding iteration . for rbp",
    ", an iteration will be counted after the number of check - to - variable message updates equals the number of edges in the ldpc graph . for node",
    "- wise rbp an iteration will be counted after a number of check - node updates equals the number of check - nodes of the code .    in @xcite ,",
    "the authors prove that if the appropriate update equations are chosen , the total number of operations per iteration of all the sequential schedules is equal to the number of operations per iterations of the flooding schedule , making their complexity per iteration equal . given that both rbp and node - wise rbp are forms of sequential updates , a sequence of message updates in the first case and a sequence of check - node updates in the second , then they also use , on average , the same number of message - generating operations per iteration ( using our definition of iteration for informed schedules ) .    in order to maintain the same complexity",
    ", we use the typical stopping rules in the decoding . stop",
    "if at the end of an iteration the decoded bits satisfy all the parity - check equations , or a maximum number of iterations is reached .    on top of the message - generation complexity , informed schedules",
    "incur two extra processes : residual computation and ordering of the residuals . as defined in section [ sec : rbp ] , the residual computation requires the value of the message that would be propagated .",
    "this requires additional complexity since there will be numerous message computations that will only be used to calculate residuals .",
    "we propose to use the min - bp check - node update approximation explained in @xcite to compute the approximate - residual as follows ,    @xmath52    where the superscript @xmath53 stands for approximate and indicates the min - bp approximation .",
    "the min - bp check - node update consists of finding the two variable - to - check messages with the smallest reliability .",
    "then , the smallest reliability is assigned as the check - to - variable message reliability for all the edges except the one where the smallest reliability came from .",
    "the second smallest reliability is assigned to that remaining edge .",
    "the proper sign is computed for all the check - to - variable messages .",
    "thus , replacing all the residual functions for approximate - residual functions in algorithms [ alg : rbp ] and [ alg : nwrbp ] , defines approximate rbp ( arbp ) decoding and node - wise arbp decoding .",
    "these significantly simpler algorithms perform as well as the ones presented in section [ sec : rbp ] , as will be seen in section [ sec : results ] .",
    "note that we only propose to use min - bp for the residual computation .",
    "for the actual propagation of messages we use the full update equations ( [ eq : v2c ] ) and ( [ eq : c2v ] ) .",
    "even for node - wise arbp , the practical trade - off between the increase in the per - iteration complexity and the decrease in the number of iterations ( and improved fer ) is difficult to address in general as it depends on specific implementation choices .",
    "our current research is addressing this trade - off in detail .",
    "the possibility of having several processors computing messages at the same time during the ldpc decoding has become an intense area of research and an important reason why ldpc codes are so successful .",
    "furthermore , codes with a specific structure have been shown to allow sss decoding while maintaining the same parallelism degree obtained for flooding decoding @xcite . in principle , the idea of having an ordered sequence of updates , that uses the most recent information as much as possible , is nt compatible with the idea of simultaneously computing messages .",
    "however , since the ordering of the queue @xmath32 based on the new results occurs after the update , it is possible that the several parallel processors can work on different parts of the graph while still using the most recent information .",
    "we define the parallel node - wise arbp scheduling strategy as the node - wise arbp strategy where instead of updating only the check - node with the largest approximate residual , @xmath54 check - nodes are updated at the same time .",
    "the @xmath54 nodes that have the largest approximate residuals are updated simultaneously .",
    "these @xmath54 check nodes are not designed to work in parallel , unlike the @xmath54 check - nodes of a @xmath55 sub - matrix as defined in @xcite .",
    "however , parallel processing may be implemented extending the hardware solutions presented in @xcite .",
    "for instance , if one or more check - nodes have in common one or more variable nodes , they will all use the same previous information and compute the incremental variations that are afterwards combined in the variable - node update .",
    "there are hardware issues , such as memory clashes , that still need to be carefully addressed when implementing parallel node - wise arbp .",
    "parallel node - wise arbp has a very small performance degradation when compared with node - wise arbp , as will be seen in section [ sec : results ] .",
    "we defined and simulated the parallel version of node - wise arbp since it s the simplest ids strategy and therefore , the most likely to be implemented .",
    "this section presents the awgn performance of the different scheduling strategies presented above .",
    "all the simulations are floating point and use the same rate-1/2 ldpc code .",
    "the blocklength of the code is 1944 and it has the same sub - matrix structure as the one presented in @xcite with sub - matrix size equal to 54x54 .    the sss results shown correspond to the sequential check - node update introduced in @xcite .",
    "this scheduling is known as layered belief propagation ( lbp ) and guarantees a parallelism degree equal to the sub - matrix size of the ldpc code ( 54 in our case ) . as shown in @xcite ,",
    "different sss strategies produce almost identical results so its selection does nt significantly affect the performance of the decoder .",
    "[ fig : all ] shows the performance of the scheduling strategies discussed above , flooding , sss ( lbp ) , rbp , arbp , node - wise rbp , node - wise arbp , and parallel node - wise arbp , as the number of iterations increases .",
    "the figure shows that rbp has a significantly better performance than sss ( lbp ) for a small number of iterations , but a sub - par performance for a larger number of iterations .",
    "specifically , the performance of rbp at 4 iterations is equal to the performance of sss ( lbp ) at 13 iterations , but the curves cross over at 19 iterations .",
    "this suggests that rbp has trouble with",
    " difficult \" errors as discussed earlier .",
    "node - wise rbp , while not as good as rbp for a small number of iterations , shows consistently better performance than sss ( lbp ) across all iterations .",
    "specifically , the performance of node - wise rbp at 18 iterations is equal to the performance of sss ( lbp ) at 50 iterations .",
    "the results for flooding are shown for comparison purposes , and replicate the theoretical and empirical results of @xcite-@xcite that claim that flooding needs twice the number of iterations as sss .",
    "[ fig : all ] also shows the performance of the approximate residual schedules and compares them with the schedules that use the exact residuals .",
    "it can be seen that both arbp and node - wise arbp perform almost indistinguishably from rbp and node - wise rbp respectively .",
    "we reiterate that the approximate residual diminishes the complexity of residual computation significantly , thus making arbp , and node - wise arbp more attractive than their exact counterparts .",
    "furthermore , fig .",
    "[ fig : all ] also shows the performance of parallel node - wise arbp .",
    "the relatively small loss in performance when compared to node - wise arbp is the price for the throughput increase resulting from the parallelism .",
    "the number @xmath54 of check - nodes processed in parallel was set to 54 , which is equal to the parallelism guaranteed by sss ( lbp ) decoding this structured ldpc code with sub - matrix size equal to 54x54 @xcite .",
    "the fer of node - wise arbp vs. snr and for 15 and 50 iterations ( maximum ) is presented in fig .",
    "[ fig : nwarbp ] .",
    "the fer of flooding and sss ( lbp ) are also presented as references .",
    "it can be seen that the snr gap between node - wise arbp and sss ( lbp ) is more pronounced for a small number of iterations and/or a large snr .",
    "[ fig:1211n ] and fig .",
    "[ fig:5611n ] show the performance of different scheduling strategies for the blocklength 1944 rate-1/2 and rate 5/6 ldpc codes selected for the ieee 802.11n standard @xcite .",
    "these simulations were run for a high number of iterations ( 200 ) and show that node - wise arbp achieves a better fer performance that sss ( lbp ) .",
    "[ fig:5611n ] also shows that even for high rate codes , node - wise arbp converges both faster and better than sss ( lbp ) .",
    "this paper shows that , while maintaining the same message - generation functions , ids can improve the performance of bp ldpc decoding .",
    "rbp and its simplification arbp are appropriate for applications that have a high target error - rate , given that rbp achieves these error - rates using significantly fewer iterations than sss .",
    "they are also appropriate for high - speed applications that only allow a small number of iterations . however , for applications that require lower error rates and allow larger delays rbp and arbp are nt appropriate .    for such applications node - wise rbp and",
    "its simplification node - wise arbp perform better than sss for any target error - rate and any number of iterations .",
    "these node - wise strategies achieve a lower error - rates by overcoming trapping set errors that sss can not solve .",
    "furthermore a parallel implementation of node - wise arbp was shown to perform nearly as well as the original node - wise arbp , making this informed scheduling more attractive for practical implementations .",
    "the improvement in performance of these informed scheduling strategies were also shown for a high - rate code ( rate 5/6 ) .",
    "however , they come with the cost of an increase in complexity per iteration due to the residual computation and its ordering .",
    "the trade - off provided by node - wise arbp between increasing the per - iteration complexity and reducing the number of iterations ( while also reducing the fer for a large number of iterations ) requires further investigation in the context of specific implementations .",
    "the ideas presented in this work may be extended to other communication solutions that use iterative bp , such as turbo codes , turbo - equalization , iterative demodulation and decoding of high - order constellations .",
    "the extensions of the ids strategies may also prove beneficial for loopy bp solutions to problems outside the communications field ."
  ],
  "abstract_text": [
    "<S> low - density parity - check ( ldpc ) codes are usually decoded by running an iterative belief - propagation , or message - passing , algorithm over the factor graph of the code . </S>",
    "<S> the traditional message - passing schedule consists of updating all the variable nodes in the graph , using the same pre - update information , followed by updating all the check nodes of the graph , again , using the same pre - update information . </S>",
    "<S> recently several studies show that sequential scheduling , in which messages are generated using the latest available information , significantly improves the convergence speed in terms of number of iterations . </S>",
    "<S> sequential scheduling raises the problem of finding the best sequence of message updates . </S>",
    "<S> this paper presents practical scheduling strategies that use the value of the messages in the graph to find the next message to be updated . </S>",
    "<S> simulation results show that these informed update sequences require significantly fewer iterations than standard sequential schedules . </S>",
    "<S> furthermore , the paper shows that informed scheduling solves some standard trapping set errors . </S>",
    "<S> therefore , it also outperforms traditional scheduling for a large numbers of iterations . </S>",
    "<S> complexity and implementability issues are also addressed .    belief propagation , message - passing schedule , error - control codes , low - density parity - check codes . </S>"
  ]
}