{
  "article_text": [
    "as a warning to the reader , we want to stress from the beginning that this paper is mostly a formal exercise : to understand how the bayesian analysis of a mixture model unravels and automatically exploits the missing data structure of the model is crucial for grasping the details of simulation methods ( not covered in this paper , see , e.g. , @xcite ) that take full advantage of the missing structures .",
    "it also allows for a comparison between exact and approximate techniques when the former are available . while the relevant references are pointed out in due time , we note here that our paper builds upon the foundational paper of @xcite .",
    "we thus assume that a sample @xmath0 from the mixture model @xmath1 is available , where @xmath2 denotes the scalar product between the vectors @xmath3 and @xmath4 .",
    "we are selecting on purpose the natural representation of an exponential family ( see , e.g. * ? ? ? * chapter 3 ) , in order to facilitate the subsequent derivation of the posterior distribution .",
    "when the components of the mixture are poisson @xmath5 distributions , if we define @xmath6 , the poisson distribution indeed is written as a natural exponential family : @xmath7 for a mixture of multinomial distributions @xmath8 , the natural representation is given by @xmath9 and the overall ( natural ) parameter is thus @xmath10 .    in the normal @xmath11 case ,",
    "the derivation is more delicate when both parameters are unknown since @xmath12 in this particular setting , the natural parameterisation is in @xmath13 while the statistic @xmath14 is two - dimensional .",
    "the moment cumulant function is then @xmath15 .",
    "as described in the standard literature on mixture estimation @xcite , the missing variable decomposition of a mixture likelihood associates each observation in the sample with one of the @xmath16 components of the mixture , i.e. @xmath17 given the component allocations @xmath18 , we end up with a cluster of ( sub)samples from different distributions from the same exponential family .",
    "priors customarily used for the analysis of these exponential families can therefore be extended to the mixtures as well .",
    "while conjugate priors do not formally exist for mixtures of exponential families , we will define _ locally conjugate priors _ as priors that are conjugate for the completed distribution , that is , for the likelihood associated with both the observations and the missing data @xmath18 .",
    "this amounts to taking regular conjugate priors for the parameters of the different components and a conjugate dirichlet prior on the weights of the mixture , @xmath19    when we consider the complete likelihood @xmath20 \\\\ & = \\prod_{j=1}^k p_j^{n_j } \\exp\\left [ \\theta_j\\cdot \\sum_{z_i = j } r(x_i ) - n_j\\psi(\\theta_j ) \\right ] \\\\ & = \\prod_{j=1}^k p_j^{n_j } \\exp\\left [ \\theta_j\\cdot s_j - n_j\\psi(\\theta_j ) \\right]\\,,\\end{aligned}\\ ] ] it is easily seen that we remain within an exponential family since there exists a sufficient statistic with fixed dimension , @xmath21 .",
    "if we use a dirichlet prior , @xmath22 on the vector of the weights @xmath23 defined on the simplex of @xmath24 , and ( generic ) conjugate priors on the @xmath25s , @xmath26 \\propto \\exp\\left [ \\theta_j\\cdot s_{0j } - \\lambda_j\\psi(\\theta_j ) \\right ] \\,,\\ ] ] the posterior associated with the complete likelihood @xmath27 is then of the same family as the prior : @xmath28   \\times p_j^{n_j } \\exp\\left [ \\theta_j\\cdot s_j - n_j\\psi(\\theta_j ) \\right ] \\\\ & = \\prod_{j=1}^k p_j^{\\alpha_j+n_j-1}\\,\\exp\\left [ \\theta_j\\cdot ( s_{0j}+s_j ) - ( \\lambda_j+n_j)\\psi(\\theta_j ) \\right]\\,;\\end{aligned}\\ ] ] the parameters of the prior are transformed from @xmath29 to @xmath30 , from @xmath31 to @xmath32 and from @xmath33 into @xmath34 .",
    "for instance , in the case of the poisson mixture , the conjugate priors are gamma @xmath35 , with corresponding posteriors ( for the complete likelihood ) , gamma @xmath36 distributions , in which @xmath37 denotes the sum of the observations in the @xmath38th group .    for a mixture of multinomial distributions , @xmath39 ,",
    "the conjugate priors are dirichlet @xmath40 distributions , with corresponding posteriors @xmath41 , @xmath42 denoting the number of observations from component @xmath38 in group @xmath43 @xmath44 , with @xmath45 .    in the normal mixture case ,",
    "the standard conjugate priors are products of normal and inverse gamma distributions , i.e. @xmath46 indeed , the corresponding posterior is @xmath47 and @xmath48 where @xmath49 is the sum of the observations allocated to component @xmath38 and @xmath50 is the sum of the squares of the differences from @xmath51 for the same group ( with the convention that @xmath52 when @xmath53 ) .      these straightforward derivations do not correspond to the observed likelihood , but to the completed likelihood .",
    "while this may be enough for some simulation methods like gibbs sampling ( see , e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ) , we need further developments for obtaining the true posterior distribution .    if we now consider the observed likelihood , it is natural to expand this likelihood as a sum of completed likelihoods over all possible configurations of the partition space of allocations , that is , a sum over @xmath54 terms . except in the very few cases that are processed below , including poisson and multinomial mixtures ( see section [ ex : fullp ] ) , this sum does not simplify into a smaller number of terms because there exists no summary statistics . from a bayesian point of view",
    ", the complexity of the model is therefore truly of magnitude @xmath55 .",
    "the observed likelihood is thus @xmath56 ( with the dependence of @xmath57 upon @xmath18 omitted for notational purposes ) and the associated posterior is , up to a constant , @xmath58 where @xmath59 is the normalising constant missing in @xmath60 i.e.   @xmath61 if @xmath62 is the normalising constant of @xmath63 , i.e. @xmath64 the posterior @xmath65 is therefore a mixture of conjugate posteriors where the parameters of the components as well as the weights can be computed in closed form !",
    "the availability of the posterior does not mean that alternative estimates like map and mmap estimates can be computed easily",
    ". however , this is a useful closed form result in the sense that moments can be computed exactly : for instance , if there is no label switching problem @xcite and , if the posterior mean is producing meaningful estimates , we have that @xmath66 = \\sum_{{\\mathbf{z}}}\\ , \\omega({\\mathbf{z}})\\ , \\frac{s_{0j}+s_j}{n_j+\\lambda_j\\,,}\\ ] ] since , for each allocation vector @xmath18 , we are in an exponential family set - up where the posterior mean of the expectation @xmath67 of @xmath4 is available in closed form .",
    "( obviously , the posterior mean only makes sense as an estimate for very discriminative priors ; see @xcite . )",
    "similarly , estimates of the weights @xmath68 are given by @xmath69 = \\sum_{{\\mathbf{z}}}\\ , \\omega({\\mathbf{z}})\\ , \\frac{n_j+\\alpha_j}{n+\\alpha_\\cdot}\\,,\\ ] ] where @xmath70 .",
    "therefore , the only computational effort required is the summation over all partitions .",
    "this decomposition further allows for a closed form expression of the marginal distributions of the various parameters of the mixture .",
    "for instance , the ( marginal ) posterior distribution of @xmath71 is given by @xmath72}{k(s_{0j}+s_j , n_j+\\lambda_j)}\\,.\\ ] ] ( note that , when the hyperparameters @xmath29 , @xmath31 , and @xmath73 are independent of @xmath38 , this posterior distribution is independent of @xmath38 . ) similarly , the posterior distribution of the vector @xmath23 is equal to @xmath74 if @xmath16 is small and @xmath75 is large , and when all hyperparameters are equal , the posterior should then have @xmath16 spikes or peaks , due to the label switching / lack of identifiability phenomenon .",
    "we will now proceed through standard examples .      in the case of a two component poisson mixture , @xmath76",
    "let us assume a uniform prior on @xmath77 ( i.e.  @xmath78 ) and exponential priors @xmath79 and @xmath80 on @xmath81 and @xmath82 , respectively . (",
    "the scales are chosen to be fairly different for the purpose of illustration . in a realistic setting",
    ", it would be sensible either to set those scales in terms of the scale of the problem , if known , or to estimate the global scale following the procedure of @xcite . )",
    "the normalising constant is then equal to @xmath83 { \\,\\text{d}\\,}\\theta\\\\     & = \\int_0^\\infty \\lambda_j^{\\xi-1}\\,\\exp ( -\\delta\\lambda_j)\\,{\\,\\text{d}\\,}\\lambda_j \\\\     & =   \\delta^{-\\xi}\\,\\gamma(\\xi)\\,,\\end{aligned}\\ ] ] with @xmath84 and @xmath85 , and the corresponding posterior is ( up to the normalisation of the weights ) @xmath86 with @xmath87 corresponding to a beta @xmath88 distribution on @xmath77 and to a gamma @xmath89 distribution on @xmath33 @xmath90 .",
    "an important feature of this example is that the sum does not need to involve all of the @xmath91 terms , simply because the individual terms in the previous sum factorise in @xmath92 , which then acts like a local sufficient statistic . since @xmath93 and @xmath94 , the posterior only requires as many distinct terms as there are distinct values of the pair @xmath95 in the completed sample .",
    "for instance , if the sample is @xmath96 , the distinct values of the pair @xmath95 are @xmath97 .",
    "there are therefore @xmath98 distinct terms in the posterior , rather than @xmath99 .    the problem of computing the number ( or cardinality ) @xmath100 of terms in the @xmath54 sum with the same statistic",
    "@xmath95 has been tackled by @xcite in that he proposes a recursive formula for computing @xmath101 in an efficient way , as expressed below for a @xmath16 component mixture :    * @xcite * if @xmath102 denotes the vector of length @xmath16 made up of zeros everywhere except at component @xmath38 where it is equal to one , if @xmath103 then @xmath104    therefore , once the @xmath105 are all computed , the posterior can be written as @xmath106 \\ , \\pi(\\theta,\\mathbf{p}|{\\mathbf{x}},n_1,s_1)\\,,\\ ] ] up to a constant , since the complete likelihood posterior only depends on the sufficient statistic @xmath95 .",
    "now , the closed - form expression allows for a straightforward representation of the marginals .",
    "for instance , the marginal in @xmath81 is given by @xmath107 up to a constant , while the marginal in @xmath82 is @xmath108 again up to a constant , and the marginal in @xmath77 is @xmath109 still up to a constant , if @xmath110 denotes the sum of all observations .",
    "as pointed out above , another interesting outcome of this closed - form representation is that marginal likelihoods ( or evidences ) can also be computed in closed form .",
    "the marginal distribution of @xmath111 is directly related to the unormalised weights @xmath112 in that @xmath113 up to the product of factorials @xmath114 ( but this is irrelevant in the computation of the bayes factor ) .    in practice",
    ", the derivation of the cardinalities @xmath100 can be done recursively as in @xcite : include each observation @xmath115 by updating all the @xmath116s in both @xmath117 and @xmath118 , and then check for duplicates . below is a nave r implementation ( for reasonable efficiency , the algorithm should be programmed in a faster language like c. ) , where ` ncomp ` denotes the number of components :    .... # matrix of sufficient statistics , last column is number of occurrences cardin = matrix(0,ncol=2*ncomp+1,nrow = ncomp )    # initialisation for ( i in 1:ncomp ) cardin[i,((2*i)-1):(2*i)]=c(1,dat[1 ] ) cardin[,2*ncomp+1]=1    # update for ( i in 2:length(dat ) ) {        ncard = dim(cardin)[1 ]      update = matrix(t(cardin),ncol=2*ncomp+1,nrow = ncomp*ncard , byrow = t )        for ( j in 0:(ncomp-1 ) ) {          update[j*ncard+(1:ncard),(2*j)+1]=              update[j*ncard+(1:ncard),(2*j)+1]+1        update[j*ncard+(1:ncard),(2*j)+2]=              update[j*ncard+(1:ncard),(2*j)+2]+dat[i ]        }        update = update[do.call(order , data.frame(update ) ) , ]      nu = dim(update)[1 ]      # changepoints      jj = c(1,(2:nu)[apply(abs(update[2:nu,1:(2*ncomp)]-                update[1:(nu-1),1:(2*ncomp)]),1,sum)>0 ] )      # duplicates or rather ncomplicates !",
    "duplicates=(1:nu)[-jj ]      if ( length(duplicates)>0 ) {          for ( dife in 1:(ncomp-1 ) ) {            ji = jj[jj+dife<=nu ]          ii = ji[apply(abs(update[ji+dife,1:(2*ncomp)]-                     update[ji,1:(2*ncomp)]),1,sum)==0 ]          if ( length(ii)>0 )           update[ii,(2*ncomp)+1]=update[ii,(2*ncomp)+1]+                              update[ii+dife,(2*ncomp)+1 ]        }        update = update[-duplicates , ]        }        cardin = update      } ....    at the end of this program , all non - empty realisations of the sufficient @xmath95 are available in the two first columns of ` cardin ` , while the corresponding @xmath100 is provided by the last column .",
    "once the @xmath100 s are available , the corresponding weights can be added as the last column of ` cardin ` , i.e.     .... w = log(cardin[,2*ncomp+1])+apply(lfactorial(cardin[,2*(1:ncomp)-1]),1,sum)+         apply(lfactorial(cardin[,2*(1:ncomp)]),1,sum)-         apply(log(xi[1:ncomp]+cardin[,2*(1:ncomp)-1 ] ) *          ( cardin[,2*(1:ncomp)]+1),1,sum)- sum(lfactorial(dat ) ) w = exp(w - max(w ) ) cardin = cbind(cardin , w ) ....    where ` xi[j ] ` denotes @xmath31 .",
    "the marginal posterior on @xmath81 can then be plotted via    .... marlam = function(lam , comp=1 ) {     sum(cardin[,2*(ncomp+1)]*dgamma(lam , shape = cardin[,2*comp]+1 ,               rate = cardin[,2*comp-1]+xi[comp]))/sum(cardin[,2*(ncomp+1 ) ] )   } lalam = seq(.01,1.2*max(dat),le=100 ) mamar = apply(as.matrix(lalam),1,marlam , comp=1 ) plot(lalam , mamar , type=\"l\",xlab = expression(mu[1]),ylab=\"\",lwd=2 ) ....    while the marginal posterior on @xmath77 is given through    .... marp = function(p , comp=1 ) { sum(cardin[,2*(ncomp+1)]*dbeta(p , shape1=cardin[,2*comp-1]+1 ,        shape2=length(dat)-cardin[,2*comp-1]+1))/sum(cardin[,2*(ncomp+1 ) ] ) } pepe = seq(.01,.99,le=99 ) papar = apply(as.matrix(pepe),1,marp ) plot(pepe , papar , type=\"l\",xlab=\"p\",ylab=\"\",lwd=2 ) ....    now , even with this considerable reduction in the complexity of the posterior distribution ( to be compared with @xmath54 ) , the number of terms in the posterior still grows very fast both with @xmath75 and with the number of components @xmath16 , as shown through a few simulated examples in table [ tab : explose ] .",
    "( the missing items in the table simply took too much time or too much memory on the local mainframe when using our ` r ` program .",
    "@xcite used a specific ` c ` program to overcome this difficulty with larger sample sizes . )",
    "the computational pressure also increases with the range of the data ; that is , for a given value of @xmath119 , the number of rows in ` cardin ` is much larger when the observations are larger , as shown for instance in the first three rows of table [ tab : explose ] : a simulated poisson @xmath120 sample of size @xmath121 is primarily made up of zeros when @xmath122 but mostly takes different values when @xmath123 .",
    "the impact on the number of sufficient statistics can be easily assessed when @xmath124 .",
    "( note that the simulated dataset corresponding to @xmath125 in table [ tab : explose ] corresponds to a sample only made up of zeros , which explains the @xmath126 values of the sufficient statistic @xmath127 when @xmath128 . )",
    "l ccc @xmath129 & @xmath128 & @xmath130 & @xmath124 + @xmath131 & 11 & 66 & 286 + @xmath132 & 52 & 885 & 8160 + @xmath133 & 166 & 7077 & 120,908 + @xmath134 & 57 & 231 & 1771 + @xmath135 & 260 & 20,607 & 566,512 + @xmath136 & 565 & 100,713 &  + @xmath137 & 87 & 4060 & 81,000 + @xmath138 & 520 & 82,758 &  + @xmath139 & 1413 & 637,020 &  + @xmath140 & 216 & 13,986 &  + @xmath141 & 789 & 271,296 &  + @xmath142 & 2627 &  &  +    an interesting comment one can make about this decomposition of the posterior distribution is that it may happen that , as already noted in @xcite , a small number of values of the local sufficient statistic @xmath95 carry most of the posterior weight .",
    "table [ tab : cumuweit ] provides some occurrences of this feature , as for instance in the case @xmath143 .",
    "l ccc @xmath129 & @xmath128 & @xmath130 & @xmath124 + @xmath132 & 20/44 & 209/675 & 1219/5760 + @xmath133 & 58/126 & 1292/4641 & 13,247/78,060 + @xmath134 & 38/40 & 346/630 & 1766/6160 + @xmath135 & 160/196 & 4533/12,819 & 80,925/419,824 + @xmath144 & 99/314 & 5597/28,206 &  + @xmath145 & 21/625 & 13,981/117,579 &  + @xmath146 & 50/829 & 62,144/211,197 &  + @xmath136 & 1/580 & 259/103,998 &  + @xmath137 & 198/466 & 20,854/70,194 & 30,052/44,950 + @xmath138 & 202/512 & 18,048/80,470 &  + @xmath147 & 1/1079 & 58,820/366,684 &  +    we now turn to a minnow dataset made of @xmath148 observations , for which we need a minimal description .",
    "as seen in figure [ fig : topminnow1 ] , the datapoints take large values , which is a drawback from a computational point of view since the number of statistics to be registered is much larger than when all datapoints are small .",
    "for this reason , we can only process the mixture model with @xmath128 components .     _",
    "( top right ) _ marginal posterior distribution of @xmath82 _ ( bottom left ) _ marginal posterior distribution of @xmath77 _ ( bottom right ) _ histogram of the minnow dataset .",
    "( the prior parameters are @xmath149 and @xmath150 to remain compatible with the data range . ) ]    if we instead use a completely symmetric prior with identical hyperparameters for @xmath81 and @xmath82 , the output of the algorithm is then also symmetric in both components , as shown by figure [ fig : topminnow2 ] .",
    "the modes of the marginals of @xmath81 and @xmath82 remain the same , nonetheless .     for a symmetric prior with hyperparameter @xmath149 . ]",
    "the case of a multinomial mixture can be dealt with similarly : if we have @xmath75 observations @xmath151 from the mixture @xmath152 where @xmath153 and @xmath154 , the conjugate priors on the @xmath155s are dirichlet distributions @xmath156 , @xmath157 and we use once again the uniform prior on @xmath77 .",
    "( a default choice for the @xmath158 s is @xmath159 . )",
    "note that the @xmath160s may differ from observation to observation , since they are irrelevant for the posterior distribution : given a partition @xmath18 of the sample , the complete posterior is indeed @xmath161 up to a normalising constant that does not depend on @xmath18 .",
    "more generally , if we consider a mixture with @xmath162 components , @xmath163 the complete posterior is also directly available , as @xmath164 once more up to a normalising constant .",
    "the corresponding normalising constant of the dirichlet distribution being @xmath165 it produces the overall weight of a given partition @xmath18 as @xmath166 where @xmath167 is the number of observations allocated to component @xmath168 , @xmath169 is the sum of the @xmath170s for the observations @xmath171 allocated to component @xmath168 and @xmath172    given that the posterior distribution only depends on those  sufficient \" statistics @xmath173 and @xmath167 , the same factorisation as in the poisson case applies , namely that we simply need to count the number of occurrences of a particular local sufficient statistic @xmath174 .",
    "the book - keeping algorithm of @xcite applies in this setting as well .",
    "what follows is a nave r program translating the above :    .... em = dim(dat)[2 ] emp = em+1 empcomp = emp*ncomp    # matrix of sufficient statistics :   # last column is number of occurrences # each series of ( em+1 ) columns contains , first , number of allocations   # and , last , sum of multinomial observations cardin = matrix(0,ncol = empcomp+1,nrow = ncomp ) ....    therefore , the @xmath175th column of ` cardin ` contains the sum of the @xmath160s for the @xmath38 s allocated to the first component .    .... # initialisation for ( i in 1:ncomp ) cardin[i , emp*(i-1)+(1:emp)]=c(1,dat[1 , ] ) cardin[,empcomp+1]=1    # update for ( i in 2:dim(dat)[1 ] ) {        ncard = dim(cardin)[1 ]      update = matrix(t(cardin),ncol = empcomp+1,nrow = ncomp*ncard , byrow = t )        for ( j in 0:(ncomp-1 ) ) {          indi = j*ncard+(1:ncard )        empj = emp*j        update[indi , empj+1]=update[indi , empj+1]+1        update[indi , empj+(2:emp)]=t(t(update[indi , empj+(2:emp)])+dat[i , ] )        }        update = update[do.call(order , data.frame(update ) ) , ]        nu = dim(update)[1 ]      # changepoints      jj = c(1,(2:nu)[apply(abs(update[2:nu,1:empcomp]-update[1:(nu-1 ) ,                    1:empcomp]),1,sum)>0 ] )      # duplicates or rather ncomplicates !",
    "duplicates=(1:nu)[-jj ]      if ( length(duplicates)>0 ) {          for ( dife in 1:(ncomp-1 ) ) {            ji = jj[jj+dife<=nu ]          ii = ji[apply(abs(update[ji+dife,1:empcomp]-                    update[ji,1:empcomp]),1,sum)==0 ]          if ( length(ii)>0 )           update[ii , empcomp+1]=update[ii , empcomp+1]+                    update[ii+dife , empcomp+1 ]        }        update = update[-duplicates , ]        }        cardin = update      # print(sum(cardin[,2*ncomp+1])-ncomp^i )    } ....    where ` dat ` is now a matrix with @xmath16 columns .    the computation of the number of replicates of a given sufficient statistic @xmath176 @xmath177 , is then provided by the last column of the matrix ` cardin ` .",
    "the overall weight is then computed as the product of @xmath177 with the normalising constant :    .... olsums = matrix(0,ncol = ncomp , nrow = dim(update)[1 ] )    for ( y in 1:ncomp )   colsums[,y]=apply(update[,(y-1)*emp+(2:emp)],1,sum )    w = log(cardin[,empcomp+1])+      apply(lfactorial(cardin[,emp*(0:(ncomp-1))+1]),1,sum)+      apply(lfactorial(cardin [ ,              ( 1:empcomp)[-1-emp*(0:(ncomp-1))]]-.5),1,sum)-      apply(lfactorial(colsums)+em*.5 - 1,1,sum)- sum(lfactorial(dat ) ) w = exp(w - max(w ) ) cardin = cbind(cardin , w ) ....    as shown in table [ tab : multi ] , once again , the reduction in the number of cases to be considered is enormous .",
    "l cccc @xmath178 & @xmath179 & @xmath180 & @xmath181 & @xmath182 + @xmath183 & @xmath184 & @xmath185 & @xmath186 &  + @xmath187 & @xmath188 & @xmath189 & @xmath190 &  + @xmath191 & @xmath192 & @xmath193 & @xmath194 &  + @xmath195 & @xmath196 & @xmath197 & @xmath198 & @xmath199 + @xmath200 & @xmath201 & @xmath202 & @xmath203 &  + @xmath204 & @xmath205 & @xmath206 & @xmath207 &  + @xmath208 & @xmath209 & @xmath210 & @xmath211 &  + @xmath212 & @xmath213 & @xmath214 & @xmath215 & @xmath216 + @xmath217 & @xmath218 & @xmath219 & @xmath220 &  + @xmath221 & @xmath222 & @xmath223 &  &  + @xmath224 & @xmath225 & @xmath226 &  &  +    _ ( missing terms are due to excessive computational or storage requirements . ) _      for a normal mixture ,",
    "the number of truly different terms in the posterior distribution is much larger than in the previous ( discrete ) cases , in the sense that only permutations of the members of a given partition within each term of the partition provide the same local sufficient statistics .",
    "therefore , the number of observations that can be handled in an exact analysis is necessarily extremely limited .",
    "as mentioned in section [ sub : locconj ] , the locally conjugate priors for normal mixtures are products of normal @xmath227 by inverse gamma @xmath228 distributions .",
    "for instance , in the case of a two - component normal mixture , @xmath229 we can pick @xmath230 , @xmath231 , @xmath232 , if a difference of one between both means is considered likely ( meaning of course that the data are previously scaled ) and if @xmath233 is the prior assumption on the variance ( possibly deduced from the range of the sample ) . obviously , the choice of a gamma distribution with @xmath234 degrees of freedom is open to discussion , as it is not without consequences on the posterior distribution .",
    "the normalising constant of the prior distribution is ( up to a true constant ) @xmath235 indeed , the corresponding posterior is @xmath236\\ ] ] and @xmath237\\,.\\ ] ] the number of different sufficient statistics @xmath238 is thus related to the number of different partitions of the dataset into at most @xmath16 groups .",
    "this is related to the bell number @xcite , which grows extremely fast .",
    "we therefore do not pursue the example of the normal mixture any further for lack of practical purpose .",
    "this paper is a chapter of the book _ mixtures : estimation and applications _ , edited by the authors jointly with mike titterington and following the icms workshop on the same topic that took place in edinburgh , march 03 - 05 , 2010 .",
    "the authors are deeply grateful to the staff at icms for the organisation of the workshop , to the funding bodies ( epsrc , lms , edinburgh mathematical society , glasgow mathematical journal trust , and royal statistical society ) for supporting this workshop , and to the participants in the workshop for their innovative and exciting contributions .",
    "lee k , marin jm , mengersen k and robert c 2009 bayesian inference on mixtures of distributions in _ perspectives in mathematical sciences i : probability and statistics _",
    "sastry nn , delampady m and rajeev b ) , pp .",
    "world scientific singapore .",
    "mengersen k and robert c 1996 testing for mixtures : a bayesian entropic approach ( with discussion ) in _ bayesian statistics 5 _ ( ed berger j , bernardo j , dawid a , lindley d and smith a ) , pp .  255276 .",
    "oxford university press ."
  ],
  "abstract_text": [
    "<S> in this paper , we show how a complete and exact bayesian analysis of a parametric mixture model is possible in some cases when components of the mixture are taken from exponential families and when conjugate priors are used . </S>",
    "<S> this restricted set - up allows us to show the relevance of the bayesian approach as well as to exhibit the limitations of a complete analysis , namely that it is impossible to conduct this analysis when the sample size is too large , when the data are not from an exponential family , or when priors that are more complex than conjugate priors are used .    </S>",
    "<S> * keywords : * bayesian inference , conjugate prior , exponential family , poisson mixture , binomial mixture , normal mixture . </S>"
  ]
}