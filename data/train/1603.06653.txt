{
  "article_text": [
    "deep , regularized neural networks work better in practice than shallow unconstrained neural networks @xcite .",
    "this regularization takes classic forms such as l2-norm ridge regression , l1-norm lasso , architectural constraints such as convolutional layers @xcite , but also uses modern techniques such as dropout @xcite .",
    "recently , especially in the subfield of autoencoding neural networks , regularization has been accomplished with variational methods @xcite@xcite . in this paper",
    "we propose information theoretic - learning @xcite divergence measures for variational regularization .    in deep learning , variational regularization forces the function implemented by a neural network to be as close as possible to an imposed prior , which is a stronger restriction than that imposed by point - wise regularization methods such as l1 or l2 norms .",
    "variational methods for deep learning were popularized by the variational autoencoder ( vae ) framework proposed by @xcite and @xcite which also brought the attention of deep learning researchers to the reparametrization trick .",
    "the gaussian reparametrization trick works as follows : the encoder ( deep ) network outputs a mean @xmath0 and a standard deviation @xmath1 , from which we sample a latent factor @xmath2 , where @xmath3 .",
    "this latent factor is then fed forward to the decoder network and the parameters @xmath0 and @xmath1 are regularized using the kl - divergence @xmath4 between the inferred distribution and the imposed prior , which has a simple form @xcite . after training , one can generate data from a vae by first sampling from the gaussian prior distribution and feeding it to the vae s decoder .",
    "this is an approach similar to the inverse cumulative distribution method and does not involve estimation of the partition function , rejection sampling , or other complicated approaches @xcite .",
    "vae s methodology has been successfully extended to convolutional autoencoders @xcite and more elaborate architectures such as laplacian pyramids for image generation @xcite .",
    "unfortunately , vae can not be used when there does not exist a simple closed form solution for the kl - divergence . to cope with that , generative adversarial networks ( gan ) were proposed @xcite .",
    "gan uses two neural networks that are trained competitively  a generator network @xmath5 for sampling data and a discriminator network @xmath6 for discerning the outputs of @xmath5 from real data .",
    "unfortunately , training @xmath5 to match a high dimensional dataset distribution using only @xmath6 s binary `` fake '' or `` legit '' outputs is not a stable or simple process .",
    "makhzani et .",
    "al . proposed adversarial autoencoders @xcite which use an adversarial discriminator @xmath6 to tell the low dimensional codes in the output of the encoder from data sampled from a desired distribution . in this way",
    "adversarial autoencoders can approximate variational regularization as long as it is possible to sample from the desired distribution .",
    "we note that although this partially solves the problem of generalized functional regularization for neural networks , adversarial autoencoders require us to train a third network , the discriminator , in addition to the encoder and decoder already being trained .    here",
    "we observe that , assuming we can sample from the desired distribution , we can use empirical distribution divergence measures proposed by information theoretic - learning ( itl ) as a measure of how close the function implemented by an encoder network is to a desired prior distribution .",
    "thus , we propose information theoretic - learning autoencoders ( itl - ae ) .    in the next section of this paper",
    "we review itl s euclidean and cauchy - schwarz divergence measures @xcite . in section 3 we propose the itl - ae and run experiments to illustrate the proposed method in section 4 .",
    "we conclude the paper afterwards .",
    "information - theoretic learning ( itl ) is a field at the intersection of machine learning and information theory @xcite which encompasses a family of algorithms that compute and optimize information - theoretic descriptors such as entropy , divergence , and mutual information .",
    "itl objectives are computed directly from samples ( non - parametrically ) using parzen windowing and renyi s entropy @xcite .",
    "parzen density estimation is a nonparametric method for estimating the pdf of a distribution empirically from data . for samples @xmath7 drawn from a distribution @xmath8 ,",
    "the parzen window estimate of @xmath8 can be computed nonparametrically as @xmath9        intuitively , as shown in fig [ fig : parzen ] , parzen estimation corresponds to centering a gaussian kernel at each sample @xmath7 drawn from @xmath8 , and then summing to estimate the pdf . the optimal kernel size @xcite depends on the density of samples , which approaches zero as the number of samples approaches infinity .",
    "renyi s @xmath10-order entropy for probability density function ( pdf ) @xmath8 is given by : @xmath11 where @xmath12 .",
    "renyi s @xmath10-order entropy can be considered a generalization of shannon entropy since @xmath13 , which is shannon entropy . for the case of @xmath14 ,",
    "equation simplifies to @xmath15 which is known as renyi s quadratic entropy .    plugging into , for @xmath14",
    ", we obtain : @xmath16 where @xmath17 is the gaussian kernel , and @xmath1 is the kernel size .",
    "the argument of the logarithm in equation is called the _ information potential _ by analogy with potential fields from physics and is denoted by @xmath18 .",
    "another important itl descriptor is renyi s cross - entropy which is given by : @xmath19 similarly to equation , cross - entropy can be estimated by @xmath20 the argument of the logarithm in equation is called _ cross - information potential _ and is denoted @xmath21 .",
    "cross - information potential can be viewed as the average sum of interactions of samples drawn from @xmath22 with the estimated pdf @xmath23 ( or vice - versa ) .",
    "itl has also described a number of divergences connected with renyi s entropy . in particular , the euclidean and cauchy - schwarz divergences are given by : @xmath24 and @xmath25 respectively .",
    "equations and can be put in terms of information potentials : @xmath26 euclidean divergence is so named because it is equivalent to the euclidean distance between pdfs .",
    "furthermore , it is equivalent to maximum mean discrepancy @xcite ( mmd ) statistical test .",
    "cauchy - schwarz divergence is named for the cauchy - schwarz inequality , which guarantees the divergence is only equal to zero when the pdfs are equal almost everywhere .",
    "@xmath27 is symmetric but , unlike @xmath28 , does not obey the triangle equality .",
    "minimizing either divergence over @xmath22 , i.e. , @xmath29 , is a tradeoff between minimizing the information potential ( maximizing entropy ) of @xmath22 and maximizing the cross - information potential ( minimizing cross - entropy ) of @xmath22 with respect to @xmath30 .",
    "intuitively , minimizing the information potential encourages samples from @xmath22 to spread out , while maximizing the cross - information potential encourages samples from @xmath22 to move toward samples from @xmath30 .",
    "let us define autoencoders as a 4-tuple @xmath31 . where @xmath32 and @xmath6 are the encoder and the decoder functions , here parameterized as neural networks .",
    "@xmath33 is the reconstruction cost function that measures the difference between original data samples @xmath34 and their respective reconstructions @xmath35 .",
    "a typical reconstruction cost is mean - squared error .",
    "@xmath36 is a functional regularization . here",
    "this functional regularization will only be applied to the encoder @xmath32 . nonetheless ,",
    "although we are only regularizing the encoder @xmath32 , the interested investigator could also regularize another intermediate layer of the autoencoder .",
    "the general cost function for the itl - ae can be summarized by the following equation : @xmath37 where the strength of regularization is controlled by the scale parameter @xmath38 , and @xmath39 is the imposed prior .",
    "the functional regularization costs investigated in this paper are the itl euclidean and cauchy - schwarz divergences",
    ". both types of divergence encourage a smooth manifold similar to the imposed prior .",
    "that is , maximizing latent code distribution entropy encourages code samples to spread out , while minimizing the cross - entropy between the latent code and the prior distributions encourages code samples to be similar to prior samples .",
    "note that if the data dimensionality is too high , itl divergence measures require larger batch sizes to be reliably estimated , this explains why li et .",
    "@xcite used batches of 1000 samples in their experiments and also why they reduced the data dimensionality with autoencoders . in our own experiments ( not shown ) , the cauchy - schwarz divergence worked better than euclidean for high - dimensional data .",
    "generative moment matching networks ( gmmns ) @xcite correspond to the specific case where the input of the decoder @xmath6 comes from a multidimensional uniform distribution and the reconstruction function @xmath33 is given by the euclidean divergence measure .",
    "gmmns could be applied to generate samples from the original input space itself or from a lower dimensional previously trained stacked autoencoder ( sca ) @xcite hidden space .",
    "an advantage of our approach compared to gmmns is that we can train all the elements in the 4-tuple @xmath40 together without the elaborate process of training layerwise stacked autoencoders for dimensionality reduction .",
    "variational autoencoders ( vae ) @xcite adapt a lower bound of the variational regularization , @xmath36 , using parametric , closed form solutions for the kl - divergence .",
    "that divergence can be defined using shannon s entropy or @xmath41 .",
    "thus , we can also interpret itl - ae as nonparametric variational autoencoders , where the likelihood of the latent distribution is estimated empirically using parzen windows .",
    "note that since we can estimate that distribution directly , we do not use the reparametrization trick . here",
    "the reparametrization trick could possibly be used for imposing extra regularization , just like how adding dropout noise regularizes neural networks .",
    "adversarial autoencoders ( aa ) @xcite have the architecture that inspired our method the most . instead of using the adversarial trick to impose regularization on the encoder , we defined that regularization from first principles , which allowed us to train a competing method with much fewer trainable parameters .",
    "our most recent experiments show that aa scales better than itl - ae for high dimensional latent codes .",
    "we leave investigation into high dimensional itl - ae for future work .",
    "in this section we show experiments using the itl - ae architecture described in the previous section .",
    "first , for a visual interpretation of the effects of variational regularization we trained autoencoders with 2-dimensional latent codes .",
    "we used as desired priors a gaussian , a laplacian , and a 2d swiss - roll distribution .",
    "the resulting codes are shown in fig .",
    "[ fig : varreg ] .",
    "note that in all of these examples the autoencoder was trained in a completely unsupervised manner . however , given the simplicity of the data and the imposed reconstruction cost , some of the numbers were clustered in separate regions of the latent space .",
    "[ fig : samples ] shows some images obtained by sampling from a linear path on the swiss - roll and random samples from the gaussian manifold .",
    "for easier comparisons and to avoid extensive hyperparameter search , we constrained our encoder and decoders , @xmath32 and @xmath6 , to have the same architecture as those used in @xcite i.e. , each network is a two hidden layer fully connected network with 1000 hidden neurons .",
    "thus , the only hyperparameters investigated in this paper were kernel size @xmath1 and scale parameter @xmath38 .",
    "for the mnist dataset , the euclidean distance worked better with smaller kernels , such as @xmath42 or @xmath43 , while the cauchy - schwarz divergence required larger kernel , @xmath44 for example .",
    "nevertheless , here we will focus in regularizing the low dimensional latent codes and leave experiments using cauchy - schwarz divergence for future work .",
    "our best results for small batch sizes common in deep learning had 3-dimensional latent codes in the output of the encoder , euclidean divergence as the regularization @xmath36 and mean - squared error as the reconstruction cost @xmath33 .",
    "as we will show in the next section , we were able to obtain competitive results and reproduce behaviors obtained by methods trained with larger networks or extra adversarial networks .",
    "we followed the log - likelihood analysis on the mnist dataset reported on the literature @xcite@xcite@xcite .",
    "after training itl - ae on the training set , we generated @xmath45 images by inputting @xmath45 samples from a @xmath46 distribution to the decoder .",
    "those generated mnist images were used estimate a distribution using parzen windows on the high dimensional image space .",
    "we calculated the log - likelihood of separate 10k samples from the test set and report the results on table [ table : ll ] .",
    "the kernel size of that parzen estimator was chose using the best results on a held - out cross - validation dataset .",
    "that kernel size was @xmath47 .",
    "note that our method obtained the second best results between all the compared fully connected generative models .",
    "remember that this was obtained with about @xmath48 less adaptive parameters than the best method adversarial autoencoders .",
    ".log - likelihood of mnist test dataset .",
    "higher the values are better . [ cols=\"^,^,^,^ \" , ]     [ table : ll ]",
    "here we derived and validated the information theoretic - learning autoencoders , a non - parametric and ( optionally ) deterministic alternative to variational autoencoders .",
    "we also revisited itl for neural networks , but this time , instead of focusing on nonparametric cost functions for non - gaussian signal processing , we focused on distribution divergences for regularizing deep architectures .",
    "although our results were competitive for fully connected architectures , future work should address the scalability of the itl estimators for large dimensional latent spaces , which is common on large neural networks and convolutional architectures as well .",
    "nitish srivastava , geoffrey hinton , alex krizhevsky , ilya sutskever , and ruslan salakhutdinov , `` dropout : a simple way to prevent neural networks from overfitting , '' , vol .",
    "1 , pp . 19291958 , 2014 .",
    "emily  l denton , soumith chintala , rob fergus , et  al .",
    ", `` deep generative image models using a ?",
    "laplacian pyramid of adversarial networks , '' in _ advances in neural information processing systems _ , 2015 , pp .",
    "14861494 .",
    "ian goodfellow , jean pouget - abadie , mehdi mirza , bing xu , david warde - farley , sherjil ozair , aaron courville , and yoshua bengio , `` generative adversarial nets , '' in _ advances in neural information processing systems _ , 2014 , pp .",
    "26722680 .",
    "arthur gretton , karsten  m borgwardt , malte rasch , bernhard schlkopf , and alex  j smola , `` a kernel method for the two - sample - problem , '' in _ advances in neural information processing systems _ , 2006 , pp ."
  ],
  "abstract_text": [
    "<S> we propose information theoretic - learning ( itl ) divergence measures for variational regularization of neural networks . </S>",
    "<S> we also explore itl - regularized autoencoders as an alternative to variational autoencoding bayes , adversarial autoencoders and generative adversarial networks for randomly generating sample data without explicitly defining a partition function . </S>",
    "<S> this paper also formalizes , generative moment matching networks under the itl framework . </S>"
  ]
}