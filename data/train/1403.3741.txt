{
  "article_text": [
    "we consider a reinforcement learning agent that takes sequential actions within an uncertain environment with an aim to maximize cumulative reward @xcite .",
    "we model the environment as a markov decision process ( mdp ) whose dynamics are not fully known to the agent .",
    "the agent can learn to improve future performance by exploring poorly - understood states and actions , but might improve its short - term rewards through a policy which exploits its existing knowledge .",
    "efficient reinforcement learning balances exploration with exploitation to earn high cumulative reward .",
    "the vast majority of efficient reinforcement learning has focused upon the _ tabula rasa _ setting , where little prior knowledge is available about the environment beyond its state and action spaces . in this",
    "setting several algorithms have been designed to attain sample complexity polynomial in the number of states @xmath2 and actions @xmath3 @xcite .",
    "stronger bounds on regret , the difference between an agent s cumulative reward and that of the optimal controller , have also been developed .",
    "the strongest results of this kind establish @xmath5 regret for particular algorithms @xcite which is close to the lower bound @xmath0 @xcite .",
    "however , in many setting of interest , due to the curse of dimensionality , @xmath2 and @xmath3 can be so enormous that even this level of regret is unacceptable .    in many practical problems the agent _",
    "will _ have some prior understanding of the environment beyond _",
    "tabula rasa_. for example , in a large production line with @xmath6 machines in sequence each with @xmath7 possible states , we may know that over a single time - step each machine can only be influenced by its direct neighbors .",
    "such simple observations can reduce the dimensionality of the learning problem exponentially , but can not easily be exploited by a _",
    "tabula rasa _ algorithm .",
    "factored mdps ( fmdps ) @xcite , whose transitions can be represented by a dynamic bayesian network ( dbn ) @xcite , are one effective way to represent these structured mdps compactly .",
    "several algorithms have been developed that exploit the known dbn structure to achieve sample complexity polynomial in the _ parameters _ of the fmdp , which may be exponentially smaller than @xmath2 or @xmath3 @xcite .",
    "however , these polynomial bounds include several high order terms .",
    "we present two algorithms , ucrl - factored and psrl , with the first near - optimal regret bounds for factored mdps .",
    "ucrl - factored is an optimistic algorithm that modifies the confidence sets of ucrl2 @xcite to take advantage of the network structure .",
    "psrl is motivated by the old heuristic of thompson sampling @xcite and has been previously shown to be efficient in non - factored mdps @xcite .",
    "these algorithms are descibed fully in section [ sec : algos ] .",
    "both algorithms make use of approximate fmdp planner in internal steps . however , even where an fmdp can be represented concisely , solving for the optimal policy may take exponentially long in the most general case @xcite .",
    "our focus in this paper is upon the statistical aspect of the learning problem and like earlier discussions we do not specify which computational methods are used @xcite .",
    "our results serve as a reduction of the reinforcement learning problem to finding an approximate solution for a given fmdp . in many cases of interest , effective approximate planning methods for fmdps do exist . investigating and extending these methods",
    "are an ongoing subject of research @xcite .",
    "we consider the problem of learning to optimize a random finite horizon mdp @xmath8 in repeated finite episodes of interaction .",
    "@xmath9 is the state space , @xmath10 is the action space , @xmath11 is the reward distibution over @xmath12 in state @xmath13 with action @xmath14 , @xmath15 is the transition probability over @xmath16 from state @xmath13 with action @xmath14 , @xmath17 is the time horizon , and @xmath18 the initial state distribution .",
    "we define the mdp and all other random variables we will consider with respect to a probability space @xmath19 .",
    "a deterministic policy @xmath20 is a function mapping each state @xmath21 and @xmath22 to an action @xmath23 . for each mdp @xmath24 and policy @xmath20",
    ", we define a value function @xmath25,\\ ] ] where @xmath26 denotes the expected reward realized when action @xmath14 is selected while in state @xmath13 , and the subscripts of the expectation operator indicate that @xmath27 , and @xmath28 for @xmath29 .",
    "a policy @xmath20 is optimal for the mdp @xmath30 if @xmath31 for all @xmath21 and @xmath32 .",
    "we will associate with each mdp @xmath30 a policy @xmath33 that is optimal for @xmath30 .",
    "the reinforcement learning agent interacts with the mdp over episodes that begin at times @xmath34 , @xmath35 . at each time",
    "@xmath36 , the agent selects an action @xmath37 , observes a scalar reward @xmath38 , and then transitions to @xmath39 .",
    "let @xmath40 denote the history of observations made _ prior _ to time @xmath36 .",
    "a reinforcement learning algorithm is a deterministic sequence @xmath41 of functions , each mapping @xmath42 to a probability distribution @xmath43 over policies which the agent will employ during the @xmath44th episode .",
    "we define the regret incurred by a reinforcement learning algorithm @xmath45 up to time @xmath1 to be : @xmath46 where @xmath47 denotes regret over the @xmath44th episode , defined with respect to the mdp @xmath48 by @xmath49 with @xmath50 and @xmath51 .",
    "note that regret is not deterministic since it can depend on the random mdp @xmath48 , the algorithm s internal random sampling and , through the history @xmath42 , on previous random transitions and random rewards .",
    "we will assess and compare algorithm performance in terms of regret and its expectation .",
    "intuitively a factored mdp is an mdp whose rewards and transitions exhibit some conditional independence structure . to formalize this definition",
    "we must introduce some more notation common to the literature @xcite .    for any subset of indices @xmath52 let us define the scope set @xmath53 :",
    "= \\bigotimes\\limits_{i \\in z } { \\mathcal{x}}_i$ ] .",
    "further , for any @xmath54 define the scope variable @xmath55 \\in { \\mathcal{x}}[z ] $ ] to be the value of the variables @xmath56 with indices @xmath57 . for singleton sets",
    "@xmath58 we will write @xmath59 $ ] for @xmath60 $ ] in the natural way .",
    "let @xmath61 be the set of functions mapping elements of a finite set @xmath62 to probability mass functions over a finite set @xmath63 .",
    "@xmath64 will denote the set of functions mapping elements of a finite set @xmath62 to @xmath65-sub - gaussian probability measures over @xmath66 with mean bounded in @xmath67 $ ] . for reinforcement learning we will write @xmath62 for @xmath68 and consider factored reward and factored transition functions which are drawn from within these families .",
    "the reward function class @xmath69 is factored over @xmath70 with scopes @xmath71 if and only if , for all @xmath72 there exist functions @xmath73 , { \\mathds{r } } } \\}_{i=1}^l $ ] such that , @xmath74 = \\sum_{i=1}^l { \\mathds{e}}\\big [ r_i \\big]\\ ] ] for @xmath75 is equal to @xmath76 with each @xmath77)$ ] and individually observed .",
    "the transition function class @xmath78 is factored over @xmath79 and @xmath80 with scopes @xmath81 if and only if , for all @xmath82 there exist some @xmath83,{\\mathcal{s}}_i}\\}_{i=1}^m$ ] such that , @xmath84 \\ \\bigg\\vert \\",
    "x[z_i ] \\right)\\ ] ]    a factored mdp ( fmdp ) is then defined to be an mdp with both factored rewards and factored transitions . writing @xmath85 a fmdp is fully characterized by the tuple @xmath86 where @xmath87 and @xmath88 are the scopes for the reward and transition functions respectively in @xmath89 for @xmath90 .",
    "we assume that the size of all scopes @xmath91 and factors @xmath92 so that the domains of @xmath93 and @xmath94 are of size at most @xmath95 .",
    "our first result shows that we can bound the expected regret of psrl .",
    "[ thm : reg psrl ] let @xmath48 be factored with graph structure @xmath96 . if @xmath97 is the distribution of @xmath48 and @xmath98 is the span of the optimal value function then we can bound the regret of psrl : @xmath99          \\hspace{-3 mm } & \\le & \\hspace{-2 mm }          \\sum_{i=1}^l \\left\\ { 5\\tau c |{\\mathcal{x}}[z^r_i]| + 12\\sigma\\sqrt{| { \\mathcal{x}}[z^r_i]|",
    "t \\log\\left(4 l | { \\mathcal{x}}[z^r_i ] | k t\\right ) } \\right\\ } \\nonumber + 2 \\sqrt{t } \\\\      & & \\hspace{-36 mm } + 4 + { \\mathds{e}}[\\psi ] \\left ( 1 + \\frac{4}{t-4 } \\right ) \\sum_{j=1}^m \\left\\ { 5\\tau |{\\mathcal{x}}[z^p_j]| + 12\\sqrt{| { \\mathcal{x}}[z^p_j]| |{\\mathcal{s}}_j |    t \\log\\left(4 m | { \\mathcal{x}}[z^p_j ] | k t\\right ) } \\right\\}\\end{aligned}\\ ] ]    we have a similar result for ucrl - factored that holds with high probability .    [ thm : reg ucrl - factored ] let @xmath48 be factored with graph structure @xmath96 .",
    "if @xmath100 is the diameter of @xmath48 , then for any @xmath48 can bound the regret of ucrl - factored : @xmath101| + 12\\sigma\\sqrt { | { \\mathcal{x}}[z^r_i]| t \\log\\left(12 l | { \\mathcal{x}}[z^r_i ] | k t / \\delta\\right ) }   \\right\\ } \\nonumber + 2\\sqrt{t}\\\\      & & \\hspace{-37 mm } + cd\\sqrt{2 t \\log(6/\\delta ) }   + \\",
    "cd \\sum_{j=1}^m \\left\\ { 5\\tau |{\\mathcal{x}}[z^p_j]| + 12\\sqrt{| { \\mathcal{x}}[z^p_j]| |{\\mathcal{s}}_j |    t \\log\\left(12 m | { \\mathcal{x}}[z^p_j ] | k t / \\delta \\right ) } \\right\\}\\end{aligned}\\ ] ] with probability at least @xmath102    both algorithms give bounds",
    "@xmath103| |s_j| t}\\right)$ ] where @xmath104 is a measure of mdp connectedness : expected span @xmath105 $ ] for psrl and scaled diameter @xmath106 for ucrl - factored .",
    "the span of an mdp is the maximum difference in value of any two states under the optimal policy @xmath107 .",
    "the diameter of an mdp is the maximum number of expected timesteps to get between any two states @xmath108 .",
    "psrl s bounds are tighter since @xmath109 and may be exponentially smaller .    however , ucrl - factored has stronger probabilistic guarantees than psrl since its bounds hold with high probability for any mdp @xmath48 not just in expectation .",
    "there is an optimistic algorithm regal @xcite which formally replaces the ucrl2 @xmath100 with @xmath98 and retains the high probability guarantees .",
    "an analogous extension to regal - factored is possible , however , no practical implementation of that algorithm exists even with an fmdp planner",
    ".    the algebra in theorems [ thm : reg psrl ] and [ thm : reg ucrl - factored ] can be overwhelming . for clarity",
    ", we present a symmetric problem instance for which we can produce a cleaner single - term upper bound .",
    "let @xmath110 be shorthand for the simple graph structure with @xmath111 , @xmath112 , @xmath113 and @xmath114 for @xmath115 and @xmath116 , we will write @xmath117 .    [ cor : reg psrl ] if @xmath97 is the distribution of @xmath48 with structure @xmath110 then we can bound the regret of psrl : @xmath118      \\le 15 m \\tau \\sqrt{j k t \\log(2mj t)}\\ ] ]    [ cor : reg ucrl - factored ] for any mdp @xmath48 with structure @xmath110 we can bound the regret of ucrl - factored : @xmath119 with probability at least @xmath102 .    both algorithms satisfy bounds of @xmath120 which is exponentially tighter than can be obtained by any @xmath110-naive algorithm .",
    "for a factored mdp with @xmath6 independent components with @xmath2 states and @xmath3 actions the bound @xmath121 is close to the lower bound @xmath122 and so the bound is near optimal .",
    "the corollaries follow directly from theorems [ thm : reg psrl ] and [ thm : reg ucrl - factored ] as shown in appendix [ sec : clean symmetric bounds ] .",
    "our analysis will rely upon the construction of confidence sets based around the empirical estimates for the underlying reward and transition functions .",
    "the confidence sets are constructed to contain the true mdp with high probability .",
    "this technique is common to the literature , but we will exploit the additional graph structure @xmath123 to sharpen the bounds .    consider a family of functions @xmath124 which takes @xmath54 to a probability distribution over @xmath125",
    ". we will write @xmath126 unless we wish to stress a particular @xmath65-algebra .",
    "let @xmath62 be a finite set , and let @xmath127 be a measurable space .",
    "the _ width _ of a set @xmath128 at @xmath54 with respect to a norm @xmath129 is @xmath130    our confidence set sequence @xmath131 is initialized with a set @xmath132 .",
    "we adapt our confidence set to the observations @xmath133 which are drawn from the true function @xmath134 at measurement points @xmath135 so that @xmath136 . each confidence set",
    "is then centered around an empirical estimate @xmath137 at time @xmath36 , defined by @xmath138 where @xmath139 is the number of time @xmath140 appears in @xmath141 and @xmath142 is the probability mass function over @xmath63 that assigns all probability to the outcome @xmath143 .",
    "our sequence of confidence sets depends on our choice of norm @xmath144 and a non - decreasing sequence @xmath145 . for each @xmath36 ,",
    "the confidence set is defined by : @xmath146 where @xmath147 is shorthand for @xmath141 and we interpret @xmath148 as a null constraint .",
    "the following result shows that we can bound the sum of confidence widths through time .",
    "[ thm : widths ] for all finite sets @xmath62 , measurable spaces @xmath127 , function classes @xmath149 with uniformly bounded widths @xmath150 and non - decreasing sequences @xmath145 :    @xmath151    the proof follows from elementary counting arguments on @xmath139 and the pigeonhole principle .",
    "a full derivation is given in appendix [ sec : widths ] .",
    "with our notation established , we are now able to introduce our algorithms for efficient learning in factored mdps . psrl and ucrl - factored",
    "proceed in episodes of fixed policies . at the start of the @xmath44th episode",
    "they produce a candidate mdp @xmath152 and then proceed with the policy which is optimal for @xmath152 . in psrl",
    ", @xmath152 is generated by a sample from the posterior for @xmath48 , whereas ucrl - factored chooses @xmath152 optimistically from the confidence set @xmath153 .",
    "both algorithms require prior knowledge of the graphical structure @xmath123 and an approximate planner for fmdps",
    ". we will write @xmath154 for a planner which returns @xmath155-optimal policy for @xmath30",
    ". we will write @xmath156 for a planner which returns an @xmath155-optimal policy for most optimistic realization from a family of mdps @xmath157 .",
    "given @xmath158 it is possible to obtain @xmath159 through extended value iteration , although this might become computationally intractable @xcite .    psrl remains identical to earlier treatment @xcite provided @xmath123 is encoded in the prior @xmath97 .",
    "ucrl - factored is a modification to ucrl2 that can exploit the graph and episodic structure of .",
    "we write @xmath160 and @xmath161 as shorthand for these confidence sets @xmath162 | , x^{t-1}_1[z^r_i],d_t^{r_i})$ ] and @xmath163,d_t^{p_j})$ ] generated from initial sets @xmath164,{\\mathds{r}}}$ ] and @xmath165,{\\mathcal{s}}_j}$ ] .",
    "we should note that ucrl2 was designed to obtain regret bounds even in mdps without episodic reset .",
    "this is accomplished by imposing artificial episodes which end whenever the number of visits to a state - action pair is doubled @xcite .",
    "it is simple to extend ucrl - factored s guarantees to this setting using this same strategy .",
    "this will not work for psrl since our current analysis requires that the episode length is independent of the sampled mdp .",
    "nevertheless , there has been good empirical performance using this method for mdps without episodic reset in simulation @xcite .",
    "* input : * prior @xmath97 encoding @xmath123 , @xmath166    * input : * graph structure @xmath123 , confidence @xmath167 , @xmath166",
    "for our common analysis of psrl and ucrl - factored we will let @xmath168 refer generally to either the sampled mdp used in psrl or the optimistic mdp chosen from @xmath153 with associated policy @xmath169 ) .",
    "we introduce the bellman operator @xmath170 , which for any mdp @xmath8 , stationary policy @xmath171 and value function @xmath172 , is defined by @xmath173 this returns the expected value of state @xmath13 where we follow the policy @xmath20 under the laws of @xmath30 , for one time step .",
    "we will streamline our discussion of @xmath174 and @xmath175 by simply writing @xmath176 in place of @xmath48 or @xmath177 and @xmath44 in place of @xmath168 or @xmath169 where appropriate ; for example @xmath178 .",
    "we will also write @xmath179 .",
    "we now break down the regret by adding and subtracting the _ imagined _ near optimal reward of policy @xmath180 , which is known to the agent . for clarity of analysis",
    "we consider only the case of @xmath181 but this changes nothing for our consideration of finite @xmath16 .",
    "@xmath182 @xmath183 relates the optimal rewards of the mdp @xmath48 to those near optimal for @xmath168 .",
    "we can bound this difference by the planning accuracy @xmath184 for psrl in expectation , since @xmath48 and @xmath152 are equal in law , and for ucrl - factored in high probability by optimism .",
    "we decompose the first term through repeated application of dynamic programming : @xmath185 where @xmath186 is a martingale difference bounded by @xmath187 , the span of @xmath188 . for",
    "ucrl - factored we can use optimism to say that @xmath189 @xcite and apply the azuma - hoeffding inequality to say that : @xmath190    the remaining term is the one step bellman error of the imagined mdp @xmath168 .",
    "crucially this term only depends on states and actions @xmath191 which are actually observed .",
    "we can now use the hlder inequality to bound @xmath192      we aim to exploit the graphical structure @xmath123 to create more efficient confidence sets @xmath153 .",
    "it is clear from that we may upper bound the deviations of @xmath193 factor - by - factor using the triangle inequality .",
    "our next result , lemma [ lem : factor bound ] , shows we can also do this for the transition functions @xmath194 and @xmath195 .",
    "this is the key result that allows us to build confidence sets around each factor @xmath196 rather than @xmath194 as a whole .",
    "[ lem : factor bound ] let the transition function class @xmath197 be factored over @xmath198 and @xmath80 with scopes @xmath81 .",
    "then , for any @xmath199 we may bound their l1 distance by the sum of the differences of their factorizations : @xmath200 ) - \\tilde{p}_i(x[z_i ] ) \\|_1\\ ] ]    we begin with the simple claim that for any @xmath201 $ ] : @xmath202 this result also holds for any @xmath203 $ ] , where @xmath204 can be verified case by case .",
    "we now consider the probability distributions @xmath205 over @xmath206 and @xmath207 over @xmath208 .",
    "we let @xmath209 be the joint probability distribution over @xmath210 .",
    "using the claim above we bound the l1 deviation @xmath211 by the deviations of their factors : @xmath212 we conclude the proof by applying this @xmath6 times to the factored transitions @xmath213 and @xmath214 .",
    "we now want to show that the true mdp lies within @xmath153 with high probability .",
    "note that posterior sampling will also allow us to then say that the sampled @xmath152 is within @xmath153 with high probability too . in order to show this , we first present a concentration result for the l1 deviation of empirical probabilities .    [",
    "lem : weissman ] for all finite sets @xmath62 , finite sets @xmath63 , function classes @xmath215 then for any @xmath54 , @xmath216 the deviation the true distribution @xmath194 to the empirical estimate after @xmath36 samples @xmath217 is bounded : @xmath218    this is a relaxation of the result proved by weissman @xcite .",
    "lemma [ lem : weissman ] ensures that for any @xmath54 @xmath219 .",
    "we then define @xmath220 with @xmath221 | k^2)$ ] . now using a union bound we conclude @xmath222 .",
    "[ lem : tail ] if @xmath223 are all independent and sub @xmath65-gaussian then @xmath224 : @xmath225    a similar argument now ensures that @xmath226 , and so @xmath227      we now have all the necessary intermediate results to complete our proof .",
    "we begin with the analysis of psrl . using equation and the fact that @xmath228 are equal in law by posterior sampling , we can say that @xmath229 .",
    "the contributions from regret in planning function @xmath158 are bounded by @xmath230 .",
    "from here we take equation , lemma [ lem : factor bound ] and theorem [ thm : widths ] to say that for any @xmath231 : @xmath99 & \\le & 4\\delta t + 2 \\sqrt{t } +          \\sum_{i=1}^l \\left\\ { 4(\\tau c |{\\mathcal{x}}[z^r_i]| + 1 ) + 4\\sqrt{2d_t^{r_i } | { \\mathcal{x}}[z^r_i]| t } \\right\\ } \\\\      & & \\hspace{-25 mm } + \\sup_{k=1, .. ,l } \\big ( { \\mathds{e}}[\\psi_k | m_k , m^ * \\in { \\mathcal{m}}_k ] \\big ) \\times \\sum_{j=1}^m \\left\\ { 4(\\tau |{\\mathcal{x}}[z^p_j]| + 1 ) + 4\\sqrt{2d_t^{p_j } | { \\mathcal{x}}[z^p_j]| t } \\right\\}\\end{aligned}\\ ] ] let @xmath232 , since @xmath233 and by posterior sampling @xmath234 = { \\mathds{e}}[\\psi]$ ] for all @xmath44 : @xmath235 \\le { \\mathds{p}}(a)^{-1 } { \\mathds{e}}[\\psi ] \\le \\left ( 1 - \\frac{4 \\delta}{k^2 } \\right)^{-1 } { \\mathds{e } } [ \\psi ] = \\left(1 + \\frac{4\\delta}{k^2 - 4\\delta } \\right ) { \\mathds{e } } [ \\psi ] \\le \\left(1 + \\frac{4\\delta}{1 - 4\\delta } \\right ) { \\mathds{e } } [ \\psi ] .\\ ] ] plugging in @xmath236 and @xmath237 and setting @xmath238 completes the proof of theorem [ thm : reg psrl ] .",
    "the analysis of ucrl - factored and theorem [ thm : reg ucrl - factored ] follows similarly from and .",
    "corollaries [ cor : reg psrl ] and [ cor : reg ucrl - factored ] follow from substituting the structure @xmath110 and upper bounding the constant and logarithmic terms .",
    "this is presented in detail in appendix [ sec : clean symmetric bounds ] .",
    "we present the first algorithms with near - optimal regret bounds in factored mdps .",
    "many practical problems for reinforcement learning will have extremely large state and action spaces , this allows us to obtain meaningful performance guarantees even in previously intractably large systems .",
    "however , our analysis leaves several important questions unaddressed .",
    "first , we assume access to an approximate fmdp planner that may be computationally prohibitive in practice .",
    "second , we assume that the graph structure is known a priori but there are other algorithms that seek to learn this from experience @xcite . finally , we might consider dimensionality reduction in large mdps more generally , where either the rewards , transitions or optimal value function are known to belong in some function class @xmath132 to obtain bounds that depend on the dimensionality of @xmath132 .",
    "osband is supported by stanford graduate fellowships courtesy of paccar inc .",
    "this work was supported in part by award cmmi-0968707 from the national science foundation .",
    "we present elementary arguments which culminate in a proof of theorem [ thm : widths ] .",
    "[ lem : rad ] for all finite sets @xmath62 and any @xmath239 : @xmath240 where @xmath241 .",
    "let @xmath242 be the largest subsequence of @xmath243 such that @xmath244 \\",
    "\\forall i$ ] .",
    "now for any @xmath54 , let @xmath245 .",
    "suppose there exist two distinct elements @xmath246 with @xmath247 so that @xmath248 .",
    "we note that for any @xmath249 so that : @xmath250 this contradicts our assumption @xmath251 $ ] and so we must conclude that @xmath252 for all @xmath253 .",
    "this means that @xmath242 forms a subsequence of unique elements in @xmath62 , the total length of which must be bounded by @xmath254 .",
    "we now provide a corollary of this result which allows for episodic delays in updating visit counts @xmath139 .",
    "we imagine that we will only update our counts every @xmath17 steps .",
    "[ cor : rad ep ] let us associate times within episodes of length @xmath17 , @xmath255 for @xmath256 and @xmath257 . for all finite sets @xmath62 and any @xmath239 : @xmath258 where @xmath259 is the @xmath17-fold composition of @xmath260 acting on @xmath155 .    by an argument of visiting times similar to lemma [ lem : rad ]",
    "we can see that the worst case scenario for the episodic case @xmath261 is to visit each @xmath140 exactly @xmath262 times before the start of an episode , and then spend the entirety of the following episode within the state .",
    "here we have upper bounded @xmath263 by @xmath264 and @xmath265 by @xmath266 to complete our result .",
    "it will be useful to define notion of radius for each confidence set at each @xmath54 , @xmath267 by the triangle inequality , we have @xmath268 for all @xmath54 .",
    "[ lem : large rad ] let us write @xmath269 for @xmath270 and associate times within episodes of length @xmath17 , @xmath255 for @xmath256 and @xmath257 .",
    "for all finite sets @xmath62 , measurable spaces @xmath127 , function classes @xmath149 , non - decreasing sequences @xmath145 , any @xmath271 and @xmath272 : @xmath273    by construction of @xmath274 and noting that @xmath275 is non - decreasing in @xmath36 , we can say that @xmath276 for all @xmath277 so that @xmath278    now let @xmath279 be the @xmath155-inverse of @xmath259 such that @xmath280 .",
    "applying corollary [ cor : rad ep ] to our expression @xmath281 times repeatedly we can say : @xmath282 where @xmath283 denotes the composition of @xmath284 @xmath281-times acting on @xmath155 . if we take @xmath281 to be the lowest integer such that @xmath285 then , @xmath286 so that the whole expression is bounded by @xmath287 . note that for all @xmath288 , @xmath289 , if we write @xmath290 then @xmath291 , which completes the proof .    using these results we are finally able to complete our proof of theorem [ thm : widths ]",
    "we first note that , via the triangle inequality @xmath292 .",
    "we streamline our notation by letting @xmath293 . reordering the sequence @xmath294 such @xmath295 we have that :",
    "@xmath296 we can see that @xmath297 . from lemma [ lem :",
    "large rad ] this means that @xmath298 , so that @xmath299 .",
    "this means that @xmath300 .",
    "therefore , @xmath301 which completes the proof of theorem [ thm : widths ] .",
    "we now provide concrete clean upper bounds for theorems [ thm : reg psrl ] and [ thm : reg ucrl - factored ] in the simple symmetric case @xmath111 , @xmath112 , @xmath113 and @xmath302 for all suitable @xmath303 and write @xmath117 . for a non - trivial problem setting we assume that @xmath304 , @xmath305 , @xmath306 .    from section [ sec : bounds ] we have that @xmath99 & \\le & 4 + 2 \\sqrt{t } +          m \\left\\ { 4(\\tau j + 1 ) + 4\\sqrt{8 \\log(4mj t^2/\\tau ) j t } \\right\\ } \\\\          & & + \\   { \\mathds{e}}[\\psi]\\left(1 + \\frac{4}{t-4}\\right ) m \\left\\ { 4(\\tau j + 1 ) + 4\\sqrt{8 k \\log(4mj t^2 / \\tau ) j t } \\right\\}\\end{aligned}\\ ] ] through looking at the constant term we know that the bounds are trivially satisfied for all @xmath307 , from here we can certainly upper bound @xmath308 . from here we can say that : @xmath99 & \\le & \\left\\ { 4 + 4m\\left(1 + \\frac{14}{13 } { \\mathds{e}}[\\psi]\\right)(\\tau j + 1 ) \\right\\ } \\\\          & & + \\sqrt{t } \\left\\ { 2 + 4\\sqrt{8j \\log(4mj t^2/\\tau ) } + 4\\sqrt{8 jk \\log(4mj t^2/\\tau ) } \\frac{14}{13 } { \\mathds{e}}[\\psi ] \\right\\ } \\\\      & \\le & 5 \\left ( 1 + { \\mathds{e}}[\\psi ] \\right ) m \\tau j + \\sqrt{t } \\left\\",
    "{ 12\\sqrt{j\\log(2mj t ) } + 12{\\mathds{e}}[\\psi ] \\sqrt{jk\\log(2mj t ) } \\right\\ } \\\\      & \\le & 5 \\left ( 1 + { \\mathds{e}}[\\psi ] \\right ) m \\tau j + 12m\\left ( 1 + { \\mathds{e } } [ \\psi ] \\sqrt{k } \\right ) \\sqrt{j t\\log(2mj t ) } \\\\      & \\le & \\min(5 m \\tau^2 j , t ) + 12 m \\tau \\sqrt{j k t \\log(2mj t ) } \\\\      & \\le & 15 m \\tau \\sqrt{j k t \\log(2mj t)}\\end{aligned}\\ ] ] where in the last steps we have used that @xmath309 and @xmath310 .",
    "we now repeat a similar procedure of upper bounds for ucrl - factored , immediately replicating @xmath100 by @xmath17 in our analysis to say that with probability @xmath311 : @xmath312 where in the last step we used a similar argument"
  ],
  "abstract_text": [
    "<S> any reinforcement learning algorithm that applies to all markov decision processes ( mdps ) will suffer @xmath0 regret on some mdp , where @xmath1 is the elapsed time and @xmath2 and @xmath3 are the cardinalities of the state and action spaces . </S>",
    "<S> this implies @xmath4 time to guarantee a near - optimal policy . in many settings of practical interest , due to the curse of dimensionality , @xmath2 and @xmath3 </S>",
    "<S> can be so enormous that this learning time is unacceptable . </S>",
    "<S> we establish that , if the system is known to be a _ factored _ mdp , it is possible to achieve regret that scales polynomially in the number of _ parameters _ encoding the factored mdp , which may be exponentially smaller than @xmath2 or @xmath3 . </S>",
    "<S> we provide two algorithms that satisfy near - optimal regret bounds in this context : posterior sampling reinforcement learning ( psrl ) and an upper confidence bound algorithm ( ucrl - factored ) . </S>"
  ]
}