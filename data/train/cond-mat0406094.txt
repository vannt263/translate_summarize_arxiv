{
  "article_text": [
    "self - consistent - field ( scf ) theories such as density functional theory and hybrid hartree - fock / density functional theory are accurate and computationally efficient .",
    "traditional gaussian - orbital quantum chemistry codes that use conventional methods@xcite are usually restricted to small systems since these methods have steep scaling of @xmath1 with respect to system size , @xmath0 .",
    "recently , significant progress has been made in the development of @xmath2 methods that overcome these bottlenecks .",
    "these methods include computation of the hartree ",
    "fock exchange matrix @xcite , the coulomb matrix  @xcite , the exchange - correlation matrix  @xcite , and iterative alternatives to eigensolution of the scf equations  @xcite .    with the advent of parallel multi - processor computers , especially those based on commodity processors",
    ", there has been a great effort in the community to parallelize quantum chemistry codes@xcite .",
    "successful parallelization of @xmath2 methods hold promise for large scale computations given the fact that with parallel linear scaling methods , an @xmath3-fold increase in processors should lead to an @xmath3-fold increase in simulation capability",
    ". however , this holds only for scalable algorithms .",
    "two of the most computationally demanding parts in a density functional application are calculation of the exchange - correlation and coulomb matrices .",
    "the @xmath2 exchange - correlation matrix calculation has been efficiently parallelized through the concept of equal time ( et ) partition@xcite . in this work",
    ", the et partition is extended to load balancing calculation of the coulomb matrix .    linear scaling computation of the coulomb matrix has been achieved via the quantum - chemical tree - code ( qctc)@xcite and the continuous fast multipole method ( cfmm)@xcite . both the tree - code@xcite and the fast multipole method@xcite were originally proposed to handle the astrophysical @xmath0-body problem .",
    "parallelization of these @xmath0-body algorithms has been an active area of research in the computer science community@xcite .",
    "even though both the @xmath0-body problem and the coulomb matrix calculation share many similarities , especially in handling the far - field multipole contribution , little work has been done on the parallel coulomb matrix calculation beyond the simple master - slave approach@xcite .",
    "it should be pointed out that parallel @xmath2 computation of the coulomb matrix is highly irregular relative to parallel @xmath4 computation of the two - electron coulomb integrals , where the jobs are significantly coarse grained , enabling the master - slave approach to work well .",
    "it is well - known that the master - slave approach faces potential contention and load imbalance problems for fine grained parallelism@xcite ( a small ratio of work load to number of processors ) .",
    "these problems have indeed been observed in quantum chemical calculations@xcite , so alternatives are needed .",
    "one may use the idea of counting the number of interactions in parallel @xmath0-body codes to load balance computation as in the orthogonal recursive bisection ( orb)@xcite or costzones methods@xcite .",
    "however , due to cost irregularities associated with different gaussian extents , angular symmetries , and non - uniform access patterns , simple counting is not an optimal approach to load balance computation of the coulomb matrix .    in this work ,",
    "our main emphasis is on load balancing the most time consuming part of qctc , which is traversal of the density tree for evaluation of coulomb matrix elements . to load balance this highly irregular tree traversal",
    ", we use the equal time ( et ) partition@xcite , which was originally proposed to parallelize computation of the exchange - correlation matrix .",
    "equal time partition works by measuring the time spent in computational sub - domains ( _ e.g. _ a line , area , or volume ) during one scf cycle . at the end of the calculation",
    ", the time spent in each sub - domain is used to predict a new overall domain decomposition for the next scf cycle , where each new sub - domain ideally incurs the same amount of work in the next scf cycle .",
    "the predicted domain decomposition will deliver an improved load balance in the next scf cycle when there is a smooth variation of the workload between successive scf cycles ( _ e.g. _ due to small changes in the electron density ) . in this way , temporal locality@xcite of the problem",
    "is exploited to achieve a continuously improved load balance .",
    "in serial , the time to build the density tree constitutes about 2% or less of the total time spent in qctc .",
    "unfortunately , amdahl s law dictates that the performance of a massively parallel program is ultimately determined by its serial parts .",
    "therefore we also need to consider parallel construction of the density tree .",
    "again , ideas from parallel @xmath0-body codes may be useful .",
    "the construction of locally essential trees , which are _ just _ sufficient for tree traversal on each processor,@xcite avoids the problem of replicating the total density tree on each processor . hashed oct - trees@xcite also solve the replication problem , where hash tables are used to allow the program to access data in an efficient manner across multiple processors .",
    "however , due to fact that these approaches entail significant code restructuring , for the present case , we have chosen a parallel replicated density tree approach .",
    "the remainder of this paper is organized as follows : in section  [ paraqctc ] we discuss our strategy to efficiently parallelize computation of the coulomb matrix @xmath5 . in section  [ sec : implementation ] we describe a computational implementation of parallel qctc . in section  [ results ]",
    "we discuss results of speedup tests performed on a few representative finite and periodic systems . in section  [ conclusions ] we summarize the main conclusions of the paper .",
    "the quantum - chemical tree - code ( qctc ) for @xmath2 calculation of the coulomb matrix has been fully described in refs .  [ ] and [ ] . here we only highlight essential aspects of the algorithm so that discussions of parallelization of qctc may be made .    the coulomb matrix element in a finite ( gas phase )",
    "case is given by @xmath6 where the charge distribution@xcite ( or simply the distribution ) @xmath7 , is a product of the ( gaussian ) basis functions @xmath8 and @xmath9 .",
    "the total density of the system , which includes both the electronic and nuclear parts , is denoted by @xmath10 . in qctc , a hierarchical multipole representation of the electron density , called a density tree ,",
    "is stored in an advanced @xmath11-d tree data structure@xcite . a compact representation of the density in terms of hermite - gaussian ( hg)@xcite basis has been used . with the help of the density tree ,",
    "qctc re - expresses the matrix element in eq .",
    "( [ eq : jab ] ) as a sum of near - field ( nf ) and far - field ( ff ) terms@xcite @xmath12 \\sum_{\\ell ' } \\sum_{m ' } m^{\\ell+\\ell'}_{m+m ' } o^{\\ell'}_{m'}[\\rho_q ] \\nonumber \\\\ & + & \\sum_{q\\in \\rm nf } \\int d { \\bf r } \\int d { \\bf r ' } \\rho_{ab } ( { \\bf r } ) \\left|{\\bf r}-{\\bf r ' } \\right|^{-1 } \\rho_q({\\bf r } ) \\label{eq : jabtree}\\end{aligned}\\ ] ] where @xmath13 is the irregular solid harmonic interaction tensor , @xmath14=\\int d{\\bf r } o^\\ell_m({\\bf r } ) f({\\bf r})$ ] is a moment of the regular solid harmonics , @xmath15 runs over the all nodes in the density tree as determined by penetration admissibility criterion ( pac ) and multipole admissibility criterion ( mac)@xcite and @xmath16 runs on the left over near - field primitive distributions in the density . for the periodic case , a periodic far - field term and a tin - foil boundary condition term@xcite",
    "are added to the rhs of eq .",
    "( [ eq : jabtree ] ) .",
    "essential to qctc is construction of the total density tree . once the density tree is built ,",
    "calculation of the coulomb matrix elements proceeds by transversing the tree and checking the pac and mac .",
    "when both pac and mac are met , the far - field contribution is calculated via the multipole approximation .",
    "the near - field contribution , however , is calculated analytically .    from eq .",
    "( [ eq : jabtree ] ) , it is easy to see that @xmath2 computation of the coulomb matrix is highly irregular ; near- and far - field contributions are determined on the fly via a pac and mac that depends on both the distributions and the density .",
    "this poses a challenge to efficiently load balance parallel tree traversal in qctc .",
    "since et partition involves measuring workload using a timer ( e.g. the mpi_wtime function in the message - passing libary mpi@xcite ) , it is important to decide what work load information to time so that the timing process itself does not incur too much overhead .",
    "this may be achieved by recording the time to traverse the tree for each distribution ( i.e. @xmath7 in eq .",
    "( [ eq : jab ] ) ) . the time and position of each distribution",
    "are stored in an array on each processor to facilitate partitioning of the 3-d bounding box ( the root bounding box ) that encloses all distributions .",
    "equal time partition@xcite is performed on the root bounding box to achieve equal time or cost in all sub - boxes ( also called et sub - boxes ) .",
    "et partition creates et sub - boxes by recursively partitioning a box into 2 sub - boxes such that each sub - box carries approximately the same amount of time . at the end of the procedure",
    "we obtain @xmath17 et sub - boxes , where @xmath3 is an integer greater than zero . assuming that the number of processors is @xmath17",
    ", each processor will handle one of the @xmath17 et sub - boxes in the next scf cycle .",
    "the restriction of a power of two for the number of processors may be removed by using a general et partitioning scheme detailed in the appendix of ref . [ ] .",
    "we have used a robust bisection method@xcite to find the plane which approximately divides the workload into half .",
    "we emphasize again the main difference between our et scheme and other parallel @xmath0-body codes@xcite is that we use an exact load timing information rather than counting the number of interactions as in the orthogonal recursive bisection ( orb)@xcite and costzones methods@xcite .    for the periodic case ,",
    "the time associated with each distribution includes the time to handle the periodic far field@xcite contribution . in this way , et partition naturally uses combined timing information to load balance computation , extending the power of et partition to situations where different timing information may be grouped together .",
    "a working hypothesis of the et partition applied to qctc is that the sum of distribution times for each et sub - box is constant irrespective of the sectioning .",
    "however , for very fine grained parallelism , shifting a bisecting plane may induce a relatively large change in the total predicted distribution time in a sub - box .",
    "this is due to the fact that the total workload may not be equally divided among the sub - boxes because the distribution times are discrete ( a distribution is either _ totally _ or not in a sub - box ) .",
    "also , for very fine grained parallelism , the total work in a sub - box is more sensitive to a change in density that may also increase the load imbalance , an effect which we have experienced in parallelization of the exchange - correlation matrix@xcite .    in the first cycle",
    ", there is no previous timing on which to base the et .",
    "in such a case , we use a reasonable heuristic where each processor handles an approximately equal number of distributions ( the total number of distributions may not be exactly divisible by the number of processors ) .      in principle , an efficient parallelization of the density tree build should make use of the fact that each processor is handling only part of the total distributions in an et sub - box . depending on the collection of distributions on each processor ,",
    "a locally essential tree@xcite may be constructed which is just sufficient for the tree traversals of all the distributions on a processor , thus avoiding replication of the entire density tree and enabling efficient use of the memory space . however , without an extensive programming task , it may be difficult to predetermine which part of the entire density tree is needed for construction of the locally essential tree . as a first attempt , we have chosen a simple approach to parallelize the entire density tree build .",
    "for simplicity of programming we assume the number of processors to be @xmath18 , where @xmath11 is an integer greater than zero . observing that there are @xmath18 subtrees at the @xmath11th tier in the entire density tree ,",
    "our current implementation adopts a simple scheme where each processor builds one of the @xmath11th - tier subtrees in the total density tree .",
    "when all processors have built a @xmath11th - tier subtree , an all - to - all exchange is carried out where all processors get the rest of the @xmath11th - tier subtrees so that a final `` merging '' up of the subtrees@xcite can be performed to obtain the entire density tree .",
    "inefficiency of the current implementation of the parallel density tree build in the limit of a large number of processors is expected .",
    "the all - to - all exchange of data between processors is expensive and does not scale with the number of processors . also , after collecting the subtrees from all other processors , a processor has to `` merge '' more subtrees upward as the number of processors is increased .",
    "this will inevitably introduce more overhead as we use more processors .",
    "even if one can overcome the all - to - all exchange problem , one still faces a problem where it may be wasteful in the use of memory to store the entire density tree on each processor .",
    "however , while the present parallel density tree build may be replaced by more sophisticated schemes , where locally essential trees are built@xcite or hashed trees are used@xcite , the current implementation delivers very good speedups up to the 128-processor level .    as a side note",
    ", our first implementation of an et parallel qctc tried to avoid the problem mentioned above by partitioning the entire density into disjoint local densities .",
    "each processor then built a local tree based on the local density .",
    "however , since a distribution on one processor does not `` see '' other local density trees , every processor had to loop through all distributions and the resulting partial coulomb matrices had to be resummed using an all - to - all communication at the end of the calculation .",
    "this turned out to be practical only below the 64-processor level .",
    "the speedup did not increase with more processors because the total intrinsic cost ( i.e. the amount of useful work ) of qctc _ increases _ rather rapidly with the number of processors .",
    "the rapid increase of intrinsic cost has at its root a break down of the hierarchical multipole approximation , as physically close charges can no longer be grouped when they reside on different processors .",
    "asymptotically , as the number of processors approach the number of charges , one reverts to the expensive @xmath19 algorithm . for periodic systems , where one has to visit the density tree many more times ( looping through periodic images ) relative to the finite case , the speedup stagnates",
    "once we pass a certain number of processors . since this version of the parallel qctc does not scale with the number of processors , we do not consider it further in this work .",
    "we have implemented a parallel qctc algorithm in mondoscf  @xcite , a suite of programs for linear scaling electronic structure theory and _ ab initio _ molecular dynamics .",
    "mondoscf has been written in fortran 90/95 with the message - passing library mpi  @xcite .",
    "timings are performed using the mpi_wtime function .",
    "we have performed scaling tests on both finite and periodic systems . for the finite systems , we have chosen taxol ( c@xmath20h@xmath21no@xmath22 ) and 2 water clusters as test cases .",
    "for the periodic systems , we have chosen pentaerythritol tetranitrate ( petn)@xcite and the @xmath23-phase of octahydro-1,3,5,7-tetranitro-1,3,5,7-tetrazocine ( @xmath23-hmx)@xcite as representative test cases .",
    "these systems are chosen because they are highly inhomogeneous , three - dimensional ( and two are periodic ) systems posing a challenge to parallel qctc .",
    "all runs were performed on a cluster of 256 4 cpu hp / compaq alphaserver es45s with the quadrics qsnet high speed interconnect .    for the purpose of performing the scaling tests ,",
    "we start the calculation with the sto-3 g basis set and a low accuracy , and switch to the final basis set and accuracy using a mixed integral approach , and run for three scf cycles .",
    "the density matrix @xmath24 is saved to disk and scaling tests of parallel qctc are performed .",
    "this procedure may not be necessary .",
    "however , we are confident that the timings are representatives of a routine calculation .    the result of the taxol scaling test",
    "is shown in fig .  [",
    "fig : taxol ] .",
    "the calculations are performed with the 6 - 31 g and 6 - 31 g * * basis sets , and a good accuracy@xcite .",
    "the results of two different speedups are presented .",
    "the first speedup , called the et speedup , measures the efficiency of the et partition for the coulomb matrix element calculation by traversing the density tree ( see section  [ etpartition ] ) and is defined by @xmath25 where @xmath26 is the time to evaluate the matrix elements by traversing the density tree with @xmath3 processors .",
    "notice that the speedups are relative to a 2-processor calculation .",
    "the second speedup , called the qctc speedup , measures the overall efficiency of parallel qctc . from fig .",
    "[ fig : taxol ] it is observed that the et speedup is excellent up to 64 processors .",
    "efficiency at the 64-processor level ( where the number of heavy atoms per processor is less than 1 ) is at least 94% .",
    "the overall parallel qctc speedup is very good up to 32 processors but degrades slightly at the 64-processor level .",
    "the overall parallel qctc efficiencies at 64 processors are 77.6% and 83.0% for the 6 - 31 g and 6 - 31 g * * basis sets , respectively .",
    "the loss of efficiency is due to the fact that the time for parallel density tree build does not decrease at the same rate ( i.e. divided by the number of processors ) as the tree traversal part . in fig .",
    "[ fig : time_ratio ] we present the @xmath27 ratio as a function of the number of processors for calculations on taxol ( along with other systems for comparisons to be made later ) .",
    "we note that if the time for parallel tree build @xmath28 were to decrease at the same rate ( i.e. divided by the number of processors ) as the time for tree traversal @xmath29 as we increase the number of processors , then the @xmath27 ratio would remain nearly constant",
    ". however , fig .",
    "[ fig : time_ratio ] shows that the @xmath27 ratio increases steadily as the number of processors is increased in all cases , a fact that has been anticipated from the discussion in section  [ sec : paralleltb ] .",
    "since the slope for the 6 - 31 g * * case is smaller than that for the 6 - 31 g case , this explains the slight increase in the overall parallel qctc performance of the 6 - 31 g * * case over the 6 - 31 g case , as shown in fig .",
    "[ fig : taxol ] .",
    "we note that our results of a parallel qctc speedup of 7.80 ( with the 6 - 31 g and 6 - 31 g * * basis sets ) with 8 processors compares favorably with the speedup of about 6.0 of sosa _",
    "et al._@xcite , which is for an entire single - point energy calculation with rhf/3 - 21 g .",
    "similar scaling tests have been performed on a 110-molecule and 200-molecule water clusters with rblyp/6 - 31 g * * at a good accuracy level .",
    "the result of the scaling tests is shown in fig .",
    "[ 110and200waterongood ] .",
    "it is found that the et speedups are rather good for both cases .",
    "the overall parallel qctc speedups are 80.3 and 85.5 for the 128-processor calculations for 110-molecule and 200-molecule water clusters , respectively .",
    "the decrease in parallel qctc efficiency is again due to the high @xmath27 ratio at the 128-processor level .",
    "these ratios are 28.8% and 26.6% for the 110-molecule and 200-molecule water clusters , respectively ( see fig .  [",
    "fig : time_ratio ] ) .    to investigate the performance of parallel qctc at a higher accuracy level",
    ", we have performed the scaling tests on a 110-molecule water cluster but with tight accuracy@xcite .",
    "the results for both good and tight accuracies are presented in fig .",
    "[ fig:110waterongoodandtight ] for comparison .",
    "it is seen that the et speedup is better for the tight case than for the good case , which is anticipated since increasing the accuracy level increases the effective granularity , which leads to a better performance in et partition@xcite .",
    "overall parallel qctc increases its efficiency from good to tight , which is due mainly to a decrease in the @xmath27 ratio , as shown in fig .",
    "[ fig : nproc_tratio_110h2o_goodandtight ] .    finally , for the periodic systems fig .",
    "[ fig:111dhmx212petn ] shows that the overall parallel qctc with rpbe/6 - 31 g * * on good accuracy is excellent . at the 128-processor level ,",
    "the @xmath30 @xmath23-hmx ( 168 atoms per simulation cell ) delivers 104.0 fold speedup , while the @xmath31 petn ( 232 atoms per simulation cell ) delivers 100.0 fold speedup .",
    "these performances are better compared to the 110-molecule ( a speedup of 80.3 ) or 200-molecule ( a speedup of 85.5 ) water cluster calculations .",
    "this is due to the smaller @xmath27 ratio ( see fig .  [",
    "fig : time_ratio ] ) for the periodic cases compared to the finite cases , which mainly results from the increase in the time spent in the tree traversal part ( e.g. 87.7 and 64.0 secs for the @xmath31 petn and 200-molecule water cluster , respectively ) .",
    "we have proposed an efficient method of parallelizing calculation of the coulomb matrix .",
    "the concept of equal time ( et ) has proven fruitful for load balancing the most time consuming part of qctc , which is traversal of the density tree for the matrix element calculation .",
    "equal time exploits the temporal locality between scf iterations to overcome strong spatial irregularities .",
    "it is expected that et should retain this property between geometry steps in an optimization or molecular dynamics run .",
    "the efficiency of the et partition ranges from 91  98 % for all test cases presented in this work at the 128-processor level .",
    "the overall qctc speedup , however , ranges from 63  81 % overall efficiency at the 128-processor level with fine grained parallelism .",
    "the decrease in efficiency is mainly due to the parallel tree build process . while the current simple implementation of the parallel tree build should eventually be replaced by a more sophisticated version ,",
    "the current implementation has enabled us to run routine calculations to address a wide range of interesting problems@xcite .    this work has been carried out under the auspices of the u.s .",
    "department of energy under contract no .",
    "w-7405-eng-36 and the asci project .",
    "most work was performed on the computing resources at the advanced computing laboratory of los alamos national laboratory ."
  ],
  "abstract_text": [
    "<S> we present parallelization of a quantum - chemical tree - code [ j. chem . phys . * 106 * , 5526 ( 1997 ) ] for linear scaling computation of the coulomb matrix . </S>",
    "<S> equal time partition [ j. chem . </S>",
    "<S> phys . * 118 * , 9128 ( 2003 ) ] is used to load balance computation of the coulomb matrix . </S>",
    "<S> equal time partition is a measurement based algorithm for domain decomposition that exploits small variation of the density between self - consistent - field cycles to achieve load balance . </S>",
    "<S> efficiency of the equal time partition is illustrated by several tests involving both finite and periodic systems . </S>",
    "<S> it is found that equal time partition is able to deliver 91  </S>",
    "<S> 98 % efficiency with 128 processors in the most time consuming part of the coulomb matrix calculation . </S>",
    "<S> the current parallel quantum chemical tree code is able to deliver 63  </S>",
    "<S> 81% overall efficiency on 128 processors with fine grained parallelism ( less than two heavy atoms per processor ) . </S>",
    "<S> * keywords * : self - consistent - field theory , linear scaling methods , @xmath0-body problem , gaussian - orbital , hierarchical methods , load balance , parallel computation , equal time partition .    * </S>",
    "<S> pacs numbers * : 31.15.-p ; 02.70.-c ; 02.60.-x </S>"
  ]
}