{
  "article_text": [
    "gaussian markov random fields(gmrfs ) are useful models in spatial statistics due to the gaussian properties together with markovian structures .",
    "they can also be formulated as conditional auto - regressions ( cars ) models @xcite .",
    "gmrfs have applications in many areas , such as spatial statistics , time - series models , analysis of longitudinal survival data , image analysis and geostatistics .",
    "see  ( * ? ? ?",
    "* chapter @xmath0 ) for more information and literature on how the gmrfs can be applied in different areas .",
    "from an analytical point of view gmrfs have good properties and can be specified through mean values @xmath1 and covariance matrices @xmath2 . while from a computational point of view gmrfs can conveniently specified through precision matrices @xmath3 ( the inverse of the covariance matrices @xmath2 ) , which are usually sparse matrices .",
    "the numerical algorithms for sparse matrices can be exploited for calculations with the sparse precision matrices and hence fast statistical inference is possible  @xcite .",
    "the numerical algorithms for sparse matrices can be applied to achieve fast simulation of the fields and evaluation the densities ( mostly , log - densities ) of gmrfs and gmrfs with conditioning on subset of variables or linear constraints .",
    "see  ( * ? ? ?",
    "* chapter 2 ) for further details .",
    "these algorithms can also be used to calculate the marginal variances  @xcite , and they can be extended to non - gaussian cases  @xcite .",
    "precision matrices @xmath3 are commonly used to specify gmrfs .",
    "this approach is natural due to the sparsity patterns of the precision matrices in markovian models . in many situations",
    "the cholesky factors are required and are crucial for simulation and inferences with gmrfs , and the cholesky factors are normally obtained with cholesky factorization routines in standard libraries .",
    "see  ( * ? ? ?",
    "* chapter @xmath4 ) for different simulation algorithms for gmrfs using cholesky factors . in order to get an even sparser cholesky factor , with the purposes of saving computational resources",
    ", @xcite showed that the cholesky factor from an incomplete cholesky factorization can be much sparser than the cholesky factor from the regular cholesky factorization .",
    "however , they provided theoretical and empirical evidence showing that the representation of sparser cholesky factor was fragile when conditioning the gmrf on a subset of the variables or on observed data .",
    "it means that the sparsity patterns of the sparser cholesky factors are destroyed when some constraints or observed data are introduced and the computational cost increases .",
    "additionally , the sparser cholesky factor from the incomplete cholesky factorization is only valid for a specific precision matrix .",
    "their approach is illustrated in figure [ fig : cholesky_diag ] with routine @xmath0 .    in this paper",
    "a different approach is chosen to solve the problem presented by @xcite .",
    "the main idea is given in the figure [ fig : cholesky_diag ] with routine @xmath4 . in this approach",
    "one rectangular matrix @xmath5 is formulated ,    @xmath6    it consists of the cholesky factor @xmath7 from the precision matrix @xmath8 of a given gmrf and the cholesky factor @xmath9 of the matrix @xmath10 .",
    "the matrix @xmath10 can be the additional effect when the gmrf is conditioned on observed data or on a subset of the variables .",
    "both @xmath11 and @xmath12 are lower triangular matrices .",
    "an incomplete orthogonal factorization is then used to factorize the matrix @xmath5 in equation to find the sparse cholesky factor for specifying the gmrf .",
    "it is shown that by using this approach an upper triangular matrix @xmath13 which is sparser than the standard cholesky factor is obtained .",
    "furthermore , this approach is applicable when the gmrf is conditioned on a subset of the variables or on observed data .",
    "since the upper triangular matrix @xmath13 is sparser in structure than the common cholesky factor , it is better for applications .",
    "( 1 , 1)(10,10 ) ( 1,1)(5,2 ) ( 3,1.5)@xmath14 ( 1,3)(5,4 ) ( 3 , 3.5)@xmath15 ( 3,2.5)@xmath16 ( 3.35,4.5)@xmath17 ( 1,5)(5,6 ) ( 3,5.5)@xmath18 ( 3,4.5)@xmath16 ( 6 , 7.5)@xmath19 ( 6,7.8)@xmath20 ( 1,7)(5,8 ) ( 3,7.5)@xmath21 ( 3,6.5)@xmath16 ( 5.1,6.5 ) ( 1,9)(5,10 ) ( 3 , 9.5 ) @xmath8 ( 3,8.5)@xmath16 ( 4.4,8.5 ) ( 6.9,7)(10.1,8 ) ( 8.5,7.5)@xmath22 \\left[\\boldsymbol{l}_1 , ~\\boldsymbol{l}_2 \\right]^{t}$ ] ( 8.5,6.5)@xmath16 ( 9.1,6.5 ) ( 7,5)(10,6 ) ( 8.5,5.5)@xmath23    the rest of this paper is organized as follows . in section [ sec : cholesky_background ] some basic theory on gmrfs , sparsity patterns for precision matrices and cholesky factors of gmrfs are presented .",
    "some basic theories on the orthogonal factorization and the incomplete orthogonal factorization are also introduced in this section . in section [ sec : cholesky_algorithm ] the algorithm for obtaining the sparse cholesky factor from the incomplete orthogonal factorization is introduced .",
    "a small example is given in order to illustrate how the algorithm works when the gmrfs are conditioned on a subset of the variables or on observed data .",
    "results for different structures on the precision matrices are given in section [ sec : cholesky_results ] .",
    "conclusion and general discussion in section [ sec : cholesky_conclusion ] ends the paper .",
    "a random vector @xmath24 is called a gmrf if it is gaussian distributed and processes a markov property . the structure of a gmrf is usually presented by a labeled graph @xmath25 , where @xmath26 is the set of vertexes @xmath27 and @xmath28 is the set of edges . the graph @xmath29 satisfies the properties that no edge between node @xmath30 and node @xmath31 if and only if @xmath32 @xcite , where @xmath33 denotes @xmath34 .",
    "if the random vector @xmath35 has a mean @xmath1 and a precision matrix @xmath36 , the probability density of the vector @xmath35 is    @xmath37    with the property @xmath38 the notation @xmath36 means that @xmath8 is a symmetric positive definite matrix .",
    "@xmath39 denotes the parameters in the precision matrix .",
    "this implies that any vector with a gaussian distribution and a symmetric positive definite covariance matrix is a gaussian random field ( grf ) , and gmrfs are grfs with markov properties .",
    "the graph @xmath29 determines the nonzero pattern of @xmath8 . if @xmath29 is fully connected , then @xmath8 is a complete dense matrix .",
    "a useful property of gmrf is that we can know whether @xmath40 and @xmath41 are conditionally independently or not directly from the precision matrix @xmath8 and the graph @xmath29 .",
    "values of mean @xmath1 do not have any influence on the pairwise conditional independence properties of the gmrfs , and hence we set @xmath42 in the following sections unless otherwise specified .",
    "the diagonal elements in the precision matrix @xmath43 are the conditional precisions of @xmath44 given all the other nodes @xmath45 . the off - diagonal elements @xmath46 can provide information about the correlations between @xmath40 and @xmath41 given on the nodes @xmath47 .",
    "these are the main differences in the interpretation between the precision matrix @xmath8 and the covariance matrix @xmath48 .",
    "the covariance matrix @xmath48 contains the marginal variance of @xmath40 and the marginal correlation between @xmath40 and @xmath41",
    ". however , with the precision matrix the marginal properties are not directly available @xcite .",
    "since @xmath8 is symmetric positive definite , there is a unique cholesky factor @xmath11 where @xmath11 is a lower triangle matrix satisfying @xmath49 .",
    "if we want to sample from the gmrf @xmath50 , the cholesky factor @xmath11 is commonly used .",
    "one algorithm for sampling gmrfs is given in section [ sec : cholesky_sampling ] .",
    "more algorithms for sampling gmrfs with different specifications are also available",
    ". see ( * ? ? ?",
    "* chapter @xmath4 ) for a detailed discussion on these algorithms .",
    "@xcite showed how to check the sparsity pattern of the cholesky factor of a gmrf .",
    "define    @xmath51    which is the future of i except j. then @xmath52 and @xmath53 is called a separating subset of @xmath30 and @xmath31 . if @xmath54 denotes that @xmath30 and @xmath31 are neighbors , then @xmath53 can not be a separating subset for @xmath30 and @xmath31 whenever @xmath54 .",
    "further , the cholesky factor of the precision matrix of a gmrf is always equally dense or denser than the lower triangle part of @xmath8 .",
    "in many situations there are more nonzero elements in @xmath11 than in the lower triangular part of @xmath8 .",
    "denote @xmath55 and @xmath56 the numbers of nonzero elements in the cholesky factor @xmath11 and the lower triangular part of precision matrix @xmath8 , respectively .",
    "the difference @xmath57 is called the fill - in .",
    "the ideal case is @xmath58 or @xmath59 , but commonly @xmath60 or even @xmath61 .",
    "it is known that the fill - in @xmath62 not only depends on the graph , but also on the order of the nodes in the graph @xcite .",
    "thus a re - ordering is usually needed before doing a cholesky factorization .",
    "it is desirable to find an optimal or approximately optimal ordering of the graph in order to make the cholesky factor of @xmath8 sparser and to save computational resources , but this is not the focus in this paper .",
    "we refer to @xcite for more information on why it is desirable to do re - ordering of the graph of a gmrf .      with an @xmath63 matrix @xmath5 ,",
    "the orthogonal factorization of @xmath5 is    @xmath64    where @xmath65 is an orthogonal matrix and @xmath66 is an upper triangular matrix .",
    "we assume without loss of the generality that @xmath67 .",
    "there exist many algorithms for orthogonal factorization , such as the standard gram - schmidt algorithm or the modified gram ",
    "schmidt ( mgs ) algorithm and the householder orthogonal factorization .",
    "we refer to @xcite and @xcite for more algorithms .",
    "if @xmath5 has full column rank , then the first @xmath68 columns of @xmath69 forms an orthonormal basis of ran(@xmath5 ) , where ran(@xmath5 ) denotes the range of @xmath5    @xmath70 the orthogonal factorization is usually used to find an orthonormal basis for a matrix .",
    "the orthogonal factorization has many advantages and some of them are given in what follows .    1 .",
    "it is numerically stable and robust both with a householder orthogonal factorization and with a orthogonal factorization using givens rotations .",
    "if the matrix @xmath5 is non - singular , it always produces an orthogonal matrix @xmath69 and an upper triangular matrix @xmath13 which satisfy equation ; 2 .",
    "it is easy to solve the linear system of equations @xmath71 using the upper triangular matrix @xmath13 since @xmath69 is an orthogonal matrix ; 3 .",
    "the normal equation has the form @xmath72 and the normal equation matrix is @xmath73 , where @xmath74 denotes the transpose of @xmath5 .",
    "then the triangular matrix @xmath13 is the cholesky factor of the normal equations matrix .",
    "a givens rotation @xmath75 is an identity matrix @xmath76 except that    @xmath77    if @xmath78 and @xmath79 , then @xmath80 rotates @xmath35 clockwise in the @xmath81-plane with @xmath82 radians , which gives    @xmath83    if we want to rotate @xmath35 counterclockwise in the @xmath81-plane with @xmath82 radians , then we can set @xmath78 and @xmath84 .",
    "it is obvious from equation that if    @xmath85    then @xmath86 . so the givens rotations can set the elements in @xmath5 to zeros one at a time .",
    "this is useful when dealing with sparse matrices . at the same time ,",
    "@xmath87 and @xmath88 are the only two values which we need for this algorithm .",
    "givens rotations are suitable for structured least squares problems such as the problems at the heart of gmrfs .",
    "there are many algorithms for incomplete factorizations of matrices , such as the incomplete triangular factorization and the incomplete orthogonal factorization .",
    "these algorithms are commonly used in practical applications @xcite .",
    "the incomplete factorizations usually have the form @xmath89 where @xmath90 is the error matrix , and @xmath91 and @xmath92 are some well - structured matrices .",
    "the incomplete factorization algorithms are usually associated with dropping strategies . a dropping strategy for an incomplete factorization specifies rules for when",
    "elements of the factors should be dropped .",
    "we returns to a detailed discussion on the dropping strategies in section [ sec : cholesky_sparse_factor ] .",
    "one of the commonly used incomplete factorization algorithms is the incomplete triangular factorization , and it is also called incomplete lu ( ilu ) factorization since @xmath91 is a _",
    "lower _ triangular matrix and @xmath92 is an _",
    "upper _ triangular matrix .",
    "this algorithm is usually applied to the square matrices , and it uses gaussian elimination together with a predefined dropping strategy .",
    "many incomplete orthogonal factorizations can be used both for square matrices and for rectangular matrices , and these algorithms usually use the modified gram - schmidt procedure together with some dropping strategies in order to return a sparse and generally non - orthogonal matrix @xmath69 and a sparse upper triangular matrix @xmath13 .",
    "@xcite proved the existence and stability of the associated incomplete orthogonal factorization .",
    "incomplete orthogonal factorization using givens rotations was proposed by @xcite .",
    "the main idea of the incomplete orthogonal factorization is to use the givens rotations to zero - out the elements in the matrix one at a time .",
    "some predefined dropping strategies are needed in order to achieve the sparsity pattern for the upper triangular matrix @xmath13 .",
    "this algorithm computes a sparse matrix @xmath69 , which is always an orthogonal matrix , together with a sparse upper triangular matrix @xmath13 .",
    "since the matrix @xmath69 is the product of the givens rotations matrices , it is always an orthogonal matrix .",
    "the incomplete orthogonal factorization has the form    @xmath93    this method was originally described and implemented by @xcite .",
    "@xcite described this incomplete orthogonal factorization with the modified gram ",
    "schmidt process using some numerical dropping strategy .",
    "another version of the incomplete orthogonal factorization is given by @xcite with givens rotations .",
    "@xcite claimed that this incomplete algorithm inherited the good properties of the orthogonal factorization .    1 .",
    "@xmath13 is a sparse triangular matrix and @xmath69 is an orthogonal matrix .",
    "@xcite pointed out that the sparsity pattern of the upper - triangular part of @xmath5 is inherited by the incomplete upper triangular matrix @xmath13 .",
    "they also pointed out that the number of nonzero elements in the upper triangular matrix @xmath13 is less than the number of nonzero elements in the upper - triangular part of @xmath5 .",
    "the error matrix @xmath94 is `` small '' in some sense and the size of the errors can be controlled by the pre - defined threshold .",
    "the triangular matrix @xmath13 is non - singular whenever @xmath5 is not singular .",
    "we can always obtain this triangular matrix in the same way as the orthogonal factorization and @xmath13 will always be an incomplete cholesky factor for the normal equation matrix @xmath95 .",
    "another merit of the incomplete orthogonal factorization with givens rotations is that we do not need to form the corresponding normal matrices @xmath69 since only the @xmath96-pair is needed in order to find the upper triangular matrix @xmath13 .",
    "more information about the givens rotations and the @xmath96-pairs are given in section [ sec : cholesky_givens ]    @xcite implemented different versions of the algorithm proposed by @xcite .",
    "there are two main differences between these versions .",
    "the first one is the order in which elements in the matrix @xmath5 are zeroed out , and the second one is the rules for dropping strategies .",
    "we refer to @xcite and @xcite for more information about this algorithm and implementations .",
    "there are also more variations for incomplete orthogonal factorization using givens rotations , such as @xcite and @xcite .",
    "@xcite proposed some modified incomplete orthogonal factorization methods and these algorithms have special storage and sparsity - preserving techniques .",
    "@xcite showed a way to adopt a diagonal compensation strategy by reusing the dropped elements .",
    "these dropped elements are added to the main diagonal elements of the same rows in the incomplete upper - triangular matrix @xmath13 .",
    "@xcite proposed practical incomplete givens orthogonalization ( igo ) methods for solving large sparse systems of linear equations .",
    "they claimed that these incomplete igo methods took the storage requirements , the accuracy of the solutions and the coding of the pre - conditioners into consideration .    in this report , we have chosen the column - wise threshold incomplete givens orthogonal ( ctigo ) factorization algorithm for finding the sparse upper triangular matrix @xmath13 .",
    "this sparse upper triangular matrix @xmath13 has sparse structure and can be used for specifying the gmrfs .",
    "the matrix @xmath69 does not need to be stored in our setting since we only need the upper - triangular matrix @xmath13 .",
    "the matrix @xmath69 only needs computed whenever it is explicitly needed .",
    "in this section we begin by introducing the background of gmrfs conditioned on a subset of the variables or on observed data .",
    "a small example is used to illustrate how the ctigo algorithm works when applied to gmrfs .",
    "* i. gmrfs with soft constraint * let @xmath35 be a gmrf and assume that we have observed some linear transformation @xmath97 with additional gaussian distributed noise @xmath98 where @xmath99 is the dimension of the vector @xmath100 , @xmath5 is a @xmath101 matrix with rank @xmath99 and @xmath102 , and @xmath103 is the precision matrix of @xmath100 .",
    "this is called  soft constraint \" by @xcite and the log - density for the model is @xmath104 where @xmath1 and @xmath8 are the mean and the precision matrix of the grmf , respectively , and `` const '' is constant .",
    "if @xmath35 has mean @xmath105 then @xmath106 here we use the canonical form @xmath107 for @xmath108 .",
    "we refer to ( * ? ? ?",
    "* chapter 2.3.2 ) for more information about the canonical form for gmrf .",
    "we can notice that for specifying the gmrfs with  soft constraint \" , the routine @xmath20 as shown in figure [ fig : cholesky_diag ] can be applied since @xmath109 with @xmath110 .",
    "models with auxiliary variables * auxiliary variables are crucial in some models to retrieve gmrf full conditionals .",
    "we look at binary regression models with auxiliary variables .",
    "assume that we have bernoulli observational model for binary responses .",
    "the binary responses have latent parameters which is a gmrf @xmath35 , and the gmrf usually depends on some hyperparameters @xmath39 .",
    "we usually choose the logit or probit models in this case , where @xmath111 where @xmath112 denotes a bernoulli distribution with probability @xmath113 for @xmath0 and @xmath114 for @xmath115 .",
    "@xmath116 is a vector of covariates and we assume it is fixed .",
    "@xmath117 is a link function @xmath118 where @xmath119 denotes the cumulative distribution function ( cdf ) for standard gaussian distribution .",
    "we can use models with auxiliary variables @xmath120 to represent these models , @xmath121 where @xmath122 is the cdf of standard logistic distribution in the logit case and @xmath123 in the probit case .",
    "we refer to ( * ? ? ?",
    "* chapter @xmath124 ) for more information about the standard logistic distribution and its cdf .",
    "let @xmath125 be a gmrf of dimension @xmath68 with mean @xmath105 , and assume that we have @xmath126 and @xmath127 . with the probit link",
    "the posterior distribution is @xmath128 the conditional distribution of @xmath35 given the auxiliary variables can then be obtained @xmath129 and this can be written in the canonical form @xmath130 a general form for the conditional distribution of @xmath35 given the auxiliary variables , for this binomial model with a probit link function , is given as @xmath131 where @xmath132 is an @xmath63 matrix .",
    "similarly , the conditional distribution of @xmath35 given the auxiliary variables for the logistic regression model can be written as @xmath133 where @xmath134 , and @xmath135 is from the model specification .",
    "see more discussions on these models in ( * ? ? ?",
    "* chapter @xmath136 ) .",
    "in all the examples in this section , the models are suitable for use routine ( 2 ) in figure [ fig : cholesky_diag ] to find the sparse cholesky factors of the precision matrices of gmrfs .",
    "as mentioned in section  [ sec : cholesky_gmrfs ] , if a vector @xmath35 is a gmrf with precision matrix @xmath8 and mean vector @xmath1 , then the density of the vector is given by equation . in practical applications",
    "it is common to set @xmath105 @xcite , which gives the probability density function @xmath137 assume that the data are of dimension @xmath99 and defined as a @xmath99-dimensional random vector    @xmath138    and has the probability density function    @xmath139    where @xmath5 is a @xmath101 matrix used to select the data location .",
    "the precision matrix @xmath140 for the noise process is a positive definite matrix with dimension @xmath141 .",
    "notice that the density function @xmath142 is not dependent on the @xmath39 , and hence the probability density function @xmath142 can be written as @xmath143 .",
    "the probability density function of @xmath144 can be found from equations and through @xmath145 \\right ) .",
    "\\end{split}\\ ] ] similarly , the density function can be written in the canonical form as    @xmath146    where @xmath147 , and @xmath148 .",
    "now we can notice that the precision matrix for the gmrf conditional on data has the form @xmath149 with @xmath150 , where @xmath151 does not depend on @xmath39 .",
    "since @xmath3 has the same form as given in routine ( @xmath4 ) in figure [ fig : cholesky_diag ] , it is possible to use the proposed routine to find the sparse cholesky factor of the precision matrix of the gmrf conditioned on data .    even though it is not the focus of this paper",
    ", it might be useful to point out that using equations - , we can find the analytical formula for the posterior density function of @xmath152 through bayes formula .",
    "it is given by @xmath153 we refer to @xcite for detailed information about this log - posterior density function .",
    "the log - posterior density function @xmath154 is crucial when doing statistical inference in bayesian statistics .",
    "the sparse structure of @xmath10 depends both on the structures of @xmath5 and of @xmath140 . in most cases ,",
    "the @xmath140 is a diagonal matrix and the matrix @xmath5 has sparse structure .",
    "therefore @xmath10 should also have a sparse structure .",
    "when the observations are conditional independent , but have a non - gaussian distribution , then we can use a gmrf approximation to obtain a sparse structure of @xmath10 as presented in section [ sec : cholesky_gmrf_approximation ] .",
    "suppose there are @xmath68 conditionally independent observations @xmath155 from a non - gaussian distribution and that @xmath156 is an indirect observation of @xmath40 .",
    "@xmath35 is a gmrf with mean @xmath157 and precision matrix @xmath8 .",
    "the full conditional @xmath158 then has the form    @xmath159    apply a second - order taylor expansion of @xmath160 around @xmath161 .",
    "in other words , construct a suitable gmrf proposal density @xmath162    @xmath163    @xmath164 should set to zero when @xmath165 .",
    "@xmath166 and @xmath167 depend on @xmath161 .",
    "the canonical parametrization of @xmath162 has the form    @xmath168    in this case @xmath10 has a diagonal structure .",
    "an important feature of is that it inherits the _ markov _ property of the prior on @xmath35 , which is useful for sampling gmrf . when @xmath169 , the canonical parametrization of the @xmath170 is changed to @xmath171 and does not change the matrix @xmath172 .",
    "as it was pointed out in section [ sec : cholesky_gmrfs ] , to sample from the gmrfs , the cholesky factor @xmath173 is one of most important factors .",
    "in order to save computational resources , a sparse cholesky factor is preferable if the approximated precision matrix is `` close '' to the original precision matrix , where `` close '' means both in structure and the elements .",
    "it has been mentioned in section [ sec : cholesky_gmrfs ] that the sparsity pattern of the cholesky factor is determined by the graph @xmath29 , and it is unnecessary to calculate the zero elements in the cholesky factor . in this section , we are going to introduce the theoretical background for finding the cholesky factor from the orthogonal factorization when the gmrf is conditioned on observed data or a subset of the variables .",
    "let @xmath174 be the observed data and assume @xmath175 has the gaussian distribution , then the density of @xmath35 conditioned on @xmath174 has the form in .",
    "in the discussed situations in section [ sec : cholesky_conditioningsubsets ] - section [ sec : cholesky_gmrf_approximation ] , the precision matrix @xmath3 can be split into two parts , the precision matrix @xmath8 of the gmrf @xmath176 and the matrix @xmath10 which is the additional effect .",
    "the matrix @xmath10 is usually a diagonal matrix or another type of sparse matrix",
    ". if the data is not gaussian distributed , then we can apply the gmrfs approximation given in and it returns the precision matrix @xmath8 with a diagonal matrix @xmath10 added .",
    "this structure satisfies the routine ( 2 ) in figure [ fig : cholesky_diag ] .",
    "let @xmath177 and assume that the cholesky factors for @xmath8 , @xmath10 and @xmath3 are @xmath11 , @xmath12 and @xmath173 , respectively .",
    "the cholesky factors @xmath11 and @xmath12 are assumed to be known .",
    "we have the following results .    [ thm_cholesky ]",
    "let @xmath178 be a zero mean gmrf with precision matrix  @xmath8 .",
    "assume that the precision matrix has the form @xmath179 when conditioned on observed data or a subset of the variables .",
    "let the cholesky factors for @xmath8 and @xmath10 be @xmath11 and @xmath12 , respectively .",
    "form @xmath180 then @xmath181 is the precision matrix @xmath3 .",
    "@xmath182 .    from observation [ thm_cholesky ]",
    "the following corollaries are established .",
    "sketched proofs for these corollaries are given .",
    "we refer to @xcite for numerical examples with corollary [ cor_sample ] .",
    "[ cor_sample ] let @xmath183 be a zero mean gmrf with precision matrix @xmath179 , and let @xmath5 have the form given in observation [ thm_cholesky ] .",
    "let @xmath184 be a vector of independent and identically distributed ( i.i.d . )",
    "standard gaussian random variables .",
    "then the solution of the least squares problem    @xmath185    is a sample from the gmrf @xmath183 .",
    "@xmath186 from observation [ thm_cholesky ] is the starting point to prove this corollary .",
    "denote @xmath187 the moore - penrose pseudo - inverse of @xmath5 , and then the solution to the least squares problem is @xmath188 @xcite . from the definition of the pseudo - inverse , @xmath189 , where @xmath190 is a singular value decomposition of @xmath5 and @xmath191 is the matrix with the reciprocals of the non - zero singular values on the diagonal .",
    "we can verify that @xmath35 has the required distribution , and it is sufficient to check the first two moments since @xmath35 has a gaussian distribution , being linear in @xmath192 .",
    "it is clear that @xmath193 .",
    "furthermore , @xmath194 calculations yield @xmath195 and , hence , @xmath196 .    therefore , it is possible to sample from a gmrf by solving the sparse least squares problem given in with some conditions on gmrfs .",
    "[ cholesky_rectangle ] the upper triangular matrix @xmath13 from the orthogonal factorization of the rectangular matrix @xmath197 is the cholesky factor of the precision matrix @xmath177 .",
    "since the upper triangular matrix from the orthogonal factorization is the cholesky factor for the normal equations matrix , this is obvious from observation [ thm_cholesky ] .    by using the orthogonal factorization of the rectangular matrix @xmath5 ,",
    "it is possible to get samples from the gmrfs when they are conditioned on data or a subset of the variables by using corollary [ cor_sample ] or the cholesky factor from observation [ thm_cholesky ] together with the sampling algorithms discussed in ( * ? ? ?",
    "* chapter 2 ) .      in this section",
    "the dropping strategy for the incomplete orthogonal factorization is introduced in order to find the incomplete cholesky factor for matrix @xmath73 .",
    "together with some dropping strategy for the incomplete orthogonal factorization of the rectangular matrix @xmath5 , a sparse upper triangular matrix @xmath13 can be obtained . from corollary [ cholesky_rectangle ] and the discussion in section [ sec : cholesky_igo ] , we know that @xmath13 is an incomplete cholesky factor or sparse cholesky factor for the precision matrix @xmath3 .",
    "this sparse cholesky factor can then be used to specify the gmrf .",
    "the dropping strategies are important when doing the incomplete orthogonal factorization . generally speaking , there are two kinds of dropping strategies .    1 .",
    "drop fill - ins based on sparsity patterns . before doing the incomplete orthogonal factorization , the sparsity pattern of the upper triangular matrix is predefined and fixed .",
    "if the factorization based only on the sparsity pattern of the original matrix , we drop all the elements which are pre - defined to be zeros .",
    "the algorithm does not consider the actual numerical values of the elements during the factorizations .",
    "2 .   drop fill - ins by using a numerical threshold .",
    "this strategy only includes the elements in @xmath13 if they are bigger than a predefined threshold value .",
    "@xcite presented one way to select the value of the threshold parameter .",
    "his strategy drops the elements which are smaller than the diagonal elements of their rows and columns , multiplied by some predefined small value ( called dropping tolerance ) . in this report",
    ", a slightly different dropping strategy is chosen . during the incomplete orthogonal factorization using givens rotations , or the column - wise threshold incomplete givens orthogonal ( ctigo ) factorization @xcite",
    ", we drop the elements according to their magnitudes with some predefined dropping tolerance .",
    "the nonzero pattern of @xmath13 is determined dynamically .    both the fixed sparsity pattern strategy and the dynamic strategy are useful in applications .",
    "the fixed sparsity pattern strategy is the candidate when the computation resources are low .",
    "it is usually faster but sometimes returns unsatisfactory results .",
    "the dynamic strategy will in most cases return satisfactory results by choosing proper dropping tolerances but it is usually more expensive both in time and computations .",
    "there are different versions of orthogonal factorizations .",
    "we refer to @xcite , @xcite and @xcite for more information . based on the research of @xcite , @xcite and @xcite",
    ", we choose the incomplete orthogonal factorization using givens rotations to find the sparse cholesky factor .",
    "this algorithm is stable and robust and always returns a sparse matrix .",
    "this algorithm inherits the advantages of orthogonal factorization .",
    "@xcite commented that there is little attention given to incomplete orthogonal factorization with givens rotations , which is actually useful in many numerical problems .    in order to use givens rotations for incomplete orthogonal factorization ,",
    "the following nonzero patterns needs to be defined ,    @xmath198    where @xmath199 is the nonzero pattern of the matrix @xmath3 , and @xmath200 , @xmath201 are the nonzero patterns of the upper and lower triangular parts of the matrix @xmath3 , respectively .",
    "@xmath202 , @xmath203 , @xmath204 @xmath205 and @xmath206 are the nonzero patterns of the lower triangular matrix @xmath11 , the lower triangular matrix @xmath12 , the lower triangular matrix @xmath173 , the matrix @xmath5 and the matrix @xmath13 , respectively . these matrices are already formulated in previous sections .    in order to use the ctigo algorithm , the rectangular matrix @xmath5 in",
    "is formed .",
    "the sparsity pattern of matrix @xmath5 is already known beforehand . however , since the dynamic strategy is chosen , there will be some fill - in during givens rotations process , and the sparsity pattern of the sparse cholesky factor @xmath13 will depend on the dropping tolerance and usually @xmath207 . for more information about ctigo algorithm ,",
    "see @xcite for theoretical issues and @xcite for implementations .",
    "a small example is explored in this section to illustrate how to use the ctigo algorithm to find the sparse cholesky factor @xmath13 . for simplicity and without loss of generality , we assume that @xmath8 is the precision matrix for a zero mean gmrf @xmath208 , and that the data are normally distributed , i.e. , @xmath209 and hence @xmath210 .",
    "assume that these matrices are given as follows    @xmath211    and    @xmath212    let @xmath11 and @xmath12 denote the cholesky factor of the two matrices @xmath8 and @xmath10 , respectively , with the sparsity patterns given in figure [ fig : cholesky_small_l1 ] and figure [ fig : cholesky_small_l2 ] .",
    "the rectangular matrix @xmath5 can then be formed as given in with the sparsity pattern given in figure [ fig : cholesky_small_a ] .",
    "apply the ctigo algorithm to the rectangular matrix @xmath5 with a dropping tolerance of @xmath213 to find the sparse incomplete cholesky factor @xmath13 .",
    "the sparsity pattern of @xmath13 is given in figure [ fig : cholesky_small_r ] .",
    "the sparsity pattern of the cholesky factor @xmath173 from the standard cholesky factorization of the precision matrix @xmath3 is given in figure [ fig : cholesky_small_l ] .",
    "we notice that the precision matrix @xmath8 is quite similar to the tridiagonal matrix except the values at two of the corners .",
    "however , there is a lot of fill - in in the cholesky factor @xmath11 .",
    "this is a common structure for the precision matrix of a gmrf , for instance , a gmrf on a torus .",
    "the same comments can be given for @xmath3 and @xmath173 .",
    "note that the upper triangular matrix @xmath13 has less nonzero elements than @xmath11 , @xmath173 and @xmath5 , @xmath214 .",
    "the sparsity pattern of @xmath13 depends on the dropping tolerance and also the elements of the matrices @xmath8 and @xmath10 , but we are not going deeper here .    as discussed in section [ sec : cholesky_igo ] and section [ sec : cholesky_sparse_factor ] , the sparse upper triangular matrix @xmath13 is an incomplete cholesky factor for the precision matrix @xmath3 of the gmrf when it is conditioned on data .",
    "the error matrix @xmath90 between the true precision matrix @xmath177 and the approximated precision matrix @xmath215 is given by @xmath216    the sparsity patterns of the precision matrix @xmath3 and its approximation @xmath217 are shown in figure [ fig : cholesky_small_q ] and figure [ fig : cholesky_small_qq ] , respectively . in order to compare the difference between the approximated covariance matrices ( inverse of the approximated precision matrix ) @xmath218 and the true covariance matrix ( inverse of the true precision matrix ) @xmath219 , we calculate the error matrix @xmath220 ,    @xmath221    the images of @xmath2 , @xmath222 and @xmath220 are given in figure [ fig : imagesq_qq_err ] , and they show that the difference between @xmath2 and @xmath223 is quite small . by",
    "chosen different dropping tolerance , the error can be made smaller and become negligible .",
    "using the incomplete orthogonal factorization with givens rotations , it leads to a sparse upper triangular matrix @xmath13 , which is a sparse incomplete cholesky factor for the precision matrix @xmath3 and can be used to specify the gmrf .",
    "hence it has the potential possibility to reduce the computational cost .",
    "we first apply the ctigo algorithm to some commonly used structures of the precision matrices in section [ sec : cholesky_result_band_matrices ] . in section [ sec : cholesky_result_spde ] , we apply the ctigo algorithm to precision matrices which are generated from the stochastic partial differential equations ( spdes ) discussed in @xcite and @xcite .      it is known that if the precision matrix @xmath224 is a band matrix with bandwidth @xmath113 , then its cholesky factor @xmath173 ( lower triangular matrix ) has the same bandwidth @xmath113 .",
    "see @xcite ( theorem 4.3.1 ) for a direct proof and ( * ? ? ?",
    "* chapter 2.4.1 ) for more information on how to finding cholesky factor efficiently in this case with algorithm @xmath225 .",
    "@xcite pointed out that if the original precision matrix @xmath3 is a band matrix , then the incomplete cholesky factor @xmath226 from the incomplete cholesky factorization will also be a band matrix with the same bandwidth @xmath113 .    in this section",
    "we consider some commonly used structures for the precision matrices .",
    "the first two examples are band matrices with different bandwidths .",
    "let @xmath227 be gaussian auto - regressive processes of order @xmath0 or @xmath4 , and then the precision matrix for the process will be a band matrix with bandwidth @xmath228 or @xmath229 , respectively .",
    "the precision matrices for the first - order random walk ( rw@xmath0 ) and the second - order random walk ( rw@xmath4 ) models have bandwidths @xmath228 and @xmath229 . since these models are intrinsic gmrfs , the precision matrices are not of full rank .",
    "we fix this by slightly modifying the elements in the precision matrices for the rw@xmath0 and rw@xmath4 models but we still called them as the precision matrices for the rw@xmath0 and the rw@xmath4 models . for more information about intrinsic gmrfs and the rw@xmath0 and rw@xmath4 models , see , for example , ( * ? ? ?",
    "* chapter 3 ) .",
    "assume that the data are gaussian distributed .",
    "then from section [ sec : cholesky_gmrfs_conditioning_data ] the matrix @xmath10 is a diagonal matrix when @xmath230 . for simplicity and without lost of generality ,",
    "assume the data @xmath174 @xmath231 @xmath232 , then the matrix @xmath10 and its cholesky factor @xmath12 are identity matrices .",
    "since we know exactly what the sparsity patterns of the precision matrices @xmath8 and @xmath10 and the cholesky factors @xmath11 and @xmath12 are , the sparsity pattern of @xmath5 is known beforehand and can be taken advantage of in the implementation . by applying the ctigo algorithm to the matrix @xmath5 with dropping tolerance @xmath233",
    ", the sparse upper triangular matrix @xmath13 can be obtained .",
    "the sparsity patterns of the matrices @xmath11 , @xmath12 , @xmath173 , @xmath5 and @xmath13 are given in figure [ fig : cholesky_rw1 ] .",
    "the sparsity patterns of the true precision matrix @xmath3 and the approximated precision matrix @xmath217 are given in figure [ fig : cholesky_rw1_qandqq ] .",
    "the image of the true covariance matrices @xmath2 , the approximated covariance matrix @xmath222 and the error matrix @xmath220 for the rw@xmath0 model are shown in figure [ fig : cholesky_rw1imq12qqr ] .",
    "note that the order of the numerical values in the error matrix @xmath220 is @xmath234 , which is essentially zero in practice applications .",
    "similarly for the rw@xmath4 model we apply the ctigo algorithm to the matrix @xmath5 with the dropping tolerance @xmath233 .",
    "the results in this case are quite similar to the results for the rw@xmath0 model .",
    "we only show the images of the true covariance matrix @xmath2 , the approximated covariance matrix @xmath222 , and the error matrix @xmath235 .",
    "the results are given in figure [ fig : cholesky_rw2_imq12qqr ] .",
    "note that the order of the numerical values in the error matrix @xmath220 is @xmath234 as for the rw@xmath0 model .",
    "see section [ sec : cholesky_result_comparsion ] from more simulation results for the rw@xmath0 and rw@xmath4 models and discussions .",
    "we can notice that the sparseness of @xmath13 is the same as @xmath173 .",
    "hence in these two cases , we do not save computational resources .",
    "however , this approach is still have the potential to be used in applications since it is robust .",
    "the next example we have chosen is a block tridiagonal matrix of order @xmath236 resulting from discretizing poisson s equation with the @xmath237-point operator on an @xmath68-by-@xmath68 mesh .",
    "thus it is called poisson matrix in this paper .",
    "the sparsity pattern of this matrix is given in figure [ fig : cholesky_possion_q ] . with the poisson matrix and @xmath10 as before , we find the cholesky factors @xmath11 and @xmath12 and form the rectangular matrix @xmath5 .",
    "we apply the ctigo algorithm to the matrix @xmath5 with dropping tolerance @xmath233 to find the sparse upper triangular matrix @xmath13 .",
    "the sparsity patterns of the matrices @xmath11 , @xmath12 , @xmath173 , @xmath5 and @xmath13 are given in figure [ fig : cholesky_possion_l1 ] - figure [ fig : cholesky_possion_r ] , respectively .",
    "the sparsity patterns of the true precision matrix @xmath3 and the approximated precision matrix @xmath217 are given in figure [ fig : cholsky_possion_qandqq ] .",
    "we can notice that the upper triangular matrix @xmath13 is sparser than the cholesky factor @xmath173 from the original precision matrix @xmath3 .",
    "it can be shown that the sparseness depends on the dropping tolerance @xmath238 .",
    "the images of the true covariance matrices @xmath2 , the approximated precision matrix @xmath222 , and the error matrix @xmath235 in this case are shown in figure [ fig : cholesky_possion_imq12qqr ] .",
    "note that the order of the numerical values in the error matrix @xmath235 is @xmath239 .",
    "this is small for practical use .",
    "more results for this band matrix are given in section [ sec : cholesky_result_comparsion ] .",
    "the next example is a precision matrix with a nearly band matrix .",
    "assume that @xmath8 is a nearly banded matrix but with the values @xmath240 and @xmath241 .",
    "we call this matrix as toeplitz matrix in this paper",
    ". the sparsity pattern of this matrix is given in figure [ fig : cholesky_toeplitz_q ] . with the dropping tolerance @xmath233",
    ", we apply the ctigo algorithm to the rectangular matrix @xmath5 .",
    "the sparsity patterns of @xmath11 , @xmath12 , @xmath173 , @xmath5 and @xmath13 are given in figure [ fig : cholesky_toeplitz_l1 ] - figure [ fig : cholesky_toeplitz_r ] , respectively .",
    "we notice that the upper triangular matrix @xmath13 is sparser than the matrix @xmath173 .",
    "we can also notice that the sparseness of @xmath13 depends on the tolerance @xmath238 .",
    "the sparsity pattern of the approximated precision matrix @xmath217 is given in fig [ fig : cholesky_toeplitz_qq ] .",
    "the image of the true covariance matrices @xmath2 , the approximated covariance matrix @xmath222 , and the error matrix @xmath235 are shown in figure [ fig : cholesky_toeplitz_imq12qqr ] .",
    "note that the order of the numerical values in the error matrix is @xmath239 with the given tolerance .",
    "more simulation results for this matrix are found in section [ sec : cholesky_result_comparsion ] .      in this section",
    "we emphasize on some particular precision matrices , namely the precision matrices from the stochastic partial differential equations ( spdes ) approach discussed by @xcite and @xcite .",
    "as pointed out by @xcite there is an explicit link between grfs and gmrfs through spdes .",
    "the important relationship which was initially used by @xcite is that the solution @xmath242 to the following spde is a grf with matrn covariance function , @xmath243 where @xmath244 is the laplacian , @xmath245 is a differential operator and @xmath246 is the dimension of the field @xmath247 .",
    "@xcite extended this approach to construct anisotropic and inhomogeneous fields with the spde @xmath248 where @xmath249 and @xmath250 control the local range and anisotropy , and @xmath251 .",
    "one important difference between @xcite and @xcite is that @xcite have chosen the neumann boundary condition but @xcite has chosen the periodic boundary condition . with neumann boundary condition",
    "the precision matrix @xmath8 is a band matrix .",
    "however , the periodic boundary condition gives elements `` in the corners '' of the precision matrix .",
    "@xcite extended the approach to multivariate settings by using systems of spdes . for more information about the spde approach ,",
    "we refer to @xcite , @xcite and @xcite .",
    "first , choose the precision matrix for @xmath8 that results from the discretization of the spde with @xmath252 and @xmath253 .",
    "the sparsity pattern of @xmath8 is given in figure [ fig : cholesky_spde1_q ] .",
    "we still assume @xmath210 .",
    "the sparsity patterns of @xmath11 , @xmath12 , @xmath173 , @xmath5 and @xmath13 are given in figure [ fig : cholesky_spde1_l1 ] -[fig : cholesky_spde1_r ] , respectively .",
    "we notice that the upper triangular matrix @xmath13 is sparser than the matrix @xmath173 .",
    "the sparsity pattern of the approximated precision matrix @xmath217 is given in figure [ fig : cholesky_spde1_qq ] .",
    "the images of the true covariance matrix @xmath2 , the approximated covariance matrix @xmath223 and the error matrix @xmath220 are shown in figure [ fig : cholesky_result_spde1_comparation ] .",
    "we can notice that the elements of in the error matrix @xmath220 are reasonably small .",
    "the second precision matrix for @xmath8 is generated from the spde with @xmath254 and @xmath255 the sparsity pattern of the precision matrix @xmath8 is given in figure [ fig : cholesky_spde2_q ] .",
    "we use the same @xmath10 as previous examples .",
    "the sparsity patterns of @xmath11 , @xmath12 , @xmath173 , @xmath5 and @xmath13 are given in figure [ fig : cholesky_spde2_l1 ] - figure [ fig : cholesky_spde2_r ] , respectively .",
    "we can notice that the upper triangular matrix @xmath13 is sparser than the matrix @xmath173 .",
    "the sparsity pattern of the approximated precision matrix @xmath217 is given in figure [ fig : cholesky_spde2_qq ] .",
    "the images of the true covariance matrix @xmath2 , the approximated covariance matrix @xmath223 and the error matrix @xmath220 are illustrated in figure [ fig : cholesky_result_spde2_comparation ] .",
    "we could notice that the order of the numerical values in the error matrix @xmath220 are also reasonably small in this case .      in this",
    "section samples from a gmrf are obtained using the sparse upper triangular matrices @xmath13 and the cholesky factors @xmath173 for the precision matrices @xmath3 .",
    "let the precision matrix @xmath177 , where @xmath8 is from the spde or , and @xmath10 is a diagonal matrix .",
    "the sampling is done as follows .",
    "* compute the cholesky factor @xmath173 with a cholesky factorization or compute the sparse upper triangular matrix @xmath13 from the ctigo algorithm ; * sample @xmath256 ; * solve the equation @xmath257 or @xmath258 ; * @xmath35 is the sample of the gmrf with precision matrix @xmath3 or @xmath259 .",
    "if the mean @xmath1 of the field is not zero , then we just need a last step @xmath260 to correct the mean .",
    "with @xmath173 the field @xmath35 has the true covariance matrix @xmath3 because @xmath261 similarly , with @xmath13 the field @xmath35 has the approximated covariance matrix @xmath262 .",
    "many other sampling algorithms are provided by ( * ? ? ?",
    "* chapter 2 ) for different parametrization of the gmrf .",
    "we can not notice any large differences between the samples using the cholesky factor @xmath173 and the samples using the sparse matrix @xmath13 based on figure [ fig : cholesky_sampling1 ] and figure [ fig : cholesky_sampling2 ] .      in this section",
    "we choose different values of @xmath238 in order to know the effect of dropping tolerance .",
    "we use the same kinds of structures for the precision matrices as discussed in section [ sec : cholesky_result_band_matrices ] with dropping tolerances @xmath263 .",
    "the @xmath0-norm for the error matrix @xmath264 is used for the comparisons .",
    "the results are given in table [ rwcomparison ] and table [ othercomparison ] . from these tables",
    ", we can see that as the dropping tolerance @xmath238 becomes smaller and smaller , the error becomes smaller and smaller .",
    "notice that by choosing @xmath239 as the dropping tolerance , the error reaches a level acceptable in many applications .",
    "if the dropping tolerance is equal to @xmath115 , then the error is also equal to zero which means no element has been zeroed out during the givens rotations and it returns the common cholesky factor .",
    "r|r + tolerance & error + 0.01 & 2.55e-04 + 0.001 & 1.66e-06 + 0.0001 & 2.13e-08 + 0.00001 & 2.50e-10 + 1.00e-6 & 2.34e-12 + 0 & 4.00e-15 +    r|r + tolerance & error + 0.01 & 0.33 + 0.001 & 1.80e-05 + 0.0001 & 1.48e-07 + 0.00001 & 1.07e-08 + 1.00e-6 & 2.15e-09 + 0 & 1.73e-14",
    "+    r|r + tolerance & error + 0.01 & 0.11 + 0.001 & 8.51e-03 + 0.0001 & 6.33e-04 + 0.00001 & 5.91e-05 + 1.00e-6 & 3.44e-06 + 0 & 1.49e-14 +    r|r + tolerance & error + 0.01 & 9.15e-03 + 0.001 & 9.59e-04 + 0.0001 & 7.91e-05 + 0.00001 & 8.70e-06 + 1.00e-6 & 7.19e-07 + 0 & 5.66e-15 +",
    "in this paper we use the ctigo algorithm to find sparse cholesky factors for specifying gmrfs .",
    "some commonly used structures of the precision matrices and two precision matrices generated from spdes have been tested . by using the incomplete orthogonal factorization with givens rotations ,",
    "a sparse incomplete cholesky factor can be found and it is usually sparser than the cholesky factor from the standard cholesky factorization .",
    "the sparsity of the incomplete cholesky factor depends on the value of the tolerance . with a good choice for the dropping tolerance",
    ", the error between the true covariance matrix and the approximated covariance matrix becomes negligible .",
    "one advantage of this approach is that it is robust .",
    "it always produces a sparse incomplete cholesky factor .",
    "since the algorithm works both for square matrices and for rectangular matrices , this approach can be applied to gmrfs conditioned on observed data or a subset of the variable . on the negative side",
    ", it seems that our current implementation of the approach is slow when the dimension of the matrix becomes large .",
    "we believe that this is due to the nature of the incomplete orthogonal factorization with dynamic dropping strategy .",
    "the orthogonal factorization is usually slower than the cholesky factorization .",
    "further , givens rotations only zero out values to zeros one at a time .",
    "this leads to the slowness of the algorithm .",
    "when the computation resources are limited , we might need to use the fixed pattern dropping strategy .",
    "however , to implement a fast ctigo algorithm is out the scope of this paper and it is for further research ."
  ],
  "abstract_text": [
    "<S> in this paper an approach for finding a sparse incomplete cholesky factor through an incomplete orthogonal factorization with givens rotations is discussed and applied to gaussian markov random fields ( gmrfs ) . </S>",
    "<S> the incomplete cholesky factor obtained from the incomplete orthogonal factorization is usually sparser than the commonly used cholesky factor obtained through the standard cholesky factorization . on the computational side </S>",
    "<S> , this approach can provide a sparser cholesky factor , which gives a computationally more efficient representation of gmrfs . on the theoretical side , this approach is stable and robust and always returns a sparse cholesky factor . </S>",
    "<S> since this approach applies both to square matrices and to rectangle matrices , it works well not only on precision matrices for gmrfs but also when the gmrfs are conditioned on a subset of the variables or on observed data . </S>",
    "<S> some common structures for precision matrices are tested in order to illustrate the usefulness of the approach . </S>",
    "<S> one drawback to this approach is that the incomplete orthogonal factorization is usually slower than the standard cholesky factorization implemented in standard libraries and currently it can be slower to build the sparse cholesky factor . </S>",
    "<S> * keywords * : gaussian markov random field ; incomplete orthogonal factorization ; upper triangular matrix , givens rotation ; sparse matrix ; precision matrix </S>"
  ]
}