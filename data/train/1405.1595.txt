{
  "article_text": [
    "canonical correlation analysis ( cca ) @xcite is one of the most classical and important tools in multivariate statistics @xcite .",
    "it has been widely used in various fields to explore the relation between two sets of variables measured on the same sample .    on the population level ,",
    "given two random vectors @xmath0 and @xmath1 , cca first seeks two vectors @xmath2 and @xmath3 such that the correlation between the projected variables @xmath4 and @xmath5 is maximized .",
    "more specifically , @xmath6 is the solution to the following optimization problem : @xmath7 which is uniquely determined up to a simultaneous sign change when there is a positive eigengap .",
    "inductively , once @xmath8 is found , one can further obtain @xmath9 by solving the above optimization problem repeatedly subject to the extra constraint that @xmath10 throughout the paper , we call the @xmath11 s canonical correlation directions .",
    "it was shown by hotelling @xcite that the @xmath12 s are the successive singular vector pairs of @xmath13 where @xmath14 and @xmath15 .",
    "when one is only given a random sample @xmath16 of size @xmath17 , classical cca estimates the canonical correlation directions by performing singular value decomposition ( svd ) on the sample counterpart of ( [ eq : cca - svd ] ) first and then premultiply the singular vectors by the inverse of square roots of the sample covariance matrices . for fixed dimensions @xmath18 and @xmath19 ,",
    "the estimators are well behaved when the sample size is large  @xcite .",
    "however , in contemporary datasets , we typically face the situation where the ambient dimension in which we observe data is very high while the sample size is small .",
    "the dimensions @xmath18 and @xmath19 can be much larger than the sample size @xmath17",
    ". for example , in cancer genomic studies , @xmath20 and @xmath21 can be gene expression and dna methylation measurements , respectively , where the dimensions @xmath18 and @xmath19 can be as large as tens of thousands while the sample size @xmath17 is typically no larger than several hundreds @xcite . when applied to datasets of such nature",
    ", classical cca faces at least three key challenges .",
    "first , the canonical correlation directions obtained through classical cca procedures involve all the variables measured on each subject , and hence are difficult to interpret .",
    "second , due to the amount of noise that increases dramatically as the ambient dimension grows , it is typically impossible to consistently estimate even the leading canonical correlation directions without any additional structural assumption @xcite . third , successive canonical correlation directions should be orthogonal with respect to the population covariance matrices which are notoriously hard to estimate in high - dimensional settings .",
    "indeed , it is not possible to obtain a substantially better estimator than the sample covariance matrix @xcite which usually behaves poorly @xcite .",
    "so , the estimation of such nuisance parameters further complicates the problem of high - dimensional cca .    motivated by genomics , neuroimaging and other applications ,",
    "there have been growing interests in imposing sparsity assumptions on the leading canonical correlation directions .",
    "see , for example , @xcite for some recent methodological developments and applications . by seeking sparse canonical correlation directions ,",
    "the estimated @xmath8 vectors only involve a small number of variables , and hence are easier to interpret .    despite these recent methodological advances , theoretical understanding about the sparse cca problem is lacking .",
    "it is unclear whether the sparse cca algorithms proposed in the literature have consistency or certain rates of convergence if the population canonical correlation directions are indeed sparse . to the best of our limited knowledge ,",
    "the only theoretical work available in the literature is @xcite . in this paper ,",
    "the authors gave a characterization for the sparse cca problem and considered an idealistic single canonical pair model where @xmath22 , the covariance between @xmath20 and @xmath21 , was assumed to have a rank one structure .",
    "they reparametrized @xmath22 as follows : @xmath23 where @xmath24 and @xmath25 .",
    "it can be shown that @xmath26 is the solution to ( [ eq : cca - def ] ) , so that they are the leading canonical correlation directions .",
    "it is worth noting that without knowledge of @xmath27 and @xmath28 , one is not able to obtain ( resp . ,",
    "estimate ) @xmath26 by simply applying singular value decomposition to @xmath22 ( resp .",
    ", sample covariance @xmath29 ) . under this model ,",
    "chen et al .",
    "@xcite studied the minimax lower bound for estimating the individual vectors @xmath30 and @xmath31 , and proposed an iterative thresholding approach for estimating @xmath30 and @xmath31 , partially motivated by @xcite .",
    "however , their results depend on how well the nuisance parameters @xmath27 and @xmath28 can be estimated , which to our surprise , turns out to be unnecessary as shown in this paper .",
    "the main objective of the current paper is to understand the fundamental limits of the sparse cca problem from a decision - theoretic point of view .",
    "such an investigation is not only interesting in its own right , but will also inform the development and evaluation of practical methodologies in the future .",
    "the model considered in this work is very general .",
    "as shown in @xcite , @xmath22 can be reparametrized as follows : @xmath32 where @xmath33 , @xmath34 and @xmath35 .",
    "then the successive columns of @xmath36 and @xmath37 are the leading canonical correlation directions .",
    "therefore , ( [ eq : cca ] ) is the most general model for covariance structure , and sparse cca actually means the leading columns of @xmath36 and @xmath37 are sparse .",
    "we can split @xmath38 as @xmath39 where @xmath40 , @xmath41 , @xmath42 , @xmath43 and @xmath44 for @xmath45 . in what follows ,",
    "we call @xmath46 the _ leading _ and @xmath47 the _ residual _ canonical correlation directions . since our primary interest lies in @xmath48 and @xmath49 , both the covariance matrices @xmath27 and @xmath28 and the residual canonical correlation directions @xmath50 and @xmath51 are nuisance parameters in our problem .",
    "this model is more general than ( [ eq : scp - model ] ) considered in @xcite .",
    "it captures the situation in real practice where one is interested in recovering the first few sparse canonical correlation directions while there might be additional directions in the population structure .    to measure the performance of a procedure",
    ", we propose to estimate the matrix @xmath52 under the following loss function : @xmath53 we choose this loss function for several reasons .",
    "first , even when the @xmath54 s are all distinct , @xmath48 and @xmath49 are only determined up to a simultaneous sign change of their columns .",
    "in contrast , the matrix @xmath52 is uniquely defined as long as @xmath55 .",
    "second , ( [ eq : loss ] ) is stronger than the squared projection error loss . for any matrix @xmath56 ,",
    "let @xmath57 stand for the projection matrix onto its column space .",
    "if the spectra of @xmath27 and @xmath28 are both bounded away from zero and infinity , then , in view of wedin s sin - theta theorem @xcite , any upper bound on the loss function ( [ eq : loss ] ) leads to an upper bound on the loss functions @xmath58 and @xmath59 for estimating the column subspaces of @xmath48 and @xmath49 , which have been used in the related problem of sparse principal component analysis @xcite .",
    "third , this loss function comes up naturally as the key component in the kullback ",
    "leibler divergence calculation for a special class of normal distributions where @xmath60 , @xmath61 and @xmath62 in ( [ eq : cca ] ) .",
    "we use weak-@xmath63 balls to quantify sparsity .",
    "let @xmath64 denote the @xmath65 norm of the @xmath66th row of @xmath48 , and let @xmath67 be the ordered row norms .",
    "one way to characterize the sparsity in @xmath48 ( and @xmath49 ) is to look at its weak-@xmath63 radius for some @xmath68 , @xmath69 } j \\bigl\\|{(u_1)_{(j ) * } } \\bigr\\|^q\\ ] ] under the tradition that @xmath70 .",
    "for instance , in the case of exact sparsity , that is , @xmath71 , @xmath72 counts the number of nonzero rows in @xmath48 .",
    "when @xmath73 , ( [ eq : weak - lq ] )  quantifies the decay of the ordered row norms of @xmath48 , which is a form of approximate sparsity .",
    "then we define the parameter space @xmath74 , as the collection of all covariance matrices @xmath75 \\ ] ] with the cca structure ( [ eq : cca ] ) and ( [ eq : splitcca ] ) , which satisfies :    1 .   @xmath76 and @xmath42 satisfying @xmath77 and @xmath78 ; 2 .   @xmath79 for @xmath80 ; 3 .   @xmath81 .    throughout the paper , we assume @xmath82 for some absolute constant @xmath83 . the key parameters @xmath84 and @xmath85 are allowed to depend on the sample size @xmath17 , while @xmath86 are treated as absolute constants .",
    "compared with the single canonical pair model ( [ eq : scp - model ] ) in @xcite , where @xmath87 , in this paper , the rank of @xmath22 can be as high as @xmath18 or @xmath19 and @xmath88 is allowed to grow .",
    "in addition , we do not need any structural assumption on @xmath27 and @xmath28 except for condition 2 on the largest and smallest eigenvalues , which implies that @xmath27 and @xmath28 are invertible .",
    "suppose we observe i.i.d .",
    "pairs @xmath89 . for two sequences @xmath90 and @xmath91 of positive numbers , we write @xmath92 if for some absolute constant @xmath93 , @xmath94 for all @xmath17 . by the minimax",
    "lower and upper bound results in section  [ sec : result ] , under mild conditions , we obtain the following tight nonasymptotic minimax rates for estimating the leading canonical directions when @xmath71 : @xmath95 \\\\[-8pt ] \\nonumber & & \\qquad\\asymp \\frac{1}{n\\lambda^2}\\biggl ( r(s_u+s_v ) + s_u \\log\\frac{{{e}}p}{s_u } + s_v \\log\\frac{{{e}}m}{s_v } \\biggr ) . \\ ] ] in section  [ sec : result ] , we give a precise statement of this result and tight minimax rates for the case of approximate sparsity , that is , @xmath96 .",
    "the result ( [ eq : rate-0 ] ) provides a precise characterization of the statistical fundamental limit of the sparse cca problem .",
    "it is worth noting that the conditions required for ( [ eq : rate-0 ] ) do not involve any additional assumptions on the nuisance parameters @xmath97 and @xmath51 .",
    "therefore , we are able to establish the remarkable fact that the fundamental limit of the sparse cca problem is _ not _ affected by those nuisance parameters",
    ". this optimality result can serve as an important guideline to evaluate procedures proposed in the literature .    to obtain minimax upper bounds",
    ", we propose an estimator by optimizing canonical correlation under sparsity constraints .",
    "a key element in analyzing the risk behavior of the estimator is a generalized sin - theta theorem .",
    "see theorem  [ thm : sintheta ] in section  [ sec : key ] .",
    "the theorem is of interest in its own right and can be useful in other problems where matrix perturbation analysis is needed .",
    "it is worth noting that the proposed procedure does _ not _ require sample splitting , which was needed in @xcite .",
    "we bypass sample splitting by establishing a new empirical process bound for the supreme of gaussian quadratic forms with rank constraint .",
    "see lemma  [ lem : ep ] in section  [ sec : key ] .",
    "the estimator is shown to be minimax rate optimal by establishing matching minimax lower bounds based on a local metric entropy approach @xcite .",
    "the current paper is related to the problem of sparse principal component analysis ( pca ) , which has received a lot of recent attention in the literature .",
    "most literature on sparse pca considers the spiked covariance model @xcite where one observes an @xmath98 data matrix , each row of which is independently sampled from a normal distribution @xmath99 with @xmath100 here , @xmath101 has orthonormal column vectors which are assumed to be sparse and @xmath102 with @xmath103 .",
    "since the first @xmath88 eigenvalues of @xmath104 are @xmath105 and the rest are all @xmath106 , the @xmath54 s are referred as `` spikes , '' and hence the name of the model .",
    "johnstone and lu @xcite proposed a diagonal thresholding estimator of the sparse principal eigenvector which is provably consistent for a range of sparsity regimes . for fixed @xmath88 , birnbaum et al .",
    "@xcite derived minimax rate optimal estimators for individual sparse principal eigenvectors , and ma @xcite proposed to directly estimate sparse principal subspaces , that is , the span of @xmath37 , and constructed an iterative thresholding algorithm for this purpose which is shown to achieve near optimal rate of convergence adaptively .",
    "cai et al .",
    "@xcite studied minimax rates and adaptive estimation for sparse principal subspaces with little constraint on @xmath88 .",
    "see also @xcite for the case of a more general model .",
    "in addition , variable selection , rank detection , computational complexity and posterior contraction rates of sparse pca have been studied .",
    "see , for instance , @xcite and the references therein .    compared with sparse pca , the sparse cca problem studied in the current paper is different and arguably more challenging in three important ways .    * in sparse pca , the sparse vectors of interest , that is , the columns of @xmath37 in ( [ eq : spiked - cov ] ) are normalized with respect to the identity matrix . in contrast , in sparse cca , the sparse vectors of interest , that is , the columns of @xmath36 and @xmath37 are normalized with respect to @xmath27 and @xmath28 , respectively , which are not only unknown but also hard to estimate in high - dimensional settings .",
    "the necessity of normalization with respect to nuisance parameters adds on to the difficulty of the sparse cca problem . * in sparse pca , especially in the spiked covariance model , there is a clean separation between `` signal '' and `` noise '' : the signal is in the spiked part and the rest are noise .",
    "however , in the parameter space considered in this paper , we allow the presence of residual canonical correlations @xmath107 , which is motivated by the situation statisticians face in practice .",
    "it is highly nontrivial to show that the presence of the residual canonical correlations does not influence the minimax estimation rates . *",
    "the covariance structures in sparse pca and sparse cca have both sparsity and low - rank structures .",
    "however , there is a subtle difference between the two . in sparse pca , the sparsity and orthogonality of @xmath37 in ( [ eq : spiked - cov ] )",
    "are coherent .",
    "this means that the columns of @xmath37 are sparse and orthogonal to each other simultaneously .",
    "such convenience is absent in the sparse cca problem .",
    "it is implied from ( [ eq : cca ] ) that @xmath108 and @xmath109 have orthogonal columns , while it is the columns of @xmath48 and @xmath49 that are sparse .",
    "the orthogonal columns and the sparse columns are different .",
    "the consequence is that in order to estimate the sparse matrices @xmath48 and @xmath49 , we must appeal to the orthogonality in the nonsparse matrices @xmath108 and @xmath109 , even when the matrices @xmath27 and @xmath28 are unknown .",
    "if we naively treat sparse cca as sparse pca , the procedure can be inconsistent ( see the simulation results in @xcite ) .",
    "the rest of the paper is organized as follows .",
    "section  [ sec : result ] presents the main results of the paper , including upper bounds in section  [ sec : upper ] and lower bounds in section  [ sec : lower ] .",
    "section  [ sec : discussion ] discusses some related issues .",
    "the proofs of the minimax upper bounds are gathered in section  [ sec : proof ] , with some auxiliary results and technical lemmas proved in section  [ sec : aux - proof ] .",
    "the proof of the lower bounds and some further technical lemmas are given in the supplementary material  @xcite .",
    "for any matrix @xmath110 , the @xmath111th row of @xmath56 is denoted by @xmath112 and the @xmath66th column by @xmath113 . for a positive integer @xmath18 , @xmath114",
    "$ ] denotes the index set @xmath115 . for any set @xmath116 , @xmath117 denotes its cardinality and @xmath118 its complement . for two subsets @xmath116 and @xmath119 of indices",
    ", we write @xmath120 for the @xmath121 submatrices formed by @xmath122 with @xmath123 . when @xmath116 or @xmath119 is the whole set , we abbreviate it with an @xmath124 , and so if @xmath125 , then @xmath126}$ ] and @xmath127 j}$ ] . for any square matrix @xmath110 , denote its trace by @xmath128",
    ". moreover , let @xmath129 denote the set of all @xmath130 orthonormal matrices and @xmath131 .",
    "for any matrix @xmath132 , @xmath133 stands for its @xmath111th largest singular value .",
    "the frobenius norm and the operator norm of @xmath56 are defined as @xmath134 and @xmath135 , respectively .",
    "the support of @xmath56 is defined as @xmath136 : \\|a_{i*}\\|>0\\}$ ] .",
    "the trace inner product of two matrices @xmath137 is defined as @xmath138 . for any number @xmath139",
    ", we use @xmath140 to denote the smallest integer that is no smaller than @xmath139 .",
    "for any two numbers @xmath139 and @xmath141 , let @xmath142 and @xmath143 .",
    "for any event @xmath144 , we use @xmath145 to denote its indicator function .",
    "we use @xmath146 to denote the probability distribution of @xmath147 and @xmath148 for the associated expectation .",
    "in this section , we state the main results of the paper . in section",
    "[ sec : upper ] , we introduce a method to estimate the leading canonical correlation directions .",
    "minimax upper bounds are obtained . section  [ sec : lower ] gives minimax lower bounds which match the upper bounds up to a constant factor .",
    "we abbreviate the parameter space @xmath149 as @xmath150 .",
    "the main idea of the estimator proposed in this paper is to maximize the canonical correlations under sparsity constraints .",
    "note that the svd approach of the classical cca @xcite can be written in the following optimization form : @xmath151 we generalize ( [ eq : optim ] ) to the high - dimensional setting by adding sparsity constraints .",
    "since the leading canonical correlation directions @xmath152 are weak @xmath63 sparse , we introduce effective sparsity for @xmath68 , which plays a key role in defining the procedure .",
    "define @xmath153 the effective sparsity of @xmath48 and @xmath49 are defined as @xmath154 for @xmath155 , it can be shown that @xmath156 for which the signal is not strong enough to be recovered from the data .",
    "it holds similarly for @xmath49 .    for @xmath17 i.i.d .",
    "observations @xmath157 , @xmath158 $ ] , we compute the sample covariance matrix @xmath159 .\\ ] ] the estimator @xmath160 for @xmath152 , the leading @xmath88 canonical correlation directions , is defined as a solution to the following optimization problem : @xmath161 \\\\[-8pt ] \\nonumber & & \\qquad\\mbox{s.t.}\\quad   a'{\\widehat}{\\sigma}_xa = b'{\\widehat } { \\sigma}_yb = i_r\\mbox { and } \\|{a } \\|_{0,w}= k_q^u , \\|{b } \\|_{0,w}= k_q^v.\\end{aligned}\\ ] ] when @xmath162 , we have @xmath163 and @xmath164 .",
    "then the program ( [ eq : pickset ] ) is just a slight generalization of the classical approach of @xcite with additional @xmath165 constraints @xmath166 and @xmath167 . by the definition of the parameter space , it is also natural to impose the @xmath63 constraints @xmath168 and @xmath169 .",
    "such constraints were used by @xcite to solve the sparse pca problem . however , their upper bounds require more assumptions due to the difficulty in analyzing @xmath63 constraints .",
    "we use @xmath165 constraints on the effective sparsity and obtain the optimal upper bound under minimal assumptions .",
    "set @xmath170 which is the minimax rate to be shown later .",
    "[ thm : upperbound1 ] we assume that @xmath171 for some sufficiently small constant @xmath172 . for any constant @xmath173 , there exists a constant @xmath174 only depending on @xmath175 and @xmath176 , such that for any @xmath177 , @xmath178 with @xmath146-probability at least @xmath179 .",
    "it will be shown in section  [ sec : lower ] that assumption ( [ eq : ass1 ] ) is necessary for consistent estimation .",
    "assumption ( [ eq : ass2 ] ) implies @xmath180 for @xmath172 , such that the eigengap is lower bounded as @xmath181 .",
    "the upper bound @xmath182 has two parts .",
    "the first part @xmath183 is caused by low rank structure , and the second part @xmath184 is caused by sparsity . if @xmath185 , the second part dominates , while the first part dominates if @xmath186 .",
    "the upper bound does not require any structural assumption on the marginal covariance matrices @xmath27 and @xmath28 other than bounds on the largest and the smallest eigenvalues .",
    "although in the high - dimensional setting , the sample covariance @xmath187 and @xmath188 are not good estimators of the matrices @xmath189 , the normalization constraints @xmath190 , together with the sparsity of @xmath191 , only involve submatrices of @xmath187 and @xmath188 . under the assumption ( [ eq : ass1 ] ) , it can be shown that a @xmath192 submatrix of @xmath187 converges to the corresponding submatrix of @xmath27 with the rate @xmath193 under operator norm uniformly over all @xmath192 submatrices .",
    "similar results hold for @xmath188 and @xmath28 .",
    "see lemma  [ lem : covdeviation45 ] in section  [ sec : bias - pf ] .",
    "these rates are dominated by the minimax rate @xmath194 in ( [ eq : defepsilon ] ) .",
    "one of the major difficulties of sparse cca is the presence of the unknown @xmath27 and @xmath28 .",
    "suppose @xmath27 and @xmath28 are known , one may work with the transformed data @xmath195 .",
    "the cross - covariance of the transformed data is @xmath196 , which is a sparse matrix . when @xmath87 , algorithms such as @xcite can obtain the sparse singular vectors from @xmath197 , which estimate @xmath48 and @xmath49 with optimal rate . when @xmath27 and @xmath28 are unknown , structural assumptions are required on the covariance matrices in order that @xmath198 and @xmath199 can be well estimated",
    ". then one can use the estimated @xmath198 and @xmath199 to transform the data and apply the same sparse singular vector estimator ( see @xcite ) . however , unless @xmath200 and @xmath201 , this method can not be extended to the case where @xmath202 , since the orthogonality of @xmath36 and @xmath37 is with respect to general covariance matrices @xmath27 and @xmath28 , respectively . in the case where @xmath200 and @xmath201 , the problem is similar to sparse pca , and the proof of theorem  [ thm : upperbound1 ] can be greatly simplified .    to obtain the convergence rate in expectation ,",
    "we propose a modified estimator .",
    "the modification is inspired by the fact that @xmath203 are bounded in frobenius norm , because @xmath204 define @xmath205 to be the truncated version of @xmath206 as @xmath207 the modification can be viewed as an improvement , because whenever@xmath208 , we have @xmath209 then it is better to estimate @xmath203 by @xmath210 .",
    "[ thm : upperbound2 ] suppose ( [ eq : ass1 ] ) and ( eq : ass2 ) hold .",
    "in addition , assume that @xmath211 for some @xmath212 , then there exists @xmath213 only depending on @xmath175 and @xmath214 , such that @xmath215    the assumptions ( [ eq : theoremexpassup1 ] ) and ( [ eq : theoremexpassup2 ] ) imply the tail probability in theorem  [ thm : upperbound1 ] is sufficiently small .",
    "once there exists a small constant @xmath216 , such that @xmath217 hold , then ( [ eq : theoremexpassup1 ] ) and ( [ eq : theoremexpassup2 ] ) also hold with some @xmath218 .",
    "notice that @xmath219 is commonly assumed in high - dimensional statistics to have convergence results in expectation .",
    "the assumption here is weaker than that .",
    "theorems [ thm : upperbound1 ] and [ thm : upperbound2 ] show that the procedure proposed in  ( [ eq : pickset ] ) attains the rate @xmath182 . in this section , we show that this rate is optimal among all estimators .",
    "more specifically , we show that the following minimax lower bounds hold for @xmath68 .",
    "[ thm : lower - bd - q ] assume that @xmath220 , and that @xmath221 for some sufficiently large constant @xmath222 .",
    "then there exists a constant @xmath223 depending only on @xmath224 and an absolute constant @xmath225 such that the minimax risk for estimating @xmath226 satisfies @xmath227    the proof of theorem  [ thm : lower - bd - q ] is given in the supplementary material @xcite .",
    "assumption ( [ eq : assumptionlower ] ) is necessary for consistent estimation .",
    "we include below discussions on two related issues .      in this paper ,",
    "we have derived tight minimax estimation rates for the leading sparse canonical correlation directions where the sparsity is depicted by the rapid decay of the ordered row norms in @xmath48 and @xmath49 ( as characterized by the weak-@xmath63 notion ) .",
    "another interesting case of sparsity is when the individual column vectors of @xmath48 and @xmath49 are sparse .",
    "for instance , when @xmath228,\\ ] ] where the @xmath229 is defined as in ( [ eq : weak - lq ] ) by treating any @xmath18-vector as a @xmath230 matrix .",
    "let @xmath231 be defined as in section  [ sec : intro ] following ( [ eq : weak - lq ] ) but with the sparsity notion changed to that in ( [ eq : col - sparse ] ) .",
    "similar to ( [ eq : x - q - u])([eq : effectivesparsity ] ) , let @xmath232 and @xmath233 and @xmath234 be analogously defined .",
    "then we have :    [ thm : col - sparse ] assume that @xmath235 , @xmath236 , @xmath237 and @xmath238 for some sufficiently large constant @xmath222 .",
    "then there is a constant @xmath223 depending only on @xmath224 and an absolute constant @xmath239 such that @xmath240 if in addition @xmath241 , @xmath242 for some small @xmath243 , @xmath244 for some @xmath245 and the conditions of theorem  [ thm : upperbound2 ] are satisfied with @xmath246 and @xmath247 , then a matching upper bound is achieved by the estimator in theorem  [ thm : upperbound2 ] with @xmath246 and @xmath247 .",
    "the proof of theorem  [ thm : col - sparse ] is given in the supplementary material @xcite .",
    "the lower bound ( [ eq : col - lowbd ] ) for individual sparsity is larger than the minimax rate ( [ eq : defepsilon ] ) for joint sparsity when @xmath248 and @xmath249 .",
    "the main purpose of proposing the estimator in ( [ eq : pickset ] ) is to determine the minimax estimation rates in sparse cca problem under weak assumptions .",
    "admittedly , it requires the knowledge of parameter space and is computationally intensive .",
    "designing adaptive and computationally efficient procedures to achieve statistically optimal performance is an interesting and important research direction . built upon the insights developed in the current paper , gao et al .",
    "@xcite have proposed an adaptive and efficient procedure for sparse cca .",
    "the procedure first obtains a crude estimator via a convex relaxation of the problem ( [ eq : pickset ] ) here which is then refined by a group sparse linear regression .",
    "the resulting estimator achieves optimal rates of convergence in estimating the leading sparse canonical directions under a prediction loss without imposing any structural assumption on @xmath27 and @xmath28 , when the residual directions are absent .",
    "notably , the procedure in @xcite requires a larger sample size than in the present paper , which has been shown to be essentially necessary for any computational efficient procedure under the gaussian cca model considered here under the assumption of planted clique hardness .",
    "the argument has also led to a computational lower bounds for the sparse pca problem under the gaussian spiked covariance model , bridging the gap between the sparse pca literature and the computational lower bounds in @xcite and @xcite .",
    "it is of great interest to further investigate if there is some adaptive and efficient estimator that attains the statistical optimality established in the current paper under full generality .",
    "this section is devoted to the proof of theorems  [ thm : upperbound1][thm : upperbound2 ] .",
    "the proof of theorems [ thm : lower - bd - q][thm : col - sparse ] is given in the supplementary material @xcite .      to prove both theorems [ thm : upperbound1 ] and [ thm : upperbound2 ] , we go through the following three steps :    we decompose the value of the loss function into multiple terms which result from different sources ;    we derive individual high probability bound for each term in the decomposition ;    we assemble the individual bounds to obtain the desired upper bounds on the loss and the risk functions .",
    "in the rest of this subsection , we carry out these three steps in order . to facilitate the presentation",
    ", we introduce below several important quantities to be used in the proof .",
    "recall the effective sparsity @xmath250 defined in ( [ eq : effectivesparsity ] ) .",
    "let @xmath251 be the index set of the rows of @xmath252 with the @xmath253 largest @xmath65 norms . in case",
    "@xmath252 has no more than @xmath254 nonzero rows , we include in @xmath255 the smallest indices of the zero rows in @xmath252 such that @xmath256 .",
    "we also define @xmath257 analogously . in what follows",
    ", we refer to them as the _ effective support sets_.    we define @xmath258 as a solution to @xmath259 \\\\[-8pt ] \\nonumber & & \\qquad \\mbox{s.t.}\\quad a'{\\sigma}_xa = b ' { \\sigma}_yb = i_r\\mbox { and } { \\operatorname{supp}}(a ) \\subset s_u , { \\operatorname{supp}}(b ) \\subset s_v.\\end{aligned}\\ ] ] in what follows , we refer to them as the _ sparse approximations _ to @xmath48 and @xmath49 . by definition ,",
    "when @xmath71 , @xmath260 , which can be derived rigorously from theorem  [ thm : sintheta ] .",
    "in addition , we define the _ oracle estimator _ @xmath261 as a solution to @xmath262 \\\\[-8pt ] \\nonumber & & \\qquad\\mbox{s.t . }",
    "\\quad a'{\\widehat}{\\sigma}_xa = b'{\\widehat } { \\sigma}_yb = i_r\\mbox { and } { \\operatorname{supp}}(a ) = s_u , { \\operatorname{supp}}(b ) = s_v.\\end{aligned}\\ ] ] in case the program ( [ eq : uv - oracle ] ) [ or ( [ eq : uv - oracle - est ] ) ] has multiple global optimizers , we define @xmath258 [ or @xmath263 by picking an arbitrary one .",
    "the introduction of ( [ eq : uv - oracle ] ) and ( [ eq : uv - oracle - est ] ) is to separate the error brought by not knowing the covariance @xmath264 and @xmath28 and by not knowing the effective supports @xmath255 and @xmath265 .",
    "the program ( [ eq : uv - oracle - est ] ) assumes known effective supports but unknown covariance and the program ( [ eq : uv - oracle ] ) assumes both known effective supports and known covariance .",
    "we note that @xmath266 by definition , the matrices @xmath267 are normalized with respect to @xmath27 and @xmath28 , and @xmath268 are normalized with respect to @xmath269 and @xmath270 .",
    "note the notation @xmath271 stands for the submatrix of @xmath56 with rows in @xmath272 and all columns .",
    "last but not least , let @xmath273 by the definition of @xmath274 in ( [ eq : pickset ] ) , we have @xmath275 and @xmath276 with probability one .",
    "remember the minimax rate @xmath182 defined in ( [ eq : defepsilon ] ) .      in the first step , we decompose the loss function into five terms as follows .",
    "[ lem : lossdecompose ] assume @xmath277 for sufficiently small @xmath223 .",
    "for any constant @xmath173 , there exists a constant @xmath174 only depending on @xmath278 and @xmath176 , such that @xmath279 with probability at least @xmath280",
    ".    see section  [ sec : lossdecompose - pf ] .",
    "in particular , lemma  [ lem : lossdecompose ] decomposes the total loss into the sum of the sparse approximation error in ( [ eq : sparseapproxerror ] ) , the oracle loss in ( [ eq : oracleloss ] ) which is present even if we have the oracle knowledge of the effective support sets @xmath255 and @xmath265 , the bias term in ( [ eq : bias ] ) caused by the presence of the residual term @xmath281 in the cca structure ( [ eq : cca ] ) and the two excess loss terms in ( [ eq : excessloss1 ] ) and ( [ eq : excessloss2 ] ) resulting from the uncertainty about the effective support sets . when @xmath71 , the sparse approximation error term ( [ eq : sparseapproxerror ] ) vanishes .",
    "we now state the bounds for the individual terms obtained in lemma  [ lem : lossdecompose ] as five separate lemmas .",
    "the proofs of these lemmas are deferred to sections [ sec : sparseapproxerror - pf][sec : excessloss2-pf ] .",
    "[ lem : sparseapproxerror ] suppose ( [ eq : ass1 ] ) and ( eq : ass2 ) hold .",
    "there exists a constant @xmath174 only depending on @xmath282 , such that @xmath283    [ lem : oracleloss ] suppose @xmath284 and that ( [ eq : ass2 ] ) holds for some sufficiently small @xmath223 . for any constant @xmath173 ,",
    "there exists a constant @xmath174 only depending on @xmath175 and @xmath176 , such that @xmath285 , \\label{eq : oraclelossclaim1}\\ ] ] with probability at least @xmath286 .",
    "moreover , if ( [ eq : ass1 ] ) also holds , then with the same probability @xmath287    the proof of lemma  [ lem : oracleloss ] is given in the supplementary material @xcite . since @xmath288 , ( [ eq : oraclelossclaim1 ] )",
    "is bounded above by @xmath289 .",
    "the error bounds in lemma  [ lem : oracleloss ] are due to the estimation error of true covariance matrices by sample covariance matrices on the subset @xmath290 .",
    "[ lem : bias ] suppose @xmath291 for some constant @xmath212 . for any constant @xmath173",
    ", there exists a constant @xmath174 only depending on @xmath292 and @xmath176 , such that @xmath293 with probability at least @xmath294 .",
    "the bias in lemma  [ lem : bias ] is @xmath210 when @xmath295 is @xmath210 .",
    "[ lem : excessloss1 ] suppose ( [ eq : ass1 ] ) holds . for any constant @xmath173 , there exists a constant @xmath174 only depending on @xmath278 and @xmath176 , such that @xmath296 with probability at least @xmath297 .",
    "[ lem : excessloss2 ] suppose ( [ eq : ass1 ] ) and ( [ eq : ass2 ] ) hold . for any constant , there exists a constant @xmath174 only depending on @xmath282 and @xmath176 , such that @xmath298 with probability at least @xmath299 .      for notational convenience ,",
    "let @xmath300 consider the event such that the conclusions of lemmas [ lem : lossdecompose][lem : excessloss2 ] hold , which occurs with probability at least @xmath301 according to the union bound . on this event , lemmas [ lem : sparseapproxerror ] and [ lem : oracleloss ]",
    "imply that @xmath302 moreover , lemma  [ lem : bias ] implies @xmath303 lemma  [ lem : excessloss1 ] implies @xmath304 and lemma  [ lem : excessloss2 ] implies @xmath305 together with lemma  [ lem : lossdecompose ] , the above bounds lead to @xmath306 under assumption ( [ eq : ass2 ] ) , we have @xmath307 , implying @xmath308 for some @xmath174 .",
    "we complete the proof by noting that the conditions of lemmas [ lem : lossdecompose][lem : excessloss2 ] are satisfied under assumptions ( [ eq : ass1 ] ) and ( [ eq : ass2 ] ) .",
    "recall the definition of @xmath309 in ( [ eq : defepsilon ] ) , and let @xmath214 be the constant in ( [ eq : theoremexpassup1 ] ) and ( [ eq : theoremexpassup2 ] ) .",
    "the result of theorem  thm : upperbound1 implies that we can choose an arbitrarily large constant @xmath310 such that @xmath311",
    ". given  @xmath176 , there exists a constant  @xmath312 , by which we can bound the risk as follows : @xmath313 \\nonumber \\\\ & & \\qquad\\quad { } + \\mathbb{e}_\\sigma\\bigl [ \\bigl\\| \\widehat{u_{1}v_{1}^{\\prime } } -u_{1}v_{1}^{\\prime } \\bigr\\|_{\\mathrm{f}}^{2 } { \\mathbf{1}_{\\{{\\| \\widehat{u_{1}v_{1}^{\\prime}}-u_{1}v_{1}^{\\prime}\\|_{\\mathrm { f}}^{2 } > c{\\varepsilon}_n ^{2 } } \\ } } } \\bigr ] \\nonumber \\\\ & & \\qquad\\leq c{\\varepsilon}_n ^{2}+\\mathbb{e}_\\sigma\\bigl [ \\bigl ( 2\\bigl\\|\\widehat { u_{1}v_{1}^{\\prime } } \\bigr\\|_{\\mathrm{f}}^{2}+2\\bigl\\| u_{1}v_{1}^{\\prime } \\bigr\\| _ { \\mathrm{f}}^{2 } \\bigr ) { \\mathbf{1}_{\\{{\\| \\widehat { u_{1}v_{1}^{\\prime}}-u_{1}v_{1}^{\\prime}\\|_{\\mathrm{f}}^{2 } > c{\\varepsilon}_n ^{2 } } \\ } } } \\bigr ] \\label{eq : theoremexp1 } \\\\ & & \\qquad\\leq c{\\varepsilon}_n ^{2}+ 6m^{2}r \\mathbb{p}_\\sigma\\bigl ( \\bigl\\| \\widehat{u}_{1}\\widehat { v}_{1}^{\\prime}-u_{1}v_{1}^{\\prime } \\bigr\\|_{\\mathrm{f}}^{2}>c{\\varepsilon}_n ^{2 } \\bigr ) \\label{eq : theoremexp2 } \\\\ & & \\qquad\\leq c_{2}{\\varepsilon}_n ^{2}. \\label{eq : theoremexp3}\\end{aligned}\\ ] ] here , inequality ( [ eq : theoremexp1 ] ) is due to the triangle inequality and the fact that @xmath314 in fact , if @xmath315 , then @xmath316 . by our definition of the estimator ,",
    "this means @xmath317 , which further implies @xmath318 .",
    "inequality ( [ eq : theoremexp2 ] ) follows from our definition of estimator @xmath205 and ( eq : boundtruth ) .",
    "the last inequality follows from the conclusion of theorem  [ thm : upperbound1 ] and assumptions ( eq : theoremexpassup1 ) and ( [ eq : theoremexpassup2 ] ) .",
    "this completes the proof .",
    "in this section , we prove lemmas [ lem : lossdecompose][lem : sparseapproxerror ] and [ lem : bias][lem : excessloss2 ] used in the proof of theorem  [ thm : upperbound1 ] and [ thm : upperbound2 ] .",
    "the proof of lemma  [ lem : oracleloss ] is given in the supplementary material @xcite . throughout the section , without further notice , @xmath319 is defined as in ( [ eq : defepsilon ] ) .        the first result is a generalized sin - theta theorem . for the definition of unitarily invariant norms ,",
    "we refer the readers to @xcite . in particular , both frobenius norm @xmath320 and operator norm @xmath321 are unitarily invariant .",
    "[ thm : sintheta ] consider matrices @xmath322 .",
    "let the svd of @xmath20 and @xmath21 be @xmath323 with @xmath324 and @xmath325 .",
    "suppose there is a positive constant @xmath326 $ ] such that @xmath327 .",
    "let @xmath328 be any unitarily invariant norm , and @xmath329 .",
    "then we have @xmath330 if further there is an absolute constant @xmath331 such that @xmath332 , then there is a constant @xmath174 only depending on @xmath333 , such that @xmath334    in addition , when @xmath20 and @xmath21 are positive semi - definite , @xmath335 , @xmath336 for @xmath337 , we recover the classical davis ",
    "kahan sin - theta theorem @xcite @xmath338 up to a constant multiplier .",
    "recall the definition of @xmath344 and @xmath345 in section  [ sec : prelim - proof ] . from here",
    "on , let @xmath346 the proof of lemma  [ lem : lossdecompose ] depends on the following two technical results .",
    "their proofs are given in the supplementary material @xcite .",
    "[ lem : diff ] under the assumption of lemma  [ lem : lossdecompose ] , for any constant @xmath173 , there exists a constant @xmath174 only depending on @xmath278 and @xmath176 , such that for any matrix @xmath56 supported on the @xmath352 , we have @xmath353 with probability at least @xmath294 .",
    "proof of lemma  [ lem : lossdecompose ] first of all , the triangle inequality and jensen s inequality together lead to @xmath354 \\\\[-8pt ] \\nonumber & & \\qquad \\leq3\\bigl ( \\bigl\\| \\widehat{u}_{1}^{\\ast}\\widehat{v}_{1}^{\\ast\\prime } -u_{1}^{\\ast}v_{1}^{\\ast\\prime } \\bigr\\| _ { \\mathrm{f}}^{2 } + \\bigl\\| \\widehat { u}_{1 } \\widehat{v}_{1}^{\\prime}-\\widehat{u}_{1}^{\\ast } \\widehat { v}_{1}^{\\ast\\prime}\\bigr\\| _ { \\mathrm{f}}^{2 } + \\bigl \\| u_{1}^{\\ast } v_{1}^{\\ast\\prime}-u_{1}v_{1}^{\\prime } \\bigr\\| _ { \\mathrm{f}}^{2 } \\bigr).\\end{aligned}\\ ] ] now , it remains to bound @xmath355 . to this end , we have @xmath356 here , ( [ eq : sigma1 ] ) is implied by lemma  [ lem : diff ] and ( [ eq : linearloss ] ) is implied by lemma  [ lem : linearloss ] . to see ( [ eq : aggregdef ] ) , we note @xmath274 is the solution to ( [ eq : pickset ] ) , and so @xmath357 , or equivalently @xmath358 equality ( [ eq : biasstructure1 ] ) comes from the cca structure ( [ eq : cca ] ) and ( [ eq : splitcca ] ) . combining ( [ eq : losssurrogate])(eq : biasstructure1 ) and rearranging the terms ,",
    "we obtain the desired result .      the major difficulty in proving the lemma lies in the presence of the residual structure @xmath107 in ( [ eq : splitcca ] ) and the possible nondiagonality of covariance matrices @xmath27 and @xmath28 . to overcome the difficulty",
    ", we introduce intermediate matrices @xmath359 defined as follows .",
    "first , we write the svd of @xmath360 as @xmath361 and let @xmath362 and @xmath363 . finally , we define @xmath364 and @xmath365 by setting @xmath366 by definition , we have @xmath367 .",
    "last but not least , we define @xmath368 \\\\[-8pt ] \\nonumber & & { } + \\bigl(i - pp^{\\prime}\\bigr ) ( \\sigma_{xs_{u}s_{u}})^{-1/2 } \\sigma_{xs_{u}\\ast}u_{2}\\lambda_{2}v_{2}^{\\prime } \\sigma _ { y\\ast s_{v}}(\\sigma_{ys_{v}s_{v}})^{-1/2 } \\bigl(i - qq^{\\prime}\\bigr).\\end{aligned}\\ ] ] we now summarize the key properties of the @xmath369 and @xmath370 matrices in the following two lemmas , the proofs of which are given in the supplementary material  @xcite .                  _",
    "@xmath385 bound for @xmath386_. since the smallest eigenvalues of @xmath27 and @xmath28 are bounded from below by some absolute positive constant , @xmath387 by lemma  [ lem : cca2 ] , @xmath380 and @xmath388 collect the @xmath88 leading left and right singular vectors of @xmath389 , and by ( [ eq : cca ] ) , @xmath390 and @xmath391 collect the @xmath88 leading left and right singular vectors of @xmath392 .",
    "thus , theorem  [ thm : sintheta ] implies @xmath393 the right - hand side of the above inequality is bounded as @xmath394 \\\\[-8pt ] \\nonumber & & \\qquad\\quad { } + \\bigl \\| u_{1s_{u}\\ast}\\lambda_{1}(v_{1s_{v}^{c}\\ast})^{\\prime } \\bigr\\| _ { \\mathrm{f } } + \\bigl \\|u_{1s_{u}^{c}\\ast } \\lambda_{1}(v_{1s_{v}^{c}\\ast } ) ^{\\prime}\\bigr\\| _ { \\mathrm{f } } \\nonumber \\\\ \\nonumber & & \\qquad\\leq c\\lambda\\bigl(\\| u_{1s_{u}^{c}\\ast}\\|_{\\mathrm{f}}+\\| v_{1s_{v}^{c}\\ast}\\| _ { \\mathrm{f}}\\bigr ) . \\nonumber\\end{aligned}\\ ] ] here , the last inequality is due to ( [ eq : intermsvd ] ) and ( [ eq : uv - intermediate ] ) . for the last term , a similar argument to that used in lemma 7 of @xcite leads to @xmath395 \\\\[-8pt ] \\nonumber \\|v_{1s_{v}^{c}\\ast}\\| _ { \\mathrm{f}}^2 & \\leq & \\frac{cq}{2-q } k_{q}^{v}\\bigl(s_{v}/k_{q}^{v } \\bigr)^{2/q } \\leq\\frac{cq}{2-q } { \\varepsilon}_n^2,\\end{aligned}\\ ] ] where the last inequalities in both displays are due to ( [ eq : x - q - u])([eq : effectivesparsity ] ) .",
    "therefore , we obtain @xmath396    _ @xmath397 bound for @xmath398_. let @xmath399 denote the @xmath378th singular value of @xmath400 .",
    "then we have @xmath401 \\\\[-8pt ] \\nonumber & & \\qquad\\leq c\\bigl\\| ( \\sigma_{xs_{u}s_{u}})^{1/2}\\bigl[u_{1s_{u}\\ast } ^{\\ast } \\bigl(v_{1s_{v}\\ast}^{\\ast}\\bigr)^{\\prime}-{\\widetilde}{u}_{1s_{u}\\ast } ( { \\widetilde}{v}_{1s_{v}\\ast})^{\\prime}\\bigr ] ( \\sigma_{ys_{v}s_{v}})^{1/2 } \\bigr\\|_{\\mathrm{f } } \\\\ & & \\qquad\\leq\\frac{c \\| ( \\sigma_{xs_{u}s_{u}})^{-1/2}\\sigma_{xys_{u}s_{v}}(\\sigma _ { ys_{v}s_{v}})^{-1/2}-\\xi\\| _ { \\mathrm{f } } } { { \\widetilde}{\\lambda}_{r}-\\lambda_{r+1}^{\\ast}}.\\nonumber\\end{aligned}\\ ] ] here , the first equality holds since both @xmath402 and @xmath403 are supported on the @xmath290 submatrix . noting that by the discussion before ( [ eq : uv - oracle ] ) , ( [ eq : uv - intermediate ] ) and lemma  [ lem : cca2 ] , @xmath404 and @xmath405 collect the leading left and right singular vectors of@xmath406 and @xmath372 , respectively , we obtain the last inequality by applying ( [ eq : mpnew ] ) in theorem  [ thm : sintheta ] . in",
    "what follows , we derive upper bound for the numerator and lower bound for the denominator in ( [ eq : bound2ndsparseappr ] ) in order .    _",
    "upper bound for @xmath407_. first , we decompose @xmath408 as @xmath409 then ( [ eq : decompsigma_xy ] ) , ( [ eq : xidef ] ) and ( [ eq : intermsvd ] ) jointly imply that @xmath410 here , the last equality is due to the definition ( [ eq : uv - intermediate ] ) .",
    "the last display , together with  ( [ eq : lqsparse ] ) and lemma  [ lem : lq3 ] , leads to @xmath411 _ lower bound for @xmath412_. the bound ( [ eq : usedlater4 ] ) , together with weyl s inequality ( @xcite , page 449 and hoffman  wielant inequality @xcite , page 63 ) implies @xmath413 \\\\[-8pt ] \\nonumber & & \\qquad \\leq\\bigl\\| ( \\sigma_{xs_{u}s_{u}})^{-1/2}\\sigma_{xys_{u}s_{v}}(\\sigma_{ys_{v}s_{v}})^{-1/2}-\\xi\\bigr\\| _ { \\mathrm{f } } \\leq c \\sqrt{\\frac{q}{2-q } } \\lambda{\\varepsilon}_n \\leq0.1\\lambda . \\ ] ] together with lemma  [ lem : cca2 ] , it further implies @xmath414 combining ( [ eq : bound2ndsparseappr ] ) , ( [ eq : usedlater4 ] ) and ( [ eq : lambda - lower ] ) , we obtain @xmath415 the proof of ( [ eq : sparseapproclaim1 ] ) is completed by combining ( [ eq : piece1 ] ) , ( [ eq : piece2 ] ) and ( [ eq : piece3 ] ) .",
    "we now control each of the three terms on the rightmost - hand side of the second last display .",
    "first , the bound we derived for ( [ eq : zhaoren ] ) , up to a constant multiplier , @xmath418 is upper bounded by the right - hand side of ( [ eq : sparseapproclaim2 ] ) .",
    "next , the bound for @xmath419 has been shown in ( [ eq : boundslamdatilda ] ) .",
    "last but not least , applying ( [ eq : mpprincipal ] ) in theorem  [ thm : sintheta ] , we obtain @xmath420 where the last inequality is due to ( [ eq : usedlater4 ] ) , ( [ eq : boundslamdatilda ] ) , ( [ eq : lambda - lower ] ) and lemma  [ lem : cca2 ] . the proof is completed by assembling the bounds for the three terms .      in this proof , we need the following technical result , which is a direct consequence of lemma  3 in @xcite by applying union bound",
    ". remember the notation @xmath421 and @xmath422 defined in ( [ eq : tu - tv ] ) .",
    "[ lem : covdeviation45 ] assume @xmath291 for some constant @xmath212 .",
    "for any constant @xmath173 , there exists some constant @xmath174 only depending on @xmath423 and @xmath176 , such that @xmath424 with probability at least @xmath294 .",
    "we first bound @xmath428 . by the definition of trace product ,",
    "we have @xmath429 define the svd of matrices @xmath48 and @xmath430 to be @xmath431 for any matrix @xmath432 , we have @xmath433 where @xmath434 with probability at least @xmath435 by lemma  [ lem : covdeviation45 ] .",
    "hence , by lemma  [ lem : subspacedist ] , we have @xmath436 we note that both @xmath437 and @xmath438 are the projection matrices of the left singular spaces of @xmath206 and @xmath203 , respectively , and the eigengap is at constant level since the @xmath88th singular value of @xmath203 is bounded below by some constant and the @xmath378th singular value of @xmath206 is zero",
    ". then a direct consequence of wedin s sin - theta theorem @xcite gives @xmath439 combining ( [ eq : fix1 ] ) and ( [ eq : fix2 ] ) , we have @xmath440 .",
    "the same argument also implies @xmath441 .",
    "therefore , @xmath442 using a similar argument , we also obtain @xmath443 by the triangle inequality , we complete the proof .",
    "notice that the matrix @xmath447 has nonzero rows with indices in @xmath448 and nonzero columns with indices in @xmath449 .",
    "hence , the enlarged matrix @xmath432 has nonzero rows and columns with indices in @xmath450 , where @xmath451 with @xmath452 .",
    "the cardinality of @xmath453 is @xmath454 .",
    "thus , we can rewrite ( [ eq : excessexpand ] ) as @xmath455 where @xmath456 .",
    "note that @xmath457 to obtain the desired bound , it suffices to show that @xmath458 is upper bounded by @xmath459 with high probability .    to this end",
    ", we note that @xmath460 has at most @xmath461 different possible configurations since @xmath255 is deterministic and @xmath462 is a random set of size @xmath254 . for the same reason , @xmath463 has at most @xmath464 different possible configurations .",
    "therefore , the subset @xmath453 has at most @xmath465 different possible configurations , which can be listed as @xmath466 .",
    "let @xmath467 for all @xmath468 $ ] .",
    "since each @xmath469 is of rank at most @xmath470 , so are the @xmath471 s .",
    "therefore , @xmath472 then the union bound leads to @xmath473 where inequality ( [ eq : epused1 ] ) is due to lemma  [ lem : ep ] .",
    "we complete the proof by choosing @xmath474 in the last display for some sufficiently large constant @xmath475 , which , by condition ( [ eq : ass1 ] ) , is bounded .          _",
    "@xmath397 bound for _ ( [ eq : excessloss2.2 ] ) . applying ( [ eq : sparseapproclaim2 ] ) in lemma  [ lem : sparseapproxerror ] , we obtain @xmath479 _ @xmath480 bound for _ ( [ eq : excessloss2.1 ] ) .",
    "we turn to bound ( [ eq : excessloss2.1 ] ) based on a strategy similar to that used in proving lemma  [ lem : excessloss1 ] .",
    "first , we write it in a form for which we could apply lemma  [ lem : ep ] . recall the random sets @xmath421 and @xmath422 defined in ( [ eq : tu - tv ] ) .",
    "then for @xmath481 and @xmath482 , @xmath483 , we have @xmath484 we now bound each term on the rightmost side . applying lemma  [ lem : ep ] with union bound and then following a similar analysis to that leading to ( [ eq : useeptobound ] ) but with @xmath453 replaced by @xmath485 and @xmath463 , we obtain that @xmath486 \\\\[-8pt ] \\nonumber & & \\bigl{\\vert}\\bigl\\langle(\\sigma _",
    "{ yt_{v}t_{v}})^{-1/2}\\widehat { \\sigma}_{yt_{v}t_{v}}(\\sigma _ { yt_{v}t_{v}})^{-1/2}-i_{|t_{v}| } , \\overline{h}_y^{t_v } \\bigr\\rangle \\bigr{\\vert}\\\\ & & \\qquad \\leq c\\sqrt { \\frac{k_{q}^{v}}{n}\\biggl ( r+\\log\\frac{em}{k_{q}^{v } } \\biggr)}\\nonumber\\end{aligned}\\ ] ] with probability at least @xmath487 and @xmath488 , respectively .    to bound @xmath489 and @xmath490 , we note that it follows from lemma  [ lem : covdeviation45 ] that all eigenvalues of @xmath491 and @xmath492 are bounded from below and above by some universal positive constants with probability at least @xmath493 under assumption ( [ eq : ass1 ] ) .",
    "thus , with the same probability we have @xmath494 and @xmath495 combining ( [ eq : excess2.1.1 ] ) , ( [ eq : excess2.1.2 ] ) and ( eq : excess2.1.3 ) , we obtain @xmath496 with probability at least @xmath497 . noting that @xmath498 , this completes the proof ."
  ],
  "abstract_text": [
    "<S> canonical correlation analysis is a widely used multivariate statistical technique for exploring the relation between two sets of variables . </S>",
    "<S> this paper considers the problem of estimating the leading canonical correlation directions in high - dimensional settings . recently , under the assumption that the leading canonical correlation directions are sparse , various procedures have been proposed for many high - dimensional applications involving massive data sets . </S>",
    "<S> however , there has been few theoretical justification available in the literature . in this paper , we establish rate - optimal nonasymptotic minimax estimation with respect to an appropriate loss function for a wide range of model spaces . </S>",
    "<S> two interesting phenomena are observed . </S>",
    "<S> first , the minimax rates are not affected by the presence of nuisance parameters , namely the covariance matrices of the two sets of random variables , though they need to be estimated in the canonical correlation analysis problem . </S>",
    "<S> second , we allow the presence of the residual canonical correlation directions . however , they do not influence the minimax rates under a mild condition on eigengap . a generalized sin - theta theorem and an empirical process bound for gaussian quadratic forms under rank constraint are used to establish the minimax upper bounds , which may be of independent interest .    </S>",
    "<S> ./style / arxiv - general.cfg    ,    ,     + </S>"
  ]
}