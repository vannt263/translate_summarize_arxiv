{
  "article_text": [
    "statistical language modeling has been applied to a wide range of applications and domains with great success . to name a few , automatic speech recognition , machine translation ,",
    "spelling correction , touch - screen `` soft '' keyboards and many natural language processing applications depend on the quality of language models ( lms ) .",
    "the performance of lms is determined mostly by several factors : the amount of training data , quality and match of the training data to the test data , and choice of modeling technique for estimation from the data .",
    "it is widely accepted that the amount of data , and the ability of a given estimation algorithm to accomodate large amounts of training are very important in providing a solution that competes successfully with the entrenched n - gram lms . at the same time , scaling up a novel algorithm to a large amount of data involves a large amount of work , and provides a significant barrier to entry for new modeling techniques . by choosing one billion words as the amount of training data we hope to strike a balance between the relevance of the benchmark in the world of abundant data , and the ease with which any researcher can evaluate a given modeling approach .",
    "this follows the work of goodman  , who explored performance of various language modeling techniques when applied to large data sets .",
    "one of the key contributions of our work is that the experiments presented in this paper can be reproduced by virtually anybody with an interest in lm , as we use a data set that is freely available on the web .",
    "another contribution is that we provide strong baseline results with the currently very popular neural network lm  @xcite .",
    "this should allow researchers who work on competitive techniques to quickly compare their results to the current state of the art .",
    "the paper is organized as follows : section 2 describes how the training data was obtained ; section 3 provides a short overview of the language modeling techniques evaluated ; finally , section 4 presents results obtained and section 5 concludes the paper .    [ cols=\"<,^,^,^,^ \" , ]      another option to reduce training complexity of the maxent models is to use a hierarchical softmax  @xcite . the idea is to estimate probabilities of groups of words , like in a class based model  only the classes that contain the positive examples need to be evaluated . in our case , we explored a binary huffman tree representation of the vocabulary , such that evaluation of frequent words takes less time .",
    "the idea of using frequencies of words for a hierarchical softmax was presented previously in  @xcite .",
    "the recurrent neural network ( rnn ) based lm have recently achieved outstanding performance on a number of tasks  @xcite .",
    "it was shown that rnn lm significantly outperforms many other language modeling techniques on the penn treebank data set  @xcite .",
    "it was also shown that rnn models scale very well to data sets with hundreds of millions of words  @xcite , although the reported training times for the largest models were in the order of weeks .",
    "we cut down training times by a factor of 20 - 50 for large problems using a number of techniques , which allow rnn training in typically 1 - 10 days with billions of words , @xmath0 vocabularies and up to 20b parameters on a single standard machine without gpus .",
    "these techniques were in order of importance : a ) parallelization of training across available cpu threads , b ) making use of simd instructions where possible , c ) reducing number of output parameters by 90% , d ) running a maximum entropy model in parallel to the rnn .",
    "because of space limitations we can not describe the exact details of the speed - ups here  they will be reported in an upcoming paper .",
    "we trained several models with varying number of neurons ( table  [ results - table ] ) using regular sgd with a learning rate of 0.05 to 0.001 using 10 iterations over the data .",
    "the maxent models running in parallel to the rnn capture a history of 9 previous words , and the models use as additional features the previous 15 words independently of order .",
    "while training times approach 2 weeks for the most complex model , slightly worse models can be trained in a few days . note that we did nt optimize for model size nor training speed , only test performance .",
    "results achieved on the benchmark data with various types of lm are reported in table  [ results - table ] .",
    "we focused on minimizing the perplexity when choosing hyper - parameters , however we also report the time required to train the models .",
    "training times are not necessarily comparable as they depend on the underlying implementation .",
    "mapreduces can potentially process larger data sets than single - machine implementations , but come with a large overhead of communication and file i / o .",
    "discussing details of the implementations is outside the scope as this paper .",
    "the best perplexity results were achieved by linearly interpolating together probabilities from all models .",
    "however , only some models had significant weight in the combination ; the weights were tuned on the held - out data . as can be seen in table  [ combination - table ] ,",
    "the best perplexity is about 35% lower than the baseline - the modified kneser - ney smoothed 5-gram model with no count cutoffs .",
    "this corresponds to about 10% reduction of cross - entropy ( bits ) .",
    "somewhat surprisingly the sbo model receives a relatively high weight in the linear combination of models , despite its poor performance in perplexity , whereas the kn baseline receives a fairly small weight relative to the other models in the combination .",
    "we introduced a new data set for measuring research progress in statistical language modeling .",
    "the benchmark data set is based on resources that are freely available on the web , thus fair comparison of various techniques is easily possible .",
    "the importance of such effort is unquestionable : it has been seen many times in the history of research that significant progress can be achieved when various approaches are measurable , reproducible , and the barrier to entry is low .",
    "the choice of approximately one billion words might seem somewhat restrictive .",
    "indeed , it can be hardly expected that new techniques will be immediately competitive on a large data set .",
    "computationally expensive techniques can still be compared using for example just the first or the first 10 partitions of this new dataset , corresponding to approx .",
    "10 million and 100 million words .",
    "however , to achieve impactful results in domains such as speech recognition and machine translation , the language modeling techniques need to be scaled up to large data sets .",
    "another contribution of this paper is the comparison of a few novel modeling approaches when trained on a large data set .",
    "as far as we know , we were able to train the largest recurrent neural network language model ever reported .",
    "the performance gain is very promising ; the perplexity reduction of 35% is large enough to let us hope for significant improvements in various applications .    in the future",
    ", we would like to encourage other researchers to participate in our efforts to make language modeling research more transparent .",
    "this would greatly help to transfer the latest discoveries into real - world applications . in the spirit of a benchmark",
    "our first goal was to achieve the best possible test perplexities regardless of model sizes or training time . however , this was a relatively limited collaborative effort , and some well known techniques are still missing .",
    "we invite other researchers to complete the picture by evaluating new , and well - known techniques on this corpus .",
    "ideally the benchmark would also contain asr or smt lattices / n - best lists , such that one can evaluate application specific performance as well ."
  ],
  "abstract_text": [
    "<S> we propose a new benchmark corpus to be used for measuring progress in statistical language modeling . with almost one billion words of training data </S>",
    "<S> , we hope this benchmark will be useful to quickly evaluate novel language modeling techniques , and to compare their contribution when combined with other advanced techniques . </S>",
    "<S> we show performance of several well - known types of language models , with the best results achieved with a recurrent neural network based language model . </S>",
    "<S> the baseline unpruned kneser - ney 5-gram model achieves perplexity 67.6 . </S>",
    "<S> a combination of techniques leads to 35% reduction in perplexity , or 10% reduction in cross - entropy ( bits ) , over that baseline .    </S>",
    "<S> the benchmark is available as a code.google.com project ; besides the scripts needed to rebuild the training / held - out data , it also makes available log - probability values for each word in each of ten held - out data sets , for each of the baseline n - gram models . </S>"
  ]
}