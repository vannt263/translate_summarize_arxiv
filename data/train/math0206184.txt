{
  "article_text": [
    "the segmentation of images and restoration of degraded images are branches of image processing that are now extensively studied for their evident practical importance as well as theoretical interest .",
    "there are many approaches to solution of these problems .",
    "we consider here methods of the bayesian segmentation of images , which generalize methods of bayesian image estimation @xcite . because of use of a prior information",
    "the methods of bayesian image estimation would be methods of the first choice for many practical problems but unfortunately they are often difficult to compute . until recently approximations of the gibbs estimators have been usually available rather then their exact values .",
    "for last decade the significant progress in the gibbs estimation have been achieved . in particular , the methods of discrete optimization for the high - dimensional gibbs estimation and segmentation have been obtained @xcite .",
    "the methods developed allowed not only an efficient evaluation of the gibbs estimates and but also their fast exact determination .    in this paper the problem of image segmentation is considered as a problem of cluster - analysis for the case of gibbs prior distributions of clusters .",
    "there are enormous number of papers devoted to the problems of bayesian cluster - analysis ( see @xcite ) .",
    "in some of them methods of classification with dependent clusters are presented @xcite .",
    "we consider the problem of classification of observations @xmath0 @xmath1 ( for instance , image intensities or texture characteristics of images etc . ) with the feature function @xmath2 that takes finite number of rational values .",
    "the clusters are identified by appropriate rational labels ( note at once that in the presented below models the case of rationaly valued feature functions and cluster labels is reduced to the case of functions and labels taking only integer values ) .",
    "the clusters are supposed dependent and distributed according to the gibbs field ( such type models occur frequently in image processing ) .",
    "the labeling gibbs field is specified on the directed fully connected graph of all pixels ( usually only graphs of connected nearest neighbors pixels were considered .",
    "often it was finite d - dimensional regular lattices ) .",
    "the discrete optimization methods that enable efficient determination of an exact solution of the segmentation problem are described . in the offered methods the problem of identification of the gibbs classifiers is reduced , first , to the integer optimization problem and then to the problem of findind of the minimum cuts of special networks .",
    "the minimum cuts of the networks can be found by fast methods @xcite that take into consideration the specific character of the networks and allow execution in concurrent mode .",
    "it can be easily seen that in models presented below the case of rationally valued feature functions and cluster labels is reduced to the case of integer classification .",
    "so , let an image @xmath3 be with the feature function @xmath4 that takes finite number integer values in @xmath5 and let @xmath6 .",
    "let @xmath7 , @xmath8 be a set of allowable cluster labels .",
    "suppose the image @xmath9 is partitioned into @xmath10 clusters ( the number @xmath10 instead of @xmath11 is taken only to simplify formulas ) and each cluster is specified by integer number @xmath12 , as well as by an appropriate fixed integer label @xmath13 .",
    "the feature vector @xmath14 of image @xmath9 is considered as a random variable with the non - identical exponential @xmath15 or with the non - identical gaussian @xmath16 gibbs distribution @xmath17 .",
    "the labels @xmath18 of clusters are supposed to be dependent random variables distributed according to the gibbs field .",
    "the segmented image @xmath19 is specified on the fully connected directed graph @xmath20 with the set of vertices @xmath21 and the set of directed arcs @xmath22 .",
    "it takes values in the space of labels @xmath23 , i.e. @xmath24 , and is either the exponential or the gaussian form @xmath25 where @xmath26 is the norming quantities ( here and below different constant will be denoted by the same letter ) , the vector @xmath27 and parameters @xmath28 .",
    "\\(i ) the gibbs models considered are extentions of the ising model .",
    "\\(ii ) note , that for some image processing problems the prior distribution @xmath29 is more preferable than gaussian gibbs one because of smaller blurring effect .",
    "moreover , its identification turns out far more computationally efficient .    for the model with the exponential conditional distribution @xmath30 and the exponential prior @xmath31 the gibbs classifier is equal to @xmath32 and respectively , for the model with the gaussian conditional distribution @xmath33 and the gaussian",
    "prior @xmath34 the gibbs classifier is of the form @xmath35 in spite of clear posing the problem of finding exact values of the gibbs classifiers @xmath36 and @xmath37 for large samples is rather complicated .",
    "so , for instance , computer images , which are frequent objects of classification , usually consist up to @xmath38 variables or more .",
    "nevertheless , it turned out possible to compute efficiently ( during polynomial time on the sample size @xmath39 and number of clusters @xmath11 ) both of these classifiers . theoretically , run - time for the first classifier does not exceed @xmath40 and for the second one is less than @xmath41 . for many applied problems real run - time was even of order @xmath42",
    "the efficient computation of the classifiers for the mixed model with the exponential conditional distributions @xmath43 and the gaussian prior @xmath44 as well as for the mixed model with the gaussian conditional distributions @xmath33 and the exponential prior @xmath31 is also available .",
    "in the case of two classes ( @xmath45 ) the gibbs classifier @xmath46 and @xmath47 coincide ( since any boolean variable @xmath48 satisfies the identity @xmath49 ) . they can be evaluated by the network flow optimization methods @xcite . moreover , in 1989 greig , porteous and seheult @xcite developed a heuristic network flow algorithm that is especially efficient for estimating the boolean gibbs estimator .",
    "these authors posed also the problem for the case more than 2 clusters .",
    "recently we have described the multiresolution network flow minimum cut algorithm @xcite that allows exact computation of boolean classifiers as well as developed algorithms of computation of the mentioned gibbs classifiers in general case .",
    "it turned out the network flow optimization methods can be used to identify @xmath46 and @xmath47 even when @xmath50 .",
    "denote the function to be minimized by @xmath51 the idea of the method is to represent the vector of labels of clusters @xmath52 by the integer valued linear combination of boolean vectors and then reduce the problem of integer minimization of the function @xmath53 to the problem of boolean minimization .",
    "the problem of boolean minimization can be solved by the network flow optimization methods .",
    "let for arbitrary integers @xmath54 and @xmath55 the indicator function @xmath56 be equal to 1 if @xmath57 and be equal to 0 otherwise . for any @xmath58 and boolean variables @xmath59 such that @xmath60 the identity @xmath61 is valid , and vice versa ,",
    "any non - increasing sequence of the boolean variables @xmath60 specifies the label @xmath58 by the formula ( [ e : mu ] ) . by analogy ,",
    "the feature functions @xmath62 are represented as sums @xmath63 of non - increasing sequence of the boolean variables @xmath64 .",
    "let for @xmath65 the vector @xmath66 be boolean , the vector @xmath67 be with coordinates @xmath68 and the norm of the vector @xmath69 , then the following proposition is satisfied .",
    "[ p : mod ] for any integers @xmath70 and @xmath71 the equality @xmath72 holds true . therefore , for any feature vector @xmath73 and the boolean vector @xmath74 the function @xmath53 can be written in the form @xmath75 where for @xmath39-dimensional boolean vector @xmath76 functions @xmath77    denote by @xmath78 boolean solutions that minimize the functions @xmath79 . for two vectors @xmath80 and @xmath81",
    "we will write @xmath82 if all corresponding pairs of their coordinates satisfy the inequality @xmath83 and will write @xmath84 if there exist at least two different pairs such that @xmath85 and @xmath86 . note that @xmath87 , and what is more , for some integer @xmath88 their coordinates satisfy the following condition    @xmath89    it is easy to show that in general the solutions @xmath90 of the ( [ e : lmin ] ) are not ordered .",
    "nevertheless , without fail there is at least one non - increasing sequence @xmath91 of solutions of ( [ e : lmin ] ) .",
    "[ t : mon ] there is a non - increasing sequence @xmath92 of solutions of ( [ e : lmin ] ) .",
    "some structural properties of the set of solutions @xmath90 are presented in    [ c : struct ] for integer @xmath93 and the sequence of vectors @xmath87 the following properties are valid :    \\(i ) if @xmath94 is any solution of ( [ e : lmin ] ) , then there is a solution @xmath95 so that @xmath96 , and vice versa , if @xmath95 is any solution of ( [ e : lmin ] ) , then there is a solution @xmath94 so that @xmath96 .",
    "\\(ii ) for each @xmath97 the set of solutions @xmath98 has the minimal @xmath99 and the maximal @xmath100 elements .",
    "\\(iii ) the set of minimal and maximal elements are ordered , i.e. @xmath101 and @xmath102 .    sentence _ ( i ) _ follows immediately from theorem [ t : mon ] . sentence _",
    "( ii ) _ is deduced from _ ( i ) _ considered for @xmath103 , property _ ( iii ) _ follows from _ ( ii ) _ and definition of the minimal and the maximal elements .    for two boolean vectors @xmath104 and @xmath105 let the vector @xmath106 , respectively , @xmath107 be with coordinates @xmath108 , respectively , with coordinates @xmath109 .",
    "if @xmath110 is any unordered sequence of solutions of ( [ e : lmin ] ) the ordered sequence of solutions can be derived from it by the logical operation @xmath111 like one - dimensional variational series @xmath112 .",
    "but it is easy to see the sum of ordered solutions @xmath90 is a solution of ( [ e : m_e ] ) .",
    "if @xmath92 is a sequence of ordered solutions of ( [ e : lmin ] ) then the sum @xmath113 minimizes @xmath53 .",
    "the problem of computing boolean solutions @xmath90 is familiar in the discrete optimization @xcite .",
    "it is equivalent to identification of the minimum cuts for specially built networks .",
    "there are fast algorithms to compute them @xcite .",
    "now denote the function to be minimized by @xmath114 to find a solution @xmath37 that minimizes the function @xmath115 the representation of the vector of cluster labels @xmath52 by the integer valued linear combination of boolean vectors is used once more .",
    "then the problem of integer minimization of the function @xmath115 is reduced to the problem of boolean minimization .",
    "denote for brevity @xmath116 , @xmath117 .",
    "the vector @xmath52 can be represented by the formula ( [ e : mu ] ) as the linear combination @xmath118 of boolean vectors @xmath119 @xmath120 , and the function @xmath121 can be written in the form @xmath122 let @xmath123 and @xmath124 , then @xmath125 where the polynomial of boolean variables @xmath126 after cancellation is written as @xmath127x_i(l)+\\\\ 2\\sum_{i\\in v}\\bigg[{\\lambda}_i+\\sum_{j\\in v}({\\beta}_{i , j}+{\\beta}_{j , i})\\bigg ] \\sum_{1\\le\\tau <",
    "l\\le k}d_{l,\\tau}x_i(\\tau)x_i(l)+\\\\ \\sum_{(i , j)\\in e}{\\beta}_{i , j } \\bigg[\\sum_{l=1}^k a_l^2 \\big(x_i(l)-x_j(l)\\big)^2 + \\sum_{l\\neq \\tau}d_{l,\\tau}\\big[\\big(x_i(l)-x_j(\\tau)\\big)^2 + ( x_j(l)-x_i(\\tau))^2\\big]\\bigg].\\end{gathered}\\ ] ] note that it has the same points of minimum as @xmath115 .",
    "let us consider another polynomial of boolean variables @xmath128x_i(l)+\\\\ 2\\sum_{i\\in v}\\bigg[{\\lambda}_i+\\sum_{j\\in v}({\\beta}_{i , j}+{\\beta}_{j , i})\\bigg ] \\sum_{1\\le\\tau < l\\le k}d_{l,\\tau}x_i(l)+\\\\ \\sum_{(i , j)\\in e}{\\beta}_{i , j } \\bigg[\\sum_{l=1}^k a_l^2 \\big(x_i(l)-x_j(l)\\big)^2 + \\sum_{l\\neq",
    "\\tau}d_{l,\\tau}\\big[\\big(x_i(l)-x_j(\\tau)\\big)^2 + ( x_j(l)-x_i(\\tau))^2\\big]\\bigg].\\end{gathered}\\ ] ] such that @xmath129 and which differs from @xmath130 by the term @xmath131 in the second line .    denote by @xmath132 any collection of boolean vectors that minimizes @xmath133 . without fail @xmath134 .",
    "this feature allows expressing solutions of the initial problem as @xmath135 .",
    "[ t : q = x ] any collection @xmath136 that minimizes @xmath137 forms the nonincreasing sequence .",
    "the polynomials @xmath130 and @xmath137 have the same set of ordered solutions and , therefore , each solution @xmath138 is specified by the formula @xmath139 .    theorem [ t : q = x ] allows determination of the classifier @xmath138 by the boolean minimization of the polynomial @xmath137 . unlike @xmath130 this polynomial",
    "can be minimized directly by the minimum network cut algorithms @xcite .",
    "the appropriate network is described in @xcite .",
    "here we show several numerical tests as well as a resust of segmentation of a real 3d us - imade of size @xmath140 .",
    "the original 2d gray - scale image in figure 1a ( see next page ) was corrupted by gaussian random noise ( figure 1b ) .",
    "the results of restoration by the extened ising model are placed in figure 1c,1d and by the classical ising model are depicted in figure 1e,1f .    in figure 2a the slice of original 3d us - image of the thyroid gland",
    "is depicted .",
    "its contour that was done by expert is drown in figure 2a .",
    "the corresponding slice of 3d segmentation of the original image by the extended ising model are placed in figure 2c , d .",
    "the full segmentation of 3d @xmath140 image takes about 40min of processor pentium - iii 800 .",
    "( a)(b )    ( c)(d )    ( e)(f )    figure 1    ( a)(b )    ( c)(d )    figure 2",
    "in the paper the bayesian methods of segmentation and estimation of gray - scale and color images are presented . for both of them it is supposed feature function of images and labels of segments take finite number of rational ( possibly , different ) values and they are distributed according to either the exponential gibbs or the gaussian gibbs distribution .",
    "the numerical tests showed the methods developed allow solution problems of practical segmentation and gibbs estimation of images of large sizes ."
  ],
  "abstract_text": [
    "<S> the network flow optimization approach is offered for bayesian segmentation of gray - scale and color images . </S>",
    "<S> it is supposed image pixels are characterized by a feature function taking finite number of arbitrary rational values ( it can be either intensity values or other characteristics of images ) . </S>",
    "<S> the clusters of homogeneous pixels are described by labels with values in another set of rational numbers . </S>",
    "<S> they are assumed to be dependent and distributed according to either the exponential or the gaussian gibbs law . </S>",
    "<S> instead traditionally used local neighborhoods of nearest pixels the completely connected graph of dependence of all pixels is employed for the gibbs prior distributions .    the methods developed reduce the problem of segmentation to the problem of determination of the minimum cut of an appropriate network .    </S>",
    "<S> _ mathematics subject classification 2000 : _ 62 , 90 , 68     + _ key words and phrases : _ image restoration , ising models , integer programming , quadratic programming , minimum network flow cut algorithm </S>"
  ]
}