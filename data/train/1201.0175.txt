{
  "article_text": [
    "information and technology make large data sets widely available for scientific discovery .",
    "much statistical analysis of such high - dimensional data involves the estimation of a covariance matrix or its inverse ( the precision matrix ) .",
    "examples include portfolio management and risk assessment ( fan , fan and lv , 2008 ) , high - dimensional classification such as fisher discriminant ( hastie , tibshirani and friedman , 2009 ) , graphic models ( meinshausen and bhlmann , 2006 ) , statistical inference such as controlling false discoveries in multiple testing ( leek and storey , 2008 ; efron , 2010 ) , finding quantitative trait loci based on longitudinal data ( yap , fan , and wu , 2009 ; xiong et al . 2011 ) , and testing the capital asset pricing model ( sentana , 2009 ) , among others .",
    "see section 5 for some of those applications .",
    "yet , the dimensionality is often either comparable to the sample size or even larger . in such cases ,",
    "the sample covariance is known to have poor performance ( johnstone , 2001 ) , and some regularization is needed .",
    "realizing the importance of estimating large covariance matrices and the challenges brought by the high dimensionality , in recent years researchers have proposed various regularization techniques to consistently estimate @xmath0 .",
    "one of the key assumptions is that the covariance matrix is sparse , namely , many entries are zero or nearly so ( bickel and levina , 2008 , rothman et al , 2009 , lam and fan 2009 , cai and zhou , 2010 , cai and liu , 2011 ) . in many applications",
    ", however , the sparsity assumption directly on @xmath0 is not appropriate .",
    "for example , financial returns depend on the equity market risks , housing prices depend on the economic health , gene expressions can be stimulated by cytokines , among others . due to the presence of common factors , it is unrealistic to assume that many outcomes are uncorrelated .",
    "an alternative method is to assume a factor model structure , as in fan , fan and lv ( 2008 ) .",
    "however , they restrict themselves to the strict factor models with known factors .",
    "a natural extension is the conditional sparsity .",
    "given the common factors , the outcomes are weakly correlated .",
    "in order to do so , we consider an approximate factor model , which has been frequently used in economic and financial studies ( chamberlain and rothschild , 1983 ; fama and french 1993 ; bai and ng , 2002 , etc ) : @xmath1 here @xmath2 is the observed response for the @xmath3th ( @xmath4 ) individual at time @xmath5 ; @xmath6 is a vector of factor loadings ; @xmath7 is a @xmath8 vector of common factors , and @xmath9 is the error term , usually called _",
    "idiosyncratic component _ , uncorrelated with @xmath7 . both @xmath10 and @xmath11",
    "diverge to infinity , while @xmath12 is assumed fixed throughout the paper , and @xmath10 is possibly much larger than @xmath11 .",
    "we emphasize that in model ( [ eq1.1 ] ) , only @xmath2 is observable .",
    "it is intuitively clear that the unknown common factors can only be inferred reliably when there are sufficiently many cases , that is , @xmath13 . in a data - rich environment",
    ", @xmath10 can diverge at a rate faster than @xmath11 .",
    "the factor model ( [ eq1.1 ] ) can be put in a matrix form as @xmath14 where @xmath15 , @xmath16 and @xmath17 .",
    "we are interested in @xmath0 , the @xmath18 covariance matrix of @xmath19 , and its inverse , which are assumed to be time - invariant . under model ( [ eq1.1 ] ) , @xmath0 is given by @xmath20 where @xmath21 is the covariance matrix of @xmath22 .",
    "the literature on approximate factor models typically assumes that the first @xmath12 eigenvalues of @xmath23 diverge at rate @xmath24 , whereas all the eigenvalues of @xmath25 are bounded as @xmath26 .",
    "this assumption holds easily when the factors are pervasive in the sense that a non - negligible fraction of factor loadings should be non - vanishing .",
    "the decomposition ( [ eq1.3 ] ) is then asymptotically identified as @xmath26 .",
    "in addition to it , in this paper we assume that @xmath25 is _ approximately sparse _ as in bickel and levina ( 2008 ) and rothman et al .",
    "( 2009 ) : for some @xmath27 , @xmath28 does not grow too fast as @xmath29 in particular , this includes the exact sparsity assumption ( @xmath30 ) under which @xmath31 , the maximum number of nonzero elements in each row .    the conditional sparsity structure of ( [ eq1.2 ] )",
    "was explored by fan , liao and mincheva ( 2011 ) in estimating the covariance matrix , when the factors @xmath32 are observable .",
    "this allows them to use regression analysis to estimate @xmath33 .",
    "this paper deals with the situation in which the factors are unobservable and have to be inferred .",
    "our approach is simple , optimization - free and it uses the data only through the sample covariance matrix .",
    "run the singular value decomposition on the sample covariance matrix @xmath34 of @xmath19 , keep the covariance matrix formed by the first @xmath12 principal components , and apply the thresholding procedure to the remaining covariance matrix .",
    "this results in a principal orthogonal complement thresholding ( poet ) estimator .",
    "when the number of common factors @xmath12 is unknown , it can be estimated from the data .",
    "see section 2 for additional details .",
    "we will investigate various properties of poet under the assumption that the data are serially dependent , which includes independent observations as a specific example .",
    "the rate of convergence under various norms for both estimated @xmath0 and @xmath25 and their precision ( inverse ) matrices will be derived .",
    "we show that the effect of estimating the unknown factors on the rate of convergence vanishes when @xmath35 , and in particular , the rate of convergence for @xmath25 achieves the optimal rate in cai and zhou ( 2012 ) .",
    "this paper focuses on the high - dimensional _ static factor model _ ( [ eq1.2 ] ) , which is innately related to the principal component analysis ( pca ) , as clarified in section 2 .",
    "this feature makes it different from the classical factor model with fixed dimensionality ( e.g. , lawley and maxwell 1971 ) . in the last ten years",
    ", much theory on the estimation and inference of the static factor model has been developed , for example , stock and watson ( 1998 , 2002 ) , bai and ng ( 2002 ) , bai ( 2003 ) , doz , giannone and reichlin ( 2011 ) , among others .",
    "our contribution is on the estimation of covariance matrices and their inverse in large factor models .",
    "the _ static _ model considered in this paper is to be distinguished from the _ dynamic factor model _ as in forni , hallin , lippi and reichlin ( 2000 ) ; the latter allows @xmath19 to also depend on @xmath7 with lags in time .",
    "their approach is based on the eigenvalues and principal components of spectral density matrices , and on the frequency domain analysis .",
    "moreover , as shown in forni and lippi ( 2001 ) , the dynamic factor model does not really impose a restriction on the data generating process , and the assumption of idiosyncrasy ( in their terminology , a @xmath10-dimensional process is idiosyncratic if all the eigenvalues of its spectral density matrix remain bounded as @xmath26 ) asymptotically identifies the decomposition of @xmath2 into the common component and idiosyncratic error .",
    "the literature includes , for example , forni et al .",
    "( 2000 , 2004 ) , forni and lippi ( 2001 ) , hallin and lika ( 2007 , 2011 ) , and many other references therein .",
    "above all , both the static and dynamic factor models are receiving increasing attention in applications of many fields where information usually is scattered through a ( very ) large number of interrelated time series .",
    "there has been extensive literature in recent years that deals with sparse principal components , which has been widely used to enhance the convergence of the principal components in high - dimensional space .",
    "daspremont , bach and el ghaoui ( 2008 ) , shen and huang ( 2008 ) , witten , tibshirani , and hastie ( 2009 ) and ma ( 2011 ) proposed and studied various algorithms for computations . more literature on sparse pca is found in johnstone and lu ( 2009 ) , amini and wainwright ( 2009 ) , zhang and el ghaoui ( 2011 ) , birnbaum et al .",
    "( 2012 ) , among others .",
    "in addition , there has also been a growing literature that theoretically studies the recovery from a low - rank plus sparse matrix estimation problem , see for example , wright et al .",
    "( 2009 ) , lin et al .",
    "( 2009 ) , cands et al .",
    "( 2011 ) , luo ( 2011 ) , agarwal , nagahban , wainwright ( 2012 ) , pati et al .",
    "it corresponds to the identifiability issue of our problem .",
    "there is a big difference between our model and those considered in the aforementioned literature . in the current paper ,",
    "the first @xmath12 eigenvalues of @xmath0 are spiked and grow at a rate @xmath24 , whereas the eigenvalues of the matrices studied in the existing literature on covariance estimation are usually assumed to be either bounded or slowly growing . due to this distinctive feature , the common components and the idiosyncratic components can be identified , and in addition , pca on the sample covariance matrix can consistently estimate the space spanned by the eigenvectors of @xmath0 .",
    "the existing methods of either thresholding directly or solving a constrained optimization method can fail in the presence of very spiked principal eigenvalues .",
    "however , there is a price to pay here : as the first @xmath36 eigenvalues are  too spiked \" , one can hardly obtain a satisfactory rate of convergence for estimating @xmath0 in absolute term , but it can be estimated accurately in relative term ( see section 3.3 for details ) .",
    "in addition , @xmath37 can be estimated accurately .",
    "we would like to further note that the low - rank plus sparse representation of our model is on the population covariance matrix , whereas cands et al .",
    "( 2011 ) , wright et al .",
    "( 2009 ) , lin et al .",
    "( 2009 ) considered such a representation on the data matrix .",
    "as there is no @xmath0 to estimate , their goal is limited to producing a low - rank plus sparse matrix decomposition of the data matrix , which corresponds to the identifiability issue of our study , and does not involve estimation and inference .",
    "in contrast , our ultimate goal is to estimate the population covariance matrices as well as the precision matrices . for this purpose",
    ", we require the idiosyncratic components and common factors to be uncorrelated and the data generating process to be strictly stationary .",
    "the covariances considered in this paper are constant over time , though slow - time - varying covariance matrices are applicable through localization in time ( time - domain smoothing ) .",
    "our consistency result on @xmath25 demonstrates that the decomposition ( [ eq1.3 ] ) is identifiable , and hence our results also shed the light of the  surprising phenomenon \" of cands et al .",
    "( 2011 ) that one can separate fully a sparse matrix from a low - rank matrix when only the sum of these two components is available .",
    "the rest of the paper is organized as follows .",
    "section 2 gives our estimation procedures and builds the relationship between the principal components analysis and the factor analysis in high - dimensional space .",
    "section 3 provides the asymptotic theory for various estimated quantities .",
    "section 4 illustrates how to choose the thresholds using cross - validation and guarantees the positive definiteness in any finite sample .",
    "specific applications of regularized covariance matrices are given in section 5 .",
    "numerical results are reported in section 6 .",
    "finally , section 7 presents a real data application on portfolio allocation .",
    "all proofs are given in the appendix . throughout the paper ,",
    "we use @xmath38 and @xmath39 to denote the minimum and maximum eigenvalues of a matrix @xmath40 .",
    "we also denote by @xmath41 , @xmath42 , @xmath43 and @xmath44 the frobenius norm , spectral norm ( also called operator norm ) , @xmath45-norm , and elementwise norm of a matrix @xmath40 , defined respectively by @xmath46 , @xmath47 , @xmath48 and @xmath49 .",
    "note that when @xmath40 is a vector , both @xmath41 and @xmath42 are equal to the euclidean norm .",
    "finally , for two sequences , we write @xmath50 if @xmath51 and @xmath52 if @xmath53 and @xmath54",
    "there are three main objectives of this paper : ( i ) understand the relationship between principal component analysis ( pca ) and the high - dimensional factor analysis ; ( ii ) estimate both covariance matrices @xmath0 and the idiosyncratic @xmath25 and their precision matrices in the presence of common factors , and ( iii ) investigate the impact of estimating the unknown factors on the covariance estimation . the propositions in section [ s2.1 ] below show that the space spanned by the principal components in the population level @xmath0 is close to the space spanned by the columns of the factor loading matrix @xmath55 .",
    "consider a factor model @xmath56 where the number of common factors , @xmath57 , is small compared to @xmath10 and @xmath11 , and thus is assumed to be fixed throughout the paper . in the model , the only observable variable is the data @xmath2 .",
    "one of the distinguished features of the factor model is that the principal eigenvalues of @xmath0 are no longer bounded , but growing fast with the dimensionality .",
    "we illustrate this in the following example .",
    "consider a single - factor model @xmath58 where @xmath59 suppose that the factor is pervasive in the sense that it has non - negligible impact on a non - vanishing proportion of outcomes .",
    "it is then reasonable to assume @xmath60 for some @xmath61 .",
    "therefore , assuming that @xmath62 , an application of ( [ eq1.3 ] ) yields , @xmath63 for all large @xmath10 , assuming @xmath64 .",
    "we now elucidate why pca can be used for the factor analysis in the presence of spiked eigenvalues .",
    "write @xmath65 as the @xmath66 loading matrix .",
    "note that the linear space spanned by the first @xmath12 principal components of @xmath23 is the same as that spanned by the columns of @xmath55 when @xmath67 is non - degenerate .",
    "thus , we can assume without loss of generality that the columns of @xmath55 are orthogonal and @xmath68 , the identity matrix .",
    "this canonical form corresponds to the identifiability condition in decomposition ( [ eq1.3 ] ) .",
    "let @xmath69 be the columns of @xmath55 , ordered such that @xmath70 is in a non - increasing order .",
    "then , @xmath71 are eigenvectors of the matrix @xmath72 with eigenvalues @xmath73 and the rest zero .",
    "we will impose the pervasiveness assumption that all eigenvalues of the @xmath74 matrix @xmath75 are bounded away from zero , which holds if the factor loadings @xmath76 are independent realizations from a non - degenerate population .",
    "since the non - vanishing eigenvalues of the matrix @xmath72 are the same as those of @xmath77 , from the pervasiveness assumption it follows that @xmath78 are all growing at rate @xmath24 .",
    "let @xmath79 be the eigenvalues of @xmath80 in a descending order and @xmath81 be their corresponding eigenvectors .",
    "then , an application of weyl s eigenvalue theorem ( see the appendix ) yields that    [ prop21 ] assume that the eigenvalues of @xmath75 are bounded away from zero for all large @xmath10 . for the factor model ( [ eq1.3 ] ) with the canonical condition @xmath82 we have @xmath83 in addition , for @xmath84 , @xmath85 .    using proposition  [ prop21 ] and the @xmath86 theorem of davis and kahn ( 1970 , see the appendix )",
    ", we have the following :    [ prop22 ] under the assumptions of proposition [ prop21 ] , if @xmath70 are distinct , then @xmath87    propositions  [ prop21 ] and [ prop22 ] state that pca and factor analysis are approximately the same if @xmath88 .",
    "this is assured through a sparsity condition on @xmath21 , which is frequently measured through @xmath89$.}\\ ] ] the intuition is that , after taking out the common factors , many pairs of the cross - sectional units become weakly correlated .",
    "this generalized notion of sparsity was used in bickel and levina ( 2008 ) and cai and liu ( 2011 ) . under this generalized measure of sparsity",
    ", we have @xmath90 if the noise variances @xmath91 are bounded .",
    "therefore , when @xmath92 , proposition  [ prop21 ] implies that we have distinguished eigenvalues between the principal components @xmath93 and the rest of the components @xmath94 and proposition  [ prop22 ] ensures that the first @xmath12 principal components are approximately the same as the columns of the factor loadings .",
    "the aforementioned sparsity assumption appears reasonable in empirical applications .",
    "boivin and ng ( 2006 ) conducted an empirical study and showed that imposing zero correlation between weakly correlated idiosyncratic components improves forecast .",
    "more recently , phan ( 2012 ) empirically estimated the level of sparsity of the idiosyncratic covariance using the uk market data .",
    "recent developments on random matrix theory , for example , johnstone and lu ( 2009 ) and paul ( 2007 ) , have shown that when @xmath95 is not negligible , the eigenvalues and eigenvectors of @xmath0 might not be consistently estimated from the sample covariance matrix .",
    "a distinguished feature of the covariance considered in this paper is that there are some very spiked eigenvalues . by propositions 2.1 and 2.2 , in the factor model",
    ", the pervasiveness condition @xmath96 implies that the first @xmath12 eigenvalues are growing at a rate @xmath10 .",
    "moreover , when @xmath10 is large , the principal components @xmath97 are close to the normalized vectors @xmath98 when @xmath99 .",
    "this provides the mathematics for using the first @xmath12 principal components as a proxy of the space spanned by the columns of the factor loading matrix @xmath55 .",
    "in addition , due to ( [ e2.4 ] ) , the signals of the first @xmath12 eigenvalues are stronger than those of the spiked covariance model considered by jung and marron ( 2009 ) and birnbaum et al .",
    "therefore , our other conditions for the consistency of principal components at the population level are much weaker than those in the spiked covariance literature . on the other hand , this also shows that , under our setting the pca is a valid approximation to factor analysis only if @xmath26 . the fact that the pca on the sample covariance is inconsistent when",
    "@xmath10 is bounded was also previously demonstrated in the literature ( see e.g. , bai ( 2003 ) ) .    with assumption ( [ e2.4 ] )",
    ", the standard literature on approximate factor models has shown that the pca on the sample covariance matrix @xmath34 can consistently estimate the space spanned by the factor loadings ( e.g. , stock and watson ( 1998 ) , bai ( 2003 ) ) .",
    "our contribution in propositions 2.1 and 2.2 is that we connect the high - dimensional factor model to the principal components , and obtain the consistency of the spectrum in the population level @xmath0 instead of the sample level @xmath34 .",
    "the spectral consistency also enhances the results in chamberlain and rothschild ( 1983 ) .",
    "this provides the rationale behind the consistency results in the factor model literature .",
    "sparsity assumption directly on @xmath0 is inappropriate in many applications due to the presence of common factors .",
    "instead , we propose a nonparametric estimator of @xmath0 based on the principal component analysis .",
    "let @xmath100 be the ordered eigenvalues of the sample covariance matrix @xmath101 and @xmath102 be their corresponding eigenvectors .",
    "then the sample covariance has the following spectral decomposition : @xmath103 where @xmath104 is the principal orthogonal complement , and @xmath12 is the number of diverging eigenvalues of @xmath0 . let us first assume @xmath12 is known .",
    "now we apply thresholding on @xmath105 .",
    "define @xmath106 where @xmath107 is a generalized shrinkage function of antoniadis and fan ( 2001 ) , employed by rothman et al .",
    "( 2009 ) and cai and liu ( 2011 ) , and @xmath108 is an entry - dependent threshold . in particular , the hard - thresholding rule @xmath109 ( bickel and levina , 2008 ) and the constant thresholding parameter @xmath110 are allowed . in practice",
    ", it is more desirable to have @xmath111 be entry - adaptive .",
    "an example of the adaptive thresholding is @xmath112 where @xmath113 is the @xmath114 diagonal element of @xmath105 .",
    "this corresponds to applying the thresholding with parameter @xmath115 to the correlation matrix of @xmath105 .",
    "the estimator of @xmath0 is then defined as : @xmath116 we will call this estimator the principal orthogonal complement thresholding ( poet ) estimator .",
    "it is obtained by thresholding the remaining components of the sample covariance matrix , after taking out the first @xmath12 principal components .",
    "one of the attractiveness of poet is that it is optimization - free , and hence is computationally appealing . ,",
    "@xmath25 , @xmath12 , the factors and loadings . ]    with the choice of @xmath111 in ( [ eq2.3 ] ) and the hard thresholding rule , our estimator encompasses many popular estimators as its specific cases .",
    "when @xmath117 , the estimator is the sample covariance matrix and when @xmath118 , the estimator becomes that based on the strict factor model ( fan , fan , and lv , 2008 ) .",
    "when @xmath119 , our estimator is the same as the thresholding estimator of bickel and levina ( 2008 ) and ( with a more general thresholding function ) rothman et al .",
    "( 2009 ) or the adaptive thresholding estimator of cai and liu ( 2011 ) with a proper choice of @xmath111 .    in practice ,",
    "the number of diverging eigenvalues ( or common factors ) can be estimated based on the sample covariance matrix .",
    "determining @xmath12 in a data - driven way is an important topic , and is well understood in the literature .",
    "we will describe the poet with a data - driven @xmath12 in section [ s2.4 ] .",
    "the poet ( [ eq2.4 ] ) has an equivalent representation using a constrained least squares method .",
    "the least squares method seeks for @xmath120 and @xmath121 such that @xmath122 subject to the normalization @xmath123 the constraints ( [ eq2.11 ] ) correspond to the normalization ( [ eq2.7 ] ) . here",
    "we assume that the mean of each variable @xmath124 has been removed , that is , @xmath125 for all @xmath126 and @xmath127 putting it in a matrix form , the optimization problem can be written as @xmath128 where @xmath129 and @xmath130 .",
    "for each given @xmath131 , the least - squares estimator of @xmath55 is @xmath132 , using the constraint ( [ eq2.11 ] ) on the factors .",
    "substituting this into ( [ eq2.12 ] ) , the objective function now becomes @xmath133 .",
    "$ ] the minimizer is now clear : the columns of @xmath134 are the eigenvectors corresponding to the @xmath12 largest eigenvalues of the @xmath135 matrix @xmath136 and @xmath137 ( see e.g. , stock and watson ( 2002 ) )",
    ".    we will show that under some mild regularity conditions , as @xmath10 and @xmath138 , @xmath139 consistently estimates the true @xmath140 uniformly over @xmath141 and @xmath142 . since @xmath25 is assumed to be sparse , we can construct an estimator of @xmath25 using the adaptive thresholding method by cai and liu ( 2011 ) as follows .",
    "let @xmath143 and @xmath144 for some pre - determined decreasing sequence @xmath145 , and large enough @xmath146 , define the adaptive threshold parameter as @xmath147 the estimated idiosyncratic covariance estimator is then given by @xmath148 where for all @xmath149 ( see antoniadis and fan , 2001 ) , @xmath150 it is easy to verify that @xmath107 includes many interesting thresholding functions such as the hard thresholding ( @xmath151 ) , soft thresholding ( @xmath152 ) , scad , and adaptive lasso ( see rothman et al .",
    "( 2009 ) ) .",
    "analogous to the decomposition ( [ eq1.3 ] ) , we obtain the following substitution estimators @xmath153 and by the sherman - morrison - woodbury formula , noting that @xmath154 @xmath155^{-1}{\\widehat { \\mbox{\\boldmath $ \\lambda$}}}_k'({\\widehat{\\mathbf{\\sigma}}}_{u , k}^{\\mathcal{t}})^{-1},\\ ] ]    in practice , the true number of factors @xmath12 might be unknown to us .",
    "however , for any determined @xmath156 , we can always construct either @xmath157 as in ( [ eq2.4 ] ) or @xmath158 as in ( [ eq2.14 ] ) to estimate @xmath159 .",
    "the following theorem shows that for each given @xmath160 , the two estimators based on either regularized pca or least squares substitution are equivalent .",
    "similar results were obtained by bai ( 2003 ) when @xmath161 and no thresholding was imposed .",
    "[ thm2.1 ] suppose that the entry - dependent threshold in ( [ eq2.2 ] ) is the same as the thresholding parameter used in ( [ eq2.13 ] ) .",
    "then for any @xmath156 , the estimator ( [ eq2.4 ] ) is equivalent to the substitution estimator ( [ eq2.14 ] ) , that is , @xmath162    in this paper , we will use a data - driven @xmath163 to construct the poet ( see section 2.4 below ) , which has two equivalent representations according to theorem [ thm2.1 ] .      determining the number of factors in a data - driven way has been an important research topic in the econometric literature .",
    "bai and ng ( 2002 ) proposed a consistent estimator as both @xmath10 and @xmath11 diverge .",
    "other recent criteria are proposed by kapetanios ( 2010 ) , onatski ( 2010 ) , alessi et al .",
    "( 2010 ) , etc .",
    "our method also allows a data - driven @xmath163 to estimate the covariance matrices . in principle , any procedure that gives a consistent estimate of @xmath12 can be adopted . in this paper",
    "we apply the well - known method in bai and ng ( 2002 ) .",
    "it estimates @xmath12 by @xmath164 where @xmath165 is a prescribed upper bound , @xmath166 is a @xmath167 matrix whose columns are @xmath168 times the eigenvectors corresponding to the @xmath160 largest eigenvalues of the @xmath135 matrix @xmath169 ; @xmath170 is a penalty function of @xmath171 such that @xmath172 and @xmath173 two examples suggested by bai and ng ( 2002 ) are @xmath174 @xmath175    throughout the paper , we let @xmath163 be the solution to ( [ eq2.16add ] ) using either ic1 or ic2 .",
    "the asymptotic results are not affected regardless of the specific choice of @xmath170 .",
    "we define the poet estimator with unknown @xmath12 as @xmath176 the procedure is as stated in section [ s2.2 ] except that @xmath177 is now data - driven .",
    "this section presents the assumptions on the model ( [ eq1.2 ] ) , in which only @xmath178 are observable .",
    "recall the identifiability condition ( [ eq2.7 ] ) .",
    "the first assumption has been one of the most essential ones in the literature of approximate factor models . under this assumption and other regularity conditions ,",
    "the number of factors , loadings and common factors can be consistently estimated ( e.g. , stock and watson ( 1998 , 2002 ) , bai and ng ( 2002 ) , bai ( 2003 ) , etc . ) .",
    "[ a35 ] all the eigenvalues of the @xmath74 matrix @xmath179 are bounded away from both zero and infinity as @xmath26 .    1 .",
    "it implies from proposition 2.1 in section 2 that the first @xmath12 eigenvalues of @xmath0 grow at rate @xmath24 .",
    "this unique feature distinguishes our work from most of other low - rank plus sparse covariances considered in the literature , e.g. , luo ( 2011 ) , pati et al .",
    "( 2012 ) , agarwal et al .",
    "( 2012 ) , birnbaum et al .",
    "( 2012 ) . )",
    "are fan et al .",
    "( 2008 , 2011 ) and bai and shi ( 2011 ) . while fan et al .",
    "( 2008 , 2011 ) assumed the factors are observable , bai and shi ( 2011 ) considered the strict factor model in which @xmath25 is diagonal . ]",
    "assumption 3.1 requires the factors to be pervasive , that is , to impact a non - vanishing proportion of individual time series .",
    "see example 2.1 for its meaning .",
    "is sparse the intuition of a sparse loading matrix is that each factor is related to only a relatively small number of stocks , assets , genes , etc . with @xmath55 being sparse",
    ", all the eigenvalues of @xmath77 and hence those of @xmath0 are bounded . ]",
    "3 .   as to be illustrated in section 3.3 below , due to the fast diverging eigenvalues",
    ", one can hardly achieve a good rate of convergence for estimating @xmath0 under either the spectral norm or frobenius norm when @xmath180 .",
    "this phenomenon arises naturally from the characteristics of the high - dimensional factor model , which is another distinguished feature compared to those convergence results in the existing literature .",
    "[ a21 ] ( i ) @xmath181 is strictly stationary .",
    "in addition , @xmath182 for all @xmath126 and @xmath127 + ( ii ) there exist constants @xmath183 such that @xmath184 , @xmath185 and @xmath186 + ( iii ) there exist @xmath187 and @xmath188 , such that for any @xmath189 , @xmath141 and @xmath84 , @xmath190    condition ( i ) requires strict stationarity as well as the non - correlation between @xmath191 and @xmath32 .",
    "these conditions are slightly stronger than those in the literature , e.g. , bai ( 2003 ) , but are still standard and simplify our technicalities .",
    "condition ( ii ) requires that @xmath25 be well - conditioned .",
    "the condition @xmath192 instead of a weaker condition @xmath193 is imposed here in order to consistently estimate @xmath12 .",
    "but it is still standard in the approximate factor model literature as in bai and ng ( 2002 ) , bai ( 2003 ) , etc .",
    "when @xmath12 is known , such a condition can be removed .",
    "our working paper shows that the results continue to hold for a growing ( known ) @xmath12 under the weaker condition @xmath193 . condition ( iii ) requires exponential - type tails , which allows us to apply the large deviation theory to @xmath194 and @xmath195 .",
    "we impose the strong mixing condition .",
    "let @xmath196 and @xmath197 denote the @xmath198-algebras generated by @xmath199 and @xmath200 respectively .",
    "in addition , define the mixing coefficient @xmath201    [ a32 ] strong mixing : there exists @xmath202 such that @xmath203 , and @xmath146 satisfying : for all @xmath204 , @xmath205    in addition , we impose the following regularity conditions .",
    "[ a33 ] there exists @xmath206 such that for all @xmath141 , @xmath142 and @xmath207 , + ( i ) @xmath208 , + ( ii ) @xmath209 ^ 4<m$ ] , + ( iii ) @xmath210 .",
    "these conditions are needed to consistently estimate the transformed common factors as well as the factor loadings .",
    "similar conditions were also assumed in bai ( 2003 ) , and bai and ng ( 2006 ) .",
    "the number of factors is assumed to be fixed .",
    "our conditions in assumption [ a33 ] are weaker than those in bai ( 2003 ) as we focus on different aspects of the study .      estimating the covariance matrix @xmath25 of the idiosyncratic components @xmath211 is important for many statistical inferences .",
    "for example , it is needed for large sample inference of the unknown factors and their loadings , for testing the capital asset pricing model ( sentana , 2009 ) , and large - scale hypothesis testing ( fan , han and gu , 2012 ) . see section 5 .",
    "we estimate @xmath25 by thresholding the principal orthogonal complements after the first @xmath163 principal components of the sample covariance are taken out : @xmath212 by theorem [ thm2.1 ] , it also has an equivalent expression given by ( [ eq2.13 ] ) , with @xmath213 .    throughout the paper ,",
    "we apply the adaptive threshold @xmath214 where @xmath146 is a sufficiently large constant , though the results hold for other types of thresholding . as in bickel and levina ( 2008 ) and",
    "cai and liu ( 2011 ) , the threshold chosen in the current paper is in fact obtained from the optimal uniform rate of convergence of @xmath215 when direct observation of @xmath9 is not available , the effect of estimating the unknown factors also contributes to this uniform estimation error , which is why @xmath216 appears in the threshold .",
    "the following theorem gives the rate of convergence of the estimated idiosyncratic covariance .",
    "let @xmath217 . in the convergence rate below , recall that @xmath218 and @xmath219 are defined in the measure of sparsity ( [ eq2.5 ] ) .",
    "[ thm31 ] suppose @xmath220 , @xmath221 , and assumptions [ a35]-[a33 ] hold . then for a sufficiently large constant @xmath146 in the threshold ( [ eq3.3 ] ) , the poet estimator @xmath222 satisfies @xmath223 if further @xmath224 , then the eigenvalues of @xmath222 are all bounded away from zero with probability approaching one , and @xmath225    when estimating @xmath25 , @xmath10 is allowed to grow exponentially fast in @xmath11 , and @xmath222 can be made consistent under the spectral norm .",
    "in addition , @xmath222 is asymptotically invertible while the classical sample covariance matrix based on the residuals is not when @xmath226    [ rem3.3 ]    1 .",
    "consistent estimation of @xmath25 indicates that @xmath25 is identifiable in ( [ eq1.3 ] ) , namely , the sparse @xmath25 can be separated perfectly from the low - rank matrix there .",
    "the result here gives another proof ( when assuming @xmath227 ) of the  surprising phenomenon \" in cands et al ( 2011 ) under different technical conditions .",
    "2 .   fan , liao and mincheva ( 2011 ) recently showed that when @xmath228 are observable and @xmath30 , the rate of convergence of the adaptive thresholding estimator is given by @xmath229 hence when the common factors are unobservable , the rate of convergence has an additional term @xmath230 , coming from the impact of estimating the unknown factors .",
    "this impact vanishes when @xmath231 , in which case the minimax rate as in cai and zhou ( 2010 ) is achieved .",
    "as @xmath10 increases , more information about the common factors is collected , which results in more accurate estimation of the common factors @xmath228 .",
    "3 .   when @xmath12 is known and grows with @xmath10 and @xmath11 , with slightly weaker assumptions , our working paper ( fan et al .",
    "2011 ) shows that under the exactly sparse case ( that is , @xmath30 ) , the result continues to hold with convergence rate @xmath232 .      since the first @xmath12 eigenvalues of @xmath0 grow with @xmath10 , one can hardly estimate @xmath0 with satisfactory accuracy in the absolute term .",
    "this problem arises not from the limitation of any estimation method , but is due to the nature of the high - dimensional factor model .",
    "we illustrate this using a simple example .",
    "[ exam31 ] consider an ideal case where we know the spectrum except for the first eigenvector of @xmath0 .",
    "let @xmath233 be the eigenvalues and vectors , and assume that the largest eigenvalue @xmath234 for some @xmath61 .",
    "let @xmath235 be the estimated first eigenvector and define the covariance estimator @xmath236 assume that @xmath235 is a good estimator in the sense that @xmath237 .",
    "however , @xmath238 which can diverge when @xmath239 .",
    "@xmath240    in the presence of very spiked eigenvalues , while the covariance @xmath0 can not be consistently estimated in absolute term , it can be well estimated in terms of the _ relative error _",
    "matrix @xmath241 which is more relevant for many applications ( see example 5.2 ) .",
    "the relative error matrix can be measured by either its spectral norm or the normalized frobenius norm defined by @xmath242\\right)^{1/2}.\\ ] ] in the last equality , there are @xmath10 terms being added in the trace operation and the factor @xmath243 plays the role of normalization . the loss ( [ eq.fan ] ) is closely related to the entropy loss , introduced by james and stein ( 1961 ) .",
    "also note that @xmath244 where @xmath245 is the weighted quadratic norm in fan et al ( 2008 ) .",
    "fan et al .",
    "( 2008 ) showed that in a large factor model , the sample covariance is such that @xmath246 which does not converge if @xmath180 . on the other hand ,",
    "theorem [ thm32 ] below shows that @xmath247 can still be convergent as long as @xmath248 . technically , the impact of high - dimensionality on the convergence rate of @xmath249 is via the number of rows in @xmath55 .",
    "we show in the appendix that @xmath55 appears in @xmath247 through @xmath250 whose eigenvalues are bounded .",
    "therefore it successfully cancels out the curse of high - dimensionality introduced by @xmath55 .",
    "compared to estimating @xmath0 , in a large approximate factor model , we can estimate the precision matrix with a satisfactory rate under the spectral norm .",
    "the intuition follows from the fact that @xmath37 has bounded eigenvalues .",
    "the following theorem summarizes the rate of convergence under various norms .",
    "[ thm32 ] under the assumptions of theorem [ thm31 ] , the poet estimator defined in ( [ eq2.16 ] ) satisfies @xmath251 in addition , if @xmath252 , then @xmath253 is nonsingular with probability approaching one , with @xmath254    1 .   when estimating @xmath37 , @xmath10 is allowed to grow exponentially fast in @xmath11 , and the estimator has the same rate of convergence as that of the estimator @xmath222 in theorem  [ thm31 ] .",
    "when @xmath10 becomes much larger than @xmath11 , the precision matrix can be estimated at the same rate as if the factors were observable .",
    "2 .   as in remark",
    "[ rem3.3 ] , when @xmath255 is known and grows with @xmath10 and @xmath11 , the working paper fan et al .",
    "( 2011 ) proves the following results ( when @xmath30 ) : instead of @xmath256 be bounded . ]",
    "@xmath257 the results state explicitly the dependence of the rate of convergence on the number of factors . 3 .",
    "the relative error @xmath258 in operator norm can be shown to have the same order as the maximum relative error of estimated eigenvalues .",
    "it does not converge to zero nor diverge .",
    "it is much smaller than @xmath259 , which is of order @xmath260 ( see example  [ exam31 ] ) .",
    "many applications of the factor model require estimating the unknown factors . in general , factor loadings in @xmath261 and the common factors",
    "@xmath262 are not separably identifiable , as for any matrix @xmath263 such that @xmath264 , @xmath265 . hence @xmath266",
    "can not be identified from @xmath267 .",
    "note that the linear space spanned by the rows of @xmath55 is the same as that by those of @xmath268 .",
    "in practice , it often does not matter which one is used .",
    "let @xmath269 denote the @xmath270 diagonal matrix of the first @xmath163 largest eigenvalues of the sample covariance matrix in decreasing order . recall that @xmath271 and define a @xmath270 matrix @xmath272 then for @xmath142 , @xmath273 note that @xmath274 depends only on the data @xmath275 and an identifiable part of parameters @xmath276 .",
    "therefore , there is no identifiability issue in @xmath274 regardless of the imposed identifiability condition .",
    "bai ( 2003 ) obtained the rate of convergence for both @xmath277 and @xmath278 for any fixed @xmath279 .",
    "however , the uniform rate of convergence is more relevant for many applications ( see example 5.1 ) .",
    "the following theorem extends those results in bai ( 2003 ) in a uniformity sense .",
    "in particular , with a more refined technique , we have improved the uniform convergence rate for @xmath278 .",
    "[ thm33 ] under the assumptions of theorem [ thm31 ] , @xmath280    as a consequence of theorem  [ thm33 ] , we obtain the following : ( recall that the constant @xmath281 is defined in assumption [ a21 ] . )    [ c31 ] under the assumptions of theorem [ thm31 ] , @xmath282",
    "the rates of convergence obtained above also explain the condition @xmath283 in theorems [ thm31 ] and [ thm32 ] .",
    "it is needed in order to estimate the common factors @xmath228 uniformly in @xmath142 .",
    "when we do not observe @xmath228 , in addition to the factor loadings , there are @xmath284 factors to estimate .",
    "intuitively , the condition @xmath283 requires the number of parameters introduced by the unknown factors be  not too many \" , so that we can consistently estimate them uniformly .",
    "technically , as demonstrated by bickel and levina ( 2008 ) , cai and liu ( 2011 ) and many other authors , achieving uniform accuracy is essential for large covariance estimations .",
    "recall that the threshold value @xmath285 , where @xmath286 is determined by the users . to make poet operational in practice",
    ", one has to choose @xmath286 to maintain the positive definiteness of the estimated covariances for any given finite sample .",
    "we write @xmath287 , where the covariance estimator depends on @xmath286 via the threshold .",
    "we choose @xmath286 in the range where @xmath288 .",
    "define @xmath289 when @xmath286 is sufficiently large , the estimator becomes diagonal , while its minimum eigenvalue must retain strictly positive .",
    "thus , @xmath290 is well defined and for all @xmath291 , @xmath292 is positive definite under finite sample .",
    "we can obtain @xmath290 by solving @xmath293 we can also approximate @xmath290 by plotting @xmath294 as a function of @xmath286 , as illustrated in figure [ mineig ] . in practice , we can choose @xmath286 in the range @xmath295 for a small @xmath296 and large enough @xmath297 choosing the threshold in a range to guarantee the finite - sample positive definiteness has also been previously suggested by fryzlewicz ( 2012 ) .     as a function of @xmath286 for three choices of thresholding rules .",
    "the plot is based on the simulated data set in section 6.2.,title=\"fig:\",width=302 ] [ mineig ]      in practice , @xmath286 can be data - driven , and chosen through multifold cross - validation . after obtaining the estimated residuals @xmath298 by the pca , we divide them randomly into two subsets , which are , for simplicity , denoted by @xmath299 and @xmath300 .",
    "the sizes of @xmath301 and @xmath302 , denoted by @xmath303 and @xmath304 , are @xmath305 and @xmath306 for example , in sparse matrix estimation , bickel and levina ( 2008 ) suggested to choose @xmath307 .",
    "we repeat this procedure @xmath308 times . at the @xmath309th split ,",
    "we denote by @xmath310 the poet estimator with the threshold @xmath311 on the training data set @xmath312 we also denote by @xmath313 the sample covariance based on the validation set , defined by @xmath314 then we choose the constant @xmath315 by minimizing a cross - validation objective function over a compact interval @xmath316 here @xmath290 is the minimum constant that guarantees the positive definiteness of @xmath292 for @xmath291 as described in the previous subsection , and @xmath165 is a large constant such that @xmath317 is diagonal .",
    "the resulting @xmath315 is data - driven , so depends on @xmath318 as well as @xmath10 and @xmath11 via the data . on the other hand , for each given @xmath319 data matrix @xmath318 , @xmath315 is a universal constant in the threshold @xmath320 in the sense that it does not change with respect to the position @xmath321 .",
    "we also note that the cross - validation is based on the estimate of @xmath25 rather than @xmath0 because poet thresholds the error covariance matrix .",
    "thus cross - validation improves the performance of thresholding .",
    "it is possible to derive the rate of convergence for @xmath322 under the current model setting , but it ought to be much more technically involved than the regular sparse matrix estimation considered by bickel and levina ( 2008 ) and cai and liu ( 2011 ) . to keep our presentation simple",
    "we do not pursue it in the current paper .",
    "we give four examples to which the results in theorems [ thm31][thm33 ] can be applied .",
    "detailed pursuits of these are beyond the scope of the paper .",
    "[ large - scale hypothesis testing ] controlling the false discovery rate in large - scale hypothesis testing based on correlated test statistics is an important and challenging problem in statistics ( leek and storey , 2008 ; efron , 2010 ; fan , et al . , 2012 ) .",
    "suppose that the test statistic for each of the hypothesis @xmath323 is @xmath324 and these test statistics @xmath325 are jointly normal @xmath326 where @xmath80 is unknown .",
    "for a given critical value @xmath327 , the false discovery proportion is then defined as @xmath328 where @xmath329 and @xmath330 are the total number of false discoveries and the total number of discoveries , respectively .",
    "our interest is to estimate @xmath331 for each given @xmath327 .",
    "note that @xmath332 is an observable quantity .",
    "only @xmath333 needs to be estimated .",
    "if the covariance @xmath80 admits the approximate factor structure ( [ eq1.3 ] ) , then the test statistics can be stochastically decomposed as @xmath334 by the principal factor approximation ( theorem 1 , fan , han , gu , 2012 ) @xmath335 when @xmath99 and the number of true significant hypothesis @xmath336 is @xmath337 , where @xmath338 is the upper @xmath327-quantile of the standard normal distribution , @xmath339 and @xmath340 .",
    "now suppose that we have @xmath341 repeated measurements from the model ( [ eq4.1 ] ) .",
    "then , by corollary  [ c31 ] , @xmath342 can be uniformly consistently estimated , and hence @xmath343 and @xmath331 can be consistently estimated .",
    "efron ( 2010 ) obtained these repeated test statistics based on the bootstrap sample from the original raw data .",
    "our theory ( theorem  [ thm33 ] ) gives a formal justification to the framework of efron ( 2007 , 2010 ) .",
    "the maximum elementwise estimation error @xmath344 appears in risk assessment as in fan , zhang and yu ( 2012 ) . for a fixed portfolio allocation vector @xmath345 , the true portfolio variance and the estimated one",
    "are given by @xmath346 and @xmath347 respectively .",
    "the estimation error is bounded by @xmath348 where @xmath349 , the @xmath45-norm of @xmath345 , is the gross exposure of the portfolio .",
    "usually a constraint is placed on the total percentage of the short positions , in which case we have a restriction @xmath350 for some @xmath351 in particular , @xmath352 corresponds to a portfolio with no - short positions ( all weights are nonnegative ) .",
    "theorem  [ thm32 ] quantifies the maximum approximation error .",
    "the above compares the absolute error of perceived risk and true risk .",
    "the relative error is bounded by @xmath353 for any allocation vector @xmath345 .",
    "theorem  [ thm32 ] quantifies this relative error .",
    "consider the following panel regression model @xmath354 where @xmath355 is a vector of observable regressors with fixed dimension .",
    "the regression error @xmath356 has a factor structure and is assumed to be independent of @xmath355 , but @xmath6 , @xmath7 and @xmath9 are all unobservable .",
    "we are interested in the common regression coefficients @xmath357 .",
    "the above panel regression model has been considered by many researchers , such as ahn , lee and schmidt ( 2001 ) , pesaran ( 2006 ) , and has broad applications in social sciences .",
    "although ols ( ordinary least squares ) produces a consistent estimator of @xmath357 , a more efficient estimation can be obtained by gls ( generalized least squares ) .",
    "the gls method depends , however , on an estimator of @xmath358 , the inverse of the covariance matrix of @xmath359 . by assuming the covariance matrix of @xmath360 to be sparse",
    ", we can successfully solve this problem by applying theorem [ thm32 ] .",
    "although @xmath356 is unobservable , it can be replaced by the regression residuals @xmath361 , obtained via first regressing @xmath362 on @xmath355 .",
    "we then apply the poet estimator to @xmath363 . by theorem [ thm32 ] ,",
    "the inverse of the resulting estimator is a consistent estimator of @xmath358 under the spectral norm .",
    "a slight difference lies in the fact that when we apply poet , @xmath364 is replaced with @xmath363 , which introduces an additional term @xmath365 in the estimation error .",
    "a celebrated financial economic theory is the capital asset pricing model ( capm , sharpe 1964 ) that makes william sharpe win the nobel prize in economics in 1990 , whose extension is the multi - factor model ( ross , 1976 , chamberlain and rothschild , 1983 ) .",
    "it states that in a frictionless market , the excessive return of any financial asset equals the excessive returns of the risk factors times its factor loadings plus noises . in the multi - period model",
    ", the excess return @xmath2 of firm @xmath3 at time @xmath366 follows model ( [ eq1.1 ] ) , in which @xmath7 is the excess returns of the risk factors at time @xmath366 . to test the null hypothesis ( [ eq1.2 ] ) , one embeds the model into the multivariate linear model @xmath367 and wishes to test @xmath368 .",
    "the f - test statistic involves the estimation of the covariance matrix @xmath25 , whose estimates are degenerate without regularization when @xmath369 .",
    "therefore , in the literature ( sentana , 2009 , and references therein ) , one focuses on the case @xmath10 is relatively small .",
    "the typical choices of parameters are @xmath370 monthly data and the number of assets @xmath371 , 10 or 25 .",
    "however , the capm should hold for all tradeable assets , not just a small fraction of assets . with our regularization technique ,",
    "non - degenerate estimate @xmath222 can be obtained and the f - test or likelihood - ratio test statistics can be employed even when @xmath372 .    to provide some insights ,",
    "let @xmath373 be the least - squares estimator of ( [ eq4.3 ] ) . then , when @xmath374 , @xmath375 for a constant @xmath376 which depends on the observed factors .",
    "when @xmath25 is known , the wald test statistic is @xmath377 .",
    "when it is unknown and @xmath10 is large , it is natural to use the f - type of test statistic @xmath378 .",
    "the difference between these two statistics is bounded by @xmath379 since under the null hypothesis @xmath380 , we have @xmath381 .",
    "thus , it follows from boundness of @xmath382 that @xmath383 theorem 3.1 provides the rate of convergence for the above difference .",
    "detailed development is out of the scope of the current paper , and we will leave it as a separate research project .",
    "in this section , we will examine the performance of the poet method in a finite sample .",
    "we will also demonstrate the effect of this estimator on the asset allocation and risk assessment . similarly to fan , et al .",
    "( 2008 , 2011 ) , we simulated from a standard fama - french three - factor model , assuming a sparse error covariance matrix and three factors . throughout this section ,",
    "the time span is fixed at @xmath384 , and the dimensionality @xmath10 increases from @xmath385 to @xmath386 .",
    "we assume that the excess returns of each of @xmath10 stocks over the risk - free interest rate follow the following model : @xmath387 the factor loadings are drawn from a trivariate normal distribution @xmath388 , the idiosyncratic errors from @xmath389 , and the factor returns @xmath7 follow a var(1 ) model . to make the simulation more realistic ,",
    "model parameters are calibrated from the financial returns , as detailed in the following section .      to calibrate the model",
    ", we use the data on annualized returns of 100 industrial portfolios from the website of kenneth french , and the data on 3-month treasury bill rates from the crsp database .",
    "these industrial portfolios are formed as the intersection of @xmath390 portfolios based on size ( market equity ) and @xmath390 portfolios based on book equity to market equity ratio .",
    "their excess returns @xmath391 are computed for the period from january @xmath392 , 2009 to december @xmath393 , 2010 . here ,",
    "we present a short outline of the calibration procedure .    1 .   given @xmath394 as the input data",
    ", we fit a fama - french - three - factor model and calculate a @xmath395 matrix @xmath396 , and @xmath397 matrix @xmath398 , using the principal components method described in section 3.1 . 2 .",
    "we summarize 100 factor loadings ( the rows of @xmath396 ) by their sample mean vector @xmath399 and sample covariance matrix @xmath400 , which are reported in table 1 .",
    "the factor loadings @xmath401 for @xmath4 are drawn from @xmath402 .",
    "+ .mean and covariance matrix used to generate @xmath403 [ cols=\"^,^,^,^\",options=\"header \" , ]",
    "we study the problem of estimating a high - dimensional covariance matrix with conditional sparsity . realizing unconditional sparsity assumption is inappropriate in many applications , we introduce a latent factor model that has a conditional sparsity feature , and propose the poet estimator to take advantage of the structure .",
    "this expands considerably the scope of the model based on the strict factor model , which assumes independent idiosyncratic noise and is too restrictive in practice . by assuming sparse error covariance matrix , we allow for the presence of the cross - sectional correlation even after taking out the common factors",
    "the sparse covariance is estimated by the adaptive thresholding technique .",
    "it is found that the rates of convergence of the estimators have an extra term approximately @xmath404 in addition to the results based on observable factors by fan et al .",
    "( 2008 , 2011 ) , which arises from the effect of estimating the unobservable factors . as we can see",
    ", this effect vanishes as the dimensionality increases , as more information about the common factors becomes available . when @xmath10 gets large enough , the effect of estimating the unknown factors is negligible , and we estimate the covariance matrices as if we knew the factors .",
    "the proposed poet also has wide applicability in statistical genomics .",
    "for example , carvalho et al . (",
    "2008 ) applied a bayesian sparse factor model to study the breast cancer hormonal pathways .",
    "their real - data results have identified about two common factors that have highly loaded genes ( about half of 250 genes ) . as a result",
    ", these factors should be treated as  pervasive \" ( see the explanation in example 2.1 ) , which will result in one or two very spiked eigenvalues of the gene expressions covariance matrix .",
    "the poet can be applied to estimate such a covariance matrix and its network model .",
    "* appendix *",
    "we estimate @xmath25 by applying the adaptive thresholding given by ( [ eq2.13 ] ) . however , the task here is slightly different from the standard problem of estimating a sparse covariance matrix in the literature , as no direct observations for @xmath33 are available . in many cases",
    "the original data are contaminated , including any type of estimate of the data when direct observations are not available .",
    "this typically happens when @xmath33 represent the error terms in regression models or when data is subject to measurement of errors .",
    "instead , we may observe @xmath405 .",
    "for instance , in the approximate factor models , @xmath406    we can estimate @xmath25 using the adaptive thresholding proposed by cai and liu ( 2011 ) : for the threshold @xmath407 define @xmath408 @xmath409 where @xmath410 satisfies : for all @xmath149 , @xmath411 @xmath412    when @xmath405 is close enough to @xmath33 , we can show that @xmath413 is also consistent .",
    "the following theorem extends the standard thresholding results in bickel and levina ( 2008 ) and cai and liu ( 2011 ) to the case when no direct observations are available , or the original data are contaminated . for the tail and mixing parameters @xmath414 and @xmath415 defined in assumptions [ a21 ] and [ a32 ] , let @xmath416 .",
    "[ tb1 ] suppose @xmath417 , and assumptions [ a21 ] and [ a32 ] hold .",
    "in addition , suppose there is a sequence @xmath418 so that @xmath419 and @xmath420 then there is a constant @xmath146 in the adaptive thresholding estimator ( [ eq3.2add ] ) with @xmath421 such that @xmath422 if further @xmath423 , then @xmath424 is invertible with probability approaching one , and @xmath425    by assumptions [ a21 ] and [ a32 ] , the conditions of lemmas a.3 and a.4 of fan , liao and mincheva ( 2011 , _ ann .",
    ", * 39 * , 3320 - 3356 ) are satisfied .",
    "hence for any @xmath426 , there are positive constants @xmath427 and @xmath428 such that each of the events @xmath429 occurs with probability at least @xmath430 . by the condition of threshold function , @xmath431 .",
    "now for @xmath432 under the event @xmath433 @xmath434 let @xmath435 then with probability at least @xmath436 , @xmath437 since @xmath296 is arbitrary , we have @xmath438 .",
    "if in addition , @xmath423 , then the minimum eigenvalue of @xmath424 is bounded away from zero with probability approaching one since @xmath184 .",
    "this then implies @xmath439",
    "we first cite two useful theorems , which are needed to prove propositions 2.1 and 2.2 . in lemma [ lb.1 ] below , let @xmath440 be the eigenvalues of @xmath80 in descending order and @xmath441 be their associated eigenvectors .",
    "correspondingly , let @xmath442 be the eigenvalues of @xmath443 in descending order and @xmath102 be their associated eigenvectors .",
    "[ lb.1 ]    1 .   *",
    "( weyl s theorem ) * @xmath444 .",
    "( @xmath86 theorem , davis and kahan , 1970 ) * @xmath445    * proof of proposition 2.1 *    since @xmath79 are the eigenvalue of @xmath0 and @xmath78 are the first @xmath12 eigenvalues of @xmath72 ( the remaining @xmath446 eigenvalues are zero ) , then by the weyl s theorem , for each @xmath84 , @xmath447 for @xmath448 , @xmath449 on the other hand , the first @xmath12 eigenvalues of @xmath450 are also the eigenvalues of @xmath77 . by the assumption , the eigenvalues of @xmath75 are bounded away from zero .",
    "thus when @xmath84 , @xmath451 are bounded away from zero for all large @xmath452    * proof of proposition 2.2 *    applying the @xmath453 theorem yields @xmath454 for a generic constant @xmath61 , @xmath455 for all large @xmath10 , since @xmath456 but @xmath457 is bounded by prosposition 2.1 . on the other hand , if @xmath458 , the same argument implies @xmath459 . if @xmath460 , @xmath461 , where @xmath462 is bounded away from zero , but @xmath463 .",
    "hence again , @xmath464    * proof of theorem [ thm2.1 ] *    the sample covariance matrix of the residuals using least squares method is given by @xmath465 where we used the normalization condition @xmath466 and @xmath467 if we show that @xmath468 , then from the decompositions of the sample covariance @xmath469 we have @xmath470 .",
    "consequently , applying thresholding on @xmath471 is equivalent to applying thresholding on @xmath472 , which gives the desired result .",
    "we now show @xmath468 indeed holds .",
    "consider again the least squares problem ( [ eq2.10 ] ) but with the following alternative normalization constraints : @xmath473 and @xmath474 is diagonal .",
    "let @xmath475 be the solution to the new optimization problem .",
    "switching the roles of @xmath55 and @xmath131 , then the solution of ( [ eq2.12 ] ) is @xmath476 and @xmath477 .",
    "in addition , @xmath478 from @xmath479 , it follows that @xmath480",
    "we will proceed by subsequently showing theorems [ thm33 ] , [ thm31 ] and [ thm32 ] .          [",
    "la2 ] suppose that the random variables @xmath487 both satisfy the exponential - type tail condition : there exist @xmath414 , @xmath488 and @xmath188 , such that @xmath489 , @xmath490 then for some @xmath415 and @xmath491 , and any @xmath189 , @xmath492        first of all , by proposition  [ prop21 ] , under assumption [ a35 ] , the @xmath500 largest eigenvalue @xmath501 of @xmath0 satisfies : for some @xmath502 @xmath503 for sufficiently large @xmath10 . using weyl s theorem , we need only to prove that @xmath504 .",
    "without loss of generality , we prove the result under the identifiability condition ( [ eq2.7 ] ) . using model ( [ eq1.2 ] ) ,",
    "@xmath505 using this and ( [ eq1.3 ] ) , @xmath506 can be decomposed as the sum of the four terms : @xmath507 we now deal them term by term .",
    "we will repeatedly use the fact that for a @xmath18 matrix @xmath40 , @xmath508 first of all , by lemma [ lb4 ] , @xmath509 which is @xmath510 if @xmath511 .",
    "consequently , by assumption [ a35 ] , we have @xmath512 we now deal with @xmath513 .",
    "it follows from lemma [ lb4 ] that @xmath514 since @xmath515 , it remains to deal with @xmath516 , which is bounded by @xmath517 which is @xmath510 since @xmath518 .",
    "since @xmath33 is weakly stationary , @xmath520 in addition , @xmath521 for some constant @xmath165 and any @xmath522 since @xmath9 has exponential tail .",
    "hence by davydov s inequality ( corollary 16.2.4 in athreya and lahiri 2006 ) , there is a constant @xmath146 , for all @xmath523 , @xmath524 , where @xmath525 is the @xmath526-mixing coefficient . by assumption [ a32 ]",
    ", @xmath527 thus uniformly in @xmath11 , @xmath528      our derivation below relies on a result obtained by bai and ng ( 2002 ) , which showed that the estimated number of factors is consistent , in the sense that @xmath163 equals the true @xmath36 with probability approaching one . note that under our assumptions [ a35]-[a33 ] , all the assumptions in bai and ng ( 2002 ) are satisfied .",
    "thus immediately we have the following lemma .",
    "\\(i ) when @xmath575 , by lemma [ lb.5 ] , all the eigenvalues of @xmath576 are bounded away from zero . using the inequality @xmath577 and the identity ( [ eb1 ] ) , we have , for some constant @xmath146 , @xmath578 each of the four terms on the right hand side above are bounded in lemma [ lb1 ] , which then yields the desired result .",
    "we first condition on @xmath575 .",
    "( i ) lemma [ lb.5 ] implies @xmath582 .",
    "also @xmath583 in addition , @xmath584 .",
    "it then follows from the definition of @xmath263 that @xmath585 .",
    "define @xmath586 applying the triangular inequality gives : @xmath587 by lemma [ lb4 ] , the first term in ( [ ec.6 ] ) is @xmath588 the second term of ( [ ec.6 ] ) can be bounded , by the cauchy - schwarz inequality and lemma [ lb3 ] , as follows : @xmath589 ( ii ) still conditioning on @xmath575 , since @xmath590 and @xmath585 , right multiplying @xmath263 gives @xmath591 . part",
    "( i ) also gives , conditioning on @xmath575 , @xmath592 .",
    "hence further left multiplying @xmath593 yields @xmath594 .",
    "due to @xmath595 , we reach the desired result .        using the facts that @xmath597 , and that @xmath598 , we have @xmath599 we bound the three terms on the right hand side respectively .",
    "it follows from lemmas [ lb4 ] and [ lb5 ] that @xmath600 for the second term , @xmath601",
    ". therefore , @xmath602 .",
    "the cauchy - schwarz inequality and lemma [ lb3 ] imply @xmath603 finally , @xmath604 and @xmath605 imply that the third term is @xmath606            we have , @xmath611 therefore , using the inequality @xmath612 , we have : @xmath613 the first part of the lemma then follows from theorem [ thm33 ] and lemma [ lb3 ] .",
    "the second part follows from corollary [ c31 ] .",
    "\\(i ) we have @xmath620 moreover , since all the eigenvalues of @xmath0 are bounded away from zero , for any matrix @xmath40 , @xmath621 .",
    "hence @xmath622 + ( ii ) by theorem [ thm31 ] , @xmath623 + ( iii ) the same argument of the proof of theorem 2 in fan , fan and lv ( 2008 ) implies that @xmath624 .",
    "thus , @xmath625 is upper bounded by @xmath626 + ( iv ) again , by @xmath624 , and lemma [ lb5 ] , @xmath627              \\(i ) by lemma [ lb5 ] , with probability approaching one , @xmath637 is bounded away from zero .",
    "hence , @xmath638 ( ii ) the result follows from part ( i ) and lemma [ lc.13 ] .",
    "part ( iii ) and ( iv ) follow from a similar argument of part ( i ) and lemma [ lb5 ] .",
    "we derive the rate for @xmath639 define @xmath640 note that @xmath641 and @xmath642 the triangular inequality gives @xmath643 using the sherman - morrison - woodbury formula , we have @xmath644 where @xmath645^{-1}{\\widehat { \\mbox{\\boldmath $ \\lambda$}}}'({\\widehat{\\mathbf{\\sigma}}}_{u , \\widehat{k}}^{\\mathcal{t}})^{-1}\\| \\cr l_3&=&\\|(({\\widehat{\\mathbf{\\sigma}}}_{u , \\widehat{k}}^{\\mathcal{t}})^{-1}-{\\mathbf{\\sigma}}_u^{-1}){\\widehat { \\mbox{\\boldmath $ \\lambda$}}}[{\\mathrm{\\bf i}}_k+{\\widehat { \\mbox{\\boldmath $ \\lambda$}}}'({\\widehat{\\mathbf{\\sigma}}}_{u , \\widehat{k}}^{\\mathcal{t}})^{-1}{\\widehat { \\mbox{\\boldmath $ \\lambda$}}}]^{-1}{\\widehat { \\mbox{\\boldmath $ \\lambda$}}}'{\\mathbf{\\sigma}}_u^{-1}\\| \\cr l_4&=&\\|{\\mathbf{\\sigma}}_u^{-1}({\\widehat { \\mbox{\\boldmath $ \\lambda$}}}-{{\\mathrm{\\bf b}}}{\\mathrm{\\bf h}}')[{\\mathrm{\\bf i}}_k+{\\widehat { \\mbox{\\boldmath $ \\lambda$}}}'({\\widehat{\\mathbf{\\sigma}}}_{u , \\widehat{k}}^{\\mathcal{t}})^{-1}{\\widehat { \\mbox{\\boldmath $ \\lambda$}}}]^{-1}{\\widehat { \\mbox{\\boldmath $ \\lambda$}}}'{\\mathbf{\\sigma}}_u^{-1}\\| \\cr l_5&=&\\|{\\mathbf{\\sigma}}_u^{-1}({\\widehat { \\mbox{\\boldmath $ \\lambda$}}}-{{\\mathrm{\\bf b}}}{\\mathrm{\\bf h}}')[{\\mathrm{\\bf i}}_k+{\\widehat { \\mbox{\\boldmath $ \\lambda$}}}'({\\widehat{\\mathbf{\\sigma}}}_{u , \\widehat{k}}^{\\mathcal{t}})^{-1}{\\widehat { \\mbox{\\boldmath $ \\lambda$}}}]^{-1}{\\mathrm{\\bf h}}{{\\mathrm{\\bf b}}}'{\\mathbf{\\sigma}}_u^{-1}\\| \\cr l_6&=&\\|{\\mathbf{\\sigma}}_u^{-1}{{\\mathrm{\\bf b}}}{\\mathrm{\\bf h}}'([{\\mathrm{\\bf i}}_k+{\\widehat { \\mbox{\\boldmath $ \\lambda$}}}'({\\widehat{\\mathbf{\\sigma}}}_{u , \\widehat{k}}^{\\mathcal{t}})^{-1}{\\widehat { \\mbox{\\boldmath $ \\lambda$}}}]^{-1}-[{\\mathrm{\\bf i}}_k+{\\mathrm{\\bf h}}{{\\mathrm{\\bf b}}}'{\\mathbf{\\sigma}}_u^{-1}{{\\mathrm{\\bf b}}}{\\mathrm{\\bf h}}']^{-1}){\\mathrm{\\bf h}}{{\\mathrm{\\bf b}}}'{\\mathbf{\\sigma}}_u^{-1}\\|.\\end{aligned}\\ ] ]    we bound each of the six terms respectively .",
    "first of all , @xmath45 is bounded by theorem [ thm31 ] .",
    "let @xmath646^{-1}$ ] , then @xmath647 note that theorem [ thm31 ] implies @xmath648 .",
    "lemma [ lc.14 ] then implies @xmath649 this shows that @xmath650 .",
    "similarly @xmath651 .",
    "in addition , since @xmath652 , @xmath653 similarly @xmath654 . finally , let @xmath655^{-1}.$ ] by lemma [ lc.14 ] , @xmath656 .",
    "then by lemma [ lc.13 ] , @xmath657 consequently , @xmath658 adding up @xmath45-@xmath659 gives @xmath660 one the other hand , using sherman - morrison - woodbury formula again implies @xmath661^{-1}-[{\\mathrm{\\bf i}}_k+{{\\mathrm{\\bf b}}}'{\\mathbf{\\sigma}}_u^{-1}{{\\mathrm{\\bf b}}}]^{-1}){{\\mathrm{\\bf b}}}'{\\mathbf{\\sigma}}_u^{-1}\\|\\cr & \\leq & o(p)\\|[({\\mathrm{\\bf h}}'{\\mathrm{\\bf h}})^{-1}+{{\\mathrm{\\bf b}}}'{\\mathbf{\\sigma}}_u^{-1}{{\\mathrm{\\bf b}}}]^{-1}-[{\\mathrm{\\bf i}}_k+{{\\mathrm{\\bf b}}}'{\\mathbf{\\sigma}}_u^{-1}{{\\mathrm{\\bf b}}}]^{-1}\\|\\cr & = & o_p(p^{-1})\\|({\\mathrm{\\bf h}}'{\\mathrm{\\bf h}})^{-1}-{\\mathrm{\\bf i}}_k\\|=o_p(\\omega_t^{1-q}m_p).\\end{aligned}\\ ] ]      we first bound @xmath663 . repeatedly using the triangular inequality",
    "yields @xmath664\\cr & \\leq&(\\max_i\\|{\\widehat { \\mathrm{\\bf b}}}_i-{\\mathrm{\\bf h}}{\\mathrm{\\bf b}}_i\\|)^2 + 2\\max_{ij}\\|{\\widehat { \\mathrm{\\bf b}}}_i-{\\mathrm{\\bf h}}{\\mathrm{\\bf b}}_i\\|\\|{\\mathrm{\\bf h}}{\\mathrm{\\bf b}}_j\\|+\\max_i\\|{\\mathrm{\\bf b}}_i\\|^2\\|{\\mathrm{\\bf h}}'{\\mathrm{\\bf h}}-{\\mathrm{\\bf i}}_k\\|\\cr & = & o_p(\\omega_t).\\end{aligned}\\ ] ] on the other hand , let @xmath665 be the @xmath321 entry of @xmath25",
    ". then @xmath666 .",
    "@xmath667 hence @xmath668 the result then follows immediately .",
    "carvalho , c. , chang , j. , lucas , j. , nevins , j. , wang , q. and west , m. ( 2008 ) .",
    "high - dimensional sparse factor modeling : applications in gene expression genomics . _ j. amer .",
    "assoc . _ * 103 * , 1438 - 1456 .",
    "johnstone , i. m. ( 2001 ) . on the distribution of the largest eigenvalue in principal components analysis .",
    "_ , * 29 * , 295327 .",
    "johnstone , i.m . and",
    "( 2009 ) . on consistency and sparsity for principal components analysis in high dimensions .",
    "_ j. amer .",
    "assoc . _ * 104 * , 682 - 693 .",
    "witten , d.m .",
    ", tibshirani , r. and hastie , t. ( 2009 ) . a penalized matrix decomposition , with applications to sparse principal components and canonical correlation analysis . _ biostatistics _ , * 10 * , 515 - 534 .",
    "wright , j. , peng , y. , m , y. , ganesh , a. and rao , s. ( 2009 ) .",
    "robust principal component analysis : exact recovery of corrupted low - rank matrices by convex optimization .",
    "_ manuscript_. microsoft research asia"
  ],
  "abstract_text": [
    "<S> this paper deals with the estimation of a high - dimensional covariance with a conditional sparsity structure and fast - diverging eigenvalues . by assuming sparse error covariance matrix in an approximate factor model , we allow for the presence of some cross - sectional correlation even after taking out common but unobservable factors . we introduce the principal orthogonal complement thresholding ( poet ) method to explore such an approximate factor structure with sparsity . </S>",
    "<S> the poet estimator includes the sample covariance matrix , the factor - based covariance matrix ( fan , fan , and lv , 2008 ) , the thresholding estimator ( bickel and levina , 2008 ) and the adaptive thresholding estimator ( cai and liu , 2011 ) as specific examples . </S>",
    "<S> we provide mathematical insights when the factor analysis is approximately the same as the principal component analysis for high - dimensional data . </S>",
    "<S> the rates of convergence of the sparse residual covariance matrix and the conditional sparse covariance matrix are studied under various norms . </S>",
    "<S> it is shown that the impact of estimating the unknown factors vanishes as the dimensionality increases . </S>",
    "<S> the uniform rates of convergence for the unobserved factors and their factor loadings are derived . </S>",
    "<S> the asymptotic results are also verified by extensive simulation studies . </S>",
    "<S> finally , a real data application on portfolio allocation is presented .    </S>",
    "<S> * keywords : * high - dimensionality , approximate factor model , unknown factors , principal components , sparse matrix , low - rank matrix , thresholding , cross - sectional correlation , diverging eigenvalues . </S>"
  ]
}