{
  "article_text": [
    "record matching ( or linkage ) , data cleansing and plagiarism detection are among the most frequent operations in many large - scale data processing systems over the web .",
    "_ minwise hashing _ ( or minhash )  @xcite is a popular technique deployed by big data industries for these tasks .",
    "minhash was originally developed for economically estimating the _ resemblance _ similarity between sets ( which can be equivalently viewed as binary vectors ) .",
    "later , because of its locality sensitive property  @xcite , minhash became a widely used hash function for creating hash buckets leading to efficient algorithms for numerous applications including spam detection  @xcite , collaborative filtering  @xcite , news personalization  @xcite , compressing social networks  @xcite , graph sampling  @xcite , record linkage  @xcite , duplicate detection  @xcite , all pair similarity  @xcite , etc .",
    "binary representations for web documents are common , largely due to the wide adoption of the `` bag of words '' ( bow ) representations for documents and images . in bow representations , the word frequencies within a document follow power law .",
    "a significant number of words ( or combinations of words ) occur rarely in a document and most of the higher order shingles in the document occur only once .",
    "it is often the case that just the presence or absence information suffices in practice  @xcite .",
    "leading search companies routinely use sparse binary representations in their large data systems  @xcite .",
    "the underlying similarity measure of interest with minhash is the resemblance , also known as the jaccard similarity .",
    "the resemblance similarity between two sets @xmath0 , @xmath1 is @xmath2 sets can be equivalently viewed as binary vectors with each component indicating the presence or absence of an attribute .",
    "the cardinality ( e.g. , @xmath3 , @xmath4 ) is the number of nonzeros in the binary vector .",
    "+ while the resemblance similarity is convenient and useful in numerous applications , there are also many scenarios where the resemblance is not the desirable similarity measure  @xcite . for instance , consider text descriptions of two restaurants :    a.    five guys burgers and fries brooklyn new york \" b.    five kitchen berkley \"    shingle based representations for strings are common in practice .",
    "typical ( first - order ) shingle based representations of these names will be ( i ) \\{five , guys , burgers , and , fries , brooklyn , new , york } and ( ii ) \\{five , kitchen , berkley}. now suppose the query is  five guys \" which in shingle representation is \\{five , guys}. we would like to match and search the records , for this query  five guys \" , based on resemblance .",
    "observe that the resemblance between query and record ( i ) is @xmath5 = 0.25 , while that with record ( ii ) is @xmath6 = 0.33 .",
    "thus , simply based on resemblance , record ( ii ) is a better match for query  five guys \" than record ( i ) , which should not be correct in this content .",
    "clearly the issue here is that the resemblance penalizes the sizes of the sets involved .",
    "shorter sets are unnecessarily favored over longer ones , which hurts the performance in record matching  @xcite and other applications .",
    "there are many other scenarios where such penalization is undesirable .",
    "for instance , in plagiarism detection , it is typically immaterial whether the text is plagiarized from a big or a small document .    to counter the often unnecessary penalization of the sizes of the sets with resemblance , a modified measure ,",
    "the _ set containment _ ( or jaccard containment ) was adopted  @xcite .",
    "jaccard containment of set @xmath0 and @xmath7 with respect to @xmath0 is defined as @xmath8 in the above example with query  five guys \" the jaccard containment with respect to query for record ( i ) will be @xmath9 and with respect to record ( ii ) it will be @xmath10 , leading to the desired ordering .",
    "it should be noted that for any fixed query @xmath0 , the ordering under jaccard containment with respect to the query , is the same as the ordering with respect to the intersection @xmath11 ( or binary inner product ) .",
    "thus , near neighbor search problem with respect to @xmath12 is equivalent to the near neighbor search problem with respect to @xmath11 .",
    "formally , we state our problem of interest .",
    "we are given a collection @xmath13 containing @xmath14 sets ( or binary vectors ) over universe @xmath15 with @xmath16 ( or binary vectors in @xmath17 ) .",
    "given a query @xmath18 , we are interested in the problem of finding @xmath19 such that @xmath20 where @xmath21 is the cardinality of the set .",
    "this is the so - called _ maximum inner product search ( mips ) _ problem .    for binary data ,",
    "the mips problem is equivalent to searching with jaccard containment with respect to the query , because the cardinality of the query does not affect the ordering and hence the @xmath22 .",
    "@xmath23 which is also referred to as the _ maximum containment search ( mcs ) _ problem .      owing to its practical significance",
    ", there have been many existing heuristics for solving the mips ( or mcs ) problem  @xcite .",
    "a notable recent work among them made use of the inverted index based approach  @xcite .",
    "inverted indexes might be suitable for problems when the sizes of documents are small and each record only contains few words .",
    "this situation , however , is not commonly observed in practice .",
    "the documents over the web are large with huge vocabulary",
    ". moreover , the vocabulary blows up very quickly once we start using higher - order shingles .",
    "in addition , there is an increasing interest in enriching the text with extra synonyms to make the search more effective and robust to semantic meanings  @xcite , at the cost of a significant increase of the sizes of the documents . furthermore ,",
    "if the query contains many words then the inverted index is not very useful . to mitigate this issue several additional heuristics",
    "were proposed , for instance , the heuristic based on minimal infrequent sets  @xcite .",
    "computing minimal infrequent sets is similar to the set cover problem which is hard in general and thus  @xcite resorted to greedy heuristics .",
    "the number of minimal infrequent sets could be huge in general and so these heuristics can be very costly .",
    "also , such heuristics require the knowledge of the entire dataset before hand which is usually not practical in a dynamic environment like the web .",
    "in addition , inverted index based approaches do not have theoretical guarantees on the query time and their performance is very much dataset dependent .",
    "locality sensitive hashing ( lsh )  @xcite based randomized techniques are common and successful in industrial practice for efficiently solving nns ( _ near neighbor search _ ) .",
    "they are some of the few known techniques that do not suffer from the curse of dimensionality .",
    "hashing based indexing schemes provide provably sub - linear algorithms for search which is a boon in this era of big data where even linear search algorithms are impractical due to latency .",
    "hashing based indexing schemes are massively parallelizable and can be updated incrementally ( on data streams ) , which makes them ideal for modern distributed systems .",
    "the prime focus of this paper will be on efficient hashing based algorithms for binary inner products .    despite the interest in jaccard containment and binary inner products",
    ", there were no hashing algorithms for these measures for a long time and minwise hashing is still a widely popular heuristic  @xcite .",
    "very recently , it was shown that general inner products for real vectors can be efficiently solved by using asymmetric locality sensitive hashing schemes  @xcite .",
    "the asymmetry is necessary for the general inner products and an impossibility of having a symmetric hash function can be easily shown using elementary arguments .",
    "thus , binary inner product ( or set intersection ) being a special case of general inner products also admits provable efficient search algorithms with these asymmetric hash functions which are based on random projections .",
    "however , it is known that random projections are suboptimal for retrieval in the sparse binary domain  @xcite .",
    "hence , it is expected that the existing asymmetric locality sensitive hashing schemes for general inner products are likely to be suboptimal for retrieving with sparse high dimensional binary - like datasets , which are common over the web .",
    "we investigate hashing based indexing schemes for the problem of near neighbor search with binary inner products and jaccard containment .",
    "binary inner products are special .",
    "the impossibility of existence of lsh for general inner products shown in  @xcite does not hold for the binary case . on the contrary",
    ", we provide an explicit construction of a provable lsh based on sampling , although our immediate investigation reveals that such an existential result is only good in theory and unlikely to be a useful hash function in practice .",
    "recent results on hashing algorithms for maximum inner product search  @xcite have shown the usefulness of asymmetric transformations in constructing provable hash functions for new similarity measures , which were otherwise impossible .",
    "going further along this line , we provide a novel ( and still very simple ) asymmetric transformation for binary data , that corrects minhash and removes the undesirable bias of minhash towards the sizes of the sets involved .",
    "such an asymmetric correction eventually leads to a provable hashing scheme for binary inner products , which we call _ asymmetric minwise hashing ( mh - alsh _ ) .",
    "our theoretical comparisons show that for binary data , which are common over the web , the new hashing scheme is provably more efficient that the recently proposed asymmetric hash functions for general inner products  @xcite .",
    "thus , we obtain a provable algorithmic improvement over the state - of - the - art hashing technique for binary inner products . the construction of our asymmetric transformation for minhash could be of independent interest in itself .",
    "the proposed asymmetric minhash significantly outperforms existing hashing schemes , in the tasks of ranking and near neighbor search with jaccard containment as the similarity measure , on four real - world high - dimensional datasets .",
    "our final proposed algorithm is simple and only requires very small modifications of the traditional minhash and hence it can be easily adopted in practice .",
    "past attempts of finding efficient algorithms , for exact near neighbor search based on space partitioning , often turned out to be a disappointment with the massive dimensionality of modern datasets  @xcite . due to the curse of dimensionality ,",
    "theoretically it is hopeless to obtain an efficient algorithm for exact near neighbor search .",
    "approximate versions of near neighbor search problem were proposed  @xcite to overcome the linear query time bottleneck .",
    "one commonly adopted such formulation is the @xmath24-approximate near neighbor ( @xmath24-nn ) .",
    "( @xmath24-approximate near neighbor or @xmath24-nn ) .",
    "@xcite given a set of points in a @xmath25-dimensional space @xmath26 , and parameters @xmath27 , @xmath28 , construct a data structure which , given any query point q , does the following with probability @xmath29 : if there exist an @xmath30-near neighbor of q in p , it reports some @xmath31-near neighbor .",
    "the usual notion of @xmath30-near neighbor is in terms of distance .",
    "since we are dealing with similarities , we define @xmath30-near neighbor of point @xmath32 as a point @xmath33 with @xmath34 , where @xmath35 is the similarity function of interest .",
    "the popular technique , with near optimal guarantees for @xmath24-nn in many interesting cases , uses the underlying theory of _ locality sensitive hashing _ ( lsh )  @xcite .",
    "lsh are family of functions , with the property that similar input objects in the domain of these functions have a higher probability of colliding in the range space than non - similar ones .",
    "more specifically , consider @xmath36 a family of hash functions mapping @xmath37 to some set @xmath38 .",
    "[ def : lsh](locality sensitive hashing )  a family @xmath36 is called @xmath39 sensitive if for any two point @xmath40 and @xmath41 chosen uniformly from @xmath36 satisfies the following :    * if @xmath42 then @xmath43 * if @xmath44 then @xmath45    for approximate nearest neighbor search typically , @xmath46 and @xmath47 is needed . note , @xmath48 as we are defining neighbors in terms of similarity . to obtain distance analogy we can resort to @xmath49    [ fct ]  @xcite given a family of @xmath39 -sensitive hash functions",
    ", one can construct a data structure for @xmath24-nn with @xmath50 query time and space @xmath51 , @xmath52    lsh trades off query time with extra preprocessing time and space that can be accomplished off - line .",
    "it requires constructing a one time data structure which costs @xmath53 space and further any @xmath24-approximate near neighbor queries can be answered in @xmath54 time in the worst case .",
    "+ a particularly interesting sufficient condition for existence of lsh is the monotonicity of the collision probability in @xmath55 .",
    "thus , if a hash function family @xmath36 satisfies , @xmath56 where @xmath57 is any strictly monotonically increasing function , then the conditions of definition  [ def : lsh ] are automatically satisfied for all @xmath47 .    the quantity @xmath58 is a property of the lsh family , and it is of particular interest because it determines the worst case query complexity of the @xmath24-approximate near neighbor search",
    ". it should be further noted , that the complexity depends on @xmath30 which is the operating threshold and @xmath24 , the approximation ratio we are ready to tolerate . in case when we have two or more lsh families for a given similarity measure , then the lsh family with smaller value of @xmath59 , for given @xmath30 and @xmath24 , is preferred .",
    "minwise hashing  @xcite is the lsh for the _ resemblance _ , also known as the _ jaccard similarity _ , between sets . in this paper , we focus on binary data vectors which can be equivalent viewed as sets .    given a set @xmath60 , the minwise hashing family applies a random permutation @xmath61 on @xmath0 and stores only the minimum value after the permutation mapping .",
    "formally minwise hashing ( or minhash ) is defined as : @xmath62    given sets @xmath0 and @xmath7 , it can be shown that the probability of collision is the resemblance @xmath63 : @xmath64 where @xmath65 , @xmath66 , and @xmath67 .",
    "it follows from eq .",
    "(  [ eq : minhash ] ) that minwise hashing is @xmath68-sensitive family of hash function when the similarity function of interest is resemblance . +",
    "even though minhash was really meant for retrieval with resemblance similarity , it is nevertheless a popular hashing scheme used for retrieving set containment or intersection for binary data  @xcite . in practice ,",
    "the ordering of inner product @xmath11 and the ordering or resemblance @xmath69 can be different because of the variation in the values of @xmath3 and @xmath4 , and as argued in section  [ sec : intro ] , which may be undesirable and lead to suboptimal results .",
    "we show later that by exploiting asymmetric transformations we can get away with the undesirable dependency on the number of nonzeros leading to a better hashing scheme for indexing set intersection ( or binary inner products ) .",
    "@xcite presented a novel lsh family for all @xmath70 ( @xmath71 $ ] ) distances .",
    "in particular , when @xmath72 , this scheme provides an lsh family for @xmath73 distance . formally , given a fixed number @xmath74 , we choose a random vector @xmath75 with each component generated from i.i.d .",
    "normal , i.e. , @xmath76 , and a scalar @xmath77 generated uniformly at random from @xmath78 $ ] .",
    "the hash function is defined as :    @xmath79    where @xmath80 is the floor operation .",
    "the collision probability under this scheme can be shown to be    @xmath81    @xmath82    where @xmath83 is the cumulative density function ( cdf ) of standard normal distribution and @xmath84 is the euclidean distance between the vectors @xmath0 and @xmath7 .",
    "this collision probability @xmath85 is a monotonically decreasing function of the distance @xmath25 and hence @xmath86 is an lsh for @xmath87 distances .",
    "this scheme is also the part of lsh package  @xcite . here",
    "@xmath74 is a parameter",
    ".      signed random projections ( srp ) or _ simhash _ is another popular lsh for the cosine similarity measure , which originates from the concept of _ * signed random projections ( srp ) * _  @xcite . given a vector @xmath0 , srp utilizes a random @xmath75 vector with each component generated from i.i.d .",
    "normal , i.e. , @xmath76 , and only stores the sign of the projection .",
    "formally simhash is given by @xmath88 it was shown in the seminal work  @xcite that collision under srp satisfies the following equation : @xmath89 where @xmath90 .",
    "the term @xmath91 is the popular * cosine similarity*.    for sets ( or equivalently binary vectors ) , the cosine similarity reduces to @xmath92    the recent work on _ coding for random projections _  @xcite has shown the advantage of srp ( and 2-bit random projections ) over l2lsh for both similarity estimation and near neighbor search .",
    "interestingly , another recent work  @xcite has shown that for binary data ( actually even sparse non - binary data ) , minhash can significantly outperform srp for near neighbor search even as we evaluate both srp and minhash in terms of the cosine similarity ( although minhash is designed for resemblance ) .",
    "this motivates us to design asymmetric minhash for achieving better performance in retrieving set containments . but",
    "first , we provide an overview of asymmetric lsh for general inner products ( not restricted to binary data ) .      the term `` alsh '' stands for _ asymmetric lsh _ , as used in a recent work  @xcite . through an elementary argument",
    ", @xcite showed that it is not possible to have a locality sensitive hashing ( lsh ) family for general unnormalized inner products .    for inner products between vectors @xmath0 and @xmath7 , it is possible to have @xmath93 .",
    "thus for any hashing scheme @xmath41 to be a valid lsh , we must have @xmath94 , which is an impossibility .",
    "it turns out that there is a simple fix , if we allow asymmetry in the hashing scheme .",
    "allowing asymmetry leads to an extended framework of asymmetric locality sensitive hashing ( alsh ) .",
    "the idea to is have a different hashing scheme for assigning buckets to the data point in the collection @xmath13 , and an altogether different hashing scheme while querying .",
    "+ * definition : * ( * _ asymmetric _ * locality sensitive hashing ( alsh ) )  a family @xmath36 , along with the two vector functions @xmath95 ( * query transformation * ) and @xmath96 ( * preprocessing transformation * ) , is called @xmath39-sensitive if for a given @xmath24-nn instance with query @xmath32 , and the hash function @xmath41 chosen uniformly from @xmath36 satisfies the following :    * if @xmath97 then @xmath98 * if @xmath99 then @xmath100    here @xmath0 is any point in the collection @xmath13 .",
    "asymmetric lsh borrows all theoretical guarantees of the lsh .",
    "[ theo : extendedlsh ] given a family of hash function @xmath36 and the associated query and preprocessing transformations @xmath101 and @xmath102 respectively , which is @xmath39 -sensitive , one can construct a data structure for @xmath24-nn with @xmath103 query time and space @xmath104 , where @xmath105 .",
    "@xcite showed that using asymmetric transformations , the problem of * maximum inner product search ( mips ) * can be reduced to the problem of approximate near neighbor search in @xmath73 .",
    "the algorithm first starts by scaling all @xmath19 by a constant large enough , such that @xmath106 .",
    "the proposed alsh family ( * l2-alsh * ) is the lsh family for @xmath73 distance with the preprocessing transformation @xmath107 and",
    "the query transformation @xmath108 defined as follows : @xmath109\\\\ \\label{eq : l2q}q^{l2}(x ) & = [ x ; 1/2; ... ;1/2;||x||^2_2 ; .... ; ||x||^{2^m}_2],\\end{aligned}\\ ] ] where [ ; ] is the concatenation .",
    "@xmath110 appends @xmath111 scalers of the form @xmath112 followed by @xmath111  1/2s \" at the end of the vector @xmath0 , while @xmath113 first appends @xmath111 `` 1/2s '' to the end of the vector @xmath0 and then @xmath111 scalers of the form @xmath112 .",
    "it was shown that this leads to provably efficient algorithm for mips .",
    "[ fact : l2-alsh ]  @xcite for the problem of @xmath24-approximate mips in a bounded space , one can construct a data structure having + @xmath114 query time and space @xmath115 , where @xmath116 is the solution to constrained optimization ( [ eq : optrho ] ) .",
    "@xmath117    here the guarantees depends on the maximum norm of the space @xmath118 . + quickly , it was realized that a very similar idea can convert the mips problem in the problem of maximum cosine similarity search which can be efficiently solve by srp leading to a new and better alsh for mips * sign - alsh *  @xcite which works as follows : the algorithm again first starts by scaling all @xmath19 by a constant large enough , such that @xmath106 . the proposed alsh family ( * sign - alsh * )",
    "is the srp family for cosine similarity with the preprocessing transformation @xmath119 and the query transformation @xmath120 defined as follows : @xmath121\\\\ \\label{eq : signq}q^{sign}(x ) & = [ x ; 0 ; ... ; 0 ; 1/2   - ||x||^2_2 ; ... ; 1/2 - ||x||^{2^m}_2],\\end{aligned}\\ ] ] where [ ; ] is the concatenation .",
    "@xmath122 appends @xmath111 scalers of the form @xmath123 followed by @xmath111  0s \" at the end of the vector @xmath0 ,",
    "while @xmath124 appends @xmath111 `` 0 '' followed by @xmath111 scalers of the form @xmath123 to the end of the vector @xmath0 .",
    "it was shown that this leads to provably efficient algorithm for mips . + as demonstrated by the recent work  @xcite on _ coding for random projections _",
    ", there is a significant advantage of srp over l2lsh for near neighbor search .",
    "thus , it is not surprising that sign - alsh outperforms l2-alsh for the mips problem .",
    "+ similar to l2lsh , the runtime guarantees for sign - alsh can be shown as :    [ theo : main ] for the problem of @xmath24-approximate mips , one can construct a data structure having @xmath125 query time and space @xmath126 , where @xmath127 is the solution to constraint optimization problem @xmath128^{2^{-m } } % \\nonumber s.t . \\ \\ \\",
    "z^ * & = \\max_{0 \\le z \\le \\frac{cs_0u^2}{v^2 } } \\frac{z}{\\sqrt{\\frac{m^2}{16 } + \\frac{mz^{2^m}}{2 }   + z^{2^{m+1}}}}\\end{aligned}\\ ] ]    there is a similar asymmetric transformation  @xcite which followed by signed random projection leads to another alsh having very similar performance to sign - alsh .",
    "the @xmath59 values , which were also very similar to the @xmath129 can be shown as @xmath130    both l2-alsh and sign - alsh work for any general inner products over @xmath37 . for sparse and high - dimensional binary dataset which are common over the web",
    ", it is known that minhash is typically the preferred choice of hashing over random projection based hash functions  @xcite .",
    "we show later that the alsh derived from minhash , which we call asymmetric minwise hashing ( _ mh - alsh _ ) , is more suitable for indexing set intersection for sparse binary vectors than the existing alshs for general inner products .",
    "in  @xcite , it was shown that there can not exist any lsh for general unnormalized inner product . the key argument used in the proof was the fact that it is possible to have @xmath0 and @xmath7 with @xmath131 . however",
    ", binary inner product ( or set intersection ) is special . for any two binary vectors @xmath0 and @xmath7 we always have @xmath132 .",
    "therefore , the argument used to show non - existence of lsh for general inner products does not hold true any more for this special case .",
    "in fact , there does exist an lsh for binary inner products ( although it is mainly for theoretical interest ) .",
    "we provide an explicit construction in this section . + our proposed lsh construction is based on sampling . simply sampling a random component leads to the popular lsh for hamming distance  @xcite .",
    "the ordering of inner product is different from that of hamming distance .",
    "the hamming distance between @xmath0 and query @xmath32 is given by @xmath133 , while we want the collision probability to be monotonic in the inner product @xmath11 .",
    "@xmath3 makes it non - monotonic in @xmath11 .",
    "note that @xmath134 has no effect on ordering of @xmath19 because it is constant for every query . to construct an lsh monotonic in binary inner product , we need an extra trick .    given a binary data vector @xmath0 , we sample a random co - ordinate ( or attribute ) .",
    "if the value of this co - ordinate is @xmath135 ( in other words if this attribute is present in the set ) , our hash value is a fixed number @xmath136 . if this randomly sampled co - ordinate has value @xmath136 ( or the attribute is absent ) then we independently generate a random integer uniformly from @xmath137 .",
    "formally , @xmath138    given two binary vectors @xmath0 and @xmath7 , we have @xmath139\\frac{a}{d } + \\frac{1}{n}\\ ] ]    the probability that both @xmath140 and @xmath141 have value 0 is @xmath142 .",
    "the only other way both can be equal is when the two independently generated random numbers become equal , which happens with probability @xmath143 .",
    "the total probability is @xmath144 which simplifies to the desired expression .",
    "@xmath145 is @xmath146\\frac{s_0}{d } + \\frac{1}{n } , \\left[\\frac{n-1}{n}\\right]\\frac{cs_0}{d } + \\frac{1}{n})$]-sensitive locality sensitive hashing for binary inner product with @xmath147\\frac{s_0}{d } + \\frac{1}{n}\\right)}}{\\log{\\left(\\left[\\frac{n-1}{n}\\right]\\frac{cs_0}{d } + \\frac{1}{n}\\right ) } } < 1 $ ]      the above lsh for binary inner product is likely to be very inefficient for sparse and high dimensional datasets . for those datasets ,",
    "typically the value of @xmath148 is very high and the sparsity ensures that @xmath11 is very small . for modern web datasets , we can have @xmath148 running into billions ( or @xmath149 ) while the sparsity is only in few hundreds or perhaps thousands  @xcite . therefore , we have @xmath150 which essentially boils down to @xmath151 .",
    "in other words , the hashing scheme becomes worthless in sparse high dimensional domain . on the other hand , if we observe the collision probability of minhash eq .",
    "(  [ eq : minhash ] ) , the denominator is @xmath152 , which is usually of the order of @xmath11 and much less than the dimensionality for sparse datasets .    another way of realizing the problem with the above lsh is to note that it is informative only if a randomly sampled co - ordinate has value equal to 1",
    ". for very sparse dataset with @xmath153 , sampling a non zero coordinate has probability @xmath150 .",
    "thus , almost all of the hashes will be independent random numbers .      in this section ,",
    "we argue why retrieving inner product based on plain minhash is a reasonable thing to do .",
    "later , we will show a provable way to improve it using asymmetric transformations .",
    "the number of nonzeros in the query , i.e. , @xmath154 does not change the identity of @xmath22 in eq.([eq : prob ] ) .",
    "let us assume that we have data of bounded sparsity and define constant @xmath155 as @xmath156 where @xmath155 is simply the maximum number of nonzeros ( or maximum cardinality of sets ) seen in the database .",
    "for sparse data seen in practice @xmath155 is likely to be small compared to @xmath148 .",
    "outliers , if any , can be handled separately . by observing that @xmath157",
    ", we also have @xmath158 thus , given the bounded sparsity , if we assume that the number of nonzeros in the query is given , then we can show that minhash is an lsh for inner products @xmath11 because the collision probability can be upper and lower bounded by purely functions of @xmath159 and @xmath134 .",
    "[ theo : minhash ] given bounded sparsity and query @xmath32 with @xmath154 , minhash is a @xmath160 sensitive for inner products @xmath11 with @xmath161    this explains why minhash might be a reasonable hashing approach for retrieving inner products or set intersection .    here ,",
    "if we remove the assumption that @xmath154 then in the worst case @xmath162 and we get @xmath163 in the denominator .",
    "note that the above is the worst case analysis and the assumption @xmath154 is needed to obtain any meaningful @xmath59 with minhash .",
    "we show the power of alsh in the next section , by providing a better hashing scheme and we do not even need the assumption of fixing @xmath154 .",
    "in this section , we provide a very simple asymmetric fix to minhash , named _ asymmetric minwise hashing ( mh - alsh ) _ , which makes the overall collision probability monotonic in the original inner product @xmath11 . for sparse binary data , which is common in practice ,",
    "we later show that the proposed hashing scheme is superior ( both theoretically as well as empirically ) compared to the existing alsh schemes for inner product  @xcite .",
    "we define the new preprocessing and query transformations @xmath164^d \\rightarrow [ 0,1]^{d+m}$ ] and @xmath165^d \\rightarrow [ 0,1]^{d+m}$ ] as : @xmath166\\\\ \\label{eq : qamin }   q'(x)&= [ x;0;0;0; ... ;0],\\end{aligned}\\ ] ] where [ ; ] is the concatenation to vector @xmath0 . for @xmath167",
    "we append @xmath168 1s and rest @xmath3 zeros , while in @xmath169 we simply append @xmath155 zeros .    at this point",
    "we can already see the power of asymmetric transformations .",
    "the original inner product between @xmath167 and @xmath169 is unchanged and its value is @xmath170 . given the query @xmath32 , the new resemblance @xmath171 between @xmath167 and @xmath172 is @xmath173 if we define our new similarity as @xmath174 , which is similar in nature to the containment @xmath175 , then the near neighbors in this new similarity are the same as near neighbors with respect to either set intersection @xmath11 or set containment @xmath175 .",
    "thus , we can instead compute near neighbors in @xmath176 which is also the resemblance between @xmath167 and @xmath172 .",
    "we can therefore use minhash on @xmath167 and @xmath172 .    observe that now we have @xmath177 in the denominator , where @xmath155 is the maximum nonzeros seen in the dataset ( the cardinality of largest set ) , which for very sparse data is likely to be much smaller than @xmath148 .",
    "thus , asymmetric minhash is a better scheme than @xmath145 with collision probability roughly @xmath142 for very sparse datasets where we usually have @xmath178 .",
    "this is an interesting example where we do have an lsh scheme but an altogether different asymmetric lsh ( alsh ) improves over existing lsh .",
    "this is not surprising because asymmetric lsh families are more powerful  @xcite .    from theoretical perspective , to obtain an upper bound on the query and space complexity of @xmath24-approximate near neighbor with binary inner products , we want the collision probability to be independent of the quantity @xmath134 .",
    "this is not difficult to achieve .",
    "the asymmetric transformation used to get rid of @xmath3 in the denominator can be reapplied to get rid of @xmath134 .",
    "+ formally , we can define @xmath179^d \\rightarrow [ 0,1]^{d+2m}$ ] and @xmath180^d \\rightarrow [ 0,1]^{d+2m}$ ] as : @xmath181 where in @xmath182 we append @xmath168 1s and rest @xmath183 zeros , while in @xmath184 we append @xmath155 zeros , then @xmath185 1s and rest zeros    again the inner product @xmath11 is unaltered , and the new resemblance then becomes @xmath186 which is independent of @xmath134 and is monotonic in @xmath11 .",
    "this allows us to achieve a formal upper bound on the complexity of @xmath24-approximate maximum inner product search with the new asymmetric minhash .    from the collision probability expression ,",
    "i.e. , eq .",
    "( [ eq : collamin ] ) , we have    minwise hashing along with query transformation @xmath187 and preprocessing transformation @xmath188 defined by equation  [ eq : p  ] is a @xmath189 sensitive asymmetric hashing family for set intersection .",
    "this leads to an important corollary .",
    "there exist an algorithm for @xmath24-approximate set intersection ( or binary inner products ) , with bounded sparsity @xmath155 , that requires @xmath190 space and @xmath191 , where @xmath192    given query @xmath32 and any point @xmath19 , the collision probability under traditional minhash is @xmath193 .",
    "this penalizes sets with high @xmath3 , which in many scenarios not desirable . to balance this negative effect",
    ", asymmetric transformation penalizes sets with smaller @xmath3 .",
    "note , that @xmath194 ones added in the transformations @xmath167 gives additional chance in proportion to @xmath195 for minhash of @xmath167 not to match with the minhash of @xmath169 .",
    "this asymmetric probabilistic correction balances the penalization inherent in minhash .",
    "this is a simple way of correcting the probability of collision which could be of independent interest in itself .",
    "we will show in our evaluation section , that despite this simplicity such correction leads to significant improvement over plain minhash .      our transformations @xmath188 and @xmath187 always create sets with @xmath196 nonzeros .",
    "in case when @xmath155 is big , hashing might take a lot of time .",
    "we can use fast consistent weighted sampling  @xcite for efficient generation of hashes .",
    "we can instead use transformations @xmath197 and @xmath198 that makes the data non - binary as follows @xmath199\\\\\\notag q'''(x ) & = [ x ; 0 ; m - { f_x}]\\notag\\end{aligned}\\ ] ] it is not difficult to see that the weighted jaccard similarity ( or weighted resemblance ) between @xmath200 and @xmath201 for given query @xmath32 and any @xmath19 is @xmath202 therefore , we can use fast consistent weighted sampling for weighted jaccard similarity on @xmath200 and @xmath203 to compute the hash values in time constant per nonzero weights , rather than maximum sparsity @xmath155 . in practice we will need many hashes for which we can utilize the recent line of work that make minhash and weighted minhash significantly much faster  @xcite .",
    "for solving the mips problem in general data types , we already know two asymmetric hashing schemes , _ l2-alsh _ and _ sign - alsh _ , as described in section  [ sec : alsh ] . in this section",
    ", we provide theoretical comparisons of the two existing alsh methods with the proposed asymmetric minwise hashing ( _ mh - alsh _ ) . as argued , the lsh scheme described in section  [ sec : lship ] is unlikely to be useful in practice because of its dependence on @xmath148 ; and hence we safely ignore it for simplicity of the discussion .",
    "before we formally compare various asymmetric lsh schemes for maximum inner product search , we argue why asymmetric minhash should be advantageous over traditional minhash for retrieving inner products .",
    "let @xmath32 be the binary query vector , and @xmath134 denotes the number of nonzeros in the query .",
    "the @xmath204 for asymmetric minhash in terms of @xmath134 and @xmath155 is straightforward from the collision probability eq.([eq : r ] ) : @xmath205 for minhash , we have from theorem  [ theo : minhash ] @xmath161 . since @xmath155 is the upper bound on the sparsity and @xmath31 is some value of inner product , we have @xmath206 . using this fact , the following theorem immediately follows    for any query q , we have @xmath207 .",
    "this result theoretically explains why asymmetric minhash is better for retrieval with binary inner products , compared to plain minhash .    for comparing asymmetric minhash with alsh for general inner products ,",
    "we compare @xmath204 with the alsh for inner products based on signed random projections .",
    "note that it was shown that @xmath208 has better theoretical @xmath59 values as compared to l2-alsh  @xcite .",
    "therefore , it suffices to show that asymmetric minhash outperforms signed random projection based alsh .",
    "both @xmath204 and @xmath209 can be rewritten in terms of ratio @xmath210 as follows .",
    "note that for binary data we have @xmath211 @xmath212 observe that @xmath155 is also the upper bound on any inner product .",
    "therefore , we have @xmath213 .",
    "we plot the values of @xmath214 and @xmath209 for @xmath215 with @xmath24 .",
    "the comparison is summarized in figure  [ fig : rho_comp ] .",
    "note that here we use @xmath216 derived from  @xcite instead of @xmath129 for convenience although the two schemes perform essentially identically .",
    "we can clearly see that irrespective of the choice of threshold @xmath210 or the approximation ratio @xmath24 , asymmetric minhash outperforms signed random projection based alsh in terms of the theoretical @xmath59 values .",
    "this is not surprising , because it is known that minwise hashing based methods are often significantly powerful for binary data compared to srp ( or simhash )  @xcite .",
    "therefore alsh based on minwise hashing outperforms alsh based on srp as shown by our theoretical comparisons .",
    "our proposal thus leads to an algorithmic improvement over state - of - the - art hashing techniques for retrieving binary inner products .",
    "in this section , we compare the different hashing schemes on the actual task of retrieving top - ranked elements based on set jaccard containment .",
    "the experiments are divided into two parts . in the first part",
    ", we show how the ranking based on various hash functions correlate with the ordering of jaccard containment . in the second part",
    ", we perform the actual lsh based bucketing experiment for retrieving top - ranked elements and compare the computational saving obtained by various hashing algorithms .",
    "we chose four publicly available high dimensional sparse datasets : _",
    "ep2006 _ , _ mnist _ , _ news20 _ , and _",
    "nytimes_. except mnist , the other three are high dimensional binary  bow \" representation of the corresponding text corpus .",
    "mnist is an image dataset consisting of 784 pixel image of handwritten digits .",
    "binarized versions of mnist are commonly used in literature .",
    "the pixel values in mnist were binarized to 0 or 1 values . for each of the four datasets , we generate two partitions .",
    "the bigger partition was used to create hash tables and is referred as the * training partition*. the small partition which we call the * query partition * is used for querying .",
    "the statistics of these datasets are summarized in table  [ tab_data ] .",
    "the datasets cover a wide spectrum of sparsity and dimensionality .",
    "[ tab_data ]      we consider the following hash functions for evaluations :    1 .",
    "* asymmetric minwise hashing ( proposed ) : * this is our proposal , the asymmetric minhash described in section  [ sec : amin ] .",
    "* traditional minwise hashing ( minhash ) : * this is the usual minwise hashing , the popular heuristic described in section  [ sec : minhash ] .",
    "this is a symmetric hash function , we use @xmath217 as define in eq.([eq : min ] ) for both query and the training set .",
    "* l2 based asymmetric lsh for inner products ( l2-alsh ) : * this is the asymmetric lsh of  @xcite for general inner products based on lsh for l2 distance .",
    "4 .   * srp based asymmetric lsh for inner products ( sign - alsh ) : * this is the asymmetric hash function of  @xcite for general inner products based on srp .",
    "we are interested in knowing , how the orderings under different competing hash functions correlate with the ordering of the underlying similarity measure which in this case is the jaccard containment .",
    "for this task , given a query @xmath32 vector , we compute the top-100 gold standard elements from the training set based on the jaccard containment @xmath175 . note that this is the same as the top-100 elements based on binary inner products . give a query @xmath32 , we compute @xmath218 different hash codes of the vector @xmath32 and all the vectors in the training set .",
    "we then compute the number of times the hash values of a vector @xmath0 in the training set matches ( or collides ) with the hash values of query @xmath32 defined by @xmath219 where @xmath220 is the indicator function .",
    "@xmath221 subscript is used to distinguish independent draws of the underlying hash function . based on @xmath222 we rank all elements in the training set .",
    "this procedure generates a sorted list for every query for every hash function . for asymmetric hash functions , in computing total collisions , on the query vector we use the corresponding @xmath101 function ( query transformation ) followed by underlying hash function ,",
    "while for elements in the training set we use the @xmath102 function ( preprocessing transformation ) followed by the corresponding hash function .",
    "we compute the precision and the recall of the top-100 gold standard elements in the ranked list generated by different hash functions . to compute precision and recall , we start at the top of the ranked item list and walk down in order ,",
    "suppose we are at the @xmath223 ranked element , we check if this element belongs to the gold standard top-100 list .",
    "if it is one of the top 100 gold standard elements , then we increment the count of _ relevant seen _ by 1 , else we move to @xmath224 . by @xmath223 step ,",
    "we have already seen @xmath33 elements , so the _ total elements seen _ is @xmath33 .",
    "the precision and recall at that point is then computed as : @xmath225 it is important to balance both .",
    "methodology which obtains higher precision at a given recall is superior .",
    "higher precision indicates higher ranking of the relevant items .",
    "we finally average these values of precision and recall over all elements in the query set .",
    "the results for @xmath226 are summarized in figure  [ fig : hashquality ] .",
    "we can clearly see , that the proposed hashing scheme always achieves better , often significantly , precision at any given recall compared to other hash functions .",
    "the two alsh schemes are usually always better than traditional minwise hashing .",
    "this confirms that fact that ranking based on collisions under minwise hashing can be different from the rankings under jaccard containment or inner products .",
    "this is expected , because minwise hashing in addition penalizes the number of nonzeros leading to a ranking very different from the ranking of inner products .",
    "sign - alsh usually performs better than l2-lsh , this is in line with the results obtained in  @xcite .",
    "+    it should be noted that ranking experiments only validate the monotonicity of the collision probability .",
    "although , better ranking is definitely a very good indicator of good hash function , it does not always mean that we will achieve faster sub - linear lsh algorithm . for bucketing the probability sensitivity around a particular threshold",
    "is the most important factor , see  @xcite for more details . what matters is the * gap * between the collision probability of good and the bad points . in the next subsection ,",
    "we compare these schemes on the actual task of near neighbor retrieval with jaccard containment .      in this section ,",
    "we evaluate the four hashing schemes on the standard @xmath227-parameterized bucketing algorithm  @xcite for sub - linear time retrieval of near neighbors based on jaccard containment . in @xmath227-parameterized lsh algorithm , we generate @xmath228 different meta - hash functions .",
    "each of these meta - hash functions is formed by concatenating @xmath218 different hash values as @xmath229,\\ ] ] where @xmath230 and @xmath231 , are @xmath232 different independent evaluations of the hash function under consideration",
    ". different competing scheme uses its own underlying randomized hash function @xmath41 .",
    "+ in general , the @xmath227-parameterized lsh works in two phases :    a.   * preprocessing phase : * we construct @xmath228 hash tables from the data by storing element @xmath0 , in the training set , at location @xmath233 in the hash - table @xmath234 .",
    "note that for vanilla minhash which is a symmetric hashing scheme @xmath235 .",
    "for other asymmetric schemes , we use their corresponding @xmath102 functions .",
    "preprocessing is a one time operation , once the hash tables are created they are fixed .",
    "b.   * query phase : * given a query @xmath32 , we report the union of all the points in the buckets @xmath236 @xmath237 , where the union is over @xmath228 hash tables . again here @xmath101 is the corresponding @xmath101 function of the asymmetric hashing scheme , for minhash @xmath238 .    typically , the performance of a bucketing algorithm is sensitive to the choice of parameters @xmath218 and @xmath228 .",
    "ideally , to find best @xmath218 and @xmath228 , we need to know the operating threshold @xmath30 and the approximation ratio @xmath24 in advance .",
    "unfortunately , the data and the queries are very diverse and therefore for retrieving top - ranked near neighbors there are no common fixed threshold @xmath30 and approximation ratio @xmath24 that work for all the queries .",
    "+ our objective is to compare the four hashing schemes and minimize the effect of @xmath218 and @xmath228 , if any , on the evaluations . this is achieved by finding best @xmath218 and @xmath228 at every recall level .",
    "we run the bucketing experiment for all combinations of @xmath239 and @xmath240 for all the four hash functions independently .",
    "these choices include the recommended optimal combinations at various thresholds .",
    "we then compute , for every @xmath218 and @xmath228 , the mean recall of top-@xmath241 pairs and the mean number of points reported , per query , to achieve that recall .",
    "the best @xmath218 and @xmath228 at every recall level is chosen independently for different @xmath241s .",
    "the plot of the mean fraction of points scanned with respect to the recall of top-@xmath241 gold standard near neighbors , where @xmath242 , is summarized in figure  [ fig : topk ] .",
    "the performance of a hashing based method varies with the variations in the similarity levels in the datasets .",
    "it can be seen that the proposed asymmetric minhash always retrieves much less number of points , and hence requires significantly less computations , compared to other hashing schemes at any recall level on all the four datasets .",
    "asymmetric minhash consistently outperforms other hash functions irrespective of the operating point .",
    "the plots clearly establish the superiority of the proposed scheme for indexing jaccard containment ( or inner products ) .",
    "l2-alsh and sign - alsh perform better than traditional minhash on ep2006 and news20 datasets while they are worse than plain minhash on nytimes and mnist datasets . if we look at the statistics of the dataset from table  [ tab_data ] , nytimes and mnist are precisely the datasets with less variations in the number of nonzeros and hence minhash performs better .",
    "in fact , for mnist dataset with very small variations in the number of nonzeros , the performance of plain minhash is very close to the performance of asymmetric minhash .",
    "this is of course expected because there is negligible effect of penalization on the ordering .",
    "ep2006 and news20 datasets have huge variations in their number of nonzeros and hence minhash performs very poorly on these datasets .",
    "what is exciting is that despite these variations in the nonzeros , asymmetric minhash always outperforms other alsh for general inner products .",
    "the difference in the performance of plain minhash and asymmetric minhash clearly establishes the utility of our proposal which is simple and does not require any major modification over traditional minhash implementation .",
    "given the fact that minhash is widely popular , we hope that our proposal will be adopted .",
    "minwise hashing ( minhash ) is a widely popular indexing scheme in practice for similarity search .",
    "minhash is originally designed for estimating set resemblance ( i.e. , normalized size of set intersections ) . in many applications",
    "the performance of minhash is severely affected because minhash has a bias towards smaller sets . in this study",
    ", we propose asymmetric corrections ( asymmetric minwise hashing , or mh - alsh ) to minwise hashing that remove this often undesirable bias .",
    "our corrections lead to a provably superior algorithm for retrieving binary inner products in the literature .",
    "rigorous experimental evaluations on the task of retrieving maximum inner products clearly establish that the proposed approach can be significantly advantageous over the existing state - of - the - art hashing schemes in practice , when the desired similarity is the inner product ( or containment ) instead of the resemblance .",
    "our proposed method requires only minimal modification of the original minwise hashing algorithm and should be straightforward to implement in practice . + * future work * :   one immediate future work would be _ asymmetric consistent weighted sampling _ for hashing weighted intersection : @xmath243 , where @xmath0 and @xmath7 are general real - valued vectors .",
    "one proposal of the new asymmetric transformation is the following:@xmath244 , \\hspace{0.3 in } q(x ) = [ x ; 0 ; m - \\sum_{i=1}^d x_i ] , \\end{aligned}\\ ] ] where @xmath245 .",
    "it is not difficult to show that the weighted jaccard similarity between @xmath246 and @xmath247 is monotonic in @xmath243 as desired . at this point",
    ", we can use existing methods for consistent weighted sampling on the new data after asymmetric transformations  @xcite . + another potentially promising topic for future work might be asymmetric minwise hashing for 3-way ( or higher - order ) similarities  @xcite          y.  bachrach , y.  finkelstein , r.  gilad - bachrach , l.  katzir , n.  koenigstein , n.  nice , and u.  paquet . speeding up the xbox recommender system using a euclidean transformation for inner - product spaces . in _ proceedings of the 8th acm conference on recommender systems",
    "_ , recsys 14 , 2014 .",
    "g.  cormode and s.  muthukrishnan .",
    "space efficient mining of multigraph streams . in _ proceedings of the twenty - fourth acm sigmod - sigact - sigart symposium on principles of database systems _ , pages 271282 .",
    "acm , 2005 .",
    "a.  s. das , m.  datar , a.  garg , and s.  rajaram .",
    "google news personalization : scalable online collaborative filtering . in _ proceedings of the 16th international conference on world wide web _ , pages 271280 .",
    "acm , 2007 .",
    "n.  koudas , s.  sarawagi , and d.  srivastava .",
    "record linkage : similarity measures and algorithms . in _ proceedings of the 2006 acm",
    "sigmod international conference on management of data _ , pages 802803 .",
    "acm , 2006 ."
  ],
  "abstract_text": [
    "<S> minwise hashing ( minhash ) is a widely popular indexing scheme in practice . </S>",
    "<S> minhash is designed for estimating set resemblance and is known to be suboptimal in many applications where the desired measure is set overlap ( i.e. , inner product between binary vectors ) or set containment . </S>",
    "<S> minhash has inherent bias towards smaller sets , which adversely affects its performance in applications where such a penalization is not desirable . in this paper , we propose asymmetric minwise hashing ( _ mh - alsh _ ) , to provide a solution to this problem . </S>",
    "<S> the new scheme utilizes asymmetric transformations to cancel the bias of traditional minhash towards smaller sets , making the final `` collision probability '' monotonic in the inner product . </S>",
    "<S> our theoretical comparisons show that for the task of retrieving with binary inner products asymmetric minhash is provably better than traditional minhash and other recently proposed hashing algorithms for general inner products . </S>",
    "<S> thus , we obtain an algorithmic improvement over existing approaches in the literature . experimental evaluations on four publicly available high - dimensional datasets validate our claims and the proposed scheme outperforms , often significantly , other hashing algorithms on the task of near neighbor retrieval with set containment . </S>",
    "<S> our proposal is simple and easy to implement in practice . </S>"
  ]
}