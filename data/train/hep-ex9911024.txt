{
  "article_text": [
    "in physics there are many situations where the outcome of an experiment is a positive integer number with a poisson distribution .",
    "this is the case for instance of the number of events of a certain type produced in high energy collisions .",
    "the statistical analysis of these processes is a difficult task when the result obtained is in the limit of the sensitivity of the experiment . in general , the number of events @xmath0 obtained in an experiment consists of background events with known mean @xmath1 and signal events , whose mean @xmath2 is the quantity that we want to determine .",
    "the problem arises when the number of events obtained @xmath0 is significantly lower than the background expected @xmath1 .",
    "this happens in some experiments on neutrino oscillations , for instance in the karmen 2 experiment @xcite .",
    "usually , after performing an experiment , one decides whether to give the results on the unknown parameter @xmath2 in the form of a central confidence interval or an upper bound .",
    "this decision ( called ` flip - flopping ' ) is based on the data and , as has been shown by feldman and cousins @xcite , introduces a bias that may cause that the intervals cover the true value @xmath2 with a smaller frequency than the stated confidence level . to solve this and other problems , they introduce a new ordering principle that unifies the treatment of central confidence intervals and upper limits",
    "this is possible because the neyman construction of confidence intervals @xcite allows the choice of the ordering principle with which the intervals are constructed .",
    "typical choices lead to the construction of either central intervals or upper confidence limits .",
    "the choice of feldman and cousins gives intervals that are two - sided or upper limits depending on the result of the experiment and not on the choice of the experimentalist .",
    "these intervals avoid the undercoverage caused by ` flip - flopping ' and are non - empty in all cases .",
    "some variants of their method have been also proposed @xcite .    to consider the feldman - cousins confidence intervals as an alternative to standard intervals , in practice one needs to calculate these intervals for arbitrary @xmath0 and @xmath1 .",
    "the tables provided in ref .",
    "@xcite , for @xmath3 , @xmath4 , are sufficient for small luminosity / statistics experiments , but for higher luminosities in general @xmath1 and @xmath0 are larger .",
    "one possibility is to extend these tables using the same systematic computational method of ref .",
    "@xcite , whose speed is not optimized and consumes a lot of time .",
    "more convenient is to develop a program which takes @xmath0 and @xmath1 as inputs and gives as output the confidence interval , requiring a minimal number of calculations .",
    "this is what is done here .",
    "the program can be used either directly to compute the confidence interval for given @xmath0 and @xmath1 or in conjunction with other routines .",
    "this is especially useful , for instance to calculate expected limits from rare high energy processes for different values of the center of mass energy or the collider luminosity , which is the case that we were primarily interested in .    in the following",
    "we introduce a procedure to compute the feldman - cousins intervals in an efficient way for arbitrary @xmath0 and @xmath1 , in principle only limited by the machine precision . in section 2",
    "we review neyman s construction of the confidence intervals for a poisson variable , emphasizing some points that simplify the numerical calculation .",
    "section 3 is more technical and devoted to explain in depth how to translate this method for the computer calculation . in section 4",
    "we present our results .",
    "the fortran implementation of the algorithm is given in the appendix .",
    "other implementations in c and _ mathematica _ @xcite ( about 100 times slower than the fortran version ) can be obtained from the author .",
    "the probability to observe @xmath5 events in a poisson process consisting of signal events with unknown mean @xmath2 and background events with known mean @xmath1 is given by the formula @xmath6 with @xmath7 , and @xmath5 restricted to integer values .",
    "the construction of the confidence intervals on the unknown variable @xmath2 follows neyman s method of the confidence belts .    the first step in this procedure is to construct , for a fixed value of @xmath1 and for different values of @xmath2 , the confidence intervals @xmath8 $ ] such that the probability to obtain a result between @xmath9 and @xmath10 is greater or equal than @xmath11 , the confidence level ( c. l. ) , @xmath12 \\,|\\ , \\mu;b ) = \\sum_{n = n_1}^{n_2 } p(n \\,|\\ , \\mu;b ) \\geq \\alpha \\ , .",
    "\\label{ec:2.2}\\ ] ] it is worth to note that for the more general case of a continuous variable @xmath13 the intervals @xmath14 $ ] satisfy @xmath15 \\,|\\ , \\mu;b ) = \\alpha$ ] . for a discrete variable @xmath5",
    "it is not possible to obtain the exact equality , and to avoid undercoverage it is replaced by the inequality in eq .",
    "( [ ec:2.2 ] ) .",
    "the choice of the intervals @xmath16 $ ] is not unique , and determines the type of confidence intervals on @xmath2 that are constructed .",
    "the most common choices are @xmath17 , @xmath18 , which gives upper confidence bounds , and @xmath19 , @xmath20 which leads to central confidence intervals .",
    "the prescription of ref .",
    "@xcite is based on a likelihood ratio @xmath21 , constructed as follows .    1 .   for any values of @xmath1 and @xmath5",
    ", one considers which value of @xmath2 would maximize the probability @xmath22 .",
    "it is straightforward to find that for @xmath23 , @xmath22 considered as a function of @xmath2 grows for @xmath24 , has a maximum at @xmath25 and decreases for @xmath26 . as @xmath2 is restricted to lie in the positive real axis , if @xmath27 the maximum is @xmath25 , otherwise the maximum is @xmath28 . in the case @xmath29 the maximum is also @xmath28 , so we define @xmath30 as the value which maximizes @xmath22 .",
    "then , for any value of @xmath2 we consider the quantity @xmath31 defined as @xmath32 on which the feldman - cousins ordering principle is based . to construct the interval",
    "@xmath8 $ ] , for each value of @xmath2 ( and fixed @xmath1 ) one takes values of @xmath5 with decreasing @xmath31 , summing up their probabilities @xmath22 until the total equals or exceeds the c. l. desired @xmath11 .",
    "thus the interval @xmath16 $ ] is the set of values of @xmath5 necessary to satisfy the inequality in eq .",
    "( [ ec:2.2 ] ) , taken with the largest @xmath31 .",
    "the simplicity of this prescription allows a fast computer implementation . instead of generating a large table of values for @xmath5 and taking those with the largest @xmath21 , we can directly find these values and add them successively to construct the interval .",
    "( this is difficult to do with the more involved prescriptions of refs . @xcite . ) for this purpose we will examine the behaviour of @xmath21 . if we consider @xmath33 as a function of the continous variable @xmath13 , we can look for its maximum .",
    "let us first consider @xmath34 , @xmath35 .",
    "for @xmath13 sufficiently small , @xmath36 and @xmath37 is increasing . for @xmath38 , @xmath39 and @xmath40",
    "grows for @xmath41 , falls for @xmath42 and has a local maximum at @xmath43 , which is then the global maximum .",
    "this is also true for @xmath34 , @xmath44 . for @xmath28 , @xmath35 and @xmath45 , @xmath36 and @xmath46 , its maximum possible value . for @xmath38 ,",
    "@xmath47 decreases with @xmath13 .",
    "hence the maximum is still @xmath43 , although not unique .",
    "the only remaining case with @xmath28 , @xmath48 in which the poisson distribution is singular must be treated separately .    with this method , and for different values of @xmath2 ( @xmath1 is fixed ) , one calculates the confidence intervals @xmath8 $ ] obtaining a confidence belt like the one showed in fig .",
    "[ fig : cb1 ] for @xmath49 and a c. l. of 0.9 . to find the confidence interval",
    "@xmath50 $ ] for a particular experimental value @xmath51 , one draws a vertical line at @xmath52 and finds the maximum and minimum values of @xmath2 for which the line intersects the confidence belt . in fig .",
    "[ fig : cb1 ] we observe that @xmath53 is the smallest @xmath2 such that @xmath54 , whereas @xmath55 is the largest @xmath2 for which @xmath56 .",
    "let us explain how the method described in section 2 is made suitable for the evaluation in a computer . for the calculations we use the fortran version of the program compiled with fort77 under linux on a pentium iii-450 ( compiled with g77 the program runs about 25% slower ) , and for the plots we also use the _ mathematica _ version .",
    "the first problem in the practical realization of the neyman construction is that , for large @xmath1 , @xmath5 or @xmath2 , the factors of the poisson probability formula in eq .",
    "( [ ec:2.1 ] ) can overflow ( or underflow ) the computer capacity in intermediate calculations .",
    "the factor @xmath57 may be very large , for instance @xmath58 is larger the biggest double precision real number in the fortran compiler used , approximately @xmath59 .",
    "however , the exponential factor in eq .",
    "( [ ec:2.1 ] ) compensates for it in the final result .",
    "for @xmath60 we evaluate p using the expression @xmath61 with the product calculated factor by factor .",
    "this extends the allowed size of the parameters of our program , with the disadvantage of a larger computing time . for @xmath62",
    "the factor @xmath57 is not too large , and can be directly calculated . in this case",
    "the factorials up to @xmath63 are calculated at the beginning of the main program and stored in the array fact to save time , whereas for @xmath64 the expression of the poisson formula is divided by @xmath65 factor by factor .    the quantity @xmath21 in eq .",
    "( [ ec:2.4 ] ) is a ratio of probabilities and can be computed without any problem cancelling out the common factors and defining a function r. ( defining @xmath66 as dim(float(n),b ) instead of",
    "max(0d0,float(n)-b ) as we do would not have improved the speed significantly . )",
    "the core of the algorithm is the subroutine nrange , used to calculate the confidence intervals @xmath8 $ ] for arbitrary @xmath2 and @xmath1 .",
    "its arguments are the variables rmu ( @xmath2 ) , b and the confidence level desired cl .",
    "the output n1 and n2 is given in a common block , together with a variable clac , the c. l. finally achieved ( in general it is greater than cl ) which is useful for other purposes .",
    "the discussion in the last section simplifies the implementation of the algorithm considerably , because we have found that the values of @xmath5 that maximize @xmath31 concentrate around @xmath67 .",
    "this improves the speed by an order of magnitude for large values of the parameters , since we do not need to calculate a large table @xmath68 and sort it . instead , we know the maximum r is one of the two integers nearest to @xmath67 .",
    "we begin with n1=int(rmu+b ) , n2=n1 + 1 . if r(n1,rmu , b ) is larger than r(n2,rmu , b ) we take n1 , decrease n1 and add p(n1,rmu , b ) to clac .",
    "otherwise , we take n2 , increase n2 and add p(n2,rmu , b ) to clac . repeating this until clac is greater than cl and taking into account that n1 must be greater than zero we obtain the desired interval @xmath16 $ ] .",
    "the singular case @xmath69 is treated separately . with the _ mathematica _ version of this subroutine we can plot confidence belts like that in fig .",
    "[ fig : cb1 ] .",
    "the calculation of the confidence interval @xmath50 $ ] is done using two funcions rmu1(n , b , cl ) and rmu2(n , b , cl ) , where n is the experimental number of events .",
    "we discuss them in turn .    as we see in fig .",
    "[ fig : cb1 ] , the lower limit @xmath53 is the minimum value of @xmath2 such that @xmath54 . within our framework ,",
    "the calculation is done looking for the minimum rmu such that n2 calculated with nrange(rmu , b , cl ) equals n. the search is done with the bisection method . starting with the limits rmumin=0d0 , rmumax = float(n)-b+1d0",
    "( rmu1 must be between these two values ) we calculate the midpoint of the interval , rmumed , and nrange(rmumed , b , cl ) .",
    "if n2 is greater or equal than n , we move rmumax to rmumed , otherwise we move rmumin to rmumed .",
    "this is repeated until the length of the interval is smaller than the desired precision delta , which we take as the maximum of 0.01 and 0.0005 times the background @xmath1 .",
    "we summarize the algorithm in fig .",
    "[ fig : df1 ] .    in principle",
    "the calculation of rmu2 would follow an analogous procedure .",
    "however , in this case we find an extra problem . except for a few cases with small @xmath1 ,",
    "the confidence belt is not as simple as in fig .",
    "[ fig : cb1 ] but displays a more elaborated structure as can be seen in the example of fig .",
    "[ fig : cb2 ] .",
    "the fact that @xmath70 is not a monotonic function of @xmath2 is due to the discreteness of @xmath5 .",
    "this causes that the set of @xmath2 values for which the vertical line at @xmath71 intersects the belt is not connected for @xmath72 . the effect is relevant since the upper limit @xmath55 is defined as the largest @xmath2 for which @xmath56 .",
    "thus a modification of the algorithm is required not to miss the small wedges in the function .",
    "for @xmath27 the behaviour is as expected and we can use the same algorithm as for @xmath53 .",
    "we have checked values of @xmath1 between 0 and 50 and have found that for @xmath27 the function @xmath70 does not have any singularity , so in this case we can safely adapt the routine rmu1 .",
    "we look for the maximum rmu such that n1 calculated with nrange(rmu , b , cl ) equals n. the search is again done with the bisection method .",
    "we start with the initial values rmumin = max(0d0,float(n)-b ) , rmumax=3d0*sqrt(float(n)+b+1d0 ) .",
    "( if rmumax is not sufficiently large , we increment it in steps of sqrt(float(n)+b+1d0 ) . )",
    "we calculate the midpoint of the interval , rmumed , and calculate nrange(rmumed , b , cl ) .",
    "if n1 is lower or equal to n , we move rmumin to rmumed , otherwise we move rmumax to rmumed .",
    "this is repeated until the length of the interval is smaller than the desired presision delta .",
    "for @xmath73 we sample the interval for possible singularities , which consumes more time .",
    "this is done in three iterations with increasing number of points . to minimize the length of the interval and optimize the density of the sampling , we take rmumin = max(0d0,float(n)-b ) and increase it in steps of 1 while n1 is lower or equal to n. as the upper limit we take rmumax=3d0*sqrt(float(n)+b+1d0 ) , sufficiently high so that the initial interval contains all the singularities for a c. l. of 0.99 or less .    in the first step",
    "we divide the interval between rmumin and rmumax in 10 parts and check if any of the points selected has n1 lower or equal to n. if it is so , we change rmumin to the largest of them ( this is always safe ) and start again with this new rmumin .",
    "this first sampling with a small number of points finds wedges like those for @xmath74 in fig .",
    "[ fig : cb2 ] and saves a lot of computing time .",
    "the narrow wedges at @xmath75 require more dense samplings .",
    "if the points calculated have n1 greater than n , we check the upper half of the interval for singularities .",
    "the second iteration divides the upper half in 20 parts and checks 19 points .",
    "if it finds any singular point with n1 lower or equal to n , it changes rmumin and starts again at the first step .",
    "if not , the third iteration divides the upper half in 500 parts .",
    "if a singular point is found , it changes rmumin and starts the first step . if not , unless some kind of singular behavior is found ( in which case a fourth sampling with 5000 points is performed ) it is assumed that there do not exist singulariries and rmumax is changed .",
    "an additional speed improvement is implemented : if in the second or third iterations the density of points is sufficiently high ( the points are closer than stepmin ) , the number of points is decreased and no more iterations are performed .",
    "the flux diagram of rmu2 is shown in fig . [",
    "fig : df2 ] .    to simplify changing the parameters of this routine ,",
    "the number of iterations and their respective number of points are stored in the variables maxit and maxdivs . the calculation for @xmath27",
    "is done with the same function with maxdivs(1)=2 and only the first iteration .",
    "one may notice that some upper limits obtained with rmu2 are different from those quoted in ref .",
    "this is again a consequence of the discreteness of @xmath5 .",
    "the upper limit @xmath55 for @xmath0 fixed is not always a decreasing function of @xmath1 ( dotted lines in fig .",
    "[ fig : corr ] . ) this behaviour is corrected in ref .",
    "@xcite forcing the function to be nonincreasing , calculating the upper limit @xmath55 from @xmath76 to @xmath48 in steps of @xmath77 .",
    "the corrected value is then the maximum of @xmath78 for @xmath38 .",
    "some people , however , find this _",
    "ad hoc _ correction questionable @xcite . at any rate",
    "we could also follow this procedure getting the same values of ref . @xcite and the solid line in fig .",
    "[ fig : corr ] .",
    "this requires a very long calculation ( 25000 different values of @xmath1 ) , for instance the time to calculate the @xmath79 line is 34 m 27 s.    of course , to obtain @xmath55 for a particular @xmath1 it is not necessary to calculate the whole interval @xmath80 $ ] and it is enough to consider approximately @xmath81 $ ] . for this purpose",
    "we use the function rmu2c .",
    "this routine examines the behaviour of rmu2 in the interval @xmath81 $ ] and corrects the value if necessary . the adjacent maxima that can be seen in fig .",
    "[ fig : corr ] are found with the simple _ golden section search _ of ref .",
    "( other more sophisticated methods offer no advantage since the function does not seem to be differentiable at the maximum . ) the initial bracketing of the maximum is very delicate as can be also seen in this figure .",
    "we do it examining rmu2(n , b1,cl ) taking b1 with increments of 0.1 until rmu2 begins to grow , then in increments of 0.05 until it begins to decrease . then the _ golden section _",
    "method is applied to find the maximum with a precision of 0.001 .",
    "this maximum is then compared to the value at b to take the largest value .",
    "this method again brings a substantial speed improvement over the blind computation in steps of @xmath77 in @xmath1 , as we will see in next section , examining the behaviour of @xmath55 we have also found that the correction is not necessary in general for @xmath82 and the function rmu2 could be used directly .",
    "there are however some exceptions , for instance @xmath83 , @xmath84 with a c. l. of 0.95 .",
    "to be conservative , we will only use rmu2 when @xmath27 .",
    "to obtain our results we use the same precision that is used in ref .",
    "@xcite , a minimum step stepmin of 0.005 in @xmath2 and an accuracy of 0.01 in the upper and lower limits of the confidence intervals . to calculate the limits in their tables ii  ix including the singular cases it is enough to consider in the third iteration maxdivs(3)=100 for confidence levels of @xmath85 , @xmath86 and @xmath87 , and maxdivs(3)=300 for a c. l. of @xmath88 . for better comparison",
    "we use maxdivs(3)=500 as we do in the rest of the calculations to ensure that all singularities are found . the running time is summarized in table [ tab : bench1 ] .",
    ".time spent in the calculation of the confidence intervals for @xmath89 and @xmath4 , with rmu2 ( @xmath90 ) and with rmu2c ( @xmath91 ) .",
    "[ tab : bench1 ] [ cols=\"^,^,^ \" , ]     to check if our algorithm in fact handles large numbers efficiently we measure the time spent to calculate the upper limit @xmath55 with rmu2 for @xmath92 between 0 and 200 , obtaining the solid line in fig .",
    "[ fig : bench2 ] .",
    "we can also use rmu2c forcing the program to look for unexisting spurious maxima in @xmath1 ( and hence also for singularities with maxdivs(3)=500 ) obtaining the dotted line .",
    "it is amazing to observe that the computing time not only does not grow quickly with @xmath1 as it could be expected , but remains almost constant for @xmath93 .",
    "this is achieved with ( _ i _ ) a fast algorithm to find the singularities if they exist , ( _ ii _ ) the optimization of nrange to calculate only the data really needed , and ( _ iii _ ) the calculation of the factorials up to @xmath94 at the beginning of the program . for @xmath95 rmu+b+float(n ) is sometimes larger than 230 and the time required begins to grow linearly with @xmath1 after the gap between 75 and 100 , as can be observed in the figure .",
    "*    i thank f. del aguila for discussions and for a critical reading of the manuscript .",
    "this work was partially supported by cicyt under contract aen961672 and by the junta de andaluca , fqm101 .",
    "....        program demo        implicit real*8 ( a - h , o - z )        dimension fact(0:170 )        common /range/",
    "n1,n2,clac        common /factorial/ fact        data cl /0.99d0/            do b=0d0,4d0,0.5d0          do n=0,20            print 100,n , b , rmu1(n , b , cl),rmu2c(n , b , cl )          enddo        enddo        do b=5d0,15d0,1d0          do n=0,20            print 100,n , b , rmu1(n , b , cl),rmu2c(n , b , cl )          enddo        enddo        stop             double precision function p(n , rmu , b )        implicit real*8 ( a - h , o - z )        dimension fact(0:170 )        common /factorial/",
    "fact        if ( ( rmu .eq . 0d0 ) .and .",
    "0d0 ) ) then     ! these lines          p=0",
    "! are not          if ( n .eq . 0 ) p=1                             ! needed if p is          return                                        ! only called        endif                                           ! from nrange        if ( rmu+b+float(n ) .le .",
    "230d0 ) then          p=(rmu+b)**n*exp(-rmu - b )          if ( n .le .",
    "170 ) then            p = p / fact(n )          else            p = p / fact(170 )     !",
    "this is not normally used because for            do i=171,n        !",
    "n > 170 rmu+b+float(n ) will be larger              p = p / float(i )    ! than 230            enddo",
    "endif        else          p = exp(-rmu )          do i=1,n            p = p*(rmu+b)/float(i )          enddo          p = p*exp(-b )        endif        return        end               double precision function r(n , rmu , b )        implicit real*8 ( a - h , o - z )        if ( n .lt .",
    "0 ) then          r=0d0          return        endif        r = exp(max(0d0,float(n)-b)-rmu )        if ( n .gt .",
    "0 ) r = r*((rmu+b)/(max(0d0,float(n)-b)+b))**n        return        end           subroutine nrange(rmu , b , cl )        implicit real*8 ( a - h , o - z )        common /range/ n1,n2,clac   !",
    "clac for future use        if ( ( rmu .eq . 0d0 ) .and . ( b .eq .",
    "0d0 ) ) then          n1=0d0                    ! special case          n2=0d0          return        endif        n1=int(rmu+b )               ! the maximum",
    "r is between        n2=n1 + 1                     ! these values        r1=r(n1,rmu , b )        r2=r(n2,rmu , b )        clac=0d0        do while ( ( clac .lt .",
    "cl ) .and .",
    ".ge . 0 ) )          if ( r1 .gt .",
    "r2 ) then            clac = clac+p(n1,rmu , b )            n1=n1 - 1            r1=r(n1,rmu , b )          else            clac = clac+p(n2,rmu , b )            n2=n2 + 1            r2=r(n2,rmu , b )          endif        enddo        do while ( clac .lt .",
    "cl )     ! no need to calculate r          clac = clac+p(n2,rmu , b )          n2=n2 + 1        enddo        n1=n1 + 1        n2=n2 - 1        return        end           double precision function rmu1(n , b , cl )        implicit real*8 ( a - h , o - z )        common /range/",
    "n1,n2,clac        call nrange(0d0,b , cl )        if ( n2 .ge .",
    "n ) then          rmu1=0d0                                 !",
    "special case          return        endif        rmumin=0d0",
    "rmumax = float(n)-b+1d0        delta = max(0.01d0,0.0005d0*b )        do while ( ( rmumax - rmumin ) .ge . delta )      ! bisection method          rmumed=(rmumin+rmumax)/2d0",
    "call nrange(rmumed , b , cl )          if ( n2 .ge .",
    "n ) then            rmumax = rmumed          else            rmumin = rmumed          endif        enddo        rmu1=(rmumin+rmumax)/2d0        return        end           double precision function rmu2(n , b , cl )        implicit real*8 ( a - h , o - z )        dimension maxdivs(4 )        logical safe , safenow , sing , changemin        common /range/",
    "n1,n2,clac        data maxdivs /10,20,500,5000/",
    "maxit=4        stepmin=0.005d0        rmumin = max(0d0,float(n)-b )        call nrange(rmumin , b , cl )        if ( float(n ) .lt .",
    "b ) then          do while ( n1 .le .",
    "n )           ! take lower limit            rmumin = rmumin+1d0            ! as high as possible            call nrange(rmumin , b , cl )          enddo          rmumin = rmumin-1d0          safe=.false .",
    "else          maxdivs(1)=2                   !",
    "use bisection method when          safe=.true .                    !",
    "there are n't sing .",
    "endif        rmumax=3d0*sqrt(float(n)+b+1d0 ) !",
    "large enough for most purposes        call nrange(rmumax , b , cl )        do while ( n1 .le .",
    "n )             ! if not , increase it          rmumax = rmumax+sqrt(float(n)+b+1d0 )          call nrange(rmumax ,",
    "b , cl )        enddo        delta = max(0.01d0,0.0005d0*b )        do while ( ( rmumax - rmumin ) .ge .",
    "delta )          step=(rmumax - rmumin)/float(maxdivs(1 ) )          rmumin2=rmumin          do i=1,maxdivs(1)-1            call nrange(rmumin+float(i)*step , b , cl )            if ( n1 .le .",
    "n ) rmumin2=rmumin+float(i)*step          enddo          if ( rmumin2 .gt .",
    "rmumin ) then            rmumin = rmumin2               ! new rmumin - > change it          else            safenow = safe                 !",
    "have to look for singularities            sing=.false .                 !",
    "if they may exist            changemin=.false .",
    "it=2            do while ( ( safenow .eq . .false . ) .and .",
    "maxit ) )              ndivs = maxdivs(it )              step=(rmumax - rmumin)/float(2*ndivs )              if ( step .lt .",
    "stepmin ) then                 !",
    "step is small                ndivs = int((rmumax - rmumin)/(2*stepmin))+1 !",
    "enough and this                step=(rmumax - rmumin)/float(2*ndivs )       ! will be the                safenow=.true .                            !",
    "last iteration              endif              call nrange((rmumin+rmumax)/2d0,b , cl )              n_prev = n1              do i=1,ndivs-1                call nrange(rmumin+float(i+ndivs)*step , b , cl )                if ( n1 .le .",
    "n ) then                  rmumin2=rmumin+float(i+ndivs)*step      ! new rmumin                  changemin=.true .                endif                if ( n1 .lt .",
    "n_prev ) sing=.true .",
    "n_prev = n1              enddo              if ( changemin .eq . .true . )",
    "then                rmumin = rmumin2                     ! change rmumin                safenow=.true .                     !",
    "exit loop              else                 if ( ( sing .eq .",
    ".false . ) .and .",
    "maxit-1 ) ) then                  safenow=.true .                   !",
    "enough iterations                endif              endif",
    "it = it+1                              ! next iteration            enddo            if ( changemin .eq . .false . )",
    "rmumax=(rmumin+rmumax)/2d0          endif        enddo        rmu2=(rmumin+rmumax)/2d0        return        end           double precision function rmu2c(n , b , cl )        implicit real*8 ( a - h , o - z )        logical sing        parameter ( r=0.61803399d0,c=1d0-r )        rmu2c = rmu2(n , b , cl )        if ( n .ge .",
    "int(b ) ) return         ! do not need correction        step1=0.1d0                       ! go downhill in steps of 0.1        step2=0.05d0                      ! go uphill in steps 0f 0.05        deltab=0.001d0                    ! final precision in b        b1max = b+1d0                       ! look for maximum up to b+1        sing=.false .",
    "b1=b                                     a1=rmu2c        do while ( ( b1 .le .",
    "b1max ) .and .",
    "( sing .eq .",
    ".false . ) ) ! go downhill          a_next = rmu2(n , b1+step1,cl )          if ( a_next .gt .",
    "a1 ) then            sing=.true .          else",
    "a1=a_next            b1=b1+step1          endif        enddo        if ( sing .eq .",
    ".false . ) return     !",
    "rmu2 is always decreasing        b2=b1+step1-step2        a2=a_next        do while ( a_next .ge .",
    "a2 )         ! go uphill          a2=a_next          b2=b2+step2          a_next = rmu2(n , b2+step2,cl )        enddo        b4=b2+step2        a4=a_next        if ( rmu2c .gt .",
    "a2 + 0.05d0 ) return   ! this",
    "maximum will not be larger        if ( b4-b2 .gt .",
    "b2-b1 ) then                    b3=b2+c*(b4-b2 )          a3=rmu2(n , b3,cl )        else          b3=b2          a3=a2          b2=b3-c*(b3-b1 )          a2=rmu2(n , b2,cl )        endif            do while ( b4-b1 .ge .",
    "deltab )          if ( a3 .gt .",
    "a2 ) then            b1=b2            b2=b3            b3=r*b2+c*b4            a1=a2            a2=a3            a3=rmu2(n , b3,cl )          else            b4=b3            b3=b2            b2=r*b3+c*b1            a4=a3            a3=a2            a2=rmu2(n , b2,cl )          endif        enddo        rmu2c = max(rmu2c , a2,a3 )        return        end ....    99 k. eitel and b. zeitnitz , hep - ex/9809007 ; b. zeitnitz , talk presented at neutrino 98 . see + http://www-ik1.fzk.de/www/karmen/karmen.e.html g. j. feldman y r. d. cousins , phys .",
    "* d57 * , 3873 ( 1998 ) j. neyman , philos . trans .",
    "london * a236 * , 333 ( 1937 ) .",
    "reprinted in _ a selection of early statistical papers on j. neyman _ ,",
    "university of california press , berkeley 1967 c. giunti , phys . rev . *",
    "d59 * , 053001 ( 1999 ) b. p.",
    "roe and m. b. woodroofe , phys . rev . *",
    "d60 * , 053009 ( 1999 ) s. wolfram , _ mathematica , a system for doing mathematics by computer _ , addison - wesley publishing company , redwood city , california , 1988 w. h. press , b. p. flannery , s. a. teukolsky y w. t. vetterling , _ numerical recipes _ , cambridge university press 1986"
  ],
  "abstract_text": [
    "<S> we present an algorithm which allows a fast numerical computation of feldman - cousins confidence intervals for poisson processes , even when the number of background events is relatively large . </S>",
    "<S> this algorithm incorporates an appropriate treatment of the singularities that arise as a consequence of the discreteness of the variable .    </S>",
    "<S>  ft108/99 + hep - ex/9911024 + november 1999 +    * computation of confidence intervals for poisson processes *    j. a. aguilar  </S>",
    "<S> saavedra + _ departamento de fsica terica y del cosmos + universidad de granada + e-18071 granada , spain _    </S>",
    "<S> pacs : 02.70.-c , 06.20.dk , 29.85.+c </S>"
  ]
}