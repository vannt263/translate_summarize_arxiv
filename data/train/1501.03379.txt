{
  "article_text": [
    "consider a lebesgue - integrable test function @xmath6 defined on a bounded measurable subspace @xmath7 ( @xmath8 ) with square integrable derivatives of order @xmath9 in each variable .",
    "our focus is numerical computation of the integral @xmath10 : =   \\int_{\\mathcal{x } } f(\\bm{x } ) d\\bm{x}$ ] .",
    "the quasi - monte carlo ( qmc ) approach is based on an approximation @xmath11 : = \\frac{1}{n } \\sum_{n=1}^n f(\\bm{x}^n)\\ ] ] where the ( possibly random ) design points @xmath12 have low discrepancy ; that is , the points are ` well - spaced ' in a precise sense defined below .",
    "this contrasts with the monte carlo ( mc ) approach whereby the design points are sampled independently from a uniform distribution over @xmath13 .",
    "mc integration achieves a root mean square error ( rmse ) convergence rate of @xmath14 whereas qmc integration can in principle achieve a rate @xmath1 on specific geometric sequences @xmath15 @xcite .",
    "it is known that this rate is best - possible @xcite and explicit algorithms to generate design points that attain this rate are now available for many ( but not all ) values of @xmath0 @xcite . challenging integration problems are common in contemporary statistics , for example when computing expectations , marginal probability densities or normalising constants , and qmc methods are therefore gaining importance in statistical applications @xcite .",
    "contrary to the above theoretical considerations , rate - optimal qmc is often not employed in practice .",
    "this is mainly due to three reasons ; either ( r1 ) the smoothness parameter @xmath0 is unknown , ( r2 ) there does not currently exist an explicit qmc rule that is rate - optimal for functions of smoothness @xmath0 , or ( r3 ) it is simply more convenient to employ a basic qmc rule based on a weaker smoothness assumption @xmath16 . in each situation",
    "there is a gap between theory and practice that , as we show in this paper , can be bridged using control functionals , as recently studied by @xcite in the monte carlo setting .",
    "previous work on variance reduction techniques for qmc includes @xcite , who considered modified importance sampling strategies , and @xcite , who considered constructing control variates for qmc .",
    "neither approach improved the asymptotic error rate , though in some cases the qmc error was reduced by a constant factor .",
    "interestingly , @xcite reports some quite negative results for control variate strategies in this setting , because the objective being minimised by qmc is not equivalent to the mc variance that is minimised by control variates .",
    "@xcite demonstrates variance reductions in qmc are possible using additive approximations , though again the asymptotics were unchanged .",
    "this paper studies a general approach to variance reduction for qmc rules , building on kernel methods and recent work in the monte carlo setting due to @xcite .",
    "the mathematics that underpins our work comes from kernel methods and discrepancy theory ; our contribution is to make an explicit connection between these fields within the context of qmc integration .",
    "this link takes the form of a ` control functional ' @xmath17 that satisfies ( i ) @xmath18 integrates to zero , ( ii ) @xmath19 is more amenable to qmc methods than @xmath20 , in a precise sense .",
    "our general approach is to replace the integrand @xmath20 by @xmath19 and target the qmc objective directly .",
    "this leads to accelerated asymptotics and , in this sense , heals a gap in previous literature identified by @xcite .",
    "theoretical analysis of convergence rates is provided , along with empirical results and a challenging application to robotics .",
    "we begin by presenting some background on qmc theory below , before describing our methodology in more detail .",
    "qmc is naturally studied in reproducing kernel hilbert spaces ( rkhs ; @xcite ) . below we draw connections with kernel methods , that are themselves naturally studied in rkhs",
    ".    * notation .",
    "* we work in a hilbert space @xmath21 , consisting of measurable functions @xmath6 . for simplicity of presentation",
    "we assume @xmath21 includes the constant functions .",
    "we follow the mainstream qmc literature by taking @xmath22^d$ ] , equipped with the euclidean norm @xmath23 .",
    "denote the scalar product and norm on @xmath21 by @xmath24 and @xmath25 respectively .",
    "suppose further that @xmath21 is a rkhs with kernel @xmath26^d \\times [ 0,1]^d \\rightarrow \\mathbb{r}$ ] ; that is , @xmath27 satisfies ( i ) @xmath28 for all @xmath29^d$ ] and ( ii ) @xmath30 for all @xmath31 and all @xmath29^d$ ] .",
    "@xmath27 is assumed to be non - trivial , i.e. @xmath32 .    * quadrature error analysis . *",
    "contemporary quadrature methods focus on minimising the ` worst case ' integration error which , for design points @xmath33 and hilbert space @xmath21 , is defined to be @xmath34 - i[f ] \\right| \\label{wce}\\end{aligned}\\ ] ] where the supremum is taken over all test functions @xmath20 belonging to the unit ball in @xmath21 .",
    "it follows from linearity that , for any function @xmath31 , the integration error obeys @xmath35 - i[f ] \\right| \\leq e_h(\\bm{x}^{1:n } ) \\|f\\|_h . \\label{kh}\\end{aligned}\\ ] ] the worst case error @xmath36 is the usual target of qmc innovation , with @xmath33 chosen to ( approximately , asymptotically ) minimise @xmath36 @xcite .",
    "note that eqn .",
    "[ wce ] is also the ` maximum mean discrepancy ' ( mmd ) , as studied extensively in the kernel methods literature @xcite .",
    "quadrature is naturally studied in rkhs because there exists a closed - form expression for the worst case error in terms of the kernel @xmath27 , which facilitates the principled selection of design points @xcite : @xmath37^d } k(\\bm{x},\\bm{y } ) d\\bm{x } d\\bm{y } - \\frac{2}{n } \\sum_{n=1}^n \\int_{[0,1]^d } k(\\bm{x}^n,\\bm{y } ) d\\bm{y } + \\frac{1}{n^2 } \\sum_{m , n=1}^n k(\\bm{x}^n,\\bm{x}^m )    \\label{discrep}\\end{aligned}\\ ] ] a similar formula can be obtained for arbitrarily weighted combinations of function values @xcite , but for simplicity we focus on qmc ( i.e. uniform weights ) .",
    "indeed , we follow the mainstream qmc literature and suppose @xmath21 is a sobolev space of known order @xmath0 ( defined below ) . in this setting , @xmath1 is the best - possible rate for the worst case error when @xmath33 are chosen deterministically and @xmath38 is best - possible rmse when @xmath33 are allowed to be random @xcite",
    ". we will refer to qmc rules that achieve these optimal rates as ` @xmath0-qmc rules ' .",
    "this paper focuses on improving performance in the situation where a ( sub - optimal ) @xmath3-qmc rule is used to integrate a test function of smoothness @xmath39 . for reasons ( r1 - 3 )",
    ", this scenario is commonly encountered in statistical applications .",
    "in contrast to qmc @xcite ( and kernel methods that aim to minimise the mmd @xcite ) , the rate constant @xmath40 is the primary target of our methodology below .",
    "* control functionals for qmc .",
    "* the approach that we pursue in this paper aims to construct a lebesgue - integrable functional @xmath41^d \\rightarrow \\mathbb{r}$ ] that satisfies @xmath42 = 0 . \\label{zero}\\end{aligned}\\ ] ] when @xmath43 has the interpretation of a random variable , @xmath44 is classically known as a ` control variate ' @xcite .",
    "when @xmath18 itself is estimated , we follow @xcite and refer to the entire mapping @xmath18 as a ` control functional ' ( cf ) . in the cf approach to estimation ,",
    "the test function @xmath20 is replaced by @xmath19 ; it is hoped that the latter is more amenable to numerical integration .",
    "clearly @xmath45 = i[f]$ ] . in this paper",
    "we construct a cf @xmath46 based on a tractable approximation @xmath47 to @xmath20 .",
    "( the dependence on @xmath48 will be explained below . )",
    "it is required that the integral @xmath49 $ ] is available in closed - form .",
    "we then set @xmath50 \\label{subtract}\\end{aligned}\\ ] ] so that @xmath46 satisfies eqn .",
    "[ zero ] . for this to make sense mathematically",
    ", it must be the case that @xmath51 and this informs our method of approximation ( the constant function with value @xmath49 $ ] belongs to @xmath21 by assumption ) .",
    "intuitively , a good cf @xmath46 will provide a close approximation to fluctuations of the test function @xmath20 , so that the functional difference @xmath52 become increasingly ` flat ' and thus more amenable to qmc methods .",
    "more precisely , motivated by eqn .",
    "[ kh ] we aim to construct a cf such that @xmath53 .",
    "this connection with functional approximation offers the possibility to leverage kernel methods for these problems , see e.g. @xcite .",
    "* control functional error analysis . *",
    "consider partitioning @xmath33 into two sets @xmath54 and @xmath55 where @xmath56 and @xmath57 as @xmath58 .",
    "the first set @xmath54 , possibly non - random , will be used in a preliminary step to construct an approximation @xmath59 to @xmath20 .",
    "then the second set @xmath55 , possibly random , is used to evaluate the ` cf estimator ' @xmath60 & : = & q[f-\\psi_n(\\cdot;\\bm{u}^{1:m});\\bm{v}^{m+1:n } ] \\nonumber \\\\   & = &   q[f - f_m(\\cdot;\\bm{u}^{1:m});\\bm{v}^{m+1:n } ]   + i[f_m(\\cdot;\\bm{u}^{1:m } ) ] .",
    "\\end{aligned}\\ ] ] we remark that if the points @xmath61 are random and marginally distributed as @xmath62^d)$ ] then @xmath63 $ ] will be an unbiased estimator for @xmath10 $ ] .",
    "error analysis for the cf estimator is based on the following :    [ theorem one ] given @xmath64 , we have @xmath65 - i[f ] | \\leq   e_h(\\bm{v}^{m+1:n } ) \\|f - f_m(\\cdot;\\bm{u}^{1:m})\\|_h . \\label{qmccf}\\end{aligned}\\ ] ]    since @xmath64 we have that @xmath66 .",
    "the result then follows by applying the fundamental inequality from eqn .",
    "[ kh ] to the function @xmath67 and using linearity of the integral operator @xmath68 .",
    "thus the cf methodology produces an estimator @xmath63 $ ] that has asymptotically zero error relative to standard qmc estimators , providing that it is possible to construct an approximation @xmath69 to @xmath20 in such a way that @xmath70 as @xmath71 .",
    "the next sections establish convergence rates for functional approximation using kernel methods",
    ".    * sobolev spaces .",
    "* to achieve consistent approximation @xmath72 it is necessary to impose regularity conditions on @xmath21 .",
    "sobolev spaces are a general setting in which to formulate such regularity assumptions ; our main reference here is @xcite .",
    "firstly suppose that @xmath73 , @xmath74 and @xmath75 . for a multi - index @xmath76",
    "we write @xmath77 .",
    "define the ` @xmath78-sobolev space of order @xmath79 ' to be @xmath80^d \\rightarrow \\mathbb{r } \\ ; | \\ ;   d^{\\bm{a}}f \\text { exists and } d^{\\bm{a}}f \\in l_p([0,1]^d ) , \\forall \\bm{a } \\in \\mathbb{n}_0^d \\text { with } |\\bm{a}| \\leq k\\}.\\end{aligned}\\ ] ] here @xmath81 denotes the weak ( or ` distributional ' ) derivative of @xmath20 ; the reader is referred to the above reference for details . clearly @xmath82 is a vector space over @xmath83 when addition and ( scalar ) multiplication are defined point - wise . for the special case @xmath84 we equip @xmath85 with the inner product @xmath86\\end{aligned}\\ ] ] and denote this inner - product space @xmath87 .",
    "defined in this way , @xmath88 is a hilbert space of functions whose ( weak ) derivatives exist up to order @xmath79 .",
    "moreover @xmath88 can be made into a rkhs with an appropriate choice of kernel ( see below ) .",
    "our results below apply also to sobolev spaces with non - integer @xmath79 , but this construction is more technical and we refer the reader to @xcite for details .    * approximation in sobolev spaces . * our assumptions",
    "are naturally stated using sobolev spaces : given two hilbert spaces @xmath21 ,",
    "@xmath89 , defined on the same element set , with norms @xmath90 , @xmath91 , we say that @xmath21 and @xmath89 are ` norm - equivalent ' , written @xmath92 , whenever there exist positive constants @xmath93 , @xmath94 such that @xmath95 for all @xmath31 .",
    "_ assumption 1 _ : @xmath96 where @xmath97 .",
    "_ assumption 2 _ : @xmath98 where @xmath99 .",
    "assumption 1 is a technical requirement to ensure the space @xmath21 ( where qmc is performed ) admits consistent functional approximation .",
    "assumption 2 ensures that the test function @xmath20 is ` smooth enough ' for @xmath3-qmc methods to converge at the @xmath3-rate .",
    "this follows from the fact that sobolev spaces are nested , so that @xmath100 .    for consistent approximation of @xmath20",
    "it is necessary to base our approximation @xmath69 in a space @xmath101 of functions that are ` at least as smooth ' as @xmath20 :    _ assumption 3 _ : @xmath102 where @xmath103 .",
    "it follows again from the nested property that @xmath104 and thus the functional difference @xmath105 exists in @xmath106 .",
    "the sobolev spaces @xmath101 can be characterised as rkhs via an appropriate reproducing kernel @xmath107 , such as the well - known matrn kernel .",
    "finally an approximation @xmath69 to @xmath20 is constructed based on the points @xmath54 as follows : @xmath108 where the weights @xmath109 are defined as the solution to the linear system of interpolation equations @xmath110 it is well - known that eqn .",
    "[ reg 1 ] is the unique minimiser of the @xmath101-norm under all functions in @xmath101 that satisfy the linear system in eqn .",
    "[ interp ] @xcite . in practice",
    "it may be necessary to regularise the linear system in order to facilitate inversion , but we do not go into details here , see e.g. @xcite .",
    "we note that @xmath111 $ ] will _ not _ have a closed - form expression when the matrn kernel is employed and for this technical reason we instead employ tensor products of polynomial kernels ( these give rise to sobolev spaces of mixed dominating smoothness - full details are provided at the end of this section ) .    *",
    "theory : deterministic case . *",
    "we begin by considering the case where the design points @xmath55 are chosen deterministically .",
    "define the ` fill distance ' @xmath112^d } \\min_{n } \\|\\bm{x } - \\bm{u}^n\\|,\\ ] ] the ` separation radius ' @xmath113 and the ` mesh ratio ' @xmath114 .",
    "the set @xmath54 is called ` quasi - uniform ' if @xmath115 as @xmath71 .",
    "[ theo1 ] under assumptions 1 - 3 the cf estimator has error bounded by @xmath65 - i[f ] |   \\leq c e_{h^{\\alpha_l}}(\\bm{v}^{m+1:n } ) h(\\bm{u}^{1:m})^{\\alpha - \\alpha_l } \\rho(\\bm{u}^{1:m})^{\\alpha_u-\\alpha_l } \\|f\\|_{h^\\alpha}\\end{aligned}\\ ] ] where @xmath116 is a constant that depends on @xmath0 , @xmath3 and @xmath117 but not on @xmath20 , @xmath55 and @xmath54 .",
    "from @xcite ( theorem 7.8 ) we have that the kernel estimator in eqn .",
    "[ reg 1 ] is consistent for the non - parametric regression problem at a rate @xmath118 where @xmath119 depends only on @xmath120 . combining this with eqn .",
    "[ qmccf ] completes the proof .    for quasi - uniform @xmath54",
    ", there is no asymptotic penalty from employing a kernel @xmath107 that imposes ` too much smoothness ' on the approximation @xmath69 , with @xmath121 . in this case",
    "@xmath122 and , since @xmath123 and @xmath48 are proportional , @xmath124 .",
    "however the rate constant @xmath119 will increase when too much smoothness is assumed so that , as a rule of thumb , we should try to select @xmath117 as close as possible to @xmath0 .",
    "our main result is stated below :    when @xmath54 is quasi - uniform , cfs accelerate @xmath3-qmc by a factor @xmath125 .    _",
    "remark : _ the improvement due to cfs appears to be mainly limited to low - dimensional integrals ( @xmath2 small ) , but in fact cfs can in principle be extended to high - dimensional integrals under additional tractability assumptions , as discussed in sec .",
    "[ discuss ] .",
    "_ remark : _ optimising the bound in theorem [ theo1 ] enables us to obtain the optimal scaling @xmath126 see the supplement for full details .",
    "the overall convergence rate of the cf estimator depends on how the design points @xmath55 are generated . for this",
    "there are many qmc methodologies available , each leading to different convergence rates for the worst case error @xmath127 ; see @xcite for a recent survey of some of these approaches",
    ". of particular interest in statistical applications is the case of random design points which we discuss below .",
    "* theory : randomised case . *",
    "modern qmc methods begin with a deterministic set / sequence of design points ( e.g. a halton sequence or a sobol sequence ) , then apply a random transformation leading to a low discrepancy set with high probability .",
    "below we consider three types of randomisation ; shifting , folding and scrambling .    _ shifting : _ in ` random shift ' qmc the design points @xmath55 are translated by a common uniform random vector @xmath128^d$ ] , so that @xmath129 for each @xmath130 . for convenience",
    "we write this ` shifted ' set as @xmath131 .",
    "applying theorem [ theo1 ] to @xmath131 and then marginalising over @xmath128^d$ ] produces a rmse bound for the cf estimator :    under assumptions 1 - 3 the random shift cf estimator has error bounded by @xmath132 - i[f]|^2 } \\leq c e_{h^{\\alpha_l}}^{\\text{sh}}(\\bm{v}^{m+1:n } ) h(\\bm{u}^{1:m})^{\\alpha - \\alpha_l } \\rho(\\bm{u}^{1:m})^{\\alpha_u-\\alpha_l } \\|f\\|_{h^\\alpha}\\end{aligned}\\ ] ] where @xmath133^d } e_{h^{\\alpha_l}}(\\bm{v}^{m+1:n}+\\bm{\\delta})^2 d\\bm{\\delta}\\end{aligned}\\ ] ] and @xmath116 is a constant that does not depend on @xmath20 , @xmath55 or @xmath54 .    for quasi - uniform @xmath54 ,",
    "cfs accelerate random shift @xmath3-qmc by a factor @xmath125 ( compare against sec .",
    "5.2 of @xcite ) .",
    "_ folding : _ a shifted and ` folded ' qmc rule takes the form @xmath134 where @xmath135 is the ` baker s transformation ' , given by @xmath136 .",
    "this transformation reduces error rates ; for example , for @xmath137^d)$ ] ( defined below ) , folding and shifting a uniform lattice @xmath138 leads to a rmse @xmath139 that is smaller than the rmse @xmath140 for a shifted lattice ( p. 59 of @xcite ) .",
    "the cf estimator here is @xmath141 : = i[f_m(\\cdot;\\bm{u}^{1:m } ) ] + q_{\\bm{b}}[f - f_m(\\cdot;\\bm{u}^{1:m});\\bm{v}^{m+1:n}+\\bm{\\delta}].\\end{aligned}\\ ] ] for convenience we denote the shifted and folded design points by @xmath142 .",
    "applying theorem [ theo1 ] to @xmath142 and then marginalising over @xmath128^d$ ] produces :    under assumptions 1 - 3 the shifted and folded cf estimator has error bounded by @xmath143 - i[f]|^2 } \\leq c e_{h^{\\alpha_l}}^{\\text{sh},\\bm{b}}(\\bm{v}^{m+1:n } ) h(\\bm{u}^{1:m})^{\\alpha - \\alpha_l } \\rho(\\bm{u}^{1:m})^{\\alpha_u-\\alpha_l } \\|f\\|_{h^\\alpha}\\end{aligned}\\ ] ] where @xmath144^d } e_{h^{\\alpha_l}}(\\bm{b}(\\bm{v}^{m+1:n}+\\bm{\\delta}))^2 d\\bm{\\delta}\\end{aligned}\\ ] ] and @xmath116 is a constant independent of @xmath20 , @xmath55 and @xmath54 .",
    "again , for quasi - uniform @xmath54 , cfs accelerate shifted and folded @xmath3-qmc by a factor @xmath125 ( compare against sec .",
    "5.9 of @xcite ) .",
    "_ scrambling : _ an explicit @xmath0-qmc rule that applies for all integer values of @xmath0 was recently discovered by @xcite . for simplicity focussing on @xmath145 ,",
    "these random design points achieve @xmath0-rates and , moreover , the rmse is controlled by a norm of the form @xmath146 .",
    "when @xmath0 is known and is an integer , one may achieve optimal rates and cfs provide no rate improvement .",
    "however , when @xmath147 , cfs can be used to transform these sub - optimal integrators into optimal integrators .",
    "* choice of kernel : * the qmc+cf methodology has some flexibility in terms of the choice of kernel @xmath107 that is used to construct the approximation @xmath69 .",
    "our main requirements here are : ( i ) @xmath107 imposes ` enough smoothness ' on @xmath69 in order to be able to faithfully approximate @xmath20 ( assumption 3 ) .",
    "moreover , @xmath107 should be tunable to achieve a pre - specified minimum level of smoothness . below we make an explicit connection between @xmath107 and the order of the associated ` native ' sobolev space that will allow us to satisfy this requirement .",
    "( ii ) the functions @xmath148 can be integrated analytically , so that @xmath111 $ ] is available in closed form .",
    "this second requirement leads us to consider tensor products of sobolev spaces , as described below .    to construct analytically integrable functional approximations we consider kernels that are given by polynomials .",
    "wendland s compactly supported functions @xcite are defined via the recursion @xmath149,\\ ] ] the base function @xmath150 with @xmath151 , and the integral operator @xmath152(r ) = \\int_r^\\infty t \\varphi(t)dt\\ ] ] ( @xmath153 ) , so that @xmath154 \\\\ 0 , & r > 1 \\end{array } \\right.\\end{aligned}\\ ] ] where @xmath155 and @xmath156 is a polynomial of degree @xmath79 ( see e.g. p.87 of @xcite for explicit formulae )",
    ". then the kernel @xmath157 has native space @xmath158 ( where the restriction @xmath159 is in principle required for the special case @xmath160 ) ( see e.g. p.109 of @xcite ) .",
    "with this kernel we can therefore guarantee a minimum level of smoothness . by rescaling",
    ", the kernel s support can be changed from the unit ball ( as above ) to balls of smaller radius .",
    "this in turn enforces sparsity on the system of interpolation equations that are the basis of the cf estimator and reduces the computational cost of inverting this linear system .",
    "wendland s kernel can not be integrated analytically in @xmath161 dimensions , violating requirement ( ii ) .",
    "however we can exploit recent work by @xcite that shows the @xmath2-dimensional tensor product space @xmath162 ) \\otimes \\dots \\otimes h^k([0,1])$ ] is norm - equivalent to @xmath163^d)$ ] , the sobolev space with dominating mixed smoothness : @xmath164^d \\rightarrow \\mathbb{r } \\ ; | \\ ; d^{\\bm{a}}f \\text { exists and } d^{\\bm{a}}f \\in l_p([0,1]^d ) , \\forall \\bm{a } \\in \\mathbb{n}_0^d \\text { with } a_i \\leq k\\}.\\end{aligned}\\ ] ] ( the distinction with @xmath162^d)$ ] is that the multi - index @xmath165 is now constrained component - wise , @xmath166 , rather than @xmath167 . ) in particular @xmath168^d ) \\subseteq h^k([0,1]^d)$ ] so that functions in @xmath169 are at least as smooth as functions in @xmath88 .",
    "we therefore propose to employ the product kernel @xmath170 whose native space is @xmath171 .",
    "the integral @xmath172^d } k_*^{(k)}(\\bm{x},\\bm{y } ) d\\bm{x}\\ ] ] of tensor products of wendland functions in eqn .",
    "[ new kernel ] can now be integrated analytically .",
    "this approach provides a convenient mechanism to control the degree of smoothness that we impose on the approximation @xmath69 .",
    "our methodology provides a variance reduction technique for qmc that is able to accelerate convergence rates , yet is also practical .",
    "the first numerical study below is a ` proof - of - principle ' designed to validate this specific claim in the empirical setting .",
    "* simulation study : * for objective assessment we exploited the test package proposed by @xcite .",
    "this package defines 6 function families , each of them characterized by some peculiarity , such as oscillation , discontinuity or corner peaks , with the property that their exact integrals are available .",
    "the ` discontinuous ' genz function provides an example where smoothness assumptions on the test function are violated .",
    "we used the matlab implementation of @xcite that is freely available at http://people.sc.fsu.edu/~jburkardt/m_src/testpack/testpack.html .    in the experiments below , we focus on the two qmc rules that are most widely used in practice . in the first experiment ,",
    "the random qmc point set @xmath55 was generated by truncating the halton sequence , scrambling the digits of the resulting points using the reverse - radix algorithm @xcite and applying a uniform random shift .",
    "this qmc rule achieves the @xmath173 rate on the subsequence @xmath174 when the test function has mixed partial derivatives of first order . to ensure that these qmc rules were implemented faithfully , we restricted attention to the case where @xmath175 so that @xmath176 was always a power of two .",
    "the training points @xmath54 were taken to be @xmath2-dimensional square lattices in all experiments .",
    "we considered the 6 genz functions in @xmath177 dimensions .",
    "the performance of qmc with and without cfs was compared , in each case ensuring that the total number of evaluations of the integrand @xmath20 was equal for all methods . for cfs ,",
    "the tensor - product wendland kernel with @xmath178 was employed ( i.e. approximation with functions @xmath179 , so @xmath180 ) .",
    "results are presented in fig .",
    "[ results fig ] .",
    "( for clarity we chose not present results for mc , since these were inferior to qmc methods in all cases considered . ) for the first 5 genz functions it holds that @xmath98 with @xmath181 and theory ( for the random case ) guarantees an acceleration of @xmath182 ; this is borne out in experimental results . in the 6th , discontinuous case the qmc+cf method does not out - perform qmc ( at least in dimension @xmath183 ) , as the functional approximation @xmath69 is poor due to violation of our continuity assumption . in all cases the performance of qmc+cf approaches that of qmc as the dimension @xmath2 increased . in higher dimensions",
    "( @xmath184 , not shown ) the qmc+cf and qmc estimators demonstrated effectively identical performance , in line with theory .",
    "the experiments were then repeated with rougher ( @xmath185 ) and smoother ( @xmath186 ) regression kernels .",
    "results in the supplement ( figs .",
    "s3 - 8 ) demonstrated a slight improvement in the performance of qmc+cf when @xmath187 , in line with theory , though generally estimates were robust to the choice of regression kernel . to further assess the generality of these conclusions , further experiments were performed using a different qmc rule ( truncated sobol sequence with scrambling due to @xcite ) .",
    "results in the supplement showed that the same conclusions can be drawn in each case .",
    "taken together , these results demonstrate that cfs can accelerate qmc , at least in low - dimensional settings , and thus complete our ` proof - of - principle ' .",
    "matlab code to reproduce these results is provided .",
    "* application to robot arm data : * to demonstrate the benefits of our methodology we consider the problem of estimating the inverse dynamics of a seven degrees - of - freedom robot arm .",
    "the task , as described in @xcite , is to map from a 21-dimensional input space ( 7 positions , 7 velocities , 7 accelerations ) to the corresponding 7 joint torques . following @xcite we present results below on just one of the mappings , from the 21 input variables to the first of the seven torques .",
    "the dataset consists of 48,933 input - output pairs , of which @xmath188 were used as a training set and the remaining 4,449 were used as a test set .",
    "the inputs were linearly rescaled to have mean zero and unit variance on the training set .",
    "the outputs were centred to have mean zero on the training set .",
    "we consider a hierarchical model based on 21-dimensional gaussian process ( gp ) regression .",
    "denote by @xmath189 a measured response variable at state @xmath190 , assumed to satisfy @xmath191 where @xmath192 are independent for @xmath193 and @xmath194 will be assumed known . in order to use training data",
    "@xmath195 to make predictions regarding an unseen test point @xmath196 , we place a gp prior @xmath197 where @xmath198 . here",
    "@xmath199 are hyper - parameters that control how training samples are used to predict the response at a new test point .",
    "a fully - bayesian treatment aims to marginalise over these hyper - parameters and we assign independent priors @xmath200 , @xmath201 in the shape / scale parametrisation , which we write jointly as @xmath202 . here",
    "@xmath203 , @xmath204 .",
    "to predict the value of the response @xmath205 corresponding to an unseen state vector @xmath196 , our estimator will be the bayesian posterior mean @xmath206 = \\int \\mathbb{e}[y_*|\\bm{y},\\bm{\\theta } ] \\pi(\\bm{\\theta } ) d\\bm{\\theta } , \\label{robot target}\\end{aligned}\\ ] ] where we implicitly condition on the covariates @xmath207 .",
    "phrasing in terms of our earlier notation , the test function is @xmath208 = \\bm{c}_{*,n } ( \\bm{c}_n + \\sigma^2 \\bm{i}_{n \\times n})^{-1 } \\bm{y}\\end{aligned}\\ ] ] where @xmath209 is the c.d.f for @xmath210 , @xmath211 and @xmath212 .",
    "each evaluation of the integrand @xmath213 requires @xmath214 operations due to the matrix inversion and this entails a prohibitive level of computation .",
    "a partial solution is provided by a ` subset of regressors ' approximation @xmath215 where @xmath216 denotes a subset of the full data ; see sec .",
    "8.3.1 of @xcite for full details .",
    "however even eqn .",
    "[ sor approx ] still represents a substantial computational burden in general . to facilitate the illustration below , which investigates the sampling distribution of estimators",
    ", we took a random subset of @xmath217 training points and a subset of regressors approximation with @xmath218 .",
    "the total computational time needed to obtain these results was 268 core - hours .",
    "for each test point @xmath196 the sampling standard deviation of @xmath219 was estimated from 10 independent realisations of the qmc procedures .",
    "for cf we used a randomly - shifted , scrambled halton sequence ( @xmath173 ) and wendland kernels with @xmath178 ( @xmath180 ) , so that theory predicts an acceleration factor of @xmath14 .",
    "the estimator standard deviations were estimated for all 4,449 test points ( with @xmath220 ) and the full results are shown in fig .",
    "[ robot fig ] .",
    "note that each test point @xmath196 corresponds to a different test function @xmath20 and thus these results are quite objective , encompassing thousands of different integration problems . for the vast majority of integration problems , cf accelerated the standard qmc estimator . here",
    "the computational time to construct a functional approximation ( inverting a @xmath221 matrix ) was negligible ( 3% ) in comparison to the cost of evaluating the function @xmath20 once . the total additional computational time associated with",
    "the qmc+cf methodology was 2% greater than for qmc , which is easily justified by the substantial variance reductions ( @xmath222 ) that are realised in this application .",
    "supplementary results ( fig .",
    "s9 ) compare qmc+cf to mc+cf ( standard mc sampling ) .",
    "qmc methods are becoming increasingly relevant in modern statistics applications @xcite and it is surely a priority to target the rate constants governing the practical performance of these algorithms .",
    "cfs provide one route to achieve this goal , providing substantial variance reductions in many of the examples we considered .",
    "indeed , cfs allow us to use a sub - optimal qmc rule ( e.g. as built into existing software packages ) and yet , with minimal additional coding , obtain a qmc+cf algorithm that attains optimal convergence rates .",
    "the focus on unknown smoothness @xmath0 distinguishes our work from previous literature on the connection between integration and functional approximation , e.g. @xcite .    functional approximation , and hence our qmc+cf methodology , has a computational cost associated with solution of a linear system . whilst negligible in our experiments , this cost could be reduced if necessary using standard approximations and/or compactly supported kernels .",
    "on the other hand , we note that qmc is often used when @xmath20 is expensive to evaluate and in such situations it is likely that evaluation of the integrand , rather than solution of a linear system , will be the main computational bottleneck .",
    "our focus was on sobolev spaces , but it is known that a faster rate @xmath223 is possible in the subspace @xmath224^d)$ ] , for any @xmath225 , and explicit point sets are available ( for integer @xmath0 ) @xcite . an immediate extension is to establish optimal rates for cfs in this class of functions . in a related direction , one can in principle obtain _ dimension - independent _ rates by imposing a ( strong ) assumption of polynomial tractability on the rkhs .",
    "this is achieved by generalising to weighted sobolev spaces , such that the integrand @xmath20 ` depends only weakly on most of the components of @xmath43 ' .",
    "further details are provided in @xcite and form part of our ongoing research .",
    "the methods that we describe are immediately applicable in a range of applications including marginalisation of hyper - parameters in classification @xcite , probabilistic inference for differential equations @xcite , computation of model evidence @xcite and approximation of the partition function in social network models @xcite .",
    "finally we note that cfs generalise to other integration methods including bayesian quadrature @xcite and related kernel - based quadrature rules @xcite , in which the worst case error is also controlled by an rkhs norm @xmath40 ; this will be the focus of our ongoing research .",
    "the authors are grateful to dan simpson , mathieu gerber and ben collyer for helpful discussions .",
    "cjo was supported by epsrc [ ep / d002060/1 ] and the arc centre of excellence for mathematics and statistical frontiers .",
    "mg was supported by epsrc [ ep / j016934/1 , ep / k034154/1 ] , an epsrc established career fellowship , the eu grant [ eu/259348 ] and a royal society wolfson research merit award .",
    "t.  gunter , m.a .",
    "osborne , r.  garnett , p.  hennig , and s.j .",
    "sampling for inference in probabilistic models with fast bayesian quadrature . in _ advances in neural information processing systems _ , pages 27892797 , 2014 .",
    "this section contains all simulated data results discussed in the paper .",
    "specifically , for each of the 6 test functions described by @xcite , we display results based on wendland s compactly supported regression kernel @xcite with smoothness parameter @xmath79 ( described above ) set equal to either          we used the matlab implementation of @xcite that is freely available ( web address given in the main text ) .",
    "the qmc design points can be generated using the in - build matlab functions ` haltonset ` , ` sobolset ` and ` scramble ` .",
    "full matlab code used to generate these results is provided in the electronic supplement .",
    "we re - ran the robot arm simulation in order to compare the qmc+cf estimator with the mc+cf estimator ; that is , a quasi - uniform set @xmath54 were used to construct a control functional @xmath69 , whilst a monte carlo sample @xmath55 were used to integrate the difference @xmath105 ."
  ],
  "abstract_text": [
    "<S> quasi - monte carlo ( qmc ) methods are being adopted in statistical applications due to the increasingly challenging nature of numerical integrals that are now routinely encountered . for integrands that are @xmath0-times differentiable </S>",
    "<S> , an @xmath0-optimal qmc rule converges at a best - possible rate @xmath1 , where @xmath2 is the dimension of the integral . </S>",
    "<S> however , in applications the value of @xmath0 can be unknown and/or a rate - optimal qmc rule can be unavailable . </S>",
    "<S> standard practice is to employ @xmath3-optimal qmc where the lower bound @xmath4 is known , but in general this does not exploit the full power of qmc . </S>",
    "<S> we present an elegant solution that uses control functionals to accelerate @xmath3-qmc by a factor @xmath5 , recovering optimal convergence rates . </S>",
    "<S> a challenging application to robotic arm data demonstrates a substantial variance reduction in predictions for mechanical torques . </S>"
  ]
}