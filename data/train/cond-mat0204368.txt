{
  "article_text": [
    "risk management has become increasingly important in financial institutions over the last decade . since the publication of jp morgan s riskmetrics@xmath0  @xcite in the nineties , risk management and risk control departments in banks have grown significantly in size and importance .",
    "the task is to fulfill regulatory requirements , to add transparency about a bank s risk profile by a quantitative assessment of risks , to develop the necessary it - solutions which allow to process the huge amount of data of a bank , and , finally , to integrate this information in a risk - return ( rorac = return on risk - adjusted capital ) based steering process of the bank .",
    "ultimately , a proper risk management and risk control process is recognized by rating agencies and investors so that shareholder value is added to the bank .",
    "banks first focused on controlling potential losses due to market fluctuation , such as changes in the s&p 500 stock index , changes in interest and currency exchange rates , which is termed market risk .",
    "internal market risk models are nowadays rather matured and accepted by regulators for the calculation of the required capital to be held as buffer against such losses .",
    "in contrast to these elaborated statistical models for market risks , credit risks ( i.e. , risks due to defaulted obligors ) have to be covered by simply 8% capital of the bank s risk - weighted assets .",
    "implicitly , this charge also includes other risks such as operational risks . since the new basel accord on capital adequacy issued by the basel committee on banking supervision in february and september 2001 @xcite , known as basel ii , it is clear that regulators will demand banks to hold equity capital against operational risks explicitly .    a common industry definition of operational risk ( or )",
    "is the risk of direct or indirect losses resulting from inadequate or failed internal processes , people and systems or from external events @xcite .",
    "see @xcite for a practice - oriented introduction to the issue .",
    "possible or - risk categories are @xcite : ( i ) human processing errors , e.g. , mishandling of software applications , reports containing incomplete information , or payments made to incorrect parties without recovery , ( ii ) human decision errors , e.g. , unnecessary rejection of a profitable trade or wrong trading strategy due to incomplete information , ( iii ) ( software or hardware ) system errors , e.g. , data delivery or data import is not executed and the software system performs calculations and generates reports based on incomplete data , ( iv ) process design error , e.g. , workflows with ambiguously defined process steps , ( v ) fraud and theft , e.g. , unauthorized actions or credit card fraud , and ( vi ) external damages , e.g. , fire or earthquake .",
    "thinking of theses categories as  operational risk processes \" it is clear that there are _ functionally defined dependencies _ between individual processes , which all together bring a big organization to work . consider the following example for illustration : a system error leads to an incomplete data import into a risk calculation engine , resulting in a wrong calculation of risk figures , and eventually to a human decision error by the trader , who closes a possibly profitable position unnecessarily to reduce a risk which in fact does not exist .    in the end misleading or lagging information , or system and workflow failures will always result in financial loss for a bank .",
    "indeed , practitioners have recognized these dependencies in operational risk events and mandated units like the internal audit and risk control departments to control processes for the bank , and generated functions like a chief operating officer ( coo ) to optimize them .",
    "operational risk error trees between the above categories have been formalized in @xcite in more detail .    since the mid nineties financial markets have also attracted physicists in academia .",
    "one of the main reason is that financial time series exhibit several statistical peculiarities , many of them being common to a wide variety of different markets and instruments . as such they could possibly be  universal \"",
    ", i.e. , independent of market details like instruments , country , and currency , and be the signature of collective phenomena in financial markets ( see @xcite and references therein ) .",
    "collective phenomena have been widely studied in physics in the context of phase transitions .",
    "collective phenomena are often responsible for insensitivity of overall system behavior to details of an underlying dynamics . specifically at phase transitions they give rise to power - law behavior , scale - invariance and self - similarity .",
    "similar properties of financial time series might therefore well be understood as a consequence of agents in a market acting collectively .",
    "bringing together ideas from physics about collective phenomena and best industry practice for risk measurement the present paper details a possible statistical approach to determine the necessary equity capital to be held by banks to cover losses due to operational risks .",
    "in physical terms our model resembles a lattice gas with heterogeneous , functionally defined couplings . in such a description ,",
    "bursts and avalanches of process failures correspond to droplet formation associated with a first order phase transition .",
    "the paper is organized as follows . in sect .",
    "[ var_concept ] we describe the value - at - risk concept for risk management and control . in sect . [ or ] we describe the approaches discussed in the context of basel ii for operational risk measurement . a new approach based on functionally dependent correlations giving rise to collective behavior",
    "is introduced in sect .",
    "[ sect_latticegas ] . finally , sect .",
    "[ sect_conclude ] summarizes our results .",
    "risk management in banks is based on diversification , hedging and equity capital as loss buffer .",
    "the bank charges its customers a premium for its risks so that ( expected ) losses in one market segment are on average compensated by profits in others .",
    "other risks , especially market and increasingly credit risks , are hedged ( insured ) via the derivative market .",
    "unexpected losses , which are not diversified or hedged , are covered by the bank s equity capital .",
    "how much capital a bank needs to cover its risks is determined by the so - called  value - at - risk \" ( var ) .",
    "var can be defined as the worst loss in excess of the expected loss that can happen under normal market conditions over a specified horizon @xmath1 at a specified confidence level @xmath2 .",
    "more formally , var measures the shortfall from the @xmath2-quantile of the loss distribution in excess of the expected loss , @xmath3 , within the time period @xmath1 discounted at the risk - free rate @xmath4 to time @xmath5 : _ q , t = ( q_q[l(t ) ] - el ) ^-rt  , [ i3]where the @xmath2-quantile worst case loss , @xmath6 $ ] , is defined at confidence level @xmath2 through ( l(t ) > q_q[l(t)])= 1-q  .",
    "[ i1 ] as indicated in ( [ i3 ] ) , var depends on the confidence level @xmath2 and the risk horizon @xmath1 .",
    "the choice of these parameters depends on the application .",
    "if var is simply used to report or compare risks , these parameter can be arbitrarily chosen , as long they are consistent . if , however , var is used as a basis for setting the amount of equity capital , the parameters must be chosen with extreme care : the confidence level must reflect the default probability of the bank within the risk horizon , and the risk horizon must be related to the liquidation period of risky assets , recovery time of ill - functioning processes , or , alternatively , to the time period necessary to raise additional funds .",
    "this explains why regulators have chosen a high confidence level of 99% and a 10-day horizon to determine the minimum capital level for market risks . for credit risks and capital allocation ,",
    "banks choose @xmath2 and @xmath1 even higher about 99.95% and one year , respectively .    in the financial industry there",
    "exist established statistical models for market and credit risk .",
    "statistical models for operational risk start now to be discussed in the risk management community , especially in the context of basel ii @xcite . whereas internal market risk models are already recognized by regulators and are also used in banks for capital allocation ,",
    "regulators are much more critical about internal statistical models for credit and operational risk .",
    "this is clearly less related to the mathematical complexity  although credit and operational risks are more difficult to model than market risks  but to problems with respect to input data which are much harder to validate than in the case of market risk .",
    "the basel committee for banking supervision has proposed three alternative approaches to operational risk measurement @xcite : the  basic - indicator approach ( bia ) \" , the  standardized approach ( sa ) \" , and the  advanced measurement approach ( ama ) \" . in the bia the required capital for operational risk",
    "is determined by multiplying a single financial indicator , which is gross income ( interest , provision , trading , and other income ) by a fixed percentage ( called the @xmath7-factor ) .",
    "the sa differs from the latter in that banks are allowed to choose business line specific weight factors , @xmath8 , for the gross income indicator , @xmath9 , of the @xmath10 business line .",
    "the total regulatory capital charge , @xmath11 , is the simple sum of the capital required per business line , rc = _ k _ k i_k  .the weight factors @xmath7 and @xmath8 are calibrated such that the required regulatory capital for operational risk would be 17 - 20% of the current regulatory capital on bank average standards .",
    "the ama consist of three sub - categories : the  internal measurement approach ( i m a ) \" , the  loss distribution approach ( lda ) \" and the  scorecard approach ( sca ) \" . it is a more advanced approach as it allows banks to use external and internal loss data as well as internal expertise .    in the i m a the required capital is calculated as the sum over multiples of the expected loss per or - risk category / business line cell rc = _ i , k _ ik",
    "el_ik  , where @xmath12 is the risk category and @xmath13 the business line . the expected loss is quantified as the product of the annual or - event probability , an exposure indicator per business line and risk category , and the loss percentage per exposure .",
    "all parameter estimates have to be disclosed to the supervisors .",
    "since the @xmath14-factor is computed on an industry based distribution , it will be possible to adjust the capital charge by a risk profile index , which accounts for the bank s specific risk profile compared to industry .",
    "the lda and sca are very similar as both approaches are based on a statistical var - model .",
    "details of the lda approach are outlined in ref .",
    "@xcite . in both approaches",
    "the bank estimates for each risk category / business line cell the probability distributions of the annual event frequency and the loss severity (= exposure @xmath15 loss fraction per exposure ) .",
    "the difference between the lda and the sca is that in the former only internal or external historical loss data are used for estimating the distribution functions .",
    "in addition to this , banks are also allowed to apply expert knowledge to estimate the distribution functions in the sca .",
    "this is a forward looking approach .",
    "it is particularly suited for operational risk , as processes that have failed are usually changed ; hence historical loss data could provide potentially misleading information . even if banks have an exhaustive internal database of losses , it can hardly be considered as representative of extreme losses .",
    "hence , expert assessments and external loss databases are necessary .",
    "the problem with the latter data source is that external historical losses must be scaled to fit the balance sheet of the bank ( it must be possible that the losses can occur in the bank ) .",
    "popular choices for the loss severity distribution functions are the lognormal , gamma , beta , weibull distribution .",
    "common choices for the loss frequency distribution function are the poisson or negative binomial distribution . in a top - down approach different or - risk categories are assumed to be independent . for each business unit and for each or - category the or - loss is simulated in a monte - carlo simulation by drawing a realization @xmath16 from the loss frequency and sampling @xmath16 realizations of the loss severity @xmath17 ( @xmath18 ) .",
    "the loss in such a sample is [ or1 ] l_ik = _",
    "m=1^n_ik x_ik^m  . drawing a histogram of outcomes of @xmath19",
    "provides the loss distribution function per risk category / business line cell .",
    "the value - at - risk is read off from the tail in excess to the expected loss as described in sect .",
    "[ var_concept ] . due to the assumption of statistical independence , the loss distribution",
    "can also be calculated analytically as the convolution product of the loss frequency and the loss severity distribution .",
    "the required capital for the bank as a whole can either be calculated as the simple summation of the capital charges across each of the risk category / business line cell .",
    "this is the method given by the basel committee on banking supervision in the internal measurement approach . or",
    ", the mc - sampling can be extended beyond the risk category / business line cell by @xmath20 , which takes diversification between the risk category / business line cell into account .    a critical point which concerns all presently discussed approaches is the correlation between or - losses .",
    "in this paper our focus will be how correlations and dependencies between or - risk events can be integrated in the lda / sca .",
    "since markowitz s centennial work on portfolio theory @xcite , _ diversification _ and _ dependencies _ between risk events are modeled by the covariance of stochastic processes . because empirically only the mean and the covariance of these processes are reliably determined from market data , it is common practice to choose correlated gaussian white noise for modeling correlations . as a consequence",
    "the loss distribution is unimodal with frequent small losses and a few extreme losses , which  dependent on the distribution of the loss severity  are responsible for fat tails in the loss distribution .",
    "collective losses or even crashes such as burst and avalanches of losses are not contained in this description .",
    "a main point in this paper is that this stochastic dependency of risk events is not sufficient for all risk categories : one frequently also observes _",
    "direct , functional and non - stochastic dependencies_. functional dependency between risk events is most pronounced in operational risk events .",
    "processes in a ( large ) organization are usually organized so as to mutually support each other .",
    "thus , if a process fails , this will usually be detrimental to other processes relying on receiving input or support of some sort from the failing process in question , so that they run a higher risk of failing as well .",
    "it therefore seems inadequate to model operational risk events individually per risk category / business line cell and aggregate losses afterwards over some covariance matrix , which would be the choice when approaching operational risks analogously to market risks . in the following",
    "we extend the lda / sca by taking the _ functional dependencies _ between processes into account .",
    "we consider a simple two state model here , i.e. , a processes can be either up and running or down . for the process corresponding to the or - event @xmath12 we designate these states as @xmath21 and @xmath22 , respectively . in following",
    "we will skip the business line index @xmath13 for simplicity .",
    "the interest is in obtaining reliable estimates of the statistics of processes that are down at any time and of the statistics of losses incurred at any time . as the loss severity incurred by a given process",
    "going into the down state may vary randomly from event to event , solving the latter problem requires convolving the statistics of down - events with the loss severity distribution related to the process failures .",
    "the reliability of individual processes will vary ( randomly ) across the set of processes , and so will the degree of functional interdependence .",
    "these random heterogeneities constitute an element of quenched disorder , whereas the loss severities incurred by down processes constitute an element of annealed disorder as they are ( randomly ) determined anew from their distribution each time a process goes down .",
    "an appealing feature for the modeler of operational risks therefore is the _ independence _ of the dynamic model of the interacting processes and the loss severity model ( i.e. the estimate of the pdfs of loss severity incurred by individual process failures . )",
    "a typical assumption for the latter is to take them as being distributed according to a log - normal distribution with suitable parameters for means and variances , which we will choose in the following .      to motivate the dynamics of the functional approach , note that all processes need a certain amount of  fueling \" or support in order to maintain a functioning state for the time increment @xmath23 within the risk horizon , @xmath24 ( think of human resources , information , input from other processes , etc . ) . here",
    ", only the generic features of the model shall be outlined .",
    "hence , the increment @xmath25 is chosen such that all processes can fully recover within this time interval , i.e. , the state @xmath26 of each process can flip each time step . for practical applications in banks , one would model the recovery process more carefully : specific death - period after the failure of the @xmath27 process would be considered , and one would differentiate between process failures being discovered and adjusted up to a certain cut - off time , e.g. , end - of - day , at which a process would have been completed @xcite .",
    "these features are not generic and can only be discussed related a specific or - event under consideration .",
    "we denote by @xmath28 the total support received by process @xmath12 at time @xmath29 , and choose it to take the form h_i(t ) = _ i - _ j w_ij n_j(t ) + _ i(t )  .",
    "[ hioft ] that is , it is composed of ( i ) the _ average _ total support @xmath30 that would be provided by a fully operational network of processes ( in which @xmath31 for all @xmath12 ) .",
    "this quantity is ( ii ) _ diminished _ by support that is missing because of failing processes which normally feed into the process in question ; ( iii ) lastly , there are fluctuations about the average which we take to be correlated gaussian white noise with  by proper renormalizing @xmath30 and @xmath32  zero mean and unit variance .",
    "correlated gaussian noise is introduced to model equal - time cross correlations between or - risk categories in analogy to the approach proposed by the basel committee for banking supervision for credit risk , _",
    "i(t ) = y(t ) + _ i(t )  , [ etai]where @xmath33 is a common factor for all or - risk categories with equal - time correlation coefficient @xmath34 , and the @xmath35 are idiosyncratic terms .",
    "note that non - linear effects could be included by modifying ( [ hioft ] ) to @xmath36 .",
    "note also that , as in credit risk modeling , the common factor @xmath37 could further be decomposed into sector - contributions @xmath38 so as to describe more complicated equal - time correlations . to keep this treatment transparent",
    ", we will present the formalism without these extensions .",
    "process @xmath12 will fail in the next time instant @xmath39 , if the total support for it falls below a critical threshold . by properly renormalizing @xmath30",
    ", we can choose this threshold to be zero , thus ( @xmath40 is the step - function : @xmath41 for @xmath42 and 0 else ) n_i(t+t ) & = & ( - _ i + _ j w_ij n_j(t ) + & & - y(t ) -_i(t ) )  .",
    "[ nioft+ ] the losses incurred by process @xmath12 are then updated according to l_i(t+t ) = l_i(t ) + n_i(t+t ) x^i_t+t  , where @xmath43 is randomly sampled from the loss severity distribution for process @xmath12 . note",
    "that the _ process dynamics _ is _ independent _ of assumptions concerning their loss severity distributions within the present model",
    ".    one can integrate over the distribution of _ idiosyncratic _ noises to obtain the conditional probability for failure of process @xmath12 given a configuration @xmath44 of down - processes and a realization of the common factor @xmath45 at time @xmath29 , [ pdown ] n_i(t+t)_n(t ) , y(t ) & & + & & .(n_i(t+t)=1 |n(t ) , y(t ) ) + & & = ( )  .here @xmath46 denotes the cumulative normal distribution .",
    "note that we have set @xmath47 , where @xmath48 is the _ unconditional _ expected probability for process failures within the time - increment @xmath25 .",
    "this is consistently justified by setting @xmath49 for all @xmath50 and @xmath51 in eq .",
    "( [ pdown ] ) .",
    "note that up to the functional term  @xmath52 \" this approach corresponds to the approach adapted by the basel committee for banking supervision for credit risk @xcite .",
    "the couplings @xmath32 can be determined by considering the transition probabilities , @xmath53 , for process @xmath12 failure within the time - increments @xmath25 , given that in the configuration at time @xmath29 process @xmath50 is down , whereas all other processes are running , and @xmath54 . introducing the shorthand @xmath55 for this configuration , we can write p_ij & = & .(n_i(t+t)=1 |c_j ) + & = & ( )  . [ pij ]",
    "this leads to w_ij = ^-1(p_ij ) - ^-1(p_i )  . [ fixwij ]",
    "analogous identities would be available for determining higher order connections @xmath56 , if nonlinear effects were taken into account .",
    "note that the probabilities for process failure depend only on the increment @xmath25 and not on the time @xmath29 due to the stationarity of the dynamics .    to illustrate how these parameter are fixed in practice ,",
    "consider the following . either from a historical loss database , or from an expert assessment the following two questions must be answered per or - risk category and business line :    1 .   what is the expected period , @xmath57 , until process @xmath12 fails for the first time in a fully operative environment , and 2 .",
    "given that only process @xmath50 has failed , what is the expected period , @xmath58 , for process @xmath12 to fail also ?    noting that with @xmath59 one finds that _ i = _ z = 1^zt ( 1-p_i ) ^z-1p_i =  , and analogously , _",
    "ij= _ z = 1^zt ( 1-p_ij ) ^z-1p_ij =  .",
    "these identities express the @xmath60 and @xmath61 in terms of estimated average times of failure , and are used to fix the model parameters completely .",
    "note that according to ( [ pij ] ) @xmath61 can be interpreted as a non - equal time correlation for process failures .",
    "note also that , incidentally , the dynamics ( [ nioft+ ] ) resembles that of a lattice gas ( defined on a graph rather than on a lattice ) , the @xmath26 denoting occupancy of a vertex , the @xmath32 interactions , and @xmath30 taking the role of chemical potentials regulating a - priori occupancy of individual vertices .",
    "the present system is heterogeneous in that ( i ) the @xmath30 vary from site to site , ( ii ) the couplings @xmath32 have a functional rather than regular geometric dependence on the indices @xmath12 and @xmath50 designating the vertices of the graph . moreover , in the physics context , one usually assumes noise sources other than gaussian so that cumulative probabilities are described by fermi - functions rather than cumulative normal distributions as above .",
    "the quantitative difference is minute , however .",
    "the model dynamics as such can not be solved analytically for a general heterogeneous network .",
    "we shall resort to monte - carlo simulations to study its salient properties .",
    "the main qualitative features can , however , also be observed in the simplified situation of a homogeneous network consisting of identical processes , having the same connectivity at each node .",
    "a mean - field analysis of such a simplified situation will be given as well .",
    "as the presence of the common factor expressed by the @xmath34-term in eq .",
    "( [ etai ] ) would influence only quantitative details of the system s behavior , we will further present the analysis without correlation to the common factor by setting @xmath62 .",
    "key features of the collective behavior of networks of interacting processes can easily be anticipated either directly from a discussion of the dynamic rules , or from the analogy with the physics of lattice gasses .",
    "consider a network in which the unconditional probabilities for process failures , @xmath60 , are small , but process interdependence is large and consequently _ conditional _ probabilities for process failures , @xmath61 , are sizeable . in such a situation , spontaneous failure of individual processes may induce subsequent failures of other processes with sufficiently high probability so as to trigger a breakdown of the whole network .",
    "if , on the other hand , process interdependence remains below a critical threshold value , individual spontaneous failures will not have such drastic consequences , and the whole network will remain in a stable overall functioning state .    of particular interest for the risk manager is the case , in which process interdependence is low enough to make a _ self - generated break - down _ of the network extremely unlikely , but parameters are nevertheless such that a stable overall functioning state of the network _ coexists _ with a phase in which nearly the complete network is in the down state ( two phase coexistence ) .",
    "in such a situation , it may be _ external strain _ which can induce a transition from a stably functional situation to overall breakdown .",
    "analogous mechanisms are believed to be responsible for occasional catastrophic breakdowns in bistable ecosystems @xcite .    with increasing unconditional probabilities for process failures",
    "it becomes meaningless to distinguish between an overall functioning , and a non - functioning phase of the network , two  phase coexistence ceases to exist  as in ( lattice ) gasses  at a critical point .      in the following we validate some of our intuitions about global network behavior using monte - carlo simulations .",
    "the monte - carlo dynamics can either be conceived as parallel dynamics ( all @xmath26 are at each time step simultaneously updated according to ( [ nioft+ ] ) or ( [ pdown ] ) ) , or as ( random ) sequential dynamics ( only a single @xmath26 is ( randomly ) selected for update according to ( [ nioft+ ] ) in any given time - step , in which case the time increment must scale with the number @xmath63 of processes in the net as @xmath64 ) .",
    "for the analysis of operational risks , losses are accumulated during a monte - carlo simulation of the process dynamics over the risk horizon , @xmath1 . runs over _",
    "many _ risk horizons then allow to measure loss distribution functions for individual processes within the network of interacting processes , or of business units or the full network by appropriate summations .    for the simulations , we choose a random setting , i.e.",
    ", unconditional failure probabilities are taken to be homogeneously distributed in the interval @xmath65 $ ] and we determine random conditional failure probabilities as @xmath66 , with @xmath67 homogeneously distributed in @xmath68 $ ] , which fixes the ratio @xmath69 .",
    "1 shows a situation where a functional network coexists with a situation in which the network is completely down , and parameters are such that spontaneous transitions between the phases are not observed during a simulation .",
    "the upper track shows the loss record of a system initialized in the  all down \" state whereas the lower track exhibits the loss record of the _ same _ network initialized in the fully functioning state .    the loss distribution for the _ functional _ network is unimodal with a bulk of small losses and a fat tail of extreme losses , which are driven by the loss severity distribution .",
    "by increasing the functional interdependence at unaltered unconditional failure probabilities , the functioning state of the network becomes unstable .",
    "a spontaneous transition into the ` down ' state is observed during a single run of 50000 monte - carlo steps .",
    "two interesting features about this transition to complete breakdown deserve mention : ( i ) the time to breakdown can vary within very wide limits ( we have not attempted to measure the distribution of times to breakdown and its evolution with system parameters such as range of values for conditional failure probabilities ) , ( ii ) there are no detectable precursors to the transition ; it occurs due to large spontaneous fluctuations carrying the system over a barrier , in analogy to droplet formation associated with first order phase transitions .",
    "we should like to emphasize that realistically the system dynamics after an overall break - down of a process network would no longer be the spontaneous internal network dynamics : recovery efforts would be started , increasing support for each process by a sufficient amount such as to reinitialize the network in working order .",
    "repeated spontaneous transitions in both directions can be observed only in a rather small network ( fig .",
    "2 ) . the corresponding loss distribution will be bimodal .",
    "3 illustrates the principle of a strain - simulation . in each case , the system , if in the operational state , is repeatedly put under external strain by turning off 5 randomly selected functioning processes every 1000@xmath70 time step , and letting the system evolve under its internal dynamics thereafter .",
    "such a disturbance can either trigger a breakdown of the system or not .",
    "in the former case , if the system is found fully down 1000 time steps later , it is reinitialized in the fully operational state and once more disturbed 1000 steps later .",
    "one observes that the operational low - loss phase which we have seen in fig . 1 to coexist with the non - operational phase is resilient against disturbances of the kind described above .",
    "the same is true if @xmath69 is increased to 2.7 . at @xmath71",
    "an external strain succeeds _ once _ during the simulation to trigger breakdown of the net , whereas at @xmath72 breakdown under external strain of the given strength is the regular response of the system ( with a few exceptions and occasional spontaneous recoveries ) .    of interest to the risk manager in the end",
    "are the total losses accumulated over a risk horizon , @xmath1 , l(t ) = _ i l_i(t )  , more specifically , the corresponding probability density function .",
    "4 presents such distribution of accumulated losses for a network that remains operational throughout the simulation . for this simulation",
    "we have chosen @xmath73 .",
    "the loss distribution has an extended tail ( barely visible on the scale of the data , with a 99.5% quantile at 1400 , and the largest aggregated loss observed during the simulation over a time span of @xmath1 at 3400 , i.e. by a factor of more than 3 larger than the expected loss for the chosen risk horizon @xmath1 .      the principle dynamic properties of the model exist independently of its heterogeneous nature . to exhibit systematic relations between dynamic features and model parameters",
    ", we shall elucidate them in the simplified setting of a homogeneous network of identical processes , which we analyze within a mean - field approximation .",
    "for this we assume a homogeneous coupling , @xmath74 and @xmath75 , where @xmath76 is the coordination number of the graph ( taken to be identical at each vertex ) , and replace time  dependent quantities in eq .",
    "( [ pdown ] ) by long - time stationary averages , @xmath77 . in a homogeneous system , averages are independent of the process index @xmath12 and time , such that @xmath78 and @xmath79 .",
    "this gives the mean - field equation ( without correlation to the common factor , @xmath80 ) [ meanfield ] n",
    "= ( ^-1(p ) + w_0 n )  . depending on @xmath81 and the average coupling strength , @xmath82",
    ", this equation has either one unique solution or three solutions , with one unstable solution at intermediate @xmath83 , and two stable solutions , @xmath84 0 or 1 .",
    "figure 5 shows stable and unstable solutions as functions of @xmath82 and @xmath85 for a given value of @xmath81 .",
    "the phase diagram ( fig .",
    "6 ) summarizes regions in the @xmath81@xmath82 plane , where operational and non - operational phases of the network coexist showing limits of stability of the low - loss and high - loss solution . for @xmath81 exceeding a critical value @xmath86 there is a unique disordered phase with relatively large values of @xmath83 .    for a sufficiently small unconditional failure probability @xmath81",
    "an initially running process network will for weak functional dependence remain in the running state , despite of spontaneous individual process failures .",
    "such a network is in a functioning state .    for stronger functional correlation",
    "the functional state of the network becomes unstable .",
    "fluctuations in the number of processes that are down at any given time can trigger a burst or avalanche of failures  a collective phenomenon corresponding to droplet formation associated with a first order phase transition .    for intermediate degrees of functional dependence ,",
    "the network allows for two ( meta)stable states , an operational and a non - operational network . while spontaneous fluctuations in the number of failing processes will in large networks fail to trigger transitions between the two states , external strain may well do , as demonstrated in the strain simulations above .",
    "seen from the functioning side , the closer the parameters are to an instability line , the smaller the strain needed to trigger avalanches of failures leading to the fully defunct state of the system .",
    "this behavior is quantified by the susceptibility @xmath87 ( fig . 7 ) . with support for each process decreasing by an amount @xmath88 ,",
    "the fraction of down processes will change by n ( w_0,p ) h with ( w_0,p ) = -  , where @xmath89 $ ] .",
    "note that the susceptibility is proportional to the sensitivity of the fraction of down - processes with respect to the unconditional probability of process failure .",
    "triggering complete failures becomes easier close to instability lines for two reasons : ( i ) the susceptibility diverges as instability lines are approached ; ( ii ) the unstable solution @xmath90 is closer to the stable equilibrium solution ; external strain only needs to push the system beyond @xmath90 to destabilize the functioning state",
    ".    such a regime could therefore be dangerous for a network of mutually supporting processes in a bank .",
    "due to the long periods in one of the metastable states , the bank would not necessarily realize the potential of big losses due to bursts and avalanches of process failures . with a basically unchanged process setup the network could collapse and cause significant losses , either due to external strain or rare fluctuations of internal dynamics .",
    "owing to the stability of the metastable states , the bank will then have to spend a lot of efforts in order to bring the network back to a functional state , which will cause additional costs .",
    "in this paper we have outlined how ideas from physics of collective phenomena and phase transitions can naturally be applied to model building for operational risk in financial institutions .",
    "our main point was that functional correlations between mutually supportive processes give rise to non - trivial temporal correlation , which could eventually lead to the collective occurrence of risk event in form of burst , avalanches and crashes . for risks associated to process failure ( operational risks )",
    "a functional dependence seems to be the appropriate way for modeling sequential correlations .    from the physics point of view",
    ", the appropriate model is rather simple , being a heterogeneous variant of the well studied lattice gas model . despite the heterogeneities",
    ", it has a first order phase transition ( driven by average interaction strength ) at sufficiently low average a - priori probability of process failures , showing coexistence between an overall functioning state ( gas ) and a state of catastrophic breakdown ( liquid ) . as the a - priori probability of process failures is increased the first order transition ends at a ( liquid / gas ) critical point .",
    "one of the most critical lessons for risk control from our analysis is the possible metastability of networks of interacting processes : the bank would not necessarily realize the potential of big losses due to bursts and avalanches of process failures , as there are no detectable precursors to such transitions . with a basically unchanged process setup the network could collapse and cause significant losses , either due to external strain or rare fluctuations of internal dynamics .",
    "owing to the stability of the metastable states , the bank will then have to spend a lot of efforts in order to bring the network back to a functional state , which will cause additional costs . to assess the metastability banks have to perform stress tests .",
    "it should be noted that realistically the system dynamics after an overall break - down of a process network would no longer be the spontaneous internal network dynamics : recovery efforts would be started , increasing support for each process by a sufficient amount such as to reinitialize the network in working order .    in a forthcoming publication",
    "we will also show that the random walk model for financial time series commonly used in banks can naturally be extended to incorporate functional dependencies leading to collective effects .",
    "this will lead to models which , while bearing some resemblance with agent - based models of markets ( see , e.g. , @xcite ) are different from them in other respects @xcite .",
    "van  den brink , _ operational risk , the new challenge for banks _",
    ", palgrave , ( hampshire ( uk ) , 2001 ) ; _ die bedeutung operativer risiken fr eigenkapitalunterlegung und risikomanagement _",
    ", tagungsbericht vom duisburger bank - symposium , rolfes ( editor ) ( duisburg , 2002 ) .",
    "d. challet and y.c .",
    "zhang physica a * 246 * , 407 ( 1997 ) ; a. cavagna , j.p .",
    "garrahan , i. giardina and d. sherrington , phys .",
    "lett . * 83 * , 4429 ( 1999 ) ; a.c.c .",
    "coolen and j.a.f .",
    "heimel , j. phys .",
    "a * 34 * , 10783 ( 1999 ) ; j. p. bouchaud ,  power - laws in economy and finance : some ideas from physics \" , cond - mat/0008103 , and references therein j.p .",
    "garrahan , e. moro , d. sherrington , quantitative finance * 1 * , 246 ( 2001 ) ; r. khn and p. neu , manuscript in preparation ."
  ],
  "abstract_text": [
    "<S> a value - at - risk based model is proposed to compute the adequate equity capital necessary to cover potential losses due to operational risks , such as human and system process failures , in banking organizations . </S>",
    "<S> exploring the analogy to a lattice gas model from physics , correlations between sequential failures are modeled by as functionally defined , heterogeneous couplings between mutually supportive processes . </S>",
    "<S> in contrast to traditional risk models for market and credit risk , where correlations are described by the covariance of gaussian processes , the dynamics of the model shows collective phenomena such as bursts and avalanches of process failures . </S>"
  ]
}