{
  "article_text": [
    "there has been increasing interest in the properties of networks with memory . in the field of complex networks , memory is emerging as a new direction of study @xcite in order to understand the properties of dynamical networks .",
    "memristors in particular , have been attracting a renewed interest , as these devices resemble swarms in solving certain optimization problems @xcite .",
    "memristors are 2-port devices which behave as resistances that changes its value as a function of current or voltage .",
    "this type of memory is a common feature in many physical systems @xcite and thus general .",
    "moreover , memristors have been proposed as building blocks for unconventional ( or neuromorphic ) computing @xcite , given that they are becoming easier to fabricate @xcite .",
    "memristive networks can serve also as simple models for further understanding the collective behavior and learning abilities of many biological systems @xcite , including the brain @xcite and its critical aspects .",
    "the behavior of memristors is in spirit similar also to slime molds @xcite .    in a recent paper @xcite ,",
    "a derivation of a `` inner memory '' differential equation for purely memristive networks was provided , and it has been shown that several phenomena can be derived using such equation .",
    "this includes an approximate solution for alternate - current ( ac ) circuit , and a study of slow relaxation for memristive systems @xcite in the direct - current ( dc ) case . in order to derive such equation ,",
    "several graph - theoretic tools were borrowed , which inherently showed the underlying freedom in describing the dynamics of the memory . in this paper , we further study such equation and the underlying properties . as an example",
    ", we provide an exact solution for the simple case of series of memristors in the _ mean field _ approximation , showing that it matches with the solution derived by simple circuit analysis .",
    "specifically , we study the backbones of the dynamics : how the cohomology structure typical of linear circuits is inherited by memristive networks and what this implies for the effective amount of memory .",
    "also , we study the properties of the equation , its symmetries and variable transformation , and the weak and strong coupling properties .",
    "we prove that oscillations occur around fixed points of the dynamics if and only if there are mixtures of passive and active elements .",
    "to conclude , we show in the limit of weak coupling memristors perform  learning \" : we cast the dynamics of the internal memory as constrained gradient descent , and provide the functional being minimized .",
    "conclusions follow .",
    "in this section we recall the basics of graph theory and cohomology which were used to derive the consistent internal memory equations of @xcite , which is the basis of the analysis which follows .",
    "first of all , we consider a graph @xmath0 ( a circuit ) with @xmath1 nodes and @xmath2 edges ( memristors ) which describes the connectivity properties .",
    "it is standard to start by choosing an orientation @xmath3 for the currents flowing in the circuit , but as we will see later the final equation is independent from this choice . in order for the graph to represent a circuit ,",
    "the graph must be connected and the degree of each node @xmath4 satisfies @xmath5 , meaning each node is attached to at least two edges .    for the sake of clarity , we use latin indices for the edges , and greek indices for the nodes ; greek indices with tildes will represent instead cycles on the graph . for instance , we will introduce a potential vector @xmath6 , and for each edge a current @xmath7 .    once an orientation @xmath3 has been assigned , and a set of oriented cycles is obtained , we can introduce two key matrices : the directed incidence matrix @xmath8 , which is a matrix of size @xmath9 , and the cycle matrix @xmath10 , which is of size @xmath11 , where @xmath12 is the number of cycles of the graph .",
    "the incidence matrix has an entry @xmath13 if an ( oriented ) edge is leaving a node , @xmath14 if it is incoming to a node , and @xmath15 otherwise .",
    "similarly , the cycle matrice has an entry @xmath13 if the directed edge is in the same direction of a chosen cycle , @xmath14 if it shares the same orientation , and @xmath15 if it does not belong to that cycle . in what follows",
    ", we will assume that an orientation for the cycles and the currents have been chosen , as in fig .",
    "[ fig : orchoice ] .",
    "one thing that should be stressed is that @xmath16 and @xmath17 are very different operators : the former is usually called _ laplacian _ and is a matrix which acts on the set of _ nodes _ , meanwhile the latter is usually called _ edge laplacian _ @xcite and acts on the set of _ edges _ .",
    "both operators are however positively defined , as @xmath18 , and in the other case @xmath19 .",
    "the conservation of the current at each node , the _ first kirchhoff law _ or current law ( kcl ) , can be written in terms of the incidence matrix @xmath20 as @xmath21 .",
    "this set of equations contains a superfluous one .",
    "thus , in order for @xmath20 to have the linear independence of the rows , it is common practice to remove one of the rows and work with the _ reduced incidence matrix _ @xmath22 . in the following",
    ", we will thus consider only results derived with this matrix rather than the full one and remove the @xmath23 . in the language of _ discrete cohomology _",
    "@xcite , the incidence matrix represents the _ boundary _ operator @xmath24 .",
    "such representation exists for any oriented graph .",
    "the incidence matrix can also be used to calculate the voltage applied to each resistor from the potential at the nodes . given a potential vector based on the nodes @xmath25 ,",
    "the vector of voltages applied to each resistor can be written as @xmath26 , where @xmath27 represents the matrix transpose .",
    "analogously , there exist a _",
    "co - boundary operator _",
    "@xmath28 which is represented by the cycle matrix @xmath29 . as for the case of the incidence matrix ,",
    "also in the case of the cycle matrix one has to consider the reduced cycle matrix @xmath30 when one row has been removed . the _ second kirchhoff law _ , or voltage law ( kvl ) can be expressed mathematically as @xmath31 .",
    "this implies that of the voltage on each resistor ( or _ mesh _ in circuits ) must be zero , which can also be written as @xmath32 .",
    "since this is true for any potential @xmath33 , it is possible to write this alternatively as @xmath34 , which implies that in general @xmath35 .",
    "effectively , this equation represents the conservation of energy , which in the language of circuits is called _ tellegen s",
    "theorem_. this fact will be important later as it establishes a duality between projectors in the memristor network dynamics .",
    "there are two ways to construct the basis of a circuits : either by using the tree and cycles decomposition , or alternatively by using the chords or the co - chords decomposition @xcite . here",
    "we consider the standard co - chords decomposition , which is based on _ spanning trees_.",
    "let us first introduce a spanning tree @xmath36 , or _",
    "co - chords _ , and the set of edges of the graph not included in the tree , _ or chords _ , are given by @xmath37 .",
    "if there is more than one tree , each tree has the same cardinality .",
    "each chord element , @xmath37 , can be assigned to a cycle , called _ fundamental loop_. the number of fundamental loops is constrained by the topology of the circuit , and is given by @xmath38 : this is the number of edges minus the cardinality of the tree , @xmath39 .",
    "we resort to the appendix of @xcite for all the details of the derivation of the equation for the internal memory dynamics .",
    "the important fact is however the fact that using the kirchhoff laws , one can derive the following well - known exact solution for the circuit , based only on the source vector @xmath40 , the resistance matrix @xmath41 and the cycle matrix : @xmath42 for the case of linear memristors , i.e. @xmath43 , we have shown in @xcite that it is possible to rearrange the right hand side in terms of only a projector operator @xmath44 on the space of cycles .",
    "this is done by carefully inverting only matrices which are invertible .",
    "since @xmath29 is a reduced incidence matrix , then @xmath45 ( which is usually called _ augmented cyclic matrix _ )",
    "is always invertible for non - zero resistances @xcite .",
    "specifically , in @xcite the following two exact equations for the internal memory of a purely memristive circuit were derived : @xmath46 where @xmath47 and @xmath48 .",
    "@xmath49 is a diagonal matrix made of only @xmath50 . by definition ,",
    "barred quantities ( @xmath51 , @xmath52 ) are multiplied by @xmath49 .",
    "these equations are true in the _ bulk _ , e.g. when all the memristors are not close to @xmath53 , and has been derived assuming @xmath54 invertible .",
    "this is in general not true close to the lower boundary @xmath55 , but in simulations we have observed a smooth behavior also in such case @xcite .",
    "this implies that together with the equations ( [ eq : exacteqc ] ) and ( [ eq : exacteqv ] ) , one should impose the constraint @xmath56 to have a faithful dynamics . of course , these equations describe a specific class of memristors and are by no means general .",
    "these can however be interpreted as first approximations for the real dynamics .",
    "elements corresponding to @xmath57 s are passive components , meanwhile elements corresponding to the @xmath13 s are active components .",
    "since @xmath29 and @xmath58 are dual , i.e. they satisfy @xmath59 , it is easy to show that for @xmath60 ( e.g. proportional to the identity ) , one has that @xmath61 given the fact that @xmath20 is numerically much easier to calculate than @xmath29 , this turns useful when performing simulations .",
    "these equations may seem quite obscure at first .",
    "below we provide an example where calculations can be made without approximations to show that known results can be re - derived .",
    "+   + * a specific example : exact mean - field solution for memristors series . *    in order to see how the equations above can be applied , let us consider a simple enough case in which calculations can be performed analytically , and which are nonetheless not trivial : this is the case of a series of @xmath1 current - controlled memristors , for which in principle the eqn .",
    "( [ eq : exacteqc ] ) would not be needed . in this case",
    "the use of the graph - theoretical machinery is an overkill , which however provides insights in the meaning of the operator @xmath62 .    for a series of memristors ,",
    "the adjacency matrix is a toeplix matrix .",
    "thus , the projector operator on the incidence matrix can be written as @xmath63 , where @xmath64 $ ] .",
    "thus , @xmath65 . we can thus calculate the inverse @xmath66 if @xmath54 has the same entries .",
    "in fact , we can use in this case the sherman - morrison identity @xcite : one has that @xmath67 , thus if all the memristors have the same initial value , one has @xmath68 .",
    "we can calculate the rhs of the dynamical equations exactly : @xmath69 we note that @xmath70 is a projector , which implies : @xmath71 eqn .",
    "( [ eq : finalex ] ) is that same that would be obtained for a series of @xmath1 identical memristors if there is only one source .",
    "using the fact that we are approximating all the memristors with the same parameter , we have use the rule for the series of resistors , to obtain : @xmath72 and using the fact that @xmath73 we obtain the final equation ( [ eq : finalex ] ) . note that if @xmath74 is not uniform ( i.e. when we do not use the mean field approximation ) , then it is not possible to neglect the correlation arising from the denseness of @xmath62 .",
    "network constraints are fundamental in order to make precise the notion of _ effective memory _ in memristive networks . in order to see this ,",
    "let us look at the constraints and how these are connected with the internal memory .",
    "the analysis which follows below applies to the case of a _ linear _ relationship between the memristor memory and either voltage or current .",
    "we consider first two specific models for the evolution of the internal memory in the  bulk \" ( far from the boundaries ) .",
    "these are @xmath75 and @xmath76 which are two different types of memristors considered in the literature and @xmath77 and @xmath78 are simply constants . in both of eqns .",
    "( [ eq : currentcontr ] ) and ( [ eq : voltagecontr ] ) one can uses the network constraints in order to obtain information on the exact amount of memory stored by the circuit .",
    "the kirchhoff current constraint can be written as : @xmath79 and if we now combine the internal memory equation for the current - controlled memristors of eqn .",
    "( [ eq : currentcontr ] ) , it is easy to see that : @xmath80 at this point we can use the the tree and co - tree splitting to write the following linear relationship : @xmath81 and thus we obtain @xmath82 this equation can be now integrated in time , to obtain , for current - controlled memristors : @xmath83 before providing an interpretation , we want to first show that such analysis applies also in the case of voltage controlled memristors . in this case , we consider the constraint : we have : @xmath84 if we use the tree - chords splitting again , we have @xmath85 which after fixing the elements of the fundamental chords , can be written as : @xmath86 and which leads to , after integration over time , the following equation for voltage - controlled memristors : @xmath87 both equations ( [ eq : memorycurrent ] ) and ( [ eq : memoryvoltage ] ) are representations of the constraints of the network . in both cases ,",
    "we can write the equation for the internal memory as : @xmath88 where @xmath89 is a linear operator which depends on the chosen spanning tree , and",
    "thus for the whole memory as @xmath90 this is general , and it is valid both for current - controlled and voltage - controlled memristors , as long as these are linear in first approximation .",
    "experimentally this is often not the case , but it may work as a first approximation .",
    "it is easy to see that eqn .",
    "( [ eq : affinerel ] ) establishes an affine relationship between the internal memory and and a subspace of chord memory .",
    "we can thus introduce the concept of _ effective _ memory of a memristive circuit @xmath0 , given by : @xmath91 where @xmath92 is the number of memristive elements and @xmath93 represents the cardinality of a maximal spanning tree in the circuit @xmath0 . since @xmath2 can grow as the number of nodes of the circuit square , meanwhile @xmath93 grows linearly in the number of nodes , this implies that denser circuits can effectively overcome the limitation of a smaller internal capacity .",
    "we note that the effective capacity of eqn .",
    "( [ eq : effcap ] ) is well defined : this number is independent from the choice of the spanning tree , and thus is a relevant physical quantity , meanwhile eqn .",
    "( [ eq : affinerel ] ) implicitly depends on the choice of the spanning tree .",
    "specifically , the number of ways in which eqn .",
    "( [ eq : affinerel ] ) can be written depends on the number of spanning trees of the circuit .",
    "as simple as such argument may look , it shows that the effective memory in a memristive circuit lives on a submanifold of the internal memory .",
    "once a spanning tree has been chosen , and the dynamical equations derived , the projection operator ensures that such sub manifold is protected and well defined in the dynamics .",
    "in this section we study the behavior of the dynamics in the weak and strong non - linear regimes .",
    "there are at least two regimes that we would like here to describe : @xmath94 , which we call _ weak coupling _ , and @xmath95 , we we call _ strong coupling_. we focus here on the case of current - controlled memristors , but a similar analysis applies also to voltage - controlled memristors .",
    "these two regimes distinguish the behavior of the operator @xmath96 which we will now try to make precise from an operatorial point of view in both limits .",
    "in the weak coupling regime , i.e. @xmath97 , the following taylor expansion of the operator applies : @xmath98 this regime will studied in detail in sec .",
    "[ sec : learning ] , showing that we can identify the  learning \" abilities . in the strong coupling regime instead , it does make sense to write @xmath99    we note that @xmath100 for large @xmath101 , can be seen as the tychonov regularization of the inverse of the operator @xmath102 .",
    "equations ( [ eq : exacteqc ] ) and ( [ eq : exacteqv ] ) were derived with the assumption that @xmath54 is an invertible ( diagonal ) matrix , i.e. that no memristor reached the @xmath103 state .",
    "we can thus write @xmath104 and study for the time being how does @xmath105 behave .",
    "the tychonov regularization converges to the moore - penrose pseudo - inverse of @xmath102 , implying that @xmath106 .",
    "thus , if we multiply the equation on the lhs by @xmath102 , we can write : @xmath107 and thus derive the conservation law , in the limit @xmath108 : @xmath109=0,\\ ] ] where @xmath110 means the vector with all the elements squared .",
    "in general , it is easy to see that this equation is true up to an arbitrary vector @xmath111 , obtaining @xmath112 which is the final conservation law in this limit , similar to what observed in @xcite .",
    "( [ eq : conslaw ] ) is true only in the approximation in which the dynamics lies in the bulk , i.e. @xmath113 .",
    "the equations ( [ eq : currentcontr ] ) and ( [ eq : voltagecontr ] ) satisfy several symmetries which we would like here to describe in detail .",
    "let us first start by saying that the dynamical equations obtained depend on the choice of a spanning tree to begin with : the operator @xmath51 should in fact be more correctly written as @xmath114 to be precise .",
    "the results we obtain do not depend on the choice of the tree @xmath36 , but the equations do .",
    "this is an example of a gauge symmetry over @xmath115 , the real line .",
    "in addition , the equations depend on the choice of a direction of the currents on the circuit .",
    "this symmetry is however easier to see : under a change of direction , @xmath116 .",
    "however , we note that @xmath51 is independent from this transformation .",
    "another symmetry of the dynamics is given by a change of active components in passive components and viceversa .",
    "formally , this implies @xmath117 : again , since @xmath49 appears twice in @xmath51 , the dynamics is unchanged .",
    "another symmetry to be expected is the transformation @xmath118 and @xmath119 , which reverses voltages and time .",
    "let us now consider a linear transformation of the @xmath120 involved , i.e. @xmath121 , where o is an invertible matrix .",
    "in this case , @xmath122 .",
    "in order to see this , let us look at how the equation transforms under a change of basis for @xmath54 .",
    "we first note that @xmath123 .",
    "thus : @xmath124 where we defined @xmath125 and @xmath126 .",
    "this shows for instance that if we choose a basis in which @xmath127 is diagonal , then @xmath51 will not be diagonal .",
    "if on the other hand we choose a basis in which @xmath51 is diagonal , as a result @xmath54 will likely not be diagonal , unless @xmath51 and @xmath54 commute .",
    "one thing that needs to be stressed , is that @xmath128 .",
    "if however @xmath129 is a permutation , then @xmath54 will still be diagonal with the elements on the diagonal permuted accordingly .",
    "one feature which become apparent in eqns .",
    "( [ eq : currentcontr ] ) and ( [ eq : voltagecontr ] ) , is the fact that not all components of the source vector affect the evolution of the internal memory .",
    "in fact , we could add an arbitrary vector @xmath130 to @xmath52 : since @xmath52 enters the equation as @xmath131 , one automatically has that @xmath132 this is a result of the network constraints , or alternatively interpreted as a gauge freedom .",
    "thus , we can easily decompose @xmath52 using the projector , as @xmath133 and automatically the component projected on the _",
    "_ ker__@xmath134 would not affect the dynamics .",
    "when @xmath135 , @xmath51 is not symmetric .",
    "this implies that @xmath51 can never be diagonalized in eqn .",
    "( [ eq : transf ] ) .",
    "as we will see below , this has important consequences .",
    "one important thing to stress is the dual role played by the cycle matrix @xmath29 and the incidence matrix @xmath58 . in eqn .",
    "( [ eq : dual ] ) we have seen that since @xmath136 , one can calculate the projector operator @xmath62      in this section we wish to show that oscillations ( complex eigenvalues ) arise only if @xmath51 is not symmetric .",
    "first we work out a simple exercise .",
    "in fact , although @xmath137 is not a symmetric matrix , it has always real eigenvalues . in order to see this",
    ", we note that the eigenvalues of any matrix product @xmath102 has the same eigenvalues of the matrix @xmath138 . in this case , since @xmath54 is diagonal and positive , the square root of the matrix is simply the square root of the diagonal elements .",
    "first we note that the eigenvalues of any matrix @xmath139 for any invertible matrix @xmath140 are the same as those of @xmath2 .",
    "if @xmath51 is symmetric and real , then @xmath138 has real eigenvalues as this is a symmetric matrix .",
    "this implies that also @xmath141 has real eigenvalues .",
    "another way to see that the matrix @xmath142 has real eigenvalues , is the fact that @xmath143 .",
    "since the inverse of a symmetric matrix is symmetric , again its eigenvalues must be real .",
    "since @xmath51 is invariant under the transformation @xmath117 , such analysis applies also for the inverse system , in which the number of passive and active component has been exchanged .",
    "on the other hand , this is _ not _ true if @xmath51 is not symmetric , and thus @xmath102 can have pairs of complex eigenvalues .",
    "let us now assume that the spectrum of @xmath102 is @xmath144 .",
    "then , the spectrum of @xmath145 will be of the form    @xmath146    and thus still possibly contain pairs of complex eigenvalues .",
    "a similar argument now will turn useful to show that in the case in which @xmath51 is symmetric ( i.e. no mixture of active / passive components ) there can not be fixed points dressed by oscillations .",
    "let us now consider the dynamics close to a fixed point @xmath147 , and linearize the dynamics as @xmath148 the lyapunov matrix @xmath149 is given by @xmath150 we have @xmath151 , and using the formula @xmath152 , we obtain :    @xmath153    which is a rather complicated expression .",
    "we wish to understand now what are the conditions such that the matrix @xmath149 has only real eigenvalues , in which case no oscillations occur .",
    "this can be done again by showing that the matrix is similar to an hermitian operator .",
    "first we note that the matrix @xmath149 in eqn .",
    "( [ eq : lyapunov2 ] ) is again of the form : @xmath154 where @xmath2 is a full matrix , meanwhile @xmath140 is diagonal .",
    "we do not consider any restriction on the elements of @xmath140 : these either positive or negative without affective what follows .",
    "the diagonal elements of @xmath140 are the vector elements @xmath155 and are real , meanwhile @xmath156 .",
    "first , we write : @xmath157 where we defined @xmath158 and where with @xmath159 we mean similarity .",
    "now we note that if @xmath140 is diagonal and contains only positive and negative elements , then @xmath160 .",
    "this is important because we can now study whether eqn .",
    "( [ eq : lyapunov3 ] ) is hermitian rather than symmetric .",
    "hermitian matrices have real eigenvalues , replacing the transposition operation with the transposition and conjugation .",
    "next we observe that @xmath161 is symmetric if and only if also @xmath162 is symmetric . to see this",
    ", we note that @xmath163 ; we note that the identity @xmath164 holds due to the fact that @xmath162 commutes with itself .",
    "this confirms what we have stated above . since @xmath158 , @xmath162 is symmetric if and only if also @xmath51 is symmetric .",
    "all these facts together show that @xmath149 is similar to a hermitian operator ( and thus with real eigenvalues ) if @xmath51 is a symmetric matrix .",
    "this proves what we had anticipated , e.g. the fact that only for mixtures of active and passive components periodicity and oscillating phenomena emerge in the dynamics .",
    "anagolously to the _ barkhausen criterion _ for circuits with feedback loops @xcite , this condition is necessary but not sufficient , i.e. there could be memristive circuits with passive / active elements mixtures which do not have oscillations .",
    "a precise statement which connects memristors to an optimization problem will be made in this section .",
    "we want first to consider one specific case : the mean field problem in which all memristors have symmetrical interactions , and the memory elements @xmath165 and for the case of only passive ( or active ) components . in this case",
    ", the factor @xmath166 can be interpreted as a tentative linear regression .",
    "let us in fact assume that we want to solve the equation : @xmath167 such equation can be solved by means of a tychonov regularization : @xmath168 gives @xmath169 and since @xmath62 is symmetric and a projector , one has @xmath170 this result might give the feeling that memristive systems are performing a specific type of optimization .",
    "however , in the general case , things are slightly more complicated and at the moment we do not have a full answer of what type of optimization these systems are performing . notwithstanding these difficulties , there is something that we can say in weak coupling regime , @xmath171 . here",
    "we want to interpret eqn .",
    "( [ eq : exacteqc ] ) as a specific dynamics , which is of interest to machine learning , and in general to optimization problems .",
    "specifically , let us consider the following time - discretized dynamics : @xmath172 which , in the approximation @xmath171 can be written as @xmath173 and using the fact that if @xmath51 is projector , then one can use the identity @xmath174 from which we can derive : @xmath175 and that , for @xmath176 , we can write as : @xmath177 where we defined @xmath178 and @xmath179 . now that we have written the dynamical equation in this fashion , it is easy to realize that the dynamics is effectively a gradient descent procedure for a constrained optimization problem .",
    "we claim that such dynamics performs a constrained optimization of the type : @xmath180 where @xmath181 . in order to see this ,",
    "let us consider rosen s gradient projection method to solve this optimization problem @xcite .",
    "the basic assumption of the gradient projection method is that @xmath182 lies in the tangent subspace of the constraints .",
    "we consider first a general update rule given by : @xmath183 where both @xmath184 and @xmath185 satisfy the linear constraint , and which depends on a vector @xmath186 .",
    "we ask for the vector @xmath186 which is in the steepest descent direction , and which satisfies @xmath187 .",
    "we can pose this problem as an optimization of the form @xmath188 we now follow the procedure of @xcite .",
    "we introduce the lagrange multipliers @xmath189 and @xmath190 , and the lagrangian : @xmath191 the euler - lagrange equations for @xmath186 are given by : @xmath192 if we multiply this equation by @xmath58 on the left hand side , we obtain the equation : @xmath193 from which we can invert for the lagrange multiplier @xmath189 : @xmath194 and thus using eqn .",
    "( [ eq : condition ] ) we finally obtain : @xmath195 such vector can be now re - inserted into the dynamical equation , which is now interpreted as a constrained gradient descent : @xmath196 the dynamics of eqn .",
    "( [ eq : gradientdescent ] ) is represented in fig .",
    "( [ fig : projection ] ) .",
    "it is easy at this point to identify , a posteriori , every element in this equation .",
    "the projector operator is given by @xmath51 .",
    "in the case in which only active or only passive element are present , we can use the duality between the loop matrix @xmath29 and the incidence matrix @xmath20 , to write @xmath197 .",
    "thus , the constraint @xmath198 can be interpreted exactly as the conservation of memory in the circuit , and @xmath20 promptly identified as the incidence matrix",
    ". the constant @xmath199 is also obtained , and all it is left to us to do is to identify @xmath200 , from which after a simple integration we obtain the functional @xmath201 .",
    "this interpretation is key to identify memristive networks as  learning \" : gradient descent is in fact one of main training algorithms in machine learning and optimization , and in particular in neural networks .",
    "such connection establishes memristive circuits as the perfect neuromorphic devices .",
    "of course , this is not the first time this was suggested @xcite , but here we have provided further evidence of the above . for instance , in @xcite it was shown that in the case of a memristor series one can use the equations for learning . using the fact that in the case of a series @xmath202 , and that @xmath203 , we can show that the functional being minimized was @xmath204 .",
    "this is interesting also for other reasons .",
    "first of all , it makes precise the notion of information overhead for the specific case of purely memristive systems . in a recent paper ,",
    "di ventra and traversa @xcite put forward the suggestion that the graph topology is part of the optimization process .",
    "in fact , the function being optimized we just proved to be @xmath205 in which both the external voltage sources _ and _ the network topology ( through @xmath62 ) appears .        for the technological application of such statement ,",
    "this poses the problem of engineering @xmath62 and choose @xmath206 in order to minimize the function of which one desires to find a minimum .",
    "in the present paper we have made several steps towards understanding the collective behavior of memristive systems using a recently derived equation for the internal memory of a purely memristive system .",
    "memristors are interesting devices with a very rich dynamics . even for the simpler memristor type ( linear ) , non - linear phenomena emerge .",
    "such equation establishes that the amount of non - linearity is controlled by a single parameter , which is the ratio between the resistance in the insulating phase and the resistance in the conducting phase of the memristor .",
    "here we focused on the technical aspects of the derived equation and tried to connect the dynamics of the internal memory to a more standard machine learning approach .",
    "specifically , we have described in detail the symmetries of these equations , and analyzed the difference between purely passive ( or active ) systems and mixtures .",
    "specifically , we have proven that close to a fixed point only in the case of mixtures the jacobian can posses complex eigenvalues .",
    "this was done by showing that the jacobian matrix is similar to a hermitian operator when the projector on the space of cycles is an orthogonal matrix , which occurs only for pure systems ( only active or passive ) .",
    "this implies that oscillating stable or unstable fixed points can not occur for purely memristive systems , unless these injected through the external sources .",
    "we have discussed also what type of optimization purely memristive systems are performing using the internal memory equation .",
    "as it turns out , in the limit of weak coupling ( e.g. when the nonlinearity is weak ) , the dynamical equation can be casted as ( linear ) constrained gradient descent equation @xcite .",
    "the functional being minimized was found to be a combination of sources and the projection operator on the space of cycles , and is quadratic in the internal memory .",
    "this is true only in the weakly nonlinear regime , but is in line with what observed in @xcite .",
    "more complex optimizations require the introduction of other circuital elements @xcite which we did not consider .",
    "however , as expected , the constraint is related to the conservation of currents at the nodes and thus enforces a network ( topological ) property of the circuit .     + * acknowledgements .",
    "* we would like to thank fabio lorenzo traversa and massimiliano di ventra for various discussions concerning memristors .",
    "i would also like to thank the anonymous editors of a previous paper for saying that such equations were not useful .      c. l. vestergaard , m. genouis , a. barrat , how memory generates heterogeneous dynamics in temporal networks , phys .",
    "e 90 , 042805 ( 2014 ) f. caravelli , a. hamma , m. di ventra , scale - free networks as an epiphenomenon of memory , epl , 109 , 2 ( 2015 ) f. caravelli , trajectories entropy in dynamical graphs with memory , front . robot .",
    "ai 3 , 18 ( 2016 ) , arxiv:1511.07135 m. dorigo , l. m. gambardella , ant colonies for the traveling salesman problem , biosystems 43 , 2 , p 73 - 81 ( 1997 )      f. l. traversa , c. ramella , f. bonani , m. di ventra , memcomputing np - complete problems in polynomial time using polynomial resources and collective states , science advances , vo . 1 , no . 6 , pag e1500031 ( 2015 )    f. l. traversa , m. di ventra , polynomial - time solution of prime factorization and np - hard problems with digital memcomputing machines , arxiv:1512.05064 y.  v. pershin , m. di  ventra .",
    "memory effects in complex materials and nanoscale systems , , 60:145227 ( 2011 )                                              r. c. wilson , f. aziz , e. r. hancock , eigenfunctions of the edge - based laplacian on a graph , arxiv:1302.3433 d. zelazo , m. mesbahi , edge agreement : graph - theoretic performance bounds and passivity analysis , ieee trans . on autom .",
    "56 , n. 3 ( 2011 )      j. sherman , j. w. morrison , adjustment of an inverse matrix corresponding to changes in the elements of a given column or a given row of the original matrix , annals of mathematical statistics .",
    "20 : 621 ( 1949 )"
  ],
  "abstract_text": [
    "<S> we discuss the properties of the dynamics of purely memristive circuits using a recently derived consistent equation for the internal memory of the circuit . </S>",
    "<S> in particular , we show that the amount of memory in a memristive circuit is constrained by the conservation laws , and that the dynamics preserves these symmetry by means of a projection on this subspace . </S>",
    "<S> we obtain these results both for current and voltage controlled linear memristors . </S>",
    "<S> moreover , we discuss other symmetries of the dynamics under various transformations , and study the weak and strong non - linear regimes . in the strong regime , we derive a constrained conservation law for the internal memory . in particular , we are able to show that for the case of purely passive or active systems , the eigenvalues of the jacobian are always real , implying that oscillations can emerge only for mixtures . </S>",
    "<S> our last result concerns the weak non - linear regime , showing that the internal memory dynamics can be interpreted as a constrained gradient descent , and provide the functional being minimized . </S>",
    "<S> these results provide another direct connection between memristors and learning . </S>"
  ]
}